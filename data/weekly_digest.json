{
  "metadata": {
    "generated_at": "2026-02-01T23:27:06Z",
    "week_start": "2026-01-25",
    "week_end": "2026-02-01",
    "version": "3.1.0"
  },
  "summary": {
    "total_papers": 3,
    "papers_by_track": {
      "spc": 0,
      "exp_design": 1,
      "reliability": 2
    }
  },
  "synthesis": "If you’ve ever wondered why a “great loss curve” can still produce a lousy decision rule—or why turning a whole system “on vs. off” in an experiment can stubbornly refuse to average out—this week had a nice theme: **local errors and local interference matter**, and our usual global metrics can be wildly overconfident.\n\n### The hook: cross-entropy can lie to you (badly) in diffusion guidance\n\nSahu et al. take aim at a quietly common assumption in classifier-guided diffusion: *if my time-dependent classifier has low cross-entropy/KL error, then the guidance gradient I’m using must be accurate.* Not so.\n\nThey construct explicit counterexamples where the conditional KL goes to zero while the **guidance vector field error stays constant—or even blows up**. The mechanism is almost too relatable: high-frequency perturbations that barely move the average loss, but wreak havoc on derivatives. If you’re using guidance in a safety-critical or “must-not-drift” setting, that’s the punchline: **optimizing the scalar objective doesn’t necessarily control the thing you actually use at sampling time (a gradient).**\n\nThe good news: under stronger (but clear) smoothness and bounded-support assumptions—think “the classifier behaves nicely and doesn’t hide razor blades in its derivatives”—they prove a bound linking per-time cross-entropy error to **L2 guidance-gradient MSE**, with explicit dependence on dimension, noise level, and class probability. They even argue the ε-dependence is unimprovable. For practitioners, this reads like a spec sheet: if your guidance is unstable at small σₜ, the theory says it’s not surprising.\n\nWhat to do Monday morning? If you’re deploying guided diffusion for inspection image synthesis, data augmentation, or anomaly generation: add **derivative-sensitive validation**. Don’t just track cross-entropy; probe gradient sanity (finite-difference checks, gradient norms vs. σₜ, or stress tests with slight input perturbations).\n\n### Self-supervised ultrasound enhancement that acts like a reliability preprocessor\n\nKhan et al. go after a very practical pain point: ultrasound images are noisy (speckle-ish), blurred (PSF-ish), and you rarely have “clean targets” or calibrated PSFs. Their move is a clever self-supervised loop: synthesize training pairs from real frames using a **physics-guided degradation model** (Gaussian PSF surrogate + noise/complex perturbations), and use **non-local low-rank denoising** to create “clean-like” targets.\n\nModel-wise, they use a Swin-style attention + convolution U-Net hybrid (SC-UNet). The headline isn’t just PSNR/SSIM gains (though they report sizable improvements under heavier noise/blur); it’s that as a plug-in preprocessor, it **improves downstream segmentation Dice** under nasty noise regimes.\n\nFor quality/reliability folks, the interesting framing is: *treat enhancement as upstream risk reduction.* If your acceptance decision, measurement, or segmentation pipeline is brittle to imaging conditions, a self-supervised enhancer that doesn’t need calibration starts to look like a controllable “virtual fixture” for the sensing process.\n\nWorth skepticism: their degradation model uses a Gaussian PSF surrogate—often a pragmatic stand-in, not reality. The segmentation lift helps credibility, but if you’re in a regulated workflow, you’d still want site-specific stress tests (different probes, depths, gain settings).\n\n### DOE for matching: randomize locally, not globally\n\nWang et al. tackle experiments where the “treatment” is a **matching plan** (who gets paired with whom)—think workforce assignment, ride-sharing, patient-provider matching. The snag is *matching interference*: choosing one edge can block others. If you naively randomize between two full matchings, variance can refuse to vanish with N (worst-case constant variance), which is exactly what makes teams lose faith in experimentation.\n\nTheir Alternating Path (AP) design exploits structure in the **disagreement set** (edges present in exactly one matching). For one-to-one matchings, it decomposes into disjoint alternating paths/cycles, then randomizes *within* each component while maintaining feasibility. You get a Horvitz–Thompson estimator with variance that scales like **O(1/N)**, and a minimax-optimal long-component randomization probability converging to **√2−1 ≈ 0.4142**—a delightfully specific number to tape to your monitor.\n\nIf you’re experimenting on assignment systems: this is a concrete blueprint for “variance that actually shrinks” without pretending units are independent.\n\n### The trend to watch\n\nAcross all three: we’re moving from global metrics (overall loss, whole-plan randomization, PSNR alone) toward **local control**—control the gradients you use, randomize where interference lives, validate enhancements by downstream task impact. Question for next week: where in our own QE pipelines are we still trusting a scalar summary when the decision hinges on something derivative, local, or constrained?",
  "papers": [
    {
      "id": "2601.21856v1",
      "title": "Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling",
      "authors": "Shujaat Khan|Syed Muhammad Atif|Jaeyoung Huh|Syed Saad Azhar",
      "submitted": "2026-01-29",
      "track": "reliability",
      "link": "https://arxiv.org/pdf/2601.21856v1"
    },
    {
      "id": "2601.21200v1",
      "title": "Provably Reliable Classifier Guidance through Cross-entropy Error Control",
      "authors": "Sharan Sahu|Arisina Banerjee|Yuchen Wu",
      "submitted": "2026-01-29",
      "track": "reliability",
      "link": "https://arxiv.org/pdf/2601.21200v1"
    },
    {
      "id": "2601.21036v1",
      "title": "Experimental Design for Matching",
      "authors": "Chonghuan Wang",
      "submitted": "2026-01-28",
      "track": "exp_design",
      "link": "https://arxiv.org/pdf/2601.21036v1"
    }
  ]
}
