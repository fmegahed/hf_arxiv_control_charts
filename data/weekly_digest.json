{
  "metadata": {
    "generated_at": "2026-02-09T12:15:02Z",
    "week_start": "2026-02-02",
    "week_end": "2026-02-09",
    "version": "3.1.0"
  },
  "summary": {
    "total_papers": 7,
    "papers_by_track": {
      "spc": 0,
      "exp_design": 2,
      "reliability": 5
    }
  },
  "synthesis": "## QE ArXiv Watch Weekly — Week ending Feb 09, 2026 (7 papers)\n\nIf you’ve ever tried to *optimize* an experiment or a design and watched gradient ascent confidently march into a lousy local optimum… this week had a satisfying theme: people are getting serious about **reliability of the whole loop**—design → inference → decision → optimization—rather than just improving one model metric.\n\n### Designing experiments when the likelihood doesn’t exist (but the simulator does)\n\nKlein et al. tackle Bayesian optimal experimental design (BOED) in the modern setting: you can simulate, but you can’t write down a likelihood. Their key move is to align **expected information gain (EIG)** with the three big simulation-based inference (SBI) paradigms (NLE/NPE/NRE), and then actually make per-trajectory EIG maximization work in practice.\n\nThe practitioner-relevant bit isn’t just “neural density estimators are cool.” It’s their diagnosis that **per-trajectory gradient EIG optimization fails due to local optima**, and the fix is delightfully pragmatic: **multiple parallel restarts + online adaptation + a diversity penalty**. On classic benchmarks (source finding, PK timepoint selection, CES preference), the restart strategy is the difference between “works sometimes” and “reliably competitive,” including reported gains over policy-based BOED (RL-BOED/DAD) in several settings.\n\nIf you’re doing sequential test planning with differentiable simulators (or even approximate differentiability), the message is: don’t throw away per-trajectory optimization—**make it robust with restarts and adaptation**.\n\nWan et al. come at sequential design from a different angle: not “learn parameters well,” but “make **good downstream decisions**.” In predict-then-optimize settings, we often over-measure what improves RMSE while the actual scheduling/allocation decision barely changes. They propose sampling experiments based on **directional uncertainty**—how much plausible models disagree in the *direction* of the cost vector that drives a linear optimizer. The result is a sequential design algorithm that targets **SPO loss** (decision loss) and comes with finite-sample excess-risk bounds.\n\nThe useful mental model: if two models predict different numbers but lead to the same optimal decision, who cares? Their acquisition function tries to focus labels where disagreement would *flip or meaningfully perturb the decision*. That’s immediately relevant for inspection prioritization, dispatch rules, or any “estimate → optimize” pipeline.\n\n### Reliability isn’t just failure probability anymore: it’s explanation stability, surrogate trust, and calibration\n\nSengupta et al. ask an uncomfortable question: when an explainer changes, is it because the system changed—or because the explanation is basically noise? Their **Explanation Reliability Index (ERI)** formalizes stability under perturbations, redundancy, model updates, mild shift, and even time. The benchmark results are a reality check: common explainers can look fine on one axis and fall apart on another (redundant features and evolving checkpoints are especially punishing). For QE teams deploying models into changing processes, ERI-Bench reads like a missing piece of your model monitoring stack: not “is the model accurate,” but “are the explanations *stable enough to act on*?”\n\nOn the classical reliability side, Buckingham et al. go after a hard regime we all care about but rarely optimize well: **extremely rare failures (10⁻⁶ to 10⁻⁸)**. Their Bayesian optimization variants (KG-MR and TS-MR) explicitly optimize *reliability* using importance sampling + quasi-MC so you’re not wasting samples where failure probability is essentially zero. The big takeaway: acquisition functions that “look near the limit state surface” can be tuned to focus on what matters for the *best* nominal design, not an even exploration of the whole boundary.\n\nSong et al. address another reliability bottleneck: simulating nonlinear stochastic dynamics for first-passage failure can be brutally slow. Their probabilistic F2NARX surrogate chops trajectories into windows, compresses with PCA, learns window-to-window dynamics with GP/SGP, then propagates uncertainty with the unscented transform. The headline for practitioners is speed: **orders of magnitude** faster than FEM/MCS while still producing uncertainty envelopes—and they fold in active learning to concentrate new simulations where failure probability is most sensitive.\n\nTwo “adjacent but useful” papers round things out: Shiha et al. propose a new two-parameter lifetime distribution (a specific mixture construction) with flexible hazard shapes and closed-form stress–strength reliability; Wang et al. show how **conformal calibration** can turn quantile regression forests into VaR estimates with finite-sample marginal coverage—less QE, more a reminder that *calibration is a reliability tool*.\n\n### The thread to watch\n\nWe’re seeing a shift from “optimize the model” to **engineer the reliability of the decision pipeline**: robust design optimization (restarts), decision-aware data acquisition (directional uncertainty), calibrated uncertainty (conformal), and stability metrics for explanations (ERI). \n\nQuestion to chew on: in your org, if you had to pick one reliability guarantee to add this quarter—**coverage, decision regret, explanation stability, or rare-event failure probability**—which one would change behavior the most?",
  "papers": [
    {
      "id": "2602.06900v1",
      "title": "Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design",
      "authors": "Samuel Klein|Willie Neiswanger|Daniel Ratner|Michael Kagan|Sean Gasiorowski",
      "submitted": "2026-02-06",
      "track": "exp_design",
      "link": "https://arxiv.org/pdf/2602.06900v1"
    },
    {
      "id": "2602.05340v1",
      "title": "Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach",
      "authors": "Beichen Wan|Mo Liu|Paul Grigas|Zuo-Jun Max Shen",
      "submitted": "2026-02-05",
      "track": "exp_design",
      "link": "https://arxiv.org/pdf/2602.05340v1"
    },
    {
      "id": "2602.05082v1",
      "title": "Reliable Explanations or Random Noise? A Reliability Metric for XAI",
      "authors": "Poushali Sengupta|Sabita Maharjan|Frank Eliassen|Shashi Raj Pandey|Yan Zhang",
      "submitted": "2026-02-04",
      "track": "reliability",
      "link": "https://arxiv.org/pdf/2602.05082v1"
    },
    {
      "id": "2602.02875v1",
      "title": "Shiha Distribution: Statistical Properties and Applications to Reliability Engineering and Environmental Data",
      "authors": "F. A. Shiha",
      "submitted": "2026-02-02",
      "track": "reliability",
      "link": "https://arxiv.org/pdf/2602.02875v1"
    },
    {
      "id": "2602.02432v1",
      "title": "Maximizing Reliability with Bayesian Optimization",
      "authors": "Jack M. Buckingham|Ivo Couckuyt|Juergen Branke",
      "submitted": "2026-02-02",
      "track": "reliability",
      "link": "https://arxiv.org/pdf/2602.02432v1"
    },
    {
      "id": "2602.01929v1",
      "title": "Probabilistic function-on-function nonlinear autoregressive model for emulation and reliability analysis of dynamical systems",
      "authors": "Zhouzhou Song|Marcos A. Valdebenito|Styfen Schär|Stefano Marelli|Bruno Sudret|Matthias G. R. Faes",
      "submitted": "2026-02-02",
      "track": "reliability",
      "link": "https://arxiv.org/pdf/2602.01929v1"
    },
    {
      "id": "2602.01912v1",
      "title": "Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration",
      "authors": "Du-Yi Wang|Guo Liang|Kun Zhang|Qianwen Zhu",
      "submitted": "2026-02-02",
      "track": "reliability",
      "link": "https://arxiv.org/pdf/2602.01912v1"
    }
  ]
}
