<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>QE ArXiv Watch Weekly</title>
    <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    <description>Weekly AI-synthesized digest of quality engineering research from arXiv. Covering Control Charts, Experimental Design, and Reliability Engineering.</description>
    <language>en-us</language>
    <copyright>CC BY 4.0 - QE ArXiv Watch</copyright>
    <managingEditor>noreply@example.com (QE ArXiv Watch)</managingEditor>
    <lastBuildDate>Sun, 01 Feb 2026 23:27:06 +0000</lastBuildDate>
    <ttl>10080</ttl>
    <image>
      <url>https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/www/favicon.svg</url>
      <title>QE ArXiv Watch Weekly</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    </image>
    <atom:link href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/data/weekly_digest.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title>QE ArXiv Watch: Week of January 25 - February 01, 2026</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
      <guid isPermaLink="false">qe-weekly-2026-02-01</guid>
      <pubDate>Sun, 01 Feb 2026 23:27:06 +0000</pubDate>
      <description><![CDATA[
<p>If you’ve ever wondered why a “great loss curve” can still produce a lousy decision rule—or why turning a whole system “on vs. off” in an experiment can stubbornly refuse to average out—this week had a nice theme: <strong>local errors and local interference matter</strong>, and our usual global metrics can be wildly overconfident.</p>

<h4>The hook: cross-entropy can lie to you (badly) in diffusion guidance</h4>

<p>Sahu et al. take aim at a quietly common assumption in classifier-guided diffusion: <em>if my time-dependent classifier has low cross-entropy/KL error, then the guidance gradient I’m using must be accurate.</em> Not so.</p>

<p>They construct explicit counterexamples where the conditional KL goes to zero while the <strong>guidance vector field error stays constant—or even blows up</strong>. The mechanism is almost too relatable: high-frequency perturbations that barely move the average loss, but wreak havoc on derivatives. If you’re using guidance in a safety-critical or “must-not-drift” setting, that’s the punchline: <strong>optimizing the scalar objective doesn’t necessarily control the thing you actually use at sampling time (a gradient).</strong></p>

<p>The good news: under stronger (but clear) smoothness and bounded-support assumptions—think “the classifier behaves nicely and doesn’t hide razor blades in its derivatives”—they prove a bound linking per-time cross-entropy error to <strong>L2 guidance-gradient MSE</strong>, with explicit dependence on dimension, noise level, and class probability. They even argue the ε-dependence is unimprovable. For practitioners, this reads like a spec sheet: if your guidance is unstable at small σₜ, the theory says it’s not surprising.</p>

<p>What to do Monday morning? If you’re deploying guided diffusion for inspection image synthesis, data augmentation, or anomaly generation: add <strong>derivative-sensitive validation</strong>. Don’t just track cross-entropy; probe gradient sanity (finite-difference checks, gradient norms vs. σₜ, or stress tests with slight input perturbations).</p>

<h4>Self-supervised ultrasound enhancement that acts like a reliability preprocessor</h4>

<p>Khan et al. go after a very practical pain point: ultrasound images are noisy (speckle-ish), blurred (PSF-ish), and you rarely have “clean targets” or calibrated PSFs. Their move is a clever self-supervised loop: synthesize training pairs from real frames using a <strong>physics-guided degradation model</strong> (Gaussian PSF surrogate + noise/complex perturbations), and use <strong>non-local low-rank denoising</strong> to create “clean-like” targets.</p>

<p>Model-wise, they use a Swin-style attention + convolution U-Net hybrid (SC-UNet). The headline isn’t just PSNR/SSIM gains (though they report sizable improvements under heavier noise/blur); it’s that as a plug-in preprocessor, it <strong>improves downstream segmentation Dice</strong> under nasty noise regimes.</p>

<p>For quality/reliability folks, the interesting framing is: <em>treat enhancement as upstream risk reduction.</em> If your acceptance decision, measurement, or segmentation pipeline is brittle to imaging conditions, a self-supervised enhancer that doesn’t need calibration starts to look like a controllable “virtual fixture” for the sensing process.</p>

<p>Worth skepticism: their degradation model uses a Gaussian PSF surrogate—often a pragmatic stand-in, not reality. The segmentation lift helps credibility, but if you’re in a regulated workflow, you’d still want site-specific stress tests (different probes, depths, gain settings).</p>

<h4>DOE for matching: randomize locally, not globally</h4>

<p>Wang et al. tackle experiments where the “treatment” is a <strong>matching plan</strong> (who gets paired with whom)—think workforce assignment, ride-sharing, patient-provider matching. The snag is <em>matching interference</em>: choosing one edge can block others. If you naively randomize between two full matchings, variance can refuse to vanish with N (worst-case constant variance), which is exactly what makes teams lose faith in experimentation.</p>

<p>Their Alternating Path (AP) design exploits structure in the <strong>disagreement set</strong> (edges present in exactly one matching). For one-to-one matchings, it decomposes into disjoint alternating paths/cycles, then randomizes <em>within</em> each component while maintaining feasibility. You get a Horvitz–Thompson estimator with variance that scales like <strong>O(1/N)</strong>, and a minimax-optimal long-component randomization probability converging to <strong>√2−1 ≈ 0.4142</strong>—a delightfully specific number to tape to your monitor.</p>

<p>If you’re experimenting on assignment systems: this is a concrete blueprint for “variance that actually shrinks” without pretending units are independent.</p>

<h4>The trend to watch</h4>

<p>Across all three: we’re moving from global metrics (overall loss, whole-plan randomization, PSNR alone) toward <strong>local control</strong>—control the gradients you use, randomize where interference lives, validate enhancements by downstream task impact. Question for next week: where in our own QE pipelines are we still trusting a scalar summary when the decision hinges on something derivative, local, or constrained?</p><h4>Featured Papers This Week</h4><ul><li><strong>Khan et al.</strong>: <a href="https://arxiv.org/pdf/2601.21856v1">Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling</a> <em>(Reliability)</em></li><li><strong>Sahu et al.</strong>: <a href="https://arxiv.org/pdf/2601.21200v1">Provably Reliable Classifier Guidance through Cross-entropy Error Control</a> <em>(Reliability)</em></li><li><strong>Wang et al.</strong>: <a href="https://arxiv.org/pdf/2601.21036v1">Experimental Design for Matching</a> <em>(Experimental Design)</em></li></ul><hr/><p><strong>Explore More:</strong> Visit the <a href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts">QE ArXiv Watch Dashboard</a> to browse all papers with AI summaries, interactive filtering, and paper chat.</p><p style="color: #666; font-size: 0.9em;">This digest is automatically generated every Monday. Questions or feedback? Open an issue on our <a href="https://github.com/fmegahed/arxiv_control_charts">GitHub repository</a>.</p>
]]></description>
    </item>
  </channel>
</rss>

