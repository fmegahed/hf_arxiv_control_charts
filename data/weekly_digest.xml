<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>QE ArXiv Watch Weekly</title>
    <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    <description>Weekly AI-synthesized digest of quality engineering research from arXiv. Covering Control Charts, Experimental Design, and Reliability Engineering.</description>
    <language>en-us</language>
    <copyright>CC BY 4.0 - QE ArXiv Watch</copyright>
    <managingEditor>noreply@example.com (QE ArXiv Watch)</managingEditor>
    <lastBuildDate>Mon, 09 Feb 2026 12:15:02 +0000</lastBuildDate>
    <ttl>10080</ttl>
    <image>
      <url>https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/www/favicon.svg</url>
      <title>QE ArXiv Watch Weekly</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    </image>
    <atom:link href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/data/weekly_digest.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title>QE ArXiv Watch: Week of February 02 - February 09, 2026</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
      <guid isPermaLink="false">qe-weekly-2026-02-09</guid>
      <pubDate>Mon, 09 Feb 2026 12:15:02 +0000</pubDate>
      <description><![CDATA[
<h3>QE ArXiv Watch Weekly — Week ending Feb 09, 2026 (7 papers)</h3>

<p>If you’ve ever tried to <em>optimize</em> an experiment or a design and watched gradient ascent confidently march into a lousy local optimum… this week had a satisfying theme: people are getting serious about <strong>reliability of the whole loop</strong>—design → inference → decision → optimization—rather than just improving one model metric.</p>

<h4>Designing experiments when the likelihood doesn’t exist (but the simulator does)</h4>

<p>Klein et al. tackle Bayesian optimal experimental design (BOED) in the modern setting: you can simulate, but you can’t write down a likelihood. Their key move is to align <strong>expected information gain (EIG)</strong> with the three big simulation-based inference (SBI) paradigms (NLE/NPE/NRE), and then actually make per-trajectory EIG maximization work in practice.</p>

<p>The practitioner-relevant bit isn’t just “neural density estimators are cool.” It’s their diagnosis that <strong>per-trajectory gradient EIG optimization fails due to local optima</strong>, and the fix is delightfully pragmatic: <strong>multiple parallel restarts + online adaptation + a diversity penalty</strong>. On classic benchmarks (source finding, PK timepoint selection, CES preference), the restart strategy is the difference between “works sometimes” and “reliably competitive,” including reported gains over policy-based BOED (RL-BOED/DAD) in several settings.</p>

<p>If you’re doing sequential test planning with differentiable simulators (or even approximate differentiability), the message is: don’t throw away per-trajectory optimization—<strong>make it robust with restarts and adaptation</strong>.</p>

<p>Wan et al. come at sequential design from a different angle: not “learn parameters well,” but “make <strong>good downstream decisions</strong>.” In predict-then-optimize settings, we often over-measure what improves RMSE while the actual scheduling/allocation decision barely changes. They propose sampling experiments based on <strong>directional uncertainty</strong>—how much plausible models disagree in the <em>direction</em> of the cost vector that drives a linear optimizer. The result is a sequential design algorithm that targets <strong>SPO loss</strong> (decision loss) and comes with finite-sample excess-risk bounds.</p>

<p>The useful mental model: if two models predict different numbers but lead to the same optimal decision, who cares? Their acquisition function tries to focus labels where disagreement would <em>flip or meaningfully perturb the decision</em>. That’s immediately relevant for inspection prioritization, dispatch rules, or any “estimate → optimize” pipeline.</p>

<h4>Reliability isn’t just failure probability anymore: it’s explanation stability, surrogate trust, and calibration</h4>

<p>Sengupta et al. ask an uncomfortable question: when an explainer changes, is it because the system changed—or because the explanation is basically noise? Their <strong>Explanation Reliability Index (ERI)</strong> formalizes stability under perturbations, redundancy, model updates, mild shift, and even time. The benchmark results are a reality check: common explainers can look fine on one axis and fall apart on another (redundant features and evolving checkpoints are especially punishing). For QE teams deploying models into changing processes, ERI-Bench reads like a missing piece of your model monitoring stack: not “is the model accurate,” but “are the explanations <em>stable enough to act on</em>?”</p>

<p>On the classical reliability side, Buckingham et al. go after a hard regime we all care about but rarely optimize well: <strong>extremely rare failures (10⁻⁶ to 10⁻⁸)</strong>. Their Bayesian optimization variants (KG-MR and TS-MR) explicitly optimize <em>reliability</em> using importance sampling + quasi-MC so you’re not wasting samples where failure probability is essentially zero. The big takeaway: acquisition functions that “look near the limit state surface” can be tuned to focus on what matters for the <em>best</em> nominal design, not an even exploration of the whole boundary.</p>

<p>Song et al. address another reliability bottleneck: simulating nonlinear stochastic dynamics for first-passage failure can be brutally slow. Their probabilistic F2NARX surrogate chops trajectories into windows, compresses with PCA, learns window-to-window dynamics with GP/SGP, then propagates uncertainty with the unscented transform. The headline for practitioners is speed: <strong>orders of magnitude</strong> faster than FEM/MCS while still producing uncertainty envelopes—and they fold in active learning to concentrate new simulations where failure probability is most sensitive.</p>

<p>Two “adjacent but useful” papers round things out: Shiha et al. propose a new two-parameter lifetime distribution (a specific mixture construction) with flexible hazard shapes and closed-form stress–strength reliability; Wang et al. show how <strong>conformal calibration</strong> can turn quantile regression forests into VaR estimates with finite-sample marginal coverage—less QE, more a reminder that <em>calibration is a reliability tool</em>.</p>

<h4>The thread to watch</h4>

<p>We’re seeing a shift from “optimize the model” to <strong>engineer the reliability of the decision pipeline</strong>: robust design optimization (restarts), decision-aware data acquisition (directional uncertainty), calibrated uncertainty (conformal), and stability metrics for explanations (ERI).</p>

<p>Question to chew on: in your org, if you had to pick one reliability guarantee to add this quarter—<strong>coverage, decision regret, explanation stability, or rare-event failure probability</strong>—which one would change behavior the most?</p><h4>Featured Papers This Week</h4><ul><li><strong>Klein et al.</strong>: <a href="https://arxiv.org/pdf/2602.06900v1">Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design</a> <em>(Experimental Design)</em></li><li><strong>Wan et al.</strong>: <a href="https://arxiv.org/pdf/2602.05340v1">Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach</a> <em>(Experimental Design)</em></li><li><strong>Sengupta et al.</strong>: <a href="https://arxiv.org/pdf/2602.05082v1">Reliable Explanations or Random Noise? A Reliability Metric for XAI</a> <em>(Reliability)</em></li><li><strong>Shiha et al.</strong>: <a href="https://arxiv.org/pdf/2602.02875v1">Shiha Distribution: Statistical Properties and Applications to Reliability Engineering and Environmental Data</a> <em>(Reliability)</em></li><li><strong>Buckingham et al.</strong>: <a href="https://arxiv.org/pdf/2602.02432v1">Maximizing Reliability with Bayesian Optimization</a> <em>(Reliability)</em></li><li><strong>Song et al.</strong>: <a href="https://arxiv.org/pdf/2602.01929v1">Probabilistic function-on-function nonlinear autoregressive model for emulation and reliability analysis of dynamical systems</a> <em>(Reliability)</em></li><li><strong>Wang et al.</strong>: <a href="https://arxiv.org/pdf/2602.01912v1">Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration</a> <em>(Reliability)</em></li></ul><hr/><p><strong>Explore More:</strong> Visit the <a href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts">QE ArXiv Watch Dashboard</a> to browse all papers with AI summaries, interactive filtering, and paper chat.</p><p style="color: #666; font-size: 0.9em;">This digest is automatically generated every Monday. Questions or feedback? Open an issue on our <a href="https://github.com/fmegahed/arxiv_control_charts">GitHub repository</a>.</p>
]]></description>
    </item>
  </channel>
</rss>

