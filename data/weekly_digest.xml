<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>QE ArXiv Watch Weekly</title>
    <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    <description>Weekly AI-synthesized digest of quality engineering research from arXiv. Covering Control Charts, Experimental Design, and Reliability Engineering.</description>
    <language>en-us</language>
    <copyright>CC BY 4.0 - QE ArXiv Watch</copyright>
    <managingEditor>noreply@example.com (QE ArXiv Watch)</managingEditor>
    <lastBuildDate>Sun, 01 Feb 2026 21:32:58 +0000</lastBuildDate>
    <ttl>10080</ttl>
    <image>
      <url>https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/www/favicon.svg</url>
      <title>QE ArXiv Watch Weekly</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    </image>
    <atom:link href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/data/weekly_digest.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title>QE ArXiv Watch: Week of January 25 - February 01, 2026</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
      <guid isPermaLink="false">qe-weekly-2026-02-01</guid>
      <pubDate>Sun, 01 Feb 2026 21:32:58 +0000</pubDate>
      <description><![CDATA[
<p>## This Week in Quality Engineering
This week’s papers share a unifying concern: when complex systems (imaging pipelines, generative models, or market-like matchings) are operated without full observability, what kinds of guarantees still survive? Across domains, the authors move beyond “good average performance” toward design- and theory-driven controls on error, variance, and downstream impact—exactly the kind of rigor quality engineers need when deploying methods into regulated or high-stakes settings.</p><p>## Spotlight: Design-Based Guarantees for Interference—Wang et al. on “Experimental Design for Matching”
Wang et al. tackle a problem that shows up in modern operations (workforce assignment, patient-to-provider matching, logistics pairing): you often have two plausible matching policies, but evaluating them experimentally is hard because changing one edge can ripple through feasibility elsewhere (“matching interference”). Their key contribution is a <strong>design-based randomized experiment</strong> that remains valid under this interference by exploiting the combinatorial structure of where the two matchings disagree.</p><p>Why it matters for practitioners and researchers:</li><li><strong>Actionable experimental protocol</strong>: The Alternating Path Randomized (AP) Design randomizes locally along alternating paths/cycles in the disagreement graph, preserving feasibility while enabling unbiased estimation (via a Horvitz–Thompson estimator).</li><li><strong>Variance that actually shrinks</strong>: Unlike the naive “flip a coin and deploy matching A or B” approach (which can have non-vanishing worst-case variance), AP achieves <strong>overall variance \(O(1/N)\)</strong>—the kind of scaling you need for confident go/no-go decisions.</li><li><strong>A tunable knob with a principled setting</strong>: Under a minimax variance criterion, the optimal long-component randomization probability converges to <strong>\(\sqrt2-1 \approx 0.4142\)</strong>, providing a crisp default when you can’t afford extensive pilot tuning.</p><p>## Research Roundup</p><p>### Control Charts & Statistical Process Monitoring
No new submissions this week.</p><p>### Experimental Design & Response Surface Methods
<strong>Wang et al. (matching experiments under interference)</strong>  
<strong>Methodological advances</strong></li><li>Introduces the <strong>disagreement set</strong> between two predetermined matchings and shows that, for one-to-one matchings, it decomposes uniquely into <strong>disjoint alternating paths and even cycles</strong>. This decomposition is the technical engine that localizes interference and makes randomization feasible.</li><li>Proposes the <strong>AP Design</strong>, which randomizes edge realization sequentially within each alternating component using a single parameter \(p\), while maintaining matching feasibility throughout.</li><li>Provides asymptotic theory tailored to finite populations: a <strong>finite-population CLT</strong> for the Horvitz–Thompson estimator under mixtures of short/long components—important for justifying normal-approximation intervals in real deployments.</p><p><strong>Novel applications/domains</strong></li><li>Demonstrated on <strong>workforce matching</strong> simulations, where AP’s variance drops markedly with problem size while the naive whole-matching switch remains noisy (e.g., variance ~0.898 at n=10 down to ~0.185 at n=50 for AP, versus ~3.09 roughly flat for the naive design).</p><p><strong>Practical takeaways</strong></li><li>If your organization is comparing two assignment/matching policies, avoid global A/B switches when interference is inherent; instead, randomize <strong>within disagreement components</strong> to recover precision.</li><li>The paper’s minimax guidance (\(p \to 0.4142\) for long components) is a pragmatic starting point for implementations where variance control is paramount.</p><p>### Reliability Engineering & Maintenance
This track is light in the classical “time-to-failure/maintenance optimization” sense, but both papers squarely address <strong>reliability of data-driven pipelines</strong>—one in medical imaging preprocessing, the other in theoretical guarantees for guided generative sampling.</p><p><strong>Khan et al. (self-supervised ultrasound enhancement)</strong></li><li><strong>Key methodological advances</strong>: A <em>blind</em> enhancement pipeline that jointly addresses speckle-like noise and PSF blur without clean targets or PSF calibration. Training pairs are synthesized using a <strong>physics-guided degradation model</strong> (Gaussian PSF surrogate blur plus noise/perturbations), while “clean-like” targets are approximated with <strong>non-local low-rank (NLLR)</strong> denoising. The restoration model is a <strong>Swin Convolutional U-Net</strong> that mixes local convolution with shifted-window attention.</li><li><strong>Novel application/domain</strong>: Ultrasound B-mode enhancement positioned explicitly as a <strong>plug-and-play preprocessor</strong> that improves downstream segmentation robustness under varying noise.</li><li><strong>Practical takeaways</strong>: The strongest quality-engineering angle is the <strong>downstream metric lift</strong>: beyond PSNR/SSIM gains, the method increases segmentation Dice on PSFHS, with the largest improvements under severe noise. For teams building inspection/diagnostic pipelines, this supports an evaluation strategy that treats enhancement as a means to <strong>stabilize decision quality</strong>, not just improve image fidelity.</p><p><strong>Sahu et al. (reliable classifier guidance for diffusion models)</strong></li><li><strong>Key methodological advances</strong>: A cautionary, then constructive, theory result. The paper shows <strong>cross-entropy (conditional KL) can go to zero while guidance gradients remain wrong</strong> (even diverging) via high-frequency perturbations (Theorem 3.1). Under smoothness and bounded-support assumptions, it derives an explicit scaling law linking cross-entropy error at each noise level to <strong>L2 guidance-gradient MSE</strong> (Theorem 3.2) and argues the dependence is essentially unimprovable (Theorem 3.3). It then translates guidance/score errors into a <strong>KL bound for discretized guided sampling</strong>, giving step complexity that remains near-linear in dimension (Corollary 3.1).</li><li><strong>Novel application/domain</strong>: Reliability of <em>training objectives as process controls</em> for generative pipelines—relevant wherever guided generation is used for synthetic data, simulation surrogates, or design-space exploration.</li><li><strong>Practical takeaways</strong>: If you use classifier-guided diffusion, <strong>monitoring cross-entropy alone is not a sufficient quality signal</strong> for guidance correctness. The theory points to the need for additional constraints/regularization that enforce the smoothness conditions under which cross-entropy meaningfully controls gradient error.</p><p>## Cross-Cutting Themes
1. <strong>Quality assurance under interference and hidden coupling</strong>: Wang et al.’s matching interference and Khan et al.’s blind PSF/noise setting both show that “local decisions” can have system-wide consequences; robust methods explicitly model (or exploit structure in) these couplings.
2. <strong>From proxy metrics to decision-quality guarantees</strong>: Khan et al. validate enhancement by downstream Dice; Sahu et al. warn that a common proxy objective (cross-entropy) may not control what guidance actually uses (gradients). The message: tie quality metrics to the quantity that drives decisions.
3. <strong>Theory-backed tuning knobs</strong>: Wang et al. deliver an asymptotically optimal \(p\); Sahu et al. provide scaling laws that clarify how error, dimension, and noise level interact—useful for setting tolerances and sampling budgets.</p><p>## Practitioner's Corner</li><li><strong>Evaluating matching/assignment policy changes</strong>: Use Wang et al.’s AP design when A/B testing two feasible matchings is confounded by interference. The design yields variance that shrinks with population size (\(O(1/N)\)) and provides a principled default \(p \approx 0.4142\) for long components.</li><li><strong>Medical/industrial imaging pipelines</strong>: Khan et al. support deploying enhancement as a <em>reliability layer</em>—not just prettier images—because improvements persist in downstream segmentation under severe noise/blur. If you manage inspection or diagnostic workflows, replicate their “preprocess → task metric” validation pattern.</li><li><strong>Governance for generative models</strong>: Sahu et al. imply that model acceptance criteria should include checks beyond training loss (e.g., gradient-field sanity checks or smoothness-enforcing constraints) when guidance gradients directly steer sampling.</p><p>## Looking Ahead
Two gaps stand out: (i) bridging these reliability results into <strong>standard QC tooling</strong> (e.g., how to create operational control limits for guidance-gradient error or segmentation robustness), and (ii) extending interference-aware experimental designs like Wang et al.’s to <strong>multi-policy</strong> comparisons and adaptive experimentation. Expect more work on “quality of decision pipelines” where the target is stable downstream performance, not just upstream fit.</p><p>Explore the full dashboard to dive into the proofs, architectures, and experimental setups—and to spot which of these ideas can be piloted in your own quality systems this quarter.</p><h4>Featured Papers This Week</h4><ul><li><strong>Khan et al.</strong>: <a href="https://arxiv.org/pdf/2601.21856v1">Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling</a> <em>(Reliability)</em></li><li><strong>Sahu et al.</strong>: <a href="https://arxiv.org/pdf/2601.21200v1">Provably Reliable Classifier Guidance through Cross-entropy Error Control</a> <em>(Reliability)</em></li><li><strong>Wang et al.</strong>: <a href="https://arxiv.org/pdf/2601.21036v1">Experimental Design for Matching</a> <em>(Experimental Design)</em></li></ul><hr/><p><strong>Explore More:</strong> Visit the <a href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts">QE ArXiv Watch Dashboard</a> to browse all papers with AI summaries, interactive filtering, and paper chat.</p><p style="color: #666; font-size: 0.9em;">This digest is automatically generated every Monday. Questions or feedback? Open an issue on our <a href="https://github.com/fmegahed/arxiv_control_charts">GitHub repository</a>.</p>
]]></description>
    </item>
  </channel>
</rss>

