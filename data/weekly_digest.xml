<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>QE ArXiv Watch Weekly</title>
    <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    <description>Weekly AI-synthesized digest of quality engineering research from arXiv. Covering Control Charts, Experimental Design, and Reliability Engineering.</description>
    <language>en-us</language>
    <copyright>CC BY 4.0 - QE ArXiv Watch</copyright>
    <managingEditor>noreply@example.com (QE ArXiv Watch)</managingEditor>
    <lastBuildDate>Mon, 02 Feb 2026 12:15:58 +0000</lastBuildDate>
    <ttl>10080</ttl>
    <image>
      <url>https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/www/favicon.svg</url>
      <title>QE ArXiv Watch Weekly</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
    </image>
    <atom:link href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts/resolve/main/data/weekly_digest.xml" rel="self" type="application/rss+xml"/>
    <item>
      <title>QE ArXiv Watch: Week of January 26 - February 02, 2026</title>
      <link>https://huggingface.co/spaces/fmegahed/arxiv_control_charts</link>
      <guid isPermaLink="false">qe-weekly-2026-02-02</guid>
      <pubDate>Mon, 02 Feb 2026 12:15:58 +0000</pubDate>
      <description><![CDATA[
<h3>QE ArXiv Watch Weekly — Week ending Feb 02, 2026 (3 papers)</h3>

<p>If you’ve ever trusted a “loss went down” plot and still ended up with a model that behaves oddly in production, you’ll appreciate this week’s best wake-up call: **cross-entropy can go to zero while the <em>gradient you actually use for guidance</em> stays wrong—or even blows up.<strong> That’s not a philosophical nit. It’s a concrete failure mode with proofs and toy experiments you can recreate.</p>

<h4>When “good classifier” doesn’t mean “good guidance” (Sahu et al.)</h4>
Classifier-guided diffusion hinges on a vector field: the gradient of the log-conditional, <code>\nabla \log p(y\mid x_t)</code>. Most training focuses on cross-entropy (conditional KL). The catch: </strong>a small KL gap doesn’t necessarily control gradient error.<strong>

<p>They build explicit high-frequency perturbations where the classifier’s KL error vanishes (think <code>O(1/n)</code>), yet the guidance-gradient error stays <code>\Omega(1)</code>, and in a stronger construction the gradient error grows like <code>\Omega(n)</code> while KL still shrinks. In other words, the classifier can be “right on average” but </strong>locally jagged**, and guidance cares about local geometry.</p>

<p>The constructive part is the remedy: if you assume bounded support and real smoothness/regularity—bounds on gradients and Hessians aligned with the true conditional—then cross-entropy error <code>\varepsilon</code> <em>does</em> imply an <code>L_2</code> guidance-gradient MSE bound scaling roughly like  
<pre><code>
\tilde O\Big(\frac{d(\varepsilon+\varepsilon^2)}{σ_t^2\,P(y)^{3/2}}\Big),
</code></pre>
which also clarifies where guidance gets fragile: <strong>high dimension <code>d</code>, low noise <code>σ_t</code>, and rare classes <code>P(y)</code>.</strong> They push through to sampler error bounds and a step complexity result that stays near-linear in dimension, which is reassuring if you’re budgeting compute.</p>

<p><strong>Practitioner takeaway:</strong> if you’re deploying guided generative models for inspection images or defect synthesis, don’t stop at “validation CE looks fine.” Add <strong>gradient-focused diagnostics</strong> (stability/Lipschitz proxies, smoothing/regularization, stress tests at low <code>σ_t</code>, and especially checks for rare-class guidance).</p>

<p>---</p>

<h4>Self-supervised ultrasound enhancement that acts like a reliability preprocessor (Khan et al.)</h4>
Ultrasound is a worst-case “measurement system”: speckle-like noise, PSF blur, and often no clean reference. The clever move here is <strong>self-supervised training pairs from real frames</strong> using a physics-guided degradation model—Gaussian PSF surrogate blur plus noise (either additive Gaussian or Fourier-domain complex perturbations)—and “clean-like” targets created via <strong>non-local low-rank denoising</strong>.

<p>The restoration net is a Swin-Convolutional U-Net (local conv + shifted-window attention), and the reported gains are not just cosmetic: across datasets and blur levels they claim <strong>+3.6 to +13.8 dB PSNR</strong> and <strong>+0.01 to +0.26 SSIM</strong>, plus sharper edges (FWHM down, gradient metrics up). The most QE-relevant result: as a plug-in preprocessor it <strong>improves downstream segmentation Dice</strong> under severe noise—exactly where pipelines usually fall apart.</p>

<p><strong>Where you might try it:</strong> any imaging-driven decision where you can’t recalibrate the sensor (fielded systems, legacy probes, multi-site variability). Treat it like a <strong>front-end robustness module</strong> and measure impact on your actual CTQ (segmentation, detection, measurement repeatability), not just PSNR.</p>

<p>---</p>

<h4>DOE meets matching: randomize locally, not globally (Wang et al.)</h4>
If you’ve ever evaluated two matching policies (assignments, pairing, workforce-to-task) and felt that “just randomize which matching we use” is too blunt—you’re right. Switching wholesale between two matchings can have <strong>non-vanishing variance</strong> because match feasibility creates interference.

<p>They exploit a beautiful graph fact: the <em>disagreement edges</em> between two one-to-one matchings decompose into disjoint alternating paths and even cycles. That decomposition localizes interference. Their <strong>Alternating Path (AP) Design</strong> randomizes within each component with a single parameter <code>p</code>, keeping feasibility intact, and pairs with a Horvitz–Thompson estimator.</p>

<p>Under worst-case bounded outcomes, variance drops like <strong><code>O(1/N)</code></strong> (versus constant variance for the naive switch). Even better, the minimax-optimal <code>p</code> for long components converges to <code>√(2-1 ≈ 0.4142</code>—a rare “use this number on Monday” result. Simulations show AP variance shrinking with size while naive variance stays ~flat, and a finite-population CLT supports normal approximations for inference.</p>

<p><strong>QE angle:</strong> this is design-based causal inference for operational systems with constraints—exactly the kind of scenario we see in allocation, routing, pairing, and staffing.</p>

<p>---</p>

<h4>The connective tissue this week</h4>
Across all three: <strong>we’re getting more honest about what our objectives actually control.</strong> Cross-entropy doesn’t guarantee good guidance; PSNR doesn’t guarantee better segmentation; global randomization doesn’t guarantee good precision under interference. The win comes from targeting the quantity that drives the decision.

<p><strong>Question to ponder:</strong> in your current pipeline, what’s the equivalent of “training the wrong loss”—the metric that looks reassuring but doesn’t control the failure mode you care about?)</p><h4>Featured Papers This Week</h4><ul><li><strong>Khan et al.</strong>: <a href="https://arxiv.org/pdf/2601.21856v1">Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling</a> <em>(Reliability)</em></li><li><strong>Sahu et al.</strong>: <a href="https://arxiv.org/pdf/2601.21200v1">Provably Reliable Classifier Guidance through Cross-entropy Error Control</a> <em>(Reliability)</em></li><li><strong>Wang et al.</strong>: <a href="https://arxiv.org/pdf/2601.21036v1">Experimental Design for Matching</a> <em>(Experimental Design)</em></li></ul><hr/><p><strong>Explore More:</strong> Visit the <a href="https://huggingface.co/spaces/fmegahed/arxiv_control_charts">QE ArXiv Watch Dashboard</a> to browse all papers with AI summaries, interactive filtering, and paper chat.</p><p style="color: #666; font-size: 0.9em;">This digest is automatically generated every Monday. Questions or feedback? Open an issue on our <a href="https://github.com/fmegahed/arxiv_control_charts">GitHub repository</a>.</p>
]]></description>
    </item>
  </channel>
</rss>

