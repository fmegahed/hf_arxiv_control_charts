id,submitted,updated,title,abstract,authors,affiliations,link_abstract,link_pdf,link_doi,comment,journal_ref,doi,primary_category,categories,pdf_url
2602.06900v1,2026-02-06T17:50:00Z,2026-02-06 17:50:00,Supercharging Simulation-Based Inference for Bayesian Optimal Experimental Design,"Bayesian optimal experimental design (BOED) seeks to maximize the expected information gain (EIG) of experiments. This requires a likelihood estimate, which in many settings is intractable. Simulation-based inference (SBI) provides powerful tools for this regime. However, existing work explicitly connecting SBI and BOED is restricted to a single contrastive EIG bound. We show that the EIG admits multiple formulations which can directly leverage modern SBI density estimators, encompassing neural posterior, likelihood, and ratio estimation. Building on this perspective, we define a novel EIG estimator using neural likelihood estimation. Further, we identify optimization as a key bottleneck of gradient based EIG maximization and show that a simple multi-start parallel gradient ascent procedure can substantially improve reliability and performance. With these innovations, our SBI-based BOED methods are able to match or outperform by up to $22\%$ existing state-of-the-art approaches across standard BOED benchmarks.",Samuel Klein|Willie Neiswanger|Daniel Ratner|Michael Kagan|Sean Gasiorowski,,https://arxiv.org/abs/2602.06900v1,https://arxiv.org/pdf/2602.06900v1,,,,,cs.LG,cs.LG|cs.AI|cs.IT|cs.NE|stat.ML,https://arxiv.org/pdf/2602.06900v1.pdf
2602.05340v1,2026-02-05T06:06:07Z,2026-02-05 06:06:07,Decision-Focused Sequential Experimental Design: A Directional Uncertainty-Guided Approach,"We consider the sequential experimental design problem in the predict-then-optimize paradigm. In this paradigm, the outputs of the prediction model are used as coefficient vectors in a downstream linear optimization problem. Traditional sequential experimental design aims to control the input variables (features) so that the improvement in prediction accuracy from each experimental outcome (label) is maximized. However, in the predict-then-optimize setting, performance is ultimately evaluated based on the decision loss induced by the downstream optimization, rather than by prediction error. This mismatch between prediction accuracy and decision loss renders traditional decision-blind designs inefficient. To address this issue, we propose a directional-based metric to quantify predictive uncertainty. This metric does not require solving an optimization oracle and is therefore computationally tractable. We show that the resulting sequential design criterion enjoys strong consistency and convergence guarantees. Under a broad class of distributions, we demonstrate that our directional uncertainty-based design attains an earlier stopping time than decision-blind designs. This advantage is further supported by real-world experiments on an LLM job allocation problem.",Beichen Wan|Mo Liu|Paul Grigas|Zuo-Jun Max Shen,,https://arxiv.org/abs/2602.05340v1,https://arxiv.org/pdf/2602.05340v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2602.05340v1.pdf
2601.21036v1,2026-01-28T20:57:51Z,2026-01-28 20:57:51,Experimental Design for Matching,"Matching mechanisms play a central role in operations management across diverse fields including education, healthcare, and online platforms. However, experimentally comparing a new matching algorithm against a status quo presents some fundamental challenges due to matching interference, where assigning a unit in one matching may preclude its assignment in the other. In this work, we take a design-based perspective to study the design of randomized experiments to compare two predetermined matching plans on a finite population, without imposing outcome or behavioral models. We introduce the notation of a disagreement set, which captures the difference between the two matching plans, and show that it admits a unique decomposition into disjoint alternating paths and cycles with useful structural properties. Based on these properties, we propose the Alternating Path Randomized Design, which sequentially randomizes along these paths and cycles to effectively manage interference. Within a minimax framework, we optimize the conditional randomization probability and show that, for long paths, the optimal choice converges to $\sqrt{2}-1$, minimizing worst-case variance. We establish the unbiasedness of the Horvitz-Thompson estimator and derive a finite-population Central Limit Theorem that accommodates complex and unstable path and cycle structures as the population grows. Furthermore, we extend the design to many-to-one matchings, where capacity constraints fundamentally alter the structure of the disagreement set. Using graph-theoretic tools, including finding augmenting paths and Euler-tour decomposition on an auxiliary unbalanced directed graph, we construct feasible alternating path and cycle decompositions that allow the design and inference results to carry over.",Chonghuan Wang,,https://arxiv.org/abs/2601.21036v1,https://arxiv.org/pdf/2601.21036v1,,,,,stat.ME,stat.ME|econ.EM|eess.SY,https://arxiv.org/pdf/2601.21036v1.pdf
2601.17587v1,2026-01-24T20:57:27Z,2026-01-24 20:57:27,Discovery of Feasible 3D Printing Configurations for Metal Alloys via AI-driven Adaptive Experimental Design,"Configuring the parameters of additive manufacturing processes for metal alloys is a challenging problem due to complex relationships between input parameters (e.g., laser power, scan speed) and quality of printed outputs. The standard trial-and-error approach to find feasible parameter configurations is highly inefficient because validating each configuration is expensive in terms of resources (physical and human labor) and the configuration space is very large. This paper combines the general principles of AI-driven adaptive experimental design with domain knowledge to address the challenging problem of discovering feasible configurations. The key idea is to build a surrogate model from past experiments to intelligently select a small batch of input configurations for validation in each iteration. To demonstrate the effectiveness of this methodology, we deploy it for Directed Energy Deposition process to print GRCop--42, a high-performance copper--chromium--niobium alloy developed by NASA for aerospace applications. Within three months, our approach yielded multiple defect-free outputs across a range of laser powers dramatically reducing time to result and resource expenditure compared to several months of manual experimentation by domain scientists with no success. By enabling high-quality GRCop--42 fabrication on readily available infrared laser platforms for the first time, we democratize access to this critical alloy, paving the way for cost-effective, decentralized production for aerospace applications.",Azza Fadhel|Nathaniel W. Zuckschwerdt|Aryan Deshwal|Susmita Bose|Amit Bandyopadhyay|Jana Doppa,,https://arxiv.org/abs/2601.17587v1,https://arxiv.org/pdf/2601.17587v1,,Proceedings of Innovative Applications of AI (IAAI) 2026 Conference,,,cs.AI,cs.AI|cs.LG,https://arxiv.org/pdf/2601.17587v1.pdf
2601.16431v1,2026-01-23T04:11:01Z,2026-01-23 04:11:01,Sequential Experimental Designs for Kriging Model,"Computer experiments have become an indispensable alternative to complex physical and engineering experiments. The Kriging model is the most widely used surrogate model, with the core goal of minimizing the discrepancy between the surrogate and true models across the entire experimental domain. However, existing sequential design methods have critical limitations: observation-based batch sequential designs are rarely studied, while one-point sequential designs have insufficient information utilization and suffer from inefficient resource utilization -- they require numerous repeated observation rounds to accumulate sufficient points, leading to prolonged experimental cycles. To address these gaps, this paper proposes two novel one-point sequential design criteria and a general batch sequential design framework. Moreover, the batch sequential design framework solves the inherent point clustering problem in naive batch selection, enabling efficient extension of any sequential criterion to batch scenarios. Simulations on some test functions demonstrate that the proposed methods outperform existing approaches in terms of fitting accuracy in most cases.",Ruonan Zheng|Min-Qian Liu|Yongdao Zhou|Xuan Chen,,https://arxiv.org/abs/2601.16431v1,https://arxiv.org/pdf/2601.16431v1,,"24pages, 5figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2601.16431v1.pdf
2601.16425v1,2026-01-23T03:34:10Z,2026-01-23 03:34:10,Bayesian Experimental Design for Model Discrepancy Calibration: A Rivalry between Kullback--Leibler Divergence and Wasserstein Distance,"Designing experiments that systematically gather data from complex physical systems is central to accelerating scientific discovery. While Bayesian experimental design (BED) provides a principled, information-based framework that integrates experimental planning with probabilistic inference, the selection of utility functions in BED is a long-standing and active topic, where different criteria emphasize different notions of information. Although Kullback--Leibler (KL) divergence has been one of the most common choices, recent studies have proposed Wasserstein distance as an alternative. In this work, we first employ a toy example to illustrate an issue of Wasserstein distance - the value of Wasserstein distance of a fixed-shape posterior depends on the relative position of its main mass within the support and can exhibit false rewards unrelated to information gain, especially with a non-informative prior (e.g., uniform distribution). We then further provide a systematic comparison between these two criteria through a classical source inversion problem in the BED literature, revealing that the KL divergence tends to lead to faster convergence in the absence of model discrepancy, while Wasserstein metrics provide more robust sequential BED results if model discrepancy is non-negligible. These findings clarify the trade-offs between KL divergence and Wasserstein metrics for the utility function and provide guidelines for selecting suitable criteria in practical BED applications.",Huchen Yang|Xinghao Dong|Jin-Long Wu,,https://arxiv.org/abs/2601.16425v1,https://arxiv.org/pdf/2601.16425v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2601.16425v1.pdf
2601.11473v1,2026-01-16T17:58:16Z,2026-01-16 17:58:16,A Probabilistic Approach to Trajectory-Based Optimal Experimental Design,"We present a novel probabilistic approach for optimal path experimental design. In this approach a discrete path optimization problem is defined on a static navigation mesh, and trajectories are modeled as random variables governed by a parametric Markov policy. The discrete path optimization problem is then replaced with an equivalent stochastic optimization problem over the policy parameters, resulting in an optimal probability model that samples estimates of the optimal discrete path. This approach enables exploration of the utility function's distribution tail and treats the utility function of the design as a black box, making it applicable to linear and nonlinear inverse problems and beyond experimental design. Numerical verification and analysis are carried out by using a parameter identification problem widely used in model-based optimal experimental design.",Ahmed Attia,,https://arxiv.org/abs/2601.11473v1,https://arxiv.org/pdf/2601.11473v1,,"42 Figures, this version includes supplementary material as appendices",,,math.OC,math.OC|cs.LG,https://arxiv.org/pdf/2601.11473v1.pdf
2601.05868v1,2026-01-09T15:44:49Z,2026-01-09 15:44:49,Sequential Bayesian Optimal Experimental Design in Infinite Dimensions via Policy Gradient Reinforcement Learning,"Sequential Bayesian optimal experimental design (SBOED) for PDE-governed inverse problems is computationally challenging, especially for infinite-dimensional random field parameters. High-fidelity approaches require repeated forward and adjoint PDE solves inside nested Bayesian inversion and design loops. We formulate SBOED as a finite-horizon Markov decision process and learn an amortized design policy via policy-gradient reinforcement learning (PGRL), enabling online design selection from the experiment history without repeatedly solving an SBOED optimization problem. To make policy training and reward evaluation scalable, we combine dual dimension reduction -- active subspace projection for the parameter and principal component analysis for the state -- with an adjusted derivative-informed latent attention neural operator (LANO) surrogate that predicts both the parameter-to-solution map and its Jacobian. We use a Laplace-based D-optimality reward while noting that, in general, other expected-information-gain utilities such as KL divergence can also be used within the same framework. We further introduce an eigenvalue-based evaluation strategy that uses prior samples as proxies for maximum a posteriori (MAP) points, avoiding repeated MAP solves while retaining accurate information-gain estimates. Numerical experiments on sequential multi-sensor placement for contaminant source tracking demonstrate approximately $100\times$ speedup over high-fidelity finite element methods, improved performance over random sensor placements, and physically interpretable policies that discover an ``upstream'' tracking strategy.",Kaichen Shen|Peng Chen,,https://arxiv.org/abs/2601.05868v1,https://arxiv.org/pdf/2601.05868v1,,,,,math.OC,math.OC|cs.LG,https://arxiv.org/pdf/2601.05868v1.pdf
2601.04557v1,2026-01-08T03:36:15Z,2026-01-08 03:36:15,The explicit constraint force method for optimal experimental design,"The explicit constraint force method (ECFM) was recently introduced as a novel formulation of the physics-informed solution reconstruction problem, and was subsequently extended to inverse problems. In both solution reconstruction and inverse problems, model parameters are estimated with the help of measurement data. In practice, experimentalists seek to design experiments such that the acquired data leads to the most robust recovery of the missing parameters in a subsequent inverse problem. While there are well-established techniques for designing experiments with standard approaches to the inverse problem, optimal experimental design (OED) has yet to be explored with the ECFM formulation. In this work, we investigate OED with a constraint force objective. First, we review traditional approaches to OED based on the Fisher information matrix, and propose an analogous formulation based on constraint forces. Next, we reflect on the different interpretations of the objective from standard and constraint force-based inverse problems. We then test our method on several example problems. These examples suggest that an experiment which is optimal in the sense of constraint forces tends to position measurements in the stiffest regions of the system. Because the responses -- and thus the measurements -- are small in these regions, this strategy is impractical in the presence of measurement noise and/or finite measurement precision. As such, our provisional conclusion is that ECFM is not a viable approach to OED.",Conor Rowan,,https://arxiv.org/abs/2601.04557v1,https://arxiv.org/pdf/2601.04557v1,,,,,math.NA,math.NA,https://arxiv.org/pdf/2601.04557v1.pdf
2512.23763v2,2025-12-28T22:26:18Z,2026-01-06 22:09:07,Neural Optimal Design of Experiment for Inverse Problems,"We introduce Neural Optimal Design of Experiments, a learning-based framework for optimal experimental design in inverse problems that avoids classical bilevel optimization and indirect sparsity regularization. NODE jointly trains a neural reconstruction model and a fixed-budget set of continuous design variables representing sensor locations, sampling times, or measurement angles, within a single optimization loop. By optimizing measurement locations directly rather than weighting a dense grid of candidates, the proposed approach enforces sparsity by design, eliminates the need for l1 tuning, and substantially reduces computational complexity. We validate NODE on an analytically tractable exponential growth benchmark, on MNIST image sampling, and illustrate its effectiveness on a real world sparse view X ray CT example. In all cases, NODE outperforms baseline approaches, demonstrating improved reconstruction accuracy and task-specific performance.",John E. Darges|Babak Maboudi Afkham|Matthias Chung,,https://arxiv.org/abs/2512.23763v2,https://arxiv.org/pdf/2512.23763v2,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2512.23763v2.pdf
2512.19057v1,2025-12-22T05:47:25Z,2025-12-22 05:47:25,Efficient Personalization of Generative Models via Optimal Experimental Design,"Preference learning from human feedback has the ability to align generative models with the needs of end-users. Human feedback is costly and time-consuming to obtain, which creates demand for data-efficient query selection methods. This work presents a novel approach that leverages optimal experimental design to ask humans the most informative preference queries, from which we can elucidate the latent reward function modeling user preferences efficiently. We formulate the problem of preference query selection as the one that maximizes the information about the underlying latent preference model. We show that this problem has a convex optimization formulation, and introduce a statistically and computationally efficient algorithm ED-PBRL that is supported by theoretical guarantees and can efficiently construct structured queries such as images or text. We empirically present the proposed framework by personalizing a text-to-image generative model to user-specific styles, showing that it requires less preference queries compared to random query selection.",Guy Schacht|Ziyad Sheebaelhamd|Riccardo De Santi|Mojmír Mutný|Andreas Krause,,https://arxiv.org/abs/2512.19057v1,https://arxiv.org/pdf/2512.19057v1,,,,,cs.LG,cs.LG|cs.IT,https://arxiv.org/pdf/2512.19057v1.pdf
2512.14378v2,2025-12-16T13:06:41Z,2025-12-17 10:02:49,On the E(s^2)-optimality of two-level supersaturated designs constructed using Wu's method of partially aliased interactions on certain two-level orthogonal arrays,"Wu [10] proposed a method for constructing two-level supersaturated designs by using a Hadamard design with n runs and n-1 columns as a staring design and by supplementing it with two-column interactions, as long as they are partially aliased. Bulutoglu and Cheng [2] proved that this method results in E(s^2)-optimal supersaturated designs when certain interaction columns are selected. In this paper, we extend these results and prove E(s^2)-optimality for supersaturated designs that are constructed using Wu's method when the starting design is any orthogonal array with n runs and n-1, n-2 or n-3 columns, as long as its main effects and two-column interactions are partially aliased with two-column interactions.",Emmanouil Androulakis|Kashinath Chatterjee|Haralambos Evangelaras,,https://arxiv.org/abs/2512.14378v2,https://arxiv.org/pdf/2512.14378v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2512.14378v2.pdf
2512.13220v1,2025-12-15T11:33:47Z,2025-12-15 11:33:47,Experimental design of a millifluidic flow-focusing method for biomimetic nanocellulose and hemicellulose-based biopolymer fibres,"The mechanical performance of plant fibres is linked to the presence of crystalline elements dispersed within an amorphous cohesive matrix. The more the crystalline reinforcement is aligned with the fibre axis, the better the mechanical properties of the fibre. With the aim of developing entirely biobased biomimetic fibres as alternatives to synthetic or resource-consuming fibres, we have studied the fabrication of hydrogel filaments made from mixtures of nanocelluloses, biobased crystalline nanoparticles acting as reinforcement, and xyloglucans, a plant wall hemicellulose with a strong affinity for cellulose surfaces. These will ensure cohesion between the nanocelluloses. To optimize the orientation of the nanocelluloses within the filaments, and thus potentially improve the mechanical properties of the fibres, we present a study on the development of a millifluidic method of flow-focusing. The developed setup uses external sheath flows to focus and align a nanocellulose suspension central flow. Different configurations in terms of concentrations, circuit designs and flow velocities are tested. 3D printed circuits are explored to produce versatile geometries and optimize the process design. To qualify the orientations during the process, observations with a polarized microscope (POM) are made, as the alignment of the nanocellulose crystalline structures creates birefringence in suspensions. Significant optical phase shifts related to the nanocellulose particles' orientations are visible by color gradients, varying with the suspensions' concentrations and flow velocities. Results demonstrate successful tuning of nanocellulose orientation into anisotropic hydrogels using different millifluidic circuit geometries, with the introduction of xyloglucans to produce new types of biosourced fibres.",Moisy Amélie|Voisin Hugo|Davy Joëlle|Cathala Bernard|Guessasma Sofiane,BIA|BIA|BIA|BIA|BIA,https://arxiv.org/abs/2512.13220v1,https://arxiv.org/pdf/2512.13220v1,,,"26{è}me Congr{è}s Fran{\c c}ais de M{é}canique, Aug 2025, Metz, France",,physics.class-ph,physics.class-ph,https://arxiv.org/pdf/2512.13220v1.pdf
2512.12574v1,2025-12-14T06:52:17Z,2025-12-14 06:52:17,Mind the Jumps: A Scalable Robust Local Gaussian Process for Multidimensional Response Surfaces with Discontinuities,"Modeling response surfaces with abrupt jumps and discontinuities remains a major challenge across scientific and engineering domains. Although Gaussian process models excel at capturing smooth nonlinear relationships, their stationarity assumptions limit their ability to adapt to sudden input-output variations. Existing nonstationary extensions, particularly those based on domain partitioning, often struggle with boundary inconsistencies, sensitivity to outliers, and scalability issues in higher-dimensional settings, leading to reduced predictive accuracy and unreliable parameter estimation.
  To address these challenges, this paper proposes the Robust Local Gaussian Process (RLGP) model, a framework that integrates adaptive nearest-neighbor selection with a sparsity-driven robustification mechanism. Unlike existing methods, RLGP leverages an optimization-based mean-shift adjustment after a multivariate perspective transformation combined with local neighborhood modeling to mitigate the influence of outliers. This approach improves predictive accuracy near discontinuities while enhancing robustness to data heterogeneity.
  Comprehensive evaluations on real-world datasets show that RLGP consistently delivers high predictive accuracy and maintains competitive computational efficiency, especially in scenarios with sharp transitions and complex response structures. Scalability tests further confirm RLGP's stability and reliability in higher-dimensional settings, where other methods struggle. These results establish RLGP as an effective and practical solution for modeling nonstationary and discontinuous response surfaces across a wide range of applications.",Isaac Adjetey|Yiyuan She,,https://arxiv.org/abs/2512.12574v1,https://arxiv.org/pdf/2512.12574v1,https://doi.org/10.1016/j.neucom.2025.132317,"25 pages, 4 figures",Neurocomputing 132317 (2025),10.1016/j.neucom.2025.132317,stat.ML,stat.ML|cs.LG|stat.AP,https://arxiv.org/pdf/2512.12574v1.pdf
2512.08513v1,2025-12-09T11:58:27Z,2025-12-09 11:58:27,Minimax and Bayes Optimal Adaptive Experimental Design for Treatment Choice,"We consider an adaptive experiment for treatment choice and design a minimax and Bayes optimal adaptive experiment with respect to regret. Given binary treatments, the experimenter's goal is to choose the treatment with the highest expected outcome through an adaptive experiment, in order to maximize welfare. We consider adaptive experiments that consist of two phases, the treatment allocation phase and the treatment choice phase. The experiment starts with the treatment allocation phase, where the experimenter allocates treatments to experimental subjects to gather observations. During this phase, the experimenter can adaptively update the allocation probabilities using the observations obtained in the experiment. After the allocation phase, the experimenter proceeds to the treatment choice phase, where one of the treatments is selected as the best. For this adaptive experimental procedure, we propose an adaptive experiment that splits the treatment allocation phase into two stages, where we first estimate the standard deviations and then allocate each treatment proportionally to its standard deviation. We show that this experiment, often referred to as Neyman allocation, is minimax and Bayes optimal in the sense that its regret upper bounds exactly match the lower bounds that we derive. To show this optimality, we derive minimax and Bayes lower bounds for the regret using change-of-measure arguments. Then, we evaluate the corresponding upper bounds using the central limit theorem and large deviation bounds.",Masahiro Kato,,https://arxiv.org/abs/2512.08513v1,https://arxiv.org/pdf/2512.08513v1,,,,,econ.EM,econ.EM|cs.LG|math.ST|stat.ME|stat.ML,https://arxiv.org/pdf/2512.08513v1.pdf
2512.06712v1,2025-12-07T08:01:50Z,2025-12-07 08:01:50,Optimal experimental design with k-space data: application to inverse hemodynamics,"Subject-specific cardiovascular models rely on parameter estimation using measurements such as 4D Flow MRI data. However, acquiring high-resolution, high-fidelity functional flow data is costly and taxing for the patient. As a result, there is growing interest in using highly undersampled MRI data to reduce acquisition time and thus the cost, while maximizing the information gain from the data. Examples of such recent work include inverse problems to estimate boundary conditions of aortic blood flow from highly undersampled k-space data. The undersampled data is selected based on a predefined sampling mask which can significantly influences the performance and the quality of the solution of the inverse problem. While there are many established sampling patterns to collect undersampled data, it remains unclear how to select the best sampling pattern for a given set of inference parameters. In this paper we propose an Optimal Experimental Design (OED) framework for MRI measurements in k-space, aiming to find optimal masks for estimating specific parameters directly from k-space. As OED is typically applied to sensor placement problems in spatial locations, this is, to our knowledge, the first time the technique is used in this context. We demonstrate that the masks optimized by employing OED consistently outperform conventional sampling patterns in terms of parameter estimation accuracy and variance, facilitating a speed-up of 10x of the acquisition time while maintaining accuracy.",Miriam Löcke|Ahmed Attia|Dariusz Ucínski|Cristóbal Bertoglio,,https://arxiv.org/abs/2512.06712v1,https://arxiv.org/pdf/2512.06712v1,,,,,physics.med-ph,physics.med-ph|math.NA,https://arxiv.org/pdf/2512.06712v1.pdf
2512.00098v1,2025-11-27T02:18:03Z,2025-11-27 02:18:03,Guarding Against Malicious Biased Threats (GAMBiT): Experimental Design of Cognitive Sensors and Triggers with Behavioral Impact Analysis,"This paper introduces GAMBiT (Guarding Against Malicious Biased Threats), a cognitive-informed cyber defense framework that leverages deviations from human rationality as a new defensive surface. Conventional cyber defenses assume rational, utility-maximizing attackers, yet real-world adversaries exhibit cognitive constraints and biases that shape their interactions with complex digital systems. GAMBiT embeds insights from cognitive science into cyber environments through cognitive triggers, which activate biases such as loss aversion, base-rate neglect, and sunk-cost fallacy, and through newly developed cognitive sensors that infer attackers' cognitive states from behavioral and network data. Three rounds of human-subject experiments (total n=61) in a simulated small business network demonstrate that these manipulations significantly disrupt attacker performance, reducing mission progress, diverting actions off the true attack path, and increasing detectability. These results demonstrate that cognitive biases can be systematically triggered to degrade the attacker's efficiency and enhance the defender's advantage. GAMBiT establishes a new paradigm in which the attacker's mind becomes part of the battlefield and cognitive manipulation becomes a proactive vector for cyber defense.",Brandon Beltz|Po-Yu Chen|James Doty|Yvonne Fonken|Nikolos Gurney|Hsiang-Wen Hsing|Sofia Hirschmann|Brett Israelsen|Nathan Lau|Mengyun Li|Stacy Marsella|Michael Murray|Jinwoo Oh|Amy Sliva|Kunal Srivastava|Stoney Trent|Peggy Wu|Ya-Ting Yang|Quanyan Zhu,,https://arxiv.org/abs/2512.00098v1,https://arxiv.org/pdf/2512.00098v1,,,,,cs.CR,cs.CR|cs.GT,https://arxiv.org/pdf/2512.00098v1.pdf
2511.20183v1,2025-11-25T11:06:08Z,2025-11-25 11:06:08,Efficient multi-fidelity Gaussian process regression for noisy outputs and non-nested experimental designs,"This paper presents a multi-fidelity Gaussian process surrogate modeling that generalizes the recursive formulation of the auto-regressive model when the high-fidelity and low-fidelity data sets are noisy and not necessarily nested. The estimation of high-fidelity parameters by the EM (expectation-maximization) algorithm is shown to be still possible in this context and a closed-form update formula is derived when the scaling factor is a parametric linear predictor function. This yields a decoupled optimization strategy for the parameter selection that is more efficient and scalable than the direct maximum likelihood maximization. The proposed approach is compared to other multi-fidelity models, and benchmarks for different application cases of increasing complexity are provided.",Nils Baillie|Baptiste Kerleguer|Cyril Feau|Josselin Garnier,,https://arxiv.org/abs/2511.20183v1,https://arxiv.org/pdf/2511.20183v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2511.20183v1.pdf
2511.07671v1,2025-11-10T22:25:01Z,2025-11-10 22:25:01,Robust Experimental Design via Generalised Bayesian Inference,"Bayesian optimal experimental design is a principled framework for conducting experiments that leverages Bayesian inference to quantify how much information one can expect to gain from selecting a certain design. However, accurate Bayesian inference relies on the assumption that one's statistical model of the data-generating process is correctly specified. If this assumption is violated, Bayesian methods can lead to poor inference and estimates of information gain. Generalised Bayesian (or Gibbs) inference is a more robust probabilistic inference framework that replaces the likelihood in the Bayesian update by a suitable loss function. In this work, we present Generalised Bayesian Optimal Experimental Design (GBOED), an extension of Gibbs inference to the experimental design setting which achieves robustness in both design and inference. Using an extended information-theoretic framework, we derive a new acquisition function, the Gibbs expected information gain (Gibbs EIG). Our empirical results demonstrate that GBOED enhances robustness to outliers and incorrect assumptions about the outcome noise distribution.",Yasir Zubayr Barlas|Sabina J. Sloman|Samuel Kaski,,https://arxiv.org/abs/2511.07671v1,https://arxiv.org/pdf/2511.07671v1,,"12 main pages, 43 pages in total",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2511.07671v1.pdf
2511.04403v2,2025-11-06T14:29:05Z,2026-01-29 15:45:20,Online Bayesian Experimental Design for Partially Observed Dynamical Systems,"Bayesian experimental design (BED) provides a principled framework for optimizing data collection by choosing experiments that are maximally informative about unknown parameters. However, existing methods cannot deal with the joint challenge of (a) partially observable dynamical systems, where only noisy and incomplete observations are available, and (b) fully online inference, which updates posterior distributions and selects designs sequentially in a computationally efficient manner. Under partial observability, dynamical systems are naturally modeled as state-space models (SSMs), where latent states mediate the link between parameters and data, making the likelihood -- and thus information-theoretic objectives like the expected information gain (EIG) -- intractable. We address these challenges by deriving new estimators of the EIG and its gradient that explicitly marginalize latent states, enabling scalable stochastic optimization in nonlinear SSMs. Our approach leverages nested particle filters for efficient online state-parameter inference with convergence guarantees. Applications to realistic models, such as the susceptible-infectious-recovered (SIR) and a moving source location task, show that our framework successfully handles both partial observability and online inference.",Sara Pérez-Vieites|Sahel Iqbal|Simo Särkkä|Dominik Baumann,,https://arxiv.org/abs/2511.04403v2,https://arxiv.org/pdf/2511.04403v2,,"20 pages, 7 figures",,,stat.ML,stat.ML|cs.LG|stat.CO,https://arxiv.org/pdf/2511.04403v2.pdf
2511.02984v2,2025-11-04T20:42:40Z,2025-12-19 01:07:48,Constructing Large Orthogonal Minimally Aliased Response Surface Designs by Concatenating Two Definitive Screening Designs,"Orthogonal minimally aliased response surface (OMARS) designs permit the study of quantitative factors at three levels using an economical number of runs. In these designs, the linear effects of the factors are neither aliased with each other nor with the quadratic effects and the two-factor interactions. Complete catalogs of OMARS designs with up to five factors have been obtained using an enumeration algorithm. However, the algorithm is computationally demanding for designs with many factors and runs. To overcome this issue, we propose a construction method for large OMARS designs that concatenates two definitive screening designs and improves the statistical features of its parent designs. The concatenation employs an algorithm that minimizes the aliasing among the second-order effects using foldover techniques and column permutations for one of the parent designs. We study the properties of the new OMARS designs and compare them with alternative designs in the literature.",Alan R. Vazquez|Peter Goos|Eric D. Schoen,,https://arxiv.org/abs/2511.02984v2,https://arxiv.org/pdf/2511.02984v2,,"35 pages, 2 figures, 7 tables",,,stat.ME,stat.ME,https://arxiv.org/pdf/2511.02984v2.pdf
2510.24349v1,2025-10-28T12:14:05Z,2025-10-28 12:14:05,Pseudo-Bayesian Optimal Designs for Fitting Fractional Polynomial Response Surface Models,"Fractional polynomial models are potentially useful for response surfaces investigations. With the availability of routines for fitting nonlinear models in statistical packages they are increasingly being used. However, as in all experiments the design should be chosen such that the model parameters are estimated as efficiently as possible. The design choice for such models involves the known nonlinear models' design difficulties but \cite{gilmour_trinca_2012b} proposed a methodology capable of producing exact designs that makes use of the computing facilities available today. In this paper, we use this methodology to find Bayesian optimal exact designs for several fractional polynomial models. The optimum designs are compared to various standard designs in response surface problems.",Luzia A. Trinca|Steven G. Gilmour,,https://arxiv.org/abs/2510.24349v1,https://arxiv.org/pdf/2510.24349v1,,31 pages,,,stat.ME,stat.ME,https://arxiv.org/pdf/2510.24349v1.pdf
2510.23434v3,2025-10-27T15:37:23Z,2025-12-28 18:55:21,Learning What to Learn: Experimental Design when Combining Experimental with Observational Evidence,"Experiments deliver credible treatment-effect estimates but, because they are costly, are often restricted to specific sites, small populations, or particular mechanisms. A common practice across several fields is therefore to combine experimental estimates with reduced-form or structural external (observational) evidence to answer broader policy questions such as those involving general equilibrium effects or external validity. We develop a unified framework for the design of experiments when combined with external evidence, i.e., choosing which experiment(s) to run and how to allocate sample size under arbitrary budget constraints. Because observational evidence may suffer bias unknown ex-ante, we evaluate designs using a minimax proportional-regret criterion that compares any candidate design to an oracle that knows the observational study bias and jointly chooses the design and estimator. This yields a transparent bias-variance trade-off that does not require the researcher to specify a bias bound and relies only on information already needed for conventional power calculations. We illustrate the framework by (i) designing cash-transfer experiments aimed at estimating general equilibrium effects and (ii) optimizing site selection for microfinance interventions.",Aristotelis Epanomeritakis|Davide Viviano,,https://arxiv.org/abs/2510.23434v3,https://arxiv.org/pdf/2510.23434v3,,,,,econ.EM,econ.EM|math.ST|stat.ME,https://arxiv.org/pdf/2510.23434v3.pdf
2510.22098v1,2025-10-25T00:49:02Z,2025-10-25 00:49:02,Beyond Reality: Designing Personal Experiences and Interactive Narratives in AR Theater,"Augmented Reality (AR) technologies are redefining how we perceive and interact with the world by seamlessly integrating digital elements into our physical surroundings. These technologies offer personalized experiences and transform familiar spaces by layering new narratives onto the real world.
  Through increased levels of perceived agency and immersive environments, my work aims to merge the human elements of live theater with the dynamic potential of virtual entities and AI agents. This approach captures the subtlety and magic of storytelling, making theater experiences available anytime and anywhere. The system I am building introduces innovative methods for theatrical production in virtual settings, informed by my research and eight published works. These contributions highlight domain-specific insights that have shaped the design of an immersive AR Theater system.
  My research in building a well-designed AR stage features avatars and interactive elements that allow users to engage with stories at their own pace, granting them full agency over their experience. However, to ensure a smooth and curated experience that aligns with the director or creator's vision, several factors must be considered, especially in open-world settings that depend on natural user movement. This requires the story to be conveyed in a controlled manner, while the interaction remains intuitive and natural for the user.",You-Jin Kim,,https://arxiv.org/abs/2510.22098v1,https://arxiv.org/pdf/2510.22098v1,,"PhD Thesis, Media Arts and Technology, University of California, Santa Barbara, defended on December 13, 2024. Available from ProQuest Dissertations & Theses Global (Order No. 31761773). This version is shared on arXiv for broader accessibility",,,cs.HC,cs.HC,https://arxiv.org/pdf/2510.22098v1.pdf
2512.08935v1,2025-10-22T07:50:33Z,2025-10-22 07:50:33,From Script to Stage: Automating Experimental Design for Social Simulations with LLMs,"The rise of large language models (LLMs) has opened new avenues for social science research. Multi-agent simulations powered by LLMs are increasingly becoming a vital approach for exploring complex social phenomena and testing theoretical hypotheses. However, traditional computational experiments often rely heavily on interdisciplinary expertise, involve complex operations, and present high barriers to entry. While LLM-driven agents show great potential for automating experimental design, their reliability and scientific rigor remain insufficient for widespread adoption. To address these challenges, this paper proposes an automated multi-agent experiment design framework based on script generation, inspired by the concept of the Decision Theater. The experimental design process is divided into three stages: (1) Script Generation - a Screenwriter Agent drafts candidate experimental scripts; (2) Script Finalization - a Director Agent evaluates and selects the final script; (3) Actor Generation - an Actor Factory creates actor agents capable of performing on the experimental ""stage"" according to the finalized script. Extensive experiment conducted across multiple social science experimental scenarios demonstrate that the generated actor agents can perform according to the designed scripts and reproduce outcomes consistent with real-world situations. This framework not only lowers the barriers to experimental design in social science but also provides a novel decision-support tool for policy-making and research. The project's source code is available at: https://anonymous.4open.science/r/FSTS-DE1E",Yuwei Guo|Zihan Zhao|Deyu Zhou|Xiaowei Liu|Ming Zhang,,https://arxiv.org/abs/2512.08935v1,https://arxiv.org/pdf/2512.08935v1,,,,,cs.HC,cs.HC|cs.CY,https://arxiv.org/pdf/2512.08935v1.pdf
2510.14848v1,2025-10-16T16:20:14Z,2025-10-16 16:20:14,A Geometric Approach to Optimal Experimental Design,"We introduce a novel geometric framework for optimal experimental design (OED). Traditional OED approaches, such as those based on mutual information, rely explicitly on probability densities, leading to restrictive invariance properties. To address these limitations, we propose the mutual transport dependence (MTD), a measure of statistical dependence grounded in optimal transport theory which provides a geometric objective for optimizing designs. Unlike conventional approaches, the MTD can be tailored to specific downstream estimation problems by choosing appropriate geometries on the underlying spaces. We demonstrate that our framework produces high-quality designs while offering a flexible alternative to standard information-theoretic techniques.",Gavin Kerrigan|Christian A. Naesseth|Tom Rainforth,,https://arxiv.org/abs/2510.14848v1,https://arxiv.org/pdf/2510.14848v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2510.14848v1.pdf
2510.00734v1,2025-10-01T10:17:07Z,2025-10-01 10:17:07,Approximation of differential entropy in Bayesian optimal experimental design,"Bayesian optimal experimental design provides a principled framework for selecting experimental settings that maximize obtained information. In this work, we focus on estimating the expected information gain in the setting where the differential entropy of the likelihood is either independent of the design or can be evaluated explicitly. This reduces the problem to maximum entropy estimation, alleviating several challenges inherent in expected information gain computation.
  Our study is motivated by large-scale inference problems, such as inverse problems, where the computational cost is dominated by expensive likelihood evaluations. We propose a computational approach in which the evidence density is approximated by a Monte Carlo or quasi-Monte Carlo surrogate, while the differential entropy is evaluated using standard methods without additional likelihood evaluations. We prove that this strategy achieves convergence rates that are comparable to, or better than, state-of-the-art methods for full expected information gain estimation, particularly when the cost of entropy evaluation is negligible. Moreover, our approach relies only on mild smoothness of the forward map and avoids stronger technical assumptions required in earlier work. We also present numerical experiments, which confirm our theoretical findings.",Chuntao Chen|Tapio Helin|Nuutti Hyvönen|Yuya Suzuki,,https://arxiv.org/abs/2510.00734v1,https://arxiv.org/pdf/2510.00734v1,,"28 pages, 3 figures",,,stat.ML,stat.ML|cs.LG|math.NA|stat.CO,https://arxiv.org/pdf/2510.00734v1.pdf
2509.25709v1,2025-09-30T03:10:29Z,2025-09-30 03:10:29,Leveraging LLMs to Improve Experimental Design: A Generative Stratification Approach,"Pre-experiment stratification, or blocking, is a well-established technique for designing more efficient experiments and increasing the precision of the experimental estimates. However, when researchers have access to many covariates at the experiment design stage, they often face challenges in effectively selecting or weighting covariates when creating their strata. This paper proposes a Generative Stratification procedure that leverages Large Language Models (LLMs) to synthesize high-dimensional covariate data to improve experimental design. We demonstrate the value of this approach by applying it to a set of experiments and find that our method would have reduced the variance of the treatment effect estimate by 10%-50% compared to simple randomization in our empirical applications. When combined with other standard stratification methods, it can be used to further improve the efficiency. Our results demonstrate that LLM-based simulation is a practical and easy-to-implement way to improve experimental design in covariate-rich settings.",George Gui|Seungwoo Kim,,https://arxiv.org/abs/2509.25709v1,https://arxiv.org/pdf/2509.25709v1,,,,,econ.EM,econ.EM,https://arxiv.org/pdf/2509.25709v1.pdf
2509.21734v1,2025-09-26T01:02:24Z,2025-09-26 01:02:24,Optimal Stopping for Sequential Bayesian Experimental Design,"In sequential Bayesian experimental design, the number of experiments is usually fixed in advance. In practice, however, campaigns may terminate early, raising the fundamental question: when should one stop? Threshold-based rules are simple to implement but inherently myopic, as they trigger termination based on a fixed criterion while ignoring the expected future information gain that additional experiments might provide. We develop a principled Bayesian framework for optimal stopping in sequential experimental design, formulated as a Markov decision process where stopping and design policies are jointly optimized. We prove that the optimal rule is to stop precisely when the immediate terminal reward outweighs the expected continuation value. To learn such policies, we introduce a policy gradient method, but show that naïve joint optimization suffers from circular dependencies that destabilize training. We resolve this with a curriculum learning strategy that gradually transitions from forced continuation to adaptive stopping. Numerical studies on a linear-Gaussian benchmark and a contaminant source detection problem demonstrate that curriculum learning achieves stable convergence and outperforms vanilla methods, particularly in settings with strong sequential dependencies.",Chen Cheng|Xun Huan,,https://arxiv.org/abs/2509.21734v1,https://arxiv.org/pdf/2509.21734v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2509.21734v1.pdf
2509.15961v1,2025-09-19T13:20:21Z,2025-09-19 13:20:21,Optimal Experimental Design of a Moving Sensor for Linear Bayesian Inverse Problems,"We optimize the path of a mobile sensor to minimize the posterior uncertainty of a Bayesian inverse problem. Along its path, the sensor continuously takes measurements of the state, which is a physical quantity modeled as the solution of a partial differential equation (PDE) with uncertain parameters. Considering linear PDEs specifically, we derive the closed-form expression of the posterior covariance matrix of the model parameters as a function of the path, and formulate the optimal experimental design problem for minimizing the posterior's uncertainty. We discretize the problem such that the cost function remains consistent under temporal refinement. Additional constraints ensure that the path avoids obstacles and remains physically interpretable through a control parameterization. The constrained optimization problem is solved using an interior-point method. We present computational results for a convection-diffusion equation with unknown initial condition.",Nicole Aretz|Thomas Lynn|Karen Willcox|Sven Leyffer,,https://arxiv.org/abs/2509.15961v1,https://arxiv.org/pdf/2509.15961v1,,,,,cs.CE,cs.CE|math.NA,https://arxiv.org/pdf/2509.15961v1.pdf
2509.10742v2,2025-09-12T23:16:35Z,2025-09-25 04:14:15,Matched-Pair Experimental Design with Active Learning,"Matched-pair experimental designs aim to detect treatment effects by pairing participants and comparing within-pair outcome differences. In many situations, the overall effect size across the entire population is small. Then, the focus naturally shifts to identifying and targeting high treatment-effect regions where the intervention is most effective. This paper proposes a matched-pair experimental design that sequentially and actively enrolls patients in high treatment-effect regions. Importantly, we frame the identification of the target region as a classification problem and propose an active learning framework tailored to matched-pair designs. Our design not only reduces the experimental cost of detecting treatment efficacy, but also ensures that the identified regions enclose the entire high-treatment-effect regions. Our theoretical analysis of the framework's label complexity and experiments in practical scenarios demonstrate the efficiency and advantages of the approach.",Weizhi Li|Gautam Dasarathy|Visar Berisha,,https://arxiv.org/abs/2509.10742v2,https://arxiv.org/pdf/2509.10742v2,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2509.10742v2.pdf
2509.01887v1,2025-09-02T02:24:55Z,2025-09-02 02:24:55,Design of Experiment for Discovering Directed Mixed Graph,"We study the problem of experimental design for accurately identifying the causal graph structure of a simple structural causal model (SCM), where the underlying graph may include both cycles and bidirected edges induced by latent confounders. The presence of cycles renders it impossible to recover the graph skeleton using observational data alone, while confounding can further invalidate traditional conditional independence (CI) tests in certain scenarios. To address these challenges, we establish lower bounds on both the maximum number of variables that can be intervened upon in a single experiment and the total number of experiments required to identify all directed edges and non-adjacent bidirected edges. Leveraging both CI tests and do see tests, and accounting for $d$ separation and $σ$ separation, we develop two classes of algorithms, i.e., bounded and unbounded, that can recover all causal edges except for double adjacent bidirected edges. We further show that, up to logarithmic factors, the proposed algorithms are tight with respect to the derived lower bounds.",Haijie Xu|Chen Zhang,,https://arxiv.org/abs/2509.01887v1,https://arxiv.org/pdf/2509.01887v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2509.01887v1.pdf
2508.21450v1,2025-08-29T09:29:02Z,2025-08-29 09:29:02,Computationally Tractable Offline Quantum Experimental Design for Nuclear Spin Detection,"The characterization of nuclear spin environments in solid-state devices plays an important role in advancing quantum technologies, yet traditional methods often demand long measurement times. To address this challenge, we extend our recently developed deep-learning-based SALI model (Signal-to-image ArtificiaL Intelligence) by introducing the surrogate information gain (SIG) to optimize the selection of data points in the measurements. This approach significantly reduces time requirements in experiments while preserving accuracy in nuclear spin detection. The SIG is a figure of merit based on the expected variance of the signal, which is more straightforward to compute than the expected information gain rooted in Bayesian estimation. We demonstrate our approach on a nitrogen-vacancy (NV) center in diamond coupled to $^{13}$C nuclei. In the high-field regime, our variance-based optimization is validated with experimental data, resulting in an 85$\%$ reduction in measurement time for a modest reduction in performance. This work also constitutes the first validation of SALI on experimental data. In the low-field regime, we explore its performance on simulated data, predicting a 60$\%$ reduction in the total experiment time by improving the temporal resolution of the measurements and applying SIG. This demonstrates the potential of integrating deep learning with optimized signal selection to enhance the efficiency of quantum sensing and nuclear spin characterization, paving the way for scaling these techniques to larger nuclear spin systems.",B. Varona-Uriarte|F. Belliardo|T. H. Taminiau|C. Bonato|E. Garrote|J. Casanova,,https://arxiv.org/abs/2508.21450v1,https://arxiv.org/pdf/2508.21450v1,,"main: 9 pages + 3 figures, supplemental material: 11 pages + 1 figure",,,quant-ph,quant-ph|physics.data-an,https://arxiv.org/pdf/2508.21450v1.pdf
2508.21184v2,2025-08-28T19:51:43Z,2025-10-18 23:14:21,BED-LLM: Intelligent Information Gathering with LLMs and Bayesian Experimental Design,"We propose a general-purpose approach for improving the ability of Large Language Models (LLMs) to intelligently and adaptively gather information from a user or other external source using the framework of sequential Bayesian experimental design (BED). This enables LLMs to act as effective multi-turn conversational agents and interactively interface with external environments. Our approach, which we call BED-LLM (Bayesian Experimental Design with Large Language Models), is based on iteratively choosing questions or queries that maximize the expected information gain (EIG) about the task of interest given the responses gathered previously. We show how this EIG can be formulated (and then estimated) in a principled way using a probabilistic model derived from the LLM's predictive distributions and provide detailed insights into key decisions in its construction and updating procedure. We find that BED-LLM achieves substantial gains in performance across a wide range of tests based on the 20 questions game and using the LLM to actively infer user preferences, compared to direct prompting of the LLM and other adaptive design strategies.",Deepro Choudhury|Sinead Williamson|Adam Goliński|Ning Miao|Freddie Bickford Smith|Michael Kirchhof|Yizhe Zhang|Tom Rainforth,,https://arxiv.org/abs/2508.21184v2,https://arxiv.org/pdf/2508.21184v2,,,,,cs.CL,cs.CL|cs.AI|stat.ML,https://arxiv.org/pdf/2508.21184v2.pdf
2508.12288v1,2025-08-17T08:35:54Z,2025-08-17 08:35:54,An optimal experimental design approach to sensor placement in continuous stochastic filtering,"Sequential filtering and spatial inverse problems assimilate data points distributed either temporally (in the case of filtering) or spatially (in the case of spatial inverse problems). Sometimes it is possible to choose the position of these data points (which we call sensors here) in advance, with the goal of maximising the expected information gain (or a different metric of performance) from future data, and this leads to an Optimal Experimental Design (OED) problem. Here we revisit an interpretation of optimising sensor placement as an integration with respect to a general probability measure $ξ$. This generalises the problem of discrete-time sensor placement (which corresponds to the special case where the probability measure is a mixture of Diracs) to an infinite-dimensional, but mathematically more well-behaved setting. We focus on the continuous-time stochastic filtering setting, whose solution is governed by the Zakai equation. We derive an expression for the Fréchet derivative of a general OED utility functional, the key to which is an adjoint (backwards in time) differential equation. This paves the way for utilising new gradient-based methods for solving the corresponding optimisation problem, as a potentially more efficient alternative to (semi-)discrete optimisation methods, e.g. based on greedy insertion and deletion of sensor placements.",Sahani Pathiraja|Claudia Schillings|Philipp Wacker,,https://arxiv.org/abs/2508.12288v1,https://arxiv.org/pdf/2508.12288v1,,,,,math.ST,math.ST,https://arxiv.org/pdf/2508.12288v1.pdf
2508.07120v1,2025-08-09T23:41:58Z,2025-08-09 23:41:58,Low Cost Bayesian Experimental Design for Quantum Frequency Estimation with Decoherence,"A two-level quantum system evolving under a time-independent Hamiltonian produces oscillatory measurement probabilities. The estimation of the associated frequency is a cornerstone problem in quantum metrology, sensing, calibration and control. In this work, we tackle this task by introducing WES: a Window Expansion Strategy for low cost adaptive Bayesian experimental design. WES employs empirical cost-reduction techniques to keep the optimization overhead low, curb scaling problems, and enable high degrees of parallelism. Unlike previous heuristics, it offers adjustable classical processing costs that determine the performance standard. As a benchmark, we analyze the performance of widely adopted heuristics, comparing them with the fundamental limits of metrology and a baseline random strategy. Numerical simulations show that WES delivers the most reliable performance and fastest learning rate, saturating the Heisenberg limit.",Alexandra Ramôa|Luís Paulo Santos|Akihito Soeda,,https://arxiv.org/abs/2508.07120v1,https://arxiv.org/pdf/2508.07120v1,,,,,quant-ph,quant-ph,https://arxiv.org/pdf/2508.07120v1.pdf
2508.07074v1,2025-08-09T18:37:01Z,2025-08-09 18:37:01,An Optimization Perspective on the Monotonicity of the Multiplicative Algorithm for Optimal Experimental Design,"We provide an optimization-based argument for the monotonicity of the multiplicative algorithm (MA) for a class of optimal experimental design problems considered in Yu (2010). Our proof avoids introducing auxiliary variables (or problems) and leveraging statistical arguments, and is much more straightforward and simpler compared to the proof in Yu (2010). The simplicity of our monotonicity proof also allows us to easily identify several sufficient conditions that ensure the strict monotonicity of MA. In addition, we provide two simple and similar-looking examples on which MA behaves very differently. These examples offer insight in the behaviors of MA, and also reveal some limitations of MA when applied to certain optimality criteria. We discuss these limitations, and pose open problems that may lead to deeper understanding of the behaviors of MA on these optimality criteria.",Renbo Zhao,,https://arxiv.org/abs/2508.07074v1,https://arxiv.org/pdf/2508.07074v1,,14 pages,,,math.OC,math.OC,https://arxiv.org/pdf/2508.07074v1.pdf
2508.03948v1,2025-08-05T22:31:12Z,2025-08-05 22:31:12,Bayesian Design of Experiments in the Presence of Nuisance Parameters,"Design of experiments has traditionally relied on the frequentist hypothesis testing framework where the optimal size of the experiment is specified as the minimum sample size that guarantees a required level of power. Sample size determination may be performed analytically when the test statistic has a known asymptotic sampling distribution and, therefore, the power function is available in analytic form. Bayesian methods have gained popularity in all stages of discovery, namely, design, analysis and decision making. Bayesian decision procedures rely on posterior summaries whose sampling distributions are commonly estimated via Monte Carlo simulations. In the design of scientific studies, the Bayesian approach incorporates uncertainty about the design value(s) instead of conditioning on a single value of the model parameter(s). Accounting for uncertainties in the design value(s) is particularly critical when the model includes nuisance parameters. In this manuscript, we propose methodology that utilizes the large-sample properties of the posterior distribution together with Bayesian additive regression trees (BART) to efficiently obtain the optimal sample size and decision criteria in fixed and adaptive designs. We introduce a fully Bayesian procedure that incorporates the uncertainty associated with the model parameters including the nuisance parameters at the design stage. The proposed approach significantly reduces the computational burden associated with Bayesian design and enables the wide adoption of Bayesian operating characteristics.",Shirin Golchi|Luke Hagar,,https://arxiv.org/abs/2508.03948v1,https://arxiv.org/pdf/2508.03948v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2508.03948v1.pdf
2507.16307v1,2025-07-22T07:48:32Z,2025-07-22 07:48:32,Perovskite-R1: A Domain-Specialized LLM for Intelligent Discovery of Precursor Additives and Experimental Design,"Perovskite solar cells (PSCs) have rapidly emerged as a leading contender in next-generation photovoltaic technologies, owing to their exceptional power conversion efficiencies and advantageous material properties. Despite these advances, challenges such as long-term stability, environmental sustainability, and scalable manufacturing continue to hinder their commercialization. Precursor additive engineering has shown promise in addressing these issues by enhancing both the performance and durability of PSCs. However, the explosive growth of scientific literature and the complex interplay of materials, processes, and device architectures make it increasingly difficult for researchers to efficiently access, organize, and utilize domain knowledge in this rapidly evolving field. To address this gap, we introduce Perovskite-R1, a specialized large language model (LLM) with advanced reasoning capabilities tailored for the discovery and design of PSC precursor additives. By systematically mining and curating 1,232 high-quality scientific publications and integrating a comprehensive library of 33,269 candidate materials, we constructed a domain-specific instruction-tuning dataset using automated question-answer generation and chain-of-thought reasoning. Fine-tuning the QwQ-32B model on this dataset resulted in Perovskite-R1, which can intelligently synthesize literature insights and generate innovative and practical solutions for defect passivation and the selection of precursor additives. Experimental validation of several model-proposed strategies confirms their effectiveness in improving material stability and performance. Our work demonstrates the potential of domain-adapted LLMs in accelerating materials discovery and provides a closed-loop framework for intelligent, data-driven advancements in perovskite photovoltaic research.",Xin-De Wang|Zhi-Rui Chen|Peng-Jie Guo|Ze-Feng Gao|Cheng Mu|Zhong-Yi Lu,,https://arxiv.org/abs/2507.16307v1,https://arxiv.org/pdf/2507.16307v1,,24 pages; 5 figures,,,cs.LG,cs.LG|cond-mat.mtrl-sci|cs.AI|physics.chem-ph,https://arxiv.org/pdf/2507.16307v1.pdf
2507.15235v1,2025-07-21T04:41:05Z,2025-07-21 04:41:05,Accelerated Bayesian Optimal Experimental Design via Conditional Density Estimation and Informative Data,"The Design of Experiments (DOEs) is a fundamental scientific methodology that provides researchers with systematic principles and techniques to enhance the validity, reliability, and efficiency of experimental outcomes. In this study, we explore optimal experimental design within a Bayesian framework, utilizing Bayes' theorem to reformulate the utility expectation--originally expressed as a nested double integral--into an independent double integral form, significantly improving numerical efficiency. To further accelerate the computation of the proposed utility expectation, conditional density estimation is employed to approximate the ratio of two Gaussian random fields, while covariance serves as a selection criterion to identify informative datasets during model fitting and integral evaluation. In scenarios characterized by low simulation efficiency and high costs of raw data acquisition, key challenges such as surrogate modeling, failure probability estimation, and parameter inference are systematically restructured within the Bayesian experimental design framework. The effectiveness of the proposed methodology is validated through both theoretical analysis and practical applications, demonstrating its potential for enhancing experimental efficiency and decision-making under uncertainty.",Miao Huang|Hongqiao Wang|Kunyu Wu,,https://arxiv.org/abs/2507.15235v1,https://arxiv.org/pdf/2507.15235v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2507.15235v1.pdf
2507.14057v1,2025-07-18T16:39:56Z,2025-07-18 16:39:56,Step-DAD: Semi-Amortized Policy-Based Bayesian Experimental Design,"We develop a semi-amortized, policy-based, approach to Bayesian experimental design (BED) called Stepwise Deep Adaptive Design (Step-DAD). Like existing, fully amortized, policy-based BED approaches, Step-DAD trains a design policy upfront before the experiment. However, rather than keeping this policy fixed, Step-DAD periodically updates it as data is gathered, refining it to the particular experimental instance. This test-time adaptation improves both the flexibility and the robustness of the design strategy compared with existing approaches. Empirically, Step-DAD consistently demonstrates superior decision-making and robustness compared with current state-of-the-art BED methods.",Marcel Hedman|Desi R. Ivanova|Cong Guan|Tom Rainforth,,https://arxiv.org/abs/2507.14057v1,https://arxiv.org/pdf/2507.14057v1,,"Accepted at Proceedings of the 42nd International Conference on Machine Learning, Vancouver, Canada. PMLR 267, 2025","Proceedings of the 42nd International Conference on Machine Learning, PMLR 267:22904-22923, 2025",,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2507.14057v1.pdf
2507.08170v1,2025-07-10T21:03:50Z,2025-07-10 21:03:50,Optimal Experimental Design for Microplastics Sampling Experiments,"Microplastics contamination is one of the most rapidly growing research topics. However, monitoring microplastics contamination in the environment presents both logistical and statistical challenges, particularly when constrained resources limit the scale of sampling and laboratory analysis. In this paper, we propose a Bayesian framework for the optimal experimental design of microplastic sampling campaigns. Our approach integrates prior knowledge and uncertainty quantification to guide decisions on how many spatial Centrosamples to collect and how many particles to analyze for polymer composition. By modeling particle counts as a Poisson distribution and polymer types as a Multinomial distribution, we developed a conjugate Bayesian model that enables efficient posterior inference. We introduce variance-based loss functions to evaluate expected information gain for both abundance and composition, and we formulate a constrained optimization problem that incorporates realistic cost structures. Our results provide principled and interpretable recommendations for allocating limited resources across the sampling and analysis phases. Through simulated scenarios and real-world-inspired examples, we demonstrate how the proposed methodology adapts to prior assumptions and cost variations, ensuring robustness and flexibility. This work contributes to the broader field of Bayesian experimental design by offering a concrete, application-driven case study that underscores the value of formal design strategies in environmental monitoring contexts.",Marco A. Aquino-López|Ana Carolina Ruiz-Fernández|Joan-Albert Sanchez-Cabeza|J. Andrés Christen,,https://arxiv.org/abs/2507.08170v1,https://arxiv.org/pdf/2507.08170v1,,"16 pages, 6 figures, research paper",,,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/2507.08170v1.pdf
2507.07359v1,2025-07-10T00:53:57Z,2025-07-10 00:53:57,Goal-Oriented Sequential Bayesian Experimental Design for Causal Learning,"We present GO-CBED, a goal-oriented Bayesian framework for sequential causal experimental design. Unlike conventional approaches that select interventions aimed at inferring the full causal model, GO-CBED directly maximizes the expected information gain (EIG) on user-specified causal quantities of interest, enabling more targeted and efficient experimentation. The framework is both non-myopic, optimizing over entire intervention sequences, and goal-oriented, targeting only model aspects relevant to the causal query. To address the intractability of exact EIG computation, we introduce a variational lower bound estimator, optimized jointly through a transformer-based policy network and normalizing flow-based variational posteriors. The resulting policy enables real-time decision-making via an amortized network. We demonstrate that GO-CBED consistently outperforms existing baselines across various causal reasoning and discovery tasks-including synthetic structural causal models and semi-synthetic gene regulatory networks-particularly in settings with limited experimental budgets and complex causal mechanisms. Our results highlight the benefits of aligning experimental design objectives with specific research goals and of forward-looking sequential planning.",Zheyu Zhang|Jiayuan Dong|Jie Liu|Xun Huan,University of Michigan|University of Michigan|University of Michigan|University of Michigan,https://arxiv.org/abs/2507.07359v1,https://arxiv.org/pdf/2507.07359v1,,"10 pages, 6 figures",,,cs.LG,cs.LG|cs.AI|stat.ME|stat.ML,https://arxiv.org/pdf/2507.07359v1.pdf
2507.03210v1,2025-07-03T22:50:05Z,2025-07-03 22:50:05,A column generation approach to exact experimental design,"In this work, we address the exact D-optimal experimental design problem by proposing an efficient algorithm that rapidly identifies the support of its continuous relaxation. Our method leverages a column generation framework to solve such a continuous relaxation, where each restricted master problem is tackled using a Primal-Dual Interior-Point-based Semidefinite Programming solver. This enables fast and reliable detection of the design's support. The identified support is subsequently used to construct a feasible exact design that is provably close to optimal. We show that, for large-scale instances in which the number of regression points exceeds by far the number of experiments, our approach achieves superior performance compared to existing branch-and-bound-based algorithms in both computational efficiency and solution quality.",Selin Ahipasaoglu|Stefano Cipolla|Jacek Gondzio,,https://arxiv.org/abs/2507.03210v1,https://arxiv.org/pdf/2507.03210v1,,,,,math.OC,math.OC,https://arxiv.org/pdf/2507.03210v1.pdf
2507.00923v1,2025-07-01T16:28:37Z,2025-07-01 16:28:37,ForLion: An R Package for Finding Optimal Experimental Designs with Mixed Factors,"Optimal design is crucial for experimenters to maximize the information collected from experiments and estimate the model parameters most accurately. ForLion algorithms have been proposed to find D-optimal designs for experiments with mixed types of factors. In this paper, we introduce the ForLion package which implements the ForLion algorithm to construct locally D-optimal designs and the EW ForLion algorithm to generate robust EW D-optimal designs. The package supports experiments under linear models (LM), generalized linear models (GLM), and multinomial logistic models (MLM) with continuous, discrete, or mixed-type factors. It provides both optimal approximate designs and an efficient function converting approximate designs into exact designs with integer-valued allocations of experimental units. Tutorials are included to show the package's usage across different scenarios.",Siting Lin|Yifei Huang|Jie Yang,,https://arxiv.org/abs/2507.00923v1,https://arxiv.org/pdf/2507.00923v1,,"31 pages, 4 figures, 5 tables",,,stat.CO,stat.CO,https://arxiv.org/pdf/2507.00923v1.pdf
2506.21754v1,2025-06-26T20:19:15Z,2025-06-26 20:19:15,Online design of experiments by active learning for nonlinear system identification,"We investigate the use of active-learning (AL) strategies to generate the input excitation signal at runtime for system identification of linear and nonlinear autoregressive and state-space models. We adapt various existing AL approaches for static model regression to the dynamic context, coupling them with a Kalman filter to update the model parameters recursively, and also cope with the presence of input and output constraints. We show the increased sample efficiency of the proposed approaches with respect to random excitation on different nonlinear system identification benchmarks.",Kui Xie|Alberto Bemporad,,https://arxiv.org/abs/2506.21754v1,https://arxiv.org/pdf/2506.21754v1,,,,,eess.SY,eess.SY,https://arxiv.org/pdf/2506.21754v1.pdf
2506.19146v1,2025-06-23T21:32:38Z,2025-06-23 21:32:38,Optimal Design of Experiment for Electrochemical Parameter Identification of Li-ion Battery via Deep Reinforcement Learning,"Accurate parameter estimation in electrochemical battery models is essential for monitoring and assessing the performance of lithium-ion batteries (LiBs). This paper presents a novel approach that combines deep reinforcement learning (DRL) with an optimal experimental design (OED) framework to identify key electrochemical parameters of LiB cell models. The proposed method utilizes the twin delayed deep deterministic policy gradient (TD3) algorithm to optimize input excitation, thereby increasing the sensitivity of the system response to electrochemical parameters. The performance of this DRL-based approach is evaluated against a nonlinear model predictive control (NMPC) method and conventional tests. Results indicate that the DRL-based method provides superior information content, reflected in higher Fisher information (FI) values and lower parameter estimation errors compared to the NMPC design and conventional test practices. Additionally, the DRL approach offers a substantial reduction in experimental time and computational resources.",Mehmet Fatih Ozkan|Samuel Filgueira da Silva|Faissal El Idrissi|Prashanth Ramesh|Marcello Canova,,https://arxiv.org/abs/2506.19146v1,https://arxiv.org/pdf/2506.19146v1,,,,,eess.SY,eess.SY,https://arxiv.org/pdf/2506.19146v1.pdf
2506.16237v1,2025-06-19T11:48:30Z,2025-06-19 11:48:30,Active MRI Acquisition with Diffusion Guided Bayesian Experimental Design,"A key challenge in maximizing the benefits of Magnetic Resonance Imaging (MRI) in clinical settings is to accelerate acquisition times without significantly degrading image quality. This objective requires a balance between under-sampling the raw k-space measurements for faster acquisitions and gathering sufficient raw information for high-fidelity image reconstruction and analysis tasks. To achieve this balance, we propose to use sequential Bayesian experimental design (BED) to provide an adaptive and task-dependent selection of the most informative measurements. Measurements are sequentially augmented with new samples selected to maximize information gain on a posterior distribution over target images. Selection is performed via a gradient-based optimization of a design parameter that defines a subsampling pattern. In this work, we introduce a new active BED procedure that leverages diffusion-based generative models to handle the high dimensionality of the images and employs stochastic optimization to select among a variety of patterns while meeting the acquisition process constraints and budget. So doing, we show how our setting can optimize, not only standard image reconstruction, but also any associated image analysis task. The versatility and performance of our approach are demonstrated on several MRI acquisitions.",Jacopo Iollo|Geoffroy Oudoumanessah|Carole Lartizien|Michel Dojat|Florence Forbes,,https://arxiv.org/abs/2506.16237v1,https://arxiv.org/pdf/2506.16237v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2506.16237v1.pdf
2506.14593v1,2025-06-17T14:58:30Z,2025-06-17 14:58:30,The use of cross validation in the analysis of designed experiments,"Cross-validation (CV) is a common method to tune machine learning methods and can be used for model selection in regression as well. Because of the structured nature of small, traditional experimental designs, the literature has warned against using CV in their analysis. The striking increase in the use of machine learning, and thus CV, in the analysis of experimental designs, has led us to empirically study the effectiveness of CV compared to other methods of selecting models in designed experiments, including the little bootstrap. We consider both response surface settings where prediction is of primary interest, as well as screening where factor selection is most important. Overall, we provide evidence that the use of leave-one-out cross-validation (LOOCV) in the analysis of small, structured is often useful. More general $k$-fold CV may also be competitive but its performance is uneven.",Maria L. Weese|Byran J. Smucker|David J. Edwards,,https://arxiv.org/abs/2506.14593v1,https://arxiv.org/pdf/2506.14593v1,,,,,stat.AP,stat.AP|stat.ML,https://arxiv.org/pdf/2506.14593v1.pdf
2506.13390v2,2025-06-16T11:53:00Z,2025-06-17 08:20:19,Experimental Design for Semiparametric Bandits,"We study finite-armed semiparametric bandits, where each arm's reward combines a linear component with an unknown, potentially adversarial shift. This model strictly generalizes classical linear bandits and reflects complexities common in practice. We propose the first experimental-design approach that simultaneously offers a sharp regret bound, a PAC bound, and a best-arm identification guarantee. Our method attains the minimax regret $\tilde{O}(\sqrt{dT})$, matching the known lower bound for finite-armed linear bandits, and further achieves logarithmic regret under a positive suboptimality gap condition. These guarantees follow from our refined non-asymptotic analysis of orthogonalized regression that attains the optimal $\sqrt{d}$ rate, paving the way for robust and efficient learning across a broad class of semiparametric bandit problems.",Seok-Jin Kim|Gi-Soo Kim|Min-hwan Oh,,https://arxiv.org/abs/2506.13390v2,https://arxiv.org/pdf/2506.13390v2,,Accepted at COLT 2025,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2506.13390v2.pdf
2506.12677v1,2025-06-15T01:10:06Z,2025-06-15 01:10:06,Dependent Randomized Rounding for Budget Constrained Experimental Design,"Policymakers in resource-constrained settings require experimental designs that satisfy strict budget limits while ensuring precise estimation of treatment effects. We propose a framework that applies a dependent randomized rounding procedure to convert assignment probabilities into binary treatment decisions. Our proposed solution preserves the marginal treatment probabilities while inducing negative correlations among assignments, leading to improved estimator precision through variance reduction. We establish theoretical guarantees for the inverse propensity weighted and general linear estimators, and demonstrate through empirical studies that our approach yields efficient and accurate inference under fixed budget constraints.",Khurram Yamin|Edward Kennedy|Bryan Wilder,,https://arxiv.org/abs/2506.12677v1,https://arxiv.org/pdf/2506.12677v1,,UAI 2025 Paper,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2506.12677v1.pdf
2506.12157v1,2025-06-13T18:23:00Z,2025-06-13 18:23:00,Optimal Experimental Design Criteria for Data-Consistent Inversion,"The ability to design effective experiments is crucial for obtaining data that can substantially reduce the uncertainty in the predictions made using computational models. An optimal experimental design (OED) refers to the choice of a particular experiment that optimizes a particular design criteria, e.g., maximizing a utility function, which measures the information content of the data. However, traditional approaches for optimal experimental design typically require solving a large number of computationally intensive inverse problems to find the data that maximizes the utility function. Here, we introduce two novel OED criteria that are specifically crafted for the data consistent inversion (DCI) framework, but do not require solving inverse problems. DCI is a specific approach for solving a class of stochastic inverse problems by constructing a pullback measure on uncertain parameters from an observed probability measure on the outputs of a quantity of interest (QoI) map. While expected information gain (EIG) has been used for both DCI and Bayesian based OED, the characteristics and properties of DCI solutions differ from those of solutions to Bayesian inverse problems which should be reflected in the OED criteria. The new design criteria developed in this study, called the expected scaling effect and the expected skewness effect, leverage the geometric structure of pre-images associated with observable data sets, allowing for an intuitive and computationally efficient approach to OED. These criteria utilize singular value computations derived from sampled and approximated Jacobians of the experimental designs. We present both simultaneous and sequential (greedy) formulations of OED based on these innovative criteria. Numerical results demonstrate the effectiveness in our approach for solving stochastic inverse problems.",Troy Butler|John Jakeman|Michael Pilosov|Scott Walsh|Timothy Wildey,,https://arxiv.org/abs/2506.12157v1,https://arxiv.org/pdf/2506.12157v1,,,,,stat.ME,stat.ME|math.PR,https://arxiv.org/pdf/2506.12157v1.pdf
2506.09722v1,2025-06-11T13:32:56Z,2025-06-11 13:32:56,Fully Bayesian Sequential Design for Mean Response Surface Prediction of Heteroscedastic Stochastic Simulations,"We present a fully Bayesian sequential strategy for predicting the mean response surface of heteroscedastic stochastic simulation functions. Leveraging dual Gaussian processes as the surrogate model and a criterion based on empirical expected integrated mean-square prediction error, our approach sequentially selects informative design points while fully accounting for parameter uncertainty. Sequential importance sampling is employed to efficiently update the posterior distribution of the parameters. Our strategy is tailored for expensive simulation functions, where achieving robust predictive accuracy under a limited budget is critical. We illustrate its potential advantages compared to existing approaches through synthetic examples. We then implement the proposed strategy on a real motivating application in seismic design of wood-frame podium buildings.",Yuying Huang|Samuel W. K. Wong,,https://arxiv.org/abs/2506.09722v1,https://arxiv.org/pdf/2506.09722v1,,"37 pages, 8 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2506.09722v1.pdf
2506.09508v2,2025-06-11T08:27:16Z,2025-12-03 21:18:52,Efficient Preference-Based Reinforcement Learning: Randomized Exploration Meets Experimental Design,"We study reinforcement learning from human feedback in general Markov decision processes, where agents learn from trajectory-level preference comparisons. A central challenge in this setting is to design algorithms that select informative preference queries to identify the underlying reward while ensuring theoretical guarantees. We propose a meta-algorithm based on randomized exploration, which avoids the computational challenges associated with optimistic approaches and remains tractable. We establish both regret and last-iterate guarantees under mild reinforcement learning oracle assumptions. To improve query complexity, we introduce and analyze an improved algorithm that collects batches of trajectory pairs and applies optimal experimental design to select informative comparison queries. The batch structure also enables parallelization of preference queries, which is relevant in practical deployment as feedback can be gathered concurrently. Empirical evaluation confirms that the proposed method is competitive with reward-based reinforcement learning while requiring a small number of preference queries.",Andreas Schlaginhaufen|Reda Ouhamma|Maryam Kamgarpour,,https://arxiv.org/abs/2506.09508v2,https://arxiv.org/pdf/2506.09508v2,,,,,cs.LG,cs.LG|cs.AI|cs.RO|stat.ML,https://arxiv.org/pdf/2506.09508v2.pdf
2506.03363v1,2025-06-03T20:15:08Z,2025-06-03 20:15:08,Probabilistic Factorial Experimental Design for Combinatorial Interventions,"A combinatorial intervention, consisting of multiple treatments applied to a single unit with potentially interactive effects, has substantial applications in fields such as biomedicine, engineering, and beyond. Given $p$ possible treatments, conducting all possible $2^p$ combinatorial interventions can be laborious and quickly becomes infeasible as $p$ increases. Here we introduce probabilistic factorial experimental design, formalized from how scientists perform lab experiments. In this framework, the experimenter selects a dosage for each possible treatment and applies it to a group of units. Each unit independently receives a random combination of treatments, sampled from a product Bernoulli distribution determined by the dosages. Additionally, the experimenter can carry out such experiments over multiple rounds, adapting the design in an active manner. We address the optimal experimental design problem within an intervention model that imposes bounded-degree interactions between treatments. In the passive setting, we provide a closed-form solution for the near-optimal design. Our results prove that a dosage of $\tfrac{1}{2}$ for each treatment is optimal up to a factor of $1+O(\tfrac{\ln(n)}{n})$ for estimating any $k$-way interaction model, regardless of $k$, and imply that $O\big(kp^{3k}\ln(p)\big)$ observations are required to accurately estimate this model. For the multi-round setting, we provide a near-optimal acquisition function that can be numerically optimized. We also explore several extensions of the design problem and finally validate our findings through simulations.",Divya Shyamal|Jiaqi Zhang|Caroline Uhler,,https://arxiv.org/abs/2506.03363v1,https://arxiv.org/pdf/2506.03363v1,,,,,cs.LG,cs.LG|stat.ME|stat.ML,https://arxiv.org/pdf/2506.03363v1.pdf
2506.03062v1,2025-06-03T16:41:11Z,2025-06-03 16:41:11,Multi-Metric Adaptive Experimental Design under Fixed Budget with Validation,"Standard A/B tests in online experiments face statistical power challenges when testing multiple candidates simultaneously, while adaptive experimental designs (AED) alone fall short in inferring experiment statistics such as the average treatment effect, especially with many metrics (e.g., revenue, safety) and heterogeneous variances. This paper proposes a fixed-budget multi-metric AED framework with a two-phase structure: an adaptive exploration phase to identify the best treatment, and a validation phase with an A/B test to verify the treatment's quality and infer statistics. We propose SHRVar, which generalizes sequential halving (SH) (Karnin et al., 2013) with a novel relative-variance-based sampling and an elimination strategy built on reward z-values. It achieves a provable error probability that decreases exponentially, where the exponent generalizes the complexity measure for SH (Karnin et al., 2013) and SHVar (Lalitha et al., 2023) with homogeneous and heterogeneous variances, respectively. Numerical experiments verify our analysis and demonstrate the superior performance of this new framework.",Qining Zhang|Tanner Fiez|Yi Liu|Wenyang Liu,,https://arxiv.org/abs/2506.03062v1,https://arxiv.org/pdf/2506.03062v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2506.03062v1.pdf
2506.01619v3,2025-06-02T12:58:25Z,2025-08-27 14:07:24,A projector--rank partition theorem for exact degrees of freedom in experimental design,"In many experimental designs\textemdash split-plots, blocked or nested layouts, fractional factorials, and studies with missing or unequal replication\textemdash standard ANOVA procedures no longer tell us exactly how many independent pieces of information each effect truly carries. We provide a general degrees of freedom $(\mathrm{df})$ partition theorem that resolves this ambiguity. For $N$ observations, we show that the total information in the data ({\ie}, $N-1$ $\mathrm{df}$) can be split exactly across experimental effects and randomization strata by projecting the data onto each stratum and counting the $\mathrm{df}$ each effect contributes there. This yields integer $\mathrm{df}$\textemdash not approximations\textemdash for any mix of fixed and random effects, blocking structures, fractionation, or imbalance. This result yields closed-form $\mathrm{df}$ tables for unbalanced split-plot, row-column, lattice, and crossed-nested designs. We introduce practical diagnostics\textemdash the $\mathrm{df}$-retention ratio $ρ$, df deficiency $δ$, and variance-inflation index $α$\textemdash that measure exactly how many $\mathrm{df}$ an effect retains under blocking or fractionation and the resulting loss of precision, thereby extending Box--Hunter's resolution idea to multi-stratum and incomplete designs. Classical results emerge as corollaries: Cochran's one-stratum identity; Yates's split-plot $\mathrm{df}$; resolution-$R$ identified when an effect retains no $\mathrm{df}$. Empirical studies on split-plot and nested designs, a blocked fractional-factorial design-selection experiment, and timing benchmarks show that our approach delivers calibrated error rates, recovers information to raise power by up to 60\% without additional runs, and is orders of magnitude faster than bootstrap-based $\mathrm{df}$ approximations.",Nagananda K G,,https://arxiv.org/abs/2506.01619v3,https://arxiv.org/pdf/2506.01619v3,,19 pages,,,math.ST,math.ST,https://arxiv.org/pdf/2506.01619v3.pdf
2506.00336v1,2025-05-31T01:29:50Z,2025-05-31 01:29:50,Structured Column Subset Selection for Bayesian Optimal Experimental Design,"We consider optimal experimental design (OED) for Bayesian inverse problems, where the experimental design variables have a certain multiway structure. Given $d$ different experimental variables with $m_i$ choices per design variable $1 \le i\le d$, the goal is to select $k_i \le m_i$ experiments per design variable. Previous work has related OED to the column subset selection problem by mapping the design variables to the columns of a matrix $\mathbf{A}$. However, this approach is applicable only to the case $d=1$ in which the columns can be selected independently. We develop an extension to the case where the design variables have a multi-way structure. Our approach is to map the matrix $\mathbf{A}$ to a tensor and perform column subset selection on mode unfoldings of the tensor. We develop an algorithmic framework with three different algorithmic templates, and randomized variants of these algorithms. We analyze the computational cost of all the proposed algorithms and also develop greedy versions to facilitate comparisons. Numerical experiments on four different applications -- time-dependent inverse problems, seismic tomography, X-ray tomography, and flow reconstruction -- demonstrate the effectiveness and scalability of our methods for structured experimental design in Bayesian inverse problems.",Hugo Díaz|Arvind K. Saibaba|Srinivas Eswar|Vishwas Rao|Zichao Wendy Di,,https://arxiv.org/abs/2506.00336v1,https://arxiv.org/pdf/2506.00336v1,,,,,math.NA,math.NA,https://arxiv.org/pdf/2506.00336v1.pdf
2505.20130v3,2025-05-26T15:29:01Z,2025-08-28 08:49:03,Balancing Interference and Correlation in Spatial Experimental Designs: A Causal Graph Cut Approach,"This paper focuses on the design of spatial experiments to optimize the amount of information derived from the experimental data and enhance the accuracy of the resulting causal effect estimator. We propose a surrogate function for the mean squared error (MSE) of the estimator, which facilitates the use of classical graph cut algorithms to learn the optimal design. Our proposal offers three key advances: (1) it accommodates moderate to large spatial interference effects; (2) it adapts to different spatial covariance functions; (3) it is computationally efficient. Theoretical results and numerical experiments based on synthetic environments and a dispatch simulator that models a city-scale ridesharing market, further validate the effectiveness of our design. A python implementation of our method is available at https://github.com/Mamba413/CausalGraphCut.",Jin Zhu|Jingyi Li|Hongyi Zhou|Yinan Lin|Zhenhua Lin|Chengchun Shi,,https://arxiv.org/abs/2505.20130v3,https://arxiv.org/pdf/2505.20130v3,,Accepted by ICML2025,,,cs.LG,cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/2505.20130v3.pdf
2505.13283v1,2025-05-19T16:06:23Z,2025-05-19 16:06:23,Accelerating Bayesian Optimal Experimental Design via Local Radial Basis Functions: Application to Soft Material Characterization,"We develop a computational approach that significantly improves the efficiency of Bayesian optimal experimental design (BOED) using local radial basis functions (RBFs). The presented RBF--BOED method uses the intrinsic ability of RBFs to handle scattered parameter points, a property that aligns naturally with the probabilistic sampling inherent in Bayesian methods. By constructing accurate deterministic surrogates from local neighborhood information, the method enables high-order approximations with reduced computational overhead. As a result, computing the expected information gain (EIG) requires evaluating only a small uniformly sampled subset of prior parameter values, greatly reducing the number of expensive forward-model simulations needed. For demonstration, we apply RBF--BOED to optimize a laser-induced cavitation (LIC) experimental setup, where forward simulations follow from inertial microcavitation rheometry (IMR) and characterize the viscoelastic properties of hydrogels. Two experimental design scenarios, single- and multi-constitutive-model problems, are explored. Results show that EIG estimates can be obtained at just 8% of the full computational cost in a five-model problem within a two-dimensional design space. This advance offers a scalable path toward optimal experimental design in soft and biological materials.",Tianyi Chu|Jonathan B. Estrada|Spencer H. Bryngelson,,https://arxiv.org/abs/2505.13283v1,https://arxiv.org/pdf/2505.13283v1,,,,,physics.comp-ph,physics.comp-ph|cond-mat.soft,https://arxiv.org/pdf/2505.13283v1.pdf
2505.09596v2,2025-05-14T17:44:51Z,2025-08-04 20:03:47,Design of Experiments for Emulations: A Selective Review from a Modeling Perspective,"Space-filling designs are crucial for efficient computer experiments, enabling accurate surrogate modeling and uncertainty quantification in many scientific and engineering applications, such as digital twin systems and cyber-physical systems. In this work, we will provide a comprehensive review on key design methodologies, including Maximin/miniMax designs, Latin hypercubes, and projection-based designs. Moreover, we will connect the space-filling design criteria like the fill distance to Gaussian process performance. Numerical studies are conducted to investigate the practical trade-offs among various design types, with the discussion on emerging challenges in high-dimensional and constrained settings. The paper concludes with future directions in adaptive sampling and machine learning integration, providing guidance for improving computational experiments.",Xinwei Deng|Lulu Kang|C. Devon Lin,,https://arxiv.org/abs/2505.09596v2,https://arxiv.org/pdf/2505.09596v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2505.09596v2.pdf
2505.09094v2,2025-05-14T02:57:09Z,2025-09-12 22:47:53,PLanet: Formalizing Assignment Procedures in the Design of Experiments,"Carefully constructed experimental designs are essential for drawing valid, generalizable conclusions from scientific experiments. Unfortunately, experimental designs can be difficult to specify, communicate clearly, and relate to alternatives. In response, we introduce a grammar of composable operators for constructing experimental assignment procedures (e.g., Latin square). The PLanet DSL implements this grammar. Researchers specify assignment requirements. PLanet compiles these into a constraint satisfaction problem over matrices that determines viable experimental plans. In an expressivity evaluation, we find that PLanet is the most expressive compared to two existing experimental design libraries. Its composability enables expression of both canonical and customized designs in HCI experiments. Case studies with three researchers reveal how PLanet helps them make complex design choices explicit, explore alternatives, and develop a deeper understanding of experimental design.",London Bielicke|Anna Zhang|Shruti Tyagi|Emery Berger|Adam Chlipala|Eunice Jun,,https://arxiv.org/abs/2505.09094v2,https://arxiv.org/pdf/2505.09094v2,,16 pages,,,cs.HC,cs.HC,https://arxiv.org/pdf/2505.09094v2.pdf
2505.03990v1,2025-05-06T22:00:17Z,2025-05-06 22:00:17,Batch Sequential Experimental Design for Calibration of Stochastic Simulation Models,Calibration of expensive simulation models involves an emulator based on simulation outputs generated across various parameter settings to replace the actual model. Noisy outputs of stochastic simulation models require many simulation evaluations to understand the complex input-output relationship effectively. Sequential design with an intelligent data collection strategy can improve the efficiency of the calibration process. The growth of parallel computing environments can further enhance calibration efficiency by enabling simultaneous evaluation of the simulation model at a batch of parameters within a sequential design. This article proposes novel criteria that determine if a new batch of simulation evaluations should be assigned to existing parameter locations or unexplored ones to minimize the uncertainty of posterior prediction. Analysis of several simulated models and real-data experiments from epidemiology demonstrates that the proposed approach results in improved posterior predictions.,Özge Sürer,,https://arxiv.org/abs/2505.03990v1,https://arxiv.org/pdf/2505.03990v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2505.03990v1.pdf
2505.01249v2,2025-05-02T13:17:08Z,2025-10-02 12:29:39,Fusing Foveal Fixations Using Linear Retinal Transformations and Bayesian Experimental Design,"Humans (and many vertebrates) face the problem of fusing together multiple fixations of a scene in order to obtain a representation of the whole, where each fixation uses a high-resolution fovea and decreasing resolution in the periphery. In this paper we explicitly represent the retinal transformation of a fixation as a linear downsampling of a high-resolution latent image of the scene, exploiting the known geometry. This linear transformation allows us to carry out exact inference for the latent variables in factor analysis (FA) and mixtures of FA models of the scene. Further, this allows us to formulate and solve the choice of ""where to look next"" as a Bayesian experimental design problem using the Expected Information Gain criterion. Experiments on the Frey faces and MNIST datasets demonstrate the effectiveness of our models.",Christopher K. I. Williams,,https://arxiv.org/abs/2505.01249v2,https://arxiv.org/pdf/2505.01249v2,https://doi.org/10.1162/neco.a.33,"19 pages, 3 figures",Neural Computation 37(12) 2235-2256 (2025),10.1162/neco.a.33,cs.CV,cs.CV|cs.LG,https://arxiv.org/pdf/2505.01249v2.pdf
2505.00629v3,2025-05-01T16:06:59Z,2026-01-06 23:47:17,EW D-optimal Designs for Experiments with Mixed Factors,"We characterize EW D-optimal designs as robust designs against unknown parameter values for experiments under a general parametric model with discrete and continuous factors. When a pilot study is available, we recommend sample-based EW D-optimal designs for subsequent experiments. Otherwise, we recommend EW D-optimal designs under a prior distribution for model parameters. We propose an EW ForLion algorithm for finding EW D-optimal designs with mixed factors, and justify that the designs found by our algorithm are EW D-optimal. To facilitate potential users in practice, we also develop a rounding algorithm that converts an approximate design with mixed factors to exact designs with prespecified grid points and the total number of experimental units. By applying our algorithms for real experiments under multinomial logistic models or generalized linear models, we show that our designs are highly efficient with respect to locally D-optimal designs and more robust against parameter value misspecifications.",Siting Lin|Yifei Huang|Jie Yang,,https://arxiv.org/abs/2505.00629v3,https://arxiv.org/pdf/2505.00629v3,,"41 pages, 13 tables, and 4 figures",,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2505.00629v3.pdf
2504.20319v2,2025-04-29T00:10:45Z,2025-11-22 01:36:02,Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach,"Bayesian experimental design (BED) offers a principled framework for optimizing data acquisition by leveraging probabilistic inference. However, practical implementations of BED are often compromised by model discrepancy, i.e., the mismatch between predictive models and true physical systems, which can potentially lead to biased parameter estimates. While data-driven approaches have been recently explored to characterize the model discrepancy, the resulting high-dimensional parameter space poses severe challenges for both Bayesian updating and design optimization. In this work, we propose a hybrid BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI) that addresses these challenges by providing a computationally efficient, gradient-free alternative to estimate the information gain for high-dimensional network parameters. The AD-EKI allows a differentiable evaluation of the utility function in BED and thus facilitates the use of standard gradient-based methods for design optimization. In the proposed hybrid framework, we iteratively optimize experimental designs, decoupling the inference of low-dimensional physical parameters handled by standard BED methods, from the high-dimensional model discrepancy handled by AD-EKI. The identified optimal designs for the model discrepancy enable us to systematically collect informative data for its calibration. The performance of the proposed method is studied by a classical convection-diffusion BED example, and the hybrid framework enabled by AD-EKI efficiently identifies informative data to calibrate the model discrepancy and robustly infers the unknown physical parameters in the modeled system. Besides addressing the challenges of BED with model discrepancy, AD-EKI also potentially fosters efficient and scalable frameworks in many other areas with bilevel optimization, such as meta-learning and structure optimization.",Huchen Yang|Xinghao Dong|Jin-Long Wu,,https://arxiv.org/abs/2504.20319v2,https://arxiv.org/pdf/2504.20319v2,https://doi.org/10.1016/j.jcp.2025.114469,"36 pages, 13 figures","Journal of Computational Physics, Volume 545, 2026, Article 114469",10.1016/j.jcp.2025.114469,cs.LG,cs.LG,https://arxiv.org/pdf/2504.20319v2.pdf
2504.19233v1,2025-04-27T13:20:57Z,2025-04-27 13:20:57,Optimal experimental design for parameter estimation in the presence of observation noise,"Using mathematical models to assist in the interpretation of experiments is becoming increasingly important in research across applied mathematics, and in particular in biology and ecology. In this context, accurate parameter estimation is crucial; model parameters are used to both quantify observed behaviour, characterise behaviours that cannot be directly measured and make quantitative predictions. The extent to which parameter estimates are constrained by the quality and quantity of available data is known as parameter identifiability, and it is widely understood that for many dynamical models the uncertainty in parameter estimates can vary over orders of magnitude as the time points at which data are collected are varied. Here, we use both local sensitivity measures derived from the Fisher Information Matrix and global measures derived from Sobol' indices to explore how parameter uncertainty changes as the number of measurements, and their placement in time, are varied. We use these measures within an optimisation algorithm to determine the observation times that give rise to the lowest uncertainty in parameter estimates. Applying our framework to models in which the observation noise is both correlated and uncorrelated demonstrates that correlations in observation noise can significantly impact the optimal time points for observing a system, and highlights that proper consideration of observation noise should be a crucial part of the experimental design process.",Jie Qi|Ruth E. Baker,,https://arxiv.org/abs/2504.19233v1,https://arxiv.org/pdf/2504.19233v1,,"24 pages in main text, 28 pages in supplementary materials",,,math.ST,math.ST,https://arxiv.org/pdf/2504.19233v1.pdf
2504.15578v1,2025-04-22T04:25:50Z,2025-04-22 04:25:50,Real-Time Optimal Design of Experiment for Parameter Identification of Li-Ion Cell Electrochemical Model,"Accurately identifying the parameters of electrochemical models of li-ion battery (LiB) cells is a critical task for enhancing the fidelity and predictive ability. Traditional parameter identification methods often require extensive data collection experiments and lack adaptability in dynamic environments. This paper describes a Reinforcement Learning (RL) based approach that dynamically tailors the current profile applied to a LiB cell to optimize the parameters identifiability of the electrochemical model. The proposed framework is implemented in real-time using a Hardware-in-the-Loop (HIL) setup, which serves as a reliable testbed for evaluating the RL-based design strategy. The HIL validation confirms that the RL-based experimental design outperforms conventional test protocols used for parameter identification in terms of both reducing the modeling errors on a verification test and minimizing the duration of the experiment used for parameter identification.",Ian Mikesell|Samuel Filgueira da Silva|Mehmet Fatih Ozkan|Faissal El Idrissi|Prashanth Ramesh|Marcello Canova,,https://arxiv.org/abs/2504.15578v1,https://arxiv.org/pdf/2504.15578v1,,,,,eess.SY,eess.SY|cs.LG,https://arxiv.org/pdf/2504.15578v1.pdf
2504.13382v1,2025-04-18T00:04:12Z,2025-04-18 00:04:12,Intelligent data collection for network discrimination in material flow analysis using Bayesian optimal experimental design,"Material flow analyses (MFAs) are powerful tools for highlighting resource efficiency opportunities in supply chains. MFAs are often represented as directed graphs, with nodes denoting processes and edges representing mass flows. However, network structure uncertainty -- uncertainty in the presence or absence of flows between nodes -- is common and can compromise flow predictions. While collection of more MFA data can reduce network structure uncertainty, an intelligent data acquisition strategy is crucial to optimize the resources (person-hours and money spent on collecting and purchasing data) invested in constructing an MFA. In this study, we apply Bayesian optimal experimental design (BOED), based on the Kullback-Leibler divergence, to efficiently target high-utility MFA data -- data that minimizes network structure uncertainty. We introduce a new method with reduced bias for estimating expected utility, demonstrating its superior accuracy over traditional approaches. We illustrate these advances with a case study on the U.S. steel sector MFA, where the expected utility of collecting specific single pieces of steel mass flow data aligns with the actual reduction in network structure uncertainty achieved by collecting said data from the United States Geological Survey and the World Steel Association. The results highlight that the optimal MFA data to collect depends on the total amount of data being gathered, making it sensitive to the scale of the data collection effort. Overall, our methods support intelligent data acquisition strategies, accelerating uncertainty reduction in MFAs and enhancing their utility for impact quantification and informed decision-making.",Jiankan Liao|Xun Huan|Daniel Cooper,,https://arxiv.org/abs/2504.13382v1,https://arxiv.org/pdf/2504.13382v1,,"21 pages for manuscript, 8 pages for supporting information and bibliography, 8 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/2504.13382v1.pdf
2504.13320v3,2025-04-17T20:16:15Z,2025-09-19 11:01:53,Gradient-Free Sequential Bayesian Experimental Design via Interacting Particle Systems,"We introduce a gradient-free framework for Bayesian Optimal Experimental Design (BOED) in sequential settings, aimed at complex systems where gradient information is unavailable. Our method combines Ensemble Kalman Inversion (EKI) for design optimization with the Affine-Invariant Langevin Dynamics (ALDI) sampler for efficient posterior sampling-both of which are derivative-free and ensemble-based. To address the computational challenges posed by nested expectations in BOED, we propose variational Gaussian and parametrized Laplace approximations that provide tractable upper and lower bounds on the Expected Information Gain (EIG). These approximations enable scalable utility estimation in high-dimensional spaces and PDE-constrained inverse problems. We demonstrate the performance of our framework through numerical experiments ranging from linear Gaussian models to PDE-based inference tasks, highlighting the method's robustness, accuracy, and efficiency in information-driven experimental design.",Robert Gruhlke|Matei Hanu|Claudia Schillings|Philipp Wacker,,https://arxiv.org/abs/2504.13320v3,https://arxiv.org/pdf/2504.13320v3,,,,,stat.ML,stat.ML|cs.LG|math.NA|stat.CO,https://arxiv.org/pdf/2504.13320v3.pdf
2504.12133v1,2025-04-16T14:46:03Z,2025-04-16 14:46:03,Coherent EUV scatterometry of 2D periodic structure profiles with mathematically optimal experimental design,"Extreme ultraviolet (EUV) scatterometry is an increasingly important metrology that can measure critical parameters of periodic nanostructured materials in a fast, accurate, and repeatable manner and with high sensitivity to nanoscale structure and material composition. Because of this, EUV scatterometry could support manufacturing of semiconductor devices or polymer metamaterials, addressing the limitations of traditional imaging methods such as resolution and field of view, sample damage, throughput, or low sensitivity. Here we use EUV scatterometry to measure the profile of an industrially relevant 2D periodic interconnect structure, using $λ= 29$ nm light from a table-top high harmonic generation source. We show that EUV scatterometry is sensitive to out-of-plane features with single-nanometer sensitivity. Furthermore, we also apply a methodology based on the Fisher information matrix to optimize experimental design parameters, such as incidence angles and wavelength, to show how measurement sensitivity can be maximized. This methodology reveals the strong dependence of measurement sensitivity on both incidence angle and wavelength $-$ even in a simple two-parameter case. Through a simultaneous optimization of incidence angles and wavelength, we determine that the most sensitive measurement of the quantities of interest can be made at a wavelength of $\sim$14 nm. In the future, by reducing sample contamination due to sample preparation, deep sub-nanometer sensitivity to axial profiles and 2D structures will be possible. Our results are an important step in guiding EUV scatterometry towards increased accuracy and throughput with a priori computations and by leveraging new experimental capabilities.",Clay Klein|Nicholas W. Jenkins|Yunzhe Shao|Yunhao Li|Seungbeom Park|Wookrae Kim|Henry C. Kapteyn|Margaret M. Murnane,,https://arxiv.org/abs/2504.12133v1,https://arxiv.org/pdf/2504.12133v1,,"16 pages, 6 figures",,,physics.optics,physics.optics,https://arxiv.org/pdf/2504.12133v1.pdf
2504.10092v1,2025-04-14T10:56:42Z,2025-04-14 10:56:42,Bayesian optimal experimental design with Wasserstein information criteria,"Bayesian optimal experimental design (OED) provides a principled framework for selecting the most informative observational settings in experiments. With rapid advances in computational power, Bayesian OED has become increasingly feasible for inference problems involving large-scale simulations, attracting growing interest in fields such as inverse problems. In this paper, we introduce a novel design criterion based on the expected Wasserstein-$p$ distance between the prior and posterior distributions. Especially, for $p=2$, this criterion shares key parallels with the widely used expected information gain (EIG), which relies on the Kullback--Leibler divergence instead. First, the Wasserstein-2 criterion admits a closed-form solution for Gaussian regression, a property which can be also leveraged for approximative schemes. Second, it can be interpreted as maximizing the information gain measured by the transport cost incurred when updating the prior to the posterior. Our main contribution is a stability analysis of the Wasserstein-1 criterion, where we provide a rigorous error analysis under perturbations of the prior or likelihood. We partially extend this study also to the Wasserstein-2 criterion. In particular, these results yield error rates when empirical approximations of priors are used. Finally, we demonstrate the computability of the Wasserstein-2 criterion and demonstrate our approximation rates through simulations.",Tapio Helin|Youssef Marzouk|Jose Rodrigo Rojo-Garcia,,https://arxiv.org/abs/2504.10092v1,https://arxiv.org/pdf/2504.10092v1,,"27 pages, 5 figures",,,stat.ME,stat.ME|math.NA|stat.CO,https://arxiv.org/pdf/2504.10092v1.pdf
2504.03491v1,2025-04-04T14:46:48Z,2025-04-04 14:46:48,Diffusion Active Learning: Towards Data-Driven Experimental Design in Computed Tomography,"We introduce Diffusion Active Learning, a novel approach that combines generative diffusion modeling with data-driven sequential experimental design to adaptively acquire data for inverse problems. Although broadly applicable, we focus on scientific computed tomography (CT) for experimental validation, where structured prior datasets are available, and reducing data requirements directly translates to shorter measurement times and lower X-ray doses. We first pre-train an unconditional diffusion model on domain-specific CT reconstructions. The diffusion model acts as a learned prior that is data-dependent and captures the structure of the underlying data distribution, which is then used in two ways: It drives the active learning process and also improves the quality of the reconstructions. During the active learning loop, we employ a variant of diffusion posterior sampling to generate conditional data samples from the posterior distribution, ensuring consistency with the current measurements. Using these samples, we quantify the uncertainty in the current estimate to select the most informative next measurement. Our results show substantial reductions in data acquisition requirements, corresponding to lower X-ray doses, while simultaneously improving image reconstruction quality across multiple real-world tomography datasets.",Luis Barba|Johannes Kirschner|Tomas Aidukas|Manuel Guizar-Sicairos|Benjamín Béjar,,https://arxiv.org/abs/2504.03491v1,https://arxiv.org/pdf/2504.03491v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2504.03491v1.pdf
2503.08740v2,2025-03-11T08:21:35Z,2025-06-27 03:24:09,Cooperative Bearing-Only Target Pursuit via Multiagent Reinforcement Learning: Design and Experiment,"This paper addresses the multi-robot pursuit problem for an unknown target, encompassing both target state estimation and pursuit control. First, in state estimation, we focus on using only bearing information, as it is readily available from vision sensors and effective for small, distant targets. Challenges such as instability due to the nonlinearity of bearing measurements and singularities in the two-angle representation are addressed through a proposed uniform bearing-only information filter. This filter integrates multiple 3D bearing measurements, provides a concise formulation, and enhances stability and resilience to target loss caused by limited field of view (FoV). Second, in target pursuit control within complex environments, where challenges such as heterogeneity and limited FoV arise, conventional methods like differential games or Voronoi partitioning often prove inadequate. To address these limitations, we propose a novel multiagent reinforcement learning (MARL) framework, enabling multiple heterogeneous vehicles to search, localize, and follow a target while effectively handling those challenges. Third, to bridge the sim-to-real gap, we propose two key techniques: incorporating adjustable low-level control gains in training to replicate the dynamics of real-world autonomous ground vehicles (AGVs), and proposing spectral-normalized RL algorithms to enhance policy smoothness and robustness. Finally, we demonstrate the successful zero-shot transfer of the MARL controllers to AGVs, validating the effectiveness and practical feasibility of our approach. The accompanying video is available at https://youtu.be/HO7FJyZiJ3E.",Jianan Li|Zhikun Wang|Susheng Ding|Shiliang Guo|Shiyu Zhao,,https://arxiv.org/abs/2503.08740v2,https://arxiv.org/pdf/2503.08740v2,,To appear in the 2025 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2025),,,cs.MA,cs.MA|cs.RO|eess.SY,https://arxiv.org/pdf/2503.08740v2.pdf
2503.07343v4,2025-03-10T13:55:56Z,2025-12-16 18:39:50,Robust a posteriori estimation of probit-lognormal seismic fragility curves via sequential design of experiments and constrained reference prior,"A seismic fragility curve expresses the probability of failure of a structure conditional to an intensity measure (IM) derived from seismic signals. When only limited data is available, the practitioner often refers to the probit-lognormal model coupled with maximum likelihood estimation (MLE) to obtain estimates of these curves. This means that only a binary indicator of the state (BIS) of the structure is known, namely a failure or non-failure state indicator, when it is subjected to a seismic signal with an intensity measure IM. In this context, the objective of this work is to propose a method for optimally estimating such curves by obtaining the most precise estimate possible with the minimum of data. The novelty of our work is twofold. First, we present and show how to mitigate the likelihood degeneracy problem which is ubiquitous with small data sets and hampers frequentist approaches such as MLE. Second, we propose a novel strategy for sequential design of experiments (DoE) that selects seismic signals from a large database of synthetic or real signals via their IM values, to be applied to structures to evaluate the corresponding BISs. This strategy relies on a criterion based on information theory in a Bayesian framework. It therefore aims to sequentially designate the IM value such that the pair (IM, BIS) has on average, with respect to the BIS of the structure, the greatest impact on the posterior distribution of the fragility curve. The methodology is applied to a case study from the nuclear industry. The results demonstrate its ability to efficiently and robustly estimate the fragility curve, and to avoid degeneracy even with a limited amount of data, i.e., less than 100. Furthermore, we demonstrate that the estimates quickly reach the model bias induced by the probit-lognormal modeling. Eventually, two criteria are suggested to help the user stop the DoE algorithm.",Antoine Van Biesbroeck|Clément Gauchy|Cyril Feau|Josselin Garnier,,https://arxiv.org/abs/2503.07343v4,https://arxiv.org/pdf/2503.07343v4,https://doi.org/10.1016/j.nucengdes.2025.114695,,Nucl. Eng. Des. 448 (2026) 114695,10.1016/j.nucengdes.2025.114695,stat.AP,stat.AP,https://arxiv.org/pdf/2503.07343v4.pdf
2503.07070v1,2025-03-10T08:53:11Z,2025-03-10 08:53:11,PIED: Physics-Informed Experimental Design for Inverse Problems,"In many science and engineering settings, system dynamics are characterized by governing PDEs, and a major challenge is to solve inverse problems (IPs) where unknown PDE parameters are inferred based on observational data gathered under limited budget. Due to the high costs of setting up and running experiments, experimental design (ED) is often done with the help of PDE simulations to optimize for the most informative design parameters to solve such IPs, prior to actual data collection. This process of optimizing design parameters is especially critical when the budget and other practical constraints make it infeasible to adjust the design parameters between trials during the experiments. However, existing experimental design (ED) methods tend to require sequential and frequent design parameter adjustments between trials. Furthermore, they also have significant computational bottlenecks due to the need for complex numerical simulations for PDEs, and do not exploit the advantages provided by physics informed neural networks (PINNs), such as its meshless solutions, differentiability, and amortized training. This work presents PIED, the first ED framework that makes use of PINNs in a fully differentiable architecture to perform continuous optimization of design parameters for IPs for one-shot deployments. PIED overcomes existing methods' computational bottlenecks through parallelized computation and meta-learning of PINN parameter initialization, and proposes novel methods to effectively take into account PINN training dynamics in optimizing the ED parameters. Through experiments based on noisy simulated data and even real world experimental data, we empirically show that given limited observation budget, PIED significantly outperforms existing ED methods in solving IPs, including challenging settings where the inverse parameters are unknown functions rather than just finite-dimensional.",Apivich Hemachandra|Gregory Kang Ruey Lau|See-Kiong Ng|Bryan Kian Hsiang Low,,https://arxiv.org/abs/2503.07070v1,https://arxiv.org/pdf/2503.07070v1,,"Accepted to 13th International Conference on Learning Representations (ICLR 2025), 31 pages",,,cs.LG,cs.LG|cs.AI|physics.comp-ph|physics.data-an|stat.ML,https://arxiv.org/pdf/2503.07070v1.pdf
2503.05905v2,2025-03-07T19:57:39Z,2025-08-18 23:48:57,Performance Comparisons of Reinforcement Learning Algorithms for Sequential Experimental Design,"Recent developments in sequential experimental design look to construct a policy that can efficiently navigate the design space, in a way that maximises the expected information gain. Whilst there is work on achieving tractable policies for experimental design problems, there is significantly less work on obtaining policies that are able to generalise well - i.e. able to give good performance despite a change in the underlying statistical properties of the experiments. Conducting experiments sequentially has recently brought about the use of reinforcement learning, where an agent is trained to navigate the design space to select the most informative designs for experimentation. However, there is still a lack of understanding about the benefits and drawbacks of using certain reinforcement learning algorithms to train these agents. In our work, we investigate several reinforcement learning algorithms and their efficacy in producing agents that take maximally informative design decisions in sequential experimental design scenarios. We find that agent performance is impacted depending on the algorithm used for training, and that particular algorithms, using dropout or ensemble approaches, empirically showcase attractive generalisation properties.",Yasir Zubayr Barlas|Kizito Salako,,https://arxiv.org/abs/2503.05905v2,https://arxiv.org/pdf/2503.05905v2,,"7 main pages, 19 pages of appendices - paper accepted at the 8th Workshop on Generalization in Planning at AAAI 2025",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2503.05905v2.pdf
2503.01566v1,2025-03-03T14:09:57Z,2025-03-03 14:09:57,Efficient Long-Term Structural Reliability Estimation with Non-Gaussian Stochastic Models: A Design of Experiments Approach,"Extreme response assessment is important in the design and operation of engineering structures, and is a crucial part of structural risk and reliability analyses. Structures should be designed in a way that enables them to withstand the environmental loads they are expected to experience over their lifetime, without designs being unnecessarily conservative and costly. An accurate risk estimate is essential but difficult to obtain because the long-term behaviour of a structure is typically too complex to calculate analytically or with brute force Monte Carlo simulation. Therefore, approximation methods are required to estimate the extreme response using only a limited number of short-term conditional response calculations. Combining surrogate models with Design of Experiments is an approximation approach that has gained popularity due to its ability to account for both long-term environment variability and short-term response variability. In this paper, we propose a method for estimating the extreme response of black-box, stochastic models with heteroscedastic non-Gaussian noise. We present a mathematically founded extreme response estimation process that enables Design of Experiment approaches that are prohibitively expensive with surrogate Monte Carlo. The theory leads us to speculate this method can robustly produce more confident extreme response estimates, and is suitable for a variety of domains. While this needs to be further validated empirically, the method offers a promising tool for reducing the uncertainty decision-makers face, allowing them to make better informed choices and create more optimal structures.",Sebastian Winter|Christian Agrell|Juan Camilo Guevara Gómez|Erik Vanem,,https://arxiv.org/abs/2503.01566v1,https://arxiv.org/pdf/2503.01566v1,,,,,stat.CO,stat.CO|stat.AP,https://arxiv.org/pdf/2503.01566v1.pdf
2503.01139v2,2025-03-03T03:43:05Z,2025-03-04 04:19:03,Can Large Language Models Help Experimental Design for Causal Discovery?,"Designing proper experiments and selecting optimal intervention targets is a longstanding problem in scientific or causal discovery. Identifying the underlying causal structure from observational data alone is inherently difficult. Obtaining interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive and time-consuming to gather sufficient interventional data to facilitate causal discovery. Previous approaches commonly utilize uncertainty or gradient signals to determine the intervention targets. However, numerical-based approaches may yield suboptimal results due to the inaccurate estimation of the guiding signals at the beginning when with limited interventional data. In this work, we investigate a different approach, whether we can leverage Large Language Models (LLMs) to assist with the intervention targeting in causal discovery by making use of the rich world knowledge about the experimental design in LLMs. Specifically, we present Large Language Model Guided Intervention Targeting (LeGIT) -- a robust framework that effectively incorporates LLMs to augment existing numerical approaches for the intervention targeting in causal discovery. Across 4 realistic benchmark scales, LeGIT demonstrates significant improvements and robustness over existing methods and even surpasses humans, which demonstrates the usefulness of LLMs in assisting with experimental design for scientific discovery.",Junyi Li|Yongqiang Chen|Chenxi Liu|Qianyi Cai|Tongliang Liu|Bo Han|Kun Zhang|Hui Xiong,,https://arxiv.org/abs/2503.01139v2,https://arxiv.org/pdf/2503.01139v2,,,,,cs.AI,cs.AI|cs.LG|stat.ME,https://arxiv.org/pdf/2503.01139v2.pdf
2503.00327v1,2025-03-01T03:26:20Z,2025-03-01 03:26:20,Bayesian Optimization for Intrinsically Noisy Response Surfaces,"While many advanced statistical methods for the design of experiments exist, it is still typical for physical experiments to be performed adaptively based on human intuition. As a consequence, experimental resources are wasted on sub-optimal experimental designs. Conversely, in the simulation-based design community, Bayesian optimization (BO) is often used to adaptively and efficiently identify the global optimum of a response surface. However, adopting these methods directly for the optimization of physical experiments is problematic due to the existence of experimental noise and the typically more stringent constraints on the experimental budget. Consequently, many simplifying assumptions need to be made in the BO framework, and it is currently not fully understood how these assumptions influence the performance of the method and the optimality of the final design. In this paper, we present an experimental study to investigate the influence of the controllable (e.g., number of samples, acquisition function, and covariance function) and noise factors (e.g., problem dimensionality, experimental noise magnitude, and experimental noise form) on the efficiency of the BO framework. The findings in this study include, that the Matér covariance function shows superior performance over all test problems and that the available experimental budget is most consequential when selecting the other settings of the BO scheme. With this study, we enable designers to make more efficient use of their physical experiments and provide insight into the use of BO with intrinsically noisy training data.",Anton van Beek,,https://arxiv.org/abs/2503.00327v1,https://arxiv.org/pdf/2503.00327v1,,"16 pages, 4 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2503.00327v1.pdf
2502.20086v1,2025-02-27T13:44:15Z,2025-02-27 13:44:15,"Subspace accelerated measure transport methods for fast and scalable sequential experimental design, with application to photoacoustic imaging","We propose a novel approach for sequential optimal experimental design (sOED) for Bayesian inverse problems involving expensive models with large-dimensional unknown parameters. The focus of this work is on designs that maximize the expected information gain (EIG) from prior to posterior, which is a computationally challenging task in the non-Gaussian setting. This challenge is amplified in sOED, as the incremental expected information gain (iEIG) must be approximated multiple times in distinct stages, with both prior and posterior distributions often being intractable. To address this, we derive a derivative-based upper bound for the iEIG, which not only guides design placement but also enables the construction of projectors onto likelihood-informed subspaces, facilitating parameter dimension reduction. By combining this approach with conditional measure transport maps for the sequence of posteriors, we develop a unified framework for sOED, together with amortized inference, scalable to high- and infinite-dimensional problems. Numerical experiments for two inverse problems governed by partial differential equations (PDEs) demonstrate the effectiveness of designs that maximize our proposed upper bound.",Tiangang Cui|Karina Koval|Roland Herzog|Robert Scheichl,,https://arxiv.org/abs/2502.20086v1,https://arxiv.org/pdf/2502.20086v1,,,,,math.OC,math.OC|math.NA,https://arxiv.org/pdf/2502.20086v1.pdf
2502.17427v1,2025-02-24T18:57:54Z,2025-02-24 18:57:54,Stronger Neyman Regret Guarantees for Adaptive Experimental Design,"We study the design of adaptive, sequential experiments for unbiased average treatment effect (ATE) estimation in the design-based potential outcomes setting. Our goal is to develop adaptive designs offering sublinear Neyman regret, meaning their efficiency must approach that of the hindsight-optimal nonadaptive design. Recent work [Dai et al, 2023] introduced ClipOGD, the first method achieving $\widetilde{O}(\sqrt{T})$ expected Neyman regret under mild conditions. In this work, we propose adaptive designs with substantially stronger Neyman regret guarantees. In particular, we modify ClipOGD to obtain anytime $\widetilde{O}(\log T)$ Neyman regret under natural boundedness assumptions. Further, in the setting where experimental units have pre-treatment covariates, we introduce and study a class of contextual ""multigroup"" Neyman regret guarantees: Given any set of possibly overlapping groups based on the covariates, the adaptive design outperforms each group's best non-adaptive designs. In particular, we develop a contextual adaptive design with $\widetilde{O}(\sqrt{T})$ anytime multigroup Neyman regret. We empirically validate the proposed designs through an array of experiments.",Georgy Noarov|Riccardo Fogliato|Martin Bertran|Aaron Roth,,https://arxiv.org/abs/2502.17427v1,https://arxiv.org/pdf/2502.17427v1,,,,,stat.ME,stat.ME|cs.LG|math.ST|stat.ML,https://arxiv.org/pdf/2502.17427v1.pdf
2502.12753v2,2025-02-18T11:15:04Z,2025-06-27 07:44:25,Green LIME: Improving AI Explainability through Design of Experiments,"In artificial intelligence (AI), the complexity of many models and processes surpasses human understanding, making it challenging to determine why a specific prediction is made. This lack of transparency is particularly problematic in critical fields like healthcare, where trust in a model's predictions is paramount. As a result, the explainability of machine learning (ML) and other complex models has become a key area of focus. Efforts to improve model explainability often involve experimenting with AI systems and approximating their behavior through interpretable surrogate mechanisms. However, these procedures can be resource-intensive. Optimal design of experiments, which seeks to maximize the information obtained from a limited number of observations, offers promising methods for improving the efficiency of these explainability techniques. To demonstrate this potential, we explore Local Interpretable Model-agnostic Explanations (LIME), a widely used method introduced by Ribeiro et al. (2016). LIME provides explanations by generating new data points near the instance of interest and passing them through the model. While effective, this process can be computationally expensive, especially when predictions are costly or require many samples. LIME is highly versatile and can be applied to a wide range of models and datasets. In this work, we focus on models involving tabular data, regression tasks, and linear models as interpretable local approximations. By utilizing optimal design of experiments' techniques, we reduce the number of function evaluations of the complex model, thereby reducing the computational effort of LIME by a significant amount. We consider this modified version of LIME to be energy-efficient or ""green"".",Alexandra Stadler|Werner G. Müller|Radoslav Harman,,https://arxiv.org/abs/2502.12753v2,https://arxiv.org/pdf/2502.12753v2,,,,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2502.12753v2.pdf
2502.09806v1,2025-02-13T22:48:09Z,2025-02-13 22:48:09,Prioritized Ranking Experimental Design Using Recommender Systems in Two-Sided Platforms,"Interdependencies between units in online two-sided marketplaces complicate estimating causal effects in experimental settings. We propose a novel experimental design to mitigate the interference bias in estimating the total average treatment effect (TATE) of item-side interventions in online two-sided marketplaces. Our Two-Sided Prioritized Ranking (TSPR) design uses the recommender system as an instrument for experimentation. TSPR strategically prioritizes items based on their treatment status in the listings displayed to users. We designed TSPR to provide users with a coherent platform experience by ensuring access to all items and a consistent realization of their treatment by all users. We evaluate our experimental design through simulations using a search impression dataset from an online travel agency. Our methodology closely estimates the true simulated TATE, while a baseline item-side estimator significantly overestimates TATE.",Mahyar Habibi|Zahra Khanalizadeh|Negar Ziaeian,,https://arxiv.org/abs/2502.09806v1,https://arxiv.org/pdf/2502.09806v1,,,,,econ.EM,econ.EM|cs.IR|cs.SI|stat.ME,https://arxiv.org/pdf/2502.09806v1.pdf
2502.08004v1,2025-02-11T22:58:18Z,2025-02-11 22:58:18,Optimizing Likelihoods via Mutual Information: Bridging Simulation-Based Inference and Bayesian Optimal Experimental Design,"Simulation-based inference (SBI) is a method to perform inference on a variety of complex scientific models with challenging inference (inverse) problems. Bayesian Optimal Experimental Design (BOED) aims to efficiently use experimental resources to make better inferences. Various stochastic gradient-based BOED methods have been proposed as an alternative to Bayesian optimization and other experimental design heuristics to maximize information gain from an experiment. We demonstrate a link via mutual information bounds between SBI and stochastic gradient-based variational inference methods that permits BOED to be used in SBI applications as SBI-BOED. This link allows simultaneous optimization of experimental designs and optimization of amortized inference functions. We evaluate the pitfalls of naive design optimization using this method in a standard SBI task and demonstrate the utility of a well-chosen design distribution in BOED. We compare this approach on SBI-based models in real-world simulators in epidemiology and biology, showing notable improvements in inference.",Vincent D. Zaballa|Elliot E. Hui,,https://arxiv.org/abs/2502.08004v1,https://arxiv.org/pdf/2502.08004v1,,Preprint. Under Review,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2502.08004v1.pdf
2502.05372v2,2025-02-07T22:54:20Z,2025-08-10 02:11:17,Active Learning of Model Discrepancy with Bayesian Experimental Design,"Digital twins have been actively explored in many engineering applications, such as manufacturing and autonomous systems. However, model discrepancy is ubiquitous in most digital twin models and has significant impacts on the performance of using those models. In recent years, data-driven modeling techniques have been demonstrated promising in characterizing the model discrepancy in existing models, while the training data for the learning of model discrepancy is often obtained in an empirical way and an active approach of gathering informative data can potentially benefit the learning of model discrepancy. On the other hand, Bayesian experimental design (BED) provides a systematic approach to gathering the most informative data, but its performance is often negatively impacted by the model discrepancy. In this work, we build on sequential BED and propose an efficient approach to iteratively learn the model discrepancy based on the data from the BED. The performance of the proposed method is validated by a classical numerical example governed by a convection-diffusion equation, for which full BED is still feasible. The proposed method is then further studied in the same numerical example with a high-dimensional model discrepancy, which serves as a demonstration for the scenarios where full BED is not practical anymore. An ensemble-based approximation of information gain is further utilized to assess the data informativeness and to enhance learning model discrepancy. The results show that the proposed method is efficient and robust to the active learning of high-dimensional model discrepancy, using data suggested by the sequential BED. We also demonstrate that the proposed method is compatible with both classical numerical solvers and modern auto-differentiable solvers.",Huchen Yang|Chuanqi Chen|Jin-Long Wu,,https://arxiv.org/abs/2502.05372v2,https://arxiv.org/pdf/2502.05372v2,https://doi.org/10.1016/j.cma.2025.118198,"38 pages, 14 figures","Computer Methods in Applied Mechanics and Engineering, Volume 446, 2025, Article 118198",10.1016/j.cma.2025.118198,cs.LG,cs.LG,https://arxiv.org/pdf/2502.05372v2.pdf
2502.03241v1,2025-02-05T14:59:14Z,2025-02-05 14:59:14,Optimal design of experiments with quantitative-sequence factors,"A new type of experiment with joint considerations of quantitative and sequence factors is recently drawing much attention in medical science, bio-engineering, and many other disciplines. The input spaces of such experiments are semi-discrete and often very large. Thus, efficient and economical experimental designs are required. Based on the transformations and aggregations of good lattice point sets, we construct a new class of optimal quantitative-sequence (QS) designs that are marginally coupled, pair-balanced, space-filling, and asymptotically orthogonal. The proposed QS designs have a certain flexibility in run and factor sizes and are especially appealing for high-dimensional cases.",Yaping Wang|Sixu Liu|Qian Xiao,,https://arxiv.org/abs/2502.03241v1,https://arxiv.org/pdf/2502.03241v1,https://doi.org/10.1360/SCM-2024-0039,This is the English version of the published paper in Chinese by SCIENCE CHINA Mathematics,"SCIENCE CHINA Mathematics, 2025, 55: 1-24 (in Chinese)",10.1360/SCM-2024-0039,stat.ME,stat.ME,https://arxiv.org/pdf/2502.03241v1.pdf
2502.01012v1,2025-02-03T03:03:21Z,2025-02-03 03:03:21,Deep Active Learning based Experimental Design to Uncover Synergistic Genetic Interactions for Host Targeted Therapeutics,"Recent technological advances have introduced new high-throughput methods for studying host-virus interactions, but testing synergistic interactions between host gene pairs during infection remains relatively slow and labor intensive. Identification of multiple gene knockdowns that effectively inhibit viral replication requires a search over the combinatorial space of all possible target gene pairs and is infeasible via brute-force experiments. Although active learning methods for sequential experimental design have shown promise, existing approaches have generally been restricted to single-gene knockdowns or small-scale double knockdown datasets. In this study, we present an integrated Deep Active Learning (DeepAL) framework that incorporates information from a biological knowledge graph (SPOKE, the Scalable Precision Medicine Open Knowledge Engine) to efficiently search the configuration space of a large dataset of all pairwise knockdowns of 356 human genes in HIV infection. Through graph representation learning, the framework is able to generate task-specific representations of genes while also balancing the exploration-exploitation trade-off to pinpoint highly effective double-knockdown pairs. We additionally present an ensemble method for uncertainty quantification and an interpretation of the gene pairs selected by our algorithm via pathway analysis. To our knowledge, this is the first work to show promising results on double-gene knockdown experimental data of appreciable scale (356 by 356 matrix).",Haonan Zhu|Mary Silva|Jose Cadena|Braden Soper|Michał Lisicki|Braian Peetoom|Sergio E. Baranzini|Shivshankar Sundaram|Priyadip Ray|Jeff Drocco,,https://arxiv.org/abs/2502.01012v1,https://arxiv.org/pdf/2502.01012v1,,,,,cs.LG,cs.LG|q-bio.QM|stat.ME,https://arxiv.org/pdf/2502.01012v1.pdf
2501.14616v2,2025-01-24T16:33:56Z,2025-01-27 05:44:42,QuIP: Experimental design for expensive simulators with many Qualitative factors via Integer Programming,"The need to explore and/or optimize expensive simulators with many qualitative factors arises in broad scientific and engineering problems. Our motivating application lies in path planning - the exploration of feasible paths for navigation, which plays an important role in robotics, surgical planning and assembly planning. Here, the feasibility of a path is evaluated via expensive virtual experiments, and its parameter space is typically discrete and high-dimensional. A carefully selected experimental design is thus essential for timely decision-making. We propose here a novel framework, called QuIP, for experimental design of Qualitative factors via Integer Programming under a Gaussian process surrogate model with an exchangeable covariance function. For initial design, we show that its asymptotic D-optimal design can be formulated as a variant of the well-known assignment problem in operations research, which can be efficiently solved to global optimality using state-of-the-art integer programming solvers. For sequential design (specifically, for active learning or black-box optimization), we show that its design criterion can similarly be formulated as an assignment problem, thus enabling efficient and reliable optimization with existing solvers. We then demonstrate the effectiveness of QuIP over existing methods in a suite of path planning experiments and an application to rover trajectory optimization.",Yen-Chun Liu|Simon Mak,,https://arxiv.org/abs/2501.14616v2,https://arxiv.org/pdf/2501.14616v2,,,,,stat.AP,stat.AP|cs.RO,https://arxiv.org/pdf/2501.14616v2.pdf
2501.11996v2,2025-01-21T09:37:14Z,2026-01-31 06:36:06,Experimental Designs for Multi-Item Multi-Period Inventory Control,"Randomized experiments, or A/B testing, are the gold standard for evaluating interventions, yet they remain underutilized in inventory management. This study addresses this gap by analyzing A/B testing strategies in multi-item, multi-period inventory systems with lost sales and capacity constraints. We examine two canonical experimental designs, namely, switchback experiments and item-level randomization, and show that both suffer from systematic bias due to interference: temporal carryover in switchbacks and cannibalization across items under capacity constraints. Under mild conditions, we characterize the direction of this bias, proving that switchback designs systematically underestimate, while item-level randomization systematically overestimate, the global treatment effect. Motivated by two-sided randomization, we propose a pairwise design over items and time and analyze its bias properties. Numerical experiments using real-world data validate our theory and provide concrete guidance for selecting experimental designs in practice.",Xinqi Chen|Xingyu Bai|Zeyu Zheng|Nian Si,,https://arxiv.org/abs/2501.11996v2,https://arxiv.org/pdf/2501.11996v2,,,,,stat.ME,stat.ME|econ.EM,https://arxiv.org/pdf/2501.11996v2.pdf
2501.10845v2,2025-01-18T18:41:28Z,2025-09-09 19:42:31,A Multi-fidelity Estimator of the Expected Information Gain for Bayesian Optimal Experimental Design,"Optimal experimental design (OED) is a framework that leverages a mathematical model of the experiment to identify optimal conditions for conducting the experiment. Under a Bayesian approach, the design objective function is typically chosen to be the expected information gain (EIG). However, EIG is intractable for nonlinear models and must be estimated numerically. Estimating the EIG generally entails some variant of Monte Carlo sampling, requiring repeated data model and likelihood evaluations $\unicode{x2013}$ each involving solving the governing equations of the experimental physics $\unicode{x2013}$ under different sample realizations. This computation becomes impractical for high-fidelity models.
  We introduce a novel multi-fidelity EIG (MF-EIG) estimator under the approximate control variate (ACV) framework. This estimator is unbiased with respect to the high-fidelity mean, and minimizes variance under a given computational budget. We achieve this by first reparameterizing the EIG so that its expectations are independent of the data models, a requirement for compatibility with ACV. We then provide specific examples under different data model forms, as well as practical enhancements of sample size optimization and sample reuse techniques. We demonstrate the MF-EIG estimator in two numerical examples: a nonlinear benchmark and a turbulent flow problem involving the calibration of shear-stress transport turbulence closure model parameters within the Reynolds-averaged Navier-Stokes model. We validate the estimator's unbiasedness and observe one- to two-orders-of-magnitude variance reduction compared to existing single-fidelity EIG estimators.",Thomas E. Coons|Xun Huan,,https://arxiv.org/abs/2501.10845v2,https://arxiv.org/pdf/2501.10845v2,https://doi.org/10.1137/25M1731812,,,10.1137/25M1731812,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2501.10845v2.pdf
2501.10030v4,2025-01-17T08:33:44Z,2025-12-20 02:22:16,"Informativity Conditions for Multiple Signals: Properties, Experimental Design, and Applications (extended version)","Recent studies highlight the importance of persistently exciting condition in single signal sequence for model identification and data-driven control methodologies. However, maintaining prolonged excitation in control signals introduces significant challenges, as continuous excitation can reduce the lifetime of mechanical devices. In this paper, we introduce three informativity conditions for various types of multi-signal data, each augmented by weight factors. We explore the interrelations between these conditions and their rank properties in linear time-invariant systems. Furthermore, we introduce open-loop experimental design methods tailored to each of the three conditions, which can synthesize the required excitation conditions either offline or online, even in the presence of limited information within each signal segment. We demonstrate the effectiveness of these informativity conditions in least-squares identification. Additionally, all three conditions can extend Willems' fundamental lemma and are utilized to assess the properties of the system. Illustrative examples confirm that these conditions yield satisfactory outcomes in both least-squares identification and the construction of data-driven controllers.",Ao Cao|Fuyong Wang,,https://arxiv.org/abs/2501.10030v4,https://arxiv.org/pdf/2501.10030v4,,,,,eess.SY,eess.SY|cs.IT,https://arxiv.org/pdf/2501.10030v4.pdf
2501.04448v1,2025-01-08T11:58:31Z,2025-01-08 11:58:31,AI-assisted design of experiments at the frontiers of computation: methods and new perspectives,"Designing the next generation colliders and detectors involves solving optimization problems in high-dimensional spaces where the optimal solutions may nest in regions that even a team of expert humans would not explore.
  Resorting to Artificial Intelligence to assist the experimental design introduces however significant computational challenges in terms of generation and processing of the data required to perform such optimizations: from the software point of view, differentiable programming makes the exploration of such spaces with gradient descent feasible; from the hardware point of view, the complexity of the resulting models and their optimization is prohibitive. To scale up to the complexity of the typical HEP collider experiment, a change in paradigma is required.
  In this contribution I will describe the first proofs-of-concept of gradient-based optimization of experimental design and implementations in neuromorphic hardware architectures, paving the way to more complex challenges.",Pietro Vischia,,https://arxiv.org/abs/2501.04448v1,https://arxiv.org/pdf/2501.04448v1,https://doi.org/10.22323/1.476.1022,Proceedings of ICHEP 2024. Published in Proceedings of Science (https://pos.sissa.it/476/1022/),"Proceedings of Science 476, ICHEP2024, 1022 (2025)",10.22323/1.476.1022,hep-ex,hep-ex|physics.ins-det,https://arxiv.org/pdf/2501.04448v1.pdf
2501.04260v2,2025-01-08T03:56:06Z,2025-01-26 10:26:44,Modeling All Response Surfaces in One for Conditional Search Spaces,"Bayesian Optimization (BO) is a sample-efficient black-box optimizer commonly used in search spaces where hyperparameters are independent. However, in many practical AutoML scenarios, there will be dependencies among hyperparameters, forming a conditional search space, which can be partitioned into structurally distinct subspaces. The structure and dimensionality of hyperparameter configurations vary across these subspaces, challenging the application of BO. Some previous BO works have proposed solutions to develop multiple Gaussian Process models in these subspaces. However, these approaches tend to be inefficient as they require a substantial number of observations to guarantee each GP's performance and cannot capture relationships between hyperparameters across different subspaces. To address these issues, this paper proposes a novel approach to model the response surfaces of all subspaces in one, which can model the relationships between hyperparameters elegantly via a self-attention mechanism. Concretely, we design a structure-aware hyperparameter embedding to preserve the structural information. Then, we introduce an attention-based deep feature extractor, capable of projecting configurations with different structures from various subspaces into a unified feature space, where the response surfaces can be formulated using a single standard Gaussian Process. The empirical results on a simulation function, various real-world tasks, and HPO-B benchmark demonstrate that our proposed approach improves the efficacy and efficiency of BO within conditional search spaces.",Jiaxing Li|Wei Liu|Chao Xue|Yibing Zhan|Xiaoxing Wang|Weifeng Liu|Dacheng Tao,,https://arxiv.org/abs/2501.04260v2,https://arxiv.org/pdf/2501.04260v2,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2501.04260v2.pdf
2501.01540v2,2025-01-02T21:15:57Z,2025-10-14 20:35:27,BoxingGym: Benchmarking Progress in Automated Experimental Design and Model Discovery,"Understanding the world and explaining it with scientific theories is a central aspiration of artificial intelligence research. Proposing theories, designing experiments to test them, and then revising them based on data are fundamental to scientific discovery. Despite the significant promise of LLM-based scientific agents, no benchmarks systematically test LLM's ability to propose scientific models, collect experimental data, and revise them in light of new data. We introduce BoxingGym, a benchmark with 10 environments for systematically evaluating both experimental design (e.g. collecting data to test a scientific theory) and model discovery (e.g. proposing and revising scientific theories). To enable tractable and quantitative evaluation, we implement each environment as a generative probabilistic model with which a scientific agent can run interactive experiments. These probabilistic models are drawn from various real-world scientific domains ranging from psychology to ecology. To quantitatively evaluate a scientific agent's ability to collect informative experimental data, we compute the expected information gain (EIG), an information-theoretic quantity which measures how much an experiment reduces uncertainty about the parameters of a generative model. A good scientific theory is a concise and predictive explanation. Therefore, to quantitatively evaluate model discovery, we ask a scientific agent to explain their model and then assess whether this explanation enables another scientific agent to make reliable predictions about this environment. In addition to this explanation-based evaluation, we compute standard model evaluation metrics such as prediction errors. We find that current LLMs, such as GPT-4o, struggle with both experimental design and model discovery. We find that augmenting the LLM-based agent with an explicit statistical model does not reliably improve these results.",Kanishk Gandhi|Michael Y. Li|Lyle Goodyear|Agam Bhatia|Louise Li|Aditi Bhaskar|Mohammed Zaman|Noah D. Goodman,,https://arxiv.org/abs/2501.01540v2,https://arxiv.org/pdf/2501.01540v2,,KG and MYL contributed equally,,,cs.LG,cs.LG|cs.AI,https://arxiv.org/pdf/2501.01540v2.pdf
2412.18329v1,2024-12-24T10:30:57Z,2024-12-24 10:30:57,Comprehensive Analysis and Experimental Design of High-Gain DC-DC Boost Converter Topologies,"Global demand for clean and eco-friendly energy sources has inspired decades of far-reaching research in power generation from renewable energy sources. Solar cells, wind, and tidal sources are limited in output power generation compared to the fast-rising power requirements of most industrial applications. Besides, the efficiency of conventional DC-DC boost converters is significantly low due to the presence of parasitic elements culminating in switching losses. This study presents three high-gain boost converter topologies for optimizing the limited voltage generation by solar Photovoltaic cells. The three converters are realized based on a modification of a classical Cuk converter. Simply put, the first proposed converter is realized by the inclusion of one capacitor and one inductor to the classical topology. Similarly, the addition of 3 capacitors, 2 diodes, and one inductor leads to the practical realization of the second proposed topology. Similarly, the third proposed topology consists of additional 3 diodes and 3 capacitors. Based on this estimation, the first, second, and third proposed high gain modified Cuk converter topologies generate output voltages 10 times, 20 times, and 29 times the input voltage respectively when the switching device is gated at 90% duty ratio. Theoretical/mathematical analysis validates the Lt-spice numerical simulation results of all the proposed converters. Furthermore, experimental prototype results were compared with Lt-spice estimation to determine the accuracy of the converters.",Webster Adepoju|Mary Sanyaolu,,https://arxiv.org/abs/2412.18329v1,https://arxiv.org/pdf/2412.18329v1,,,,,eess.SY,eess.SY,https://arxiv.org/pdf/2412.18329v1.pdf
2412.17158v1,2024-12-22T20:55:43Z,2024-12-22 20:55:43,MOODE: An R Package for Multi-Objective Optimal Design of Experiments,"We describe the R package MOODE and demonstrate its use to find multi-objective optimal experimental designs. Multi-Objective Optimal Design of Experiments (MOODE) targets the experimental objectives directly, ensuring that the full set of research questions is answered as economically as possible. In particular, individual criteria aimed at optimizing inference are combined with lack-of-fit and MSE-based components in compound optimality criteria to target multiple and competing objectives reflecting the priorities and aims of the experimentation. The package implements either a point exchange or coordinate exchange algorithm as appropriate to find nearly optimal designs. We demonstrate the functionality of MOODE through the application of the methodology to two case studies of varying complexity.",Vasiliki Koutra|Olga Egorova|Steven G. Gilmour|Luzia A. Trinca,,https://arxiv.org/abs/2412.17158v1,https://arxiv.org/pdf/2412.17158v1,,,,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2412.17158v1.pdf
2412.14284v1,2024-12-18T19:24:23Z,2024-12-18 19:24:23,Optimal design of experiments for functional linear models with dynamic factors,"In this work we build optimal experimental designs for precise estimation of the functional coefficient of a function-on-function linear regression model where both the response and the factors are continuous functions of time. After obtaining the variance-covariance matrix of the estimator of the functional coefficient which minimizes the integrated sum of square of errors, we extend the classical definition of optimal design to this estimator, and we provide the expression of the A-optimal and of the D-optimal designs. Examples of optimal designs for dynamic experimental factors are then computed through a suitable algorithm, and we discuss different scenarios in terms of the set of basis functions used for their representation. Finally, we present an example with simulated data to illustrate the feasibility of our methodology.",Caterina May|Theodoros Ladas|Davide Pigoli|Kalliopi Mylona,,https://arxiv.org/abs/2412.14284v1,https://arxiv.org/pdf/2412.14284v1,,15 figures,,,stat.ME,stat.ME,https://arxiv.org/pdf/2412.14284v1.pdf
2412.04699v1,2024-12-06T01:21:47Z,2024-12-06 01:21:47,Driving Thermoelectric Optimization in AgSbTe2 via Design of Experiments and Machine Learning,"Systemic optimization of thermoelectric materials is arduous due to their conflicting electrical and thermal properties. A strategy based on Design of Experiments and machine learning is developed to optimize the thermoelectric efficiency of AgSb1+xTe2+y, an established thermoelectric. From eight experiments, high thermoelectric performance in AgSb1.021Te2.04 is revealed with a peak and average thermoelectric figure of merit of 1.61 +/- 0.24 at 600 K and 1.18 +/- 0.18 (300 - 623 K), respectively, which is over 30% higher than the best literature values for AgSb1+xTe2+y. Ag-deficiency and suppression of secondary phases in AgSb1.021Te2.04 improves the electrical properties and reduces the thermal conductivity (~0.4 W m-1 K-1). Our strategy is implemented into an open-source graphical user interface, and it can be used to optimize the methodologies, properties, and processes across different scientific fields.",Jan-Hendrik Pöhls|Chun-Wan Timothy Lo|Marissa MacIver|Yu-Chih Tseng|Yurij Mozharivskyj,,https://arxiv.org/abs/2412.04699v1,https://arxiv.org/pdf/2412.04699v1,,"15 pages, 4 figures",,,cond-mat.mtrl-sci,cond-mat.mtrl-sci,https://arxiv.org/pdf/2412.04699v1.pdf
2412.03727v3,2024-12-04T21:45:35Z,2025-02-10 05:18:01,Online Experimental Design With Estimation-Regret Trade-off Under Network Interference,"Network interference has attracted significant attention in the field of causal inference, encapsulating various sociological behaviors where the treatment assigned to one individual within a network may affect the outcomes of others, such as their neighbors. A key challenge in this setting is that standard causal inference methods often assume independent treatment effects among individuals, which may not hold in networked environments. To estimate interference-aware causal effects, a traditional approach is to inherit the independent settings, where practitioners randomly assign experimental participants into different groups and compare their outcomes. While effective in offline settings, this strategy becomes problematic in sequential experiments, where suboptimal decision persists, leading to substantial regret. To address this issue, we introduce a unified interference-aware framework for online experimental design. Compared to existing studies, we extend the definition of arm space by utilizing the statistical concept of exposure mapping, which allows for a more flexible and context-aware representation of treatment effects in networked settings. Crucially, we establish a Pareto-optimal trade-off between estimation accuracy and regret under the network concerning both time period and arm space, which remains superior to baseline models even without network interference. Furthermore, we propose an algorithmic implementation and discuss its generalization across different learning settings and network topology.",Zhiheng Zhang|Zichen Wang,,https://arxiv.org/abs/2412.03727v3,https://arxiv.org/pdf/2412.03727v3,,36 pages,,,cs.LG,cs.LG|math.OC|math.ST,https://arxiv.org/pdf/2412.03727v3.pdf
2412.00654v1,2024-12-01T03:17:53Z,2024-12-01 03:17:53,Performance Analysis of Sequential Experimental Design for Calibration in Parallel Computing Environments,"The unknown parameters of simulation models often need to be calibrated using observed data. When simulation models are expensive, calibration is usually carried out with an emulator. The effectiveness of the calibration process can be significantly improved by using a sequential selection of parameters to build an emulator. The expansion of parallel computing environments--from multicore personal computers to many-node servers to large-scale cloud computing environments--can lead to further calibration efficiency gains by allowing for the evaluation of the simulation model at a batch of parameters in parallel in a sequential design. However, understanding the performance implications of different sequential approaches in parallel computing environments introduces new complexities since the rate of the speed-up is affected by many factors, such as the run time of a simulation model and the variability in the run time. This work proposes a new performance model to understand and benchmark the performance of different sequential procedures for the calibration of simulation models in parallel environments. We provide metrics and a suite of techniques for visualizing the numerical experiment results and demonstrate these with a novel sequential procedure. The proposed performance model, as well as the new sequential procedure and other state-of-art techniques, are implemented in the open-source Python software package Parallel Uncertainty Quantification (PUQ), which allows users to run a simulation model in parallel.",Özge Sürer|Stefan M. Wild,,https://arxiv.org/abs/2412.00654v1,https://arxiv.org/pdf/2412.00654v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/2412.00654v1.pdf
2411.14332v2,2024-11-21T17:25:16Z,2025-06-08 01:57:56,Continuous nonlinear adaptive experimental design with gradient flow,"In computational inverse problems, the optimal experimental design (OED) problem seeks the best locations in time and space at which to take measurements. We investigate the nonlinear OED problem in the context of continuously-indexed design space for the measurements. In contrast to traditional approaches that select experiments from a finite measurement set, a continuous design space is often a better reflection of practical experimental options, where there is considerable flexibility concerning where and when to take measurements. The continuously-indexed space introduces computational challenges, and we address them by employing gradient-flow and optimal transport techniques, complemented by an adaptive strategy for bi-level optimization. Numerical results on the Lorenz 63 system and Schrodinger equation demonstrate that our solver identifies good measurement times / locations and achieves improved reconstruction of unknown parameters in inverse problems.",Ruhui Jin|Qin Li|Stephen O. Mussmann|Stephen J. Wright,,https://arxiv.org/abs/2411.14332v2,https://arxiv.org/pdf/2411.14332v2,,,,,math.NA,math.NA,https://arxiv.org/pdf/2411.14332v2.pdf
2411.11625v1,2024-11-18T14:58:09Z,2024-11-18 14:58:09,Modeling the Modeler: A Normative Theory of Experimental Design,"We consider an analyst whose goal is to identify a subject's utility function through revealed preference analysis. We argue the analyst's preference about which experiments to run should adhere to three normative principles: The first, Structural Invariance, requires that the value of a choice experiment only depends on what the experiment may potentially reveal. The second, Identification Separability, demands that the value of identification is independent of what would have been counterfactually identified had the subject had a different utility. Finally, Information Monotonicity asks that more informative experiments are preferred. We provide a representation theorem, showing that these three principles characterize Expected Identification Value maximization, a functional form that unifies several theories of experimental design. We also study several special cases and discuss potential applications.",Fernando Payró|Evan Piermont,,https://arxiv.org/abs/2411.11625v1,https://arxiv.org/pdf/2411.11625v1,,,,,econ.TH,econ.TH,https://arxiv.org/pdf/2411.11625v1.pdf
2411.11015v1,2024-11-17T09:35:23Z,2024-11-17 09:35:23,Near-real-time design of experiments for seismic monitoring of volcanoes,"Monitoring the seismic activity of volcanoes is crucial for hazard assessment and eruption forecasting. The layout of each seismic network determines the information content of recorded data about volcanic earthquakes, and experimental design methods optimise sensor locations to maximise that information. We provide a code package that implements Bayesian experimental design to optimise seismometer networks to locate seismicity at any volcano, and a practical guide to make this easily and rapidly implementable by any volcano seismologist. This work is the first to optimise travel-time, amplitude and array source location methods simultaneously, making it suitable for a wide range of volcano monitoring scenarios. The code-package is designed to be straightforward to use and can be adapted to a wide range of scenarios, and automatically links to existing global databases of topography and properties of volcanoes worldwide to allow rapid deployment. Any user should be able to obtain an initial design within minutes using a combination of generic and volcano-specific information to guide the design process, and to refine the design for their specific scenario within hours, if more specific prior information is available.",Dominik Strutz|Andrew Curtis,,https://arxiv.org/abs/2411.11015v1,https://arxiv.org/pdf/2411.11015v1,https://doi.org/10.26443/seismica.v4i1.1452,,"Seismica, 4(1) (2025)",10.26443/seismica.v4i1.1452,physics.geo-ph,physics.geo-ph,https://arxiv.org/pdf/2411.11015v1.pdf
2411.09225v2,2024-11-14T06:42:30Z,2025-05-22 09:33:31,fdesigns: Bayesian Optimal Designs of Experiments for Functional Models in R,"This paper describes the R package fdesigns that implements a methodology for identifying Bayesian optimal experimental designs for models whose factor settings are functions, known as profile factors. This type of experiments which involve factors that vary dynamically over time, presenting unique challenges in both estimation and design due to the infinite-dimensional nature of functions. The package fdesigns implements a dimension reduction method leveraging basis functions of the B-spline basis system. The package fdesigns contains functions that effectively reduce the design problem to the optimisation of basis coefficients for functional linear functional generalised linear models, and it accommodates various options. Applications of the fdesigns package are demonstrated through a series of examples that showcase its capabilities in identifying optimal designs for functional linear and generalised linear models. The examples highlight how the package's functions can be used to efficiently design experiments involving both profile and scalar factors, including interactions and polynomial effects.",Damianos Michaelides|Antony Overstall|Dave Woods,,https://arxiv.org/abs/2411.09225v2,https://arxiv.org/pdf/2411.09225v2,,,,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2411.09225v2.pdf
2411.07038v1,2024-11-11T14:45:08Z,2024-11-11 14:45:08,Designing Reliable Experiments with Generative Agent-Based Modeling: A Comprehensive Guide Using Concordia by Google DeepMind,"In social sciences, researchers often face challenges when conducting large-scale experiments, particularly due to the simulations' complexity and the lack of technical expertise required to develop such frameworks. Agent-Based Modeling (ABM) is a computational approach that simulates agents' actions and interactions to evaluate how their behaviors influence the outcomes. However, the traditional implementation of ABM can be demanding and complex. Generative Agent-Based Modeling (GABM) offers a solution by enabling scholars to create simulations where AI-driven agents can generate complex behaviors based on underlying rules and interactions. This paper introduces a framework for designing reliable experiments using GABM, making sophisticated simulation techniques more accessible to researchers across various fields. We provide a step-by-step guide for selecting appropriate tools, designing the model, establishing experimentation protocols, and validating results.",Alejandro Leonardo García Navarro|Nataliia Koneva|Alfonso Sánchez-Macián|José Alberto Hernández|Manuel Goyanes,,https://arxiv.org/abs/2411.07038v1,https://arxiv.org/pdf/2411.07038v1,,,,,cs.AI,cs.AI,https://arxiv.org/pdf/2411.07038v1.pdf
2411.02956v2,2024-11-05T09:54:13Z,2025-03-18 23:51:18,On Distributional Discrepancy for Experimental Design with General Assignment Probabilities,"We investigate experimental design for randomized controlled trials (RCTs) with both equal and unequal treatment-control assignment probabilities. Our work makes progress on the connection between the distributional discrepancy minimization (DDM) problem introduced by Harshaw et al. (2024) and the design of RCTs. We make two main contributions: First, we prove that approximating the optimal solution of the DDM problem within a certain constant error is NP-hard. Second, we introduce a new Multiplicative Weights Update (MWU) algorithm for the DDM problem, which improves the Gram-Schmidt walk algorithm used by Harshaw et al. (2024) when assignment probabilities are unequal. Building on the framework of Harshaw et al. (2024) and our MWU algorithm, we then develop the MWU design, which reduces the worst-case mean squared error in estimating the average treatment effect. Finally, we present a comprehensive simulation study comparing our design with commonly used designs.",Anup B. Rao|Peng Zhang,,https://arxiv.org/abs/2411.02956v2,https://arxiv.org/pdf/2411.02956v2,,The first result comes from our previous work at arxiv.org/abs/2211.14658,,,stat.ME,stat.ME,https://arxiv.org/pdf/2411.02956v2.pdf
2411.02740v5,2024-11-05T02:16:23Z,2026-01-14 19:50:14,An information-matching approach to optimal experimental design and active learning,"The efficacy of mathematical models heavily depends on the quality of the training data, yet collecting sufficient data is often expensive and challenging. Many modeling applications require inferring parameters only as a means to predict other quantities of interest (QoI). Because models often contain many unidentifiable (sloppy) parameters, QoIs often depend on a relatively small number of parameter combinations. Therefore, we introduce an information-matching criterion based on the Fisher Information Matrix to select the most informative training data from a candidate pool. This method ensures that the selected data contain sufficient information to learn only those parameters that are needed to constrain downstream QoIs. It is formulated as a convex optimization problem, making it scalable to large models and datasets. We demonstrate the effectiveness of this approach across various modeling problems in diverse scientific fields, including power systems and underwater acoustics. Finally, we use information-matching as a query function within an Active Learning loop for material science applications. In all these applications, we find that a relatively small set of optimal training data can provide the necessary information for achieving precise predictions. These results are encouraging for diverse future applications, particularly active learning in large machine learning models.",Yonatan Kurniawan|Tracianne B. Neilsen|Benjamin L. Francis|Alex M. Stankovic|Mingjian Wen|Ilia Nikiforov|Ellad B. Tadmor|Vasily V. Bulatov|Vincenzo Lordi|Mark K. Transtrum,"Brigham Young University, Provo, UT, USA|Brigham Young University, Provo, UT, USA|Achilles Heel Technologies, Orem, UT, USA|SLAC National Accelerator Laboratory, Menlo Park, CA, USA|University of Electronic Science and Technology of China, Chengdu, China|University of Minnesota, Minneapolis, MN, USA|University of Minnesota, Minneapolis, MN, USA|Lawrence Livermore National Laboratory|Lawrence Livermore National Laboratory|Brigham Young University, Provo, UT, USA",https://arxiv.org/abs/2411.02740v5,https://arxiv.org/pdf/2411.02740v5,,,,,cs.LG,cs.LG|cond-mat.mtrl-sci|physics.app-ph|physics.comp-ph|physics.data-an,https://arxiv.org/pdf/2411.02740v5.pdf
2411.02064v2,2024-11-04T13:06:46Z,2025-01-02 15:34:23,Amortized Bayesian Experimental Design for Decision-Making,"Many critical decisions, such as personalized medical diagnoses and product pricing, are made based on insights gained from designing, observing, and analyzing a series of experiments. This highlights the crucial role of experimental design, which goes beyond merely collecting information on system parameters as in traditional Bayesian experimental design (BED), but also plays a key part in facilitating downstream decision-making. Most recent BED methods use an amortized policy network to rapidly design experiments. However, the information gathered through these methods is suboptimal for down-the-line decision-making, as the experiments are not inherently designed with downstream objectives in mind. In this paper, we present an amortized decision-aware BED framework that prioritizes maximizing downstream decision utility. We introduce a novel architecture, the Transformer Neural Decision Process (TNDP), capable of instantly proposing the next experimental design, whilst inferring the downstream decision, thus effectively amortizing both tasks within a unified workflow. We demonstrate the performance of our method across several tasks, showing that it can deliver informative designs and facilitate accurate decision-making.",Daolang Huang|Yujia Guo|Luigi Acerbi|Samuel Kaski,,https://arxiv.org/abs/2411.02064v2,https://arxiv.org/pdf/2411.02064v2,,"20 pages, 6 figures. Accepted at the 38th Conference on Neural Information Processing Systems (NeurIPS 2024)",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2411.02064v2.pdf
2410.20017v1,2024-10-26T00:17:33Z,2024-10-26 00:17:33,Off-Policy Selection for Initiating Human-Centric Experimental Design,"In human-centric tasks such as healthcare and education, the heterogeneity among patients and students necessitates personalized treatments and instructional interventions. While reinforcement learning (RL) has been utilized in those tasks, off-policy selection (OPS) is pivotal to close the loop by offline evaluating and selecting policies without online interactions, yet current OPS methods often overlook the heterogeneity among participants. Our work is centered on resolving a pivotal challenge in human-centric systems (HCSs): how to select a policy to deploy when a new participant joining the cohort, without having access to any prior offline data collected over the participant? We introduce First-Glance Off-Policy Selection (FPS), a novel approach that systematically addresses participant heterogeneity through sub-group segmentation and tailored OPS criteria to each sub-group. By grouping individuals with similar traits, FPS facilitates personalized policy selection aligned with unique characteristics of each participant or group of participants. FPS is evaluated via two important but challenging applications, intelligent tutoring systems and a healthcare application for sepsis treatment and intervention. FPS presents significant advancement in enhancing learning outcomes of students and in-hospital care outcomes.",Ge Gao|Xi Yang|Qitong Gao|Song Ju|Miroslav Pajic|Min Chi,,https://arxiv.org/abs/2410.20017v1,https://arxiv.org/pdf/2410.20017v1,,,,,cs.LG,cs.LG|cs.AI|cs.HC,https://arxiv.org/pdf/2410.20017v1.pdf
2410.18734v1,2024-10-24T13:39:58Z,2024-10-24 13:39:58,Response Surface Designs for Crossed and Nested Multi-Stratum Structures,"Response surface designs are usually described as being run under complete randomization of the treatment combinations to the experimental units. In practice, however, it is often necessary or beneficial to run them under some kind of restriction to the randomization, leading to multi-stratum designs. In particular, some factors are often hard to set, so they cannot have their levels reset for each experimental unit. This paper presents a general solution to designing response surface experiments in any multi-stratum structure made up of crossing and/or nesting of unit factors. A stratum-by-stratum approach to constructing designs using compound optimal design criteria is used and illustrated. It is shown that good designs can be found even for large experiments in complex structures.",Luzia A. Trinca|Steven G. Gilmour,,https://arxiv.org/abs/2410.18734v1,https://arxiv.org/pdf/2410.18734v1,,"Submitted to Technometrics, 43 pages, 4 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2410.18734v1.pdf
2410.17392v1,2024-10-22T20:00:00Z,2024-10-22 20:00:00,Experimental Designs for Optimizing Last-Mile Delivery,"Companies like Amazon and UPS are heavily invested in last-mile delivery problems. Optimizing last-delivery operations not only creates tremendous cost savings for these companies but also generate broader societal and environmental benefits in terms of better delivery service and reduced air pollutants and greenhouse gas emissions. Last-mile delivery is readily formulated as the Travelling Salesman Problem (TSP), where a salesperson must visit several cities and return to the origin with the least cost. A solution to this problem is a Hamiltonian circuit in an undirected graph. Many methods exist for solving the TSP, but they often assume the travel costs are fixed. In practice, travel costs between delivery zones are random quantities, as they are subject to variation from traffic, weather, and other factors. Innovations such as truck-drone last-mile delivery creates even more uncertainties due to scarce data. A Bayesian D-optimal experimental design in conjunction with a regression model are proposed to estimate these unknown travel costs, and subsequently search for a highly efficient solution to the TSP. This framework can naturally be extended to incorporate the use of drones and any other emerging technology that has use in last-mile delivery.",Nicholas Rios|Jie Xu,,https://arxiv.org/abs/2410.17392v1,https://arxiv.org/pdf/2410.17392v1,,"22 Pages, 2 Figures with 4 subfigure panels each, To be submitted to Quality Engineering",,,stat.AP,stat.AP|math.OC,https://arxiv.org/pdf/2410.17392v1.pdf
2410.16923v1,2024-10-22T11:56:22Z,2024-10-22 11:56:22,A Toolbox for Design of Experiments for Energy Systems in Co-Simulation and Hardware Tests,"In context of highly complex energy system experiments, sensitivity analysis is gaining more and more importance to investigate the effects changing parameterization has on the outcome. Thus, it is crucial how to design an experiment to efficiently use the available resources. This paper describes the functionality of a toolbox designed to support the users in design of experiment for (co-)simulation and hardware tests. It provides a structure for object-oriented description of the parameterization and variations and performs sample generation based on this to provide a complete parameterization for the recommended experiment runs. After execution of the runs, it can also be used for analysis of the results to calculate and visualize the effects. The paper also presents two application cases using the toolbox which show how it can be implemented in sensitivity analysis studies with the co-simulation framework mosaik and a hybrid energy storage experiment.",Jan Sören Schwarz|Leonard Enrique Ramos Perez|Minh Cong Pham|Kai Heussen|Quoc Tuan Tran,,https://arxiv.org/abs/2410.16923v1,https://arxiv.org/pdf/2410.16923v1,https://doi.org/10.1109/OSMSES62085.2024.10668967,"7 pages, 6 figures, 2 tables, conference proceedings of OSMSES 2024","2024 Open Source Modelling and Simulation of Energy Systems (OSMSES), Vienna, Austria, 2024, pp. 1-7",10.1109/OSMSES62085.2024.10668967,cs.CE,cs.CE,https://arxiv.org/pdf/2410.16923v1.pdf
2410.11826v2,2024-10-15T17:53:07Z,2025-03-13 11:23:03,Bayesian Experimental Design via Contrastive Diffusions,"Bayesian Optimal Experimental Design (BOED) is a powerful tool to reduce the cost of running a sequence of experiments. When based on the Expected Information Gain (EIG), design optimization corresponds to the maximization of some intractable expected contrast between prior and posterior distributions. Scaling this maximization to high dimensional and complex settings has been an issue due to BOED inherent computational complexity. In this work, we introduce a pooled posterior distribution with cost-effective sampling properties and provide a tractable access to the EIG contrast maximization via a new EIG gradient expression. Diffusion-based samplers are used to compute the dynamics of the pooled posterior and ideas from bi-level optimization are leveraged to derive an efficient joint sampling-optimization loop. The resulting efficiency gain allows to extend BOED to the well-tested generative capabilities of diffusion models. By incorporating generative models into the BOED framework, we expand its scope and its use in scenarios that were previously impractical. Numerical experiments and comparison with state-of-the-art methods show the potential of the approach.",Jacopo Iollo|Christophe Heinkelé|Pierre Alliez|Florence Forbes,,https://arxiv.org/abs/2410.11826v2,https://arxiv.org/pdf/2410.11826v2,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2410.11826v2.pdf
2410.11390v1,2024-10-15T08:33:29Z,2024-10-15 08:33:29,Experimental Design Using Interlacing Polynomials,"We present a unified deterministic approach for experimental design problems using the method of interlacing polynomials. Our framework recovers the best-known approximation guarantees for the well-studied D/A/E-design problems with simple analysis. Furthermore, we obtain improved non-trivial approximation guarantee for E-design in the challenging small budget regime. Additionally, our approach provides an optimal approximation guarantee for a generalized ratio objective that generalizes both D-design and A-design.",Lap Chi Lau|Robert Wang|Hong Zhou,,https://arxiv.org/abs/2410.11390v1,https://arxiv.org/pdf/2410.11390v1,,16 pages,,,cs.DS,cs.DS|cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/2410.11390v1.pdf
2410.21294v1,2024-10-14T08:05:56Z,2024-10-14 08:05:56,"Optimization of Complex Process, Based on Design Of Experiments, a Generic Methodology","MicroLED displays are the result of a complex manufacturing chain. Each stage of this process, if optimized, contributes to achieving the highest levels of final efficiencies. Common works carried out by Pollen Metrology, Aledia, and Universit{é} Clermont-Auvergne led to a generic process optimization workflow. This software solution offers a holistic approach where stages are chained together for gaining a complete optimal solution. This paper highlights key corners of the methodology, validated by the experiments and process experts: data cleaning and multi-objective optimization.",Julien Baderot|Yann Cauchepin|Alexandre Seiller|Richard Fontanges|Sergio Martinez|Johann Foucher|Emmanuel Fuchs|Mehdi Daanoune|Vincent Grenier|Vincent Barra|Arnaud Guillin,UCA|UCA|UCA|UCA|UCA|UCA|UCA|UCA|UCA|UCA|UCA,https://arxiv.org/abs/2410.21294v1,https://arxiv.org/pdf/2410.21294v1,,"Eurodisplay 2024, Sep 2024, Grenoble, France",,,cs.NE,cs.NE|math.OC,https://arxiv.org/pdf/2410.21294v1.pdf
2410.06953v1,2024-10-09T14:51:54Z,2024-10-09 14:51:54,Control System Design and Experiments for Autonomous Underwater Helicopter Docking Procedure Based on Acoustic-inertial-optical Guidance,"A control system structure for the underwater docking procedure of an Autonomous Underwater Helicopter (AUH) is proposed in this paper, which utilizes acoustic-inertial-optical guidance. Unlike conventional Autonomous Underwater Vehicles (AUVs), the maneuverability requirements for AUHs are more stringent during the docking procedure, requiring it to remain stationary or have minimal horizontal movement while moving vertically. The docking procedure is divided into two stages: Homing and Landing, each stage utilizing different guidance methods. Additionally, a segmented aligning strategy operating at various altitudes and a linear velocity decision are both adopted in Landing stage. Due to the unique structure of the Subsea Docking System (SDS), the AUH is required to dock onto the SDS in a fixed orientation with specific attitude and altitude. Therefore, a particular criterion is proposed to determine whether the AUH has successfully docked onto the SDS. Furthermore, the effectiveness and robustness of the proposed control method in AUH's docking procedure are demonstrated through pool experiments and sea trials.",Haoda Li|Xinyu An|Rendong Feng|Zhenwei Rong|Zhuoyu Zhang|Zhipeng Li|Liming Zhao|Ying Chen,,https://arxiv.org/abs/2410.06953v1,https://arxiv.org/pdf/2410.06953v1,,,,,cs.RO,cs.RO,https://arxiv.org/pdf/2410.06953v1.pdf
2410.05552v3,2024-10-07T23:22:51Z,2024-11-12 00:58:02,Optimal Adaptive Experimental Design for Estimating Treatment Effect,"Given n experiment subjects with potentially heterogeneous covariates and two possible treatments, namely active treatment and control, this paper addresses the fundamental question of determining the optimal accuracy in estimating the treatment effect. Furthermore, we propose an experimental design that approaches this optimal accuracy, giving a (non-asymptotic) answer to this fundamental yet still open question. The methodological contribution is listed as following. First, we establish an idealized optimal estimator with minimal variance as benchmark, and then demonstrate that adaptive experiment is necessary to achieve near-optimal estimation accuracy. Secondly, by incorporating the concept of doubly robust method into sequential experimental design, we frame the optimal estimation problem as an online bandit learning problem, bridging the two fields of statistical estimation and bandit learning. Using tools and ideas from both bandit algorithm design and adaptive statistical estimation, we propose a general low switching adaptive experiment framework, which could be a generic research paradigm for a wide range of adaptive experimental design. Through novel lower bound techniques for non-i.i.d. data, we demonstrate the optimality of our proposed experiment. Numerical result indicates that the estimation accuracy approaches optimal with as few as two or three policy updates.",Jiachun Li|David Simchi-Levi|Yunxiao Zhao,,https://arxiv.org/abs/2410.05552v3,https://arxiv.org/pdf/2410.05552v3,,"Delete unrelated figure, update new lower bound results",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2410.05552v3.pdf
2410.12824v1,2024-10-01T15:45:41Z,2024-10-01 15:45:41,Optimization of Actuarial Neural Networks with Response Surface Methodology,"In the data-driven world of actuarial science, machine learning (ML) plays a crucial role in predictive modeling, enhancing risk assessment and pricing strategies. Neural networks, specifically combined actuarial neural networks (CANN), are vital for tasks such as mortality forecasting and pricing. However, optimizing hyperparameters (e.g., learning rates, layers) is essential for resource efficiency.
  This study utilizes a factorial design and response surface methodology (RSM) to optimize CANN performance. RSM effectively explores the hyperparameter space and captures potential curvature, outperforming traditional grid search. Our results show accurate performance predictions, identifying critical hyperparameters. By dropping statistically insignificant hyperparameters, we reduced runs from 288 to 188, with negligible loss in accuracy, achieving near-optimal out-of-sample Poisson deviance loss.",Belguutei Ariuntugs|Kehelwala Dewage Gayan Madurang,,https://arxiv.org/abs/2410.12824v1,https://arxiv.org/pdf/2410.12824v1,,This work was presented at the Actuarial Research Conference (ARC) 2024.Research abstract submitted and presented at ARC 2024. More details can be found at \url{https://sites.google.com/view/arc2024/home},,,q-fin.RM,q-fin.RM|cs.LG,https://arxiv.org/pdf/2410.12824v1.pdf
2409.18392v1,2024-09-27T02:14:13Z,2024-09-27 02:14:13,PNOD: An Efficient Projected Newton Framework for Exact Optimal Experimental Designs,"Computing the exact optimal experimental design has been a longstanding challenge in various scientific fields. This problem, when formulated using a specific information function, becomes a mixed-integer nonlinear programming (MINLP) problem, which is typically NP-hard, thus making the computation of a globally optimal solution extremely difficult. The branch and bound (BnB) method is a widely used approach for solving such MINLPs, but its practical efficiency heavily relies on the ability to solve continuous relaxations effectively within the BnB search tree. In this paper, we propose a novel projected Newton framework, combining with a vertex exchange method for efficiently solving the associated subproblems, designed to enhance the BnB method. This framework offers strong convergence guarantees by utilizing recent advances in solving self-concordant optimization and convex quadratic programming problems. Extensive numerical experiments on A-optimal and D-optimal design problems, two of the most commonly used models, demonstrate the framework's promising numerical performance. Specifically, our framework significantly improves the efficiency of node evaluation within the BnB search tree and enhances the accuracy of solutions compared to state-of-the-art methods. The proposed framework is implemented in an open source Julia package called \texttt{PNOD.jl}, which opens up possibilities for its application in a wide range of real-world scenarios.",Ling Liang|Haizhao Yang,,https://arxiv.org/abs/2409.18392v1,https://arxiv.org/pdf/2409.18392v1,,"24 pages, 9 figures",,,stat.ME,stat.ME|math.OC,https://arxiv.org/pdf/2409.18392v1.pdf
2409.15906v2,2024-09-24T09:21:11Z,2025-07-28 17:06:44,Sensitivity-preserving of Fisher Information Matrix through random data down-sampling for experimental design,"The quality of numerical reconstructions of unknown parameters in inverse problems heavily relies on the chosen data. It is crucial to select data that is sensitive to the parameters, which can be expressed through a sufficient conditioning of the Fisher Information Matrix. We propose a general framework that provides an efficient down-sampling strategy that can select experimental setups that preserves this conditioning, as opposed to the standard optimization approach in optimal experimental design. Matrix sketching techniques from randomized linear algebra is heavily leaned on to achieve this goal. The method requires drawing samples from a sensitivity-informed distribution, and gradient free sampling methods are integrated to execute the data selection. Numerical experiments demonstrate the effectiveness of this method in selecting sensor locations for Schrödinger potential reconstruction.",Kathrin Hellmuth|Christian Klingenberg|Qin Li,,https://arxiv.org/abs/2409.15906v2,https://arxiv.org/pdf/2409.15906v2,,,,,math.NA,math.NA|math.OC,https://arxiv.org/pdf/2409.15906v2.pdf
2409.10802v3,2024-09-17T00:39:14Z,2025-03-05 03:56:36,Bayesian Optimal Experimental Design for Robot Kinematic Calibration,"This paper develops a Bayesian optimal experimental design for robot kinematic calibration on ${\mathbb{S}^3 \!\times\! \mathbb{R}^3}$. Our method builds upon a Gaussian process approach that incorporates a geometry-aware kernel based on Riemannian Matérn kernels over ${\mathbb{S}^3}$. To learn the forward kinematics errors via Bayesian optimization with a Gaussian process, we define a geodesic distance-based objective function. Pointwise values of this function are sampled via noisy measurements taken using fiducial markers on the end-effector using a camera and computed pose with the nominal kinematics. The corrected Denavit-Hartenberg parameters are obtained using an efficient quadratic program that operates on the collected data sets. The effectiveness of the proposed method is demonstrated via simulations and calibration experiments on NASA's ocean world lander autonomy testbed (OWLAT).",Ersin Das|Thomas Touma|Joel W. Burdick,,https://arxiv.org/abs/2409.10802v3,https://arxiv.org/pdf/2409.10802v3,,ICRA 2025,,,cs.RO,cs.RO,https://arxiv.org/pdf/2409.10802v3.pdf
2409.09963v2,2024-09-16T03:40:19Z,2024-11-01 22:52:14,A global optimum-informed greedy algorithm for A-optimal experimental design,"Optimal experimental design (OED) concerns itself with identifying ideal methods of data collection, e.g.~via sensor placement. The \emph{greedy algorithm}, that is, placing one sensor at a time, in an iteratively optimal manner, stands as an extremely robust and easily executed algorithm for this purpose. However, it is a priori unclear whether this algorithm leads to sub-optimal regimes. Taking advantage of the author's recent work on non-smooth convex optimality criteria for OED, we here present a framework for rejection of sub-optimal greedy indices, and study the numerical benefits this offers.",Christian Aarset,,https://arxiv.org/abs/2409.09963v2,https://arxiv.org/pdf/2409.09963v2,https://doi.org/10.1007/978-3-031-87213-6_24,,,10.1007/978-3-031-87213-6_24,math.OC,math.OC,https://arxiv.org/pdf/2409.09963v2.pdf
2409.09141v2,2024-09-13T18:53:20Z,2024-10-03 02:05:29,Sequential infinite-dimensional Bayesian optimal experimental design with derivative-informed latent attention neural operator,"We develop a new computational framework to solve sequential Bayesian optimal experimental design (SBOED) problems constrained by large-scale partial differential equations with infinite-dimensional random parameters. We propose an adaptive terminal formulation of the optimality criteria for SBOED to achieve adaptive global optimality. We also establish an equivalent optimization formulation to achieve computational simplicity enabled by Laplace and low-rank approximations of the posterior. To accelerate the solution of the SBOED problem, we develop a derivative-informed latent attention neural operator (LANO), a new neural network surrogate model that leverages (1) derivative-informed dimension reduction for latent encoding, (2) an attention mechanism to capture the dynamics in the latent space, (3) an efficient training in the latent space augmented by projected Jacobian, which collectively leads to an efficient, accurate, and scalable surrogate in computing not only the parameter-to-observable (PtO) maps but also their Jacobians. We further develop the formulation for the computation of the MAP points, the eigenpairs, and the sampling from posterior by LANO in the reduced spaces and use these computations to solve the SBOED problem. We demonstrate the superior accuracy of LANO compared to two other neural architectures and the high accuracy of LANO compared to the finite element method (FEM) for the computation of MAP points and eigenvalues in solving the SBOED problem with application to the experimental design of the time to take MRI images in monitoring tumor growth. We show that the proposed computational framework achieves an amortized $180\times$ speedup.",Jinwoo Go|Peng Chen,,https://arxiv.org/abs/2409.09141v2,https://arxiv.org/pdf/2409.09141v2,,,,,cs.CE,cs.CE,https://arxiv.org/pdf/2409.09141v2.pdf
2409.09093v1,2024-09-12T08:55:16Z,2024-09-12 08:55:16,Response Surface Methodology coupled with desirability functions for multi-objective optimization: minimizing indoor overheating hours and maximizing useful daylight illuminance,"Response Surface Methodology (RSM) and desirability functions were employed in a case study to optimize the thermal and daylight performance of a computational model of a tropical housing typology. Specifically, this approach simultaneously optimized Indoor Overheating Hours (IOH) and Useful Daylight Illuminance (UDI) metrics through an Overall Desirability (D). The lack of significant association between IOH and other annual daylight metrics enabled a focused optimization of IOH and UDI. Each response required only 138 simulation runs (~30 hours for 276 runs) to determine the optimal values for passive strategies: window-to-wall ratio (WWR) and roof overhang depth across four orientations, totalling eight factors. First, initial screening based on $2_V^{8-2}$ fractional factorial design, identified four key factors using stepwise and Lasso regression, narrowed down to three: roof overhang depth on the south and west, WWR on the west, and WWR on the south. Then, RSM optimization yielded an optimal solution (roof overhang: 3.78 meters, west WWR: 3.76%, south WWR: 29.3%) with a D of 0.625 (IOH: 8.33%, UDI: 79.67%). Finally, robustness analysis with 1,000 bootstrap replications provided 95% confidence intervals for the optimal values. This study optimally balances thermal comfort and daylight with few experiments using a computationally-efficient multi-objective approach.",Juan Gamero-Salinas|Jesús López-Fidalgo,,https://arxiv.org/abs/2409.09093v1,https://arxiv.org/pdf/2409.09093v1,https://doi.org/10.1038/s41598-025-96376-x,,"Sci Rep 15, 12173 (2025)",10.1038/s41598-025-96376-x,eess.SY,eess.SY,https://arxiv.org/pdf/2409.09093v1.pdf
2409.05354v2,2024-09-09T06:27:54Z,2024-11-28 10:35:55,Recursive Nested Filtering for Efficient Amortized Bayesian Experimental Design,"This paper introduces the Inside-Out Nested Particle Filter (IO-NPF), a novel, fully recursive, algorithm for amortized sequential Bayesian experimental design in the non-exchangeable setting. We frame policy optimization as maximum likelihood estimation in a non-Markovian state-space model, achieving (at most) $\mathcal{O}(T^2)$ computational complexity in the number of experiments. We provide theoretical convergence guarantees and introduce a backward sampling algorithm to reduce trajectory degeneracy. IO-NPF offers a practical, extensible, and provably consistent approach to sequential Bayesian experimental design, demonstrating improved efficiency over existing methods.",Sahel Iqbal|Hany Abdulsamad|Sara Pérez-Vieites|Simo Särkkä|Adrien Corenflos,,https://arxiv.org/abs/2409.05354v2,https://arxiv.org/pdf/2409.05354v2,,Accepted to NeurIPS BDU Workshop 2024,,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2409.05354v2.pdf
2408.14669v2,2024-08-26T22:20:37Z,2025-08-26 19:04:18,Inspection-Guided Randomization: A Flexible and Transparent Restricted Randomization Framework for Better Experimental Design,"Randomized experiments are considered the gold standard for estimating causal effects. However, out of the set of possible randomized assignments, some may be likely to produce poor effect estimates and misleading conclusions. Restricted randomization is an experimental design strategy that filters out undesirable treatment assignments, but its application has primarily been limited to ensuring covariate balance in two-arm studies where the target estimand is the average treatment effect. Other experimental settings with different design desiderata and target effect estimands could also stand to benefit from a restricted randomization approach. We introduce Inspection-Guided Randomization (IGR), a transparent and flexible framework for restricted randomization that filters out undesirable treatment assignments by inspecting assignments against analyst-specified, domain-informed design desiderata. In IGR, the acceptable treatment assignments are locked in ex ante and pre-registered in the trial protocol, thus safeguarding against $p$-hacking and promoting reproducibility. Through illustrative simulation studies motivated by education and behavioral health interventions, we demonstrate how IGR can be used to improve effect estimates compared to benchmark designs in group formation experiments and experiments with interference.",Maggie Wang|René F. Kizilcec|Michael Baiocchi,,https://arxiv.org/abs/2408.14669v2,https://arxiv.org/pdf/2408.14669v2,https://doi.org/10.3102/10769986251342292,"Updates to prior version: removed multi-arm trial vignette and moved group formation vignette to appendix, added effect estimates from linear regression estimator, added theoretical results on IGR bias and type I error rate inflation in re-randomization. Journal of Educational and Behavioral Statistics (2025)",,10.3102/10769986251342292,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2408.14669v2.pdf
2409.11215v1,2024-08-26T12:30:55Z,2024-08-26 12:30:55,Computational and experimental design of fast and versatile magnetic soft robotic low Re swimmers,"Miniaturized magnetic soft robots have shown extraordinary capabilities of contactless manipulation, complex path maneuvering, precise localization, and quick actuation, which have equipped them to cater to challenging biomedical applications such as targeted drug delivery, internal wound healing, and laparoscopic surgery. However, despite their successful fabrication by several different research groups, a thorough design strategy encompassing the optimized kinematic performance of the three fundamental biomimetic swimming modes at miniaturized length scales has not been reported till now. Here, we resolve this by designing magnetic soft robotic swimmers (MSRSs) from the class of helical and undulatory low Reynolds number (Re) swimmers using a fully coupled, experimentally calibrated computational fluid dynamics model. We study (and compare) their swimming performance, and report their steady-state swimming speed for different non-dimensional numbers that capture the competition by magnetic loading, non-linear elastic deformation and viscous solid-fluid coupling. We investigate their stability for different initial spatial orientations to ensure robustness during real-life applications. Our results show that the helical 'finger-shaped' swimmer is, by far, the fastest low Re swimmer in terms of body lengths per cycle, but that the undulatory 'carangiform' swimmer proved to be the most versatile, bi-directional swimmer with maximum stability.",R Pramanik|M Park|Z Ren|M Sitti|RWCP Verstappen|PR Onck,,https://arxiv.org/abs/2409.11215v1,https://arxiv.org/pdf/2409.11215v1,,,,,cs.RO,cs.RO,https://arxiv.org/pdf/2409.11215v1.pdf
2408.09607v2,2024-08-18T22:25:08Z,2024-08-24 19:56:40,Experimental Design For Causal Inference Through An Optimization Lens,"The study of experimental design offers tremendous benefits for answering causal questions across a wide range of applications, including agricultural experiments, clinical trials, industrial experiments, social experiments, and digital experiments. Although valuable in such applications, the costs of experiments often drive experimenters to seek more efficient designs. Recently, experimenters have started to examine such efficiency questions from an optimization perspective, as experimental design problems are fundamentally decision-making problems. This perspective offers a lot of flexibility in leveraging various existing optimization tools to study experimental design problems. This manuscript thus aims to examine the foundations of experimental design problems in the context of causal inference as viewed through an optimization lens.",Jinglong Zhao,,https://arxiv.org/abs/2408.09607v2,https://arxiv.org/pdf/2408.09607v2,,,,,stat.ME,stat.ME|econ.EM,https://arxiv.org/pdf/2408.09607v2.pdf
2408.09582v1,2024-08-18T19:45:49Z,2024-08-18 19:45:49,A Likelihood-Free Approach to Goal-Oriented Bayesian Optimal Experimental Design,"Conventional Bayesian optimal experimental design seeks to maximize the expected information gain (EIG) on model parameters. However, the end goal of the experiment often is not to learn the model parameters, but to predict downstream quantities of interest (QoIs) that depend on the learned parameters. And designs that offer high EIG for parameters may not translate to high EIG for QoIs. Goal-oriented optimal experimental design (GO-OED) thus directly targets to maximize the EIG of QoIs.
  We introduce LF-GO-OED (likelihood-free goal-oriented optimal experimental design), a computational method for conducting GO-OED with nonlinear observation and prediction models. LF-GO-OED is specifically designed to accommodate implicit models, where the likelihood is intractable. In particular, it builds a density ratio estimator from samples generated from approximate Bayesian computation (ABC), thereby sidestepping the need for likelihood evaluations or density estimations. The overall method is validated on benchmark problems with existing methods, and demonstrated on scientific applications of epidemiology and neural science.",Atlanta Chakraborty|Xun Huan|Tommie Catanach,,https://arxiv.org/abs/2408.09582v1,https://arxiv.org/pdf/2408.09582v1,,,,,stat.CO,stat.CO|stat.AP|stat.ME|stat.ML,https://arxiv.org/pdf/2408.09582v1.pdf
2408.07143v2,2024-08-13T18:24:54Z,2024-09-06 12:50:33,Optimal Experimental Design for Universal Differential Equations,"Complex dynamic systems are typically either modeled using expert knowledge in the form of differential equations or via data-driven universal approximation models such as artificial neural networks (ANN). While the first approach has advantages with respect to interpretability, transparency, data-efficiency, and extrapolation, the second approach is able to learn completely unknown functional relations from data and may result in models that can be evaluated more efficiently. To combine the complementary advantages, universal differential equations (UDE) have been suggested. They replace unknown terms in the differential equations with ANN. Such hybrid models allow to both encode prior domain knowledge, such as first principles, and to learn unknown mechanisms from data. Often, data for the training of UDE can only be obtained via costly experiments. We consider optimal experimental design (OED) for planning of experiments and generating data needed to train UDE. The number of weights in the embedded ANN usually leads to an overfitting of the regression problem. To make the OED problem tractable for optimization, we propose and compare dimension reduction methods that are based on lumping of weights and singular value decomposition of the Fisher information matrix (FIM), respectively. They result in lower-dimensional variational differential equations, which are easier to solve and yield regular FIM. Our numerical results showcase the advantages of OED for UDE, such as increased data-efficiency and better extrapolation properties.",Christoph Plate|Carl Julius Martensen|Sebastian Sager,,https://arxiv.org/abs/2408.07143v2,https://arxiv.org/pdf/2408.07143v2,,,,,math.OC,math.OC,https://arxiv.org/pdf/2408.07143v2.pdf
2408.06521v1,2024-08-12T22:41:05Z,2024-08-12 22:41:05,All the single cells: single-cell transcriptomics/epigenomics experimental design and analysis considerations for glial biologists,"Single-cell transcriptomics, epigenomics, and other 'omics applied at single-cell resolution can significantly advance hypotheses and understanding of glial biology. Omics technologies are revealing a large and growing number of new glial cell subtypes, defined by their gene expression profile. These subtypes have significant implications for understanding glial cell function, cell-cell communications, and glia-specific changes between homeostasis and conditions such as neurological disease. For many, the training in how to analyze, interpret, and understand these large datasets has been through reading and understanding literature from other fields like biostatistics. Here, we provide a primer for glial biologists on experimental design and analysis of single-cell RNA-seq datasets. Our goal is to further the understanding of why decisions might be made about datasets and to enhance biologists' ability to interpret and critique their work and the work of others. We review the steps involved in single-cell analysis with a focus on decision points and particular notes for glia. The goal of this primer is to ensure that single-cell 'omics experiments continue to advance glial biology in a rigorous and replicable way.",Katherine E. Prater|Kevin Z. Lin,,https://arxiv.org/abs/2408.06521v1,https://arxiv.org/pdf/2408.06521v1,,"66 pages, 1 table, 5 figures",,,q-bio.QM,q-bio.QM,https://arxiv.org/pdf/2408.06521v1.pdf
2408.12607v1,2024-08-08T06:24:57Z,2024-08-08 06:24:57,Interactive Design-of-Experiments: Optimizing a Cooling System,"The optimization of cooling systems is important in many cases, for example for cabin and battery cooling in electric cars. Such an optimization is governed by multiple, conflicting objectives and it is performed across a multi-dimensional parameter space. The extent of the parameter space, the complexity of the non-linear model of the system, as well as the time needed per simulation run and factors that are not modeled in the simulation necessitate an iterative, semi-automatic approach. We present an interactive visual optimization approach, where the user works with a p-h diagram to steer an iterative, guided optimization process. A deep learning (DL) model provides estimates for parameters, given a target characterization of the system, while numerical simulation is used to compute system characteristics for an ensemble of parameter sets. Since the DL model only serves as an approximation of the inverse of the cooling system and since target characteristics can be chosen according to different, competing objectives, an iterative optimization process is realized, developing multiple sets of intermediate solutions, which are visually related to each other. The standard p-h diagram, integrated interactively in this approach, is complemented by a dual, also interactive visual representation of additional expressive measures representing the system characteristics. We show how the known four-points semantic of the p-h diagram meaningfully transfers to the dual data representation. When evaluating this approach in the automotive domain, we found that our solution helped with the overall comprehension of the cooling system and that it lead to a faster convergence during optimization.",Rainer Splechtna|Majid Behravan|Mario Jelovic|Denis Gracanin|Helwig Hauser|Kresimir Matkovic,,https://arxiv.org/abs/2408.12607v1,https://arxiv.org/pdf/2408.12607v1,https://doi.org/10.1109/TVCG.2024.3456356,Will be presented at IEEE VIS 2024,"IEEE Transactions on Visualization and Computer Graphics Jan. 2025, pp. 44-53, vol. 31",10.1109/TVCG.2024.3456356,cs.HC,cs.HC|cs.LG,https://arxiv.org/pdf/2408.12607v1.pdf
2408.02166v1,2024-08-04T23:31:07Z,2024-08-04 23:31:07,Efficient Approximate Methods for Design of Experiments for Copolymer Engineering,"We develop a set of algorithms to solve a broad class of Design of Experiment (DoE) problems efficiently. Specifically, we consider problems in which one must choose a subset of polymers to test in experiments such that the learning of the polymeric design rules is optimal. This subset must be selected from a larger set of polymers permissible under arbitrary experimental design constraints. We demonstrate the performance of our algorithms by solving several pragmatic nucleic acid therapeutics engineering scenarios, where limitations in synthesis of chemically diverse nucleic acids or feasibility of measurements in experimental setups appear as constraints. Our approach focuses on identifying optimal experimental designs from a given set of experiments, which is in contrast to traditional, generative DoE methods like BIBD. Finally, we discuss how these algorithms are broadly applicable to well-established optimal DoE criteria like D-optimality.",Swagatam Mukhopadhyay,,https://arxiv.org/abs/2408.02166v1,https://arxiv.org/pdf/2408.02166v1,,,,,q-bio.QM,q-bio.QM,https://arxiv.org/pdf/2408.02166v1.pdf
2407.16212v1,2024-07-23T06:33:37Z,2024-07-23 06:33:37,Optimal experimental design: Formulations and computations,"Questions of `how best to acquire data' are essential to modeling and prediction in the natural and social sciences, engineering applications, and beyond. Optimal experimental design (OED) formalizes these questions and creates computational methods to answer them. This article presents a systematic survey of modern OED, from its foundations in classical design theory to current research involving OED for complex models. We begin by reviewing criteria used to formulate an OED problem and thus to encode the goal of performing an experiment. We emphasize the flexibility of the Bayesian and decision-theoretic approach, which encompasses information-based criteria that are well-suited to nonlinear and non-Gaussian statistical models. We then discuss methods for estimating or bounding the values of these design criteria; this endeavor can be quite challenging due to strong nonlinearities, high parameter dimension, large per-sample costs, or settings where the model is implicit. A complementary set of computational issues involves optimization methods used to find a design; we discuss such methods in the discrete (combinatorial) setting of observation selection and in settings where an exact design can be continuously parameterized. Finally we present emerging methods for sequential OED that build non-myopic design policies, rather than explicit designs; these methods naturally adapt to the outcomes of past experiments in proposing new experiments, while seeking coordination among all experiments to be performed. Throughout, we highlight important open questions and challenges.",Xun Huan|Jayanth Jagalur|Youssef Marzouk,,https://arxiv.org/abs/2407.16212v1,https://arxiv.org/pdf/2407.16212v1,https://doi.org/10.1017/S0962492924000023,Appears in Acta Numerica 2024. This version contains an evolving set of post-publication additions and corrections,Acta Numerica 33 (2024) 715-840,10.1017/S0962492924000023,stat.ME,stat.ME|math.NA|stat.CO,https://arxiv.org/pdf/2407.16212v1.pdf
2407.15468v2,2024-07-22T08:28:01Z,2025-07-21 09:30:32,Asymptotic efficiency for Sobol' and Cram{é}r-von Mises indices under two designs of experiments,"A variety of indices aim to quantify the impact of input variables on a response, typically the output from a complex computer code or black-box model. Most commonly used, the Sobol' index typically measures the influence of some inputs from an explained variance perspective. However, some situations may require a more targeted analysis of some inputs influence. With no prior information, distribution-based measures appear to be appealing. In this purpose, so-called Cram{é}r-von Mises indices (and their generalization) have been proposed in the literature, defined as an excess probability integrated over the output distribution that aim to reflect influence on the whole distribution of the output rather than on the variance solely. Inference of these various indices has remained a challenging topic especially in presence of many inputs. While several Sobol' indices estimators are known to be optimal under regularity conditions, the issue of asymptotic efficiency for Cram{é}r-von Mises indices has been unaddressed in the literature so far. For these indices, we derive in this paper the efficiency bounds and discuss the known methods to achieve such optimal bounds. Two estimation contexts are considered: the so-called Pick-Freeze scheme and the Given-Data setting, for which the estimation is produced from a unique input-output sample.",Thierry Klein|Agnès Lagnoux|Paul Rochet|Thi Mong Ngoc Nguyen,"ENAC, IMT|IMT|OPTIM|",https://arxiv.org/abs/2407.15468v2,https://arxiv.org/pdf/2407.15468v2,,,,,math.ST,math.ST,https://arxiv.org/pdf/2407.15468v2.pdf
2407.06173v2,2024-07-08T17:48:57Z,2024-12-10 19:30:55,Large Row-Constrained Supersaturated Designs for High-throughput Screening,"High-throughput screening, in which multiwell plates are used to test large numbers of compounds against specific targets, is widely used across many areas of the biological sciences and most prominently in drug discovery. We propose a statistically principled approach to these screening experiments, using the machinery of supersaturated designs and the Lasso. To accommodate limitations on the number of biological entities that can be applied to a single microplate well, we present a new class of row-constrained supersaturated designs. We develop a computational procedure to construct these designs, provide some initial lower bounds on the average squared off-diagonal values of their main-effects information matrix, and study the impact of the constraint on design quality. We also show via simulation that the proposed constrained row screening method is statistically superior to existing methods and demonstrate the use of the new methodology on a real drug-discovery system.",Byran J. Smucker|Stephen E. Wright|Isaac Williams|Richard C. Page|Andor J. Kiss|Surendra Bikram Silwal|Maria Weese|David J. Edwards,,https://arxiv.org/abs/2407.06173v2,https://arxiv.org/pdf/2407.06173v2,,Additional supplementary materials can be found at https://sites.miamioh.edu/byran-smucker/research-2/,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2407.06173v2.pdf
2406.19835v2,2024-06-28T11:18:55Z,2024-10-07 12:16:42,Surrogate model for Bayesian optimal experimental design in chromatography,"We applied Bayesian Optimal Experimental Design (OED) in the estimation of parameters involved in the Equilibrium Dispersive Model for chromatography with two components with the Langmuir adsorption isotherm. The coefficients estimated were Henry's coefficients, the total absorption capacity and the number of theoretical plates, while the design variables were the injection time and the initial concentration. The Bayesian OED algorithm is based on nested Monte Carlo estimation, which becomes computationally challenging due to the simulation time of the PDE involved in the dispersive model. This complication was relaxed by introducing a surrogate model based on Piecewise Sparse Linear Interpolation. Using the surrogate model instead the original reduces significantly the simulation time and it approximates the solution of the PDE with high degree of accuracy. The estimation of the parameters over strategical design points provided by OED reduces the uncertainty in the estimation of parameters. Additionally, the Bayesian OED methodology indicates no improvements when increasing the number of measurements in temporal nodes above a threshold value.",Jose Rodrigo Rojo-Garcia|Heikki Haario|Tapio Helin|Tuomo Sainio,,https://arxiv.org/abs/2406.19835v2,https://arxiv.org/pdf/2406.19835v2,,23 pages and 8 figures,,,stat.AP,stat.AP|math.NA,https://arxiv.org/pdf/2406.19835v2.pdf
2406.14003v3,2024-06-20T05:13:33Z,2024-10-16 16:51:13,Deep Optimal Experimental Design for Parameter Estimation Problems,"Optimal experimental design is a well studied field in applied science and engineering. Techniques for estimating such a design are commonly used within the framework of parameter estimation. Nonetheless, in recent years parameter estimation techniques are changing rapidly with the introduction of deep learning techniques to replace traditional estimation methods. This in turn requires the adaptation of optimal experimental design that is associated with these new techniques. In this paper we investigate a new experimental design methodology that uses deep learning. We show that the training of a network as a Likelihood Free Estimator can be used to significantly simplify the design process and circumvent the need for the computationally expensive bi-level optimization problem that is inherent in optimal experimental design for non-linear systems. Furthermore, deep design improves the quality of the recovery process for parameter estimation problems. As proof of concept we apply our methodology to two different systems of Ordinary Differential Equations.",Md Shahriar Rahim Siddiqui|Arman Rahmim|Eldad Haber,,https://arxiv.org/abs/2406.14003v3,https://arxiv.org/pdf/2406.14003v3,https://doi.org/10.1088/1402-4896/ad88b1,,,10.1088/1402-4896/ad88b1,stat.ML,stat.ML|cs.AI|cs.LG|stat.ME,https://arxiv.org/pdf/2406.14003v3.pdf
2406.13425v2,2024-06-19T10:31:57Z,2025-03-30 15:19:54,Coupled Input-Output Dimension Reduction: Application to Goal-oriented Bayesian Experimental Design and Global Sensitivity Analysis,"We introduce a new method to jointly reduce the dimension of the input and output space of a function between high-dimensional spaces. Choosing a reduced input subspace influences which output subspace is relevant and vice versa. Conventional methods focus on reducing either the input or output space, even though both are often reduced simultaneously in practice. Our coupled approach naturally supports goal-oriented dimension reduction, where either an input or output quantity of interest is prescribed. We consider, in particular, goal-oriented sensor placement and goal-oriented sensitivity analysis, which can be viewed as dimension reduction where the most important output or, respectively, input components are chosen. Both applications present difficult combinatorial optimization problems with expensive objectives such as the expected information gain and Sobol' indices. By optimizing gradient-based bounds, we can determine the most informative sensors and most influential parameters as the largest diagonal entries of some diagnostic matrices, thus bypassing the combinatorial optimization and objective evaluation.",Qiao Chen|Elise Arnaud|Ricardo Baptista|Olivier Zahm,,https://arxiv.org/abs/2406.13425v2,https://arxiv.org/pdf/2406.13425v2,,,,,stat.ML,stat.ML|cs.LG|math.ST,https://arxiv.org/pdf/2406.13425v2.pdf
2406.11940v1,2024-06-17T17:27:18Z,2024-06-17 17:27:18,Model-Based Inference and Experimental Design for Interference Using Partial Network Data,"The stable unit treatment value assumption states that the outcome of an individual is not affected by the treatment statuses of others, however in many real world applications, treatments can have an effect on many others beyond the immediately treated. Interference can generically be thought of as mediated through some network structure. In many empirically relevant situations however, complete network data (required to adjust for these spillover effects) are too costly or logistically infeasible to collect. Partially or indirectly observed network data (e.g., subsamples, aggregated relational data (ARD), egocentric sampling, or respondent-driven sampling) reduce the logistical and financial burden of collecting network data, but the statistical properties of treatment effect adjustments from these design strategies are only beginning to be explored. In this paper, we present a framework for the estimation and inference of treatment effect adjustments using partial network data through the lens of structural causal models. We also illustrate procedures to assign treatments using only partial network data, with the goal of either minimizing estimator variance or optimally seeding. We derive single network asymptotic results applicable to a variety of choices for an underlying graph model. We validate our approach using simulated experiments on observed graphs with applications to information diffusion in India and Malawi.",Steven Wilkins Reeves|Shane Lubold|Arun G. Chandrasekhar|Tyler H. McCormick,,https://arxiv.org/abs/2406.11940v1,https://arxiv.org/pdf/2406.11940v1,,,,,stat.ME,stat.ME|cs.SI|econ.EM|stat.ML|stat.OT,https://arxiv.org/pdf/2406.11940v1.pdf
2406.02866v1,2024-06-05T02:27:56Z,2024-06-05 02:27:56,A Design Experience for Interactive Narrative Based on The User Behavior,"Research on interactive narrative experiences in physical spaces is becoming more popular, growing into an established new media art format with the development of technology and evolution of audience aesthetics. However, the methods of designing interactive narratives are still similar to the basic video narratology of traditional designers, directors, and producers. This paper provides a design method based on the user's physical behavior and proposes an art installation by this method, where the aim of the installation is to transmit a more vivid story to users, presenting a new research inspiration of interactive narratology for designers and researchers.",Yuan Yao|Haipeng Mi,,https://arxiv.org/abs/2406.02866v1,https://arxiv.org/pdf/2406.02866v1,,to appear at Cumulus Conference Proceedings Roma 2021,Cumulus Conference Proceedings Roma 2021,,cs.HC,cs.HC,https://arxiv.org/pdf/2406.02866v1.pdf
2406.02470v2,2024-06-04T16:40:55Z,2025-07-29 08:24:30,Meta-Designing Quantum Experiments with Language Models,"Artificial Intelligence (AI) can solve complex scientific problems beyond human capabilities, but the resulting solutions offer little insight into the underlying physical principles. One prominent example is quantum physics, where computers can discover experiments for the generation of specific quantum states, but it is unclear how finding general design concepts can be automated. Here, we address this challenge by training a transformer-based language model to create human-readable Python code, which solves an entire class of problems in a single pass. This strategy, which we call meta-design, enables scientists to gain a deeper understanding and extrapolate to larger experiments without additional optimization. To demonstrate the effectiveness of our approach, we uncover previously unknown experimental generalizations of important quantum states, e.g. from condensed matter physics. The underlying methodology of meta-design can naturally be extended to fields such as materials science or engineering.",Sören Arlt|Haonan Duan|Felix Li|Sang Michael Xie|Yuhuai Wu|Mario Krenn,,https://arxiv.org/abs/2406.02470v2,https://arxiv.org/pdf/2406.02470v2,,"8+23 pages, 5 figures",,,quant-ph,quant-ph|cs.LG,https://arxiv.org/pdf/2406.02470v2.pdf
2406.01541v1,2024-06-03T17:19:30Z,2024-06-03 17:19:30,Adaptive discretization algorithms for locally optimal experimental design,"We develop adaptive discretization algorithms for locally optimal experimental design of nonlinear prediction models. With these algorithms, we refine and improve a pertinent state-of-the-art algorithm in various respects. We establish novel termination, convergence, and convergence rate results for the proposed algorithms. In particular, we prove a sublinear convergence rate result under very general assumptions on the design criterion and, most notably, a linear convergence result under the additional assumption that the design criterion is strongly convex and the design space is finite. Additionally, we prove the finite termination at approximately optimal designs, including upper bounds on the number of iterations until termination. And finally, we illustrate the practical use of the proposed algorithms by means of two application examples from chemical engineering: one with a stationary model and one with a dynamic model.",Jochen Schmid|Philipp Seufert|Michael Bortz,,https://arxiv.org/abs/2406.01541v1,https://arxiv.org/pdf/2406.01541v1,,42 pages,,,math.OC,math.OC|math.ST,https://arxiv.org/pdf/2406.01541v1.pdf
2405.11548v3,2024-05-19T13:26:33Z,2024-06-22 07:37:33,Adaptive Online Experimental Design for Causal Discovery,"Causal discovery aims to uncover cause-and-effect relationships encoded in causal graphs by leveraging observational, interventional data, or their combination. The majority of existing causal discovery methods are developed assuming infinite interventional data. We focus on data interventional efficiency and formalize causal discovery from the perspective of online learning, inspired by pure exploration in bandit problems. A graph separating system, consisting of interventions that cut every edge of the graph at least once, is sufficient for learning causal graphs when infinite interventional data is available, even in the worst case. We propose a track-and-stop causal discovery algorithm that adaptively selects interventions from the graph separating system via allocation matching and learns the causal graph based on sampling history. Given any desired confidence value, the algorithm determines a termination condition and runs until it is met. We analyze the algorithm to establish a problem-dependent upper bound on the expected number of required interventional samples. Our proposed algorithm outperforms existing methods in simulations across various randomly generated causal graphs. It achieves higher accuracy, measured by the structural hamming distance (SHD) between the learned causal graph and the ground truth, with significantly fewer samples.",Muhammad Qasim Elahi|Lai Wei|Murat Kocaoglu|Mahsa Ghasemi,,https://arxiv.org/abs/2405.11548v3,https://arxiv.org/pdf/2405.11548v3,,To appear in Proceedings of ICML 24,,,cs.LG,cs.LG|stat.AP,https://arxiv.org/pdf/2405.11548v3.pdf
2405.10135v1,2024-05-16T14:31:30Z,2024-05-16 14:31:30,Self-supervised feature distillation and design of experiments for efficient training of micromechanical deep learning surrogates,Machine learning surrogate emulators are needed in engineering design and optimization tasks to rapidly emulate computationally expensive physics-based models. In micromechanics problems the local full-field response variables are desired at microstructural length scales. While there has been a great deal of work on establishing architectures for these tasks there has been relatively little work on establishing microstructural experimental design strategies. This work demonstrates that intelligent selection of microstructural volume elements for subsequent physics simulations enables the establishment of more accurate surrogate models. There exist two key challenges towards establishing a suitable framework: (1) microstructural feature quantification and (2) establishment of a criteria which encourages construction of a diverse training data set. Three feature extraction strategies are used as well as three design criteria. A novel contrastive feature extraction approach is established for automated self-supervised extraction of microstructural summary statistics. Results indicate that for the problem considered up to a 8\% improvement in surrogate performance may be achieved using the proposed design and training strategy. Trends indicate this approach may be even more beneficial when scaled towards larger problems. These results demonstrate that the selection of an efficient experimental design is an important consideration when establishing machine learning based surrogate models.,Patxi Fernandez-Zelaia|Jason Mayeur|Jiahao Cheng|Yousub Lee|Kevin Knipe|Kai Kadau,,https://arxiv.org/abs/2405.10135v1,https://arxiv.org/pdf/2405.10135v1,,,,,cs.CE,cs.CE|cond-mat.mtrl-sci,https://arxiv.org/pdf/2405.10135v1.pdf
2405.08636v1,2024-05-14T14:14:23Z,2024-05-14 14:14:23,Optimal design of experiments in the context of machine-learning inter-atomic potentials: improving the efficiency and transferability of kernel based methods,"Data-driven, machine learning (ML) models of atomistic interactions are often based on flexible and non-physical functions that can relate nuanced aspects of atomic arrangements into predictions of energies and forces. As a result, these potentials are as good as the training data (usually results of so-called ab initio simulations) and we need to make sure that we have enough information for a model to become sufficiently accurate, reliable and transferable. The main challenge stems from the fact that descriptors of chemical environments are often sparse high-dimensional objects without a well-defined continuous metric. Therefore, it is rather unlikely that any ad hoc method of choosing training examples will be indiscriminate, and it will be easy to fall into the trap of confirmation bias, where the same narrow and biased sampling is used to generate train- and test- sets. We will demonstrate that classical concepts of statistical planning of experiments and optimal design can help to mitigate such problems at a relatively low computational cost. The key feature of the method we will investigate is that they allow us to assess the informativeness of data (how much we can improve the model by adding/swapping a training example) and verify if the training is feasible with the current set before obtaining any reference energies and forces -- a so-called off-line approach. In other words, we are focusing on an approach that is easy to implement and doesn't require sophisticated frameworks that involve automated access to high-performance computational (HPC).",Bartosz Barzdajn|Christopher P. Race,,https://arxiv.org/abs/2405.08636v1,https://arxiv.org/pdf/2405.08636v1,,,,,cond-mat.mtrl-sci,cond-mat.mtrl-sci|cs.LG,https://arxiv.org/pdf/2405.08636v1.pdf
2405.07412v1,2024-05-13T01:19:51Z,2024-05-13 01:19:51,Non-intrusive optimal experimental design for large-scale nonlinear Bayesian inverse problems using a Bayesian approximation error approach,"We consider optimal experimental design (OED) for nonlinear inverse problems within the Bayesian framework. Optimizing the data acquisition process for large-scale nonlinear Bayesian inverse problems is a computationally challenging task since the posterior is typically intractable and commonly-encountered optimality criteria depend on the observed data. Since these challenges are not present in OED for linear Bayesian inverse problems, we propose an approach based on first linearizing the associated forward problem and then optimizing the experimental design. Replacing an accurate but costly model with some linear surrogate, while justified for certain problems, can lead to incorrect posteriors and sub-optimal designs if model discrepancy is ignored. To avoid this, we use the Bayesian approximation error (BAE) approach to formulate an A-optimal design objective for sensor selection that is aware of the model error. In line with recent developments, we prove that this uncertainty-aware objective is independent of the exact choice of linearization. This key observation facilitates the formulation of an uncertainty-aware OED objective function using a completely trivial linear map, the zero map, as a surrogate to the forward dynamics. The base methodology is also extended to marginalized OED problems, accommodating uncertainties arising from both linear approximations and unknown auxiliary parameters. Our approach only requires parameter and data sample pairs, hence it is particularly well-suited for black box forward models. We demonstrate the effectiveness of our method for finding optimal designs in an idealized subsurface flow inverse problem and for tsunami detection.",Karina Koval|Ruanui Nicholson,,https://arxiv.org/abs/2405.07412v1,https://arxiv.org/pdf/2405.07412v1,,,,,math.NA,math.NA,https://arxiv.org/pdf/2405.07412v1.pdf
2405.04624v1,2024-05-07T19:11:45Z,2024-05-07 19:11:45,Adaptive design of experiments methodology for noise resistance with unreplicated experiments,"A new gradient-based adaptive sampling method is proposed for design of experiments applications which balances space filling, local refinement, and error minimization objectives while reducing reliance on delicate tuning parameters. High order local maximum entropy approximants are used for metamodelling, which take advantage of boundary-corrected kernel density estimation to increase accuracy and robustness on highly clumped datasets, as well as conferring the resulting metamodel with some robustness against data noise in the common case of unreplicated experiments. Two-dimensional test cases are analyzed against full factorial and latin hypercube designs and compare favourably. The proposed method is then applied in a unique manner to the problem of adaptive spatial resolution in time-varying non-linear functions, opening up the possibility to adapt the method to solve partial differential equations.",Lucas Caparini|Gwynn J. Elfring|Mauricio Ponga,,https://arxiv.org/abs/2405.04624v1,https://arxiv.org/pdf/2405.04624v1,,"33 pages, 14 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2405.04624v1.pdf
2405.04592v1,2024-05-07T18:10:54Z,2024-05-07 18:10:54,Integrating knowledge-guided symbolic regression and model-based design of experiments to automate process flow diagram development,"New products must be formulated rapidly to succeed in the global formulated product market; however, key product indicators (KPIs) can be complex, poorly understood functions of the chemical composition and processing history. Consequently, scale-up must currently undergo expensive trial-and-error campaigns. To accelerate process flow diagram (PFD) optimisation and knowledge discovery, this work proposed a novel digital framework to automatically quantify process mechanisms by integrating symbolic regression (SR) within model-based design of experiments (MBDoE). Each iteration, SR proposed a Pareto front of interpretable mechanistic expressions, and then MBDoE designed a new experiment to discriminate between them while balancing PFD optimisation. To investigate the framework's performance, a new process model capable of simulating general formulated product synthesis was constructed to generate in-silico data for different case studies. The framework could effectively discover ground-truth process mechanisms within a few iterations, indicating its great potential for use within the general chemical industry for digital manufacturing and product innovation.",Alexander W. Rogers|Amanda Lane|Cesar Mendoza|Simon Watson|Adam Kowalski|Philip Martin|Dongda Zhang,,https://arxiv.org/abs/2405.04592v1,https://arxiv.org/pdf/2405.04592v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2405.04592v1.pdf
2405.03529v2,2024-05-06T14:47:19Z,2024-05-09 16:11:43,Quasi-Monte Carlo for Bayesian design of experiment problems governed by parametric PDEs,"This paper contributes to the study of optimal experimental design for Bayesian inverse problems governed by partial differential equations (PDEs). We derive estimates for the parametric regularity of multivariate double integration problems over high-dimensional parameter and data domains arising in Bayesian optimal design problems. We provide a detailed analysis for these double integration problems using two approaches: a full tensor product and a sparse tensor product combination of quasi-Monte Carlo (QMC) cubature rules over the parameter and data domains. Specifically, we show that the latter approach significantly improves the convergence rate, exhibiting performance comparable to that of QMC integration of a single high-dimensional integral. Furthermore, we numerically verify the predicted convergence rates for an elliptic PDE problem with an unknown diffusion coefficient in two spatial dimensions, offering empirical evidence supporting the theoretical results and highlighting practical applicability.",Vesa Kaarnioja|Claudia Schillings,,https://arxiv.org/abs/2405.03529v2,https://arxiv.org/pdf/2405.03529v2,,"43 pages, 3 figures",,,math.NA,math.NA,https://arxiv.org/pdf/2405.03529v2.pdf
2405.02449v1,2024-05-03T19:33:44Z,2024-05-03 19:33:44,Quality-Weighted Vendi Scores And Their Application To Diverse Experimental Design,"Experimental design techniques such as active search and Bayesian optimization are widely used in the natural sciences for data collection and discovery. However, existing techniques tend to favor exploitation over exploration of the search space, which causes them to get stuck in local optima. This ``collapse"" problem prevents experimental design algorithms from yielding diverse high-quality data. In this paper, we extend the Vendi scores -- a family of interpretable similarity-based diversity metrics -- to account for quality. We then leverage these quality-weighted Vendi scores to tackle experimental design problems across various applications, including drug discovery, materials discovery, and reinforcement learning. We found that quality-weighted Vendi scores allow us to construct policies for experimental design that flexibly balance quality and diversity, and ultimately assemble rich and diverse sets of high-performing data points. Our algorithms led to a 70%-170% increase in the number of effective discoveries compared to baselines.",Quan Nguyen|Adji Bousso Dieng,,https://arxiv.org/abs/2405.02449v1,https://arxiv.org/pdf/2405.02449v1,,"Published in International Conference on Machine Learning, ICML 2024. Code can be found in the Vertaix GitHub: https://github.com/vertaix/Quality-Weighted-Vendi-Score. Paper dedicated to Kwame Nkrumah",,,stat.ML,stat.ML|cond-mat.mtrl-sci|cs.LG|q-bio.BM,https://arxiv.org/pdf/2405.02449v1.pdf
2404.15797v1,2024-04-24T10:43:31Z,2024-04-24 10:43:31,Optimal Experimental Design for Large-Scale Inverse Problems via Multi-PDE-constrained Optimization,"Accurate parameter dependent electro-chemical numerical models for lithium-ion batteries are essential in industrial application. The exact parameters of each battery cell are unknown and a process of estimation is necessary to infer them. The parameter estimation generates an accurate model able to reproduce real cell data. The field of optimal input/experimental design deals with creating the experimental settings facilitating the estimation problem. Here we apply two different input design algorithms that aim at maximizing the observability of the true, unknown parameters: in the first algorithm, we design the applied current and the starting voltage. This lets the algorithm collect information on different states of charge, but requires long experimental times (60 000 s). In the second algorithm, we generate a continuous current, composed of concatenated optimal intervals. In this case, the experimental time is shorter (7000 s) and numerical experiments with virtual data give an even better accuracy results, but experiments with real battery data reveal that the accuracy could decrease hundredfold. As the design algorithms are built independent of the model, the same results and motivation are applicable to more complex battery cell models and, moreover, to other applications.",Andrea Petrocchi|Matthias K. Scharrer|Franz Pichler|Stefan Volkwein,,https://arxiv.org/abs/2404.15797v1,https://arxiv.org/pdf/2404.15797v1,,"29 pages, 8 figures",,,math.ST,math.ST|math.OC,https://arxiv.org/pdf/2404.15797v1.pdf
2404.08927v1,2024-04-13T08:30:36Z,2024-04-13 08:30:36,PDXpower: A Power Analysis Tool for Experimental Design in Pre-clinical Xenograft Studies for Uncensored and Censored Outcomes,"In cancer research, leveraging patient-derived xenografts (PDXs) in pre-clinical experiments is a crucial approach for assessing innovative therapeutic strategies. Addressing the inherent variability in treatment response among and within individual PDX lines is essential. However, the current literature lacks a user-friendly statistical power analysis tool capable of concurrently determining the required number of PDX lines and animals per line per treatment group in this context. In this paper, we present a simulation-based R package for sample size determination, named `\textbf{PDXpower}', which is publicly available at The Comprehensive R Archive Network \url{https://CRAN.R-project.org/package=PDXpower}. The package is designed to estimate the necessary number of both PDX lines and animals per line per treatment group for the design of a PDX experiment, whether for an uncensored outcome, or a censored time-to-event outcome. Our sample size considerations rely on two widely used analytical frameworks: the mixed effects ANOVA model for uncensored outcomes and Cox's frailty model for censored data outcomes, which effectively account for both inter-PDX variability and intra-PDX correlation in treatment response. Step-by-step illustrations for utilizing the developed package are provided, catering to scenarios with or without preliminary data.",Shanpeng Li|Donatello Telesca|Harley I. Kornblum|David Nathanson|Frank Pajonk|Elvis Han Cui|Joycelynne Palmer|Gang Li,,https://arxiv.org/abs/2404.08927v1,https://arxiv.org/pdf/2404.08927v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2404.08927v1.pdf
2404.08846v2,2024-04-12T23:27:46Z,2024-05-31 02:37:10,Experimental Design for Active Transductive Inference in Large Language Models,"One emergent ability of large language models (LLMs) is that query-specific examples can be included in the prompt at inference time. In this work, we use active learning for adaptive prompt design and call it Active In-context Prompt Design (AIPD). We design the LLM prompt by adaptively choosing few-shot examples from a training set to optimize performance on a test set. The training examples are initially unlabeled and we obtain the label of the most informative ones, which maximally reduces uncertainty in the LLM prediction. We propose two algorithms, GO and SAL, which differ in how the few-shot examples are chosen. We analyze these algorithms in linear models: first GO and then use its equivalence with SAL. We experiment with many different tasks in small, medium-sized, and large language models; and show that GO and SAL outperform other methods for choosing few-shot examples in the LLM prompt at inference time.",Subhojyoti Mukherjee|Anusha Lalitha|Aniket Deshmukh|Ge Liu|Yifei Ma|Branislav Kveton,,https://arxiv.org/abs/2404.08846v2,https://arxiv.org/pdf/2404.08846v2,,,,,cs.LG,cs.LG|cs.CL,https://arxiv.org/pdf/2404.08846v2.pdf
2404.13056v2,2024-04-08T14:44:21Z,2025-04-28 03:26:06,Variational Bayesian Optimal Experimental Design with Normalizing Flows,"Bayesian optimal experimental design (OED) seeks experiments that maximize the expected information gain (EIG) in model parameters. Directly estimating the EIG using nested Monte Carlo is computationally expensive and requires an explicit likelihood. Variational OED (vOED), in contrast, estimates a lower bound of the EIG without likelihood evaluations by approximating the posterior distributions with variational forms, and then tightens the bound by optimizing its variational parameters. We introduce the use of normalizing flows (NFs) for representing variational distributions in vOED; we call this approach vOED-NFs. Specifically, we adopt NFs with a conditional invertible neural network architecture built from compositions of coupling layers, and enhanced with a summary network for data dimension reduction. We present Monte Carlo estimators to the lower bound along with gradient expressions to enable a gradient-based simultaneous optimization of the variational parameters and the design variables. The vOED-NFs algorithm is then validated in two benchmark problems, and demonstrated on a partial differential equation-governed application of cathodic electrophoretic deposition and an implicit likelihood case with stochastic modeling of aphid population. The findings suggest that a composition of 4--5 coupling layers is able to achieve lower EIG estimation bias, under a fixed budget of forward model runs, compared to previous approaches. The resulting NFs produce approximate posteriors that agree well with the true posteriors, able to capture non-Gaussian and multi-modal features effectively.",Jiayuan Dong|Christian Jacobsen|Mehdi Khalloufi|Maryam Akram|Wanjiao Liu|Karthik Duraisamy|Xun Huan,,https://arxiv.org/abs/2404.13056v2,https://arxiv.org/pdf/2404.13056v2,https://doi.org/10.1016/j.cma.2024.117457,,Computer Methods in Applied Mechanics and Engineering 433 (2025) 117457,10.1016/j.cma.2024.117457,cs.LG,cs.LG|cs.CE|stat.CO|stat.ME|stat.ML,https://arxiv.org/pdf/2404.13056v2.pdf
2404.04036v1,2024-04-05T11:39:53Z,2024-04-05 11:39:53,"Which Experimental Design is Better Suited for VQA Tasks? Eye Tracking Study on Cognitive Load, Performance, and Gaze Allocations","We conducted an eye-tracking user study with 13 participants to investigate the influence of stimulus-question ordering and question modality on participants using visual question-answering (VQA) tasks. We examined cognitive load, task performance, and gaze allocations across five distinct experimental designs, aiming to identify setups that minimize the cognitive burden on participants. The collected performance and gaze data were analyzed using quantitative and qualitative methods. Our results indicate a significant impact of stimulus-question ordering on cognitive load and task performance, as well as a noteworthy effect of question modality on task performance. These findings offer insights for the experimental design of controlled user studies in visualization research.",Sita A. Vriend|Sandeep Vidyapu|Amer Rama|Kun-Ting Chen|Daniel Weiskopf,,https://arxiv.org/abs/2404.04036v1,https://arxiv.org/pdf/2404.04036v1,https://doi.org/10.1145/3649902.3653519,Accepted at ETVIS 2024,,10.1145/3649902.3653519,cs.HC,cs.HC,https://arxiv.org/pdf/2404.04036v1.pdf
2404.00075v1,2024-03-28T20:17:58Z,2024-03-28 20:17:58,BEACON: Bayesian Experimental design Acceleration with Conditional Normalizing flows $-$ a case study in optimal monitor well placement for CO$_2$ sequestration,"CO$_2$ sequestration is a crucial engineering solution for mitigating climate change. However, the uncertain nature of reservoir properties, necessitates rigorous monitoring of CO$_2$ plumes to prevent risks such as leakage, induced seismicity, or breaching licensed boundaries. To address this, project managers use borehole wells for direct CO$_2$ and pressure monitoring at specific locations. Given the high costs associated with drilling, it is crucial to strategically place a limited number of wells to ensure maximally effective monitoring within budgetary constraints. Our approach for selecting well locations integrates fluid-flow solvers for forecasting plume trajectories with generative neural networks for plume inference uncertainty. Our methodology is extensible to three-dimensional domains and is developed within a Bayesian framework for optimal experimental design, ensuring scalability and mathematical optimality. We use a realistic case study to verify these claims by demonstrating our method's application in a large scale domain and optimal performance as compared to baseline well placement.",Rafael Orozco|Abhinav Gahlot|Felix J. Herrmann,,https://arxiv.org/abs/2404.00075v1,https://arxiv.org/pdf/2404.00075v1,,,,,cs.LG,cs.LG|math-ph,https://arxiv.org/pdf/2404.00075v1.pdf
2403.18072v2,2024-03-26T19:49:58Z,2025-02-02 00:33:42,Goal-Oriented Bayesian Optimal Experimental Design for Nonlinear Models using Markov Chain Monte Carlo,"Optimal experimental design (OED) provides a systematic approach to quantify and maximize the value of experimental data. Under a Bayesian approach, conventional OED maximizes the expected information gain (EIG) on model parameters. However, we are often interested in not the parameters themselves, but predictive quantities of interest (QoIs) that depend on the parameters in a nonlinear manner. We present a computational framework of predictive goal-oriented OED (GO-OED) suitable for nonlinear observation and prediction models, which seeks the experimental design providing the greatest EIG on the QoIs. In particular, we propose a nested Monte Carlo estimator for the QoI EIG, featuring Markov chain Monte Carlo for posterior sampling and kernel density estimation for evaluating the posterior-predictive density and its Kullback-Leibler divergence from the prior-predictive. The GO-OED design is then found by maximizing the EIG over the design space using Bayesian optimization. We demonstrate the effectiveness of the overall nonlinear GO-OED method, and illustrate its differences versus conventional non-GO-OED, through various test problems and an application of sensor placement for source inversion in a convection-diffusion field.",Shijie Zhong|Wanggang Shen|Tommie Catanach|Xun Huan,,https://arxiv.org/abs/2403.18072v2,https://arxiv.org/pdf/2403.18072v2,,"28 pages, 19 figures",,,stat.CO,stat.CO|cs.LG|stat.ME|stat.ML,https://arxiv.org/pdf/2403.18072v2.pdf
2403.13893v2,2024-03-20T18:05:52Z,2024-09-28 14:48:59,DAVED: Data Acquisition via Experimental Design for Data Markets,"The acquisition of training data is crucial for machine learning applications. Data markets can increase the supply of data, particularly in data-scarce domains such as healthcare, by incentivizing potential data providers to join the market. A major challenge for a data buyer in such a market is choosing the most valuable data points from a data seller. Unlike prior work in data valuation, which assumes centralized data access, we propose a federated approach to the data acquisition problem that is inspired by linear experimental design. Our proposed data acquisition method achieves lower prediction error without requiring labeled validation data and can be optimized in a fast and federated procedure. The key insight of our work is that a method that directly estimates the benefit of acquiring data for test set prediction is particularly compatible with a decentralized market setting.",Charles Lu|Baihe Huang|Sai Praneeth Karimireddy|Praneeth Vepakomma|Michael Jordan|Ramesh Raskar,,https://arxiv.org/abs/2403.13893v2,https://arxiv.org/pdf/2403.13893v2,,"31 pages, 16 figures, To appear in NeurIPS 2024",,,cs.LG,cs.LG,https://arxiv.org/pdf/2403.13893v2.pdf
2403.10317v1,2024-03-15T14:07:46Z,2024-03-15 14:07:46,Application of machine learning to experimental design in quantum mechanics,"The recent advances in machine learning hold great promise for the fields of quantum sensing and metrology. With the help of reinforcement learning, we can tame the complexity of quantum systems and solve the problem of optimal experimental design. Reinforcement learning is a powerful model-free technique that allows an agent, typically a neural network, to learn the best strategy to reach a certain goal in a completely a priori unknown environment. However, in general, we know something about the quantum system with which the agent is interacting, at least that it follows the rules of quantum mechanics. In quantum metrology, we typically have a model for the system, and only some parameters of the evolution or the initial state are unknown. We present here a general machine learning technique that can optimize the precision of quantum sensors, exploiting the knowledge we have on the system through model-aware reinforcement learning. This framework has been implemented in the Python package qsensoropt, which is able to optimize a broad class of problems found in quantum metrology and quantum parameter estimation. The agent learns an optimal adaptive strategy that, based on previous outcomes, decides the next measurements to perform. We have explored some applications of this technique to NV centers and photonic circuits. So far, we have been able to certify better results than the current state-of-the-art controls for many cases. The machine learning technique developed here can be applied in all scenarios where the quantum system is well-characterized and relatively simple and small. In these cases, we can extract every last bit of information from a quantum sensor by appropriately controlling it with a trained neural network. The qsensoropt software is available on PyPI and can be installed with pip.",Federico Belliardo|Fabio Zoratti|Vittorio Giovannetti,,https://arxiv.org/abs/2403.10317v1,https://arxiv.org/pdf/2403.10317v1,https://doi.org/10.1142/S0219749924500023,"4 pages, 2 figures",International Journal of Quantum Information (2024),10.1142/S0219749924500023,quant-ph,quant-ph,https://arxiv.org/pdf/2403.10317v1.pdf
2403.09443v1,2024-03-14T14:37:25Z,2024-03-14 14:37:25,Sequential optimal experimental design for vapor-liquid equilibrium modeling,"We propose a general methodology of sequential locally optimal design of experiments for explicit or implicit nonlinear models, as they abound in chemical engineering and, in particular, in vapor-liquid equilibrium modeling. As a sequential design method, our method iteratively alternates between performing experiments, updating parameter estimates, and computing new experiments. Specifically, our sequential design method computes a whole batch of new experiments in each iteration and this batch of new experiments is designed in a two-stage locally optimal manner. In essence, this means that in every iteration the combined information content of the newly proposed experiments and of the already performed experiments is maximized. In order to solve these two-stage locally optimal design problems, a recent and efficient adaptive discretization algorithm is used. We demonstrate the benefits of the proposed methodology on the example of of the parameter estimation for the non-random two-liquid model for narrow azeotropic vapor-liquid equilibria. As it turns out, our sequential optimal design method requires substantially fewer experiments than traditional factorial design to achieve the same model precision and prediction quality. Consequently, our method can contribute to a substantially reduced experimental effort in vapor-liquid equilibrium modeling and beyond.",Martin Bubel|Jochen Schmid|Volodymyr Kozachynskyi|Erik Esche|Michael Bortz,,https://arxiv.org/abs/2403.09443v1,https://arxiv.org/pdf/2403.09443v1,,"30 pages, 8 figures",,,math.OC,math.OC,https://arxiv.org/pdf/2403.09443v1.pdf
2403.03589v2,2024-03-06T10:24:44Z,2024-06-18 18:20:08,Active Adaptive Experimental Design for Treatment Effect Estimation with Covariate Choices,"This study designs an adaptive experiment for efficiently estimating average treatment effects (ATEs). In each round of our adaptive experiment, an experimenter sequentially samples an experimental unit, assigns a treatment, and observes the corresponding outcome immediately. At the end of the experiment, the experimenter estimates an ATE using the gathered samples. The objective is to estimate the ATE with a smaller asymptotic variance. Existing studies have designed experiments that adaptively optimize the propensity score (treatment-assignment probability). As a generalization of such an approach, we propose optimizing the covariate density as well as the propensity score. First, we derive the efficient covariate density and propensity score that minimize the semiparametric efficiency bound and find that optimizing both covariate density and propensity score minimizes the semiparametric efficiency bound more effectively than optimizing only the propensity score. Next, we design an adaptive experiment using the efficient covariate density and propensity score sequentially estimated during the experiment. Lastly, we propose an ATE estimator whose asymptotic variance aligns with the minimized semiparametric efficiency bound.",Masahiro Kato|Akihiro Oga|Wataru Komatsubara|Ryo Inokuchi,,https://arxiv.org/abs/2403.03589v2,https://arxiv.org/pdf/2403.03589v2,,,,,stat.ME,stat.ME|cs.LG|econ.EM|stat.ML,https://arxiv.org/pdf/2403.03589v2.pdf
2402.18337v1,2024-02-28T13:59:20Z,2024-02-28 13:59:20,Probabilistic Bayesian optimal experimental design using conditional normalizing flows,"Bayesian optimal experimental design (OED) seeks to conduct the most informative experiment under budget constraints to update the prior knowledge of a system to its posterior from the experimental data in a Bayesian framework. Such problems are computationally challenging because of (1) expensive and repeated evaluation of some optimality criterion that typically involves a double integration with respect to both the system parameters and the experimental data, (2) suffering from the curse-of-dimensionality when the system parameters and design variables are high-dimensional, (3) the optimization is combinatorial and highly non-convex if the design variables are binary, often leading to non-robust designs. To make the solution of the Bayesian OED problem efficient, scalable, and robust for practical applications, we propose a novel joint optimization approach. This approach performs simultaneous (1) training of a scalable conditional normalizing flow (CNF) to efficiently maximize the expected information gain (EIG) of a jointly learned experimental design (2) optimization of a probabilistic formulation of the binary experimental design with a Bernoulli distribution. We demonstrate the performance of our proposed method for a practical MRI data acquisition problem, one of the most challenging Bayesian OED problems that has high-dimensional (320 $\times$ 320) parameters at high image resolution, high-dimensional (640 $\times$ 386) observations, and binary mask designs to select the most informative observations.",Rafael Orozco|Felix J. Herrmann|Peng Chen,,https://arxiv.org/abs/2402.18337v1,https://arxiv.org/pdf/2402.18337v1,,,,,cs.LG,cs.LG|cs.CV,https://arxiv.org/pdf/2402.18337v1.pdf
2402.16000v3,2024-02-25T05:57:31Z,2025-04-03 18:38:32,Bayesian D-Optimal Experimental Designs via Column Subset Selection,"This paper tackles optimal sensor placement for Bayesian linear inverse problems, a popular version of the more general Optimal Experimental Design (OED) problem, using the D-optimality criterion. This is done by establishing connections between sensor placement and Column Subset Selection Problem (CSSP), which is a well-studied problem in Numerical Linear Algebra (NLA). In particular, we use the Golub-Klema-Stewart (GKS) approach which involves computing the truncated Singular Value Decomposition (SVD) followed by a pivoted QR factorization on the right singular vectors. The algorithms are further accelerated by using randomization to compute the low-rank approximation as well as for sampling the indices. The resulting algorithms are robust, computationally efficient, amenable to parallelization, require virtually no parameter tuning, and come with strong theoretical guarantees. One of the proposed algorithms is also adjoint-free which is beneficial in situations, where the adjoint is expensive to evaluate or is not available. Additionally, we develop a method for data completion without solving the inverse problem. Numerical experiments on model inverse problems involving the heat equation and seismic tomography in two spatial dimensions demonstrate the performance of our approaches.",Srinivas Eswar|Vishwas Rao|Arvind K. Saibaba,,https://arxiv.org/abs/2402.16000v3,https://arxiv.org/pdf/2402.16000v3,,"31 pages, 9 figures",,,math.NA,math.NA,https://arxiv.org/pdf/2402.16000v3.pdf
2402.15053v1,2024-02-23T02:14:44Z,2024-02-23 02:14:44,Nonlinear Bayesian optimal experimental design using logarithmic Sobolev inequalities,"We study the problem of selecting $k$ experiments from a larger candidate pool, where the goal is to maximize mutual information (MI) between the selected subset and the underlying parameters. Finding the exact solution is to this combinatorial optimization problem is computationally costly, not only due to the complexity of the combinatorial search but also the difficulty of evaluating MI in nonlinear/non-Gaussian settings. We propose greedy approaches based on new computationally inexpensive lower bounds for MI, constructed via log-Sobolev inequalities. We demonstrate that our method outperforms random selection strategies, Gaussian approximations, and nested Monte Carlo (NMC) estimators of MI in various settings, including optimal design for nonlinear models with non-additive noise.",Fengyi Li|Ayoub Belhadji|Youssef Marzouk,,https://arxiv.org/abs/2402.15053v1,https://arxiv.org/pdf/2402.15053v1,,,,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2402.15053v1.pdf
2402.12077v2,2024-02-19T11:55:17Z,2024-12-04 16:00:45,Single and Multi-Objective Real-Time Optimisation of an Industrial Injection Moulding Process via a Bayesian Adaptive Design of Experiment Approach,"Minimising cycle time without inducing quality defects is a major challenge in the injection moulding (IM). Design of Experiment methods (DoE) have been widely studied for optimisation of the IM, however existing methods have limitations, including the need for a large number of experiments and a pre-determined search space. Bayesian adaptive design of experiment (ADoE) is an iterative process where the results of the previous experiments are used to make an informed selection for the next design. In this study, for the first time, an experimental ADoE approach, based on Bayesian optimisation, was developed in injection moulding using process and sensor data to optimise the quality and cycle time in real-time. A novel approach for the real-time characterisation of post-production shrinkage was introduced, utilising in-mould sensor data on temperature differential during part cooling. This characterisation approach was verified by post-production metrology results.
  A single and multi-objective optimisation of the cycle time and temperature differential in an injection moulded component is proposed. The multi-objective optimisation techniques, composite desirability function and Nondominated Sorting Genetic Algorithm (NSGA-II) using Response Surface Methodology (RSM) model, are compared with the real-time novel ADoE approach. ADoE achieved almost a 50% reduction in the number of experiments required for the single optimisation of temperature differential, and an almost 30% decrease for the optimisation of temperature differential and cycle time together compared to composite desirability function and NSGA-II. Also, the optimal settings identified by ADoE for multiobjective optimisation were similar to the selected Pareto optimal solution found by the NSGA-II.",Mandana Kariminejad|David Tormey|Caitríona Ryan|Christopher O'Hara|Albert Weinert|Marion McAfee,,https://arxiv.org/abs/2402.12077v2,https://arxiv.org/pdf/2402.12077v2,https://doi.org/10.1038/s41598-024-80405-2,"19 pages, 12 figures",,10.1038/s41598-024-80405-2,eess.SY,eess.SY,https://arxiv.org/pdf/2402.12077v2.pdf
2402.11862v1,2024-02-19T06:06:00Z,2024-02-19 06:06:00,"The Effects of Group Discussion and Role-playing Training on Self-efficacy, Support-seeking, and Reporting Phishing Emails: Evidence from a Mixed-design Experiment","Organizations rely on phishing interventions to enhance employees' vigilance and safe responses to phishing emails that bypass technical solutions. While various resources are available to counteract phishing, studies emphasize the need for interactive and practical training approaches. To investigate the effectiveness of such an approach, we developed and delivered two anti-phishing trainings, group discussion and role-playing, at a European university. We conducted a pre-registered experiment (N = 105), incorporating repeated measures at three time points, a control group, and three in-situ phishing tests. Both trainings enhanced employees' anti-phishing self-efficacy and support-seeking intention in within-group analyses. Only the role-playing training significantly improved support-seeking intention when compared to the control group. Participants in both trainings reported more phishing tests and demonstrated heightened vigilance to phishing attacks compared to the control group. We discuss practical implications for evaluating and improving phishing interventions and promoting safe responses to phishing threats within organizations.",Xiaowei Chen|Margault Sacré|Gabriele Lenzini|Samuel Greiff|Verena Distler|Anastasia Sergeeva,,https://arxiv.org/abs/2402.11862v1,https://arxiv.org/pdf/2402.11862v1,https://doi.org/10.1145/3613904.3641943,The paper is conditionally accepted in ACM CHI Conference 2024,In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI 2024),10.1145/3613904.3641943,cs.HC,cs.HC,https://arxiv.org/pdf/2402.11862v1.pdf
2402.11156v2,2024-02-17T00:51:29Z,2024-06-08 14:56:22,"Efficient Low-Rank Matrix Estimation, Experimental Design, and Arm-Set-Dependent Low-Rank Bandits","We study low-rank matrix trace regression and the related problem of low-rank matrix bandits. Assuming access to the distribution of the covariates, we propose a novel low-rank matrix estimation method called LowPopArt and provide its recovery guarantee that depends on a novel quantity denoted by B(Q) that characterizes the hardness of the problem, where Q is the covariance matrix of the measurement distribution. We show that our method can provide tighter recovery guarantees than classical nuclear norm penalized least squares (Koltchinskii et al., 2011) in several problems. To perform efficient estimation with a limited number of measurements from an arbitrarily given measurement set A, we also propose a novel experimental design criterion that minimizes B(Q) with computational efficiency. We leverage our novel estimator and design of experiments to derive two low-rank linear bandit algorithms for general arm sets that enjoy improved regret upper bounds. This improves over previous works on low-rank bandits, which make somewhat restrictive assumptions that the arm set is the unit ball or that an efficient exploration distribution is given. To our knowledge, our experimental design criterion is the first one tailored to low-rank matrix estimation beyond the naive reduction to linear regression, which can be of independent interest.",Kyoungseok Jang|Chicheng Zhang|Kwang-Sung Jun,,https://arxiv.org/abs/2402.11156v2,https://arxiv.org/pdf/2402.11156v2,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2402.11156v2.pdf
2402.09772v2,2024-02-15T07:52:12Z,2024-02-20 08:31:13,Optimal Experimental Design for Partially Observable Pure Birth Processes,"We develop an efficient algorithm to find optimal observation times by maximizing the Fisher information for the birth rate of a partially observable pure birth process involving $n$ observations. Partially observable implies that at each of the $n$ observation time points for counting the number of individuals present in the pure birth process, each individual is observed independently with a fixed probability $p$, modeling detection difficulties or constraints on resources. We apply concepts and techniques from generating functions, using a combination of symbolic and numeric computation, to establish a recursion for evaluating and optimizing the Fisher information. Our numerical results reveal the efficacy of this new method. An implementation of the algorithm is available publicly.",Ali Eshragh|Matthew P. Skerritt|Bruno Salvy|Thomas McCallum,,https://arxiv.org/abs/2402.09772v2,https://arxiv.org/pdf/2402.09772v2,,,,,math.ST,math.ST|math.NA,https://arxiv.org/pdf/2402.09772v2.pdf
2402.07868v4,2024-02-12T18:29:17Z,2024-05-29 12:15:40,Nesting Particle Filters for Experimental Design in Dynamical Systems,"In this paper, we propose a novel approach to Bayesian experimental design for non-exchangeable data that formulates it as risk-sensitive policy optimization. We develop the Inside-Out SMC$^2$ algorithm, a nested sequential Monte Carlo technique to infer optimal designs, and embed it into a particle Markov chain Monte Carlo framework to perform gradient-based policy amortization. Our approach is distinct from other amortized experimental design techniques, as it does not rely on contrastive estimators. Numerical validation on a set of dynamical systems showcases the efficacy of our method in comparison to other state-of-the-art strategies.",Sahel Iqbal|Adrien Corenflos|Simo Särkkä|Hany Abdulsamad,,https://arxiv.org/abs/2402.07868v4,https://arxiv.org/pdf/2402.07868v4,,Accepted to ICML 2024,,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2402.07868v4.pdf
2401.07971v3,2024-01-15T21:29:26Z,2024-08-17 13:39:21,Tractable Optimal Experimental Design using Transport Maps,"We present a flexible method for computing Bayesian optimal experimental designs (BOEDs) for inverse problems with intractable posteriors. The approach is applicable to a wide range of BOED problems and can accommodate various optimality criteria, prior distributions and noise models. The key to our approach is the construction of a transport-map-based surrogate to the joint probability law of the design, observational and inference random variables. This order-preserving transport map is constructed using tensor trains and can be used to efficiently sample from (and evaluate approximate densities of) conditional distributions that are required in the evaluation of many commonly-used optimality criteria. The algorithm is also extended to sequential data acquisition problems, where experiments can be performed in sequence to update the state of knowledge about the unknown parameters. The sequential BOED problem is made computationally feasible by preconditioning the approximation of the joint density at the current stage using transport maps constructed at previous stages. The flexibility of our approach in finding optimal designs is illustrated with some numerical examples inspired by disease modeling and the reconstruction of subsurface structures in aquifers.",Karina Koval|Roland Herzog|Robert Scheichl,,https://arxiv.org/abs/2401.07971v3,https://arxiv.org/pdf/2401.07971v3,,,,,stat.CO,stat.CO|math.NA|math.OC,https://arxiv.org/pdf/2401.07971v3.pdf
2401.06692v3,2024-01-12T16:56:54Z,2024-07-08 02:52:05,An Experimental Design Framework for Label-Efficient Supervised Finetuning of Large Language Models,"Supervised finetuning (SFT) on instruction datasets has played a crucial role in achieving the remarkable zero-shot generalization capabilities observed in modern large language models (LLMs). However, the annotation efforts required to produce high quality responses for instructions are becoming prohibitively expensive, especially as the number of tasks spanned by instruction datasets continues to increase. Active learning is effective in identifying useful subsets of samples to annotate from an unlabeled pool, but its high computational cost remains a barrier to its widespread applicability in the context of LLMs. To mitigate the annotation cost of SFT and circumvent the computational bottlenecks of active learning, we propose using experimental design. Experimental design techniques select the most informative samples to label, and typically maximize some notion of uncertainty and/or diversity. In our work, we implement a framework that evaluates several existing and novel experimental design techniques and find that these methods consistently yield significant gains in label efficiency with little computational overhead. On generative tasks, our methods achieve the same generalization performance with only $50\%$ of annotation cost required by random sampling.",Gantavya Bhatt|Yifang Chen|Arnav M. Das|Jifan Zhang|Sang T. Truong|Stephen Mussmann|Yinglun Zhu|Jeffrey Bilmes|Simon S. Du|Kevin Jamieson|Jordan T. Ash|Robert D. Nowak,,https://arxiv.org/abs/2401.06692v3,https://arxiv.org/pdf/2401.06692v3,,Accepted to Findings of the Association for Computational Linguistics: ACL 2024,,,cs.CL,cs.CL|cs.AI|cs.LG,https://arxiv.org/pdf/2401.06692v3.pdf
2401.04996v1,2024-01-10T08:28:36Z,2024-01-10 08:28:36,Distributed Experimental Design Networks,"As edge computing capabilities increase, model learning deployments in diverse edge environments have emerged. In experimental design networks, introduced recently, network routing and rate allocation are designed to aid the transfer of data from sensors to heterogeneous learners. We design efficient experimental design network algorithms that are (a) distributed and (b) use multicast transmissions. This setting poses significant challenges as classic decentralization approaches often operate on (strictly) concave objectives under differentiable constraints. In contrast, the problem we study here has a non-convex, continuous DR-submodular objective, while multicast transmissions naturally result in non-differentiable constraints. From a technical standpoint, we propose a distributed Frank-Wolfe and a distributed projected gradient ascent algorithm that, coupled with a relaxation of non-differentiable constraints, yield allocations within a $1-1/e$ factor from the optimal. Numerical evaluations show that our proposed algorithms outperform competitors with respect to model learning quality.",Yuanyuan Li|Lili Su|Carlee Joe-Wong|Edmund Yeh|Stratis Ioannidis,,https://arxiv.org/abs/2401.04996v1,https://arxiv.org/pdf/2401.04996v1,,Technical report for paper accepted by INFOCOM 2024,,,cs.NI,cs.NI,https://arxiv.org/pdf/2401.04996v1.pdf
2401.04864v2,2024-01-10T00:50:26Z,2024-09-16 14:20:46,Toward Microgravity Mass Gauging with Electrical Capacitance Volume Sensing: Sensor Design and Experiment,"The use of capacitance sensors for fuel mass gauging has been in consideration since the early days of manned space flight. However, certain difficulties arise when considering tanks in microgravity environments. Surface tension effects lead to fluid wetting of the interior surface of the tank, leaving large interior voids, while thrust/settling effects can lead to dispersed two-phase mixtures. With the exception of capacitance-based sensing, few sensing technologies are well suited for measuring annular, stratified, and dispersed fluid configurations. Two modalities of capacitance measurement are compared: Electrical Capacitance Volume Tomography (ECVT) and Electrical Capacitance Volume Sensing (ECVS). ECVT is a non-invasive imaging modality first introduced in 2006. ECVS is a measurement modality introduced in this paper that is derived from ECVT technology but does not reconstruct an image as part of the mass measurement. To optimize the design of future capacitance-based spherical tank mass gauging sensors, different electrode plate layouts are evaluated in a mass gauging context. Prototype sensors are constructed, and experiments are conducted with fluid fills in various orientations. The plate layouts and their effect on the performance of the sensor as a fuel mass gauge are analyzed through the use of imaging and averaging techniques.",M. A. Charleston|S. M. Chowdhury|Q. M. Marashdeh|B. J. Straiton|F. L. Teixeira,,https://arxiv.org/abs/2401.04864v2,https://arxiv.org/pdf/2401.04864v2,https://doi.org/10.1109/JSEN.2024.3455928,"13 pages, 23 figures, 5 tables. This is the peer-reviewed accepted manuscript. The final, published article appears in: IEEE Sensors Journal Print ISSN: 1530-437X Online ISSN: 1558-1748 Digital Object Identifier: 10.1109/JSEN.2024.3455928","IEEE Sensors Journal 2024, Print ISSN: 1530-437X Online ISSN: 1558-1748",10.1109/JSEN.2024.3455928,eess.SY,eess.SY|physics.ins-det,https://arxiv.org/pdf/2401.04864v2.pdf
2401.03756v4,2024-01-08T09:29:07Z,2025-06-19 14:27:47,Adaptive Experimental Design for Policy Learning,"This study investigates the contextual best arm identification (BAI) problem, aiming to design an adaptive experiment to identify the best treatment arm conditioned on contextual information (covariates). We consider a decision-maker who assigns treatment arms to experimental units during an experiment and recommends the estimated best treatment arm based on the contexts at the end of the experiment. The decision-maker uses a policy for recommendations, which is a function that provides the estimated best treatment arm given the contexts. In our evaluation, we focus on the worst-case expected regret, a relative measure between the expected outcomes of an optimal policy and our proposed policy. We derive a lower bound for the expected simple regret and then propose a strategy called Adaptive Sampling-Policy Learning (PLAS). We prove that this strategy is minimax rate-optimal in the sense that its leading factor in the regret upper bound matches the lower bound as the number of experimental units increases.",Masahiro Kato|Kyohei Okumura|Takuya Ishihara|Toru Kitagawa,,https://arxiv.org/abs/2401.03756v4,https://arxiv.org/pdf/2401.03756v4,,arXiv admin note: text overlap with arXiv:2302.02988,,,cs.LG,cs.LG|cs.AI|econ.EM|stat.ME|stat.ML,https://arxiv.org/pdf/2401.03756v4.pdf
2312.16985v3,2023-12-28T12:04:15Z,2024-12-03 16:13:04,Model-aware reinforcement learning for high-performance Bayesian experimental design in quantum metrology,"Quantum sensors offer control flexibility during estimation by allowing manipulation by the experimenter across various parameters. For each sensing platform, pinpointing the optimal controls to enhance the sensor's precision remains a challenging task. While an analytical solution might be out of reach, machine learning offers a promising avenue for many systems of interest, especially given the capabilities of contemporary hardware. We have introduced a versatile procedure capable of optimizing a wide range of problems in quantum metrology, estimation, and hypothesis testing by combining model-aware reinforcement learning (RL) with Bayesian estimation based on particle filtering. To achieve this, we had to address the challenge of incorporating the many non-differentiable steps of the estimation in the training process, such as measurements and the resampling of the particle filter. Model-aware RL is a gradient-based method, where the derivatives of the sensor's precision are obtained through automatic differentiation (AD) in the simulation of the experiment. Our approach is suitable for optimizing both non-adaptive and adaptive strategies, using neural networks or other agents. We provide an implementation of this technique in the form of a Python library called qsensoropt, alongside several pre-made applications for relevant physical platforms, namely NV centers, photonic circuits, and optical cavities. This library will be released soon on PyPI. Leveraging our method, we've achieved results for many examples that surpass the current state-of-the-art in experimental design. In addition to Bayesian estimation, leveraging model-aware RL, it is also possible to find optimal controls for the minimization of the Cramér-Rao bound, based on Fisher information.",Federico Belliardo|Fabio Zoratti|Florian Marquardt|Vittorio Giovannetti,,https://arxiv.org/abs/2312.16985v3,https://arxiv.org/pdf/2312.16985v3,https://doi.org/10.22331/q-2024-12-10-1555,"45 pages, 10 figures","Quantum 8, 1555 (2024)",10.22331/q-2024-12-10-1555,quant-ph,quant-ph,https://arxiv.org/pdf/2312.16985v3.pdf
2312.14810v4,2023-12-22T16:32:51Z,2024-09-09 17:46:38,"Accurate, scalable, and efficient Bayesian optimal experimental design with derivative-informed neural operators","We consider optimal experimental design (OED) problems in selecting the most informative observation sensors to estimate model parameters in a Bayesian framework. Such problems are computationally prohibitive when the parameter-to-observable (PtO) map is expensive to evaluate, the parameters are high-dimensional, and the optimization for sensor selection is combinatorial and high-dimensional. To address these challenges, we develop an accurate, scalable, and efficient computational framework based on derivative-informed neural operators (DINO). We propose to use derivative-informed dimension reduction to reduce the parameter dimensions, based on which we train DINO with derivative information as an accurate and efficient surrogate for the PtO map and its derivative. Moreover, we derive DINO-enabled efficient formulations in computing the maximum a posteriori (MAP) point, the eigenvalues of approximate posterior covariance, and three commonly used optimality criteria for the OED problems. Furthermore, we provide detailed error analysis for the approximations of the MAP point, the eigenvalues, and the optimality criteria. We also propose a modified swapping greedy algorithm for the sensor selection optimization and demonstrate that the proposed computational framework is scalable to preserve the accuracy for increasing parameter dimensions and achieves high computational efficiency, with an over 1000$\times$ speedup accounting for both offline construction and online evaluation costs, compared to high-fidelity Bayesian OED solutions for a three-dimensional nonlinear convection-diffusion-reaction example with tens of thousands of parameters.",Jinwoo Go|Peng Chen,,https://arxiv.org/abs/2312.14810v4,https://arxiv.org/pdf/2312.14810v4,,,,,cs.CE,cs.CE|math.OC|stat.ME,https://arxiv.org/pdf/2312.14810v4.pdf
2312.13953v1,2023-12-21T15:40:39Z,2023-12-21 15:40:39,Subtle Sound Design: Designing for experience blend in a historic house museum,"In this article, we present and discuss a user-study prototype, developed for Bakkehuset historic house museum in Copenhagen. We examine how the prototype - a digital sound installation - can expand visitors' experiences of the house and offer encounters with immaterial cultural heritage. Historic house museums often hold back on utilizing digital communication tools inside the houses, since a central purpose of this type of museum is to preserve an original environment. Digital communication tools however hold great potential for facilitating rich encounters with cultural heritage and in particular with the immaterial aspects of museum collections and their histories. In this article we present our design steps and choices, aiming at subtly and seamlessly adding a digital dimension to a historic house. Based on qualitative interviews, we evaluate how the sound installation at Bakkehuset is sensed, interpreted, and used by visitors as part of their museum experience. In turn, we shed light on the historic house museum as a distinct design context for designing hybrid visitor experiences and point to the potentials of digital communication tools in this context.",Mia F. Yates|Anders Sundnes Løvlie,,https://arxiv.org/abs/2312.13953v1,https://arxiv.org/pdf/2312.13953v1,https://doi.org/10.1145/3633476,,Mia F. Yates and Anders S. Løvlie. 2023. Subtle Sound Design: Designing for Experience Blend in a Historic House Museum. J. Comput. Cult. Herit. (November 2023),10.1145/3633476,cs.HC,cs.HC,https://arxiv.org/pdf/2312.13953v1.pdf
2312.12645v1,2023-12-19T22:41:43Z,2023-12-19 22:41:43,Revisiting the effect of greediness on the efficacy of exchange algorithms for generating exact optimal experimental designs,"Coordinate exchange (CEXCH) is a popular algorithm for generating exact optimal experimental designs. The authors of CEXCH advocated for a highly greedy implementation - one that exchanges and optimizes single element coordinates of the design matrix. We revisit the effect of greediness on CEXCHs efficacy for generating highly efficient designs. We implement the single-element CEXCH (most greedy), a design-row (medium greedy) optimization exchange, and particle swarm optimization (PSO; least greedy) on 21 exact response surface design scenarios, under the $D$- and $I-$criterion, which have well-known optimal designs that have been reproduced by several researchers. We found essentially no difference in performance of the most greedy CEXCH and the medium greedy CEXCH. PSO did exhibit better efficacy for generating $D$-optimal designs, and for most $I$-optimal designs than CEXCH, but not to a strong degree under our parametrization. This work suggests that further investigation of the greediness dimension and its effect on CEXCH efficacy on a wider suite of models and criterion is warranted.",William T. Gullion|Stephen J. Walsh,,https://arxiv.org/abs/2312.12645v1,https://arxiv.org/pdf/2312.12645v1,,,,,stat.ME,stat.ME|stat.CO|stat.OT,https://arxiv.org/pdf/2312.12645v1.pdf
2312.10383v1,2023-12-16T08:34:29Z,2023-12-16 08:34:29,Bayesian experimental design for head imaging by electrical impedance tomography,"This work considers the optimization of electrode positions in head imaging by electrical impedance tomography. The study is motivated by maximizing the sensitivity of electrode measurements to conductivity changes when monitoring the condition of a stroke patient, which justifies adopting a linearized version of the complete electrode model as the forward model. The algorithm is based on finding a (locally) A-optimal measurement configuration via gradient descent with respect to the electrode positions. The efficient computation of the needed derivatives of the complete electrode model is one of the focal points. Two algorithms are introduced and numerically tested on a three-layer head model. The first one assumes a region of interest and a Gaussian prior for the conductivity in the brain, and it can be run offline, i.e., prior to taking any measurements. The second algorithm first computes a reconstruction of the conductivity anomaly caused by the stroke with an initial electrode configuration by combining lagged diffusivity iteration with sequential linearizations, which can be interpreted to produce an approximate Gaussian probability density for the conductivity perturbation. It then resorts to the first algorithm to find new, more informative positions for the available electrodes with the constructed density as the prior.",N. Hyvönen|A. Jääskeläinen|R. Maity|A. Vavilov,,https://arxiv.org/abs/2312.10383v1,https://arxiv.org/pdf/2312.10383v1,,"24 pages, 10 figures",,,math.NA,math.NA|math.ST,https://arxiv.org/pdf/2312.10383v1.pdf
2312.10233v2,2023-12-15T22:02:54Z,2024-04-13 05:54:57,Identifiability and Characterization of Transmon Qutrits Through Bayesian Experimental Design,"Robust control of a quantum system is essential to utilize the current noisy quantum hardware to their full potential, such as quantum algorithms. To achieve such a goal, systematic search for an optimal control for any given experiment is essential. Design of optimal control pulses require accurate numerical models, and therefore, accurate characterization of the system parameters. We present an online, Bayesian approach for quantum characterization of qutrit systems which automatically and systematically identifies the optimal experiments that provide maximum information on the system parameters, thereby greatly reducing the number of experiments that need to be performed on the quantum testbed. Unlike most characterization protocols that provide point-estimates of the parameters, the proposed approach is able to estimate their probability distribution. The applicability of the Bayesian experimental design technique was demonstrated on test problems where each experiment was defined by a parameterized control pulse. In addition to this, we also presented an approach for iterative pulse extension which is robust under uncertainties in transition frequencies and coherence times, and shot noise, despite being initialized with wide uninformative priors. Furthermore, we provide a mathematical proof of the theoretical identifiability of the model parameters and present conditions on the quantum state under which the parameters are identifiable. The proof and conditions for identifiability are presented for both closed and open quantum systems using the Schroedinger equation and the Lindblad master equation respectively.",Sohail Reddy,,https://arxiv.org/abs/2312.10233v2,https://arxiv.org/pdf/2312.10233v2,,,,,quant-ph,quant-ph|math-ph,https://arxiv.org/pdf/2312.10233v2.pdf
2312.06405v1,2023-12-11T14:25:04Z,2023-12-11 14:25:04,Optimizing Resonator Frequency Stability in Flip-Chip Architectures: A Novel Experimental Design Approach,"In multi-qubit superconducting systems utilizing flip-chip technology, achieving high accuracy in resonator frequencies is of paramount importance, particularly when multiple resonators share a common Purcell filter with restricted bandwidth. Nevertheless, variations in inter-chip spacing can considerably influence these frequencies. To tackle this issue, we present and experimentally validate the effectiveness of a resonator design. In our design, we etch portions of the metal on the bottom chip that faces the resonator structure on the top chip. This enhanced design substantially improves frequency stability by a factor of over 3.5 compared to the non-optimized design, as evaluated by the root mean square error of a linear fitting of the observed frequency distribution, which is intended to be linear. This advancement is crucial for successful scale-up and achievement of high-fidelity quantum operations.",Yuan Li|Tianhui Wang|Jingjing Hu|Dengfeng Li|Shuoming An,,https://arxiv.org/abs/2312.06405v1,https://arxiv.org/pdf/2312.06405v1,,,,,quant-ph,quant-ph,https://arxiv.org/pdf/2312.06405v1.pdf
2312.04026v1,2023-12-07T03:46:41Z,2023-12-07 03:46:41,Independent-Set Design of Experiments for Estimating Treatment and Spillover Effects under Network Interference,"Interference is ubiquitous when conducting causal experiments over networks. Except for certain network structures, causal inference on the network in the presence of interference is difficult due to the entanglement between the treatment assignments and the interference levels. In this article, we conduct causal inference under interference on an observed, sparse but connected network, and we propose a novel design of experiments based on an independent set. Compared to conventional designs, the independent-set design focuses on an independent subset of data and controls their interference exposures through the assignments to the rest (auxiliary set). We provide a lower bound on the size of the independent set from a greedy algorithm , and justify the theoretical performance of estimators under the proposed design. Our approach is capable of estimating both spillover effects and treatment effects. We justify its superiority over conventional methods and illustrate the empirical performance through simulations.",Chencheng Cai|Xu Zhang|Edoardo M. Airoldi,,https://arxiv.org/abs/2312.04026v1,https://arxiv.org/pdf/2312.04026v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2312.04026v1.pdf
2312.02852v1,2023-12-05T16:09:31Z,2023-12-05 16:09:31,Expert-guided Bayesian Optimisation for Human-in-the-loop Experimental Design of Known Systems,"Domain experts often possess valuable physical insights that are overlooked in fully automated decision-making processes such as Bayesian optimisation. In this article we apply high-throughput (batch) Bayesian optimisation alongside anthropological decision theory to enable domain experts to influence the selection of optimal experiments. Our methodology exploits the hypothesis that humans are better at making discrete choices than continuous ones and enables experts to influence critical early decisions. At each iteration we solve an augmented multi-objective optimisation problem across a number of alternate solutions, maximising both the sum of their utility function values and the determinant of their covariance matrix, equivalent to their total variability. By taking the solution at the knee point of the Pareto front, we return a set of alternate solutions at each iteration that have both high utility values and are reasonably distinct, from which the expert selects one for evaluation. We demonstrate that even in the case of an uninformed practitioner, our algorithm recovers the regret of standard Bayesian optimisation.",Tom Savage|Ehecatl Antonio del Rio Chanona,,https://arxiv.org/abs/2312.02852v1,https://arxiv.org/pdf/2312.02852v1,,NeurIPS 2023 Workshop on Adaptive Experimental Design and Active Learning in the Real World. Main text: 6 pages,,,cs.LG,cs.LG|cs.HC|math.OC,https://arxiv.org/pdf/2312.02852v1.pdf
2311.09705v1,2023-11-16T09:32:51Z,2023-11-16 09:32:51,"edibble: An R package to encapsulate elements of experimental designs for better planning, management and workflow","I present an R package called edibble that facilitates the design of experiments by encapsulating elements of the experiment in a series of composable functions. This package is an interpretation of ""the grammar of experimental designs"" by Tanaka (2023) in the R programming language. The main features of the edibble package are demonstrated, illustrating how it can be used to create a wide array of experimental designs. The implemented system aims to encourage cognitive thinking for holistic planning and data management of experiments in a streamlined workflow. This workflow can increase the inherent value of experimental data by reducing potential errors or noise with careful preplanning, as well as, ensuring fit-for-purpose analysis of experimental data.",Emi Tanaka,,https://arxiv.org/abs/2311.09705v1,https://arxiv.org/pdf/2311.09705v1,,32 pages,,,stat.CO,stat.CO|stat.OT,https://arxiv.org/pdf/2311.09705v1.pdf
2311.05794v4,2023-11-09T23:57:32Z,2024-10-15 15:25:03,An Experimental Design for Anytime-Valid Causal Inference on Multi-Armed Bandits,"Experimentation is crucial for managers to rigorously quantify the value of a change and determine if it leads to a statistically significant improvement over the status quo. As companies increasingly mandate that all changes undergo experimentation before widespread release, two challenges arise: (1) minimizing the proportion of customers assigned to the inferior treatment and (2) increasing experimentation velocity by enabling data-dependent stopping. This paper addresses both challenges by introducing the Mixture Adaptive Design (MAD), a new experimental design for multi-armed bandit (MAB) algorithms that enables anytime-valid inference on the Average Treatment Effect (ATE) for \emph{any} MAB algorithm. Intuitively, MAD ""mixes"" any bandit algorithm with a Bernoulli design, where at each time step, the probability of assigning a unit via the Bernoulli design is determined by a user-specified deterministic sequence that can converge to zero. This sequence lets managers directly control the trade-off between regret minimization and inferential precision. Under mild conditions on the rate the sequence converges to zero, we provide a confidence sequence that is asymptotically anytime-valid and guaranteed to shrink around the true ATE. Hence, when the true ATE converges to a non-zero value, the MAD confidence sequence is guaranteed to exclude zero in finite time. Therefore, the MAD enables managers to stop experiments early while ensuring valid inference, enhancing both the efficiency and reliability of adaptive experiments. Empirically, we demonstrate that the MAD achieves finite-sample anytime-validity while accurately and precisely estimating the ATE, all without incurring significant losses in reward compared to standard bandit designs.",Biyonka Liang|Iavor Bojinov,,https://arxiv.org/abs/2311.05794v4,https://arxiv.org/pdf/2311.05794v4,,,,,stat.ME,stat.ME|cs.LG,https://arxiv.org/pdf/2311.05794v4.pdf
2311.04308v1,2023-11-07T19:26:16Z,2023-11-07 19:26:16,Application of Response Surface Method and Genetic Algorithm in the Design of High-Efficiency Prototype Vehicle,"Breakthroughs in aerodynamic optimization have made it possible to develop efficient modes of transport with lesser exploitation of valuable resources. This makes it crucial for technical professionals such as engineers and scientists to understand the methodologies behind carrying out such optimizations. A common approach towards improving the aerodynamic properties of a vehicle is to alter its physical shape, which has concurrently been a very strenuous process given the time consumed to remodel the vehicle for each simulation process. This research aims to tackle this problem by using intelligent techniques to automate the step-by-step process of remodeling the car and arriving at a final optimized solution with a significantly lower drag coefficient, a quantity used to measure the amount of drag force acting on a vehicle. This is achieved by assigning particular parameters to ensure guided improvement of the airfoil in a process known as parametrization, followed by implementing a response surface methodology primarily to circumvent the strenuous task of performing a large number of CFD simulations by employing surrogate models to generate a response surface between selected independent variables. Further, evolutionary algorithms such as Genetic Algorithm have gained momentum in the optimization studies carried out during product design by selecting the optimum parameters from the available design spaces on the basis of natural evolution. The proposed method of optimization has been successfully implemented on a prototype vehicle with an improvement of 26.6% and 51.1% in the drag coefficient and drag area respectively.",Paras Singh|Harshit Gupta|Ojas Vinayak|Aryan Tyagi,,https://arxiv.org/abs/2311.04308v1,https://arxiv.org/pdf/2311.04308v1,,"Accepted at the 14th Asian Computational Fluid Dynamics Conference (ACFD 2023): 16 pages, 9 figures",,,physics.flu-dyn,physics.flu-dyn,https://arxiv.org/pdf/2311.04308v1.pdf
2311.01195v1,2023-11-02T12:46:03Z,2023-11-02 12:46:03,Batch Bayesian Optimization for Replicable Experimental Design,"Many real-world experimental design problems (a) evaluate multiple experimental conditions in parallel and (b) replicate each condition multiple times due to large and heteroscedastic observation noise. Given a fixed total budget, this naturally induces a trade-off between evaluating more unique conditions while replicating each of them fewer times vs. evaluating fewer unique conditions and replicating each more times. Moreover, in these problems, practitioners may be risk-averse and hence prefer an input with both good average performance and small variability. To tackle both challenges, we propose the Batch Thompson Sampling for Replicable Experimental Design (BTS-RED) framework, which encompasses three algorithms. Our BTS-RED-Known and BTS-RED-Unknown algorithms, for, respectively, known and unknown noise variance, choose the number of replications adaptively rather than deterministically such that an input with a larger noise variance is replicated more times. As a result, despite the noise heteroscedasticity, both algorithms enjoy a theoretical guarantee and are asymptotically no-regret. Our Mean-Var-BTS-RED algorithm aims at risk-averse optimization and is also asymptotically no-regret. We also show the effectiveness of our algorithms in two practical real-world applications: precision agriculture and AutoML.",Zhongxiang Dai|Quoc Phong Nguyen|Sebastian Shenghong Tay|Daisuke Urano|Richalynn Leong|Bryan Kian Hsiang Low|Patrick Jaillet,,https://arxiv.org/abs/2311.01195v1,https://arxiv.org/pdf/2311.01195v1,,Accepted to NeurIPS 2023,,,cs.LG,cs.LG|cs.AI,https://arxiv.org/pdf/2311.01195v1.pdf
2310.19961v1,2023-10-30T19:25:43Z,2023-10-30 19:25:43,ExPT: Synthetic Pretraining for Few-Shot Experimental Design,"Experimental design is a fundamental problem in many science and engineering fields. In this problem, sample efficiency is crucial due to the time, money, and safety costs of real-world design evaluations. Existing approaches either rely on active data collection or access to large, labeled datasets of past experiments, making them impractical in many real-world scenarios. In this work, we address the more challenging yet realistic setting of few-shot experimental design, where only a few labeled data points of input designs and their corresponding values are available. We approach this problem as a conditional generation task, where a model conditions on a few labeled examples and the desired output to generate an optimal input design. To this end, we introduce Experiment Pretrained Transformers (ExPT), a foundation model for few-shot experimental design that employs a novel combination of synthetic pretraining with in-context learning. In ExPT, we only assume knowledge of a finite collection of unlabelled data points from the input domain and pretrain a transformer neural network to optimize diverse synthetic functions defined over this domain. Unsupervised pretraining allows ExPT to adapt to any design task at test time in an in-context fashion by conditioning on a few labeled data points from the target task and generating the candidate optima. We evaluate ExPT on few-shot experimental design in challenging domains and demonstrate its superior generality and performance compared to existing methods. The source code is available at https://github.com/tung-nd/ExPT.git.",Tung Nguyen|Sudhanshu Agrawal|Aditya Grover,,https://arxiv.org/abs/2310.19961v1,https://arxiv.org/pdf/2310.19961v1,,2023 Conference on Neural Information Processing Systems (NeurIPS),,,cs.LG,cs.LG|cs.AI,https://arxiv.org/pdf/2310.19961v1.pdf
2310.18500v1,2023-10-27T21:29:48Z,2023-10-27 21:29:48,Designing Randomized Experiments to Predict Unit-Specific Treatment Effects,"Typically, a randomized experiment is designed to test a hypothesis about the average treatment effect and sometimes hypotheses about treatment effect variation. The results of such a study may then be used to inform policy and practice for units not in the study. In this paper, we argue that given this use, randomized experiments should instead be designed to predict unit-specific treatment effects in a well-defined population. We then consider how different sampling processes and models affect the bias, variance, and mean squared prediction error of these predictions. The results indicate, for example, that problems of generalizability (differences between samples and populations) can greatly affect bias both in predictive models and in measures of error in these models. We also examine when the average treatment effect estimate outperforms unit-specific treatment effect predictive models and implications of this for planning studies.",Elizabeth Tipton|Michalis Mamakos,,https://arxiv.org/abs/2310.18500v1,https://arxiv.org/pdf/2310.18500v1,,"46 pages, 3 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2310.18500v1.pdf
2310.17440v2,2023-10-26T14:50:07Z,2025-01-02 13:32:04,Gibbs optimal design of experiments,"Bayesian optimal design is a well-established approach to planning experiments. A distribution for the responses, i.e. a statistical model, is assumed which is dependent on unknown parameters. A utility function is then specified giving gain in information in estimating the true values of the parameters, using the Bayesian posterior distribution. A Bayesian optimal design is given by maximising expectation of the utility with respect to the distribution implied by statistical model and prior distribution for the true parameter values. The approach accounts for the experimental aim, via specification of the utility, and of assumed sources of uncertainty. However, it is predicated on the statistical model being correct. Recently, a new type of statistical inference, known as Gibbs inference, has been proposed. This is Bayesian-like, i.e. uncertainty for unknown quantities is represented by a posterior distribution, but does not necessarily require specification of a statistical model. The resulting inference is less sensitive to misspecification of the statistical model. This paper introduces Gibbs optimal design: a framework for optimal design of experiments under Gibbs inference. A computational approach to find designs in practice is outlined and the framework is demonstrated on exemplars including linear models, and experiments with count and time-to-event responses.",Antony M. Overstall|Jacinta Holloway-Brown|James M. McGree,,https://arxiv.org/abs/2310.17440v2,https://arxiv.org/pdf/2310.17440v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2310.17440v2.pdf
2310.14850v1,2023-10-23T12:15:34Z,2023-10-23 12:15:34,Fast computation and characterization of forced response surfaces via spectral submanifolds and parameter continuation,"For mechanical systems subject to periodic excitation, forced response curves (FRCs) depict the relationship between the amplitude of the periodic response and the forcing frequency. For nonlinear systems, this functional relationship is different for different forcing amplitudes. Forced response surfaces (FRSs), which relate the response amplitude to both forcing frequency and forcing amplitude, are then required in such settings. Yet, FRSs have been rarely computed in the literature due to the higher numerical effort they require. Here, we use spectral submanifolds (SSMs) to construct reduced-order models (ROMs) for high-dimensional mechanical systems and then use multidimensional manifold continuation of fixed points of the SSM-based ROMs to efficiently extract the FRSs. Ridges and trenches in an FRS characterize the main features of the forced response. We show how to extract these ridges and trenches directly without computing the FRS via reduced optimization problems on the ROMs. We demonstrate the effectiveness and efficiency of the proposed approach by calculating the FRSs and their ridges and trenches for a plate with a 1:1 internal resonance and for a shallow shell with a 1:2 internal resonance.",Mingwu Li|Shobhit Jain|George Haller,,https://arxiv.org/abs/2310.14850v1,https://arxiv.org/pdf/2310.14850v1,https://doi.org/10.1007/s11071-024-09482-2,,,10.1007/s11071-024-09482-2,math.DS,math.DS,https://arxiv.org/pdf/2310.14850v1.pdf
2310.13224v1,2023-10-20T02:02:51Z,2023-10-20 02:02:51,Adaptive Experimental Design for Intrusion Data Collection,"Intrusion research frequently collects data on attack techniques currently employed and their potential symptoms. This includes deploying honeypots, logging events from existing devices, employing a red team for a sample attack campaign, or simulating system activity. However, these observational studies do not clearly discern the cause-and-effect relationships between the design of the environment and the data recorded. Neglecting such relationships increases the chance of drawing biased conclusions due to unconsidered factors, such as spurious correlations between features and errors in measurement or classification. In this paper, we present the theory and empirical data on methods that aim to discover such causal relationships efficiently. Our adaptive design (AD) is inspired by the clinical trial community: a variant of a randomized control trial (RCT) to measure how a particular ``treatment'' affects a population. To contrast our method with observational studies and RCT, we run the first controlled and adaptive honeypot deployment study, identifying the causal relationship between an ssh vulnerability and the rate of server exploitation. We demonstrate that our AD method decreases the total time needed to run the deployment by at least 33%, while still confidently stating the impact of our change in the environment. Compared to an analogous honeypot study with a control group, our AD requests 17% fewer honeypots while collecting 19% more attack recordings than an analogous honeypot study with a control group.",Kate Highnam|Zach Hanif|Ellie Van Vogt|Sonali Parbhoo|Sergio Maffeis|Nicholas R. Jennings,,https://arxiv.org/abs/2310.13224v1,https://arxiv.org/pdf/2310.13224v1,,CAMLIS'23 Pre-publication - TO BE UPDATED!!,,,cs.CR,cs.CR,https://arxiv.org/pdf/2310.13224v1.pdf
2310.10783v3,2023-10-16T19:36:20Z,2024-11-24 20:30:06,Laplace-based strategies for Bayesian optimal experimental design with nuisance uncertainty,"Finding the optimal design of experiments in the Bayesian setting typically requires estimation and optimization of the expected information gain functional. This functional consists of one outer and one inner integral, separated by the logarithm function applied to the inner integral. When the mathematical model of the experiment contains uncertainty about the parameters of interest and nuisance uncertainty, (i.e., uncertainty about parameters that affect the model but are not themselves of interest to the experimenter), two inner integrals must be estimated. Thus, the already considerable computational effort required to determine good approximations of the expected information gain is increased further. The Laplace approximation has been applied successfully in the context of experimental design in various ways, and we propose two novel estimators featuring the Laplace approximation to alleviate the computational burden of both inner integrals considerably. The first estimator applies Laplace's method followed by a Laplace approximation, introducing a bias. The second estimator uses two Laplace approximations as importance sampling measures for Monte Carlo approximations of the inner integrals. Both estimators use Monte Carlo approximation for the remaining outer integral estimation. We provide four numerical examples demonstrating the applicability and effectiveness of our proposed estimators.",Arved Bartuska|Luis Espath|Raúl Tempone,,https://arxiv.org/abs/2310.10783v3,https://arxiv.org/pdf/2310.10783v3,,"20 pages, 10 figures",,,math.NA,math.NA,https://arxiv.org/pdf/2310.10783v3.pdf
2310.13713v1,2023-10-13T13:21:22Z,2023-10-13 13:21:22,Subjective visualization experiences: impact of visual design and experimental design,"In contrast to objectively measurable aspects (such as accuracy, reading speed, or memorability), the subjective experience of visualizations has only recently gained importance, and we have less experience how to measure it. We explore how subjective experience is affected by chart design using multiple experimental methods. We measure the effects of changes in color, orientation, and source annotation on the perceived readability and trustworthiness of simple bar charts. Three different experimental designs (single image rating, forced choice comparison, and semi-structured interviews) provide similar but different results. We find that these subjective experiences are different from what prior work on objective dimensions would predict. Seemingly inconsequential choices, like orientation, have large effects for some methods, indicating that study design alters decision-making strategies. Next to insights into the effect of chart design, we provide methodological insights, such as a suggested need to carefully isolate individual elements in charts to study subjective experiences.",Laura Koesten|Drew Dimmery|Michael Gleicher|Torsten Möller,,https://arxiv.org/abs/2310.13713v1,https://arxiv.org/pdf/2310.13713v1,,"19 pages, 5 figures, 2 tables",,,cs.HC,cs.HC,https://arxiv.org/pdf/2310.13713v1.pdf
2310.07315v1,2023-10-11T09:02:03Z,2023-10-11 09:02:03,Consistency of some sequential experimental design strategies for excursion set estimation based on vector-valued Gaussian processes,"We tackle the extension to the vector-valued case of consistency results for Stepwise Uncertainty Reduction sequential experimental design strategies established in [Bect et al., A supermartingale approach to Gaussian process based sequential design of experiments, Bernoulli 25, 2019]. This lead us in the first place to clarify, assuming a compact index set, how the connection between continuous Gaussian processes and Gaussian measures on the Banach space of continuous functions carries over to vector-valued settings. From there, a number of concepts and properties from the aforementioned paper can be readily extended. However, vector-valued settings do complicate things for some results, mainly due to the lack of continuity for the pseudo-inverse mapping that affects the conditional mean and covariance function given finitely many pointwise observations. We apply obtained results to the Integrated Bernoulli Variance and the Expected Measure Variance uncertainty functionals employed in [Fossum et al., Learning excursion sets of vector-valued Gaussian random fields for autonomous ocean sampling, The Annals of Applied Statistics 15, 2021] for the estimation for excursion sets of vector-valued functions.",Philip Stange|David Ginsbourger,,https://arxiv.org/abs/2310.07315v1,https://arxiv.org/pdf/2310.07315v1,,,,,math.ST,math.ST|math.PR|stat.ML,https://arxiv.org/pdf/2310.07315v1.pdf
2310.04390v1,2023-10-06T17:30:12Z,2023-10-06 17:30:12,Experimental Designs for Heteroskedastic Variance,"Most linear experimental design problems assume homogeneous variance although heteroskedastic noise is present in many realistic settings. Let a learner have access to a finite set of measurement vectors $\mathcal{X}\subset \mathbb{R}^d$ that can be probed to receive noisy linear responses of the form $y=x^{\top}θ^{\ast}+η$. Here $θ^{\ast}\in \mathbb{R}^d$ is an unknown parameter vector, and $η$ is independent mean-zero $σ_x^2$-sub-Gaussian noise defined by a flexible heteroskedastic variance model, $σ_x^2 = x^{\top}Σ^{\ast}x$. Assuming that $Σ^{\ast}\in \mathbb{R}^{d\times d}$ is an unknown matrix, we propose, analyze and empirically evaluate a novel design for uniformly bounding estimation error of the variance parameters, $σ_x^2$. We demonstrate the benefits of this method with two adaptive experimental design problems under heteroskedastic noise, fixed confidence transductive best-arm identification and level-set identification and prove the first instance-dependent lower bounds in these settings. Lastly, we construct near-optimal algorithms and demonstrate the large improvements in sample complexity gained from accounting for heteroskedastic variance in these designs empirically.",Justin Weltz|Tanner Fiez|Alexander Volfovsky|Eric Laber|Blake Mason|Houssam Nassif|Lalit Jain,,https://arxiv.org/abs/2310.04390v1,https://arxiv.org/pdf/2310.04390v1,,,"Conference on Neural Information Processing Systems (NeurIPS'23), New Orleans, pp. 65967-66005, 2023",,math.ST,math.ST,https://arxiv.org/pdf/2310.04390v1.pdf
2309.17241v1,2023-09-29T13:51:03Z,2023-09-29 13:51:03,Measuring the Robustness of Predictive Probability for Early Stopping in Experimental Design,"Physical experiments in the national security domain are often expensive and time-consuming. Test engineers must certify the compatibility of aircraft and their weapon systems before they can be deployed in the field, but the testing required is time consuming, expensive, and resource limited. Adopting Bayesian adaptive designs are a promising way to borrow from the successes seen in the clinical trials domain. The use of predictive probability (PP) to stop testing early and make faster decisions is particularly appealing given the aforementioned constraints. Given the high-consequence nature of the tests performed in the national security space, a strong understanding of new methods is required before being deployed. Although PP has been thoroughly studied for binary data, there is less work with continuous data, which often in reliability studies interested in certifying the specification limits of components. A simulation study evaluating the robustness of this approach indicate early stopping based on PP is reasonably robust to minor assumption violations, especially when only a few interim analyses are conducted. A post-hoc analysis exploring whether release requirements of a weapon system from an aircraft are within specification with desired reliability resulted in stopping the experiment early and saving 33% of the experimental runs.",Daniel Ries|Victoria R. C. Sieck|Philip Jones|Julie Shaffer,,https://arxiv.org/abs/2309.17241v1,https://arxiv.org/pdf/2309.17241v1,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2309.17241v1.pdf
2309.16961v2,2023-09-29T04:03:32Z,2024-07-29 18:07:32,A review of Design of Experiments courses offered to undergraduate students at American universities,"Design of Experiments (DoE) is a relevant class to undergraduate students in the sciences, because it teaches them how to plan, conduct, and analyze experiments. In the literature on DoE, there are several contributions to its pedagogy, such as easy-to-use class experiments, virtual experiments, and software to construct experimental designs. However, there are virtually no systematic evaluations of the actual DoE pedagogy. To address this issue, we build the first database of DoE courses offered to undergraduate students in the United States. The database has records on courses offered from 2019 to 2022 at the best universities in the US News Best National Universities ranking of 2022. Specifically, it has data on 18 general and content-specific features of 206 courses. To study the DoE pedagogy, we analyze the database using descriptive statistics and text mining. Based on our analysis, we provide instructors with recommendations and teaching material to enhance their DoE courses. The database and material are included in the supplement of this article.",Alan R. Vazquez|Xiaocong Xuan,,https://arxiv.org/abs/2309.16961v2,https://arxiv.org/pdf/2309.16961v2,https://doi.org/10.1080/00031305.2024.2368803,"33 pages, 1 figures",,10.1080/00031305.2024.2368803,stat.OT,stat.OT,https://arxiv.org/pdf/2309.16961v2.pdf
2309.08923v1,2023-09-16T08:28:15Z,2023-09-16 08:28:15,Fast Approximation of the Shapley Values Based on Order-of-Addition Experimental Designs,"Shapley value is originally a concept in econometrics to fairly distribute both gains and costs to players in a coalition game. In the recent decades, its application has been extended to other areas such as marketing, engineering and machine learning. For example, it produces reasonable solutions for problems in sensitivity analysis, local model explanation towards the interpretable machine learning, node importance in social network, attribution models, etc. However, its heavy computational burden has been long recognized but rarely investigated. Specifically, in a $d$-player coalition game, calculating a Shapley value requires the evaluation of $d!$ or $2^d$ marginal contribution values, depending on whether we are taking the permutation or combination formulation of the Shapley value. Hence it becomes infeasible to calculate the Shapley value when $d$ is reasonably large. A common remedy is to take a random sample of the permutations to surrogate for the complete list of permutations. We find an advanced sampling scheme can be designed to yield much more accurate estimation of the Shapley value than the simple random sampling (SRS). Our sampling scheme is based on combinatorial structures in the field of design of experiments (DOE), particularly the order-of-addition experimental designs for the study of how the orderings of components would affect the output. We show that the obtained estimates are unbiased, and can sometimes deterministically recover the original Shapley value. Both theoretical and simulations results show that our DOE-based sampling scheme outperforms SRS in terms of estimation accuracy. Surprisingly, it is also slightly faster than SRS. Lastly, real data analysis is conducted for the C. elegans nervous system and the 9/11 terrorist network.",Liuqing Yang|Yongdao Zhou|Haoda Fu|Min-Qian Liu|Wei Zheng,,https://arxiv.org/abs/2309.08923v1,https://arxiv.org/pdf/2309.08923v1,,,,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2309.08923v1.pdf
2309.08797v1,2023-09-15T22:43:20Z,2023-09-15 22:43:20,On the Asymptotics of Graph Cut Objectives for Experimental Designs of Network A/B Testing,"A/B testing is an effective way to assess the potential impacts of two treatments. For A/B tests conducted by IT companies, the test users of A/B testing are often connected and form a social network. The responses of A/B testing can be related to the network connection of test users. This paper discusses the relationship between the design criteria of network A/B testing and graph cut objectives. We develop asymptotic distributions of graph cut objectives to enable rerandomization algorithms for the design of network A/B testing under two scenarios.",Qiong Zhang,,https://arxiv.org/abs/2309.08797v1,https://arxiv.org/pdf/2309.08797v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2309.08797v1.pdf
2309.04009v1,2023-09-07T20:26:06Z,2023-09-07 20:26:06,Computing D-Optimal solutions for huge-scale linear and quadratic response-surface models,"We consider algorithmic approaches to the D-optimality problem for cases where the input design matrix is large and highly structured, in particular implicitly specified as a full quadratic or linear response-surface model in several levels of several factors. Using row generation techniques of mathematical optimization, in the context of discrete local-search and continuous relaxation aimed at branch-and-bound solution, we are able to design practical algorithms.",Gabriel Ponte|Marcia Fampa|Jon Lee,,https://arxiv.org/abs/2309.04009v1,https://arxiv.org/pdf/2309.04009v1,,,,,math.OC,math.OC,https://arxiv.org/pdf/2309.04009v1.pdf
2309.02042v1,2023-09-05T08:34:58Z,2023-09-05 08:34:58,Bayesian experimental design for linear elasticity,"This work considers Bayesian experimental design for the inverse boundary value problem of linear elasticity in a two-dimensional setting. The aim is to optimize the positions of compactly supported pressure activations on the boundary of the examined body in order to maximize the value of the resulting boundary deformations as data for the inverse problem of reconstructing the Lamé parameters inside the object. We resort to a linearized measurement model and adopt the framework of Bayesian experimental design, under the assumption that the prior and measurement noise distributions are mutually independent Gaussians. This enables the use of the standard Bayesian A-optimality criterion for deducing optimal positions for the pressure activations. The (second) derivatives of the boundary measurements with respect to the Lamé parameters and the positions of the boundary pressure activations are deduced to allow minimizing the corresponding objective function, i.e., the trace of the covariance matrix of the posterior distribution, by a gradient-based optimization algorithm. Two-dimensional numerical experiments are performed to demonstrate the functionality of our approach.",Sarah Eberle-Blick|Nuutti Hyvönen,,https://arxiv.org/abs/2309.02042v1,https://arxiv.org/pdf/2309.02042v1,,"23 pages, 11 figures",,,math.NA,math.NA,https://arxiv.org/pdf/2309.02042v1.pdf
2308.10702v2,2023-08-21T13:13:56Z,2023-10-26 18:07:20,Bayesian Optimal Experimental Design for Constitutive Model Calibration,"Computational simulation is increasingly relied upon for high-consequence engineering decisions, and a foundational element to solid mechanics simulations, such as finite element analysis (FEA), is a credible constitutive or material model. Calibration of these complex models is an essential step; however, the selection, calibration and validation of material models is often a discrete, multi-stage process that is decoupled from material characterization activities, which means the data collected does not always align with the data that is needed. To address this issue, an integrated workflow for delivering an enhanced characterization and calibration procedure (Interlaced Characterization and Calibration (ICC)) is introduced. This framework leverages Bayesian optimal experimental design (BOED) to select the optimal load path for a cruciform specimen in order to collect the most informative data for model calibration. The critical first piece of algorithm development is to demonstrate the active experimental design for a fast model with simulated data. For this demonstration, a material point simulator that models a plane stress elastoplastic material subject to bi-axial loading was chosen. The ICC framework is demonstrated on two exemplar problems in which BOED is used to determine which load step to take, e.g., in which direction to increment the strain, at each iteration of the characterization and calibration cycle. Calibration results from data obtained by adaptively selecting the load path within the ICC algorithm are compared to results from data generated under two naive static load paths that were chosen a priori based on human intuition. In these exemplar problems, data generated in an adaptive setting resulted in calibrated model parameters with reduced measures of uncertainty compared to the static settings.",Denielle Ricciardi|Tom Seidl|Brian Lester|Amanda Jones|Elizabeth Jones,,https://arxiv.org/abs/2308.10702v2,https://arxiv.org/pdf/2308.10702v2,,"39 pages, 13 figures",,,cs.CE,cs.CE|stat.AP,https://arxiv.org/pdf/2308.10702v2.pdf
2308.09888v2,2023-08-19T02:48:44Z,2023-12-12 21:21:08,On Estimating the Gradient of the Expected Information Gain in Bayesian Experimental Design,"Bayesian Experimental Design (BED), which aims to find the optimal experimental conditions for Bayesian inference, is usually posed as to optimize the expected information gain (EIG). The gradient information is often needed for efficient EIG optimization, and as a result the ability to estimate the gradient of EIG is essential for BED problems. The primary goal of this work is to develop methods for estimating the gradient of EIG, which, combined with the stochastic gradient descent algorithms, result in efficient optimization of EIG. Specifically, we first introduce a posterior expected representation of the EIG gradient with respect to the design variables. Based on this, we propose two methods for estimating the EIG gradient, UEEG-MCMC that leverages posterior samples generated through Markov Chain Monte Carlo (MCMC) to estimate the EIG gradient, and BEEG-AP that focuses on achieving high simulation efficiency by repeatedly using parameter samples. Theoretical analysis and numerical studies illustrate that UEEG-MCMC is robust agains the actual EIG value, while BEEG-AP is more efficient when the EIG value to be optimized is small. Moreover, both methods show superior performance compared to several popular benchmarks in our numerical experiments.",Ziqiao Ao|Jinglai Li,,https://arxiv.org/abs/2308.09888v2,https://arxiv.org/pdf/2308.09888v2,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2308.09888v2.pdf
2308.01829v1,2023-08-03T15:40:08Z,2023-08-03 15:40:08,Not All Actions Are Created Equal: Bayesian Optimal Experimental Design for Safe and Optimal Nonlinear System Identification,Uncertainty in state or model parameters is common in robotics and typically handled by acquiring system measurements that yield information about the uncertain quantities of interest. Inputs to a nonlinear dynamical system yield outcomes that produce varying amounts of information about the underlying uncertain parameters of the system. To maximize information gained with respect to these uncertain parameters we present a Bayesian approach to data collection for system identification called Bayesian Optimal Experimental Design (BOED). The formulation uses parameterized trajectories and cubature to compute maximally informative system trajectories which obtain as much information as possible about unknown system parameters while also ensuring safety under mild assumptions. The proposed method is applicable to non-linear and non-Gaussian systems and is applied to a high-fidelity vehicle model from the literature. It is shown the proposed approach requires orders of magnitude fewer samples compared to state-of-the-art BOED algorithms from the literature while simultaneously providing safety guarantees.,Parker Ewen|Gitesh Gunjal|Joey Wilson|Jinsun Liu|Challen Enninful Adu|Ram Vasudevan,,https://arxiv.org/abs/2308.01829v1,https://arxiv.org/pdf/2308.01829v1,,,,,cs.RO,cs.RO|eess.SY,https://arxiv.org/pdf/2308.01829v1.pdf
2307.15245v1,2023-07-28T00:48:05Z,2023-07-28 00:48:05,A Practical Recipe for Federated Learning Under Statistical Heterogeneity Experimental Design,"Federated Learning (FL) has been an area of active research in recent years. There have been numerous studies in FL to make it more successful in the presence of data heterogeneity. However, despite the existence of many publications, the state of progress in the field is unknown. Many of the works use inconsistent experimental settings and there are no comprehensive studies on the effect of FL-specific experimental variables on the results and practical insights for a more comparable and consistent FL experimental setup. Furthermore, the existence of several benchmarks and confounding variables has further complicated the issue of inconsistency and ambiguity. In this work, we present the first comprehensive study on the effect of FL-specific experimental variables in relation to each other and performance results, bringing several insights and recommendations for designing a meaningful and well-incentivized FL experimental setup. We further aid the community by releasing FedZoo-Bench, an open-source library based on PyTorch with pre-implementation of 22 state-of-the-art methods, and a broad set of standardized and customizable features available at https://github.com/MMorafah/FedZoo-Bench. We also provide a comprehensive comparison of several state-of-the-art (SOTA) methods to better understand the current state of the field and existing limitations.",Mahdi Morafah|Weijia Wang|Bill Lin,,https://arxiv.org/abs/2307.15245v1,https://arxiv.org/pdf/2307.15245v1,,,,,cs.LG,cs.LG|cs.AI,https://arxiv.org/pdf/2307.15245v1.pdf
2307.13871v1,2023-07-26T00:18:23Z,2023-07-26 00:18:23,Emulating Expert Insight: A Robust Strategy for Optimal Experimental Design,"The challenge of optimal design of experiments (DOE) pervades materials science, physics, chemistry, and biology. Bayesian optimization has been used to address this challenge in vast sample spaces, although it requires framing experimental campaigns through the lens of maximizing some observable. This framing is insufficient for epistemic research goals that seek to comprehensively analyze a sample space, without an explicit scalar objective (e.g., the characterization of a wafer or sample library). In this work, we propose a flexible formulation of scientific value that recasts a dataset of input conditions and higher-dimensional observable data into a continuous, scalar metric. Intuitively, the scientific value function measures where observables change significantly, emulating the perspective of experts driving an experiment, and can be used in collaborative analysis tools or as an objective for optimization techniques. We demonstrate this technique by exploring simulated phase boundaries from different observables, autonomously driving a variable temperature measurement of a ferroelectric material, and providing feedback from a nanoparticle synthesis campaign. The method is seamlessly compatible with existing optimization tools, can be extended to multi-modal and multi-fidelity experiments, and can integrate existing models of an experimental system. Because of its flexibility, it can be deployed in a range of experimental settings for autonomous or accelerated experiments.",Matthew R. Carbone|Hyeong Jin Kim|Chandima Fernando|Shinjae Yoo|Daniel Olds|Howie Joress|Brian DeCost|Bruce Ravel|Yugang Zhang|Phillip M. Maffettone,,https://arxiv.org/abs/2307.13871v1,https://arxiv.org/pdf/2307.13871v1,,,,,cond-mat.mtrl-sci,cond-mat.mtrl-sci|physics.comp-ph,https://arxiv.org/pdf/2307.13871v1.pdf
2307.11297v4,2023-07-21T01:56:04Z,2023-09-20 11:49:23,Fused Spectatorship: Designing Bodily Experiences Where Spectators Become Players,"Spectating digital games can be exciting. However, due to its vicarious nature, spectators often wish to engage in the gameplay beyond just watching and cheering. To blur the boundaries between spectators and players, we propose a novel approach called ""Fused Spectatorship"", where spectators watch their hands play games by loaning bodily control to a computational Electrical Muscle Stimulation (EMS) system. To showcase this concept, we designed three games where spectators loan control over both their hands to the EMS system and watch them play these competitive and collaborative games. A study with 12 participants suggested that participants could not distinguish if they were watching their hands play, or if they were playing the games themselves. We used our results to articulate four spectator experience themes and four fused spectator types, the behaviours they elicited and offer one design consideration to support each of these behaviours. We also discuss the ethical design considerations of our approach to help game designers create future fused spectatorship experiences.",Rakesh Patibanda|Aryan Saini|Nathalie Overdevest|Maria F. Montoya|Xiang Li|Yuzheng Chen|Shreyas Nisal|Josh Andres|Jarrod Knibbe|Elise van den Hoven|Florian 'Floyd' Mueller,,https://arxiv.org/abs/2307.11297v4,https://arxiv.org/pdf/2307.11297v4,https://doi.org/10.1145/3611049,This paper is going to be published at Annual Symposium on Computer-Human Interaction in Play (CHI PLAY) 2023,Annual Symposium on Computer-Human Interaction in Play (CHI PLAY) 2023,10.1145/3611049,cs.HC,cs.HC,https://arxiv.org/pdf/2307.11297v4.pdf
2307.10161v1,2023-07-19T17:44:38Z,2023-07-19 17:44:38,Variability estimation in a non-linear crack growth simulation model with controlled parameters using Designed Experiments testing,"Variability in multiple independent input parameters makes it difficult to estimate the resultant variability in the system's overall response. The Propagation of Errors and Monte-Carlo techniques are two major methods to predict the variability of a system. However, in the former method, the formalism can lead to an inaccurate estimate for systems that have parameters varying over a wide range. For the latter, the results give a direct estimate of the variance of the response, but for complex systems with many parameters, the number of trials necessary to yield an accurate estimate can be very large to the point the technique becomes impractical. In this study, the effectiveness of the Tolerance Design method to estimate variability in complex systems is studied. We use a linear elastic 3 point bending beam model and a nonlinear extended finite elements crack growth model to test and compare the PE and MC methods with the TD method. Results from an MC estimate, using 10,000 trials, serve as a reference to validate the result in both cases. We find that the PE method works suboptimal for a coefficient of variance above 5% in the input variables. In addition, we find that the TD method works very well with moderately sized trials of designed experiment for both models. Our results demonstrate how the variability estimation methods perform in the deterministic domain of numerical simulations and can assist in designing physical tests by providing a guideline performance measure.",Seungju Yeoa|Paul Funkenbuscha|Hesam Askari,,https://arxiv.org/abs/2307.10161v1,https://arxiv.org/pdf/2307.10161v1,,,,,cond-mat.other,cond-mat.other,https://arxiv.org/pdf/2307.10161v1.pdf
2307.09878v1,2023-07-19T10:17:35Z,2023-07-19 10:17:35,Amortised Experimental Design and Parameter Estimation for User Models of Pointing,"User models play an important role in interaction design, supporting automation of interaction design choices. In order to do so, model parameters must be estimated from user data. While very large amounts of user data are sometimes required, recent research has shown how experiments can be designed so as to gather data and infer parameters as efficiently as possible, thereby minimising the data requirement. In the current article, we investigate a variant of these methods that amortises the computational cost of designing experiments by training a policy for choosing experimental designs with simulated participants. Our solution learns which experiments provide the most useful data for parameter estimation by interacting with in-silico agents sampled from the model space thereby using synthetic data rather than vast amounts of human data. The approach is demonstrated for three progressively complex models of pointing.",Antti Keurulainen|Isak Westerlund|Oskar Keurulainen|Andrew Howes,,https://arxiv.org/abs/2307.09878v1,https://arxiv.org/pdf/2307.09878v1,https://doi.org/10.1145/3544548.3581483,,"Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (CHI '23), April 23--28, 2023, Hamburg, Germany",10.1145/3544548.3581483,cs.AI,cs.AI,https://arxiv.org/pdf/2307.09878v1.pdf
2307.08449v1,2023-07-17T12:45:19Z,2023-07-17 12:45:19,Multi-fidelity experimental design for ice-sheet simulation,"Computer simulations are becoming an essential tool in many scientific fields from molecular dynamics to aeronautics. In glaciology, future predictions of sea level change require input from ice sheet models. Due to uncertainties in the forcings and the parameter choices for such models, many different realisations of the model are needed in order to produce probabilistic forecasts of sea level change. For these reasons, producing robust probabilistic forecasts from an ensemble of model simulations over regions of interest can be extremely expensive for many ice sheet models. Multi-fidelity experimental design (MFED) is a strategy that models the high-fidelity output of the simulator by combining information from various resolutions in an attempt to minimize the computational costs of the process and maximize the accuracy of the posterior. In this paper, we present an application of MFED to an ice-sheet simulatorand demonstrate potential computational savings by modelling the relationship between spatial resolutions. We also analyze the behavior of MFED strategies using theoretical results from sub-modular maximization.",Pierre Thodoroff|Markus Kaiser|Rosie Williams|Robert Arthern|Scott Hosking|Neil Lawrence|James Byrne|Ieva Kazlauskaite,,https://arxiv.org/abs/2307.08449v1,https://arxiv.org/pdf/2307.08449v1,,,,,physics.comp-ph,physics.comp-ph|physics.ao-ph,https://arxiv.org/pdf/2307.08449v1.pdf
2307.06343v2,2023-07-12T13:28:01Z,2025-05-27 13:44:57,Sequential Experimental Design for X-Ray CT Using Deep Reinforcement Learning,"In X-ray Computed Tomography (CT), projections from many angles are acquired and used for 3D reconstruction. To make CT suitable for in-line quality control, reducing the number of angles while maintaining reconstruction quality is necessary. Sparse-angle tomography is a popular approach for obtaining 3D reconstructions from limited data. To optimize its performance, one can adapt scan angles sequentially to select the most informative angles for each scanned object. Mathematically, this corresponds to solving an optimal experimental design (OED) problem. OED problems are high-dimensional, non-convex, bi-level optimization problems that cannot be solved online, i.e., during the scan. To address these challenges, we pose the OED problem as a partially observable Markov decision process in a Bayesian framework, and solve it through deep reinforcement learning. The approach learns efficient non-greedy policies to solve a given class of OED problems through extensive offline training rather than solving a given OED problem directly via numerical optimization. As such, the trained policy can successfully find the most informative scan angles online. We use a policy training method based on the Actor-Critic approach and evaluate its performance on 2D tomography with synthetic data.",Tianyuan Wang|Felix Lucka|Tristan van Leeuwen,,https://arxiv.org/abs/2307.06343v2,https://arxiv.org/pdf/2307.06343v2,https://doi.org/10.1109/TCI.2024.3414273,Accepted for publication in IEEE Transactions on Computational Imaging,"IEEE Transactions on Computational Imaging, vol. 10, pp. 953 - 968, 2024",10.1109/TCI.2024.3414273,eess.IV,eess.IV|cs.CV|cs.LG,https://arxiv.org/pdf/2307.06343v2.pdf
2307.11593v2,2023-07-11T13:47:58Z,2023-07-24 19:57:47,Towards a unified language in experimental designs propagated by a software framework,"Experiments require human decisions in the design process, which in turn are reformulated and summarized as inputs into a system (computational or otherwise) to generate the experimental design. I leverage this system to promote a language of experimental designs by proposing a novel computational framework, called ""the grammar of experimental designs"", to specify experimental designs based on an object-oriented programming system that declaratively encapsulates the experimental structure. The framework aims to engage human cognition by building experimental designs with modular functions that modify a targeted singular element of the experimental design object. The syntax and semantics of the framework are built upon consideration from multiple perspectives. While the core framework is language-agnostic, the framework is implemented in the `edibble` R-package. A range of examples is shown to demonstrate the utility of the framework.",Emi Tanaka,,https://arxiv.org/abs/2307.11593v2,https://arxiv.org/pdf/2307.11593v2,,,,,cs.OH,cs.OH|q-bio.QM|stat.ME,https://arxiv.org/pdf/2307.11593v2.pdf
2307.05159v1,2023-07-11T10:31:58Z,2023-07-11 10:31:58,Experimental designs for controlling the correlation of estimators in two parameter models,"The state of the art related to parameter correlation in two-parameter models has been reviewed in this paper. The apparent contradictions between the different authors regarding the ability of D--optimality to simultaneously reduce the correlation and the area of the confidence ellipse in two-parameter models were analyzed. Two main approaches were found: 1) those who consider that the optimality criteria simultaneously control the precision and correlation of the parameter estimators; and 2) those that consider a combination of criteria to achieve the same objective. An analytical criterion combining in its structure both the optimality of the precision of the estimators of the parameters and the reduction of the correlation between their estimators is provided. The criterion was tested both in a simple linear regression model, considering all possible design spaces, and in a non-linear model with strong correlation of the estimators of the parameters (Michaelis--Menten) to show its performance. This criterion showed a superior behavior to all the strategies and criteria to control at the same time the precision and the correlation.",Edgar Benitez|Jesús López-Fidalgo,,https://arxiv.org/abs/2307.05159v1,https://arxiv.org/pdf/2307.05159v1,,"30 pages, 8 figures, 5 tables",,,math.ST,math.ST,https://arxiv.org/pdf/2307.05159v1.pdf
2307.04354v1,2023-07-10T05:33:41Z,2023-07-10 05:33:41,Policy Finetuning in Reinforcement Learning via Design of Experiments using Offline Data,"In some applications of reinforcement learning, a dataset of pre-collected experience is already available but it is also possible to acquire some additional online data to help improve the quality of the policy. However, it may be preferable to gather additional data with a single, non-reactive exploration policy and avoid the engineering costs associated with switching policies.
  In this paper we propose an algorithm with provable guarantees that can leverage an offline dataset to design a single non-reactive policy for exploration. We theoretically analyze the algorithm and measure the quality of the final policy as a function of the local coverage of the original dataset and the amount of additional data collected.",Ruiqi Zhang|Andrea Zanette,,https://arxiv.org/abs/2307.04354v1,https://arxiv.org/pdf/2307.04354v1,,43 pages,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2307.04354v1.pdf
2307.01039v2,2023-07-03T14:17:46Z,2024-01-23 12:14:21,"Variational Bayesian experimental design for geophysical applications: seismic source location, amplitude versus offset inversion, and estimating CO2 saturations in a subsurface reservoir","This paper introduces variational design methods that are novel to Geophysics, and discusses their benefits and limitations in the context of geophysical applications and more established design methods. Variational methods rely on functional approximations to probability distributions and model-data relationships. They can be used to design experiments that best resolve either all model parameters, or the answer to specific questions about the system to be interrogated. The methods are tested in three schematic geophysical applications: (i) estimating a source location given arrival times at sensor locations, and (ii) estimating the contrast in seismic wavefield velocity across a stratal interface given measurements of the amplitudes of seismic wavefield reflections from that interface, and (iii) designing a survey to best constrain CO2 saturation in a subsurface storage scenario. Variational methods allow the value of an experiment to be calculated and optimised simultaneously, which results in substantial savings in computational cost. In the context of designing a survey to best constrain CO2 in a subsurface storage scenario, we show that optimal designs may change substantially depending on the questions of interest. Overall, this work indicates that optimal design methods should be used more widely in Geophysics, as they are in other scientifically advanced fields.",Dominik Strutz|Andrew Curtis,,https://arxiv.org/abs/2307.01039v2,https://arxiv.org/pdf/2307.01039v2,https://doi.org/10.1093/gji/ggad492,Submitted to Geophysical Journal International in June 2023,,10.1093/gji/ggad492,physics.geo-ph,physics.geo-ph,https://arxiv.org/pdf/2307.01039v2.pdf
2306.17615v2,2023-06-30T12:40:43Z,2023-08-11 08:09:17,Scalable method for Bayesian experimental design without integrating over posterior distribution,"We address the computational efficiency in solving the A-optimal Bayesian design of experiments problems for which the observational map is based on partial differential equations and, consequently, is computationally expensive to evaluate. A-optimality is a widely used and easy-to-interpret criterion for Bayesian experimental design. This criterion seeks the optimal experimental design by minimizing the expected conditional variance, which is also known as the expected posterior variance. This study presents a novel likelihood-free approach to the A-optimal experimental design that does not require sampling or integrating the Bayesian posterior distribution. The expected conditional variance is obtained via the variance of the conditional expectation using the law of total variance, and we take advantage of the orthogonal projection property to approximate the conditional expectation. We derive an asymptotic error estimation for the proposed estimator of the expected conditional variance and show that the intractability of the posterior distribution does not affect the performance of our approach. We use an artificial neural network (ANN) to approximate the nonlinear conditional expectation in the implementation of our method. We then extend our approach for dealing with the case that the domain of experimental design parameters is continuous by integrating the training process of the ANN into minimizing the expected conditional variance. Through numerical experiments, we demonstrate that our method greatly reduces the number of observation model evaluations compared with widely used importance sampling-based approaches. This reduction is crucial, considering the high computational cost of the observational models. Code is available at https://github.com/vinh-tr-hoang/DOEviaPACE.",Vinh Hoang|Luis Espath|Sebastian Krumscheid|Raúl Tempone,,https://arxiv.org/abs/2306.17615v2,https://arxiv.org/pdf/2306.17615v2,,,,,math.NA,math.NA|stat.CO|stat.ML,https://arxiv.org/pdf/2306.17615v2.pdf
2306.15731v1,2023-06-27T18:15:41Z,2023-06-27 18:15:41,Stochastic Gradient Bayesian Optimal Experimental Designs for Simulation-based Inference,"Simulation-based inference (SBI) methods tackle complex scientific models with challenging inverse problems. However, SBI models often face a significant hurdle due to their non-differentiable nature, which hampers the use of gradient-based optimization techniques. Bayesian Optimal Experimental Design (BOED) is a powerful approach that aims to make the most efficient use of experimental resources for improved inferences. While stochastic gradient BOED methods have shown promising results in high-dimensional design problems, they have mostly neglected the integration of BOED with SBI due to the difficult non-differentiable property of many SBI simulators. In this work, we establish a crucial connection between ratio-based SBI inference algorithms and stochastic gradient-based variational inference by leveraging mutual information bounds. This connection allows us to extend BOED to SBI applications, enabling the simultaneous optimization of experimental designs and amortized inference functions. We demonstrate our approach on a simple linear model and offer implementation details for practitioners.",Vincent D. Zaballa|Elliot E. Hui,,https://arxiv.org/abs/2306.15731v1,https://arxiv.org/pdf/2306.15731v1,,Presented at ICML 2023 workshop on Differentiable Everything,,,cs.LG,cs.LG|q-bio.QM|stat.ME,https://arxiv.org/pdf/2306.15731v1.pdf
2306.14510v1,2023-06-26T08:40:14Z,2023-06-26 08:40:14,Deep Bayesian Experimental Design for Quantum Many-Body Systems,"Bayesian experimental design is a technique that allows to efficiently select measurements to characterize a physical system by maximizing the expected information gain. Recent developments in deep neural networks and normalizing flows allow for a more efficient approximation of the posterior and thus the extension of this technique to complex high-dimensional situations. In this paper, we show how this approach holds promise for adaptive measurement strategies to characterize present-day quantum technology platforms. In particular, we focus on arrays of coupled cavities and qubit arrays. Both represent model systems of high relevance for modern applications, like quantum simulations and computing, and both have been realized in platforms where measurement and control can be exploited to characterize and counteract unavoidable disorder. Thus, they represent ideal targets for applications of Bayesian experimental design.",Leopoldo Sarra|Florian Marquardt,,https://arxiv.org/abs/2306.14510v1,https://arxiv.org/pdf/2306.14510v1,,,,,quant-ph,quant-ph|cs.LG,https://arxiv.org/pdf/2306.14510v1.pdf
2306.10584v1,2023-06-18T15:31:26Z,2023-06-18 15:31:26,Optical Integrated Sensing and Communication for Cooperative Mobile Robotics Design and Experiments,"Integrated Sensing and Communication (ISAC) is an emerging technology that integrates wireless sensing and communication into a single system, transforming many applications, including cooperative mobile robotics. However, in scenarios where radio communications are unavailable, alternative approaches are needed. In this paper, we propose a new optical ISAC (OISAC) scheme for cooperative mobile robots by integrating camera sensing and screen-camera communication (SCC). Unlike previous throughput-oriented SCC designs that work with stationary SCC links, our OISAC scheme is designed for real-time control of mobile robots. It addresses new problems such as image blur and long image display delay. As a case study, we consider the leader-follower formation control problem, an essential part of cooperative mobile robotics. The proposed OISAC scheme enables the follower robot to simultaneously acquire the information shared by the leader and sense the relative pose to the leader using only RGB images captured by its onboard camera. We then design a new control law that can leverage all the information acquired by the camera to achieve stable and accurate formations. We design and conduct real-world experiments involving uniform and nonuniform motions to evaluate the proposed system and demonstrate the advantages of applying OISAC over a benchmark approach that uses extended Kalman filtering (EKF) to estimate the leader's states. Our results show that the proposed OISAC-augmented leader-follower formation system achieves better performance in terms of accuracy, stability, and robustness.",Shengqian Wang|He Chen,,https://arxiv.org/abs/2306.10584v1,https://arxiv.org/pdf/2306.10584v1,,"8 pages, 7 figures",,,cs.NI,cs.NI,https://arxiv.org/pdf/2306.10584v1.pdf
2306.10430v2,2023-06-17T21:47:19Z,2024-12-23 17:56:29,Variational Sequential Optimal Experimental Design using Reinforcement Learning,"We present variational sequential optimal experimental design (vsOED), a novel method for optimally designing a finite sequence of experiments within a Bayesian framework with information-theoretic criteria. vsOED employs a one-point reward formulation with variational posterior approximations, providing a provable lower bound to the expected information gain. Numerical methods are developed following an actor-critic reinforcement learning approach, including derivation and estimation of variational and policy gradients to optimize the design policy, and posterior approximation using Gaussian mixture models and normalizing flows. vsOED accommodates nuisance parameters, implicit likelihoods, and multiple candidate models, while supporting flexible design criteria that can target designs for model discrimination, parameter inference, goal-oriented prediction, and their weighted combinations. We demonstrate vsOED across various engineering and science applications, illustrating its superior sample efficiency compared to existing sequential experimental design algorithms.",Wanggang Shen|Jiayuan Dong|Xun Huan,,https://arxiv.org/abs/2306.10430v2,https://arxiv.org/pdf/2306.10430v2,https://doi.org/10.1016/j.cma.2025.118068,,"Computer Methods in Applied Mechanics and Engineering, Volume 444, 1 September 2025, Article 118068",10.1016/j.cma.2025.118068,stat.ML,stat.ML|cs.AI|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/2306.10430v2.pdf
2306.05484v1,2023-06-08T18:10:37Z,2023-06-08 18:10:37,Task-specific experimental design for treatment effect estimation,"Understanding causality should be a core requirement of any attempt to build real impact through AI. Due to the inherent unobservability of counterfactuals, large randomised trials (RCTs) are the standard for causal inference. But large experiments are generically expensive, and randomisation carries its own costs, e.g. when suboptimal decisions are trialed. Recent work has proposed more sample-efficient alternatives to RCTs, but these are not adaptable to the downstream application for which the causal effect is sought. In this work, we develop a task-specific approach to experimental design and derive sampling strategies customised to particular downstream applications. Across a range of important tasks, real-world datasets, and sample sizes, our method outperforms other benchmarks, e.g. requiring an order-of-magnitude less data to match RCT performance on targeted marketing tasks.",Bethany Connolly|Kim Moore|Tobias Schwedes|Alexander Adam|Gary Willis|Ilya Feige|Christopher Frye,,https://arxiv.org/abs/2306.05484v1,https://arxiv.org/pdf/2306.05484v1,,"To appear in ICML 2023; 8 pages, 7 figures, 4 appendices",,,stat.ME,stat.ME|cs.LG|stat.ML,https://arxiv.org/pdf/2306.05484v1.pdf
2306.02015v1,2023-06-03T06:19:20Z,2023-06-03 06:19:20,Machine learning enabled experimental design and parameter estimation for ultrafast spin dynamics,"Advanced experimental measurements are crucial for driving theoretical developments and unveiling novel phenomena in condensed matter and material physics, which often suffer from the scarcity of facility resources and increasing complexities. To address the limitations, we introduce a methodology that combines machine learning with Bayesian optimal experimental design (BOED), exemplified with x-ray photon fluctuation spectroscopy (XPFS) measurements for spin fluctuations. Our method employs a neural network model for large-scale spin dynamics simulations for precise distribution and utility calculations in BOED. The capability of automatic differentiation from the neural network model is further leveraged for more robust and accurate parameter estimation. Our numerical benchmarks demonstrate the superior performance of our method in guiding XPFS experiments, predicting model parameters, and yielding more informative measurements within limited experimental time. Although focusing on XPFS and spin fluctuations, our method can be adapted to other experiments, facilitating more efficient data collection and accelerating scientific discoveries.",Zhantao Chen|Cheng Peng|Alexander N. Petsch|Sathya R. Chitturi|Alana Okullo|Sugata Chowdhury|Chun Hong Yoon|Joshua J. Turner,,https://arxiv.org/abs/2306.02015v1,https://arxiv.org/pdf/2306.02015v1,,,,,cond-mat.mtrl-sci,cond-mat.mtrl-sci|cs.LG|physics.comp-ph|physics.data-an,https://arxiv.org/pdf/2306.02015v1.pdf
2306.01211v5,2023-06-02T00:10:48Z,2024-08-05 17:59:18,Priming bias versus post-treatment bias in experimental designs,"Conditioning on variables affected by treatment can induce post-treatment bias when estimating causal effects. Although this suggests that researchers should measure potential moderators before administering the treatment in an experiment, doing so may also bias causal effect estimation if the covariate measurement primes respondents to react differently to the treatment. This paper formally analyzes this trade-off between post-treatment and priming biases in three experimental designs that vary when moderators are measured: pre-treatment, post-treatment, or a randomized choice between the two. We derive nonparametric bounds for interactions between the treatment and the moderator under each design and show how to use substantive assumptions to narrow these bounds. These bounds allow researchers to assess the sensitivity of their empirical findings to priming and post-treatment bias. We then apply the proposed methodology to a survey experiment on electoral messaging.",Matthew Blackwell|Jacob R. Brown|Sophie Hill|Kosuke Imai|Teppei Yamamoto,,https://arxiv.org/abs/2306.01211v5,https://arxiv.org/pdf/2306.01211v5,https://doi.org/10.1017/pan.2025.3,"28 pages (main text), 22 pages (supplementary materials), 5 figures",Polit. Anal. 33 (2025) 361-377,10.1017/pan.2025.3,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2306.01211v5.pdf
2305.17562v2,2023-05-27T19:46:04Z,2024-06-16 15:33:42,Mixed-integer linear programming for computing optimal experimental designs,"The problem of computing an exact experimental design that is optimal for the least-squares estimation of the parameters of a regression model is considered. We show that this problem can be solved via mixed-integer linear programming (MILP) for a wide class of optimality criteria, including the criteria of A-, I-, G- and MV-optimality. This approach improves upon the current state-of-the-art mathematical programming formulation, which uses mixed-integer second-order cone programming. The key idea underlying the MILP formulation is McCormick relaxation, which critically depends on finite interval bounds for the elements of the covariance matrix of the least-squares estimator corresponding to an optimal exact design. We provide both analytic and algorithmic methods for constructing these bounds. We also demonstrate the unique advantages of the MILP approach, such as the possibility of incorporating multiple design constraints into the optimization problem, including constraints on the variances and covariances of the least-squares estimator.",Radoslav Harman|Samuel Rosa,,https://arxiv.org/abs/2305.17562v2,https://arxiv.org/pdf/2305.17562v2,https://doi.org/10.1016/j.jspi.2024.106200,Accepted manuscript,"Journal of Statistical Planning and Inference, Volume 234, 2025, 106200",10.1016/j.jspi.2024.106200,stat.CO,stat.CO,https://arxiv.org/pdf/2305.17562v2.pdf
2305.17187v2,2023-05-26T18:22:42Z,2023-10-13 18:21:32,Clip-OGD: An Experimental Design for Adaptive Neyman Allocation in Sequential Experiments,"From clinical development of cancer therapies to investigations into partisan bias, adaptive sequential designs have become increasingly popular method for causal inference, as they offer the possibility of improved precision over their non-adaptive counterparts. However, even in simple settings (e.g. two treatments) the extent to which adaptive designs can improve precision is not sufficiently well understood. In this work, we study the problem of Adaptive Neyman Allocation in a design-based potential outcomes framework, where the experimenter seeks to construct an adaptive design which is nearly as efficient as the optimal (but infeasible) non-adaptive Neyman design, which has access to all potential outcomes. Motivated by connections to online optimization, we propose Neyman Ratio and Neyman Regret as two (equivalent) performance measures of adaptive designs for this problem. We present Clip-OGD, an adaptive design which achieves $\widetilde{O}(\sqrt{T})$ expected Neyman regret and thereby recovers the optimal Neyman variance in large samples. Finally, we construct a conservative variance estimator which facilitates the development of asymptotically valid confidence intervals. To complement our theoretical results, we conduct simulations using data from a microeconomic experiment.",Jessica Dai|Paula Gradu|Christopher Harshaw,,https://arxiv.org/abs/2305.17187v2,https://arxiv.org/pdf/2305.17187v2,,NeurIPS 2023,,,stat.ME,stat.ME|cs.DS,https://arxiv.org/pdf/2305.17187v2.pdf
2305.16506v1,2023-05-25T22:18:43Z,2023-05-25 22:18:43,Sequential Bayesian experimental design for calibration of expensive simulation models,"Simulation models of critical systems often have parameters that need to be calibrated using observed data. For expensive simulation models, calibration is done using an emulator of the simulation model built on simulation output at different parameter settings. Using intelligent and adaptive selection of parameters to build the emulator can drastically improve the efficiency of the calibration process. The article proposes a sequential framework with a novel criterion for parameter selection that targets learning the posterior density of the parameters. The emergent behavior from this criterion is that exploration happens by selecting parameters in uncertain posterior regions while simultaneously exploitation happens by selecting parameters in regions of high posterior density. The advantages of the proposed method are illustrated using several simulation experiments and a nuclear physics reaction model.",Özge Sürer|Matthew Plumlee|Stefan M. Wild,,https://arxiv.org/abs/2305.16506v1,https://arxiv.org/pdf/2305.16506v1,https://doi.org/10.1080/00401706.2023.2246157,,,10.1080/00401706.2023.2246157,stat.ME,stat.ME,https://arxiv.org/pdf/2305.16506v1.pdf
2305.07040v1,2023-05-11T13:21:26Z,2023-05-11 13:21:26,Sequential Experimental Design for Spectral Measurement: Active Learning Using a Parametric Model,"In this study, we demonstrate a sequential experimental design for spectral measurements by active learning using parametric models as predictors. In spectral measurements, it is necessary to reduce the measurement time because of sample fragility and high energy costs. To improve the efficiency of experiments, sequential experimental designs are proposed, in which the subsequent measurement is designed by active learning using the data obtained before the measurement. Conventionally, parametric models are employed in data analysis; when employed for active learning, they are expected to afford a sequential experimental design that improves the accuracy of data analysis. However, due to the complexity of the formulas, a sequential experimental design using general parametric models has not been realized. Therefore, we applied Bayesian inference-based data analysis using the exchange Monte Carlo method to realize a sequential experimental design with general parametric models. In this study, we evaluated the effectiveness of the proposed method by applying it to Bayesian spectral deconvolution and Bayesian Hamiltonian selection in X-ray photoelectron spectroscopy. Using numerical experiments with artificial data, we demonstrated that the proposed method improves the accuracy of model selection and parameter estimation while reducing the measurement time compared with the results achieved without active learning or with active learning using the Gaussian process regression.",Tomohiro Nabika|Kenji Nagata|Shun Katakami|Masaichiro Mizumaki|Masato Okada,,https://arxiv.org/abs/2305.07040v1,https://arxiv.org/pdf/2305.07040v1,,,,,cs.LG,cs.LG|physics.data-an,https://arxiv.org/pdf/2305.07040v1.pdf
2310.19172v1,2023-05-09T09:55:53Z,2023-05-09 09:55:53,Identification of the Most Significant Parameter for Optimizing the Performance of RPL Routing Protocol in IoT Using Taguchi Design of Experiments,"Internet of Things (IoT) consists of a wide variety of devices with limited power sources. Due to the adhered reason, energy consumption is considered as one of the major challenges in the IoT environment. In this research article, an attempt is made to optimize the existing Routing Protocol (RPL) towards a green technology. It focuses on finding the most significant parameter in the RPL using Taguchi Design of Experiments. It emphasizes the effects of five input factors, such as Network Size, Mobility Speed, DIO_DOUBLING, DIO_MIN_INTERVAL, and Redundancy Constant on only one output parameter Power Consumption. The findings show that DIO_MIN_INTERVAL is the leading factor that has a significant effect on the power consumption in RPL. After determining the most significant factor that affects the power consumption, measures can be taken to optimize the performance of RPL by applying some optimization techniques. COOJA simulator is used to carry out the simulations required for this research article.",Chandra Sekhar Sanaboina|Pallamsetty Sanaboina,,https://arxiv.org/abs/2310.19172v1,https://arxiv.org/pdf/2310.19172v1,,12 Pages,,,cs.NI,cs.NI,https://arxiv.org/pdf/2310.19172v1.pdf
2305.03855v1,2023-05-05T21:43:00Z,2023-05-05 21:43:00,Robust A-Optimal Experimental Design for Bayesian Inverse Problems,"Optimal design of experiments for Bayesian inverse problems has recently gained wide popularity and attracted much attention, especially in the computational science and Bayesian inversion communities. An optimal design maximizes a predefined utility function that is formulated in terms of the elements of an inverse problem, an example being optimal sensor placement for parameter identification. The state-of-the-art algorithmic approaches following this simple formulation generally overlook misspecification of the elements of the inverse problem, such as the prior or the measurement uncertainties. This work presents an efficient algorithmic approach for designing optimal experimental design schemes for Bayesian inverse problems such that the optimal design is robust to misspecification of elements of the inverse problem. Specifically, we consider a worst-case scenario approach for the uncertain or misspecified parameters, formulate robust objectives, and propose an algorithmic approach for optimizing such objectives. Both relaxation and stochastic solution approaches are discussed with detailed analysis and insight into the interpretation of the problem and the proposed algorithmic approach. Extensive numerical experiments to validate and analyze the proposed approach are carried out for sensor placement in a parameter identification problem.",Ahmed Attia|Sven Leyffer|Todd Munson,,https://arxiv.org/abs/2305.03855v1,https://arxiv.org/pdf/2305.03855v1,,"25 pages, 11 figures",,,math.OC,math.OC|cs.LG,https://arxiv.org/pdf/2305.03855v1.pdf
2305.01942v1,2023-05-03T07:47:00Z,2023-05-03 07:47:00,Experimental Design for Any $p$-Norm,"We consider a general $p$-norm objective for experimental design problems that captures some well-studied objectives (D/A/E-design) as special cases. We prove that a randomized local search approach provides a unified algorithm to solve this problem for all $p$. This provides the first approximation algorithm for the general $p$-norm objective, and a nice interpolation of the best known bounds of the special cases.",Lap Chi Lau|Robert Wang|Hong Zhou,,https://arxiv.org/abs/2305.01942v1,https://arxiv.org/pdf/2305.01942v1,,29 pages,,,cs.DS,cs.DS|cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/2305.01942v1.pdf
2304.08701v2,2023-04-18T02:23:04Z,2023-04-21 00:11:53,Bayesian D-Optimal Design of Experiments with Quantitative and Qualitative Responses,"Systems with both quantitative and qualitative responses are widely encountered in many applications. Design of experiment methods are needed when experiments are conducted to study such systems. Classic experimental design methods are unsuitable here because they often focus on one type of response. In this paper, we develop a Bayesian D-optimal design method for experiments with one continuous and one binary response. Both noninformative and conjugate informative prior distributions on the unknown parameters are considered. The proposed design criterion has meaningful interpretations regarding the D-optimality for the models for both types of responses. An efficient point-exchange search algorithm is developed to construct the local D-optimal designs for given parameter values. Global D-optimal designs are obtained by accumulating the frequencies of the design points in local D-optimal designs, where the parameters are sampled from the prior distributions. The performances of the proposed methods are evaluated through two examples.",Lulu Kang|Xinwei Deng|Ran Jin,,https://arxiv.org/abs/2304.08701v2,https://arxiv.org/pdf/2304.08701v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2304.08701v2.pdf
2304.03210v1,2023-04-06T16:44:19Z,2023-04-06 16:44:19,Causal Discovery and Optimal Experimental Design for Genome-Scale Biological Network Recovery,"Causal discovery of genome-scale networks is important for identifying pathways from genes to observable traits - e.g. differences in cell function, disease, drug resistance and others. Causal learners based on graphical models rely on interventional samples to orient edges in the network. However, these models have not been shown to scale up the size of the genome, which are on the order of 1e3-1e4 genes. We introduce a new learner, SP-GIES, that jointly learns from interventional and observational datasets and achieves almost 4x speedup against an existing learner for 1,000 node networks. SP-GIES achieves an AUC-PR score of 0.91 on 1,000 node networks, and scales up to 2,000 node networks - this is 4x larger than existing works. We also show how SP-GIES improves downstream optimal experimental design strategies for selecting interventional experiments to perform on the system. This is an important step forward in realizing causal discovery at scale via autonomous experimental design.",Ashka Shah|Arvind Ramanathan|Valerie Hayot-Sasson|Rick Stevens,,https://arxiv.org/abs/2304.03210v1,https://arxiv.org/pdf/2304.03210v1,,To be published in Platform for Advanced Scientific Computing 2023 (PASC23) conference proceedings,,,q-bio.MN,q-bio.MN|cs.DC,https://arxiv.org/pdf/2304.03210v1.pdf
2303.09104v3,2023-03-16T06:29:38Z,2023-05-10 07:14:28,A bit-parallel tabu search algorithm for finding E($s^2$)-optimal and minimax-optimal supersaturated designs,"We prove the equivalence of two-symbol supersaturated designs (SSDs) with $N$ (even) rows, $m$ columns, $s_{\rm max} = 4t +i$, where $i\in\{0,2\}$, $t \in \mathbb{Z}^{\geq 0}$ and resolvable incomplete block designs (RIBDs) whose any two blocks intersect in at most $(N+4t+i)/4$ points. Using this equivalence, we formulate the search for two-symbol E($s^2$)-optimal and minimax-optimal SSDs with $s_{\max} \in \{2,4,6\}$ as a search for RIBDs whose blocks intersect accordingly. This allows developing a bit-parallel tabu search (TS) algorithm. The TS algorithm found E($s^2$)-optimal and minimax-optimal SSDs achieving the sharpest known E($s^2$) lower bound with $s_{\max} \in \{2,4,6\}$ of sizes $(N,m)=(16,25), (16,26), (16,27), (18,23),(18,24),(18,25),(18,26),(18,27),(18, 28),$ $(18,29),(20,21),(22,22),(22,23),(24,24)$, and $(24,25)$. In each of these cases no such SSD could previously be found.",Luis B. Morales|Dursun A. Bulutoglu,,https://arxiv.org/abs/2303.09104v3,https://arxiv.org/pdf/2303.09104v3,,,,,cs.DM,cs.DM,https://arxiv.org/pdf/2303.09104v3.pdf
2303.02227v1,2023-03-03T21:41:01Z,2023-03-03 21:41:01,Online simulator-based experimental design for cognitive model selection,"The problem of model selection with a limited number of experimental trials has received considerable attention in cognitive science, where the role of experiments is to discriminate between theories expressed as computational models. Research on this subject has mostly been restricted to optimal experiment design with analytically tractable models. However, cognitive models of increasing complexity, with intractable likelihoods, are becoming more commonplace. In this paper, we propose BOSMOS: an approach to experimental design that can select between computational models without tractable likelihoods. It does so in a data-efficient manner, by sequentially and adaptively generating informative experiments. In contrast to previous approaches, we introduce a novel simulator-based utility objective for design selection, and a new approximation of the model likelihood for model selection. In simulated experiments, we demonstrate that the proposed BOSMOS technique can accurately select models in up to 2 orders of magnitude less time than existing LFI alternatives for three cognitive science tasks: memory retention, sequential signal detection and risky choice.",Alexander Aushev|Aini Putkonen|Gregoire Clarte|Suyog Chandramouli|Luigi Acerbi|Samuel Kaski|Andrew Howes,,https://arxiv.org/abs/2303.02227v1,https://arxiv.org/pdf/2303.02227v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2303.02227v1.pdf
2303.01684v2,2023-03-03T02:56:05Z,2023-03-30 18:28:16,BO-Muse: A human expert and AI teaming framework for accelerated experimental design,"In this paper we introduce BO-Muse, a new approach to human-AI teaming for the optimization of expensive black-box functions. Inspired by the intrinsic difficulty of extracting expert knowledge and distilling it back into AI models and by observations of human behavior in real-world experimental design, our algorithm lets the human expert take the lead in the experimental process. The human expert can use their domain expertise to its full potential, while the AI plays the role of a muse, injecting novelty and searching for areas of weakness to break the human out of over-exploitation induced by cognitive entrenchment. With mild assumptions, we show that our algorithm converges sub-linearly, at a rate faster than the AI or human alone. We validate our algorithm using synthetic data and with human experts performing real-world experiments.",Sunil Gupta|Alistair Shilton|Arun Kumar A|Shannon Ryan|Majid Abdolshah|Hung Le|Santu Rana|Julian Berk|Mahad Rashid|Svetha Venkatesh,,https://arxiv.org/abs/2303.01684v2,https://arxiv.org/pdf/2303.01684v2,,"34 Pages, 7 Figures and 5 Tables",,,cs.LG,cs.LG|cs.AI,https://arxiv.org/pdf/2303.01684v2.pdf
2302.14545v2,2023-02-28T13:10:04Z,2023-11-29 10:20:19,Modern Bayesian Experimental Design,"Bayesian experimental design (BED) provides a powerful and general framework for optimizing the design of experiments. However, its deployment often poses substantial computational challenges that can undermine its practical use. In this review, we outline how recent advances have transformed our ability to overcome these challenges and thus utilize BED effectively, before discussing some key areas for future development in the field.",Tom Rainforth|Adam Foster|Desi R Ivanova|Freddie Bickford Smith,,https://arxiv.org/abs/2302.14545v2,https://arxiv.org/pdf/2302.14545v2,,Accepted for publication in Statistical Science,,,stat.ML,stat.ML|cs.AI|cs.LG|stat.CO,https://arxiv.org/pdf/2302.14545v2.pdf
2302.14015v2,2023-02-27T18:14:13Z,2023-07-13 19:26:18,CO-BED: Information-Theoretic Contextual Optimization via Bayesian Experimental Design,"We formalize the problem of contextual optimization through the lens of Bayesian experimental design and propose CO-BED -- a general, model-agnostic framework for designing contextual experiments using information-theoretic principles. After formulating a suitable information-based objective, we employ black-box variational methods to simultaneously estimate it and optimize the designs in a single stochastic gradient scheme. In addition, to accommodate discrete actions within our framework, we propose leveraging continuous relaxation schemes, which can naturally be integrated into our variational objective. As a result, CO-BED provides a general and automated solution to a wide range of contextual optimization problems. We illustrate its effectiveness in a number of experiments, where CO-BED demonstrates competitive performance even when compared to bespoke, model-specific alternatives.",Desi R. Ivanova|Joel Jennings|Tom Rainforth|Cheng Zhang|Adam Foster,,https://arxiv.org/abs/2302.14015v2,https://arxiv.org/pdf/2302.14015v2,,"Proceedings of the 40th International Conference on Machine Learning (ICML 2023); 9 pages, 7 figures",,,stat.ML,stat.ML|cs.AI|cs.LG|stat.CO,https://arxiv.org/pdf/2302.14015v2.pdf
2302.10607v2,2023-02-21T11:32:59Z,2023-06-02 11:16:33,Differentiable Multi-Target Causal Bayesian Experimental Design,"We introduce a gradient-based approach for the problem of Bayesian optimal experimental design to learn causal models in a batch setting -- a critical component for causal discovery from finite data where interventions can be costly or risky. Existing methods rely on greedy approximations to construct a batch of experiments while using black-box methods to optimize over a single target-state pair to intervene with. In this work, we completely dispose of the black-box optimization techniques and greedy heuristics and instead propose a conceptually simple end-to-end gradient-based optimization procedure to acquire a set of optimal intervention target-state pairs. Such a procedure enables parameterization of the design space to efficiently optimize over a batch of multi-target-state interventions, a setting which has hitherto not been explored due to its complexity. We demonstrate that our proposed method outperforms baselines and existing acquisition strategies in both single-target and multi-target settings across a number of synthetic datasets.",Yashas Annadani|Panagiotis Tigas|Desi R. Ivanova|Andrew Jesson|Yarin Gal|Adam Foster|Stefan Bauer,,https://arxiv.org/abs/2302.10607v2,https://arxiv.org/pdf/2302.10607v2,,Camera-ready version ICML 2023,,,cs.LG,cs.LG|cs.AI|stat.ME,https://arxiv.org/pdf/2302.10607v2.pdf
2302.09046v1,2023-02-17T18:23:48Z,2023-02-17 18:23:48,Design of Experiments with Sequential Randomizations on Multiple Timescales: The Hybrid Experimental Design,"Psychological interventions, especially those leveraging mobile and wireless technologies, often include multiple components that are delivered and adapted on multiple timescales (e.g., coaching sessions adapted monthly based on clinical progress, combined with motivational messages from a mobile device adapted daily based on the person's daily emotional state). The hybrid experimental design (HED) is a new experimental approach that enables researchers to answer scientific questions about the construction of psychological interventions in which components are delivered and adapted on different timescales. These designs involve sequential randomizations of study participants to intervention components, each at an appropriate timescale (e.g., monthly randomization to different intensities of coaching sessions and daily randomization to different forms of motivational messages). The goal of the current manuscript is twofold. The first is to highlight the flexibility of the HED by conceptualizing this experimental approach as a special form of a factorial design in which different factors are introduced at multiple timescales. We also discuss how the structure of the HED can vary depending on the scientific question(s) motivating the study. The second goal is to explain how data from various types of HEDs can be analyzed to answer a variety of scientific questions about the development of multi-component psychological interventions. For illustration we use a completed HED to inform the development of a technology-based weight loss intervention that integrates components that are delivered and adapted on multiple timescales.",Inbal Nahum-Shani|John J. Dziak|Hanna Venera|Angela F. Pfammatter|Bonnie Spring|Walter Dempsey,"Institute for Social Research, University of Michigan|Institute for Health Research and Policy, University of Illinois Chicago|Institute for Social Research, University of Michigan|College of Education, Health, and Human Sciences, University of Tennessee Knoxville|Feinberg School of Medicine, Northwestern University|Institute for Social Research, University of Michigan",https://arxiv.org/abs/2302.09046v1,https://arxiv.org/pdf/2302.09046v1,,"59 pages, 4 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2302.09046v1.pdf
2302.06837v1,2023-02-14T05:23:34Z,2023-02-14 05:23:34,Adaptive design of experiment via normalizing flows for failure probability estimation,"Failure probability estimation problem is an crucial task in engineering. In this work we consider this problem in the situation that the underlying computer models are extremely expensive, which often arises in the practice, and in this setting, reducing the calls of computer model is of essential importance. We formulate the problem of estimating the failure probability with expensive computer models as an sequential experimental design for the limit state (i.e., the failure boundary) and propose a series of efficient adaptive design criteria to solve the design of experiment (DOE). In particular, the proposed method employs the deep neural network (DNN) as the surrogate of limit state function for efficiently reducing the calls of expensive computer experiment. A map from the Gaussian distribution to the posterior approximation of the limit state is learned by the normalizing flows for the ease of experimental design. Three normalizing-flows-based design criteria are proposed in this work for deciding the design locations based on the different assumption of generalization error. The accuracy and performance of the proposed method is demonstrated by both theory and practical examples.",Hongji Wang|Tiexin Guo|Jinglai Li|Hongqiao Wang,,https://arxiv.org/abs/2302.06837v1,https://arxiv.org/pdf/2302.06837v1,,"failure probability, normalizing flows, adaptive design of experiment. arXiv admin note: text overlap with arXiv:1509.04613",,,stat.ME,stat.ME,https://arxiv.org/pdf/2302.06837v1.pdf
2302.05005v1,2023-02-10T01:24:52Z,2023-02-10 01:24:52,Near-Optimal Experimental Design Under the Budget Constraint in Online Platforms,"A/B testing, or controlled experiments, is the gold standard approach to causally compare the performance of algorithms on online platforms. However, conventional Bernoulli randomization in A/B testing faces many challenges such as spillover and carryover effects. Our study focuses on another challenge, especially for A/B testing on two-sided platforms -- budget constraints. Buyers on two-sided platforms often have limited budgets, where the conventional A/B testing may be infeasible to be applied, partly because two variants of allocation algorithms may conflict and lead some buyers to exceed their budgets if they are implemented simultaneously. We develop a model to describe two-sided platforms where buyers have limited budgets. We then provide an optimal experimental design that guarantees small bias and minimum variance. Bias is lower when there is more budget and a higher supply-demand rate. We test our experimental design on both synthetic data and real-world data, which verifies the theoretical results and shows our advantage compared to Bernoulli randomization.",Yongkang Guo|Yuan Yuan|Jinshan Zhang|Yuqing Kong|Zhihua Zhu|Zheng Cai,,https://arxiv.org/abs/2302.05005v1,https://arxiv.org/pdf/2302.05005v1,,,,,cs.LG,cs.LG|cs.PF,https://arxiv.org/pdf/2302.05005v1.pdf
2302.04518v1,2023-02-09T09:25:39Z,2023-02-09 09:25:39,"Introduction To Gaussian Process Regression In Bayesian Inverse Problems, With New ResultsOn Experimental Design For Weighted Error Measures","Bayesian posterior distributions arising in modern applications, including inverse problems in partial differential equation models in tomography and subsurface flow, are often computationally intractable due to the large computational cost of evaluating the data likelihood. To alleviate this problem, we consider using Gaussian process regression to build a surrogate model for the likelihood, resulting in an approximate posterior distribution that is amenable to computations in practice. This work serves as an introduction to Gaussian process regression, in particular in the context of building surrogate models for inverse problems, and presents new insights into a suitable choice of training points. We show that the error between the true and approximate posterior distribution can be bounded by the error between the true and approximate likelihood, measured in the $L^2$-norm weighted by the true posterior, and that efficiently bounding the error between the true and approximate likelihood in this norm suggests choosing the training points in the Gaussian process surrogate model based on the true posterior.",Tapio Helin|Andrew Stuart|Aretha Teckentrup|Konstantinos Zygalakis,,https://arxiv.org/abs/2302.04518v1,https://arxiv.org/pdf/2302.04518v1,,,,,stat.ML,stat.ML|math.NA|math.ST,https://arxiv.org/pdf/2302.04518v1.pdf
2302.02942v2,2023-02-06T17:19:47Z,2024-02-19 11:11:06,Empirical quantification of predictive uncertainty due to model discrepancy by training with an ensemble of experimental designs: an application to ion channel kinetics,"When mathematical biology models are used to make quantitative predictions for clinical or industrial use, it is important that these predictions come with a reliable estimate of their accuracy (uncertainty quantification). Because models of complex biological systems are always large simplifications, model discrepancy arises - where a mathematical model fails to recapitulate the true data generating process. This presents a particular challenge for making accurate predictions, and especially for making accurate estimates of uncertainty in these predictions. Experimentalists and modellers must choose which experimental procedures (protocols) are used to produce data to train their models. We propose to characterise uncertainty owing to model discrepancy with an ensemble of parameter sets, each of which results from training to data from a different protocol. The variability in predictions from this ensemble provides an empirical estimate of predictive uncertainty owing to model discrepancy, even for unseen protocols. We use the example of electrophysiology experiments, which are used to investigate the kinetics of the hERG potassium ion channel. Here, 'information-rich' protocols allow mathematical models to be trained using numerous short experiments performed on the same cell. Typically, assuming independent observational errors and training a model to an individual experiment results in parameter estimates with very little dependence on observational noise. Moreover, parameter sets arising from the same model applied to different experiments often conflict - indicative of model discrepancy. Our methods will help select more suitable mathematical models of hERG for future studies, and will be widely applicable to a range of biological modelling problems.",Joseph G. Shuttleworth|Chon Lok Lei|Dominic G. Whittaker|Monique J. Windley|Adam P. Hill|Simon P. Preston|Gary R. Mirams,,https://arxiv.org/abs/2302.02942v2,https://arxiv.org/pdf/2302.02942v2,https://doi.org/10.1007/s11538-023-01224-6,Final published version with a typographical error in Table 1 (the value of q_6) corrected,"Bulletin of Mathematical Biology, 86(1), 2 (2024)",10.1007/s11538-023-01224-6,stat.CO,stat.CO|math.DS|math.OC|q-bio.QM,https://arxiv.org/pdf/2302.02942v2.pdf
2301.12357v3,2023-01-29T04:33:13Z,2024-03-01 01:24:03,SPEED: Experimental Design for Policy Evaluation in Linear Heteroscedastic Bandits,"In this paper, we study the problem of optimal data collection for policy evaluation in linear bandits. In policy evaluation, we are given a target policy and asked to estimate the expected reward it will obtain when executed in a multi-armed bandit environment. Our work is the first work that focuses on such optimal data collection strategy for policy evaluation involving heteroscedastic reward noise in the linear bandit setting. We first formulate an optimal design for weighted least squares estimates in the heteroscedastic linear bandit setting that reduces the MSE of the value of the target policy. We then use this formulation to derive the optimal allocation of samples per action during data collection. We then introduce a novel algorithm SPEED (Structured Policy Evaluation Experimental Design) that tracks the optimal design and derive its regret with respect to the optimal design. Finally, we empirically validate that SPEED leads to policy evaluation with mean squared error comparable to the oracle strategy and significantly lower than simply running the target policy.",Subhojyoti Mukherjee|Qiaomin Xie|Josiah Hanna|Robert Nowak,,https://arxiv.org/abs/2301.12357v3,https://arxiv.org/pdf/2301.12357v3,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2301.12357v3.pdf
2301.11501v1,2023-01-27T02:33:33Z,2023-01-27 02:33:33,Practical Frequency-Hopping MIMO Joint Radar Communications: Design and Experiment,"Joint radar and communications (JRC) can realize two radio frequency (RF) functions using one set of resources, greatly saving hardware, energy and spectrum for wireless systems needing both functions. Frequency-hopping (FH) MIMO radar is a popular candidate for JRC, as the achieved communication symbol rate can greatly exceed radar pulse repetition frequency. However, practical transceiver imperfections can fail many existing theoretical designs. In this work, we unveil for the first time the non-trivial impact of hardware imperfections on FH-MIMO JRC and analytically model the impact. We also design new waveforms and, accordingly, develop a low-complexity algorithm to jointly estimate the hardware imperfections of unsynchronized receiver. Moreover, employing low-cost software-defined radios and commercial off-the-shelf (COTS) products, we build the first FH-MIMO JRC experiment platform with radar and communications simultaneously validated over the air. Corroborated by simulation and experiment results, the proposed designs achieves high performances for both radar and communications.",Jiangtao Liu|Kai Wu|Tao Su|J. Andrew Zhang,,https://arxiv.org/abs/2301.11501v1,https://arxiv.org/pdf/2301.11501v1,,11 pages; 12 figures,,,eess.SP,eess.SP,https://arxiv.org/pdf/2301.11501v1.pdf
2301.08336v3,2023-01-19T21:51:58Z,2023-12-19 18:40:53,PyOED: An Extensible Suite for Data Assimilation and Model-Constrained Optimal Design of Experiments,"This paper describes PyOED, a highly extensible scientific package that enables developing and testing model-constrained optimal experimental design (OED) for inverse problems. Specifically, PyOED aims to be a comprehensive Python toolkit for model-constrained OED. The package targets scientists and researchers interested in understanding the details of OED formulations and approaches. It is also meant to enable researchers to experiment with standard and innovative OED technologies with a wide range of test problems (e.g., simulation models). OED, inverse problems (e.g., Bayesian inversion), and data assimilation (DA) are closely related research fields, and their formulations overlap significantly. Thus, PyOED is continuously being expanded with a plethora of Bayesian inversion, DA, and OED methods as well as new scientific simulation models, observation error models, and observation operators. These pieces are added such that they can be permuted to enable testing OED methods in various settings of varying complexities. The PyOED core is completely written in Python and utilizes the inherent object-oriented capabilities; however, the current version of PyOED is meant to be extensible rather than scalable. Specifically, PyOED is developed to enable rapid development and benchmarking of OED methods with minimal coding effort and to maximize code reutilization. This paper provides a brief description of the PyOED layout and philosophy and provides a set of exemplary test cases and tutorials to demonstrate the potential of the package.",Abhijit Chowdhary|Shady E. Ahmed|Ahmed Attia,,https://arxiv.org/abs/2301.08336v3,https://arxiv.org/pdf/2301.08336v3,,"22 pages, 8 figures",,,cs.MS,cs.MS,https://arxiv.org/pdf/2301.08336v3.pdf
2211.15860v1,2022-11-29T01:25:29Z,2022-11-29 01:25:29,Bayesian Experimental Design for Symbolic Discovery,"This study concerns the formulation and application of Bayesian optimal experimental design to symbolic discovery, which is the inference from observational data of predictive models taking general functional forms. We apply constrained first-order methods to optimize an appropriate selection criterion, using Hamiltonian Monte Carlo to sample from the prior. A step for computing the predictive distribution, involving convolution, is computed via either numerical integration, or via fast transform methods.",Kenneth L. Clarkson|Cristina Cornelio|Sanjeeb Dash|Joao Goncalves|Lior Horesh|Nimrod Megiddo,,https://arxiv.org/abs/2211.15860v1,https://arxiv.org/pdf/2211.15860v1,,,,,cs.LG,cs.LG|stat.CO,https://arxiv.org/pdf/2211.15860v1.pdf
2211.15053v1,2022-11-28T04:17:35Z,2022-11-28 04:17:35,Distinguishing representational geometries with controversial stimuli: Bayesian experimental design and its application to face dissimilarity judgments,"Comparing representations of complex stimuli in neural network layers to human brain representations or behavioral judgments can guide model development. However, even qualitatively distinct neural network models often predict similar representational geometries of typical stimulus sets. We propose a Bayesian experimental design approach to synthesizing stimulus sets for adjudicating among representational models efficiently. We apply our method to discriminate among candidate neural network models of behavioral face dissimilarity judgments. Our results indicate that a neural network trained to invert a 3D-face-model graphics renderer is more human-aligned than the same architecture trained on identification, classification, or autoencoding. Our proposed stimulus synthesis objective is generally applicable to designing experiments to be analyzed by representational similarity analysis for model comparison.",Tal Golan|Wenxuan Guo|Heiko H. Schütt|Nikolaus Kriegeskorte,,https://arxiv.org/abs/2211.15053v1,https://arxiv.org/pdf/2211.15053v1,,,SVRHM 2022 Workshop @ NeurIPS (Oral),,q-bio.NC,q-bio.NC|cs.AI|cs.NE|stat.AP,https://arxiv.org/pdf/2211.15053v1.pdf
2211.04399v2,2022-11-08T17:42:13Z,2023-11-03 11:44:47,Stability estimates for the expected utility in Bayesian optimal experimental design,We study stability properties of the expected utility function in Bayesian optimal experimental design. We provide a framework for this problem in a non-parametric setting and prove a convergence rate of the expected utility with respect to a likelihood perturbation. This rate is uniform over the design space and its sharpness in the general setting is demonstrated by proving a lower bound in a special case. To make the problem more concrete we proceed by considering non-linear Bayesian inverse problems with Gaussian likelihood and prove that the assumptions set out for the general case are satisfied and regain the stability of the expected utility with respect to perturbations to the observation map. Theoretical convergence rates are demonstrated numerically in three different examples.,Duc-Lam Duong|Tapio Helin|Jose Rodrigo Rojo-Garcia,,https://arxiv.org/abs/2211.04399v2,https://arxiv.org/pdf/2211.04399v2,https://doi.org/10.1088/1361-6420/ad04ec,21 pages; 6 figures,Inverse Problems 39 125008 (2023),10.1088/1361-6420/ad04ec,math.ST,math.ST|math.NA,https://arxiv.org/pdf/2211.04399v2.pdf
2211.02230v1,2022-11-04T02:35:25Z,2022-11-04 02:35:25,Bayesian Sequential Experimental Design for a Partially Linear Model with a Gaussian Process Prior,"We study the problem of sequential experimental design to estimate the parametric component of a partially linear model with a Gaussian process prior. We consider an active learning setting where an experimenter adaptively decides which data to collect to achieve their goal efficiently. The experimenter's goals may vary, such as reducing the classification error probability or improving the accuracy of estimating the parameters of the data generating process. This study aims to improve the accuracy of estimating the parametric component of a partially linear model. Under some assumptions, the parametric component of a partially linear model can be regarded as a causal parameter, the average treatment effect (ATE) or the average causal effect (ACE). We propose a Bayesian sequential experimental design algorithm for a partially linear model with a Gaussian process prior, which is also considered as a sequential experimental design tailored to the estimation of ATE or ACE. We show the effectiveness of the proposed method through numerical experiments based on synthetic and semi-synthetic data.",Shunsuke Horii,,https://arxiv.org/abs/2211.02230v1,https://arxiv.org/pdf/2211.02230v1,,"6 pages, 2 figures, accepted to Neurips workshop GPSMDMS 2022",,,stat.ME,stat.ME,https://arxiv.org/pdf/2211.02230v1.pdf
2210.15576v1,2022-10-27T16:13:48Z,2022-10-27 16:13:48,Regret Bounds and Experimental Design for Estimate-then-Optimize,"In practical applications, data is used to make decisions in two steps: estimation and optimization. First, a machine learning model estimates parameters for a structural model relating decisions to outcomes. Second, a decision is chosen to optimize the structural model's predicted outcome as if its parameters were correctly estimated. Due to its flexibility and simple implementation, this ``estimate-then-optimize'' approach is often used for data-driven decision-making. Errors in the estimation step can lead estimate-then-optimize to sub-optimal decisions that result in regret, i.e., a difference in value between the decision made and the best decision available with knowledge of the structural model's parameters. We provide a novel bound on this regret for smooth and unconstrained optimization problems. Using this bound, in settings where estimated parameters are linear transformations of sub-Gaussian random vectors, we provide a general procedure for experimental design to minimize the regret resulting from estimate-then-optimize. We demonstrate our approach on simple examples and a pandemic control application.",Samuel Tan|Peter I. Frazier,,https://arxiv.org/abs/2210.15576v1,https://arxiv.org/pdf/2210.15576v1,,,,,math.OC,math.OC|stat.ML,https://arxiv.org/pdf/2210.15576v1.pdf
2210.14369v1,2022-10-25T22:29:16Z,2022-10-25 22:29:16,Adaptive Experimental Design and Counterfactual Inference,"Adaptive experimental design methods are increasingly being used in industry as a tool to boost testing throughput or reduce experimentation cost relative to traditional A/B/N testing methods. This paper shares lessons learned regarding the challenges and pitfalls of naively using adaptive experimentation systems in industrial settings where non-stationarity is prevalent, while also providing perspectives on the proper objectives and system specifications in these settings. We developed an adaptive experimental design framework for counterfactual inference based on these experiences, and tested it in a commercial environment.",Tanner Fiez|Sergio Gamez|Arick Chen|Houssam Nassif|Lalit Jain,,https://arxiv.org/abs/2210.14369v1,https://arxiv.org/pdf/2210.14369v1,,"In Workshops of the Conference on Recommender Systems (RecSys), 2022",,,cs.LG,cs.LG|stat.ME,https://arxiv.org/pdf/2210.14369v1.pdf
2210.15345v3,2022-10-25T19:13:20Z,2023-11-17 21:45:53,PopArt: Efficient Sparse Regression and Experimental Design for Optimal Sparse Linear Bandits,"In sparse linear bandits, a learning agent sequentially selects an action and receive reward feedback, and the reward function depends linearly on a few coordinates of the covariates of the actions. This has applications in many real-world sequential decision making problems. In this paper, we propose a simple and computationally efficient sparse linear estimation method called PopArt that enjoys a tighter $\ell_1$ recovery guarantee compared to Lasso (Tibshirani, 1996) in many problems. Our bound naturally motivates an experimental design criterion that is convex and thus computationally efficient to solve. Based on our novel estimator and design criterion, we derive sparse linear bandit algorithms that enjoy improved regret upper bounds upon the state of the art (Hao et al., 2020), especially w.r.t. the geometry of the given action set. Finally, we prove a matching lower bound for sparse linear bandits in the data-poor regime, which closes the gap between upper and lower bounds in prior work.",Kyoungseok Jang|Chicheng Zhang|Kwang-Sung Jun,,https://arxiv.org/abs/2210.15345v3,https://arxiv.org/pdf/2210.15345v3,,"10 pages, 1 figures, published in the 2022 Conference on Neural Information Processing Systems",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2210.15345v3.pdf
2210.13423v2,2022-10-24T17:39:07Z,2022-11-28 22:00:23,Experimental design for causal query estimation in partially observed biomolecular networks,"Estimating a causal query from observational data is an essential task in the analysis of biomolecular networks. Estimation takes as input a network topology, a query estimation method, and observational measurements on the network variables. However, estimations involving many variables can be experimentally expensive, and computationally intractable. Moreover, using the full set of variables can be detrimental, leading to bias, or increasing the variance in the estimation. Therefore, designing an experiment based on a well-chosen subset of network components can increase estimation accuracy, and reduce experimental and computational costs. We propose a simulation-based algorithm for selecting sub-networks that support unbiased estimators of the causal query under a constraint of cost, ranked with respect to the variance of the estimators. The simulations are constructed based on historical experimental data, or based on known properties of the biological system. Three case studies demonstrated the effectiveness of well-chosen network subsets for estimating causal queries from observational data. All the case studies are reproducible and available at https://github.com/srtaheri/Simplified_LVM.",Sara Mohammad-Taheri|Vartika Tewari|Rohan Kapre|Ehsan Rahiminasab|Karen Sachs|Charles Tapley Hoyt|Jeremy Zucker|Olga Vitek,,https://arxiv.org/abs/2210.13423v2,https://arxiv.org/pdf/2210.13423v2,,,,,q-bio.BM,q-bio.BM,https://arxiv.org/pdf/2210.13423v2.pdf
2210.08397v1,2022-10-15T23:28:30Z,2022-10-15 23:28:30,Taxonomy of A Decision Support System for Adaptive Experimental Design in Field Robotics,"Experimental design in field robotics is an adaptive human-in-the-loop decision-making process in which an experimenter learns about system performance and limitations through interactions with a robot in the form of constructed experiments. This can be challenging because of system complexity, the need to operate in unstructured environments, and the competing objectives of maximizing information gain while simultaneously minimizing experimental costs. Based on the successes in other domains, we propose the use of a Decision Support System (DSS) to amplify the human's decision-making abilities, overcome their inherent shortcomings, and enable principled decision-making in field experiments. In this work, we propose common terminology and a six-stage taxonomy of DSSs specifically for adaptive experimental design of more informative tests and reduced experimental costs. We construct and present our taxonomy using examples and trends from DSS literature, including works involving artificial intelligence and Intelligent DSSs. Finally, we identify critical technical gaps and opportunities for future research to direct the scientific community in the pursuit of next-generation DSSs for experimental design.",Jason M. Gregory|Sarah Al-Hussaini|Ali-akbar Agha-mohammadi|Satyandra K. Gupta,,https://arxiv.org/abs/2210.08397v1,https://arxiv.org/pdf/2210.08397v1,,"10 pages, 3 figures, presented at the AI-HRI Symposium at AAAI Fall Symposium Series (FSS) 2022",,,cs.RO,cs.RO|cs.AI|cs.HC,https://arxiv.org/pdf/2210.08397v1.pdf
2210.06891v4,2022-10-13T10:36:24Z,2024-03-17 11:45:52,Experimental Design for Multi-Channel Imaging via Task-Driven Feature Selection,"This paper presents a data-driven, task-specific paradigm for experimental design, to shorten acquisition time, reduce costs, and accelerate the deployment of imaging devices. Current approaches in experimental design focus on model-parameter estimation and require specification of a particular model, whereas in imaging, other tasks may drive the design. Furthermore, such approaches often lead to intractable optimization problems in real-world imaging applications. Here we present a new paradigm for experimental design that simultaneously optimizes the design (set of image channels) and trains a machine-learning model to execute a user-specified image-analysis task. The approach obtains data densely-sampled over the measurement space (many image channels) for a small number of acquisitions, then identifies a subset of channels of prespecified size that best supports the task. We propose a method: TADRED for TAsk-DRiven Experimental Design in imaging, to identify the most informative channel-subset whilst simultaneously training a network to execute the task given the subset. Experiments demonstrate the potential of TADRED in diverse imaging applications: several clinically-relevant tasks in magnetic resonance imaging; and remote sensing and physiological applications of hyperspectral imaging. Results show substantial improvement over classical experimental design, two recent application-specific methods within the new paradigm, and state-of-the-art approaches in supervised feature selection. We anticipate further applications of our approach. Code is available: https://github.com/sbb-gh/experimental-design-multichannel",Stefano B. Blumberg|Paddy J. Slator|Daniel C. Alexander,,https://arxiv.org/abs/2210.06891v4,https://arxiv.org/pdf/2210.06891v4,,Accepted In: International Conference on Learning Representations (ICLR) 2024,,,cs.LG,cs.LG|cs.AI|q-bio.NC,https://arxiv.org/pdf/2210.06891v4.pdf
2210.03283v2,2022-10-07T02:12:34Z,2022-10-20 01:12:29,Design Amortization for Bayesian Optimal Experimental Design,"Bayesian optimal experimental design is a sub-field of statistics focused on developing methods to make efficient use of experimental resources. Any potential design is evaluated in terms of a utility function, such as the (theoretically well-justified) expected information gain (EIG); unfortunately however, under most circumstances the EIG is intractable to evaluate. In this work we build off of successful variational approaches, which optimize a parameterized variational model with respect to bounds on the EIG. Past work focused on learning a new variational model from scratch for each new design considered. Here we present a novel neural architecture that allows experimenters to optimize a single variational model that can estimate the EIG for potentially infinitely many designs. To further improve computational efficiency, we also propose to train the variational model on a significantly cheaper-to-evaluate lower bound, and show empirically that the resulting model provides an excellent guide for more accurate, but expensive to evaluate bounds on the EIG. We demonstrate the effectiveness of our technique on generalized linear models, a class of statistical models that is widely used in the analysis of controlled experiments. Experiments show that our method is able to greatly improve accuracy over existing approximation strategies, and achieve these results with far better sample efficiency.",Noble Kennamer|Steven Walton|Alexander Ihler,,https://arxiv.org/abs/2210.03283v2,https://arxiv.org/pdf/2210.03283v2,,,,,cs.LG,cs.LG|stat.CO|stat.ML|stat.OT,https://arxiv.org/pdf/2210.03283v2.pdf
2209.13126v1,2022-09-27T02:56:08Z,2022-09-27 02:56:08,Design of experiments for the calibration of history-dependent models via deep reinforcement learning and an enhanced Kalman filter,"Experimental data is costly to obtain, which makes it difficult to calibrate complex models. For many models an experimental design that produces the best calibration given a limited experimental budget is not obvious. This paper introduces a deep reinforcement learning (RL) algorithm for design of experiments that maximizes the information gain measured by Kullback-Leibler (KL) divergence obtained via the Kalman filter (KF). This combination enables experimental design for rapid online experiments where traditional methods are too costly. We formulate possible configurations of experiments as a decision tree and a Markov decision process (MDP), where a finite choice of actions is available at each incremental step. Once an action is taken, a variety of measurements are used to update the state of the experiment. This new data leads to a Bayesian update of the parameters by the KF, which is used to enhance the state representation. In contrast to the Nash-Sutcliffe efficiency (NSE) index, which requires additional sampling to test hypotheses for forward predictions, the KF can lower the cost of experiments by directly estimating the values of new data acquired through additional actions. In this work our applications focus on mechanical testing of materials. Numerical experiments with complex, history-dependent models are used to verify the implementation and benchmark the performance of the RL-designed experiments.",Ruben Villarreal|Nikolaos N. Vlassis|Nhon N. Phan|Tommie A. Catanach|Reese E. Jones|Nathaniel A. Trask|Sharlotte L. B. Kramer|WaiChing Sun,,https://arxiv.org/abs/2209.13126v1,https://arxiv.org/pdf/2209.13126v1,,"40 pages, 20 figures",,,cs.LG,cs.LG,https://arxiv.org/pdf/2209.13126v1.pdf
2209.11534v2,2022-09-23T11:47:37Z,2022-12-20 14:13:12,An Interdisciplinary Perspective on Evaluation and Experimental Design for Visual Text Analytics: Position Paper,"Appropriate evaluation and experimental design are fundamental for empirical sciences, particularly in data-driven fields. Due to the successes in computational modeling of languages, for instance, research outcomes are having an increasingly immediate impact on end users. As the gap in adoption by end users decreases, the need increases to ensure that tools and models developed by the research communities and practitioners are reliable, trustworthy, and supportive of the users in their goals. In this position paper, we focus on the issues of evaluating visual text analytics approaches. We take an interdisciplinary perspective from the visualization and natural language processing communities, as we argue that the design and validation of visual text analytics include concerns beyond computational or visual/interactive methods on their own. We identify four key groups of challenges for evaluating visual text analytics approaches (data ambiguity, experimental design, user trust, and ""big picture"" concerns) and provide suggestions for research opportunities from an interdisciplinary perspective.",Kostiantyn Kucher|Nicole Sultanum|Angel Daza|Vasiliki Simaki|Maria Skeppstedt|Barbara Plank|Jean-Daniel Fekete|Narges Mahyar,,https://arxiv.org/abs/2209.11534v2,https://arxiv.org/pdf/2209.11534v2,https://doi.org/10.1109/BELIV57783.2022.00008,"Published in Proceedings of the 2022 IEEE Workshop on Evaluation and Beyond - Methodological Approaches to Visualization (BELIV '22). ACM 2012 CCS: Human-centered computing, Visualization, Visualization design and evaluation methods",,10.1109/BELIV57783.2022.00008,cs.HC,cs.HC|cs.CL,https://arxiv.org/pdf/2209.11534v2.pdf
2209.01885v1,2022-09-05T10:31:48Z,2022-09-05 10:31:48,Explaining the optimistic performance evaluation of newly proposed methods: a cross-design validation experiment,"The constant development of new data analysis methods in many fields of research is accompanied by an increasing awareness that these new methods often perform better in their introductory paper than in subsequent comparison studies conducted by other researchers. We attempt to explain this discrepancy by conducting a systematic experiment that we call ""cross-design validation of methods"". In the experiment, we select two methods designed for the same data analysis task, reproduce the results shown in each paper, and then re-evaluate each method based on the study design (i.e., data sets, competing methods, and evaluation criteria) that was used to show the abilities of the other method. We conduct the experiment for two data analysis tasks, namely cancer subtyping using multi-omic data and differential gene expression analysis. Three of the four methods included in the experiment indeed perform worse when they are evaluated on the new study design, which is mainly caused by the different data sets. Apart from illustrating the many degrees of freedom existing in the assessment of a method and their effect on its performance, our experiment suggests that the performance discrepancies between original and subsequent papers may not only be caused by the non-neutrality of the authors proposing the new method but also by differences regarding the level of expertise and field of application.",Christina Nießl|Sabine Hoffmann|Theresa Ullmann|Anne-Laure Boulesteix,"Institute for Medical Information Processing, Biometry and Epidemiology, LMU Munich, Germany|Institute for Medical Information Processing, Biometry and Epidemiology, LMU Munich, Germany|Institute for Medical Information Processing, Biometry and Epidemiology, LMU Munich, Germany|Institute for Medical Information Processing, Biometry and Epidemiology, LMU Munich, Germany",https://arxiv.org/abs/2209.01885v1,https://arxiv.org/pdf/2209.01885v1,https://doi.org/10.1002/bimj.202200238,,"Biometrical Journal 66(1) (2024), 2200238",10.1002/bimj.202200238,stat.ME,stat.ME,https://arxiv.org/pdf/2209.01885v1.pdf
2209.00490v1,2022-09-01T14:20:56Z,2022-09-01 14:20:56,The Role of Pairwise Matching in Experimental Design for an Incidence Outcome,"We consider the problem of evaluating designs for a two-arm randomized experiment with an incidence (binary) outcome under a nonparametric general response model. Our two main results are that the priori pair matching design of Greevy et al. (2004) is (1) the optimal design as measured by mean squared error among all block designs which includes complete randomization. And (2), this pair-matching design is minimax, i.e. it provides the lowest mean squared error under an adversarial response model. Theoretical results are supported by simulations and clinical trial data.",Adam Kapelner|Abba M. Krieger|David Azriel,,https://arxiv.org/abs/2209.00490v1,https://arxiv.org/pdf/2209.00490v1,,"23 pages, 2 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2209.00490v1.pdf
2208.11743v1,2022-08-24T19:10:11Z,2022-08-24 19:10:11,EEG4Students: An Experimental Design for EEG Data Collection and Machine Learning Analysis,"Using Machine Learning and Deep Learning to predict cognitive tasks from electroencephalography (EEG) signals has been a fast-developing area in Brain-Computer Interfaces (BCI). However, during the COVID-19 pandemic, data collection and analysis could be more challenging. The remote experiment during the pandemic yields several challenges, and we discuss the possible solutions. This paper explores machine learning algorithms that can run efficiently on personal computers for BCI classification tasks. The results show that Random Forest and RBF SVM perform well for EEG classification tasks. Furthermore, we investigate how to conduct such BCI experiments using affordable consumer-grade devices to collect EEG-based BCI data. In addition, we have developed the data collection protocol, EEG4Students, that grants non-experts who are interested in a guideline for such data collection. Our code and data can be found at https://github.com/GuangyaoDou/EEG4Students.",Guangyao Dou|Zheng Zhou,,https://arxiv.org/abs/2208.11743v1,https://arxiv.org/pdf/2208.11743v1,,,,,cs.LG,cs.LG|eess.SP,https://arxiv.org/pdf/2208.11743v1.pdf
2208.09953v1,2022-08-21T19:47:41Z,2022-08-21 19:47:41,Do-AIQ: A Design-of-Experiment Approach to Quality Evaluation of AI Mislabel Detection Algorithm,"The quality of Artificial Intelligence (AI) algorithms is of significant importance for confidently adopting algorithms in various applications such as cybersecurity, healthcare, and autonomous driving. This work presents a principled framework of using a design-of-experimental approach to systematically evaluate the quality of AI algorithms, named as Do-AIQ. Specifically, we focus on investigating the quality of the AI mislabel data algorithm against data poisoning. The performance of AI algorithms is affected by hyperparameters in the algorithm and data quality, particularly, data mislabeling, class imbalance, and data types. To evaluate the quality of the AI algorithms and obtain a trustworthy assessment on the quality of the algorithms, we establish a design-of-experiment framework to construct an efficient space-filling design in a high-dimensional constraint space and develop an effective surrogate model using additive Gaussian process to enable the emulation of the quality of AI algorithms. Both theoretical and numerical studies are conducted to justify the merits of the proposed framework. The proposed framework can set an exemplar for AI algorithm to enhance the AI assurance of robustness, reproducibility, and transparency.",J. Lian|K. Choi|B. Veeramani|A. Hu|L. Freeman|E. Bowen|X. Deng,,https://arxiv.org/abs/2208.09953v1,https://arxiv.org/pdf/2208.09953v1,,,,,stat.ML,stat.ML|cs.LG|stat.AP|stat.ME,https://arxiv.org/pdf/2208.09953v1.pdf
2208.05366v2,2022-08-10T14:28:05Z,2023-06-14 12:51:22,Optimal response surface designs in the presence of model contamination,"Complete reliance on the fitted model in response surface experiments is risky and relaxing this assumption, whether out of necessity or intentionally, requires an experimenter to account for multiple conflicting objectives. This work provides a methodological framework of a compound optimality criterion comprising elementary criteria responsible for: (i) the quality of the confidence region-based inference to be done using the fitted model (DP-/LP-optimality); (ii) improving the ability to test for the lack-of-fit from specified potential model contamination in the form of extra polynomial terms; and (iii) simultaneous minimisation of the variance and bias of the fitted model parameters arising from this misspecification. The latter two components have been newly developed in accordance with the model-independent 'pure error' approach to the error estimation. The compound criteria and design construction were adapted to restricted randomisation frameworks: blocked and multistratum experiments, where the stratum-by-stratum approach was adopted. A point-exchange algorithm was employed for searching for nearly optimal designs. The theoretical work is accompanied by one real and two illustrative examples to explore the relationship patterns among the individual components and characteristics of the optimal designs, demonstrating the attainable compromises across the competing objectives and driving some general practical recommendations.",Olga Egorova|Steven G. Gilmour,,https://arxiv.org/abs/2208.05366v2,https://arxiv.org/pdf/2208.05366v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2208.05366v2.pdf
2208.02726v1,2022-08-04T15:49:30Z,2022-08-04 15:49:30,Algebraic Experimental Design: Theory and Computation,"Over the past several decades, algebraic geometry has provided innovative approaches to biological experimental design that resolved theoretical questions and improved computational efficiency. However, guaranteeing uniqueness and perfect recovery of models are still open problems. In this work we study the problem of uniqueness of wiring diagrams. We use as a modeling framework polynomial dynamical systems and utilize the correspondence between simplicial complexes and square-free monomial ideals from Stanley-Reisner theory to develop theory and construct an algorithm for identifying input data sets $V\subset \mathbb F_p^n$ that are guaranteed to correspond to a unique minimal wiring diagram regardless of the experimental output. We apply the results on a tumor-suppression network mediated by epidermal derived growth factor receptor and demonstrate how careful experimental design decisions can lead to a unique minimal wiring diagram identification. One of the insights of the theoretical work is the connection between the uniqueness of a wiring diagram for a given $V\subset \mathbb F_p^n$ and the uniqueness of the reduced Gröbner basis of the polynomial ideal $I(V)\subset \mathbb F_p[x_1,\ldots, x_n]$. We discuss existing results and introduce a new necessary condition on the points in $V$ for uniqueness of the reduced Gröbner basis of $I(V)$. These results also point to the importance of the relative proximity of the experimental input points on the number of minimal wiring diagrams, which we then study computationally. We find that there is a concrete heuristic way to generate data that tends to result in fewer minimal wiring diagrams.",Elena S. Dimitrova|Cameron H. Fredrickson|Nicholas A. Rondoni|Brandilyn Stigler|Alan Veliz-Cuba,,https://arxiv.org/abs/2208.02726v1,https://arxiv.org/pdf/2208.02726v1,https://doi.org/10.1137/22M1513241,"16 pages, 2 figures (one in color but prints well in b&w), 2 tables","SIAM Journal on Applied Algebra and Geometry, 8 (2024), 284-301",10.1137/22M1513241,math.AG,math.AG|q-bio.MN,https://arxiv.org/pdf/2208.02726v1.pdf
2208.00292v3,2022-07-30T18:58:01Z,2025-06-13 08:07:45,Functional-Coefficient Models for Multivariate Time Series in Designed Experiments: with Applications to Brain Signals,"To study the neurophysiological basis of attention deficit hyperactivity disorder (ADHD), clinicians use electroencephalography (EEG) which record neuronal electrical activity on the cortex. Instead of focusing on single-channel spectral power, a novel framework for investigating interactions (dependence) between channels in the entire network is proposed. Although dependence measures such as coherence and partial directed coherence (PDC) are well explored in studying brain connectivity, these measures only capture linear dependence. Moreover, in designed clinical experiments, these dependence measures are observed to vary across subjects even within a homogeneous group. To address these limitations, we propose the mixed-effects functional-coefficient autoregressive (MXFAR) model which captures between-subject variation by incorporating subject-specific random effects. The advantages of the MXFAR model are the following: (i) it captures potential non-linear dependence between channels; (ii) it is nonparametric and hence flexible and robust to model mis-specification; (iii) it can capture differences between groups when they exist; (iv) it accounts for variation across subjects; (v) the framework easily incorporates well-known inference methods from mixed-effects models; (vi) it can be generalized to accommodate various covariates and factors. Then, we formulate a novel non-linear spectral measure, the functional partial directed coherence (fPDC), to extract dynamic cross-dependence patterns at different frequency oscillations. Finally, we apply the proposed MXFAR-fPDC framework to analyze multichannel EEG signals and report novel findings on altered brain functional networks in ADHD patients.",Paolo Victor Redondo|Raphael Huser|Hernando Ombao,,https://arxiv.org/abs/2208.00292v3,https://arxiv.org/pdf/2208.00292v3,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2208.00292v3.pdf
2207.09183v2,2022-07-19T10:42:59Z,2023-02-23 18:48:47,Evaluation of Combinatorial Optimisation Algorithms for c-Optimal Experimental Designs with Correlated Observations,"We show how combinatorial optimisation algorithms can be applied to the problem of identifying c-optimal experimental designs when there may be correlation between and within experimental units and evaluate the performance of relevant algorithms. We assume the data generating process is a generalised linear mixed model and show that the c-optimal design criterion is a monotone supermodular function amenable to a set of simple minimisation algorithms. We evaluate the performance of three relevant algorithms: the local search, the greedy search, and the reverse greedy search. We show that the local and reverse greedy searches provide comparable performance with the worst design outputs having variance $<10\%$ greater than the best design, across a range of covariance structures. We show that these algorithms perform as well or better than multiplicative methods that generate weights to place on experimental units. We extend these algorithms to identifying moole-robust c-optimal designs.",Samuel I Watson|Yi Pan,,https://arxiv.org/abs/2207.09183v2,https://arxiv.org/pdf/2207.09183v2,,,,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2207.09183v2.pdf
2207.05250v1,2022-07-12T01:20:11Z,2022-07-12 01:20:11,Efficient Real-world Testing of Causal Decision Making via Bayesian Experimental Design for Contextual Optimisation,"The real-world testing of decisions made using causal machine learning models is an essential prerequisite for their successful application. We focus on evaluating and improving contextual treatment assignment decisions: these are personalised treatments applied to e.g. customers, each with their own contextual information, with the aim of maximising a reward. In this paper we introduce a model-agnostic framework for gathering data to evaluate and improve contextual decision making through Bayesian Experimental Design. Specifically, our method is used for the data-efficient evaluation of the regret of past treatment assignments. Unlike approaches such as A/B testing, our method avoids assigning treatments that are known to be highly sub-optimal, whilst engaging in some exploration to gather pertinent information. We achieve this by introducing an information-based design objective, which we optimise end-to-end. Our method applies to discrete and continuous treatments. Comparing our information-theoretic approach to baselines in several simulation studies demonstrates the superior performance of our proposed approach.",Desi R. Ivanova|Joel Jennings|Cheng Zhang|Adam Foster,,https://arxiv.org/abs/2207.05250v1,https://arxiv.org/pdf/2207.05250v1,,"ICML 2022 Workshop on Adaptive Experimental Design and Active Learning in the Real World. 16 pages, 5 figures",,,stat.ML,stat.ML|cs.AI|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/2207.05250v1.pdf
2207.05714v1,2022-07-11T12:45:31Z,2022-07-11 12:45:31,Bayesian Experimental Design for Computed Tomography with the Linearised Deep Image Prior,"We investigate adaptive design based on a single sparse pilot scan for generating effective scanning strategies for computed tomography reconstruction. We propose a novel approach using the linearised deep image prior. It allows incorporating information from the pilot measurements into the angle selection criteria, while maintaining the tractability of a conjugate Gaussian-linear model. On a synthetically generated dataset with preferential directions, linearised DIP design allows reducing the number of scans by up to 30% relative to an equidistant angle baseline.",Riccardo Barbano|Johannes Leuschner|Javier Antorán|Bangti Jin|José Miguel Hernández-Lobato,,https://arxiv.org/abs/2207.05714v1,https://arxiv.org/pdf/2207.05714v1,,,,,cs.CV,cs.CV|cs.LG,https://arxiv.org/pdf/2207.05714v1.pdf
2207.03170v1,2022-07-07T08:59:48Z,2022-07-07 08:59:48,AFFORCE: Actionable Framework for Designing Crowdsourcing Experiences for Older Adults,"In this article we propose a unique framework for designing attractive and engaging crowdsourcing systems for older adults, which is called AFFORCE (Actionable Framework For Crowdsourcing Experiences). We first categorize and map mitigating factors and barriers to crowdsourcing for older adults to finally discuss, present and combine system elements addressing them into an actionable reference framework. This innovative framework is based on our experience with the design of crowdsourcing systems for older adults in exploratory cases and studies, related work, as well as our and related research at the intersection of older adults' use of ICT, crowdsourcing and citizen science.",Kinga Skorupska|Radosław Nielek|Wiesław Kopeć,,https://arxiv.org/abs/2207.03170v1,https://arxiv.org/pdf/2207.03170v1,https://doi.org/10.1145/3486622.3494026,"7 pages, 6 figures, presented at the WI-IAT 2021 Conference",,10.1145/3486622.3494026,cs.HC,cs.HC|cs.CY,https://arxiv.org/pdf/2207.03170v1.pdf
2206.12509v2,2022-06-24T22:46:58Z,2022-07-07 21:37:06,Mutual-Information Based Optimal Experimental Design for Hyperpolarized $^{13}$C-Pyruvate MRI,"A key parameter of interest recovered from hyperpolarized (HP) MRI measurements is the apparent pyruvate-to-lactate exchange rate, $k_{PL}$, for measuring tumor metabolism. This manuscript presents an information-theory-based optimal experimental design (OED) approach that minimizes the uncertainty in the rate parameter, $k_{PL}$, recovered from HP-MRI measurements. Mutual information (MI) is employed to measure the information content of the HP measurements with respect to the first-order exchange kinetics of the pyruvate conversion to lactate. Flip angles of the pulse sequence acquisition are optimized with respect to the mutual information. Further, a spatially varying model (high-fidelity) based on the Block-Torrey equations is proposed and utilized as a control. A time-varying flip angle scheme leads to a higher parameter optimization that can further improve the quantitative value of mutual information over a constant flip angle scheme. However, the constant flip angle scheme leads to the best accuracy and precision when considering inference from noise-corrupted data. For the particular MRI data examined here, pyruvate and lactate flip angles of 35 and 28 degrees, respectively, were the best choice in terms of accuracy and precision of the parameter recovery. Moreover, the recovery of rate parameter $k_{PL}$ from the data generated from the high-fidelity model highlights the influence of diffusion and strength of vascular source on the recovered rate parameter. Since the existing pharmacokinetic models for HP-MRI do not account for spatial variation, the optimized design parameters may not be fully optimal in a more general 3D setting.",Prashant K. Jha|Christopher Walker|Drew Mitchell|J. Tinsley Oden|Dawid Schellingerhout|James A. Bankson|David T. Fuentes,,https://arxiv.org/abs/2206.12509v2,https://arxiv.org/pdf/2206.12509v2,,"20 pages, 8 figures",,,cs.IT,cs.IT|physics.bio-ph,https://arxiv.org/pdf/2206.12509v2.pdf
2206.13503v4,2022-06-24T14:46:19Z,2023-02-21 20:26:40,On the Importance of Application-Grounded Experimental Design for Evaluating Explainable ML Methods,"Most existing evaluations of explainable machine learning (ML) methods rely on simplifying assumptions or proxies that do not reflect real-world use cases; the handful of more robust evaluations on real-world settings have shortcomings in their design, resulting in limited conclusions of methods' real-world utility. In this work, we seek to bridge this gap by conducting a study that evaluates three popular explainable ML methods in a setting consistent with the intended deployment context. We build on a previous study on e-commerce fraud detection and make crucial modifications to its setup relaxing the simplifying assumptions made in the original work that departed from the deployment context. In doing so, we draw drastically different conclusions from the earlier work and find no evidence for the incremental utility of the tested methods in the task. Our results highlight how seemingly trivial experimental design choices can yield misleading conclusions, with lessons about the necessity of not only evaluating explainable ML methods using tasks, data, users, and metrics grounded in the intended deployment contexts but also developing methods tailored to specific applications. In addition, we believe the design of this experiment can serve as a template for future study designs evaluating explainable ML methods in other real-world contexts.",Kasun Amarasinghe|Kit T. Rodolfa|Sérgio Jesus|Valerie Chen|Vladimir Balayan|Pedro Saleiro|Pedro Bizarro|Ameet Talwalkar|Rayid Ghani,,https://arxiv.org/abs/2206.13503v4,https://arxiv.org/pdf/2206.13503v4,,,,,cs.LG,cs.LG|cs.HC,https://arxiv.org/pdf/2206.13503v4.pdf
2206.07532v2,2022-06-15T13:29:15Z,2022-12-14 03:27:47,Current state and prospects of R-packages for the design of experiments,"Re-running an experiment is generally costly and, in some cases, impossible due to limited resources; therefore, the design of an experiment plays a critical role in increasing the quality of experimental data. In this paper, we describe the current state of R-packages for the design of experiments through an exploratory data analysis of package downloads, package metadata, and a comparison of characteristics with other topics. We observed that experimental designs in practice appear to be sufficiently manufactured by a small number of packages, and the development of experimental designs often occurs in silos. We also discuss the interface designs of widely utilized R packages in the field of experimental design and discuss their future prospects for advancing the field in practice.",Emi Tanaka|Dewi Amaliah,,https://arxiv.org/abs/2206.07532v2,https://arxiv.org/pdf/2206.07532v2,,"14 pages, 8 figures, 1 supplementary material",,,stat.OT,stat.OT|stat.CO,https://arxiv.org/pdf/2206.07532v2.pdf
2205.13698v2,2022-05-27T01:23:11Z,2022-11-28 23:05:13,Characterizing the robustness of Bayesian adaptive experimental designs to active learning bias,"Bayesian adaptive experimental design is a form of active learning, which chooses samples to maximize the information they give about uncertain parameters. Prior work has shown that other forms of active learning can suffer from active learning bias, where unrepresentative sampling leads to inconsistent parameter estimates. We show that active learning bias can also afflict Bayesian adaptive experimental design, depending on model misspecification. We analyze the case of estimating a linear model, and show that worse misspecification implies more severe active learning bias. At the same time, model classes incorporating more ""noise"" - i.e., specifying higher inherent variance in observations - suffer less from active learning bias. Finally, we demonstrate empirically that insights from the linear model can predict the presence and degree of active learning bias in nonlinear contexts, namely in a (simulated) preference learning experiment.",Sabina J. Sloman|Daniel M. Oppenheimer|Stephen B. Broomell|Cosma Rohilla Shalizi,,https://arxiv.org/abs/2205.13698v2,https://arxiv.org/pdf/2205.13698v2,,,,,stat.ME,stat.ME|stat.ML,https://arxiv.org/pdf/2205.13698v2.pdf
2205.13627v3,2022-05-26T20:56:25Z,2023-01-15 21:59:24,Experimental Design for Linear Functionals in Reproducing Kernel Hilbert Spaces,"Optimal experimental design seeks to determine the most informative allocation of experiments to infer an unknown statistical quantity. In this work, we investigate the optimal design of experiments for {\em estimation of linear functionals in reproducing kernel Hilbert spaces (RKHSs)}. This problem has been extensively studied in the linear regression setting under an estimability condition, which allows estimating parameters without bias. We generalize this framework to RKHSs, and allow for the linear functional to be only approximately inferred, i.e., with a fixed bias. This scenario captures many important modern applications, such as estimation of gradient maps, integrals, and solutions to differential equations. We provide algorithms for constructing bias-aware designs for linear functionals. We derive non-asymptotic confidence sets for fixed and adaptive designs under sub-Gaussian noise, enabling us to certify estimation with bounded error with high probability.",Mojmír Mutný|Andreas Krause,,https://arxiv.org/abs/2205.13627v3,https://arxiv.org/pdf/2205.13627v3,,,NeurIPS 2022,,cs.AI,cs.AI|math.ST,https://arxiv.org/pdf/2205.13627v3.pdf
2205.09914v1,2022-05-20T01:07:41Z,2022-05-20 01:07:41,Robust Expected Information Gain for Optimal Bayesian Experimental Design Using Ambiguity Sets,"The ranking of experiments by expected information gain (EIG) in Bayesian experimental design is sensitive to changes in the model's prior distribution, and the approximation of EIG yielded by sampling will have errors similar to the use of a perturbed prior. We define and analyze \emph{robust expected information gain} (REIG), a modification of the objective in EIG maximization by minimizing an affine relaxation of EIG over an ambiguity set of distributions that are close to the original prior in KL-divergence. We show that, when combined with a sampling-based approach to estimating EIG, REIG corresponds to a `log-sum-exp' stabilization of the samples used to estimate EIG, meaning that it can be efficiently implemented in practice. Numerical tests combining REIG with variational nested Monte Carlo (VNMC), adaptive contrastive estimation (ACE) and mutual information neural estimation (MINE) suggest that in practice REIG also compensates for the variability of under-sampled estimators.",Jinwoo Go|Tobin Isaac,,https://arxiv.org/abs/2205.09914v1,https://arxiv.org/pdf/2205.09914v1,,"The 38th Conference on Uncertainty in Artificial Intelligence, 2022",,,stat.ML,stat.ML|cs.AI|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/2205.09914v1.pdf
2205.02726v2,2022-05-05T15:57:06Z,2025-05-13 20:33:51,Asymptotic Efficiency Bounds for a Class of Experimental Designs,"We consider an experimental design setting in which units are assigned to treatment after being sampled sequentially from an infinite population. We derive asymptotic efficiency bounds that apply to data from any experiment that assigns treatment as a (possibly randomized) function of covariates and past outcome data, including stratification on covariates and adaptive designs. For estimating the average treatment effect of a binary treatment, our results show that no further first order asymptotic efficiency improvement is possible relative to an estimator that achieves the Hahn (1998) bound in an experimental design where the propensity score is chosen to minimize this bound. Our results also apply to settings with multiple treatments with possible constraints on treatment, as well as covariate based sampling of a single outcome.",Timothy B. Armstrong,,https://arxiv.org/abs/2205.02726v2,https://arxiv.org/pdf/2205.02726v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2205.02726v2.pdf
2205.02232v3,2022-05-04T13:19:04Z,2023-08-17 19:30:13,Experimental Design for Causal Effect Identification,"Pearl's do calculus is a complete axiomatic approach to learn the identifiable causal effects from observational data. When such an effect is not identifiable, it is necessary to perform a collection of often costly interventions in the system to learn the causal effect. In this work, we consider the problem of designing the collection of interventions with the minimum cost to identify the desired effect. First, we prove that this problem is NP-hard, and subsequently propose an algorithm that can either find the optimal solution or a logarithmic-factor approximation of it. This is done by establishing a connection between our problem and the minimum hitting set problem. Additionally, we propose several polynomial-time heuristic algorithms to tackle the computational complexity of the problem. Although these algorithms could potentially stumble on sub-optimal solutions, our simulations show that they achieve small regrets on random graphs.",Sina Akbari|Jalal Etesami|Negar Kiyavash,,https://arxiv.org/abs/2205.02232v3,https://arxiv.org/pdf/2205.02232v3,,"53 pages, 13 figures, extending the findings of our ICML2022 paper",,,cs.LG,cs.LG|cs.AI|stat.ME,https://arxiv.org/pdf/2205.02232v3.pdf
2205.01281v1,2022-05-03T03:08:11Z,2022-05-03 03:08:11,A correlation structure for the analysis of Gaussian and non-Gaussian responses in crossover experimental designs with repeated measures,"In this study, we propose a family of correlation structures for crossover designs with repeated measures for both, Gaussian and non-Gaussian responses using generalized estimating equations (GEE). The structure considers two matrices: one that models between-period correlation and another one that models within-period correlation. The overall correlation matrix, which is used to build the GEE, corresponds to the Kronecker between these matrices. A procedure to estimate the parameters of the correlation matrix is proposed, its statistical properties are studied and a comparison with standard models using a single correlation matrix is carried out. A simulation study showed a superior performance of the proposed structure in terms of the quasi-likelihood criterion, efficiency, and the capacity to explain complex correlation phenomena/patterns in longitudinal data from crossover designs",N. A. Cruz|O. O. Melo|C. A. Martinez,,https://arxiv.org/abs/2205.01281v1,https://arxiv.org/pdf/2205.01281v1,https://doi.org/10.1007/s00362-022-01391-z,"29 pages, 5 tables, 5 figures. Stat Papers (2023)",,10.1007/s00362-022-01391-z,stat.ME,stat.ME,https://arxiv.org/pdf/2205.01281v1.pdf
2204.12695v2,2022-04-27T04:38:26Z,2023-06-19 06:59:00,Nondominated-Solution-based Multi-objective Greedy Sensor Selection for Optimal Design of Experiments,"In this study, a nondominated-solution-based multi-objective greedy method is proposed and applied to a sensor selection problem based on the multiple indices of the optimal design of experiments. The proposed method simultaneously considers multiple set functions and applies the idea of Pareto ranking for the selection of sets. Specifically, a new index is iteratively added to the nondominated solutions of sets, and the multi-objective functions are evaluated for new sets. The nondominated solutions are selected from the examined solutions, and the next sets are then considered. With this procedure, the multi-objective optimization of multiple set functions can be conducted with reasonable computational costs. This paper defines a new class of greedy algorithms which includes the proposed nondominated-solution-based multi-objective greedy algorithm and the group greedy algorithm, and the characteristics of those algorithms are theoretically discussed. Then, the proposed method is applied to the sensor selection problem and its performance is evaluated. The results of the test case show that the proposed method not only gives the Pareto-optimal front of the multi-objective optimization problem but also produces sets of sensors in terms of D-, A-, and E-optimality, that are superior to the sets selected by pure greedy methods that consider only a single objective function.",Kumi Nakai|Yasuo Sasaki|Takayuki Nagata|Keigo Yamada|Yuji Saito|Taku Nonomura,,https://arxiv.org/abs/2204.12695v2,https://arxiv.org/pdf/2204.12695v2,https://doi.org/10.1109/TSP.2022.3224643,Accepted manuscript for publication in IEEE Transactions on Signal Processing,"IEEE Transactions on Signal Processing, vol. 70, pp. 5694-5707, 2022",10.1109/TSP.2022.3224643,eess.SP,eess.SP|math.OC,https://arxiv.org/pdf/2204.12695v2.pdf
2204.08743v3,2022-04-19T08:36:40Z,2022-04-25 08:36:49,On the Use of Causal Graphical Models for Designing Experiments in the Automotive Domain,"Randomized field experiments are the gold standard for evaluating the impact of software changes on customers. In the online domain, randomization has been the main tool to ensure exchangeability. However, due to the different deployment conditions and the high dependence on the surrounding environment, designing experiments for automotive software needs to consider a higher number of restricted variables to ensure conditional exchangeability. In this paper, we show how at Volvo Cars we utilize causal graphical models to design experiments and explicitly communicate the assumptions of experiments. These graphical models are used to further assess the experiment validity, compute direct and indirect causal effects, and reason on the transportability of the causal conclusions.",David Issa Mattos|Yuchu Liu,,https://arxiv.org/abs/2204.08743v3,https://arxiv.org/pdf/2204.08743v3,,In submission,,,cs.SE,cs.SE,https://arxiv.org/pdf/2204.08743v3.pdf
2204.06687v1,2022-04-14T01:12:26Z,2022-04-14 01:12:26,Designing Experiments Toward Shrinkage Estimation,"We consider how increasingly available observational data can be used to improve the design of randomized controlled trials (RCTs). We seek to design a prospective RCT, with the intent of using an Empirical Bayes estimator to shrink the causal estimates from our trial toward causal estimates obtained from an observational study. We ask: how might we design the experiment to better complement the observational study in this setting?
  We propose using an estimator that shrinks each component of the RCT causal estimator toward its observational counterpart by a factor proportional to its variance. First, we show that the risk of this estimator can be computed efficiently via numerical integration. We then propose algorithms for determining the best allocation of units to strata (the best ""design""). We consider three options: Neyman allocation; a ""naive"" design assuming no unmeasured confounding in the observational study; and a ""defensive"" design accounting for the imperfect parameter estimates we would obtain from the observational study with unmeasured confounding.
  We also incorporate results from sensitivity analysis to establish guardrails on the designs, so that our experiment could be reasonably analyzed with and without shrinkage. We demonstrate the superiority of these experimental designs with a simulation study involving causal inference on a rare, binary outcome.",Evan T. R. Rosenman|Luke Miratrix,,https://arxiv.org/abs/2204.06687v1,https://arxiv.org/pdf/2204.06687v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2204.06687v1.pdf
2204.05218v1,2022-04-11T16:01:50Z,2022-04-11 16:01:50,An Optimal Experimental Design Approach for Light Configurations in Photometric Stereo,"This paper presents a technique for finding the surface normal of an object from a set of images obtained under different lighting positions. The method presented is based on the principles of Photometric Stereo (PS) combined with Optimum Experimental Design (OED) and Parameter Estimation (PE). Unclear by the approach of photometric stereo, and many models based thereon, is how to position the light sources. So far, this is done by using heuristic approaches this leads to suboptimal and non-data driven positioning of the light sources. But what if the optimal positions of the light sources are calculated for photometric stereo? To this end, in this contribution, the effect of positioning the light sources on the quality of the normal vector for PS is evaluated. Furthermore, a new approach in this direction is derived and formulated. For the calculation of the surface normal of a Lambertian surface, the approach based on calibrated photometric stereo; for the estimation the optimal position of the light sources the approach is premised on parameter estimation and optimum experimental design. The approach is tested using synthetic and real-data. Based on results it can be seen that the surface normal estimated with the new method is more detailed than with conventional methods.",Hamza Gardi|Sebastian F. Walter|Christoph S. Garbe,,https://arxiv.org/abs/2204.05218v1,https://arxiv.org/pdf/2204.05218v1,,"16 pages, 11 figures",,,cs.CV,cs.CV|cs.GR,https://arxiv.org/pdf/2204.05218v1.pdf
2203.14011v2,2022-03-26T07:32:57Z,2022-09-16 05:15:04,Approximations for Optimal Experimental Design in Power System Parameter Estimation,"This paper is about computationally tractable methods for power system parameter estimation and Optimal Experiment Design (OED). Here, the main motivation is that OED has the potential to significantly increase the accuracy of power system parameter estimates, for example, if only a few batches of data are available. The problem is, however, that solving the exact OED problem for larger power grids turns out to be computationally expensive and, in many cases, even computationally intractable. Therefore, the present paper proposes three numerical approximation techniques, which increase the computational tractability of OED for power systems. These approximation techniques are bench-marked on a 5-bus and a 14-bus case studies.",Xu Du|Alexander Engelmann|Timm Faulwasser|Boris Houska,,https://arxiv.org/abs/2203.14011v2,https://arxiv.org/pdf/2203.14011v2,,,,,eess.SY,eess.SY,https://arxiv.org/pdf/2203.14011v2.pdf
2203.07120v4,2022-03-14T14:08:46Z,2023-04-11 05:47:05,Neural Message Passing for Objective-Based Uncertainty Quantification and Optimal Experimental Design,"Various real-world scientific applications involve the mathematical modeling of complex uncertain systems with numerous unknown parameters. Accurate parameter estimation is often practically infeasible in such systems, as the available training data may be insufficient and the cost of acquiring additional data may be high. In such cases, based on a Bayesian paradigm, we can design robust operators retaining the best overall performance across all possible models and design optimal experiments that can effectively reduce uncertainty to enhance the performance of such operators maximally. While objective-based uncertainty quantification (objective-UQ) based on MOCU (mean objective cost of uncertainty) provides an effective means for quantifying uncertainty in complex systems, the high computational cost of estimating MOCU has been a challenge in applying it to real-world scientific/engineering problems. In this work, we propose a novel scheme to reduce the computational cost for objective-UQ via MOCU based on a data-driven approach. We adopt a neural message-passing model for surrogate modeling, incorporating a novel axiomatic constraint loss that penalizes an increase in the estimated system uncertainty. As an illustrative example, we consider the optimal experimental design (OED) problem for uncertain Kuramoto models, where the goal is to predict the experiments that can most effectively enhance robust synchronization performance through uncertainty reduction. We show that our proposed approach can accelerate MOCU-based OED by four to five orders of magnitude, without any visible performance loss compared to the state-of-the-art. The proposed approach applies to general OED tasks, beyond the Kuramoto model.",Qihua Chen|Xuejin Chen|Hyun-Myung Woo|Byung-Jun Yoon,,https://arxiv.org/abs/2203.07120v4,https://arxiv.org/pdf/2203.07120v4,https://doi.org/10.1016/j.engappai.2023.106171,"14 pages, 5 figures, accepted by Engineering Applications of Artificial Intelligence",,10.1016/j.engappai.2023.106171,cs.LG,cs.LG|math.OC,https://arxiv.org/pdf/2203.07120v4.pdf
2203.04272v1,2022-03-08T18:47:01Z,2022-03-08 18:47:01,Policy-Based Bayesian Experimental Design for Non-Differentiable Implicit Models,"For applications in healthcare, physics, energy, robotics, and many other fields, designing maximally informative experiments is valuable, particularly when experiments are expensive, time-consuming, or pose safety hazards. While existing approaches can sequentially design experiments based on prior observation history, many of these methods do not extend to implicit models, where simulation is possible but computing the likelihood is intractable. Furthermore, they often require either significant online computation during deployment or a differentiable simulation system. We introduce Reinforcement Learning for Deep Adaptive Design (RL-DAD), a method for simulation-based optimal experimental design for non-differentiable implicit models. RL-DAD extends prior work in policy-based Bayesian Optimal Experimental Design (BOED) by reformulating it as a Markov Decision Process with a reward function based on likelihood-free information lower bounds, which is used to learn a policy via deep reinforcement learning. The learned design policy maps prior histories to experiment designs offline and can be quickly deployed during online execution. We evaluate RL-DAD and find that it performs competitively with baselines on three benchmarks.",Vincent Lim|Ellen Novoseller|Jeffrey Ichnowski|Huang Huang|Ken Goldberg,,https://arxiv.org/abs/2203.04272v1,https://arxiv.org/pdf/2203.04272v1,,"15 pages, 3 figures",,,cs.LG,cs.LG|cs.AI|stat.ME,https://arxiv.org/pdf/2203.04272v1.pdf
2203.02025v1,2022-03-03T21:19:37Z,2022-03-03 21:19:37,Online Balanced Experimental Design,"e consider the experimental design problem in an online environment, an important practical task for reducing the variance of estimates in randomized experiments which allows for greater precision, and in turn, improved decision making. In this work, we present algorithms that build on recent advances in online discrepancy minimization which accommodate both arbitrary treatment probabilities and multiple treatments. The proposed algorithms are computational efficient, minimize covariate imbalance, and include randomization which enables robustness to misspecification. We provide worst case bounds on the expected mean squared error of the causal estimate and show that the proposed estimator is no worse than an implicit ridge regression, which are within a logarithmic factor of the best known results for offline experimental design. We conclude with a detailed simulation study showing favorable results relative to complete randomization as well as to offline methods for experimental design with time complexities exceeding our algorithm.",David Arbour|Drew Dimmery|Tung Mai|Anup Rao,,https://arxiv.org/abs/2203.02025v1,https://arxiv.org/pdf/2203.02025v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2203.02025v1.pdf
2203.02016v3,2022-03-03T20:59:04Z,2022-10-21 17:15:00,"Interventions, Where and How? Experimental Design for Causal Models at Scale","Causal discovery from observational and interventional data is challenging due to limited data and non-identifiability: factors that introduce uncertainty in estimating the underlying structural causal model (SCM). Selecting experiments (interventions) based on the uncertainty arising from both factors can expedite the identification of the SCM. Existing methods in experimental design for causal discovery from limited data either rely on linear assumptions for the SCM or select only the intervention target. This work incorporates recent advances in Bayesian causal discovery into the Bayesian optimal experimental design framework, allowing for active causal discovery of large, nonlinear SCMs while selecting both the interventional target and the value. We demonstrate the performance of the proposed method on synthetic graphs (Erdos-Rènyi, Scale Free) for both linear and nonlinear SCMs as well as on the \emph{in-silico} single-cell gene regulatory network dataset, DREAM.",Panagiotis Tigas|Yashas Annadani|Andrew Jesson|Bernhard Schölkopf|Yarin Gal|Stefan Bauer,,https://arxiv.org/abs/2203.02016v3,https://arxiv.org/pdf/2203.02016v3,,Presented at the thirty-sixth Conference on Neural Information Processing Systems (2022),,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2203.02016v3.pdf
2202.07472v1,2022-02-14T04:29:04Z,2022-02-14 04:29:04,Sequential Bayesian experimental designs via reinforcement learning,"Bayesian experimental design (BED) has been used as a method for conducting efficient experiments based on Bayesian inference. The existing methods, however, mostly focus on maximizing the expected information gain (EIG); the cost of experiments and sample efficiency are often not taken into account. In order to address this issue and enhance practical applicability of BED, we provide a new approach Sequential Experimental Design via Reinforcement Learning to construct BED in a sequential manner by applying reinforcement learning in this paper. Here, reinforcement learning is a branch of machine learning in which an agent learns a policy to maximize its reward by interacting with the environment. The characteristics of interacting with the environment are similar to the sequential experiment, and reinforcement learning is indeed a method that excels at sequential decision making.
  By proposing a new real-world-oriented experimental environment, our approach aims to maximize the EIG while keeping the cost of experiments and sample efficiency in mind simultaneously. We conduct numerical experiments for three different examples. It is confirmed that our method outperforms the existing methods in various indices such as the EIG and sampling efficiency, indicating that our proposed method and experimental environment can make a significant contribution to application of BED to the real world.",Hikaru Asano,,https://arxiv.org/abs/2202.07472v1,https://arxiv.org/pdf/2202.07472v1,,Bachelor thesis,,,cs.LG,cs.LG|stat.ME,https://arxiv.org/pdf/2202.07472v1.pdf
2202.06416v1,2022-02-13T21:43:15Z,2022-02-13 21:43:15,State-of-the-Art Review of Design of Experiments for Physics-Informed Deep Learning,"This paper presents a comprehensive review of the design of experiments used in the surrogate models. In particular, this study demonstrates the necessity of the design of experiment schemes for the Physics-Informed Neural Network (PINN), which belongs to the supervised learning class. Many complex partial differential equations (PDEs) do not have any analytical solution; only numerical methods are used to solve the equations, which is computationally expensive. In recent decades, PINN has gained popularity as a replacement for numerical methods to reduce the computational budget. PINN uses physical information in the form of differential equations to enhance the performance of the neural networks. Though it works efficiently, the choice of the design of experiment scheme is important as the accuracy of the predicted responses using PINN depends on the training data. In this study, five different PDEs are used for numerical purposes, i.e., viscous Burger's equation, Shrödinger equation, heat equation, Allen-Cahn equation, and Korteweg-de Vries equation. A comparative study is performed to establish the necessity of the selection of a DoE scheme. It is seen that the Hammersley sampling-based PINN performs better than other DoE sample strategies.",Sourav Das|Solomon Tesfamariam,,https://arxiv.org/abs/2202.06416v1,https://arxiv.org/pdf/2202.06416v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2202.06416v1.pdf
2202.03312v1,2022-02-07T15:56:50Z,2022-02-07 15:56:50,Sensitivity driven experimental design to facilitate control of dynamical systems,"Control of nonlinear dynamical systems is a complex and multifaceted process. Essential elements of many engineering systems include high fidelity physics-based modeling, offline trajectory planning, feedback control design, and data acquisition strategies to reduce uncertainties. This article proposes an optimization centric perspective which couples these elements in a cohesive framework. We introduce a novel use of hyper-differential sensitivity analysis to understand the sensitivity of feedback controllers to parametric uncertainty in physics-based models used for trajectory planning. These sensitivities provide a foundation to define an optimal experimental design which seeks to acquire data most relevant in reducing demand on the feedback controller. Our proposed framework is illustrated on the Zermelo navigation problem and a hypersonic trajectory control problem using data from NASA's X-43 hypersonic flight tests.",Joseph Hart|Bart van Bloemen Waanders|Lisa Hood|Julie Parish,,https://arxiv.org/abs/2202.03312v1,https://arxiv.org/pdf/2202.03312v1,,23 pages,,,math.OC,math.OC,https://arxiv.org/pdf/2202.03312v1.pdf
2202.02407v1,2022-02-04T21:56:40Z,2022-02-04 21:56:40,An Experimental Design Approach for Regret Minimization in Logistic Bandits,"In this work we consider the problem of regret minimization for logistic bandits. The main challenge of logistic bandits is reducing the dependence on a potentially large problem dependent constant $κ$ that can at worst scale exponentially with the norm of the unknown parameter $θ_{\ast}$. Abeille et al. (2021) have applied self-concordance of the logistic function to remove this worst-case dependence providing regret guarantees like $O(d\log^2(κ)\sqrt{\dotμT}\log(|\mathcal{X}|))$ where $d$ is the dimensionality, $T$ is the time horizon, and $\dotμ$ is the variance of the best-arm. This work improves upon this bound in the fixed arm setting by employing an experimental design procedure that achieves a minimax regret of $O(\sqrt{d \dotμT\log(|\mathcal{X}|)})$. Our regret bound in fact takes a tighter instance (i.e., gap) dependent regret bound for the first time in logistic bandits. We also propose a new warmup sampling algorithm that can dramatically reduce the lower order term in the regret in general and prove that it can replace the lower order term dependency on $κ$ to $\log^2(κ)$ for some instances. Finally, we discuss the impact of the bias of the MLE on the logistic bandit problem, providing an example where $d^2$ lower order regret (cf., it is $d$ for linear bandits) may not be improved as long as the MLE is used and how bias-corrected estimators may be used to make it closer to $d$.",Blake Mason|Kwang-Sung Jun|Lalit Jain,,https://arxiv.org/abs/2202.02407v1,https://arxiv.org/pdf/2202.02407v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2202.02407v1.pdf
2202.00821v3,2022-02-02T00:23:05Z,2022-06-17 07:37:16,Optimizing Sequential Experimental Design with Deep Reinforcement Learning,"Bayesian approaches developed to solve the optimal design of sequential experiments are mathematically elegant but computationally challenging. Recently, techniques using amortization have been proposed to make these Bayesian approaches practical, by training a parameterized policy that proposes designs efficiently at deployment time. However, these methods may not sufficiently explore the design space, require access to a differentiable probabilistic model and can only optimize over continuous design spaces. Here, we address these limitations by showing that the problem of optimizing policies can be reduced to solving a Markov decision process (MDP). We solve the equivalent MDP with modern deep reinforcement learning techniques. Our experiments show that our approach is also computationally efficient at deployment time and exhibits state-of-the-art performance on both continuous and discrete design spaces, even when the probabilistic model is a black box.",Tom Blau|Edwin V. Bonilla|Iadine Chades|Amir Dezfouli,,https://arxiv.org/abs/2202.00821v3,https://arxiv.org/pdf/2202.00821v3,,,International Conference on Machine Learning (2022),,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2202.00821v3.pdf
2201.08572v2,2022-01-21T07:15:41Z,2022-01-24 03:27:03,Research and experimental design of Astrojax double balls trajectory based on double pendulum system,"Based on the double pendulum and Lagrange equation, the moving particles are captured by a binocular three-dimensional capture camera. Two trajectory models of Astrojax and the relationship between trajectory empirical formula and parameters are established. Through research, the calculated trajectory of this formula and related parameters fit well with the actual measured trajectory, and can accurately predict and change the trajectory of the model. The equipment and materials required in the experiment are simple and easy to obtain, and the experimental theme is relatively interesting and novel, which can be applied as an extended experiment in college physics experiment course, so that students can understand the motion characteristics of the double pendulum and learn physics from life. The designing experiment can not only improve students' interest in learning, but also broaden their knowledge and cultivate their practical ability.",Bin Duan|Zihao Bai|Yulong Zhang|Qingyuan Zhang|Sixing Fang|Bohui Shao,,https://arxiv.org/abs/2201.08572v2,https://arxiv.org/pdf/2201.08572v2,,Comments updated: The last author is the corresponding author; The paper is in Chinese language,,,physics.ed-ph,physics.ed-ph,https://arxiv.org/pdf/2201.08572v2.pdf
2201.07925v2,2022-01-20T00:15:45Z,2022-09-06 17:31:24,Large-scale Bayesian optimal experimental design with derivative-informed projected neural network,"We address the solution of large-scale Bayesian optimal experimental design (OED) problems governed by partial differential equations (PDEs) with infinite-dimensional parameter fields. The OED problem seeks to find sensor locations that maximize the expected information gain (EIG) in the solution of the underlying Bayesian inverse problem. Computation of the EIG is usually prohibitive for PDE-based OED problems. To make the evaluation of the EIG tractable, we approximate the (PDE-based) parameter-to-observable map with a derivative-informed projected neural network (DIPNet) surrogate, which exploits the geometry, smoothness, and intrinsic low-dimensionality of the map using a small and dimension-independent number of PDE solves. The surrogate is then deployed within a greedy algorithm-based solution of the OED problem such that no further PDE solves are required. We analyze the EIG approximation error in terms of the generalization error of the DIPNet and show they are of the same order. Finally, the efficiency and accuracy of the method are demonstrated via numerical experiments on OED problems governed by inverse scattering and inverse reactive transport with up to 16,641 uncertain parameters and 100 experimental design variables, where we observe up to three orders of magnitude speedup relative to a reference double loop Monte Carlo method.",Keyi Wu|Thomas O'Leary-Roseberry|Peng Chen|Omar Ghattas,,https://arxiv.org/abs/2201.07925v2,https://arxiv.org/pdf/2201.07925v2,,,,,math.NA,math.NA|math.OC,https://arxiv.org/pdf/2201.07925v2.pdf
2201.04830v1,2022-01-13T08:20:12Z,2022-01-13 08:20:12,Experimental Design Networks: A Paradigm for Serving Heterogeneous Learners under Networking Constraints,"Significant advances in edge computing capabilities enable learning to occur at geographically diverse locations. In general, the training data needed in those learning tasks are not only heterogeneous but also not fully generated locally. In this paper, we propose an experimental design network paradigm, wherein learner nodes train possibly different Bayesian linear regression models via consuming data streams generated by data source nodes over a network. We formulate this problem as a social welfare optimization problem in which the global objective is defined as the sum of experimental design objectives of individual learners, and the decision variables are the data transmission strategies subject to network constraints. We first show that, assuming Poisson data streams, the global objective is a continuous DR-submodular function. We then propose a Frank-Wolfe type algorithm that outputs a solution within a 1-1/e factor from the optimal. Our algorithm contains a novel gradient estimation component which is carefully designed based on Poisson tail bounds and sampling. Finally, we complement our theoretical findings through extensive experiments. Our numerical evaluation shows that the proposed algorithm outperforms several baseline algorithms both in maximizing the global objective and in the quality of the trained models.",Yuezhou Liu|Yuanyuan Li|Lili Su|Edmund Yeh|Stratis Ioannidis,,https://arxiv.org/abs/2201.04830v1,https://arxiv.org/pdf/2201.04830v1,,Extended version of paper accepted by INFOCOM 2022,,,cs.NI,cs.NI,https://arxiv.org/pdf/2201.04830v1.pdf
2201.06998v2,2022-01-12T21:01:09Z,2022-06-27 20:06:01,Ensemble-Based Experimental Design for Targeting Data Acquisition to Inform Climate Models,"Data required to calibrate uncertain GCM parameterizations are often only available in limited regions or time periods, for example, observational data from field campaigns, or data generated in local high-resolution simulations. This raises the question of where and when to acquire additional data to be maximally informative about parameterizations in a GCM. Here we construct a new ensemble-based parallel algorithm to automatically target data acquisition to regions and times that maximize the uncertainty reduction, or information gain, about GCM parameters. The algorithm uses a Bayesian framework that exploits a quantified distribution of GCM parameters as a measure of uncertainty. This distribution is informed by time-averaged climate statistics restricted to local regions and times. The algorithm is embedded in the recently developed calibrate-emulate-sample (CES) framework, which performs efficient model calibration and uncertainty quantification with only $\mathcal{O}(10^2)$ model evaluations, compared with $\mathcal{O}(10^5)$ evaluations typically needed for traditional approaches to Bayesian calibration. We demonstrate the algorithm with an idealized GCM, with which we generate surrogates of local data. In this perfect-model setting, we calibrate parameters and quantify uncertainties in a quasi-equilibrium convection scheme in the GCM. We consider targeted data that are (i) localized in space for statistically stationary simulations, and (ii) localized in space and time for seasonally varying simulations. In these proof-of-concept applications, the calculated information gain reflects the reduction in parametric uncertainty obtained from Bayesian inference when harnessing a targeted sample of data. The largest information gain typically, but not always, results from regions near the intertropical convergence zone (ITCZ).",Oliver R. A. Dunbar|Michael F. Howland|Tapio Schneider|Andrew M. Stuart,,https://arxiv.org/abs/2201.06998v2,https://arxiv.org/pdf/2201.06998v2,https://doi.org/10.1029/2022MS002997,,,10.1029/2022MS002997,stat.AP,stat.AP,https://arxiv.org/pdf/2201.06998v2.pdf
2201.03042v1,2022-01-09T16:09:21Z,2022-01-09 16:09:21,Computing optimal experimental designs on finite sets by log-determinant gradient flow,"Optimal experimental designs are probability measures with finite support enjoying an optimality property for the computation of least squares estimators. We present an algorithm for computing optimal designs on finite sets based on the long-time asymptotics of the gradient flow of the log-determinant of the so called information matrix. We prove the convergence of the proposed algorithm, and provide a sharp estimate on the rate its convergence. Numerical experiments are performed on few test cases using the new matlab package OptimalDesignComputation.",Federico Piazzon,,https://arxiv.org/abs/2201.03042v1,https://arxiv.org/pdf/2201.03042v1,,,,,math.NA,math.NA|math.ST,https://arxiv.org/pdf/2201.03042v1.pdf
2201.00222v1,2022-01-01T17:55:01Z,2022-01-01 17:55:01,Multi-fidelity Bayesian experimental design to quantify extreme-event statistics,"In this work, we develop a multi-fidelity Bayesian experimental design framework to efficiently quantify the extreme-event statistics of an input-to-response (ItR) system with given input probability and expensive function evaluations. The key idea here is to leverage low-fidelity samples whose responses can be computed with a cost of a certain fraction of that for high-fidelity samples, in an optimized configuration to reduce the total computational cost. To accomplish this goal, we employ a multi-fidelity Gaussian process as the surrogate model of the ItR function, and develop a new acquisition based on which the optimized next sample can be selected in terms of its location in the sample space and the fidelity level. In addition, we develop an inexpensive analytical evaluation of the acquisition and its derivative, avoiding numerical integrations that are prohibitive for high-dimensional problems. The new method is tested in a bi-fidelity context for a series of synthetic problems with varying dimensions, low-fidelity model accuracy and computational costs. Comparing with the single-fidelity method and the bi-fidelity method with a pre-defined fidelity hierarchy, our method consistently shows the best (or among the best) performance for all the test cases. Finally, we demonstrate the superiority of our method in solving an engineering problem of estimating the extreme ship motion statistics in irregular waves, using computational fluid dynamics (CFD) with two different grid resolutions as the high and low fidelity models.",Xianliang Gong|Yulin Pan,,https://arxiv.org/abs/2201.00222v1,https://arxiv.org/pdf/2201.00222v1,,,,,physics.flu-dyn,physics.flu-dyn,https://arxiv.org/pdf/2201.00222v1.pdf
2112.14811v1,2021-12-29T20:02:35Z,2021-12-29 20:02:35,Active Learning-Based Optimization of Scientific Experimental Design,"Active learning (AL) is a machine learning algorithm that can achieve greater accuracy with fewer labeled training instances, for having the ability to ask oracles to label the most valuable unlabeled data chosen iteratively and heuristically by query strategies. Scientific experiments nowadays, though becoming increasingly automated, are still suffering from human involvement in the designing process and the exhaustive search in the experimental space. This article performs a retrospective study on a drug response dataset using the proposed AL scheme comprised of the matrix factorization method of alternating least square (ALS) and deep neural networks (DNN). This article also proposes an AL query strategy based on expected loss minimization. As a result, the retrospective study demonstrates that scientific experimental design, instead of being manually set, can be optimized by AL, and the proposed query strategy ELM sampling shows better experimental performance than other ones such as random sampling and uncertainty sampling.",Ruoyu Wang,,https://arxiv.org/abs/2112.14811v1,https://arxiv.org/pdf/2112.14811v1,https://doi.org/10.1109/ICAICE54393.2021.00060,2021 2nd International Conference on Artificial Intelligence and Computer Engineering (ICAICE 2021),,10.1109/ICAICE54393.2021.00060,cs.LG,cs.LG|cs.AI,https://arxiv.org/pdf/2112.14811v1.pdf
2112.10944v2,2021-12-21T02:25:23Z,2021-12-23 07:15:09,Reinforcement Learning based Sequential Batch-sampling for Bayesian Optimal Experimental Design,"Engineering problems that are modeled using sophisticated mathematical methods or are characterized by expensive-to-conduct tests or experiments, are encumbered with limited budget or finite computational resources. Moreover, practical scenarios in the industry, impose restrictions, based on logistics and preference, on the manner in which the experiments can be conducted. For example, material supply may enable only a handful of experiments in a single-shot or in the case of computational models one may face significant wait-time based on shared computational resources. In such scenarios, one usually resorts to performing experiments in a manner that allows for maximizing one's state-of-knowledge while satisfying the above mentioned practical constraints. Sequential design of experiments (SDOE) is a popular suite of methods, that has yielded promising results in recent years across different engineering and practical problems. A common strategy, that leverages Bayesian formalism is the Bayesian SDOE, which usually works best in the one-step-ahead or myopic scenario of selecting a single experiment at each step of a sequence of experiments. In this work, we aim to extend the SDOE strategy, to query the experiment or computer code at a batch of inputs. To this end, we leverage deep reinforcement learning (RL) based policy gradient methods, to propose batches of queries that are selected taking into account entire budget in hand. The algorithm retains the sequential nature, inherent in the SDOE, while incorporating elements of reward based on task from the domain of deep RL. A unique capability of the proposed methodology is its ability to be applied to multiple tasks, for example optimization of a function, once its trained. We demonstrate the performance of the proposed algorithm on a synthetic problem, and a challenging high-dimensional engineering problem.",Yonatan Ashenafi|Piyush Pandita|Sayan Ghosh,,https://arxiv.org/abs/2112.10944v2,https://arxiv.org/pdf/2112.10944v2,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2112.10944v2.pdf
2112.10548v1,2021-12-20T14:20:42Z,2021-12-20 14:20:42,Model predictive control guided with optimal experimental design for pulse-based parallel cultivation,"Optimal experimental design for parameter precision attempts to maximize the information content in experimental data for a most effective identification of parametric model. With the recent developments in miniaturization and parallelization of cultivation platforms for high-throughput screening of optimal growth conditions massive amounts of informative data can be generated with few experiments. Increasing the quantity of the data means to increase the number of parameters and experimental design variables which might deteriorate the identifiability and hamper the online computation of optimal inputs. To reduce the problem complexity, in this work, we introduce an auxiliary controller at a lower level that tracks the optimal feeding strategy computed by a high-level optimizer in an online fashion. The hierarchical framework is especially interesting for the operation under constraints. The key aspect of this method are discussed together with an in silico study considering parallel glucose limited bacterial fed batch cultivations.",Jong Woo Kim|Niels Krausch|Judit Aizpuru|Tilman Barz|Sergio Lucia|Ernesto C. Martínez|Peter Neubauer|Mariano Nicolas Cruz Bournazou,"Technische Universität Berlin, Chair of Bioprocess Engineering, Strasse des 17. Juni 135, 10623 Berlin, Germany|Technische Universität Berlin, Chair of Bioprocess Engineering, Strasse des 17. Juni 135, 10623 Berlin, Germany|Technische Universität Berlin, Chair of Bioprocess Engineering, Strasse des 17. Juni 135, 10623 Berlin, Germany|AIT Austrian Institute of Technology GmbH, Giefingasse 2, 1210 Vienna, Austria|Technische Universität Dortmund, Department of Biochemical and Chemical Engineering, Emil-Figge-Strasse 70, 44227 Dortmund, Germany|INGAR|Technische Universität Berlin, Chair of Bioprocess Engineering, Strasse des 17. Juni 135, 10623 Berlin, Germany|Technische Universität Berlin, Chair of Bioprocess Engineering, Strasse des 17. Juni 135, 10623 Berlin, Germany",https://arxiv.org/abs/2112.10548v1,https://arxiv.org/pdf/2112.10548v1,,"6 pages, 4 figures, submitted to IFAC Conference",,,q-bio.QM,q-bio.QM,https://arxiv.org/pdf/2112.10548v1.pdf
2112.09206v2,2021-12-16T21:23:21Z,2023-05-03 08:12:41,Empirical likelihood for the analysis of experimental designs,"Empirical likelihood enables a nonparametric, likelihood-driven style of inference without restrictive assumptions routinely made in parametric models. We develop a framework for applying empirical likelihood to the analysis of experimental designs, addressing issues that arise from blocking and multiple hypothesis testing. In addition to popular designs such as balanced incomplete block designs, our approach allows for highly unbalanced, incomplete block designs. We derive an asymptotic multivariate chi-square distribution for a set of empirical likelihood test statistics and propose two single-step multiple testing procedures: asymptotic Monte Carlo and nonparametric bootstrap. Both procedures asymptotically control the generalised family-wise error rate and efficiently construct simultaneous confidence intervals for comparisons of interest without explicitly considering the underlying covariance structure. A simulation study demonstrates that the performance of the procedures is robust to violations of standard assumptions of linear mixed models. We also present an application to experiments on a pesticide.",Eunseop Kim|Steven N. MacEachern|Mario Peruggia,,https://arxiv.org/abs/2112.09206v2,https://arxiv.org/pdf/2112.09206v2,https://doi.org/10.1080/10485252.2023.2206919,,Journal of Nonparametric Statistics. 35 (2023) 709-732,10.1080/10485252.2023.2206919,stat.ME,stat.ME,https://arxiv.org/pdf/2112.09206v2.pdf
2112.06794v4,2021-12-13T16:57:16Z,2023-02-23 18:42:01,Small-noise approximation for Bayesian optimal experimental design with nuisance uncertainty,"Calculating the expected information gain in optimal Bayesian experimental design typically relies on nested Monte Carlo sampling. When the model also contains nuisance parameters, which are parameters that contribute to the overall uncertainty of the system but are of no interest in the Bayesian design framework, this introduces a second inner loop. We propose and derive a small-noise approximation for this additional inner loop. The computational cost of our method can be further reduced by applying a Laplace approximation to the remaining inner loop. Thus, we present two methods, the small-noise Double-loop Monte Carlo and small-noise Monte Carlo Laplace methods. Moreover, we demonstrate that the total complexity of these two approaches remains comparable to the case without nuisance uncertainty. To assess the efficiency of these methods, we present three examples, and the last example includes the partial differential equation for the electrical impedance tomography experiment for composite laminate materials.",Arved Bartuska|Luis Espath|Raúl Tempone,,https://arxiv.org/abs/2112.06794v4,https://arxiv.org/pdf/2112.06794v4,https://doi.org/10.1016/j.cma.2022.115320,"22 pages, 11 figures, Added references, Addressed referee suggestions, Corrected equations in Section 5 and Appendix B; No significant impact on numerical results, Updated figures, Corrected notation in Section 6, Corrected typos",Comput. Methods Appl. Mech. Engrg. 399 (2022) 115320,10.1016/j.cma.2022.115320,math.NA,math.NA,https://arxiv.org/pdf/2112.06794v4.pdf
2112.05244v2,2021-12-09T23:13:57Z,2022-03-15 17:24:54,An Experimental Design Perspective on Model-Based Reinforcement Learning,"In many practical applications of RL, it is expensive to observe state transitions from the environment. For example, in the problem of plasma control for nuclear fusion, computing the next state for a given state-action pair requires querying an expensive transition function which can lead to many hours of computer simulation or dollars of scientific research. Such expensive data collection prohibits application of standard RL algorithms which usually require a large number of observations to learn. In this work, we address the problem of efficiently learning a policy while making a minimal number of state-action queries to the transition function. In particular, we leverage ideas from Bayesian optimal experimental design to guide the selection of state-action queries for efficient learning. We propose an acquisition function that quantifies how much information a state-action pair would provide about the optimal solution to a Markov decision process. At each iteration, our algorithm maximizes this acquisition function, to choose the most informative state-action pair to be queried, thus yielding a data-efficient RL approach. We experiment with a variety of simulated continuous control problems and show that our approach learns an optimal policy with up to $5$ -- $1,000\times$ less data than model-based RL baselines and $10^3$ -- $10^5\times$ less data than model-free RL baselines. We also provide several ablated comparisons which point to substantial improvements arising from the principled method of obtaining data.",Viraj Mehta|Biswajit Paria|Jeff Schneider|Stefano Ermon|Willie Neiswanger,,https://arxiv.org/abs/2112.05244v2,https://arxiv.org/pdf/2112.05244v2,,Conference paper at ICLR 2022,,,cs.LG,cs.LG|cs.AI|cs.IT|cs.RO|stat.ML,https://arxiv.org/pdf/2112.05244v2.pdf
2112.04889v2,2021-12-07T13:28:34Z,2021-12-13 08:32:39,Artificial Intelligence and Design of Experiments for Assessing Security of Electricity Supply: A Review and Strategic Outlook,"Assessing the effects of the energy transition and liberalization of energy markets on resource adequacy is an increasingly important and demanding task. The rising complexity in energy systems requires adequate methods for energy system modeling leading to increased computational requirements. Furthermore, with complexity, uncertainty increases likewise calling for probabilistic assessments and scenario analyses. To adequately and efficiently address these various requirements, new methods from the field of data science are needed to accelerate current methods. With our systematic literature review, we want to close the gap between the three disciplines (1) assessment of security of electricity supply, (2) artificial intelligence, and (3) design of experiments. For this, we conduct a large-scale quantitative review on selected fields of application and methods and make a synthesis that relates the different disciplines to each other. Among other findings, we identify metamodeling of complex security of electricity supply models using AI methods and applications of AI-based methods for forecasts of storage dispatch and (non-)availabilities as promising fields of application that have not sufficiently been covered, yet. We end with deriving a new methodological pipeline for adequately and efficiently addressing the present and upcoming challenges in the assessment of security of electricity supply.",Jan Priesmann|Justin Münch|Elias Ridha|Thomas Spiegel|Marius Reich|Mario Adam|Lars Nolting|Aaron Praktiknjo,,https://arxiv.org/abs/2112.04889v2,https://arxiv.org/pdf/2112.04889v2,,,,,cs.AI,cs.AI|eess.SY,https://arxiv.org/pdf/2112.04889v2.pdf
2112.01709v2,2021-12-03T04:36:32Z,2024-07-03 13:49:39,Optimized variance estimation under interference and complex experimental designs,"Unbiased and consistent variance estimators generally do not exist for design-based treatment effect estimators because experimenters never observe more than one potential outcome for any unit. The problem is exacerbated by interference and complex experimental designs. Experimenters must accept conservative variance estimators in these settings, but they can strive to minimize conservativeness. In this paper, we show that the task of constructing a minimally conservative variance estimator can be interpreted as an optimization problem that aims to find the lowest estimable upper bound of the true variance given the experimenter's risk preference and knowledge of the potential outcomes. We characterize the set of admissible bounds in the class of quadratic forms, and we demonstrate that the optimization problem is a convex program for many natural objectives. The resulting variance estimators are guaranteed to be conservative regardless of whether the background knowledge used to construct the bound is correct, but the estimators are less conservative if the provided information is reasonably accurate. Numerical results show that the resulting variance estimators can be considerably less conservative than existing estimators, allowing experimenters to draw more informative inferences about treatment effects.",Christopher Harshaw|Joel A. Middleton|Fredrik Sävje,,https://arxiv.org/abs/2112.01709v2,https://arxiv.org/pdf/2112.01709v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2112.01709v2.pdf
2112.01687v1,2021-12-03T02:51:15Z,2021-12-03 02:51:15,Differential Property Prediction: A Machine Learning Approach to Experimental Design in Advanced Manufacturing,"Advanced manufacturing techniques have enabled the production of materials with state-of-the-art properties. In many cases however, the development of physics-based models of these techniques lags behind their use in the lab. This means that designing and running experiments proceeds largely via trial and error. This is sub-optimal since experiments are cost-, time-, and labor-intensive. In this work we propose a machine learning framework, differential property classification (DPC), which enables an experimenter to leverage machine learning's unparalleled pattern matching capability to pursue data-driven experimental design. DPC takes two possible experiment parameter sets and outputs a prediction of which will produce a material with a more desirable property specified by the operator. We demonstrate the success of DPC on AA7075 tube manufacturing process and mechanical property data using shear assisted processing and extrusion (ShAPE), a solid phase processing technology. We show that by focusing on the experimenter's need to choose between multiple candidate experimental parameters, we can reframe the challenging regression task of predicting material properties from processing parameters, into a classification task on which machine learning models can achieve good performance.",Loc Truong|WoongJo Choi|Colby Wight|Lizzy Coda|Tegan Emerson|Keerti Kappagantula|Henry Kvinge,,https://arxiv.org/abs/2112.01687v1,https://arxiv.org/pdf/2112.01687v1,,,,,cs.LG,cs.LG|cs.AI,https://arxiv.org/pdf/2112.01687v1.pdf
2112.00278v1,2021-12-01T05:05:26Z,2021-12-01 05:05:26,Synthetic Design: An Optimization Approach to Experimental Design with Synthetic Controls,"We investigate the optimal design of experimental studies that have pre-treatment outcome data available. The average treatment effect is estimated as the difference between the weighted average outcomes of the treated and control units. A number of commonly used approaches fit this formulation, including the difference-in-means estimator and a variety of synthetic-control techniques. We propose several methods for choosing the set of treated units in conjunction with the weights. Observing the NP-hardness of the problem, we introduce a mixed-integer programming formulation which selects both the treatment and control sets and unit weightings. We prove that these proposed approaches lead to qualitatively different experimental units being selected for treatment. We use simulations based on publicly available data from the US Bureau of Labor Statistics that show improvements in terms of mean squared error and statistical power when compared to simple and commonly used alternatives such as randomized trials.",Nick Doudchenko|Khashayar Khosravi|Jean Pouget-Abadie|Sebastien Lahaie|Miles Lubin|Vahab Mirrokni|Jann Spiess|Guido Imbens,,https://arxiv.org/abs/2112.00278v1,https://arxiv.org/pdf/2112.00278v1,,,,,stat.ME,stat.ME|stat.ML,https://arxiv.org/pdf/2112.00278v1.pdf
2111.13612v1,2021-11-26T17:18:53Z,2021-11-26 17:18:53,Optimal Design of Experiments for Simulation-Based Inference of Mechanistic Acyclic Biological Networks,"Biological signaling pathways based upon proteins binding to one another to relay a signal for genetic expression, such as the Bone Morphogenetic Protein (BMP) signaling pathway, can be modeled by mass action kinetics and conservation laws that result in non-closed form polynomial equations. Accurately determining parameters of biological pathways that represent physically relevant features, such as binding affinity of proteins and their associated uncertainty, presents a challenge for biological models lacking an explicit likelihood function. Additionally, parameterizing non-closed form biological models requires copious amounts of data from expensive perturbation-response experiments to fit model parameters. We present an algorithm (SBIDOEMAN) for determining optimal experiments and parameters of systems biology models with implicit likelihoods. We evaluate our algorithm using simulations of held-out true parameter values and demonstrate an improvement in the rate of accurate parameter inference over random and equidistant experimental designs when evaluated on two simple models of the BMP signaling pathway with an implicit likelihood function.",Vincent Zaballa|Elliot Hui,,https://arxiv.org/abs/2111.13612v1,https://arxiv.org/pdf/2111.13612v1,,Accepted to Learning Meaningful Representations of Life workshop at NeurIPs 2021,,,q-bio.QM,q-bio.QM|q-bio.MN,https://arxiv.org/pdf/2111.13612v1.pdf
2111.02329v1,2021-11-03T16:24:05Z,2021-11-03 16:24:05,Implicit Deep Adaptive Design: Policy-Based Experimental Design without Likelihoods,"We introduce implicit Deep Adaptive Design (iDAD), a new method for performing adaptive experiments in real-time with implicit models. iDAD amortizes the cost of Bayesian optimal experimental design (BOED) by learning a design policy network upfront, which can then be deployed quickly at the time of the experiment. The iDAD network can be trained on any model which simulates differentiable samples, unlike previous design policy work that requires a closed form likelihood and conditionally independent experiments. At deployment, iDAD allows design decisions to be made in milliseconds, in contrast to traditional BOED approaches that require heavy computation during the experiment itself. We illustrate the applicability of iDAD on a number of experiments, and show that it provides a fast and effective mechanism for performing adaptive design with implicit models.",Desi R. Ivanova|Adam Foster|Steven Kleinegesse|Michael U. Gutmann|Tom Rainforth,,https://arxiv.org/abs/2111.02329v1,https://arxiv.org/pdf/2111.02329v1,,"33 pages, 8 figures. Published as a conference paper at NeurIPS 2021",,,stat.ML,stat.ML|cs.AI|cs.LG|stat.CO,https://arxiv.org/pdf/2111.02329v1.pdf
2110.15632v1,2021-10-29T09:04:01Z,2021-10-29 09:04:01,Bayesian Optimal Experimental Design for Simulator Models of Cognition,"Bayesian optimal experimental design (BOED) is a methodology to identify experiments that are expected to yield informative data. Recent work in cognitive science considered BOED for computational models of human behavior with tractable and known likelihood functions. However, tractability often comes at the cost of realism; simulator models that can capture the richness of human behavior are often intractable. In this work, we combine recent advances in BOED and approximate inference for intractable models, using machine-learning methods to find optimal experimental designs, approximate sufficient summary statistics and amortized posterior distributions. Our simulation experiments on multi-armed bandit tasks show that our method results in improved model discrimination and parameter estimation, as compared to experimental designs commonly used in the literature.",Simon Valentin|Steven Kleinegesse|Neil R. Bramley|Michael U. Gutmann|Christopher G. Lucas,,https://arxiv.org/abs/2110.15632v1,https://arxiv.org/pdf/2110.15632v1,,"Accepted as a poster at the NeurIPS 2021 Workshop ""AI for Science""",,,cs.LG,cs.LG,https://arxiv.org/pdf/2110.15632v1.pdf
2110.15335v2,2021-10-28T17:47:31Z,2022-03-20 19:19:16,Bayesian Sequential Optimal Experimental Design for Nonlinear Models Using Policy Gradient Reinforcement Learning,"We present a mathematical framework and computational methods to optimally design a finite number of sequential experiments. We formulate this sequential optimal experimental design (sOED) problem as a finite-horizon partially observable Markov decision process (POMDP) in a Bayesian setting and with information-theoretic utilities. It is built to accommodate continuous random variables, general non-Gaussian posteriors, and expensive nonlinear forward models. sOED then seeks an optimal design policy that incorporates elements of both feedback and lookahead, generalizing the suboptimal batch and greedy designs. We solve for the sOED policy numerically via policy gradient (PG) methods from reinforcement learning, and derive and prove the PG expression for sOED. Adopting an actor-critic approach, we parameterize the policy and value functions using deep neural networks and improve them using gradient estimates produced from simulated episodes of designs and observations. The overall PG-sOED method is validated on a linear-Gaussian benchmark, and its advantages over batch and greedy designs are demonstrated through a contaminant source inversion problem in a convection-diffusion field.",Wanggang Shen|Xun Huan,,https://arxiv.org/abs/2110.15335v2,https://arxiv.org/pdf/2110.15335v2,https://doi.org/10.1016/j.cma.2023.116304,"Preprint 37 pages, 16 figures",Computer Methods in Applied Mechanics and Engineering 416 (2023) 116304,10.1016/j.cma.2023.116304,cs.LG,cs.LG|stat.CO|stat.ME|stat.ML,https://arxiv.org/pdf/2110.15335v2.pdf
2110.12568v2,2021-10-25T01:01:59Z,2021-12-13 21:25:15,Analyzing a Complex Game for the South China Sea Fishing Dispute using Response Surface Methodologies,"The South China Sea (SCS) is one of the most economically valuable resources on the planet, and as such has become a source of territorial disputes between its bordering nations. Among other things, states compete to harvest the multitude of fish species in the SCS. In an effort to gain a competitive advantage states have turned to increased maritime patrols, as well as the use of ""maritime militias,"" which are fishermen armed with martial assets to resist the influence of patrols. This conflict suggests a game of strategic resource allocation where states allocate patrols intelligently to earn the greatest possible utility. The game, however, is quite computationally challenging when considering its size (there are several distinct fisheries in the SCS), the nonlinear nature of biomass growth, and the influence of patrol allocations on costs imposed on fishermen. Further, uncertainty in player behavior attributed to modeling error requires a robust analysis to fully capture the dispute's dynamics. To model such a complex scenario, this paper employs a response surface methodology to assess optimal patrolling strategies and their impact on realized utilities. The methodology developed successfully finds strategies which are more robust to behavioral uncertainty than a more straight-forward method.",Michael Macgregor Perry,,https://arxiv.org/abs/2110.12568v2,https://arxiv.org/pdf/2110.12568v2,,,,,econ.GN,econ.GN,https://arxiv.org/pdf/2110.12568v2.pdf
2110.11875v1,2021-10-22T16:01:39Z,2021-10-22 16:01:39,GeneDisco: A Benchmark for Experimental Design in Drug Discovery,"In vitro cellular experimentation with genetic interventions, using for example CRISPR technologies, is an essential step in early-stage drug discovery and target validation that serves to assess initial hypotheses about causal associations between biological mechanisms and disease pathologies. With billions of potential hypotheses to test, the experimental design space for in vitro genetic experiments is extremely vast, and the available experimental capacity - even at the largest research institutions in the world - pales in relation to the size of this biological hypothesis space. Machine learning methods, such as active and reinforcement learning, could aid in optimally exploring the vast biological space by integrating prior knowledge from various information sources as well as extrapolating to yet unexplored areas of the experimental design space based on available data. However, there exist no standardised benchmarks and data sets for this challenging task and little research has been conducted in this area to date. Here, we introduce GeneDisco, a benchmark suite for evaluating active learning algorithms for experimental design in drug discovery. GeneDisco contains a curated set of multiple publicly available experimental data sets as well as open-source implementations of state-of-the-art active learning policies for experimental design and exploration.",Arash Mehrjou|Ashkan Soleymani|Andrew Jesson|Pascal Notin|Yarin Gal|Stefan Bauer|Patrick Schwab,,https://arxiv.org/abs/2110.11875v1,https://arxiv.org/pdf/2110.11875v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2110.11875v1.pdf
2110.08072v1,2021-10-15T13:03:09Z,2021-10-15 13:03:09,GaussED: A Probabilistic Programming Language for Sequential Experimental Design,"Sequential algorithms are popular for experimental design, enabling emulation, optimisation and inference to be efficiently performed. For most of these applications bespoke software has been developed, but the approach is general and many of the actual computations performed in such software are identical. Motivated by the diverse problems that can in principle be solved with common code, this paper presents GaussED, a simple probabilistic programming language coupled to a powerful experimental design engine, which together automate sequential experimental design for approximating a (possibly nonlinear) quantity of interest in Gaussian processes models. Using a handful of commands, GaussED can be used to: solve linear partial differential equations, perform tomographic reconstruction from integral data and implement Bayesian optimisation with gradient data.",Matthew A. Fisher|Onur Teymur|Chris. J. Oates,,https://arxiv.org/abs/2110.08072v1,https://arxiv.org/pdf/2110.08072v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/2110.08072v1.pdf
2110.04898v1,2021-10-10T20:38:32Z,2021-10-10 20:38:32,Response surface single loop reliability-based design optimization with higher-order reliability assessment,"Reliability-based design optimization (RBDO) aims at determination of the optimal design in the presence of uncertainty. The available Single-Loop approaches for RBDO are based on the First-Order Reliability Method (FORM) for the computation of the probability of failure, along with different approximations in order to avoid the expensive inner loop aiming at finding the Most Probable Point (MPP). However, the use of FORM in RBDO may not lead to sufficient accuracy depending on the degree of nonlinearity of the limit-state function. This is demonstrated for an extensively studied reliability-based design for vehicle crashworthiness problem solved in this paper, where all RBDO methods based on FORM strongly violates the probabilistic constraints. The Response Surface Single Loop (RSSL) method for RBDO is proposed based on the higher order probability computation for quadratic models previously presented by the authors. The RSSL-method bypasses the concept of an MPP and has high accuracy and efficiency. The method can solve problems with both constant and varying standard deviation of design variables and is particularly well suited for typical industrial applications where general quadratic response surface models can be used. If the quadratic response surface models of the deterministic constraints are valid in the whole region of interest, the method becomes a true single loop method with accuracy higher than traditional SORM. In other cases, quadratic response surface models are fitted to the deterministic constraints around the deterministic solution and the RBDO problem is solved using the proposed single loop method.",Rami Mansour|Mårten Olsson,,https://arxiv.org/abs/2110.04898v1,https://arxiv.org/pdf/2110.04898v1,https://doi.org/10.1007/s00158-015-1386-x,"17 pages, 10 figures",Struct Multi disc Optim 54 (2016) 63-79,10.1007/s00158-015-1386-x,math.OC,math.OC|cs.CE|stat.CO,https://arxiv.org/pdf/2110.04898v1.pdf
2110.03433v1,2021-10-07T13:15:34Z,2021-10-07 13:15:34,From the Head or the Heart? An Experimental Design on the Impact of Explanation on Cognitive and Affective Trust,"Automated vehicles (AVs) are social robots that can potentially benefit our society. According to the existing literature, AV explanations can promote passengers' trust by reducing the uncertainty associated with the AV's reasoning and actions. However, the literature on AV explanations and trust has failed to consider how the type of trust
  - cognitive versus affective - might alter this relationship. Yet, the existing literature has shown that the implications associated with trust vary widely depending on whether it is cognitive or affective. To address this shortcoming and better understand the impacts of explanations on trust in AVs, we designed a study to investigate the effectiveness of explanations on both cognitive and affective trust. We expect these results to be of great significance in designing AV explanations to promote AV trust.",Qiaoning Zhang|X. Jessie Yang|Lionel P. Robert,,https://arxiv.org/abs/2110.03433v1,https://arxiv.org/pdf/2110.03433v1,,Presented at AI-HRI symposium as part of AAAI-FSS 2021 (arXiv:2109.10836),,,cs.HC,cs.HC|cs.AI|cs.RO,https://arxiv.org/pdf/2110.03433v1.pdf
2110.01458v2,2021-09-29T21:23:58Z,2021-10-05 21:35:36,Designing Complex Experiments by Applying Unsupervised Machine Learning,"Design of experiments (DOE) is playing an essential role in learning and improving a variety of objects and processes. The article discusses the application of unsupervised machine learning to support the pragmatic designs of complex experiments. Complex experiments are characterized by having a large number of factors, mixed-level designs, and may be subject to constraints that eliminate some unfeasible trials for various reasons. Having such attributes, it is very challenging to design pragmatic experiments that are economically, operationally, and timely sound. It means a significant decrease in the number of required trials from a full factorial design, while still attempting to achieve the defined objectives. A beta variational autoencoder (beta-VAE) has been applied to represent trials of the initial full factorial design after filtering out unfeasible trials on the low dimensional latent space. Regarding visualization and interpretability, the paper is limited to 2D representations. Beta-VAE supports (1) orthogonality of the latent space dimensions, (2) isotropic multivariate standard normal distribution of the representation on the latent space, (3) disentanglement of the latent space representation by levels of factors, (4) propagation of the applied constraints of the initial design into the latent space, and (5) generation of trials by decoding latent space points. Having an initial design representation on the latent space with such properties, it allows for the generation of pragmatic design of experiments (G-DOE) by specifying the number of trials and their pattern on the latent space, such as square or polar grids. Clustering and aggregated gradient metrics have been shown to guide grid specification.",Alex Glushkovsky,,https://arxiv.org/abs/2110.01458v2,https://arxiv.org/pdf/2110.01458v2,,"18 pages, 17 figures, and 2 tables",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2110.01458v2.pdf
2109.09220v1,2021-09-19T20:44:33Z,2021-09-19 20:44:33,Unifying Design-based Inference: On Bounding and Estimating the Variance of any Linear Estimator in any Experimental Design,"This paper provides a design-based framework for variance (bound) estimation in experimental analysis. Results are applicable to virtually any combination of experimental design, linear estimator (e.g., difference-in-means, OLS, WLS) and variance bound, allowing for unified treatment and a basis for systematic study and comparison of designs using matrix spectral analysis. A proposed variance estimator reproduces Eicker-Huber-White (aka. ""robust"", ""heteroskedastic consistent"", ""sandwich"", ""White"", ""Huber-White"", ""HC"", etc.) standard errors and ""cluster-robust"" standard errors as special cases. While past work has shown algebraic equivalences between design-based and the so-called ""robust"" standard errors under some designs, this paper motivates them for a wide array of design-estimator-bound triplets. In so doing, it provides a clearer and more general motivation for variance estimators.",Joel A. Middleton,,https://arxiv.org/abs/2109.09220v1,https://arxiv.org/pdf/2109.09220v1,,25 Pages,,,stat.ME,stat.ME|econ.EM,https://arxiv.org/pdf/2109.09220v1.pdf
2109.03457v4,2021-09-08T06:54:32Z,2022-08-31 08:53:45,Uncertainty Quantification and Experimental Design for Large-Scale Linear Inverse Problems under Gaussian Process Priors,"We consider the use of Gaussian process (GP) priors for solving inverse problems in a Bayesian framework. As is well known, the computational complexity of GPs scales cubically in the number of datapoints. We here show that in the context of inverse problems involving integral operators, one faces additional difficulties that hinder inversion on large grids. Furthermore, in that context, covariance matrices can become too large to be stored. By leveraging results about sequential disintegrations of Gaussian measures, we are able to introduce an implicit representation of posterior covariance matrices that reduces the memory footprint by only storing low rank intermediate matrices, while allowing individual elements to be accessed on-the-fly without needing to build full posterior covariance matrices. Moreover, it allows for fast sequential inclusion of new observations. These features are crucial when considering sequential experimental design tasks. We demonstrate our approach by computing sequential data collection plans for excursion set recovery for a gravimetric inverse problem, where the goal is to provide fine resolution estimates of high density regions inside the Stromboli volcano, Italy. Sequential data collection plans are computed by extending the weighted integrated variance reduction (wIVR) criterion to inverse problems. Our results show that this criterion is able to significantly reduce the uncertainty on the excursion volume, reaching close to minimal levels of residual uncertainty. Overall, our techniques allow the advantages of probabilistic models to be brought to bear on large-scale inverse problems arising in the natural sciences.",Cédric Travelletti|David Ginsbourger|Niklas Linde,,https://arxiv.org/abs/2109.03457v4,https://arxiv.org/pdf/2109.03457v4,,,,,stat.ML,stat.ML|cs.LG|stat.AP|stat.CO|stat.ME,https://arxiv.org/pdf/2109.03457v4.pdf
2108.13894v1,2021-08-31T14:51:15Z,2021-08-31 14:51:15,Scanning the landscape of axion dark matter detectors: applying gradient descent to experimental design,"The hunt for dark matter remains one of the principal objectives of modern physics and cosmology. Searches for dark matter in the form of axions are proposed or underway across a range of experimental collaborations. As we look to the next generation of detectors, a natural question to ask is whether there are new experimental designs waiting to be discovered and how we might find them. Here we take a new approach to the experimental design procedure by using gradient descent techniques to search for optimal detector designs. We provide a proof of principle for this technique by searching 1D detectors varying the bulk properties of the detector until the optimal detector design is obtained. Remarkably, we find the detector is capable of out-performing a human designed experiment on which the search was initiated. This opens the door to further gradient descent searches of more complex 2D and 3D designs across a wider variety of materials and boundary geometries of the detector. There is also an opportunity to use more sophisticated gradient descent algorithms to complete a more exhaustive scan of the landscape of designs.",J. I. McDonald,,https://arxiv.org/abs/2108.13894v1,https://arxiv.org/pdf/2108.13894v1,https://doi.org/10.1103/PhysRevD.105.083010,"6 pages, 3 figures, comments welcome",,10.1103/PhysRevD.105.083010,hep-ph,hep-ph|hep-ex,https://arxiv.org/pdf/2108.13894v1.pdf
2108.07224v1,2021-08-16T16:58:57Z,2021-08-16 16:58:57,Multimodal Information Gain in Bayesian Design of Experiments,"One of the well-known challenges in optimal experimental design is how to efficiently estimate the nested integrations of the expected information gain. The Gaussian approximation and associated importance sampling have been shown to be effective at reducing the numerical costs. However, they may fail due to the non-negligible biases and the numerical instabilities. A new approach is developed to compute the expected information gain, when the posterior distribution is multimodal - a situation previously ignored by the methods aiming at accelerating the nested numerical integrations. Specifically, the posterior distribution is approximated using a mixture distribution constructed by multiple runs of global search for the modes and weighted local Laplace approximations. Under any given probability of capturing all the modes, we provide an estimation of the number of runs of searches, which is dimension independent. It is shown that the novel global-local multimodal approach can be significantly more accurate and more efficient than the other existing approaches, especially when the number of modes is large. The methods can be applied to the designs of experiments with both calibrated and uncalibrated observation noises.",Quan Long,,https://arxiv.org/abs/2108.07224v1,https://arxiv.org/pdf/2108.07224v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/2108.07224v1.pdf
2108.06580v1,2021-08-14T16:46:49Z,2021-08-14 16:46:49,An Experimental-Design Perspective on Population Genetic Variation,"We consider the hypothesis that Evolution promotes population-wide genome patterns that, under randomization, ensures the External Validity of adaptations across population members. An adaptation is Externally Valid (EV) if its effect holds under a wide range of population genetic variations. A prediction following the hypothesis is that pairwise base substitutions in segregating regions must be 'random' as in Erdos-Renyi-Gilbert random graphs, but with edge probabilities derived from Experimental-Design concepts. We demonstrate these probabilities, and consequent mutation rates, in the full-genomes of 2504 humans, 1135 flowering plants, 1170 flies, 453 domestic sheep and 1223 brown rats.",Andre F. Ribeiro,,https://arxiv.org/abs/2108.06580v1,https://arxiv.org/pdf/2108.06580v1,,,,,q-bio.PE,q-bio.PE,https://arxiv.org/pdf/2108.06580v1.pdf
2108.06451v1,2021-08-14T03:07:15Z,2021-08-14 03:07:15,Fixation and Creativity in Data Visualization Design: Experiences and Perspectives of Practitioners,"Data visualization design often requires creativity, and research is needed to understand its nature and means for promoting it. The current visualization literature on creativity is not well developed, especially with respect to the experiences of professional data visualization designers. We conducted semi-structured interviews with 15 data visualization practitioners, focusing on a specific aspect of creativity known as design fixation. Fixation occurs when designers adhere blindly or prematurely to a set of ideas that limit creative outcomes. We present practitioners' experiences and perspectives from their own design practice, specifically focusing on their views of (i) the nature of fixation, (ii) factors encouraging fixation, and (iii) factors discouraging fixation. We identify opportunities for future research related to chart recommendations, inspiration, and perspective shifts in data visualization design.",Paul Parsons|Prakash Shukla|Chorong Park,,https://arxiv.org/abs/2108.06451v1,https://arxiv.org/pdf/2108.06451v1,,"IEEE 2021 Visualization Conference (VIS), short papers",,,cs.HC,cs.HC,https://arxiv.org/pdf/2108.06451v1.pdf
2108.05605v2,2021-08-12T08:49:28Z,2022-02-07 20:25:49,Optimising experimental design in neutron reflectometry,"Using the Fisher information (FI), the design of neutron reflectometry experiments can be optimised, leading to greater confidence in parameters of interest and better use of experimental time [Durant, Wilkins, Butler, & Cooper (2021). J. Appl. Cryst. 54, 1100-1110]. In this work, the FI is utilised in optimising the design of a wide range of reflectometry experiments. Two lipid bilayer systems are investigated to determine the optimal choice of measurement angles and liquid contrasts, in addition to the ratio of the total counting time that should be spent measuring each condition. The reduction in parameter uncertainties with the addition of underlayers to these systems is then quantified, using the FI, and validated through the use of experiment simulation and Bayesian sampling methods. For a ""one-shot"" measurement of a degrading lipid monolayer, it is shown that the common practice of measuring null-reflecting water is indeed optimal, but that the optimal measurement angle is dependent on the deuteration state of the monolayer. Finally, the framework is used to demonstrate the feasibility of measuring magnetic signals as small as $0.01μ_{B}/\text{atom}$ in layers only $20Å$ thick, given the appropriate experimental design, and that time to reach a given level of confidence in the small magnetic moment is quantifiable.",James H. Durant|Lucas Wilkins|Joshaniel F. K. Cooper,,https://arxiv.org/abs/2108.05605v2,https://arxiv.org/pdf/2108.05605v2,https://doi.org/10.1107/S1600576722003831,Revised submission to the Journal of Applied Crystallography,"J. Appl. Cryst. (2022). 55, 769-781",10.1107/S1600576722003831,physics.data-an,physics.data-an|cond-mat.soft|physics.comp-ph|physics.med-ph|stat.AP,https://arxiv.org/pdf/2108.05605v2.pdf
2108.03732v1,2021-08-08T20:57:22Z,2021-08-08 20:57:22,"Discussion: ""Bayesian Optimal Design of Experiments for Inferring the Statistical Expectation of Expensive Black-Box Functions"" (Pandita, P., Bilionis, I., and Panchal, J., 2019. ASME. J. Mech. Des. 141(10): 101404)","The authors of the discussed paper simplified the information-based acquisition on estimating statistical expectation and developed analytical computation for each involved quantity under uniform input distribution. In this discussion, we show that (1) the last three terms of the acquisition always add up to zero, leaving a concise form with a much more intuitive interpretation of the acquisition; (2) the analytical computation of the acquisition can be generalized to arbitrary input distribution, greatly broadening the application of the developed framework.",Xianliang Gong|Yulin Pan,,https://arxiv.org/abs/2108.03732v1,https://arxiv.org/pdf/2108.03732v1,,,,,stat.CO,stat.CO|cs.IT,https://arxiv.org/pdf/2108.03732v1.pdf
2108.02196v5,2021-08-04T17:49:58Z,2025-04-23 15:10:38,Synthetic Controls for Experimental Design,"This article studies experimental design in settings where the experimental units are large aggregate entities (e.g., markets), and only one or a small number of units can be exposed to the treatment. In such settings, randomization of the treatment may result in treated and control groups with very different characteristics at baseline, inducing biases. We propose a variety of experimental non-randomized synthetic control designs (Abadie, Diamond and Hainmueller, 2010, Abadie and Gardeazabal, 2003) that select the units to be treated, as well as the untreated units to be used as a control group. Average potential outcomes are estimated as weighted averages of the outcomes of treated units for potential outcomes with treatment, and weighted averages the outcomes of control units for potential outcomes without treatment. We analyze the properties of estimators based on synthetic control designs and propose new inferential techniques. We show that in experimental settings with aggregate units, synthetic control designs can substantially reduce estimation biases in comparison to randomization of the treatment.",Alberto Abadie|Jinglong Zhao,,https://arxiv.org/abs/2108.02196v5,https://arxiv.org/pdf/2108.02196v5,,,,,stat.ME,stat.ME|econ.EM,https://arxiv.org/pdf/2108.02196v5.pdf
2108.01683v2,2021-08-03T18:04:04Z,2021-09-01 13:41:48,Optimization of the Observing Cadence for the Rubin Observatory Legacy Survey of Space and Time: a pioneering process of community-focused experimental design,"Vera C. Rubin Observatory is a ground-based astronomical facility under construction, a joint project of the National Science Foundation and the U.S. Department of Energy, designed to conduct a multi-purpose 10-year optical survey of the southern hemisphere sky: the Legacy Survey of Space and Time. Significant flexibility in survey strategy remains within the constraints imposed by the core science goals of probing dark energy and dark matter, cataloging the Solar System, exploring the transient optical sky, and mapping the Milky Way. The survey's massive data throughput will be transformational for many other astrophysics domains and Rubin's data access policy sets the stage for a huge potential users' community. To ensure that the survey science potential is maximized while serving as broad a community as possible, Rubin Observatory has involved the scientific community at large in the process of setting and refining the details of the observing strategy. The motivation, history, and decision-making process of this strategy optimization are detailed in this paper, giving context to the science-driven proposals and recommendations for the survey strategy included in this Focus Issue.",Federica B. Bianco|Željko Ivezić|R. Lynne Jones|Melissa L. Graham|Phil Marshall|Abhijit Saha|Michael A. Strauss|Peter Yoachim|Tiago Ribeiro|Timo Anguita|Franz E. Bauer|Eric C. Bellm|Robert D. Blum|William N. Brandt|Sarah Brough|Màrcio Catelan|William I. Clarkson|Andrew J. Connolly|Eric Gawiser|John Gizis|Renee Hlozek|Sugata Kaviraj|Charles T. Liu|Michelle Lochner|Ashish A. Mahabal|Rachel Mandelbaum|Peregrine McGehee|Eric H. Neilsen|Knut A. G. Olsen|Hiranya Peiris|Jason Rhodes|Gordon T. Richards|Stephen Ridgway|Megan E. Schwamb|Dan Scolnic|Ohad Shemmer|Colin T. Slater|Anže Slosar|Stephen J. Smartt|Jay Strader|Rachel Street|David E. Trilling|Aprajita Verma|A. K. Vivas|Risa H. Wechsler|Beth Willman,,https://arxiv.org/abs/2108.01683v2,https://arxiv.org/pdf/2108.01683v2,https://doi.org/10.3847/1538-4365/ac3e72,Submitted as the opening paper of the Astrophysical Journal Focus Issue on Rubin LSST cadence and survey strategy,,10.3847/1538-4365/ac3e72,astro-ph.IM,astro-ph.IM,https://arxiv.org/pdf/2108.01683v2.pdf
2107.12809v4,2021-07-27T13:30:56Z,2023-10-08 21:27:56,Bayesian Optimisation for Sequential Experimental Design with Applications in Additive Manufacturing,"Bayesian optimization (BO) is an approach to globally optimizing black-box objective functions that are expensive to evaluate. BO-powered experimental design has found wide application in materials science, chemistry, experimental physics, drug development, etc. This work aims to bring attention to the benefits of applying BO in designing experiments and to provide a BO manual, covering both methodology and software, for the convenience of anyone who wants to apply or learn BO. In particular, we briefly explain the BO technique, review all the applications of BO in additive manufacturing, compare and exemplify the features of different open BO libraries, unlock new potential applications of BO to other types of data (e.g., preferential output). This article is aimed at readers with some understanding of Bayesian methods, but not necessarily with knowledge of additive manufacturing; the software performance overview and implementation instructions are instrumental for any experimental-design practitioner. Moreover, our review in the field of additive manufacturing highlights the current knowledge and technological trends of BO. This article has a supplementary material online.",Mimi Zhang|Andrew Parnell|Dermot Brabazon|Alessio Benavoli,,https://arxiv.org/abs/2107.12809v4,https://arxiv.org/pdf/2107.12809v4,,,,,cs.LG,cs.LG|cs.CE,https://arxiv.org/pdf/2107.12809v4.pdf
2107.09912v2,2021-07-21T07:25:37Z,2021-07-22 23:20:28,Design of Experiments for Stochastic Contextual Linear Bandits,"In the stochastic linear contextual bandit setting there exist several minimax procedures for exploration with policies that are reactive to the data being acquired. In practice, there can be a significant engineering overhead to deploy these algorithms, especially when the dataset is collected in a distributed fashion or when a human in the loop is needed to implement a different policy. Exploring with a single non-reactive policy is beneficial in such cases. Assuming some batch contexts are available, we design a single stochastic policy to collect a good dataset from which a near-optimal policy can be extracted. We present a theoretical analysis as well as numerical experiments on both synthetic and real-world datasets.",Andrea Zanette|Kefan Dong|Jonathan Lee|Emma Brunskill,,https://arxiv.org/abs/2107.09912v2,https://arxiv.org/pdf/2107.09912v2,,Initial submission,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2107.09912v2.pdf
2107.03544v3,2021-07-08T00:25:59Z,2021-11-25 19:13:38,The Micro-Randomized Trial for Developing Digital Interventions: Experimental Design and Data Analysis Considerations,"Just-in-time adaptive interventions (JITAIs) are time-varying adaptive interventions that use frequent opportunities for the intervention to be adapted--weekly, daily, or even many times a day. The micro-randomized trial (MRT) has emerged for use in informing the construction of JITAIs. MRTs can be used to address research questions about whether and under what circumstances JITAI components are effective, with the ultimate objective of developing effective and efficient JITAI. The purpose of this article is to clarify why, when, and how to use MRTs; to highlight elements that must be considered when designing and implementing an MRT; and to review primary and secondary analyses methods for MRTs. We briefly review key elements of JITAIs and discuss a variety of considerations that go into planning and designing an MRT. We provide a definition of causal excursion effects suitable for use in primary and secondary analyses of MRT data to inform JITAI development. We review the weighted and centered least-squares (WCLS) estimator which provides consistent causal excursion effect estimators from MRT data. We describe how the WCLS estimator along with associated test statistics can be obtained using standard statistical software such as R (R Core Team, 2019). Throughout we illustrate the MRT design and analyses using the HeartSteps MRT, for developing a JITAI to increase physical activity among sedentary individuals. We supplement the HeartSteps MRT with two other MRTs, SARA and BariFit, each of which highlights different research questions that can be addressed using the MRT and experimental design considerations that might arise.",Tianchen Qian|Ashley E. Walton|Linda M. Collins|Predrag Klasnja|Stephanie T. Lanza|Inbal Nahum-Shani|Mashifiqui Rabbi|Michael A. Russell|Maureen A. Walton|Hyesun Yoo|Susan A. Murphy,,https://arxiv.org/abs/2107.03544v3,https://arxiv.org/pdf/2107.03544v3,,"arXiv admin note: substantial text overlap with arXiv:2005.05880, arXiv:2004.10241",,,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/2107.03544v3.pdf
2107.01306v5,2021-07-02T23:57:43Z,2023-11-29 20:37:44,The Effect of the Prior and the Experimental Design on the Inference of the Precision Matrix in Gaussian Chain Graph Models,"Here, we investigate whether (and how) experimental design could aid in the estimation of the precision matrix in a Gaussian chain graph model, especially the interplay between the design, the effect of the experiment and prior knowledge about the effect. Estimation of the precision matrix is a fundamental task to infer biological graphical structures like microbial networks. We compare the marginal posterior precision of the precision matrix under four priors: flat, conjugate Normal-Wishart, Normal-MGIG and a general independent. Under the flat and conjugate priors, the Laplace-approximated posterior precision is not a function of the design matrix rendering useless any efforts to find an optimal experimental design to infer the precision matrix. In contrast, the Normal-MGIG and general independent priors do allow for the search of optimal experimental designs, yet there is a sharp upper bound on the information that can be extracted from a given experiment. We confirm our theoretical findings via a simulation study comparing i) the KL divergence between prior and posterior and ii) the Stein's loss difference of MAPs between random and no experiment. Our findings provide practical advice for domain scientists conducting experiments to better infer the precision matrix as a representation of a biological network.",Yunyi Shen|Claudia Solis-Lemus,,https://arxiv.org/abs/2107.01306v5,https://arxiv.org/pdf/2107.01306v5,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2107.01306v5.pdf
2107.00003v1,2021-06-30T02:38:17Z,2021-06-30 02:38:17,Understanding Adversarial Examples Through Deep Neural Network's Response Surface and Uncertainty Regions,"Deep neural network (DNN) is a popular model implemented in many systems to handle complex tasks such as image classification, object recognition, natural language processing etc. Consequently DNN structural vulnerabilities become part of the security vulnerabilities in those systems. In this paper we study the root cause of DNN adversarial examples. We examine the DNN response surface to understand its classification boundary. Our study reveals the structural problem of DNN classification boundary that leads to the adversarial examples. Existing attack algorithms can generate from a handful to a few hundred adversarial examples given one clean image. We show there are infinitely many adversarial images given one clean sample, all within a small neighborhood of the clean sample. We then define DNN uncertainty regions and show transferability of adversarial examples is not universal. We also argue that generalization error, the large sample theoretical guarantee established for DNN, cannot adequately capture the phenomenon of adversarial examples. We need new theory to measure DNN robustness.",Juan Shu|Bowei Xi|Charles Kamhoua,,https://arxiv.org/abs/2107.00003v1,https://arxiv.org/pdf/2107.00003v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2107.00003v1.pdf
2106.00332v2,2021-06-01T09:06:56Z,2021-10-24 08:56:14,Accelerating Optimal Experimental Design for Robust Synchronization of Uncertain Kuramoto Oscillator Model Using Machine Learning,"Recent advances in objective-based uncertainty quantification (objective-UQ) have shown that such a goal-driven approach for quantifying model uncertainty is extremely useful in real-world problems that aim at achieving specific objectives based on complex uncertain systems. Central to this objective-UQ is the concept of mean objective cost of uncertainty (MOCU), which provides effective means of quantifying the impact of uncertainty on the operational goals at hand. MOCU is especially useful for optimal experimental design (OED) as the potential efficacy of an experimental (or data acquisition) campaign can be quantified by estimating the MOCU that is expected to remain after the campaign. However, MOCU-based OED tends to be computationally expensive, which limits its practical applicability. In this paper, we propose a novel machine learning (ML) scheme that can significantly accelerate MOCU computation and expedite MOCU-based experimental design. The main idea is to use an ML model to efficiently search for the optimal robust operator under model uncertainty, a necessary step for computing MOCU. We apply the proposed ML-based OED acceleration scheme to design experiments aimed at optimally enhancing the control performance of uncertain Kuramoto oscillator models. Our results show that the proposed scheme results in up to 154-fold speed improvement without any degradation of the OED performance.",Hyun-Myung Woo|Youngjoon Hong|Bongsuk Kwon|Byung-Jun Yoon,,https://arxiv.org/abs/2106.00332v2,https://arxiv.org/pdf/2106.00332v2,https://doi.org/10.1109/TSP.2021.3130967,,,10.1109/TSP.2021.3130967,math.OC,math.OC,https://arxiv.org/pdf/2106.00332v2.pdf
2105.14024v2,2021-05-28T18:00:00Z,2021-11-24 15:44:18,Near-Optimal Multi-Perturbation Experimental Design for Causal Structure Learning,"Causal structure learning is a key problem in many domains. Causal structures can be learnt by performing experiments on the system of interest. We address the largely unexplored problem of designing a batch of experiments that each simultaneously intervene on multiple variables. While potentially more informative than the commonly considered single-variable interventions, selecting such interventions is algorithmically much more challenging, due to the doubly-exponential combinatorial search space over sets of composite interventions. In this paper, we develop efficient algorithms for optimizing different objective functions quantifying the informativeness of a budget-constrained batch of experiments. By establishing novel submodularity properties of these objectives, we provide approximation guarantees for our algorithms. Our algorithms empirically perform superior to both random interventions and algorithms that only select single-variable interventions.",Scott Sussex|Andreas Krause|Caroline Uhler,"Department of Computer Science, ETH Zürich|Department of Computer Science, ETH Zürich|Laboratory for Information & Decision Systems, Massachusetts Institute of Technology",https://arxiv.org/abs/2105.14024v2,https://arxiv.org/pdf/2105.14024v2,,"10 pages, 2 figures, appendix, to be published in 35th Conference on Neural Information Processing Systems (NeurIPS 2021), fixed typos and clarified wording",,,cs.LG,cs.LG,https://arxiv.org/pdf/2105.14024v2.pdf
2105.05806v1,2021-05-12T17:10:56Z,2021-05-12 17:10:56,High-Dimensional Experimental Design and Kernel Bandits,"In recent years methods from optimal linear experimental design have been leveraged to obtain state of the art results for linear bandits. A design returned from an objective such as $G$-optimal design is actually a probability distribution over a pool of potential measurement vectors. Consequently, one nuisance of the approach is the task of converting this continuous probability distribution into a discrete assignment of $N$ measurements. While sophisticated rounding techniques have been proposed, in $d$ dimensions they require $N$ to be at least $d$, $d \log(\log(d))$, or $d^2$ based on the sub-optimality of the solution. In this paper we are interested in settings where $N$ may be much less than $d$, such as in experimental design in an RKHS where $d$ may be effectively infinite. In this work, we propose a rounding procedure that frees $N$ of any dependence on the dimension $d$, while achieving nearly the same performance guarantees of existing rounding procedures. We evaluate the procedure against a baseline that projects the problem to a lower dimensional space and performs rounding which requires $N$ to just be at least a notion of the effective dimension. We also leverage our new approach in a new algorithm for kernelized bandits to obtain state of the art results for regret minimization and pure exploration. An advantage of our approach over existing UCB-like approaches is that our kernel bandit algorithms are also robust to model misspecification.",Romain Camilleri|Julian Katz-Samuels|Kevin Jamieson,,https://arxiv.org/abs/2105.05806v1,https://arxiv.org/pdf/2105.05806v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/2105.05806v1.pdf
2105.05539v1,2021-05-12T09:40:28Z,2021-05-12 09:40:28,A new framework for experimental design using Bayesian Evidential Learning: the case of wellhead protection area,"In this contribution, we predict the wellhead protection area (WHPA, target), the shape and extent of which is influenced by the distribution of hydraulic conductivity (K), from a small number of tracing experiments (predictor). Our first objective is to make stochastic predictions of the WHPA within the Bayesian Evidential Learning (BEL) framework, which aims to find a direct relationship between predictor and target using machine learning. This relationship is learned from a small set of training models (400) sampled from the prior distribution of K. The associated 400 pairs of simulated predictors and targets are obtained through forward modelling. Newly collected field data can then be directly used to predict the approximate posterior distribution of the corresponding WHPA. The uncertainty range of the posterior WHPA distribution is affected by the number and position of data sources (injection wells). Our second objective is to extend BEL to identify the optimal design of data source locations that minimizes the posterior uncertainty of the WHPA. This can be done explicitly, without averaging or approximating because once trained, the BEL model allows the computation of the posterior uncertainty corresponding to any new input data. We use the Modified Hausdorff Distance and the Structural Similarity index metrics to estimate the posterior uncertainty range of the WHPA. Increasing the number of injection wells effectively reduces the derived posterior WHPA uncertainty. Our approach can also estimate which injection wells are more informative than others, as validated through a k-fold cross-validation procedure. Overall, the application of BEL to experimental design makes it possible to identify the data sources maximizing the information content of any measurement data.",Robin Thibaut|Eric Laloy|Thomas Hermans,,https://arxiv.org/abs/2105.05539v1,https://arxiv.org/pdf/2105.05539v1,https://doi.org/10.1016/j.jhydrol.2021.126903,,,10.1016/j.jhydrol.2021.126903,cs.LG,cs.LG,https://arxiv.org/pdf/2105.05539v1.pdf
2105.04379v1,2021-05-10T13:59:25Z,2021-05-10 13:59:25,Gradient-based Bayesian Experimental Design for Implicit Models using Mutual Information Lower Bounds,"We introduce a framework for Bayesian experimental design (BED) with implicit models, where the data-generating distribution is intractable but sampling from it is still possible. In order to find optimal experimental designs for such models, our approach maximises mutual information lower bounds that are parametrised by neural networks. By training a neural network on sampled data, we simultaneously update network parameters and designs using stochastic gradient-ascent. The framework enables experimental design with a variety of prominent lower bounds and can be applied to a wide range of scientific tasks, such as parameter estimation, model discrimination and improving future predictions. Using a set of intractable toy models, we provide a comprehensive empirical comparison of prominent lower bounds applied to the aforementioned tasks. We further validate our framework on a challenging system of stochastic differential equations from epidemiology.",Steven Kleinegesse|Michael U. Gutmann,,https://arxiv.org/abs/2105.04379v1,https://arxiv.org/pdf/2105.04379v1,,Under review,,,stat.ML,stat.ML|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/2105.04379v1.pdf
2104.01673v1,2021-04-04T19:34:38Z,2021-04-04 19:34:38,Efficient Experimental Design for Regularized Linear Models,"Regularized linear models, such as Lasso, have attracted great attention in statistical learning and data science. However, there is sporadic work on constructing efficient data collection for regularized linear models. In this work, we propose an experimental design approach, using nearly orthogonal Latin hypercube designs, to enhance the variable selection accuracy of the regularized linear models. Systematic methods for constructing such designs are presented. The effectiveness of the proposed method is illustrated with several examples.",C. Devon Lin|Peter Chien|Xinwei Deng,,https://arxiv.org/abs/2104.01673v1,https://arxiv.org/pdf/2104.01673v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2104.01673v1.pdf
2104.00301v1,2021-04-01T07:12:50Z,2021-04-01 07:12:50,Edge-promoting adaptive Bayesian experimental design for X-ray imaging,"This work considers sequential edge-promoting Bayesian experimental design for (discretized) linear inverse problems, exemplified by X-ray tomography. The process of computing a total variation type reconstruction of the absorption inside the imaged body via lagged diffusivity iteration is interpreted in the Bayesian framework. Assuming a Gaussian additive noise model, this leads to an approximate Gaussian posterior with a covariance structure that contains information on the location of edges in the posterior mean. The next projection geometry is then chosen through A-optimal Bayesian design, which corresponds to minimizing the trace of the updated posterior covariance matrix that accounts for the new projection. Two and three-dimensional numerical examples based on simulated data demonstrate the functionality of the introduced approach.",Tapio Helin|Nuutti Hyvönen|Juha-Pekka Puska,,https://arxiv.org/abs/2104.00301v1,https://arxiv.org/pdf/2104.00301v1,,"21 pages, 9 figures",,,stat.ME,stat.ME|math.NA,https://arxiv.org/pdf/2104.00301v1.pdf
2103.16509v1,2021-03-30T17:10:19Z,2021-03-30 17:10:19,Designing Experiments for Data-Driven Control of Nonlinear Systems,"In a recent paper we have shown that data collected from linear systems excited by persistently exciting inputs during low-complexity experiments, can be used to design state- and output-feedback controllers, including optimal Linear Quadratic Regulators (LQR), by solving linear matrix inequalities (LMI) and semidefinite programs. We have also shown how to stabilize in the first approximation unknown nonlinear systems using data. In contrast to the case of linear systems, however, in the case of nonlinear systems the conditions for learning a controller directly from data may not be fulfilled even when the data are collected in experiments performed using persistently exciting inputs. In this paper we show how to design experiments that lead to the fulfilment of these conditions.",Claudio De Persis|Pietro Tesi,,https://arxiv.org/abs/2103.16509v1,https://arxiv.org/pdf/2103.16509v1,,"Submitted to the ""24th International Symposium on Mathematical Theory of Networks and Systems"" on January 2020",,,eess.SY,eess.SY,https://arxiv.org/pdf/2103.16509v1.pdf
2103.15628v1,2021-03-29T14:08:23Z,2021-03-29 14:08:23,Design of Experiments with Imputable Feature Data: An Entropy-Based Approach,"Tactical selection of experiments to estimate an underlying model is an innate task across various fields. Since each experiment has costs associated with it, selecting statistically significant experiments becomes necessary. Classic linear experimental design deals with experiment selection so as to minimize (functions of) variance in estimation of regression parameter. Typically, standard algorithms for solving this problem assume that data associated with each experiment is fully known. This isn't often true since missing data is a common problem. For instance, remote sensors often miss data due to poor connection. Hence experiment selection under such scenarios is a widespread but challenging task. Though decoupling the tasks and using standard data imputation methods like matrix completion followed by experiment selection might seem a way forward, they perform sub-optimally since the tasks are naturally interdependent. Standard design of experiments is an NP hard problem, and the additional objective of imputing for missing data amplifies the computational complexity. In this paper, we propose a maximum-entropy-principle based framework that simultaneously addresses the problem of design of experiments as well as the imputation of missing data. Our algorithm exploits homotopy from a suitably chosen convex function to the non-convex cost function; hence avoiding poor local minima. Further, our proposed framework is flexible to incorporate additional application specific constraints. Simulations on various datasets show improvement in the cost value by over 60% in comparison to benchmark algorithms applied sequentially to the imputation and experiment selection problems.",Raj K. Velicheti|Amber Srivastava|Srinivasa M. Salapaka,,https://arxiv.org/abs/2103.15628v1,https://arxiv.org/pdf/2103.15628v1,,,,,math.OC,math.OC,https://arxiv.org/pdf/2103.15628v1.pdf
2103.15229v1,2021-03-28T21:47:02Z,2021-03-28 21:47:02,Bayesian Optimal Experimental Design for Inferring Causal Structure,"Inferring the causal structure of a system typically requires interventional data, rather than just observational data. Since interventional experiments can be costly, it is preferable to select interventions that yield the maximum amount of information about a system. We propose a novel Bayesian method for optimal experimental design by sequentially selecting interventions that minimize the expected posterior entropy as rapidly as possible. A key feature is that the method can be implemented by computing simple summaries of the current posterior, avoiding the computationally burdensome task of repeatedly performing posterior inference on hypothetical future datasets drawn from the posterior predictive. After deriving the method in a general setting, we apply it to the problem of inferring causal networks. We present a series of simulation studies in which we find that the proposed method performs favorably compared to existing alternative methods. Finally, we apply the method to real and simulated data from a protein-signaling network.",Michele Zemplenyi|Jeffrey W. Miller,Harvard University|Harvard University,https://arxiv.org/abs/2103.15229v1,https://arxiv.org/pdf/2103.15229v1,,"22 pages, 12 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2103.15229v1.pdf
2103.09303v4,2021-03-16T20:07:34Z,2021-10-01 15:39:11,Self-Validated Ensemble Models for Design of Experiments,"One of the possible objectives when designing experiments is to build or formulate a model for predicting future observations. When the primary objective is prediction, some typical approaches in the planning phase are to use well-established small-sample experimental designs in the design phase (e.g., Definitive Screening Designs) and to construct predictive models using widely used model selection algorithms such as LASSO. These design and analytic strategies, however, do not guarantee high prediction performance, partly due to the small sample sizes that prevent partitioning the data into training and validation sets, a strategy that is commonly used in machine learning models to improve out-of-sample prediction. In this work, we propose a novel framework for building high-performance predictive models from experimental data that capitalizes on the advantage of having both training and validation sets. However, instead of partitioning the data into two mutually exclusive subsets, we propose a weighting scheme based on the fractional random weight bootstrap that emulates data partitioning by assigning anti-correlated training and validation weights to each observation. The proposed methodology, called Self-Validated Ensemble Modeling (SVEM), proceeds in the spirit of bagging so that it iterates through bootstraps of anti-correlated weights and fitted models, with the final SVEM model being the average of the bootstrapped models. We investigate the performance of the SVEM algorithm with several model-building approaches such as stepwise regression, Lasso, and the Dantzig selector. Finally, through simulation and case studies, we show that SVEM generally generates models with better prediction performance in comparison to one-shot model selection approaches.",Trent Lemkus|Philip Ramsey|Christopher Gotwalt|Maria Weese,,https://arxiv.org/abs/2103.09303v4,https://arxiv.org/pdf/2103.09303v4,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2103.09303v4.pdf
2103.08973v3,2021-03-16T11:01:19Z,2021-06-01 11:05:04,Determining the maximum information gain and optimising experimental design in neutron reflectometry using the Fisher information,"An approach based on the Fisher information (FI) is developed to quantify the maximum information gain and optimal experimental design in neutron reflectometry experiments. In these experiments, the FI can be analytically calculated and used to provide sub-second predictions of parameter uncertainties. This approach can be used to influence real-time decisions about measurement angle, measurement time, contrast choice and other experimental conditions based on parameters of interest. The FI provides a lower bound on parameter estimation uncertainties and these are shown to decrease with the square root of measurement time, providing useful information for the planning and scheduling of experimental work. As the FI is computationally inexpensive to calculate, it can be computed repeatedly during the course of an experiment, saving costly beam time by signalling that sufficient data has been obtained; or saving experimental datasets by signalling that an experiment needs to continue. The approach's predictions are validated through the introduction of an experiment simulation framework that incorporates instrument-specific incident flux profiles, and through the investigation of measuring the structural properties of a phospholipid bilayer.",James H. Durant|Lucas Wilkins|Keith Butler|Joshaniel F. K. Cooper,,https://arxiv.org/abs/2103.08973v3,https://arxiv.org/pdf/2103.08973v3,https://doi.org/10.1107/S160057672100563X,Revised submission to the Journal of Applied Crystallography,"J. Appl. Cryst. (2021). 54, 1100-1110",10.1107/S160057672100563X,physics.data-an,physics.data-an|cond-mat.soft|physics.comp-ph|stat.AP,https://arxiv.org/pdf/2103.08973v3.pdf
2103.08438v2,2021-03-15T15:07:10Z,2021-09-16 16:09:42,Task-driven assessment of experimental designs in diffusion MRI: a computational framework,"This paper proposes a task-driven computational framework for assessing diffusion MRI experimental designs which, rather than relying on parameter-estimation metrics, directly measures quantitative task performance. Traditional computational experimental design (CED) methods may be ill-suited to experimental tasks, such as clinical classification, where outcome does not depend on parameter-estimation accuracy or precision alone. Current assessment metrics evaluate experiments' ability to faithfully recover microstructural parameters rather than their task performance. The method we propose addresses this shortcoming. For a given MRI experimental design, experiments are simulated start-to-finish and task performance is computed from ROC curves and associated summary metrics (e.g. AUC). Two experiments were performed: first, a validation of the pipeline's task performance predictions against clinical results, comparing in-silico predictions to real-world ROC/AUC; and second, a demonstration of the pipeline's advantages over traditional CED approaches, using two simulated clinical classification tasks. Comparison with clinical datasets validates our method's predictions of (a) the qualitative form of ROC curves, (b) the relative task performance of different experimental designs, and (c) the absolute performance of each experimental design. Furthermore, we show that our method outperforms traditional task-agnostic assessment methods, enabling improved, more useful experimental design. Compared to current approaches, such task-driven assessment is more likely to identify experimental designs that perform well in practice. Our method is not limited to diffusion MRI; the pipeline generalises to any task-based quantitative MRI application, and provides the foundation for developing future task-driven end-to end CED frameworks.",Sean C. Epstein|Timothy J. P. Bray|Margaret A. Hall-Craggs|Hui Zhang,,https://arxiv.org/abs/2103.08438v2,https://arxiv.org/pdf/2103.08438v2,https://doi.org/10.1371/journal.pone.0258442,"25 pages, 5 figures, 2 tables",,10.1371/journal.pone.0258442,physics.med-ph,physics.med-ph|eess.IV,https://arxiv.org/pdf/2103.08438v2.pdf
2103.08594v1,2021-03-14T21:10:03Z,2021-03-14 21:10:03,A Hybrid Gradient Method to Designing Bayesian Experiments for Implicit Models,"Bayesian experimental design (BED) aims at designing an experiment to maximize the information gathering from the collected data. The optimal design is usually achieved by maximizing the mutual information (MI) between the data and the model parameters. When the analytical expression of the MI is unavailable, e.g., having implicit models with intractable data distributions, a neural network-based lower bound of the MI was recently proposed and a gradient ascent method was used to maximize the lower bound. However, the approach in Kleinegesse et al., 2020 requires a pathwise sampling path to compute the gradient of the MI lower bound with respect to the design variables, and such a pathwise sampling path is usually inaccessible for implicit models. In this work, we propose a hybrid gradient approach that leverages recent advances in variational MI estimator and evolution strategies (ES) combined with black-box stochastic gradient ascent (SGA) to maximize the MI lower bound. This allows the design process to be achieved through a unified scalable procedure for implicit models without sampling path gradients. Several experiments demonstrate that our approach significantly improves the scalability of BED for implicit models in high-dimensional design space.",Jiaxin Zhang|Sirui Bi|Guannan Zhang,,https://arxiv.org/abs/2103.08594v1,https://arxiv.org/pdf/2103.08594v1,,This short paper has been accepted by NeurIPS 2020 Workshop on Machine Learning and the Physical Sciences. arXiv admin note: substantial text overlap with arXiv:2103.08026,,,cs.LG,cs.LG|stat.ME|stat.ML,https://arxiv.org/pdf/2103.08594v1.pdf
2103.08026v1,2021-03-14T20:28:51Z,2021-03-14 20:28:51,A Scalable Gradient-Free Method for Bayesian Experimental Design with Implicit Models,"Bayesian experimental design (BED) is to answer the question that how to choose designs that maximize the information gathering. For implicit models, where the likelihood is intractable but sampling is possible, conventional BED methods have difficulties in efficiently estimating the posterior distribution and maximizing the mutual information (MI) between data and parameters. Recent work proposed the use of gradient ascent to maximize a lower bound on MI to deal with these issues. However, the approach requires a sampling path to compute the pathwise gradient of the MI lower bound with respect to the design variables, and such a pathwise gradient is usually inaccessible for implicit models. In this paper, we propose a novel approach that leverages recent advances in stochastic approximate gradient ascent incorporated with a smoothed variational MI estimator for efficient and robust BED. Without the necessity of pathwise gradients, our approach allows the design process to be achieved through a unified procedure with an approximate gradient for implicit models. Several experiments show that our approach outperforms baseline methods, and significantly improves the scalability of BED in high-dimensional problems.",Jiaxin Zhang|Sirui Bi|Guannan Zhang,,https://arxiv.org/abs/2103.08026v1,https://arxiv.org/pdf/2103.08026v1,,This paper has been accepted by the 24th International Conference on Artificial Intelligence and Statistics (AISTATS 2021),,,cs.LG,cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/2103.08026v1.pdf
2103.03706v1,2021-03-05T14:31:05Z,2021-03-05 14:31:05,DOPE: D-Optimal Pooling Experimental design with application for SARS-CoV-2 screening,"Testing individuals for the presence of severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), the pathogen causing the coronavirus disease 2019 (COVID-19), is crucial for curtailing transmission chains. Moreover, rapidly testing many potentially infected individuals is often a limiting factor in controlling COVID-19 outbreaks. Hence, pooling strategies, wherein individuals are grouped and tested simultaneously, are employed. We present a novel pooling strategy that implements D-Optimal Pooling Experimental design (DOPE). DOPE defines optimal pooled tests as those maximizing the mutual information between data and infection states. We estimate said mutual information via Monte-Carlo sampling and employ a discrete optimization heuristic for maximizing it. DOPE outperforms common pooling strategies both in terms of lower error rates and fewer tests utilized. DOPE holds several additional advantages: it provides posterior distributions of the probability of infection, rather than only binary classification outcomes; it naturally incorporates prior information of infection probabilities and test error rates; and finally, it can be easily extended to include other, newly discovered information regarding COVID-19. Hence, we believe that implementation of Bayesian D-optimal experimental design holds a great promise for the efforts of combating COVID-19 and other future pandemics.",Yair Daon|Amit Huppert|Uri Obolski,,https://arxiv.org/abs/2103.03706v1,https://arxiv.org/pdf/2103.03706v1,,"18 pages, 3 figures",,,stat.AP,stat.AP|q-bio.QM|stat.CO,https://arxiv.org/pdf/2103.03706v1.pdf
2103.03280v4,2021-03-04T19:29:15Z,2022-05-16 09:57:20,Finding Efficient Trade-offs in Multi-Fidelity Response Surface Modeling,"In the context of optimization approaches to engineering applications, time-consuming simulations are often utilized which can be configured to deliver solutions for various levels of accuracy, commonly referred to as different fidelity levels. It is common practice to train hierarchical surrogate models on the objective functions in order to speed-up the optimization process. These operate under the assumption that there is a correlation between the high- and low-fidelity versions of the problem that can be exploited to cheaply gain information. In the practical scenario where the computational budget has to be allocated between multiple fidelities, limited guidelines are available to help make that division. In this paper we evaluate a range of different choices for a two-fidelity setup that provide helpful intuitions about the trade-off between evaluating in high- or low-fidelity. We present a heuristic method based on subsampling from an initial Design of Experiments (DoE) to find a suitable division of the computational budget between the fidelity levels. This enables the setup of multi-fidelity optimizations which utilize the available computational budget efficiently, independent of the multi-fidelity model used.",Sander van Rijn|Sebastian Schmitt|Matthijs van Leeuwen|Thomas Bäck,,https://arxiv.org/abs/2103.03280v4,https://arxiv.org/pdf/2103.03280v4,https://doi.org/10.1080/0305215X.2022.2052286,"12 pages, 9 figures. This is an original manuscript of an article published by Taylor & Francis in Engineering Optimization on 2022-05-16, available online: http://www.tandfonline.com/10.1080/0305215X.2022.2052286",,10.1080/0305215X.2022.2052286,cs.CE,cs.CE,https://arxiv.org/pdf/2103.03280v4.pdf
2103.02989v3,2021-03-04T12:29:46Z,2021-10-22 10:50:48,A convex approach to optimum design of experiments with correlated observations,"Optimal design of experiments for correlated processes is an increasingly relevant and active research topic. Present methods have restricted possibilities to judge their quality. To fill this gap, we complement the virtual noise approach by a convex formulation leading to an equivalence theorem comparable to the uncorrelated case and to an algorithm giving an upper performance bound against which alternative design methods can be judged. Moreover, a method for generating exact designs follows naturally. We exclusively consider estimation problems on a finite design space with a fixed number of elements. A comparison on some classical examples from the literature as well as a real application is provided.",Andrej Pázman|Markus Hainy|Werner G. Müller,,https://arxiv.org/abs/2103.02989v3,https://arxiv.org/pdf/2103.02989v3,,,,,math.ST,math.ST|stat.ME,https://arxiv.org/pdf/2103.02989v3.pdf
2103.02438v2,2021-03-03T14:43:48Z,2021-06-11 12:18:18,Deep Adaptive Design: Amortizing Sequential Bayesian Experimental Design,"We introduce Deep Adaptive Design (DAD), a method for amortizing the cost of adaptive Bayesian experimental design that allows experiments to be run in real-time. Traditional sequential Bayesian optimal experimental design approaches require substantial computation at each stage of the experiment. This makes them unsuitable for most real-world applications, where decisions must typically be made quickly. DAD addresses this restriction by learning an amortized design network upfront and then using this to rapidly run (multiple) adaptive experiments at deployment time. This network represents a design policy which takes as input the data from previous steps, and outputs the next design using a single forward pass; these design decisions can be made in milliseconds during the live experiment. To train the network, we introduce contrastive information bounds that are suitable objectives for the sequential setting, and propose a customized network architecture that exploits key symmetries. We demonstrate that DAD successfully amortizes the process of experimental design, outperforming alternative strategies on a number of problems.",Adam Foster|Desi R. Ivanova|Ilyas Malik|Tom Rainforth,,https://arxiv.org/abs/2103.02438v2,https://arxiv.org/pdf/2103.02438v2,,Published as a conference paper at ICML 2021,,,stat.ML,stat.ML|cs.AI|cs.LG|stat.CO,https://arxiv.org/pdf/2103.02438v2.pdf
2102.11108v4,2021-02-22T15:27:18Z,2022-03-07 15:55:29,Sequential Bayesian experimental design for estimation of extreme-event probability in stochastic dynamical systems,"We consider an input-to-response (ItR) system characterized by (1) parameterized input with a known probability distribution and (2) stochastic ItR function with heteroscedastic randomness. Our purpose is to efficiently quantify the extreme response probability when the ItR function is expensive to evaluate. The problem setup arises often in physics and engineering problems, with randomness in ItR coming from either intrinsic uncertainties (say, as a solution to a stochastic equation) or additional (critical) uncertainties that are not incorporated in a low-dimensional input parameter space (as a result of dimension reduction applied to the original high-dimensional input space). To reduce the required sampling numbers, we develop a sequential Bayesian experimental design method leveraging the variational heteroscedastic Gaussian process regression (VHGPR) to account for the stochastic ItR, along with a new criterion to select the next-best samples sequentially. The validity of our new method is first tested in two synthetic problems with the stochastic ItR functions defined artificially. Finally, we demonstrate the application of our method to an engineering problem of estimating the extreme ship motion probability in irregular waves, where the uncertainty in ItR naturally originates from standard wave group parameterization, which reduces the original high-dimensional wave field into a two-dimensional parameter space.",Xianliang Gong|Yulin Pan,,https://arxiv.org/abs/2102.11108v4,https://arxiv.org/pdf/2102.11108v4,https://doi.org/10.1016/j.cma.2022.114979,,,10.1016/j.cma.2022.114979,stat.ME,stat.ME|physics.flu-dyn,https://arxiv.org/pdf/2102.11108v4.pdf
2102.10237v1,2021-02-20T02:55:18Z,2021-02-20 02:55:18,Designing Experiments Informed by Observational Studies,"The increasing availability of passively observed data has yielded a growing methodological interest in ""data fusion."" These methods involve merging data from observational and experimental sources to draw causal conclusions -- and they typically require a precarious tradeoff between the unknown bias in the observational dataset and the often-large variance in the experimental dataset. We propose an alternative approach to leveraging observational data, which avoids this tradeoff: rather than using observational data for inference, we use it to design a more efficient experiment. We consider the case of a stratified experiment with a binary outcome, and suppose pilot estimates for the stratum potential outcome variances can be obtained from the observational study. We extend results from Zhao et al. (2019) in order to generate confidence sets for these variances, while accounting for the possibility of unmeasured confounding. Then, we pose the experimental design problem as one of regret minimization, subject to the constraints imposed by our confidence sets. We show that this problem can be converted into a convex minimization and solved using conventional methods. Lastly, we demonstrate the practical utility of our methods using data from the Women's Health Initiative.",Evan Rosenman|Art B. Owen,,https://arxiv.org/abs/2102.10237v1,https://arxiv.org/pdf/2102.10237v1,https://doi.org/10.1515/jci-2021-0010,,"Rosenman, E. & Owen, A. (2021). Designing experiments informed by observational studies. Journal of Causal Inference, 9(1), 147-171",10.1515/jci-2021-0010,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2102.10237v1.pdf
2102.09446v2,2021-02-18T16:08:41Z,2021-10-12 15:53:16,Experimental Designs for Accelerated Degradation Tests Based on Linear Mixed Effects Models,"Accelerated degradation tests are used to provide accurate estimation of lifetime properties of highly reliable products within a relatively short testing time. There data from particular tests at high levels of stress (e.\,g.\ temperature, voltage, or vibration) are extrapolated, through a physically meaningful model, to obtain estimates of lifetime quantiles under normal use conditions. In this work, we consider repeated measures accelerated degradation tests with multiple stress variables, where the degradation paths are assumed to follow a linear mixed effects model which is quite common in settings when repeated measures are made. We derive optimal experimental designs for minimizing the asymptotic variance for estimating the median failure time under normal use conditions when the time points for measurements are either fixed in advance or are also to be optimized.",Helmi Shat|Rainer Schwabe,,https://arxiv.org/abs/2102.09446v2,https://arxiv.org/pdf/2102.09446v2,,arXiv admin note: text overlap with arXiv:2106.09379,,,stat.AP,stat.AP,https://arxiv.org/pdf/2102.09446v2.pdf
2102.06627v2,2021-02-12T17:13:18Z,2022-01-05 17:42:51,An efficient method for goal-oriented linear Bayesian optimal experimental design: Application to optimal sensor placemen,"Optimal experimental design (OED) plays an important role in the problem of identifying uncertainty with limited experimental data. In many applications, we seek to minimize the uncertainty of a predicted quantity of interest (QoI) based on the solution of the inverse problem, rather than the inversion model parameter itself. In these scenarios, we develop an efficient method for goal-oriented optimal experimental design (GOOED) for large-scale Bayesian linear inverse problem that finds sensor locations to maximize the expected information gain (EIG) for a predicted QoI. By deriving a new formula to compute the EIG, exploiting low-rank structures of two appropriate operators, we are able to employ an online-offline decomposition scheme and a swapping greedy algorithm to maximize the EIG at a cost measured in model solutions that is independent of the problem dimensions. We provide detailed error analysis of the approximated EIG, and demonstrate the efficiency, accuracy, and both data- and parameter-dimension independence of the proposed algorithm for a contaminant transport inverse problem with infinite-dimensional parameter field.",Keyi Wu|Peng Chen|Omar Ghattas,,https://arxiv.org/abs/2102.06627v2,https://arxiv.org/pdf/2102.06627v2,,,,,math.OC,math.OC|math.NA,https://arxiv.org/pdf/2102.06627v2.pdf
2102.05954v1,2021-02-11T11:38:15Z,2021-02-11 11:38:15,Demarcating Endogenous and Exogenous Opinion Dynamics: An Experimental Design Approach,"The networked opinion diffusion in online social networks (OSN) is often governed by the two genres of opinions - endogenous opinions that are driven by the influence of social contacts among users, and exogenous opinions which are formed by external effects like news, feeds etc. Accurate demarcation of endogenous and exogenous messages offers an important cue to opinion modeling, thereby enhancing its predictive performance. In this paper, we design a suite of unsupervised classification methods based on experimental design approaches, in which, we aim to select the subsets of events which minimize different measures of mean estimation error. In more detail, we first show that these subset selection tasks are NP-Hard. Then we show that the associated objective functions are weakly submodular, which allows us to cast efficient approximation algorithms with guarantees. Finally, we validate the efficacy of our proposal on various real-world datasets crawled from Twitter as well as diverse synthetic datasets. Our experiments range from validating prediction performance on unsanitized and sanitized events to checking the effect of selecting optimal subsets of various sizes. Through various experiments, we have found that our method offers a significant improvement in accuracy in terms of opinion forecasting, against several competitors.",Paramita Koley|Avirup Saha|Sourangshu Bhattacharya|Niloy Ganguly|Abir De,,https://arxiv.org/abs/2102.05954v1,https://arxiv.org/pdf/2102.05954v1,https://doi.org/10.1145/3449361,"25 Pages, Accepted in ACM TKDD, 2021","ACM Trans. Knowl. Discov. Data. 1, 1, Article 1 (January 2021), 25 pages",10.1145/3449361,cs.SI,cs.SI|cs.AI|cs.LG,https://arxiv.org/pdf/2102.05954v1.pdf
2102.03782v2,2021-02-07T11:34:39Z,2021-10-31 21:02:38,Using Gaussian Processes to Design Dynamic Experiments for Black-Box Model Discrimination under Uncertainty,"Diverse domains of science and engineering use parameterised mechanistic models. Engineers and scientists can often hypothesise several rival models to explain a specific process or phenomenon. Consider a model discrimination setting where we wish to find the best mechanistic, dynamic model candidate and the best model parameter estimates. Typically, several rival mechanistic models can explain the available data, so design of dynamic experiments for model discrimination helps optimally collect additional data by finding experimental settings that maximise model prediction divergence. We argue there are two main approaches in the literature for solving the optimal design problem: (i) the analytical approach, using linear and Gaussian approximations to find closed-form expressions for the design objective, and (ii) the data-driven approach, which often relies on computationally intensive Monte Carlo techniques. Olofsson et al. (ICML 35, 2018) introduced Gaussian process (GP) surrogate models to hybridise the analytical and data-driven approaches, which allowed for computationally efficient design of experiments for discriminating between black-box models. In this study, we demonstrate that we can extend existing methods for optimal design of dynamic experiments to incorporate a wider range of problem uncertainty. We also extend the Olofsson et al. (2018) method of using GP surrogate models for discriminating between dynamic black-box models. We evaluate our approach on a well-known case study from literature, and explore the consequences of using GP surrogates to approximate gradient-based methods.",Simon Olofsson|Eduardo S. Schultz|Adel Mhamdi|Alexander Mitsos|Marc Peter Deisenroth|Ruth Misener,,https://arxiv.org/abs/2102.03782v2,https://arxiv.org/pdf/2102.03782v2,,,,,cs.LG,cs.LG|stat.AP,https://arxiv.org/pdf/2102.03782v2.pdf
2101.11616v1,2021-01-27T17:46:59Z,2021-01-27 17:46:59,Near-Field Millimeter Wave Vector Measurements -- Experimental Design & Measurement Interpretation,"Near-field imaging experiments exist both in optics and microwaves with often different methods and theoretical supports. For millimeter waves or THz waves, techniques from both fields can be merged to identify materials at the micron scale on the surface or in near-surface volumes. The principle of such near-field vector imaging at the frequency of 60 GHz is discussed in detail here. We develop techniques for extracting vector voltages and methods for extracting the normalized near-field vector reflection on the sample. In particular, the subharmonic IQ mixer imbalance, which produced corrupted outputs either due to amplitude or phase differences, must be taken into account and compensated for to avoid any systematic errors. We provide a method to fully characterize these imperfections and to isolate the only contribution of the near-field interaction between the probe and the sample. The effects of the mechanical modulation waveform and harmonic rank used for signal acquisition are also discussed.",Laurent Chusseau|Thibaut Auriac|Jérémy Raoult,,https://arxiv.org/abs/2101.11616v1,https://arxiv.org/pdf/2101.11616v1,,"8 pages, 11 figures, submitted to IEEE Transactions on Intrumentation Meas",,,physics.ins-det,physics.ins-det,https://arxiv.org/pdf/2101.11616v1.pdf
2101.09384v1,2021-01-23T00:12:34Z,2021-01-23 00:12:34,Algebraic Model Selection and Experimental Design in Biological Data Science,"Design of experiments and model selection, though essential steps in data science, are usually viewed as unrelated processes in the study and analysis of biological networks. Not accounting for their inter-relatedness has the potential to introduce bias and increase the risk of missing salient features in the modeling process. We propose a data-driven computational framework to unify experimental design and model selection for discrete data sets and minimal polynomial models. We use a special affine transformation, called a linear shift, to provide both the data sets and the polynomial terms that form a basis for a model. This framework enables us to address two important questions that arise in biological data science research: finding the data which identify a set of known interactions and finding identifiable interactions given a set of data. We present the theoretical foundation for a web-accessible database. As an example, we apply this methodology to a previously constructed pharmacodynamic model of epidermal derived growth factor receptor (EGFR) signaling.",Anyu Zhang|Jingzhen Hu|Qingzhong Liang|Elena S. Dimitrova|Brandilyn Stigler,,https://arxiv.org/abs/2101.09384v1,https://arxiv.org/pdf/2101.09384v1,https://doi.org/10.1016/j.aam.2021.102282,"22 pages, 8 figures, 6 tables","Advances in Applied Mathematics, 133 (2022)",10.1016/j.aam.2021.102282,math.AG,math.AG|q-bio.QM,https://arxiv.org/pdf/2101.09384v1.pdf
2101.09219v1,2021-01-22T17:07:11Z,2021-01-22 17:07:11,Two-phase approaches to optimal model-based design of experiments: how many experiments and which ones?,"Model-based experimental design is attracting increasing attention in chemical process engineering. Typically, an iterative procedure is pursued: an approximate model is devised, prescribed experiments are then performed and the resulting data is exploited to refine the model. To help to reduce the cost of trial-and-error approaches, strategies for model-based design of experiments suggest experimental points where the expected gain in information for the model is the largest. It requires the resolution of a large nonlinear, generally nonconvex, optimization problem, whose solution may greatly depend on the starting point. We present two discretization strategies that can assist the experimenter in setting the number of relevant experiments and performing an optimal selection, and we compare them against two pattern-based strategies that are independent of the problem. The validity of the approaches is demonstrated on an academic example and two test problems from chemical engineering including a vapor liquid equilibrium and reaction kinetics.",Charlie Vanaret|Philipp Seufert|Jan Schwientek|Gleb Karpov|Gleb Ryzhakov|Ivan Oseledets|Norbert Asprion|Michael Bortz,,https://arxiv.org/abs/2101.09219v1,https://arxiv.org/pdf/2101.09219v1,https://doi.org/10.1016/j.compchemeng.2020.107218,,"Computers & Chemical Engineering, Volume 146, March 2021, 107218",10.1016/j.compchemeng.2020.107218,math.OC,math.OC,https://arxiv.org/pdf/2101.09219v1.pdf
2101.05958v1,2021-01-15T03:54:12Z,2021-01-15 03:54:12,Stochastic Learning Approach to Binary Optimization for Optimal Design of Experiments,"We present a novel stochastic approach to binary optimization for optimal experimental design (OED) for Bayesian inverse problems governed by mathematical models such as partial differential equations. The OED utility function, namely, the regularized optimality criterion, is cast into a stochastic objective function in the form of an expectation over a multivariate Bernoulli distribution. The probabilistic objective is then solved by using a stochastic optimization routine to find an optimal observational policy. The proposed approach is analyzed from an optimization perspective and also from a machine learning perspective with correspondence to policy gradient reinforcement learning. The approach is demonstrated numerically by using an idealized two-dimensional Bayesian linear inverse problem, and validated by extensive numerical experiments carried out for sensor placement in a parameter identification setup.",Ahmed Attia|Sven Leyffer|Todd Munson,,https://arxiv.org/abs/2101.05958v1,https://arxiv.org/pdf/2101.05958v1,https://doi.org/10.1137/21M1404363,"34 pages, 12 figures",,10.1137/21M1404363,math.OC,math.OC|cs.LG,https://arxiv.org/pdf/2101.05958v1.pdf
2101.04034v1,2021-01-11T17:10:47Z,2021-01-11 17:10:47,Colorectal Polyp Detection in Real-world Scenario: Design and Experiment Study,"Colorectal polyps are abnormal tissues growing on the intima of the colon or rectum with a high risk of developing into colorectal cancer, the third leading cause of cancer death worldwide. Early detection and removal of colon polyps via colonoscopy have proved to be an effective approach to prevent colorectal cancer. Recently, various CNN-based computer-aided systems have been developed to help physicians detect polyps. However, these systems do not perform well in real-world colonoscopy operations due to the significant difference between images in a real colonoscopy and those in the public datasets. Unlike the well-chosen clear images with obvious polyps in the public datasets, images from a colonoscopy are often blurry and contain various artifacts such as fluid, debris, bubbles, reflection, specularity, contrast, saturation, and medical instruments, with a wide variety of polyps of different sizes, shapes, and textures. All these factors pose a significant challenge to effective polyp detection in a colonoscopy. To this end, we collect a private dataset that contains 7,313 images from 224 complete colonoscopy procedures. This dataset represents realistic operation scenarios and thus can be used to better train the models and evaluate a system's performance in practice. We propose an integrated system architecture to address the unique challenges for polyp detection. Extensive experiments results show that our system can effectively detect polyps in a colonoscopy with excellent performance in real time.",Xinzi Sun|Dechun Wang|Chenxi Zhang|Pengfei Zhang|Zinan Xiong|Yu Cao|Benyuan Liu|Xiaowei Liu|Shuijiao Chen,,https://arxiv.org/abs/2101.04034v1,https://arxiv.org/pdf/2101.04034v1,,8 pages,,,cs.CV,cs.CV,https://arxiv.org/pdf/2101.04034v1.pdf
2101.00772v1,2021-01-04T04:47:00Z,2021-01-04 04:47:00,Gaussian Function On Response Surface Estimation,"We propose a new framework for 2-D interpreting (features and samples) black-box machine learning models via a metamodeling technique, by which we study the output and input relationships of the underlying machine learning model. The metamodel can be estimated from data generated via a trained complex model by running the computer experiment on samples of data in the region of interest. We utilize a Gaussian process as a surrogate to capture the response surface of a complex model, in which we incorporate two parts in the process: interpolated values that are modeled by a stationary Gaussian process Z governed by a prior covariance function, and a mean function mu that captures the known trends in the underlying model. The optimization procedure for the variable importance parameter theta is to maximize the likelihood function. This theta corresponds to the correlation of individual variables with the target response. There is no need for any pre-assumed models since it depends on empirical observations. Experiments demonstrate the potential of the interpretable model through quantitative assessment of the predicted samples.",Mohammadhossein Toutiaee|John Miller,,https://arxiv.org/abs/2101.00772v1,https://arxiv.org/pdf/2101.00772v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2101.00772v1.pdf
2012.14941v1,2020-12-29T20:57:59Z,2020-12-29 20:57:59,The wealth of nations and the health of populations: A quasi-experimental design of the impact of sovereign debt crises on child mortality,"The wealth of nations and the health of populations are intimately strongly associated, yet the extent to which economic prosperity (GDP per capita) causes improved health remains disputed. The purpose of this article is to analyze the impact of sovereign debt crises (SDC) on child mortality, using a sample of 57 low- and middle-income countries surveyed by the Demographic and Health Survey between the years 1990 and 2015. These surveys supply 229 household data and containing about 3 million childbirth history records. This focus on SDC instead of GDP provides a quasi-experimental moment in which the influence of unobserved confounding is less than a moment analyzing the normal fluctuations of GDP. This study measures child mortality at six thresholds: neonatal, under-one (infant), under-two, under-three, under-four, and under-five mortality. Using a machine-learning (ML) model for causal inference, this study finds that while an SDC causes an adverse yet statistically insignificant effect on neonatal mortality, all other child mortality group samples are adversely affected between a probability of 0.12 to 0.14 (all statistically significant at the 95-percent threshold). Through this ML, this study also finds that the most important treatment heterogeneity moderator, in the entire adjustment set, is whether a child is born in a low-income country.",Adel Daoud,,https://arxiv.org/abs/2012.14941v1,https://arxiv.org/pdf/2012.14941v1,,,,,econ.GN,econ.GN|stat.OT,https://arxiv.org/pdf/2012.14941v1.pdf
2012.15726v1,2020-12-22T20:37:57Z,2020-12-22 20:37:57,Refined bounds for randomized experimental design,"Experimental design is an approach for selecting samples among a given set so as to obtain the best estimator for a given criterion. In the context of linear regression, several optimal designs have been derived, each associated with a different criterion: mean square error, robustness, \emph{etc}. Computing such designs is generally an NP-hard problem and one can instead rely on a convex relaxation that considers probability distributions over the samples. Although greedy strategies and rounding procedures have received a lot of attention, straightforward sampling from the optimal distribution has hardly been investigated. In this paper, we propose theoretical guarantees for randomized strategies on E and G-optimal design. To this end, we develop a new concentration inequality for the eigenvalues of random matrices using a refined version of the intrinsic dimension that enables us to quantify the performance of such randomized strategies. Finally, we evidence the validity of our analysis through experiments, with particular attention on the G-optimal design applied to the best arm identification problem for linear bandits.",Geovani Rizk|Igor Colin|Albert Thomas|Moez Draief,,https://arxiv.org/abs/2012.15726v1,https://arxiv.org/pdf/2012.15726v1,,,,,math.ST,math.ST|cs.LG|stat.ME,https://arxiv.org/pdf/2012.15726v1.pdf
2012.11949v1,2020-12-22T12:05:05Z,2020-12-22 12:05:05,Quantum-state estimation problem via optimal design of experiments,"In this paper, we study the quantum-state estimation problem in the framework of optimal design of experiments. We first find the optimal designs about arbitrary qubit models for popular optimality criteria such as A-, D-, and E-optimal designs. We also give the one-parameter family of optimality criteria which includes these criteria. We then extend a classical result in the design problem, the Kiefer-Wolfowitz theorem, to a qubit system showing the D-optimal design is equivalent to a certain type of the A-optimal design. We next compare and analyze several optimal designs based on the efficiency. We explicitly demonstrate that an optimal design for a certain criterion can be highly inefficient for other optimality criteria.",Jun Suzuki,,https://arxiv.org/abs/2012.11949v1,https://arxiv.org/pdf/2012.11949v1,https://doi.org/10.1142/S0219749920400079,"24 pages, 3 figures","Published in Special Issue on Quantum Tomography, IJQI Vol. 19, No. 08, 2040007 (2021)",10.1142/S0219749920400079,quant-ph,quant-ph,https://arxiv.org/pdf/2012.11949v1.pdf
2012.04067v1,2020-12-07T21:20:49Z,2020-12-07 21:20:49,Resource-Constrained Optimal Experimental Design,"The goal of this paper is to make Optimal Experimental Design (OED) computationally feasible for problems involving significant computational expense. We focus exclusively on the Mean Objective Cost of Uncertainty (MOCU), which is a specific methodology for OED, and we propose extensions to MOCU that leverage surrogates and adaptive sampling. We focus on reducing the computational expense associated with evaluating a large set of control policies across a large set of uncertain variables. We propose reducing the computational expense of MOCU by approximating intermediate calculations associated with each parameter/control pair with a surrogate. This surrogate is constructed from sparse sampling and (possibly) refined adaptively through a combination of sensitivity estimation and probabilistic knowledge gained directly from the experimental measurements prescribed from MOCU. We demonstrate our methods on example problems and compare performance relative to surrogate-approximated MOCU with no adaptive sampling and to full MOCU. We find evidence that adaptive sampling does improve performance, but the decision on whether to use surrogate-approximated MOCU versus full MOCU will depend on the relative expense of computation versus experimentation. If computation is more expensive than experimentation, then one should consider using our approach.",Anthony M. DeGennaro|Francis J. Alexander,,https://arxiv.org/abs/2012.04067v1,https://arxiv.org/pdf/2012.04067v1,,,,,math.OC,math.OC,https://arxiv.org/pdf/2012.04067v1.pdf
2012.12098v2,2020-12-07T13:35:54Z,2020-12-29 16:10:46,Planning of Measurement Series for Thermodynamic Properties based on Optimal Experimental Design,"Decreasing the time required for accurate thermodynamic property measurements is extremely desirable for model development that can respond to the needs of science and industry within a short time frame. Here, we demonstrate the application of optimal experimental design to measurements of thermodynamic properties. The technique is exemplified using the fitting of a Schilling-type equation from published $(p, ρ, T)$-measurements of ethylene gylcol and propylene gylcol. The analysis shows that a fixed-exponent fit using $(p,T)$-measurements along the five most informative isotherms produces models of relative density errors comparable to those obtained using the data along all investigated isotherms, \ie, eight or nine. It is also argued that a calculation of optimal isotherms prior to the measurement series can further increase the precision at no additional experimental effort.",Ophelia Frotscher|Roland Herzog|Markus Richter,,https://arxiv.org/abs/2012.12098v2,https://arxiv.org/pdf/2012.12098v2,https://doi.org/10.1007/s10765-021-02827-8,,,10.1007/s10765-021-02827-8,physics.chem-ph,physics.chem-ph,https://arxiv.org/pdf/2012.12098v2.pdf
2012.03330v2,2020-12-06T17:32:30Z,2021-02-01 14:10:28,Better Experimental Design by Hybridizing Binary Matching with Imbalance Optimization,"We present a new experimental design procedure that divides a set of experimental units into two groups in order to minimize error in estimating an additive treatment effect. One concern is minimizing error at the experimental design stage is large covariate imbalance between the two groups. Another concern is robustness of design to misspecification in response models. We address both concerns in our proposed design: we first place subjects into pairs using optimal nonbipartite matching, making our estimator robust to complicated non-linear response models. Our innovation is to keep the matched pairs extant, take differences of the covariate values within each matched pair and then we use the greedy switching heuristic of Krieger et al. (2019) or rerandomization on these differences. This latter step greatly reduce covariate imbalance to the rate $O_p(n^{-4})$ in the case of one covariate that are uniformly distributed. This rate benefits from the greedy switching heuristic which is $O_p(n^{-3})$ and the rate of matching which is $O_p(n^{-1})$. Further, our resultant designs are shown to be as random as matching which is robust to unobserved covariates. When compared to previous designs, our approach exhibits significant improvement in the mean squared error of the treatment effect estimator when the response model is nonlinear and performs at least as well when it the response model is linear. Our design procedure is found as a method in the open source R package available on CRAN called GreedyExperimentalDesign.",Abba M. Krieger|David Azriel|Adam Kapelner,,https://arxiv.org/abs/2012.03330v2,https://arxiv.org/pdf/2012.03330v2,,"18 pages, 2 tables, 2 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2012.03330v2.pdf
2012.01956v2,2020-12-03T14:30:47Z,2021-03-26 15:35:09,Cosmology with galaxy cluster weak lensing: statistical limits and experimental design,"We forecast constraints on the amplitude of matter clustering sigma_8(z) achievable with the combination of cluster weak lensing and number counts, in current and next-generation weak lensing surveys. We advocate an approach, analogous to galaxy-galaxy lensing, in which the observables in each redshift bin are the mean number counts and the mean weak lensing profile of clusters above a mass proxy threshold. The primary astrophysical nuisance parameter is the logarithmic scatter between the mass proxy and true mass near the threshold. For surveys similar to the Dark Energy Survey (DES), the Roman Space Telescope High Latitude Survey (HLS), and the Rubin Observatory Legacy Survey of Space and Time (LSST), we forecast aggregate precision on sigma_8 of 0.26%, 0.24%, and 0.10%, respectively, if the mass-observable scatter has an external prior better than 0.01. These constraints would be degraded by about 20% for a 0.05 prior on scatter in the case of DES or HLS and for a 0.016 prior for LSST. A one-month observing program with Roman Space Telescope targeting approximately 2500 massive clusters could achieve a 0.5% constraint on sigma_8(z=0.7) on its own, or a ~0.33% constraint in combination with the HLS. Realizing the constraining power of clusters requires accurate knowledge of the mass-observable relation and stringent control of systematics. We provide analytic approximations to our numerical results that allow easy scaling to other survey assumptions or other methods of cluster mass estimation.",Hao-Yi Wu|David H. Weinberg|Andrés N. Salcedo|Benjamin D. Wibking,,https://arxiv.org/abs/2012.01956v2,https://arxiv.org/pdf/2012.01956v2,https://doi.org/10.3847/1538-4357/abdc23,"23 pages, 16 figures; replaced to match published version",,10.3847/1538-4357/abdc23,astro-ph.CO,astro-ph.CO,https://arxiv.org/pdf/2012.01956v2.pdf
2012.03849v1,2020-11-25T22:25:21Z,2020-11-25 22:25:21,Correct block-design experiments mitigate temporal correlation bias in EEG classification,"It is argued in [1] that [2] was able to classify EEG responses to visual stimuli solely because of the temporal correlation that exists in all EEG data and the use of a block design. We here show that the main claim in [1] is drastically overstated and their other analyses are seriously flawed by wrong methodological choices. To validate our counter-claims, we evaluate the performance of state-of-the-art methods on the dataset in [2] reaching about 50% classification accuracy over 40 classes, lower than in [2], but still significant. We then investigate the influence of EEG temporal correlation on classification accuracy by testing the same models in two additional experimental settings: one that replicates [1]'s rapid-design experiment, and another one that examines the data between blocks while subjects are shown a blank screen. In both cases, classification accuracy is at or near chance, in contrast to what [1] reports, indicating a negligible contribution of temporal correlation to classification accuracy. We, instead, are able to replicate the results in [1] only when intentionally contaminating our data by inducing a temporal correlation. This suggests that what Li et al. [1] demonstrate is that their data are strongly contaminated by temporal correlation and low signal-to-noise ratio. We argue that the reason why Li et al. [1] observe such high correlation in EEG data is their unconventional experimental design and settings that violate the basic cognitive neuroscience design recommendations, first and foremost the one of limiting the experiments' duration, as instead done in [2]. Our analyses in this paper refute the claims of the ""perils and pitfalls of block-design"" in [1]. Finally, we conclude the paper by examining a number of other oversimplistic statements, inconsistencies, misinterpretation of machine learning concepts, speculations and misleading claims in [1].",Simone Palazzo|Concetto Spampinato|Joseph Schmidt|Isaak Kavasidis|Daniela Giordano|Mubarak Shah,,https://arxiv.org/abs/2012.03849v1,https://arxiv.org/pdf/2012.03849v1,,,,,cs.CV,cs.CV|q-bio.NC,https://arxiv.org/pdf/2012.03849v1.pdf
2011.10575v2,2020-11-20T13:39:45Z,2020-11-25 10:51:24,Design of Experiments for Verifying Biomolecular Networks,"There is a growing trend in molecular and synthetic biology of using mechanistic (non machine learning) models to design biomolecular networks. Once designed, these networks need to be validated by experimental results to ensure the theoretical network correctly models the true system. However, these experiments can be expensive and time consuming. We propose a design of experiments approach for validating these networks efficiently. Gaussian processes are used to construct a probabilistic model of the discrepancy between experimental results and the designed response, then a Bayesian optimization strategy used to select the next sample points. We compare different design criteria and develop a stopping criterion based on a metric that quantifies this discrepancy over the whole surface, and its uncertainty. We test our strategy on simulated data from computer models of biochemical processes.",Ruby Sedgwick|John Goertz|Molly Stevens|Ruth Misener|Mark van der Wilk,,https://arxiv.org/abs/2011.10575v2,https://arxiv.org/pdf/2011.10575v2,,"Comment: Updated to correct typo ""that that"" => ""that""",,,q-bio.QM,q-bio.QM|cs.LG|stat.ML,https://arxiv.org/pdf/2011.10575v2.pdf
2011.10009v2,2020-11-19T18:08:11Z,2021-04-14 12:38:52,Safe model-based design of experiments using Gaussian processes,"Construction of kinetic models has become an indispensable step in the development and scale up of processes in the industry. Model-based design of experiments (MBDoE) has been widely used for the purpose of improving parameter precision in nonlinear dynamic systems. This process needs to account for both parametric and structural uncertainty, as the feasibility constraints imposed on the system may well turn out to be violated leading to unsafe experimental conditions when an optimally designed experiment is performed. In this work, a Gaussian process is utilized in a two-fold manner: 1) to quantify the uncertainty realization of the physical system and calculate the plant-model mismatch, 2) to compute the optimal experimental design while accounting for the parametric uncertainty. This method provides a guarantee for the probabilistic satisfaction of the constraints in the context of model-based design of experiments. The method is assisted with the use of adaptive trust-regions in order to facilitate a satisfactory local approximation. The proposed method is able to allow the design of optimal experiments starting from limited preliminary knowledge of the parameter set, leading to a safe exploration of the parameter space. The performance of this method is demonstrated through illustrative case studies regarding the parameter identification of the kinetic model in flow reactors.",Panagiotis Petsagkourakis|Federico Galvanin,,https://arxiv.org/abs/2011.10009v2,https://arxiv.org/pdf/2011.10009v2,,,,,eess.SY,eess.SY|math.OC,https://arxiv.org/pdf/2011.10009v2.pdf
2011.08174v9,2020-11-16T18:58:54Z,2024-05-03 15:45:42,Policy design in experiments with unknown interference,"This paper studies experimental designs for estimation and inference on policies with spillover effects. Units are organized into a finite number of large clusters and interact in unknown ways within each cluster. First, we introduce a single-wave experiment that, by varying the randomization across cluster pairs, estimates the marginal effect of a change in treatment probabilities, taking spillover effects into account. Using the marginal effect, we propose a test for policy optimality. Second, we design a multiple-wave experiment to estimate welfare-maximizing treatment rules. We provide strong theoretical guarantees and an implementation in a large-scale field experiment.",Davide Viviano|Jess Rudder,,https://arxiv.org/abs/2011.08174v9,https://arxiv.org/pdf/2011.08174v9,,,,,econ.EM,econ.EM|cs.LG|stat.ME,https://arxiv.org/pdf/2011.08174v9.pdf
2011.04562v2,2020-11-09T17:07:26Z,2021-02-01 15:35:55,On proportional volume sampling for experimental design in general spaces,"Optimal design for linear regression is a fundamental task in statistics. For finite design spaces, recent progress has shown that random designs drawn using proportional volume sampling (PVS) lead to approximation guarantees for A-optimal design. PVS strikes the balance between design nodes that jointly fill the design space, while marginally staying in regions of high mass under the solution of a relaxed convex version of the original problem. In this paper, we examine some of the statistical implications of a new variant of PVS for (possibly Bayesian) optimal design. Using point process machinery, we treat the case of a generic Polish design space. We show that not only are the A-optimality approximation guarantees preserved, but we obtain similar guarantees for D-optimal design that tighten recent results. Moreover, we show that PVS can be sampled in polynomial time. Unfortunately, in spite of its elegance and tractability, we demonstrate on a simple example that the practical implications of general PVS are likely limited. In the second part of the paper, we focus on applications and investigate the use of PVS as a subroutine for stochastic search heuristics. We demonstrate that PVS is a robust addition to the practitioner's toolbox, especially when the regression functions are nonstandard and the design space, while low-dimensional, has a complicated shape (e.g., nonlinear boundaries, several connected components).",Arnaud Poinas|Rémi Bardenet,,https://arxiv.org/abs/2011.04562v2,https://arxiv.org/pdf/2011.04562v2,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/2011.04562v2.pdf
2011.00576v2,2020-11-01T17:59:19Z,2021-02-27 01:07:39,Experimental Design for Regret Minimization in Linear Bandits,"In this paper we propose a novel experimental design-based algorithm to minimize regret in online stochastic linear and combinatorial bandits. While existing literature tends to focus on optimism-based algorithms--which have been shown to be suboptimal in many cases--our approach carefully plans which action to take by balancing the tradeoff between information gain and reward, overcoming the failures of optimism. In addition, we leverage tools from the theory of suprema of empirical processes to obtain regret guarantees that scale with the Gaussian width of the action set, avoiding wasteful union bounds. We provide state-of-the-art finite time regret guarantees and show that our algorithm can be applied in both the bandit and semi-bandit feedback regime. In the combinatorial semi-bandit setting, we show that our algorithm is computationally efficient and relies only on calls to a linear maximization oracle. In addition, we show that with slight modification our algorithm can be used for pure exploration, obtaining state-of-the-art pure exploration guarantees in the semi-bandit setting. Finally, we provide, to the best of our knowledge, the first example where optimism fails in the semi-bandit regime, and show that in this setting our algorithm succeeds.",Andrew Wagenmaker|Julian Katz-Samuels|Kevin Jamieson,,https://arxiv.org/abs/2011.00576v2,https://arxiv.org/pdf/2011.00576v2,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2011.00576v2.pdf
2011.00098v1,2020-10-30T20:37:26Z,2020-10-30 20:37:26,Generator Controller Tuning Considering Stochastic Load Variation Using Analysis of Variance and Response Surface Method,"This article proposes a method for generator controller tuning in a power system affected by stochastic loads. The method uses the Analysis of Variance to detect the controllers with significant effect over the quality of the system response. Such quality is measured with an objective function defined as a weighted average of the Integral Absolute Error of each controller. The significant variables are then varied over a specified region in order to characterize the objective function through a regression model, which is then optimized. The method was applied to the system IEEE14 and the results were compared with benchmark parameters, showing better performance.",Frank A. Ibarra|Daniel Turizo|César Orozco-Henao|Javier Guerrero,,https://arxiv.org/abs/2011.00098v1,https://arxiv.org/pdf/2011.00098v1,https://doi.org/10.1109/ISGTEurope.2019.8905549,"5 pages, 6 figures, 2019 ISGT-Europe, Bucharest, Romania, 2019",,10.1109/ISGTEurope.2019.8905549,eess.SY,eess.SY,https://arxiv.org/pdf/2011.00098v1.pdf
2010.16262v2,2020-10-30T13:38:09Z,2020-12-15 11:12:46,Experimental design for MRI by greedy policy search,"In today's clinical practice, magnetic resonance imaging (MRI) is routinely accelerated through subsampling of the associated Fourier domain. Currently, the construction of these subsampling strategies - known as experimental design - relies primarily on heuristics. We propose to learn experimental design strategies for accelerated MRI with policy gradient methods. Unexpectedly, our experiments show that a simple greedy approximation of the objective leads to solutions nearly on-par with the more general non-greedy approach. We offer a partial explanation for this phenomenon rooted in greater variance in the non-greedy objective's gradient estimates, and experimentally verify that this variance hampers non-greedy models in adapting their policies to individual MR images. We empirically show that this adaptivity is key to improving subsampling designs.",Tim Bakker|Herke van Hoof|Max Welling,,https://arxiv.org/abs/2010.16262v2,https://arxiv.org/pdf/2010.16262v2,,"Accepted to NeurIPS 2020 (spotlight), 15-12-2020: Fixed typos, Figure 9, and pseudocode",,,cs.CV,cs.CV|cs.LG|cs.NE,https://arxiv.org/pdf/2010.16262v2.pdf
2010.15966v1,2020-10-29T22:02:18Z,2020-10-29 22:02:18,Machine Learning for Experimental Design: Methods for Improved Blocking,"Restricting randomization in the design of experiments (e.g., using blocking/stratification, pair-wise matching, or rerandomization) can improve the treatment-control balance on important covariates and therefore improve the estimation of the treatment effect, particularly for small- and medium-sized experiments. Existing guidance on how to identify these variables and implement the restrictions is incomplete and conflicting. We identify that differences are mainly due to the fact that what is important in the pre-treatment data may not translate to the post-treatment data. We highlight settings where there is sufficient data to provide clear guidance and outline improved methods to mostly automate the process using modern machine learning (ML) techniques. We show in simulations using real-world data, that these methods reduce both the mean squared error of the estimate (14%-34%) and the size of the standard error (6%-16%).",Brian Quistorff|Gentry Johnson,,https://arxiv.org/abs/2010.15966v1,https://arxiv.org/pdf/2010.15966v1,,16 pages,,,econ.EM,econ.EM,https://arxiv.org/pdf/2010.15966v1.pdf
2010.15805v2,2020-10-29T17:43:06Z,2020-12-18 05:00:04,A Local Search Framework for Experimental Design,"We present a local search framework to design and analyze both combinatorial algorithms and rounding algorithms for experimental design problems. This framework provides a unifying approach to match and improve all known results in D/A/E-design and to obtain new results in previously unknown settings.
  For combinatorial algorithms, we provide a new analysis of the classical Fedorov's exchange method. We prove that this simple local search algorithm works well as long as there exists an almost optimal solution with good condition number. Moreover, we design a new combinatorial local search algorithm for E-design using the regret minimization framework.
  For rounding algorithms, we provide a unified randomized exchange algorithm to match and improve previous results for D/A/E-design. Furthermore, the algorithm works in the more general setting to approximately satisfy multiple knapsack constraints, which can be used for weighted experimental design and for incorporating fairness constraints into experimental design.",Lap Chi Lau|Hong Zhou,,https://arxiv.org/abs/2010.15805v2,https://arxiv.org/pdf/2010.15805v2,,Improved probability bound in Theorem 1.4. A preliminary version accepted by SODA 2021,,,cs.DS,cs.DS|cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/2010.15805v2.pdf
2010.15196v1,2020-10-28T19:30:51Z,2020-10-28 19:30:51,A fast and scalable computational framework for large-scale and high-dimensional Bayesian optimal experimental design,"We develop a fast and scalable computational framework to solve large-scale and high-dimensional Bayesian optimal experimental design problems. In particular, we consider the problem of optimal observation sensor placement for Bayesian inference of high-dimensional parameters governed by partial differential equations (PDEs), which is formulated as an optimization problem that seeks to maximize an expected information gain (EIG). Such optimization problems are particularly challenging due to the curse of dimensionality for high-dimensional parameters and the expensive solution of large-scale PDEs. To address these challenges, we exploit two essential properties of such problems: the low-rank structure of the Jacobian of the parameter-to-observable map to extract the intrinsically low-dimensional data-informed subspace, and the high correlation of the approximate EIGs by a series of approximations to reduce the number of PDE solves. We propose an efficient offline-online decomposition for the optimization problem: an offline stage of computing all the quantities that require a limited number of PDE solves independent of parameter and data dimensions, and an online stage of optimizing sensor placement that does not require any PDE solve. For the online optimization, we propose a swapping greedy algorithm that first construct an initial set of sensors using leverage scores and then swap the chosen sensors with other candidates until certain convergence criteria are met. We demonstrate the efficiency and scalability of the proposed computational framework by a linear inverse problem of inferring the initial condition for an advection-diffusion equation, and a nonlinear inverse problem of inferring the diffusion coefficient of a log-normal diffusion equation, with both the parameter and data dimensions ranging from a few tens to a few thousands.",Keyi Wu|Peng Chen|Omar Ghattas,,https://arxiv.org/abs/2010.15196v1,https://arxiv.org/pdf/2010.15196v1,,,,,math.NA,math.NA|cs.CE,https://arxiv.org/pdf/2010.15196v1.pdf
2010.15602v1,2020-10-26T07:03:49Z,2020-10-26 07:03:49,Designing learning experiences for online teaching and learning,"Teaching is about constantly innovating strategies, ways and means to engage diverse students in active and meaningful learning. In line with this, SUTD adopts various student-centric teaching and learning teaching methods and approaches. This means that our graduate/undergraduate instructors have to be ready to teach using these student student-centric teaching and learning pedagogies. In this article, I share my experiences of redesigning this teaching course that is typically conducted face-to-face to a synchronous online course and also invite one of the participant in this course to reflect on his experience as a student.",Nachamma Sockalingam|Junhua Liu,,https://arxiv.org/abs/2010.15602v1,https://arxiv.org/pdf/2010.15602v1,,,,,cs.CY,cs.CY|cs.CL,https://arxiv.org/pdf/2010.15602v1.pdf
2010.09329v1,2020-10-19T09:08:22Z,2020-10-19 09:08:22,Data-driven sparse sensor placement based on A-optimal design of experiment with ADMM,"The present study proposes a sensor selection method based on the proximal splitting algorithm and the A-optimal design of experiment using the alternating direction method of multipliers (ADMM) algorithm. The performance of the proposed method was evaluated with a random sensor problem and compared with the previously proposed methods such as the greedy method and the convex relaxation. The performance of the proposed method is better than an existing method in terms of the A-optimality criterion. In addition, the proposed method requires longer computational time than the greedy method but it is quite shorter than the convex relaxation in large-scale problems. The proposed method was applied to the data-driven sparse-sensor-selection problem. A data set adopted is the NOAA OISST V2 mean sea surface temperature set. At the number of sensors larger than that of the latent state variables, the proposed method showed similar and better performances compared with previously proposed methods in terms of the A-optimality criterion and reconstruction error.",Takayuki Nagata|Taku Nonomura|Kumi Nakai|Keigo Yamada|Yuji Saito|Shunsuke Ono,,https://arxiv.org/abs/2010.09329v1,https://arxiv.org/pdf/2010.09329v1,https://doi.org/10.1109/JSEN.2021.3073978,,"IEEE Sensors Journal, Vol. 21, No. 13, pp. 15248-15257 (2021)",10.1109/JSEN.2021.3073978,eess.SY,eess.SY,https://arxiv.org/pdf/2010.09329v1.pdf
2009.12820v3,2020-09-27T11:27:49Z,2021-04-25 18:46:07,Experimental Design for Overparameterized Learning with Application to Single Shot Deep Active Learning,"The impressive performance exhibited by modern machine learning models hinges on the ability to train such models on a very large amounts of labeled data. However, since access to large volumes of labeled data is often limited or expensive, it is desirable to alleviate this bottleneck by carefully curating the training set. Optimal experimental design is a well-established paradigm for selecting data point to be labeled so to maximally inform the learning process. Unfortunately, classical theory on optimal experimental design focuses on selecting examples in order to learn underparameterized (and thus, non-interpolative) models, while modern machine learning models such as deep neural networks are overparameterized, and oftentimes are trained to be interpolative. As such, classical experimental design methods are not applicable in many modern learning setups. Indeed, the predictive performance of underparameterized models tends to be variance dominated, so classical experimental design focuses on variance reduction, while the predictive performance of overparameterized models can also be, as is shown in this paper, bias dominated or of mixed nature. In this paper we propose a design strategy that is well suited for overparameterized regression and interpolation, and we demonstrate the applicability of our method in the context of deep learning by proposing a new algorithm for single shot deep active learning.",Neta Shoham|Haim Avron,,https://arxiv.org/abs/2009.12820v3,https://arxiv.org/pdf/2009.12820v3,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2009.12820v3.pdf
2009.07582v1,2020-09-16T10:06:49Z,2020-09-16 10:06:49,Sensitivity Analysis of a Graphene Field-Effect Transistors by means of Design of Experiments,"Graphene, due to its unique electronic structure favoring high carrier mobility, is considered a promising material for use in high-speed electronic devices in the post-silicon electronic era. For this reason, experimental research on graphene-based field-effect transistors (GFETs) has rapidly increased in the last years. However, despite the continuous progress in the optimization of such devices many critical issues remain to be solved such as their reproducibility and performance uniformity against possible variations originated by the manufacturing processes or the operating conditions. In the present work, changes of the ID-VDS characteristics of a Graphene Field-Effect Transistors, caused by a tolerance of 10% in the active channel (i.e. its length and width) and in the top oxide thickness are numerically investigated in order to assess the reliability of such devices. Design of Experiments (DoE) is adopted with the aim to identify the most influential factors on the electrical performance of the device, so that the fabrication process may be suitably optimized.",Giovanni Spinelli|Patrizia Lamberti|Vincenzo Tucci|Francisco Pasadas|David Jiménez,,https://arxiv.org/abs/2009.07582v1,https://arxiv.org/pdf/2009.07582v1,https://doi.org/10.1016/j.matcom.2020.06.005,"13 pages,8 figures, 2 tables","Mathematics and Computers in Simulation, Elsevier, 2020. ISSN: 0378-4754",10.1016/j.matcom.2020.06.005,cond-mat.mes-hall,cond-mat.mes-hall,https://arxiv.org/pdf/2009.07582v1.pdf
2009.06501v1,2020-09-14T15:04:54Z,2020-09-14 15:04:54,Designing experiments for estimating an appropriate outlet size for a silo type problem,"The problem of jam formation during the discharge by gravity of granular material through a two-dimensional silo has a number of practical applications. In many problems the estimation of the minimum outlet size which guarantees that the time to the next jamming event is long enough is crucial. Assuming that the time is modeled by an exponential distribution with two unknown parameters, this goal translates to the optimal estimation of a non-linear transformation of the parameters. We obtain $c$-optimum experimental designs with that purpose, applying the graphic Elfving method. Since the optimal designs depend on the nominal values of the parameters, a sensitivity study is additionally provided. Finally, a simulation study checks the performance of the approximations made, first with the Fisher Information matrix, then with the linearization of the function to be estimated. The results are useful for experimenting in a laboratory and translating then the results to a larger scenario. Apart from the application a general methodology is developed in the paper for the problem of precise estimation of a one-dimensional parametric transformation in a non-linear model.",Jesus Lopez-Fidalgo|Caterina May|Jose Antonio Moler,,https://arxiv.org/abs/2009.06501v1,https://arxiv.org/pdf/2009.06501v1,,11 Figures,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2009.06501v1.pdf
2009.06626v1,2020-09-13T16:04:13Z,2020-09-13 16:04:13,"Optimal Bounds on Nonlinear Partial Differential Equations in Model Certification, Validation, and Experimental Design","We demonstrate that the recently developed Optimal Uncertainty Quantification (OUQ) theory, combined with recent software enabling fast global solutions of constrained non-convex optimization problems, provides a methodology for rigorous model certification, validation, and optimal design under uncertainty. In particular, we show the utility of the OUQ approach to understanding the behavior of a system that is governed by a partial differential equation -- Burgers' equation. We solve the problem of predicting shock location when we only know bounds on viscosity and on the initial conditions. Through this example, we demonstrate the potential to apply OUQ to complex physical systems, such as systems governed by coupled partial differential equations. We compare our results to those obtained using a standard Monte Carlo approach, and show that OUQ provides more accurate bounds at a lower computational cost. We discuss briefly about how to extend this approach to more complex systems, and how to integrate our approach into a more ambitious program of optimal experimental design.",M. McKerns|F. J. Alexander|K. S. Hickmann|T. J. Sullivan|D. E. Vaughan,"Information Sciences, Los Alamos National Laboratory|Computational Science Initiative, Brookhaven National Laboratory|Verification and Analysis, Los Alamos National Laboratory|Institute of Mathematics, Free University of Berlin|Verification and Analysis, Los Alamos National Laboratory",https://arxiv.org/abs/2009.06626v1,https://arxiv.org/pdf/2009.06626v1,https://doi.org/10.1142/9789811204579_0014|https://doi.org/10.1142/11389,"Preprint of an article published in the Handbook on Big Data and Machine Learning in the Physical Sciences, Volume 2: Advanced Analysis Solutions for Leading Experimental Techniques (K Kleese-van Dam, K Yager, S Campbell, R Farnsworth, and M van Dam), May 2020, World Scientific Publishing Co. Pte. Ltd","in: 978-981-120-444-9 (World Scientific, 2020)",10.1142/9789811204579_0014,math.NA,math.NA|math.PR|physics.comp-ph,https://arxiv.org/pdf/2009.06626v1.pdf
2009.03860v4,2020-09-08T16:49:15Z,2021-09-05 03:01:31,Designing Transportable Experiments,"We consider the problem of designing a randomized experiment on a source population to estimate the Average Treatment Effect (ATE) on a target population. We propose a novel approach which explicitly considers the target when designing the experiment on the source. Under the covariate shift assumption, we design an unbiased importance-weighted estimator for the target population's ATE. To reduce the variance of our estimator, we design a covariate balance condition (Target Balance) between the treatment and control groups based on the target population. We show that Target Balance achieves a higher variance reduction asymptotically than methods that do not consider the target population during the design phase. Our experiments illustrate that Target Balance reduces the variance even for small sample sizes.",My Phan|David Arbour|Drew Dimmery|Anup B. Rao,,https://arxiv.org/abs/2009.03860v4,https://arxiv.org/pdf/2009.03860v4,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2009.03860v4.pdf
2009.02176v2,2020-09-04T13:22:58Z,2021-06-25 11:34:35,Separated response surfaces for flows in parametrised domains: comparison of a priori and a posteriori PGD algorithms,"Reduced order models (ROM) are commonly employed to solve parametric problems and to devise inexpensive response surfaces to evaluate quantities of interest in real-time. There are many families of ROMs in the literature and choosing among them is not always a trivial task. This work presents a comparison of the performance of a priori and a posteriori proper generalised decomposition (PGD) algorithms for an incompressible Stokes flow problem in a geometrically parametrised domain. This problem is particularly challenging as the geometric parameters affect both the solution manifold and the computational spatial domain. The difficulty is further increased because multiple geometric parameters are considered and extended ranges of values are analysed for the parameters and this leads to significant variations in the flow features. Using a set of numerical experiments involving geometrically parametrised microswimmers, the two PGD algorithms are extensively compared in terms of their accuracy and their computational cost, expressed as a function of the number of full-order solves required.",Matteo Giacomini|Luca Borchini|Ruben Sevilla|Antonio Huerta,,https://arxiv.org/abs/2009.02176v2,https://arxiv.org/pdf/2009.02176v2,https://doi.org/10.1016/j.finel.2021.103530,"58 pages, 31 figures","Finite Elements in Analysis and Design, Vol. 196, 103530, 2021",10.1016/j.finel.2021.103530,math.NA,math.NA|cs.CE,https://arxiv.org/pdf/2009.02176v2.pdf
2008.03989v1,2020-08-10T09:42:59Z,2020-08-10 09:42:59,Optimal Bayesian experimental design for subsurface flow problems,"Optimal Bayesian design techniques provide an estimate for the best parameters of an experiment in order to maximize the value of measurements prior to the actual collection of data. In other words, these techniques explore the space of possible observations and determine an experimental setup that produces maximum information about the system parameters on average. Generally, optimal Bayesian design formulations result in multiple high-dimensional integrals that are difficult to evaluate without incurring significant computational costs as each integration point corresponds to solving a coupled system of partial differential equations. In the present work, we propose a novel approach for development of polynomial chaos expansion (PCE) surrogate model for the design utility function. In particular, we demonstrate how the orthogonality of PCE basis polynomials can be utilized in order to replace the expensive integration over the space of possible observations by direct construction of PCE approximation for the expected information gain. This novel technique enables the derivation of a reasonable quality response surface for the targeted objective function with a computational budget comparable to several single-point evaluations. Therefore, the proposed technique reduces dramatically the overall cost of optimal Bayesian experimental design. We evaluate this alternative formulation utilizing PCE on few numerical test cases with various levels of complexity to illustrate the computational advantages of the proposed approach.",Alexander Tarakanov|Ahmed H. Elsheikh,,https://arxiv.org/abs/2008.03989v1,https://arxiv.org/pdf/2008.03989v1,https://doi.org/10.1016/j.cma.2020.113208,"30 pages, 9 figures. Published in Computer Methods in Applied Mechanics and Engineering","Computer Methods in Applied Mechanics and Engineering, Volume 370, 2020, 113208, ISSN 0045-7825",10.1016/j.cma.2020.113208,physics.comp-ph,physics.comp-ph|cs.LG,https://arxiv.org/pdf/2008.03989v1.pdf
2008.00547v1,2020-08-02T19:31:18Z,2020-08-02 19:31:18,Robust Experimental Designs for Model Calibration,"A computer model can be used for predicting an output only after specifying the values of some unknown physical constants known as calibration parameters. The unknown calibration parameters can be estimated from real data by conducting physical experiments. This paper presents an approach to optimally design such a physical experiment. The problem of optimally designing physical experiment, using a computer model, is similar to the problem of finding optimal design for fitting nonlinear models. However, the problem is more challenging than the existing work on nonlinear optimal design because of the possibility of model discrepancy, that is, the computer model may not be an accurate representation of the true underlying model. Therefore, we propose an optimal design approach that is robust to potential model discrepancies. We show that our designs are better than the commonly used physical experimental designs that do not make use of the information contained in the computer model and other nonlinear optimal designs that ignore potential model discrepancies. We illustrate our approach using a toy example and a real example from industry.",Arvind Krishna|V. Roshan Joseph|Shan Ba|William A. Brenneman|William R. Myers,Georgia Institute of Technology|Georgia Institute of Technology|LinkedIn Corporation|Procter & Gamble Company|Procter & Gamble Company,https://arxiv.org/abs/2008.00547v1,https://arxiv.org/pdf/2008.00547v1,https://doi.org/10.1080/00224065.2021.1930618,"25 pages, 10 figures",,10.1080/00224065.2021.1930618,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2008.00547v1.pdf
2007.14476v3,2020-07-28T20:43:55Z,2022-06-25 12:47:52,Optimal Experimental Design for Inverse Problems in the Presence of Observation Correlations,"Optimal experimental design (OED) is the general formalism of sensor placement and decisions about the data collection strategy for engineered or natural experiments. This approach is prevalent in many critical fields such as battery design, numerical weather prediction, geosciences, and environmental and urban studies. State-of-the-art computational methods for experimental design, however, do not accommodate correlation structure in observational errors produced by many expensive-to-operate devices such as X-ray machines or radar and satellite retrievals. Discarding evident data correlations leads to biased results, poor data collection decisions, and waste of valuable resources. We present a general formulation of the OED formalism for model-constrained large-scale Bayesian linear inverse problems, where measurement errors are generally correlated. The proposed approach utilizes the Hadamard product of matrices to formulate the weighted likelihood and is valid for both finite- and infinite- dimensional Bayesian inverse problems. We also discuss widely used approaches for relaxation of the binary OED problem, in light of the proposed pointwise weighting approach, and present a clear interpretation of the relaxed design and its effect on the observational error covariance. Extensive numerical experiments are carried out for empirical verification of the proposed approach by using an advection-diffusion model, where the objective is to optimally place a small set of sensors, under a limited budget, to predict the concentration of a contaminant in a bounded domain.",Ahmed Attia|Emil Constantinescu,,https://arxiv.org/abs/2007.14476v3,https://arxiv.org/pdf/2007.14476v3,,"40 pages, 14 figures, 2 tables",,,math.OC,math.OC|stat.CO,https://arxiv.org/pdf/2007.14476v3.pdf
2007.07495v1,2020-07-15T05:58:09Z,2020-07-15 05:58:09,Experimental Design for Bathymetry Editing,We describe an application of machine learning to a real-world computer assisted labeling task. Our experimental results expose significant deviations from the IID assumption commonly used in machine learning. These results suggest that the common random split of all data into training and testing can often lead to poor performance.,Julaiti Alafate|Yoav Freund|David T. Sandwell|Brook Tozer,,https://arxiv.org/abs/2007.07495v1,https://arxiv.org/pdf/2007.07495v1,,Published as a workshop paper at ICML 2020 Workshop on Real World Experiment Design and Active Learning,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2007.07495v1.pdf
2007.06336v1,2020-07-13T12:12:07Z,2020-07-13 12:12:07,On the importance of block randomisation when designing proteomics experiments,"Randomisation is used in experimental design to reduce the prevalence of unanticipated confounders. Complete randomisation can however create unbalanced designs, for example, grouping all samples of the same condition in the same batch. Block randomisation is an approach that can prevent severe imbalances in sample allocation with respect to both known and unknown confounders. This feature provides the reader with an introduction to blocking and randomisation, insights into how to effectively organise samples during experimental design, with special considerations with respect to proteomics.",Bram Burger|Marc Vaudel|Harald Barsnes,,https://arxiv.org/abs/2007.06336v1,https://arxiv.org/pdf/2007.06336v1,,"9 pages, 4 figures",,,q-bio.QM,q-bio.QM|stat.AP,https://arxiv.org/pdf/2007.06336v1.pdf
2007.06117v2,2020-07-12T22:36:15Z,2021-03-28 02:39:13,Optimal Experimental Design for Uncertain Systems Based on Coupled Differential Equations,"We consider the optimal experimental design problem for an uncertain Kuramoto model, which consists of N interacting oscillators described by coupled ordinary differential equations. The objective is to design experiments that can effectively reduce the uncertainty present in the coupling strengths between the oscillators, thereby minimizing the cost of robust control of the uncertain Kuramoto model. We demonstrate the importance of quantifying the operational impact of the potential experiments in designing optimal experiments.",Youngjoon Hong|Bongsuk Kwon|Byung-Jun Yoon,,https://arxiv.org/abs/2007.06117v2,https://arxiv.org/pdf/2007.06117v2,https://doi.org/10.1109/ACCESS.2021.3071038,,,10.1109/ACCESS.2021.3071038,math.OC,math.OC|cs.LG,https://arxiv.org/pdf/2007.06117v2.pdf
2007.00402v1,2020-07-01T11:47:15Z,2020-07-01 11:47:15,Sequential Bayesian optimal experimental design for structural reliability analysis,"Structural reliability analysis is concerned with estimation of the probability of a critical event taking place, described by $P(g(\textbf{X}) \leq 0)$ for some $n$-dimensional random variable $\textbf{X}$ and some real-valued function $g$. In many applications the function $g$ is practically unknown, as function evaluation involves time consuming numerical simulation or some other form of experiment that is expensive to perform. The problem we address in this paper is how to optimally design experiments, in a Bayesian decision theoretic fashion, when the goal is to estimate the probability $P(g(\textbf{X}) \leq 0)$ using a minimal amount of resources. As opposed to existing methods that have been proposed for this purpose, we consider a general structural reliability model given in hierarchical form. We therefore introduce a general formulation of the experimental design problem, where we distinguish between the uncertainty related to the random variable $\textbf{X}$ and any additional epistemic uncertainty that we want to reduce through experimentation. The effectiveness of a design strategy is evaluated through a measure of residual uncertainty, and efficient approximation of this quantity is crucial if we want to apply algorithms that search for an optimal strategy. The method we propose is based on importance sampling combined with the unscented transform for epistemic uncertainty propagation. We implement this for the myopic (one-step look ahead) alternative, and demonstrate the effectiveness through a series of numerical experiments.",Christian Agrell|Kristina Rognlien Dahl,,https://arxiv.org/abs/2007.00402v1,https://arxiv.org/pdf/2007.00402v1,https://doi.org/10.1007/s11222-021-10000-2,"27 pages, 13 figures","Statistics and Computing, vol. 31, no. 27 (2021)",10.1007/s11222-021-10000-2,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2007.00402v1.pdf
2006.14888v3,2020-06-26T09:56:09Z,2021-10-14 14:40:39,Properties of restricted randomization with implications for experimental design,"Recently, there as been an increasing interest in the use of heavily restricted randomization designs which enforces balance on observed covariates in randomized controlled trials. However, when restrictions are strict, there is a risk that the treatment effect estimator will have a very high mean squared error. In this paper, we formalize this risk and propose a novel combinatoric-based approach to describe and address this issue. First, we validate our new approach by re-proving some known properties of complete randomization and restricted randomization. Second, we propose a novel diagnostic measure for restricted designs that only use the information embedded in the combinatorics of the design. Third, we show that the variance of the mean squared error of the difference-in-means estimator in a randomized experiment is a linear function of this diagnostic measure. Finally, we identify situations in which restricted designs can lead to an increased risk of getting a high mean squared error and discuss how our diagnostic measure can be used to detect such designs. Our results have implications for any restricted randomization design and can be used to evaluate the trade-off between enforcing balance on observed covariates and avoiding too restrictive designs.",Mattias Nordin|Mårten Schultzberg,,https://arxiv.org/abs/2006.14888v3,https://arxiv.org/pdf/2006.14888v3,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2006.14888v3.pdf
2006.12394v3,2020-06-22T16:21:54Z,2021-04-08 20:21:01,Output-Weighted Optimal Sampling for Bayesian Experimental Design and Uncertainty Quantification,"We introduce a class of acquisition functions for sample selection that leads to faster convergence in applications related to Bayesian experimental design and uncertainty quantification. The approach follows the paradigm of active learning, whereby existing samples of a black-box function are utilized to optimize the next most informative sample. The proposed method aims to take advantage of the fact that some input directions of the black-box function have a larger impact on the output than others, which is important especially for systems exhibiting rare and extreme events. The acquisition functions introduced in this work leverage the properties of the likelihood ratio, a quantity that acts as a probabilistic sampling weight and guides the active-learning algorithm towards regions of the input space that are deemed most relevant. We demonstrate superiority of the proposed approach in the uncertainty quantification of a hydrological system as well as the probabilistic quantification of rare events in dynamical systems and the identification of their precursors.",Antoine Blanchard|Themistoklis Sapsis,,https://arxiv.org/abs/2006.12394v3,https://arxiv.org/pdf/2006.12394v3,,,,,stat.ML,stat.ML|cs.IT|cs.LG,https://arxiv.org/pdf/2006.12394v3.pdf
2006.09670v1,2020-06-17T05:51:34Z,2020-06-17 05:51:34,LazyIter: A Fast Algorithm for Counting Markov Equivalent DAGs and Designing Experiments,"The causal relationships among a set of random variables are commonly represented by a Directed Acyclic Graph (DAG), where there is a directed edge from variable $X$ to variable $Y$ if $X$ is a direct cause of $Y$. From the purely observational data, the true causal graph can be identified up to a Markov Equivalence Class (MEC), which is a set of DAGs with the same conditional independencies between the variables. The size of an MEC is a measure of complexity for recovering the true causal graph by performing interventions. We propose a method for efficient iteration over possible MECs given intervention results. We utilize the proposed method for computing MEC sizes and experiment design in active and passive learning settings. Compared to previous work for computing the size of MEC, our proposed algorithm reduces the time complexity by a factor of $O(n)$ for sparse graphs where $n$ is the number of variables in the system. Additionally, integrating our approach with dynamic programming, we design an optimal algorithm for passive experiment design. Experimental results show that our proposed algorithms for both computing the size of MEC and experiment design outperform the state of the art.",Ali AhmadiTeshnizi|Saber Salehkaleybar|Negar Kiyavash,,https://arxiv.org/abs/2006.09670v1,https://arxiv.org/pdf/2006.09670v1,,"11 pages, 2 figures, ICML",,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/2006.09670v1.pdf
2006.06725v1,2020-06-11T18:31:24Z,2020-06-11 18:31:24,Designing Experiments: Student Learning Experience and Behaviour in Undergraduate Physics Laboratories,"We investigated physics students' learning experience and behaviour in a second-year laboratory by analyzing transcribed audio recordings of laboratory sessions. One student group was given both a problem and procedure and asked to analyze and explain their results. Another was provided with only the problem and asked to design and execute the experiment, interpret the data, and draw conclusions. These two approaches involved different levels of student inquiry and they have been described as guided and open inquiry respectively. The latter gave students more opportunities to practice ""designing experiments,"" one of the six major learning outcomes in the recommendations for the undergraduate physics laboratory curriculum by the American Association of Physics Teachers (AAPT). Qualitative analysis was performed of the audio transcripts to identify emergent themes and it was augmented by quantitative analysis for a richer understanding of students' experiences. An important finding is that significant improvements can be made to undergraduate laboratories impacting both student learning experience and behaviour by increasing the level of inquiry in laboratory experiments. This is most easily achieved by requiring students to design their own experimental procedures.",Bei Cai|Lindsay A. Mainhood|Ryan Groome|Corinne Laverty|Alastair McLean,,https://arxiv.org/abs/2006.06725v1,https://arxiv.org/pdf/2006.06725v1,https://doi.org/10.1103/PhysRevPhysEducRes.17.020109,16 pages and 2 figures. Paper is being reviewed by Physical Review Physics Education Research,"Phys. Rev. Phys. Educ. Res. 17, 020109 (2021)",10.1103/PhysRevPhysEducRes.17.020109,physics.ed-ph,physics.ed-ph,https://arxiv.org/pdf/2006.06725v1.pdf
2006.05591v2,2020-06-10T01:07:08Z,2022-12-24 16:15:57,Adaptive Experimental Design with Temporal Interference: A Maximum Likelihood Approach,"Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different inventory management algorithms in an online retail site. Standard randomized controlled trials are typically not feasible, since the goal is to estimate policy performance on the entire system. Instead, the typical current practice involves dynamically alternating between the two policies for fixed lengths of time, and comparing the average performance of each over the intervals in which they were run as an estimate of the treatment effect. However, this approach suffers from *temporal interference*: one algorithm alters the state of the system as seen by the second algorithm, biasing estimates of the treatment effect. Further, the simple non-adaptive nature of such designs implies they are not sample efficient.
  We develop a benchmark theoretical model in which to study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies). We assume estimation of the steady state reward for each chain proceeds via nonparametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efficient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martingale analysis of Markov chains via Poisson's equation, we characterize efficient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efficient online experimental design that adaptively samples the two Markov chains.",Peter Glynn|Ramesh Johari|Mohammad Rasouli,,https://arxiv.org/abs/2006.05591v2,https://arxiv.org/pdf/2006.05591v2,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2006.05591v2.pdf
2006.05021v2,2020-06-09T02:44:31Z,2021-09-14 03:55:30,CLAIMED: A CLAssification-Incorporated Minimum Energy Design to explore a multivariate response surface with feasibility constraints,"Motivated by the problem of optimization of force-field systems in physics using large-scale computer simulations, we consider exploration of a deterministic complex multivariate response surface. The objective is to find input combinations that generate output close to some desired or ""target"" vector. In spite of reducing the problem to exploration of the input space with respect to a one-dimensional loss function, the search is nontrivial and challenging due to infeasible input combinations, high dimensionalities of the input and output space and multiple ""desirable"" regions in the input space and the difficulty of emulating the objective function well with a surrogate model. We propose an approach that is based on combining machine learning techniques with smart experimental design ideas to locate multiple good regions in the input space.",Mert Y. Sengul|Yao Song|Linglin He|Adri C. T. van Duin|Ying Hung|Tirthankar Dasgupta,,https://arxiv.org/abs/2006.05021v2,https://arxiv.org/pdf/2006.05021v2,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2006.05021v2.pdf
2006.03359v1,2020-06-05T10:45:53Z,2020-06-05 10:45:53,Effects of a scalar fifth force on the dynamics of a charged particle as a new experimental design to test chameleon theories,"This article describes the dynamics of a charge particle in a electromagnetic field in presence of a scalar fifth force. Focusing to the fifth force that would be induced by a chameleon field, the profile of which can be designed properly in the laboratory, it draws its physical effects on the cyclotron motion of a particle in a static and uniform magnetic field. The fifth force induces a drift of the trajectory that is estimated analytically and compared to numerical computations for profiles motivated by the ones of a chameleon field within two nested cylinders. The magnitude of the effect and the detectability of this drift are discussed to argue that this may offer a new experimental design to test small fifth force in the laboratory. More important, at the macroscopic level it induces a current that can in principle also be measured, and would even allow one to access the transverse profile of the scalar field within the cavity. In both cases, aligning the magnetic field with the local gravity field suppresses the effects of Newtonian gravity that would be several order larger than the ones of the fifth force otherwise and the Newtonian gravity of the cavity on the particle is also argued to be negligible. Given this insight, this experimental set-up, with its two effects -- on a single particle and at the macroscopic level -- may require attention to demonstrate its actual feasability in the laboratory.",Jean-Philippe Uzan|Martin Pernot-Borràs|Joel Bergé,,https://arxiv.org/abs/2006.03359v1,https://arxiv.org/pdf/2006.03359v1,https://doi.org/10.1103/PhysRevD.102.044059,"16 pages, 21 figures","Phys. Rev. D 102, 044059 (2020)",10.1103/PhysRevD.102.044059,gr-qc,gr-qc|astro-ph.CO|hep-ph|physics.ins-det,https://arxiv.org/pdf/2006.03359v1.pdf
2006.04554v3,2020-06-03T18:58:06Z,2021-08-10 20:17:42,Batch greedy maximization of non-submodular functions: Guarantees and applications to experimental design,"We propose and analyze batch greedy heuristics for cardinality constrained maximization of non-submodular non-decreasing set functions. We consider the standard greedy paradigm, along with its distributed greedy and stochastic greedy variants. Our theoretical guarantees are characterized by the combination of submodularity and supermodularity ratios. We argue how these parameters define tight modular bounds based on incremental gains, and provide a novel reinterpretation of the classical greedy algorithm using the minorize-maximize (MM) principle. Based on that analogy, we propose a new class of methods exploiting any plausible modular bound. In the context of optimal experimental design for linear Bayesian inverse problems, we bound the submodularity and supermodularity ratios when the underlying objective is based on mutual information. We also develop novel modular bounds for the mutual information in this setting, and describe certain connections to polyhedral combinatorics. We discuss how algorithms using these modular bounds relate to established statistical notions such as leverage scores and to more recent efforts such as volume sampling. We demonstrate our theoretical findings on synthetic problems and on a real-world climate monitoring example.",Jayanth Jagalur-Mohan|Youssef Marzouk,,https://arxiv.org/abs/2006.04554v3,https://arxiv.org/pdf/2006.04554v3,,,,,math.OC,math.OC|cs.DM|cs.DS|cs.LG,https://arxiv.org/pdf/2006.04554v3.pdf
2005.12998v3,2020-05-26T19:32:29Z,2021-01-29 04:32:32,Optimal Experimental Design for Infinite-dimensional Bayesian Inverse Problems Governed by PDEs: A Review,"We present a review of methods for optimal experimental design (OED) for Bayesian inverse problems governed by partial differential equations with infinite-dimensional parameters. The focus is on problems where one seeks to optimize the placement of measurement points, at which data are collected, such that the uncertainty in the estimated parameters is minimized. We present the mathematical foundations of OED in this context and survey the computational methods for the class of OED problems under study. We also outline some directions for future research in this area.",Alen Alexanderian,,https://arxiv.org/abs/2005.12998v3,https://arxiv.org/pdf/2005.12998v3,,37 pages; minor revisions; added more references; article accepted for publication in Inverse Problems,,,math.OC,math.OC,https://arxiv.org/pdf/2005.12998v3.pdf
2005.08414v3,2020-05-18T01:02:31Z,2021-07-28 05:13:20,Unbiased MLMC stochastic gradient-based optimization of Bayesian experimental designs,"In this paper we propose an efficient stochastic optimization algorithm to search for Bayesian experimental designs such that the expected information gain is maximized. The gradient of the expected information gain with respect to experimental design parameters is given by a nested expectation, for which the standard Monte Carlo method using a fixed number of inner samples yields a biased estimator. In this paper, applying the idea of randomized multilevel Monte Carlo (MLMC) methods, we introduce an unbiased Monte Carlo estimator for the gradient of the expected information gain with finite expected squared $\ell_2$-norm and finite expected computational cost per sample. Our unbiased estimator can be combined well with stochastic gradient descent algorithms, which results in our proposal of an optimization algorithm to search for an optimal Bayesian experimental design. Numerical experiments confirm that our proposed algorithm works well not only for a simple test problem but also for a more realistic pharmacokinetic problem.",Takashi Goda|Tomohiko Hironaka|Wataru Kitade|Adam Foster,,https://arxiv.org/abs/2005.08414v3,https://arxiv.org/pdf/2005.08414v3,https://doi.org/10.1137/20M1338848,"major revision, 26 pages, 6 figures","SIAM Journal on Scientific Computing, Volume 44, Issue 1, A286-A311, 2022",10.1137/20M1338848,stat.CO,stat.CO|math.NA|stat.ML,https://arxiv.org/pdf/2005.08414v3.pdf
2005.04051v2,2020-05-07T17:06:07Z,2020-10-20 17:29:53,The numerical statistical fan for noisy experimental designs,"Identifiability of polynomial models is a key requirement for multiple regression. We consider an analogue of the so-called statistical fan, the set of all maximal identifiable hierarchical models, for cases of noisy experimental designs or measured covariate vectors with a given tolerance vector. This gives rise to the definition of the numerical statistical fan. It includes all maximal hierarchical models that avoid approximate linear dependence of the model terms. We develop an algorithm to compute the numerical statistical fan using recent results on the computation of all border bases of a design ideal from the field of algebra. The ideas are applied to data from a thermal spraying process. It turns out that the numerical statistical fan is effectively computable and much smaller than the respective statistical fan. The gained enhanced knowledge of the space of all stable identifiable hierarchical models enables improved model selection procedures.",Arkadius Kalka|Sonja Kuhnt,,https://arxiv.org/abs/2005.04051v2,https://arxiv.org/pdf/2005.04051v2,,"24 pages, 2 figures. Some proofs and more Literature were added",,,stat.ME,stat.ME|math.AC|math.NA,https://arxiv.org/pdf/2005.04051v2.pdf
2005.03151v1,2020-05-06T21:43:50Z,2020-05-06 21:43:50,On the Optimality of Randomization in Experimental Design: How to Randomize for Minimax Variance and Design-Based Inference,"I study the minimax-optimal design for a two-arm controlled experiment where conditional mean outcomes may vary in a given set. When this set is permutation symmetric, the optimal design is complete randomization, and using a single partition (i.e., the design that only randomizes the treatment labels for each side of the partition) has minimax risk larger by a factor of $n-1$. More generally, the optimal design is shown to be the mixed-strategy optimal design (MSOD) of Kallus (2018). Notably, even when the set of conditional mean outcomes has structure (i.e., is not permutation symmetric), being minimax-optimal for variance still requires randomization beyond a single partition. Nonetheless, since this targets precision, it may still not ensure sufficient uniformity in randomization to enable randomization (i.e., design-based) inference by Fisher's exact test to appropriately detect violations of null. I therefore propose the inference-constrained MSOD, which is minimax-optimal among all designs subject to such uniformity constraints. On the way, I discuss Johansson et al. (2020) who recently compared rerandomization of Morgan and Rubin (2012) and the pure-strategy optimal design (PSOD) of Kallus (2018). I point out some errors therein and set straight that randomization is minimax-optimal and that the ""no free lunch"" theorem and example in Kallus (2018) are correct.",Nathan Kallus,,https://arxiv.org/abs/2005.03151v1,https://arxiv.org/pdf/2005.03151v1,,,,,stat.ME,stat.ME|math.OC|stat.ML,https://arxiv.org/pdf/2005.03151v1.pdf
2004.12508v6,2020-04-26T23:41:33Z,2020-07-22 08:19:33,Noisy Adaptive Group Testing using Bayesian Sequential Experimental Design,"When the infection prevalence of a disease is low, Dorfman showed 80 years ago that testing groups of people can prove more efficient than testing people individually. Our goal in this paper is to propose new group testing algorithms that can operate in a noisy setting (tests can be mistaken) to decide adaptively (looking at past results) which groups to test next, with the goal to converge to a good detection, as quickly, and with as few tests as possible. We cast this problem as a Bayesian sequential experimental design problem. Using the posterior distribution of infection status vectors for $n$ patients, given observed tests carried out so far, we seek to form groups that have a maximal utility. We consider utilities such as mutual information, but also quantities that have a more direct relevance to testing, such as the AUC of the ROC curve of the test. Practically, the posterior distributions on $\{0,1\}^n$ are approximated by sequential Monte Carlo (SMC) samplers and the utility maximized by a greedy optimizer. Our procedures show in simulations significant improvements over both adaptive and non-adaptive baselines, and are far more efficient than individual tests when disease prevalence is low. Additionally, we show empirically that loopy belief propagation (LBP), widely regarded as the SoTA decoder to decide whether an individual is infected or not given previous tests, can be unreliable and exhibit oscillatory behavior. Our SMC decoder is more reliable, and can improve the performance of other group testing algorithms.",Marco Cuturi|Olivier Teboul|Quentin Berthet|Arnaud Doucet|Jean-Philippe Vert,,https://arxiv.org/abs/2004.12508v6,https://arxiv.org/pdf/2004.12508v6,,"Latest version, with updated experiments, new conclusions on LBP vs SMC decoding and new approach",,,stat.ME,stat.ME|cs.LG|stat.AP,https://arxiv.org/pdf/2004.12508v6.pdf
2004.11307v3,2020-04-23T16:41:17Z,2021-03-02 18:01:17,Designing Optimal Experiments: An Application to Proton Compton Scattering,"Interpreting measurements requires a physical theory, but the theory's accuracy may vary across the experimental domain. To optimize experimental design, and so to ensure that the substantial resources necessary for modern experiments are focused on acquiring the most valuable data, both the theory uncertainty and the expected pattern of experimental errors must be considered. We develop a Bayesian approach to this problem, and apply it to the example of proton Compton scattering. Chiral Effective Field Theory ($χ$EFT) predicts the functional form of the scattering amplitude for this reaction, so that the electromagnetic polarizabilities of the nucleon can be inferred from data. With increasing photon energy, both experimental rates and sensitivities to polarizabilities increase, but the accuracy of $χ$EFT decreases. Our physics-based model of $χ$EFT truncation errors is combined with present knowledge of the polarizabilities and reasonable assumptions about experimental capabilities at HI$γ$S and MAMI to assess the information gain from measuring specific observables at specific kinematics, \emph{i.e.}, to determine the relative amount by which new data are apt to shrink uncertainties. The strongest gains would likely come from new data on the spin observables $Σ_{2x}$ and $Σ_{2x^\prime}$ at $ω\simeq140$ to $200$ MeV and $40^\circ$ to $120^\circ$. These would tightly constrain $γ_{E1E1}-γ_{E1M2}$. New data on the differential cross section between $100$ and $200$\,MeV and over a wide angle range will substantially improve constraints on $α_{E1}-β_{M1}$, $γ_π$ and $γ_{M1M1}-γ_{M1E2}$. Good signals also exist around $160$ MeV for $Σ_3$ and $Σ_{2z^\prime}$. Such data will be pivotal in the continuing quest to pin down the scalar polarizabilities and refine understanding of the spin polarizabilities.",J. A. Melendez|R. J. Furnstahl|H. W. Griesshammer|J. A. McGovern|D. R. Phillips|M. T. Pratola,,https://arxiv.org/abs/2004.11307v3,https://arxiv.org/pdf/2004.11307v3,https://doi.org/10.1140/epja/s10050-021-00382-2,47 pages LaTeX2e (pdflatex) including 33 figures as .jpg files using includegraphics. Improved presentation with corrections do not affect the conclusions by details of the figures. Higher-resolution figures are available at https://home.gwu.edu/~hgrie/Compton/one-N-comprehensive-observables-delta4.v2.0.high-resolution-figures.tgz. Final version with minor corrections published in EPJA57(2021)81,,10.1140/epja/s10050-021-00382-2,nucl-th,nucl-th|nucl-ex|physics.data-an,https://arxiv.org/pdf/2004.11307v3.pdf
2005.05880v1,2020-04-23T11:41:00Z,2020-04-23 11:41:00,The Micro-Randomized Trial for Developing Digital Interventions: Experimental Design Considerations,"Just-in-time adaptive interventions (JITAIs) are time-varying adaptive interventions that use frequent opportunities for the intervention to be adapted such as weekly, daily, or even many times a day. This high intensity of adaptation is facilitated by the ability of digital technology to continuously collect information about an individual's current context and deliver treatments adapted to this information. The micro-randomized trial (MRT) has emerged for use in informing the construction of JITAIs. MRTs operate in, and take advantage of, the rapidly time-varying digital intervention environment. MRTs can be used to address research questions about whether and under what circumstances particular components of a JITAI are effective, with the ultimate objective of developing effective and efficient components. The purpose of this article is to clarify why, when, and how to use MRTs; to highlight elements that must be considered when designing and implementing an MRT; and to discuss the possibilities this emerging optimization trial design offers for future research in the behavioral sciences, education, and other fields. We briefly review key elements of JITAIs, and then describe three case studies of MRTs, each of which highlights research questions that can be addressed using the MRT and experimental design considerations that might arise. We also discuss a variety of considerations that go into planning and designing an MRT, using the case studies as examples.",Ashley E. Walton|Linda M. Collins|Predrag Klasnja|Inbal Nahum-Shani|Mashfiqui Rabbi|Maureen A. Walton|Susan A. Murphy,,https://arxiv.org/abs/2005.05880v1,https://arxiv.org/pdf/2005.05880v1,,,,,cs.HC,cs.HC|stat.ME,https://arxiv.org/pdf/2005.05880v1.pdf
2004.09668v2,2020-04-20T23:05:43Z,2020-05-09 13:52:50,Global Sensitivity Methods for Design of Experiments in Lithium-ion Battery Context,"Battery management systems may rely on mathematical models to provide higher performance than standard charging protocols. Electrochemical models allow us to capture the phenomena occurring inside a lithium-ion cell and therefore, could be the best model choice. However, to be of practical value, they require reliable model parameters. Uncertainty quantification and optimal experimental design concepts are essential tools for identifying systems and estimating parameters precisely. Approximation errors in uncertainty quantification result in sub-optimal experimental designs and consequently, less-informative data, and higher parameter unreliability. In this work, we propose a highly efficient design of experiment method based on global parameter sensitivities. This novel concept is applied to the single-particle model with electrolyte and thermal dynamics (SPMeT), a well-known electrochemical model for lithium-ion cells. The proposed method avoids the simplifying assumption of output-parameter linearization (i.e., local parameter sensitivities) used in conventional Fisher information matrix-based experimental design strategies. Thus, the optimized current input profile results in experimental data of higher information content and in turn, in more precise parameter estimates.",Andrea Pozzi|Xiangzhong Xie|Davide M Raimondo|René Schenkendorf,,https://arxiv.org/abs/2004.09668v2,https://arxiv.org/pdf/2004.09668v2,,Accepted for 21st IFAC World Congress,,,cs.CE,cs.CE|eess.SY,https://arxiv.org/pdf/2004.09668v2.pdf
2004.09065v2,2020-04-20T05:44:28Z,2020-07-01 03:16:58,Optimal Experimental Design for Mathematical Models of Hematopoiesis,"The hematopoietic system has a highly regulated and complex structure in which cells are organized to successfully create and maintain new blood cells. Feedback regulation is crucial to tightly control this system, but the specific mechanisms by which control is exerted are not completely understood. In this work, we aim to uncover the underlying mechanisms in hematopoiesis by conducting perturbation experiments, where animal subjects are exposed to an external agent in order to observe the system response and evolution. Developing a proper experimental design for these studies is an extremely challenging task. To address this issue, we have developed a novel Bayesian framework for optimal design of perturbation experiments. We model the numbers of hematopoietic stem and progenitor cells in mice that are exposed to a low dose of radiation. We use a differential equations model that accounts for feedback and feedforward regulation. A significant obstacle is that the experimental data are not longitudinal, rather each data point corresponds to a different animal. This model is embedded in a hierarchical framework with latent variables that capture unobserved cellular population levels. We select the optimum design based on the amount of information gain, measured by the Kullback-Leibler divergence between the probability distributions before and after observing the data. We evaluate our approach using synthetic and experimental data. We show that a proper design can lead to better estimates of model parameters even with relatively few subjects. Additionally, we demonstrate that the model parameters show a wide range of sensitivities to design options. Our method should allow scientists to find the optimal design by focusing on their specific parameters of interest and provide insight to hematopoiesis. Our approach can be extended to more complex models where latent components are used.",Luis Martinez Lomeli|Abdon Iniguez|Babak Shahbaba|John S Lowengrub|Vladimir Minin,,https://arxiv.org/abs/2004.09065v2,https://arxiv.org/pdf/2004.09065v2,,,,,stat.ME,stat.ME|q-bio.QM|stat.AP,https://arxiv.org/pdf/2004.09065v2.pdf
2004.08084v1,2020-04-17T07:05:25Z,2020-04-17 07:05:25,First-Order Methods for Optimal Experimental Design Problems with Bound Constraints,"We consider a class of convex optimization problems over the simplex of probability measures. Our framework comprises optimal experimental design (OED) problems, in which the measure over the design space indicates which experiments are being selected. Due to the presence of additional bound constraints, the measure possesses a Lebesgue density and the problem can be cast as an optimization problem over the space of essentially bounded functions. For this class of problems, we consider two first-order methods including FISTA and a proximal extrapolated gradient method, along with suitable stopping criteria. Finally, acceleration strategies targeting the dimension of the subproblems in each iteration are discussed. Numerical experiments accompany the analysis throughout the paper.",Roland Herzog|Eric Legler,,https://arxiv.org/abs/2004.08084v1,https://arxiv.org/pdf/2004.08084v1,,,,,math.NA,math.NA|math.OC,https://arxiv.org/pdf/2004.08084v1.pdf
2006.02524v1,2020-04-14T21:52:06Z,2020-04-14 21:52:06,An experimental design for the control and assembly of magnetic microwheels,"Superparamagnetic colloidal particles can be reversibly assembled into wheel-like structures called microwheels ($μ$wheels) which roll on surfaces due to friction and can be driven at user-controlled speeds and directions using rotating magnetic fields. Here, we describe the hardware and software to create and control the magnetic fields that assemble and direct wheel motion and the optics to visualize them. Motivated by portability, adaptability and low-cost, an extruded aluminum heat dissipating frame incorporating open optics and audio speaker coils outfitted with high magnetic permeability cores was constructed. Open-source software was developed to define the magnitude, frequency, and orientation of the magnetic field, allowing for real time joystick control of $μ$wheels through two-dimensional (2D) and three-dimensional (3D) fluidic environments. With this combination of hardware and software, $μ$wheels translate at speeds up to 50 $μ$m/s through sample sizes up to 5 cm x 5 cm x 5 cm using 0.75-2.5 mT magnetic fields with rotation frequencies of 5-40 Hz. Heat dissipation by aluminum coil clamps maintained sample temperatures within 3 C of ambient temperature, a range conducive for biological applications. With this design, $μ$wheels can be manipulated and imaged in 2D and 3D networks at length scales of micrometers to centimeters",E. J. Roth|C. J. Zimmermann|D. Disharoon|T. O. Tasci|D. W. M. Marr|K. B. Neeves,,https://arxiv.org/abs/2006.02524v1,https://arxiv.org/pdf/2006.02524v1,,,,,physics.app-ph,physics.app-ph|physics.flu-dyn|physics.ins-det,https://arxiv.org/pdf/2006.02524v1.pdf
2004.00792v2,2020-04-02T03:19:11Z,2020-08-04 12:58:58,Sequential online subsampling for thinning experimental designs,"We consider a design problem where experimental conditions (design points $X_i$) are presented in the form of a sequence of i.i.d.\ random variables, generated with an unknown probability measure $μ$, and only a given proportion $α\in(0,1)$ can be selected. The objective is to select good candidates $X_i$ on the fly and maximize a concave function $Φ$ of the corresponding information matrix. The optimal solution corresponds to the construction of an optimal bounded design measure $ξ_α^*\leq μ/α$, with the difficulty that $μ$ is unknown and $ξ_α^*$ must be constructed online. The construction proposed relies on the definition of a threshold $τ$ on the directional derivative of $Φ$ at the current information matrix, the value of $τ$ being fixed by a certain quantile of the distribution of this directional derivative. Combination with recursive quantile estimation yields a nonlinear two-time-scale stochastic approximation method. It can be applied to very long design sequences since only the current information matrix and estimated quantile need to be stored. Convergence to an optimum design is proved. Various illustrative examples are presented.",Luc Pronzato|HaiYing Wang,,https://arxiv.org/abs/2004.00792v2,https://arxiv.org/pdf/2004.00792v2,,"35 pages, 14 figures",,,stat.ME,stat.ME|math.ST|stat.CO,https://arxiv.org/pdf/2004.00792v2.pdf
2004.00715v2,2020-04-01T21:18:28Z,2020-06-30 11:48:28,An approximate KLD based experimental design for models with intractable likelihoods,"Data collection is a critical step in statistical inference and data science, and the goal of statistical experimental design (ED) is to find the data collection setup that can provide most information for the inference. In this work we consider a special type of ED problems where the likelihoods are not available in a closed form. In this case, the popular information-theoretic Kullback-Leibler divergence (KLD) based design criterion can not be used directly, as it requires to evaluate the likelihood function. To address the issue, we derive a new utility function, which is a lower bound of the original KLD utility. This lower bound is expressed in terms of the summation of two or more entropies in the data space, and thus can be evaluated efficiently via entropy estimation methods. We provide several numerical examples to demonstrate the performance of the proposed method.",Ziqiao Ao|Jinglai Li,,https://arxiv.org/abs/2004.00715v2,https://arxiv.org/pdf/2004.00715v2,,To appear in AISTATS 2020,,,stat.CO,stat.CO|stat.ML,https://arxiv.org/pdf/2004.00715v2.pdf
2003.13783v1,2020-03-30T19:52:27Z,2020-03-30 19:52:27,Quasi-experimental Designs for Assessing Response on Social Media to Policy Changes,"Regulation of tobacco products is rapidly evolving. Understanding public sentiment in response to changes is very important as authorities assess how to effectively protect population health. Social media systems are widely recognized to be useful for collecting data about human preferences and perceptions. However, how social media data may be used, in rapid policy change settings, given challenges of narrow time periods and specific locations and non-representative the population using social media is an open question. In this paper we apply quasi-experimental designs, which have been used previously in observational data such as social media, to control for time and location confounders on social media, and then use content analysis of Twitter and Reddit posts to illustrate the content of reactions to tobacco flavor bans and the effect of taxation on e-cigarettes. Conclusions distill the potential role of social media in settings of rapidly changing regulation, in complement to what is learned by traditional denominator-based representative surveys.",Yijun Tian|Rumi Chunara,,https://arxiv.org/abs/2003.13783v1,https://arxiv.org/pdf/2003.13783v1,,to be published in ICWSM 2020,,,cs.SI,cs.SI,https://arxiv.org/pdf/2003.13783v1.pdf
2003.10395v2,2020-03-23T17:10:32Z,2020-10-26 07:53:08,Inverse heat source problem and experimental design for determining iron loss distribution,"Iron loss determination in the magnetic core of an electrical machine, such as a motor or a transformer, is formulated as an inverse heat source problem. The sensor positions inside the object are optimized in order to minimize the uncertainty in the reconstruction in the sense of the A-optimality of Bayesian experimental design. This paper focuses on the problem formulation and an efficient numerical solution of the discretized sensor optimization and source reconstruction problems. A semirealistic linear model is discretized by finite elements and studied numerically.",Antti Hannukainen|Nuutti Hyvönen|Lauri Perkkiö,,https://arxiv.org/abs/2003.10395v2,https://arxiv.org/pdf/2003.10395v2,,"30 pages, 15 figures",,,math.NA,math.NA,https://arxiv.org/pdf/2003.10395v2.pdf
2003.09379v1,2020-03-20T16:52:10Z,2020-03-20 16:52:10,Sequential Bayesian Experimental Design for Implicit Models via Mutual Information,"Bayesian experimental design (BED) is a framework that uses statistical models and decision making under uncertainty to optimise the cost and performance of a scientific experiment. Sequential BED, as opposed to static BED, considers the scenario where we can sequentially update our beliefs about the model parameters through data gathered in the experiment. A class of models of particular interest for the natural and medical sciences are implicit models, where the data generating distribution is intractable, but sampling from it is possible. Even though there has been a lot of work on static BED for implicit models in the past few years, the notoriously difficult problem of sequential BED for implicit models has barely been touched upon. We address this gap in the literature by devising a novel sequential design framework for parameter estimation that uses the Mutual Information (MI) between model parameters and simulated data as a utility function to find optimal experimental designs, which has not been done before for implicit models. Our approach uses likelihood-free inference by ratio estimation to simultaneously estimate posterior distributions and the MI. During the sequential BED procedure we utilise Bayesian optimisation to help us optimise the MI utility. We find that our framework is efficient for the various implicit models tested, yielding accurate parameter estimates after only a few iterations.",Steven Kleinegesse|Christopher Drovandi|Michael U. Gutmann,,https://arxiv.org/abs/2003.09379v1,https://arxiv.org/pdf/2003.09379v1,,,,,stat.ML,stat.ML|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/2003.09379v1.pdf
2003.09041v1,2020-03-19T22:56:11Z,2020-03-19 22:56:11,Design and Experiments with LoCO AUV: A Low Cost Open-Source Autonomous Underwater Vehicle,"In this paper we present LoCO AUV, a Low-Cost, Open Autonomous Underwater Vehicle. LoCO is a general-purpose, single-person-deployable, vision-guided AUV, rated to a depth of 100 meters. We discuss the open and expandable design of this underwater robot, as well as the design of a simulator in Gazebo. Additionally, we explore the platform's preliminary local motion control and state estimation abilities, which enable it to perform maneuvers autonomously. In order to demonstrate its usefulness for a variety of tasks, we implement a variety of our previously presented human-robot interaction capabilities on LoCO, including gestural control, diver following, and robot communication via motion. Finally, we discuss the practical concerns of deployment and our experiences in using this robot in pools, lakes, and the ocean. All design details, instructions on assembly, and code will be released under a permissive, open-source license.",Chelsey Edge|Sadman Sakib Enan|Michael Fulton|Jungseok Hong|Jiawei Mo|Kimberly Barthelemy|Hunter Bashaw|Berik Kallevig|Corey Knutson|Kevin Orpen|Junaed Sattar,,https://arxiv.org/abs/2003.09041v1,https://arxiv.org/pdf/2003.09041v1,,"13 pages, 11 figures",,,cs.RO,cs.RO,https://arxiv.org/pdf/2003.09041v1.pdf
2003.08421v5,2020-03-18T18:22:36Z,2025-11-21 22:40:26,Experimental Design under Network Interference,"This paper studies how to design two-wave experiments in the presence of spillovers for precise inference on treatment effects. We consider units connected through a single network, local dependence among individuals, and a general class of estimands encompassing average treatment and average spillover effects. We introduce a statistical framework for designing two-wave experiments with networks, where the researcher optimizes over participants and treatment assignments to minimize the variance of the estimators of interest, using a first-wave (pilot) experiment to estimate the variance. We derive guarantees for inference on treatment effects and regret guarantees on the variance obtained from the proposed design mechanism. Our results illustrate the existence of a trade-off in the choice of the pilot study and formally characterize the pilot's size relative to the main experiment.
  Simulations using simulated and real-world networks illustrate the advantages of the method.",Davide Viviano,,https://arxiv.org/abs/2003.08421v5,https://arxiv.org/pdf/2003.08421v5,,,,,econ.EM,econ.EM|stat.ME,https://arxiv.org/pdf/2003.08421v5.pdf
2003.07315v4,2020-03-16T16:38:47Z,2021-09-23 10:54:43,Properties of using Fisher information gain for Bayesian design of experiments,"The Bayesian decision-theoretic approach to design of experiments involves specifying a design (values of all controllable variables) to maximise the expected utility function (expectation with respect to the distribution of responses and parameters). For most common utility functions, the expected utility is rarely available in closed form and requires a computationally expensive approximation which then needs to be maximised over the space of all possible designs. This hinders practical use of the Bayesian approach to find experimental designs. However, recently, a new utility called Fisher information gain has been proposed. The resulting expected Fisher information gain reduces to the prior expectation of the trace of the Fisher information matrix. Since the Fisher information is often available in closed form, this significantly simplifies approximation and subsequent identification of optimal designs. In this paper, it is shown that for exponential family models, maximising the expected Fisher information gain is equivalent to maximising an alternative objective function over a reduced-dimension space, simplifying even further the identification of optimal designs. However, if this function does not have enough global maxima, then designs that maximise the expected Fisher information gain lead to non-identifiablility.",Antony M. Overstall,,https://arxiv.org/abs/2003.07315v4,https://arxiv.org/pdf/2003.07315v4,,,,,math.ST,math.ST,https://arxiv.org/pdf/2003.07315v4.pdf
2002.11256v1,2020-02-26T01:57:36Z,2020-02-26 01:57:36,Incorporating Expert Prior Knowledge into Experimental Design via Posterior Sampling,"Scientific experiments are usually expensive due to complex experimental preparation and processing. Experimental design is therefore involved with the task of finding the optimal experimental input that results in the desirable output by using as few experiments as possible. Experimenters can often acquire the knowledge about the location of the global optimum. However, they do not know how to exploit this knowledge to accelerate experimental design. In this paper, we adopt the technique of Bayesian optimization for experimental design since Bayesian optimization has established itself as an efficient tool for optimizing expensive black-box functions. Again, it is unknown how to incorporate the expert prior knowledge about the global optimum into Bayesian optimization process. To address it, we represent the expert knowledge about the global optimum via placing a prior distribution on it and we then derive its posterior distribution. An efficient Bayesian optimization approach has been proposed via posterior sampling on the posterior distribution of the global optimum. We theoretically analyze the convergence of the proposed algorithm and discuss the robustness of incorporating expert prior. We evaluate the efficiency of our algorithm by optimizing synthetic functions and tuning hyperparameters of classifiers along with a real-world experiment on the synthesis of short polymer fiber. The results clearly demonstrate the advantages of our proposed method.",Cheng Li|Sunil Gupta|Santu Rana|Vu Nguyen|Antonio Robles-Kelly|Svetha Venkatesh,,https://arxiv.org/abs/2002.11256v1,https://arxiv.org/pdf/2002.11256v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2002.11256v1.pdf
2002.08129v3,2020-02-19T12:09:42Z,2020-08-14 15:04:46,Bayesian Experimental Design for Implicit Models by Mutual Information Neural Estimation,"Implicit stochastic models, where the data-generation distribution is intractable but sampling is possible, are ubiquitous in the natural sciences. The models typically have free parameters that need to be inferred from data collected in scientific experiments. A fundamental question is how to design the experiments so that the collected data are most useful. The field of Bayesian experimental design advocates that, ideally, we should choose designs that maximise the mutual information (MI) between the data and the parameters. For implicit models, however, this approach is severely hampered by the high computational cost of computing posteriors and maximising MI, in particular when we have more than a handful of design variables to optimise. In this paper, we propose a new approach to Bayesian experimental design for implicit models that leverages recent advances in neural MI estimation to deal with these issues. We show that training a neural network to maximise a lower bound on MI allows us to jointly determine the optimal design and the posterior. Simulation studies illustrate that this gracefully extends Bayesian experimental design for implicit models to higher design dimensions.",Steven Kleinegesse|Michael U. Gutmann,,https://arxiv.org/abs/2002.08129v3,https://arxiv.org/pdf/2002.08129v3,,Accepted at the thirty-seventh International Conference on Machine Learning (ICML) 2020. Camera-ready version,,,stat.ML,stat.ML|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/2002.08129v3.pdf
2002.05670v5,2020-02-13T17:49:42Z,2021-09-26 23:09:42,Experimental Design in Two-Sided Platforms: An Analysis of Bias,"We develop an analytical framework to study experimental design in two-sided marketplaces. Many of these experiments exhibit interference, where an intervention applied to one market participant influences the behavior of another participant. This interference leads to biased estimates of the treatment effect of the intervention. We develop a stochastic market model and associated mean field limit to capture dynamics in such experiments, and use our model to investigate how the performance of different designs and estimators is affected by marketplace interference effects. Platforms typically use two common experimental designs: demand-side (""customer"") randomization (CR) and supply-side (""listing"") randomization (LR), along with their associated estimators. We show that good experimental design depends on market balance: in highly demand-constrained markets, CR is unbiased, while LR is biased; conversely, in highly supply-constrained markets, LR is unbiased, while CR is biased. We also introduce and study a novel experimental design based on two-sided randomization (TSR) where both customers and listings are randomized to treatment and control. We show that appropriate choices of TSR designs can be unbiased in both extremes of market balance, while yielding relatively low bias in intermediate regimes of market balance.",Ramesh Johari|Hannah Li|Inessa Liskovich|Gabriel Weintraub,,https://arxiv.org/abs/2002.05670v5,https://arxiv.org/pdf/2002.05670v5,,,,,stat.ME,stat.ME|econ.EM,https://arxiv.org/pdf/2002.05670v5.pdf
2002.05308v7,2020-02-13T02:04:17Z,2025-02-20 16:32:53,Efficient Adaptive Experimental Design for Average Treatment Effect Estimation,"We study how to efficiently estimate average treatment effects (ATEs) using adaptive experiments. In adaptive experiments, experimenters sequentially assign treatments to experimental units while updating treatment assignment probabilities based on past data. We start by defining the efficient treatment-assignment probability, which minimizes the semiparametric efficiency bound for ATE estimation. Our proposed experimental design estimates and uses the efficient treatment-assignment probability to assign treatments. At the end of the proposed design, the experimenter estimates the ATE using a newly proposed Adaptive Augmented Inverse Probability Weighting (A2IPW) estimator. We show that the asymptotic variance of the A2IPW estimator using data from the proposed design achieves the minimized semiparametric efficiency bound. We also analyze the estimator's finite-sample properties and develop nonparametric and nonasymptotic confidence intervals that are valid at any round of the proposed design. These anytime valid confidence intervals allow us to conduct rate-optimal sequential hypothesis testing, allowing for early stopping and reducing necessary sample size.",Masahiro Kato|Takuya Ishihara|Junya Honda|Yusuke Narita,,https://arxiv.org/abs/2002.05308v7,https://arxiv.org/pdf/2002.05308v7,,,,,stat.ML,stat.ML|cs.LG|econ.EM,https://arxiv.org/pdf/2002.05308v7.pdf
2002.07872v1,2020-02-08T19:01:49Z,2020-02-08 19:01:49,"Elastica catastrophe machine: theory, design and experiments","The theory, the design and the experimental validation of a catastrophe machine based on a flexible element are addressed for the first time. A general theoretical framework is developed by extending that of the classical catastrophe machines made up of discrete elastic systems. The new formulation, based on the nonlinear solution of the elastica, is enhanced by considering the concept of the universal snap surface. Among the infinite set of elastica catastrophe machines, two families are proposed and investigated to explicitly assess their features. The related catastrophe locus is disclosed in a large variety of shapes, very different from those generated by the classical counterpart. Substantial changes in the catastrophe locus properties, such as convexity and number of bifurcation points, are achievable by tuning the design parameters of the proposed machines towards the design of very efficient snapping devices. Experiments performed on the physical realization of the elastica catastrophe machine fully validate the present theoretical approach. The developed model can find applications in mechanics at different scales, for instance, in the design of new devices involving actuation or hysteresis loop mechanisms to achieve energy harvesting, locomotion, and wave mitigation.",Alessandro Cazzolli|Diego Misseroni|Francesco Dal Corso,,https://arxiv.org/abs/2002.07872v1,https://arxiv.org/pdf/2002.07872v1,https://doi.org/10.1016/j.jmps.2019.103735,"31 pages, 18 figures",Journal of the Mechanics and Physics of Solids (2020): 103735,10.1016/j.jmps.2019.103735,nlin.CD,nlin.CD,https://arxiv.org/pdf/2002.07872v1.pdf
2002.01918v3,2020-02-05T18:51:25Z,2020-05-05 13:32:50,Design and Experiments with a Low-Cost Single-Motor Modular Aquatic Robot,"We present a novel design for a low-cost robotic boat powered by a single actuator, useful for both modular and swarming applications. The boat uses the conservation of angular momentum and passive flippers to convert the motion of a single motor into an adjustable paddling motion for propulsion and steering. We develop design criteria for modularity and swarming and present a prototype implementing these criteria. We identify significant mechanical sensitivities with the presented design, theorize about the cause of the sensitivities, and present an improved design for future work.",Gedaliah Knizhnik|Mark Yim,,https://arxiv.org/abs/2002.01918v3,https://arxiv.org/pdf/2002.01918v3,https://doi.org/10.1109/UR49135.2020.9144872,Accepted to the International Conference on Ubiquitous Robots (UR 2020). 8 pages,,10.1109/UR49135.2020.9144872,cs.RO,cs.RO,https://arxiv.org/pdf/2002.01918v3.pdf
2001.05849v2,2019-12-28T01:26:20Z,2021-06-07 19:33:48,Application of Deep Learning in Generating Desired Design Options: Experiments Using Synthetic Training Dataset,"Most design methods contain a forward framework, asking for primary specifications of a building to generate an output or assess its performance. However, architects urge for specific objectives though uncertain of the proper design parameters. Deep Learning (DL) algorithms provide an intelligent workflow in which the system can learn from sequential training experiments. This study applies a method using DL algorithms towards generating demanded design options. In this study, an object recognition problem is investigated to initially predict the label of unseen sample images based on training dataset consisting of different types of synthetic 2D shapes; later, a generative DL algorithm is applied to be trained and generate new shapes for given labels. In the next step, the algorithm is trained to generate a window/wall pattern for desired light/shadow performance based on the spatial daylight autonomy (sDA) metrics. The experiments show promising results both in predicting unseen sample shapes and generating new design options.",Zohreh Shaghaghian|Wei Yan,,https://arxiv.org/abs/2001.05849v2,https://arxiv.org/pdf/2001.05849v2,,"10 pages, 12 figures, 1 table",Proceedings of the 2020 Building Performance Analysis Conference and SimBuild. 2020 535-544,,cs.CV,cs.CV,https://arxiv.org/pdf/2001.05849v2.pdf
1912.08915v2,2019-12-18T22:16:31Z,2020-03-30 02:15:54,Optimal experimental design under irreducible uncertainty for linear inverse problems governed by PDEs,"We present a method for computing A-optimal sensor placements for infinite-dimensional Bayesian linear inverse problems governed by PDEs with irreducible model uncertainties. Here, irreducible uncertainties refers to uncertainties in the model that exist in addition to the parameters in the inverse problem, and that cannot be reduced through observations. Specifically, given a statistical distribution for the model uncertainties, we compute the optimal design that minimizes the expected value of the posterior covariance trace. The expected value is discretized using Monte Carlo leading to an objective function consisting of a sum of trace operators and a binary-inducing penalty. Minimization of this objective requires a large number of PDE solves in each step. To make this problem computationally tractable, we construct a composite low-rank basis using a randomized range finder algorithm to eliminate forward and adjoint PDE solves. We also present a novel formulation of the A-optimal design objective that requires the trace of an operator in the observation rather than the parameter space. The binary structure is enforced using a weighted regularized $\ell_0$-sparsification approach. We present numerical results for inference of the initial condition in a subsurface flow problem with inherent uncertainty in the flow fields and in the initial times.",Karina Koval|Alen Alexanderian|Georg Stadler,,https://arxiv.org/abs/1912.08915v2,https://arxiv.org/pdf/1912.08915v2,https://doi.org/10.1088/1361-6420/ab89c5,,,10.1088/1361-6420/ab89c5,math.OC,math.OC|math.NA|stat.ME,https://arxiv.org/pdf/1912.08915v2.pdf
1912.08567v1,2019-12-18T12:39:36Z,2019-12-18 12:39:36,Teaching Design of Experiments using Hasse diagrams,"Hasse diagrams provide a principled means for visualizing the structure of statistical designs constructed by crossing and nesting of experimental factors. They have long been applied for automated construction of linear models and their associated linear subspaces for complex designs. Here, we argue that they could also provide a central component for planning and teaching introductory or service courses in experimental design.
  Specifically, we show how Hasse diagrams allow constructing most elementary designs and finding many of their properties, such as degrees of freedom, error strata, experimental units and denominators for F-tests. Linear (mixed) models for analysis directly correspond to the diagrams, which facilitates both defining a model and specifying it in statistical software. We demonstrate how instructors can seamlessly use Hasse diagrams to construct designs by combining simple unit- and treatment structures, identify pseudo-replication, and discuss a design's randomization, unit-treatment versus treatment-treatment interactions, or complete confounding. These features commend Hasse diagrams as a powerful tool for unifying ideas and concepts.",Hans-Michael Kaltenbach,,https://arxiv.org/abs/1912.08567v1,https://arxiv.org/pdf/1912.08567v1,,18 pages; 8 figures,,,stat.AP,stat.AP,https://arxiv.org/pdf/1912.08567v1.pdf
1912.07412v1,2019-12-13T18:11:12Z,2019-12-13 18:11:12,"Optimization of Model Parameters, Uncertainty Quantification and Experimental Designs for a Global Marine Biogeochemical Model","Methods for model parameter estimation, uncertainty quantification and experimental design are summarized in this paper. They are based on the generalized least squares estimator and different approximations of its covariance matrix using the first and second derivative of the model regarding its parameters. The methods have been applied to a model for phosphate and dissolved organic phosphorus concentrations in the global ocean. As a result, model parameters have been determined which considerably improved the consistency of the model with measurement results. The uncertainties regarding the estimated model parameters caused by uncertainties in the measurement results have been quantified as well as the uncertainties associated with the corresponding model output implied by the uncertainty in the model parameters. This allows to better assess the model parameters as well as the model output. Furthermore, it has been determined to what extent new measurements can reduce these uncertainties. For this, the information content of new measurements has been predicted depending on the measured process as well as the time and the location of the measurement. This is very useful for planning new measurements.",Joscha Reimer,,https://arxiv.org/abs/1912.07412v1,https://arxiv.org/pdf/1912.07412v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1912.07412v1.pdf
1912.06622v2,2019-12-13T17:34:03Z,2019-12-26 16:03:58,Solving Optimal Experimental Design with Sequential Quadratic Programming and Chebyshev Interpolation,"We propose an optimization algorithm to compute the optimal sensor locations in experimental design in the formulation of Bayesian inverse problems, where the parameter-to-observable mapping is described through an integral equation and its discretization results in a continuously indexed matrix whose size depends on the mesh size n. By approximating the gradient and Hessian of the objective design criterion from Chebyshev interpolation, we solve a sequence of quadratic programs and achieve the complexity $\mathcal{O}(n\log^2(n))$. An error analysis guarantees the integrality gap shrinks to zero as $n\to\infty$, and we apply the algorithm on a two-dimensional advection-diffusion equation, to determine the LIDAR's optimal sensing directions for data collection.",Jing Yu|Mihai Anitescu,,https://arxiv.org/abs/1912.06622v2,https://arxiv.org/pdf/1912.06622v2,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1912.06622v2.pdf
1911.12615v2,2019-11-28T09:40:06Z,2020-12-22 22:30:27,"Data Transmission based on Exact Inverse Periodic Nonlinear Fourier Transform, Part II: Waveform Design and Experiment","The nonlinear Fourier transform has the potential to overcome limits on performance and achievable data rates which arise in modern optical fiber communication systems when nonlinear interference is treated as noise. The periodic nonlinear Fourier transform (PNFT) has been much less investigated compared to its counterpart based on vanishing boundary conditions. In this paper, we design a first experiment based on the PNFT in which information is encoded in the invariant nonlinear main spectrum. To this end, we propose a method to construct a set of periodic waveforms each having the same fixed period, by employing the exact inverse PNFT algorithm developed in Part I. We demonstrate feasibility of the transmission scheme in experiment in good agreement with simulations and obtain a bit-error ratio of $10^{-3}$ over a distance of 2000 km. It is shown that the transmission reach is significantly longer than expected from a naive estimate based on group velocity dispersion and cyclic prefix length, which is explained through a dominating solitonic component in the transmitted waveform. Our constellation design can be generalized to an arbitrary number of nonlinear degrees of freedom.",Jan-Willem Goossens|Hartmut Hafermann|Yves Jaouën,,https://arxiv.org/abs/1911.12615v2,https://arxiv.org/pdf/1911.12615v2,https://doi.org/10.1109/JLT.2020.3013163,"Final accepted version. 10 pages, 7 figures","Journal of Lightwave Technology, vol. 38, no. 23, pp. 6520-6528, 1 Dec.1, 2020",10.1109/JLT.2020.3013163,eess.SP,eess.SP|cs.IT,https://arxiv.org/pdf/1911.12615v2.pdf
1911.10231v1,2019-11-22T19:47:11Z,2019-11-22 19:47:11,Design and Experiments with a Robot-Driven Underwater Holographic Microscope for Low-Cost In Situ Particle Measurements,"Microscopic analysis of micro particles in situ in diverse water environments is necessary for monitoring water quality and localizing contamination sources. Conventional sensors such as optical microscopes and fluorometers often require complex sample preparation, are restricted to small sample volumes, and are unable to simultaneously capture all pertinent details of a sample such as particle size, shape, concentration, and three dimensional motion. In this article we propose a novel and cost-effective robotic system for mobile microscopic analysis of particles in situ at various depths which are fully controlled by the robot system itself. A miniature underwater digital in-line holographic microscope (DIHM) performs high resolution imaging of microparticles (e.g., algae cells, plastic debris, sediments) while movement allows measurement of particle distributions covering a large area of water. The main contribution of this work is the creation of a low-cost, comprehensive, and small underwater robotic holographic microscope that can assist in a variety of tasks in environmental monitoring and overall assessment of water quality such as contaminant detection and localization. The resulting system provides some unique capabilities such as expanded and systematic coverage of large bodies of water at a low cost. Several challenges such as the trade-off between image quality and cost are addressed to satisfy the aforementioned goals.",Kevin Mallery|Dario Canelon|Jiarong Hong|Nikolaos Papanikolopoulos,,https://arxiv.org/abs/1911.10231v1,https://arxiv.org/pdf/1911.10231v1,,"7 pages, 8 figures",,,cs.RO,cs.RO|physics.optics,https://arxiv.org/pdf/1911.10231v1.pdf
1911.05570v4,2019-11-13T16:00:13Z,2020-09-08 02:38:11,Kriging prediction with isotropic Matérn correlations: Robustness and experimental design,"This work investigates the prediction performance of the kriging predictors. We derive some error bounds for the prediction error in terms of non-asymptotic probability under the uniform metric and $L_p$ metrics when the spectral densities of both the true and the imposed correlation functions decay algebraically. The Matérn family is a prominent class of correlation functions of this kind. Our analysis shows that, when the smoothness of the imposed correlation function exceeds that of the true correlation function, the prediction error becomes more sensitive to the space-filling property of the design points. In particular, the kriging predictor can still reach the optimal rate of convergence, if the experimental design scheme is quasi-uniform. Lower bounds of the kriging prediction error are also derived under the uniform metric and $L_p$ metrics. An accurate characterization of this error is obtained, when an oversmoothed correlation function and a space-filling design is used.",Rui Tuo|Wenjia Wang,,https://arxiv.org/abs/1911.05570v4,https://arxiv.org/pdf/1911.05570v4,,,,,math.ST,math.ST,https://arxiv.org/pdf/1911.05570v4.pdf
1911.03764v6,2019-11-09T19:46:29Z,2023-09-26 02:47:40,Optimal Experimental Design for Staggered Rollouts,"In this paper, we study the design and analysis of experiments conducted on a set of units over multiple time periods where the starting time of the treatment may vary by unit. The design problem involves selecting an initial treatment time for each unit in order to most precisely estimate both the instantaneous and cumulative effects of the treatment. We first consider non-adaptive experiments, where all treatment assignment decisions are made prior to the start of the experiment. For this case, we show that the optimization problem is generally NP-hard, and we propose a near-optimal solution. Under this solution, the fraction entering treatment each period is initially low, then high, and finally low again. Next, we study an adaptive experimental design problem, where both the decision to continue the experiment and treatment assignment decisions are updated after each period's data is collected. For the adaptive case, we propose a new algorithm, the Precision-Guided Adaptive Experiment (PGAE) algorithm, that addresses the challenges at both the design stage and at the stage of estimating treatment effects, ensuring valid post-experiment inference accounting for the adaptive nature of the design. Using realistic settings, we demonstrate that our proposed solutions can reduce the opportunity cost of the experiments by over 50%, compared to static design benchmarks.",Ruoxuan Xiong|Susan Athey|Mohsen Bayati|Guido Imbens,,https://arxiv.org/abs/1911.03764v6,https://arxiv.org/pdf/1911.03764v6,,Forthcoming in Management Science,,,econ.EM,econ.EM|stat.ME|stat.ML,https://arxiv.org/pdf/1911.03764v6.pdf
1911.02192v2,2019-11-06T04:06:59Z,2021-02-05 19:14:59,Optimal Design of Experiments on Riemannian Manifolds,"The theory of optimal design of experiments has been traditionally developed on an Euclidean space. In this paper, new theoretical results and an algorithm for finding the optimal design of an experiment located on a Riemannian manifold are provided. It is shown that analogously to the results in Euclidean spaces, D-optimal and G-optimal designs are equivalent on manifolds, and we provide a lower bound for the maximum prediction variance of the response evaluated over the manifold. In addition, a converging algorithm that finds the optimal experimental design on manifold data is proposed. Numerical experiments demonstrate the importance of considering the manifold structure in a designed experiment when present, and the superiority of the proposed algorithm.",Hang Li|Enrique Del Castillo,,https://arxiv.org/abs/1911.02192v2,https://arxiv.org/pdf/1911.02192v2,,,10.1080/01621459.2022.2146587,,math.ST,math.ST|stat.ME,https://arxiv.org/pdf/1911.02192v2.pdf
1910.12043v1,2019-10-26T10:30:45Z,2019-10-26 10:30:45,Bayesian Experimental Design for Finding Reliable Level Set under Input Uncertainty,"In the manufacturing industry, it is often necessary to repeat expensive operational testing of machine in order to identify the range of input conditions under which the machine operates properly. Since it is often difficult to accurately control the input conditions during the actual usage of the machine, there is a need to guarantee the performance of the machine after properly incorporating the possible variation in input conditions. In this paper, we formulate this practical manufacturing scenario as an Input Uncertain Reliable Level Set Estimation (IU-rLSE) problem, and provide an efficient algorithm for solving it. The goal of IU-rLSE is to identify the input range in which the outputs smaller/greater than a desired threshold can be obtained with high probability when the input uncertainty is properly taken into consideration. We propose an active learning method to solve the IU-rLSE problem efficiently, theoretically analyze its accuracy and convergence, and illustrate its empirical performance through numerical experiments on artificial and real data.",Shogo Iwazaki|Yu Inatsu|Ichiro Takeuchi,,https://arxiv.org/abs/1910.12043v1,https://arxiv.org/pdf/1910.12043v1,,"18 pages, 8 figures",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1910.12043v1.pdf
1910.08408v3,2019-10-18T13:27:53Z,2020-06-24 13:45:46,Identification of Model Uncertainty via Optimal Design of Experiments Applied to a Mechanical Press,"In engineering applications almost all processes are described with the help of models. Especially forming machines heavily rely on mathematical models for control and condition monitoring. Inaccuracies during the modeling, manufacturing and assembly of these machines induce model uncertainty which impairs the controller's performance. In this paper we propose an approach to identify model uncertainty using parameter identification, optimal design of experiments and hypothesis testing. The experimental setup is characterized by optimal sensor positions such that specific model parameters can be determined with minimal variance. This allows for the computation of confidence regions in which the real parameters or the parameter estimates from different test sets have to lie. We claim that inconsistencies in the estimated parameter values, considering their approximated confidence ellipsoids as well, cannot be explained by data uncertainty but are indicators of model uncertainty. The proposed method is demonstrated using a component of the 3D Servo Press, a multi-technology forming machine that combines spindles with eccentric servo drives.",Tristan Gally|Peter Groche|Florian Hoppe|Anja Kuttich|Alexander Matei|Marc E. Pfetsch|Martin Rakowitsch|Stefan Ulbrich,,https://arxiv.org/abs/1910.08408v3,https://arxiv.org/pdf/1910.08408v3,https://doi.org/10.1007/s11081-021-09600-8,,,10.1007/s11081-021-09600-8,stat.ML,stat.ML|cs.LG|math.OC|stat.AP,https://arxiv.org/pdf/1910.08408v3.pdf
1910.08063v1,2019-10-17T17:51:32Z,2019-10-17 17:51:32,Bayesian analysis of multifidelity computer models with local features and non-nested experimental designs: Application to the WRF model,"We propose a multi-fidelity Bayesian emulator for the analysis of the Weather Research and Forecasting (WRF) model when the available simulations are not generated based on hierarchically nested experimental design. The proposed procedure, called Augmented Bayesian Treed Co-Kriging, extends the scope of co-kriging in two major ways. We introduce a binary treed partition latent process in the multifidelity setting to account for non-stationary and potential discontinuities in the model outputs at different fidelity levels. Moreover, we introduce an efficient imputation mechanism which allows the practical implementation of co-kriging when the experimental design is non-hierarchically nested by enabling the specification of semi-conjugate priors. Our imputation strategy allows the design of an efficient RJ-MCMC implementation that involves collapsed blocks and direct simulation from conditional distributions. We develop the Monte Carlo recursive emulator which provides a Monte Carlo proxy for the full predictive distribution of the model output at each fidelity level, in a computationally feasible manner. The performance of our method is demonstrated on a benchmark example, and compared against existing methods. The proposed method is used for the analysis of a large-scale climate modeling application which involves the WRF model.",Bledar A. Konomi|Georgios Karagiannis,,https://arxiv.org/abs/1910.08063v1,https://arxiv.org/pdf/1910.08063v1,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/1910.08063v1.pdf
1910.03962v1,2019-10-09T12:57:35Z,2019-10-09 12:57:35,Optimal experimental design via Bayesian optimization: active causal structure learning for Gaussian process networks,"We study the problem of causal discovery through targeted interventions. Starting from few observational measurements, we follow a Bayesian active learning approach to perform those experiments which, in expectation with respect to the current model, are maximally informative about the underlying causal structure. Unlike previous work, we consider the setting of continuous random variables with non-linear functional relationships, modelled with Gaussian process priors. To address the arising problem of choosing from an uncountable set of possible interventions, we propose to use Bayesian optimisation to efficiently maximise a Monte Carlo estimate of the expected information gain.",Julius von Kügelgen|Paul K Rubenstein|Bernhard Schölkopf|Adrian Weller,,https://arxiv.org/abs/1910.03962v1,https://arxiv.org/pdf/1910.03962v1,,"Working paper. Accepted as a poster at the NeurIPS 2019 workshop, ""Do the right thing"": machine learning and causal inference for improved decision making. (6 pages + references + appendix)",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1910.03962v1.pdf
1909.12570v4,2019-09-27T09:08:35Z,2021-08-09 09:38:03,Bayesian decision-theoretic design of experiments under an alternative model,"Traditionally Bayesian decision-theoretic design of experiments proceeds by choosing a design to minimise expectation of a given loss function over the space of all designs. The loss function encapsulates the aim of the experiment, and the expectation is taken with respect to the joint distribution of all unknown quantities implied by the statistical model that will be fitted to observed responses. In this paper, an extended framework is proposed whereby the expectation of the loss is taken with respect to a joint distribution implied by an alternative statistical model. Motivation for this includes promoting robustness, ensuring computational feasibility and for allowing realistic prior specification when deriving a design. To aid in exploring the new framework, an asymptotic approximation to the expected loss under an alternative model is derived, and the properties of different loss functions are established. The framework is then demonstrated on a linear regression versus full-treatment model scenario, on estimating parameters of a non-linear model under model discrepancy and a cubic spline model under an unknown number of basis functions.",Antony M. Overstall|James M. McGree,,https://arxiv.org/abs/1909.12570v4,https://arxiv.org/pdf/1909.12570v4,,Supplementary material appears as an appendix,,,stat.ME,stat.ME,https://arxiv.org/pdf/1909.12570v4.pdf
1909.09859v1,2019-09-21T17:41:12Z,2019-09-21 17:41:12,DECoVaC: Design of Experiments with Controlled Variability Components,"Reproducible research in Machine Learning has seen a salutary abundance of progress lately: workflows, transparency, and statistical analysis of validation and test performance. We build on these efforts and take them further. We offer a principled experimental design methodology, based on linear mixed models, to study and separate the effects of multiple factors of variation in machine learning experiments. This approach allows to account for the effects of architecture, optimizer, hyper-parameters, intentional randomization, as well as unintended lack of determinism across reruns. We illustrate that methodology by analyzing Matching Networks, Prototypical Networks and TADAM on the miniImagenet dataset.",Thomas Boquet|Laure Delisle|Denis Kochetkov|Nathan Schucher|Parmida Atighehchian|Boris Oreshkin|Julien Cornebise,,https://arxiv.org/abs/1909.09859v1,https://arxiv.org/pdf/1909.09859v1,,,,,stat.ME,stat.ME|cs.AI,https://arxiv.org/pdf/1909.09859v1.pdf
1909.06279v1,2019-09-13T15:05:05Z,2019-09-13 15:05:05,An Efficient Interval Uncertainty Optimization Approach Based on Quasi-sparse Response Surface,"The structure uncertainty optimization problem is usually treated as double-loop optimization process, which is computation-intensive. In this paper, an efficient interval uncertainty optimization approach based on Quasi-sparse response surface (QSRS) is proposed for structure uncertainty optimization. In which, 1) with l_1 norm and l_2 norm penalty method, a few appropriate basis functions are selected form a large number of basis functions to construct QSRS accurately and only a few sampling points is required, 2) as the orthogonal chebyshev polynomials is employed as QSRS basis functions, the local uncertainty can be evaluated by the combination of QSRS and interval arithmetic. Hence, the inner optimization process is eliminated. One mathematical problem and one engineering problem are used to validate the efficiency of the proposed approach. The results show that only 25% sampling points is needed than recently published approach.",Kefeng Wang|Pu Li|Yanfeng Zhang|Yunbao Huang,,https://arxiv.org/abs/1909.06279v1,https://arxiv.org/pdf/1909.06279v1,,"l_1 norm and l_2 norm penalty, Uncertainty optimization, Quasi-sparse response surface, Interval uncertainty",,,math.OC,math.OC,https://arxiv.org/pdf/1909.06279v1.pdf
1909.04568v3,2019-09-10T15:26:55Z,2020-02-09 06:23:53,"BINOCULARS for Efficient, Nonmyopic Sequential Experimental Design","Finite-horizon sequential experimental design (SED) arises naturally in many contexts, including hyperparameter tuning in machine learning among more traditional settings. Computing the optimal policy for such problems requires solving Bellman equations, which are generally intractable. Most existing work resorts to severely myopic approximations by limiting the decision horizon to only a single time-step, which can underweight exploration in favor of exploitation. We present BINOCULARS: Batch-Informed NOnmyopic Choices, Using Long-horizons for Adaptive, Rapid SED, a general framework for deriving efficient, nonmyopic approximations to the optimal experimental policy. Our key idea is simple and surprisingly effective: we first compute a one-step optimal batch of experiments, then select a single point from this batch to evaluate. We realize BINOCULARS for Bayesian optimization and Bayesian quadrature -- two notable SED problems with radically different objectives -- and demonstrate that BINOCULARS significantly outperforms myopic alternatives in real-world scenarios.",Shali Jiang|Henry Chai|Javier Gonzalez|Roman Garnett,,https://arxiv.org/abs/1909.04568v3,https://arxiv.org/pdf/1909.04568v3,,"13 pages, 4 figures, 6 tables, 1 algorithm",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1909.04568v3.pdf
1909.03861v1,2019-09-09T13:48:36Z,2019-09-09 13:48:36,"Bayesian Design of Experiments: Implementation, Validation and Application to Chemical Kinetics","Bayesian experimental design (BED) is a tool for guiding experiments founded on the principle of expected information gain. I.e., which experiment design will inform the most about the model can be predicted before experiments in a laboratory are conducted. BED is also useful when specific physical questions arise from the model which are answered from certain experiments but not from other experiments. BED can take two forms, and these two forms are expressed in three example models in this work. The first example takes the form of a Bayesian linear regression, but also this example is a benchmark for checking numerical and analytical solutions. One of two parameters is an estimator of the synthetic experimental data, and the BED task is choosing among which of the two parameters to inform (limited experimental observability). The second example is a chemical reaction model with a parameter space of informed reaction free energy and temperature. The temperature is an independent experimental design variable explored for information gain. The second and third examples are of the form of adjusting an independent variable in the experimental setup. The third example is a catalytic membrane reactor similar to a plug-flow reactor. For this example, a grid search over the independent variables, temperature and volume, for the greatest information gain is conducted. Also, maximum information gain is conducted is optimized with two algorithms: the differential evolution algorithm and steepest ascent, both of which benefitted in terms of initial guess from the grid search.",Eric A. Walker|Kishore Ravisankar,,https://arxiv.org/abs/1909.03861v1,https://arxiv.org/pdf/1909.03861v1,,"9 figures, supporting information, code available https://bitbucket.org/ericawalk/datascience/src/master/",,,physics.chem-ph,physics.chem-ph,https://arxiv.org/pdf/1909.03861v1.pdf
1909.03280v1,2019-09-07T14:56:56Z,2019-09-07 14:56:56,Gaussian Process and Design of Experiments for Surrogate Modeling of Optical Properties of Fractal Aggregates,"A systematic approach based on the principles of supervised learning and design of experiments concepts is introduced to build a surrogate model for estimating the optical properties of fractal aggregates. The surrogate model is built on Gaussian process (GP) regression, and the input points for the GP regression are sampled with an adaptive sequential design algorithm. The covariance functions used are the squared exponential covariance function and the Matern covariance function both with Automatic Relevance Determination (ARD). The optical property considered is extinction efficiency of soot aggregates. The strengths and weaknesses of the proposed methodology are first tested with RDG-FA. Then, surrogate models are developed for the sampled points, for which the extinction efficiency is calculated by DDA. Four different uniformly gridded databases are also constructed for comparison. It is observed that the estimations based on the surrogate model designed with Matern covariance functions is superior to the estimations based on databases in terms of the accuracy of the estimations and the total number of input points they require. Finally, a preliminary surrogate model for S 11 is built to correct RDG-FA predictions with the aim of combining the speed of RDG-FA with the accuracy of DDA.",Ozan Burak Ericok|Atay Kaan Ozbek|Ali Taylan Cemgil|Hakan Erturk,,https://arxiv.org/abs/1909.03280v1,https://arxiv.org/pdf/1909.03280v1,https://doi.org/10.1016/j.jqsrt.2019.106643,"19 pages, 8 figures",,10.1016/j.jqsrt.2019.106643,physics.optics,physics.optics|physics.comp-ph,https://arxiv.org/pdf/1909.03280v1.pdf
1908.05357v1,2019-08-14T21:40:13Z,2019-08-14 21:40:13,Sequential Computer Experimental Design for Estimating an Extreme Probability or Quantile,"A computer code can simulate a system's propagation of variation from random inputs to output measures of quality. Our aim here is to estimate a critical output tail probability or quantile without a large Monte Carlo experiment. Instead, we build a statistical surrogate for the input-output relationship with a modest number of evaluations and then sequentially add further runs, guided by a criterion to improve the estimate. We compare two criteria in the literature. Moreover, we investigate two practical questions: how to design the initial code runs and how to model the input distribution. Hence, we close the gap between the theory of sequential design and its application.",Hao Chen|William J. Welch,,https://arxiv.org/abs/1908.05357v1,https://arxiv.org/pdf/1908.05357v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1908.05357v1.pdf
1907.09065v1,2019-07-22T00:48:24Z,2019-07-22 00:48:24,Accelerating Experimental Design by Incorporating Experimenter Hunches,"Experimental design is a process of obtaining a product with target property via experimentation. Bayesian optimization offers a sample-efficient tool for experimental design when experiments are expensive. Often, expert experimenters have 'hunches' about the behavior of the experimental system, offering potentials to further improve the efficiency. In this paper, we consider per-variable monotonic trend in the underlying property that results in a unimodal trend in those variables for a target value optimization. For example, sweetness of a candy is monotonic to the sugar content. However, to obtain a target sweetness, the utility of the sugar content becomes a unimodal function, which peaks at the value giving the target sweetness and falls off both ways. In this paper, we propose a novel method to solve such problems that achieves two main objectives: a) the monotonicity information is used to the fullest extent possible, whilst ensuring that b) the convergence guarantee remains intact. This is achieved by a two-stage Gaussian process modeling, where the first stage uses the monotonicity trend to model the underlying property, and the second stage uses `virtual' samples, sampled from the first, to model the target value optimization function. The process is made theoretically consistent by adding appropriate adjustment factor in the posterior computation, necessitated because of using the `virtual' samples. The proposed method is evaluated through both simulations and real world experimental design problems of a) new short polymer fiber with the target length, and b) designing of a new three dimensional porous scaffolding with a target porosity. In all scenarios our method demonstrates faster convergence than the basic Bayesian optimization approach not using such `hunches'.",Cheng Li|Santu Rana|Sunil Gupta|Vu Nguyen|Svetha Venkatesh|Alessandra Sutti|David Rubin|Teo Slezak|Murray Height|Mazher Mohammed|Ian Gibson,,https://arxiv.org/abs/1907.09065v1,https://arxiv.org/pdf/1907.09065v1,,IEEE International Conference on Data Mining (ICDM) 2018,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/1907.09065v1.pdf
1907.06139v1,2019-07-13T22:02:33Z,2019-07-13 22:02:33,Optimal experimental design for biophysical modelling in multidimensional diffusion MRI,"Computational models of biophysical tissue properties have been widely used in diffusion MRI (dMRI) research to elucidate the link between microstructural properties and MR signal formation. For brain tissue, the research community has developed the so-called Standard Model (SM) that has been widely used. However, in clinically applicable acquisition protocols, the inverse problem that recovers the SM parameters from a set of MR diffusion measurements using pairs of short pulsed field gradients was shown to be ill-posed. Multidimensional dMRI was shown to solve this problem by combining linear and planar tensor encoding data. Given sufficient measurements, multiple choices of b-tensor sets provide enough information to estimate all SM parameters. However, in the presence of noise, some sets will provide better results. In this work, we develop a framework for optimal experimental design of multidimensional dMRI sequences applicable to the SM. This framework is based on maximising the determinant of the Fisher information matrix, which is averaged over the full SM parameter space. This averaging provides a fairly objective information metric tailored for the expected signal but that only depends on the acquisition configuration. The optimisation of this metric can be further restricted to any subclass of desirable design constraints like, for instance, hardware-specific constraints. In this work, we compute the optimal acquisitions over the set of all b-tensors with fixed eigenvectors.",Santiago Coelho|Jose M. Pozo|Sune N. Jespersen|Alejandro F. Frangi,,https://arxiv.org/abs/1907.06139v1,https://arxiv.org/pdf/1907.06139v1,,"Accepted for publication in MICCAI 2019, 9 pages, 2 figures",,,physics.med-ph,physics.med-ph|physics.bio-ph,https://arxiv.org/pdf/1907.06139v1.pdf
1907.04044v1,2019-07-09T08:42:32Z,2019-07-09 08:42:32,Optimal experimental designs for treatment contrasts in heteroscedastic models with covariates,"In clinical trials, the response of a given subject often depends on the selected treatment as well as on some covariates. We study optimal approximate designs of experiments in the models with treatment and covariate effects. We allow for the variances of the responses to depend on the chosen treatments, which introduces heteroscedasticity into the models. For estimating systems of treatment contrasts and linear functions of the covariates, we extend known results on D-optimality of product designs by providing product designs that are optimal with respect to general eigenvalue-based criteria. In particular, A- and E-optimal product designs are obtained. We then formulate a method based on linear programming for constructing optimal designs with smaller supports from the optimal product designs. The sparser designs can be more easily converted to practically applicable exact designs. The provided results and the proposed sparsification method are demonstrated on some examples.",Samuel Rosa,,https://arxiv.org/abs/1907.04044v1,https://arxiv.org/pdf/1907.04044v1,,"21 pages, 2 figures, 7 tables",,,math.ST,math.ST,https://arxiv.org/pdf/1907.04044v1.pdf
1907.02179v2,2019-07-04T01:18:20Z,2020-04-28 13:03:22,Sequential Experimental Design for Predator-Prey Functional Response Experiments,"Understanding functional response within a predator-prey dynamic is a cornerstone for many quantitative ecological studies. Over the past 60 years, the methodology for modelling functional response has gradually transitioned from the classic mechanistic models to more statistically oriented models. To obtain inferences on these statistical models, a substantial number of experiments need to be conducted. The obvious disadvantages of collecting this volume of data include cost, time and the sacrificing of animals. Therefore, optimally designed experiments are useful as they may reduce the total number of experimental runs required to attain the same statistical results. In this paper, we develop the first sequential experimental design method for predator-prey functional response experiments. To make inferences on the parameters in each of the statistical models we consider, we use sequential Monte Carlo, which is computationally efficient and facilitates convenient estimation of important utility functions. It provides coverage of experimental goals including parameter estimation, model discrimination as well as a combination of these. The results of our simulation study illustrate that for predator-prey functional response experiments sequential design outperforms static design for our experimental goals. R code for implementing the methodology is available via https://github.com/haydenmoffat/sequential_design_for_predator_prey_experiments.",Hayden Moffat|Markus Hainy|Nikos E. Papanikolaou|Christopher Drovandi,,https://arxiv.org/abs/1907.02179v2,https://arxiv.org/pdf/1907.02179v2,,"Main Text: 23 pages, 7 Figures - Supplementary Text: 11 pages, 5 Figures",,,stat.AP,stat.AP|stat.CO,https://arxiv.org/pdf/1907.02179v2.pdf
1906.09737v2,2019-06-24T06:03:34Z,2020-06-28 10:21:37,Quantum State Discrimination as Bayesian Experimental Design,"We show that quantum state discrimination sits neatly in the framework of Bayesian experimental design. In this setting, the two main branches of quantum state discrimination (minimal error and maximal confidence) simply correspond to two different utility functions. This view allows straightforward extensions and mixing of different discrimination tasks by examining the utility functions, and to describe multi-objective discrimination tasks. In addition, the probability of success and the total confidence are resource monotones quantifying the usefulness of measurements. We give general conditions under which utility functions lead to resource monotones in the resource theory of quantum measurement.",Thomas Guff|Yuval R. Sanders|Nathan A. McMahon|Alexei Gilchrist,,https://arxiv.org/abs/1906.09737v2,https://arxiv.org/pdf/1906.09737v2,,"13 pages, 1 figure",,,quant-ph,quant-ph,https://arxiv.org/pdf/1906.09737v2.pdf
1906.08399v1,2019-06-20T00:38:06Z,2019-06-20 00:38:06,Sequential Experimental Design for Transductive Linear Bandits,"In this paper we introduce the transductive linear bandit problem: given a set of measurement vectors $\mathcal{X}\subset \mathbb{R}^d$, a set of items $\mathcal{Z}\subset \mathbb{R}^d$, a fixed confidence $δ$, and an unknown vector $θ^{\ast}\in \mathbb{R}^d$, the goal is to infer $\text{argmax}_{z\in \mathcal{Z}} z^\topθ^\ast$ with probability $1-δ$ by making as few sequentially chosen noisy measurements of the form $x^\topθ^{\ast}$ as possible. When $\mathcal{X}=\mathcal{Z}$, this setting generalizes linear bandits, and when $\mathcal{X}$ is the standard basis vectors and $\mathcal{Z}\subset \{0,1\}^d$, combinatorial bandits. Such a transductive setting naturally arises when the set of measurement vectors is limited due to factors such as availability or cost. As an example, in drug discovery the compounds and dosages $\mathcal{X}$ a practitioner may be willing to evaluate in the lab in vitro due to cost or safety reasons may differ vastly from those compounds and dosages $\mathcal{Z}$ that can be safely administered to patients in vivo. Alternatively, in recommender systems for books, the set of books $\mathcal{X}$ a user is queried about may be restricted to well known best-sellers even though the goal might be to recommend more esoteric titles $\mathcal{Z}$. In this paper, we provide instance-dependent lower bounds for the transductive setting, an algorithm that matches these up to logarithmic factors, and an evaluation. In particular, we provide the first non-asymptotic algorithm for linear bandits that nearly achieves the information theoretic lower bound.",Tanner Fiez|Lalit Jain|Kevin Jamieson|Lillian Ratliff,,https://arxiv.org/abs/1906.08399v1,https://arxiv.org/pdf/1906.08399v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1906.08399v1.pdf
1906.07500v1,2019-06-18T11:21:19Z,2019-06-18 11:21:19,Prediction properties of optimum response surface designs,Prediction capability is considered an important issue in response surface methodology. Following the line of argument that a design should have several desirable properties we have extended an existing compound design criterion to include prediction properties. Prediction of responses and of differences in response are considered. Point and interval predictions are allowed for. Extensions of existing graphical tools for inspecting prediction performances of the designs in the whole region of experimentation are also introduced. The methods are illustrated with two examples.,Heloisa M. de Oliveira|Cesar B. A. de Oliveira|Steven G. Gilmour|Luzia A. Trinca,,https://arxiv.org/abs/1906.07500v1,https://arxiv.org/pdf/1906.07500v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1906.07500v1.pdf
1906.04133v1,2019-06-10T17:10:51Z,2019-06-10 17:10:51,Bayesian experimental design using regularized determinantal point processes,"In experimental design, we are given $n$ vectors in $d$ dimensions, and our goal is to select $k\ll n$ of them to perform expensive measurements, e.g., to obtain labels/responses, for a linear regression task. Many statistical criteria have been proposed for choosing the optimal design, with popular choices including A- and D-optimality. If prior knowledge is given, typically in the form of a $d\times d$ precision matrix $\mathbf A$, then all of the criteria can be extended to incorporate that information via a Bayesian framework. In this paper, we demonstrate a new fundamental connection between Bayesian experimental design and determinantal point processes, the latter being widely used for sampling diverse subsets of data. We use this connection to develop new efficient algorithms for finding $(1+ε)$-approximations of optimal designs under four optimality criteria: A, C, D and V. Our algorithms can achieve this when the desired subset size $k$ is $Ω(\frac{d_{\mathbf A}}ε + \frac{\log 1/ε}{ε^2})$, where $d_{\mathbf A}\leq d$ is the $\mathbf A$-effective dimension, which can often be much smaller than $d$. Our results offer direct improvements over a number of prior works, for both Bayesian and classical experimental design, in terms of algorithm efficiency, approximation quality, and range of applicable criteria.",Michał Dereziński|Feynman Liang|Michael W. Mahoney,,https://arxiv.org/abs/1906.04133v1,https://arxiv.org/pdf/1906.04133v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1906.04133v1.pdf
1905.10940v1,2019-05-27T02:35:58Z,2019-05-27 02:35:58,A Practical Spectrum Sharing Scheme for Cognitive Radio Networks: Design and Experiments,"Spectrum shortage is a fundamental problem in wireless networks and this problem becomes increasingly acute with the rapid proliferation of wireless devices. To address this problem, spectrum sharing in the context of cognitive radio networks (CRNs) has been considered a promising solution. In this paper, we propose a practical spectrum sharing scheme for a small CRN that comprises a pair of primary users and a pair of secondary users by leveraging the multiple-input and multiple-output (MIMO) technology. In our scheme, we assume that the secondary users take full responsibility for cross-network interference cancellation (IC). We also assume that the secondary users have no knowledge about the primary network, including its signal waveform, frame structure, and network protocol. The key components of our proposed scheme are two MIMO-based interference management techniques: blind beamforming (BBF) and blind interference cancellation (BIC). We have built a prototype of our scheme on a wireless testbed and demonstrated that the prototyped secondary network can coexist with commercial Wi-Fi devices (primary users). Experimental results further show that, for a secondary device with two or three antennas, BBF and BIC achieve an average of 25dB and 33dB IC capability in an office environment, respectively.",Pedram Kheirkhah Sangdeh|Hossein Pirayesh|Adnan Quadri|Huacheng Zeng,,https://arxiv.org/abs/1905.10940v1,https://arxiv.org/pdf/1905.10940v1,,,,,cs.NI,cs.NI|eess.SP,https://arxiv.org/pdf/1905.10940v1.pdf
1905.10493v1,2019-05-25T01:22:03Z,2019-05-25 01:22:03,Safely and Quickly Deploying New Features with a Staged Rollout Framework Using Sequential Test and Adaptive Experimental Design,"During the rapid development cycle for Internet products (websites and mobile apps), new features are developed and rolled out to users constantly. Features with code defects or design flaws can cause outages and significant degradation of user experience. The traditional method of code review and change management can be time-consuming and error-prone. In order to make the feature rollout process safe and fast, this paper proposes a methodology for rolling out features in an automated way using an adaptive experimental design. Under this framework, a feature is gradually ramped up from a small proportion of users to a larger population based on real-time evaluation of the performance of important metrics. If there are any regression detected during the ramp-up step, the ramp-up process stops and the feature developer is alerted. There are two main algorithm components powering this framework: 1) a continuous monitoring algorithm - using a variant of the sequential probability ratio test (SPRT) to monitor the feature performance metrics and alert feature developers when a metric degradation is detected, 2) an automated ramp-up algorithm - deciding when and how to ramp up to the next stage with larger sample size. This paper presents one monitoring algorithm and three ramping up algorithms including time-based, power-based, and risk-based (a Bayesian approach) schedules. These algorithms are evaluated and compared on both simulated data and real data. There are three benefits provided by this framework for feature rollout: 1) for defective features, it can detect the regression early and reduce negative effect, 2) for healthy features, it rolls out the feature quickly, 3) it reduces the need for manual intervention via the automation of the feature rollout process.",Zhenyu Zhao|Mandie Liu|Anirban Deb,,https://arxiv.org/abs/1905.10493v1,https://arxiv.org/pdf/1905.10493v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1905.10493v1.pdf
1904.11849v1,2019-04-26T13:45:58Z,2019-04-26 13:45:58,Quantum process tomography via optimal design of experiments,"Quantum process tomography --- a primitive in many quantum information processing tasks --- can be cast within the framework of the theory of design of experiment (DoE), a branch of classical statistics that deals with the relationship between inputs and outputs of an experimental setup. Such a link potentially gives access to the many ideas of the rich subject of classical DoE for use in quantum problems. The classical techniques from DoE cannot, however, be directly applied to the quantum process tomography due to the basic structural differences between the classical and quantum estimation problems. Here, we properly formulate quantum process tomography as a DoE problem, and examine several examples to illustrate the link and the methods. In particular, we discuss the common issue of nuisance parameters, and point out interesting features in the quantum problem absent in the usual classical setting.",Yonatan Gazit|Hui Khoon Ng|Jun Suzuki,,https://arxiv.org/abs/1904.11849v1,https://arxiv.org/pdf/1904.11849v1,https://doi.org/10.1103/PhysRevA.100.012350,"18 pages, 4 figures","Phys. Rev. A 100, 012350 (2019)",10.1103/PhysRevA.100.012350,quant-ph,quant-ph,https://arxiv.org/pdf/1904.11849v1.pdf
1904.08626v1,2019-04-18T07:52:54Z,2019-04-18 07:52:54,Ontology-based Design of Experiments on Big Data Solutions,"Big data solutions are designed to cope with data of huge Volume and wide Variety, that need to be ingested at high Velocity and have potential Veracity issues, challenging characteristics that are usually referred to as the ""4Vs of Big Data"". In order to evaluate possibly complex big data solutions, stress tests require to assess a large number of combinations of sub-components jointly with the possible big data variations. A formalization of the Design of Experiments (DoE) on big data solutions is aimed at ensuring the reproducibility of the experiments, facilitating their partitioning in sub-experiments and guaranteeing the consistency of their outcomes in a global assessment. In this paper, an ontology-based approach is proposed to support the evaluation of a big data system in two ways. Firstly, the approach formalizes a decomposition and recombination of the big data solution, allowing for the aggregation of component evaluation results at inter-component level. Secondly, existing work on DoE is translated into an ontology for supporting the selection of experiments. The proposed ontology-based approach offers the possibility to combine knowledge from the evaluation domain and the application domain. It exploits domain and inter-domain specific restrictions on the factor combinations in order to reduce the number of experiments. Contrary to existing approaches, the proposed use of ontologies is not limited to the assertional description and exploitation of past experiments but offers richer terminological descriptions for the development of a DoE from scratch. As an application example, a maritime big data solution to the problem of detecting and predicting vessel suspicious behaviour through mobility analysis is selected. The article is concluded with a sketch of future works.",Maximilian Zocholl|Elena Camossi|Anne-Laure Jousselme|Cyril Ray,,https://arxiv.org/abs/1904.08626v1,https://arxiv.org/pdf/1904.08626v1,,Pre-print and extended version of the poster paper presented at the 14th International Conference on Semantic Systems,,,cs.AI,cs.AI,https://arxiv.org/pdf/1904.08626v1.pdf
1904.08848v1,2019-04-16T14:35:29Z,2019-04-16 14:35:29,Design of experiments for Quick U-building method for building energy performance measurement,"Quick U-building (QUB) is a method for short time measurement of energy performance of buildings, typically one night. It uses the indoor air temperature response to power delivered to the indoor air by electric heaters. This paper introduces a method for estimating the expected measurement error as a function of the amplitude and the time duration of the input signal based on the decomposition of the time response of a state-space model into a sum of exponentials by using the eigenvalues of the state matrix. It is shown that the buildings have a group of dominant time constants, which gives an exponential response, and many very short and very large time constants, which have a small influence on the response. The analysis of the eigenvalues demonstrates that the QUB experiment may be done in a rather short time as compared with the largest time constant of the building.",Christian Ghiaus|Florent Alzetto,CETHIL|LPS,https://arxiv.org/abs/1904.08848v1,https://arxiv.org/pdf/1904.08848v1,https://doi.org/10.1080/19401493.2018.1561753,,"Journal of Building Performance Simulation, Taylor & Francis, 2019, 12 (4), pp.465-479",10.1080/19401493.2018.1561753,eess.SP,eess.SP|eess.SY,https://arxiv.org/pdf/1904.08848v1.pdf
1904.05703v5,2019-04-11T14:05:15Z,2021-11-17 08:43:27,Bayesian experimental design without posterior calculations: an adversarial approach,"Most computational approaches to Bayesian experimental design require making posterior calculations repeatedly for a large number of potential designs and/or simulated datasets. This can be expensive and prohibit scaling up these methods to models with many parameters, or designs with many unknowns to select. We introduce an efficient alternative approach without posterior calculations, based on optimising the expected trace of the Fisher information, as discussed by Walker (2016). We illustrate drawbacks of this approach, including lack of invariance to reparameterisation and encouraging designs in which one parameter combination is inferred accurately but not any others. We show these can be avoided by using an adversarial approach: the experimenter must select their design while a critic attempts to select the least favourable parameterisation. We present theoretical properties of this approach and show it can be used with gradient based optimisation methods to find designs efficiently in practice.",Dennis Prangle|Sophie Harbisher|Colin S Gillespie,,https://arxiv.org/abs/1904.05703v5,https://arxiv.org/pdf/1904.05703v5,,V5 has minor typo corrections and presentational changes,,,stat.CO,stat.CO,https://arxiv.org/pdf/1904.05703v5.pdf
1903.11697v1,2019-03-27T20:54:16Z,2019-03-27 20:54:16,Bayesian Experimental Design for Oral Glucose Tolerance Tests (OGTT),"OGTT is a common test, frequently used to diagnose insulin resistance or diabetes, in which a patient's blood sugar is measured at various times over the course of a few hours. Recent developments in the study of OGTT results have framed it as an inverse problem which has been the subject of Bayesian inference. This is a powerful new tool for analyzing the results of an OGTT test,and the question arises as to whether the test itself can be improved. It is of particular interest to discover whether the times at which a patient's glucose is measured can be changed to improve the effectiveness of the test. The purpose of this paper is to explore the possibility of finding a better experimental design, that is, a set of times to perform the test. We review the theory of Bayesian experimental design and propose an estimator for the expected utility of a design. We then study the properties of this estimator and propose a new method for quantifying the uncertainty in comparisons between designs. We implement this method to find a new design and the proposed design is compared favorably to the usual testing scheme.",Nicolás E. Kuschinski|J. Andrés Christen|Adriana Monroy|Silvestre Alavez,,https://arxiv.org/abs/1903.11697v1,https://arxiv.org/pdf/1903.11697v1,,"24 pages, 5 figures",,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/1903.11697v1.pdf
1903.11187v1,2019-03-26T23:00:16Z,2019-03-26 23:00:16,A layered multiple importance sampling scheme for focused optimal Bayesian experimental design,"We develop a new computational approach for ""focused"" optimal Bayesian experimental design with nonlinear models, with the goal of maximizing expected information gain in targeted subsets of model parameters. Our approach considers uncertainty in the full set of model parameters, but employs a design objective that can exploit learning trade-offs among different parameter subsets. We introduce a new layered multiple importance sampling scheme that provides consistent estimates of expected information gain in this focused setting. This sampling scheme yields significant reductions in estimator bias and variance for a given computational effort, making optimal design more tractable for a wide range of computationally intensive problems.",Chi Feng|Youssef M. Marzouk,,https://arxiv.org/abs/1903.11187v1,https://arxiv.org/pdf/1903.11187v1,,"33 pages, 16 figures",,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/1903.11187v1.pdf
1903.05480v3,2019-03-13T13:34:13Z,2020-01-14 14:49:02,Variational Bayesian Optimal Experimental Design,"Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments.",Adam Foster|Martin Jankowiak|Eli Bingham|Paul Horsfall|Yee Whye Teh|Tom Rainforth|Noah Goodman,,https://arxiv.org/abs/1903.05480v3,https://arxiv.org/pdf/1903.05480v3,,"Published as a conference paper at the Thirty-third Conference on Neural Information Processing Systems, Vancouver 2019. https://papers.nips.cc/paper/9553-variational-bayesian-optimal-experimental-design.pdf",,,stat.ML,stat.ML|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/1903.05480v3.pdf
1902.10347v1,2019-02-27T06:14:36Z,2019-02-27 06:14:36,ABCD-Strategy: Budgeted Experimental Design for Targeted Causal Structure Discovery,"Determining the causal structure of a set of variables is critical for both scientific inquiry and decision-making. However, this is often challenging in practice due to limited interventional data. Given that randomized experiments are usually expensive to perform, we propose a general framework and theory based on optimal Bayesian experimental design to select experiments for targeted causal discovery. That is, we assume the experimenter is interested in learning some function of the unknown graph (e.g., all descendants of a target node) subject to design constraints such as limits on the number of samples and rounds of experimentation. While it is in general computationally intractable to select an optimal experimental design strategy, we provide a tractable implementation with provable guarantees on both approximation and optimization quality based on submodularity. We evaluate the efficacy of our proposed method on both synthetic and real datasets, thereby demonstrating that our method realizes considerable performance gains over baseline strategies such as random sampling.",Raj Agrawal|Chandler Squires|Karren Yang|Karthik Shanmugam|Caroline Uhler,,https://arxiv.org/abs/1902.10347v1,https://arxiv.org/pdf/1902.10347v1,,To appear in AISTATS 2019,,,stat.ME,stat.ME,https://arxiv.org/pdf/1902.10347v1.pdf
1902.01352v2,2019-02-04T18:14:37Z,2019-11-24 13:16:57,Optimal block designs for experiments on networks,"We propose a method for constructing optimal block designs for experiments on networks. The response model for a given network interference structure extends the linear network effects model to incorporate blocks. The optimality criteria are chosen to reflect the experimental objectives and an exchange algorithm is used to search across the design space for obtaining an efficient design when an exhaustive search is not possible. Our interest lies in estimating the direct comparisons among treatments, in the presence of nuisance network effects that stem from the underlying network interference structure governing the experimental units, or in the network effects themselves. Comparisons of optimal designs under different models, including the standard treatment models, are examined by comparing the variance and bias of treatment effect estimators. We also suggest a way of defining blocks, while taking into account the interrelations of groups of experimental units within a network, using spectral clustering techniques to achieve optimal modularity. We expect connected units within closed-form communities to behave similarly to an external stimulus. We provide evidence that our approach can lead to efficiency gains over conventional designs such as randomized designs that ignore the network structure and we illustrate its usefulness for experiments on networks.",Vasiliki Koutra|Steven G. Gilmour|Ben M. Parker,,https://arxiv.org/abs/1902.01352v2,https://arxiv.org/pdf/1902.01352v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1902.01352v2.pdf
1902.01011v1,2019-02-04T02:29:46Z,2019-02-04 02:29:46,Global Fitting of the Response Surface via Estimating Multiple Contours of a Simulator,"Computer simulators are nowadays widely used to understand complex physical systems in many areas such as aerospace, renewable energy, climate modeling, and manufacturing. One fundamental issue in the study of computer simulators is known as experimental design, that is, how to select the input settings where the computer simulator is run and the corresponding response is collected. Extra care should be taken in the selection process because computer simulators can be computationally expensive to run. The selection shall acknowledge and achieve the goal of the analysis. This article focuses on the goal of producing more accurate prediction which is important for risk assessment and decision making. We propose two new methods of design approaches that sequentially select input settings to achieve this goal. The approaches make novel applications of simultaneous and sequential contour estimations. Numerical examples are employed to demonstrate the effectiveness of the proposed approaches.",Feng Yang|C. Devon Lin|Pritam Ranjan,,https://arxiv.org/abs/1902.01011v1,https://arxiv.org/pdf/1902.01011v1,,24 pages,,,stat.ME,stat.ME|stat.AP|stat.CO|stat.ML,https://arxiv.org/pdf/1902.01011v1.pdf
1902.00995v1,2019-02-04T00:16:32Z,2019-02-04 00:16:32,Minimax experimental design: Bridging the gap between statistical and worst-case approaches to least squares regression,"In experimental design, we are given a large collection of vectors, each with a hidden response value that we assume derives from an underlying linear model, and we wish to pick a small subset of the vectors such that querying the corresponding responses will lead to a good estimator of the model. A classical approach in statistics is to assume the responses are linear, plus zero-mean i.i.d. Gaussian noise, in which case the goal is to provide an unbiased estimator with smallest mean squared error (A-optimal design). A related approach, more common in computer science, is to assume the responses are arbitrary but fixed, in which case the goal is to estimate the least squares solution using few responses, as quickly as possible, for worst-case inputs. Despite many attempts, characterizing the relationship between these two approaches has proven elusive. We address this by proposing a framework for experimental design where the responses are produced by an arbitrary unknown distribution. We show that there is an efficient randomized experimental design procedure that achieves strong variance bounds for an unbiased estimator using few responses in this general model. Nearly tight bounds for the classical A-optimality criterion, as well as improved bounds for worst-case responses, emerge as special cases of this result. In the process, we develop a new algorithm for a joint sampling distribution called volume sampling, and we propose a new i.i.d. importance sampling method: inverse score sampling. A key novelty of our analysis is in developing new expected error bounds for worst-case regression by controlling the tail behavior of i.i.d. sampling via the jointness of volume sampling. Our result motivates a new minimax-optimality criterion for experimental design which can be viewed as an extension of both A-optimal design and sampling for worst-case regression.",Michał Dereziński|Kenneth L. Clarkson|Michael W. Mahoney|Manfred K. Warmuth,,https://arxiv.org/abs/1902.00995v1,https://arxiv.org/pdf/1902.00995v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1902.00995v1.pdf
1903.02062v1,2019-02-01T11:06:29Z,2019-02-01 11:06:29,Design of experiments aided holistic testing of cyber-physical energy systems,"The complex and often safety-critical nature of cyber-physical energy systems makes validation a key challenge in facilitating the energy transition, especially when it comes to the testing on system level. Reliable and reproducible validation experiments can be guided by the concept of design of experiments, which is, however, so far not fully adopted by researchers. This paper suggests a structured guideline for design of experiments application within the holistic testing procedure suggested by the European ERIGrid project. In this paper, a general workflow as well as a practical example are provided with the aim to give domain experts a basic understanding of design of experiments compliant testing.",Arjen van der Meer|Cornelius Steinbrink|Kai Heussen|Daniel Morales Bondy|Merkebu Zenebe Degefa|Filip Pröstl Andren|Thomas Strasser|Sebastian Lehnhoff|Peter Palensky,,https://arxiv.org/abs/1903.02062v1,https://arxiv.org/pdf/1903.02062v1,https://doi.org/10.1109/MSCPES.2018.8405401,2018 Workshop on Modeling and Simulation of Cyber-Physical Energy Systems (MSCPES),,10.1109/MSCPES.2018.8405401,cs.SE,cs.SE|eess.SY,https://arxiv.org/pdf/1903.02062v1.pdf
1901.06080v1,2019-01-18T04:17:25Z,2019-01-18 04:17:25,Accelerated Experimental Design for Pairwise Comparisons,"Pairwise comparison labels are more informative and less variable than class labels, but generating them poses a challenge: their number grows quadratically in the dataset size. We study a natural experimental design objective, namely, D-optimality, that can be used to identify which $K$ pairwise comparisons to generate. This objective is known to perform well in practice, and is submodular, making the selection approximable via the greedy algorithm. A naïve greedy implementation has $O(N^2d^2K)$ complexity, where $N$ is the dataset size, $d$ is the feature space dimension, and $K$ is the number of generated comparisons. We show that, by exploiting the inherent geometry of the dataset--namely, that it consists of pairwise comparisons--the greedy algorithm's complexity can be reduced to $O(N^2(K+d)+N(dK+d^2) +d^2K).$ We apply the same acceleration also to the so-called lazy greedy algorithm. When combined, the above improvements lead to an execution time of less than 1 hour for a dataset with $10^8$ comparisons; the naïve greedy algorithm on the same dataset would require more than 10 days to terminate.",Yuan Guo|Jennifer Dy|Deniz Erdogmus|Jayashree Kalpathy-Cramer|Susan Ostmo|J. Peter Campbell|Michael F. Chiang|Stratis Ioannidis,,https://arxiv.org/abs/1901.06080v1,https://arxiv.org/pdf/1901.06080v1,,,,,cs.DS,cs.DS|cs.LG,https://arxiv.org/pdf/1901.06080v1.pdf
1901.03478v2,2019-01-11T04:51:54Z,2020-03-11 03:35:14,Deep Learning for Ranking Response Surfaces with Applications to Optimal Stopping Problems,"In this paper, we propose deep learning algorithms for ranking response surfaces, with applications to optimal stopping problems in financial mathematics. The problem of ranking response surfaces is motivated by estimating optimal feedback policy maps in stochastic control problems, aiming to efficiently find the index associated to the minimal response across the entire continuous input space $\mathcal{X} \subseteq \mathbb{R}^d$. By considering points in $\mathcal{X}$ as pixels and indices of the minimal surfaces as labels, we recast the problem as an image segmentation problem, which assigns a label to every pixel in an image such that pixels with the same label share certain characteristics. This provides an alternative method for efficiently solving the problem instead of using sequential design in our previous work [R. Hu and M. Ludkovski, SIAM/ASA Journal on Uncertainty Quantification, 5 (2017), 212--239].
  Deep learning algorithms are scalable, parallel and model-free, i.e., no parametric assumptions needed on the response surfaces. Considering ranking response surfaces as image segmentation allows one to use a broad class of deep neural networks, e.g., UNet, SegNet, DeconvNet, which have been widely applied and numerically proved to possess high accuracy in the field. We also systematically study the dependence of deep learning algorithms on the input data generated on uniform grids or by sequential design sampling, and observe that the performance of deep learning is {\it not} sensitive to the noise and locations (close to/away from boundaries) of training data. We present a few examples including synthetic ones and the Bermudan option pricing problem to show the efficiency and accuracy of this method.",Ruimeng Hu,,https://arxiv.org/abs/1901.03478v2,https://arxiv.org/pdf/1901.03478v2,,,,,stat.ML,stat.ML|cs.LG|q-fin.CP,https://arxiv.org/pdf/1901.03478v2.pdf
1812.03183v2,2018-12-07T19:01:38Z,2019-02-28 14:08:37,A hybrid machine-learning algorithm for designing quantum experiments,"We introduce a hybrid machine-learning algorithm for designing quantum optics experiments that produce specific quantum states. Our algorithm successfully found experimental schemes to produce all 5 states we asked it to, including Schrödinger cat states and cubic phase states, all to a fidelity of over $96\%$. Here we specifically focus on designing realistic experiments, and hence all of the algorithm's designs only contain experimental elements that are available with current technology. The core of our algorithm is a genetic algorithm that searches for optimal arrangements of the experimental elements, but to speed up the initial search we incorporate a neural network that classifies quantum states. The latter is of independent interest, as it quickly learned to accurately classify quantum states given their photon-number distributions.",L. O'Driscoll|R. Nichols|P. A. Knott,,https://arxiv.org/abs/1812.03183v2,https://arxiv.org/pdf/1812.03183v2,,,,,quant-ph,quant-ph|cs.AI|cs.LG|cs.NE,https://arxiv.org/pdf/1812.03183v2.pdf
1812.01032v2,2018-12-03T19:06:33Z,2019-10-14 15:54:09,Designing quantum experiments with a genetic algorithm,"We introduce a genetic algorithm that designs quantum optics experiments for engineering quantum states with specific properties. Our algorithm is powerful and flexible, and can easily be modified to find methods of engineering states for a range of applications. Here we focus on quantum metrology. First, we consider the noise-free case, and use the algorithm to find quantum states with a large quantum Fisher information (QFI). We find methods, which only involve experimental elements that are available with current or near-future technology, for engineering quantum states with up to a 100-fold improvement over the best classical state, and a 20-fold improvement over the optimal Gaussian state. Such states are a superposition of the vacuum with a large number of photons (around $80$), and can hence be seen as Schrödinger-cat-like states. We then apply the two most dominant noise sources in our setting -- photon loss and imperfect heralding -- and use the algorithm to find quantum states that still improve over the optimal Gaussian state with realistic levels of noise. This will open up experimental and technological work in using exotic non-Gaussian states for quantum-enhanced phase measurements. Finally, we use the Bayesian mean square error to look beyond the regime of validity of the QFI, finding quantum states with precision enhancements over the alternatives even when the experiment operates in the regime of limited data.",Rosanna Nichols|Lana Mineh|Jesús Rubio|Jonathan C. F. Matthews|Paul A. Knott,,https://arxiv.org/abs/1812.01032v2,https://arxiv.org/pdf/1812.01032v2,https://doi.org/10.1088/2058-9565/ab4d89,"11 pages + Appendix, 9 figures",Quantum Sci. Technol. 4 045012 (2019),10.1088/2058-9565/ab4d89,quant-ph,quant-ph|cs.AI|cs.LG|cs.NE,https://arxiv.org/pdf/1812.01032v2.pdf
1811.12682v1,2018-11-30T09:28:39Z,2018-11-30 09:28:39,"Large Datasets, Bias and Model Oriented Optimal Design of Experiments",We review recent literature that proposes to adapt ideas from classical model based optimal design of experiments to problems of data selection of large datasets. Special attention is given to bias reduction and to protection against confounders. Some new results are presented. Theoretical and computational comparisons are made.,Elena Pesce|Eva Riccomagno,,https://arxiv.org/abs/1811.12682v1,https://arxiv.org/pdf/1811.12682v1,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/1811.12682v1.pdf
1811.11469v3,2018-11-28T10:02:31Z,2020-02-03 14:42:27,Multilevel Double Loop Monte Carlo and Stochastic Collocation Methods with Importance Sampling for Bayesian Optimal Experimental Design,"An optimal experimental set-up maximizes the value of data for statistical inferences and predictions. The efficiency of strategies for finding optimal experimental set-ups is particularly important for experiments that are time-consuming or expensive to perform. For instance, in the situation when the experiments are modeled by Partial Differential Equations (PDEs), multilevel methods have been proven to dramatically reduce the computational complexity of their single-level counterparts when estimating expected values. For a setting where PDEs can model experiments, we propose two multilevel methods for estimating a popular design criterion known as the expected information gain in simulation-based Bayesian optimal experimental design. The expected information gain criterion is of a nested expectation form, and only a handful of multilevel methods have been proposed for problems of such form. We propose a Multilevel Double Loop Monte Carlo (MLDLMC), which is a multilevel strategy with Double Loop Monte Carlo (DLMC), and a Multilevel Double Loop Stochastic Collocation (MLDLSC), which performs a high-dimensional integration by deterministic quadrature on sparse grids. For both methods, the Laplace approximation is used for importance sampling that significantly reduces the computational work of estimating inner expectations. The optimal values of the method parameters are determined by minimizing the average computational work, subject to satisfying the desired error tolerance. The computational efficiencies of the methods are demonstrated by estimating the expected information gain for Bayesian inference of the fiber orientation in composite laminate materials from an electrical impedance tomography experiment. MLDLSC performs better than MLDLMC when the regularity of the quantity of interest, with respect to the additive noise and the unknown parameters, can be exploited.",Joakim Beck|Ben Mansour Dia|Luis F. R. Espath|Raul Tempone,,https://arxiv.org/abs/1811.11469v3,https://arxiv.org/pdf/1811.11469v3,,,,,math.NA,math.NA,https://arxiv.org/pdf/1811.11469v3.pdf
1811.08656v2,2018-11-21T10:00:24Z,2019-09-30 17:03:16,Optimal design of experiments for a lithium-ion cell: parameters identification of an isothermal single particle model with electrolyte dynamics,"Advanced battery management systems rely on mathematical models to guarantee optimal functioning of Lithium-ion batteries. The Pseudo-Two Dimensional (P2D) model is a very detailed electrochemical model suitable for simulations. On the other side, its complexity prevents its usage in control and state estimation. Therefore, it is more appropriate the use of simplified electrochemical models such as the Single Particle Model with electrolyte dynamics (SPMe), which exhibits good adherence to real data when suitably calibrated. This work focuses on a Fisher-based optimal experimental design for identifying the SPMe parameters. The proposed approach relies on a nonlinear optimization to minimize the covariance parameters matrix. At first, the parameters are estimated by considering the SPMe as the real plant. Subsequently, a more realistic scenario is considered where the P2D model is used to reproduce a real battery behavior. Results show the effectiveness of the optimal experimental design when compared to standard strategies.",Andrea Pozzi|Gabriele Ciaramella|Stefan Volkwein|Davide M. Raimondo,,https://arxiv.org/abs/1811.08656v2,https://arxiv.org/pdf/1811.08656v2,https://doi.org/10.1021/acs.iecr.8b04580,"Published in Ind. Eng. Chem. Res. 2019, 58, 3, 1286-1299","Ind. Eng. Chem. Res. 2019, 58, 3, 1286-1299",10.1021/acs.iecr.8b04580,eess.SY,eess.SY|math.OC,https://arxiv.org/pdf/1811.08656v2.pdf
1811.01524v3,2018-11-05T06:04:30Z,2018-11-16 08:00:03,Maximum temporal amplitude and designs of experiments for generation of extreme waves,"This paper aims to describe a deterministic generation of extreme waves in a typical towing tank. Such a generation involves an input signal to be provided at the wavemaker in such a way that at a certain position in the wave tank, say at a position of a tested object, a large amplitude wave emerges. For the purpose, we consider a model called a spatial nonlinear Schrödinger equation describing the spatial propagation of a slowly varying envelope of a signal. Such a model has an exact solution known as (spatial) Soliton on a Finite Background (SFB) that is a nonlinear extension of Benjamin-Feir instability. This spatial-SFB is characterized by wave focusing leading to almost time-periodic extreme waves that appear in between phase singularities. Although phase singularities and wave focusing have been subject to a number of studies, this spatial-SFB written in the field variables has many interesting properties among which are the existence of many critical values related to the modulation length of the monochromatic signal in the far fields. These properties will be used in choosing parameters for designing experiments on extreme wave generation. In doing so, a quantity called maximum temporal amplitude (MTA) is used. This quantity measures at each location the maximum over time of the wave elevation. For a given modulation length of SFB and desired maximum amplitude at a position in a towing tank, the MTA readily shows the maximum signal that is required at the wavemaker and the amplitude amplification factor of the requested signal. Some examples of such a generation in realistic laboratory variables will be displayed.", Marwan| Andonowati|N. Karjanto,,https://arxiv.org/abs/1811.01524v3,https://arxiv.org/pdf/1811.01524v3,,"6 pages, 5 figures, one table, Proceedings of the 11th Asian Congress of Fluid Mechanics, 22-25 May 2006, Kuala Lumpur, Malaysia, pages 978-983",,,physics.flu-dyn,physics.flu-dyn|nlin.PS,https://arxiv.org/pdf/1811.01524v3.pdf
1810.11867v1,2018-10-28T19:28:59Z,2018-10-28 19:28:59,Experimental Design for Cost-Aware Learning of Causal Graphs,"We consider the minimum cost intervention design problem: Given the essential graph of a causal graph and a cost to intervene on a variable, identify the set of interventions with minimum total cost that can learn any causal graph with the given essential graph. We first show that this problem is NP-hard. We then prove that we can achieve a constant factor approximation to this problem with a greedy algorithm. We then constrain the sparsity of each intervention. We develop an algorithm that returns an intervention design that is nearly optimal in terms of size for sparse graphs with sparse interventions and we discuss how to use it when there are costs on the vertices.",Erik M. Lindgren|Murat Kocaoglu|Alexandros G. Dimakis|Sriram Vishwanath,,https://arxiv.org/abs/1810.11867v1,https://arxiv.org/pdf/1810.11867v1,,In NIPS 2018,,,cs.LG,cs.LG|cs.DM|stat.ML,https://arxiv.org/pdf/1810.11867v1.pdf
1810.09912v2,2018-10-23T15:24:29Z,2019-02-23 13:48:19,Efficient Bayesian Experimental Design for Implicit Models,"Bayesian experimental design involves the optimal allocation of resources in an experiment, with the aim of optimising cost and performance. For implicit models, where the likelihood is intractable but sampling from the model is possible, this task is particularly difficult and therefore largely unexplored. This is mainly due to technical difficulties associated with approximating posterior distributions and utility functions. We devise a novel experimental design framework for implicit models that improves upon previous work in two ways. First, we use the mutual information between parameters and data as the utility function, which has previously not been feasible. We achieve this by utilising Likelihood-Free Inference by Ratio Estimation (LFIRE) to approximate posterior distributions, instead of the traditional approximate Bayesian computation or synthetic likelihood methods. Secondly, we use Bayesian optimisation in order to solve the optimal design problem, as opposed to the typically used grid search or sampling-based methods. We find that this increases efficiency and allows us to consider higher design dimensions.",Steven Kleinegesse|Michael Gutmann,,https://arxiv.org/abs/1810.09912v2,https://arxiv.org/pdf/1810.09912v2,,Added references and fixed typos. Results and figures remain unchanged,,,stat.ML,stat.ML|cs.LG|stat.CO|stat.ME,https://arxiv.org/pdf/1810.09912v2.pdf
1810.02561v3,2018-10-05T08:02:28Z,2019-03-08 15:24:29,GPdoemd: a Python package for design of experiments for model discrimination,"Model discrimination identifies a mathematical model that usefully explains and predicts a given system's behaviour. Researchers will often have several models, i.e. hypotheses, about an underlying system mechanism, but insufficient experimental data to discriminate between the models, i.e. discard inaccurate models. Given rival mathematical models and an initial experimental data set, optimal design of experiments suggests maximally informative experimental observations that maximise a design criterion weighted by prediction uncertainty. The model uncertainty requires gradients, which may not be readily available for black-box models. This paper (i) proposes a new design criterion using the Jensen-Rényi divergence, and (ii) develops a novel method replacing black-box models with Gaussian process surrogates. Using the surrogates, we marginalise out the model parameters with approximate inference. Results show these contributions working well for both classical and new test instances. We also (iii) introduce and discuss GPdoemd, the open-source implementation of the Gaussian process surrogate method.",Simon Olofsson|Lukas Hebing|Sebastian Niedenführ|Marc Peter Deisenroth|Ruth Misener,,https://arxiv.org/abs/1810.02561v3,https://arxiv.org/pdf/1810.02561v3,https://doi.org/10.1016/j.compchemeng.2019.03.010,,"Computers & Chemical Engineering, Volume 125, 2019, Pages 54-70",10.1016/j.compchemeng.2019.03.010,cs.MS,cs.MS|stat.ML,https://arxiv.org/pdf/1810.02561v3.pdf
1809.07160v1,2018-08-30T03:56:47Z,2018-08-30 03:56:47,"On-chip correlation-based Brillouin sensing: design, experiment and simulation","Wavelength-scale SBS waveguides are enabling novel on-chip functionalities. The micro- and nano-scale SBS structures and the complexity of the SBS waveguides require a characterization technique to monitor the local geometry-dependent SBS responses along the waveguide. In this work, we experimentally demonstrate detection of longitudinal features down to 200$μ$m on a silicon-chalcogenide waveguide using the Brillouin optical correlation domain analysis (BOCDA) technique. We provide simulation and analysis on how multiple acoustic and optical modes and geometrical variations influence the Brillouin spectrum.",Atiyeh Zarifi|Birgit Stiller|Moritz Merklein|Yang Liu|Blair Morrison|Alvaro Casas-Bedoya|Gang Ren|Thach G. Nguyen|Khu Vu|Duk-Yong Choi|Arnan Mitchell|Stephen J. Madden|Benjamin J. Eggleton,,https://arxiv.org/abs/1809.07160v1,https://arxiv.org/pdf/1809.07160v1,https://doi.org/10.1364/JOSAB.36.000146,"8 pages, 6 figures",,10.1364/JOSAB.36.000146,physics.app-ph,physics.app-ph|physics.optics,https://arxiv.org/pdf/1809.07160v1.pdf
1808.05776v1,2018-08-17T07:11:00Z,2018-08-17 07:11:00,Optimum Experimental Design for Interface Identification Problems,"The identification of the interface of an inclusion in a diffusion process is considered. This task is viewed as a parameter identification problem in which the parameter space bears the structure of a shape manifold. A corresponding optimum experimental design (OED) problem is formulated in which the activation pattern of an array of sensors in space and time serves as experimental condition. The goal is to improve the estimation precision within a certain subspace of the infinite dimensional tangent space of shape variations to the manifold, and to find those shape variations of best and worst identifiability. Numerical results for the OED problem obtained by a simplicial decomposition algorithm are presented.",Tommy Etling|Roland Herzog|Martin Siebenborn,,https://arxiv.org/abs/1808.05776v1,https://arxiv.org/pdf/1808.05776v1,https://doi.org/10.1137/18M1208125,,,10.1137/18M1208125,math.OC,math.OC|math.NA,https://arxiv.org/pdf/1808.05776v1.pdf
1808.04513v2,2018-08-14T03:21:15Z,2020-02-09 21:12:49,Ridge Rerandomization: An Experimental Design Strategy in the Presence of Collinearity,"Randomization ensures that observed and unobserved covariates are balanced, on average. However, randomizing units to treatment and control often leads to covariate imbalances in realization, and such imbalances can inflate the variance of estimators of the treatment effect. One solution to this problem is rerandomization---an experimental design strategy that randomizes units until some balance criterion is fulfilled---which yields more precise estimators of the treatment effect if covariates are correlated with the outcome. Most rerandomization schemes in the literature utilize the Mahalanobis distance, which may not be preferable when covariates are correlated or vary in importance. As an alternative, we introduce an experimental design strategy called ridge rerandomization, which utilizes a modified Mahalanobis distance that addresses collinearities among covariates and automatically places a hierarchy of importance on the covariates according to their eigenstructure. This modified Mahalanobis distance has connections to principal components and the Euclidean distance, and---to our knowledge---has remained unexplored. We establish several theoretical properties of this modified Mahalanobis distance and our ridge rerandomization scheme. These results guarantee that ridge rerandomization is preferable over randomization and suggest when ridge rerandomization is preferable over standard rerandomization schemes. We also provide simulation evidence that suggests that ridge rerandomization is particularly preferable over typical rerandomization schemes in high-dimensional or high-collinearity settings.",Zach Branson|Stephane Shao,,https://arxiv.org/abs/1808.04513v2,https://arxiv.org/pdf/1808.04513v2,,"33 pages, 8 figures",,,math.ST,math.ST,https://arxiv.org/pdf/1808.04513v2.pdf
1808.02157v1,2018-08-06T23:57:26Z,2018-08-06 23:57:26,Experimental Design Modulates Variance in BOLD Activation: The Variance Design General Linear Model,"Typical fMRI studies have focused on either the mean trend in the blood-oxygen-level-dependent (BOLD) time course or functional connectivity (FC). However, other statistics of the neuroimaging data may contain important information. Despite studies showing links between the variance in the BOLD time series (BV) and age and cognitive performance, a formal framework for testing these effects has not yet been developed. We introduce the Variance Design General Linear Model (VDGLM), a novel framework that facilitates the detection of variance effects. We designed the framework for general use in any fMRI study by modeling both mean and variance in BOLD activation as a function of experimental design. The flexibility of this approach allows the VDGLM to i) simultaneously make inferences about a mean or variance effect while controlling for the other and ii) test for variance effects that could be associated with multiple conditions and/or noise regressors. We demonstrate the use of the VDGLM in a working memory application and show that engagement in a working memory task is associated with whole-brain decreases in BOLD variance.",Garren Gaut|Xiangrui Li|Zhong-Lin Lu|Mark Steyvers,,https://arxiv.org/abs/1808.02157v1,https://arxiv.org/pdf/1808.02157v1,,"18 pages, 7 figures",,,q-bio.NC,q-bio.NC,https://arxiv.org/pdf/1808.02157v1.pdf
1808.00731v1,2018-08-02T09:44:06Z,2018-08-02 09:44:06,Removal of the points that do not support an E-optimal experimental design,"We propose a method of removal of design points that cannot support any E-optimal experimental design of a linear regression model with uncorrelated observations. The proposed method can be used to reduce the size of some large E-optimal design problems such that they can be efficiently solved by semidefinite programming. This paper complements the results of Pronzato [Pronzato, L., 2013. A delimitation of the support of optimal designs for Kiefer's $φ_p$-class of criteria. Statistics & Probability Letters 83, 2721--2728], who studied the same problem for analytically simpler criteria of design optimality.",Radoslav Harman|Samuel Rosa,,https://arxiv.org/abs/1808.00731v1,https://arxiv.org/pdf/1808.00731v1,,"11 pages, 2 figures",,,math.ST,math.ST,https://arxiv.org/pdf/1808.00731v1.pdf
1807.09979v3,2018-07-26T07:09:56Z,2019-01-15 13:33:34,Bayesian Optimal Design of Experiments For Inferring The Statistical Expectation Of A Black-Box Function,"Bayesian optimal design of experiments (BODE) has been successful in acquiring information about a quantity of interest (QoI) which depends on a black-box function. BODE is characterized by sequentially querying the function at specific designs selected by an infill-sampling criterion. However, most current BODE methods operate in specific contexts like optimization, or learning a universal representation of the black-box function. The objective of this paper is to design a BODE for estimating the statistical expectation of a physical response surface. This QoI is omnipresent in uncertainty propagation and design under uncertainty problems. Our hypothesis is that an optimal BODE should be maximizing the expected information gain in the QoI. We represent the information gain from a hypothetical experiment as the Kullback-Liebler (KL) divergence between the prior and the posterior probability distributions of the QoI. The prior distribution of the QoI is conditioned on the observed data and the posterior distribution of the QoI is conditioned on the observed data and a hypothetical experiment. The main contribution of this paper is the derivation of a semi-analytic mathematical formula for the expected information gain about the statistical expectation of a physical response. The developed BODE is validated on synthetic functions with varying number of input-dimensions. We demonstrate the performance of the methodology on a steel wire manufacturing problem.",Piyush Pandita|Ilias Bilionis|Jitesh Panchal,,https://arxiv.org/abs/1807.09979v3,https://arxiv.org/pdf/1807.09979v3,,"27 pages, 19 figures",,,math.OC,math.OC|cs.LG|stat.ML,https://arxiv.org/pdf/1807.09979v3.pdf
1807.07024v3,2018-07-10T09:14:37Z,2020-08-06 21:31:32,Optimal design of experiments to identify latent behavioral types,"Bayesian optimal experiments that maximize the information gained from collected data are critical to efficiently identify behavioral models. We extend a seminal method for designing Bayesian optimal experiments by introducing two computational improvements that make the procedure tractable: (1) a search algorithm from artificial intelligence that efficiently explores the space of possible design parameters, and (2) a sampling procedure which evaluates each design parameter combination more efficiently. We apply our procedure to a game of imperfect information to evaluate and quantify the computational improvements. We then collect data across five different experimental designs to compare the ability of the optimal experimental design to discriminate among competing behavioral models against the experimental designs chosen by a ""wisdom of experts"" prediction experiment. We find that data from the experiment suggested by the optimal design approach requires significantly less data to distinguish behavioral models (i.e., test hypotheses) than data from the experiment suggested by experts. Substantively, we find that reinforcement learning best explains human decision-making in the imperfect information game and that behavior is not adequately described by the Bayesian Nash equilibrium. Our procedure is general and computationally efficient and can be applied to dynamically optimize online experiments.",Stefano Balietti|Brennan Klein|Christoph Riedl,,https://arxiv.org/abs/1807.07024v3,https://arxiv.org/pdf/1807.07024v3,https://doi.org/10.1007/s10683-020-09680-w,,Exp. econ. 24 (2021) 772-799,10.1007/s10683-020-09680-w,stat.AP,stat.AP|cs.AI|cs.GT,https://arxiv.org/pdf/1807.07024v3.pdf
1806.11299v1,2018-06-29T08:28:27Z,2018-06-29 08:28:27,"Testing General Relativity with geodetic VLBI: what profit from a single, specially designed experiment?","Context. We highlight the capabilities of the geodetic VLBI technique to test General relativity in the classical astrometric style, i.e., measuring the deflection of light in the vicinity of the Sun.
  Aims. In previous studies, the parameter was estimated by global analyses of thousands of geodetic VLBI sessions. Here we estimate from a single session where the Sun has approached two strong reference radio sources 0229+131 and 0235+164 at an elongation angle of 1-3 degrees.
  Methods. The AUA020 VLBI session of 1 May 2017 was designed to obtain more than 1000 group delays from the two radio sources. The Solar corona effect was effectively calibrated with the dual-frequency observations even at small elongation from the Sun.
  Results. We obtained with a precision better than what is obtained through global analyses of thousands of standard geodetic sessions over decades. Current results demonstrate that the modern VLBI technology is capable of establishing new limits on observational test of General Relativity.",Oleg Titov|Anastasiia Girdiuk|Sebastien Lambert|Jim Lovell|Jamie McCallum|Stas Shabala|Lucia McCallum|David Mayer|Matthias Schartner|Aletta de Witt|Fengchun Shu|Alexei Melnikov|Dmitrii Ivanov|Andrei Mikhailov|Sangoh Yi|Benedikt Soja|Bo Xia|T Jiang,,https://arxiv.org/abs/1806.11299v1,https://arxiv.org/pdf/1806.11299v1,https://doi.org/10.1051/0004-6361/201833459,Accepted for publication in Astronomy and Astrophysics,"A&A 618, A8 (2018)",10.1051/0004-6361/201833459,astro-ph.IM,astro-ph.IM,https://arxiv.org/pdf/1806.11299v1.pdf
1806.10655v2,2018-06-27T19:36:44Z,2019-03-24 22:43:48,An Optimal Experimental Design Framework for Adaptive Inflation and Covariance Localization for Ensemble Filters,"We develop an optimal experimental design framework for adapting the covariance inflation and localization in data assimilation problems. Covariance inflation and localization are ubiquitously employed to alleviate the effect of using ensembles of finite sizes in all practical data assimilation systems. The choice of both the inflation factor and the localization radius can have a significant impact on the performance of the assimilation scheme. These parameters are generally tuned by trial and error, rendering them expensive to optimize in practice. Spatially and temporally varying inflation parameter and localization radii have been recently proposed and have been empirically proven to enhance the performance of the employed assimilation filter. In this study, we present a variational framework for adaptive tuning of the inflation and localization parameters. Each of these parameters is optimized independently, with an objective to minimize the uncertainty in the posterior state. The proposed framework does not assume uncorrelated observations or prior errors and can in principle be applied without expert knowledge about the model and the observations. Thus, it is adequate for handling dense as well as sparse observational networks. We present the mathematical formulation, algorithmic description of the approach, and numerical experiments using the two-layer Lorenz-96 model.",Ahmed Attia|Emil Constantinescu,,https://arxiv.org/abs/1806.10655v2,https://arxiv.org/pdf/1806.10655v2,,"31 pages, 15 figures",,,cs.CE,cs.CE|stat.AP,https://arxiv.org/pdf/1806.10655v2.pdf
1806.06460v2,2018-06-17T22:36:59Z,2018-07-26 04:51:26,Experimental Design of a Prescribed Burn Instrumentation,"Observational data collected during experiments, such as the planned Fire and Smoke Model Evaluation Experiment (FASMEE), are critical for progressing and transitioning coupled fire-atmosphere models like WRF-SFIRE and WRF-SFIRE-CHEM into operational use. Historical meteorological data, representing typical weather conditions for the anticipated burn locations and times, have been processed to initialize and run a set of simulations representing the planned experimental burns. Based on an analysis of these numerical simulations, this paper provides recommendations on the experimental setup that include the ignition procedures, size and duration of the burns, and optimal sensor placement. New techniques are developed to initialize coupled fire-atmosphere simulations with weather conditions typical of the planned burn locations and time of the year. Analysis of variation and sensitivity analysis of simulation design to model parameters by repeated Latin Hypercube Sampling are used to assess the locations of the sensors. The simulations provide the locations of the measurements that maximize the expected variation of the sensor outputs with the model parameters.",Adam K. Kochanski|Aimé Fournier|Jan Mandel,,https://arxiv.org/abs/1806.06460v2,https://arxiv.org/pdf/1806.06460v2,https://doi.org/10.3390/atmos9080296,"35 pages, 4 tables, 28 figures","Atmosphere 2018, 9, 296",10.3390/atmos9080296,physics.ao-ph,physics.ao-ph|physics.data-an|stat.AP,https://arxiv.org/pdf/1806.06460v2.pdf
1806.02588v2,2018-06-07T09:51:23Z,2018-07-11 12:09:30,Designing Experiments to Measure Incrementality on Facebook,"The importance of Facebook advertising has risen dramatically in recent years, with the platform accounting for almost 20% of the global online ad spend in 2017. An important consideration in advertising is incrementality: how much of the change in an experimental metric is an advertising campaign responsible for. To measure incrementality, Facebook provide lift studies. As Facebook lift studies differ from standard A/B tests, the online experimentation literature does not describe how to calculate parameters such as power and minimum sample size. Facebook also offer multi-cell lift tests, which can be used to compare campaigns that don't have statistically identical audiences. In this case, there is no literature describing how to measure the significance of the difference in incrementality between cells, or how to estimate the power or minimum sample size. We fill these gaps in the literature by providing the statistical power and required sample size calculation for Facebook lift studies. We then generalise the statistical significance, power, and required sample size calculation to multi-cell lift studies. We represent our results theoretically in terms of the distributions of test metrics and in practical terms relating to the metrics used by practitioners, making all of our code publicly available.",C. H. Bryan Liu|Elaine M. Bettaney|Benjamin Paul Chamberlain,,https://arxiv.org/abs/1806.02588v2,https://arxiv.org/pdf/1806.02588v2,,"Accepted into 2018 AdKDD & TargetAd Workshop in conjunction with KDD 2018; 6 pages, 4 figures, and 2 tables",,,stat.ME,stat.ME|cs.DM|stat.AP,https://arxiv.org/pdf/1806.02588v2.pdf
1805.12253v1,2018-05-30T22:53:22Z,2018-05-30 22:53:22,Sequential Experimental Design for Optimal Structural Intervention in Gene Regulatory Networks Based on the Mean Objective Cost of Uncertainty,"Scientists are attempting to use models of ever increasing complexity, especially in medicine, where gene-based diseases such as cancer require better modeling of cell regulation. Complex models suffer from uncertainty and experiments are needed to reduce this uncertainty. Because experiments can be costly and time-consuming it is desirable to determine experiments providing the most useful information. If a sequence of experiments is to be performed, experimental design is needed to determine the order. A classical approach is to maximally reduce the overall uncertainty in the model, meaning maximal entropy reduction. A recently proposed method takes into account both model uncertainty and the translational objective, for instance, optimal structural intervention in gene regulatory networks, where the aim is to alter the regulatory logic to maximally reduce the long-run likelihood of being in a cancerous state. The mean objective cost of uncertainty (MOCU) quantifies uncertainty based on the degree to which model uncertainty affects the objective. Experimental design involves choosing the experiment that yields the greatest reduction in MOCU. This paper introduces finite-horizon dynamic programming for MOCU-based sequential experimental design and compares it to the greedy approach, which selects one experiment at a time without consideration of the full horizon of experiments. A salient aspect of the paper is that it demonstrates the advantage of MOCU-based design over the widely used entropy-based design for both greedy and dynamic-programming strategies and investigates the effect of model conditions on the comparative performances.",Mahdi Imani|Roozbeh Dehghannasiri|Ulisses M. Braga-Neto|Edward R. Dougherty,,https://arxiv.org/abs/1805.12253v1,https://arxiv.org/pdf/1805.12253v1,,,,,q-bio.MN,q-bio.MN|eess.SY|stat.ME,https://arxiv.org/pdf/1805.12253v1.pdf
1805.10020v1,2018-05-25T08:01:53Z,2018-05-25 08:01:53,Gaussian process emulation for discontinuous response surfaces with applications for cardiac electrophysiology models,"Mathematical models of biological systems are beginning to be used for safety-critical applications, where large numbers of repeated model evaluations are required to perform uncertainty quantification and sensitivity analysis. Most of these models are nonlinear both in variables and parameters/inputs which has two consequences. First, analytic solutions are rarely available so repeated evaluation of these models by numerically solving differential equations incurs a significant computational burden. Second, many models undergo bifurcations in behaviour as parameters are varied. As a result, simulation outputs often contain discontinuities as we change parameter values and move through parameter/input space.
  Statistical emulators such as Gaussian processes are frequently used to reduce the computational cost of uncertainty quantification, but discontinuities render a standard Gaussian process emulation approach unsuitable as these emulators assume a smooth and continuous response to changes in parameter values.
  In this article, we propose a novel two-step method for building a Gaussian Process emulator for models with discontinuous response surfaces. We first use a Gaussian Process classifier to detect boundaries of discontinuities and then constrain the Gaussian Process emulation of the response surface within these boundaries. We introduce a novel `certainty metric' to guide active learning for a multi-class probabilistic classifier.
  We apply the new classifier to simulations of drug action on a cardiac electrophysiology model, to propagate our uncertainty in a drug's action through to predictions of changes to the cardiac action potential. The proposed two-step active learning method significantly reduces the computational cost of emulating models that undergo multiple bifurcations.",Sanmitra Ghosh|David J. Gavaghan|Gary R. Mirams,,https://arxiv.org/abs/1805.10020v1,https://arxiv.org/pdf/1805.10020v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1805.10020v1.pdf
1805.09964v1,2018-05-25T03:36:09Z,2018-05-25 03:36:09,Myopic Bayesian Design of Experiments via Posterior Sampling and Probabilistic Programming,"We design a new myopic strategy for a wide class of sequential design of experiment (DOE) problems, where the goal is to collect data in order to to fulfil a certain problem specific goal. Our approach, Myopic Posterior Sampling (MPS), is inspired by the classical posterior (Thompson) sampling algorithm for multi-armed bandits and leverages the flexibility of probabilistic programming and approximate Bayesian inference to address a broad set of problems. Empirically, this general-purpose strategy is competitive with more specialised methods in a wide array of DOE tasks, and more importantly, enables addressing complex DOE goals where no existing method seems applicable. On the theoretical side, we leverage ideas from adaptive submodularity and reinforcement learning to derive conditions under which MPS achieves sublinear regret against natural benchmark policies.",Kirthevasan Kandasamy|Willie Neiswanger|Reed Zhang|Akshay Krishnamurthy|Jeff Schneider|Barnabas Poczos,,https://arxiv.org/abs/1805.09964v1,https://arxiv.org/pdf/1805.09964v1,,,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/1805.09964v1.pdf
1805.01143v1,2018-05-03T07:31:20Z,2018-05-03 07:31:20,Experimental Design via Generalized Mean Objective Cost of Uncertainty,"The mean objective cost of uncertainty (MOCU) quantifies the performance cost of using an operator that is optimal across an uncertainty class of systems as opposed to using an operator that is optimal for a particular system. MOCU-based experimental design selects an experiment to maximally reduce MOCU, thereby gaining the greatest reduction of uncertainty impacting the operational objective. The original formulation applied to finding optimal system operators, where optimality is with respect to a cost function, such as mean-square error; and the prior distribution governing the uncertainty class relates directly to the underlying physical system. Here we provide a generalized MOCU and the corresponding experimental design. We then demonstrate how this new formulation includes as special cases MOCU-based experimental design methods developed for materials science and genomic networks when there is experimental error. Most importantly, we show that the classical Knowledge Gradient and Efficient Global Optimization experimental design procedures are actually implementations of MOCU-based experimental design under their modeling assumptions.",Shahin Boluki|Xiaoning Qian|Edward R. Dougherty,,https://arxiv.org/abs/1805.01143v1,https://arxiv.org/pdf/1805.01143v1,,,,,eess.SP,eess.SP|stat.AP,https://arxiv.org/pdf/1805.01143v1.pdf
1804.08975v2,2018-04-24T12:10:00Z,2018-09-19 13:04:49,On the design of experiments based on plastic scintillators using Geant4 simulations,"Plastic scintillators are widely used as particle detectors in many fields, mainly, medicine, particle physics and astrophysics. Traditionally, they are coupled to a photo-multplier (PMT) but now silicon photo-multipliers (SiPM) are evolving as a promising robust alternative, specially in space born experiments since plastic scintillators may be a light option for low Earth orbit missions. Therefore it is timely to make a new analysis of the optimal design for experiments based on plastic scintillators in realistic conditions in such a configuration.
  We analyze here their response to an isotropic flux of electron and proton primaries in the energy range from 1 MeV to 1 GeV, a typical scenario for cosmic ray or space weather experiments, through detailed GEANT4 simulations. First, we focus on the effect of increasing the ratio between the plastic volume and the area of the photo-detector itself and, second, on the benefits of using a reflective coating around the plastic, the most common technique to increase light collection efficiency. In order to achieve a general approach, it is necessary to consider several detector setups. Therefore, we have performed a full set of simulations using the highly tested GEANT4 simulation tool: several parameters have been analyzed such as the energy lost in the coating, the deposited energy in the scintillator, the optical absorption, the fraction of scintillation photons that are not detected, the light collection at the photo-detector, the pulse shape and its time parameters and finally, other design parameters as the surface roughness, the coating reflectivity and the case of a scintillator with two decay components. This work could serve as a guide on the design of future experiments based on the use of plastic scintillators.",G. Ros|G. Sáez-Cano|G. A. Medina-Tanco|A. D. Supanitsky,,https://arxiv.org/abs/1804.08975v2,https://arxiv.org/pdf/1804.08975v2,https://doi.org/10.1016/j.radphyschem.2018.09.021,"29 pages. 17 figures. Keywords: Plastic scintillator, Geant4, Simulations, Coating, Low Earth Orbit, Pulses, Efficiency","Radiation Physics and Chemestry, 2018",10.1016/j.radphyschem.2018.09.021,astro-ph.IM,astro-ph.IM|physics.ins-det,https://arxiv.org/pdf/1804.08975v2.pdf
1804.02655v1,2018-04-08T09:01:41Z,2018-04-08 09:01:41,Efficient Computational Algorithm for Optimal Continuous Experimental Designs,A simple yet efficient computational algorithm for computing the continuous optimal experimental design for linear models is proposed. An alternative proof the monotonic convergence for $D$-optimal criterion on continuous design spaces are provided. We further show that the proposed algorithm converges to the $D$-optimal design. We also provide an algorithm for the $A$-optimality and conjecture that the algorithm convergence monotonically on continuous design spaces. Different numerical examples are used to demonstrated the usefulness and performance of the proposed algorithms.,Jiangtao Duan|Wei Gao|Hon Keung Tony Ng,,https://arxiv.org/abs/1804.02655v1,https://arxiv.org/pdf/1804.02655v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1804.02655v1.pdf
1803.07018v3,2018-03-19T16:22:22Z,2019-01-15 09:35:32,Bayesian design of experiments for intractable likelihood models using coupled auxiliary models and multivariate emulation,"A Bayesian design is given by maximising an expected utility over a design space. The utility is chosen to represent the aim of the experiment and its expectation is taken with respect to all unknowns: responses, parameters and/or models. Although straightforward in principle, there are several challenges to finding Bayesian designs in practice. Firstly, the utility and expected utility are rarely available in closed form and require approximation. Secondly, the design space can be of high-dimensionality. In the case of intractable likelihood models, these problems are compounded by the fact that the likelihood function, whose evaluation is required to approximate the expected utility, is not available in closed form. A strategy is proposed to find Bayesian designs for intractable likelihood models. It relies on the development of an automatic, auxiliary modelling approach, using multivariate Gaussian process emulators, to approximate the likelihood function. This is then combined with a copula-based approach to approximate the marginal likelihood (a quantity commonly required to evaluate many utility functions). These approximations are demonstrated on examples of stochastic process models involving experimental aims of both parameter estimation and model comparison.",Antony M. Overstall|James M. McGree,,https://arxiv.org/abs/1803.07018v3,https://arxiv.org/pdf/1803.07018v3,,Minor & final update,,,stat.ME,stat.ME,https://arxiv.org/pdf/1803.07018v3.pdf
1803.06536v2,2018-03-17T16:40:05Z,2018-06-09 10:58:34,Optimal Design of Experiments for Nonlinear Response Surface Models,"Many chemical and biological experiments involve multiple treatment factors and often it is convenient to fit a nonlinear model in these factors. This nonlinear model can be mechanistic, empirical or a hybrid of the two. Motivated by experiments in chemical engineering, we focus on D-optimal design for multifactor nonlinear response surfaces in general. In order to find and study optimal designs, we first implement conventional point and coordinate exchange algorithms. Next, we develop a novel multiphase optimisation method to construct D-optimal designs with improved properties. The benefits of this method are demonstrated by application to two experiments involving nonlinear regression models. The designs obtained are shown to be considerably more informative than designs obtained using traditional design optimality algorithms.",Yuanzhi Huang|Steven Gilmour|Kalliopi Mylona|Peter Goos,,https://arxiv.org/abs/1803.06536v2,https://arxiv.org/pdf/1803.06536v2,https://doi.org/10.1111/rssc.12313,,,10.1111/rssc.12313,stat.CO,stat.CO|stat.AP,https://arxiv.org/pdf/1803.06536v2.pdf
1802.09582v1,2018-02-26T20:13:14Z,2018-02-26 20:13:14,A graph-theoretic framework for algorithmic design of experiments,"In this paper, we demonstrate that considering experiments in a graph-theoretic manner allows us to exploit automorphisms of the graph to reduce the number of evaluations of candidate designs for those experiments, and thus find optimal designs faster. We show that the use of automorphisms for reducing the number of evaluations required of an optimality criterion function is effective on designs where experimental units have a network structure. Moreover, we show that we can take block designs with no apparent network structure, such as one-way blocked experiments, row-column experiments, and crossover designs, and add block nodes to induce a network structure. Considering automorphisms can thus reduce the amount of time it takes to find optimal designs for a wide class of experiments.",Ben M. Parker|Steven G Gilmour|Vasiliki Koutra,,https://arxiv.org/abs/1802.09582v1,https://arxiv.org/pdf/1802.09582v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1802.09582v1.pdf
1802.06517v2,2018-02-19T04:56:59Z,2018-06-11 04:40:55,Goal-Oriented Optimal Design of Experiments for Large-Scale Bayesian Linear Inverse Problems,"We develop a framework for goal-oriented optimal design of experiments (GOODE) for large-scale Bayesian linear inverse problems governed by PDEs. This framework differs from classical Bayesian optimal design of experiments (ODE) in the following sense: we seek experimental designs that minimize the posterior uncertainty in the experiment end-goal, e.g., a quantity of interest (QoI), rather than the estimated parameter itself. This is suitable for scenarios in which the solution of an inverse problem is an intermediate step and the estimated parameter is then used to compute a QoI. In such problems, a GOODE approach has two benefits: the designs can avoid wastage of experimental resources by a targeted collection of data, and the resulting design criteria are computationally easier to evaluate due to the often low-dimensionality of the QoIs. We present two modified design criteria, A-GOODE and D-GOODE, which are natural analogues of classical Bayesian A- and D-optimal criteria. We analyze the connections to other ODE criteria, and provide interpretations for the GOODE criteria by using tools from information theory. Then, we develop an efficient gradient-based optimization framework for solving the GOODE optimization problems. Additionally, we present comprehensive numerical experiments testing the various aspects of the presented approach. The driving application is the optimal placement of sensors to identify the source of contaminants in a diffusion and transport problem. We enforce sparsity of the sensor placements using an $\ell_1$-norm penalty approach, and propose a practical strategy for specifying the associated penalty parameter.",Ahmed Attia|Alen Alexanderian|Arvind K. Saibaba,,https://arxiv.org/abs/1802.06517v2,https://arxiv.org/pdf/1802.06517v2,https://doi.org/10.1088/1361-6420/aad210,"25 pages, 13 figures",,10.1088/1361-6420/aad210,cs.CE,cs.CE|math.NA|math.OC|stat.AP,https://arxiv.org/pdf/1802.06517v2.pdf
1802.04170v2,2018-02-12T16:34:06Z,2018-05-31 07:39:19,Design of Experiments for Model Discrimination Hybridising Analytical and Data-Driven Approaches,"Healthcare companies must submit pharmaceutical drugs or medical devices to regulatory bodies before marketing new technology. Regulatory bodies frequently require transparent and interpretable computational modelling to justify a new healthcare technology, but researchers may have several competing models for a biological system and too little data to discriminate between the models. In design of experiments for model discrimination, the goal is to design maximally informative physical experiments in order to discriminate between rival predictive models. Prior work has focused either on analytical approaches, which cannot manage all functions, or on data-driven approaches, which may have computational difficulties or lack interpretable marginal predictive distributions. We develop a methodology introducing Gaussian process surrogates in lieu of the original mechanistic models. We thereby extend existing design and model discrimination methods developed for analytical models to cases of non-analytical models in a computationally efficient manner.",Simon Olofsson|Marc Peter Deisenroth|Ruth Misener,,https://arxiv.org/abs/1802.04170v2,https://arxiv.org/pdf/1802.04170v2,,,Proc.Mach.Learn.Res. 80 (2018) pp. 3908-3917,,stat.AP,stat.AP|stat.ML,https://arxiv.org/pdf/1802.04170v2.pdf
1801.09124v3,2018-01-27T18:57:19Z,2019-03-22 11:56:39,Ascent with Quadratic Assistance for the Construction of Exact Experimental Designs,"In the area of statistical planning, there is a large body of theoretical knowledge and computational experience concerning so-called optimal approximate designs of experiments. However, for an approximate design to be executed in practice, it must be converted into an exact, i.e., integer, design, which is usually done via rounding procedures. Although rapid, rounding procedures have many drawbacks; in particular, they often yield worse exact designs than heuristics that do not require approximate designs at all.
  In this paper, we build on an alternative principle of utilizing optimal approximate designs for the computation of optimal, or nearly-optimal, exact designs. The principle, which we call ascent with quadratic assistance (AQuA), is an integer programming method based on the quadratic approximation of the design criterion in the neighborhood of the optimal approximate information matrix.
  To this end, we present quadratic approximations of all Kiefer's criteria with an integer parameter, including D- and A-optimality and, by a model transformation, I-optimality. Importantly, we prove a low-rank property of the associated quadratic forms, which enables us to apply AQuA to large design spaces, for example via mixed integer conic quadratic solvers. We numerically demonstrate the robustness and superior performance of the proposed method for models under various types of constraints. More precisely, we compute optimal size-constrained exact designs for the model of spring-balance weighing, and optimal symmetric marginally restricted exact designs for the Scheffe mixture model. We also show how can iterative application of AQuA be used for a stratified information-based subsampling of large datasets under a lower bound on the quality and an upper bound on the cost of the subsample.",Lenka Filová|Radoslav Harman,,https://arxiv.org/abs/1801.09124v3,https://arxiv.org/pdf/1801.09124v3,,Substantially reworked version to emphasize the advantages of the proposed approach,,,stat.CO,stat.CO,https://arxiv.org/pdf/1801.09124v3.pdf
1801.05661v1,2018-01-17T13:49:47Z,2018-01-17 13:49:47,A Randomized Exchange Algorithm for Computing Optimal Approximate Designs of Experiments,"We propose a class of subspace ascent methods for computing optimal approximate designs that covers both existing as well as new and more efficient algorithms. Within this class of methods, we construct a simple, randomized exchange algorithm (REX). Numerical comparisons suggest that the performance of REX is comparable or superior to the performance of state-of-the-art methods across a broad range of problem structures and sizes. We focus on the most commonly used criterion of D-optimality that also has applications beyond experimental design, such as the construction of the minimum volume ellipsoid containing a given set of data-points. For D-optimality, we prove that the proposed algorithm converges to the optimum. We also provide formulas for the optimal exchange of weights in the case of the criterion of A-optimality. These formulas enable one to use REX for computing A-optimal and I-optimal designs.",Radoslav Harman|Lenka Filová|Peter Richtárik,,https://arxiv.org/abs/1801.05661v1,https://arxiv.org/pdf/1801.05661v1,,"23 pages, 2 figures",,,stat.CO,stat.CO,https://arxiv.org/pdf/1801.05661v1.pdf
1801.01506v4,2018-01-04T19:00:10Z,2020-06-15 18:00:39,Time Delay Lens Modeling Challenge: I. Experimental Design,"Strong gravitational lenses with measured time delay are a powerful tool to measure cosmological parameters, especially the Hubble constant ($H_0$). Recent studies show that by combining just three multiply-imaged AGN systems, one can determine $H_0$ to 2.4% precision. Furthermore, the number of time-delay lens systems is growing rapidly, enabling the determination of $H_0$ to 1% precision in the near future. However, as the precision increases it is important to ensure that systematic errors and biases remain subdominant. For this purpose, challenges with simulated datasets are a key component in this process. Following the experience of the past challenge on time delay, where it was shown that time delays can indeed be measured precisely and accurately at the sub-percent level, we now present the ""Time Delay Lens Modeling Challenge"" (TDLMC). The goal of this challenge is to assess the present capabilities of lens modeling codes and assumptions and test the level of accuracy of inferred cosmological parameters given realistic mock datasets. We invite scientists to model a set of simulated HST observations of 50 mock lens systems. The systems are organized in rungs, with the complexity and realism increasing going up the ladder. The goal of the challenge is to infer $H_0$ for each rung, given the HST images, the time delay, and stellar velocity dispersion of the deflector for a fixed background cosmology. The TDLMC challenge starts with the mock data release on 2018 January 8th. The deadline for blind submission is different for each rung. The deadline for Rung 0-1 is 2018 September 8; the deadline for Rung 2 is 2019 April 8 and the one for Rung 3 is 2019 September 8. This first paper gives an overview of the challenge including the data design, and a set of metrics to quantify the modeling performance and challenge details.",Xuheng Ding|Tommaso Treu|Anowar J. Shajib|Dandan Xu|Geoff C. -F. Chen|Anupreeta More|Giulia Despali|Matteo Frigo|Christopher D. Fassnacht|Daniel Gilman|Stefan Hilbert|Philip J. Marshall|Dominique Sluse|Simona Vegetti,,https://arxiv.org/abs/1801.01506v4,https://arxiv.org/pdf/1801.01506v4,,"6 pages, 2 figures, submitted to MNRAS",,,astro-ph.CO,astro-ph.CO|astro-ph.GA,https://arxiv.org/pdf/1801.01506v4.pdf
1712.06916v4,2017-12-19T13:31:59Z,2018-11-20 10:05:54,Experimental Design Issues in Big Data. The Question of Bias,"Data can be collected in scientific studies via a controlled experiment or passive observation. Big data is often collected in a passive way, e.g. from social media. In studies of causation great efforts are made to guard against bias and hidden confounders or feedback which can destroy the identification of causation by corrupting or omitting counterfactuals (controls). Various solutions of these problems are discussed, including randomization.",Elena Pesce|Eva Riccomagno|Henry P. Wynn,,https://arxiv.org/abs/1712.06916v4,https://arxiv.org/pdf/1712.06916v4,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1712.06916v4.pdf
1712.06028v1,2017-12-16T22:31:52Z,2017-12-16 22:31:52,"A Spectral Approach for the Design of Experiments: Design, Analysis and Algorithms","This paper proposes a new approach to construct high quality space-filling sample designs. First, we propose a novel technique to quantify the space-filling property and optimally trade-off uniformity and randomness in sample designs in arbitrary dimensions. Second, we connect the proposed metric (defined in the spatial domain) to the objective measure of the design performance (defined in the spectral domain). This connection serves as an analytic framework for evaluating the qualitative properties of space-filling designs in general. Using the theoretical insights provided by this spatial-spectral analysis, we derive the notion of optimal space-filling designs, which we refer to as space-filling spectral designs. Third, we propose an efficient estimator to evaluate the space-filling properties of sample designs in arbitrary dimensions and use it to develop an optimization framework to generate high quality space-filling designs. Finally, we carry out a detailed performance comparison on two different applications in 2 to 6 dimensions: a) image reconstruction and b) surrogate modeling on several benchmark optimization functions and an inertial confinement fusion (ICF) simulation code. We demonstrate that the propose spectral designs significantly outperform existing approaches especially in high dimensions.",Bhavya Kailkhura|Jayaraman J. Thiagarajan|Charvi Rastogi|Pramod K. Varshney|Peer-Timo Bremer,,https://arxiv.org/abs/1712.06028v1,https://arxiv.org/pdf/1712.06028v1,,,,,stat.ML,stat.ML|cs.AI,https://arxiv.org/pdf/1712.06028v1.pdf
1712.05453v1,2017-12-14T20:55:51Z,2017-12-14 20:55:51,Experimental design trade-offs for gene regulatory network inference: an in silico study of the yeast Saccharomyces cerevisiae cell cycle,"Time-series of high throughput gene sequencing data intended for gene regulatory network (GRN) inference are often short due to the high costs of sampling cell systems. Moreover, experimentalists lack a set of quantitative guidelines that prescribe the minimal number of samples required to infer a reliable GRN model. We study the temporal resolution of data vs quality of GRN inference in order to ultimately overcome this deficit. The evolution of a Markovian jump process model for the Ras/cAMP/PKA pathway of proteins and metabolites in the G1 phase of the Saccharomyces cerevisiae cell cycle is sampled at a number of different rates. For each time-series we infer a linear regression model of the GRN using the LASSO method. The inferred network topology is evaluated in terms of the area under the precision-recall curve AUPR. By plotting the AUPR against the number of samples, we show that the trade-off has a, roughly speaking, sigmoid shape. An optimal number of samples corresponds to values on the ridge of the sigmoid.",Johan Markdahl|Nicolo Colombo|Johan Thunberg|Jorge Goncalves,,https://arxiv.org/abs/1712.05453v1,https://arxiv.org/pdf/1712.05453v1,,,,,q-bio.QM,q-bio.QM|math.OC,https://arxiv.org/pdf/1712.05453v1.pdf
1712.01620v1,2017-12-05T13:53:46Z,2017-12-05 13:53:46,Efficient sequential experimental design for surrogate modeling of nested codes,"Thanks to computing power increase, the certification and the conception of complex systems relies more and more on simulation. To this end, predictive codes are needed, which have generally to be evaluated in a huge number of input points. When the computational cost of these codes is high, surrogate models are introduced to emulate the response of these codes. In this paper, we consider the situation when the system response can be modeled by two nested computer codes. By two nested computer codes, we mean that some inputs of the second code are outputs of the first code. More precisely, the idea is to propose sequential designs to improve the accuracy of the nested code's predictor by exploiting the nested structure of the codes. In particular, a selection criterion is proposed to allow the modeler to choose the code to call, depending on the expected learning rate and the computational cost of each code. The sequential designs are based on the minimization of the prediction variance, so adaptations of the Gaussian process formalism are proposed for this particular configuration in order to quickly evaluate the mean and the variance of the predictor. The proposed methods are then applied to examples.",Sophie Marque-Pucheu|Guillaume Perrin|Josselin Garnier,,https://arxiv.org/abs/1712.01620v1,https://arxiv.org/pdf/1712.01620v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1712.01620v1.pdf
1712.00229v1,2017-12-01T08:23:22Z,2017-12-01 08:23:22,Efficient determination of optimised multi-arm multi-stage experimental designs with control of generalised error-rates,"Primarily motivated by the drug development process, several publications have now presented methodology for the design of multi-arm multi-stage experiments with normally distributed outcome variables of known variance. Here, we extend these past considerations to allow the design of what we refer to as an abcd multi-arm multi-stage experiment. We provide a proof of how strong control of the a-generalised type-I familywise error-rate can be ensured. We then describe how to attain the power to reject at least b out of c false hypotheses, which is related to controlling the b-generalised type-II familywise error-rate. Following this, we detail how a design can be optimised for a scenario in which rejection of any d null hypotheses brings about termination of the experiment. We achieve this by proposing a highly computationally efficient approach for evaluating the performance of a candidate design. Finally, using a real clinical trial as a motivating example, we explore the effect of the design's control parameters on the statistical operating characteristics.",Michael Grayling|James Wason|Adrian Mander,,https://arxiv.org/abs/1712.00229v1,https://arxiv.org/pdf/1712.00229v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1712.00229v1.pdf
1711.11488v3,2017-11-30T16:12:15Z,2021-01-15 15:29:37,Summary of effect aliasing structure (SEAS): new descriptive statistics for factorial and supersaturated designs,"In the assessment and selection of supersaturated designs, the aliasing structure of interaction effects is usually ignored by traditional criteria such as $E(s^2)$-optimality. We introduce the Summary of Effect Aliasing Structure (SEAS) for assessing the aliasing structure of supersaturated designs, and other non-regular fractional factorial designs, that takes account of interaction terms and provides more detail than usual summaries such as (generalized) resolution and wordlength patterns. The new summary consists of three criteria, abbreviated as MAP: (1) Maximum dependency aliasing pattern; (2) Average square aliasing pattern; and (3) Pairwise dependency ratio. These criteria provided insight when traditional criteria fail to differentiate between designs. We theoretically study the relationship between the MAP criteria and traditional quantities, and demonstrate the use of SEAS for comparing some example supersaturated designs, including designs suggested in the literature. We also propose a variant of SEAS to measure the aliasing structure for individual columns of a design, and use it to choose assignments of factors to columns for an $E(s^2)$-optimal design.",Frederick Kin Hing Phoa|Yi-Hua Liao|David C. Woods|Shah-Kae Chou,,https://arxiv.org/abs/1711.11488v3,https://arxiv.org/pdf/1711.11488v3,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1711.11488v3.pdf
1711.05878v1,2017-11-16T01:18:29Z,2017-11-16 01:18:29,Efficient D-optimal design of experiments for infinite-dimensional Bayesian linear inverse problems,"We develop a computational framework for D-optimal experimental design for PDE-based Bayesian linear inverse problems with infinite-dimensional parameters. We follow a formulation of the experimental design problem that remains valid in the infinite-dimensional limit. The optimal design is obtained by solving an optimization problem that involves repeated evaluation of the log-determinant of high-dimensional operators along with their derivatives. Forming and manipulating these operators is computationally prohibitive for large-scale problems. Our methods exploit the low-rank structure in the inverse problem in three different ways, yielding efficient algorithms. Our main approach is to use randomized estimators for computing the D-optimal criterion, its derivative, as well as the Kullback--Leibler divergence from posterior to prior. Two other alternatives are proposed based on a low-rank approximation of the prior-preconditioned data misfit Hessian, and a fixed low-rank approximation of the prior-preconditioned forward operator. Detailed error analysis is provided for each of the methods, and their effectiveness is demonstrated on a model sensor placement problem for initial state reconstruction in a time-dependent advection-diffusion equation in two space dimensions.",Alen Alexanderian|Arvind K. Saibaba,,https://arxiv.org/abs/1711.05878v1,https://arxiv.org/pdf/1711.05878v1,,"27 pages, 9 figures",,,math.NA,math.NA,https://arxiv.org/pdf/1711.05878v1.pdf
1711.05174v1,2017-11-14T16:21:57Z,2017-11-14 16:21:57,Near-Optimal Discrete Optimization for Experimental Design: A Regret Minimization Approach,"The experimental design problem concerns the selection of k points from a potentially large design pool of p-dimensional vectors, so as to maximize the statistical efficiency regressed on the selected k design points. Statistical efficiency is measured by optimality criteria, including A(verage), D(eterminant), T(race), E(igen), V(ariance) and G-optimality. Except for the T-optimality, exact optimization is NP-hard.
  We propose a polynomial-time regret minimization framework to achieve a $(1+\varepsilon)$ approximation with only $O(p/\varepsilon^2)$ design points, for all the optimality criteria above.
  In contrast, to the best of our knowledge, before our work, no polynomial-time algorithm achieves $(1+\varepsilon)$ approximations for D/E/G-optimality, and the best poly-time algorithm achieving $(1+\varepsilon)$-approximation for A/V-optimality requires $k = Ω(p^2/\varepsilon)$ design points.",Zeyuan Allen-Zhu|Yuanzhi Li|Aarti Singh|Yining Wang,,https://arxiv.org/abs/1711.05174v1,https://arxiv.org/pdf/1711.05174v1,,"33 pages, 4 tables. A preliminary version of this paper titled ""Near-Optimal Experimental Design via Regret Minimization"" with weaker results appeared in the Proceedings of the 34th International Conference on Machine Learning (ICML 2017), Sydney",,,stat.ML,stat.ML|cs.LG|stat.CO,https://arxiv.org/pdf/1711.05174v1.pdf
1711.03166v1,2017-11-08T21:10:36Z,2017-11-08 21:10:36,Notes on the design of experiments and beam diagnostics with synchrotron light detected by a gated photomultiplier for the Fermilab superconducting electron linac and for the Integrable Optics Test Accelerator (IOTA),"We outline the design of beam experiments for the electron linac at the Fermilab Accelerator Science and Technology (FAST) facility and for the Integrable Optics Test Accelerator (IOTA), based on synchrotron light emitted by the electrons in bend dipoles, detected with gated microchannel-plate photomultipliers (MCP-PMTs). The system can be used both for beam diagnostics (e.g., beam intensity with full dynamic range, turn-by-turn beam vibrations, etc.) and for scientific experiments, such as the direct observation of the time structure of the radiation emitted by single electrons in a storage ring. The similarity between photon pulses and spectrum at the downstream end of the electron linac and in the IOTA ring allows one to test the apparatus during commissioning of the linac.",Giulio Stancari|Aleksandr Romanov|Jinhao Ruan|James Santucci|Randy Thurman-Keup|Alexander Valishev,,https://arxiv.org/abs/1711.03166v1,https://arxiv.org/pdf/1711.03166v1,,"8 pages, 2 figures, 2 tables",,,physics.acc-ph,physics.acc-ph,https://arxiv.org/pdf/1711.03166v1.pdf
1711.01501v2,2017-11-04T22:28:01Z,2018-01-30 21:14:32,Approximate Supermodularity Bounds for Experimental Design,"This work provides performance guarantees for the greedy solution of experimental design problems. In particular, it focuses on A- and E-optimal designs, for which typical guarantees do not apply since the mean-square error and the maximum eigenvalue of the estimation error covariance matrix are not supermodular. To do so, it leverages the concept of approximate supermodularity to derive non-asymptotic worst-case suboptimality bounds for these greedy solutions. These bounds reveal that as the SNR of the experiments decreases, these cost functions behave increasingly as supermodular functions. As such, greedy A- and E-optimal designs approach (1-1/e)-optimality. These results reconcile the empirical success of greedy experimental design with the non-supermodularity of the A- and E-optimality criteria.",Luiz F. O. Chamon|Alejandro Ribeiro,,https://arxiv.org/abs/1711.01501v2,https://arxiv.org/pdf/1711.01501v2,,"15 pages, NIPS 2017",,,cs.LG,cs.LG|cs.DM|math.OC|math.ST,https://arxiv.org/pdf/1711.01501v2.pdf
1710.11088v4,2017-10-30T17:34:59Z,2019-01-03 11:27:07,Robust Cooperative Manipulation without Force/Torque Measurements: Control Design and Experiments,"This paper presents two novel control methodologies for the cooperative manipulation of an object by N robotic agents. Firstly, we design an adaptive control protocol which employs quaternion feedback for the object orientation to avoid potential representation singularities. Secondly, we propose a control protocol that guarantees predefined transient and steady-state performance for the object trajectory. Both methodologies are decentralized, since the agents calculate their own signals without communicating with each other, as well as robust to external disturbances and model uncertainties. Moreover, we consider that the grasping points are rigid, and avoid the need for force/torque measurements. Load distribution is also included via a grasp matrix pseudo-inverse to account for potential differences in the agents' power capabilities. Finally, simulation and experimental results with two robotic arms verify the theoretical findings.",Christos K. Verginis|Matteo Mastellaro|Dimos V. Dimarogonas,,https://arxiv.org/abs/1710.11088v4,https://arxiv.org/pdf/1710.11088v4,,,,,cs.RO,cs.RO|eess.SY,https://arxiv.org/pdf/1710.11088v4.pdf
1710.03500v1,2017-10-10T10:29:26Z,2017-10-10 10:29:26,Fast Bayesian experimental design: Laplace-based importance sampling for the expected information gain,"In calculating expected information gain in optimal Bayesian experimental design, the computation of the inner loop in the classical double-loop Monte Carlo requires a large number of samples and suffers from underflow if the number of samples is small. These drawbacks can be avoided by using an importance sampling approach. We present a computationally efficient method for optimal Bayesian experimental design that introduces importance sampling based on the Laplace method to the inner loop. We derive the optimal values for the method parameters in which the average computational cost is minimized according to the desired error tolerance. We use three numerical examples to demonstrate the computational efficiency of our method compared with the classical double-loop Monte Carlo, and a more recent single-loop Monte Carlo method that uses the Laplace method as an approximation of the return value of the inner loop. The first example is a scalar problem that is linear in the uncertain parameter. The second example is a nonlinear scalar problem. The third example deals with the optimal sensor placement for an electrical impedance tomography experiment to recover the fiber orientation in laminate composites.",Joakim Beck|Ben Mansour Dia|Luis FR Espath|Quan Long|Raul Tempone,,https://arxiv.org/abs/1710.03500v1,https://arxiv.org/pdf/1710.03500v1,https://doi.org/10.1016/j.cma.2018.01.053,"42 pages, 35 figures",,10.1016/j.cma.2018.01.053,math.NA,math.NA,https://arxiv.org/pdf/1710.03500v1.pdf
1709.02317v1,2017-09-07T15:37:50Z,2017-09-07 15:37:50,Computing optimal experimental designs with respect to a compound Bayes risk criterion,"We consider the problem of computing optimal experimental design on a finite design space with respect to a compound Bayes risk criterion, which includes the linear criterion for prediction in a random coefficient regression model. We show that the problem can be restated as constrained A-optimality in an artificial model. This permits using recently developed computational tools, for instance the algorithms based on the second-order cone programming for optimal approximate design, and mixed-integer second-order cone programming for optimal exact designs. We demonstrate the use of the proposed method for the problem of computing optimal designs of a random coefficient regression model with respect to an integrated mean squared error criterion.",Radoslav Harman|Maryna Prus,,https://arxiv.org/abs/1709.02317v1,https://arxiv.org/pdf/1709.02317v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1709.02317v1.pdf
1708.04740v1,2017-08-16T01:55:02Z,2017-08-16 01:55:02,Optimal Experimental Design for Constrained Inverse Problems,"In this paper, we address the challenging problem of optimal experimental design (OED) of constrained inverse problems. We consider two OED formulations that allow reducing the experimental costs by minimizing the number of measurements. The first formulation assumes a fine discretization of the design parameter space and uses sparsity promoting regularization to obtain an efficient design. The second formulation parameterizes the design and seeks optimal placement for these measurements by solving a small-dimensional optimization problem. We consider both problems in a Bayes risk as well as an empirical Bayes risk minimization framework. For the unconstrained inverse state problem, we exploit the closed form solution for the inner problem to efficiently compute derivatives for the outer OED problem. The empirical formulation does not require an explicit solution of the inverse problem and therefore allows to integrate constraints efficiently. A key contribution is an efficient optimization method for solving the resulting, typically high-dimensional, bilevel optimization problem using derivative-based methods. To overcome the lack of non-differentiability in active set methods for inequality constraints problems, we use a relaxed interior point method. To address the growing computational complexity of empirical Bayes OED, we parallelize the computation over the training models. Numerical examples and illustrations from tomographic reconstruction, for various data sets and under different constraints, demonstrate the impact of constraints on the optimal design and highlight the importance of OED for constrained problems.",Lars Ruthotto|Julianne Chung|Matthias Chung,,https://arxiv.org/abs/1708.04740v1,https://arxiv.org/pdf/1708.04740v1,,"19 pages, 8 figures",,,math.NA,math.NA|math.OC,https://arxiv.org/pdf/1708.04740v1.pdf
1708.01481v2,2017-08-04T13:09:31Z,2018-08-07 20:38:34,Multivariate Design of Experiments for Engineering Dimensional Analysis,"We consider the design of dimensional analysis experiments when there is more than a single response. We first give a brief overview of dimensional analysis experiments and the dimensional analysis (DA) procedure. The validity of the DA method for univariate responses was established by the Buckingham $Π$-Theorem in the early 20th century. We extend the theorem to the multivariate case, develop basic criteria for multivariate design of DA and give guidelines for design construction. Finally, we illustrate the construction of designs for DA experiments for an example involving the design of a heat exchanger.",Daniel J. Eck|Christopher J. Nachtsheim|R. Dennis Cook|Thomas A. Albrecht,,https://arxiv.org/abs/1708.01481v2,https://arxiv.org/pdf/1708.01481v2,,,,,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/1708.01481v2.pdf
1707.08384v1,2017-07-26T11:35:58Z,2017-07-26 11:35:58,Sequential design of experiments to estimate a probability of exceeding a threshold in a multi-fidelity stochastic simulator,"In this article, we consider a stochastic numerical simulator to assess the impact of some factors on a phenomenon. The simulator is seen as a black box with inputs and outputs. The quality of a simulation, hereafter referred to as fidelity, is assumed to be tunable by means of an additional input of the simulator (e.g., a mesh size parameter): high-fidelity simulations provide more accurate results, but are time-consuming. Using a limited computation-time budget, we want to estimate, for any value of the physical inputs, the probability that a certain scalar output of the simulator will exceed a given critical threshold at the highest fidelity level. The problem is addressed in a Bayesian framework, using a Gaussian process model of the multi-fidelity simulator. We consider a Bayesian estimator of the probability, together with an associated measure of uncertainty, and propose a new multi-fidelity sequential design strategy, called Maximum Speed of Uncertainty Reduction (MSUR), to select the value of physical inputs and the fidelity level of new simulations. The MSUR strategy is tested on an example.",Rémi Stroh|Séverine Demeyer|Nicolas Fischer|Julien Bect|Emmanuel Vazquez,LNE|LNE|LNE|L2S|GdR MASCOT-NUM,https://arxiv.org/abs/1707.08384v1,https://arxiv.org/pdf/1707.08384v1,,"61th World Statistics Congress of the International Statistical Institute (ISI 2017), Jul 2017, Marrakech, Morocco",,,stat.CO,stat.CO|stat.ME|stat.ML,https://arxiv.org/pdf/1707.08384v1.pdf
1707.00289v1,2017-07-02T13:41:03Z,2017-07-02 13:41:03,Efficient experimental design of high-fidelity three-qubit quantum gates via genetic programming,"We have designed efficient quantum circuits for the three-qubit Toffoli (controlled-controlled NOT) and the Fredkin (controlled-SWAP) gate, optimized via genetic programming methods. The gates thus obtained were experimentally implemented on a three-qubit NMR quantum information processor, with a high fidelity. Toffoli and Fredkin gates in conjunction with the single-qubit Hadamard gates form a universal gate set for quantum computing, and are an essential component of several quantum algorithms. Genetic algorithms are stochastic search algorithms based on the logic of natural selection and biological genetics and have been widely used for quantum information processing applications. The numerically optimized rf pulse profiles of the three-qubit quantum gates achieve $> 99\%$ fidelity. The optimization was performed under the constraint that the experimentally implemented pulses are of short duration and can be implemented with high fidelity. Therefore the gate implementations do not suffer from the drawbacks of rf offset errors or debilitating effects of decoherence during gate action. We demonstrate the advantage of our pulse sequences by comparing our results with existing experimental schemes.",Amit Devra|Prithviraj Prabhu|Harpreet Singh| Arvind|Kavita Dorai,,https://arxiv.org/abs/1707.00289v1,https://arxiv.org/pdf/1707.00289v1,https://doi.org/10.1007/s11128-018-1835-8,,"Quantum Inf Process 17, 67 (2018)",10.1007/s11128-018-1835-8,quant-ph,quant-ph,https://arxiv.org/pdf/1707.00289v1.pdf
1706.09835v2,2017-06-29T16:28:11Z,2017-07-03 15:27:03,Linear Estimation of Treatment Effects in Demand Response: An Experimental Design Approach,"Demand response aims to stimulate electricity consumers to modify their loads at critical time periods. In this paper, we consider signals in demand response programs as a binary treatment to the customers and estimate the average treatment effect, which is the average change in consumption under the demand response signals. More specifically, we propose to estimate this effect by linear regression models and derive several estimators based on the different models. From both synthetic and real data, we show that including more information about the customers does not always improve estimation accuracy: the interaction between the side information and the demand response signal must be carefully modeled. In addition, we compare the traditional linear regression model with the modified covariate method which models the interaction between treatment effect and covariates. We analyze the variances of these estimators and discuss different cases where each respective estimator works the best. The purpose of these comparisons is not to claim the superiority of the different methods, rather we aim to provide practical guidance on the most suitable estimator to use under different settings. Our results are validated using data collected by Pecan Street and EnergyPlus.",Pan Li|Baosen Zhang,,https://arxiv.org/abs/1706.09835v2,https://arxiv.org/pdf/1706.09835v2,,Companion for a submission to IEEE Transaction on Power Systems,,,math.OC,math.OC|eess.SY,https://arxiv.org/pdf/1706.09835v2.pdf
1705.09395v1,2017-05-25T23:24:11Z,2017-05-25 23:24:11,Optimal Experimental Design Using A Consistent Bayesian Approach,"We consider the utilization of a computational model to guide the optimal acquisition of experimental data to inform the stochastic description of model input parameters. Our formulation is based on the recently developed consistent Bayesian approach for solving stochastic inverse problems which seeks a posterior probability density that is consistent with the model and the data in the sense that the push-forward of the posterior (through the computational model) matches the observed density on the observations almost everywhere. Given a set a potential observations, our optimal experimental design (OED) seeks the observation, or set of observations, that maximizes the expected information gain from the prior probability density on the model parameters. We discuss the characterization of the space of observed densities and a computationally efficient approach for rescaling observed densities to satisfy the fundamental assumptions of the consistent Bayesian approach. Numerical results are presented to compare our approach with existing OED methodologies using the classical/statistical Bayesian approach and to demonstrate our OED on a set of representative PDE-based models.",Scott N. Walsh|Tim M. Wildey|John D. Jakeman,,https://arxiv.org/abs/1705.09395v1,https://arxiv.org/pdf/1705.09395v1,https://doi.org/10.1115/1.4037457,,,10.1115/1.4037457,stat.CO,stat.CO|math.NA,https://arxiv.org/pdf/1705.09395v1.pdf
1705.09677v1,2017-05-24T16:34:00Z,2017-05-24 16:34:00,Elementary Symmetric Polynomials for Optimal Experimental Design,"We revisit the classical problem of optimal experimental design (OED) under a new mathematical model grounded in a geometric motivation. Specifically, we introduce models based on elementary symmetric polynomials; these polynomials capture ""partial volumes"" and offer a graded interpolation between the widely used A-optimal design and D-optimal design models, obtaining each of them as special cases. We analyze properties of our models, and derive both greedy and convex-relaxation algorithms for computing the associated designs. Our analysis establishes approximation guarantees on these algorithms, while our empirical results substantiate our claims and demonstrate a curious phenomenon concerning our greedy method. Finally, as a byproduct, we obtain new results on the theory of elementary symmetric polynomials that may be of independent interest.",Zelda Mariet|Suvrit Sra,,https://arxiv.org/abs/1705.09677v1,https://arxiv.org/pdf/1705.09677v1,,,,,math.ST,math.ST,https://arxiv.org/pdf/1705.09677v1.pdf
1705.08096v3,2017-05-23T06:52:29Z,2018-12-26 07:58:24,acebayes: An R Package for Bayesian Optimal Design of Experiments via Approximate Coordinate Exchange,"We describe the R package acebayes and demonstrate its use to find Bayesian optimal experimental designs. A decision-theoretic approach is adopted, with the optimal design maximising an expected utility. Finding Bayesian optimal designs for realistic problems is challenging, as the expected utility is typically intractable and the design space may be high-dimensional. The package implements the approximate coordinate exchange algorithm to optimise (an approximation to) the expected utility via a sequence of conditional one-dimensional optimisation steps. At each step, a Gaussian process regression model is used to approximate, and subsequently optimise, the expected utility as the function of a single design coordinate (the value taken by one controllable variable for one run of the experiment). In addition to functions for bespoke design problems with user-defined utility functions, acebayes provides functions tailored to finding designs for common generalised linear and nonlinear models. The package provides a step-change in the complexity of problems that can be addressed, enabling designs to be found for much larger numbers of variables and runs than previously possible. We provide tutorials on the application of the methodology for four illustrative examples of varying complexity where designs are found for the goals of parameter estimation, model selection and prediction. These examples demonstrate previously unseen functionality of acebayes.",Antony Overstall|David Woods|Maria Adamou,,https://arxiv.org/abs/1705.08096v3,https://arxiv.org/pdf/1705.08096v3,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1705.08096v3.pdf
1705.03944v1,2017-05-10T20:19:05Z,2017-05-10 20:19:05,Efficient design of experiments for sensitivity analysis based on polynomial chaos expansions,"Global sensitivity analysis aims at quantifying respective effects of input random variables (or combinations thereof) onto variance of a physical or mathematical model response. Among the abundant literature on sensitivity measures, Sobol' indices have received much attention since they provide accurate information for most of models. We consider a problem of experimental design points selection for Sobol' indices estimation. Based on the concept of $D$-optimality, we propose a method for constructing an adaptive design of experiments, effective for calculation of Sobol' indices based on Polynomial Chaos Expansions. We provide a set of applications that demonstrate the efficiency of the proposed approach.",E. Burnaev|I. Panin|B. Sudret,,https://arxiv.org/abs/1705.03944v1,https://arxiv.org/pdf/1705.03944v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1705.03944v1.pdf
1705.00956v3,2017-05-02T13:23:53Z,2017-06-04 19:33:02,Experimental Design for Non-Parametric Correction of Misspecified Dynamical Models,"We consider a class of misspecified dynamical models where the governing term is only approximately known. Under the assumption that observations of the system's evolution are accessible for various initial conditions, our goal is to infer a non-parametric correction to the misspecified driving term such as to faithfully represent the system dynamics and devise system evolution predictions for unobserved initial conditions.
  We model the unknown correction term as a Gaussian Process and analyze the problem of efficient experimental design to find an optimal correction term under constraints such as a limited experimental budget. We suggest a novel formulation for experimental design for this Gaussian Process and show that approximately optimal (up to a constant factor) designs may be efficiently derived by utilizing results from the literature on submodular optimization. Our numerical experiments exemplify the effectiveness of these techniques.",Gal Shulkind|Lior Horesh|Haim Avron,,https://arxiv.org/abs/1705.00956v3,https://arxiv.org/pdf/1705.00956v3,,A couple of (??) were corrected,,,stat.ML,stat.ML,https://arxiv.org/pdf/1705.00956v3.pdf
1704.07423v2,2017-04-21T05:07:35Z,2017-07-04 16:26:17,High-Dimensional Materials and Process Optimization using Data-driven Experimental Design with Well-Calibrated Uncertainty Estimates,"The optimization of composition and processing to obtain materials that exhibit desirable characteristics has historically relied on a combination of scientist intuition, trial and error, and luck. We propose a methodology that can accelerate this process by fitting data-driven models to experimental data as it is collected to suggest which experiment should be performed next. This methodology can guide the scientist to test the most promising candidates earlier, and can supplement scientific intuition and knowledge with data-driven insights. A key strength of the proposed framework is that it scales to high-dimensional parameter spaces, as are typical in materials discovery applications. Importantly, the data-driven models incorporate uncertainty analysis, so that new experiments are proposed based on a combination of exploring high-uncertainty candidates and exploiting high-performing regions of parameter space. Over four materials science test cases, our methodology led to the optimal candidate being found with three times fewer required measurements than random guessing on average.",Julia Ling|Max Hutchinson|Erin Antono|Sean Paradiso|Bryce Meredig,,https://arxiv.org/abs/1704.07423v2,https://arxiv.org/pdf/1704.07423v2,https://doi.org/10.1007/s40192-017-0098-z,,"Integrating Materials and Manufacturing Innovation, (2017)",10.1007/s40192-017-0098-z,stat.ML,stat.ML|cond-mat.mtrl-sci,https://arxiv.org/pdf/1704.07423v2.pdf
1704.03995v4,2017-04-13T05:16:33Z,2019-03-30 07:17:28,Optimal experimental design that minimizes the width of simultaneous confidence bands,"We propose an optimal experimental design for a curvilinear regression model that minimizes the band-width of simultaneous confidence bands. Simultaneous confidence bands for curvilinear regression are constructed by evaluating the volume of a tube about a curve that is defined as a trajectory of a regression basis vector (Naiman, 1986). The proposed criterion is constructed based on the volume of a tube, and the corresponding optimal design that minimizes the volume of tube is referred to as the tube-volume optimal (TV-optimal) design. For Fourier and weighted polynomial regressions, the problem is formalized as one of minimization over the cone of Hankel positive definite matrices, and the criterion to minimize is expressed as an elliptic integral. We show that the Möbius group keeps our problem invariant, and hence, minimization can be conducted over cross-sections of orbits. We demonstrate that for the weighted polynomial regression and the Fourier regression with three bases, the tube-volume optimal design forms an orbit of the Möbius group containing D-optimal designs as representative elements.",Satoshi Kuriki|Henry P. Wynn,,https://arxiv.org/abs/1704.03995v4,https://arxiv.org/pdf/1704.03995v4,,"34 pages, 2 figures, 1 table",,,math.ST,math.ST|stat.ME,https://arxiv.org/pdf/1704.03995v4.pdf
1703.07747v1,2017-03-22T16:56:48Z,2017-03-22 16:56:48,MIMIX: a Bayesian Mixed-Effects Model for Microbiome Data from Designed Experiments,"Recent advances in bioinformatics have made high-throughput microbiome data widely available, and new statistical tools are required to maximize the information gained from these data. For example, analysis of high-dimensional microbiome data from designed experiments remains an open area in microbiome research. Contemporary analyses work on metrics that summarize collective properties of the microbiome, but such reductions preclude inference on the fine-scale effects of environmental stimuli on individual microbial taxa. Other approaches model the proportions or counts of individual taxa as response variables in mixed models, but these methods fail to account for complex correlation patterns among microbial communities. In this paper, we propose a novel Bayesian mixed-effects model that exploits cross-taxa correlations within the microbiome, a model we call MIMIX (MIcrobiome MIXed model). MIMIX offers global tests for treatment effects, local tests and estimation of treatment effects on individual taxa, quantification of the relative contribution from heterogeneous sources to microbiome variability, and identification of latent ecological subcommunities in the microbiome. MIMIX is tailored to large microbiome experiments using a combination of Bayesian factor analysis to efficiently represent dependence between taxa and Bayesian variable selection methods to achieve sparsity. We demonstrate the model using a simulation experiment and on a 2x2 factorial experiment of the effects of nutrient supplement and herbivore exclusion on the foliar fungal microbiome of $\textit{Andropogon gerardii}$, a perennial bunchgrass, as part of the global Nutrient Network research initiative.",Neal S. Grantham|Brian J. Reich|Elizabeth T. Borer|Kevin Gross,,https://arxiv.org/abs/1703.07747v1,https://arxiv.org/pdf/1703.07747v1,,"21 pages, 6 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/1703.07747v1.pdf
1703.05511v2,2017-03-16T09:01:22Z,2018-03-13 06:18:28,An Induced Natural Selection Heuristic for Finding Optimal Bayesian Experimental Designs,"Bayesian optimal experimental design has immense potential to inform the collection of data so as to subsequently enhance our understanding of a variety of processes. However, a major impediment is the difficulty in evaluating optimal designs for problems with large, or high-dimensional, design spaces. We propose an efficient search heuristic suitable for general optimisation problems, with a particular focus on optimal Bayesian experimental design problems. The heuristic evaluates the objective (utility) function at an initial, randomly generated set of input values. At each generation of the algorithm, input values are ""accepted"" if their corresponding objective (utility) function satisfies some acceptance criteria, and new inputs are sampled about these accepted points. We demonstrate the new algorithm by evaluating the optimal Bayesian experimental designs for the previously considered death, pharmacokinetic and logistic regression models. Comparisons to the current ""gold-standard"" method are given to demonstrate the proposed algorithm as a computationally-efficient alternative for moderately-large design problems (i.e., up to approximately 40-dimensions).",David J. Price|Nigel G. Bean|Joshua V. Ross|Jonathan Tuke,,https://arxiv.org/abs/1703.05511v2,https://arxiv.org/pdf/1703.05511v2,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1703.05511v2.pdf
1703.05312v1,2017-03-15T08:09:27Z,2017-03-15 08:09:27,On optimal experimental designs for Sparse Polynomial Chaos Expansions,"Uncertainty quantification (UQ) has received much attention in the literature in the past decade. In this context, Sparse Polynomial chaos expansions (PCE) have been shown to be among the most promising methods because of their ability to model highly complex models at relatively low computational costs. A least-square minimization technique may be used to determine the coefficients of the sparse PCE by relying on the so called experimental design (ED), i.e. the sample points where the original computational model is evaluated. An efficient sampling strategy is then needed to generate an accurate PCE at low computational cost. This paper is concerned with the problem of identifying an optimal experimental design that maximizes the accuracy of the surrogate model over the whole input space within a given computational budget. A novel sequential adaptive strategy where the ED is enriched sequentially by capitalizing on the sparsity of the underlying metamodel is introduced. A comparative study between several state-of-the-art methods is performed on four numerical models with varying input dimensionality and computational complexity. It is shown that the optimal sequential design based on the S-value criterion yields accurate, stable and computationally efficient PCE.",N. Fajraoui|S. Marelli|B. Sudret,,https://arxiv.org/abs/1703.05312v1,https://arxiv.org/pdf/1703.05312v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1703.05312v1.pdf
1702.00815v2,2017-02-01T17:25:41Z,2019-07-22 08:06:02,Optimal Experimental Design of Field Trials using Differential Evolution,"When setting up field experiments, to test and compare a range of genotypes (e.g. maize hybrids), it is important to account for any possible field effect that may otherwise bias performance estimates of genotypes. To do so, we propose a model-based method aimed at optimizing the allocation of the tested genotypes and checks between fields and placement within field, according to their kinship. This task can be formulated as a combinatorial permutation-based problem. We used Differential Evolution concept to solve this problem. We then present results of optimal strategies for between-field and within-field placements of genotypes and compare them to existing optimization strategies, both in terms of convergence time and result quality. The new algorithm gives promising results in terms of convergence and search space exploration.",Vitaliy Feoktistov|Stephane Pietravalle|Nicolas Heslot,,https://arxiv.org/abs/1702.00815v2,https://arxiv.org/pdf/1702.00815v2,,"7 pages, 5 figures",,,cs.NE,cs.NE|q-bio.QM,https://arxiv.org/pdf/1702.00815v2.pdf
1702.06001v2,2017-02-01T04:38:31Z,2017-10-16 09:53:37,Optimal design of experiments by combining coarse and fine measurements,"In many contexts it is extremely costly to perform enough high quality experimental measurements to accurately parameterize a predictive quantitative model. However, it is often much easier to carry out large numbers of experiments that indicate whether each sample is above or below a given threshold. Can many such categorical or ""coarse"" measurements be combined with a much smaller number of high resolution or ""fine"" measurements to yield accurate models? Here, we demonstrate an intuitive strategy, inspired by statistical physics, wherein the coarse measurements are used to identify the salient features of the data, while the fine measurements determine the relative importance of these features. A linear model is inferred from the fine measurements, augmented by a quadratic term that captures the correlation structure of the coarse data. We illustrate our strategy by considering the problems of predicting the antimalarial potency and aqueous solubility of small organic molecules from their 2D molecular structure.",Alpha A. Lee|Michael P. Brenner|Lucy J. Colwell,,https://arxiv.org/abs/1702.06001v2,https://arxiv.org/pdf/1702.06001v2,https://doi.org/10.1103/PhysRevLett.119.208101,To appear in Physical Review Letters,"Phys. Rev. Lett. 119, 208101 (2017)",10.1103/PhysRevLett.119.208101,physics.data-an,physics.data-an|cond-mat.soft|cond-mat.stat-mech|physics.chem-ph,https://arxiv.org/pdf/1702.06001v2.pdf
1611.07256v6,2016-11-22T11:40:25Z,2020-02-04 07:22:36,Adaptive Design of Experiments for Conservative Estimation of Excursion Sets,"We consider the problem of estimating the set of all inputs that leads a system to some particular behavior. The system is modeled by an expensive-to-evaluate function, such as a computer experiment, and we are interested in its excursion set, i.e. the set of points where the function takes values above or below some prescribed threshold. The objective function is emulated with a Gaussian Process (GP) model based on an initial design of experiments enriched with evaluation results at (batch-)sequentially determined input points. The GP model provides conservative estimates for the excursion set, which control false positives while minimizing false negatives. We introduce adaptive strategies that sequentially select new evaluations of the function by reducing the uncertainty on conservative estimates. Following the Stepwise Uncertainty Reduction approach we obtain new evaluations by minimizing adapted criteria. Tractable formulae for the conservative criteria are derived, which allow more convenient optimization. The method is benchmarked on random functions generated under the model assumptions in different scenarios of noise and batch size. We then apply it to a reliability engineering test case. Overall, the proposed strategy of minimizing false negatives in conservative estimation achieves competitive performance both in terms of model-based and model-free indicators.",Dario Azzimonti|David Ginsbourger|Clément Chevalier|Julien Bect|Yann Richet,"IDSIA|Idiap, IMSV|UNINE|L2S, GdR MASCOT-NUM|IRSN, GdR MASCOT-NUM",https://arxiv.org/abs/1611.07256v6,https://arxiv.org/pdf/1611.07256v6,https://doi.org/10.1080/00401706.2019.1693427,,"Technometrics, 63(1):13-26, 2021",10.1080/00401706.2019.1693427,stat.ME,stat.ME|math.ST|stat.ML,https://arxiv.org/pdf/1611.07256v6.pdf
1611.04330v1,2016-11-14T10:45:09Z,2016-11-14 10:45:09,Adaptive Experimental Design for Path-following Performance Assessment of Unmanned Vehicles,"The definition of Good Experimental Methodologies (GEMs) in robotics is a topic of widespread interest due also to the increasing employment of robots in everyday civilian life. The present work contributes to the ongoing discussion on GEMs for Unmanned Surface Vehicles (USVs). It focuses on the definition of GEMs and provides specific guidelines for path-following experiments. Statistically designed experiments (DoE) offer a valid basis for developing an empirical model of the system being investigated. A two-step adaptive experimental procedure for evaluating path-following performance and based on DoE, is tested on the simulator of the Charlie USV. The paper argues the necessity of performing extensive simulations prior to the execution of field trials.",Eleonora Saggini|Eva Riccomagno|Massimo Caccia|Henry P. Wynn,,https://arxiv.org/abs/1611.04330v1,https://arxiv.org/pdf/1611.04330v1,,,,,cs.RO,cs.RO|stat.AP,https://arxiv.org/pdf/1611.04330v1.pdf
1611.03780v2,2016-11-11T16:55:20Z,2019-02-15 22:21:51,Randomized Experimental Design via Geographic Clustering,"Web-based services often run randomized experiments to improve their products. A popular way to run these experiments is to use geographical regions as units of experimentation, since this does not require tracking of individual users or browser cookies. Since users may issue queries from multiple geographical locations, geo-regions cannot be considered independent and interference may be present in the experiment. In this paper, we study this problem, and first present GeoCUTS, a novel algorithm that forms geographical clusters to minimize interference while preserving balance in cluster size. We use a random sample of anonymized traffic from Google Search to form a graph representing user movements, then construct a geographically coherent clustering of the graph. Our main technical contribution is a statistical framework to measure the effectiveness of clusterings. Furthermore, we perform empirical evaluations showing that the performance of GeoCUTS is comparable to hand-crafted geo-regions with respect to both novel and existing metrics.",David Rolnick|Kevin Aydin|Jean Pouget-Abadie|Shahab Kamali|Vahab Mirrokni|Amir Najmi,,https://arxiv.org/abs/1611.03780v2,https://arxiv.org/pdf/1611.03780v2,,10 pages,,,cs.SI,cs.SI|cs.DS,https://arxiv.org/pdf/1611.03780v2.pdf
1610.06731v3,2016-10-21T10:24:08Z,2017-12-18 10:58:19,Minimax Error of Interpolation and Optimal Design of Experiments for Variable Fidelity Data,"Engineering problems often involve data sources of variable fidelity with different costs of obtaining an observation. In particular, one can use both a cheap low fidelity function (e.g. a computational experiment with a CFD code) and an expensive high fidelity function (e.g. a wind tunnel experiment) to generate a data sample in order to construct a regression model of a high fidelity function. The key question in this setting is how the sizes of the high and low fidelity data samples should be selected in order to stay within a given computational budget and maximize accuracy of the regression model prior to committing resources on data acquisition.
  In this paper we obtain minimax interpolation errors for single and variable fidelity scenarios for a multivariate Gaussian process regression. Evaluation of the minimax errors allows us to identify cases when the variable fidelity data provides better interpolation accuracy than the exclusively high fidelity data for the same computational budget.
  These results allow us to calculate the optimal shares of variable fidelity data samples under the given computational budget constraint. Real and synthetic data experiments suggest that using the obtained optimal shares often outperforms natural heuristics in terms of the regression accuracy.",Alexey Zaytsev|Evgeny Burnaev,,https://arxiv.org/abs/1610.06731v3,https://arxiv.org/pdf/1610.06731v3,,"25 pages, 5 figures, 2 tables",,,stat.ML,stat.ML|math.ST|stat.AP,https://arxiv.org/pdf/1610.06731v3.pdf
1610.06427v1,2016-10-20T14:22:57Z,2016-10-20 14:22:57,On weighted optimality of experimental designs,"When the experimental objective is expressed by a set of estimable functions, and any eigenvalue-based optimality criterion is selected, we prove the equivalence of the recently introduced weighted optimality and the 'standard' optimality criteria for estimating this set of functions of interest. Also, given a weighted eigenvalue-based criterion, we construct a system of estimable functions, so that the optimality for estimating this system of functions is equivalent to the weighted optimality. This allows one to use the large body of existing theoretical and computational results for the standard optimality criteria for estimating a system of interest to derive theorems and numerical algorithms for the weighted optimality of experimental designs. Moreover, we extend the theory of weighted optimality so that it captures the experimental objective consisting of any system of estimable functions, which was not the case in the literature on weighted optimality so far. For any set of estimable functions, we propose a corresponding weight matrix of a simple form, and with a straightforward interpretation. Given a set of estimable functions with their corresponding weights, we show that it is useful to distinguish between the primary weights selected by the experimenters and the secondary weights implied by the weight matrix.",Samuel Rosa,,https://arxiv.org/abs/1610.06427v1,https://arxiv.org/pdf/1610.06427v1,,24 pages,,,math.ST,math.ST,https://arxiv.org/pdf/1610.06427v1.pdf
1610.03688v1,2016-10-12T12:35:46Z,2016-10-12 12:35:46,On the optimal experimental design for heat and moisture parameter estimation,"In the context of estimating material properties of porous walls based on in-site measurements and identification method, this paper presents the concept of Optimal Experiment Design (OED). It aims at searching the best experimental conditions in terms of quantity and position of sensors and boundary conditions imposed to the material. These optimal conditions ensure to provide the maximum accuracy of the identification method and thus the estimated parameters. The search of the OED is done by using the Fisher information matrix and a priori knowledge of the parameters. The methodology is applied for two case studies. The first one deals with purely conductive heat transfer. The concept of optimal experiment design is detailed and verified with 100 inverse problems for different experiment designs. The second case study combines a strong coupling between heat and moisture transfer through a porous building material. The methodology presented is based on a scientific formalism for efficient planning of experimental work that can be extended to the optimal design of experiments related to other problems in thermal and fluid sciences.",Julien Berger|Denys Dutykh|Nathan Mendes,PUCPR|LAMA|PUCPR,https://arxiv.org/abs/1610.03688v1,https://arxiv.org/pdf/1610.03688v1,https://doi.org/10.1016/j.expthermflusci.2016.10.008,"32 pages, 14 figures, 3 tables, 34 references. Other author's papers can be downloaded at http://www.denys-dutykh.com/","Experimental Thermal and Fluid Science (2017), Vol. 81, pp. 109-122",10.1016/j.expthermflusci.2016.10.008,physics.comp-ph,physics.comp-ph|cs.CE|math.NA|physics.class-ph,https://arxiv.org/pdf/1610.03688v1.pdf
1608.01118v3,2016-08-03T09:01:07Z,2018-08-30 13:13:49,A supermartingale approach to Gaussian process based sequential design of experiments,"Gaussian process (GP) models have become a well-established frameworkfor the adaptive design of costly experiments, and notably of computerexperiments.  GP-based sequential designs have been found practicallyefficient for various objectives, such as global optimization(estimating the global maximum or maximizer(s) of a function),reliability analysis (estimating a probability of failure) or theestimation of level sets and excursion sets.  In this paper, we studythe consistency of an important class of sequential designs, known asstepwise uncertainty reduction (SUR) strategies.  Our approach relieson the key observation that the sequence of residual uncertaintymeasures, in SUR strategies, is generally a supermartingale withrespect to the filtration generated by the observations.  Thisobservation enables us to establish generic consistency results for abroad class of SUR strategies.  The consistency of several popularsequential design strategies is then obtained by means of this generalresult.  Notably, we establish the consistency of two SUR strategiesproposed by Bect, Ginsbourger, Li, Picheny and Vazquez (Stat. Comp.,2012)---to the best of our knowledge, these are the first proofs ofconsistency for GP-based sequential design algorithms dedicated to theestimation of excursion sets and their measure.  We also establish anew, more general proof of consistency for the expected improvementalgorithm for global optimization which, unlike previous results inthe literature, applies to any GP with continuous sample paths.",Julien Bect|François Bachoc|David Ginsbourger,"L2S, GdR MASCOT-NUM|IMT|IMSV",https://arxiv.org/abs/1608.01118v3,https://arxiv.org/pdf/1608.01118v3,,,,,stat.ML,stat.ML|math.PR|math.ST,https://arxiv.org/pdf/1608.01118v3.pdf
1606.05892v3,2016-06-19T17:34:21Z,2016-10-26 16:19:47,Bayesian design of experiments for generalised linear models and dimensional analysis with industrial and scientific application,"The design of an experiment can be always be considered at least implicitly Bayesian, with prior knowledge used informally to aid decisions such as the variables to be studied and the choice of a plausible relationship between the explanatory variables and measured responses. Bayesian methods allow uncertainty in these decisions to be incorporated into design selection through prior distributions that encapsulate information available from scientific knowledge or previous experimentation. Further, a design may be explicitly tailored to the aim of the experiment through a decision-theoretic approach using an appropriate loss function. We review the area of decision-theoretic Bayesian design, with particular emphasis on recent advances in computational methods. For many problems arising in industry and science, experiments result in a discrete response that is well described by a member of the class of generalised linear models. We describe how Gaussian process emulation, commonly used in computer experiments, can play an important role in facilitating Bayesian design for realistic problems. A main focus is the combination of Gaussian process regression to approximate the expected loss with cyclic descent (coordinate exchange) optimisation algorithms to allow optimal designs to be found for previously infeasible problems. We also present the first optimal design results for statistical models formed from dimensional analysis, a methodology widely employed in the engineering and physical sciences to produce parsimonious and interpretable models. Using the famous paper helicopter experiment, we show the potential for the combination of Bayesian design, generalised linear models and dimensional analysis to produce small but informative experiments.",David C. Woods|Antony M. Overstall|Maria Adamou|Timothy W. Waite,,https://arxiv.org/abs/1606.05892v3,https://arxiv.org/pdf/1606.05892v3,https://doi.org/10.1080/08982112.2016.12460452,,"Quality Engineering, 29, 91-103, 2017",10.1080/08982112.2016.12460452,stat.ME,stat.ME,https://arxiv.org/pdf/1606.05892v3.pdf
1606.05279v1,2016-06-16T17:22:11Z,2016-06-16 17:22:11,Causal Inference in Rebuilding and Extending the Recondite Bridge between Finite Population Sampling and Experimental Design,"This article considers causal inference for treatment contrasts from a randomized experiment using potential outcomes in a finite population setting. Adopting a Neymanian repeated sampling approach that integrates such causal inference with finite population survey sampling, an inferential framework is developed for general mechanisms of assigning experimental units to multiple treatments. This framework extends classical methods by allowing the possibility of randomization restrictions and unequal replications. Novel conditions that are ""milder"" than strict additivity of treatment effects, yet permit unbiased estimation of the finite population sampling variance of any treatment contrast estimator, are derived. The consequences of departures from such conditions are also studied under the criterion of minimax bias, and a new justification for using the Neymanian conservative sampling variance estimator in experiments is provided. The proposed approach can readily be extended to the case of treatments with a general factorial structure.",Rahul Mukerjee|Tirthankar Dasgupta|Donald B. Rubin,,https://arxiv.org/abs/1606.05279v1,https://arxiv.org/pdf/1606.05279v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1606.05279v1.pdf
1606.02162v1,2016-06-07T14:56:59Z,2016-06-07 14:56:59,A Distributed Force-Directed Algorithm on Giraph: Design and Experiments,"In this paper we study the problem of designing a distributed graph visualization algorithm for large graphs. The algorithm must be simple to implement and the computing infrastructure must not require major hardware or software investments. We design, implement, and experiment a force-directed algorithm in Giraph, a popular open source framework for distributed computing, based on a vertex-centric design paradigm. The algorithm is tested both on real and artificial graphs with up to million edges, by using a rather inexpensive PaaS (Platform as a Service) infrastructure of Amazon. The experiments show the scalability and effectiveness of our technique when compared to a centralized implementation of the same force-directed model. We show that graphs with about one million edges can be drawn in less than 8 minutes, by spending about 1\$ per drawing in the cloud computing infrastructure.",Alessio Arleo|Walter Didimo|Giuseppe Liotta|Fabrizio Montecchiani,,https://arxiv.org/abs/1606.02162v1,https://arxiv.org/pdf/1606.02162v1,,,,,cs.DS,cs.DS,https://arxiv.org/pdf/1606.02162v1.pdf
1605.08473v1,2016-05-26T23:20:37Z,2016-05-26 23:20:37,Robust designs for experiments with blocks,"For experiments running in field plots or over time, the observations are often correlated due to spatial or serial correlation, which leads to correlated errors in a linear model analyzing the treatment means. Without knowing the exact correlation matrix of the errors, it is not possible to compute the generalized least squares estimator for the treatment means and use it to construct optimal designs for the experiments. In this paper we propose to use neighbourhoods to model the covariance matrix of the errors, and apply a modified generalized least squares estimator to construct robust designs for experiments with blocks. A minimax design criterion is investigated, and a simulated annealing algorithm is developed to find robust designs. We have derived several theoretical results, and representative examples are presented.",Rena K. Mann|Roderick Edwards|Julie Zhou,,https://arxiv.org/abs/1605.08473v1,https://arxiv.org/pdf/1605.08473v1,,23 pages,,,stat.ME,stat.ME,https://arxiv.org/pdf/1605.08473v1.pdf
1605.05524v2,2016-05-18T11:28:52Z,2016-05-20 11:12:23,Sequential design of experiments for estimating percentiles of black-box functions,"Estimating percentiles of black-box deterministic functions with random inputs is a challenging task when the number of function evaluations is severely restricted, which is typical for computer experiments. This article proposes two new sequential Bayesian methods for percentile estimation based on the Gaussian Process metamodel. Both rely on the Stepwise Uncertainty Reduction paradigm, hence aim at providing a sequence of function evaluations that reduces an uncertainty measure associated with the percentile estimator. The proposed strategies are tested on several numerical examples, showing that accurate estimators can be obtained using only a small number of functions evaluations.",T Labopin-Richard|V Picheny,"IMT|UBIA, INRA Toulouse",https://arxiv.org/abs/1605.05524v2,https://arxiv.org/pdf/1605.05524v2,,,,,math.ST,math.ST,https://arxiv.org/pdf/1605.05524v2.pdf
1605.05071v1,2016-05-17T09:25:13Z,2016-05-17 09:25:13,Maximizing the information gain of a single ion microscope using bayes experimental design,"We show nanoscopic transmission microscopy, using a deterministic single particle source and compare the resulting images in terms of signal-to-noise ratio, with those of conventional Poissonian sources. Our source is realized by deterministic extraction of laser-cooled calcium ions from a Paul trap. Gating by the extraction event allows for the suppression of detector dark counts by six orders of magnitude. Using the Bayes experimental design method, the deterministic characteristics of this source are harnessed to maximize information gain, when imaging structures with a parametrizable transmission function. We demonstrate such optimized imaging by determining parameter values of one and two dimensional transmissive structures.",Georg Jacob|Karin Groot-Berning|Ulrich G. Poschinger|Ferdinand Schmidt-Kaler|Kilian Singer,,https://arxiv.org/abs/1605.05071v1,https://arxiv.org/pdf/1605.05071v1,https://doi.org/10.1117/12.2227745,"8 pages, 7 figures, From SPIE Conference Volume 9900, Quantum Optics, Jürgen Stuhler; Andrew J. Shields; Brussels, Belgium, April 03, 2016","Proc. SPIE 9900, Quantum Optics, 99001A (April 29, 2016)",10.1117/12.2227745,quant-ph,quant-ph,https://arxiv.org/pdf/1605.05071v1.pdf
1604.08320v1,2016-04-28T06:32:27Z,2016-04-28 06:32:27,Sequential Bayesian optimal experimental design via approximate dynamic programming,"The design of multiple experiments is commonly undertaken via suboptimal strategies, such as batch (open-loop) design that omits feedback or greedy (myopic) design that does not account for future effects. This paper introduces new strategies for the optimal design of sequential experiments. First, we rigorously formulate the general sequential optimal experimental design (sOED) problem as a dynamic program. Batch and greedy designs are shown to result from special cases of this formulation. We then focus on sOED for parameter inference, adopting a Bayesian formulation with an information theoretic design objective. To make the problem tractable, we develop new numerical approaches for nonlinear design with continuous parameter, design, and observation spaces. We approximate the optimal policy by using backward induction with regression to construct and refine value function approximations in the dynamic program. The proposed algorithm iteratively generates trajectories via exploration and exploitation to improve approximation accuracy in frequently visited regions of the state space. Numerical results are verified against analytical solutions in a linear-Gaussian setting. Advantages over batch and greedy design are then demonstrated on a nonlinear source inversion problem where we seek an optimal policy for sequential sensing.",Xun Huan|Youssef M. Marzouk,,https://arxiv.org/abs/1604.08320v1,https://arxiv.org/pdf/1604.08320v1,,"Preprint 34 pages, 12 figures (36 small figures). v1 submitted to the SIAM/ASA Journal on Uncertainty Quantification on April 27, 2016",,,stat.ME,stat.ME|math.OC|stat.CO|stat.ML,https://arxiv.org/pdf/1604.08320v1.pdf
1604.03480v1,2016-04-12T17:03:31Z,2016-04-12 17:03:31,Optimal Experimental Designs for Estimating Henry's Law Constants via the Method of Phase Ratio Variation,"When measuring Henry's Law constants ($k_H$) using the phase ratio method via headspace gas chromatography (GC), the value of $k_H$ of the compound under investigation is calculated from the ratio of the slope to the intercept of a linear regression of the the inverse GC response versus the ratio of gas to liquid volumes of a series of vials drawn from the same parent solution. Thus, an experimenter will collect measurements consisting of the independent variable (the gas/liquid volume ratio) and dependent variable (the inverse GC peak area). There is a choice of values of the independent variable during measurement. A review of the literature found that the common approach is a simple uniformly spacing of the liquid volumes. We present an optimal experimental design which estimates $k_H$ with minimum error and provides multiple means for building confidence intervals for such estimates. We illustrate efficiency improvements of our new design with an example measuring the $k_H$ for napthalene in aqueous solution as well as simulations on previous studies. The designs can be easily computed using our open source software optDesignSlopeInt, an R package on CRAN. We also discuss applicability of this method to other fields.",Adam Kapelner|Abba Krieger|William J. Blanford,,https://arxiv.org/abs/1604.03480v1,https://arxiv.org/pdf/1604.03480v1,,"21 pages, 2 figures, 3 tables",,,stat.AP,stat.AP,https://arxiv.org/pdf/1604.03480v1.pdf
1602.06975v1,2016-02-22T21:40:07Z,2016-02-22 21:40:07,Design and experiment of electronic speckle shearing phase-shifting pattern interferometer,"An electronic speckle shearing phase-shifting pattern interferometer (ESSPPI) based on Michelson interferometer was based in this paper. A rotatable mirror driven by a step motor in one of its reflective arm is used to generate an adjustable shearing and the mirror driven by piezoelectric transducer (PZT) in the other reflective arm was used to realize phaseshifting. In the experiments, the deformation of an aluminum plate with the same extern-force on different positions and different forces on the same position is measured. Meanwhile, the phase distribution and phase-unwrap image of the aluminum plate with the extern-force on its center position is obtained by the four-step phase-shifting method.",Tianhua Xu|Chao Jing|Wencai Jing|Hongxia Zhang|Dagong Jia|Yimo Zhang,,https://arxiv.org/abs/1602.06975v1,https://arxiv.org/pdf/1602.06975v1,https://doi.org/10.1007/s11801-008-7079-5,3 pages,"Optoelectronics Letters, Vol. 4, No. 1, 59-61, 2008",10.1007/s11801-008-7079-5,physics.ins-det,physics.ins-det|physics.optics,https://arxiv.org/pdf/1602.06975v1.pdf
1602.05135v3,2016-02-16T18:43:47Z,2016-06-14 15:02:13,The limitations of model-based experimental design and parameter estimation in sloppy systems,"We explore the relationship among model fidelity, experimental design, and parameter estimation in sloppy models. We show that the approximate nature of mathematical models poses challenges for experimental design in sloppy models. In many models of complex biological processes it is unknown what are the relevant physics that must be included to explain collective behaviors. As a consequence, models are often overly complex, with many practically unidentifiable parameters. Furthermore, which details are relevant/irrelevant vary among potential experiments. By selecting complementary experiments, experimental design may inadvertently make details that were ommitted from the model become relevant. When this occurs, the model will fail to give a good fit to the data. We use a simple hyper-model of model error to quantify a model's inadequacy and apply it to two models of complex biological processes (EGFR signaling and DNA repair) with optimally selected experiments. We find that although parameters may be accurately estimated, the error in the model renders it less predictive than it was in the sloppy regime where model error is small. We introduce the concept of a \emph{sloppy system}--a sequence of models of increasing complexity that become sloppy in the limit of microscopic accuracy. We explore the limits of accurate parameter estimation in sloppy systems and argue that system identification better approached by considering a hierarchy of models of varying detail rather than focusing parameter estimation in a single model.",Andrew White|Malachi Tolman|Howard D. Thames|Hubert Rodney Withers|Kathy A. Mason|Mark K. Transtrum,,https://arxiv.org/abs/1602.05135v3,https://arxiv.org/pdf/1602.05135v3,https://doi.org/10.1371/journal.pcbi.1005227,"27 pages, 8 figures",,10.1371/journal.pcbi.1005227,q-bio.QM,q-bio.QM,https://arxiv.org/pdf/1602.05135v3.pdf
1601.06702v1,2016-01-25T18:12:42Z,2016-01-25 18:12:42,Experimental Design : Optimizing Quantities of Interest to Reliably Reduce the Uncertainty in Model Input Parameters,"As stakeholders and policy makers increasingly rely upon quantitative predictions from advanced computational models, a problem of fundamental importance is the quantification and reduction of uncertainties in both model inputs and output data. The typical work-flow in the end-to-end quantification of uncertainties requires first formulating and solving stochastic inverse problems (SIPs) using output data on available quantities of interest (QoI). The solution to a SIP is often written in terms of a probability measure, or density, on the space of model inputs. Then, we can formulate and solve a stochastic forward problem (SFP) where the uncertainty on model inputs is propagated through the model to make quantitative predictions on either unobservable or future QoI data.
  In this work, we use a measure-theoretic framework to formulate and solve both SIPs and SFPs. From this perspective, we quantify the geometric characteristics of using hypothetical sets of QoI that describe both the precision and accuracy in solutions to both the SIP and SFP. This leads to a natural definition of the optimal experimental design, i.e., what the optimal configuration a finite set of sensors is in space-time. Several numerical examples and applications are discussed including the ADvanced CIRCulation (ADCIRC) model using simulated data from Hurricane Gustav to determine an optimal placement of buoys in the Gulf of Mexico to capture high water marks of the storm surge in order to reduce uncertainties in bottom roughness characteristics.",Scott Walsh,,https://arxiv.org/abs/1601.06702v1,https://arxiv.org/pdf/1601.06702v1,,,,,math.NA,math.NA,https://arxiv.org/pdf/1601.06702v1.pdf
1601.00992v3,2016-01-05T21:41:35Z,2018-03-30 09:47:35,"Models, Methods and Network Topology: Experimental Design for the Study of Interference","How should a network experiment be designed to achieve high statistical power? Ex- perimental treatments on networks may spread. Randomizing assignment of treatment to nodes enhances learning about the counterfactual causal effects of a social network experiment and also requires new methodology (ex. Aronow and Samii 2017a; Bow- ers et al. 2013; Toulis and Kao 2013). In this paper we show that the way in which a treatment propagates across a social network affects the statistical power of an ex- perimental design. As such, prior information regarding treatment propagation should be incorporated into the experimental design. Our findings justify reconsideration of standard practice in circumstances where units are presumed to be independent even in simple experiments: information about treatment effects is not maximized when we assign half the units to treatment and half to control. We also present an exam- ple in which statistical power depends on the extent to which the network degree of nodes is correlated with treatment assignment probability. We recommend that re- searchers think carefully about the underlying treatment propagation model motivat- ing their study in designing an experiment on a network.",Jake Bowers|Bruce A. Desmarais|Mark Frederickson|Nahomi Ichino|Hsuan-Wei Lee|Simi Wang,,https://arxiv.org/abs/1601.00992v3,https://arxiv.org/pdf/1601.00992v3,https://doi.org/10.1016/j.socnet.2018.01.010,,"Jake Bowers, Bruce A. Desmarais, Mark Frederickson, Nahomi Ichino, Hsuan-Wei Lee, Simi Wang, Models, methods and network topology: Experimental design for the study of interference, Social Networks, Volume 54, 2018, Pages 196-208",10.1016/j.socnet.2018.01.010,stat.ME,stat.ME,https://arxiv.org/pdf/1601.00992v3.pdf
1601.00477v1,2016-01-04T12:23:43Z,2016-01-04 12:23:43,Optimal block designs for experiments with responses drawn from a Poisson distribution,"Optimal block designs for additive models achieve their efficiency by dividing experimental units among relatively homogenous blocks and allocating treatments equally to blocks. Responses in many modern experiments, however, are drawn from distributions such as the one- and two-parameter exponential families, e.g., RNA sequence counts from a negative binomial distribution. These violate additivity. Yet, designs generated by assuming additivity continue to be used, because better approaches are not available, and because the issues are not widely recognised. We solve this problem for single-factor experiments in which treatments, taking categorical values only, are arranged in blocks and responses drawn from a Poisson distribution. We derive expressions for two objective functions, based on D_A- and C-optimality, with efficient estimation of linear contrasts of the fixed effects parameters in a Poisson generalised linear mixed model (GLMM) being the objective. These objective functions are shown to be computational efficient, requiring no matrix inversion. Using simulated annealing to generate Poisson GLMM-based locally optimal designs, we show that the replication numbers of treatments in these designs are inversely proportional to the relative magnitudes of the treatments' expected counts. Importantly, for non-negligible treatment effect sizes, Poisson GLMM-based optimal designs may be substantially more efficient than their classically optimal counterparts.",Stephen Bush|Katya Ruggiero,,https://arxiv.org/abs/1601.00477v1,https://arxiv.org/pdf/1601.00477v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1601.00477v1.pdf
1511.07827v2,2015-11-24T18:28:54Z,2016-03-22 16:54:36,Stopping criteria for boosting automatic experimental design using real-time fMRI with Bayesian optimization,"Bayesian optimization has been proposed as a practical and efficient tool through which to tune parameters in many difficult settings. Recently, such techniques have been combined with real-time fMRI to propose a novel framework which turns on its head the conventional functional neuroimaging approach. This closed-loop method automatically designs the optimal experiment to evoke a desired target brain pattern. One of the challenges associated with extending such methods to real-time brain imaging is the need for adequate stopping criteria, an aspect of Bayesian optimization which has received limited attention. In light of high scanning costs and limited attentional capacities of subjects an accurate and reliable stopping criteria is essential. In order to address this issue we propose and empirically study the performance of two stopping criteria.",Romy Lorenz|Ricardo P Monti|Ines R Violante|Aldo A Faisal|Christoforos Anagnostopoulos|Robert Leech|Giovanni Montana,,https://arxiv.org/abs/1511.07827v2,https://arxiv.org/pdf/1511.07827v2,,Oral presentation at MLINI 2015 - 5th NIPS Workshop on Machine Learning and Interpretation in Neuroimaging: Beyond the Scanner,,,q-bio.NC,q-bio.NC|stat.ML,https://arxiv.org/pdf/1511.07827v2.pdf
1511.03395v5,2015-11-11T06:10:08Z,2017-06-06 21:15:03,Prediction uncertainty and optimal experimental design for learning dynamical systems,"Dynamical systems are frequently used to model biological systems. When these models are fit to data it is necessary to ascertain the uncertainty in the model fit. Here we present prediction deviation, a new metric of uncertainty that determines the extent to which observed data have constrained the model's predictions. This is accomplished by solving an optimization problem that searches for a pair of models that each provide a good fit for the observed data, yet have maximally different predictions. We develop a method for estimating a priori the impact that additional experiments would have on the prediction deviation, allowing the experimenter to design a set of experiments that would most reduce uncertainty. We use prediction deviation to assess uncertainty in a model of interferon-alpha inhibition of viral infection, and to select a sequence of experiments that reduces this uncertainty. Finally we prove a theoretical result which shows that prediction deviation provides bounds on the trajectories of the underlying true model. These results show that prediction deviation is a meaningful metric of uncertainty that can be used for optimal experimental design.",Benjamin Letham|Portia A. Letham|Cynthia Rudin|Edward P. Browne,,https://arxiv.org/abs/1511.03395v5,https://arxiv.org/pdf/1511.03395v5,https://doi.org/10.1063/1.4953795,,"Chaos 26, 063110 (2016)",10.1063/1.4953795,stat.AP,stat.AP,https://arxiv.org/pdf/1511.03395v5.pdf
1511.03046v1,2015-11-10T10:13:45Z,2015-11-10 10:13:45,Improvement of code behaviour in a design of experiments by metamodeling,"It is now common practice in nuclear engineering to base extensive studies on numerical computer models. These studies require to run computer codes in potentially thousands of numerical configurations and without expert individual controls on the computational and physical aspects of each simulations.In this paper, we compare different statistical metamodeling techniques and show how metamodels can help to improve the global behaviour of codes in these extensive studies. We consider the metamodeling of the Germinal thermalmechanical code by Kriging, kernel regression and neural networks. Kriging provides the most accurate predictions while neural networks yield the fastest metamodel functions. All three metamodels can conveniently detect strong computation failures. It is however significantly more challenging to detect code instabilities, that is groups of computations that are all valid, but numerically inconsistent with one another. For code instability detection, we find that Kriging provides the most useful tools.",François Bachoc|Jean-Marc Martinez|Karim Ammar,"IMT, GdR MASCOT-NUM|MoVe|LPEC",https://arxiv.org/abs/1511.03046v1,https://arxiv.org/pdf/1511.03046v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1511.03046v1.pdf
1510.08661v1,2015-10-29T12:09:54Z,2015-10-29 12:09:54,Optimal experimental designs for fMRI via circulant biased weighing designs,"Functional magnetic resonance imaging (fMRI) technology is popularly used in many fields for studying how the brain reacts to mental stimuli. The identification of optimal fMRI experimental designs is crucial for rendering precise statistical inference on brain functions, but research on this topic is very lacking. We develop a general theory to guide the selection of fMRI designs for estimating a hemodynamic response function (HRF) that models the effect over time of the mental stimulus, and for studying the comparison of two HRFs. We provide a useful connection between fMRI designs and circulant biased weighing designs, establish the statistical optimality of some well-known fMRI designs and identify several new classes of fMRI designs. Construction methods of high-quality fMRI designs are also given.",Ching-Shui Cheng|Ming-Hung Kao,,https://arxiv.org/abs/1510.08661v1,https://arxiv.org/pdf/1510.08661v1,https://doi.org/10.1214/15-AOS1352,Published at http://dx.doi.org/10.1214/15-AOS1352 in the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Statistics 2015, Vol. 43, No. 6, 2565-2587",10.1214/15-AOS1352,math.ST,math.ST,https://arxiv.org/pdf/1510.08661v1.pdf
1510.05248v1,2015-10-18T14:20:33Z,2015-10-18 14:20:33,Design of Experiments for Screening,"The aim of this paper is to review methods of designing screening experiments, ranging from designs originally developed for physical experiments to those especially tailored to experiments on numerical models. The strengths and weaknesses of the various designs for screening variables in numerical models are discussed. First, classes of factorial designs for experiments to estimate main effects and interactions through a linear statistical model are described, specifically regular and nonregular fractional factorial designs, supersaturated designs and systematic fractional replicate designs. Generic issues of aliasing, bias and cancellation of factorial effects are discussed. Second, group screening experiments are considered including factorial group screening and sequential bifurcation. Third, random sampling plans are discussed including Latin hypercube sampling and sampling plans to estimate elementary effects. Fourth, a variety of modelling methods commonly employed with screening designs are briefly described. Finally, a novel study demonstrates six screening methods on two frequently-used exemplars, and their performances are compared.",David C. Woods|Susan M. Lewis,,https://arxiv.org/abs/1510.05248v1,https://arxiv.org/pdf/1510.05248v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1510.05248v1.pdf
1510.03502v1,2015-10-13T01:20:29Z,2015-10-13 01:20:29,Estimates of the coverage of parameter space by Latin Hypercube and Orthogonal sampling: connections between Populations of Models and Experimental Designs,In this paper we use counting arguments to prove that the expected percentage coverage of a $d$ dimensional parameter space of size $n$ when performing $k$ trials with either Latin Hypercube sampling or Orthogonal sampling (when $n=p^d$) is the same. We then extend these results to an experimental design setting by projecting onto a 2 dimensional subspace. In this case the coverage is equivalent to the Orthogonal sampling setting when the dimension of the parameter space is two. These results are confirmed by simulations. The ideas presented here have particular relevance when attempting to perform uncertainty quantification or when building populations of models.,Diane Donovan|Kevin Burrage|Pamela Burrage|Thomas A McCourt|Harold Bevan Thompson|Emine Sule Yazici,,https://arxiv.org/abs/1510.03502v1,https://arxiv.org/pdf/1510.03502v1,,"15 pages, 2 figures. arXiv admin note: text overlap with arXiv:1502.06559",,,math.ST,math.ST,https://arxiv.org/pdf/1510.03502v1.pdf
1509.04613v1,2015-09-11T04:50:09Z,2015-09-11 04:50:09,Gaussian process surrogates for failure detection: a Bayesian experimental design approach,"An important task of uncertainty quantification is to identify {the probability of} undesired events, in particular, system failures, caused by various sources of uncertainties. In this work we consider the construction of Gaussian {process} surrogates for failure detection and failure probability estimation. In particular, we consider the situation that the underlying computer models are extremely expensive, and in this setting, determining the sampling points in the state space is of essential importance. We formulate the problem as an optimal experimental design for Bayesian inferences of the limit state (i.e., the failure boundary) and propose an efficient numerical scheme to solve the resulting optimization problem. In particular, the proposed limit-state inference method is capable of determining multiple sampling points at a time, and thus it is well suited for problems where multiple computer simulations can be performed in parallel. The accuracy and performance of the proposed method is demonstrated by both academic and practical examples.",Hongqiao Wang|Guang Lin|Jinglai Li,,https://arxiv.org/abs/1509.04613v1,https://arxiv.org/pdf/1509.04613v1,https://doi.org/10.1016/j.jcp.2016.02.053,,,10.1016/j.jcp.2016.02.053,stat.CO,stat.CO|math.NA|math.PR,https://arxiv.org/pdf/1509.04613v1.pdf
1509.02179v3,2015-09-07T20:22:38Z,2016-10-26 18:06:40,Kriging Metamodels and Experimental Design for Bermudan Option Pricing,"We investigate two new strategies for the numerical solution of optimal stopping problems within the Regression Monte Carlo (RMC) framework of Longstaff and Schwartz. First, we propose the use of stochastic kriging (Gaussian process) meta-models for fitting the continuation value. Kriging offers a flexible, nonparametric regression approach that quantifies approximation quality. Second, we connect the choice of stochastic grids used in RMC to the Design of Experiments paradigm. We examine space-filling and adaptive experimental designs; we also investigate the use of batching with replicated simulations at design sites to improve the signal-to-noise ratio. Numerical case studies for valuing Bermudan Puts and Max-Calls under a variety of asset dynamics illustrate that our methods offer significant reduction in simulation budgets over existing approaches.",Michael Ludkovski,,https://arxiv.org/abs/1509.02179v3,https://arxiv.org/pdf/1509.02179v3,,"27 pages, 6 figures",,,q-fin.CP,q-fin.CP|stat.ME,https://arxiv.org/pdf/1509.02179v3.pdf
1509.00980v2,2015-09-03T08:27:20Z,2016-07-12 22:17:24,Sequential Design for Ranking Response Surfaces,"We propose and analyze sequential design methods for the problem of ranking several response surfaces. Namely, given $L \ge 2$ response surfaces over a continuous input space $\cal X$, the aim is to efficiently find the index of the minimal response across the entire $\cal X$. The response surfaces are not known and have to be noisily sampled one-at-a-time. This setting is motivated by stochastic control applications and requires joint experimental design both in space and response-index dimensions. To generate sequential design heuristics we investigate stepwise uncertainty reduction approaches, as well as sampling based on posterior classification complexity. We also make connections between our continuous-input formulation and the discrete framework of pure regret in multi-armed bandits. To model the response surfaces we utilize kriging surrogates. Several numerical examples using both synthetic data and an epidemics control problem are provided to illustrate our approach and the efficacy of respective adaptive designs.",Ruimeng Hu|Mike Ludkovski,,https://arxiv.org/abs/1509.00980v2,https://arxiv.org/pdf/1509.00980v2,https://doi.org/10.1137/15M1045168,"26 pages, 7 figures (updated several sections and figures)",,10.1137/15M1045168,stat.ML,stat.ML|q-fin.CP|stat.CO,https://arxiv.org/pdf/1509.00980v2.pdf
1507.04267v1,2015-07-15T15:46:45Z,2015-07-15 15:46:45,Plackett-Burman experimental design for pulsed-DC-plasma deposition of DLC coatings,"The influence of technological parameters of pulsed-DC chemical vapour deposition on the deposition rate, the mechanical properties and the residual stress of diamond-like carbon (DLC) coatings deposited onto a martensitic steel substrate, using a Ti buffer layer between coating and substrate, has been studied. For this purpose, a Plackett-Burman experiment design and Pareto charts were used to identify the most significant process parameters, such as deposition time, methane flux, chamber pressure, power, pulse frequency, substrate roughness and thickness of titanium thin film. The substrate surfaces, which were previously cleaned by argon plasma, and the DLC coatings were characterized by scanning electron microscopy (SEM) and atomic force microscopy (AFM). The mechanical properties (elastic modulus and hardness) and the residual stress of DLC coatings were determined by the nanoindentation technique and calotte grinding method, respectively. Finally, the causes of the relative effect of different process variables were discussed.",Luis F. Pantoja-Suárez|Miguel Morales|José-Luis andújar|Joan Esteve|Merce Segarra|Enric Bertran,,https://arxiv.org/abs/1507.04267v1,https://arxiv.org/pdf/1507.04267v1,,"12 pages, 8 figures and 4 tables",,,cond-mat.mtrl-sci,cond-mat.mtrl-sci,https://arxiv.org/pdf/1507.04267v1.pdf
1507.03063v1,2015-07-11T03:41:20Z,2015-07-11 03:41:20,Incentive-Compatible Experimental Design,"We consider the design of experiments to evaluate treatments that are administered by self-interested agents, each seeking to achieve the highest evaluation and win the experiment. For example, in an advertising experiment, a company wishes to evaluate two marketing agents in terms of their efficacy in viral marketing, and assign a contract to the winner agent. Contrary to traditional experimental design, this problem has two new implications. First, the experiment induces a game among agents, where each agent can select from multiple versions of the treatment it administers. Second, the action of one agent -- selection of treatment version -- may affect the actions of another agent, with the resulting strategic interference complicating the evaluation of agents. An incentive-compatible experiment design is one with an equilibrium where each agent selects its natural action, which is the action that would maximize the performance of the agent if there was no competition (e.g., expected number of conversions if agent was assigned the contract). Under a general formulation of experimental design, we identify sufficient conditions that guarantee incentive-compatible experiments. These conditions rely on the existence of statistics that can estimate how agents would perform without competition, and their use in constructing score functions to evaluate the agents. In the setting with no strategic interference, we also study the power of the design, i.e., the probability that the best agent wins, and show how to improve the power of incentive-compatible designs. From the technical side, our theory uses a range of statistical methods such as hypothesis testing, variance-stabilizing transformations and the Delta method, all of which rely on asymptotics.",Panos Toulis|David C. Parkes|Elery Pfeffer|James Zou,,https://arxiv.org/abs/1507.03063v1,https://arxiv.org/pdf/1507.03063v1,https://doi.org/10.1145/2764468.2764525,,,10.1145/2764468.2764525,stat.ME,stat.ME,https://arxiv.org/pdf/1507.03063v1.pdf
1507.00803v4,2015-07-03T01:44:51Z,2017-05-18 14:30:09,Model-assisted design of experiments in the presence of network correlated outcomes,"We consider the problem of how to assign treatment in a randomized experiment, in which the correlation among the outcomes is informed by a network available pre-intervention. Working within the potential outcome causal framework, we develop a class of models that posit such a correlation structure among the outcomes. Then we leverage these models to develop restricted randomization strategies for allocating treatment optimally, by minimizing the mean square error of the estimated average treatment effect. Analytical decompositions of the mean square error, due both to the model and to the randomization distribution, provide insights into aspects of the optimal designs. In particular, the analysis suggests new notions of balance based on specific network quantities, in addition to classical covariate balance. The resulting balanced, optimal restricted randomization strategies are still design unbiased, in situations where the model used to derive them does not hold. We illustrate how the proposed treatment allocation strategies improve on allocations that ignore the network structure, with extensive simulations.",Guillaume W. Basse|Edoardo M. Airoldi,,https://arxiv.org/abs/1507.00803v4,https://arxiv.org/pdf/1507.00803v4,,"56 pages, 6 figures",,,stat.ME,stat.ME|cs.SI|physics.soc-ph|stat.ML,https://arxiv.org/pdf/1507.00803v4.pdf
1506.04182v1,2015-06-12T21:08:35Z,2015-06-12 21:08:35,Model Exploration Using OpenMOLE - a workflow engine for large scale distributed design of experiments and parameter tuning,"OpenMOLE is a scientific workflow engine with a strong emphasis on workload distribution.
Workflows are designed using a high level Domain Specific Language (DSL) built on top of Scala. It exposes natural parallelism constructs to easily delegate the workload resulting from a workflow to a wide range of distributed computing environments. In this work, we briefly expose the strong assets of OpenMOLE and demonstrate its efficiency at exploring the parameter set of an agent simulation model. We perform a multi-objective optimisation on this model using computationally expensive Genetic Algorithms (GA). OpenMOLE hides the complexity of designing such an experiment thanks to its DSL, and transparently distributes the optimisation process. The example shows how an initialisation of the GA with a population of 200,000 individuals can be evaluated in one hour on the European Grid Infrastructure.",Romain Reuillon|Mathieu Leclaire|Jonathan Passerat-Palmbach,"ISC-PIF|ISC-PIF, GC|BioMedIA",https://arxiv.org/abs/1506.04182v1,https://arxiv.org/pdf/1506.04182v1,,"IEEE High Performance Computing and Simulation conference 2015, Jun 2015, Amsterdam, Netherlands",,,cs.DC,cs.DC,https://arxiv.org/pdf/1506.04182v1.pdf
1506.02088v1,2015-06-05T23:53:10Z,2015-06-05 23:53:10,The Automatic Neuroscientist: automated experimental design with real-time fMRI,"A standard approach in functional neuroimaging explores how a particular cognitive task activates a set of brain regions (one task-to-many regions mapping). Importantly though, the same neural system can be activated by inherently different tasks. To date, there is no approach available that systematically explores whether and how distinct tasks probe the same neural system (many tasks-to-region mapping). In our work, presented here we propose an alternative framework, the Automatic Neuroscientist, which turns the typical fMRI approach on its head. We use real-time fMRI in combination with state-of-the-art optimisation techniques to automatically design the optimal experiment to evoke a desired target brain state. Here, we present two proof-of-principle studies involving visual and auditory stimuli. The data demonstrate this closed-loop approach to be very powerful, hugely speeding up fMRI and providing an accurate estimation of the underlying relationship between stimuli and neural responses across an extensive experimental parameter space. Finally, we detail four scenarios where our approach can be applied, suggesting how it provides a novel description of how cognition and the brain interrelate.",Romy Lorenz|Ricardo Pio Monti|Ines R. Violante|Christoforos Anagnostopoulos|Aldo A. Faisal|Giovanni Montana|Robert Leech,,https://arxiv.org/abs/1506.02088v1,https://arxiv.org/pdf/1506.02088v1,https://doi.org/10.1016/j.neuroimage.2016.01.032,"22 pages, 7 figures, work presented at OHBM 2015",NeuroImage 129 (2016) 320-334,10.1016/j.neuroimage.2016.01.032,q-bio.NC,q-bio.NC,https://arxiv.org/pdf/1506.02088v1.pdf
1505.07734v5,2015-05-28T15:47:01Z,2016-05-27 19:39:06,MPI Benchmarking Revisited: Experimental Design and Reproducibility,"The Message Passing Interface (MPI) is the prevalent programming model used on today's supercomputers. Therefore, MPI library developers are looking for the best possible performance (shortest run-time) of individual MPI functions across many different supercomputer architectures. Several MPI benchmark suites have been developed to assess the performance of MPI implementations. Unfortunately, the outcome of these benchmarks is often neither reproducible nor statistically sound. To overcome these issues, we show which experimental factors have an impact on the run-time of blocking collective MPI operations and how to control them. We address the problem of process and clock synchronization in MPI benchmarks. Finally, we present a new experimental method that allows us to obtain reproducible and statistically sound MPI measurements.",Sascha Hunold|Alexandra Carpen-Amarie,,https://arxiv.org/abs/1505.07734v5,https://arxiv.org/pdf/1505.07734v5,,"38 pages, 44 figures",,,cs.DC,cs.DC,https://arxiv.org/pdf/1505.07734v5.pdf
1504.06226v1,2015-04-23T15:39:29Z,2015-04-23 15:39:29,Optimal design of experiments via linear programming,"We investigate the possibility of extending some results of Pazman and Pronzato (2014) to a larger set of optimality criteria. Namely, in a linear regression model the problem of computing D-, A-, E_k-optimal designs, of combining these optimality criteria, and the ""criterion robust"" problem of Harman (2004) are reformulated here as ""infinite-dimensional"" linear programming problems. Approximate optimum designs can then be computed by a modified cutting-plane method, and this is checked on examples. Finally, the expressions for these criteria are reformulated in terms of the response function of an even nonlinear model.",Katarina Burclova|Andrej Pazman,,https://arxiv.org/abs/1504.06226v1,https://arxiv.org/pdf/1504.06226v1,,14 pages,,,stat.CO,stat.CO,https://arxiv.org/pdf/1504.06226v1.pdf
1504.05427v2,2015-04-21T13:28:17Z,2015-05-29 14:39:29,Signal Recovery on Graphs: Random versus Experimentally Designed Sampling,"We study signal recovery on graphs based on two sampling strategies: random sampling and experimentally designed sampling. We propose a new class of smooth graph signals, called approximately bandlimited, which generalizes the bandlimited class and is similar to the globally smooth class. We then propose two recovery strategies based on random sampling and experimentally designed sampling. The proposed recovery strategy based on experimentally designed sampling is similar to the leverage scores used in the matrix approximation. We show that while both strategies are unbiased estimators for the low-frequency components, the convergence rate of experimentally designed sampling is much faster than that of random sampling when a graph is irregular. We validate the proposed recovery strategies on three specific graphs: a ring graph, an Erdős-Rényi graph, and a star graph. The simulation results support the theoretical analysis.",Siheng Chen|Rohan Varma|Aarti Singh|Jelena Kovačević,,https://arxiv.org/abs/1504.05427v2,https://arxiv.org/pdf/1504.05427v2,,Correct some typos,,,cs.IT,cs.IT|stat.ML,https://arxiv.org/pdf/1504.05427v2.pdf
1503.00021v4,2015-02-27T21:50:03Z,2016-04-29 03:05:27,Mercer kernels and integrated variance experimental design: connections between Gaussian process regression and polynomial approximation,"This paper examines experimental design procedures used to develop surrogates of computational models, exploring the interplay between experimental designs and approximation algorithms. We focus on two widely used approximation approaches, Gaussian process (GP) regression and non-intrusive polynomial approximation. First, we introduce algorithms for minimizing a posterior integrated variance (IVAR) design criterion for GP regression. Our formulation treats design as a continuous optimization problem that can be solved with gradient-based methods on complex input domains, without resorting to greedy approximations. We show that minimizing IVAR in this way yields point sets with good interpolation properties, and that it enables more accurate GP regression than designs based on entropy minimization or mutual information maximization. Second, using a Mercer kernel/eigenfunction perspective on GP regression, we identify conditions under which GP regression coincides with pseudospectral polynomial approximation. Departures from these conditions can be understood as changes either to the kernel or to the experimental design itself. We then show how IVAR-optimal designs, while sacrificing discrete orthogonality of the kernel eigenfunctions, can yield lower approximation error than orthogonalizing point sets. Finally, we compare the performance of adaptive Gaussian process regression and adaptive pseudospectral approximation for several classes of target functions, identifying features that are favorable to the GP + IVAR approach.",Alex A. Gorodetsky|Youssef M. Marzouk,,https://arxiv.org/abs/1503.00021v4,https://arxiv.org/pdf/1503.00021v4,,,,,math.NA,math.NA|stat.CO,https://arxiv.org/pdf/1503.00021v4.pdf
1502.07873v1,2015-02-27T12:21:53Z,2015-02-27 12:21:53,Fast Bayesian Optimal Experimental Design for Seismic Source Inversion,"We develop a fast method for optimally designing experiments in the context of statistical seismic source inversion. In particular, we efficiently compute the optimal number and locations of the receivers or seismographs. The seismic source is modeled by a point moment tensor multiplied by a time-dependent function. The parameters include the source location, moment tensor components, and start time and frequency in the time function. The forward problem is modeled by elastodynamic wave equations. We show that the Hessian of the cost functional, which is usually defined as the square of the weighted L2 norm of the difference between the experimental data and the simulated data, is proportional to the measurement time and the number of receivers. Consequently, the posterior distribution of the parameters, in a Bayesian setting, concentrates around the ""true"" parameters, and we can employ Laplace approximation and speed up the estimation of the expected Kullback-Leibler divergence (expected information gain), the optimality criterion in the experimental design procedure. Since the source parameters span several magnitudes, we use a scaling matrix for efficient control of the condition number of the original Hessian matrix. We use a second-order accurate finite difference method to compute the Hessian matrix and either sparse quadrature or Monte Carlo sampling to carry out numerical integration. We demonstrate the efficiency, accuracy, and applicability of our method on a two-dimensional seismic source inversion problem.",Quan Long|Mohammad Motamed|Raul Tempone,,https://arxiv.org/abs/1502.07873v1,https://arxiv.org/pdf/1502.07873v1,https://doi.org/10.1016/j.cma.2015.03.021,,,10.1016/j.cma.2015.03.021,stat.CO,stat.CO|math.NA,https://arxiv.org/pdf/1502.07873v1.pdf
1502.06559v1,2015-02-19T10:29:35Z,2015-02-19 10:29:35,"Populations of models, Experimental Designs and coverage of parameter space by Latin Hypercube and Orthogonal Sampling","In this paper we have used simulations to make a conjecture about the coverage of a $t$ dimensional subspace of a $d$ dimensional parameter space of size $n$ when performing $k$ trials of Latin Hypercube sampling. This takes the form $P(k,n,d,t)=1-e^{-k/n^{t-1}}$. We suggest that this coverage formula is independent of $d$ and this allows us to make connections between building Populations of Models and Experimental Designs. We also show that Orthogonal sampling is superior to Latin Hypercube sampling in terms of allowing a more uniform coverage of the $t$ dimensional subspace at the sub-block size level.",Kevin Burrage|Pamela Burrage|Diane Donovan|Bevan Thompson,,https://arxiv.org/abs/1502.06559v1,https://arxiv.org/pdf/1502.06559v1,,"9 pages, 5 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/1502.06559v1.pdf
1501.00264v4,2015-01-01T09:33:58Z,2016-06-30 11:28:24,Bayesian Design of Experiments using Approximate Coordinate Exchange,"The construction of decision-theoretic Bayesian designs for realistically-complex nonlinear models is computationally challenging, as it requires the optimization of analytically intractable expected utility functions over high-dimensional design spaces. We provide the most general solution to date for this problem through a novel approximate coordinate exchange algorithm. This methodology uses a Gaussian process emulator to approximate the expected utility as a function of a single design coordinate in a series of conditional optimization steps. It has flexibility to address problems for any choice of utility function and for a wide range of statistical models with different numbers of variables, numbers of runs and randomization restrictions. In contrast to existing approaches to Bayesian design, the method can find multi-variable designs in large numbers of runs without resorting to asymptotic approximations to the posterior distribution or expected utility. The methodology is demonstrated on a variety of challenging examples of practical importance, including design for pharmacokinetic models and design for mixed models with discrete data. For many of these models, Bayesian designs are not currently available. Comparisons are made to results from the literature, and to designs obtained from asymptotic approximations.",Antony Overstall|David Woods,,https://arxiv.org/abs/1501.00264v4,https://arxiv.org/pdf/1501.00264v4,,"19 pages, 4 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/1501.00264v4.pdf
1412.0613v2,2014-12-01T19:50:44Z,2015-03-31 17:52:34,Novel experimental design for high pressure - high temperature electrical resistance measurements in a 'Paris-Edinburgh' large volume press,"We present a novel experimental design for high sensitivity measurements of the electrical resistance of samples at high pressures (0-6GPa) and high temperatures (300-1000K) in a 'Paris-Edinburgh' type large volume press. Uniquely, the electrical measurements are carried out directly on a small sample, thus greatly increasing the sensitivity of the measurement. The sensitivity to even minor changes in electrical resistance can be used to clearly identify phase transitions in material samples. Electrical resistance measurements are relatively simple and rapid to execute and the efficacy of the present experimental design is demonstrated by measuring the electrical resistance of Pb, Sn and Bi across a wide domain of temperature-pressure phase space and employing it to identify the loci of phase transitions. Based on these results, the phase diagrams of these elements are reconstructed to high accuracy and found to be in excellent agreement with previous studies. In particular, by mapping the locations of several well-studied reference points in the phase diagram of Sn and Bi, it is demonstrated that a standard calibration exists for the temperature and pressure, thus eliminating the need for direct or indirect temperature and pressure measurements. The present technique will allow simple and accurate mapping of phase diagrams under extreme conditions and may be of particular importance in advancing studies of liquid state anomalies.",Shlomi Matityahu|Moran Emuna|Eyal Yahel|Guy Makov|Yaron Greenberg,,https://arxiv.org/abs/1412.0613v2,https://arxiv.org/pdf/1412.0613v2,https://doi.org/10.1063/1.4918606,"8 pages, 12 figures","Rev. Sci. Instrum. 86, 043902 (2015)",10.1063/1.4918606,cond-mat.mtrl-sci,cond-mat.mtrl-sci,https://arxiv.org/pdf/1412.0613v2.pdf
1410.5899v2,2014-10-22T02:31:07Z,2015-11-03 05:01:12,A Fast and Scalable Method for A-Optimal Design of Experiments for Infinite-dimensional Bayesian Nonlinear Inverse Problems,"We address the problem of optimal experimental design (OED) for Bayesian nonlinear inverse problems governed by PDEs. The goal is to find a placement of sensors, at which experimental data are collected, so as to minimize the uncertainty in the inferred parameter field. We formulate the OED objective function by generalizing the classical A-optimal experimental design criterion using the expected value of the trace of the posterior covariance. We seek a method that solves the OED problem at a cost (measured in the number of forward PDE solves) that is independent of both the parameter and sensor dimensions. To facilitate this, we construct a Gaussian approximation to the posterior at the maximum a posteriori probability (MAP) point, and use the resulting covariance operator to define the OED objective function. We use randomized trace estimation to compute the trace of this (implicitly defined) covariance operator. The resulting OED problem includes as constraints the PDEs characterizing the MAP point, and the PDEs describing the action of the covariance operator to vectors. The sparsity of the sensor configurations is controlled using sparsifying penalty functions. We elaborate our OED method for the problem of determining the sensor placement to best infer the coefficient of an elliptic PDE. Adjoint methods are used to compute the gradient of the PDE-constrained OED objective function. We provide numerical results for inference of the permeability field in a porous medium flow problem, and demonstrate that the number of PDE solves required for the evaluation of the OED objective function and its gradient is essentially independent of both the parameter and sensor dimensions. The number of quasi-Newton iterations for computing an OED also exhibits the same dimension invariance properties.",Alen Alexanderian|Noemi Petra|Georg Stadler|Omar Ghattas,,https://arxiv.org/abs/1410.5899v2,https://arxiv.org/pdf/1410.5899v2,,30 pages; minor revisions; accepted for publication in SIAM Journal on Scientific Computing,,,math.OC,math.OC|stat.CO|stat.ME,https://arxiv.org/pdf/1410.5899v2.pdf
1409.0269v3,2014-09-01T00:03:51Z,2015-02-08 17:08:32,Parameter-free methods distinguish Wnt pathway models and guide design of experiments,"The canonical Wnt signaling pathway, mediated by $β$-catenin, is crucially involved in development, adult stem cell tissue maintenance and a host of diseases including cancer. We undertake analysis of different mathematical models of Wnt from the literature, and compare them to a new mechanistic model of Wnt signaling that targets spatial localization of key molecules. Using Bayesian methods we infer parameters for each of the models to mammalian Wnt signaling data and find that all models can fit this time course. We are able to overcome this lack of data by appealing to algebraic methods (concepts from chemical reaction network theory and matroid theory) to analyze the models without recourse to specific parameter values. These approaches provide insight into Wnt signaling: The new model (unlike any other investigated) permits a bistable switch in the system via control of shuttling and degradation parameters, corresponding to stem-like vs committed cell states in the differentiation hierarchy. Our analysis also identifies groups of variables that must be measured to fully characterize and discriminate between competing models, and thus serves as a guide for performing minimal experiments for model comparison.",Adam L. MacLean|Zvi Rosen|Helen M. Byrne|Heather A. Harrington,,https://arxiv.org/abs/1409.0269v3,https://arxiv.org/pdf/1409.0269v3,https://doi.org/10.1073/pnas.1416655112,"37 pages, 6 figures; errors fixed and comparison with data",,10.1073/pnas.1416655112,q-bio.QM,q-bio.QM|math.AG|q-bio.MN,https://arxiv.org/pdf/1409.0269v3.pdf
1408.6323v2,2014-08-27T06:23:43Z,2014-09-03 05:18:53,On Bayesian A- and D-optimal experimental designs in infinite dimensions,"We consider Bayesian linear inverse problems in infinite-dimensional separable Hilbert spaces, with a Gaussian prior measure and additive Gaussian noise model, and provide an extension of the concept of Bayesian D-optimality to the infinite-dimensional case. To this end, we derive the infinite-dimensional version of the expression for the Kullback-Leibler divergence from the posterior measure to the prior measure, which is subsequently used to derive the expression for the expected information gain. We also study the notion of Bayesian A-optimality in the infinite-dimensional setting, and extend the well known (in the finite-dimensional case) equivalence of the Bayes risk of the MAP estimator with the trace of the posterior covariance, for the Gaussian linear case, to the infinite-dimensional Hilbert space case.",Alen Alexanderian|Philip Gloor|Omar Ghattas,,https://arxiv.org/abs/1408.6323v2,https://arxiv.org/pdf/1408.6323v2,,"16 pages, minor changes, corrected typos",,,math.ST,math.ST,https://arxiv.org/pdf/1408.6323v2.pdf
1408.2698v1,2014-08-12T12:02:31Z,2014-08-12 12:02:31,Approximate D-optimal Experimental Design with Simultaneous Size and Cost Constraints,"Consider an experiment with a finite set of design points representing permissible trial conditions. Suppose that each trial is associated with a cost that depends on the selected design point. In this paper, we study the problem of constructing an approximate D-optimal experimental design with simultaneous restrictions on the size and on the total cost. For the problem of size-and-cost constrained D-optimality, we formulate an equivalence theorem and rules for the removal of redundant design points. We also propose a simple monotonically convergent ""barycentric"" algorithm that allows us to numerically compute a size-and-cost constrained approximate D-optimal design.",Radoslav Harman|Eva Benková,,https://arxiv.org/abs/1408.2698v1,https://arxiv.org/pdf/1408.2698v1,,,,,stat.CO,stat.CO,https://arxiv.org/pdf/1408.2698v1.pdf
1408.0251v2,2014-07-31T10:29:00Z,2015-03-18 13:12:42,Modeling Cassava Yield: A Response Surface Approach,This paper reports on application of theory of experimental design using graphical techniques in R programming language and application of nonlinear bootstrap regression method to demonstrate the invariant property of parameter estimates of the Inverse polynomial Model (IPM) in a nonlinear surface.,Adeshina Oyedele Bello,,https://arxiv.org/abs/1408.0251v2,https://arxiv.org/pdf/1408.0251v2,,,"International Journal on Computational Sciences & Applications (IJCSA) Vol.4, No.3, June 2014",,stat.CO,stat.CO|stat.AP,https://arxiv.org/pdf/1408.0251v2.pdf
1406.2816v1,2014-06-11T08:19:40Z,2014-06-11 08:19:40,Computation of the Response Surface in the Tensor Train data format,"We apply the Tensor Train (TT) approximation to construct the Polynomial Chaos Expansion (PCE) of a random field, and solve the stochastic elliptic diffusion PDE with the stochastic Galerkin discretization. We compare two strategies of the polynomial chaos expansion: sparse and full polynomial (multi-index) sets. In the full set, the polynomial orders are chosen independently in each variable, which provides higher flexibility and accuracy. However, the total amount of degrees of freedom grows exponentially with the number of stochastic coordinates. To cope with this curse of dimensionality, the data is kept compressed in the TT decomposition, a recurrent low-rank factorization. PCE computations on sparse grids sets are extensively studied, but the TT representation for PCE is a novel approach that is investigated in this paper. We outline how to deduce the PCE from the covariance matrix, assemble the Galerkin operator, and evaluate some post-processing (mean, variance, Sobol indices), staying within the low-rank framework. The most demanding are two stages. First, we interpolate PCE coefficients in the TT format using a few number of samples, which is performed via the block cross approximation method. Second, we solve the discretized equation (large linear system) via the alternating minimal energy algorithm. In the numerical experiments we demonstrate that the full expansion set encapsulated in the TT format is indeed preferable in cases when high accuracy and high polynomial orders are required.",Sergey Dolgov|Boris N. Khoromskij|Alexander Litvinenko|Hermann G. Matthies,,https://arxiv.org/abs/1406.2816v1,https://arxiv.org/pdf/1406.2816v1,,28 pages. Submitted to SIAM J. of Uncertainty Quantification,,,math.NA,math.NA|math.PR,https://arxiv.org/pdf/1406.2816v1.pdf
1405.7430v1,2014-05-29T00:37:28Z,2014-05-29 00:37:28,"BayesOpt: A Bayesian Optimization Library for Nonlinear Optimization, Experimental Design and Bandits","BayesOpt is a library with state-of-the-art Bayesian optimization methods to solve nonlinear optimization, stochastic bandits or sequential experimental design problems. Bayesian optimization is sample efficient by building a posterior distribution to capture the evidence and prior knowledge for the target function. Built in standard C++, the library is extremely efficient while being portable and flexible. It includes a common interface for C, C++, Python, Matlab and Octave.",Ruben Martinez-Cantin,,https://arxiv.org/abs/1405.7430v1,https://arxiv.org/pdf/1405.7430v1,,,,,cs.LG,cs.LG,https://arxiv.org/pdf/1405.7430v1.pdf
1405.2818v1,2014-05-12T16:08:35Z,2014-05-12 16:08:35,Objective Bayesian Model Discrimination in Follow-up Experimental Designs,"An initial screening experiment may lead to ambiguous conclusions regarding the factors which are active in explaining the variation of an outcome variable: thus adding follow-up runs becomes necessary. We propose a fully Bayes objective approach to follow-up designs, using prior distributions suitably tailored to model selection. We adopt a model criterion based on a weighted average of Kullback-Leibler divergences between predictive distributions for all possible pairs of models. When applied to real data, our method produces results which compare favorably to previous analyses based on subjective weakly informative priors.",Guido Consonni|Laura Deldossi,,https://arxiv.org/abs/1405.2818v1,https://arxiv.org/pdf/1405.2818v1,,20 pages; 2 figures; plus Supplementary Materials,,,stat.ME,stat.ME,https://arxiv.org/pdf/1405.2818v1.pdf
1403.3805v2,2014-03-15T13:49:56Z,2014-08-29 10:06:37,$E$-optimal designs for second-order response surface models,"$E$-optimal experimental designs for a second-order response surface model with $k\geq1$ predictors are investigated. If the design space is the $k$-dimensional unit cube, Galil and Kiefer [J. Statist. Plann. Inference 1 (1977a) 121-132] determined optimal designs in a restricted class of designs (defined by the multiplicity of the minimal eigenvalue) and stated their universal optimality as a conjecture. In this paper, we prove this claim and show that these designs are in fact $E$-optimal in the class of all approximate designs. Moreover, if the design space is the unit ball, $E$-optimal designs have not been found so far and we also provide a complete solution to this optimal design problem. The main difficulty in the construction of $E$-optimal designs for the second-order response surface model consists in the fact that for the multiplicity of the minimum eigenvalue of the ""optimal information matrix"" is larger than one (in contrast to the case $k=1$) and as a consequence the corresponding optimality criterion is not differentiable at the optimal solution. These difficulties are solved by considering nonlinear Chebyshev approximation problems, which arise from a corresponding equivalence theorem. The extremal polynomials which solve these Chebyshev problems are constructed explicitly leading to a complete solution of the corresponding $E$-optimal design problems.",Holger Dette|Yuri Grigoriev,,https://arxiv.org/abs/1403.3805v2,https://arxiv.org/pdf/1403.3805v2,https://doi.org/10.1214/14-AOS1241,Published in at http://dx.doi.org/10.1214/14-AOS1241 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Statistics 2014, Vol. 42, No. 4, 1635-1656",10.1214/14-AOS1241,stat.ME,stat.ME,https://arxiv.org/pdf/1403.3805v2.pdf
1402.7263v2,2014-02-28T14:39:52Z,2014-08-07 09:06:54,Heuristic construction of exact experimental designs under multiple resource constraints,"The aim of this paper is twofold. First, we introduce ""resource constraints"" as a general concept that covers many practical restrictions on experimental design. Second, for computing efficient exact designs of experiments under any combination of resource constraints, we propose a tabu search heuristic that uses some ideas of the Detmax procedure. To illustrate the scope and performance of our heuristic, we computed D-efficient designs for 1) a block model with limits on the numbers of blocks and on the availability of experimental material; 2) a quadratic regression model with simultaneous marginal and cost constraints; 3) a non-linear regression model with simultaneous direct and cost constraints. As we show, the proposed heuristic generates comparable or better results than algorithms specialized for computing optimal designs under less general constraints.",Radoslav Harman|Alena Bachratá|Lenka Filová,,https://arxiv.org/abs/1402.7263v2,https://arxiv.org/pdf/1402.7263v2,,"Compared to the first version, the second version of the manuscript contains a new running example to illustrate basic concepts and a new example of a block design with resource constraints. Some parts of the manuscript have been reformulated with the aim to add more details and clarity",,,stat.CO,stat.CO,https://arxiv.org/pdf/1402.7263v2.pdf
1402.6350v4,2014-02-25T21:17:34Z,2014-12-04 00:07:44,Fast prediction of deterministic functions using sparse grid experimental designs,"Random field models have been widely employed to develop a predictor of an expensive function based on observations from an experiment. The traditional framework for developing a predictor with random field models can fail due to the computational burden it requires. This problem is often seen in cases where the input of the expensive function is high dimensional. While many previous works have focused on developing an approximative predictor to resolve these issues, this article investigates a different solution mechanism. We demonstrate that when a general set of designs is employed, the resulting predictor is quick to compute and has reasonable accuracy. The fast computation of the predictor is made possible through an algorithm proposed by this work. This paper also demonstrates methods to quickly evaluate the likelihood of the observations and describes some fast maximum likelihood estimates for unknown parameters of the random field. The computational savings can be several orders of magnitude when the input is located in a high dimensional space. Beyond the fast computation of the predictor, existing research has demonstrated that a subset of these designs generate predictors that are asymptotically efficient. This work details some empirical comparisons to the more common space-filling designs that verify the designs are competitive in terms of resulting prediction accuracy.",Matthew Plumlee,,https://arxiv.org/abs/1402.6350v4,https://arxiv.org/pdf/1402.6350v4,,This document is in-press at the Journal of the American Statistical Association. A MATLAB package released along with this document is available at http://www.mathworks.com/matlabcentral/fileexchange/45668-sparse-grid-designs,,,stat.ME,stat.ME,https://arxiv.org/pdf/1402.6350v4.pdf
1401.7508v1,2014-01-29T13:27:11Z,2014-01-29 13:27:11,Two Models of Nonadaptive Group Testing for Designing Screening Experiments,"We discuss two non-standard models of nonadaptive combinatorial search which develop the conventional disjunct search model for a small number of defective elements contained in a finite ground set or a population. The first model is called a search of defective supersets. The second model is called a search of defective subsets in the presence of inhibitors. For these models, we study the constructive search methods based on the known constructions for the disjunct model.",A. G. D'yachkov|A. J. Macula|D. C. Torney|P. A. Vilenkin,,https://arxiv.org/abs/1401.7508v1,https://arxiv.org/pdf/1401.7508v1,,10 pages,"Proceedings of the 6th International Workshop on Model Oriented Design and Analysis, pp. 63-75, Physica-Verlag Heidelberg, Puchberg/Schneeberg, Austria, 2001",,cs.IT,cs.IT,https://arxiv.org/pdf/1401.7508v1.pdf
1401.7505v1,2014-01-29T13:16:40Z,2014-01-29 13:16:40,Lectures on Designing Screening Experiments,Designing Screening Experiments (DSE) is a class of information - theoretical models for multiple - access channels (MAC). We discuss the combinatorial model of DSE called a disjunct channel model. This model is the most important for applications and closely connected with the superimposed code concept. We give a detailed survey of lower and upper bounds on the rate of superimposed codes. The best known constructions of superimposed codes are considered in paper. We also discuss the development of these codes (non-adaptive pooling designs) intended for the clone - library screening problem. We obtain lower and upper bounds on the rate of binary codes for the combinatorial model of DSE called an adder channel model. We also consider the concept of universal decoding for the probabilistic DSE model called a symmetric model of DSE.,Arkadii G. D'yachkov,,https://arxiv.org/abs/1401.7505v1,https://arxiv.org/pdf/1401.7505v1,,66 pages,"Lecture Note Series 10, Feb. 2004, Combinatorial and Computational Mathematics Center, Pohang University of Science and Technology (POSTECH), Korea Republic, (monograph, pp. 112)",,cs.IT,cs.IT,https://arxiv.org/pdf/1401.7505v1.pdf
1401.6251v1,2014-01-24T03:48:00Z,2014-01-24 03:48:00,Comparing Simulated Emission from Molecular Clouds Using Experimental Design,"We propose a new approach to comparing simulated observations that enables us to determine the significance of the underlying physical effects. We utilize the methodology of experimental design, a subfield of statistical analysis, to establish a framework for comparing simulated position-position-velocity data cubes to each other. We propose three similarity metrics based on methods described in the literature: principal component analysis, the spectral correlation function, and the Cramer multi-variate two sample similarity statistic. Using these metrics, we intercompare a suite of mock observational data of molecular clouds generated from magnetohydrodynamic simulations with varying physical conditions. Using this framework, we show that all three metrics are sensitive to changing Mach number and temperature in the simulation sets, but cannot detect changes in magnetic field strength and initial velocity spectrum. We highlight the shortcomings of one-factor-at-a-time designs commonly used in astrophysics and propose fractional factorial designs as a means to rigorously examine the effects of changing physical properties while minimizing the investment of computational resources.",Miayan Yeremi|Mallory Flynn|Stella Offner|Jason Loeppky|Erik Rosolowsky,UBC Okanagan|UBC Okanagan|Yale|UBC Okanagan|UBC Okanagan,https://arxiv.org/abs/1401.6251v1,https://arxiv.org/pdf/1401.6251v1,https://doi.org/10.1088/0004-637X/783/2/93,Accepted by ApJ,,10.1088/0004-637X/783/2/93,astro-ph.GA,astro-ph.GA,https://arxiv.org/pdf/1401.6251v1.pdf
1401.5617v1,2014-01-22T10:47:46Z,2014-01-22 10:47:46,Exploring Hoover and Perez's experimental designs using global sensitivity analysis,This paper investigates variable-selection procedures in regression that make use of global sensitivity analysis. The approach is combined with existing algorithms and it is applied to the time series regression designs proposed by Hoover and Perez. A comparison of an algorithm employing global sensitivity analysis and the (optimized) algorithm of Hoover and Perez shows that the former significantly improves the recovery rates of original specifications.,William Becker|Paolo Paruolo|Andrea Saltelli,,https://arxiv.org/abs/1401.5617v1,https://arxiv.org/pdf/1401.5617v1,,"27 pages, 2 figures",,,stat.CO,stat.CO,https://arxiv.org/pdf/1401.5617v1.pdf
1401.5039v1,2014-01-20T20:05:52Z,2014-01-20 20:05:52,Experimental Design for Human-in-the-Loop Driving Simulations,"This report describes a new experimental setup for human-in-the-loop simulations. A force feedback simulator with four axis motion has been setup for real-time driving experiments. The simulator will move to simulate the forces a driver feels while driving, which allows for a realistic experience for the driver. This setup allows for flexibility and control for the researcher in a realistic simulation environment. Experiments concerning driver distraction can also be carried out safely in this test bed, in addition to multi-agent experiments. All necessary code to run the simulator, the additional sensors, and the basic processing is available for use.",Katherine Driggs-Campbell|Guillaume Bellegarda|Victor Shia|S. Shankar Sastry|Ruzena Bajcsy,,https://arxiv.org/abs/1401.5039v1,https://arxiv.org/pdf/1401.5039v1,,,,,eess.SY,eess.SY|cs.HC,https://arxiv.org/pdf/1401.5039v1.pdf
1311.5599v1,2013-11-21T22:16:00Z,2013-11-21 22:16:00,Compressive Measurement Designs for Estimating Structured Signals in Structured Clutter: A Bayesian Experimental Design Approach,"This work considers an estimation task in compressive sensing, where the goal is to estimate an unknown signal from compressive measurements that are corrupted by additive pre-measurement noise (interference, or clutter) as well as post-measurement noise, in the specific setting where some (perhaps limited) prior knowledge on the signal, interference, and noise is available. The specific aim here is to devise a strategy for incorporating this prior information into the design of an appropriate compressive measurement strategy. Here, the prior information is interpreted as statistics of a prior distribution on the relevant quantities, and an approach based on Bayesian Experimental Design is proposed. Experimental results on synthetic data demonstrate that the proposed approach outperforms traditional random compressive measurement designs, which are agnostic to the prior information, as well as several other knowledge-enhanced sensing matrix designs based on more heuristic notions.",Swayambhoo Jain|Akshay Soni|Jarvis Haupt,,https://arxiv.org/abs/1311.5599v1,https://arxiv.org/pdf/1311.5599v1,,"5 pages, 4 figures. Accepted for publication at The Asilomar Conference on Signals, Systems, and Computers 2013",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1311.5599v1.pdf
1311.3261v1,2013-11-13T19:27:02Z,2013-11-13 19:27:02,Experimental Design for Dynamics Identification of Cellular Processes,"We address the problem of using nonlinear models to design experiments to characterize the dynamics of cellular processes by using the approach of the Maximally Informative Next Experiment (MINE), which was introduced in [W. Dong, et al. Systems biology of the clock in neurospora crassa. {\em PLoS ONE}, page e3105, 2008] and independently in [M. M. Donahue, et al. Experiment design through dynamical characterization of non-linear systems biology models utilising sparse grids. {\em IET System Biology}, 4:249--262, 2010]. In this approach, existing data is used to define a probability distribution on the parameters; the next measurement point is the one that yields the largest model output variance with this distribution. Building upon this approach, we introduce the Expected Dynamics Estimator (EDE), which is the expected value using this distribution of the output as a function of time. We prove the consistency of this estimator (uniform convergence to true dynamics) even when the chosen experiments cluster in a finite set of points. We extend this proof of consistency to various practical assumptions on noisy data and moderate levels of model mismatch. Through the derivation and proof, we develop a relaxed version of MINE that is more computationally tractable and robust than the original formulation. The results are illustrated with numerical examples on two nonlinear ordinary differential equation models of biomolecular and cellular processes.",Vu Dinh|Ann E. Rundell|Gregery T. Buzzard,,https://arxiv.org/abs/1311.3261v1,https://arxiv.org/pdf/1311.3261v1,,,,,q-bio.QM,q-bio.QM,https://arxiv.org/pdf/1311.3261v1.pdf
1311.2335v1,2013-11-11T02:38:44Z,2013-11-11 02:38:44,A First-Order Algorithm for the A-Optimal Experimental Design Problem: A Mathematical Programming Approach,"We develop and analyse a first-order algorithm for the A-optimal experimental design problem. The problem is first presented as a special case of a parametric family of optimal design problems for which duality results and optimality conditions are given. Then, two first-order (Frank-Wolfe type) algorithms are presented, accompanied by a detailed time-complexity analysis of the algorithms and computational results on various sized problems.",Selin Damla Ahipasaoglu,,https://arxiv.org/abs/1311.2335v1,https://arxiv.org/pdf/1311.2335v1,,,,,stat.CO,stat.CO|math.OC,https://arxiv.org/pdf/1311.2335v1.pdf
1310.4830v3,2013-10-17T20:00:14Z,2014-12-15 22:24:17,Strong Lens Time Delay Challenge: I. Experimental Design,"The time delays between point-like images in gravitational lens systems can be used to measure cosmological parameters. The number of lenses with measured time delays is growing rapidly; the upcoming \emph{Large Synoptic Survey Telescope} (LSST) will monitor $\sim10^3$ strongly lensed quasars. In an effort to assess the present capabilities of the community to accurately measure the time delays, and to provide input to dedicated monitoring campaigns and future LSST cosmology feasibility studies, we have invited the community to take part in a ""Time Delay Challenge"" (TDC). The challenge is organized as a set of ""ladders,"" each containing a group of simulated datasets to be analyzed blindly by participating teams. Each rung on a ladder consists of a set of realistic mock observed lensed quasar light curves, with the rungs' datasets increasing in complexity and realism. The initial challenge described here has two ladders, TDC0 and TDC1. TDC0 has a small number of datasets, and is designed to be used as a practice set by the participating teams. The (non-mandatory) deadline for completion of TDC0 was the TDC1 launch date, December 1, 2013. The TDC1 deadline was July 1 2014. Here we give an overview of the challenge, we introduce a set of metrics that will be used to quantify the goodness-of-fit, efficiency, precision, and accuracy of the algorithms, and we present the results of TDC0. Thirteen teams participated in TDC0 using 47 different methods. Seven of those teams qualified for TDC1, which is described in the companion paper II.",Gregory Dobler|Christopher Fassnacht|Tommaso Treu|Phillip J. Marshall|Kai Liao|Alireza Hojjati|Eric Linder|Nicholas Rumbaugh,CUSP/NYU|UC Davis|UCSB|KIPAC|UCSB/BNU|UBC/Simon Fraser|LBL|UC Davis,https://arxiv.org/abs/1310.4830v3,https://arxiv.org/pdf/1310.4830v3,https://doi.org/10.1088/0004-637X/799/2/168,"referee's comments incorporated, to appear in ApJ; challenge data and instructions can be accessed at http://timedelaychallenge.org","2015, ApJ, 799, 168",10.1088/0004-637X/799/2/168,astro-ph.IM,astro-ph.IM|astro-ph.CO,https://arxiv.org/pdf/1310.4830v3.pdf
1310.2623v2,2013-10-09T20:13:50Z,2014-12-22 16:51:29,Controlling Networks of Nonlinearly-Coupled Nodes using Response Surfaces,"Control of complex processes is a major goal of network analyses. Most approaches to control nonlinearly coupled systems require the network topology and/or network dynamics. Unfortunately, neither the full set of participating nodes nor the network topology is known for many important systems. On the other hand, system responses to perturbations are often easily measured. We show how the collection of such responses (a response surface) can be used for network control. Analyses of model systems show that response surfaces are smooth and hence can be approximated using low order polynomials. Importantly, these approximations are largely insensitive to stochastic fluctuations in data or measurement errors. They can be used to compute how a small set of nodes need to be altered in order to direct the network close to a pre-specified target state. These ideas, illustrated on a nonlinear electrical circuit, can prove useful in many contexts including in reprogramming cellular states.",Jason Shulman|Frank Malatino|Alexander Mo|Killian Ryan|Gemunu H. Gunaratne,,https://arxiv.org/abs/1310.2623v2,https://arxiv.org/pdf/1310.2623v2,https://doi.org/10.1038/srep07574,"8 pages, 6 figures","Sci. Rep. 4, 7574 (2014)",10.1038/srep07574,q-bio.MN,q-bio.MN|physics.bio-ph,https://arxiv.org/pdf/1310.2623v2.pdf
1309.7687v1,2013-09-30T01:05:15Z,2013-09-30 01:05:15,Chemical communication between synthetic and natural cells: a possible experimental design,"The bottom-up construction of synthetic cells is one of the most intriguing and interesting research arenas in synthetic biology. Synthetic cells are built by encapsulating biomolecules inside lipid vesicles (liposomes), allowing the synthesis of one or more functional proteins. Thanks to the in situ synthesized proteins, synthetic cells become able to perform several biomolecular functions, which can be exploited for a large variety of applications. This paves the way to several advanced uses of synthetic cells in basic science and biotechnology, thanks to their versatility, modularity, biocompatibility, and programmability.  In the previous WIVACE (2012) we presented the state-of-the-art of semi-synthetic minimal cell (SSMC) technology and introduced, for the first time, the idea of chemical communication between synthetic cells and natural cells. The development of a proper synthetic communication protocol should be seen as a tool for the nascent field of bio/chemical-based Information and Communication Technologies (bio-chem-ICTs) and ultimately aimed at building soft-wet-micro-robots.  In this contribution (WIVACE, 2013) we present a blueprint for realizing this project, and show some preliminary experimental results. We firstly discuss how our research goal (based on the natural capabilities of biological systems to manipulate chemical signals) finds a proper place in the current scientific and technological contexts. Then, we shortly comment on the experimental approaches from the viewpoints of (i) synthetic cell construction, and (ii) bioengineering of microorganisms, providing up-to-date results from our laboratory. Finally, we shortly discuss how autopoiesis can be used as a theoretical framework for defining synthetic minimal life, minimal cognition, and as bridge between synthetic biology and artificial intelligence. 
",Giordano Rampioni|Luisa Damiano|Marco Messina|Francesca D'Angelo|Livia Leoni|Pasquale Stano,Science Dept. University Roma Tre|Univ. Bergamo|Science Dept. University Roma Tre|Science Dept. University Roma Tre|Science Dept. University Roma Tre|Science Dept. University Roma Tre,https://arxiv.org/abs/1309.7687v1,https://arxiv.org/pdf/1309.7687v1,https://doi.org/10.4204/EPTCS.130.4,"In Proceedings Wivace 2013, arXiv:1309.7122","EPTCS 130, 2013, pp. 14-26",10.4204/EPTCS.130.4,cs.CE,cs.CE|q-bio.MN,https://arxiv.org/pdf/1309.7687v1.pdf
1308.4084v2,2013-08-19T17:59:23Z,2014-05-27 22:52:32,A-optimal design of experiments for infinite-dimensional Bayesian linear inverse problems with regularized $\ell_0$-sparsification,"We present an efficient method for computing A-optimal experimental designs for infinite-dimensional Bayesian linear inverse problems governed by partial differential equations (PDEs). Specifically, we address the problem of optimizing the location of sensors (at which observational data are collected) to minimize the uncertainty in the parameters estimated by solving the inverse problem, where the uncertainty is expressed by the trace of the posterior covariance. Computing optimal experimental designs (OEDs) is particularly challenging for inverse problems governed by computationally expensive PDE models with infinite-dimensional (or, after discretization, high-dimensional) parameters. To alleviate the computational cost, we exploit the problem structure and build a low-rank approximation of the parameter-to-observable map, preconditioned with the square root of the prior covariance operator. This relieves our method from expensive PDE solves when evaluating the optimal experimental design objective function and its derivatives. Moreover, we employ a randomized trace estimator for efficient evaluation of the OED objective function. We control the sparsity of the sensor configuration by employing a sequence of penalty functions that successively approximate the $\ell_0$-""norm""; this results in binary designs that characterize optimal sensor locations. We present numerical results for inference of the initial condition from spatio-temporal observations in a time-dependent advection-diffusion problem in two and three space dimensions. We find that an optimal design can be computed at a cost, measured in number of forward PDE solves, that is independent of the parameter and sensor dimensions. We demonstrate numerically that $\ell_0$-sparsified experimental designs obtained via a continuation method outperform $\ell_1$-sparsified designs.",Alen Alexanderian|Noemi Petra|Georg Stadler|Omar Ghattas,,https://arxiv.org/abs/1308.4084v2,https://arxiv.org/pdf/1308.4084v2,,"27 pages, accepted for publication in SIAM Journal on Scientific Computing",,,stat.CO,stat.CO|math.NA|math.OC|stat.ME,https://arxiv.org/pdf/1308.4084v2.pdf
1308.1196v2,2013-08-06T07:24:36Z,2018-02-23 05:08:42,The Group Lasso for Design of Experiments,"We introduce an application of the group lasso to design of experiments. Note that we are NOT trying to explain experimental design for the group lasso. Conversely, we explain how we can use the idea of the group lasso in experimental design, showing that the problem of constructing an optimal design matrix can be transformed into a problem of the group lasso. In some numerical examples, we show that we can obtain the orthogonal arrays as the solutions of the group lasso problems.",Kentaro Tanaka|Masami Miyakawa,,https://arxiv.org/abs/1308.1196v2,https://arxiv.org/pdf/1308.1196v2,,"11 pages, 6 tables",,,stat.ML,stat.ML,https://arxiv.org/pdf/1308.1196v2.pdf
1306.4245v1,2013-06-18T15:35:09Z,2013-06-18 15:35:09,Pair versus Solo Programming -- An Experience Report from a Course on Design of Experiments in Software Engineering,"This paper presents an experience report about an experiment that evaluates duration and effort of pair and solo programming. The experiment was performed as part of a course on Design of Experiments (DOE) in Software Engineering (SE) at Autonomous University of Yucatan (UADY). A total of 21 junior student subjects enrolled in the bachelor's degree program in SE participated in the experiment. During the experiment, subjects (7 pairs and 7 solos) wrote two small programs in two sessions. Results show a significant difference (at alpha=0.1) in favor of pair programming regarding duration (28% decrease), and a significant difference (at alpha=0.1) in favor of solo programming with respect to effort (30% decrease). With only a difference of 1%, our results regarding duration and effort are practically the same as those reported by Nosek in 1998.",Omar S. Gómez|José L. Batún|Raúl A. Aguilar,,https://arxiv.org/abs/1306.4245v1,https://arxiv.org/pdf/1306.4245v1,,"9 pages; International Journal of Computer Science Issues, 2013",,,cs.SE,cs.SE,https://arxiv.org/pdf/1306.4245v1.pdf
1306.1460v2,2013-06-06T16:27:09Z,2013-06-11 18:50:56,"Alfven Wave Collisions, The Fundamental Building Block of Plasma Turbulence III: Theory for Experimental Design","Turbulence in space and astrophysical plasmas is governed by the nonlinear interactions between counterpropagating Alfven waves. Here we present the theoretical considerations behind the design of the first laboratory measurement of an Alfven wave collision, the fundamental interaction underlying Alfvenic turbulence. By interacting a relatively large-amplitude, low-frequency Alfven wave with a counterpropagating, smaller-amplitude, higher-frequency Alfven wave, the experiment accomplishes the secular nonlinear transfer of energy to a propagating daughter Alfven wave. The predicted properties of the nonlinearly generated daughter Alfven wave are outlined, providing a suite of tests that can be used to confirm the successful measurement of the nonlinear interaction between counterpropagating Alfven waves in the laboratory.",G. G. Howes|K. D. Nielson|D. J. Drake|J. W. R. Schroeder|F. Skiff|C. A. Kletzing|T. A. Carter,,https://arxiv.org/abs/1306.1460v2,https://arxiv.org/pdf/1306.1460v2,https://doi.org/10.1063/1.4812808,"14 pages, 4 figures, accepted to Physics of Plasmas",,10.1063/1.4812808,astro-ph.SR,astro-ph.SR|physics.plasm-ph,https://arxiv.org/pdf/1306.1460v2.pdf
1305.7385v1,2013-05-31T13:08:52Z,2013-05-31 13:08:52,Can Small Islands Protect Nearby Coasts From Tsunamis? An Active Experimental Design Approach,"Small islands in the vicinity of the mainland are believed to offer protection from wind and waves and thus coastal communities have been developed in these areas. However, what happens when it comes to tsunamis is not clear. Will these islands act as natural barriers ? Recent post-tsunami survey data, supported by numerical simulations, reveal that the run-up on coastal areas behind small islands was significantly higher than on neighboring locations not affected by the presence of the island. To study the conditions of this run- up amplification, we solve numerically the nonlinear shallow water equations (NSWE). We use the simplified geometry of a conical island sitting on a flat bed in front of a uniform sloping beach. By doing so, the experimental setup is defined by five physical parameters, namely the island slope, the beach slope, the water depth, the distance between the island and the plane beach and the incoming wavelength, while the wave height was kept fixed. The objective is twofold: Find the maximum run-up amplification with the least number of simulations. To achieve this goal, we build an emulator based on Gaussian Processes to guide the selection of the query points in the parameter space.",Themistoklis S. Stefanakis|Emile Contal|Nicolas Vayatis|Frédéric Dias|Costas E. Synolakis,,https://arxiv.org/abs/1305.7385v1,https://arxiv.org/pdf/1305.7385v1,https://doi.org/10.1098/rspa.2014.0575,,,10.1098/rspa.2014.0575,physics.flu-dyn,physics.flu-dyn|math.OC|physics.ao-ph,https://arxiv.org/pdf/1305.7385v1.pdf
1304.1455v1,2013-04-04T18:12:40Z,2013-04-04 18:12:40,Designing Experiments to Understand the Variability in Biochemical Reaction Networks,"Exploiting the information provided by the molecular noise of a biological process has proven to be valuable in extracting knowledge about the underlying kinetic parameters and sources of variability from single cell measurements. However, quantifying this additional information a priori, to decide whether a single cell experiment might be beneficial, is currently only possibly in very simple systems where either the chemical master equation is computationally tractable or a Gaussian approximation is appropriate. Here we show how the information provided by distribution measurements can be approximated from the first four moments of the underlying process. The derived formulas are generally valid for any stochastic kinetic model including models that comprise both intrinsic and extrinsic noise. This allows us to propose an optimal experimental design framework for heterogeneous cell populations which we employ to compare the utility of dual reporter and perturbation experiments for separating extrinsic and intrinsic noise in a simple model of gene expression. Subsequently, we compare the information content of different experiments which have been performed in an engineered light-switch gene expression system in yeast and show that well chosen gene induction patterns may allow one to identify features of the system which remain hidden in unplanned experiments.",Jakob Ruess|Andreas Milias-Argeitis|John Lygeros,,https://arxiv.org/abs/1304.1455v1,https://arxiv.org/pdf/1304.1455v1,https://doi.org/10.1098/rsif.2013.0588,,J R Soc Interface 10:20130588 (2013),10.1098/rsif.2013.0588,q-bio.QM,q-bio.QM,https://arxiv.org/pdf/1304.1455v1.pdf
1302.5724v4,2013-02-22T22:12:11Z,2013-07-11 15:51:08,Budget Feasible Mechanisms for Experimental Design,"In the classical experimental design setting, an experimenter E has access to a population of $n$ potential experiment subjects $i\in \{1,...,n\}$, each associated with a vector of features $x_i\in R^d$. Conducting an experiment with subject $i$ reveals an unknown value $y_i\in R$ to E. E typically assumes some hypothetical relationship between $x_i$'s and $y_i$'s, e.g., $y_i \approx βx_i$, and estimates $β$ from experiments, e.g., through linear regression. As a proxy for various practical constraints, E may select only a subset of subjects on which to conduct the experiment.
  We initiate the study of budgeted mechanisms for experimental design. In this setting, E has a budget $B$. Each subject $i$ declares an associated cost $c_i >0$ to be part of the experiment, and must be paid at least her cost. In particular, the Experimental Design Problem (EDP) is to find a set $S$ of subjects for the experiment that maximizes $V(S) = \log\det(I_d+\sum_{i\in S}x_i\T{x_i})$ under the constraint $\sum_{i\in S}c_i\leq B$; our objective function corresponds to the information gain in parameter $β$ that is learned through linear regression methods, and is related to the so-called $D$-optimality criterion. Further, the subjects are strategic and may lie about their costs.
  We present a deterministic, polynomial time, budget feasible mechanism scheme, that is approximately truthful and yields a constant factor approximation to EDP. In particular, for any small $δ> 0$ and $ε> 0$, we can construct a (12.98, $ε$)-approximate mechanism that is $δ$-truthful and runs in polynomial time in both $n$ and $\log\log\frac{B}{εδ}$. We also establish that no truthful, budget-feasible algorithms is possible within a factor 2 approximation, and show how to generalize our approach to a wide class of learning problems, beyond linear regression.",Thibaut Horel|Stratis Ioannidis|S. Muthukrishnan,,https://arxiv.org/abs/1302.5724v4,https://arxiv.org/pdf/1302.5724v4,https://doi.org/10.1007/978-3-642-54423-1_62,,"LATIN 2014: Theoretical Informatics. Lecture Notes in Computer Science Volume 8392, 2014, pp 719-730",10.1007/978-3-642-54423-1_62,cs.GT,cs.GT,https://arxiv.org/pdf/1302.5724v4.pdf
1302.2203v1,2013-02-09T06:18:04Z,2013-02-09 06:18:04,A Factor Framework for Experimental Design for Performance Evaluation of Commercial Cloud Services,"Given the diversity of commercial Cloud services, performance evaluations of candidate services would be crucial and beneficial for both service customers (e.g. cost-benefit analysis) and providers (e.g. direction of service improvement). Before an evaluation implementation, the selection of suitable factors (also called parameters or variables) plays a prerequisite role in designing evaluation experiments. However, there seems a lack of systematic approaches to factor selection for Cloud services performance evaluation. In other words, evaluators randomly and intuitively concerned experimental factors in most of the existing evaluation studies. Based on our previous taxonomy and modeling work, this paper proposes a factor framework for experimental design for performance evaluation of commercial Cloud services. This framework capsules the state-of-the-practice of performance evaluation factors that people currently take into account in the Cloud Computing domain, and in turn can help facilitate designing new experiments for evaluating Cloud services.",Zheng Li|Liam O'Brien|He Zhang|Rainbow Cai,,https://arxiv.org/abs/1302.2203v1,https://arxiv.org/pdf/1302.2203v1,https://doi.org/10.1109/CloudCom.2012.6427525,"8 pages, Proceedings of the 4th International Conference on Cloud Computing Technology and Science (CloudCom 2012), pp. 169-176, Taipei, Taiwan, December 03-06, 2012",,10.1109/CloudCom.2012.6427525,cs.DC,cs.DC,https://arxiv.org/pdf/1302.2203v1.pdf
1301.2435v1,2013-01-11T09:49:27Z,2013-01-11 09:49:27,Toxicity profiling of engineered nanomaterials via multivariate dose-response surface modeling,"New generation in vitro high-throughput screening (HTS) assays for the assessment of engineered nanomaterials provide an opportunity to learn how these particles interact at the cellular level, particularly in relation to injury pathways. These types of assays are often characterized by small sample sizes, high measurement error and high dimensionality, as multiple cytotoxicity outcomes are measured across an array of doses and durations of exposure. In this paper we propose a probability model for the toxicity profiling of engineered nanomaterials. A hierarchical structure is used to account for the multivariate nature of the data by modeling dependence between outcomes and thereby combining information across cytotoxicity pathways. In this framework we are able to provide a flexible surface-response model that provides inference and generalizations of various classical risk assessment parameters. We discuss applications of this model to data on eight nanoparticles evaluated in relation to four cytotoxicity parameters.",Trina Patel|Donatello Telesca|Saji George|André E. Nel,,https://arxiv.org/abs/1301.2435v1,https://arxiv.org/pdf/1301.2435v1,https://doi.org/10.1214/12-AOAS563,Published in at http://dx.doi.org/10.1214/12-AOAS563 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2012, Vol. 6, No. 4, 1707-1729",10.1214/12-AOAS563,stat.AP,stat.AP,https://arxiv.org/pdf/1301.2435v1.pdf
1212.3194v1,2012-12-13T15:23:51Z,2012-12-13 15:23:51,Sparsely Sampling the Sky: A Bayesian Experimental Design Approach,"The next generation of galaxy surveys will observe millions of galaxies over large volumes of the universe. These surveys are expensive both in time and cost, raising questions regarding the optimal investment of this time and money. In this work we investigate criteria for selecting amongst observing strategies for constraining the galaxy power spectrum and a set of cosmological parameters. Depending on the parameters of interest, it may be more efficient to observe a larger, but sparsely sampled, area of sky instead of a smaller contiguous area. In this work, by making use of the principles of Bayesian Experimental Design, we will investigate the advantages and disadvantages of the sparse sampling of the sky and discuss the circumstances in which a sparse survey is indeed the most efficient strategy. For the Dark Energy Survey (DES), we find that by sparsely observing the same area in a smaller amount of time, we only increase the errors on the parameters by a maximum of 0.45%. Conversely, investing the same amount of time as the original DES to observe a sparser but larger area of sky we can in fact constrain the parameters with errors reduced by 28%.",P. Paykari|A. H. Jaffe,,https://arxiv.org/abs/1212.3194v1,https://arxiv.org/pdf/1212.3194v1,https://doi.org/10.1093/mnras/stt982,,,10.1093/mnras/stt982,astro-ph.CO,astro-ph.CO,https://arxiv.org/pdf/1212.3194v1.pdf
1212.2228v3,2012-12-10T21:47:11Z,2014-12-26 22:54:32,Gradient-based stochastic optimization methods in Bayesian experimental design,"Optimal experimental design (OED) seeks experiments expected to yield the most useful data for some purpose. In practical circumstances where experiments are time-consuming or resource-intensive, OED can yield enormous savings. We pursue OED for nonlinear systems from a Bayesian perspective, with the goal of choosing experiments that are optimal for parameter inference. Our objective in this context is the expected information gain in model parameters, which in general can only be estimated using Monte Carlo methods. Maximizing this objective thus becomes a stochastic optimization problem.
  This paper develops gradient-based stochastic optimization methods for the design of experiments on a continuous parameter space. Given a Monte Carlo estimator of expected information gain, we use infinitesimal perturbation analysis to derive gradients of this estimator. We are then able to formulate two gradient-based stochastic optimization approaches: (i) Robbins-Monro stochastic approximation, and (ii) sample average approximation combined with a deterministic quasi-Newton method. A polynomial chaos approximation of the forward model accelerates objective and gradient evaluations in both cases. We discuss the implementation of these optimization methods, then conduct an empirical comparison of their performance. To demonstrate design in a nonlinear setting with partial differential equation forward models, we use the problem of sensor placement for source inversion. Numerical results yield useful guidelines on the choice of algorithm and sample sizes, assess the impact of estimator bias, and quantify tradeoffs of computational cost versus solution quality and robustness.",Xun Huan|Youssef M. Marzouk,,https://arxiv.org/abs/1212.2228v3,https://arxiv.org/pdf/1212.2228v3,https://doi.org/10.1615/Int.J.UncertaintyQuantification.2014006730,"Preprint 40 pages, 10 figures (121 small figures). v1 submitted to the International Journal for Uncertainty Quantification on December 10, 2012; v2 submitted on September 10, 2013. v2 changes: (a) clarified algorithm stopping criteria and other parameters; (b) emphasized paper contributions, plus other minor edits; v3 submitted on December 26, 2014. v3 changes: minor edits",International Journal for Uncertainty Quantification 4 (2014) 479-510,10.1615/Int.J.UncertaintyQuantification.2014006730,stat.CO,stat.CO|math.OC|stat.ME,https://arxiv.org/pdf/1212.2228v3.pdf
1212.0511v1,2012-12-03T19:58:19Z,2012-12-03 19:58:19,Design of Experiments for Calibration of Planar Anthropomorphic Manipulators,The paper presents a novel technique for the design of optimal calibration experiments for a planar anthropomorphic manipulator with n degrees of freedom. Proposed approach for selection of manipulator configurations allows essentially improving calibration accuracy and reducing parameter identification errors. The results are illustrated by application examples that deal with typical anthropomorphic manipulators.,Alexandr Klimchik|Yier Wu|Stéphane Caro|Anatol Pashkevich,"EMN, IRCCyN|EMN, IRCCyN|IRCCyN|EMN, IRCCyN",https://arxiv.org/abs/1212.0511v1,https://arxiv.org/pdf/1212.0511v1,https://doi.org/10.1109/AIM.2011.6027017,"Advanced Intelligent Mechatronics (AIM), 2011 IEEE/ASME International Conference on, Budapest : Hungary (2011)",,10.1109/AIM.2011.6027017,cs.RO,cs.RO,https://arxiv.org/pdf/1212.0511v1.pdf
1211.1312v1,2012-11-06T17:08:05Z,2012-11-06 17:08:05,Optimum Experimental Design for EGDM Modeled Organic Semiconductor Devices,"We apply optimum experimental design (OED) to organic semiconductors modeled by the extended Gaussian disorder model (EGDM) which was developed by Pasveer et al. We present an extended Gummel method to decouple the corresponding system of equations and use automatic differentiation to get derivatives with the required accuracy for OED. We show in two examples, whose parameters are taken from Pasveer et al. and Mensfoort and Coehoorn that the linearized confidence regions of the parameters can be reduced significantly by applying OED resulting in new experiments with a different setup.",Christoph Karl Felix Weiler|Stefan Körkel,,https://arxiv.org/abs/1211.1312v1,https://arxiv.org/pdf/1211.1312v1,https://doi.org/10.1063/1.4794365,,,10.1063/1.4794365,math-ph,math-ph|physics.comp-ph,https://arxiv.org/pdf/1211.1312v1.pdf
1210.3739v3,2012-10-13T20:32:45Z,2014-10-15 16:25:47,Control Theory and Experimental Design in Diffusion Processes,"This paper considers the problem of designing time-dependent, real-time control policies for controllable nonlinear diffusion processes, with the goal of obtaining maximally-informative observations about parameters of interest. More precisely, we maximize the expected Fisher information for the parameter obtained over the duration of the experiment, conditional on observations made up to that time. We propose to accomplish this with a two-step strategy: when the full state vector of the diffusion process is observable continuously, we formulate this as an optimal control problem and apply numerical techniques from stochastic optimal control to solve it. When observations are incomplete, infrequent, or noisy, we propose using standard filtering techniques to first estimate the state of the system, then apply the optimal control policy using the posterior expectation of the state. We assess the effectiveness of these methods in 3 situations: a paradigmatic bistable model from statistical physics, a model of action potential generation in neurons, and a model of a simple ecological system.",Giles Hooker|Kevin K. Lin|Bruce Rogers,,https://arxiv.org/abs/1210.3739v3,https://arxiv.org/pdf/1210.3739v3,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1210.3739v3.pdf
1209.4019v4,2012-09-18T16:28:44Z,2018-01-29 20:13:20,Experimental design for Partially Observed Markov Decision Processes,"This paper deals with the question of how to most effectively conduct experiments in Partially Observed Markov Decision Processes so as to provide data that is most informative about a parameter of interest. Methods from Markov decision processes, especially dynamic programming, are introduced and then used in an algorithm to maximize a relevant Fisher Information. The algorithm is then applied to two POMDP examples. The methods developed can also be applied to stochastic dynamical systems, by suitable discretization, and we consequently show what control policies look like in the Morris-Lecar Neuron model, and simulation results are presented. We discuss how parameter dependence within these methods can be dealt with by the use of priors, and develop tools to update control policies online. This is demonstrated in another stochastic dynamical system describing growth dynamics of DNA template in a PCR model.",Leifur Thorbergsson|Giles Hooker,,https://arxiv.org/abs/1209.4019v4,https://arxiv.org/pdf/1209.4019v4,,"39 pages, 3 figures",,,stat.OT,stat.OT,https://arxiv.org/pdf/1209.4019v4.pdf
1209.0720v1,2012-09-04T18:12:13Z,2012-09-04 18:12:13,On the design of experiments to study extreme field limits,We propose experiments on the collision of high intensity electromagnetic pulses with electron bunches and on the collision of multiple electromagnetic pulses for studying extreme field limits in the nonlinear interaction of electromagnetic waves. The effects of nonlinear QED will be revealed in these laser plasma experiments.,S. S. Bulanov|M. Chen|C. B. Schroeder|E. Esarey|W. P. Leemans|S. V. Bulanov|T. Zh. Esirkepov|M. Kando|J. K. Koga|A. G. Zhidkov|P. Chen|V. D. Mur|N. B. Narozhny|V. S. Popov|A. G. R. Thomas|G. Korn,,https://arxiv.org/abs/1209.0720v1,https://arxiv.org/pdf/1209.0720v1,https://doi.org/10.1063/1.4773805,"7 pages, 3 figures, 1 table; 15th Advanced Accelerator Concepts Workshop (AAC 2012), Austin, Texas, 10-15 June, 2012",,10.1063/1.4773805,physics.plasm-ph,physics.plasm-ph|hep-ph,https://arxiv.org/pdf/1209.0720v1.pdf
1207.2968v1,2012-07-12T13:59:34Z,2012-07-12 13:59:34,The algebraic method in experimental design,The algebraic method provides useful techniques to identify models in designs and to understand aliasing of polynomial models. The present note surveys the topic of Gröbner bases in experimental design and then describes the notion of confounding and the algebraic fan of a design. The ideas are illustrated with a variety of design examples ranging from Latin squares to screening designs.,Hugo Maruri-Aguilar|Henry P. Wynn,,https://arxiv.org/abs/1207.2968v1,https://arxiv.org/pdf/1207.2968v1,,,,,stat.ME,stat.ME|math.AC,https://arxiv.org/pdf/1207.2968v1.pdf
1206.4172v1,2012-06-19T10:34:40Z,2012-06-19 10:34:40,Efficient response surface methods based on generic surrogate models,"Surrogate models are used for global approximation of responses generated by expensive computer experiments like CFD applications. In this paper, we make use of structural similarities which are shared by a class of related problems. We identify these structures by applying statistical shape models. They are used to build a generic surrogate model approximation to sample data of a new problem of the same class. In a variable fidelity framework the generic surrogate model is combined with the sample data to generate an efficient and globally accurate interpolation model, which requires less costly sample evaluations than ordinary response surface methods. We demonstrate our method with an aerodynamic test case and show that it significantly improves the approximation quality.",Benjamin Rosenbaum|Volker Schulz,,https://arxiv.org/abs/1206.4172v1,https://arxiv.org/pdf/1206.4172v1,,,,,math.NA,math.NA,https://arxiv.org/pdf/1206.4172v1.pdf
1205.0525v1,2012-05-02T19:13:54Z,2012-05-02 19:13:54,"Tevatron Beam Halo Collimation System: Design, Operational Experience and New Methods","Collimation of proton and antiproton beams in the Tevatron collider is required to protect CDF and D0 detectors and minimize their background rates, to keep irradiation of superconducting magnets under control, to maintain long-term operational reliability, and to reduce the impact of beam-induced radiation on the environment. In this article we briefly describe the design, practical implementation and performance of the collider collimation system, methods to control transverse and longitudinal beam halo and two novel collimation techniques tested in the Tevatron.",Nikolai Mokhov|Jerry Annala|Richard Carrigan|Michael Church|Alexander Drozhdin|Todd Johnson|Reilly Robert|Vladimir Shiltsev|Guilio Stancari|Dean Still|Alexander Valishev|Xiao-Long Zhang|Viktoriya Zvoda,Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab|Fermilab,https://arxiv.org/abs/1205.0525v1,https://arxiv.org/pdf/1205.0525v1,https://doi.org/10.1088/1748-0221/6/08/T08005,25 pp,JINST 6 (2011) T08005,10.1088/1748-0221/6/08/T08005,physics.acc-ph,physics.acc-ph,https://arxiv.org/pdf/1205.0525v1.pdf
1203.5587v1,2012-03-26T06:46:57Z,2012-03-26 06:46:57,Response surface methodology: Asymptotic normality of the optimal solution,Sensitivity analysis of the optimal solution in response surface methodology is studied and an explicit form of the effect of perturbation of the regression coefficients on the optimal solution is obtained. The characterisation of the critical point of the convex program corresponding to the optimum of a response surface model is also studied. The asymptotic normality of the optimal solution follows by standard methods.,Jose A. Diaz-Garcia|Jose E. Rodriguez|Rogelio Ramos-Quiroga,,https://arxiv.org/abs/1203.5587v1,https://arxiv.org/pdf/1203.5587v1,,10 pages,,,math.ST,math.ST,https://arxiv.org/pdf/1203.5587v1.pdf
1203.3391v2,2012-03-15T15:26:30Z,2012-05-18 09:09:26,Adaptive experimental design for one-qubit state estimation with finite data based on a statistical update criterion,"We consider 1-qubit mixed quantum state estimation by adaptively updating measurements according to previously obtained outcomes and measurement settings. Updates are determined by the average-variance-optimality (A-optimality) criterion, known in the classical theory of experimental design and applied here to quantum state estimation. In general, A-optimization is a nonlinear minimization problem; however, we find an analytic solution for 1-qubit state estimation using projective measurements, reducing computational effort. We compare numerically two adaptive and two nonadaptive schemes for finite data sets and show that the A-optimality criterion gives more precise estimates than standard quantum tomography.",Takanori Sugiyama|Peter S. Turner|Mio Murao,,https://arxiv.org/abs/1203.3391v2,https://arxiv.org/pdf/1203.3391v2,https://doi.org/10.1103/PhysRevA.85.052107,"15 pages, 7 figures","Phys. Rev. A 85, 052107 (2012)",10.1103/PhysRevA.85.052107,quant-ph,quant-ph|math.ST|stat.ML,https://arxiv.org/pdf/1203.3391v2.pdf
1201.0942v2,2012-01-04T17:35:27Z,2012-12-06 08:11:50,Competitive Comparison of Optimal Designs of Experiments for Sampling-based Sensitivity Analysis,"Nowadays, the numerical models of real-world structures are more precise, more complex and, of course, more time-consuming. Despite the growth of a computational effort, the exploration of model behaviour remains a complex task. The sensitivity analysis is a basic tool for investigating the sensitivity of the model to its inputs. One widely used strategy to assess the sensitivity is based on a finite set of simulations for a given sets of input parameters, i.e. points in the design space. An estimate of the sensitivity can be then obtained by computing correlations between the input parameters and the chosen response of the model. The accuracy of the sensitivity prediction depends on the choice of design points called the design of experiments. The aim of the presented paper is to review and compare available criteria determining the quality of the design of experiments suitable for sampling-based sensitivity analysis.",Eliska Janouchova|Anna Kucerova,,https://arxiv.org/abs/1201.0942v2,https://arxiv.org/pdf/1201.0942v2,https://doi.org/10.1016/j.compstruc.2013.04.009,"18 pages, 15 figures, 4 tables, CSC2011 special issue, corrected and extended after the first review","Computers & Structures, 124, 47-60, 2013",10.1016/j.compstruc.2013.04.009,cs.CE,cs.CE|math.NA|stat.ME,https://arxiv.org/pdf/1201.0942v2.pdf
1112.1109v1,2011-12-05T22:04:23Z,2011-12-05 22:04:23,"Isostaticity, auxetic response, surface modes, and conformal invariance in twisted kagome lattices","Model lattices consisting of balls connected by central-force springs provide much of our understanding of mechanical response and phonon structure of real materials. Their stability depends critically on their coordination number $z$. $d$-dimensional lattices with $z=2d$ are at the threshold of mechanical stability and are isostatic. Lattices with $z<2d$ exhibit zero-frequency ""floppy"" modes that provide avenues for lattice collapse. The physics of systems as diverse as architectural structures, network glasses, randomly packed spheres, and biopolymer networks is strongly influenced by a nearby isostatic lattice. We explore elasticity and phonons of a special class of two-dimensional isostatic lattices constructed by distorting the kagome lattice. We show that the phonon structure of these lattices, characterized by vanishing bulk moduli and thus negative Poisson ratios and auxetic elasticity, depends sensitively on boundary conditions and on the nature of the kagome distortions. We construct lattices that under free boundary conditions exhibit surface floppy modes only or a combination of both surface and bulk floppy modes; and we show that bulk floppy modes present under free boundary conditions are also present under periodic boundary conditions but that surface modes are not. In the the long-wavelength limit, the elastic theory of all these lattices is a conformally invariant field theory with holographic properties, and the surface waves are Rayleigh waves. We discuss our results in relation to recent work on jammed systems. Our results highlight the importance of network architecture in determining floppy-mode structure.",Kai Sun|Anton Souslov|Xiaoming Mao|T. C. Lubensky,,https://arxiv.org/abs/1112.1109v1,https://arxiv.org/pdf/1112.1109v1,https://doi.org/10.1073/pnas.1119941109,"12 pages, 7 figures","Proceedings of the National Academy of Sciences of the United States of America, vol 109, no. 31, 12369-12374 (2012)",10.1073/pnas.1119941109,cond-mat.soft,cond-mat.soft|cond-mat.stat-mech|cond-mat.str-el,https://arxiv.org/pdf/1112.1109v1.pdf
1111.0935v1,2011-11-03T18:03:06Z,2011-11-03 18:03:06,Adaptive Hamiltonian Estimation Using Bayesian Experimental Design,"Using Bayesian experimental design techniques, we have shown that for a single two-level quantum mechanical system under strong (projective) measurement, the dynamical parameters of a model Hamiltonian can be estimated with exponentially improved accuracy over offline estimation strategies. To achieve this, we derive an adaptive protocol which finds the optimal experiments based on previous observations. We show that the risk associated with this algorithm is close to the global optimum, given a uniform prior. Additionally, we show that sampling at the Nyquist rate is not optimal.",Christopher Ferrie|Christopher E. Granade|D. G. Cory,,https://arxiv.org/abs/1111.0935v1,https://arxiv.org/pdf/1111.0935v1,https://doi.org/10.1063/1.3703632,"8 pages, 3 figures. To appear in Bayesian Inference And Maximum Entropy Methods In Science And Engineering: Proceedings of the 31th International Workshop on Bayesian Inference and Maximum Entropy Methods in Science and Engineering","AIP Conf. Proc. 1443, pp. 165-173, 2011",10.1063/1.3703632,quant-ph,quant-ph|math.PR,https://arxiv.org/pdf/1111.0935v1.pdf
1111.0562v1,2011-11-02T16:56:31Z,2011-11-02 16:56:31,Model-driven system development: Experimental design and report of the pilot experiment,"This report describes de design of an experiment that intends to compare two variants of a modeldriven system development method, so as to assess the impact of requirements engineering practice in the quality of the conceptual models. The conceptual modelling method being assessed is the OO-Method [Pastor and Molina 2007]. One of its variants includes Communication Analysis, a communication-oriented requirements engineering method [España, González et al. 2009] and a set of guidelines to derive conceptual models from requirements models [España, Ruiz et al. 2011; González, España et al. 2011]. The other variant is an ad-hoc, text-based requirements practice similar to the one that is applied in industrial projects by OO-Method practitioners. The goal of the research, summarised according to the Goal/Question/Metric template [Basili and Rombach 1988], is to:
  *) analyse the resulting models of two model-based information systems analysis method variants; namely, the OO-Method (OOM) and the integration of Communication Analysis and the OO-Method (CA+OOM),
  *) for the purpose of carrying out a comparative evaluation
  *) with respect to performance of the subject and acceptance of the method;
  *) from the viewpoint of the information systems researcher
  *) in the context of bachelor students.",Sergio España|Nelly Condori|Roel Wieringa|Arturo González|Óscar Pastor,,https://arxiv.org/abs/1111.0562v1,https://arxiv.org/pdf/1111.0562v1,,"83 pages, 45 figures, 16 tables",,,cs.SE,cs.SE,https://arxiv.org/pdf/1111.0562v1.pdf
1108.4944v1,2011-08-24T20:28:01Z,2011-08-24 20:28:01,Investigating Systematic Uncertainty and Experimental Design with Projectile Launchers,"The proper choice of a measurement technique that minimizes systematic and random uncertainty is an essential part of experimental physics. These issues are difficult to teach in the introductory laboratory, though: because most experiments involve only a single measurement technique, students are often unable to make a clear distinction between random and systematic uncertainties, or to compare the uncertainties associated with different techniques. In this paper, we describe an experiment suitable for an introductory college level (or advanced high school) course that uses velocity measurements to clearly show students the effects of both random and systematic uncertainties.",Chad Orzel|Gary Reich|Jonathan Marr,,https://arxiv.org/abs/1108.4944v1,https://arxiv.org/pdf/1108.4944v1,,"6 pages, 1 figure Submitted to The Physics Teacher",,,physics.ed-ph,physics.ed-ph,https://arxiv.org/pdf/1108.4944v1.pdf
1108.4146v3,2011-08-20T22:49:15Z,2012-11-30 23:34:15,Simulation-based optimal Bayesian experimental design for nonlinear systems,"The optimal selection of experimental conditions is essential to maximizing the value of data for inference and prediction, particularly in situations where experiments are time-consuming and expensive to conduct. We propose a general mathematical framework and an algorithmic approach for optimal experimental design with nonlinear simulation-based models; in particular, we focus on finding sets of experiments that provide the most information about targeted sets of parameters.
  Our framework employs a Bayesian statistical setting, which provides a foundation for inference from noisy, indirect, and incomplete data, and a natural mechanism for incorporating heterogeneous sources of information. An objective function is constructed from information theoretic measures, reflecting expected information gain from proposed combinations of experiments. Polynomial chaos approximations and a two-stage Monte Carlo sampling method are used to evaluate the expected information gain. Stochastic approximation algorithms are then used to make optimization feasible in computationally intensive and high-dimensional settings. These algorithms are demonstrated on model problems and on nonlinear parameter estimation problems arising in detailed combustion kinetics.",Xun Huan|Youssef M. Marzouk,,https://arxiv.org/abs/1108.4146v3,https://arxiv.org/pdf/1108.4146v3,https://doi.org/10.1016/j.jcp.2012.08.013,"Preprint 53 pages, 17 figures (54 small figures). v1 submitted to the Journal of Computational Physics on August 4, 2011; v2 submitted on August 12, 2012. v2 changes: (a) addition of Appendix B and Figure 17 to address the bias in the expected utility estimator; (b) minor language edits; v3 submitted on November 30, 2012. v3 changes: minor edits",Journal of Computational Physics 232 (2013) 288-317,10.1016/j.jcp.2012.08.013,stat.ML,stat.ML|stat.CO|stat.ME,https://arxiv.org/pdf/1108.4146v3.pdf
1108.1689v1,2011-08-08T12:26:56Z,2011-08-08 12:26:56,A nonlinear preconditioner for experimental design problems,"We address the slow convergence and poor stability of quasi-newton sequential quadratic programming (SQP) methods that is observed when solving experimental design problems, in particular when they are large. Our findings suggest that this behavior is due to the fact that these problems often have bad absolute condition numbers. To shed light onto the structure of the problem close to the solution, we formulate a model problem (based on the $A$-criterion), that is defined in terms of a given initial design that is to be improved. We prove that the absolute condition number of the model problem grows without bounds as the quality of the initial design improves. Additionally, we devise a preconditioner that ensures that the condition number will instead stay uniformly bounded. Using numerical experiments, we study the effect of this reformulation on relevant cases of the general problem, and find that it leads to significant improvements in stability and convergence behavior.",M. S. Mommer|A. Sommer|J. P. Schlöder|H. G. Bock,,https://arxiv.org/abs/1108.1689v1,https://arxiv.org/pdf/1108.1689v1,,"11 pages, 5 figures",,,math.OC,math.OC|eess.SY,https://arxiv.org/pdf/1108.1689v1.pdf
1107.1445v1,2011-07-07T16:46:43Z,2011-07-07 16:46:43,Bayesian experimental design for the active nitridation of graphite by atomic nitrogen,"The problem of optimal data collection to efficiently learn the model parameters of a graphite nitridation experiment is studied in the context of Bayesian analysis using both synthetic and real experimental data. The paper emphasizes that the optimal design can be obtained as a result of an information theoretic sensitivity analysis. Thus, the preferred design is where the statistical dependence between the model parameters and observables is the highest possible. In this paper, the statistical dependence between random variables is quantified by mutual information and estimated using a k-nearest neighbor based approximation. It is shown, that by monitoring the inference process via measures such as entropy or Kullback-Leibler divergence, one can determine when to stop the data collection process. The methodology is applied to select the most informative designs on both a simulated data set and on an experimental data set, previously published in the literature. It is also shown that the sequential Bayesian analysis used in the experimental design can also be useful in detecting conflicting information between measurements and model predictions.",Gabriel Terejanu|Rochan R. Upadhyay|Kenji Miki,,https://arxiv.org/abs/1107.1445v1,https://arxiv.org/pdf/1107.1445v1,,"Preprint submitted to Experimental Thermal and Fluid Science, February 2011",,,physics.data-an,physics.data-an|cs.IT|stat.AP,https://arxiv.org/pdf/1107.1445v1.pdf
1105.3816v1,2011-05-19T08:58:14Z,2011-05-19 08:58:14,On construction of optimal mixed-level supersaturated designs,"Supersaturated design (SSD) has received much recent interest because of its potential in factor screening experiments. In this paper, we provide equivalent conditions for two columns to be fully aliased and consequently propose methods for constructing $E(f_{\mathrm{NOD}})$- and $χ^2$-optimal mixed-level SSDs without fully aliased columns, via equidistant designs and difference matrices. The methods can be easily performed and many new optimal mixed-level SSDs have been obtained. Furthermore, it is proved that the nonorthogonality between columns of the resulting design is well controlled by the source designs. A rather complete list of newly generated optimal mixed-level SSDs are tabulated for practical use.",Fasheng Sun|Dennis K. J. Lin|Min-Qian Liu,,https://arxiv.org/abs/1105.3816v1,https://arxiv.org/pdf/1105.3816v1,https://doi.org/10.1214/11-AOS877,Published in at http://dx.doi.org/10.1214/11-AOS877 the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Statistics 2011, Vol. 39, No. 2, 1310-1333",10.1214/11-AOS877,math.ST,math.ST,https://arxiv.org/pdf/1105.3816v1.pdf
1103.6085v1,2011-03-31T05:19:33Z,2011-03-31 05:19:33,Experimental Design for the Gemini Planet Imager,"The Gemini Planet Imager (GPI) is a high performance adaptive optics system being designed and built for the Gemini Observatory. GPI is optimized for high contrast imaging, combining precise and accurate wavefront control, diffraction suppression, and a speckle-suppressing science camera with integral field and polarimetry capabilities. The primary science goal for GPI is the direct detection and characterization of young, Jovian-mass exoplanets. For plausible assumptions about the distribution of gas giant properties at large semi-major axes, GPI will be capable of detecting more than 10% of gas giants more massive than 0.5 M_J around stars younger than 100 Myr and nearer than 75 parsecs. For systems younger than 1 Gyr, gas giants more massive than 8 M_J and with semi-major axes greater than 15 AU are detected with completeness greater than 50%. A survey targeting young stars in the solar neighborhood will help determine the formation mechanism of gas giant planets by studying them at ages where planet brightness depends upon formation mechanism. Such a survey will also be sensitive to planets at semi-major axes comparable to the gas giants in our own solar system. In the simple, and idealized, situation in which planets formed by either the ""hot-start"" model of Burrows et al. (2003) or the core accretion model of Marley et al. (2007), a few tens of detected planets are sufficient to distinguish how planets form.",James McBride|James R. Graham|Bruce Macintosh|Steven V. W. Beckwith|Christian Marois|Lisa A. Poyneer|Sloane J. Wiktorowicz,,https://arxiv.org/abs/1103.6085v1,https://arxiv.org/pdf/1103.6085v1,https://doi.org/10.1086/660733,"15 pages, 9 figures, revised after referee's comments and resubmitted to PASP",,10.1086/660733,astro-ph.EP,astro-ph.EP,https://arxiv.org/pdf/1103.6085v1.pdf
1101.3663v2,2011-01-19T11:03:00Z,2012-02-01 12:30:27,A robust optimization approach to experimental design for model discrimination of dynamical systems,"A high-ranking goal of interdisciplinary modeling approaches in the natural sciences are quantitative prediction of system dynamics and model based optimization. For this purpose, mathematical modeling, numerical simulation and scientific computing techniques are indispensable. Quantitative modeling closely combined with experimental investigations is required if the model is supposed to be used for sound mechanistic analysis and model predictions. Typically, before an appropriate model of a experimental system is found different hypothetical models might be reasonable and consistent with previous knowledge and available data. The parameters of the model up to an estimated confidence region are generally not known a priori. Therefore one has to incorporate possible parameter configurations of different models into a model discrimination algorithm. In this article we present a numerical algorithm which calculates a design of experiments which allows an optimal discrimination of different hypothetic candidate models of a given dynamic system for the most inappropriate parameter configurations within a parameter range via a worst case estimate. The design criterion comprises optimal measurement time points. The used criterion is derived from the Kullback-Leibler divergence. The underlying optimization problem can be classified as a semi infinite optimization problem which we solve in an outer approximation approach stabilized by a homotopy strategy. We present the theoretical framework as well as the numerical algorithmic realization.",Dominik Skanda|Dirk Lebiedz,,https://arxiv.org/abs/1101.3663v2,https://arxiv.org/pdf/1101.3663v2,,,,,math.OC,math.OC,https://arxiv.org/pdf/1101.3663v2.pdf
1101.2501v2,2011-01-13T08:23:32Z,2011-10-21 02:08:42,On the design of experiments for the study of extreme field limits in the interaction of laser with ultrarelativistic electron beam,"We propose the experiments on the collision of laser light and high intensity electromagnetic pulses generated by relativistic flying mirrors, with electron bunches produced by a conventional accelerator and with laser wake field accelerated electrons for studying extreme field limits in the nonlinear interaction of electromagnetic waves. The regimes of dominant radiation reaction, which completely changes the electromagnetic wave-matter interaction, will be revealed in the laser plasma experiments. This will result in a new powerful source of ultra short high brightness gamma-ray pulses. A possibility of the demonstration of the electron-positron pair creation in vacuum in a multi-photon processes can be realized. This will allow modeling under terrestrial laboratory conditions neutron star magnetospheres, cosmological gamma ray bursts and the Leptonic Era of the Universe.",S. V. Bulanov|T. Zh. Esirkepov|Y. Hayashi|M. Kando|H. Kiriyama|J. K. Koga|K. Kondo|H. Kotaki|A. S. Pirozhkov|S. S. Bulanov|A. G. Zhidkov|P. Chen|D. Neely|Y. Kato|N. B. Narozhny|G. Korn,,https://arxiv.org/abs/1101.2501v2,https://arxiv.org/pdf/1101.2501v2,https://doi.org/10.1016/j.nima.2011.09.029,"33 pages, 5 figures, 1 table",,10.1016/j.nima.2011.09.029,hep-ph,hep-ph|astro-ph.HE|physics.acc-ph|physics.plasm-ph,https://arxiv.org/pdf/1101.2501v2.pdf
1009.1909v3,2010-09-09T22:47:16Z,2012-10-12 23:55:15,Computing Optimal Experimental Designs via Interior Point Method,"In this paper, we study optimal experimental design problems with a broad class of smooth convex optimality criteria, including the classical A-, D- and p th mean criterion. In particular, we propose an interior point (IP) method for them and establish its global convergence. Furthermore, by exploiting the structure of the Hessian matrix of the aforementioned optimality criteria, we derive an explicit formula for computing its rank. Using this result, we then show that the Newton direction arising in the IP method can be computed efficiently via Sherman-Morrison-Woodbury formula when the size of the moment matrix is small relative to the sample size. Finally, we compare our IP method with the widely used multiplicative algorithm introduced by Silvey et al. [29]. The computational results show that the IP method generally outperforms the multiplicative algorithm both in speed and solution quality.",Zhaosong Lu|Ting Kei Pong,,https://arxiv.org/abs/1009.1909v3,https://arxiv.org/pdf/1009.1909v3,,,,,stat.CO,stat.CO|math.OC,https://arxiv.org/pdf/1009.1909v3.pdf
1008.4973v1,2010-08-29T23:37:19Z,2010-08-29 23:37:19,Entropy-Based Search Algorithm for Experimental Design,"The scientific method relies on the iterated processes of inference and inquiry. The inference phase consists of selecting the most probable models based on the available data; whereas the inquiry phase consists of using what is known about the models to select the most relevant experiment. Optimizing inquiry involves searching the parameterized space of experiments to select the experiment that promises, on average, to be maximally informative. In the case where it is important to learn about each of the model parameters, the relevance of an experiment is quantified by Shannon entropy of the distribution of experimental outcomes predicted by a probable set of models. If the set of potential experiments is described by many parameters, we must search this high-dimensional entropy space. Brute force search methods will be slow and computationally expensive. We present an entropy-based search algorithm, called nested entropy sampling, to select the most informative experiment for efficient experimental design. This algorithm is inspired by Skilling's nested sampling algorithm used in inference and borrows the concept of a rising threshold while a set of experiment samples are maintained. We demonstrate that this algorithm not only selects highly relevant experiments, but also is more efficient than brute force search. Such entropic search techniques promise to greatly benefit autonomous experimental design.",N. K. Malakar|K. H. Knuth,,https://arxiv.org/abs/1008.4973v1,https://arxiv.org/pdf/1008.4973v1,https://doi.org/10.1063/1.3573612,"8 pages, 3 figures. To appear in the proceedings of MaxEnt 2010, held in Chamonix, France",,10.1063/1.3573612,stat.ML,stat.ML|cs.LG|physics.comp-ph|physics.data-an,https://arxiv.org/pdf/1008.4973v1.pdf
1007.4152v2,2010-07-23T15:17:57Z,2011-12-05 10:55:58,"Approximation of a Maximum-Submodular-Coverage problem involving spectral functions, with application to Experimental Design","We study a family of combinatorial optimization problems defined by a parameter $p\in[0,1]$, which involves spectral functions applied to positive semidefinite matrices, and has some application in the theory of optimal experimental design. This family of problems tends to a generalization of the classical maximum coverage problem as $p$ goes to 0, and to a trivial instance of the knapsack problem as $p$ goes to 1.
  In this article, we establish a matrix inequality which shows that the objective function is submodular for all $p\in[0,1]$, from which it follows that the greedy approach, which has often been used for this problem, always gives a design within $1-1/e$ of the optimum. We next study the design found by rounding the solution of the continuous relaxed problem, an approach which has been applied by several authors. We prove an inequality which generalizes a classical result from the theory of optimal designs, and allows us to give a rounding procedure with an approximation factor which tends to 1 as $p$ goes to 1.",Guillaume Sagnol,,https://arxiv.org/abs/1007.4152v2,https://arxiv.org/pdf/1007.4152v2,,,,,math.OC,math.OC|math.CO,https://arxiv.org/pdf/1007.4152v2.pdf
1001.3011v1,2010-01-18T10:39:39Z,2010-01-18 10:39:39,A Multivariate Variance Components Model for Analysis of Covariance in Designed Experiments,"  Traditional methods for covariate adjustment of treatment means in designed experiments are inherently conditional on the observed covariate values. In order to develop a coherent general methodology for analysis of covariance, we propose a multivariate variance components model for the joint distribution of the response and covariates. It is shown that, if the design is orthogonal with respect to (random) blocking factors, then appropriate adjustments to treatment means can be made using the univariate variance components model obtained by conditioning on the observed covariate values. However, it is revealed that some widely used models are incorrectly specified, leading to biased estimates and incorrect standard errors. The approach clarifies some issues that have been the source of ongoing confusion in the statistics literature.",James G. Booth|Walter T. Federer|Martin T. Wells|Russell D. Wolfinger,,https://arxiv.org/abs/1001.3011v1,https://arxiv.org/pdf/1001.3011v1,https://doi.org/10.1214/09-STS294,Published in at http://dx.doi.org/10.1214/09-STS294 the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Statistical Science 2009, Vol. 24, No. 2, 223-237",10.1214/09-STS294,stat.ME,stat.ME,https://arxiv.org/pdf/1001.3011v1.pdf
1001.1654v2,2010-01-11T13:37:38Z,2010-02-19 09:30:31,Algorithmic Differentiation of Linear Algebra Functions with Application in Optimum Experimental Design (Extended Version),"  We derive algorithms for higher order derivative computation of the rectangular $QR$ and eigenvalue decomposition of symmetric matrices with distinct eigenvalues in the forward and reverse mode of algorithmic differentiation (AD) using univariate Taylor propagation of matrices (UTPM). Linear algebra functions are regarded as elementary functions and not as algorithms. The presented algorithms are implemented in the BSD licensed AD tool \texttt{ALGOPY}. Numerical tests show that the UTPM algorithms derived in this paper produce results close to machine precision accuracy. The theory developed in this paper is applied to compute the gradient of an objective function motivated from optimum experimental design: $\nabla_x Φ(C(J(F(x,y))))$, where $Φ= \{λ_1 : λ_1 C\}$, $C = (J^T J)^{-1}$, $J = \frac{\dd F}{\dd y}$ and $F = F(x,y)$.",S. F. Walter|L. Lehmann,,https://arxiv.org/abs/1001.1654v2,https://arxiv.org/pdf/1001.1654v2,,,,,cs.DS,cs.DS|cs.MS|math.NA,https://arxiv.org/pdf/1001.1654v2.pdf
0912.3995v4,2009-12-21T00:08:19Z,2010-06-09 23:24:13,Gaussian Process Optimization in the Bandit Setting: No Regret and Experimental Design,"Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multi-armed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.",Niranjan Srinivas|Andreas Krause|Sham M. Kakade|Matthias Seeger,,https://arxiv.org/abs/0912.3995v4,https://arxiv.org/pdf/0912.3995v4,https://doi.org/10.1109/TIT.2011.2182033,,,10.1109/TIT.2011.2182033,cs.LG,cs.LG,https://arxiv.org/pdf/0912.3995v4.pdf
0908.2222v1,2009-08-16T08:30:48Z,2009-08-16 08:30:48,Transaction-Oriented Simulation In Ad Hoc Grids: Design and Experience,"  In this paper we analyse the requirements of performing parallel transaction-oriented simulations within loosely coupled systems like ad hoc grids. We focus especially on the space-parallel approach to parallel simulation and on discrete event synchronisation algorithms that are suitable for transaction-oriented simulation and the target environment of ad hoc grids. To demonstrate our findings, a Java-based parallel simulator for the transaction-oriented language GPSS/H is implemented on the basis of the most promising shock-resistant Time Warp (SRTW) synchronisation algorithm and using the grid framework ProActive. The analysis of our parallel simulator, based on experiments using the Grid5000 platform, shows that the SRTW algorithm can successfully reduce the number of rolled back transaction moves but it also reveals circumstances in which the SRTW algorithm can be outperformed by the normal Time Warp algorithm. Finally, possible improvements to the SRTW algorithm are proposed in order to avoid such problems.",Gerald Krafft|Vladimir Getov,,https://arxiv.org/abs/0908.2222v1,https://arxiv.org/pdf/0908.2222v1,,"7 pages, 6 figures, 1 tables, Proceedings of the 2008 High Performance Computing and Simulation Conference (HPCS 2008), 3 - 6 June 2008, Nicosia, Cyprus. pp. 38-44; http://westminsterresearch.wmin.ac.uk/5341/",,,cs.DC,cs.DC,https://arxiv.org/pdf/0908.2222v1.pdf
0812.3937v2,2008-12-20T05:52:41Z,2009-08-17 10:04:46,"Experimental designs for multiple-level responses, with application to a large-scale educational intervention","  Educational research often studies subjects that are in naturally clustered groups of classrooms or schools. When designing a randomized experiment to evaluate an intervention directed at teachers, but with effects on teachers and their students, the power or anticipated variance for the treatment effect needs to be examined at both levels. If the treatment is applied to clusters, power is usually reduced. At the same time, a cluster design decreases the probability of contamination, and contamination can also reduce power to detect a treatment effect. Designs that are optimal at one level may be inefficient for estimating the treatment effect at another level. In this paper we study the efficiency of three designs and their ability to detect a treatment effect: randomize schools to treatment, randomize teachers within schools to treatment, and completely randomize teachers to treatment. The three designs are compared for both the teacher and student level within the mixed model framework, and a simulation study is conducted to compare expected treatment variances for the three designs with various levels of correlation within and between clusters. We present a computer program that study designers can use to explore the anticipated variances of treatment effects under proposed experimental designs and settings.",Brenda Jenney|Sharon Lohr,,https://arxiv.org/abs/0812.3937v2,https://arxiv.org/pdf/0812.3937v2,https://doi.org/10.1214/08-AOAS216,Published in at http://dx.doi.org/10.1214/08-AOAS216 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2009, Vol. 3, No. 2, 691-709",10.1214/08-AOAS216,stat.AP,stat.AP,https://arxiv.org/pdf/0812.3937v2.pdf
0812.3789v1,2008-12-19T13:53:15Z,2008-12-19 13:53:15,Optimizing Nuclear Reaction Analysis (NRA) using Bayesian Experimental Design,"  Nuclear Reaction Analysis with ${}^{3}$He holds the promise to measure Deuterium depth profiles up to large depths. However, the extraction of the depth profile from the measured data is an ill-posed inversion problem. Here we demonstrate how Bayesian Experimental Design can be used to optimize the number of measurements as well as the measurement energies to maximize the information gain. Comparison of the inversion properties of the optimized design with standard settings reveals huge possible gains. Application of the posterior sampling method allows to optimize the experimental settings interactively during the measurement process.",U. von Toussaint|T. Schwarz-Selinger|S. Gori,,https://arxiv.org/abs/0812.3789v1,https://arxiv.org/pdf/0812.3789v1,https://doi.org/10.1063/1.3039019,"Bayesian Inference and Maximum Entropy Conference 2008, AIP Conference proceedings 1073, p. 348-358, 4 figures",,10.1063/1.3039019,physics.acc-ph,physics.acc-ph|physics.data-an,https://arxiv.org/pdf/0812.3789v1.pdf
0810.0901v2,2008-10-06T07:59:45Z,2010-08-12 20:29:23,Large Scale Variational Inference and Experimental Design for Sparse Generalized Linear Models,"Many problems of low-level computer vision and image processing, such as denoising, deconvolution, tomographic reconstruction or super-resolution, can be addressed by maximizing the posterior distribution of a sparse linear model (SLM). We show how higher-order Bayesian decision-making problems, such as optimizing image acquisition in magnetic resonance scanners, can be addressed by querying the SLM posterior covariance, unrelated to the density's mode. We propose a scalable algorithmic framework, with which SLM posteriors over full, high-resolution images can be approximated for the first time, solving a variational optimization problem which is convex iff posterior mode finding is convex. These methods successfully drive the optimization of sampling trajectories for real-world magnetic resonance imaging through Bayesian experimental design, which has not been attempted before. Our methodology provides new insight into similarities and differences between sparse reconstruction and approximate Bayesian inference, and has important implications for compressive sensing of real-world images.",Matthias W. Seeger|Hannes Nickisch,,https://arxiv.org/abs/0810.0901v2,https://arxiv.org/pdf/0810.0901v2,,"34 pages, 6 figures, technical report (submitted)",,,stat.ML,stat.ML,https://arxiv.org/pdf/0810.0901v2.pdf
0809.4938v1,2008-09-29T11:28:47Z,2008-09-29 11:28:47,Optimal experimental designs for inverse quadratic regression models,"  In this paper optimal experimental designs for inverse quadratic regression models are determined. We consider two different parameterizations of the model and investigate local optimal designs with respect to the $c$-, $D$- and $E$-criteria, which reflect various aspects of the precision of the maximum likelihood estimator for the parameters in inverse quadratic regression models. In particular it is demonstrated that for a sufficiently large design space geometric allocation rules are optimal with respect to many optimality criteria. Moreover, in numerous cases the designs with respect to the different criteria are supported at the same points. Finally, the efficiencies of different optimal designs with respect to various optimality criteria are studied, and the efficiency of some commonly used designs are investigated.",H. Dette|C. Kiss,,https://arxiv.org/abs/0809.4938v1,https://arxiv.org/pdf/0809.4938v1,,24 pages,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/0809.4938v1.pdf
0808.3055v1,2008-08-22T17:00:32Z,2008-08-22 17:00:32,Minimal average degree aberration and the state polytope for experimental designs,"  For a particular experimental design, there is interest in finding which polynomial models can be identified in the usual regression set up. The algebraic methods based on Groebner bases provide a systematic way of doing this. The algebraic method does not in general produce all estimable models but it can be shown that it yields models which have minimal average degree in a well-defined sense and in both a weighted and unweighted version. This provides an alternative measure to that based on ""aberration"" and moreover is applicable to any experimental design. A simple algorithm is given and bounds are derived for the criteria, which may be used to give asymptotic Nyquist-like estimability rates as model and sample sizes increase.",Yael Berstein|Hugo Maruri-Aguilar|Shmuel Onn|Eva Riccomagno|Henry Wynn,,https://arxiv.org/abs/0808.3055v1,https://arxiv.org/pdf/0808.3055v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/0808.3055v1.pdf
0808.1753v2,2008-08-12T23:47:21Z,2008-09-23 15:41:44,Index wiki database: design and experiments,"  With the fantastic growth of Internet usage, information search in documents of a special type called a ""wiki page"" that is written using a simple markup language, has become an important problem. This paper describes the software architectural model for indexing wiki texts in three languages (Russian, English, and German) and the interaction between the software components (GATE, Lemmatizer, and Synarcher). The inverted file index database was designed using visual tool DBDesigner. The rules for parsing Wikipedia texts are illustrated by examples. Two index databases of Russian Wikipedia (RW) and Simple English Wikipedia (SEW) are built and compared. The size of RW is by order of magnitude higher than SEW (number of words, lexemes), though the growth rate of number of pages in SEW was found to be 14% higher than in Russian, and the rate of acquisition of new words in SEW lexicon was 7% higher during a period of five months (from September 2007 to February 2008). The Zipf's law was tested with both Russian and Simple Wikipedias. The entire source code of the indexing software and the generated index databases are freely available under GPL (GNU General Public License).",A. A. Krizhanovsky,,https://arxiv.org/abs/0808.1753v2,https://arxiv.org/pdf/0808.1753v2,,"18 pages, 4 tables, 4 figures; FLINS'08, Corpus Linguistics'08, AIS/CAD'08; v2: table 3 changed",,,cs.IR,cs.IR|cs.CL,https://arxiv.org/pdf/0808.1753v2.pdf
0807.2491v1,2008-07-16T03:05:39Z,2008-07-16 03:05:39,A student designed experiment measuring the speed of sound as a function of altitude,"  Relatively inexpensive and readily commercially available equipment (such as digital recorders, MP3 portable speakers and tie-pin microphones), allowed a team of students from McNeese State University to measure the speed of sound in the atmosphere as a function of altitude. The experiment was carried as a payload (in the context of a NASA funded student program called La-ACES) on a high altitude balloon that reached a maximum altitude of 101,000 feet. Not withstanding substantial environmental noise, our particular experimental design allowed for the filtering of the signal out of the noise, thus achieving remarkable accuracy and precision. The speed of sound measurement was then used to set limits on the abundances of the main molecular components of the atmosphere (diatomic nitrogen and oxygen). Bayesian analysis was used to set meaningful values on the uncertainty of our limits. It is our experience that students find intutive and appealing this type of probability method.",G. Santostasi|D. Hughes|P. Maharjan|C. McAdon|N. T. Nguyesn|S. Poudel|S. Pradhan|D. Roshan|M. Wagle,,https://arxiv.org/abs/0807.2491v1,https://arxiv.org/pdf/0807.2491v1,,"9 pages, 7 figures",,,physics.ed-ph,physics.ed-ph|physics.ao-ph,https://arxiv.org/pdf/0807.2491v1.pdf
0805.2630v2,2008-05-17T22:48:22Z,2013-06-18 15:13:17,Sequential Design of Experiments via Linear Programming,"  The celebrated multi-armed bandit problem in decision theory models the basic trade-off between exploration, or learning about the state of a system, and exploitation, or utilizing the system. In this paper we study the variant of the multi-armed bandit problem where the exploration phase involves costly experiments and occurs before the exploitation phase; and where each play of an arm during the exploration phase updates a prior belief about the arm. The problem of finding an inexpensive exploration strategy to optimize a certain exploitation objective is NP-Hard even when a single play reveals all information about an arm, and all exploration steps cost the same.
  We provide the first polynomial time constant-factor approximation algorithm for this class of problems. We show that this framework also generalizes several problems of interest studied in the context of data acquisition in sensor networks. Our analyses also extends to switching and setup costs, and to concave utility objectives.
  Our solution approach is via a novel linear program rounding technique based on stochastic packing. In addition to yielding exploration policies whose performance is within a small constant factor of the adaptive optimal policy, a nice feature of this approach is that the resulting policies explore the arms sequentially without revisiting any arm. Sequentiality is a well-studied concept in decision theory, and is very desirable in domains where multiple explorations can be conducted in parallel, for instance, in the sensor network context.",Sudipto Guha|Kamesh Munagala,,https://arxiv.org/abs/0805.2630v2,https://arxiv.org/pdf/0805.2630v2,,"The results and presentation in this paper are subsumed by the article ""Approximation algorithms for Bayesian multi-armed bandit problems"" http://arxiv.org/abs/1306.3525",,,cs.DS,cs.DS,https://arxiv.org/pdf/0805.2630v2.pdf
0802.4381v1,2008-02-29T13:02:37Z,2008-02-29 13:02:37,Optimal experimental design and some related control problems,"  This paper traces the strong relations between experimental design and control, such as the use of optimal inputs to obtain precise parameter estimation in dynamical systems and the introduction of suitably designed perturbations in adaptive control. The mathematical background of optimal experimental design is briefly presented, and the role of experimental design in the asymptotic properties of estimators is emphasized. Although most of the paper concerns parametric models, some results are also presented for statistical learning and prediction with nonparametric models.",Luc Pronzato,I3S,https://arxiv.org/abs/0802.4381v1,https://arxiv.org/pdf/0802.4381v1,https://doi.org/10.1016/j.automatica.2007.05.016,Available at http://www.elsevier.com/locate/automatica,Automatica / Automatica J IFAC; Automatika; Automatica IFAC J 44 (2008) 303-325,10.1016/j.automatica.2007.05.016,math.OC,math.OC,https://arxiv.org/pdf/0802.4381v1.pdf
0802.0498v1,2008-02-04T21:01:05Z,2008-02-04 21:01:05,Experimental design and model selection: The example of exoplanet detection,"  We apply the Minimum Description Length model selection approach to the detection of extra-solar planets, and use this example to show how specification of the experimental design affects the prior distribution on the model parameter space and hence the posterior likelihood which, in turn, determines which model is regarded as most `correct'. Our analysis shows how conditioning on the experimental design can render a non-compact parameter space effectively compact, so that the MDL model selection problem becomes well-defined.",Vijay Balasubramanian|Klaus Larjo|Ravi Sheth,,https://arxiv.org/abs/0802.0498v1,https://arxiv.org/pdf/0802.0498v1,,"12 pages, 2 figures; To appear in the Festschrift for Jorma Rissanen; UPR-1187",,,astro-ph,astro-ph,https://arxiv.org/pdf/0802.0498v1.pdf
0801.0254v1,2007-12-31T23:52:47Z,2007-12-31 23:52:47,Design of experiments and biochemical network inference,  Design of experiments is a branch of statistics that aims to identify efficient procedures for planning experiments in order to optimize knowledge discovery. Network inference is a subfield of systems biology devoted to the identification of biochemical networks from experimental data. Common to both areas of research is their focus on the maximization of information gathered from experimentation. The goal of this paper is to establish a connection between these two areas coming from the common use of polynomial models and techniques from computational algebra.,Reinhard Laubenbacher|Brandilyn Stigler,,https://arxiv.org/abs/0801.0254v1,https://arxiv.org/pdf/0801.0254v1,,"To appear in ""Algebraic and geometric methods in statistics,"" P. Gibilisco, E. Riccomagno, M.-P. Rogantin, H. P. Wynn, eds., Cambridge University Press, 2008","Algebraic and Geometric Methods in Statistics. Eds: Gibilisco, Riccomagno, Rogantin, Wynn, Cambridge University Press (2008)",,q-bio.MN,q-bio.MN|stat.AP,https://arxiv.org/pdf/0801.0254v1.pdf
0711.1930v1,2007-11-13T07:55:22Z,2007-11-13 07:55:22,Bootstrap Confidence Regions for Optimal Operating Conditions in Response Surface Methodology,"  This article concerns the application of bootstrap methodology to construct a likelihood-based confidence region for operating conditions associated with the maximum of a response surface constrained to a specified region. Unlike classical methods based on the stationary point, proper interpretation of this confidence region does not depend on unknown model parameters. In addition, the methodology does not require the assumption of normally distributed errors. The approach is demonstrated for concave-down and saddle system cases in two dimensions. Simulation studies were performed to assess the coverage probability of these regions.",Roger D. Gibb|I-Li Lu|Walter H. Carter,,https://arxiv.org/abs/0711.1930v1,https://arxiv.org/pdf/0711.1930v1,,Submitted to the Electronic Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics (http://www.imstat.org),,,stat.ME,stat.ME,https://arxiv.org/pdf/0711.1930v1.pdf
0709.4323v2,2007-09-27T07:22:03Z,2008-02-04 00:37:28,Markov basis for design of experiments with three-level factors,"  We consider Markov basis arising from fractional factorial designs with three-level factors. Once we have a Markov basis, $p$ values for various conditional tests are estimated by the Markov chain Monte Carlo procedure. For designed experiments with a single count observation for each run, we formulate a generalized linear model and consider a sample space with the same sufficient statistics to the observed data. Each model is characterized by a covariate matrix, which is constructed from the main and the interaction effects we intend to measure. We investigate fractional factorial designs with $3^{p-q}$ runs noting correspondences to the models for $3^{p-q}$ contingency tables.",Satoshi Aoki|Akimichi Takemura,,https://arxiv.org/abs/0709.4323v2,https://arxiv.org/pdf/0709.4323v2,,17 pages,"Algebraic and Geometric Methods in Statistics, Cambridge University Press, 2009, 225--238",,stat.ME,stat.ME,https://arxiv.org/pdf/0709.4323v2.pdf
0709.2997v1,2007-09-19T11:50:48Z,2007-09-19 11:50:48,Two polynomial representations of experimental design,"  In the context of algebraic statistics an experimental design is described by a set of polynomials called the design ideal. This, in turn, is generated by finite sets of polynomials. Two types of generating sets are mostly used in the literature: Groebner bases and indicator functions. We briefly describe them both, how they are used in the analysis and planning of a design and how to switch between them. Examples include fractions of full factorial designs and designs for mixture experiments.",Roberto Notari|Eva Riccomagno|Maria-Piera Rogantin,,https://arxiv.org/abs/0709.2997v1,https://arxiv.org/pdf/0709.2997v1,,13 pages,"Journal of Statistical Theory and Practice, 1:3-4, 329-346 (2008)",,stat.ME,stat.ME|stat.CO,https://arxiv.org/pdf/0709.2997v1.pdf
0707.4618v1,2007-07-31T13:54:07Z,2007-07-31 13:54:07,Nonlinear Matroid Optimization and Experimental Design,"  We study the problem of optimizing nonlinear objective functions over matroids presented by oracles or explicitly. Such functions can be interpreted as the balancing of multi-criteria optimization. We provide a combinatorial polynomial time algorithm for arbitrary oracle-presented matroids, that makes repeated use of matroid intersection, and an algebraic algorithm for vectorial matroids.
  Our work is partly motivated by applications to minimum-aberration model-fitting in experimental design in statistics, which we discuss and demonstrate in detail.",Yael Berstein|Jon Lee|Hugo Maruri-Aguilar|Shmuel Onn|Eva Riccomagno|Robert Weismantel|Henry Wynn,,https://arxiv.org/abs/0707.4618v1,https://arxiv.org/pdf/0707.4618v1,,,"SIAM Journal on Discrete Mathematics, 22:901--919, 2008",,math.CO,math.CO|cs.CC|cs.DM|math.OC,https://arxiv.org/pdf/0707.4618v1.pdf
0705.1759v1,2007-05-12T10:25:22Z,2007-05-12 10:25:22,Finite Element Model Updating Using Response Surface Method,"  This paper proposes the response surface method for finite element model updating. The response surface method is implemented by approximating the finite element model surface response equation by a multi-layer perceptron. The updated parameters of the finite element model were calculated using genetic algorithm by optimizing the surface response equation. The proposed method was compared to the existing methods that use simulated annealing or genetic algorithm together with a full finite element model for finite element model updating. The proposed method was tested on an unsymmetri-cal H-shaped structure. It was observed that the proposed method gave the updated natural frequen-cies and mode shapes that were of the same order of accuracy as those given by simulated annealing and genetic algorithm. Furthermore, it was observed that the response surface method achieved these results at a computational speed that was more than 2.5 times as fast as the genetic algorithm and a full finite element model and 24 times faster than the simulated annealing.",Tshilidzi Marwala,,https://arxiv.org/abs/0705.1759v1,https://arxiv.org/pdf/0705.1759v1,,9 pages,,,cs.CE,cs.CE,https://arxiv.org/pdf/0705.1759v1.pdf
math/0611463v1,2006-11-15T12:49:29Z,2006-11-15 12:49:29,Markov chain Monte Carlo tests for designed experiments,"  We consider conditional exact tests of factor effects in designed experiments for discrete response variables. Similarly to the analysis of contingency tables, a Markov chain Monte Carlo method can be used for performing exact tests, when large-sample approximations are poor and the enumeration of the conditional sample space is infeasible. For designed experiments with a single observation for each run, we formulate log-linear or logistic models and consider a connected Markov chain over an appropriate sample space. In particular, we investigate fractional factorial designs with $2^{p-q}$ runs, noting correspondences to the models for $2^{p-q}$ contingency tables.",Satoshi Aoki|Akimichi Takemura,,https://arxiv.org/abs/math/0611463v1,https://arxiv.org/pdf/math/0611463v1,https://doi.org/10.1016/j.jspi.2009.09.010,,"Journal of Statistical Planning and Inference, 140 (2010), 817-830",10.1016/j.jspi.2009.09.010,math.ST,math.ST,https://arxiv.org/pdf/math/0611463v1.pdf
math/0611192v1,2006-11-07T15:11:57Z,2006-11-07 15:11:57,Regression tree models for designed experiments,"  Although regression trees were originally designed for large datasets, they can profitably be used on small datasets as well, including those from replicated or unreplicated complete factorial experiments. We show that in the latter situations, regression tree models can provide simpler and more intuitive interpretations of interaction effects as differences between conditional main effects. We present simulation results to verify that the models can yield lower prediction mean squared errors than the traditional techniques. The tree models span a wide range of sophistication, from piecewise constant to piecewise simple and multiple linear, and from least squares to Poisson and logistic regression.",Wei-Yin Loh,,https://arxiv.org/abs/math/0611192v1,https://arxiv.org/pdf/math/0611192v1,https://doi.org/10.1214/074921706000000464,Published at http://dx.doi.org/10.1214/074921706000000464 in the IMS Lecture Notes--Monograph Series (http://www.imstat.org/publications/lecnotes.htm) by the Institute of Mathematical Statistics (http://www.imstat.org),"IMS Lecture Notes--Monograph Series 2006, Vol. 49, 210-228",10.1214/074921706000000464,math.ST,math.ST,https://arxiv.org/pdf/math/0611192v1.pdf
cond-mat/0610507v1,2006-10-18T11:40:38Z,2006-10-18 11:40:38,Experimental Designs for Binary Data in Switching Measurements on Superconducting Josephson Junctions,"  We study the optimal design of switching measurements of small Josephson junction circuits which operate in the macroscopic quantum tunnelling regime. Starting from the D-optimality criterion we derive the optimal design for the estimation of the unknown parameters of the underlying Gumbel type distribution. As a practical method for the measurements, we propose a sequential design that combines heuristic search for initial estimates and maximum likelihood estimation. The presented design has immediate applications in the area of superconducting electronics implying faster data acquisition. The presented experimental results confirm the usefulness of the method. KEY WORDS: optimal design, D-optimality, logistic regression, complementary log-log link, quantum physics, escape measurements",Juha Karvanen|Juha J. Vartiainen|Andrey Timofeev|Jukka Pekola,,https://arxiv.org/abs/cond-mat/0610507v1,https://arxiv.org/pdf/cond-mat/0610507v1,https://doi.org/10.1111/j.1467-9876.2007.00572.x,,"Journal of the Royal Statistical Society: Series C (Applied Statistics) 2007, Vol. 56, 167-181",10.1111/j.1467-9876.2007.00572.x,cond-mat.supr-con,cond-mat.supr-con|physics.data-an|stat.AP,https://arxiv.org/pdf/cond-mat/0610507v1.pdf
q-bio/0610024v1,2006-10-12T21:50:34Z,2006-10-12 21:50:34,Optimal experimental design in an EGFR signaling and down-regulation model,"  We apply the methods of optimal experimental design to a differential equation model for epidermal growth factor receptor (EGFR) signaling, trafficking, and down-regulation. The model incorporates the role of a recently discovered protein complex made up of the E3 ubiquitin ligase, Cbl, the guanine exchange factor (GEF), Cool-1 (Beta-Pix), and the Rho family G protein Cdc42. The complex has been suggested to be important in disrupting receptor down-regulation. We demonstrate that the model interactions can accurately reproduce the experimental observations, that they can be used to make predictions with accompanying uncertainties, and that we can apply ideas of optimal experimental design to suggest new experiments that reduce the uncertainty on unmeasurable components of the system.",Fergal P. Casey|Dan Baird|Qiyu Feng|Ryan N. Gutenkunst|Joshua J. Waterfall|Christopher R. Myers|Kevin S. Brown|Richard A. Cerione|James P. Sethna,,https://arxiv.org/abs/q-bio/0610024v1,https://arxiv.org/pdf/q-bio/0610024v1,,"13 pages, 9 figures in main text. 7 pages, 7 figures in supplementary material. Submitted to IEE Proceedings Systems Biology",,,q-bio.MN,q-bio.MN|q-bio.QM,https://arxiv.org/pdf/q-bio/0610024v1.pdf
math/0609028v1,2006-09-01T13:19:17Z,2006-09-01 13:19:17,Some control design experiments with HIFOO,  A new MATLAB package called HIFOO was recently proposed for H-infinity fixed-order controller design. This document illustrates how some standard controller design examples can be solved with this software.,Didier Henrion,"LAAS, Fel-Cvut",https://arxiv.org/abs/math/0609028v1,https://arxiv.org/pdf/math/0609028v1,,,,,math.OC,math.OC,https://arxiv.org/pdf/math/0609028v1.pdf
math/0603079v1,2006-03-03T11:34:41Z,2006-03-03 11:34:41,Construction of optimal multi-level supersaturated designs,"  A supersaturated design is a design whose run size is not large enough for estimating all the main effects. The goodness of multi-level supersaturated designs can be judged by the generalized minimum aberration criterion proposed by Xu and Wu [Ann. Statist. 29 (2001) 1066--1077]. A new lower bound is derived and general construction methods are proposed for multi-level supersaturated designs. Inspired by the Addelman--Kempthorne construction of orthogonal arrays, several classes of optimal multi-level supersaturated designs are given in explicit form: Columns are labeled with linear or quadratic polynomials and rows are points over a finite field. Additive characters are used to study the properties of resulting designs. Some small optimal supersaturated designs of 3, 4 and 5 levels are listed with their properties.",Hongquan Xu|C. F. J. Wu,,https://arxiv.org/abs/math/0603079v1,https://arxiv.org/pdf/math/0603079v1,https://doi.org/10.1214/009053605000000688,Published at http://dx.doi.org/10.1214/009053605000000688 in the Annals of Statistics (http://www.imstat.org/aos/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Statistics 2005, Vol. 33, No. 6, 2811-2836",10.1214/009053605000000688,math.ST,math.ST,https://arxiv.org/pdf/math/0603079v1.pdf
nucl-th/0508034v1,2005-08-18T12:45:45Z,2005-08-18 12:45:45,The Piecewise Moments Method: A Generalized Lanczos Technique for Nuclear Response Surfaces,"  For some years Lanczos moments methods have been combined with large-scale shell-model calculations in evaluations of the spectral distributions of certain operators. This technique is of great value because the alternative, a state-by-state summation over final states, is generally not feasible. The most celebrated application is to the Gamow-Teller operator, which governs beta decay and neutrino reactions in the allowed limit. The Lanczos procedure determines the nuclear response along a line q=0 in the (omega,q) plane, where omega and q are the energy and three-momentum transfered to the nucleus. However, generalizing such treatments from the allowed limit to general electroweak response functions at arbitrary momentum transfers seems considerably more difficult: the response function must be determined over the entire (omega,q) plane for an operator O(q) that is not fixed, but depends explicitly on q. Such operators arise in any semileptonic process where the momentum transfer is comparable to (or larger than) the inverse nuclear size. Here we show, for Slater determinants built on harmonic oscillator basis functions, that the nuclear response for any multipole operator O(q) can be determined efficiently over the full response plane by a generalization of the standard Lanczos moments method. We describe the Piecewise Moments Method and thoroughly explore its convergence properties for the test case of electromagnetic responses in a full sd-shell calculation of 28Si. We discuss possible extensions to a variety of electroweak processes, including charged- and neutral-current neutrino scattering.",Wick C. Haxton|Kenneth M. Nollett|Kathryn M. Zurek,,https://arxiv.org/abs/nucl-th/0508034v1,https://arxiv.org/pdf/nucl-th/0508034v1,https://doi.org/10.1103/PhysRevC.72.065501,"17 pages, 10 figures, to be submitted to Phys. Rev. C",Phys.Rev. C72 (2005) 065501,10.1103/PhysRevC.72.065501,nucl-th,nucl-th,https://arxiv.org/pdf/nucl-th/0508034v1.pdf
quant-ph/0507043v1,2005-07-05T05:10:28Z,2005-07-05 05:10:28,Quantum stream cipher by Yuen 2000 protocol: Design and experiment by intensity modulation scheme,"  This paper shall investigate Yuen protocol, so called Y-00, which can realize a randomized stream cipher with high bit rate(Gbps) for long distance(several hundreds km). The randomized stream cipher with randomization by quantum noise based on Y-00 is called quantum stream cipher in this paper, and it may have security against known plaintext attacks which has no analog with any conventional symmetric key ciphers. We present a simple cryptanalysis based on an attacker's heterodyne measurement and the quantum unambiguous measurement to make clear the strength of Y-00 in real communication. In addition, we give a design for the implementation of an intensity modulation scheme and report the experimental demonstration of 1 Gbps quantum stream cipher through 20 km long transmission line.",Osamu Hirota|Masaki Sohma|Masaru Fuse|Kentaro Kato,,https://arxiv.org/abs/quant-ph/0507043v1,https://arxiv.org/pdf/quant-ph/0507043v1,https://doi.org/10.1103/PhysRevA.72.022335,This paper will appear in Phys. Rev. A,,10.1103/PhysRevA.72.022335,quant-ph,quant-ph,https://arxiv.org/pdf/quant-ph/0507043v1.pdf
gr-qc/0410044v1,2004-10-08T19:37:21Z,2004-10-08 19:37:21,Experimental Design for the LATOR Mission,"  This paper discusses experimental design for the Laser Astrometric Test Of Relativity (LATOR) mission. LATOR is designed to reach unprecedented accuracy of 1 part in 10^8 in measuring the curvature of the solar gravitational field as given by the value of the key Eddington post-Newtonian parameter γ. This mission will demonstrate the accuracy needed to measure effects of the next post-Newtonian order (~G^2) of light deflection resulting from gravity's intrinsic non-linearity. LATOR will provide the first precise measurement of the solar quadrupole moment parameter, J2, and will improve determination of a variety of relativistic effects including Lense-Thirring precession. The mission will benefit from the recent progress in the optical communication technologies -- the immediate and natural step above the standard radio-metric techniques. The key element of LATOR is a geometric redundancy provided by the laser ranging and long-baseline optical interferometry. We discuss the mission and optical designs, as well as the expected performance of this proposed mission. LATOR will lead to very robust advances in the tests of Fundamental physics: this mission could discover a violation or extension of general relativity, or reveal the presence of an additional long range interaction in the physical law. There are no analogs to the LATOR experiment; it is unique and is a natural culmination of solar system gravity experiments.",Slava G. Turyshev|Michael Shao|Kenneth L. Nordtvedt,,https://arxiv.org/abs/gr-qc/0410044v1,https://arxiv.org/pdf/gr-qc/0410044v1,https://doi.org/10.1142/S0218271804006528,"16 pages, 17 figures, invited talk given at ``The 2004 NASA/JPL Workshop on Physics for Planetary Exploration.'' April 20-22, 2004, Solvang, CA",Int.J.Mod.Phys. D13 (2004) 2035-2064,10.1142/S0218271804006528,gr-qc,gr-qc|astro-ph|physics.space-ph,https://arxiv.org/pdf/gr-qc/0410044v1.pdf
math/0410090v1,2004-10-05T10:22:49Z,2004-10-05 10:22:49,Construction of E(s^2)-optimal supersaturated designs,"  Booth and Cox proposed the E(s^2) criterion for constructing two-level supersaturated designs. Nguyen [Technometrics 38 (1996) 69-73] and Tang and Wu [Canad. J. Statist 25 (1997) 191-201] independently derived a lower bound for E(s^2). This lower bound can be achieved only when m is a multiple of N-1, where m is the number of factors and N is the run size. We present a method that uses difference families to construct designs that satisfy this lower bound. We also derive better lower bounds for the case where the Nguyen-Tang-Wu bound is not achievable. Our bounds cover more cases than a bound recently obtained by Butler, Mead, Eskridge and Gilmour [J.
  R. Stat. Soc. Ser. B Stat. Methodol. 63 (2001) 621-632]. New E(s^2)-optimal designs are obtained by using a computer to search for designs that achieve the improved bounds.",Dursun A. Bulutoglu|Ching-Shui Cheng,,https://arxiv.org/abs/math/0410090v1,https://arxiv.org/pdf/math/0410090v1,https://doi.org/10.1214/009053604000000472,Published by the Institute of Mathematical Statistics (http://www.imstat.org) in the Annals of Statistics (http://www.imstat.org/aos/) at http://dx.doi.org/10.1214/009053604000000472,"Annals of Statistics 2004, Vol. 32, No. 4, 1662-1678",10.1214/009053604000000472,math.ST,math.ST,https://arxiv.org/pdf/math/0410090v1.pdf
astro-ph/0305146v1,2003-05-09T02:07:32Z,2003-05-09 02:07:32,A Laboratory Investigation of Supersonic Clumpy Flows: Experimental Design and Theoretical Analysis,"  We present a design for high energy density laboratory experiments studying the interaction of hypersonic shocks with a large number of inhomogeneities. These ``clumpy'' flows are relevant to a wide variety of astrophysical environments including the evolution of molecular clouds, outflows from young stars, Planetary Nebulae and Active Galactic Nuclei. The experiment consists of a strong shock (driven by a pulsed power machine or a high intensity laser) impinging on a region of randomly placed plastic rods. We discuss the goals of the specific design and how they are met by specific choices of target components. An adaptive mesh refinement hydrodynamic code is used to analyze the design and establish a predictive baseline for the experiments. The simulations confirm the effectiveness of the design in terms of articulating the differences between shocks propagating through smooth and clumpy environments. In particular, we find significant differences between the shock propagation speeds in a clumpy medium compared to a smooth one with the same average density. The simulation results are of general interest for foams in both inertial confinement fusion and laboratory astrophysics studies. Our results highlight the danger of using average properties of inhomogeneous astrophysical environments when comparing timescales for critical processes such as shock crossing and gravitational collapse times.",A. Y. Poludnenko|K. K. Dannenberg|R. P. Drake|A. Frank|J. Knauer|D. D. Meyerhofer|M. Furnish|J. R. Asay,"Univ. of Rochester, Labor. for Laser Energetics|Univ. of Michigan;|Univ. of Michigan;|Univ. of Rochester, Labor. for Laser Energetics|LLE;|LLE;|Sandia Natl. Laboratories;|Washington State Univ",https://arxiv.org/abs/astro-ph/0305146v1,https://arxiv.org/pdf/astro-ph/0305146v1,https://doi.org/10.1086/381792,"7 pages, 6 figures. Submitted to the Astrophysical Journal. For additional information, including simulation animations and the pdf and ps files of the paper with embedded high-quality images, see http://pas.rochester.edu/~wma",Astrophys.J. 604 (2004) 213-221,10.1086/381792,astro-ph,astro-ph|physics.flu-dyn|physics.plasm-ph,https://arxiv.org/pdf/astro-ph/0305146v1.pdf
physics/0204068v1,2002-04-23T21:10:16Z,2002-04-23 21:10:16,Inductive Logic: From Data Analysis to Experimental Design,"  In celebration of the work of Richard Threlkeld Cox, we explore inductive logic and its role in science touching on both experimental design and analysis of experimental results. In this exploration we demonstrate that the duality between the logic of assertions and the logic of questions has important consequences. We discuss the conjecture that the relevance or bearing, b, of a question on an issue can be expressed in terms of the probabilities, p, of the assertions that answer the question via the entropy.
  In its application to the scientific method, the logic of questions, inductive inquiry, can be applied to design an experiment that most effectively addresses a scientific issue. This is performed by maximizing the relevance of the experimental question to the scientific issue to be resolved. It is shown that these results are related to the mutual information between the experiment and the scientific issue, and that experimental design is akin to designing a communication channel that most efficiently communicates information relevant to the scientific issue to the experimenter. Application of the logic of assertions, inductive inference (Bayesian inference) completes the experimental process by allowing the researcher to make inferences based on the information obtained from the experiment.",Kevin H. Knuth,,https://arxiv.org/abs/physics/0204068v1,https://arxiv.org/pdf/physics/0204068v1,https://doi.org/10.1063/1.1477061,"13 Pages, 2 Figures, Presented at MaxEnt2001, APL Johns Hopkins University, August 4-9 2001",,10.1063/1.1477061,physics.data-an,physics.data-an|physics.ins-det,https://arxiv.org/pdf/physics/0204068v1.pdf
