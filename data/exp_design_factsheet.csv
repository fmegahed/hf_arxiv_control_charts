is_exp_design_paper,design_type,design_objective,optimality_criterion,number_of_factors,application_domain,evaluation_type,code_used,software_platform,code_availability_source,software_urls,summary,key_equations,key_results,limitations_stated,limitations_unstated,future_work_stated,future_work_unstated,id,pdf_url,pdf_path,llm_provider,llm_model,repeat_id,extracted_at
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,0705.1759v1,https://arxiv.org/pdf/0705.1759v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:50:11Z
TRUE,Optimal design|Other,Model discrimination|Parameter estimation|Other,Not applicable,Variable/General (experimental-design application uses k factors; algorithms assume fixed dimension d/k in complexity results),Theoretical/simulation only|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops polynomial-time algorithms for minimizing an arbitrary nonlinear objective over matroid bases (nonlinear matroid optimization), using a comparison oracle for the objective and either an independence oracle (general matroids) or an explicit matrix representation (vector matroids). It gives (i) a combinatorial algorithm based on repeated matroid intersection when the number of distinct weight values is fixed, and (ii) a more efficient algebraic algorithm for vectorial matroids using determinant identities and multivariate interpolation to recover all achievable weight-profiles of bases. Experimental design enters as an application: selecting an identifiable set of monomials (a polynomial model) for a fixed set of design points is formulated as choosing a base of a vector matroid that minimizes an ‘aberration’ function of average (or weighted) monomial degrees. The work thus supports minimum-aberration model fitting for multivariate polynomial regression under prescribed design points, reducing it to matroid-base optimization rather than proposing classical run-order/factorial/RSM templates. No empirical study or software is provided; results are algorithmic and complexity-theoretic, with illustrative formulations (e.g., minimum-norm spanning tree; minimum-aberration model fitting).","Nonlinear matroid optimization is posed as selecting a base $B\in\mathcal{B}(M)$ to minimize $f(W(B))$ where $W(B)=(w_1(B),\ldots,w_d(B))$ and $w_i(B)=\sum_{j\in B} w_{i,j}$. In the vector-matroid algorithm, achievable profiles are identified via coefficients $g_u$ of the polynomial $g(y)=\sum_{u\in Z} g_u y^u$ with $g_u=\sum_{B\in\mathcal{B}(M):\,W(B)=u} \det(A_B)^2$, and evaluated by the identity $g(y)=\det(A Y A^T)$ where $Y=\mathrm{diag}_j\big(\prod_{i=1}^d y_i^{w_{i,j}}\big)$; coefficients are recovered by interpolation. For experimental design, aberration is $A(B)=f\big(\frac{1}{|B|}\sum_{\alpha\in B}\alpha\big)$ (or generalized $A(B)=f(W(B))$), and identifiability is encoded by invertibility of the model matrix built from design points and monomials.","For fixed dimension $d$ (and fixed number of distinct weight values $p$ in the oracle-matroid case), the paper proves existence of polynomial-time algorithms to find a matroid base minimizing an arbitrary nonlinear objective given only by a comparison oracle (Theorems 1.1 and 1.3). It also establishes intractability boundaries: variable dimension $d=n$ or binary-encoded weights yield problems requiring examination of all bases / NP-completeness even for uniform/graphic-related matroids (Propositions 2.3–2.5). In experimental design, it yields Corollary 1.5: for fixed number of factors $k$, a minimum-aberration polynomial model identifiable by a given rational design can be computed in time polynomial in design size and maximum exponent magnitude (under the stated encodings/assumptions). The vector-matroid approach computes all achievable weight-profiles by determinant evaluation and solves the aberration minimization by scanning those profiles with oracle comparisons. No numerical ARL-style performance metrics apply; results are complexity/algorithmic.","The paper notes that the general-matroid combinatorial algorithm is “quite heavy,” invoking matroid intersection roughly $n^{p^d}$ times, and thus may be impractical despite polynomial-time guarantees; it motivates a more efficient algebraic method for vectorial matroids. It also emphasizes that tractability depends strongly on fixed $d$ (and fixed $p$ in the oracle case) and on unary vs. binary weight encoding, with several explicit intractability results outside these regimes.","The experimental-design contribution is primarily a reduction and computational guarantee for minimum-aberration model selection under a fixed set of design points, not a method for constructing or optimizing the design points themselves (the ‘inverse problem’ is deferred). The approach targets polynomial (monomial) model selection and identifiability via matrix invertibility; it does not address statistical issues like noise, lack of fit, regularization, replication, or optimality criteria based on variance (e.g., D-/A-/I-optimality) under stochastic errors. Practical scalability may still be limited because the vector-matroid algorithm’s interpolation step scales with $(mq+1)^d$ (pseudo-polynomial in max weight) and relies on potentially large-integer determinant computations, which can be heavy for large designs/exponent bounds. No implementation guidance, benchmarks, or empirical case studies are provided to demonstrate runtime on realistic DOE instances.","The paper states that computational aspects of choosing the design points themselves (the inverse problem in experimental design) are of interest and “will be considered elsewhere.” It also positions the vector-matroid algorithm as preferable for practical matroids arising in applications, suggesting further use of the developed methods as black boxes for such application settings.","Extending the DOE application from deterministic interpolation/identifiability to noisy-response settings (e.g., least squares model selection with variance-based criteria) would make the method more directly usable in practice. Developing specialized, more scalable algorithms for the minimum-aberration model-fitting instance structure (beyond general matroid machinery), plus producing open-source implementations and benchmarks on standard DOE problems, would strengthen practical impact. Additional work could explore integration with classical optimal design criteria (D-, A-, I-optimal) and constraints (blocking/split-plot/randomization restrictions) by mapping them to matroid/greedoid or related combinatorial structures where possible.",0707.4618v1,https://arxiv.org/pdf/0707.4618v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:50:45Z
TRUE,Factorial (fractional)|Mixture|Screening|Other,Parameter estimation|Screening|Model discrimination|Other,Not applicable,"Variable/General (examples include 2 factors, 3 factors, 4 factors, 5 factors, and a 9-component mixture screening design)",Theoretical/simulation only|Other,Other,TRUE,Other,In text/Appendix,http://cocoa.dima.unige.it,"The paper develops and compares two algebraic (polynomial) representations of an experimental design via its design ideal: (i) reduced Gröbner bases (“Gröbner representation”) and (ii) indicator-function/separator polynomials (“indicator representation”). It explains how each representation can be derived from design points and used to analyze key design properties such as aliasing/confounding, orthogonality, regularity, strength of orthogonal arrays, and identifiable (saturated) regression models via quotient-ring bases. A main contribution is an efficient algorithm to switch from a Gröbner-basis representation to an indicator-function representation using linear algebra and normal forms, avoiding expensive lexicographic Gröbner computations. The methods cover both fractional factorial designs (including complex root-of-unity coding) and mixture designs through homogeneous/projective constructions using the design cone and separator functions. Implementations and examples are provided, including a large fractional simplex-centroid mixture screening design from the chemical literature.","The design ideal is defined as $I(F)=\{f\in k[x_1,\ldots,x_m]: f(\zeta)=0\ \forall \zeta\in F\}$. For a fraction $F\subset D$, the indicator function is $\mathcal F(\zeta)=1$ if $\zeta\in F$ and $0$ otherwise, represented by a polynomial on $D$; with root-of-unity coding its coefficients are $b_\alpha=\frac{1}{\#D}\sum_{\zeta\in F} x^\alpha(\zeta)$. For mixture designs, separator/indicator functions are ratios of homogeneous polynomials of the same degree, e.g. $S_F(\mathbf x)/(\sum_i x_i)^s$, to define functions on the affine cone.","The paper provides an explicit, more efficient switching algorithm from Gröbner generators of $I(F)$ to an indicator polynomial by selecting $N-n$ derived polynomials tied to the monomials in $\mathrm{Est}_D\setminus \mathrm{Est}_F$, then solving a uniquely solvable linear system obtained from normal forms modulo $I(D\setminus F)$. It shows how Gröbner bases yield identifiable hierarchical model supports via standard monomials (Gbasis/LT), and how indicator-function coefficients encode regularity/orthogonality/strength properties for fractional factorials under complex coding. For a 9-component fractional simplex-centroid screening design, the authors report computing indicator/separator functions in seconds in Maple; the indicator in the full simplex-centroid design has 444 terms, while in a smaller superset design it has 70 terms and the separator has 165 terms.","The authors note that replicated design points can be considered but introduce technical issues (multiplicities and non-uniqueness of ideals) that are outside the scope of the paper. They also note that many indicator-function properties depend intrinsically on factor-level coding and that complex coding is needed for some results, and that obtaining point coordinates from generating sets may require solving polynomial systems.","The work is largely algebraic and does not provide a systematic empirical comparison of computational complexity and scalability across modern Gröbner/linear-algebra implementations beyond a few timing anecdotes. Practical guidance is limited on how to choose term orderings, supersets $D$, or degrees $s$ to optimize interpretability and computation for real experimental planning. The methodology focuses on exact design-point sets and does not address common applied complications (noise, missing runs, randomization restrictions, process constraints beyond mixture/simplex constraints, or model-robustness/equivalence-theorem style optimality).",None stated.,"Developing software packages (e.g., in R/Python) that automate the switching algorithms and model-basis extraction, with benchmarks on large modern designs, would improve accessibility and adoption. Extending the framework to handle replicated points/multiplicities systematically and to incorporate randomization restrictions (e.g., split-plot/blocked structures) within the ideal/quotient-ring approach would broaden applicability. Robustness studies for non-ideal conditions (measurement error, missing runs, and approximate design points) and integration with optimal-design criteria (e.g., D-/I-optimality over polynomial models using algebraic representations) are natural next steps.",0709.2997v1,https://arxiv.org/pdf/0709.2997v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:51:11Z
TRUE,Factorial (fractional)|Other,Parameter estimation|Model discrimination|Other,Not applicable,"Variable/General (three-level factors; examples include 3 factors (3^(3-1)), 4 factors (3^(4-1)), and 5 factors (3^(5-2)))",Semiconductor/electronics|Theoretical/simulation only,Other,TRUE,Other,Not provided,www.4ti2.de,"The paper develops and studies Markov bases induced by three-level fractional factorial designs (3^(p−q) runs) to enable conditional exact tests for main and interaction effects when the response is a single count per run. The approach formulates a Poisson generalized linear model where each null hypothesis corresponds to a covariate matrix X encoding selected main/interaction contrasts; the conditional sample space is the fiber F(X'y0) of all nonnegative integer count vectors sharing the same sufficient statistics. The main methodological contribution is characterizing and computing Markov bases (including degree-2 and degree-3 moves and “three-element fibers”) for practically important three-level fractional factorial designs (e.g., 3^(4−1) resolution IV and two 3^(5−2) designs) and relating these models to (generally non-hierarchical) models for 3×3×3 contingency tables. Using computed Markov bases, the paper supports Markov chain Monte Carlo (Metropolis–Hastings) estimation of p-values for conditional tests where large-sample chi-square approximations may be unreliable. It also discusses connections between classical aliasing notation, design ideals/Gröbner bases, and toric ideals/Markov bases, emphasizing that Markov bases depend on the chosen statistical model (X) rather than only on the design points.","Counts y_i are modeled as independent Poisson with canonical link log(μ_i)=β_0+β_1 x_{i1}+⋯+β_{ν} x_{iν}, with sufficient statistics X' y where X is the covariate matrix built from main/interaction contrasts. Conditional testing is performed on the fiber F(X'y_0)={y: X'y=X'y_0, y_i∈ℕ}, and MCMC uses Markov basis moves from the kernel of X to connect all tables in the fiber. Three-level interaction components are defined via mod-3 contrasts such as AB: a+b (mod 3) and AB^2: a+2b (mod 3), and aliasing relations for fractional designs are expressed via mod-3 equations (e.g., D=ABC ⇔ ABCD^2=I).","For the 3^(4−1) resolution IV design defined by D=ABC, the main-effects model has a minimal Markov basis consisting of 54 degree-2 moves and 24 degree-3 moves, with degree-2 and degree-3 moves connecting distinct three-element fibers. Adding interaction terms changes the Markov basis substantially: e.g., for main effects + A×B, a minimal basis has 27 indispensable degree-2 moves plus 54 degree-3 moves; for main effects + A×B + A×C + B×C, there is a unique minimal Markov basis with 27 indispensable degree-6 moves and 27 indispensable degree-8 moves. For the 3^(5−2) design D=AB, E=AB^2 C, the main-effects model yields a minimal Markov basis with mixed degrees (including indispensable degree-2/4/6 and dispensable degree-3 moves), while several augmented models have unique minimal Markov bases dominated by indispensable higher-degree moves. Computations were performed with 4ti2, and the paper highlights the frequent emergence of three-element fibers and degree-3 moves as a distinctive feature of three-level fractional factorial settings.","The authors note that the connection between the Markov basis approach (toric ideals driven by the model matrix X) and the Gröbner basis/design-ideal approach (driven by design points and term order) is “not yet very well developed,” and call for further study of a closer relationship. They also remark that their discussion mainly targets fractional factorial designs and that extensions to other design families (e.g., Plackett–Burman and balanced incomplete block designs) are not addressed in the paper.","The work largely reports computed Markov bases for specific important designs and models; it does not provide a general constructive characterization or scalable algorithm for Markov bases for arbitrary 3-level fractional factorial designs as p grows. Practical guidance on MCMC implementation (mixing, burn-in, diagnostics, computational cost) and how basis degree affects convergence is limited, even though bases can contain high-degree moves. The modeling assumptions emphasize independent Poisson counts (and briefly logistic/binomial extensions), but robustness to overdispersion, dependence, or model misspecification—common in industrial count data—is not analyzed.","The paper suggests clarifying and deepening the relationship between design ideals/Gröbner bases (as in Pistone, Riccomagno and Wynn) and Markov bases/toric ideals, noting that Markov bases depend on the modeled effects encoded in X whereas Gröbner bases depend only on design points and term order. It also proposes extending the framework beyond fractional factorial designs to other design classes such as Plackett–Burman designs and balanced incomplete block designs.","Develop general, design-theoretic conditions (beyond case-by-case computation) predicting when three-element fibers and degree-3 (or higher) indispensable moves must occur in 3-level fractional factorial designs. Provide systematic MCMC guidance and diagnostics tailored to high-degree Markov moves (e.g., adaptive proposals, comparison of generating sets, mixing-time studies) and release reproducible code/scripts for the reported 4ti2 computations. Extend the conditional-testing framework to more realistic industrial count models (overdispersed/negative binomial, random effects, autocorrelation) and to multistratum/split-plot and mixed-level designs, where aliasing and constraints become more complex.",0709.4323v2,https://arxiv.org/pdf/0709.4323v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:51:39Z
TRUE,Response surface,Optimization,Not applicable,2 factors (k=2 in demonstrations); general k discussed,Theoretical/simulation only,Simulation study,TRUE,SAS,Not provided,NA,"The paper proposes a likelihood-based bootstrap (percentile) approach to construct confidence regions for the operating conditions that maximize a response surface within a bounded experimental region (the constrained maximizer x_cm), rather than for the stationary point. The method uses residual bootstrap from a fitted response surface model, recomputes the constrained maximizer for each bootstrap fit, and then forms a highest-density (smallest content) region by estimating the conditional density of the bootstrap maximizers via boundary-corrected kernel density estimation. This avoids reliance on unknown second-order surface curvature for interpretation and does not require normally distributed errors (assuming exchangeable errors for residual bootstrap). The approach is demonstrated for two-dimensional second-order response surfaces under concave-down and saddle cases, and Monte Carlo simulation studies assess empirical coverage under different bandwidth selectors, bootstrap sizes, and sample sizes. Simulations use a rotatable central composite design with center runs (n=13 baseline) and show coverage improves toward nominal as sample size increases.","The response surface is approximated by a second-order polynomial $g(x) \approx \beta_0 + x\beta + x'Bx$, with stationary point $x_{sp} = -\tfrac{1}{2}B^{-1}\beta$ (used only for discussion/contrast). Bootstrap samples are generated by resampling standardized residuals and refitting $y^* = X\hat\theta + \hat\varepsilon_s^*$, then recomputing the constrained maximizer $\hat x_{cm}^*$ via numerical optimization. The confidence region is the highest-density region of a kernel density estimate $\hat f(x)=\frac{1}{b h_1 h_2}\sum_{i=1}^b\prod_{j=1}^2 K\big((x_j-\hat x_{cm,i,j}^*)/h_j\big)$, taking the smallest-content contour capturing $(1-\alpha)b$ bootstrap maximizers.","In simulations of 500 experiments using a rotatable central composite design with 5 center runs (n=13) and Gaussian noise $N(0,3^2)$, confidence regions were built with b=2000 bootstrap samples. Coverage probabilities under the Normal rule-of-thumb and Wand–Jones plug-in bandwidth selectors were statistically indistinguishable for both the concave-down and saddle response surfaces; plug-in bandwidths were slightly larger on average (e.g., concave-down: $h_1$ 0.214 vs 0.196; $h_2$ 0.233 vs 0.213). Increasing the number of bootstrap samples from 2000 to 4000 or 6000 did not materially improve coverage. Coverage approached nominal levels as sample size increased (evaluated via replications of the same central composite design, including n=26 and n=208).","The authors note that proper interpretation assumes the constrained maximizer $x_{cm}$ is unique; if the maximum is not unique, $\hat x_{cm}$ may not be consistent and further model investigation/tests may be needed. They also state residual bootstrap requires exchangeable errors and that for nonlinear regression models a direct analogue of standardized residuals is generally not available. Bandwidth selection for kernel density estimation can be challenging in cases like bimodality or when bootstrap maximizers lie on different boundaries, potentially requiring more robust scale estimators or variable bandwidth methods.","The method’s performance depends on accurately solving a constrained numerical optimization problem for each bootstrap replicate; convergence issues or multiple local maxima could materially affect the estimated distribution of $\hat x_{cm}^*$, especially in higher dimensions, but this is not systematically studied. The simulations focus on k=2 and a rectangular region; kernel density estimation and highest-density region construction become harder and less reliable as dimension grows (curse of dimensionality), and practical guidance for k>3 is limited. The work does not provide readily reproducible implementation details (e.g., exact kernel/bandwidth settings, optimization tolerances) or shared code, which makes independent replication and sensitivity analysis difficult.","They discuss improving finite-sample accuracy via bootstrap calibration (Loh’s calibration) to better match desired coverage probabilities. They also suggest that more challenging density shapes (e.g., bimodality) may need robust scale estimators for bandwidth selection, and boundary-mass patterns may call for variable kernel density estimators. Extensions to higher-dimensional factor spaces and to experimental regions of arbitrary shape via multivariate boundary kernels are discussed, as well as extending the method to multiple-response optimization using desirability functions and providing confidence regions for the desirability-optimal operating condition.","Developing a self-contained, implementable algorithm for constrained maximization that is robust to multiple local optima (e.g., multi-start strategies) and studying its impact on coverage would strengthen practical use. A systematic study under non-exchangeable errors (autocorrelation, heteroscedasticity) and with estimated model parameters from Phase I-type data would clarify robustness. For k>2, replacing kernel-density HDR regions with alternative region constructions (e.g., convex hull/alpha-shapes, level sets from Gaussian mixtures, or Bayesian posterior credible regions) could mitigate dimensionality issues and boundary complications. Packaging the method in open-source software with reproducible defaults and diagnostics (uniqueness checks, bandwidth sensitivity, optimization diagnostics) would improve adoption.",0711.1930v1,https://arxiv.org/pdf/0711.1930v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:52:17Z
TRUE,Factorial (full)|Factorial (fractional)|Other,Parameter estimation|Screening|Model discrimination|Prediction|Cost reduction|Other,Not applicable,"Variable/General (n factors/variables; examples include n=3 genes; discussion emphasizes n large, r small)",Healthcare/medical|Other,Other,TRUE,None / Not applicable,Not provided,NA,"The paper connects classical design of experiments (DOE)—especially full and fractional factorial designs—to biochemical network inference by showing both problems can be formulated using polynomial models and ideals of points in a polynomial ring. For DOE, a fraction (subset) of a full factorial design is encoded via the vanishing ideal I(F), and Gröbner bases are used to characterize which polynomial models are identifiable and how confounding arises. A key identifiability condition is stated: a polynomial (linear-in-parameters) model is uniquely identifiable from a set of design points if the associated model matrix has full rank. For biochemical network inference, discrete-time state transitions are modeled as polynomial dynamical systems over a finite field; all models fitting the data form an affine space f + I(p1,…,pr), analogous to the DOE quotient-space view. The authors emphasize the experimental-design challenge in biology where measurements are costly (r ≪ n) and many states/perturbations are infeasible, and they discuss dependence of inferred models on term order (Gröbner basis choice), motivating term-order–invariant or aggregated approaches.","Fractional design points F={p1,…,pr} are encoded by the vanishing ideal I(F)={g∈k[x1,…,xn]: g(pi)=0 ∀i}. Identifiability uses the model/design matrix X(S,X) with entries (i,j)=Tj(pi) for monomial support S={T1,…,Tt}; the model f=∑j aj Tj is uniquely identifiable iff X(S,X) has full rank. In network inference, polynomial dynamical systems F=(f1,…,fn): k^n→k^n must satisfy F(pi)=qi, and the set of all feasible transition functions for a node is f + I(p1,…,pr).","The paper’s principal formal result is the full-rank criterion (Theorem 2.2, cited from Robbiano) for unique identifiability of a linear-in-parameters polynomial model from a given set of design points. An explicit worked example discretizes a 3-gene time course into k=F3 with r=4 observed transitions and constructs one polynomial model via normal forms under a chosen Gröbner basis/term order. The authors quantify model multiplicity in that example: with |k|=3 and m=r=4 basis monomials, there are 3^4 possible transition functions per node, 3^12 models for a fixed term order, and (in that example) 5 distinct term orders yielding 3^60 possible models across those term orders. They note computational experiments suggesting that, for network inference, several shorter time courses under different perturbations may be more informative than one highly resolved time course, though they do not provide a formal proof.","They note that inferred models (and wiring diagrams) depend on the choice of term order used to compute Gröbner bases/normal forms, and there is generally no natural term order, so this dependence cannot be avoided. They also state that discretization of biological data is subtle and results can depend strongly on the discretization method, with information loss as a tradeoff. Additionally, they emphasize practical constraints: r is typically very small due to cost, and many biologically feasible perturbations/states are unavailable, limiting design choices.","The paper is primarily conceptual and does not provide a concrete DOE construction algorithm (e.g., optimal fractions under explicit criteria) tailored to realistic biological constraints beyond general discussion. It does not quantify robustness of algebraic-identifiability conclusions under measurement noise, model misspecification, or errors introduced by discretization, which are central in gene-expression experiments. Comparisons to alternative experimental design frameworks for network inference (e.g., Bayesian experimental design, active learning, information-theoretic criteria) are not developed, so guidance for practitioners remains largely qualitative.","They explicitly state that much research remains on designing optimal biological experiments for network identification under cost and feasibility constraints. They highlight ongoing work on reducing/eliminating term-order dependence—potentially producing wiring diagrams without full dynamic models—as an active focus. They also call for a theoretical justification of the observation (from computational experiments) that multiple shorter time courses under different perturbations can be more useful than a single highly resolved time course, to guide time-course design.","Develop explicit, constraint-aware optimal design criteria (e.g., D-/I-optimal or information-gain objectives) for selecting perturbations/time points under biological feasibility constraints, and evaluate them on benchmark networks. Extend the algebraic framework to noisy/uncertain observations via probabilistic or robust algebraic methods (e.g., error-tolerant ideals, Bayesian finite-field models) and study sensitivity to discretization choices. Provide software implementations (with reproducible code) that compute candidate designs, identifiability diagnostics, and term-order–aggregated model summaries for realistic n≫r regimes.",0801.0254v1,https://arxiv.org/pdf/0801.0254v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:52:53Z
TRUE,Other,Model discrimination|Other,Not applicable,"Variable/General (example model has 5 parameters: T, D, η, τ, b)",Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper studies how specifying an experimental design (the observable subset of the sample space and finite measurement schedule) changes the effective prior/measure on a model’s parameter space in Bayesian/MDL model selection via the Fisher-information (Jeffreys) volume element. It shows that when an experiment can only observe a bounded region of the sample space, the induced Fisher information for an “effective” observed distribution may decay in the tails of a non-compact parameter space, making the Jeffreys/MDL volume finite and the model selection problem well-defined. As a worked example, it analyzes exoplanet transit light-curve models with a non-compact orbital period parameter T, deriving the Fisher information for Gaussian measurement noise and demonstrating how experimental duration (e.g., whether two transits are observed) controls identifiability of T. The analysis quantifies that observing a second transit dramatically increases information about T, whereas observing only one transit can make the Fisher information determinant essentially vanish (up to exponentially small corrections). Practically, it argues experimental design should explicitly account for identifiability/compactification effects when comparing models using MDL/Bayesian evidence.","The induced (Jeffreys) prior/volume form is $\omega(\Theta) \propto \sqrt{\det J(\Theta)}$, where $J_{ij}$ is the Fisher information matrix defined via the local curvature of KL divergence. Conditioning on the experimental design restricts observations to $M$ and adds an “outside” probability mass $\Theta_{\mathrm{Out}}$, yielding an effective distribution (Eq. 7) used to compute $J$. For the transit light-curve with Gaussian noise, the Fisher information simplifies to $J_{ij}=\sigma^{-2}\sum_{k=1}^N \partial_{\theta_i}y(\theta;t_k)\,\partial_{\theta_j}y(\theta;t_k)$ (Eq. 18), leading to tail behavior $\sqrt{\det J}\sim T^{-3}$ as $T\to\infty$ (Eq. 31).","For the exoplanet light-curve model, the Fisher-information measure decays in the non-compact period direction: $\sqrt{\det J}\sim 1/T^3$ for large $T$, implying the Jeffreys/MDL volume becomes finite once the experimental setup is fixed. Comparing designs that detect two transits vs. one transit, an order-of-magnitude estimate gives $J_{\text{short}}/J_{\text{long}}\sim (\eta/T)^6\ll 1$ (Eq. 30), showing two-transit designs are vastly more informative about $T$. In an explicit calculation, for the “long” (two-dip) setup the determinant is $\det(J_{ij}^{\text{long}})=64\,n m\,X^4\,(T_1^2+T_3^2)\approx 64\,n m\,X^4\,T_3^2$ (Eq. 35), whereas for the “short” (one-dip) setup $\det(J_{ij}^{\text{short}})=0$ up to $e^{-c}$ corrections (Eq. 36), indicating near-nonidentifiability without a second transit and adequate “anchoring” points.","The analysis assumes sharp transit edges by taking $c\gg 1$ and treats $c$ as fixed (not a parameter), noting this simplifies calculations and is “not physically very restrictive.” It also assumes Gaussian measurement noise with equal standard deviation across observations for simplicity. The authors note the earlier determinant-ratio estimate overestimates $\det(J^{\text{short}})$ because cancellations make it (nearly) zero, indicating sensitivity to design specifics.","The DOE conclusions rely on Fisher-information/Jeffreys-volume asymptotics and may not directly translate to finite-sample Bayesian evidence when nuisance parameters are unknown/estimated (e.g., $\sigma$, baseline trends, stellar variability). The example focuses on an idealized transit model and sparse, stylized sampling schemes; realistic observation cadences, gaps, heteroscedastic noise, time-correlated (red) noise, and systematics could materially change identifiability and scaling. The work frames “experimental design” as truncation/visibility of the sample space rather than optimizing measurement times under explicit cost constraints, so it does not provide an algorithmic DOE procedure practitioners can apply directly.","The authors suggest turning the analysis around to design experiments that discriminate well between models in chosen parameter regions by ensuring the Fisher information is large there. They also propose determining general conditions under which experimental design makes non-compact parameter spaces effectively compact, building on the arguments in Section 2.","Develop an explicit optimal design framework (e.g., choose observation times/cadence and duration) that maximizes a criterion based on the Fisher information or expected MDL/Bayes factor under realistic constraints (limited telescope time, gaps). Extend the analysis to more realistic noise models (heteroscedastic and time-correlated), unknown variance/self-starting settings, and model misspecification (stellar variability, limb darkening) to assess robustness of the compactification effect. Validate with simulated and real survey cadences (e.g., Kepler/TESS-like sampling) and provide open-source implementations to compute design-dependent parameter-space volumes and expected model-selection performance.",0802.0498v1,https://arxiv.org/pdf/0802.0498v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:53:36Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Computer experiment|Other,Parameter estimation|Model discrimination|Prediction|Optimization|Robustness|Cost reduction|Other,D-optimal|A-optimal|I-optimal (IV-optimal)|G-optimal|E-optimal|V-optimal|Bayesian D-optimal|Bayesian A-optimal|Compound criterion|Space-filling|Minimax/Maximin|Other,Variable/General (covers general p-parameter models; examples include p=4 pharmacokinetics compartment model; linear regression with multiple parameters; dynamical models with pF parameters in transfer function),Healthcare/medical|Pharmaceutical|Energy/utilities|Theoretical/simulation only|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This survey paper presents the mathematical foundations of optimal experimental design (DOE) and emphasizes its close relationship with control, especially input design for dynamical systems and the exploration–exploitation tradeoff in adaptive/dual control. It develops DOE for parametric regression via Fisher information matrices and reviews classical optimality criteria (notably D-, A-, E-, and G-optimality) plus equivalence theorems and constructive algorithms (e.g., Fedorov–Wynn). It also discusses sequential design and robust/minimax criteria, including frequency-domain input spectrum design for system identification and links to robust control objectives (e.g., H∞-motivated constraints). Beyond parametric DOE, it covers DOE issues in statistical learning and computer experiments (Kriging/Gaussian processes), contrasting space-filling designs (Latin hypercubes, maximin/minimax distances) with model-based IMSE/max-variance criteria. Examples include a weighing problem illustrating large efficiency gains from optimal designs and a pharmacokinetic compartment model where a locally D-optimal sampling-time design yields substantially tighter estimator distributions than a conventional design with the same number of samples.","For nonlinear regression y(u_k)=η(θ̄,u_k)+ε_k, the (per-sample) Fisher information matrix is M_F(ξ,θ)=∫_U I(u) [∂η(θ,u)/∂θ][∂η(θ,u)/∂θ]^T ξ(du), reducing to M(ξ,θ)=∫ σ^{-2}(u) J(θ,u)J(θ,u)^T ξ(du) under normal errors. For an N-point (discrete) design U_1^N, M_F(U_1^N,θ)= I·(1/N)∑_{i=1}^N J(θ,u_i)J(θ,u_i)^T (up to constants as defined in the paper). The D-optimality equivalence theorem uses d_θ(u,ξ)= I·J(θ,u)^T M_F(ξ,θ)^{-1} J(θ,u), with ξ_D D-optimal iff max_{u∈U} d_θ(u,ξ_D)=p (and equality at support points).","In the weighing example, using eight orthogonal (Hadamard) weighings yields estimator variances σ^2/8 for each weight using only 8 measurements, whereas naive one-at-a-time weighing would need 64 measurements to match that precision. In the pharmacokinetic compartment-model example (4 parameters), an 8-sample locally D-optimal sampling-time design t*=(1,1,10,10,74,74,720,720) produces a much more concentrated approximate marginal density for K_EL than a conventional design t=(5,10,30,60,120,180,360,720) at the same sample size. The paper states that approximate design theory implies optimal information matrices can be achieved with a finite number of support points (via Carathéodory), and in frequency-domain input design an optimal discrete spectrum exists with finitely many excited frequencies (with an improved upper bound tied to the number of parameters). It also reports known results that adaptive-control/forced-certainty schemes can lead to inconsistency without sufficient excitation, motivating designed perturbations and Bayesian-imbedding-based consistency conditions.","The author notes the survey is not exhaustive: only scalar observations are considered; Bayesian techniques are only slightly touched; measurement errors are assumed independent (correlated errors would need special treatment); distributed-parameter systems are not considered; nonparametric modelling is treated briefly and only for static systems. The paper also stresses that many results are not new individually, but are collected and connected in one document.","Because this is a broad survey, many topics are discussed at a high level without implementation details (e.g., step-by-step procedures for real practitioners, tuning guidance, and computational complexity comparisons across algorithms). Several claims about performance gains are illustrated with a small number of examples rather than a systematic benchmark across many models and constraints. The treatment of “code/computation” is absent, which limits reproducibility for the illustrative simulations/figures if readers want to replicate them directly.","The concluding section highlights open directions: DOE for correlated errors; DOE and estimation/control for nonlinear parameterizations, especially in adaptive contexts; designing informative experiments without persistence of excitation (e.g., nonstationary/vanishing-amplitude perturbations); DOE for nonparametric models and active learning, including links with reinforcement learning; and developing practical algorithms for minimax/robust DOE criteria motivated by control objectives. It also points to improving global optimization under Kriging with more active, multi-step lookahead strategies and better handling of noisy evaluations.","Providing open-source reference implementations (e.g., for Fedorov–Wynn updates, minimax robust input design via LMIs, and Kriging-based sequential EI) would materially accelerate adoption and enable fair empirical comparisons. More work could unify parametric and nonparametric DOE under common decision-theoretic objectives (e.g., value of information) with explicit constraints from real experiments (batching, replication costs, time ordering). Empirical validation on modern, higher-dimensional control-identification problems (MIMO, constraints, closed-loop data with autocorrelation) and robustness to model misspecification would help translate the theory to contemporary applications.",0802.4381v1,https://arxiv.org/pdf/0802.4381v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:54:14Z
TRUE,Sequential/adaptive|Bayesian design|Other,Optimization|Parameter estimation|Screening|Cost reduction|Other,Other,Variable/General (n arms/options; exploration budget C; arm state spaces possibly multi-level),Theoretical/simulation only|Network/cybersecurity|Healthcare/medical|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper studies sequential design of experiments under a “future utilization” objective: a budgeted exploration phase (costly experiments) followed by a single exploitation decision choosing the arm with the best posterior expected reward (or more general concave utility). The setting is Bayesian, with each arm having a Markovian state space capturing posterior updates; plays have costs and may include switching/setup costs, and rewards satisfy a martingale property under Bayesian updating. The main methodological contribution is a polynomial-time constant-factor approximation framework based on a linear programming relaxation over per-arm state spaces plus a novel LP-rounding method via stochastic packing, yielding sequential (non-revisiting) exploration policies. The results include constant-factor guarantees for (i) budgeted bandits with switching/concave play costs, (ii) non-adaptive policies in the special two-level (one-sample-reveals-all) case with bounded adaptivity gap, and (iii) a Lagrangean (reward-minus-cost) variant, and extensions to concave utility objectives. The paper is primarily theoretical (approximation algorithms) and positions its framework as unifying/applicable to data acquisition domains such as sensor networks, active learning, and clinical trial-style experimentation.","A key LP relaxation uses per-state variables $w_u$ (probability arm reaches state $u$), $z_u$ (probability arm is played in state $u$), and $x_u$ (probability arm is selected for exploitation in state $u$), maximizing $\sum_i\sum_{u\in S_i} x_u r_u$ subject to an exploration budget $\sum_i \big(h_i z_{\rho_i}+\sum_{u\in S_i} c_u z_u\big)\le C$, a single-selection constraint $\sum_i\sum_{u\in S_i} x_u\le 1$, and flow constraints $\sum_{v\in S_i} z_v p_{vu}=w_u$ with $x_u+z_u\le w_u$. Policies are constructed by interpreting the LP as defining a per-arm randomized stopping/playing rule (sample $q\sim U[0,w_u]$; play if $q\le z_u$, exploit if $z_u<q\le z_u+x_u$), then ordering arms by a ratio like $R(\phi_i)/(P(\phi_i)+C(\phi_i)/C)$ and executing them sequentially without revisits.","For the budgeted future-utilization objective, the paper gives a polynomial-time 4-approximation sequential policy (achieves at least $\text{OPT}/4$) derived from the LP bound, and also a bicriteria guarantee: with budget relaxed to $\alpha C$, a modified policy achieves value $\frac{\alpha}{2(1+\alpha)}\gamma^*$ where $\gamma^*$ is the LP optimum. For the Lagrangean version (maximize $R(\pi)-C(\pi)$), the sequential greedy policy achieves a 2-approximation (at least $\text{OPT}/2$). For non-adaptive strategies, the adaptivity gap can be unbounded for multi-level state spaces (shown as $\Omega(\sqrt{n})$), but for two-level state spaces a non-adaptive strategy within factor 7 of the LP bound is proved. Extensions to positive concave utilities yield an $\text{OPT}/(8(1-\varepsilon)^{-1})$-type constant factor (stated as at least $\text{OPT}\cdot(1-\varepsilon)/8$) via discretization and scaling.",None stated.,"The work is theoretical and does not provide empirical/simulation validation or real-data case studies demonstrating practical performance, tuning sensitivity, or computational overhead on large real instances. The approximation factors are constants but may be loose for practice (e.g., 4, 7, 8), and the LP requires explicit enumeration of each arm’s state space, which can be large or hard to specify in realistic Bayesian models. The approach assumes independent arms and Markovian posterior-state dynamics; correlations across arms, shared parameters, or contextual effects (common in real DOE/clinical/sensor settings) are not addressed. While switching/setup costs are modeled, other practical constraints (delayed observations, batching, covariates, nonstationarity) are not treated.","The authors mention open questions including whether the budgeted learning problem is APX-hard (hard to approximate within some constant factor) and whether the LP can be strengthened by additional convex constraints to tighten bounds. They also suggest exploring alternative objectives beyond utility maximization, such as minimizing residual information.","Develop computational experiments and benchmarks (synthetic and domain-specific) to quantify typical-case performance versus common heuristics (e.g., index-style methods, Thompson sampling variants adapted to a fixed exploration budget). Extend the framework to settings with dependent arms or shared latent parameters (hierarchical Bayes), which are common in DOE and active learning. Create scalable implementations (e.g., approximate LP/column generation, pruning/aggregation of posterior states) to handle large or continuous state spaces. Study robustness to model misspecification and provide self-starting or nonparametric variants when priors or transition models are uncertain.",0805.2630v2,https://arxiv.org/pdf/0805.2630v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:54:38Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,TRUE,MATLAB,Not provided,http://phys.lsu.edu/aces/|http://saturn.jpl.nasa.gov/home/index.cfm|http://www.pbs.org/wgbh/nova/titan/sounds.html,"This paper describes a student-built measurement apparatus and procedure to estimate the speed of sound in the atmosphere as a function of altitude using two microphones, a speaker emitting a known-frequency sinusoid, and time-delay estimation from stereo audio recordings during a high-altitude balloon flight. The authors emphasize an experimental setup that improves measurement precision in a noisy environment through geometry (microphone separation) and signal processing (a narrow band-pass Chebyshev filter around 1920 Hz, averaging/decimation, and outlier handling). They compare measured speed-of-sound profiles with an ideal-gas temperature-based model and report close agreement in the troposphere (average percent difference ~0.2% with ~0.7% scatter), with larger discrepancies (still <~2%) in regions where balloon ascent-rate correction is limited by low-rate GPS altitude sampling. They then use Bayesian model comparison to infer bounds on atmospheric composition (nitrogen/oxygen mixture) from speed-of-sound sensitivity to molar mass, concluding the data support nitrogen abundance roughly in the 73–85% range (odds ~4:1). The work is primarily an educational/measurement case study rather than a contribution to formal design of experiments methodology.","Speed of sound in a gas is modeled as $c=\sqrt{\gamma p/\rho}$, and for an ideal gas as $c=\sqrt{\gamma (R/M_{gas}) T}=\sqrt{\gamma (R/M_{gas})(\vartheta+273.15)}$. Measurement uses $c=d/\Delta t$, where $d$ is the fixed microphone separation and $\Delta t$ is the per-cycle time delay between channel peaks of the filtered 1920 Hz sine. Bayesian model comparison assumes Gaussian measurement error $p(e|I)=\frac{1}{\sqrt{2\pi}\sigma}\exp\{-e^2/(2\sigma^2)\}$ and computes likelihoods $p(D|M_i,I)$ and odds ratios $O_{12}=p(D|M_1,I)/p(D|M_i,I)$ under equal priors.","In laboratory characterization, the measured speed of sound distribution is approximately Gaussian with mean offset by ~5% from the accepted value (treated as a calibration/alignment issue) and standard deviation ~0.7%. In flight (troposphere), the average percent difference between measured values and the temperature-based ideal-gas model is ~0.2% with typical relative error (standard deviation) ~0.7%; discrepancies remain under ~2% even through tropopause/stratosphere regions. The Bayesian comparison of nitrogen-percentage models indicates the 79% N2 model is most likely, and the authors report a conservative inferred nitrogen range of ~73–85% (odds about 4:1) given their uncertainties.","GPS altitude data were sampled too slowly (about once per minute), which limited the accuracy of ascent-velocity correction and prevented a good fit between theory and experiment in the tropopause and stratosphere regions. The authors note an initial ~5% calibration offset in lab measurements attributed mainly to imperfect alignment and handled by adjusting the effective microphone spacing. They also state that, despite small error bars, the data are not sufficient to set very strict limits on atmospheric composition because the sensitivity is small relative to uncertainties.","The work does not formalize an experimental design (e.g., randomized/replicated run structure, planned factor-level settings, or optimality-based design) and treats altitude as an observational/trajectory variable rather than a controllable factor, limiting DOE interpretability. The time-delay estimation method (peak picking) may be less robust than cross-correlation/phase-based estimators under noise and could bias $\Delta t$ when waveform hinted distortions occur; this is not systematically analyzed. The Bayesian composition inference relies on strong assumptions (binary N2/O2 mixture, fixed $\gamma\approx1.4$, ideal-gas behavior, independence/Gaussian errors) and does not propagate uncertainty from temperature sensor placement/time synchronization or microphone spacing drift with temperature. Code is described but not shared, which limits reproducibility.",None stated.,"Provide an open-source implementation of the Matlab signal generation/processing pipeline and document parameter choices (filter design, windowing, synchronization) to enable replication. Extend the inference to account for additional atmospheric constituents and uncertainty in $\gamma$, temperature measurement, and ascent-rate correction (hierarchical/Bayesian measurement-error model). Improve the estimator for time delay (e.g., cross-correlation or phase delay) and benchmark robustness under controlled injected-noise experiments and repeated balloon flights. Consider a more explicit experimental plan (replicates, calibration runs across temperatures/pressures) to separate alignment/calibration bias from in-flight effects.",0807.2491v1,https://arxiv.org/pdf/0807.2491v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:55:15Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Other,TRUE,Other,Personal website|Package registry (CRAN/PyPI)|Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher)|In text/Appendix|Upon request|Not provided|Not applicable (No code used),http://whinger.narod.ru/paper/ru/wiki_index_db_Krizhanovsky08.pdf|http://en.wikipedia.org/wiki/Wikitext|http://en.wikipedia.org/wiki/Inverted_index|http://rupostagger.sourceforge.net|http://synarcher.sourceforge.net|http://www.fabforce.net/dbdesigner4|http://api.futef.com/apidocs.html|http://json.org/|http://modis.ispras.ru/sedna/|http://wikixmldb.dyndns.org/help/use-cases/|http://simple.wikipedia.org|http://simple.wikipedia.org/wiki/Sakura|http://simple.wiktionary.org/wiki/User:AKA_MBG/English_Simple_Wikipedia_20080214_freq_wordlist|http://ru.wiktionary.org/wiki/Конкорданс:Русскоязычная_Википедия/20080220|http://en.wikipedia.org/wiki/SI_prefix|http://en.wikipedia.org/wiki/Zipf%27s_law#Related_laws|http://arxiv.org/abs/0710.0169|ftp://ftp.digital.com/pub/DEC/SRC/publications/monika/sigir98.pdf|http://www.inma.ucl.ac.be/~blondel/publications/areas.html|http://www-db.stanford.edu/~backrub/google.html|http://www.cs.technion.ac.il/~gabr/papers/ijcai-2007-sim.pdf|http://socio.ch/intcom/t_hgeser16.pdf|http://www.cs.cornell.edu/home/kleinber|http://arxiv.org/abs/cs/0606097|http://km.aifb.uni-karlsruhe.de/ws/semwebmine2002/papers/application/lee_yang.pdf|http://www2005.org/cdrom/contents.htm|http://nlp.stanford.edu/fsnlp/|http://research.microsoft.com/~melnik/publications.html|http://www.cs.waikato.ac.nz/~dnk2/publications/nzcsrsc07.pdf|http://www.cs.waikato.ac.nz/~dnk2/publications/cikm07.pdf|http://pierre.senellart.com/publications/ollivier2006finding.pdf|http://www.cs.ualberta.ca/~lindek/papers.htm|http://arxiv.org/abs/0803.2220|http://arxiv.org/abs/0806.3787|http://www.eml-research.de/english/homes/ponzetto/pubs/acl07.pdf|http://www.pewinternet.org/pdfs/PIP_Wikipedia07.pdf|http://www.soi.city.ac.uk/~ser/idfpapers/Robertson_idf_JDoc.pdf|http://robotics.stanford.edu/users/sahami/papers-dir/www2006.pdf|http://arxiv.org/abs/0804.2354|http://aot.ru/docs/sokirko/sokirko-candid-eng.html|http://elara.tk.informatik.tu-darmstadt.de/publications/2008/lrec08_camera_ready.pdf|http://www.gate.ac.uk,"The paper presents the architecture and implementation of WikIDF, a wiki-text indexing system that converts MediaWiki wikitext to natural-language text, lemmatizes it (Russian/English/German), and builds a relational inverted-index database suitable for TF–IDF retrieval. It specifies a minimal database schema (term, page, term_page) designed explicitly to support TF, DF, corpus frequency, and TF–IDF weighting, and documents parsing rules (largely regex-based) for stripping or transforming wiki/HTML markup (links, templates, categories, refs, etc.). The authors build and compare index databases for Russian Wikipedia and Simple English Wikipedia at two time points (Sep 2007 vs Feb 2008), reporting corpus/index sizes, parsing times, and growth rates. They also test Zipf’s law on the resulting corpora, fitting power-law approximations for the top 100 and 10,000 words and comparing slopes between the two Wikipedias. The work contributes practical, open tooling and a reusable indexed corpus/database for researchers and developers working with wiki-based full-text search.","The indexing database is built to support TF–IDF weighting: for term $t_i$, document weight is $w(t_i)=TF_i\cdot idf(t_i)$ with $idf(t_i)=\log(D/DF_i)$, where $D$ is the number of documents, $TF_i$ is stored as term frequency per (term,page) (field term_page.term_freq), and $DF_i$ is the document frequency (field term.doc_freq). The paper also fits Zipf-type curves for frequency vs. rank using power-law forms $y(x)=e^{a}/x^{b}$ for the top 100 and 10,000 words in each corpus.","Russian Wikipedia is reported to be about an order of magnitude larger than Simple English Wikipedia in Feb 2008: articles 239.29k vs 25.22k (≈9.5×), lexemes 1.434M vs 0.1487M (≈9.6×), and total words 32.93M vs 2.2839M (≈14.4×). Over ~5 months (Sep 2007→Feb 2008), Simple English grew faster: article count +31% vs +17% (≈14% higher growth rate), and lexeme count +23% vs +16% (≈7% higher). The archived index DB dump size is 77.5 MB (RW 08) vs 7.15 MB (SEW 08); the index size is stated as 26–38% of the size of the text dump. Zipf-law fits are provided, e.g., for RW: $y_{100}(x)=e^{14.51}/x^{0.819}$ and $y_{10K}(x)=e^{16.13}/x^{1.048}$; for SEW: $y_{100}(x)=e^{12.83}/x^{0.974}$ and $y_{10K}(x)=e^{14.29}/x^{1.174}$.","The authors state that the database is created only once and treated as read-only; issues of index merge, updating, maintenance, or corruption are out of scope. They also note that their template-removal handles only limited nesting (the function is called twice to remove {{template in {{template}}}}, but higher nesting levels are not treated).","The work is not a DOE/experimental-design study; the “experiments” are largely descriptive benchmarks and corpus statistics without controlled factor variation or statistical inference about system components. Performance results (e.g., parsing time, index size) are tied to a specific hardware/software stack and may not generalize to other environments or larger corpora (they mention industrial-scale English Wikipedia would be needed for definitive answers in places). The evaluation of retrieval quality is limited; TF–IDF support is described, but there is no systematic IR effectiveness evaluation (precision/recall, MAP, etc.) against baselines.","The paper suggests that a definitive answer about differences observed in Zipf-law behavior would require solving an industrial-scale problem—indexing the huge English Wikipedia. It also explicitly leaves several additional analyses “to the reader,” including tracking rank changes over time using multiple dumps and various histogram/summary analyses derived from the index DB.","A natural extension would be to support incremental indexing and updates (Phase/streaming operation) rather than a one-time read-only build, including strategies for corruption recovery and reindexing. Another direction is a full IR evaluation of search effectiveness (e.g., relevance judgments, standard metrics) and comparison to modern ranking (BM25, language models) beyond TF–IDF. Robust parsing and normalization for deeper template nesting and broader MediaWiki markup (plus multilingual tokenization beyond the three lemmatizer languages) would also strengthen general applicability.",0808.1753v2,https://arxiv.org/pdf/0808.1753v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:56:12Z
TRUE,Optimal design|Response surface|Factorial (fractional)|Computer experiment|Other,Parameter estimation|Screening|Model discrimination|Other,Not applicable,"Variable/General (d factors; examples include d=2, d=4, d=7; also general d≥2)",Theoretical/simulation only|Other,Other,TRUE,None / Not applicable|Other,Not provided,http://cocoa.dima.unige.it|http://www.singular.uni-kl.de,"The paper develops a general aberration criterion for polynomial regression models identifiable by a given experimental design, using algebraic geometry (design ideals and Gröbner bases). It defines weighted linear aberration as the average (possibly weighted) total degree of the monomials in an identifiable model basis, and connects minimization of this criterion to vertices of the design’s state polytope (equivalently, models in the algebraic fan). For generic designs, the state polytope coincides with the corner-cut polytope, implying generic designs achieve minimal aberration across all weight vectors; a greedy algorithm is given to compute a minimal-aberration model for a fixed weight vector. The authors derive upper/lower bounds on the minimal aberration via an “equivalent simplex” construction (depending on the geometric mean of weights) and discuss asymptotic “Nyquist-like” identifiability rates as n and d grow. Examples compare CCDs, full factorials, generic designs, Latin hypercube designs (generic with probability 1 under random construction), and two-level fractional factorial designs via distributions of model degrees in their algebraic fans.","Weighted linear aberration for a model basis L (|L|=n) with weights w (wi≥0, Σwi=1) is defined as $$A(w,L)=\frac{1}{n}\sum_{\alpha\in L}\sum_{i=1}^d w_i\,\alpha_i=\frac{w^T\alpha_L}{n},\quad \alpha_L=\sum_{\alpha\in L}\alpha.$$ A model basis L is identifiable by design D if the model matrix $X=[x^\alpha]_{x\in D,\alpha\in L}$ is invertible. The state polytope is $$S(D)=\mathrm{conv}\{\alpha_L: L \in \mathrm{La}(D)\},$$ and minimizing $A(w,L)$ over identifiable staircase models corresponds to minimizing the linear functional $w^T\alpha$ over vertices of $S(D)$.","For generic designs with n points in d dimensions, the state polytope equals the corner-cut polytope $CC(n,d)$ and the algebraic fan contains all corner-cut models (Theorem 2), ensuring existence of minimal-aberration models for any weight vector. A greedy algorithm that orders candidate exponents by $\omega(\alpha)=\frac{w^T\alpha}{n}$ and retains linearly independent design columns constructs a minimal-aberration model that lies in the algebraic fan (Theorem 5). The minimal aberration $A^*(w,n)$ for generic designs is bounded by an equivalent-simplex value $A(w,S(w))$ as $$A(w,S(w)) - 1 \le A^*(w,n) \le A(w,S(w)) + 1,$$ where $A(w,S(w))=(nd!)^{1/d}\,\frac{d}{d+1}\,g(w)$ and $g(w)=(\prod_i w_i)^{1/d}$ is the geometric mean of weights (Theorem 6). The paper also proposes a smooth approximation to the corner-cut polytope yielding an approximate minimal aberration $$\tilde A(w)= d\,b^{1/d}g(w) - a\sum_i w_i$$ (Theorem 7) that empirically tracks $A^*(w,n)$ closely for moderate/large n in examples.",None stated.,"The work’s aberration notion and guarantees are tied to polynomial regression modeling and (primarily) staircase/corner-cut model bases arising from Gröbner-basis/term-order methods; it does not characterize all estimable (non-staircase) models for a design, and the algebraic fan can be a strict subset of all identifiable staircases for non-generic designs. Practical implementation depends on Gröbner basis/fan computations whose complexity can become prohibitive as n and d grow, yet the paper does not provide runtime/scale benchmarks or software implementations. Empirical comparisons are largely illustrative (examples) rather than systematic simulation studies over broad design classes and noise/replication settings typical in DOE applications.","The paper suggests extending beyond linear aberration to more general concave aberration functions and to defining aberration using distributions other than uniform over the equivalent simplex region (Section 5.1). It also explicitly identifies as future research linking the paper’s minimal linear aberration (model-based) to the traditional minimum aberration criteria for fractional factorial designs based on generators (Section 5.2), conjecturing a correspondence at least among orthogonal $2^d$ fractions.","Develop scalable, open-source software (e.g., R/Python) to compute algebraic fans/state polytopes (or approximations) and to implement the greedy minimal-aberration selection with diagnostics, enabling routine use by practitioners. Extend the theory to common practical complications: unknown/estimated parameters (Phase I), replication/blocked or split-plot structures, and correlated or non-Gaussian errors where “low-degree” preference may interact with robustness. Provide systematic computational studies comparing minimal linear aberration against classical criteria (D-/I-optimality, minimum aberration, generalized resolution) across many design families, and quantify when the state-polytope approximation is accurate as a function of (n,d).",0808.3055v1,https://arxiv.org/pdf/0808.3055v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:56:51Z
TRUE,Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Other,D-optimal|A-optimal|E-optimal|Other,1 factor (univariate predictor u; 3 parameters in inverse quadratic model),Food/agriculture|Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper derives local optimal approximate designs for inverse quadratic regression models under two parameterizations, with the design variable being a single continuous predictor over either a bounded interval $[s,t]$ or the unbounded region $[s,\infty)$. It provides explicit locally optimal designs for multiple criteria: c-optimality (including $D_1$-optimality for discriminating inverse linear vs. inverse quadratic models, and extrapolation/prediction at a point $x_e$), D-optimality, and E-optimality. A main finding is that when the design region is sufficiently large, optimal designs often follow geometric allocation rules and are supported at common Chebyshev points; for bounded regions, the support adjusts to include boundary points when constraints truncate the unconstrained support. The derivations use approximate design theory, equivalence theorems, and Chebyshev system arguments to establish optimality and to compute optimal weights. The paper also reports efficiencies of several optimal designs and compares them to a uniform design used in an applied lactation-curve study, showing substantial potential efficiency gains from using the derived optimal designs.","The inverse quadratic mean functions are $\eta_1(u,\theta)=\dfrac{u}{\theta_0+\theta_1 u+\theta_2 u^2}$ and $\eta_2(u,\theta)=\dfrac{\theta_0 u}{\theta_1+u+\theta_2 u^2}$, with information matrix $M(\xi,\theta)=\int f(u,\theta)f(u,\theta)^T\,d\xi(u)$ where $f=\partial\eta/\partial\theta$. Optimality criteria studied include c-optimality minimizing $c^T M^{-}(\xi,\theta)c$, D-optimality (maximize $\det M$), and E-optimality (maximize the minimum eigenvalue $\lambda_{\min}(M)$). For large regions $[0,\infty)$, many optimal designs place support at geometrically spaced points $(\rho^{-1}u^*,u^*,\rho u^*)$ where $u^*=\sqrt{\theta_0/\theta_2}$ for $\eta_1$ (or $u^*=\sqrt{\theta_1/\theta_2}$ for $\eta_2$) and $\rho$ is given explicitly (e.g., Eq. (3.8) for several criteria, Eq. (4.2) for D-optimality).","Closed-form local D-, E-, and several c-optimal (including $D_1$ and extrapolation) designs are provided for both parameterizations, typically supported on three points that are Chebyshev points and often geometrically spaced. For D-optimality on $[0,\infty)$ the optimal weights are equal (1/3,1/3,1/3) at the three geometric support points, with an explicit formula for the geometric ratio $\rho$. In the real-data illustration with design space $[1,14]$ (lactation curve example), the uniform 8-point design had efficiencies 69.92% (D), 50.33% (E), 45.85% ($D_1$), and 33.82% ($c_e$); local optimal designs achieved 100% for their own criterion and varying cross-efficiencies (e.g., D-optimal had 94.18% E-efficiency and 75.28% $D_1$-efficiency). The paper highlights that commonly used uniform designs can be substantially inefficient relative to derived local optimal designs in these models.","The results are local optimal designs, i.e., they depend on assumed/nominal parameter values (the paper repeatedly refers to “local” optimality and uses estimated parameters for the application example). The model and design theory are developed under assumptions including normally distributed, independent observations with constant variance, and parameter constraints ensuring positivity/continuity of the mean function on the design region.","Because designs are local, practical use may require robust or Bayesian design approaches to mitigate sensitivity to parameter misspecification; the paper mainly studies local criteria and only briefly examines efficiencies under other criteria (not robustness to parameter uncertainty). The work is for a univariate predictor; extensions to multi-factor inverse polynomial settings or correlated/heteroscedastic errors (common in dose-response/time series) are not addressed. Implementation details for practitioners (e.g., algorithms for constrained intervals beyond provided cases, or software) are not provided, which may limit accessibility despite closed forms.",None stated.,"Develop robust (e.g., maximin) or Bayesian optimal designs for inverse quadratic regression to address uncertainty in nominal parameter values and to provide designs suitable for Phase I planning. Extend the design theory to settings with heteroscedasticity, autocorrelation, or non-normal errors, which are plausible in agricultural/biological applications. Provide software implementations (R/Python) to compute the derived designs and efficiencies on arbitrary bounded regions and to support practitioners with diagnostic/robustness tools.",0809.4938v1,https://arxiv.org/pdf/0809.4938v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:57:25Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design|Other,Prediction|Parameter estimation|Optimization|Other,Other,Variable/General,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,MATLAB|Other,Personal website,http://mloss.org/software/view/269/,"The paper develops scalable variational Bayesian inference methods for sparse linear models (SLMs) and uses the resulting posterior covariance to drive Bayesian experimental design. A key contribution is a convexity characterization: the variational inference objective is convex if and only if the model potentials are log-concave, matching the convexity profile of MAP estimation. The authors propose double-loop (bound-optimization) algorithms that reduce large-scale inference to repeated penalized least-squares (MAP-like) inner problems plus Gaussian variance/covariance approximations (via Lanczos/PCA), enabling problems with full high-resolution images. The experimental-design component is sequential/adaptive: candidate measurement designs (e.g., MRI k-space phase-encode trajectories) are ranked by approximate expected information gain computed from the Gaussian posterior approximation. Empirically, Bayesian-optimized MRI sampling trajectories on real-world scanner data substantially outperform standard low-pass, equispaced, and variable-density random sampling for sub-Nyquist reconstructions.","Bayesian experimental design is based on an approximate information gain score using the variational Gaussian posterior covariance $A^{-1}$: $\Delta(X^*) = -\log|A| + \log|A + \sigma^{-2}X_*^T X_*| = \log|I + \sigma^{-2}X_* A^{-1} X_*^T|$. The variational posterior covariance is $\mathrm{Cov}_Q[u\mid y]=A^{-1}$ with $A=\sigma^{-2}X^TX + B^T\Gamma^{-1}B$, where $\Gamma=\mathrm{diag}(\gamma)$ are variational width parameters. Design scoring is approximated efficiently using a Lanczos low-rank approximation to $A^{-1}$ (via $Q_k^T A Q_k=T_k$) to evaluate $\Delta(X^*)\approx \log|I+V_*^T V_*|$ where $V_* = \sigma^{-1}X_* Q_k L_k^{-T}$.","In MRI sampling optimization experiments on real-world 256×256 brain data, sequential Bayesian optimization of phase-encode designs (“op”) yields lower MAP reconstruction error than equispaced (“eq”), center-out low-pass (“ct”), and variable-density random (“rd”) designs for the same number of acquired columns $N_{col}$, with the biggest gains at stronger undersampling (e.g., around 1/4 Nyquist, starting from 32 central columns). The paper also reports that, for Laplace potentials, the “type A” bound for $\log|A|$ converges in very few outer-loop iterations (about 2 in their test), whereas “type B” may require many more (20+ without convergence in the shown setup). Variance estimates from Lanczos can be substantially biased low, yet the algorithms and resulting reconstructions/design rankings are empirically robust to these approximation errors in their SLM imaging settings.",None stated.,"The experimental-design evaluation is primarily demonstrated for MRI phase-encode selection with specific priors/operators (wavelets/derivatives) and relies on Gaussian variational approximations whose quality may degrade under model mismatch (e.g., non-Gaussian noise, coil effects, motion, or strong nonlinearity). Design quality depends on approximate covariance/variance computations (Lanczos/PCA) that can severely underestimate marginal variances; while the authors note robustness, the approach may be less reliable in other regimes or domains without careful validation. The work focuses on information-gain (entropy) objectives; alternative utility functions (task-based diagnosis/segmentation) and constraints (hardware, SAR, gradient limits) are not fully integrated in the core DOE formulation.","The authors propose extending the approach to more advanced joint design scenarios such as multi-slice MRI, 3D MRI, and parallel MRI with array coils, and speeding up computations via improved inner-loop optimization, modern Lanczos variants, and parallel/GPU implementations. They also mention the longer-term goal of enabling real-time MRI through automatic online sampling optimization.","Provide broader empirical benchmarks and ablations for design optimality across multiple scanners/protocols, with strict train/test separation and clinical task-based endpoints. Extend the DOE framework to incorporate hard acquisition constraints explicitly (trajectory smoothness, gradient/slew limits, SAR) and to multi-objective utilities beyond entropy (e.g., ROI fidelity, diagnostic classification performance). Develop calibrated uncertainty diagnostics to detect when the Gaussian variational approximation or Lanczos covariance approximation is unreliable, possibly via randomized trace/variance estimators or hybrid VI–MCMC checks.",0810.0901v2,https://arxiv.org/pdf/0810.0901v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:57:58Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Cost reduction|Other,Other,Variable/General (design variable is the next measurement setting: initial 3He beam energy; examples shown with sequential choices across ~500–3000 keV),Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.iter.org|http://ftp.stat.duke.edu/WorkingPapers/95-39.ps,"The paper applies Bayesian Experimental Design (BED) to optimize Nuclear Reaction Analysis (NRA) measurements for reconstructing deuterium depth profiles using a 3He beam, an ill-posed inverse problem where each data point is time-consuming. The design variable is the choice (and number) of measurement energies, selected sequentially to maximize expected information gain, using Kullback–Leibler divergence between posterior and prior as the utility. For a linear-in-parameters discretized depth-profile model, the authors derive a closed-form expected-utility expression enabling fast one-dimensional energy scans to choose the next energy. For a more realistic nonlinear model (e.g., exponential decay profile parameters), they compute expected utility numerically using posterior sampling, allowing interactive “on-the-fly” optimization during measurement. Simulations (mock tungsten sample data) indicate large potential gains versus standard equidistant energy settings, including reported improvements exceeding a factor of 100 in a linear-case conditioning measure, and reduced need for additional measurements once marginal utility diminishes.","The measurement model is linear in the depth profile: $d_i=\mu N_i\int_0^{x(E_i^0)}\!\sigma(E)\,c(x)\,dx+\varepsilon_i$ with Gaussian noise. In the linear discretized form $\mathbf d=\mathbf M\mathbf c+\varepsilon$, the posterior is Gaussian with precision $\mathbf A=\mathbf M^\top\Sigma\mathbf M$ and mean $\mathbf c_0=\mathbf A^{-1}\mathbf M^\top\Sigma\mathbf d$. Using KL-divergence as utility, the linear-design expected utility has closed form $EU(d,\eta)=\tfrac12(\log(1+G)-r)$ with $G=\frac{\mathbf m(\eta)^\top\mathbf A^{-1}\mathbf m(\eta)}{\sigma^2}$ (for Gaussian predictive $r=0$), where $\eta$ is the candidate next energy and $\mathbf m(\eta)$ the corresponding model row. For nonlinear design, $EU$ is computed numerically and the marginal likelihood term is approximated with posterior samples: $\int p(D\mid\alpha,\eta)p(\alpha\mid d)\,d\alpha\approx \frac1N\sum_i p(D\mid\alpha_i,\eta)$.","In the linear sequential design example, the expected-utility scan selects successive next energies (illustrated in the paper) such as ~1250 keV, then ~960 keV, then ~490 keV after initial boundary/calibration measurements, showing how utility evolves over cycles. Compared to equidistant energy selection with the same number of measurements, the BED-optimized setting is reported to improve a linear-problem conditioning measure by a factor greater than 100. In the nonlinear example (estimating parameters like decay length and background concentration for an exponential profile), the first measurement at 500 keV leads the expected utility to favor a high-energy measurement (~3 MeV), then a mid-range energy (~1500 keV), after which the utility drops markedly, indicating diminishing returns from further measurements. The posterior-sampling implementation is reported to find the best next energy in under ~5 minutes on then-contemporary hardware (Linux PC, 2 GHz).","For the linear analytic treatment, the authors note approximations that change integration limits: the predicted datum integral is extended from $\int_0^\infty dD$ to $\int_{-\infty}^\infty dD$ (argued acceptable given counts/uncertainties), and more problematically the parameter integral is similarly extended from a bounded domain to $(-\infty,\infty)$, which they state “definitely affects the results.” They also state that uncertainty in the design matrix entries due to energy straggling is beyond the paper’s scope. They simplify the data uncertainty model (e.g., $\sigma_i=\max(5\%\,d_i,\sqrt{d_i})$) and acknowledge “there is room for improvement.”","The work evaluates designs primarily on simulated/mock data; the magnitude of gains may depend on how well the assumed cross-section, stopping power, and noise models match real experiments, including systematic errors (e.g., beam-current drift, deuterium loss correction bias, detector calibration). The approach optimizes energies but does not explicitly incorporate practical constraints such as energy-switching overheads, discrete energy grids, or multi-objective tradeoffs (time, dose-induced depletion, and uncertainty) beyond information gain. The Bayesian optimization depends on the chosen prior and model class for the concentration profile; model misspecification could lead to suboptimal or misleading energy choices. No reusable software artifact is provided, which limits reproducibility and ease of adoption by practitioners.","They conclude that posterior-sampling BED enables optimizing many sequential measurements “on the fly,” opening the door to new applications of BED in ion beam analysis as well as other physical areas. They also reference forthcoming work on quantitative depth profiling of deuterium to very large depths (listed as “to be submitted”), implying continued development and application in this domain. No more specific future-work items are explicitly detailed in the provided text beyond broader application/extension.","A valuable extension would be to validate the BED-optimized energy schedules on real NRA datasets (Phase II) and quantify improvements in reconstruction error, not only conditioning/entropy proxies. Incorporating additional nuisance parameters and systematics (energy calibration uncertainty, stopping-power uncertainty, deuterium depletion dynamics, correlated noise) into the Bayesian model would make the designs more robust. Multi-objective or constrained BED (including time per energy change, total dose limits, and required calibration points) could better reflect operational realities. Publishing an open-source implementation (e.g., in Python/R/Matlab) with templates for NRA would improve reproducibility and adoption.",0812.3789v1,https://arxiv.org/pdf/0812.3789v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:58:41Z
TRUE,Factorial (full)|Split-plot|Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,"Variable/General (key design factors are randomization level: school vs within-school teacher vs completely randomized teachers; plus student-to-teacher class assignment structure c, m, n, a)",Other,Simulation study|Other,TRUE,SAS,Supplementary material (Journal/Publisher),http://cresmet.asu.edu/msp/newindex.shtml|http://www.mdrc.org/publications/417/abstract.html|http://www.nap.edu/openbook.php?record_id=10735&page=303|http://sitemaker.umich.edu/group-based/optimal_design_software|http://ies.ed.gov/ncee/wwc/pdf/study_standards_final.pdf,"The paper develops and compares randomized experimental designs for interventions applied to teachers in multiple schools when outcomes are measured at multiple levels (teacher and student). It studies three randomization schemes: (1) cluster randomization by school, (2) randomization of teachers within schools (blocked by school), and (3) complete randomization of teachers across schools. Using mixed-effects models for teacher responses and a cross-classified/additive teacher-effects model for student responses (allowing each student to have multiple teachers), the authors derive information/anticipated-variance expressions for the treatment effect under each design and show that designs efficient for teacher-level effects can be inefficient or even non-estimable for student-level effects depending on the student-to-teacher assignment structure. They analyze how contamination/noncompliance/attrition can inflate variances, often favoring school-level randomization when contamination risk is substantial. A SAS macro (multleveldesign) is provided to simulate anticipated treatment-effect variances and empirical power across designs for user-specified numbers of schools/teachers/students, variance components, and contamination levels.","Teacher-level model: $\mathbf T_i = \mathbf X_i\beta + \mathbf 1_m v_i + \epsilon_i$ with $\mathrm{Cov}(\mathbf T_i)=\sigma_v^2\mathbf J_m+\sigma_\epsilon^2\mathbf I_m$; treatment indicator is last column of $\mathbf X_i$ with entries $\pm1$. Student-level model: $\mathbf Y_i = \mathbf B_i\gamma + \mathbf D_i(\mathbf X_i\theta + \mathbf t_i)+\mathbf 1_n s_i + \eta_i$, with $\Sigma_i=\sigma_s^2\mathbf J_n+\sigma_t^2\mathbf D_i\mathbf D_i' + \sigma_\eta^2\mathbf I_n$ and treatment effect $\theta_p$. Design comparisons use treatment information (inverse variance) from mixed-model GLS information, e.g., teacher information $I_T=\sum_i \mathbf X_i'\mathbf V_i^{-1}\mathbf X_i$ and (no student covariates) student information $I_S=\sum_i \mathbf X_i'\mathbf D_i'\Sigma_i^{-1}\mathbf D_i\mathbf X_i$, then take expectations over randomization; explicit expected-information formulas are given for each randomization scheme (Table 1).","For teacher outcomes, randomizing teachers within schools yields higher treatment information than randomizing by school, with the completely randomized teacher design typically intermediate (Table 1). For student outcomes, expected treatment information depends strongly on the student-to-teacher assignment matrix $\mathbf D_i$; in a balanced setting closed forms are derived (eqs. 3.7–3.8). A key non-estimability result: if each student takes classes from all teachers in the school ($c=m$), then under within-school teacher randomization the student-level treatment effect has zero information (not estimable), whereas randomize-by-school remains estimable and most efficient for students in that case. Contamination inflates anticipated variance for within-school and completely randomized teacher designs (with explicit variance-inflation factors derived), and simulations show contamination can make school-level randomization preferable overall despite its lower efficiency for teacher-level effects in the uncontaminated case.","The authors note that student-level information is analytically tractable only in special cases because it depends in a complex way on the student-to-class/teacher assignment matrix $\mathbf D_i$, motivating the simulation program. They also state their simulations assume students are assigned randomly to teachers within a school, and discuss that self-selection of students into teachers could bias treatment-effect estimates. They mention that covariates were not included in the macro demonstrations (though they could be incorporated via residual variance components after covariate adjustment).","The design comparison is largely framed through anticipated variance/information under linear mixed models with normality/independence assumptions for random effects and errors; robustness to misspecification (e.g., non-normal outcomes, heteroskedasticity, or correlated errors over time) is not examined. The treatment is encoded as $\pm 1$ with a single average effect; heterogeneous treatment effects across schools/teachers and interference/spillovers beyond the simple contamination indicator are not modeled. Practical randomization constraints (e.g., unequal school sizes, limited number of clusters, stratification/matching, or minimum detectable effect calculations under realistic recruitment/attrition processes) are only partially addressed via simulation rather than fully integrated design optimization. The provided tool is SAS-based, which may limit accessibility and reproducibility if code is not mirrored in open-source platforms.",None stated.,"Extend the framework to explicitly incorporate unequal cluster sizes with stratified/matched randomization and to optimize designs under budget constraints for multilevel outcomes (e.g., choose $a,m,n,c$ subject to cost). Develop robust/self-starting planning methods that account for uncertainty in variance components and student-to-teacher assignment mechanisms, possibly via Bayesian design criteria. Generalize to non-Gaussian outcomes (binary/ordinal test scores), longitudinal outcomes, and multivariate endpoints with joint power criteria across levels. Provide open-source implementations (e.g., R/Python) and benchmarking against existing power/design software for cluster/multisite trials, including diagnostic tools to flag non-estimability given a proposed $\mathbf D_i$ structure.",0812.3937v2,https://arxiv.org/pdf/0812.3937v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:59:20Z
FALSE,NA,NA,Not applicable,Not specified,Theoretical/simulation only|Other,Simulation study,TRUE,Other,Not provided,https://www.grid5000.fr|http://proactive.inria.fr/|http://perun.hscs.wmin.ac.uk/~gerald/|http://perun.hscs.wmin.ac.uk/~vsg/,"The paper analyzes requirements for performing parallel transaction-oriented simulations on loosely coupled ad hoc grids, emphasizing space-parallel partitioning and synchronization choices. It implements a Java-based parallel GPSS/H simulator using the ProActive grid framework, supporting both standard Time Warp (TW) and Shock-Resistant Time Warp (SRTW) via per-LP control components (LPCC) and indicators/actuators to limit optimism. Performance is evaluated on Grid’5000 using two small, deliberately simple two-partition GPSS/H models to compare rollback behavior and throughput between SRTW and TW. Results show SRTW can reduce rolled-back work (about 19% fewer rolled-back transaction moves in one scenario) but can also significantly underperform TW when its local-only control triggers unnecessary throttling/cancelback in a no-rollback scenario. The authors propose improving SRTW by adding global progress window awareness (e.g., compute GFT alongside GVT) and suggest studying behavior under communication delays typical of Internet-based grids.",Not applicable,"On Grid’5000 (Azur cluster), for Simulation Model 1 SRTW reduced LP2 rolled-back transaction moves from 383,022 (TW) to 308,790 (SRTW), a reduction of 74,232 (~19%), and reduced total simulated transaction moves from 503,022 to 428,790. This corresponded to higher average simulation performance: 5,502 time units/s (SRTW) versus 4,637 time units/s (TW). For Simulation Model 2, TW achieved 165.7 time units/s while SRTW dropped to 27.7 time units/s due to LPCC actuator limits inducing cancelback despite no rollbacks occurring.","The authors note that SRTW’s adaptive optimization is based purely on local information, which can be insufficient and can cause SRTW to perform significantly worse than TW in some scenarios (e.g., throttling an LP that is behind and not causing rollbacks). They also frame the evaluation as using deliberately simple example models to isolate algorithmic effects, implying limited representativeness of real workloads.","The evaluation uses only two very small, two-partition toy GPSS/H models and a single hardware/network setup, which limits generalizability to larger models, more LPs, and heterogeneous/adversarial grid conditions. Comparisons focus mainly on rollback counts and throughput without a broader sensitivity analysis over SRTW parameters (e.g., sensor sampling interval, clustering settings) or statistical replication across runs. The work does not report implementation overheads of LPCC/GVT or scalability curves (speedup vs. LP count), which are central for assessing practical benefits on grids.","They suggest modifying SRTW to make LPs aware of their position within the global progress window (GPW), potentially by extending required GVT computation to also compute global furthest time (GFT). They also propose implementing and comparing the adaptive throttle scheme of Tay et al. (1997). Finally, they propose investigating how SRTW (or an improved version) reacts to communication delays likely in Internet-based grids.","Evaluate the simulator and SRTW variants on larger, more realistic GPSS/H models (e.g., manufacturing/queueing networks) and with more than two LPs to assess scalability and robustness. Perform replicated experiments under controlled network impairment (latency, jitter, packet loss) and node heterogeneity to quantify when SRTW helps or harms. Provide an open-source implementation or reproducibility package (scripts, configs, model files) and add automated tuning/diagnostics for LPCC parameters to reduce sensitivity and improve practitioner adoption.",0908.2222v1,https://arxiv.org/pdf/0908.2222v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T00:59:47Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Optimization|Prediction|Other,Other,"Variable/General (domain $D\subset\mathbb{R}^d$; examples include 1D synthetic, 46 sensors, 357 sensors)",Environmental monitoring|Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper studies sequential optimization of an unknown expensive-to-evaluate function under noise, formalized as a Gaussian-process (GP) bandit problem, and analyzes the GP-UCB algorithm that selects $x_t=\arg\max_x \mu_{t-1}(x)+\sqrt{\beta_t}\,\sigma_{t-1}(x)$. It proves the first sublinear cumulative-regret bounds for this nonparametric setting, both when $f$ is sampled from a GP prior and in an agnostic setting where $f$ has bounded RKHS norm with bounded martingale-difference noise. The regret bounds are expressed in terms of the maximum mutual information gain $\gamma_T$, creating an explicit connection to Bayesian experimental design (maximum entropy/information-gain design) and leveraging submodularity/greedy information-gain arguments to bound $\gamma_T$. Using kernel spectral properties (squared exponential and Matérn), the authors derive explicit sublinear regret rates, with particularly weak dimensional dependence for the squared-exponential kernel. Empirically, GP-UCB is compared against EI/MPI and baselines on synthetic data and real temperature-sensor and traffic-sensor datasets, showing competitive or favorable performance.","GP posterior updates: $\mu_T(x)=k_T(x)^\top (K_T+\sigma^2 I)^{-1}y_T$ and $\sigma_T^2(x)=k_T(x,x)=k(x,x)-k_T(x)^\top (K_T+\sigma^2 I)^{-1}k_T(x)$. Sampling rule (GP-UCB): $x_t=\arg\max_{x\in D}\ \mu_{t-1}(x)+\sqrt{\beta_t}\,\sigma_{t-1}(x)$. Experimental-design information gain: $I(y_A;f)=\tfrac12\log\lvert I+\sigma^{-2}K_A\rvert$, and its worst-case maximum over $|A|=T$ is $\gamma_T=\max_{|A|=T} I(y_A;f_A)$; regret bounds scale as $\tilde O(\sqrt{T\beta_T\gamma_T})$.","For finite decision sets, GP-UCB achieves (w.h.p.) $R_T\le \sqrt{C_1 T\beta_T\gamma_T}$ with $\beta_t=2\log(|D|t^2\pi^2/(6\delta))$ and $C_1=8/\log(1+\sigma^{-2})$. For compact convex $D\subset[0,r]^d$ under a smoothness condition on GP sample-path derivatives, $R_T=\tilde O(\sqrt{dT\gamma_T})$ (plus constants) w.h.p. For the RKHS-bounded (agnostic) case with bounded martingale-difference noise, $R_T=\tilde O(\sqrt{T(B\sqrt{\gamma_T}+\gamma_T)})$ w.h.p. Kernel-specific information-gain bounds include: linear kernel $\gamma_T=O(d\log T)$, squared-exponential kernel $\gamma_T=O((\log T)^{d+1})$, and Matérn ($\nu>1$) $\gamma_T=O\big(T^{\frac{d(d+1)}{2\nu+d(d+1)}}\log T\big)$, yielding sublinear regret for common kernels.","The extension to continuous/compact decision spaces requires a smoothness assumption on the kernel ensuring GP sample paths have bounded derivatives with high probability; this excludes highly erratic processes. In particular, the paper notes the Ornstein–Uhlenbeck kernel (Matérn with $\nu=1/2$) violates the differentiability condition (sample paths are nondifferentiable a.e.), and the authors conjecture a Theorem-2-style result may not hold there. For the RKHS setting, the theorem assumes knowledge of an upper bound on $\|f\|_k$ (though the authors note guess-and-doubling can be used if unknown).","The practical performance depends strongly on correct kernel choice/hyperparameters and on how the argmax of the UCB acquisition is solved; the paper largely treats acquisition maximization as tractable and does not analyze optimization error in (6). The theoretical bounds primarily address cumulative regret and worst-case information gain; they do not give tight guidance on finite-sample constants (the experiments even scale $\beta_t$ down by a factor of 5 via cross-validation). Comparisons are limited to a few classic GP-optimization heuristics (EI/MPI and mean/variance-only), omitting later strong baselines (e.g., entropy search / PES / Thompson sampling variants) and broader robustness checks (model misspecification beyond RKHS-boundedness, heteroskedastic noise, or strong temporal/spatial correlations in noise).",None stated.,"Extend the continuous-domain regret analysis to rougher kernels/processes (e.g., Matérn with small $\nu$, OU) by using discretization arguments based on Hölder continuity rather than differentiability. Analyze the impact of approximate maximization of the GP-UCB acquisition (global-search heuristics) on regret, yielding end-to-end guarantees with optimization error. Develop and release reference implementations and benchmarking suites, and study robustness to kernel/hyperparameter misspecification, heteroskedastic noise, and correlated/no-nonstationary observation noise in real sensing applications.",0912.3995v4,https://arxiv.org/pdf/0912.3995v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:00:28Z
TRUE,Optimal design|Other,Other,Other,Not specified,Theoretical/simulation only,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),www.github.com/b45ch1/algopy,"The paper derives forward- and reverse-mode algorithmic differentiation (AD) algorithms for higher-order derivatives of two core linear algebra functions: the rectangular QR decomposition and the eigenvalue decomposition of symmetric matrices with distinct eigenvalues, using univariate Taylor propagation of matrices (UTPM). Linear algebra routines are treated as elementary functions (rather than differentiated through their algorithmic steps) to improve numerical stability, accuracy, and efficiency. The theory is implemented in the BSD-licensed Python AD tool ALGOPY and evaluated via numerical tests and runtime comparisons against an alternative traced approach (PYADOLC), reporting near machine-precision agreement for derivatives. As an application motivated by optimum experimental design, the methods are used to compute gradients of an objective of the form \(\nabla_x \Phi(C(J(F(x,y))))\) where \(C=(J^T J)^{-1}\) and \(\Phi\) is based on an extreme eigenvalue (largest eigenvalue) of \(C\). The work supports stable gradient evaluation for design-optimization objectives that depend on QR- and eigendecompositions of Jacobian-based information matrices.","The optimum experimental design–motivated objective is expressed as \(\nabla_x\, \Phi(C(J(F(x,y))))\), where \(J=\frac{dF}{dy}\), \(C=(J^T J)^{-1}\), and \(\Phi\) selects an eigenvalue (stated as the largest eigenvalue \(\lambda_1\)) of \(C\). The paper provides explicit push-forward (UTPM) recursions for QR (Alg. 8) and symmetric eigendecomposition (Alg. 10), and corresponding reverse-mode pullback formulas (Alg. 9 and Alg. 11) to backpropagate adjoints through these decompositions. In the example program, the computational pipeline is \(F\to J\to (Q,R)=\mathrm{qr}(J)\to D=\mathrm{solve}(R,I)\to C=DD^T\to (\Lambda,U)=\mathrm{eig}(C)\to \Phi=\Lambda_{11}\).","Runtime ratios for UTPM push-forward versus normal evaluation are reported as approximately 11.79 for QR with \(A\in\mathbb{R}^{100\times 5}\), degree \(D=4\), with five parallel evaluations, and approximately 11.88 for eigendecomposition with \(A\in\mathbb{R}^{20\times 20}\), \(D=4\), five parallel evaluations. In the optimum experimental design gradient-check example using \(F(x,y)=Bxy\) with \(N_x=11\), the absolute error between symbolic and AD-computed gradient is reported as \(|(\nabla_y\Phi)_{\text{symbolic}}-(\nabla_y\Phi)_{\text{AD}}|=4.4\times 10^{-15}\), indicating near machine-precision agreement. The plots and discussion indicate that for larger matrices the UTPM implementation in ALGOPY outperforms an alternative traced approach using PYADOLC for both push-forward and lifted pullback at \(D=4\).","The authors note that runtime measurements show “significant fluctuation” in relative runtime ratios, so tests were repeated 10 times and summarized with mean and standard deviation. They also stress that the presented plots “only indicate the actual runtime ratio that would be obtained by efficient FORTRAN/C/C++ implementation” and that there are “many possibilities to improve” PYADOLC performance (e.g., buffer sizes, direct LAPACK calls), implying the comparison is indicative rather than definitive. They additionally describe ALGOPY as “pre-alpha” with an API “very likely to change,” limiting immediate production use.","The optimum experimental design component is primarily motivational and demonstrated via a simple analytic toy model; the paper does not develop or compare experimental design algorithms (e.g., exchange algorithms) or provide empirical OED case studies with real experimental data. The eigenvalue-decomposition results assume symmetric matrices with distinct eigenvalues; this excludes repeated/clustered eigenvalues where gradients can be ill-defined or require subspace-based differentiation, which is common in practice for information matrices. The evaluation emphasizes accuracy and runtime on selected matrix sizes and AD configurations; broader benchmarks across conditioning regimes, noise, and alternative linear algebra backends (e.g., MKL/OpenBLAS differences) are not explored. Code availability is via a repository, but reproducibility of specific experiments would depend on versions and scripts not explicitly enumerated in the text excerpt.",None stated.,"Extend the differentiated eigendecomposition to handle repeated or nearly repeated eigenvalues (e.g., using eigen-subspace derivatives or regularization) to broaden applicability in optimum experimental design objectives based on extreme eigenvalues. Provide a dedicated optimum experimental design study (e.g., A-, D-, E-optimal design construction) demonstrating end-to-end benefits of the proposed AD-through-linear-algebra approach on realistic design problems and constraints. Add comprehensive reproducibility artifacts (benchmark scripts, fixed seeds, environment specifications) and performance profiling across multiple BLAS/LAPACK implementations and hardware. Develop higher-level interfaces in ALGOPY (or companion packages) that directly support common OED criteria and constraints to make the methodology more accessible to practitioners.",1001.1654v2,https://arxiv.org/pdf/1001.1654v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:01:02Z
TRUE,Split-plot|Other,Parameter estimation|Other,Not applicable,Variable/General (examples include RCB with t treatments and b blocks; split-plot with whole-plot and split-plot factors; Latin square with two blocking factors; BIB/incomplete block with t treatments and k per block),Food/agriculture|Manufacturing (general)|Theoretical/simulation only|Other,Other,TRUE,None / Not applicable|Other,Not provided,NA,"The paper develops a multivariate variance-components (mixed) model for the joint distribution of a response and one or more random covariates in designed experiments, aiming to provide a coherent foundation for ANCOVA in blocked/clustered designs. For randomized complete blocks (RCB), it shows that the commonly used “univariate mixed ANCOVA” (random blocks with a single covariate slope) is implicitly misspecified unless the between-block variance of the covariate is zero, which can yield biased adjusted treatment means and incorrect standard errors. Starting from a bivariate (or multivariate) mixed model, the implied conditional model for the response includes both within-block covariate terms and block-mean (between-block) covariate terms, clarifying why intra- and inter-block regression slopes generally differ. For orthogonal blocking designs, the conditional model reduces to a standard univariate linear mixed model so that appropriate covariate adjustment can be done with standard software; for nonorthogonal/unbalanced cases, the paper provides a general multivariate variance-components formulation and an EM algorithm for ML estimation. The methodology is illustrated on classical examples including Pearce’s apple yield RCB data and Zelen’s resistor incomplete-block (BIB) study, demonstrating meaningful differences between naive univariate and proper multivariate-based adjustments in some settings.","In the classical RCB ANCOVA, adjusted treatment means are $\hat\mu_{i,adj}=\bar y_{i\cdot}-\hat\gamma(\bar z_{i\cdot}-\bar z_{\cdot\cdot})$ with OLS slope $\hat\gamma=\frac{z^T(C_t\otimes C_b)y}{z^T(C_t\otimes C_b)z}$. Under the bivariate random-block model for $(Y,Z)$, the implied conditional response model becomes $Y_{ij}=\mu+\tau_i+B_j+\gamma_e z_{ij}+\gamma_b \bar z_{\cdot j}+E_{ij}$, separating within-block slope $\gamma_e=\sigma_{e,yz}/\sigma^2_{e,z}$ and an additional between-block adjustment component $\gamma_b$ (a function of block and error covariances). For orthogonal blocking designs, the joint covariance is structured as $V=\sum_{l=0}^q G_l\otimes A_l$, yielding a univariate conditional mixed model with covariate effects decomposed along the orthogonal projectors $A_l$.","For Pearce’s apple yield RCB data (6 treatments, 4 blocks), the univariate mixed model estimates $\hat\sigma_e^2=194.55$, $\hat\sigma_b^2=553.98$, giving $\hat\rho=0.9447$ and slope $\hat\gamma\approx 28.89$ versus OLS $\hat\gamma\approx 28.40$; adjusted means are similar across approaches but standard errors differ substantially (fixed-block SEs about 6–7 vs mixed-model SEs about 13–14). Under the bivariate mixed model, the estimated inter-block slope is reported as $\hat\gamma_{be}=37.25$, notably different from the intra-block slope $\hat\gamma_e=28.40$, supporting the claim that equal-slope univariate mixed ANCOVA can be misspecified when covariate block means vary. In Zelen’s resistor BIB example, estimated treatment effects and SEs differ materially between the univariate mixed model and the bivariate-based conditional model (e.g., for treatment $l_1w_1$, effect −0.519 (SE 0.112) vs −0.449 (SE 0.233)), though an interaction contrast yields similar conclusions (estimate 0.022 vs 0.018; SE 0.056 vs 0.061). For an unbalanced version of the apple study (missing two treatments in one block), adjusted means are evaluated at the ML estimate of the covariate population mean $\hat\mu_z=8.2080$ (distinct from the raw overall mean 8.3182), and SEs increase for treatments with fewer blocks observed.","The authors note that their primary modeling strategy assumes covariates are not affected by treatments (e.g., measured pre-treatment), and while the multivariate model can be modified when treatments affect covariates, they caution there are additional inferential issues (including “hidden extrapolation,” per Bartlett (1936)). They also state that in nonorthogonal settings the convenient factorization that permits fitting a univariate mixed model may fail (e.g., when covariate data are partially observed or numbers of covariate measurements vary by block), requiring full multivariate modeling. They further acknowledge reliance on a joint multivariate normal model, while remarking that conditional moment calculations extend to elliptically contoured families.","The paper focuses on linear mixed-model ANCOVA; it does not fully develop robustness to common DOE complications such as autocorrelation, heteroscedasticity beyond the variance-components structures considered, or influential outliers in covariates/responses that can distort within- vs between-block slope estimates. Practical guidance for diagnosing misspecification (e.g., testing necessity of block-mean covariate terms or assessing sensitivity to covariance-structure assumptions) is limited, and the EM algorithm discussion is largely methodological without benchmarking computational cost or convergence behavior in realistic large DOE settings. The approach centers on random covariates and blocking; extensions to high-dimensional covariates or modern regularization approaches are not addressed.",None stated.,"Develop diagnostic and model-selection procedures for deciding when block/cluster-mean covariate terms (between-block effects) are needed and for assessing sensitivity to covariance-structure assumptions (e.g., via likelihood-based tests, information criteria, or posterior predictive checks). Extend the framework to generalized linear mixed models (non-Gaussian responses) and to settings with autocorrelated or spatially correlated errors common in field trials. Provide software implementations (e.g., R/SAS code) and case-study-driven guidance for practitioners, including handling multiple covariates with missingness patterns and comparing ML/REML/Bayesian estimation in finite samples.",1001.3011v1,https://arxiv.org/pdf/1001.3011v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:01:48Z
TRUE,Optimal design|Other,Parameter estimation|Cost reduction|Other,D-optimal|Other,Variable/General (s candidate experiments; select N < s in underinstrumented case; also budgeted version with costs c_i and budget B),Theoretical/simulation only|Network/cybersecurity|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper studies a family of combinatorial optimization problems motivated by exact (integer) optimal experimental design under Kiefer’s $\Phi_p$ criteria. Given PSD information matrices $M_i$, it considers selecting up to $N$ experiments (allowing replication, binary selection, and also a budgeted/costed variant) to maximize the spectral function $\varphi_p(n)=\mathrm{tr}(\sum_i n_i M_i)^p$, which interpolates between rank maximization as $p\to 0^+$ and a trivial equal-weight knapsack at $p=1$. A key contribution is a matrix inequality implying submodularity of $I\mapsto \mathrm{tr}(\sum_{i\in I} M_i)^p$ for all $p\in[0,1]$, yielding the classical greedy $(1-1/e)$ approximation guarantee for these design-selection problems. The paper also analyzes rounding from the continuous (approximate-design) relaxation, proving posterior bounds depending on the relaxed optimum and deriving prior approximation factors for specific rounding procedures; the rounding guarantee improves as $p\to 1$ and can beat the greedy bound in many regimes. Applications motivating the work include selecting a small number of network-traffic measurement experiments in underinstrumented settings, though the paper’s results are largely theoretical and algorithmic.","The discrete design problem is $\max_{n\in\mathbb{N}^s:\,\sum_i n_i\le N}\;\varphi_p(n)$ where $\varphi_p(n)=\mathrm{tr}\left(\sum_{i=1}^s n_i M_i\right)^p=\sum_{k=1}^m \lambda_k^p$ (eigenvalues of the information matrix). The binary variant restricts $n\in\{0,1\}^s$ and the budgeted variant uses $\sum_i c_i n_i\le B$. The continuous relaxation optimizes over weights $w\ge 0$ with $\sum_i w_i\le N$, maximizing the concave Kiefer criterion (equivalently $\varphi_p(w)$ up to a $1/p$ power), and rounding guarantees are expressed via bounds like $\frac{1}{N}\sum_i n_i^p (w_i^*)^{1-p}\le \varphi_p(n)/\varphi_p(w^*)$.","The paper proves submodularity of $I\mapsto \mathrm{tr}(\sum_{i\in I} M_i)^p$ for all $p\in[0,1]$, implying that the greedy algorithm achieves a $(1-1/e)$ approximation ratio (or $1-(1-1/N)^N$) for the cardinality-constrained replicated and binary problems. It provides posterior rounding bounds for any integer design $n$: $\frac{1}{N}\sum_i n_i^p (w_i^*)^{1-p}\le \varphi_p(n)/\varphi_p(w^*)$ (and analogues for binary and budgeted constraints). For rounding from the continuous optimum, prior guarantees include (binary) selecting the top-$N$ coordinates yields at least $(N/s)^{1-p}$ when $p\le 1-\ln N/\ln s$, and (replicated) an incremental rounding algorithm achieves a piecewise-defined factor $F(p,N/s)$ that increases with $p$ and tends to 1 as $p\to 1$. For the budgeted submodular maximization, it cites known guarantees: adapted greedy about $1-e^{-\beta}\approx 0.35$ and greedy plus small enumeration recovers $1-1/e$.","The paper notes that meaningful multiplicative approximation factors for $\Phi_p$ criteria cannot be used for $p\le 0$ in instances where no feasible design yields full-rank information, because the criterion is identically 0 until full rank; thus the approximation analysis is restricted to $p\in[0,1]$. It also remarks that the total-curvature refinement of greedy has no general instance-independent upper bound for $p\in[0,1)$ and must be computed per instance. Additionally, it acknowledges that the closed-form bound in Theorem 3.8 is not tight in general because it is derived via a simpler (suboptimal) rounding used for analysis.","The work is primarily theoretical and does not provide extensive empirical benchmarking on real experimental design datasets to validate how often the worst-case bounds are approached in practice. The analysis focuses on the $\Phi_p$/spectral-trace family and PSD information matrices; robustness to model misspecification, correlated/heteroscedastic noise, or constraints such as blocking/split-plot structures is not addressed. Practical implementation details (numerical methods, scaling to very large $s$ with expensive eigenvalue computations, and heuristics for speeding greedy/rounding) are not developed into reusable software. Comparisons to other modern submodular maximization methods (e.g., continuous greedy, multilinear relaxation with randomized rounding) are not explored for these spectral objectives.",None stated.,"Develop scalable implementations and computational studies comparing greedy, incremental rounding, and more recent submodular maximization algorithms (e.g., continuous-greedy/pipage rounding) on large-scale design libraries, especially for underinstrumented regimes. Extend the approximation/rounding analysis to settings with additional experimental-design structure (e.g., blocking/split-plot constraints, multiple resources, or sequential/adaptive selection) and to robustness against non-ideal statistical assumptions (correlation, non-Gaussian noise, misspecification). Provide open-source code and numerical recipes for efficiently updating $\mathrm{tr}(\cdot)^p$ objectives and marginal gains (eigenvalue/Cholesky updates) to enable practical adoption. Investigate tighter instance-dependent bounds (e.g., curvature-like measures tailored to spectral functions) that better predict when rounding beats greedy.",1007.4152v2,https://arxiv.org/pdf/1007.4152v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:02:30Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Prediction|Optimization|Other,Other,Variable/General (examples shown: 2D experiment space; robotic arm example uses 2 design variables for measurement location; model has 3 parameters).,Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Nested Entropy Sampling (NES), an entropy-based search algorithm for Bayesian experimental design that efficiently searches a high-dimensional space of candidate experiments to find those that are maximally informative. The design criterion is to maximize Shannon entropy of the predictive distribution of experimental outcomes (equivalently maximizing expected information gain when the goal is to learn all model parameters), with a note that mutual information applies when targeting a subset of parameters. NES is inspired by Skilling’s nested sampling and works by maintaining a set of candidate experiments, iteratively raising a hard entropy threshold (discarding the lowest-entropy sample) while exploring replacements that satisfy the threshold, thereby contracting toward global maxima in “entropy space.” Performance is evaluated via simulations on synthetic 2D entropy landscapes generated from mixtures of Gaussians and via a simulated autonomous robotic-arm measurement-location selection task, showing that NES finds high-entropy experiments while requiring far fewer entropy evaluations than brute-force grid search. In the robotic-arm example, NES (with 25 samples) evaluates entropy at 855 of 3721 candidate points (compression efficiency 4.35), saving about 77% of computations relative to brute force while still identifying maximal-entropy measurement locations (possibly multiple optima).","Bayesian updating is given by the posterior $p(\theta\mid D,I)\propto p(\theta\mid I)\,p(D\mid\theta,I)$. The expected utility of an experiment $e$ is $EU_e=\sum_d p(d\mid e,M,D)\,U_{d,e}$, and using Shannon-information utility leads to selecting $\hat e=\arg\max_e\left[-\sum_d p(d\mid e,M,D)\log p(d\mid e,M,D)\right]$, i.e., maximizing predictive entropy. The NES algorithm sets an entropy threshold $H^*=\min\{H(e_i)\}$ over a maintained sample set and replaces the minimum with a new trial experiment accepted only if $H_{trial}>H^*$, iterating until convergence to $H_{max}$. Efficiency is summarized by compression efficiency $CE=n/m$ (total candidate experiments divided by number of entropy computations performed).","On synthetic 2D entropy landscapes defined by mixtures of Gaussians, repeated runs (e.g., 100 per setting) show a tradeoff: increasing the number of maintained samples $N$ increases the probability of successful convergence to the global maximum but decreases compression efficiency (more computations). In the intelligent robotic arm measurement-location selection example, brute-force search would compute entropies at all 3721 grid points, whereas NES with 25 samples computed entropy at 855 points, yielding $CE=4.35$ and reducing computations by about 77% while still locating maximal-entropy regions (sometimes multiple equally optimal locations).","The authors note that with a small number of samples the algorithm can get stuck on local peaks, reducing the probability of success; increasing the number of samples improves convergence probability but increases computations/time. They also note that multimodal entropy spaces can lead to convergence at multiple peaks, requiring choosing among multiple optimal experiments (e.g., randomly or via an additional cost function).","The work does not provide theoretical guarantees (e.g., bounds on convergence rate or probability of global optimality) for general high-dimensional, continuous experiment spaces; demonstrations are primarily 2D and discretized into cells. The design criterion is purely entropy-based (information gain) and may not reflect practical constraints (cost, feasibility, safety) beyond a brief mention of an add-on cost function, and there is limited exploration of robustness to model misspecification or posterior approximation error. Implementation details that affect performance (proposal mechanism/step-size adaptation, computational complexity scaling with dimension, stopping rules) are only sketched, which can make reproducibility and fair comparison with alternative optimizers difficult.",Future work will compare the NES algorithm with other available optimization algorithms.,"Provide theoretical analysis of NES (e.g., convergence diagnostics, expected runtime, and scaling with dimension and multimodality) and develop principled stopping criteria. Extend and test the method under realistic experimental constraints (cost-aware or constrained Bayesian design, continuous design spaces, and noisy/approximate forward models) and under model misspecification. Release a reference implementation and benchmark NES against established Bayesian optimization/active-learning methods (e.g., Gaussian-process Bayesian optimization, Thompson sampling, entropy search/PES) on standardized test problems and real experimental platforms.",1008.4973v1,https://arxiv.org/pdf/1008.4973v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:03:06Z
TRUE,Optimal design,Parameter estimation|Other,D-optimal|A-optimal|Other,"Variable/General (design space $X\subset\mathbb{R}^m$; examples use $m=4$ and $k=3$; $n$ up to 100,000 design points)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Personal website,www.math.sfu.ca/∼zhaosong,"The paper addresses approximate optimal experimental design on a finite design space by minimizing a broad class of smooth convex optimality criteria applied to the information (moment) matrix, including classical A-, D-, c- and pth-mean criteria. It proposes a globally convergent interior-point (Newton/barrier) method over the simplex of design weights and shows how to compute Newton directions efficiently by exploiting low-rank structure of the criterion Hessian and using the Sherman–Morrison–Woodbury formula when the number of candidate design points $n$ is much larger than the information matrix dimension $m^2$. For A-, D-, and pth-mean criteria, the authors derive explicit gradient/Hessian expressions and provide a closed-form rank formula for the Hessian, enabling efficient partial eigen-decompositions. The method is benchmarked in MATLAB against the widely used multiplicative algorithm (Silvey et al.) and the SDP solver SDPT3 on several large synthetic design spaces (up to $n=100{,}000$), showing the interior-point approach generally improves speed and solution quality, especially for pth-mean criteria with $p<-1$ where the multiplicative algorithm may fail to converge. The work advances computational methodology for constructing optimal approximate designs under common convex criteria, particularly in large-scale discrete design spaces.","The approximate design problem is posed over weights $w\in\Omega=\{w\ge 0,\ \sum_i w_i=1\}$ with moment matrix $M(w)=\sum_{i=1}^n w_iA_i$ and objective $\Phi(M(w))=\Psi(C_K(M(w)))$, where $C_K(M)=\big(K^T M^\dagger K\big)^{-1}$. Classical criteria include A: $\mathrm{tr}(K^T X^\dagger K)$, D: $\log\det(K^T X^\dagger K)$, and pth-mean: $\mathrm{tr}\big((K^T X^\dagger K)^{-p}\big)$ (with $p<0$). The interior-point method solves barrier subproblems $\min_{\tilde w} f(\tilde w)-\mu\sum_i\log\tilde w_i-\mu\log(1-e^T\tilde w)$ via Newton steps $d=-(\nabla^2 f_\mu)^{-1}\nabla f_\mu$, with efficient linear algebra via Sherman–Morrison–Woodbury using the low-rank structure of $\nabla^2\phi(Mw)$.","In MATLAB experiments on four synthetic design spaces with $A_i=x_ix_i^T$, candidate set sizes up to $n=100{,}000$ (and $n\gg m^2$), the proposed interior-point method typically achieved substantially lower CPU time than the multiplicative algorithm while producing equal or better objective values for A-, D-, and pth-mean criteria. Against SDPT3 (used for A and D via SDP reformulations), the interior-point method was usually faster and often attained comparable or smaller objective values; SDPT3 sometimes terminated early with infeasibility for some A-criterion instances (reported as markedly worse objectives). For pth-mean criteria with $p<-1$, the interior-point approach yielded markedly better objective values than the multiplicative algorithm, consistent with the lack of convergence guarantees for the latter in that regime.","The authors report numerical difficulties for pth-mean criteria with large $|p|$ and for certain larger polynomial-regression-like design spaces (e.g., including a $s_i^4$ term with $n\ge 50{,}000$ and random $K$), where the Newton direction cannot be computed accurately due to numerical errors and the barrier method may fail to terminate with a good solution. They note that barrier methods can deteriorate as $\mu\to 0$, and that performance depends on accurately/efficiently computing the Newton direction.","The method is developed for approximate designs on a finite candidate set (weight optimization on a simplex) and does not directly address exact/integers (run-by-run) design construction or constraints such as minimum run counts per point. Empirical validation is largely synthetic; there is no real-data experimental-design case study showing end-to-end impact on estimation/prediction in an applied domain. Practical robustness to model misspecification, heteroskedasticity, correlated errors, or constraints beyond the simplex (e.g., costs, blocking/split-plot structure) is not analyzed, and comparisons omit several modern large-scale optimal design approaches (e.g., coordinate/Frank–Wolfe variants, proximal/first-order methods) that could be competitive when Newton steps are ill-conditioned.","They suggest developing a primal–dual interior-point method instead of a pure barrier (primal) method to mitigate deterioration as $\mu\to 0$ and the associated numerical issues observed in some hard instances. They note this would require working on an equivalent nonlinear semidefinite programming reformulation because the feasible set of the original design problem is not closed in general, and leave this as future research.","Implementations in more modern, open ecosystems (e.g., an R/Python package) with reproducible benchmarks and robust numerical linear algebra (preconditioning, stabilized eigen/SVD updates) could broaden adoption and reduce failures on ill-conditioned instances. Extending the approach to constrained design settings (cost constraints, multiple resource constraints, blocking/split-plot/hard-to-change factors, or exact designs) and to other criteria (e.g., I- or G-optimality) would increase practical scope. Additional theory and experiments for unknown-parameter (Bayesian/locally optimal) designs or sequential/adaptive updates on streaming data would align the algorithm with common applied DOE workflows.",1009.1909v3,https://arxiv.org/pdf/1009.1909v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:03:45Z
TRUE,Other,Other,Not applicable,"Variable/General (multiple experimental configurations; key controllables include laser intensity/amplitude a0, wavelength λ, pulse duration τ, focusing/spot size w⊥/S, electron beam energy γe, geometry/collision configuration, and use of relativistic flying mirror/LWFA vs RF accelerator).",Other|Theoretical/simulation only,Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes and analyzes table-top experimental configurations to reach extreme-field regimes in laser–electron interactions, including (i) petawatt laser collisions with RF-accelerator electron bunches, (ii) collisions with laser-wakefield-accelerated (LWFA) electrons, and (iii) LWFA electrons colliding with an ultra-intense pulse produced via a relativistic flying mirror (RFM). It frames the “design of experiments” as choosing physical parameters (laser amplitude/intensity, wavelength, pulse duration, focusing, and electron energy) to enter radiation-reaction-dominated and strong-field QED regimes characterized by dimensionless invariants (e.g., a0, εrad, χe, χγ) and by thresholds for Schwinger vacuum pair creation and Breit–Wheeler cascades. The work derives scaling laws and threshold estimates for observable outcomes (gamma-ray burst generation, e−e+ production, and avalanche onset), and compares feasibility across configurations (RF vs LWFA vs LWFA+RFM). Numerical integration results are shown to illustrate how radiation friction qualitatively changes electron dynamics, impacting achievable χe and χγ. The main practical implication is that LWFA synchronization/density advantages and RFM intensification can substantially increase pair-production yield and access to nonlinear vacuum/QED effects with near-term high-power laser systems.","Key control/physics parameters defining the proposed experimental regimes include the normalized laser amplitude $a=eE/(m_ec\omega)=eE\lambda/(2\pi m_ec^2)$ (Eq. 1) and the radiation-reaction parameter $\varepsilon_{rad}=2r_e/(3\lambda_0)$ (Eq. 3), with radiation-reaction dominance when $a>\varepsilon_{rad}^{-1/3}$ (Eq. 5). Strong-field QED is characterized by invariants $\chi_e=(e/(m_e^3c^4))\sqrt{(F_{\mu\nu}p^\nu)^2}$ (Eq. 11) and $\chi_\gamma=(e/(m_e^3c^4))\sqrt{(F_{\mu\nu}k^\nu)^2}$ (Eq. 12), which govern photon emission and Breit–Wheeler pair creation probabilities, with prolific rates when $\chi_e,\chi_\gamma\gtrsim 1$. For Schwinger vacuum pair creation, the rate is given in tunneling form (Eq. 16/18) and integrated over the effective 4-volume to estimate pair counts (Eq. 21), while RFM-based intensification scalings for reflected intensity are given by Eqs. (43)–(44).","The paper estimates that radiation-reaction effects become important around $I\gtrsim 2\times 10^{23}\,\mathrm{W/cm^2}$ for $\lambda\approx 1\,\mu$m (from $a>\varepsilon_{rad}^{-1/3}$). It predicts first observable Schwinger pairs in a focused-field (TM-mode) scenario at roughly $I^*\sim 10^{27}\,\mathrm{W/cm^2}$ and avalanche/cascade onset around $I^*\sim 2.5\times 10^{27}\,\mathrm{W/cm^2}$ (via $\chi$ reaching order unity within ~0.1 period). Using RFM scaling and pair-count estimate (Eq. 45), it argues RFM could enable first detectable pairs at source-laser intensity of order $\sim 10^{22}\,\mathrm{W/cm^2}$ with $a_0\sim 100$ (subject to focusing/compression assumptions). For a 30 fs PW laser, peak invariants summarized in Table 1 are approximately: RF accelerator (150 MeV) $\chi_e\approx 0.1,\chi_\gamma\approx 0.01$; LWFA (1.25 GeV) $\chi_e\approx 0.5,\chi_\gamma\approx 0.1$; LWFA+RFM (with $\gamma_{ph,W}=5$) $\chi_e\approx 2.5,\chi_\gamma\approx 2.5$, indicating feasibility improvements with LWFA and especially with RFM intensification.",None stated.,"Although framed as “design of experiments,” the paper does not provide a formal DOE methodology (e.g., optimal design criteria, run planning, factor screening), nor an explicit experimental run matrix; instead it gives physics-based scaling/threshold guidance. Several feasibility claims depend on idealized field models (e.g., focusing to ~wavelength spots, TM-mode approximations, RFM reflection/focusing efficiency, negligible timing jitter) and may be sensitive to real-world imperfections (shot-to-shot laser fluctuations, alignment, plasma density gradients, and beam quality). Comparisons across configurations use order-of-magnitude estimates and limited numerical examples, without a comprehensive uncertainty/robustness analysis or systematic parameter scans. Implementation details for diagnostics (gamma spectroscopy, pair detection) and background/systematic-error control are not developed, which can dominate practicality at low expected yields in sub-threshold regimes.",None stated.,"A valuable next step would be a systematic experimental-parameter study (including uncertainty and sensitivity analysis) mapping feasible regions in $(a_0,\tau,\lambda,w_\perp,\gamma_e)$ for each configuration, to translate scaling laws into a concrete run plan and tolerances. Extending models to include realistic laser/plasma spatiotemporal structure, electron-beam phase space, jitter, and diagnostic response would improve experimental readiness and allow power/yield predictions with credible intervals. Open, reproducible simulation/analysis code and benchmark cases (e.g., PIC+QED modules) would enable cross-validation and facilitate adoption by experimental groups. Finally, adding explicit design recommendations for sequential experimentation (e.g., start in radiation-reaction regime, then increase $\chi$ toward cascade onset) would align the work more directly with modern DOE/sequential design practice.",1101.2501v2,https://arxiv.org/pdf/1101.2501v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:04:27Z
TRUE,Optimal design|Bayesian design|Robust parameter design|Sequential/adaptive|Other,Model discrimination|Cost reduction|Other,Other,"Variable/General (design variables include initial conditions, perturbation magnitudes/times, and measurement time points; parameters robustified over ranges; examples include 4 unknown parameters in glycolytic model 2 and 1 unknown parameter in Dictyostelium example)",Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable|Other,Not provided,http://www.hsl.rl.ac.uk,"The paper develops a robust optimal design of experiments framework for discriminating between competing dynamical-system (ODE) models when model parameters are only known within ranges. It formulates model discrimination via KL-optimality (Kullback–Leibler divergence between likelihoods) and then robustifies the design using a worst-case (max–min) criterion over parameter uncertainty sets and (optionally) multiple candidate models. The design variables include initial state values, magnitudes/times of system perturbations, and the number/placement of discrete measurement time points under practical constraints (minimum spacing between measurements and no simultaneous measurement and perturbation, enforced via smoothed Heaviside switches). The resulting semi-infinite optimization problem is solved by an outer-approximation (cutting-plane–style) method with a homotopy strategy to stabilize successive constraint additions; implementation uses a multiple-shooting transcription with derivative-based NLP solving. Numerical demonstrations on two biological systems (glycolytic oscillations and Dictyostelium signal sensing) show that the homotopy-stabilized outer-approximation procedure improves robustness and convergence compared with cold-starting successive robustification steps.","The discrimination objective is derived from the KL divergence between the two models’ observation likelihoods, leading (for Gaussian observation noise) to a sum over measurement times with switching terms: $I(P_{r2}:P_{r1},O_1)=\frac12\sum_{i=1}^n H(t_i)\,He(c_i)\sum_{k=1}^o\left[\frac{(v_{r2,k}^i)^2+(O_{r2,k}^i-O_{r1,k}^i)^2}{(v_{r1,k}^i)^2}-2\log\frac{v_{r2,k}^i}{v_{r1,k}^i}-1\right]$ (Eq. 35). Robust design is posed as a max–min problem over parameter sets, converted to a semi-infinite program via an auxiliary variable $\tau$: maximize $\tau$ s.t. $\min_{\theta_{r1}\in\Theta_{r1},\theta_{r2}\in\Theta_{r2}} I(P_{r2}(\theta_{r2}):P_{r1}(\theta_{r1}),O_1;\xi)-\tau\ge 0$ for all model pairs (Eq. 36), with design variables $\xi=(y_I,t,c)$ and feasibility constraints.","Two case studies illustrate the method: (1) glycolytic oscillation models where model-2 parameters are robustified over wide ranges (e.g., $q_2,r_s,\mu\in[10^{-7},100]$, $L_2\in[100,300]$) and designs choose discrete measurement times and (in a second scenario) a few allowed perturbations (e.g., at the 21st/41st/61st/81st candidate times with $c_i\in[10^{-7},10]$). (2) Dictyostelium signaling models where a piecewise-constant input is represented via perturbations to ligand state $S$ and one uncertain parameter $k_{i2}\in[0,2]$ is robustified. Reported plots show decreasing robustification gaps $\Delta_{RG}$ across outer-approximation iterations and more stable objective trajectories when using the homotopy strategy versus without it (where large objective jumps occur due to infeasible warm starts). Overall, the homotopy approach is shown qualitatively to be “significantly superior” for stabilizing successive robustification steps in both examples.","The authors note that their Step 1 (finding worst-case parameters to augment the outer-approximation set) is handled heuristically using random starts plus local optimization, and they explicitly state that making Step 1 more effective was not their primary goal. They also state that their formulation assumes a shared time discretization for measurements and perturbations/controls and that allowing fully independent time schemes for controls and distinct observables would require a more general formulation. They remark that a mixed-integer formulation for the discontinuous measurement/perturbation decisions would be expensive and that efficient solution strategies for mixed-integer max–min problems are not obvious.","The numerical results are primarily illustrative and do not provide broad benchmarking against alternative model-discrimination design criteria (e.g., T-optimality implementations, Bayesian expected utility designs) or against non-robust counterparts across many scenarios, so general performance/optimality claims are hard to quantify. The approach depends on local NLP solves (IPOPT) inside an outer-approximation loop, so global optimality is not guaranteed and solutions may be sensitive to initialization, smoothing parameters for the switching functions, and the random-search procedure used for worst-case parameter discovery. The statistical model assumes independent Gaussian measurement errors and (in examples) homoscedasticity, which may be unrealistic for time-series biological data with autocorrelation, missingness, or non-Gaussian noise.",They state that developing a suitable and efficient strategy for sorting models into null vs. alternative hypotheses is subject to further work. They state that incorporating more general formulations with independent time schemes (generic controls and distinct observables measured independently) is subject to further work. They also state that a step-size strategy for the homotopy procedure is a next step for efficiency improvements.,"Providing an open-source implementation (or at least reproducible scripts) would substantially improve adoption and enable independent verification; packaging the method with standard ODE/sensitivity toolchains would help practitioners. Extending the framework to handle correlated and non-Gaussian observation models (e.g., GLM noise, heavy tails, autocorrelation) and to include parameter-estimation uncertainty from finite Phase I data (instead of fixed parameter boxes) would strengthen practical robustness. More systematic comparisons on benchmark suites (including computational cost and robustness to initial guesses) and extensions to larger model sets/multivariate observations would clarify scalability and best-use regimes.",1101.3663v2,https://arxiv.org/pdf/1101.3663v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:05:13Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://planetimager.org,"This paper presents performance modeling and survey-planning simulations for the Gemini Planet Imager (GPI), a high-contrast direct-imaging instrument aimed at detecting and characterizing young Jovian-mass exoplanets. The authors combine instrument noise/contrast simulations (including atmospheric and quasi-static wavefront error sources and post-processing assumptions) with Monte Carlo planet-population simulations to predict detection rates and completeness across planet mass and semi-major axis. They compare detectability under two planet evolution/formation model families (hot-start vs. core-accretion) and show detection rates depend strongly on target age, distance, and host-star properties, with young nearby stars yielding the highest yields. The work also proposes a simple power-law target-prioritization model for ordering observations by predicted detection probability and discusses how many detections are needed to distinguish idealized formation scenarios. Overall, it is an instrumentation/survey simulation and planning study rather than a designed-experiments (DOE) methodology paper.","The survey target-ordering heuristic models detection probability as a product of power laws: $p = A\,[\log(t/1\,\mathrm{Myr})]^\alpha\,(d/40\,\mathrm{pc})^\beta\,(M/M_\odot)^\gamma$. For the hot-start evolutionary approximation (Burrows et al. 1997 fit over a specified mass/age range), the paper gives power-law regressions such as $T_{\mathrm{eff}}(M,t)\propto t^{-0.29} M^{0.47}$, $L(M,t)\propto t^{-1.21} M^{1.87}$, and $R(M,t)\propto t^{-0.034} M^{-0.013}$.","Using assumed planet population power laws (e.g., $dN/dM\propto M^{-1.31}$ and $dN/da\propto a^{-0.61}$ extrapolated to wide separations), the authors predict that for stars younger than 1 Gyr within 80 pc, detection rates are about 4% (core-accretion cooling model) versus about 12% (hot-start). For a young-star sample (age < 100 Myr) they report higher rates, about 13% (core-accretion) versus about 21% (hot-start), assuming the semi-major-axis distribution extends to ~70 AU; a volume-limited very-nearby sample yields only ~1–2%. Instrument contrast predictions suggest ~better than $10^{-7}$ in 1 hour at H band for bright targets (around I~5 mag), with performance degrading for fainter stars. They also report that ordering targets by the fitted detection-probability proxy can recover roughly two-thirds of detectable planets in the first third of observed targets in a simulated sample.","The authors note multiple simplifying assumptions: they assume no angular differential imaging (ADI) speckle attenuation (a pessimistic assumption) and use a simple three-wavelength “double difference” spectral subtraction rather than more sophisticated algorithms. They state telescope vibration effects are not included due to uncertainty in the Gemini vibration environment, though a tilt error budget is discussed. They also emphasize that extrapolating radial-velocity-derived distributions to large semi-major axes is uncertain and that uncertainties in cool-planet atmospheres (e.g., clouds/condensation) affect spectra and detectability.","Despite mentioning algorithmic simplifications, the study still relies on idealized and partially decoupled simulation pipelines (separate dynamic vs. quasi-static error simulations), which may miss interaction effects and temporal evolution across long integrations. The planet population model assumes specific functional forms and cutoffs (and often effectively one planet per star for yield counts), which can materially bias predicted yields and completeness; more flexible hierarchical/occurrence-rate modeling is not explored. Practical observing constraints (weather losses, scheduling overheads, heterogeneous exposure times, and follow-up cadence impacts) are not fully propagated into the yield estimates, which can affect realizable survey performance.","They suggest that real GPI operations will likely use more sophisticated PSF subtraction approaches (e.g., LOCI/SOSIE-type frameworks) than the simple spectral differencing used in the simulations. They also indicate that interpreting real detections to discriminate formation models will require segregating detected-planet distributions by properties such as semi-major axis, host mass, and planet metallicity, and that well-constrained stellar ages will be vital. The discussion implies the value of discovering and characterizing many more multi-planet systems (like HR 8799) to better test formation scenarios.","A natural extension is to incorporate occurrence-rate inference (Bayesian hierarchical modeling) that jointly fits population parameters and survey selection effects, rather than adopting fixed extrapolated power laws and cutoffs. More comprehensive end-to-end simulations could include correlated noise, telescope/instrument vibration realizations, and time-dependent aberration evolution coupled to realistic observing sequences and post-processing pipelines. Providing open-source simulation code and benchmark datasets would improve reproducibility and facilitate community comparisons across instruments and algorithms.",1103.6085v1,https://arxiv.org/pdf/1103.6085v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:05:52Z
TRUE,Supersaturated|Screening|Other,Screening|Parameter estimation|Cost reduction,E-optimal|Other,Variable/General (mixed-level SSDs with more factors than runs; many specific n×m examples tabulated),Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),http://support.sas.com/techsup/technote/ts723.html,"The paper develops theoretical and constructive methods for building optimal mixed-level supersaturated designs (SSDs) intended for screening many factors with limited runs. It gives equivalent conditions for when two columns are fully aliased (via induced matrices), then proposes construction methods that generate E(fNOD)-optimal and/or χ²-optimal mixed-level SSDs while avoiding fully aliased columns. The constructions use equidistant designs and normalized difference matrices combined through Kronecker-sum-like operations (denoted ⊕A) and level-combining transformations, yielding large families of new optimal designs. The authors also prove that nonorthogonality (measured by fNOD) in the constructed designs is controlled by the nonorthogonality of the source designs, preserving orthogonality relationships under certain conditions. Extensive tables list many newly generated optimal mixed-level SSDs for practitioners to use.","Design optimality is based on minimizing average nonorthogonality: E(fNOD)=\frac{2}{m(m-1)}\sum_{i<j} fNOD(f_i,f_j), where fNOD(f_i,f_j)=\sum_{a=0}^{q_i-1}\sum_{b=0}^{q_j-1}\left(n_{ab}(f_i,f_j)-\frac{n}{q_i q_j}\right)^2. A related criterion is \chi^2(F)=\sum_{i<j} \frac{q_i q_j}{n} fNOD(f_i,f_j). Constructions combine designs via an Abelian-group Kronecker-sum operation (e.g., F ⊕_A D'), and via level-combining columns such as q_2(f_1-\frac{q_1-1}{2})\oplus(f_2-\frac{q_2-1}{2})+\frac{q_1q_2-1}{2} whose induced matrix is X_1\otimes X_2.","Theorems 2–5 give sufficient conditions under which constructed mixed-level SSDs are E(fNOD)-optimal and/or χ²-optimal and have no fully aliased columns, typically by ensuring row coincidence numbers (or weighted coincidence numbers) take at most two nearest values. Example 1 constructs a new optimal symmetric SSD F(18,3^12) from L9(3^4) and ND(3,2,3), with coincidence numbers 4 and 3 and no fully aliased columns. Example 2 constructs an E(fNOD)-optimal mixed-level SSD F(24,2^24 3^5) with constant coincidence number 13; Example 3 constructs a χ²-optimal F(24,2^72 3^10) with constant natural weighted coincidence number 78. Large catalogs (Tables 6–9) enumerate many newly generated optimal SSDs across run sizes (e.g., 24, 32, 36, …, 135) and mixed level structures.","The paper notes that analyzing data from mixed-level SSDs is “very important but complicated,” and that when there are many more factors than runs, severe nonorthogonality may prevent correct identification of active factors by existing analysis methods. It emphasizes that the statistical data analysis for SSDs remains an “important and challenging topic for further research,” implying the paper focuses on design construction rather than downstream inference methodology.","The constructions assume balanced columns and rely on availability of suitable equidistant designs and normalized difference matrices; for some parameter combinations these building blocks may not exist or may be hard to obtain. Optimality is with respect to E(fNOD) and/or χ² criteria, which are surrogate nonorthogonality measures and do not directly guarantee best screening performance under specific modeling assumptions (e.g., sparsity, interactions, noise). The work is largely combinatorial/theoretical and does not empirically benchmark factor-selection accuracy or robustness under practical complications like correlated errors, missing runs, or model misspecification.","The authors explicitly state that data analysis for SSDs “remains an important and challenging topic for further research,” motivated by high-dimensional, low-sample-size settings where nonorthogonality may hinder correct screening. They suggest that developments in high-dimensional analysis (e.g., genetics-style problems with thousands of variables and few observations) may be relevant to advancing SSD data analysis.","Develop self-starting or robust versions of these constructions that maintain good properties when balance is imperfect or when runs are missing. Provide software implementations to generate the proposed SSD families automatically from catalogs of difference matrices/orthogonal arrays, and include diagnostic tools for aliasing/nonorthogonality. Empirically evaluate downstream screening performance (power/FDR) under sparse main-effects and interaction contamination, and extend constructions to settings with constraints (blocked/split-plot/hard-to-change factors) or correlated observations.",1105.3816v1,https://arxiv.org/pdf/1105.3816v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:06:14Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Cost reduction|Other,Other,"Variable/General (illustrative 1D design variable in Example 1; 3 control variables in nitridation case: $\xi=\{F_{N_2,in},P_1,X_{N,1}\}$)",Energy/utilities|Other,Simulation study|Other,TRUE,C/C++|Other,Not provided,NA,"The paper develops a Bayesian sequential optimal experimental design approach for calibrating a graphite nitridation model, aiming to reduce posterior uncertainty in key model parameters. Designs are chosen by maximizing the expected Shannon information gain, shown to be equivalent to maximizing the mutual information (MI) between parameters and prospective observations, interpreted as an information-theoretic sensitivity/ dependence measure. MI is estimated from Monte Carlo samples using a k-nearest-neighbor (kNN) estimator (Kraskov et al., 2004), while Bayesian inference is performed via adaptive MCMC (Hybrid Gibbs Transitional MCMC) using the QUESO UQ library. The methodology is demonstrated on (i) a toy nonlinear regression example (contrasting MI-maximization vs maximum-entropy and random sampling) and (ii) a real engineering case for active nitridation of graphite where the controls are inflow conditions and observables include mass loss and outlet pressure. The paper also proposes monitoring entropy and KL divergence across sequential posteriors to decide when to stop experimentation and to diagnose potential conflict between data and model predictions.","The design utility is the expected information gain in parameter entropy: $U(d,\xi)=\int p(\theta\mid d,D)\log p(\theta\mid d,D)d\theta-\int p(\theta\mid D)\log p(\theta\mid D)d\theta$, and the optimal design solves $\xi^*=\arg\max_{\xi\in\Xi}\mathbb{E}_{d}[U(d,\xi)]$. They show $\mathbb{E}_{d}[U(d,\xi)] = I(\theta; d\mid D,\xi)$ (mutual information), so selection is $\xi^* = \arg\max I(\theta; d\mid D,\xi)$. MI is estimated from joint samples using Kraskov’s kNN estimator (Eq. 15), and sequential Bayesian updating uses the recursive Bayes rule (Eq. 3) with predictive distribution $p(d\mid D,\xi)=\int p(d\mid\theta,\xi)p(\theta\mid D)d\theta$ (Eq. 6).","In the toy problem, MI-maximization (IM) and maximum-entropy (ME) coincide under additive noise, but under multiplicative (design-dependent) noise ME can perform worse than random sampling while IM achieves faster uncertainty reduction and recovers parameters accurately using only ~2 experiments (as illustrated by posterior marginals after stage 2). In the nitridation case study, both ME and IM prioritize high-inflow scenarios early and outperform an “ascending” (original order) strategy in simulated-data learning rate; IM provides an adequate estimate after the first ~2 stages in the simulated case. For simulated nitridation data, monitoring KL divergence suggests experiments could be stopped after about stage >5 when KL falls below a threshold (example threshold shown at 0.5 nats). For real nitridation data, KL divergence remains large/erratic across stages, which the authors interpret as evidence of conflicting information between measurements and model predictions (potential model discrepancy or inconsistent measurements).",None stated.,"The approach relies on discretizing the design space and estimating MI via kNN, which can degrade in higher-dimensional joint spaces and is sensitive to the choice of k and sample size; the paper notes estimator efficiency decreases with dimensionality but does not fully quantify design robustness to these choices. The sequential greedy design maximizes one-step-ahead MI and is not guaranteed to be globally optimal over an N-experiment batch, potentially missing better multi-step design sequences. Practical adoption may be limited by computational cost (repeated MCMC and Monte Carlo forward propagation per candidate design), especially for more complex physics models than the simplified 1D-flow nitridation model used here.",None stated.,"Extend the method to continuous (non-discretized) design optimization and/or use surrogate models to reduce the computational burden of repeated MI evaluations. Develop robustness checks and adaptive selection of k/sample sizes for MI and KL estimators, especially for higher-dimensional parameter/observation spaces. Incorporate more realistic discrepancy models (non-Gaussian errors, correlated/heteroskedastic noise, model bias terms) and formal conflict/outlier handling to address the real-data inconsistencies observed in the nitridation case study.",1107.1445v1,https://arxiv.org/pdf/1107.1445v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:06:49Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation,A-optimal|Other,Variable/General (examples include J∈R^{50×7} and an ODE model estimating 4 parameters; m up to 500 candidate measurements with a budget of 20 or 30),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Not provided,http://www.iwr.uni-heidelberg.de/groups/agbock/FILES/Bock1981.pdf|http://www.cs.berkeley.edu/~fateman/papers/overload-small.pdf|http://www.sbsi-sol-optimize.com/manuals/QPOPT%20Manual.pdf|http://www.jstor.org/stable/2337232|http://www.optimization-online.org/DB_HTML/2011/05/3037.html|http://dx.doi.org/10.1007/BF01395810,"The paper studies slow convergence and stability issues of quasi-Newton sequential quadratic programming (SQP) methods when solving relaxed optimal experimental design problems where measurement-selection weights and (optionally) external controls are optimized jointly. The design criterion is based on minimizing the A-optimality objective, i.e., $\operatorname{Tr}((J^T W(w) J)^{-1})$ (or with prior information $\alpha^{-1}I$ added), where $W(w)=\operatorname{diag}(w)$ and $\sum_i w_i=m_{\max}$ with $0\le w_i\le 1$. They introduce a structured two-weight model problem and prove that its absolute condition number blows up as the prior-information parameter $\alpha\to 0$ (i.e., as the initial design gets better), explaining stagnation near the solution. A nonlinear left-preconditioning transformation $h(z)=-z^{-2}$ is proposed, leading to the preconditioned objective $-\{\operatorname{Tr}((J^T W(w) J)^{-1})\}^{-2}$ which preserves minimizers but yields uniformly bounded absolute conditioning in the model case. Numerical experiments on randomized linear-design instances and a nonlinear ODE-based design example (FitzHugh–Nagumo) show substantially improved SQP stability, fewer iterations, and elimination of QP subproblem breakdowns under the preconditioned formulation.","Relaxed sampling/measurement selection design: minimize $\operatorname{Tr}((J^T(q)\,W(w)\,J(q))^{-1})$ subject to $w\in[0,1]^m$, $\sum_i w_i=m_{\max}$ and controls $q\in\Theta$ (when present), with $W(w)=\mathrm{diag}(w)$. Model problem with prior information: minimize $\operatorname{Tr}((\alpha^{-1}I + w_1 v_1 v_1^T + w_2 v_2 v_2^T)^{-1})$ s.t. $w_1+w_2=1$, $0\le w_i\le 1$. Proposed left-preconditioning: apply $h(z)=-z^{-2}$ to the objective, yielding $\min -\{\operatorname{Tr}((J^T W(w)J)^{-1})\}^{-2}$ (and analogously for the model problem), which keeps the same minimizer but improves conditioning.","For the two-weight orthogonal model problem ($v_1^Tv_2=0$, $\|v_1\|=\|v_2\|=1$), the optimizer is $w_1=w_2=1/2$ and the absolute condition number grows unboundedly as $\alpha\to 0$, scaling like $\kappa_{\mathrm{abs}}\sim \alpha^{-3}$ (exact expression given in Eq. (4)). Under the transformed objective $\tilde f=-f^{-2}$, the same minimizer is retained and the absolute condition number is constant, $\kappa_{\mathrm{abs}}=2$ for all $\alpha>0$. In randomized linear design experiments (e.g., selecting 20 of 50 candidate measurements with varying prior information $\alpha\in[10^{-6},1]$), the preconditioned SQP iteration counts stabilize (around ~40) while the unpreconditioned formulation becomes increasingly difficult and exhibits QP-solver iteration-limit breakdowns for small $\alpha$. In size-scaling tests with matrices $J\in\mathbb{R}^{50n\times 7}$ for $n=1,\dots,10$, preconditioning substantially lowers average SQP iterations across sizes. In the FitzHugh–Nagumo nonlinear ODE design, the reported average iterations drop from about 260.8 (unpreconditioned) to 46.0 (preconditioned), an average speed-up factor of about 3.9 (Table 1).","The authors state they cannot provide a complete theoretical justification that bad absolute conditioning is the sole cause of slow SQP convergence for the full problem (2), but present it as a strongly supported hypothesis. They also note the understanding of why relaxed designs often yield (nearly) integer weights is incomplete (citing prior work), and rounding may still be needed when non-integer weights occur. For the size-scaling experiment, they state it is not possible to tell conclusively whether iteration counts increase with problem size for the preconditioned variant from that experiment alone.","The preconditioner $h(z)=-z^{-2}$ is motivated and proved effective for a particular structured A-criterion model; there is no analogous conditioning proof for general $J(q)$, other optimality criteria (e.g., D-, I-, G-optimal), or broader constraint sets, so generality is partly empirical. Performance is evaluated mainly by SQP iteration counts/breakdowns rather than wall-clock time across implementations; the method uses expensive ingredients (higher-order sensitivities, quad-double arithmetic, AD) that may dominate runtime in practice. The nonlinear example is limited to a single ODE benchmark (FitzHugh–Nagumo) with small parameter dimension, leaving robustness across diverse real experimental systems, noise models, and larger parameter sets less established. Because the nonlinear problem can have multiple local minima, improvements in convergence speed may depend on initialization and may not translate directly to better global design quality.","The conclusion frames the presented evidence as supporting the conditioning hypothesis and suggests the preconditioning transformation is particularly beneficial for larger problems, but does not lay out a detailed list of specific future research directions beyond this outlook.","Extend the conditioning analysis and preconditioning ideas beyond A-optimality to other common criteria (D-, I-, G-, E-optimal) and to Bayesian/robust design formulations with parameter uncertainty. Study behavior under unknown/estimated nuisance parameters, autocorrelated or heteroscedastic errors, and Phase I/Phase II settings to improve practical applicability. Provide software artifacts (e.g., an open-source implementation) and benchmark wall-clock performance on standardized OED test suites, including large-scale and constrained-control problems. Investigate global-optimization or multi-start strategies combined with the preconditioned formulation, and analyze how the transformation affects landscape properties (e.g., local minima structure) and constraint handling in SQP.",1108.1689v1,https://arxiv.org/pdf/1108.1689v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:07:37Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Model discrimination|Other,Bayesian D-optimal|Other,"Variable/General (design variables d of dimension n_d; examples include 1D (d), 2D (T0, φ), and batch designs with N experiments giving N·n_d design variables)",Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,C/C++|Other,Not provided,http://www.jhuapl.edu/SPSA/|http://sourceforge.net/projects/cantera/,"The paper develops a simulation-based Bayesian optimal experimental design framework for nonlinear models, targeting parameter inference by maximizing expected Shannon information gain (mutual information) from prior to posterior. Expected information gain is estimated via a nested Monte Carlo scheme (with a sample-reuse variant to reduce cost), and the resulting noisy objective is optimized using stochastic approximation methods, chiefly SPSA and Nelder–Mead nonlinear simplex. To make repeated likelihood evaluations feasible for expensive simulators, the authors build generalized polynomial chaos surrogates that depend jointly on model parameters and design variables, with coefficients computed by dimension-adaptive sparse quadrature. The approach supports single-experiment design, batch design of multiple experiments over a product design space, and discusses (but does not fully solve) sequential design extensions. Demonstrations include a toy nonlinear model and a shock-tube autoignition combustion-kinetics application, showing that optimized designs can substantially outperform naive choices (e.g., factorial corner designs) and that surrogates can yield orders-of-magnitude computational savings with acceptable design accuracy.","The design criterion is expected information gain: $U(d)=\mathbb{E}_{y\mid d}\left[ D_{\mathrm{KL}}\big(p(\theta\mid y,d)\,\|\,p(\theta)\big)\right]$, equivalent to mutual information $I(\theta;y\mid d)$. Using Bayes’ rule, it can be written as $U(d)=\iint \{\log p(y\mid \theta,d)-\log p(y\mid d)\}\,p(y\mid \theta,d)p(\theta)\,d\theta\,dy$, estimated by nested Monte Carlo with evidence $p(y\mid d)=\int p(y\mid \theta,d)p(\theta)\,d\theta$ approximated by an inner Monte Carlo sum. For expensive forward models $y=G(\theta,d)+\varepsilon$, $G$ is replaced by a polynomial chaos surrogate built via non-intrusive spectral projection and dimension-adaptive sparse quadrature.","In the toy nonlinear example, the single-experiment expected-utility surface exhibits local maxima (e.g., near $d=0.2$ and $d=1.0$), while the optimal two-experiment batch design is a mixed pair (approximately $(0.2,1.0)$) rather than repeating the single-experiment optimum, illustrating non-additivity of information gain across experiments. In the combustion-kinetics case with design variables $(T_0,\phi)$, both full-model and PC-surrogate evaluations identify a similar single-experiment optimum near $(T_0,\phi)\approx(975\,\mathrm{K},0.5)$. For two-experiment batch design in a 4D space, stochastic optimization concentrates both experiments near $T_0\approx 975\,\mathrm{K}$ (with less sensitivity in $\phi$), and a two-experiment optimal design can outperform a four-point factorial design in posterior tightness. The surrogate approach reduces computational cost dramatically; the paper reports roughly 3.5 orders-of-magnitude speedup in one study (e.g., $\sim 10^4$ full-model runs to build a surrogate versus $\sim 5\times 10^7$ full-model evaluations for direct optimization).","The authors note that they do not incorporate explicit experimental costs or resource constraints in the objective, emphasizing that such additions are problem-specific. They also state that a rigorous treatment of sequential design via dynamic programming is beyond the paper’s scope, discussing only greedy sequential updating. They highlight that computational expense remains a central challenge, motivating the surrogate-and-stochastic-optimization strategy.","The optimization relies on stochastic objective estimates whose bias/variance tradeoffs (nested Monte Carlo and sample reuse) can affect design ranking, yet guidance for selecting $n_{\text{in}}$, $n_{\text{out}}$, and optimizer hyperparameters is largely empirical and problem-dependent. The polynomial chaos surrogate assumes sufficient smoothness of outputs in joint parameter–design space; performance may degrade for strongly non-smooth responses or discontinuities, and the framework may need alternative surrogates (e.g., GP/emulators) in such regimes. The demonstrations are limited to low-to-moderate design dimension (e.g., 2D, 4D) and a small number of inferred parameters; scalability to much higher-dimensional design or parameter spaces is not fully established.","The paper suggests developing more efficient, adaptive construction of polynomial chaos expansions, potentially coupled directly with stochastic optimization. It proposes incorporating uncertainty in imposed design conditions via a hierarchical Bayesian treatment. It also calls for studying structural model inadequacy and its impact on ‘optimal’ designs, and extending the framework to sequential experimental design using dynamic programming ideas and possibly sequential Monte Carlo.","Implementing and benchmarking alternative surrogate models (e.g., Gaussian-process emulators, multifidelity surrogates, or local adaptive surrogates) could improve robustness when outputs are non-smooth in $(\theta,d)$. Providing principled, automated tuning rules for nested Monte Carlo budgets and stochastic-optimization parameters (possibly via adaptive sampling or variance reduction) would increase practical usability. Extending the framework to constrained, multi-objective design (information gain plus explicit experimental cost/risk) and to larger-scale multivariate experiments with correlated errors would broaden applicability.",1108.4146v3,https://arxiv.org/pdf/1108.4146v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:08:33Z
FALSE,NA,NA,Not applicable,Not specified,Other,Case study (real dataset),FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper describes an instructional physics laboratory activity comparing three measurement techniques (stopwatch time-of-flight, maximum-height by eye with a meter stick, and high-speed camera time-of-flight) to illustrate random versus systematic uncertainty in velocity estimation for a spring-loaded projectile launcher. Students collect repeated measurements (typically 10 time readings and 10 height readings per student) and compute average launch speed, using the variability across repeats to quantify statistical uncertainty (standard deviation of the mean). A high-speed camera measurement is added to isolate and demonstrate systematic timing bias from human reaction time in stopwatch measurements. Using data from 26 students, the authors show that height-based speed estimates have much smaller uncertainty than stopwatch time-of-flight and that camera time-of-flight agrees closely with height-based results while stopwatch results exhibit systematic offsets. The work is framed as “experimental design” pedagogically, but it does not propose or evaluate formal DOE methods (e.g., factorial/optimal designs) beyond comparing measurement methods and replication.","Launch speed from time-of-flight: $v_0=\frac{1}{t}\left(\tfrac{1}{2}gt^2-h\right)=\tfrac{1}{2}gt-\frac{h}{t}$, where $t$ is measured flight time, $h$ is launch height above the floor, and $g$ is gravitational acceleration. Launch speed from maximum height: $v_0=\sqrt{2gH}$, where $H$ is the measured peak height. Uncertainty is characterized via the standard deviation and standard deviation of the mean over repeated trials.","For the Pasco launcher (medium setting), typical time-of-flight is ~$1.4\,\text{s}$ with uncertainty ~$0.04\,\text{s}$ (SD of mean over 10 trials), while typical maximum height is ~$2.15\,\text{m}$ with uncertainty ~$0.007\,\text{m}$. The corresponding velocity uncertainties are about ~$0.2\,\text{m/s}$ (stopwatch time-of-flight) versus ~$0.01\,\text{m/s}$ (maximum-height method). High-speed camera time-of-flight (250 fps) produces speeds close to the height-based speeds, while stopwatch-based speeds show larger scatter and systematic offsets attributable to reaction time. In the class dataset (26 students), only 2/26 height-based uncertainties exceeded ~$0.02\,\text{m/s}$ (the camera-based uncertainty benchmark mentioned).",None stated.,"The study is an instructional demonstration rather than a statistically designed experiment: there is no randomized or counterbalanced order of methods, and potential learning/fatigue/order effects could influence stopwatch reaction-time errors. The comparison is based on one class and a specific apparatus (including multiple launchers), so generalizability to other settings/launch speeds and to different student populations is unclear. Systematic differences between launchers are noted but not modeled or controlled, which can confound between-group comparisons.",None stated.,"A more formal study could randomize or counterbalance method order and explicitly model launcher-to-launcher variability (e.g., treating launcher as a blocking factor) to separate apparatus variation from measurement-method effects. Extending the activity to additional techniques (e.g., photogates, smartphone high-speed video) and to non-ideal effects (air drag, spin) would broaden the systematic-uncertainty discussion. Providing a simple analysis script and standardized data-collection template would improve reproducibility across courses.",1108.4944v1,https://arxiv.org/pdf/1108.4944v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:08:59Z
TRUE,Other,Model discrimination|Parameter estimation|Other,Not applicable,2 factors (method variant; OO-Method modelling competence) plus 3 problems/objects; blocked subject–object study with 2 groups,Other,Case study (real dataset)|Other,TRUE,None / Not applicable|Other,Not provided,http://www.surveygizmo.com/s/157014/in-take-survey|http://www.surveygizmo.com/s/175633/olivanova-assessment|http://www.surveygizmo.com/s/157035/problem-1-survey|http://www.surveygizmo.com/s/157312/problem-2-survey-a|http://www.surveygizmo.com/s/157313/problem-2-survey-b|http://www.surveygizmo.com/s/157314/problem-3-survey|http://www.surveygizmo.com/s/190622/ca-assessment|http://www.surveygizmo.com/s/174619/conceptual-model-quality-evaluation-likert|http://www.random.org|http://www.care-t.com|http://arxiv.org/abs/1101.0105,"This technical report designs and documents a pilot controlled experiment comparing two variants of a model-driven system development approach: OO-Method alone versus Communication Analysis integrated with OO-Method (CA+OOM). The experiment is a blocked subject–object study in which student subjects solve three information-systems problems; subjects are randomly assigned to two groups that receive the CA treatment at different points (staggered design) to mitigate maturation/history. Response variables include efficiency (time to finish) and effectiveness via conceptual-model quality dimensions (syntactic correctness, semantic validity, semantic completeness, and pragmatic comprehension), measured using Likert ratings and (primarily) a statement-checking template; acceptance is measured via post-task questionnaires (perceived ease of use, perceived usefulness, intention to use). The report also describes instrumentation (training, competence tests, helpdesk-based requirements elicitation) and logs pilot operation results/lessons learned; due to only three completing subjects, no statistical hypothesis testing is performed.","Key response-variable formulas include: $\text{Time} =$ hours spent (self-reported); $\text{Validity}_{QUE}=1-\frac{\#\text{wrong answers}}{\#\text{questions}}$; $\text{Completeness}_{QUE}=\frac{\#\text{correct answers}}{\#\text{questions}}$; $\text{Validity}_{STA}=1-\frac{\#\text{incorrect statements}}{\#\text{statements}}$; $\text{Completeness}_{STA}=\frac{\#\text{correct statements}}{\#\text{statements}}$; comprehension measures are means of Likert difficulty ratings over answered items (QUE/STA). Acceptance measures are means of Likert items per construct (PEOU, PU, ITU).","In the pilot, only 3 subjects completed the course/experiment, so the authors report descriptive data rather than statistically testing hypotheses. They report model completeness (via statement template) ranging roughly from ~49% to ~96% across subjects/problems, and note improvements after Communication Analysis training for some subjects. They also report operational metrics for helpdesk elicitation, estimating an average answering time per question of about 5:12 minutes in Problem 3 and discuss scalability constraints for larger samples. Time spent by the evaluator to assess models varied from ~0:53 to ~3:00 hours per model depending on problem and completeness.","The authors state that the pilot had too few completing subjects (3), preventing statistical analysis and significant conclusions. They note limited experimental control because problem-solving work was done as homework over weeks, introducing environmental noise and potential confounds. They acknowledge measurement limitations: only one evaluator and primarily one quality-assessment method (list of statements) was used; Likert and question-list instruments were not used due to time/resources, and validity errors were not checked systematically.","The design uses a fixed problem order for all subjects (Problems 1→2→3), so learning/fatigue effects are confounded with problem differences even with staggered treatment timing. The “modelling competence” factor is partly measured with an OO-Method test/exercise that may not capture requirements-elicitation skill, leading to imperfect blocking and potential imbalance (the report documents reclassification/swapping). Reliance on self-reported time and acceptance ratings may introduce bias; no preregistered analysis plan or specified statistical model is detailed for the full study (e.g., mixed-effects for repeated measures).","The authors propose repeating/replicating the experiment with a sufficient number of subjects and are open to external collaboration/replication packages. They recommend improving instrumentation, including developing/using the list-of-questions (QUE) and Likert (RAT) quality instruments, adding multiple evaluators and inter-reviewer agreement procedures, and improving Communication Analysis competence assessment with modelling and elicitation exercises. They also suggest studying the quality-assessment techniques themselves and improving helpdesk-based elicitation protocols/tools for scalability.","Provide a pre-registered statistical analysis approach for the intended full study (e.g., linear mixed-effects models with subject random effects, problem fixed effects, and treatment-by-competence interactions) to reduce researcher degrees of freedom. Add counterbalancing or Latin-square style rotation of problem order (or parallel problem sets) to reduce confounding with maturation/fatigue. Release anonymized datasets, instruments, and elicitation catalogues (or a replication package) to enable independent replication and benchmarking; implement an automated helpdesk/Q&A mapping tool and evaluate its impact on elicitation consistency and experimenter workload.",1111.0562v1,https://arxiv.org/pdf/1111.0562v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:09:45Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Prediction|Other,Other,1 factor (unknown Hamiltonian frequency parameter ω; later discussion generalizes to more complex systems),Other,Exact distribution theory|Simulation study|Other,TRUE,MATLAB,Not provided,http://arxiv.org/abs/1102.3700|http://arxiv.org/abs/quant-ph/0603116|http://dx.doi.org/10.1063/1.1751377|http://dx.doi.org/10.1063/1.2956962|http://dx.doi.org/10.1016/j.nimb.2010.02.062|http://dx.doi.org/10.1214/aoms/1177728069|http://dx.doi.org/10.1103/PhysRevA.67.042322|http://dx.doi.org/10.1063/1.1773161,"The paper develops an online adaptive Bayesian experimental design protocol to estimate dynamical parameters of a simple qubit Hamiltonian under strong (projective) measurement. At each step, the next experiment (measurement time t) is chosen to maximize expected utility computed from the current posterior, using information gain (posterior entropy) as the main utility, and also discussing a greedy alternative based on minimizing posterior variance (equivalently local Bayes risk under squared-error loss for a single step). For the single-parameter model $H=(\omega/2)\sigma_z$, the design variable is the evolution time t and the measurement outcome is binary in the $\sigma_x$ basis with likelihood $\cos^2(\omega t/2)$. The authors derive recursive expressions for local (greedy) and globally optimal Bayes risks, and show numerically (by exploring the full decision tree under a uniform prior) that the greedy strategy is close to globally optimal in this setting. Performance comparisons indicate exponentially improving accuracy (decreasing posterior variance/AMSE) versus offline strategies, and show that Nyquist-rate sampling is unnecessary and can be suboptimal, including under a more realistic noisy/relaxing model.","Bayesian updating is based on $\Pr(\Theta\mid D,E)=\Pr(D\mid\Theta,E)\Pr(\Theta\mid E)/\Pr(D\mid E)$ and the predictive distribution $\Pr(D_1\mid E_1,D,E)=\int \Pr(D_1\mid\Theta,E_1)\Pr(\Theta\mid D,E)\,d\Theta$. The expected-utility design rule is $U(E_1)=\sum_{D_1}\Pr(D_1\mid E_1,D,E)\,U(D_1,E_1)$ with information-gain utility $U(D_1,E_1)=\int \Pr(\Theta\mid D_1,E_1,D,E)\log \Pr(\Theta\mid\cdot)\,d\Theta$ (maximize expected posterior entropy reduction). For the qubit example, $H=(\omega/2)\sigma_z$ and with experiment $E\equiv t$, the likelihood is $\Pr(D=0\mid\omega,t)=\cos^2(\omega t/2)$ (binary outcome in the $\sigma_x$ basis). The greedy alternative uses utility as negative posterior variance $V(D_1,E_1)=-\int \Pr(\Theta\mid\cdot)\,(\Theta^2-\mu)^2\,d\Theta$ with $\mu=\int \Pr(\Theta\mid\cdot)\,\Theta\,d\Theta$; the paper also gives recursive forms for local and global Bayes risks over decision trees.","The authors report (building on and comparing to prior work) that adaptive Bayesian design yields exponentially improved estimation accuracy (posterior variance/AMSE decreases approximately exponentially with the number of measurements N), whereas offline strategies improve at best linearly in N. By numerically exploring every branch of the decision tree under a uniform prior on $\omega\in[0,1]$ (rather than Monte Carlo alone), they find the greedy, locally-optimal strategy has Bayes risk close to the globally optimal strategy for this special-case prior. In design comparisons restricted to $t\in[0,N_{\max}\pi]$ with $N_{\max}=12$, the “optimized” strategies (global utility maximization via MATLAB fmincon initialized at Nyquist times) outperform Nyquist-rate sampling, demonstrating Nyquist sampling can be suboptimal for strong-measurement probability sampling. They also show (via a second model) that the advantage persists and becomes more pronounced under added noise (25%) and an additional relaxation process ($T_2$) that exponentially decays the signal (half-value at $t=10\pi$).","They note that squared-error loss (and thus AMSE) is not practically ideal for estimating quantum-system parameters when the downstream goal (e.g., robust control) benefits from a full distribution over Hamiltonians rather than a single point estimate; they argue relative entropy is a more appropriate loss for probability estimators. They also emphasize that their near-optimality finding for the greedy strategy is demonstrated numerically for the special case of a uniform prior (i.e., it is not claimed for arbitrary priors).","The main worked example is a single-parameter, single-qubit Hamiltonian with a highly idealized measurement model; scalability of exhaustive decision-tree evaluation to higher-dimensional Hamiltonians is not demonstrated, and the computational cost of online optimization in larger settings is left unclear. The design space is artificially restricted in comparisons (measurement times limited to $[0,N_{\max}\pi]$ and optimization initialized at Nyquist times), which may affect conclusions about absolute optimality and robustness of the optimizer. Practical implementation issues—e.g., model mismatch beyond the simple noise/relaxation variant, calibration errors in t, and correlated/temporally drifting noise—are not systematically analyzed. No released code or reproducible workflow is provided, making it harder to validate numerical decision-tree computations and optimization details.","They state an expectation that Bayesian adaptive methods will remain useful for more complicated quantum systems, especially for applications such as optimal control where having a distribution over Hamiltonians is more valuable than a single best estimate. They also discuss that using relative entropy (information gain) as the success metric is more appropriate for estimating distributions and motivates the information-gain-maximizing algorithm as optimal under that loss.","Develop scalable approximations for multi-parameter Hamiltonian learning (e.g., particle filters, variational Bayes, Bayesian experimental design with surrogate models) and analyze computational/accuracy tradeoffs relative to exhaustive decision trees. Extend the framework to handle unknown nuisance parameters (readout error, decoherence rates, timing jitter) with joint Bayesian estimation and robust/adversarial design criteria. Provide theoretical guarantees (bounds on Bayes risk, regret, or convergence rates) beyond the uniform-prior/single-qubit setting and study sensitivity to prior misspecification. Release open-source implementations and reproducible benchmarks across standard quantum characterization tasks (multi-qubit, process tomography, adaptive control experiments) to validate performance in realistic laboratory conditions.",1111.0935v1,https://arxiv.org/pdf/1111.0935v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:10:28Z
FALSE,NA,NA,Not applicable,Not specified,Theoretical/simulation only,Exact distribution theory|Simulation study|Other,TRUE,Other,Not provided,NA,"This paper is a theoretical study of mechanical response and phonon/zero-mode structure in two-dimensional isostatic lattices obtained by twisting (rotating triangles in) the kagome lattice by an angle \(\alpha\). It shows that for the twisted kagome family the bulk modulus \(B\) vanishes for \(\alpha\neq 0\), leading to maximally auxetic behavior (Poisson ratio approaching \(-1\)) and strong sensitivity of floppy modes to boundary conditions (free vs periodic vs rigid). Using Maxwell/Calladine counting with states of self stress, the authors explain why free-boundary samples exhibit many surface floppy modes while periodic-boundary samples have only the trivial zero modes. They derive a long-wavelength effective elastic energy in which \(\phi\) (an internal unit-cell distortion) couples gauge-like to dilatation, producing conformal invariance when \(B=0\) and implying Rayleigh-type surface waves as the continuum limit of edge floppy modes. Surface-mode decay and dispersion are characterized via determinant conditions on the lattice dynamical matrix and compared against numerical calculations and transfer-matrix results for various \(\alpha\).","Key relations include Maxwell–Calladine counting \(N_0=dN-N_B+S\) linking zero modes \(N_0\), constraints \(N_B\), and states of self-stress \(S\). The effective long-wavelength energy for the twisted kagome lattice is given (Eq. (1)) as \(E=\tfrac12\int d^2x\,[2\mu\tilde u_{ij}^2+K(\phi+\xi u_{ii})^2+V(\partial_i\phi)^2-W\,\Gamma_{ijk}u_{ij}\partial_k\phi]\), where the combination \((\phi+\xi u_{ii})\) enforces vanishing bulk modulus. Surface-mode decay is obtained from \(\det[\omega^2 I-D(\mathbf q_{\parallel}, i\ell^{-1})]=0\) (and for zero-frequency surface modes \(\det[D(\mathbf q_{\parallel}, i\ell^{-1})]=0\)), yielding a decay length \(\ell(\mathbf q_{\parallel})\) (Appendix B).","For the twisted kagome lattice with nearest-neighbor springs only, the bulk modulus is rigorously \(B=0\) for all \(\alpha\neq 0\), while the shear modulus remains \(\mu=\sqrt{3}k/8\), implying Poisson ratio \(\sigma=(B-\mu)/(B+\mu)\to -1\). Under periodic boundary conditions the twisted kagome lattice has only the trivial zero modes (reported as \(N_0=S=2\) at \(\mathbf q=0\)), whereas under free boundary conditions it retains \(N_0=2(N_x+N_y)-1\) zero modes that are surface-localized. Twisting produces characteristic frequency and length scales analogous to adding weak next-nearest-neighbor springs: \(\omega_\alpha\sim\sqrt{k|\sin\alpha|}\) and \(\ell_\alpha\sim 1/\omega_\alpha\), and surface-mode decay approaches the Rayleigh limit \(q_y'\approx q_x\) as \(q_x\to 0\). The paper also identifies other kagome-derived isostatic lattices (e.g., uniaxial pgg symmetry) where both surface and bulk floppy modes can coexist, with isotropic behavior near \(\Gamma\) but anisotropic/zero-mode structure at larger wavenumber.",None stated.,"This is not a DOE/experimental-design study; results are primarily analytical with supporting numerics and may not address robustness to disorder, damping, or non-central-force interactions beyond brief mentions (e.g., adding NNN springs/bending). The work focuses on idealized periodic lattices and specific boundary conditions; practical experimental realizations (fabrication tolerances, finite stiffness hinges, friction, imperfections) are not validated with real data. Several claims (e.g., scaling behavior of surface-mode decay and behavior for other surface orientations) are only partially explored, with some directions deferred to future work.","The authors state that surface behavior for other surface orientations (e.g., surfaces along different directions such as \(x=0\) rather than \(y=0\)) is more complicated and will be treated in a future publication.","Extending the analysis to disordered or weakly perturbed twisted kagome networks (random bond stiffness, missing bonds, geometric disorder) would clarify robustness of auxeticity and edge-mode localization. Developing self-starting/finite-size predictive formulas for edge-mode counts and decay lengths across arbitrary boundary shapes could improve applicability to engineered metamaterials. Providing open-source implementations for eigenmode computation/transfer-matrix evaluation and benchmarking against experiments or high-fidelity simulations (including bending, friction, and nonlinearities) would strengthen practical adoption.",1112.1109v1,https://arxiv.org/pdf/1112.1109v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:11:08Z
TRUE,Optimal design|Computer experiment|Sequential/adaptive|Other,Prediction|Screening|Parameter estimation|Other,D-optimal|Space-filling|Minimax/Maximin|Other,Variable/General (examples include 2 factors for analytical tests; 8 factors for 25-bar truss; 10 factors for 10-bar truss),Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper reviews and competitively compares criteria for constructing “optimal” designs of experiments (DoE) used in sampling-based sensitivity analysis of expensive numerical models, with an emphasis on discrete (or discretized) design spaces. It studies space-filling criteria (Audze–Eglais potential energy, Euclidean maximin distance, modified L2 discrepancy) and regression-oriented optimality (D-optimality), as well as orthogonality/correlation-based criteria (condition number, Pearson, Spearman, Kendall). Designs are generated both as unrestricted (free) designs and under Latin Hypercube (and mixed Latin Hypercube) restrictions, and also via sequential augmentation strategies. Optimization of each criterion is performed using simulated annealing, and designs are compared by cross-evaluation under all criteria plus performance in estimating sensitivity via Spearman rank correlations on analytical test functions and truss-structure models. The main conclusion is that ML2-optimized LH designs give the best overall sensitivity-prediction performance and projective (non-collapsing) properties, while CN and correlation-only criteria can yield poor space-filling unless constrained by LH sampling; D-optimal designs require a Bayesian-type modification to avoid duplicated points in discrete settings.","Space-filling/optimality criteria include: Audze–Eglais potential energy $E_{AE}=\sum_{i<j} 1/L_{ij}^2$; Euclidean maximin formulated as minimizing $E_{EMM}=-\min_{i<j} L_{ij}$; modified $L_2$ discrepancy $E_{ML2}$ computed on variables scaled to $[0,1]$ (given as a closed-form double-sum/product expression); and D-optimality as $E_{Dopt}=-\det(Z^T Z)$ where $Z$ is the regression model matrix (illustrated for second-order polynomials). Orthogonality criteria are based on condition number $E_{CN}=\mathrm{cond}(X^T X)=\lambda_{max}/\lambda_{min}$ and correlation-sum objectives such as $E_{PMCC}=\sqrt{\sum_{i<j} c_{ij}^2}$, $E_{SRCC}=\sqrt{\sum_{i<j} \rho_{ij}^2}$, $E_{KRCC}=\sqrt{\sum_{i<j} \tau_{ij}^2}$. Sensitivity is assessed via Spearman correlation between each input and response, and the SA error metric is $\varepsilon=\frac{1}{k}\sum_{i=1}^k |\tilde\rho_{x_i,z}-\rho_{x_i,z}|$.","Across analytical-function tests, ML2-optimized Latin hypercubes achieved the lowest average sensitivity-correlation error and smaller variance than competing criteria; D-optimal designs were typically slightly worse but competitive. Correlation-based criteria (PMCC/SRCC/KRCC) produced very poor space-filling in unrestricted designs, but under LH restrictions their SA accuracy became “very good” albeit with higher variance. Condition-number (CN) designs were consistently poor, including on the 10-bar truss where CN showed very large errors (e.g., mean errors on the order of ~15–27% depending on response component and design size per Table 4), while most other criteria yielded much smaller mean errors. For projective (non-collapsing) behavior under projection, ML2 designs had the fewest redundant (collapsing) points on average, outperforming other space-filling criteria. D-optimality in discrete spaces tended to create duplicated points unless modified (Bayesian/regularization-style extension of the information matrix), which the authors implemented to obtain usable D-optimal designs.","The authors note they restrict attention mainly to DoE in discrete domains (continuous domains only via discretization), leaving other continuous-domain DoE approaches out of scope. They acknowledge that simulated annealing provides no guarantee of reaching the global optimum and that they did not pursue more robust/reliable optimization beyond their chosen SA setup, presenting results “without any deeper search” for improved optimizers. They also highlight a drawback of D-optimality in this setting: the need for a Bayesian modification/extension of the information matrix to eliminate duplicates, and uncertainty about which additional regression terms are sufficient/appropriate.","The study’s conclusions are partly contingent on the specific simulated-annealing settings (cooling schedule, iteration budgets, move operators) and may change under alternative optimizers or tuning; sensitivity to these choices is not systematically analyzed. The evaluation focuses on SRCC-based sensitivity estimation (monotonic relationships), so findings may not generalize to variance-based/global SA measures (e.g., Sobol’ indices) or non-monotonic models. Comparisons mix multiple design sizes and discretization schemes; the impact of discretization granularity and rounding (for mixed-level variables) on criterion values and SA accuracy is not isolated. Practical guidance on choosing sample size versus dimension (especially for complex models like the 25-bar case where all designs performed poorly) could be strengthened with power/accuracy targets or stopping rules.","The paper explicitly flags that, for the Bayesian-style extension used to avoid duplicated points in D-optimal designs, it is “not obvious which terms should be added into the matrix $Z$” and that this question “should be the subject to additional research.” It also notes broader continuous-domain DoE generation possibilities are beyond the paper’s scope, implicitly leaving them for future work.","Extending the competitive comparison to additional SA objectives (e.g., Sobol’ indices, derivative-based measures, model-discrepancy goals) would clarify whether ML2’s advantage holds beyond SRCC-based screening. A systematic robustness study varying noise, model stochasticity, autocorrelation, and unknown/estimated input distributions could improve applicability to real simulation/experimental settings. Developing and releasing reproducible software (e.g., implementations of the criteria and SA optimizer operators for free vs. LH/mixed LH) would enable broader adoption and benchmarking. Investigating multi-objective or constrained optimal designs that jointly optimize space-filling, orthogonality, and projective/non-collapsing properties—possibly with adaptive/sequential stopping based on bootstrap CI widths—would better match practical sensitivity-analysis workflows.",1201.0942v2,https://arxiv.org/pdf/1201.0942v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:11:51Z
TRUE,Sequential/adaptive|Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Other,A-optimal|Other,Variable/General (1-qubit state parameterized by 3 real parameters / Bloch vector components),Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an adaptive experimental design for one-qubit mixed-state tomography, where each new projective measurement is chosen based on previous outcomes via the classical A-optimality (average-variance) criterion. The authors derive an analytic solution for the A-optimal update in the 1-qubit case with rank-1 projective measurements, substantially reducing the computational burden relative to generic nonlinear optimization. Estimation is performed using maximum likelihood, and performance is evaluated using two loss functions: squared Hilbert–Schmidt distance and infidelity. Monte Carlo studies compare two adaptive schemes (A-optimal tuned to each loss) against nonadaptive XYZ (Pauli) tomography and uniformly random measurement directions, showing adaptive A-optimal designs achieve markedly lower expected infidelity and exhibit near O(N^{-1}) scaling versus the nonadaptive O(N^{-3/4}) behavior. The work positions A-optimality as a practical, statistically principled update rule for finite-data quantum state estimation when fast updates are required.","The projective measurement along Bloch direction $a$ uses POVM elements $\Pi_{\pm}(a)=\tfrac12(I\pm a\cdot\sigma)$ with outcome probabilities $p(\pm;a|s)=\tfrac12(1\pm s\cdot a)$. The single-trial Fisher information is $F(a,s)=\dfrac{aa^T}{1-(a\cdot s)^2}$ and the (approximated) cumulative Fisher matrix is $\tilde F_{n+1}=\sum_{i=1}^{n+1}F(a_i,s)$. The A-optimal next measurement chooses $a_{n+1}=\arg\min_{\|a\|=1}\operatorname{tr}\{H(\hat s_n)\,\tilde F_{n+1}^{-1}\}$, and the paper provides an analytic closed-form solution via an eigenvector expression (Theorem 1).","Monte Carlo simulations (up to $N=1000$ for average expected loss; up to $N=10000$ pointwise) show adaptive A-optimal schemes (AHS and AIF) yield substantially smaller average expected infidelity than nonadaptive XYZ tomography and uniformly random selection. For Bures-averaged expected infidelity at $N=1000$, the paper reports approximately $4.2\times10^{-3}$ (AHS) and $3.5\times10^{-3}$ (AIF), and notes AIF’s slope is close to $-1$ near $N=1000$ (adaptive bound), while XYZ/URS are near $-3/4$ (nonadaptive limit). The authors also state their A-optimal results around $N=1000$ are about 2× smaller than values inferred for the HH11 Bayesian/entropy-style adaptive criterion in prior work.","The authors note that A-optimality is based on asymptotic (Cramér–Rao) theory, so for small sample sizes the bound may not characterize the true error well and performance can be worse than nonadaptive schemes for some settings. They also point out that the adaptive update uses a “dummy” estimator $\hat s_n$; when $N$ is small this estimate can be unreliable and lead to poor measurement choices. For higher-dimensional systems, they state the 1-qubit analytic simplification relies on special structure and that extending A-optimality will require new solutions or constraints due to increasing nonlinear optimization complexity.","The method assumes i.i.d. state preparation and uses rank-1 projective measurements with perfect calibration; robustness to SPAM errors, drift, or mis-specified measurement models is not addressed. The update rule relies on Fisher information and requires the true state to be full-rank (interior of Bloch ball) to avoid divergences; behavior near-pure states and practical regularization/self-starting strategies are only partially handled by ad hoc initial measurement choices. Simulations use MLE with a Newton–Raphson routine but do not report computational timings, convergence failures, or sensitivity to initialization—key practical factors for real-time adaptivity. No public code is provided, which limits reproducibility of the numerical comparisons and implementation details (e.g., optimization over measurement directions, random seeds).","They explicitly discuss generalizing A-optimality to higher-dimensional estimation (states and processes), noting that new approaches are needed because the 1-qubit analytic solution does not readily extend. As a possible direction, they suggest constraining the candidate measurement class to a discrete set to reduce computational cost, while highlighting that the trade-off between reduced computation and increased estimation error remains open.","Develop self-starting/regularized A-optimal updates that remain stable near the Bloch-sphere boundary (nearly pure states) and under unknown nuisance parameters (e.g., detector efficiencies). Extend the approach to correlated/noisy settings (time-dependent drift, autocorrelated outcomes) and to experimental constraints such as limited waveplate settings or discrete measurement menus, with principled guarantees. Provide open-source implementations and benchmarks including wall-clock update times and real-data demonstrations (e.g., photonic polarization qubits) to validate real-time feasibility. Generalize to multi-qubit and process tomography using scalable approximations (e.g., low-rank models, compressed sensing hybrids, Bayesian A-optimal criteria) and compare against modern adaptive Bayesian/particle-filter methods.",1203.3391v2,https://arxiv.org/pdf/1203.3391v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:12:28Z
TRUE,Response surface|Other,Optimization|Parameter estimation,Not applicable,Variable/General (n factors; second-order polynomial with p = 1 + n + n(n+1)/2 parameters),Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper studies response surface methodology (RSM) optimization when the response is modeled by a second-order polynomial and the regression coefficients are estimated by least squares. It formulates the RSM optimum as a convex program that minimizes the estimated response over a spherical experimental region (a hypersphere constraint), and it characterizes the optimizer using Kuhn–Tucker conditions and sensitivity analysis via the implicit function theorem. The main methodological contribution is deriving an explicit expression for the derivative of the optimal operating conditions x*(\hat\beta) with respect to the estimated regression coefficients, enabling propagation of estimation uncertainty. Using standard asymptotic theory for least squares estimators, it establishes asymptotic normality of the estimated optimal solution x*(\hat\beta) and provides the corresponding asymptotic covariance matrix. The results support constructing confidence regions/intervals and hypothesis tests for the estimated optimum rather than reporting a single point estimate.","The response surface is a second-order polynomial written as \(\hat y(x)=z'(x)\hat\beta=\hat\beta_0+\hat\beta_1'x + x'\hat B x\), and the RSM optimization is \(\min_x \hat y(x)\) subject to \(x'x\le c^2\). The Lagrangian is \(L(x,\lambda;\hat\beta)=\hat y(x)+\lambda(x'x-c^2)\) and the KKT stationarity condition is \(\hat\beta_1+2(\hat B+\lambda I)x=0\) with complementarity \(\lambda(x'x-c^2)=0\). Under regularity conditions, \(\sqrt{N}(x^*(\hat\beta)-x^*(\beta))\Rightarrow N(0,\Xi)\) where \(\Xi=(\partial x^*/\partial\hat\beta)\,\Sigma\, (\partial x^*/\partial\hat\beta)'\), and an explicit form for \(\partial x^*/\partial\hat\beta\) is derived; in the interior case (\(\lambda=0\)), \(x^*(\hat\beta)=-(1/2)\hat B^{-1}\hat\beta_1\).","The paper’s principal result is an explicit sensitivity (Jacobian) expression for \(\partial x^*(\hat\beta)/\partial\hat\beta\) for the constrained spherical-region RSM problem, enabling delta-method inference for the optimizer. It proves that if \(\sqrt{N}(\hat\beta-\beta)\Rightarrow N(0,\Sigma)\) (as in least squares under normal errors), then \(\sqrt{N}(x^*(\hat\beta)-x^*(\beta))\Rightarrow N(0,\Xi)\) with \(\Xi\) given by the Jacobian-sandwich form. A corollary covers the interior (inactive-constraint) case, yielding the closed-form optimizer \(x^*(\hat\beta)=-(1/2)\hat B^{-1}\hat\beta_1\) and a simplified asymptotic covariance expression. The conclusion highlights that these asymptotic results can be used to build confidence intervals/regions for optimal factor settings, but should be applied cautiously when sample sizes are small.","The authors note that in many applications (especially industrial ones) the number of observations is relatively small, so the asymptotic results should be applied with caution; they frame their results as a first approximation to the exact (finite-sample) problem.","The work is largely theoretical and does not validate the asymptotic approximations via simulation or real case studies, so finite-sample performance and robustness are unknown. It assumes a correct second-order polynomial model with i.i.d. normal errors and relies on convexity/positive-definiteness and uniqueness/strict complementarity conditions; these can fail in practical RSM (e.g., saddle points, nonconvex surfaces, lack of identifiability, or model misspecification). The experimental region is specialized to a hypersphere constraint, whereas many RSM applications use box constraints or irregular feasible regions, potentially changing the KKT structure and sensitivity formulas.",None stated.,"Assess finite-sample accuracy of the derived asymptotic normal approximation (and resulting confidence regions for x*) using Monte Carlo studies across common RSM designs (e.g., CCD, Box–Behnken) and varying signal-to-noise ratios. Extend the sensitivity/inference framework to other experimental regions (hypercubes/box constraints and general linear constraints) and to nonconvex response surfaces where multiple local optima can arise. Develop robust/self-starting variants that account for model misspecification, non-normal errors, or correlated observations, and provide practical software implementations to compute x* and its estimated covariance reliably.",1203.5587v1,https://arxiv.org/pdf/1203.5587v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:12:53Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Case study (real dataset),TRUE,Other,Not provided,http://www-ap.fnal.gov/MARS/|http://www-bd.fnal.gov/webhelp_edit/pa1028/pa10281_1.html,"This paper describes the Tevatron Run II two-stage beam halo collimation system (4 primary and 8 secondary collimators), its hardware/software automation and feedback control, and operational performance in reducing detector backgrounds and protecting accelerator components. Design-stage evaluation relied on multi-turn particle tracking with the STRUCT code and Monte-Carlo shower/transport simulations with the MARS code to predict loss distributions and shielding needs; the installed system has operated since 2001 with multiple upgrades. Operational results include automated scraping sequences taking ~7 minutes and large reductions in halo-loss-related detector rates (e.g., factors on the order of 10–100+, depending on counter and species, as summarized over many stores). The paper also reports on two novel halo-control methods tested in the Tevatron: bent-crystal collimation (channeling/volume reflection directing halo into absorbers) and hollow electron beam collimation (a tunable diffusion enhancer that scrapes halo while minimally perturbing the core). Overall, it is an accelerator collimation design-and-operations study with simulation and experimental validation, not a statistical DOE/experimental design methodology paper.","Key relations include the crystal channeling angular acceptance condition $\theta<\theta_c=\sqrt{2U_0/(pc)}$ (Eq. 1), with a stated critical angle of about $\theta_c\approx 7\,\mu\text{rad}$ for 980 GeV/c protons in (110) Si. For bent crystals, channeling requires bend radius $R>R_c=pv/(eE_m)$, with $E_m\approx 6\,\text{GV/cm}$ and $R_c\approx 1.6\,\text{m}$ at 980 GeV/c as quoted. For the hollow electron beam collimator, the paper characterizes transverse kicks as small and tunable (e.g., order 0.3 $\mu$rad for 980-GeV antiprotons under a representative 1 A, 2 m overlap, 3 mm hole-radius setup), used to enhance halo diffusion rather than impose a hard aperture.","The Run II automated halo removal process is reported to take as little as ~7 minutes and to lower detector-observed halo loss rates substantially (example store: CDF proton halo loss rate drops by ~100× from 2–3 MHz to 20–30 kHz; antiproton rate drops ~4–5× from 8–10 kHz to ~2 kHz). Over ~100 stores (Jan–May 2011), halo-loss reduction factors are reported as 112 (CDF proton), 80 (CDF antiproton), 13 (D0 proton), and 19 (D0 antiproton). For bent-crystal collimation (T980), an O-shaped Si crystal scan showed a channeling efficiency of $\eta_c=78\pm12\%$ (including multipass effects) and volume-reflection whole-arc efficiency $\eta_r=52\pm12\%$; using the crystal at channeling angle reduced CDF losses on the opposite side of the ring by about a factor of two. Hollow electron beam collimation tests showed smooth scraping with removal rates of a few percent per hour for affected antiproton trains under representative settings, while reporting no observed instabilities or measurable emittance growth over several hours at currents up to ~1 A in strong solenoidal fields.",None stated.,"As a DOE factsheet itemization, the work does not present controlled factorial/response-surface style experimentation; many reported studies are parasitic during collider stores, so operating conditions and confounders (beam-beam effects, IBS, tune settings, orbit jitter, collimator positions) may vary and complicate causal attribution. The simulation tools (STRUCT/MARS and others) are described at a high level, but reproducibility is limited because key configuration details (lattice files, halo initial conditions, material/geometry models, parameter settings) and code are not provided here. Performance results are system- and machine-specific (Tevatron optics, instrumentation, apertures), which may limit direct generalization to other colliders without additional comparative studies and scaling analyses.","The paper notes that applicability of the hollow electron beam collimator concept to the LHC is under study, and that to increase versatility, larger cathodes and higher electron-beam currents appear feasible with experimental tests planned in that direction.","A structured, pre-planned experimental campaign (e.g., randomized or blocked scans over tune, hole size, current, and collimator settings) could better separate effects and quantify interactions and robustness under varying operational regimes. Public release of representative simulation inputs and analysis scripts (even if underlying accelerator codes are restricted) would improve reproducibility and facilitate cross-machine validation. Additional long-duration studies focused on core impact metrics (emittance, loss spikes, impedance effects) across broader conditions—especially during transient phases like squeeze—would help establish operational envelopes for adoption in other machines.",1205.0525v1,https://arxiv.org/pdf/1205.0525v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:13:27Z
TRUE,Response surface|Computer experiment|Sequential/adaptive|Other,Prediction|Cost reduction|Other,Space-filling|Other,"2 factors (Mach number Ma, angle of attack α); variable n samples (e.g., 5–50)",Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes “generic surrogate modeling” to build more sample-efficient response surfaces for expensive computer experiments by exploiting structural similarity across a class of related problems. A database of previously built surrogate response surfaces is aligned via affine transformations and decomposed with POD/PCA to obtain basis functions; a new case is first approximated from a small set of expensive evaluations using a (possibly transformed) gappy-POD fit, yielding a generic surrogate model (GSM). The GSM is then fused with the expensive samples using hierarchical Kriging, treating the GSM as a low-fidelity trend and producing an interpolating surrogate with improved global accuracy. The approach is demonstrated on CFD-based aerodynamic coefficients (lift, drag, pitching moment) over Mach number and angle of attack, using a 24-airfoil database and comparing against ordinary Kriging. Results show hierarchical Kriging with GSM (especially with alignment) generally reduces average and max errors versus ordinary Kriging for a given sample budget, and adaptive sampling can further improve performance.","Kriging model: $y(x)=\sum_{k=1}^K\beta_k f_k(x)+z(x)$ with stationary GP covariance and product correlation $R(w-x,\theta)=\prod_{k=1}^d R^{(k)}(|w^{(k)}-x^{(k)}|,\theta^{(k)})$. Generic surrogate model is formed by gappy-POD least squares fit (optionally with transformation parameters) to sparse samples: minimize $\tfrac12\sum_{i=1}^n(\phi(x_i)-\tilde\phi(x_i(p),p,a))^2+\tfrac\delta2 p^Tp$, where $\tilde\phi(x)=y(x)V_l\Sigma_l^{-1}a^{(\psi)}+p_5$. Hierarchical Kriging replaces the regression term with the GSM trend: $\phi(x)=\beta\tilde\phi(x)+z(x)$, yielding predictor $\hat\phi(x)=\beta\tilde\phi(x)+r(x)^TR^{-1}(\varphi-\beta\Phi)$ (equivalently via block system).","Database built from 24 airfoils (23 NACA 4-digit + RAE 2822) with 400 CFD samples per airfoil on a $20\times 20$ grid over $\Omega=[0.2,0.9]\times[-4^\circ,12^\circ]$, totaling 9600 CFD runs; validation for a new airfoil uses a $40\times 40$ grid (1600 points). POD rank selected to capture ≥99.9% of variance: with alignment, ranks were 4 (cl), 4 (cd), 5 (cm) versus 5, 5, 6 without alignment. Across Latin hypercube sample sizes 5–50 (10 LHS replicates each), hierarchical Kriging using the GSM trend (with/without alignment) generally outperformed ordinary Kriging in both relative mean error $\eta_1$ and max error $\eta_\infty$, with the largest benefit typically for sample sizes up to ~30. Two adaptive sampling rules were tested (maximize predicted MSE; maximize disagreement between GSM-hierarchical and ordinary Kriging), with the disagreement-based rule showing more robust improvements at larger sample sizes in the reported test case.","For very small sample sizes, the gappy-POD approximation can have too many degrees of freedom (basis elements plus optional transformation parameters) relative to the number of samples, leading to overfitting the few sampled points and unfavorable behavior elsewhere in the domain. The authors also note that adaptive sampling benefits may appear only after a “starting phase” (they observed improvements versus Latin hypercube only after roughly 40–60 total samples in related studies).","The method assumes access to a sizable, accurate offline database of surrogate models for the problem class; generating this database can be extremely expensive and may limit applicability outside settings with reusable simulation campaigns. Performance likely depends on the chosen admissible alignment transformation and on how well new cases lie within the span of the database-derived POD subspace; out-of-class geometries/physics changes could degrade results. The paper does not provide open-source implementation details, and key practical choices (e.g., kernel form, hyperparameter optimization robustness, numerical conditioning of POD/gappy fits) may affect reproducibility and stability.",None stated.,"Test robustness to model misspecification (nonstationarity, discontinuities like shocks) and to imperfect/cheaper database surrogates, and quantify how database size/diversity affects GSM benefit. Extend to higher-dimensional input spaces, multi-output/multivariate responses, and constrained/adaptive selection of POD rank and transformation complexity to avoid small-sample overfitting. Provide a publicly available implementation and benchmark against other multi-fidelity/co-kriging and modern Bayesian optimization or active learning acquisition functions for sampling.",1206.4172v1,https://arxiv.org/pdf/1206.4172v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:14:06Z
TRUE,Factorial (full)|Factorial (fractional)|Response surface|Screening|Definitive screening|Computer experiment|Other,Parameter estimation|Screening|Optimization|Prediction|Model discrimination|Other,Not applicable,"Variable/General (examples include k=3,6,7,10; up to k=30 mentioned for screening designs)",Theoretical/simulation only,Other,TRUE,None / Not applicable|Other,Not provided,http://cocoa.dima.unige.it|http://www.math.uiuc.edu/Macaulay2/|http://www.singular.uni-kl.de|http://home.imf.au.dk/jensen/software/gfan/gfan.html,"This survey/review paper presents the “algebraic method” for designed experiments, showing how Gröbner bases and related computational algebra tools characterize estimable polynomial models for a given design via the design ideal I(D). For a fixed monomial term order, it explains how the quotient ring K[x]/I(D) yields a saturated staircase (hierarchical) monomial basis of size |D| and provides a unique polynomial interpolator on the design points, thereby making aliasing/confounding explicit through normal forms. The paper extends this to varying term orders, defining the algebraic fan (set of all staircase models identified by the design), and links it to Gröbner fans and state polytopes; it discusses linear aberration and “corner cut” (generic) designs. The methodology is illustrated across a wide range of DOE examples, including regular fractional factorials, Plackett–Burman screening designs, three-level screening/definitive-screening-type constructions, Latin and Graeco-Latin squares, BIBDs, Latin hypercube samples for computer experiments, and central composite response surface designs. Computations are described as performed using computer algebra systems (notably CoCoA and gfan), emphasizing practical model/alias structure exploration rather than proposing a new specific design family.","Designs are represented as finite point sets D \subset \mathbb{R}^k with associated design ideal $I(D)=\{f: f(x)=0,\ x\in D\}$. For a chosen monomial order, division by a Gröbner basis yields $f=\sum_i s_i(x)g_i(x)+r(x)$ where $r(x)=\mathrm{NF}(f)$ is the unique normal form modulo $I(D)$, and the set of monomials not divisible by leading terms forms a staircase basis $\{x^\alpha:\alpha\in L\}$ with $|L|=|D|$. Algebraic aliasing is $f\sim_D g\iff f-g\in I(D)\iff \mathrm{NF}(f)=\mathrm{NF}(g)$; indicator functions for fractions are defined by $g(x)=1$ on $D$ and 0 on $N\setminus D$ (e.g., in a $2^3$ example $g(x_1,x_2,x_3)=\tfrac12(1-x_1x_2x_3)$).","The paper reports (via examples) that the algebraic fan can be large even for standard screening designs: PB(8) (7 factors, 8 runs) admits 610 identifiable hierarchical multilinear models in four fan classes, with model total degrees ranging from 7 to 10. A regular $2^{6-2}$ fraction with generators {ABCD, CDEF} has an algebraic fan of 132 models grouped into six equivalence classes, with model total degrees reported between 26 and 32. A 7-factor, 15-run three-level screening design (constructed by folding a $3^k$ fraction and adding the origin) has 18,368 staircase models in 25 equivalence classes, with model total degrees ranging from 21 to 31. For a 3-factor, 6-run Latin hypercube example (L1), the fan contains 27 models in six types (up to permutations), and L1 is described as “generic,” identifying all corner-cut models for k=3, n=6.",None stated.,"As a survey centered on algebraic characterization, it does not provide a unified statistical performance evaluation (e.g., MSE/variance, power, or robustness under noise) of models/designs selected via different term orders; conclusions are largely structural (estimability/aliasing) and example-driven. Practical guidance for choosing term orders/aberration criteria in real experiments is limited, and computational scalability is acknowledged as challenging (fan computation can be expensive) but not benchmarked systematically. The treatment largely assumes exact design points and polynomial/algebraic representations; issues common in practice (measurement error, constraints, randomization restrictions, or correlated errors) are not developed.","The paper notes that computation of the algebraic fan remains expensive and points to ongoing investigation of alternative approaches (e.g., polynomial-time methods based on partial orderings, matrices, and zonotopes; and efficient algorithms leveraging links to linear algebra for zero-dimensional ideals). It also mentions generalizations from linear aberration to nonlinear cost functions and references related work, implying further development and application of these criteria.","Develop decision frameworks that connect algebraic-fan/model identifiability to statistical operating characteristics (variance/power/robustness) under realistic error structures, enabling principled selection among term orders or fan models. Extend the algebraic method to common constrained/randomization DOE settings (split-plot, blocked/restricted randomization, and correlated observations) with corresponding ideals and aliasing diagnostics. Provide reproducible software implementations (e.g., R/Python interfaces) and benchmark studies for fan/state-polytope computations on larger modern screening problems, including guidance on approximate or sampling-based exploration when full fan enumeration is infeasible.",1207.2968v1,https://arxiv.org/pdf/1207.2968v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:14:44Z
TRUE,Other,Model discrimination|Prediction|Other,Not applicable,"Variable/General (laser intensity/a0, wavelength, pulse duration, polarization; electron-beam energy; collision geometry; number of colliding pulses)",Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper proposes and analyzes experimental schemes (rather than statistical DOE methods) to probe extreme-field limits in nonlinear QED using high-intensity lasers. Two main experiment designs are discussed: (i) all-optical setups with multiple colliding laser pulses to approach vacuum breakdown and avalanche pair production, and (ii) laser–electron-beam collisions to access radiation-reaction, quantum-recoil, and photon-to-pair conversion regimes at lower laser intensities. The authors derive threshold conditions (in terms of the dimensionless laser amplitude a0 and related parameters) that delineate the onset of classical radiation-dominated dynamics, the quantum regime, and (for the all-optical case) the Schwinger pair-production limit. They argue that near-term feasibility is highest for the laser–e-beam configuration, where present PW-class lasers plus GeV-scale electron beams can reach chi parameters of order unity and enable cascade-type processes. The contribution is primarily a physics-driven experiment design/feasibility roadmap with analytic scaling laws, not an optimization-based DOE framework.","The design is organized around threshold inequalities for the laser strength parameter a0. For colliding pulses: radiation-reaction dominance when $a>a_{\rm rad}=\varepsilon_{\rm rad}^{-1/3}$; quantum recoil when $a>a_Q=((2/3)\alpha)^2\varepsilon_{\rm rad}^{-1}$; and Schwinger-limit scaling $a>a_S=((2/3)\alpha)\varepsilon_{\rm rad}^{-1}$. For laser–e-beam collisions: radiation-reaction threshold $a>a_{\rm rad}=(\varepsilon_{\rm rad}\,\omega\tau_{\rm laser}\,\gamma_{e,0})^{-1/2}$ and quantum regime threshold $a>a_Q=((2/3)\alpha)\gamma_e^{-1}\varepsilon_{\rm rad}^{-1}$ (linked to $\chi\sim 1$).","For two colliding 0.8 µm pulses, the paper estimates $a_{\rm rad}\approx 400$ (corresponding to $I\approx 3.5\times 10^{23}$ W/cm$^2$) and $a_Q\approx 1.6\times 10^3$ ($I\approx 5.5\times 10^{24}$ W/cm$^2$); reaching the Schwinger critical field for linearly polarized colliding pulses would require $a_S\approx 3\times 10^5$ ($I\approx 2.3\times 10^{29}$ W/cm$^2$), far beyond near-term capability. For a 10 GeV electron beam colliding with a 0.8 µm pulse of duration $\omega\tau_{\rm laser}=20\pi$, the radiation-reaction regime is estimated at $a_{\rm rad}\approx 10$ ($I\approx 2.2\times 10^{20}$ W/cm$^2$) and quantum recoil at $a_Q\approx 20$ ($I\approx 8.7\times 10^{20}$ W/cm$^2$). Table I indicates that for a 30 fs PW laser with $a\approx 100$, $\chi_e$ increases from ~0.1 (150 MeV) to ~5 (10 GeV), and $\chi_\gamma$ reaches ~1 at 10 GeV, supporting near-term pair-production via high-energy photons.","They state (in the conclusions and discussion) that the all-optical colliding-pulse scheme requires intensities/energies too large for near-term realization; even with multi-pulse or temporal-substructure enhancements, the required total laser energy is around 10 kJ in 10 fs, which prevents experimental verification in the near term. They therefore limit practical emphasis to the laser–electron-beam scheme as feasible with present PW-class lasers and available GeV-scale beams.","The paper provides mainly analytic threshold/scaling estimates, with limited end-to-end experimental feasibility analysis (e.g., sensitivity to realistic focal profiles, pointing/timing jitter, electron-beam energy spread/emittance, background processes, and detector acceptances). It does not frame the proposed experiments as a statistical DOE problem (no factor screening, randomization/blocking, replication strategy, or uncertainty quantification for parameter estimation). Also, cascade and pair-production yields can be highly sensitive to modeling assumptions (radiation reaction model choice, quantum emission rates, finite pulse effects), but robustness of conclusions to these choices is not systematically assessed here.","No explicit future-work section is stated beyond the general implication that near-term experiments should prioritize the laser–e-beam interaction scheme and that PW-class lasers plus GeV beams should enable observation of radiation reaction, quantum recoil, and positron production/cascade processes.","A natural extension would be a full experimental optimization and uncertainty study: incorporate realistic laser/e-beam distributions and diagnostics to predict measurable signals and backgrounds, and determine tolerances on alignment and timing. Developing a statistical DOE-style plan (factor ranges, sequential experiments, replication) could help efficiently map the transition between regimes and validate scaling laws. Providing open simulation workflows (PIC + QED modules) and benchmarking across codes would also strengthen reproducibility and guide facility-specific design choices.",1209.0720v1,https://arxiv.org/pdf/1209.0720v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:15:20Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Other,Other,"Variable/General (focus on a single unknown parameter θ at a time; examples include 1 parameter such as p, θ, gCa, Cm, φ, b)",Healthcare/medical|Finance/economics|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops experimental design methods for Partially Observed Markov Decision Processes (POMDPs), where an experimenter selects controls sequentially to obtain data that is most informative about an unknown parameter θ. It formulates design as maximizing the Fisher Information of the observed process and derives a recursive structure that enables a dynamic-programming policy, then introduces a tractable approximation—Partial Observation Fisher Information (POFI)—that conditions only on the most recent m observations/controls. The authors provide theoretical mixing-condition results showing the POFI criterion converges exponentially fast to the true Fisher Information as m increases, and they compare POFI to a Full Observation Fisher Information (FOFI) surrogate that treats the latent state as observed. They extend the approach to stochastic continuous-time dynamical systems by discretizing time/state/observation spaces into a POMDP and propose handling θ-dependence via priors and online updating using a modified Value Iteration Algorithm (VIA). Performance is demonstrated via simulation studies in discrete POMDP examples, an adversarial game model, a Morris–Lecar neuron model, and a PCR growth dynamics model, generally showing POFI-based adaptive controls can yield more precise parameter estimates than fixed, random, or FOFI-based strategies in partially observed settings.","The design criterion is the (observed-data) Fisher Information $FI = \mathbb{E}\left[\sum_{t=0}^{T-1}\left(\frac{\partial}{\partial\theta}\log p(y_{t+1}\mid y_{0:t},u_{0:t},\theta)\right)^2\right]$, maximized over a control policy via dynamic programming. A tractable approximation, POFI, truncates history to the last $m{+}1$ observations/controls: $POFI_m = \mathbb{E}\left[\sum_{t=0}^{T-1}\left(\frac{\partial}{\partial\theta}\log p(y_{t+1}\mid y_{t-m:t},u_{t-m:t},\nu_{t-m},\theta)\right)^2\right]$; under mixing, $|FI-POFI_m|\le c_1(T-1-m)\rho^{m/2}$. Online adaptation uses a discounted value-iteration update $v^{n+1}(z)=\max_u \sum_\theta\sum_{y'}\{(\partial_\theta\log p(y'\mid z,u,\theta))^2+\lambda v^n(z')\}p(y'\mid z,u,\theta)\pi(\theta\mid\cdot)$, where $z$ encodes the truncated history and $\pi$ is the (updated) posterior.","In the 3-state/6-state illustrative POMDP, simulation (500 runs, length 1000) shows POFI reduces estimation variability versus random control and FOFI; for example, standard deviation drops from 0.0621 (Random) and 0.0798 (FOFI) to 0.0469 (POFI m=1), with RMSE about 0.047 for POFI m=1–2. In the adversarial game example (T=500, 1000 simulations), POFI with larger lags improves stability; e.g., st.dev. decreases from 0.2675 (FOFI) to 0.2489 (POFI m=3) with RMSE about 0.25. In the Morris–Lecar neuron discretization, both POFI (m=1) and FOFI outperform a fixed-control baseline across parameters (Cm, gCa, φ), with POFI slightly better in RMSE (e.g., φ RMSE 0.0105 for POFI vs 0.01183 for FOFI, fixed 0.03). In the PCR model (T=200, 600 simulations), prior-averaged POFI/FOFI improve over fixed control, and online updating via VIA often further reduces RMSE (e.g., uniform prior with VIA: RMSE 0.6048 for POFI vs 0.6192 for FOFI, fixed 0.7572).","The authors note that the dynamic-programming implementation scales poorly: POFI cost grows exponentially in the lag length m, forcing small m (often m=1) and limiting feasibility for large state/observation spaces or finer discretizations. They state that practical numerical implementation restricts them to low-dimensional state spaces and to systems known up to the parameters being estimated. They also indicate that choosing m is problem-specific and leave systematic selection of m to future work.","The approach relies heavily on discretization for continuous systems, which can introduce discretization bias and sensitivity to grid design; results may change materially with different binning resolutions but this is not systematically studied. The Fisher-information-based objective targets local (asymptotic) precision and may be brittle under model misspecification, nonstationarity, or violations of conditional independence assumptions; robustness of policies to these issues is not fully evaluated. Comparisons focus on FOFI, random, and fixed controls; broader benchmarks against alternative Bayesian/adaptive design criteria (e.g., mutual information, expected posterior variance reduction) or modern POMDP solvers are limited.","They suggest future work on problem-specific methods to choose the truncation lag m (at minimum via simulation), extensions to higher-dimensional systems and larger state spaces using approximate dynamic programming, and exploring additional design quantities (e.g., timing/type of observations). They also highlight extending beyond single-parameter targeting to richer Fisher-information-matrix criteria (trace/T-optimal; potentially determinant-based criteria with more substantial extensions) and investigating other objectives such as model selection or test power within their framework.","Developing adaptive/grid-refinement or continuous-state approximations (e.g., particle-based belief-state designs) could reduce discretization error and improve scalability. Establishing finite-sample performance guarantees (beyond asymptotic Fisher information) and evaluating robustness under misspecification/autocorrelation would make the method more reliable for practice. Providing open-source implementations and standardized benchmarking across common POMDP/dynamical-system design problems would improve reproducibility and facilitate adoption.",1209.4019v4,https://arxiv.org/pdf/1209.4019v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:16:00Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Other,Other,Variable/General (designs time-varying control input(s) for diffusion/SDE systems; examples include 1D and 2D state models; focuses on estimating a single parameter but notes extensions to multiple parameters),Healthcare/medical|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Not provided,NA,"The paper formulates adaptive (closed-loop) experimental design for controllable nonlinear diffusion processes, where the experimenter chooses a time-varying control input in real time to maximize information about an unknown model parameter. The design criterion is the expected Fisher information accumulated over the experiment; with full state observation, the problem is cast as a stochastic optimal control problem and solved numerically via Markov chain (finite-difference) approximation and dynamic programming to obtain a precomputed control policy $u_t=F(x_t,t)$. For partial/noisy/infrequent observations, the authors propose estimating the latent state online via filtering (implemented with a particle filter) and then applying the precomputed full-observation optimal policy to the filtered state estimate. The approach is demonstrated in simulations on (i) a bistable double-well potential model, (ii) the Morris–Lecar neuron model (dynamic clamp current injection), and (iii) a chemostat ecological model (appendix), showing that adaptive control can substantially reduce estimator variance when informative regions of state space are rarely visited under uncontrolled dynamics. The work positions closed-loop control-based design as a principled alternative to open-loop perturbation schedules in stochastic nonlinear dynamical systems.","The system is modeled as a controlled diffusion $d x_t = f(x_t,\theta,u_t)\,dt + \Sigma(x_t)^{1/2} dW_t$. The design objective is to maximize expected Fisher information over time, e.g. $I(\theta,u)=\mathbb{E}\int_0^T \|\partial_\theta f(x_t,\theta,u_t)\|^2_{\Sigma(x_t)}\,dt$ (with $\|z\|^2_{\Sigma}=z^\top\Sigma^{-1}z$). The optimal control policy is computed by dynamic programming on a time-discretized approximation, yielding a recursion for the “Fisher information to go” and policy $F(x,t)=\arg\max_{u\in U}\,\mathbb{E}[\mathrm{FITG}_{t+\Delta t}(X_{t+\Delta t}) + \|\partial_\theta f(x,\theta,u)\|^2_{\Sigma(x)}]$.","In the double-well barrier-height example, dynamic control greatly reduces the standard deviation of the barrier estimate relative to the best tested constant control: for $T=4$, std. dev. drops from 0.3133 (constant $u=0$) to 0.0595 (dynamic) under continuous observation, and from 0.6048 to 0.1094 under infrequent noisy observations; for $T=30$, std. dev. drops from 0.2888 to 0.0210 (continuous) and from 0.5953 to 0.0488 (noisy/infrequent). In the Morris–Lecar neuron simulations estimating $g_{Ca}$ with $T=1000$ ms, dynamic control modestly improves precision (std. dev. 0.0111 vs 0.0141 under full observation; 0.0158 vs 0.0185 under noisy partial observation). In the chemostat model (appendix), dynamic control offers limited gains over the best constant dilution rate in long experiments because trajectories converge to a stable fixed point where the optimal policy becomes effectively constant.","The authors note that direct application of their numerical dynamic programming approach becomes prohibitively expensive as the number of state variables grows beyond a few dimensions (storage and computation scale exponentially with dimension). They also state that for partial observation the proposed filter-then-control approach is only an approximation and is not generally optimal for nonlinear/non-quadratic settings; exact Fisher information under partial observation is harder to compute and breaks the Markov structure needed for straightforward dynamic programming. They additionally caution that without bounds/penalties, optimal controls can become unrealistically large (ill-posed) and practical constraints must be imposed on allowable control values.","Performance comparisons are largely simulation-based and depend on discretization choices (state grid, time step, finite control set), particle-filter tuning, and assumed priors; sensitivity of the learned policies to these numerical/design choices is not systematically studied. The method optimizes (expected) Fisher information under a specified model and may be brittle under model misspecification (incorrect drift/diffusion form, unmodeled disturbances, actuator limits, delays), but robustness to misspecification is not explored. The “filter then plug-in” policy for partial observation ignores control’s effect on future information via belief-state evolution; a belief-state (POMDP) formulation could behave differently, especially with low SNR or sparse sampling.","They propose developing new numerical techniques to handle higher-dimensional systems (e.g., sparse grids, approximate dynamic programming, Monte Carlo plus machine learning approximations) to mitigate the curse of dimensionality. They mention extending the framework to alternative Bayesian design criteria (e.g., information gain, D-optimality/log-determinant objectives) and to multivariate/vector parameters via matrix criteria (trace, determinant). They also suggest exploring online updating of parameter uncertainty (e.g., posterior-based averaging or parameter-as-state approaches) and recomputing/combining policies over parameter grids when the optimal control depends strongly on unknown parameters.","A practical next step would be developing self-contained, real-time implementable algorithms that jointly approximate the optimal policy and filtering (e.g., belief-state RL / approximate POMDP solvers) under hard actuator constraints and computation budgets typical of lab instrumentation. Broader validation on real experimental datasets (e.g., dynamic clamp recordings or chemostat time series) and benchmarking against modern adaptive design baselines (Bayesian optimal design with particle methods, MPC-based information designs) would strengthen evidence of practical gains. Extending robustness analysis to autocorrelated/process noise misspecification, unknown diffusion parameters, and delayed/quantized control inputs would make the approach more deployable.",1210.3739v3,https://arxiv.org/pdf/1210.3739v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:16:43Z
TRUE,Optimal design,Parameter estimation|Cost reduction,A-optimal|Other,"Variable/General (controls are device length L and temperature T across 9 experiments; parameters estimated are 3: (µ0, σ, Nt))",Semiconductor/electronics,Simulation study|Other,TRUE,Fortran|Other,Not provided,NA,"The paper applies optimum experimental design (OED) to reduce parameter uncertainty in organic semiconductor device modeling using the extended Gaussian disorder model (EGDM). The experimental controls are device length and temperature (and a fixed voltage sweep per experiment), while the estimated physical parameters are the zero-temperature mobility µ0, the Gaussian DOS width σ, and the site density Nt. The OED objective minimizes the average parameter variance by minimizing (1/Np)·trace(Covp), where Covp is approximated via a linearized error-propagation approach based on the Fisher-information-like matrix JacᵀJac. Derivatives needed for gradient-based optimization are computed to machine precision using automatic differentiation, combined with sensitivity equations for the EGDM PDE/DAE discretization, and the underlying device equations are solved with an extended Gummel method. In two simulation examples (parameter sets from Pasveer et al. and Mensfoort & Coehoorn), optimized choices of (L,T) across nine experiments substantially shrink linearized confidence regions compared with the starting experimental plan.","The linearized parameter covariance is computed as $\mathrm{Cov}_p = (\mathrm{Jac}^T\,\mathrm{Jac})^{-1}$ with $\mathrm{Jac}(u(p,q),p,q) = -\Sigma^{-1}\, d_p J(u(p,q),p,q)\{I_p\}$, where $q$ are experimental controls (here $L,T$) and $J$ is the modeled/measured current density. The OED problem is posed as $\min_q \; \frac{1}{N_p}\,\mathrm{trace}(\mathrm{Cov}_p)$, i.e., A-optimal design via trace minimization; confidence intervals use $\theta_i = \gamma(\alpha)\sqrt{(\mathrm{Cov}_p)_{ii}}$. Required sensitivities are obtained from the implicit relation $F(u(p,q),p,q)=0$ using $\partial_p u = -(\partial_u F)^{-1}\partial_p F$ and analogous formulas for $\partial_q u$ and mixed second derivatives.","For the Pasveer-parameter example, the optimized design yielded $(L^*,T^*)\approx ((50,339.1,471.6)\,\mathrm{nm},\;(277.2,281.8,350)\,\mathrm{K})$ (reported as length units used in the paper) and reduced the objective (mean squared semi-axes of the confidence ellipsoid) to 0.07 times the unoptimized value; reported confidence-interval radii decreased from 79.90%→19.85% (p1), 5.21%→1.27% (p2), and 21.62%→7.21% (p3). For the Mensfoort/Coehoorn example, $(L^*,T^*)\approx ((50,187.8,296.8),\;(200,274.7,350))$ and the objective reduced to 0.16 times the unoptimized value; confidence-interval radii decreased from 44.76%→15.98% (p1), 7.20%→3.42% (p2), and 35.42%→16.51% (p3). Simulated I–V curves under the optimized plans span higher/wider current-density ranges, indicating increased information content.","The authors note that the optimizer returns exact real-valued optima for lengths and temperatures, but laboratory equipment may not realize these precise values; they suggest the results should be treated as guidelines and that nearby values yield similar performance. They also state the study uses unipolar layer devices “for simplicity,” implying the presented numerical results are not yet demonstrated on more complex multilayer devices in the paper.","The OED criterion relies on a linearized covariance approximation and an assumed heteroscedastic, diagonal measurement-noise model ($\sigma_i = 0.1J_i + 0.1$), so robustness to model mismatch, correlated errors, or non-Gaussian noise is not established. The demonstrated designs optimize only length and temperature (with a fixed voltage sweep), so potential gains from optimizing the voltage design itself (e.g., number/placement of voltage points) are not explored. Results are shown primarily via simulation for two parameter sets; broader benchmarking across different operating regimes, identifiability issues, and practical constraints (manufacturing tolerances, discrete available temperatures/lengths) is not quantified.","They state the approach, shown for unipolar layer devices, can be applied to more complex models including multi-layer devices, trap generation models, and exciton rate equations, and more generally to other models based on the van Roosbroeck system.","A natural extension is to include the voltage sweep design (number and placement of voltage points) as decision variables and to incorporate discrete/rounded design constraints reflecting real laboratory settings. It would also be valuable to study robust or Bayesian OED formulations to account for prior uncertainty in p and for model discrepancy, and to validate the optimized designs on real experimental datasets rather than simulation-only demonstrations.",1211.1312v1,https://arxiv.org/pdf/1211.1312v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:17:22Z
TRUE,Optimal design,Parameter estimation|Cost reduction,D-optimal|Other,Variable/General (n-link planar manipulator; design variables are manipulator joint configurations across m experiments),Manufacturing (general)|Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an optimal design-of-experiments methodology for calibrating planar anthropomorphic manipulators by selecting manipulator configurations (joint angles) that improve identification accuracy of geometric parameters (link length deviations and joint zero offsets). The calibration model is linearized around nominal parameters, leading to a least-squares estimator and an expression for the parameter covariance that depends on the summed information matrix \(\sum_{i=1}^m J_i^T J_i\). Because standard A-optimality is uninformative here (trace does not depend on the plan) and D-optimality is problematic due to mixed units across matrix blocks, the authors introduce a modified D*-optimal criterion that optimizes relevant normalized blocks separately. They derive analytical optimality conditions (e.g., certain sums of sines/cosines equal zero) for 2-, 3-, and 4-link manipulators and show that, under these conditions, the covariance becomes diagonal with closed-form standard deviations scaling as \(\sigma/\sqrt{m}\) (and for angular parameters also inversely with link length). A Monte Carlo simulation study (10,000 repetitions) for 2–4 link examples confirms that the optimal plans outperform random plans and that empirical identification errors agree with the analytical predictions.","The linearized calibration model is \(P_i = P_i^0 + J_i\,\Delta\Pi + \varepsilon_i\), where \(J_i\) is the Jacobian of the end-effector position w.r.t. the geometric parameters (link length deviations and joint offset deviations). The least-squares estimate is \(\Delta\Pi = (J_a^T J_a)^{-1} J_a^T\,\Delta P\) with stacked Jacobian \(J_a=[J_1^T\;\cdots\;J_m^T]^T\). Under iid Cartesian measurement noise with variance \(\sigma^2\), the covariance is \(\mathrm{cov}(\Delta\Pi)=\sigma^2\left(\sum_{i=1}^m J_i^T J_i\right)^{-1}\); the proposed D*-criterion optimizes determinants of normalized block matrices (diagonal vs off-diagonal blocks) derived from \(\sum J_i^T J_i\).","Analytically, the authors show that for plans satisfying their optimality conditions, the information matrix becomes block-diagonal/diagonal, yielding \(\sigma_{l_i}=\sigma/\sqrt{m}\) for linear parameters and \(\sigma_{q_i}=\sigma/(\sqrt{m}\,l_i)\) for angular offsets (planar case). For a 4-link example (\(l=[260,180,120,100]\) mm, \(\sigma=0.1\) mm), Monte Carlo results over 10,000 trials match the analytical accuracy and demonstrate that random plans give much poorer determinant values (both D and D* criteria) than the optimal plans. Reported identification accuracy improves with more experiments; e.g., for linear parameters in the 4-link case, error decreases from about 0.022 mm (few points) to about 0.005 mm (20 points) under optimal planning, with corresponding angular errors decreasing to the order of a few thousandths of a degree (values reported per joint in Table III).",None stated.,"The design and covariance results rely on a linearization of a nonlinear kinematic model and assume small deviations and convergence of the iterative procedure; performance may degrade for larger parameter errors or poor initial nominal values. Measurement noise is assumed iid on Cartesian coordinates with negligible joint-angle error and no autocorrelation; many real calibration systems have heteroscedasticity, outliers, and joint sensing errors. The proposed D*-criterion is heuristic/structure-driven (blockwise determinants) rather than a single unified optimality criterion; its statistical optimality beyond the derived conditions is not fully characterized. Simulations are shown for a limited set of planar manipulators (2–4 links) and do not include real experimental validation or constraints such as joint limits, collision avoidance, or workspace obstacles.",Future work will focus on extending these results to non-planar manipulators.,"Develop a unified optimality framework (e.g., dimensionless reparameterization or Bayesian/compound criteria) that avoids ad hoc blockwise optimization while handling mixed units more formally. Extend the approach to account for joint measurement errors, correlated/heteroscedastic noise, and robust estimation (outliers), and study sensitivity to linearization/initialization. Incorporate practical constraints (joint limits, singularities, collision avoidance, accessibility of measurement poses) into the design optimization and evaluate computational methods for constrained optimal design. Validate on real robot calibration datasets and provide an implementation (e.g., toolbox) to generate optimal pose sets for arbitrary n-link and spatial (3D) kinematic chains.",1212.0511v1,https://arxiv.org/pdf/1212.0511v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:17:58Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction,Other,Variable/General (demonstration: 2 unknown parameters for source location; 2 design variables for sensor location; 5 observations over time),Environmental monitoring|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://web.mit.edu/aeroastro/labs/uqlab/index.html,"The paper develops gradient-based stochastic optimization methods for Bayesian optimal experimental design in continuous design spaces, targeting designs that maximize expected information gain (mutual information) about model parameters. Because the expected information gain generally lacks closed form, it is estimated via a nested Monte Carlo estimator, making design optimization a stochastic optimization problem. The authors derive gradients of the Monte Carlo estimator using infinitesimal perturbation analysis and present two optimization strategies: Robbins–Monro stochastic approximation and sample-average approximation (SAA) solved with a quasi-Newton (BFGS) method. To reduce computational cost, they build global polynomial chaos surrogates of the forward model to accelerate objective/gradient evaluations. The methods are demonstrated on a PDE-based sensor placement problem for contaminant source inversion, with numerical experiments comparing algorithm performance and showing tradeoffs among estimator bias (inner-loop sample size), variance (outer-loop size), and computational cost.","The design objective is the expected information gain $U(d)=\mathbb{E}_{Y|d}[D_{\mathrm{KL}}(f_{\Theta|Y,d}(\cdot|Y,d)\|f_\Theta(\cdot))]$, equivalent to mutual information between parameters and observables given design $d$. It is approximated by a nested Monte Carlo estimator $\hat U_{N,M}(d)=\frac1N\sum_{i=1}^N\{\log f(y^{(i)}|\theta^{(i)},d)-\log[\frac1M\sum_{j=1}^M f(y^{(i)}|\tilde\theta^{(i,j)},d)]\}$. Gradients are obtained by differentiating this estimator (after transforming design-dependent observation noise to design-independent variables) using infinitesimal perturbation analysis; polynomial chaos surrogates of $G(\theta,d)$ provide cheap analytic $\nabla_d G(\theta,d)$ for the likelihood derivatives.","In the PDE sensor-placement example on $[0,1]^2$, the expected information gain surface is smooth but nonconvex with four symmetric maxima at the domain corners; the center is worst. Increasing outer-loop samples $N$ reduces variance of the objective/gradient estimates, while increasing inner-loop samples $M$ reduces bias but exhibits diminishing returns unless $N$ is sufficiently large. Across a grid of $(N,M)$ values and 1000 runs per setting, both RM and SAA-BFGS increasingly concentrate solutions near corners as $N$ and $M$ increase, with RM showing slightly better mean-square error vs. runtime in their implementation. The study provides empirical guidance that balanced allocation between $N$ and $M$ is important and that larger sample sizes can reduce iteration counts despite higher per-iteration cost.","The authors note that the nested Monte Carlo estimator of expected information gain is biased for finite inner-loop size $M$, and they instead optimize a related objective $\bar U_M$ for which the estimator is unbiased, approaching the true objective only as $M\to\infty$. They also acknowledge model error in the demonstration (polynomial chaos surrogate and PDE discretization mismatch vs. the finer-grid model used to generate synthetic data) and state that treating model error is beyond the scope of the study. They emphasize that conclusions about algorithm superiority (RM vs. SAA-BFGS) are problem-dependent and may change with implementation details and tuning.","The approach relies on accurate global polynomial chaos surrogates over the joint parameter–design space; in higher dimensions or with nonsmooth/chaotic forward models, constructing such surrogates can be impractical or inaccurate. The expected information gain objective is approximated via nested Monte Carlo, which can remain expensive and sensitive to estimator variance/bias; alternative estimators (e.g., lower-variance MI estimators or multilevel methods) are not explored here. The work focuses on open-loop (batch) design and does not provide fully worked methods for real-time sequential Bayesian design under model/measurement nonidealities such as correlated noise, non-Gaussian errors, or constraints beyond simple box bounds.","They identify rigorous sequential/closed-loop Bayesian experimental design (where data from earlier experiments guide later design choices) as an important future direction. They also suggest improving SAA optimality-gap estimation (e.g., common random number stream methods) and pursuing bias-reduction techniques for the nested Monte Carlo estimator (e.g., jackknife). The authors note that understanding the impact of model error on optimal experimental design is an important direction for future work.","Extending the gradient-based OED framework to high-dimensional design spaces (many sensors, trajectories, or time-dependent designs) would benefit from scalable optimizers (e.g., L-BFGS with constraints) and variance-reduced/mini-batch gradient estimators. Incorporating explicit model-discrepancy modeling (Bayesian calibration with discrepancy terms) into the design criterion could improve robustness to surrogate/PDE discretization error. Developing multilevel or multifidelity estimators for expected information gain (combining coarse and fine models) could reduce the cost of nested Monte Carlo while controlling bias/variance. Providing open-source implementations and standardized benchmarks would facilitate reproducibility and clearer practitioner guidance.",1212.2228v3,https://arxiv.org/pdf/1212.2228v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:18:43Z
TRUE,Optimal design|Bayesian design|Computer experiment|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|Other,"Variable/General (survey design parameters such as patch size M, patch spacing/positions, observed vs sampled area fraction f; 60 power-spectrum bins used)",Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a Bayesian experimental design framework to compare sparse vs contiguous sky-sampling strategies for galaxy surveys, with the goal of constraining the galaxy power spectrum and derived cosmological parameters. It defines utilities/figures-of-merit based on Fisher-matrix functionals—A-optimality (trace), D-optimality (determinant), and an entropy/Kullback–Leibler information-gain criterion relative to a prior (SDSS-LRG-like) Fisher matrix. The survey ‘design’ is parameterized by a mask geometry consisting of a regular grid of square observing patches (size M) and their spacing, analyzed under two regimes: (i) constant total sampled area with shrinking observed patches (reduced observing time), and (ii) constant observed area with increasing patch separation (larger sampled footprint at fixed time). Using Fisher forecasts for 60 top-hat k-bins of the power spectrum and a 5-parameter flat ΛCDM model, the study finds that in regime (i) sparsity induces only negligible degradation (errors increase by at most ~0.45% for parameters as stated in the abstract), while in regime (ii) sparsifying to cover a larger total area at fixed time can reduce parameter errors substantially (up to ~28% as stated in the abstract), with aliasing effects generally outweighed by increased volume/footprint. The work advances survey-design/SPC-style optimization in cosmology by explicitly linking sky-sampling geometry to window functions, Fisher information, and Bayesian design criteria for choosing observing strategies.","The expected-utility framework is written as $\mathbb{E}[U\mid e,o]=\sum_i P(M_i\mid o)\int d\theta_i\, U(\theta_i,e,o)P(\theta_i\mid o,M_i)$. Utilities used include A-optimality $=\ln(\mathrm{tr}(F))$, D-optimality $=\ln|F|$, and an entropy/KL form $E=\tfrac12\left(\ln|F|-\ln|\Pi|-\mathrm{tr}(I-\Pi F^{-1})\right)$ with prior Fisher matrix $\Pi$ and posterior $F=\Pi+L$. Fisher matrices are computed from the data covariance $C=C_S+C_N$ via $F_{ij}=\tfrac12\mathrm{tr}\left[C_{,i}C^{-1}C_{,j}C^{-1}\right]$, with $C_S$ expressed through the survey window function $W_{ij}(k)$ and binned power spectrum $P(k)=\sum_B w_B(k)P_B$; cosmological-parameter Fisher matrices use the Jacobian mapping $F_{\alpha\beta}=\sum_{ab}F_{ab}\,\partial P_a/\partial \lambda_\alpha\,\partial P_b/\partial \lambda_\beta$. The sparse mask geometry is encoded through the weighting/window functions for a regular grid of square patches (size $M$) and depth $L$, producing sinc/Dirichlet-kernel factors that determine aliasing and inter-bin correlations.","For the DES-inspired study, sparsely observing the same total sky footprint in less time (constant total sampled area; smaller observed patches) yields only very small degradation in parameter constraints; the abstract reports a maximum parameter-error increase of 0.45% (with detailed discussion showing degradation driven mainly by increased inter-bin correlations/aliasing rather than larger variances). When keeping observing time fixed but spreading patches to cover a larger total sampled area (constant observed area; larger footprint), sparsity improves constraints; the abstract reports parameter-error reductions up to 28% (and the text notes gains up to ~27% for some parameters, relative to contiguous DES). Fisher-matrix diagnostics show that in the constant-total-area case the diagonal elements (variances) change negligibly with sparsity while off-diagonals grow (aliasing), whereas in the constant-observed-area case diagonal elements increase (better constraints) and nearby-bin correlations can decrease due to increased total survey size despite additional long-range aliasing peaks tied to patch spacing. Overall, the paper concludes sparse sampling can be a cost/time-efficient substitute for contiguous surveys, and can be superior if it increases the total sampled footprint at fixed observing time.","The authors note the Fisher-matrix/forecasting approach assumes approximate Gaussianity around the likelihood peak and is most appropriate in high signal-to-noise regimes; results should be treated as indicative when distributions are non-Gaussian or strongly degenerate. They also explicitly restrict to a flat-sky approximation (suitable for DES-sized areas) and state Euclid-scale (full-sky) treatment is not investigated. In the design, they fix the number of power-spectrum bins ($n_{\mathrm{bin}}=60$) across cases for fairness, acknowledging that in reality binning should depend on survey volume (via $k_{\min}$).","The sparse designs considered are highly stylized (regular grids of square patches), which can introduce artificial periodicities and scale-dependent information loss; more realistic telescope tiling and masking (irregular boundaries, variable depth, chip gaps, seeing variations) could materially change window functions and aliasing. Forecasts appear to neglect key observational/systematic effects (photometric redshift uncertainties, selection-function errors, spatially varying completeness, intrinsic alignments, non-linear clustering/bias evolution), which can dominate over statistical Fisher errors and interact with sparse sampling. The optimization is performed over a limited design space (primarily patch size and spacing under two constraints) rather than a broader cost model that includes overheads (slew/readout), calibration time, and operational constraints that affect real survey efficiency.","They propose investigating an optimal patch shape (noting squares may be worst for induced correlations) and moving from fixed, deterministic patch positions to a numerical approach where patches are randomly distributed on the sky to avoid loss of information at specific scales and produce a more even information loss across scales. They also mention future work to explicitly use expected utility functions (averaging over fiducial-parameter uncertainty) rather than relying on current-model assumptions that reduce differences among fiducials.","Extend the design study to full-sky/spherical geometry (e.g., Euclid-like surveys) and incorporate realistic survey systematics and calibration requirements to test whether sparse designs remain advantageous under dominant systematics. Consider sequential/adaptive survey strategies where early data inform subsequent pointing decisions (Bayesian adaptive design), and evaluate robustness to model misspecification (e.g., non-ΛCDM extensions, scale-dependent bias). Provide open-source implementations of window-function/Fisher computations and benchmarking to facilitate reuse and reproducibility, and validate key conclusions with end-to-end mock catalogs or simulation-based likelihood analyses beyond Fisher forecasts.",1212.3194v1,https://arxiv.org/pdf/1212.3194v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:19:29Z
FALSE,Other,Other,Not applicable,Not specified (grid of 10 doses × 7 exposure times; 8 nanomaterials; 4 outcomes; replicates k),Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,R,Not provided,NA,"The paper proposes a Bayesian hierarchical model to analyze high-throughput screening (HTS) nanotoxicology data consisting of multiple cytotoxicity outcomes measured over a two-dimensional grid of dose and exposure time. For each engineered nanomaterial (ENM) and outcome, the mean response surface is modeled additively as a dose effect plus a time effect, with an optional semi-parametric dose–time interaction selected via reversible-jump MCMC. Dose, time, and interaction components are represented using linear B-splines with two random interior knots (change-points), enabling inference on interpretable risk-assessment quantities such as maximal safe dose/time and maximal response. Robustness to outliers is handled through a scaled normal–gamma mixture yielding a t-distributed error model with ENM-specific variance inflation. The approach is demonstrated on data for eight nanoparticles and four cytotoxicity endpoints measured across 10 doses and 7 exposure times, with posterior summaries used to visualize fitted response surfaces and safe exposure regions; a supplemental simulation study assesses estimation and prior sensitivity.","Observations follow $y_{ijk}(d,t)=m_{ij}(d,t)+\varepsilon_{ijk}(d,t)$ with $\varepsilon_{ijk}(d,t)\sim N(0,\sigma^2_{\varepsilon j}/\tau_i)$ and $\tau_i\sim \text{Gamma}(\nu/2,\nu/2)$, implying a marginal $t$ error. The mean surface is additive with optional interaction: $m_{ij}(d,t)=\alpha_{ij}+f_{ij}(d)+g_{ij}(t)+\mathbb{1}(\rho_{ij}=1)\,h_{ij}(dt)$, where $f_{ij}(d)=B(d,\phi_{ij})'\beta_{ij}$, $g_{ij}(t)=B(t,\psi_{ij})'\gamma_{ij}$, and $h_{ij}(dt)=B(dt,\chi_{ij})'\delta_{ij}$ using degree-1 B-splines with two random interior knots. Interaction inclusion is governed by $\rho_{ij}\sim \text{Bern}(\pi)$ with $\pi\sim U(0,1)$ and updated via reversible-jump MCMC.","The application analyzes 8 ENMs and 4 cytotoxicity outcomes observed over a 10-dose by 7-time grid; posterior predictive checks show empirical mean responses lie within 95% posterior predictive intervals across particles/outcomes and the PIT histogram appears close to uniform. For selected particles (e.g., quantum dot and platinum), the posterior inclusion probability for dose–time interaction is reported as very high for mitochondrial superoxide formation (e.g., $\hat\rho\approx 0.99$), indicating strong evidence of interaction in those pathways. The fitted surfaces and derived “safe exposure regions” provide interpretable summaries such as maximal safe dose conditional on exposure time and posterior intervals for these quantities. A supplemental simulation study reports robustness to model misspecification and limited sensitivity to reasonable prior variations.","The authors note a core challenge is balancing model complexity with interpretability; their additive/partly linear structure sacrifices some generality relative to fully nonparametric surfaces. They also state that retaining outcomes on their original bounded scale (e.g., via binomial/beta models) would be more natural than the logit-normal/t-error approach but would substantially increase computational complexity and might require numerical/analytical approximations. They acknowledge that more general interaction forms (to capture synergistic dose–time effects) could be scientifically important but would reduce interpretability.","The method is tailored to gridded dose–time designs and assumes independence of errors across dose/time/replicates conditional on parameters; real HTS assays may exhibit within-plate/within-well correlation or temporal autocorrelation not explicitly modeled. The interaction is restricted to a function of the product $dt$ rather than a general bivariate surface, which may miss interactions not well represented by that structure. The paper does not provide an openly available implementation, which can hinder reproducibility and uptake, and the computational burden of RJMCMC may be substantial for larger HTS panels (more ENMs/outcomes/dose–time points).","They suggest extending the model to allow more general functional interactions between dose and exposure duration (beyond the current semi-parametric $h(dt)$ form). They propose modeling outcomes on their original scale using generalized multivariate models (e.g., binomial/beta) instead of transforming and using a t-error, acknowledging increased computational demands. They also note the hierarchy can be adapted to incorporate multiple cell lines (e.g., via an additional hierarchical level) and expanded to include covariates to relate ENM physicochemical properties to toxicity.","Developing faster inference (e.g., variational Bayes, HMC in Stan with continuous relaxations, or marginalization strategies) could make the approach scalable to modern HTS experiments with hundreds/thousands of materials and endpoints. Incorporating structured correlation (e.g., plate/batch effects, temporal correlation across exposure times, or multivariate residual covariance across endpoints) would likely improve calibration and interpretability. Providing a packaged software implementation with documented workflows and benchmarking against alternative multivariate dose–response surface methods would support broader adoption and reproducibility.",1301.2435v1,https://arxiv.org/pdf/1301.2435v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:20:14Z
TRUE,Factorial (full)|Other,Parameter estimation|Screening|Prediction|Other,Not applicable,Variable/General (framework enumerates many factors; worked example uses 3 factors at 2 levels each),Network/cybersecurity|Other,Case study (real dataset)|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper proposes a tree-structured factor framework to support experimental design for performance evaluation of commercial cloud services (IaaS/PaaS), addressing the ad hoc factor selection seen in prior studies. It synthesizes factors from a systematic literature review (46 studies) and prior taxonomy/modeling work, organizing them into input-process factors (Workload and Computing Resource) and output-process factors (Capacity/metrics). Workload factors are grouped into Terminal, Activity, and Object; computing resource factors span Communication, Computation (including CPU characteristics), Memory/Cache, Storage, and VM Instance; capacity factors correspond to metrics for throughput, latency, transaction speed, availability, reliability, scalability, variability. An application example demonstrates selecting three factors (activity direction, object size, VM type) and using a two-level full factorial design (8 runs) with randomized run order to evaluate EC2 disk I/O throughput, followed by effect analysis (Pareto plot) to assess factor/interaction importance. Overall, the contribution is primarily a DOE-oriented factor-selection “dictionary” tailored to cloud performance experiments, with a small illustrative full-factorial DOE.","The worked example uses a two-level full factorial design with three factors (A: activity direction; B: object size; C: VM type), yielding an 8-run design matrix with randomized trial order. The response is disk data throughput (MB/s), and factor/interaction effects (A, B, C, AB, AC, BC, ABC) are assessed via an effects/Pareto plot at significance level $\alpha=0.05$ (Lenth’s PSE is referenced in the plot).","In the illustrative EC2 disk I/O example, none of the main effects or interactions crosses the Pareto significance reference line at $\alpha=0.05$, suggesting no statistically significant influence of the three factors (activity direction, object size, VM type) on throughput for the borrowed dataset. However, object size (factor B) shows the largest relative effect: throughput for small-size data (Char) is much lower than for large-size data (Block), interpreted as transaction overhead dominating small transfers. Example throughput values reported include 73.5 MB/s (Write, Block, M1.small), 22.3 MB/s (Read, Char, M1.small), and 64.3 MB/s (Read, Block, M1.large).","The authors note the framework is derived from empirical evaluation practices in academic publications only (excluding blogs/technical websites) and is limited to commercial cloud services and performance evaluation (not other qualities like security). They also restrict scope to IaaS and PaaS (excluding SaaS) because SaaS would introduce an effectively unbounded and domain-specific factor set. They state the framework is intended to supplement, not replace, expert judgment and cannot be exhaustive because cloud computing is still maturing/chaotic.","The paper does not develop or validate new optimal design constructions; the DOE contribution is largely factor categorization plus a basic full-factorial example, so guidance on choosing efficient fractional/optimal designs under budget constraints is limited. The example reuses results from another study rather than executing/replicating the experiment, so practical issues (noise, blocking, randomization constraints, repeat runs, measurement error) are not demonstrated end-to-end. The framework is largely descriptive (state-of-practice) and may inherit biases/omissions from the SLR corpus; it is not quantitatively validated for completeness or usefulness across newer cloud services and modern architectures.","They propose (1) collecting feedback from external experts to supplement and expand the factor framework over time as cloud computing evolves, and (2) formally introducing and adapting suitable experimental design and analysis techniques for evaluating commercial cloud services using the currently available factors.","Useful next steps would include prescriptive DOE guidance tailored to cloud constraints (e.g., split-plot designs for hard-to-change factors like region/VM type; blocked designs for time-of-day/provider variability; sequential designs to manage cost). Developing publicly available tooling/templates that map evaluation goals to recommended factors, designs, and analysis workflows would increase adoption. More empirical validation via multiple real cloud case studies (with replication and cost/time accounting) would test robustness and demonstrate how the framework improves efficiency and inference quality compared with ad hoc designs.",1302.2203v1,https://arxiv.org/pdf/1302.2203v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:20:46Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Cost reduction,D-optimal,Variable/General (d-dimensional feature vectors; n candidate subjects/experiments),Theoretical/simulation only|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper studies a budget-constrained, strategic version of experimental design where each candidate subject/experiment i has known features x_i and privately known cost c_i, and the experimenter must select a subset under total budget B. The design criterion is the (Bayesian) D-optimality/information-gain objective V(S)=log det(I_d+\sum_{i\in S} x_i x_i^T), corresponding to information gained about regression parameters under ridge regression with a homotropic prior. The main contribution is a deterministic, polynomial-time, budget-feasible mechanism that is approximately truthful (\u03b4-truthful) and achieves a constant-factor approximation (~12.98) to the optimal design value, improving on prior general submodular-mechanism results that would require randomness or exponential time for determinism. Methodologically, the authors introduce a concave convex-relaxation L(\lambda)=log det(I_d+\sum_i \lambda_i x_i x_i^T) and relate it within constant factors to the multilinear extension plus pipage rounding, then show how to compute an \""almost monotone\"" approximation needed for mechanism design using a barrier method on a perturbed feasible region. They also prove an impossibility result: no truthful, budget-feasible mechanism can approximate within factor 2, and discuss extensions beyond linear regression to other learning models where information gain is submodular.","The design value for a selected set S is V(S)=\log\det\left(I_d+\sum_{i\in S} x_i x_i^T\right), motivated as (twice) the mutual information gain I(\beta;y_S) under a Gaussian prior and linear model. The concave relaxation replaces discrete selection with fractional variables \lambda\in[0,1]^n: L(\lambda)=\log\det\left(I_d+\sum_{i=1}^n \lambda_i x_i x_i^T\right) subject to \sum_i c_i\lambda_i\le B; it is related to the multilinear extension F(\lambda)=\mathbb{E}_{S\sim P_\lambda}[V(S)] via \tfrac{1}{2}L(\lambda)\le F(\lambda)\le L(\lambda). The mechanism uses an \""almost monotone\"" (\u03b4-decreasing) approximation of the optimum of (P_c) computed by solving a perturbed problem (P_{c,\alpha}) with a barrier method.","They give a deterministic polynomial-time mechanism that is \u03b4-truthful and budget feasible with approximation factor \u224812.98 (additive +\u03b5) for the budgeted D-optimal design problem under strategic costs. They prove a lower bound showing no truthful, budget-feasible, individually rational mechanism can achieve a factor-2 approximation for this objective. On the optimization side, they show the concave relaxation optimum L_c^* approximates the true optimum as OPT \le L_c^* \le 2\,OPT + 2\max_i V(\{i\}), and provide an algorithm that computes an \u03b5-accurate, \u03b4-decreasing approximation in time polynomial in (n,d,\log\log(\cdot)).","The paper notes that EDP is NP-hard and that standard approximation algorithms (e.g., greedy) do not preserve truthfulness for this objective, motivating the need for their more complex relaxation-based mechanism. It also emphasizes that the convex program can only be solved approximately (not exactly), which necessitates developing an \""almost monotone\"" solver to be usable in mechanism design. No other explicit practical limitations (e.g., distributional misspecification, data issues) are highlighted as limitations.","The design criterion is restricted to a Bayesian D-optimality/log-det objective (homotropic prior in the main development), so applicability to other optimality criteria (A-, I-, G-optimality) or constraints (e.g., stratification, ethics, balance) is not established beyond discussion. The work is primarily theoretical (mechanism design + convex relaxation) and does not provide empirical validation on real experimental-design datasets or guidance on choosing \u03b4/\u03b5 in practical deployments. The model assumes feature vectors x_i are verifiable and measurements y_i cannot be manipulated, which may be unrealistic in many human-subject or online settings where both features and outcomes can be gamed or noisy in adversarial ways.","They suggest generalizing the approach beyond linear regression to broader learning tasks where information gain remains submodular under independent noise (e.g., generalized linear regression and logistic regression) and investigating whether the convex relaxation technique extends. They also point to exploring similar \""swap expectation and scalarization\"" concave relaxations for other experimental-design optimality criteria (as noted in convex optimization references) and using them to design budget-feasible mechanisms as an open problem.","Developing practical, implementable tooling (e.g., software) for computing the relaxation and threshold payments at scale, including numerical stability and runtime benchmarks, would make the mechanism usable in real procurement/subject-selection pipelines. Extending the mechanism to handle unknown/estimated noise variance, correlated errors, or adaptive/sequential data collection (online design) would align better with real experimental workflows. Empirical comparisons against standard budgeted D-optimal heuristics (greedy, exchange algorithms) under strategic behavior, and robustness analyses when verifiability assumptions fail, would strengthen the case for adoption.",1302.5724v4,https://arxiv.org/pdf/1302.5724v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:21:13Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Cost reduction|Other,D-optimal|A-optimal|E-optimal|Other,Variable/General (design over input/perturbation pattern and measurement times; parameters of interest vs nuisance parameters via Ds-optimality),Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://control.ee.ethz.ch/∼jakorues/DesignSI/,"The paper proposes an optimal experimental design framework for single-cell “distribution” measurements of stochastic biochemical reaction networks, including heterogeneous populations with intrinsic and possibly time-varying extrinsic noise. It derives moment dynamics for stochastic hybrid models where an extrinsic reaction rate follows an SDE, and then approximates Fisher information from the first four central moments (via CLT-based asymptotic normality of sample mean/variance) so that the CME need not be solved. The approximated Fisher information is embedded into an experiment-optimization problem using determinant-based criteria, including Ds-optimality to target a subset of parameters while treating others as nuisance, and a Bayesian (prior-averaged) robust design objective. The framework is demonstrated in silico on a gene-expression model to compare unplanned time-course, dual-reporter, and optimally designed perturbation/measurement-time experiments, showing large information gains from designed experiments and different tradeoffs by objective. It is also applied to a yeast light-switch gene-expression system to compare published light-pulse experiments and to find a new light-pulse/measurement schedule with substantially higher information for extrinsic fluctuation parameters.","Extrinsic variability is modeled by an SDE for a reaction rate, e.g. $\mathrm{d}a_t=r(\mu_a-a_t)\,\mathrm{d}t+s\sqrt{a_t}\,\mathrm{d}W_t$ (Eq. 1), and population moments satisfy $\frac{\mathrm{d}}{\mathrm{d}t}\mathbb{E}[\psi(a_t,x(t))]=\mathbb{E}[(\mathcal{L}\psi)(a_t,x(t))]$ with an extended generator (Eq. 2). Fisher information is approximated from the first four central moments using CLT arguments, yielding expressions like $\tilde I_m(\theta)=n(\partial_\theta\mu_1)^2/\mu_2$ (Eq. 3) and a mean+variance information $\tilde I_J(\theta)$ involving $\mu_2,\mu_3,\mu_4$ and their derivatives (Eq. 4). Optimal design uses Ds-optimality with $I_s=I_{11}-I_{12}I_{22}^{-1}I_{21}$ (Eq. 5) and a prior-averaged determinant objective $e^*=\arg\max_{e\in\mathcal{E}}\mathbb{E}_\theta[\det I_s(\theta,e)]$ (Eq. 6).","In the in-silico gene-expression example (10 distribution measurements over a 300 time-unit horizon), information from unplanned single-reporter time courses is extremely low for several parameters (e.g., normalized information ≈0.0037 for $\mu_a$ and $c$, and 0.0185 for $V_a$), while unplanned dual-reporter experiments greatly increase information (≈10.31 for $\mu_a$, 18.69 for $V_a$, 11.32 for $c$, and 271.52 for $r$). Optimally designed perturbation patterns and measurement times substantially boost information for targeted parameters, especially for $r$ (≈515.61), and combining optimal perturbations with dual reporters yields the highest information overall (e.g., ≈975.43 for $r$ and 36.90 for $V_a$). For the yeast light-switch system, the paper reports that certain published light-pulse experiments provide much more information about the extrinsic mean-reversion speed $r$ than longer, more heavily sampled experiments, and that an OED-found light-pulse pattern can provide close to four times more information about $r$ than any experiment in the referenced study.","The Fisher information approximation relies on having a sufficiently large measured cell population so that the central limit theorem makes the sample mean and variance approximately jointly Gaussian. Moment equations may be non-closed, in which case exact computation is not possible and moment-closure/approximation methods must be used. The authors also note practical difficulties that true parameters are unknown for design, motivating evaluation at an estimate or the use of a prior distribution (robust/Bayesian design).","The approach uses only information captured by sample mean and variance (augmented via dependence on moments up to fourth order), but does not directly exploit full time-resolved single-cell trajectories or richer summary statistics beyond mean/variance, which can matter in strongly non-Gaussian or multimodal regimes. Performance depends on accuracy of moment dynamics (and any closure), and on correct modeling of extrinsic variability via the chosen SDE form; misspecification could lead to misleading design recommendations. The optimization over input patterns/measurement times is described as MCMC-like and heuristic, so global optimality and computational scaling to large design spaces/models may be uncertain without further guarantees/benchmarks.","The paper suggests an iterative workflow: use a prior $\pi(\theta)$ to design an experiment, collect data, perform parameter inference to obtain a posterior, and then use the posterior as the new prior to compute a new optimal experiment, repeating until uncertainty is sufficiently reduced. It also motivates using designed perturbation patterns (e.g., pulse sequences) to uncover system features (such as time-varying extrinsic noise) that remain hidden in unplanned experiments.","Extending the framework to explicitly handle autocorrelated measurement noise, cell-lineage dependence, or time-series measurements from the same cells (breaking the i.i.d. sampling assumption) would broaden applicability. Developing principled, scalable optimization algorithms with convergence/optimality guarantees (and releasing reference software) would improve reproducibility and adoption. Additional validation on diverse real single-cell datasets and robustness studies under extrinsic-noise model misspecification (e.g., different SDEs or non-diffusive rate processes) would clarify when the moment/FIM approximation yields reliable experimental designs.",1304.1455v1,https://arxiv.org/pdf/1304.1455v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:22:00Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Optimization|Prediction|Cost reduction|Other,Space-filling|Minimax/Maximin|Other,"5 factors (island slope, beach slope, water depth, island–beach distance, incoming wavelength via ω); wave height fixed",Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies whether small offshore islands protect or amplify tsunami impact on nearby mainland coasts using numerical simulations of the nonlinear shallow water equations (NSWE) in a simplified conical-island/plane-beach geometry. It frames the problem as a computer experiment with five physical input parameters and a scalar objective: maximize run-up amplification behind the island while minimizing the number of expensive simulator runs. The authors first construct a space-filling initial design using maximin Latin Hypercube Sampling (200 simulations) and then apply an active (sequential, batch) experimental design approach using a Gaussian Process (GP) surrogate to choose new query points via the GP-UCB-PE algorithm (upper confidence bound + pure exploration in a relevant region). The computational experiments show that, over the considered parameter ranges, the island never reduces run-up; instead it tends to amplify run-up, with a reported maximum amplification of about 70% and a median amplification factor around 1.3. The work contributes a practical active-design workflow (including a rank-correlation stopping rule) for efficient optimization in tsunami simulation settings and similar high-cost computer experiments.","The simulator response is emulated with a Gaussian Process posterior, with mean and variance (Eqs. 1–2): $\hat\mu_T(x)=k_T(x)^\top C_T^{-1}Y_T$ and $\hat\sigma_T^2(x)=k(x,x)-k_T(x)^\top C_T^{-1}k_T(x)$. The active design uses GP upper/lower confidence bounds $\hat f_t^{\pm}(x)=\hat\mu_t(x)\pm\sqrt{\beta_t}\,\hat\sigma_{t-1}(x)$ (Eqs. 5–6) to pick batch points: first $x_t^0=\arg\max_x \hat f_t^+(x)$ (Eq. 8), then additional points maximize posterior variance within the relevant region $R_t=\{x:\hat f_t^+(x)\ge y_t^\bullet\}$ where $y_t^\bullet=\max_x \hat f_t^-(x)$ (Eq. 7, Eq. 9).","Using 200 NSWE simulations selected by maximin Latin Hypercube Sampling over five parameters, the authors report that the island did not provide protection in any tested configuration; run-up behind the island was always amplified relative to a lateral beach location. They report a maximum run-up amplification of approximately 70% (amplification factor up to about 1.7) and a median amplification factor of about 1.3. Local sensitivity around the maximum suggests water depth, beach slope, and wave cyclic frequency $\omega$ are most influential among the five parameters. They also state that active experimental design can reduce computational cost by more than 60% compared to the passive LHS approach (and orders of magnitude vs. a full grid in 5D).","The stopping criterion depends on an empirically chosen threshold for the rank-correlation change, and the authors note that more research is needed to make it more robust and/or relate it to problem dimensionality. They also emphasize the method’s finite-horizon behavior is what matters in practice, whereas theoretical guarantees are asymptotic and do not provide short-term constants needed for practical stopping decisions.","The study uses a highly idealized geometry (conical island, flat shelf, uniform plane beach) and fixed wave height/forcing shape, which may limit generalizability to realistic bathymetries and tsunami sources. The GP surrogate/active design performance may depend strongly on kernel choice and hyperparameter tuning, but systematic robustness checks across kernels/priors and noise models are limited. The input design starts from a precomputed finite candidate set (LHS points), so “optimization” is effectively over that discrete set rather than continuous adaptive placement, which can constrain achievable maxima. No implementation details or shared code are provided, which limits reproducibility of the DOE/active-learning workflow and the simulation setup.","The authors suggest developing more robust stopping criteria, ideally derived from the learning algorithm or tied to problem dimensionality rather than an empirically set threshold. They also propose extending active learning to multi-objective optimization and Pareto front tracking. Additionally, they note the possibility of including numerical parameters (e.g., spatial discretization and virtual sensor placement) alongside physical parameters within the active optimization framework.","Validate the active-design conclusions on more realistic bathymetries and a broader set of tsunami source/time histories (including varying wave heights and spectra) to test robustness of the ‘islands amplify run-up’ finding. Extend the DOE to handle model discrepancy and structured numerical errors (mesh/time-step dependence) via multi-fidelity or Bayesian model calibration, rather than treating discretization effects as simple Gaussian noise. Provide an open-source implementation (or reproducible notebooks) for the GP-UCB-PE workflow and the experimental setup to enable reuse in other hazard-modeling contexts. Explore constrained and risk-averse design objectives (e.g., quantiles of run-up, worst-case over uncertain sources) which better match civil defense decision-making than single-scenario maximization.",1305.7385v1,https://arxiv.org/pdf/1305.7385v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:22:40Z
FALSE,NA,NA,Not applicable,NA,Other,NA,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper develops theoretical considerations to guide the design of a laboratory experiment to measure the nonlinear interaction (“collision”) between counterpropagating, perpendicularly polarized Alfvén waves, viewed as the fundamental interaction underlying Alfvénic plasma turbulence. Using incompressible MHD in Elsässer form, it explains why achieving a measurable nonlinear signal in the laboratory favors an imbalanced setup: a large-amplitude, low-frequency “distorting” wave interacting with a smaller-amplitude, higher-frequency “probe” wave. The core experimental-design insight is to ensure the distorting wave effectively contains a $k_{\parallel}=0$ component over the finite interaction region (via choosing its parallel wavelength $\lambda_{\parallel}^{-}>2L$), enabling resonant three-wave interactions and secular energy transfer to a propagating daughter Alfvén wave. The paper predicts diagnostic signatures of the daughter wave (perpendicular wavevector addition, unchanged $k_{\parallel}$ and frequency relative to the probe, polarization relationships, phase shift, and an amplitude scaling) that can be used to verify the nonlinear interaction in the LAPD experiment.","Key governing equations are the incompressible MHD equations in symmetrized Elsässer form: $\partial_t \mathbf{z}^\pm \mp \mathbf{v}_A\cdot\nabla \mathbf{z}^\pm = -\mathbf{z}^\mp\cdot\nabla \mathbf{z}^\pm - \nabla P/\rho_0$ with $\nabla^2 P/\rho_0 = -\nabla\cdot(\mathbf{z}^-\cdot\nabla\mathbf{z}^+)$. Resonant three-wave conditions are $\mathbf{k}_1+\mathbf{k}_2=\mathbf{k}_3$ and $\omega_1+\omega_2=\omega_3$; in the experiment the effective $k_{\parallel,2}=0$ component implies $\mathbf{k}_{\perp 3}=\mathbf{k}_{\perp 1}+\mathbf{k}_{\perp 2}$ and $k_{\parallel 3}=k_{\parallel 1}$. The predicted daughter-wave amplitude scaling is $\delta B_{\perp 3}/B_0\sim \tfrac14(\delta B_{\perp 1}/B_0)(\delta B_{\perp 2}/B_0)(k_{\perp 1}/k_{\parallel 1})$.","The paper identifies a practical condition for enabling a measurable resonant three-wave interaction in the finite interaction region: choose the distorting wave to have parallel wavelength exceeding twice the interaction length, $\lambda_{\parallel}^{-}>2L$, so that the windowed waveform contains a significant effective $k_{\parallel}=0$ Fourier component. For the LAPD configuration discussed, it quotes $L=8.6\,\mathrm{m}$, $\lambda_{\parallel}^{-}=29.1\,\mathrm{m}$ (distorting/Loop wave), and $\lambda_{\parallel}^{+}=6.4\,\mathrm{m}$ (probe/ASW wave), satisfying $\lambda_{\parallel}^{-}>2L\approx17.2\,\mathrm{m}$. It also quotes typical wave amplitudes: Loop antenna up to $\delta B/B_0\sim0.01$ (used $\sim0.002$ in this experiment) and ASW probe $\delta B/B_0\sim2\times10^{-5}$. The predicted daughter wave has the same propagation direction and frequency as the probe ($k_{\parallel 3}=k_{\parallel 1}$, $\omega_3=\omega_1$) but a distinct perpendicular wavevector ($\mathbf{k}_{\perp 3}=\mathbf{k}_{\perp 1}+\mathbf{k}_{\perp 2}$) and polarization relation $\delta B_{x3}=-(k_{\perp2}/k_{\perp1})\,\delta B_{y3}$.",None stated.,"Although framed as “experimental design,” the paper does not present a statistical design-of-experiments (DOE) framework (e.g., factors/levels, randomization, replication, blocking, or formal optimal design criteria); it is primarily physics-theory guidance for configuring a plasma experiment. The predicted signatures rely on idealized assumptions (incompressible MHD, weak nonlinearity, anisotropy $k_\perp\gg k_\parallel$, negligible dispersion, and clean separation of Fourier modes), which may be challenged by laboratory nonidealities (boundaries, antenna coupling, noise, and spatial inhomogeneity). Quantitative power/uncertainty analysis for detectability (SNR vs. measurement noise, required repeats, and sensitivity to parameter drift) is not developed here.",The paper notes companion and forthcoming works: Paper IV for detailed experimental setup/procedure and analysis; and a forthcoming Paper V using nonlinear gyrokinetic simulations of localized wavepacket collisions to illustrate resonant three-wave transfer with asymmetric waveforms and interpret the $k_{\parallel}=0$ component as magnetic shear (connecting field-line wander and turbulence).,"A natural extension would be to formalize the laboratory-configuration choices into an explicit experimental plan (varying driving frequency, amplitude, interaction length, polarization angle, and plasma parameters) with replication to quantify uncertainty and robustness of the detected daughter-wave signatures. Additional work could test sensitivity to departures from assumptions (compressibility, dispersion near ion-cyclotron effects, background gradients, and mild turbulence/stronger nonlinearity) and develop self-consistent calibration/estimation procedures for the effective $k_{\parallel}=0$ content over the interaction window. Releasing analysis/processing code and standardized datasets would also improve reproducibility and enable broader benchmarking across alternative antenna waveforms and diagnostics.",1306.1460v2,https://arxiv.org/pdf/1306.1460v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:23:31Z
TRUE,Other,Parameter estimation|Other,Not applicable,3 factors (1 treatment factor: programming type; 2 blocking factors: program and tool support; each with 2 levels),Other,Other,TRUE,R,Not provided,NA,"This paper reports a controlled experiment conducted in a university course to compare pair programming versus solo programming. A Latin square design is used, with two blocking factors—program (calculator vs. encoder) and tool support (IDE vs. text editor)—and one treatment factor (pair vs. solo). Outcomes are duration (minutes to complete the task) and effort (person-minutes; pair effort defined as 2×duration). Using ANOVA under the Latin square model and follow-up contrasts (Scheffé), results at alpha=0.1 indicate pair programming significantly reduces duration by about 28%, while solo programming significantly reduces effort by about 30%. The paper also reports assumption checks (Levene, Kolmogorov–Smirnov, Tukey nonadditivity) and a post-hoc power analysis implemented in R, and notes results are close to those reported by Nosek (1998).","The Latin square ANOVA model is $y_{ijk}=\mu+\alpha_i+\beta_j+\tau_k+\varepsilon_{ijk}$, where $\alpha_i$ and $\beta_j$ are the two block effects (rows/columns), $\tau_k$ is the treatment (pair vs. solo) effect, and errors are assumed $\varepsilon_{ijk}\sim N(0,\sigma^2)$. Effort is defined as person-minutes; for pairs, total effort is computed as $\text{Effort}=2\times\text{Duration}$ (solo effort equals duration). Effect size is computed via Cohen's $d$ using the reported ANOVA $F$ statistic: $d=\sqrt{\frac{F(n_1+n_2)}{n_1 n_2}}$.","For duration, ANOVA gives $F=2.9843$ with p=0.0969 (treated as significant at \u000ealpha=0.1); mean duration is 129.64 minutes (solo) vs 93.07 minutes (pair), a difference of 36.57 minutes favoring pairs (~28% decrease), with 95% CI [6.16, 66.99]. For effort, ANOVA gives $F=2.8953$ with p=0.1017 (considered significant by the authors at \u000ealpha=0.1); mean effort is 129.64 (solo) vs 186.14 (pair), a difference of 56.5 minutes favoring solo (~30% decrease), with 95% CI [8.80, 104.20]. Reported effect sizes are medium (Cohen's d≈0.653 for duration and d≈0.643 for effort), and post-hoc power is about 0.51 and 0.50 respectively at \u000ealpha=0.1 with n=14 per group.","The authors note low statistical power (~50%) and plan to estimate sample size beforehand to target ~80% power in replications. They report violation of homogeneity of variances (Levene p=0.0594 for duration; p=0.0241 for effort), attributed to different program complexities, and suggest using programs of similar complexity in future. They also note a measurement reliability issue: some solo-programmer times for the first session were completed at home due to time constraints, which may affect results.","The Latin square is described with two blocking factors, but the experiment involves repeated participation across two sessions and includes pairs (two-person units), raising potential dependence/cluster effects not modeled in a simple Latin-square ANOVA. The study uses students with limited pair-programming experience and a single course setting, limiting external validity beyond this academic context. The choice of \u000ealpha=0.1 and treating p=0.1017 as significant increases the chance of Type I error; multiple outcomes (duration and effort) are tested without adjustment.","The authors plan future replications to gain more insight into the effect of pair programming, including performing an a priori power analysis to increase power to about 80%. They also plan to use programs with similar complexity to reduce variance heterogeneity. Additionally, they intend to use another experimental design to explicitly assess possible interactions between treatments and blocks.","Future studies could use mixed-effects or hierarchical models to account for clustering (pair as a unit) and repeated measures across sessions, and to separate individual- and pair-level variability. Replications with professional developers and in industrial settings would strengthen external validity and assess whether results hold with experienced pairs. Providing an analysis script and dataset in a repository would improve reproducibility and enable sensitivity analyses (e.g., robustness to nonconstant variance and alternative \u000ealpha levels).",1306.4245v1,https://arxiv.org/pdf/1306.4245v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:24:04Z
TRUE,Optimal design|Factorial (full)|Other,Parameter estimation|Cost reduction,A-optimal,Variable/General (examples with 3–9 factors; typically 2-level factors; candidate set size $G=2^F$ in examples),Theoretical/simulation only,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/tanaken-basis/explasso,"The paper proposes a machine-learning-based approach to optimal design of experiments by reformulating the selection of design points from a candidate set as a group lasso optimization problem. Using an A-optimality objective (minimizing the sum of variances of unbiased linear estimators for selected parameters) plus a group sparsity penalty, the method selects a small subset of candidate runs while controlling estimator variance. The constrained formulation is cast as second-order cone programming, and a Lagrangian-relaxed version connects directly to standard group lasso solvers (allowing approximate unbiasedness/confounding when penalties are finite). To break symmetries that can prevent sparse/unique solutions, the authors introduce a heuristic algorithm to set unequal penalty weights $\lambda_g$. Numerical examples with two-level factorial candidate sets show the method can recover orthogonal arrays (e.g., L4 and L8) and can also produce non-orthogonal arrays depending on the model/estimands, with computation time growing rapidly with the number of factors.","The regression-style group lasso is $\hat\beta=\arg\min_\beta \|y-X\beta\|_2^2+\sum_{g=1}^G \lambda_g\|\beta_{I_g}\|_2$. For DOE, with candidate-point model matrix $M$, unbiasedness constraints $M\beta_j=e_j$ yield estimator variance proportional to $\|\beta_j\|_2^2$. The proposed A-optimal sparse design formulation is $\min_{\{\beta_j\}} \sum_{j\in J}\|\beta_j\|_2^2+\sum_{g=1}^G \lambda_g\|\beta_{I_g}\|_2$ s.t. $M\beta_j=e_j$ for $j\in J$ (SOCP), and a relaxed version adds $\sum_{j\in J}\kappa_j\|M\beta_j-e_j\|_2^2$ instead of hard constraints.","For 3 two-level factors under a main-effects model, solving the proposed SOCP with asymmetric penalties $\lambda_g$ (constructed by the paper’s Algorithm 1) selects 4 runs equivalent to the L4 orthogonal array; reported compute time is 0.035 s. For 4 two-level factors with specified two-factor interaction terms, the method selects 8 runs equivalent to the L8 orthogonal array with compute time 0.163 s; with an additional interaction term it selects 9 runs (a non-orthogonal array) with compute time 0.343 s. A timing study for main-effects models with 1–9 two-level factors shows steep growth (seconds): 1:0.021, 2:0.029, 3:0.037, 4:0.136, 5:0.325, 6:1.301, 7:6.987, 8:81.172, 9:742.403.","The authors note that when the number of factors or levels is large, enumerating all candidate design points makes the number of variables in the formulations (6) or (7) grow explosively, making the optimization difficult to solve. They also highlight that obtaining sparse solutions depends on carefully choosing tuning parameters (the $\lambda_g$ and, for the relaxed problem, the $\kappa_j$), and that their Algorithm 1 is only one heuristic and “not necessarily desirable.”","The approach is demonstrated mainly on two-level full-factorial candidate sets and relatively small numbers of factors; scalability beyond that regime is not addressed with stronger algorithmic remedies (e.g., column generation or randomized candidate generation). The method targets A-optimality for linear (polynomial) models with unbiased linear estimators; performance under model misspecification, non-Gaussian errors, heteroskedasticity, or correlated errors is not analyzed. Practical guidance for selecting $\lambda_g$ and $\kappa_j$ (e.g., via principled calibration to achieve a target run size or efficiency) is limited, and results may be sensitive to the asymmetry heuristic. Comparisons against established computational optimal design methods (e.g., exchange algorithms, coordinate-exchange, Fedorov, modern optimal design software) are not provided, so relative efficiency and robustness are unclear.",They propose investigating how to choose/reduce the candidate design points in advance to control problem size when factors/levels are large. They also propose studying improved methods for determining the tuning parameters $\lambda_g$ and $\kappa_j$.,"Develop scalable variants that avoid full candidate enumeration (e.g., column generation, greedy/Frank–Wolfe-style selection, or stochastic search over candidate points) and provide complexity/optimality guarantees. Add principled tuning strategies that directly target a desired number of runs or design efficiency (e.g., continuation paths in $\lambda$, information-based calibration, or Bayesian/empirical-Bayes selection). Extend the framework to other optimality criteria (D-, I-, or compound criteria), mixed-level and constrained regions, and to split-plot/blocked designs with random effects. Provide broader empirical benchmarking against standard optimal design algorithms and release a reproducible pipeline (including solver settings) for fair comparisons.",1308.1196v2,https://arxiv.org/pdf/1308.1196v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:24:30Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Other,A-optimal|Bayesian A-optimal,Variable/General (infinite-dimensional parameter field; design variables are Ns sensor-location weights),Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper develops scalable algorithms for Bayesian A-optimal experimental design (sensor placement) in infinite-dimensional linear inverse problems governed by PDEs. The design chooses a sparse subset of candidate sensor locations by optimizing weights in [0,1] to minimize the trace of the posterior covariance (average posterior variance) of an inferred parameter field (e.g., an initial condition). Computational efficiency is achieved by building a low-rank surrogate of the prior-preconditioned parameter-to-observable map using randomized SVD and by estimating the trace objective with randomized (Gaussian) trace estimators, avoiding repeated expensive PDE solves during design optimization. Sparsity and near-binary (0–1) sensor configurations are obtained via a continuation scheme of nonconvex penalties approximating the ℓ0 “norm”, which the authors show outperforms ℓ1-sparsified designs. Numerical studies on 2D/3D time-dependent advection–diffusion demonstrate that the cost in forward PDE solves and the number of quasi-Newton iterations are largely insensitive to parameter and sensor dimensions.","A-optimal objective: minimize $\operatorname{tr}(\Gamma_{\text{post}}(w))$ where $\Gamma_{\text{post}}(w)=\big(F^*W^{1/2}\Gamma_{\text{noise}}^{-1}W^{1/2}F+\Gamma_{\text{prior}}^{-1}\big)^{-1}$ (discretized form). The optimization problem is $\min_{0\le w_i\le 1}\;\operatorname{tr}(\Gamma_{\text{post}}(w))+\gamma\,\Phi(w)$, with $\Phi$ chosen as an $\ell_1$ penalty or a continuation family $\Phi_\varepsilon$ approximating $\|w\|_0$ to induce binary sensor selections. Trace is approximated by a randomized estimator $\Theta(w)=\frac1{N_{tr}}\sum_{i=1}^{N_{tr}}\langle z^{(i)},H(w)^{-1}z^{(i)}\rangle_M$ with gradient using $\partial\Theta/\partial w_j=-\frac1{N_{tr}}\sum_i\langle q^{(i)},(\partial H_{\text{misfit}}/\partial w_j)q^{(i)}\rangle_M$ and $\partial H_{\text{misfit}}/\partial w_j=F^*E_jF$.","In 2D studies (e.g., $N_s=122$ candidates, $n\approx 1012$ parameters), low-rank surrogates of the prior-preconditioned map converge quickly; objective values stabilize for surrogate ranks around $r\gtrsim 40$ in a reported test. The number of interior-point quasi-Newton iterations and function evaluations for solving the design problem is reported to be largely insensitive to parameter dimension and to the number of candidate sensor locations (e.g., iterations stay on the order of ~60–80 across multiple discretizations and sensor grids). $\ell_0$-approximating (continuation) sparsification yields binary designs and consistently improves the exact $\operatorname{tr}(\Gamma_{\text{post}})$ compared with $\ell_1$-sparsified designs and with random/uniform sensor placements; the paper notes diminishing returns beyond roughly ~20 sensors in the 2D example. Trace estimation accuracy improves with more probe vectors; an example reports average trace-estimation errors decreasing from ~15% (1 vector) to ~1.5% (100 vectors), while designs computed with trace estimation still outperform random designs in terms of exact posterior trace.","The authors state the method relies on linearity of the parameter-to-observable map and on Gaussian prior and noise distributions (posterior Gaussian), which may not hold in general. They note efficiency depends on the existence of a low-rank approximation of the prior-preconditioned parameter-to-observable map, which is tied to properties/ill-posedness of the forward and observation operators. They also acknowledge that using continuous weights with sparsification gives only indirect control of the number of sensors through the regularization parameter $\gamma$, rather than exact cardinality control.","The approach focuses on A-optimality (trace of posterior covariance), which may not align with task-specific utilities (e.g., decision-theoretic, worst-case/G-optimal objectives) or nonuniform spatial importance; extensions would require re-deriving efficient estimators and gradients. The binary design recovery uses nonconvex continuation and an interior-point solver, which can be sensitive to initialization and may not guarantee global optimality; the paper’s comparisons may not cover alternative mixed-integer or combinatorial sensor-placement solvers. Practical implementation requires PDE solvers, adjoints, and prior square-root applications (elliptic solves); although claimed scalable, performance can still depend on preconditioners, solver tolerances, and the chosen ranks/trace-probe counts, and these tuning aspects are not fully standardized for practitioners.","They suggest extending the framework to other infinite-dimensional OED criteria and to nonlinear parameter-to-observable maps. For nonlinear maps, they note challenges including non-Gaussian posteriors, dependence of (linearized) operators on state/parameter/data, inability to precompute low-rank surrogates a priori, potential non-uniqueness of optimal designs, and misfit Hessians depending on (often unavailable) observations.","Developing explicit cardinality-constrained formulations (e.g., $\|w\|_0\le k$) via efficient approximations or mixed-integer relaxations could provide direct sensor-count control while leveraging low-rank structure. Robust versions addressing model error, correlated/heteroscedastic noise, or misspecified priors (and studying robustness of selected sensor sets) would improve practical reliability. Providing open-source implementations (e.g., MATLAB/Python) and benchmark suites for PDE-based sensor placement would facilitate reproducibility and adoption, and could enable systematic comparisons with alternative sparse Bayesian or submodular selection methods.",1308.4084v2,https://arxiv.org/pdf/1308.4084v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:25:10Z
FALSE,NA,NA,Not applicable,Not specified,Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,http://partsregistry.org,"The paper proposes a conceptual experimental blueprint for establishing chemical communication between bottom-up semi-synthetic minimal cells (liposome-encapsulated cell-free expression systems) and engineered bacteria, using quorum-sensing N-acyl-homoserine lactones (AHLs) as diffusible signal molecules. It outlines sender and receiver architectures for synthetic cells (constitutive expression of an AHL synthase; or constitutive expression of an AHL receptor coupled to an inducible reporter cassette), and corresponding requirements for the natural partner (AHL-negative but receptor-positive reporter bacteria). Preliminary laboratory results are reported showing (i) successful protein expression (GFP) inside giant vesicles produced by the droplet-transfer method, and (ii) stability testing of liposomes in bacterial growth media and in bacterial culture using a calcein-release assay. It also reports validation of engineered bacterial reporter strains (ΔS PS::lux and PS::mCherry) that respond to exogenous signal molecule S and do not respond to empty liposomes, supporting feasibility of the intended communication channel. Overall, the contribution is an experimental plan/protocol blueprint rather than a formal design-of-experiments (DOE) methodology paper, with limited quantitative performance analysis and no optimization or factorial design component.",Not applicable,"Giant vesicles produced by the droplet-transfer method achieved an encapsulation efficiency of about 40% (based on capture/release of fluorescent probes) and supported in situ GFP expression observed by confocal microscopy. Liposome stability testing via calcein-release indicated low leakage/lysis over 240 minutes in LB medium and when incubated with a bacterial culture, with near-complete fluorescence release only after adding cholate (positive-lysis control). An engineered reporter bacterium ΔS PS::lux showed strong induction of luminescence when grown with exogenous signal molecule S over a 240-minute time course, and both lux- and mCherry-based reporter systems were inactive with empty liposomes but produced signal when incubated with S-loaded liposomes, indicating S can diffuse across the liposome membrane.","The authors note that luminescence (luxCDABE) reporting cannot detect promoter activation at the single-cell level, which is problematic if the signal produced by synthetic cells is low and localized near individual synthetic cells; they propose fluorescence (mCherry) as an improvement. They also indicate that membrane-protein production in SSMCs is currently troublesome, motivating a design choice to avoid protocols requiring membrane receptors/export devices and to favor water-soluble components. They acknowledge that the overall design is simple and “not yet proved.”","The work does not provide a rigorous, quantitative end-to-end demonstration of synthetic-cell-produced AHL generation at measurable levels (e.g., chemical quantification or calibrated dose–response), relying instead on feasibility arguments and partial validations. The “preliminary results” are limited in scope and do not systematically evaluate variability, reproducibility, or robustness across batches of vesicles/bacterial cultures (important given heterogeneity in vesicle size/encapsulation). It also does not address challenges from non-ideal conditions common in biological signaling experiments (e.g., degradation kinetics of AHLs, background activation, parameter uncertainty, or time delays) with formal modeling or statistical design/analysis.","They state that next they will test whether more complex genetic circuits can be reconstructed inside giant vesicles prepared by the droplet-transfer method. They also indicate ongoing/near-term work to validate engineered bacterial strains and to focus first on the bacteria-as-receiver/SSMC-as-sender direction, with refinement toward single-cell fluorescent reporting. They mention that future advancements could lead to synthetic cells that interact with natural cells (and other synthetic cells) as soft-wet micro-robots capable of manipulating/‘computing’ biological chemical signals.","A natural next step is a quantitatively calibrated communication assay: measure AHL production rates inside SSMCs, diffusion/partitioning across membranes, and bacterial reporter dose–response using independent chemical analytics (e.g., LC–MS) alongside reporters. Developing a statistically powered experimental plan (e.g., varying vesicle size distributions, encapsulation levels, promoter strengths, and substrate concentrations) would help identify key drivers and improve reproducibility. Extending the approach to noisy/realistic environments (mixed microbial communities, flow/microfluidic gradients, and AHL-degrading enzymes) and adding controls for cross-talk between quorum-sensing systems would strengthen generalizability. Packaging the protocols and analysis in an open, reproducible software workflow (even simple data/plot scripts) would improve adoptability.",1309.7687v1,https://arxiv.org/pdf/1309.7687v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:25:46Z
FALSE,Response surface|Other,Prediction|Optimization|Other,Not applicable,Variable/General (examples use 4–5 master nodes; network has 16 nodes in circuit example),Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.nature.com/scientificreports,"The paper proposes a control methodology for nonlinearly coupled networks when topology and detailed dynamics are unknown, using experimentally measured static responses to perturbations (“response surfaces”). It shows (via an electrical circuit network with JFET-based nonlinearities, plus a second circuit with inhibitory interactions) that these response surfaces are smooth and can be well-approximated by low-order polynomials (planar and quadratic fits) built from a limited set of mutant/perturbation measurements (e.g., single and selected double mutants). Using the fitted surface, the method computes master-node settings that minimize a weighted squared distance to a desired target state; if the target is too far, additional master nodes are selected iteratively based on which output constraints dominate the residual distance. Robustness to measurement noise is evaluated by adding synthetic Gaussian noise (5% and 10%) to mutant measurements and quantifying the resulting deviation of approximations from the measured surface. The approach is positioned as a practical alternative to topology-based or trajectory-based control methods, with potential relevance to biological reprogramming tasks (e.g., gene regulatory networks).","The planar (first-order) response-surface approximation is given by $\mathbf{X}(\boldsymbol{\ell})=\mathbf{X}^{(0)}+\sum_{n=1}^4 \ell_n\,(\mathbf{X}^{(n)}-\mathbf{X}^{(0)})$ where $\mathbf{X}^{(0)}$ is the unperturbed state and $\mathbf{X}^{(n)}$ are single-knockout mutant states (Eq. 1). A quadratic approximation augments this with linear and pairwise product terms in master-node deviations (Eq. 3). The target-setting is obtained by minimizing a weighted squared distance $\sum_m w_m\,(T_m-X_m(\boldsymbol{\ell}))^2$, leading to normal-equation conditions $\sum_m w_m\,(T_m-X_m(\boldsymbol{\ell}^*))\,(X_m^{(n)}-X_m^{(0)})=0$ for each master-node direction (Eq. 4).","On a 25×25 grid for a 2D cross-section (varying two master nodes), the mean deviation between the measured response surface and the planar approximation was 152 mV, versus 24 mV for a quadratic approximation built using three additional mutant points. With synthetic 5% Gaussian noise added to mutant measurements, mean deviations increased to 255 mV (planar) and 207 mV (quadratic); at 10% noise, deviations were about 450 mV (planar) and 430 mV (quadratic), compared to a mean surface magnitude of 5.19 V. For error estimation using limited extra mutants, mean planar-prediction errors reported include 38.4 mV (six “half-value” double-master mutants), 18.5 mV (double knockouts), 28.2 mV (triple knockouts), and 35.7 mV (quadruple knockout). In an example weighted-target task, solving the planar system produced master settings (e.g., $X_1=0.89,X_2=0.96,X_3=0.71,X_4=1.44$) that achieved close realized node values and a small weighted error ($U\approx1.2\times10^{-2}$); a harder target required adding node 16 as an additional master node to reduce $U$ to $\approx2.1\times10^{-2}$.","The authors note their method uses only stationary (equilibrium) responses, so it “cannot be used to perform dynamic feedback control.” They also state the approach may need extension in the presence of bistability when the response surface is folded, suggesting either replacing the bistable master node or using additional mutants to estimate multiple branches.","The work does not formalize an experimental-design strategy (e.g., optimal selection of mutants/perturbations) for constructing polynomial approximations under resource constraints; mutant selection is heuristic (single knockouts, midpoints, etc.). Performance is demonstrated primarily on small physical circuits and synthetic noise injection; generalization to high-dimensional biological systems may face practical issues (feasible intervention ranges, nonstationarity, hidden confounding, and strong context dependence) not quantified here. The approach assumes the response surface is sufficiently smooth and well-approximated locally by low-order polynomials; failure modes and diagnostics for non-smooth or highly curved regions are not rigorously developed.","They suggest extending the algorithm to handle bistability/folded response surfaces, either by replacing bistable master nodes or using additional mutants to estimate multiple branches. They also discuss (as a prospective application direction) applying the method to gene regulatory networks and cellular reprogramming, including optimizing transcription-factor levels and potentially adding additional genes beyond the initial master set.","Developing a principled DOE/active-learning scheme (e.g., sequential design) to choose the most informative additional mutants for improving polynomial/surrogate accuracy would make the method more resource-efficient. Providing uncertainty quantification (confidence regions for the closest-point solution and master-node settings) under measurement noise and replicate variability would improve practical decision-making. Extending the approach to accommodate temporal dynamics (even partial) or drifting equilibria, and validating on real biological perturbation datasets, would strengthen applicability beyond controlled circuit experiments.",1310.2623v2,https://arxiv.org/pdf/1310.2623v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:26:28Z
TRUE,Computer experiment|Sequential/adaptive|Other,Model discrimination|Prediction|Other,Not applicable,"Variable/General (7 rungs varying cadence/sampling, season length, noise model, microlensing; plus lens/light-curve parameters drawn from OM10 catalog)",Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab)|Not provided,https://github.com/gdobler/mules|http://timedelaychallenge.org|http://www.cosmograil.org|http://gavo.mpa-garching.mpg.de/Millennium/|http://authorea.com,"This paper defines the experimental design for the Strong Lens Time Delay Challenge (TDC), a blinded community challenge to evaluate and compare algorithms for estimating gravitational-lens time delays from mock observed quasar light curves. The authors design two “ladders” (TDC0 for practice/validation and TDC1 at LSST-like scale) with multiple rungs whose datasets increase in realism by varying sampling cadence, observing season length, photometric noise properties, and inclusion of microlensing. Intrinsic quasar variability is simulated with a damped random walk/continuous autoregressive (CAR/DRW) process; multiple imaging and time delays are drawn from the OM10 mock LSST lens catalog; microlensing is generated via magnification maps traced along source trajectories; and observations are sampled with realistic gaps and noise. The challenge is evaluated with four metrics—efficiency (submission fraction), reduced chi-square for uncertainty calibration, claimed precision, and fractional accuracy—used both for qualification (TDC0) and ranking (TDC1). TDC0 participation and qualification outcomes are summarized, and access to the challenge data is provided via the project website.","The intrinsic quasar magnitude time series is modeled as a CAR(1)/DRW process: $M(t)=e^{-t/\tau}M(0)+\bar{M}(1-e^{-t/\tau})+\sigma\int_0^t e^{-(t-s)/\tau}\,dB(s)$. Algorithm performance is scored using reduced $\chi^2=\frac{1}{fN}\sum_i\left(\frac{\tilde{\Delta t}_i-\Delta t_i}{\delta_i}\right)^2$, average relative uncertainty (precision) $P=\frac{1}{fN}\sum_i \frac{\delta_i}{\Delta t_i}$, and average fractional residual (accuracy) $A=\frac{1}{fN}\sum_i\frac{\tilde{\Delta t}_i-\Delta t_i}{\Delta t_i}$ (with $f$ the fraction of light curves with estimates). The cosmological context includes the time-delay distance $D_{\Delta t}=\frac{D_d D_s}{D_{ds}}$.","TDC0 is specified as 7 rungs that vary: sampling cadence (1 day, 2 weeks, or “opsimish” Gaussian with mean 12 days and SD 2 days), season duration (12 months or 4 months), noise (0.03 “uni” or “opsimish” noise of 0.053 nanomaggies with error 0.016), and microlensing (off for rungs 0–5, on for rung 6). Qualification thresholds for proceeding to TDC1 are $f>0.3$, $0.5<\chi^2<2$, $P<15\%$, and $|A|<15\%$. By the TDC1 close (July 1, 2014), 13 teams submitted to TDC0 using 47 methods, and 7 teams qualified for TDC1.","For this first challenge, uncertainties arising from contamination by light from the foreground lens/source galaxy were not taken into account; the authors note this could be important especially for fainter images and should be addressed in future challenges. They also note that while DRW/CAR describes existing data well, it is not yet clear it will remain adequate for longer baselines, higher cadence, or multi-filter light curves (where more complex stochastic models may be needed).","The design focuses on simulated light curves; algorithm performance may be sensitive to any mismatch between simulation assumptions (DRW variability, microlensing modeling choices, noise model, cadence/gap structure) and real LSST systematics (e.g., calibration drifts, seeing-dependent blending, correlated errors). The evaluation metrics collapse performance into global averages (A, P, $\chi^2$, f), which may mask regime-dependent failures (e.g., very short/very long delays, low SNR, strong microlensing) unless stratified analyses are reported. No explicit optimality criterion is used to choose the rung settings; the rung grid is scenario-based rather than formally optimized for information about specific algorithm weaknesses.","The authors anticipate future challenges that further increase simulation complexity to stimulate gradual algorithmic improvements, explicitly mentioning that foreground-light contamination and other added realism should be addressed in future iterations. They also state that all simulation software is written in Python and will be made publicly available after completion of the challenge.","Provide a formal experimental-design rationale (e.g., factorial/Latin-hypercube over cadence, season length, noise, microlensing strength) to enable clearer attribution of performance changes to individual factors and interactions. Expand scenarios to include multiband joint modeling, correlated photometric systematics, and blending/host-galaxy contamination consistent with LSST image-domain effects. Release standardized baselines and an evaluation toolkit (with stratified metrics by delay length/SNR/microlensing regime) to improve reproducibility and diagnostic power across future TDC rounds.",1310.4830v3,https://arxiv.org/pdf/1310.4830v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:27:05Z
TRUE,Optimal design,Parameter estimation,A-optimal|Other,Variable/General (linear model with n parameters; design space has m candidate regression vectors),Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Not provided,http://arxiv.org/abs/1303.5046v1|http://arxiv.org/abs/1311.2335v1|http://www.tandfonline.com/doi/full/10.1080/01621459.2013.806268#.UfqSb64Sbdw,"The paper develops and analyzes first-order (Frank–Wolfe type) algorithms for the A-optimal approximate experimental design problem on a finite candidate set of regression vectors, under a standard linear model with independent normal errors. It frames A-optimality as maximizing $-\mathrm{Trace}(M(u)^{-1})$ over the probability simplex (approximate design weights) and connects it via strong duality to an ellipsoidal inclusion problem that maximizes $\mathrm{Trace}(H^{1/2})$ for an enclosing ellipsoid. Two algorithms are proposed: a basic Frank–Wolfe method that returns an $\epsilon$-primal-feasible design and an enhanced variant with Wolfe away steps that returns an $\epsilon$-approximate optimal design; both use closed-form/analytic step-size selection aided by rank-one update formulas. The paper provides iteration-complexity bounds (e.g., $O(\ln m+\epsilon^{-1})$ for the basic method and bounds for the away-step variant) and shows local linear convergence for the away-step schemes under additional regularity. Computational experiments on randomly generated instances (implemented in MATLAB) demonstrate that away steps and improved initializations substantially reduce runtime, and that the proposed first-order approach is dramatically faster than an SDP reformulation solved via SDPT3/CVX on the tested sizes.","Approximate A-optimal design is posed as $\max_{u\ge 0,\, e^Tu=1}\;\hat g(u):=-\mathrm{Trace}(M(u)^{-1})$ (equivalently $\max g(u):=-\ln\mathrm{Trace}(M(u)^{-1})$) where $M(u)=\sum_{i=1}^m u_i x_i x_i^T$. The dual ellipsoidal problem is $\min_{H\succ 0}\; f(H):=-2\ln\mathrm{Trace}(H^{1/2})$ s.t. $x_i^T H x_i\le 1$; strong duality yields $H^*=\frac{(M(u^*))^{-2}}{\mathrm{Trace}(M(u^*)^{-1})}$ with complementarity $x_i^T H^* x_i=1$ for $u_i^*>0$. The gradient used for pivoting is $\alpha_i(u)=x_i^T(M(u)^{-2})x_i$, and an iteration updates weights by $u^+=(1-\tau)u+\tau e_j$ (or an away step) with step size chosen to maximize the objective, using rank-one update identities for $M(u^+)^{-1}$ and the resulting objective change.","The paper proves that the basic Frank–Wolfe algorithm returns an $\epsilon$-primal-feasible design in at most $O(\ln m+\epsilon^{-1})$ iterations under a bounded-ellipsoidal-distance assumption, with each iteration implementable in $O(nm)$ arithmetic using rank-one updates. With Wolfe away steps, the method reaches an $\epsilon$-approximate optimal design in at most $O(m+\epsilon^{-1})$ iterations (drop steps are bounded by add steps plus a constant tied to the active set). With stronger initialization based on a 1-approximate D-optimal design (WA-TY), the iteration bound improves to $O(n\ln n+\epsilon^{-1})$ (and active-set/core-set size bounds of the same order). Computationally (MATLAB), the away-step variant is consistently faster than the basic method, and the proposed first-order method substantially outperforms an SDP reformulation solved by SDPT3/CVX (reported speedups often tens to hundreds of times on tested small/medium instances).","The convergence/complexity analysis depends on Assumption 1, requiring a uniform bound $\omega_j(u)=x_j^T M(u)^{-1}x_j\le \omega$ along iterates; the constant $\omega$ is data- and trajectory-dependent and can be large when design points are nearly confined to a lower-dimensional subspace. The local linear convergence rate is also stated to depend on problem data/dimensions and does not yield better global bounds than the earlier $O(\epsilon^{-1})$-type rates.","The work focuses on approximate (continuous-weight) designs on a finite candidate set; it does not address constructing exact integer-replicate designs beyond citing standard rounding/discretization references, nor continuous design regions where candidate generation matters. The computational claims are based on MATLAB timing and random-instance generators; no public code or standardized benchmark suite is provided, and comparisons are limited (e.g., few competing modern A-optimal solvers beyond SDP and related initializations). The modeling assumptions largely follow the classical linear model with independent Gaussian errors; robustness to model misspecification, heteroskedasticity, or correlated errors is not studied.",None stated.,"Develop self-contained exact-design extraction/rounding procedures with guarantees from the approximate A-optimal solutions produced by the algorithms, including tradeoffs between support size and integer feasibility. Extend the approach to correlated/heteroskedastic error structures (generalized least squares information matrices) and to constraints on runs/costs (e.g., budgeted or group/blocked designs). Provide open-source implementations and compare systematically against state-of-the-art A-optimal design solvers on shared benchmarks, including very large-scale candidate sets and streaming/column-generation settings.",1311.2335v1,https://arxiv.org/pdf/1311.2335v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:27:42Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Prediction|Model discrimination|Cost reduction|Other,G-optimal|Bayesian D-optimal|Not applicable|Other,Variable/General (N parameters in nonlinear ODE models; examples include 2 parameters and 19 parameters),Healthcare/medical|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper develops a model-based sequential design of experiments (DOE) framework for identifying the observable output dynamics of cellular processes governed by nonlinear ODE models, building on the Maximally Informative Next Experiment (MINE) idea. Existing measurements induce a probability distribution over model parameters via a normalized likelihood, and the next measurement time is selected where the predicted model output variance (under this distribution) is maximal or near-maximal (a relaxed criterion within a constant factor). The authors introduce the Expected Dynamics Estimator (EDE), which estimates dynamics by averaging model outputs over the induced parameter distribution, and prove EDE consistency (uniform convergence to true dynamics) under multiple settings: noiseless data, sequential variance-based sampling even when samples cluster, noisy Gaussian measurements with finite time grids and replication, and bounded model mismatch with output discretization. Computational variants are proposed to improve tractability and robustness, and numerical examples on biochemical and T-cell signaling models (including high-dimensional parameter spaces) illustrate rapid convergence and practical benefits of near-max-variance selection and resampling. The work advances DOE for systems biology by shifting the target from parameter identification to accurate dynamics (response) identification with provable convergence under practical constraints.","Model: $\dot{x}=\alpha(\omega,x)$, $y(t)=f(\omega,t)=\beta(\omega,x(t))$. Parameter posterior surrogate (likelihood-weighted) distribution: $p_n(\omega)=c_n\exp\{-\sum_{i=1}^n (d_i-f(\omega,t_i))^2\}$ (and variants using $|\cdot|^r$ and replication-averaged data on grids). Expected Dynamics Estimator: $\hat D_n(t)=\mathbb{E}_{p_n}[f(\omega,t)]$, and sequential design chooses $t_{n+1}$ to maximize (or approximately maximize) $\mathrm{Var}_{p_n}(f(\omega,t))$; relaxed criterion: $\mathrm{Var}_{p_n}(f(\omega,t))\le C\,\mathrm{Var}_{p_n}(f(\omega,t_{n+1}))$.","The authors prove uniform consistency of the EDE ($\hat D_n(t)\to g(t)$) for (i) random space-filling time samples from an absolutely continuous distribution, and (ii) sequential MINE-style designs selecting the time of maximal (or near-maximal) posterior predictive variance, including cases where sampling times cluster on a finite set. With noisy Gaussian data, consistency is shown on a finite measurement-time grid when replication counts grow at selected points and the likelihood is formed using replicate averages weighted by replicate counts. With bounded model mismatch and adaptive discretization, the EDE converges to dynamics that are $\varepsilon$-equivalent to the true dynamics for almost every discretization resolution $\varepsilon$ above the mismatch bound. Numerical examples demonstrate fast reduction of $L_\infty$ dynamics error after only a handful of sequential experiments (e.g., in a 2-parameter chain-reaction ODE and a 19-parameter T-cell signaling model), and show that the relaxed near-max-variance rule can improve practical convergence under noise by enabling resampling priorities.","The authors note that their framework focuses on interpolation/identification of observable outputs over the measured interval and does not address extrapolation to unobserved quantities. For noisy-data convergence they require restricting measurements to a finite set of time points (a discretized grid) to guarantee replication/averaging, and in several proofs they assume a correct model (later relaxed to bounded mismatch) and sometimes finite parameter space or discretized outputs to avoid measure-zero/support issues. They also acknowledge that their approach is not fully Bayesian experimental design because it does not choose design points by maximizing an expected utility function.","Much of the theory relies on independence assumptions (e.g., i.i.d. Gaussian measurement noise, and effectively independent/replicated observations at grid points) and does not treat temporally correlated noise or process autocorrelation common in time-series biological data. The MINE/EDE approach assumes the model structure is sufficiently expressive; when mismatch is more complex than a bounded uniform error, the discretization-based guarantees may be weak and performance could depend heavily on chosen grids and rounding rules. Practical computational performance depends on MCMC quality and surrogate/interpolation error (sparse grids), but the paper does not provide rigorous guidelines for diagnosing MCMC convergence or propagating surrogate uncertainty into design decisions.","The authors propose extending the theoretical validation to other MINE criteria suggested in prior work beyond the variance-based criterion analyzed here. They also suggest extending the framework to the extrapolation setting, where measurements of one output are used to infer unobservable outputs that are theoretically identifiable, and studying which unobservable outputs are identifiable under specific experimental constraints.","Developing self-starting/online variants that jointly adapt the measurement-time grid and replication allocation (e.g., using Bayesian decision-theoretic utilities) could improve efficiency under noise and constraints. Extending theory and methods to correlated noise, partially observed states with measurement delays, and multivariate outputs (including multichannel time courses) would broaden applicability. Providing open-source implementations and standardized benchmarks against alternative Bayesian optimal design and active learning methods (e.g., expected information gain, mutual information, or entropy reduction) would strengthen empirical validation and adoption.",1311.3261v1,https://arxiv.org/pdf/1311.3261v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:28:26Z
TRUE,Optimal design|Bayesian design|Computer experiment|Other,Parameter estimation|Prediction|Other,Bayesian A-optimal|Other,"Variable/General (designs an m×n sensing/measurement matrix A; examples use n=100 and m∈{20,40,60,80})",Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,http://spams-devel.gforge.inria.fr,"The paper proposes a Bayesian experimental design approach for constructing a compressive sensing measurement (sensing) matrix A for estimating a structured signal x in the presence of structured pre-measurement interference (“clutter”) c and post-measurement noise w under the linear model y = A(x+c)+w. Prior knowledge about x and c is encoded via mixture models specified only through first- and second-order statistics (mixture weights and covariance matrices), and the sensing design is chosen to minimize estimation error for x while treating clutter as a nuisance. The authors restrict inference to the linear MMSE (Wiener filter) estimator and derive an objective equivalent to maximizing a trace term (reducing LMMSE MSE) subject to a Frobenius-norm (energy) constraint on A; they note this corresponds to a Bayes A-optimality-type criterion. Because the resulting optimization is non-convex, they present an iterative alternating scheme over singular values and a subspace (eigenvector selection) with a waterfilling-style update. Synthetic experiments (n=100, mixture counts 10/10, rank-6 covariances) show the designed matrices consistently yield higher reconstruction SNR than random Gaussian projections and two heuristic knowledge-enhanced baselines across multiple measurement counts and energy budgets.","Measurement model: $y = A(x+c) + w$. With linear MMSE estimator $\hat x_{\mathrm{LMMSE}}(y)=\Sigma_x A^\top\big(A(\Sigma_x+\Sigma_c)A^\top+I\big)^{-1}y$, the design objective becomes maximizing $\mathrm{tr}\{\Sigma_xA^\top(A(\Sigma_x+\Sigma_c)A^\top+I)^{-1}A\Sigma_x\}$ subject to an energy constraint $\|A\|_F\le \alpha$ (equivalently minimizing $\mathrm{tr}$ of the LMMSE error covariance). The iterative solution uses a whitening transform $Y^\top(\Sigma_x+\Sigma_c)Y=I$ and alternating updates including a waterfilling form $\gamma_i^* = \big(\sqrt{b_i/(c_i v^*)}-1\big)_+$ with $\sum_i c_i\gamma_i^* = \alpha^2$.","Across synthetic trials (1000 per setting) with $n=100$, $m_x=m_c=10$, and rank-6 signal/clutter covariance models, the proposed Bayesian design produces higher reconstruction SNR than (i) standard random Gaussian sensing matrices and (ii) two heuristic knowledge-enhanced designs. Improvements hold consistently across measurement counts $m\in\{20,40,60,80\}$ and across the tested sensing energy budgets $\alpha^2$, as shown in Fig. 1. The paper also reports that incorporating cancellation/clutter suppression into the sensing matrix (“annihilate-then-estimate”) empirically outperforms designing to estimate $x+c$ and separating later (“estimate-then-annihilate”) in their experiments.","The authors note they restrict the estimator class to estimators linear in the observations $y$ for analytical tractability, rather than the full (generally intractable) MMSE estimator. They also describe their conclusions about strategy comparisons as preliminary and state that further investigation is needed to make the qualitative claims definitive. The experimental validation is performed on synthetic data.","The design assumes accurate knowledge of first- and second-order statistics (mixture weights/covariances) for both signal and clutter; performance may degrade under covariance/mixture misspecification or nonstationarity. The approach is derived for uncorrelated $x$, $c$, and $w$ and assumes $\Sigma_x+\Sigma_c$ invertible; correlated nuisance terms or singular/ill-conditioned sums may complicate applicability. The optimization is non-convex and solved by an alternating heuristic, so convergence to a global optimum is not guaranteed and sensitivity to initialization/iteration count is not fully characterized. Empirical results focus on reconstruction SNR with a specific downstream group-lasso pipeline (SpaMS) rather than direct evaluation of the LMMSE MSE objective, and comparisons omit some modern adaptive/learned sensing baselines.",They state that a more thorough investigation of the similarities between their MSE-based design and mutual-information-based Bayesian experimental design (via known connections between MMSE and mutual information gradients) is deferred to subsequent work. They also indicate further investigation is needed to make definitive claims about the relative merits of “annihilate-then-estimate” versus “estimate-then-annihilate” strategies beyond their preliminary study.,"Extend the design to handle unknown/estimated covariances (Phase I learning) with robustness to covariance uncertainty and model mismatch, e.g., robust or minimax Bayesian designs. Develop versions accommodating temporal/spatial correlation, non-Gaussian clutter/noise, and possibly correlated $x$ and $c$. Provide stronger convergence/optimality guarantees or improved optimization (e.g., manifold optimization with multiple restarts) and characterize sensitivity to initialization. Validate on real sensing applications (e.g., radar/sonar imaging, MRI) and release reference implementations to support reproducibility and practitioner adoption.",1311.5599v1,https://arxiv.org/pdf/1311.5599v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:29:06Z
FALSE,NA,NA,Not applicable,Not specified,Transportation/logistics|Other,Other,TRUE,Python|MATLAB|Other,Personal website,http://www.purl.org/simulator_code/,"This technical report describes an experimental platform (test bed) for human-in-the-loop driving simulations using a Force Dynamics 401CR motion-base simulator integrated with the PreScan simulation environment for real-time experiments (~200 Hz). The work focuses on system integration and instrumentation: a UDP-based communication pipeline (PreScan/Simulink to Python to the motion platform), modular Simulink data-logging blocks, and additional driver-monitoring sensors (camera/Kinect, eye-tracking glasses, a custom capacitive touch sensor on the steering wheel, and an Android texting app). The setup is intended to enable safe studies of driver distraction and multi-agent driving scenarios while allowing researchers to control simulation variables (road, weather, obstacles, vehicle dynamics). The report also documents calibration of the touch sensor and describes basic MATLAB scripts for unpacking and preprocessing logged vehicle/radar/lane-marker data. Safety/IRB considerations and simulator safety interlocks are outlined to support human-subject experimentation.",Not applicable,"The simulator/software integration is reported to run in real time at approximately 200 Hz. The steering-wheel capacitive touch sensor is calibrated across five resistor values and hand distances from 0 to 10 cm; the authors select a 13 kΩ resistor for the final design because it provides the most boolean (touch/no-touch) response. The texting-distraction Android app triggers messages randomly in a 30–60 second window and reports events (pickup/touch/put-down) via Bluetooth; touch sensor data are transmitted every 10 ms. No statistical performance metrics (e.g., power, effect sizes) for a formal experimental design are reported because the document primarily describes the platform rather than results of a completed designed experiment.",None stated,"The report does not specify a formal DOE plan (factors/levels, randomization, blocking, counterbalancing, sample size/power), so reproducible inference about distraction effects would require additional experimental-design detail. Most setup parameters (e.g., motion gains) are described as hand-tuned via trial-and-error and driver feedback, which may reduce repeatability and introduce experimenter/subject bias unless standardized calibration protocols are provided. External validity may be limited if simulator behavior and sensor outputs are not validated against real-vehicle ground truth, and the platform’s results may be sensitive to simulator sickness, learning effects, and within-subject fatigue if not explicitly controlled.","The report states the platform will be used for many future experiments, with a primary focus on human interaction with semi-autonomous or autonomous vehicles, including building driver models and smart active safety systems.","Provide reference DOE templates for common studies (e.g., distraction modality × traffic density × automation level) with recommended counterbalancing, randomization, and sample-size guidance to standardize use across labs. Extend the platform to handle non-ideal sensing and communication delays (noise, dropouts, latency) to better match real-vehicle conditions and evaluate robustness. Release a packaged, versioned software distribution (e.g., GitHub repo and/or MATLAB toolbox/Python package) plus example experiment scripts and data schemas to improve reproducibility and facilitate community contributions.",1401.5039v1,https://arxiv.org/pdf/1401.5039v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:29:35Z
FALSE,NA,NA,Not applicable,Not specified,Finance/economics|Theoretical/simulation only,Simulation study,TRUE,MATLAB,Public repository (GitHub/GitLab)|Not provided,http://www.csus.edu/indiv/p/perezs/Data/data.htm,"The paper studies variable-selection/model-selection procedures for time-series regressions and proposes using Global Sensitivity Analysis (GSA), specifically Sobol/Homma–Saltelli total-effect indices, to rank candidate regressors. The proposed GSA-based ranking is combined with a bottom-up testing sequence and benchmarked against the Hoover–Perez (HP) general-to-specific algorithm on HP’s Monte Carlo “experimental designs” (11 data-generating processes) built from fixed US macroeconomic series with AR(1) errors and lagged dependent variables. Performance is evaluated via Monte Carlo recovery rates of the true DGP and an “effective DGP” (EDGP) that removes weak regressors, plus potency/gauge metrics. Across HP designs, the full GSA-based algorithm materially reduces the failure rate in recovering the EDGP (reported roughly from ~5% to ~1% under tuned settings), while it does not improve recovery of exact DGPs when regressors are too weak. The authors also replicate and correct an issue in the original HP MATLAB simulation code (AR vs MA generation), documenting large effects on benchmark results.","HP designs generate data via $y_t=\sum_{i=1}^{k}\beta_i^* x_{it}^*+u_t$ with $u_t=\rho u_{t-1}+\varepsilon_t$ and $\varepsilon_t\sim N(0,\sigma^2)$, which can be rewritten as a regression including current/lagged $x^*$ plus lagged $y_t$. GSA uses the total-effect index $S_{T i}=\frac{\mathbb{E}_{\gamma_{-i}}(\mathrm{Var}_{\gamma_i}(q\mid \gamma_{-i}))}{\mathrm{Var}(q)}=1-\frac{\mathrm{Var}_{\gamma_{-i}}(\mathbb{E}_{\gamma_i}(q\mid \gamma_{-i}))}{\mathrm{Var}(q)}$, with $q(\gamma)$ taken as BIC. A Monte Carlo estimator is given by $\hat\sigma^2_{T i}=\frac{1}{4N}\sum_{\ell=1}^N (q_{i\ell}-q_\ell)^2$ and $\hat V=\frac{1}{N-1}\sum_{\ell=1}^N (q_\ell-\bar q)^2$, so $\hat S_{T i}=\hat\sigma^2_{T i}/\hat V$.","Using HP’s 11 simulation DGPs (with $p=40$ candidate regressors and sample size $n=139$), the full GSA-based algorithm (with adaptive $\alpha$ and optional skipping) increases average EDGP recovery to about 98.9% versus about 94.3% for an optimized HP benchmark, corresponding to roughly a five-fold reduction in EDGP failure rate (~5.4% to ~1.1%). Large gains are concentrated in difficult designs (e.g., DGP 3 and 6A), while performance is similar on easier designs. For DGPs containing “weak” regressors (notably DGP 6 and 9), neither method recovers the exact true DGP in the reported runs, motivating evaluation against an EDGP defined via a parametricness index. The paper also reports that correcting HP’s original simulation code (AR(1) vs inadvertently MA(1) for $u_t$) substantially changes benchmark C1 match rates for some DGPs (e.g., DGP 2 and 7).","The authors explicitly limit scope to a first “experimental exploration” and intentionally abstain from implementing or comparing to alternative model-selection algorithms beyond the (optimized) HP benchmark. They note HP’s experimental designs represent only a small subset of possible time-series econometric situations (a “single planet” in a “galaxy”), so conclusions are conditional on this limited design set. They also acknowledge that when regressors are weak (low signal-to-noise), recovery of the exact DGP is not improved by GSA and performance should be judged against an EDGP instead. They mention future extensions (e.g., to information-criterion model selection and multi-model inference) are not analyzed due to space limitations.","Although termed “designs of experiments,” the work does not develop DOE methodology; the “designs” are simulation DGPs for benchmarking algorithms, so general DOE concepts (optimal run allocation, factorial/RSM/mixture design construction) are not addressed. Results hinge on a specific ranking-based search plus tuning (adaptive thresholds, skipping), and performance may be sensitive to correlation structures, nonstationarity, structural breaks, and error distributions beyond the Gaussian AR(1) settings considered. Comparisons are primarily against HP (and variants), omitting many modern high-dimensional or penalized approaches (e.g., Lasso/elastic net in time series, Bayesian model averaging with explicit priors, stability selection), which could change conclusions about relative benefit. The paper references MATLAB code availability for HP but does not provide a reproducible implementation of the new GSA algorithm and all experimental settings in a public repository, which may hinder replication.",The authors state that extensions of the GSA approach to make information-criterion-based methods and multi-model inference operational are left to future research. They also call for more research on using GSA methods in model selection beyond this initial exploration on HP’s designs.,"A natural next step is releasing a fully reproducible implementation (e.g., MATLAB/R/Python package) of the GSA-based algorithm with scripts to regenerate all tables/figures, including the corrected HP simulation engine. Methodologically, robustness studies under non-Gaussian innovations, conditional heteroskedasticity, autocorrelation misspecification, structural breaks, and different sample sizes would clarify when GSA rankings remain beneficial. Extending the approach to multivariate/time-varying parameter models (VARs, state space) and to high-dimensional settings with many candidate regressors could test scalability. Finally, integrating GSA-based rankings with modern regularization/Bayesian methods (e.g., using $S_{T i}$ as prior weights or screening before Lasso/stability selection) could yield more generally competitive procedures.",1401.5617v1,https://arxiv.org/pdf/1401.5617v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:30:20Z
TRUE,Factorial (full)|Factorial (fractional)|Screening,Screening|Parameter estimation|Cost reduction,Not applicable,"4 factors (Mach number M, magnetic field strength B0, initial velocity spectrum shape kv, temperature T), each at 2 levels; plus consideration of interactions up to 4-way in the full factorial",Other,Simulation study|Other,TRUE,None / Not applicable|Other,Not provided,http://enzo-project.org|http://yt-project.org/|http://astropy.org,"The paper proposes using design of experiments (DOE) to rigorously compare synthetic PPV (position–position–velocity) molecular-cloud emission cubes from MHD simulations and to quantify which physical inputs significantly affect similarity metrics. It runs a 2-level full factorial design (16 runs) over four simulation factors—Mach number, magnetic-field strength, temperature, and initial velocity-spectrum shape—and analyzes main effects and interactions using linear models; it also contrasts this with one-factor-at-a-time (OFAT) studies and demonstrates a 2^(4−1) fractional factorial alternative (8 runs). Three distance/similarity responses are defined for comparing cubes: a PCA-eigenvalue distance, a spectral correlation function (SCF) surface distance, and a multivariate Cramér two-sample statistic computed on transformed cube data. Using Lenth’s method for unreplicated factorial experiments, they find all three metrics are strongly sensitive to Mach number and temperature, while none detect magnetic field strength as a main effect; some interactions (notably Mach×Temperature and Mach×Magnetic-field) appear depending on the metric. Practically, the work argues that factorial/fractional-factorial DOE provides more reliable inference than OFAT while reducing computational cost for simulation campaigns.","They define a scalar DOE response for simulation i by averaging a cube-distance metric over Nt=8 time outputs: $y_i=\frac{1}{N_t}\sum_{j=1}^{N_t} d[O_1(t_j),O_i(t_j)]$. The PCA distance uses covariance eigenvalues $\lambda$ from channel–channel covariance matrices and computes $d_{\rm PCA}(O_1,O_2)=\left[\frac{\sum_j(\lambda_{1j}-\lambda_{2j})^2}{(\sum_j|\lambda_{1j}|)(\sum_j|\lambda_{2j}|)}\right]^{1/2}$; SCF distance sums squared differences of SCF surfaces over spatial lags: $d_{\rm SCF}=\left(\sum_{\ell}[S_1(\ell)-S_2(\ell)]^2\right)^{1/2}$. DOE analysis fits the linear model $Y=X\beta+\varepsilon$ with factorial-coded columns and uses Lenth’s pseudo-standard error $\Psi$ to test significance of unreplicated effects via $\tau_i=\hat\beta_i/\Psi$.","In the 2^4 full factorial analysis, all three similarity metrics identify Mach number and temperature as significant main effects; magnetic field strength and initial velocity-spectrum shape are not significant main effects. Interaction effects differ by metric: Mach×Temperature is significant for PCA and Cramér but not for SCF; PCA and SCF show a negative Mach×Magnetic-field interaction even though magnetic field alone is not detected. OFAT would misleadingly suggest a magnetic-field effect (Table 2), illustrating confounding/interaction bias in OFAT. A 2^(4−1) fractional factorial (8 runs) recovers qualitatively consistent conclusions with reduced significance and introduces aliasing of two-factor interactions (e.g., M:B aliased with k:T).","The authors note a major shortcoming that the explored parameter ranges are exaggerated (e.g., 100 µG fields on 10 pc scales are not observed), so quantitative realism is limited. They also acknowledge that their similarity statistics, as formulated, fail to detect magnetic-field effects as a main effect, indicating the statistics need adaptation or replacement to measure magnetism. They discuss that averaging responses over multiple late time steps could potentially wash out time-dependent effects, though they check time variation and find main effects largely stable.","DOE inference is based on unreplicated factorial runs and relies on Lenth’s method, which assumes most effects are negligible; if many effects were active or noise were non-constant, significance calls could be unreliable. The response definition compares each run to a single fiducial simulation, so effect estimates depend on that choice and may differ if a different baseline (e.g., center point) were used. The paper does not provide an explicit power analysis or guidance on how many replicates/seeds would be required to robustly estimate variance $\sigma^2$ for general simulation campaigns.","They state future work will extend the framework to compare simulations directly to observational data and to optimize simulation parameters to match observations, requiring similarity metrics that account for telescope effects (noise, resolution, varying dataset sizes). They also propose using the framework to benchmark/validate many other cloud-structure statistics from the literature and to develop or adapt statistics that are sensitive to magnetic fields. They mention extensions to more values/continuous parameter spaces, nonlinear response models, and efficient exploration of high-dimensional spaces.",A natural next step is to add replicated runs (different random seeds) at selected design points to directly estimate run-to-run variance and enable conventional ANOVA/t-tests alongside (or instead of) Lenth’s method. Incorporating additional DOE elements such as center points (to test curvature) and sequential/adaptive designs could support response-surface modeling and parameter optimization more efficiently than 2-level designs. Providing open-source implementations of the three distance metrics and DOE analysis (including design-matrix construction and aliasing diagnostics) would improve reproducibility and adoption in the astrophysics community.,1401.6251v1,https://arxiv.org/pdf/1401.6251v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:31:02Z
TRUE,Screening|Other,Screening|Parameter estimation|Other,Not applicable,Variable/General (t factors; s significant factors; N tests),Healthcare/medical|Other|Theoretical/simulation only,Exact distribution theory|Other,TRUE,None / Not applicable,Not provided,http://www.research.att.com/~njas/codes/Andw/index.html,"These lecture notes develop the information-theoretic and combinatorial foundations of designing screening experiments (DSE), i.e., non-adaptive group testing/pooling designs for identifying an unknown set of s significant factors among t candidates using N tests represented by a binary N×t incidence matrix. The paper formalizes several deterministic multi-access-channel models for test outcomes—disjunct (D-model/Boolean OR), symmetric disjunct with erasures (SD-model), and adder (A-model/sum)—and defines zero-error identifiability via distinct outcomes for all s-subsets. A central contribution is a detailed survey of bounds and constructions for superimposed (s-disjunct) and list-decoding superimposed codes, including Kautz–Singleton and Reed–Solomon–based constructions and concatenated designs, with explicit rate bounds and sufficient conditions for s-disjunctness (e.g., via weight/correlation). It further studies constrained group testing (Rényi search model) with bounded row weight k and provides optimal constructions achieving lower bounds on N for certain parameter regimes. For the adder channel, it introduces Bs-codes (distinct multiset sums of s columns) and surveys entropy and random-coding bounds on achievable rates; finally it treats probabilistic/symmetric DSE with universal decoding and derives random-coding error exponents.","Screening designs are encoded by a binary test matrix X∈{0,1}^{N×t} with xi(u)=1 indicating factor u is included in test i. Deterministic test-output models are defined by MAC functions: adder fA(x1,…,xs)=∑_{i=1}^s xi (output alphabet {0,…,s}); disjunct fD(x1,…,xs)=1{∑ xi>0} (Boolean OR); symmetric disjunct fSD outputs 0 if sum=0, 1 if sum=s (or n as parameterized), and * otherwise. A list-decoding superimposed (s,L,N)-code requires that the Boolean OR of any s columns covers at most L−1 other columns; a Bs-code requires all multiset sums ∑_{k=1}^s x(uk) (with 1≤u1≤…≤us≤t) be distinct.","For superimposed/list-decoding superimposed codes, the notes compile best-known upper/lower bounds on rates R(s,L) and on the disjunct-model design rate RD(s), including asymptotics such as R(s,1)≈(log e)/(2 s^2) and RD(s)≈2·(log e)/(s^2) for large s (as quoted), and an upper bound RD(s)≤R(s−1,2) with asymptotic 4 log s / s^2. Kautz–Singleton-type sufficient condition: a constant-weight code with column weight w and maximum pairwise correlation λ is s-disjunct if s≤⌊(w−1)/λ⌋, enabling practical verification in O(t^2) correlation checks rather than O(t^{s+1}). Reed–Solomon/concatenated constructions are provided with explicit parameter tables (N,w,λ,t) for s=2…8 demonstrating rates that can exceed certain random-coding lower bounds in the tabulated regimes. For the adder model, an entropy upper bound is given RA(s)≤Hs(1/2)/s (binomial entropy), and random-coding lower bounds yield R(s)≥\tilde H_s/(2s−1) with asymptotics about (log s)/(4s) (up to constants) for large s.",None stated.,"The work is largely theoretical (bounds/constructions) and does not address common applied issues in screening experiments such as test errors, dilution effects, or correlated/structured factors beyond the specific probabilistic symmetric-MAC model in Section 6. Practical guidance for choosing designs under resource constraints (e.g., laboratory pooling constraints, batch effects) is limited, and there is no accompanying implementation or software for constructing the proposed code families beyond mathematical recipes and parameter tables. Comparisons are primarily via asymptotic rates and existence/construction results rather than comprehensive empirical benchmarks across realistic operating conditions.","The notes explicitly flag open questions, e.g., improving the entropy bound for (s,N)-designs when s≥3, and existence questions for characteristic matrices (e.g., whether CHS(q,4)-matrices exist for q<13 and for k≥5 whether CHS(q,k) exists with q<k^2), as well as seeking nontrivial lower bounds in Rényi’s constrained model when the simple bound N≥⌈st/k⌉ is not tight.","Developing robust/noisy and dilution-aware versions of these non-adaptive pooling designs (with provable performance under false positives/negatives) and extending constructions to constrained laboratory settings (maximum pool size, limited overlap, multi-stage workflows) would improve applicability. Providing efficient, open-source construction/verification software (e.g., for RS/concatenated designs and decoding) and standardized benchmarks would enable wider adoption and fairer comparisons. Extending the theory to adaptive/sequential designs, high-dimensional regimes with unknown s, and modern sparse-recovery connections (compressive sensing) could unify and strengthen guarantees and practical performance.",1401.7505v1,https://arxiv.org/pdf/1401.7505v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:31:43Z
TRUE,Screening|Other,Screening|Parameter estimation|Cost reduction,Not applicable,Variable/General (population size t; defective set size |p|≤s; also complexes with subset size ≤ℓ; inhibitors ≤ι),Theoretical/simulation only|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops two nonstandard nonadaptive group-testing models aimed at “designing screening experiments” for identifying rare defective items with a single batch of tests: (i) search for defective supersets (“complexes”), where a test is positive iff the pool contains at least one entire defective subset, and (ii) search for defective items in the presence of inhibitors, where inhibitors can force a negative test even when defectives are present. For each model, the authors define the corresponding nonadaptive designs (superset (s,ℓ)-designs and inhibitory (s,ι)-designs) and connect them to superimposed-code constructions that enable simple decoding rather than exhaustive search. They express the test outcome vectors in terms of Boolean disjunction/conjunction operations on columns of an incidence matrix and prove lemmas showing that certain superimposed-code properties imply identifiability in the proposed models. A concatenated construction is provided: a q-ary separating (s,ℓ)-code (external code) combined with a binary superimposed (s,ℓ)-code (internal code), with Reed–Solomon (MDS) codes supplying large q-ary separating codes under stated parameter conditions. The paper also lists example small codes and tabulated achievable lengths for superimposed (2,2)-codes, framing the work as combinatorial design methodology for efficient screening under these generalized test-result rules.","Tests are represented by an N×t binary incidence matrix X with column x(u) indicating membership of item u in each test. In the standard disjunct model the outcome vector is the disjunction of defective columns: r(X,p)=\bigvee_{u\in p} x(u). In the defective-superset (complex) model the outcome is a disjunction over defective subsets of their columnwise conjunctions: r(X,\mathcal{P})=\bigvee_{P\in\mathcal{P}} \bigwedge_{u\in P} x(u). In the inhibitor model the outcome is “inhibition” of the defective disjunction by the inhibitor disjunction: r(X,p,I)=\bigvee_{u\in p} x(u) \;\backslash\; \bigvee_{u\in I} x(u), where (a\backslash b)_n=1 iff a_n=1 and b_n=0.","The main results are implication theorems: (1) any superimposed s-code implies a disjunct s-design (known result restated); (2) any superimposed (s,ℓ)-code implies a superset (s,ℓ)-design with a decoding procedure that avoids exhaustive search over complexes; and (3) any inhibitory (s,ι)-code (defined as a superimposed (s+ι)-code) implies an inhibitory (s,ι)-design with a constructive decoding notion of “ι-acceptable” items. Constructively, a concatenation lemma shows how to build a binary superimposed (s,ℓ)-code of size t(q) and length N(q)·N′ from a q-ary separating (s,ℓ)-code and a binary internal superimposed (s,ℓ)-code of size q. Using MDS/Reed–Solomon external codes, they state sufficient conditions (e.g., n≥sℓ(k−1)+1 and q^k≥s+ℓ) for separation and give resulting parameterized existence (Lemma 8). Example constructions include a superimposed 2-code of size t=12 with length N=9, and reported achievable upper bounds for superimposed (2,2)-codes (a table of t vs. N bounds).",None stated.,"The work is primarily combinatorial/existence and construction-focused and does not evaluate robustness to testing errors, noise, dilution effects, or dependence—assumptions that are often critical in practical group testing/screening experiments. The proposed generalized models (complexes and inhibitors) are analyzed in a worst-case identifiability setting but without empirical/simulation comparisons of required test counts versus alternative modern designs/decoders, and without guidance on parameter choice (s,ℓ,ι) under uncertainty. Practical implementation aspects (e.g., decoding complexity constants, scalability for large t, and handling unknown/estimated model parameters in Phase I) are not developed, and no software/code is provided.","The paper notes that some results are to be published in more detailed form elsewhere (e.g., a more detailed treatment referenced as [16] and an improved table of codes to be published), implying further elaboration and extension of constructions and tabulations.","Natural extensions include incorporating noisy tests (false positives/negatives) and deriving analogous error-correcting versions of superset and inhibitor designs with explicit tradeoffs in N versus error tolerance. Another direction is to develop and benchmark practical decoding algorithms (beyond the theoretical acceptability checks) and to compare test-count efficiency against modern sparse-recovery or probabilistic group-testing designs. Extending the framework to probabilistic (Bayesian) settings—where s,ℓ,ι are random or uncertain—could yield adaptive/sequential or prior-optimized nonadaptive designs. Finally, software implementations and searchable repositories of best-known small (s,ℓ) designs would improve practical uptake.",1401.7508v1,https://arxiv.org/pdf/1401.7508v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:32:10Z
TRUE,Computer experiment|Computer experiment|Other,Prediction|Cost reduction|Other,Not applicable,"Variable/General (examples include d=10, d=30, d=60, and d=70)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Personal website,http://www.mathworks.com/matlabcentral/fileexchange/4566,"The paper proposes using sparse grid experimental designs for computer experiments (deterministic simulators) so Gaussian process/kriging predictors can be computed exactly but much faster than with general space-filling designs. Under a separable covariance assumption, it derives algorithms that exploit the sparse-grid/lattice structure to compute kriging weights by inverting many small component covariance matrices rather than one large N×N matrix, and provides fast formulas for predictive variance. It also develops fast maximum likelihood estimation for unknown GP parameters by computing Σ^{-1}A for general matrices A and by giving a sparse-grid-based expression for log|Σ|, enabling rapid likelihood evaluation without forming/inverting the full correlation matrix. Empirical studies (Monte Carlo for Matérn random fields and benchmark deterministic test functions) show sparse grids are competitive with common space-filling designs for smooth functions and outperform lattice grids in high dimensions, while being worse for very rough functions. The main practical implication is that sparse grid designs can yield orders-of-magnitude computational savings for high-dimensional GP emulation with little loss in prediction accuracy in many smooth settings.","Sparse grid design is defined as $\mathcal X_{SG}(\eta)=\bigcup_{\mathbf j\in G(\eta)} \mathcal X_{1,j_1}\times\cdots\times \mathcal X_{d,j_d}$ with $G(\eta)=\{\mathbf j\in\mathbb N^d:\sum_i j_i=\eta\}$. The kriging predictor is $\hat y(x_0)=\mu(x_0)+\sigma(x_0)^\top w$ with $w=\Sigma^{-1}(y-\mu)$, and the paper’s Algorithm 1 computes $w$ via sums of Kronecker-structured inverses of small component matrices $S_{i,j}$ over sparse-grid index sets. Predictive variance is given without forming $\Sigma^{-1}$ as $\mathbb E_{Y=y}(\hat y(x_0)-Y(x_0))^2=C(x_0,x_0)-\sum_{\mathbf j\in J(\eta)}\prod_{i=1}^d \Delta_{i,j_i}(x_0)$, and Theorem 1 provides a fast expression for $\log|\Sigma|$ in terms of determinants of the component matrices $S_{i,j}$.","In a reported timing comparison with $d=10$ and sparse-grid level $\eta=14$, computing the kriging weights took 0.35 seconds using the proposed algorithm versus 40 seconds using direct inversion of the full covariance matrix. In a larger example with $N=467{,}321$ (reported as $\eta=73$, $d=70$), the proposed method computed weights in 14.7 seconds; extrapolating $O(N^3)$ scaling suggests direct inversion would take about 81 days. Monte Carlo RMSPE studies in $[0,1]^{10}$ with Matérn covariance show sparse grids outperform lattice grids and are similar to space-filling designs for smoother processes (higher differentiability), but are inferior when the process is very rough (e.g., low-$\nu$ Matérn). In deterministic-function tests (e.g., corner peak at $d=30$ and Rosenbrock at $d=60$), sparse-grid MLE-based GP predictors achieved lower median absolute prediction error than space-filling designs at comparable or larger design sizes, while also enabling much larger feasible N due to computational savings.","The paper notes that sparse grid designs appear inferior to space-filling designs when the underlying function/random field is very rough (e.g., essentially non-differentiable), based on simulation evidence. It also states the proposed approach relies on a separable covariance structure; when the covariance is not separable, the problem is not solved by the proposed designs and needs more work.","The core computational guarantees depend on nested component designs and strict separability across dimensions; in practice, many simulators exhibit anisotropy, nonstationarity, and cross-dimensional interactions that violate separability, potentially degrading both accuracy and computational advantages. The empirical comparisons largely use specific construction choices for the component designs (described as ad-hoc) and particular GP model forms (e.g., Matérn with shared lengthscale), so performance may be sensitive to these design/construction and modeling choices. The paper focuses on prediction and likelihood computation; it does not provide extensive guidance for constrained input regions, categorical factors, or sequential/adaptive sampling, which are common in practical computer experiments.","The paper suggests developing “optimal sparse grid designs” that better close performance gaps with space-filling designs, potentially using faster criteria such as integrated prediction error (via the predictive-variance shortcut) or maximum entropy (via the fast log-determinant expression). It also identifies as future work handling cases where the covariance function is not separable, mentioning possibilities like local separability and using local sparse grid designs for heterogeneous behavior.","A natural extension is to create sequential/adaptive sparse-grid augmentation rules (e.g., refinement based on posterior variance or error indicators) to better handle rough or locally complex functions while retaining fast Kronecker/sparse-grid computations. Another direction is robustifying to model misspecification by supporting nonseparable kernels through approximations that preserve partial Kronecker structure (e.g., sum-of-separable kernels, low-rank plus separable, or sparsified precision models). Packaging the methods in an open-source implementation (e.g., R/Python) with utilities for constructing nested component designs and fitting GP hyperparameters would improve reproducibility and adoption.",1402.6350v4,https://arxiv.org/pdf/1402.6350v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:32:54Z
TRUE,Optimal design|Other,Parameter estimation|Cost reduction|Other,D-optimal|Not applicable,Variable/General (examples include: block model with v=16 treatments; quadratic regression with 2 factors; nonlinear regression with time as 1 factor),Manufacturing (general)|Energy/utilities|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|MATLAB,Personal website,http://www.iam.fmph.uniba.sk/design/,"The paper introduces “resource constraints” as a unifying way to model many practical restrictions on exact experimental designs, formulated as linear inequalities on run counts (e.g., size, costs, direct bounds, marginal/strata limits, material availability, and spacing constraints). It proposes a tabu-search heuristic inspired by Detmax to compute efficient exact designs under arbitrary combinations of these resource constraints, with excursions via forward/backward single-run moves and a tabu list based on rounded criterion values. The method is demonstrated primarily for D-optimality in regression settings, producing D-efficient exact designs for (i) block designs (treatment-pair comparisons) under block/material limits, (ii) a two-factor quadratic model under marginal and cost constraints, and (iii) a nonlinear pharmacokinetic-style model (via local linearization) under simultaneous direct and cost constraints. Across examples, the heuristic typically matches or improves upon specialized competing methods (simulated annealing, IQP/MISOCP, and a prior heuristic), often achieving efficiencies exceeding 99.9% relative to approximate D-optimal designs within fixed time budgets. The work advances constrained optimal design practice by providing a general-purpose algorithm applicable beyond standard fixed-run-size settings where classic exchange algorithms are less suitable.","Resource constraints are modeled as linear inequalities on exact design counts: \(\sum_{i=1}^n a_{ri}\,\xi_i \le b_r\) for resources \(r=1,\dots,k\). For linear regression, the D-criterion is \(\phi_D(\xi) = \big[\det(\sum_{i=1}^n \xi_i f_i f_i^T)\big]^{1/m}\). The heuristic’s local evaluation uses residual resources \(r(\zeta)=b-A\zeta\), computes per-point augmentation limits \(d_i(\zeta)=\left\lfloor\min_{r:a_{ri}>0} r_r(\zeta)/a_{ri}\right\rfloor\), finds the largest feasible approximate step \(\gamma(\zeta)\), and sets \(\mathrm{val}(\zeta)=\phi(\zeta+\gamma(\zeta)d(\zeta))\) to guide tabu-search moves.","In the quadratic two-factor sintering example with marginal and cost constraints (B varied from 1100 to 3900), the algorithm produced exact designs with D-efficiency consistently above 99.99% relative to approximate D-optimal designs. In the nonlinear sampling-time example (start times s=0,…,167), Algorithm 1’s results were generally the same or better than the prior heuristic [48], with one noted difficult case at s=35 where 120s runs yielded 94.46% efficiency but longer runs (≈200–300s) found the optimum. For the block-design example (v=16, N from 15 to 120 under the standard size constraint), the method systematically matched or slightly exceeded a simulated annealing competitor and consistently found theoretically D-optimal designs for several N values where the optimum is known. The paper also reports stable performance across 10 random restarts in most instances, indicating robustness of the heuristic search path selection.","The authors note that resource constraints of the form \(A\xi\le b\) do not cover all reasonable restrictions, explicitly mentioning that some equality constraints on \(\xi_i\) values and limits on transitional costs cannot be represented. They also emphasize that for large instances there is “no practical hope” of rapidly producing provably optimal solutions, motivating the heuristic approach and implying no global-optimality guarantee.","Because the method is heuristic, performance and solution quality can depend on tuning parameters (e.g., tabu attribute rounding, backmax, time limit) and may vary for hard instances; a systematic sensitivity analysis is not fully developed. The empirical evaluation focuses largely on D-optimality and a small set of example problems, so evidence for other monotone criteria (A-, I-, G-optimality) and broader benchmarking against modern exchange/coordinate-exchange or metaheuristics is limited. Practical deployment would benefit from clearer guidance on selecting initial designs and stopping rules, and from complexity/runtime scaling studies as design space size and number of constraints increase.","The authors suggest multiple heuristic variants that could improve performance in specific situations, including alternative choices of initial designs, characteristic attributes, and local heuristic evaluations, as well as different definitions of “failed” excursions and variations in handling the tabu list. They also mention implementation-speed enhancements using update formulas (e.g., determinant/information-matrix updates) and note that the approach extends straightforwardly to other monotone criteria and to models beyond standard regression.","A natural extension is to add principled parameter auto-tuning (adaptive nround/backmax, dynamic tabu tenure) and stronger diversification/intensification strategies to reduce sensitivity to hard cases like s=35. Providing open-source, reproducible implementations (with test suites and benchmarks) and comparing against state-of-the-art exact design solvers (modern MIP/MISOCP, coordinate-exchange variants under constraints) would strengthen practical adoption. Methodologically, integrating approximate-design bounds (e.g., dual certificates) into the heuristic could yield anytime optimality gaps and improve reliability for practitioners under strict constraints.",1402.7263v2,https://arxiv.org/pdf/1402.7263v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:33:30Z
TRUE,Response surface|Optimal design,Parameter estimation,E-optimal,Variable/General (k ≥ 1 predictors),Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper derives E-optimal approximate designs for the second-order (quadratic) response surface regression model with k≥1 predictors. It proves that previously conjectured designs on the k-dimensional unit cube are truly E-optimal over the full class of approximate designs (not just a restricted subclass), resolving an open question from Galil and Kiefer (1977a). It also gives, for the first time, an explicit complete solution for E-optimal designs when the design region is the k-dimensional Euclidean unit ball. The main technical challenge—nondifferentiability of the E-criterion due to multiplicity of the minimum eigenvalue—is handled via a dual nonlinear Chebyshev approximation formulation and explicit construction of extremal polynomials satisfying the E-optimality equivalence theorem. The paper additionally discusses constructing E-optimal cube designs with a minimal number of support points (via barycenter sets) and compares with rotatable designs on the ball, showing E-optimal designs on the ball are not rotatable in general.","The model is the second-order response surface regression with regression vector $f(x)=(1,x_1^2,\ldots,x_k^2,x_1,\ldots,x_k,x_1x_2,\ldots,x_{k-1}x_k)^T$. E-optimality maximizes $\Phi_{-\infty}(\xi)=\lambda_{\min}(M(\xi))$ where $M(\xi)=\int f(x)f(x)^T\,d\xi(x)$. E-optimality is verified using an extremal polynomial $d(x,\xi)=\sum_i w_i(f(x)^T q_i)^2$ that must satisfy $\max_{x\in\mathcal X} d(x,\xi)\le \lambda_{\min}(M(\xi))$ with equality at support points; explicit forms are given for cube (Eq. 3.6) and ball (Eq. 4.10).","On the cube $[-1,1]^k$, E-optimal designs (for all k) can be chosen with symmetric information matrices having moments $a=c=2/5$ and $b=1/5$, giving $\lambda_{\min}=1/5$ with multiplicity $k(k+1)/2$; an explicit extremal polynomial is $d(x)=\tfrac15\bigl(1-\tfrac4k\sum_{i=1}^k x_i^2(1-x_i^2)\bigr)$. On the unit ball $\{\|x\|_2\le1\}$, a symmetric E-optimal design places masses $\xi(F_0)=\tfrac{k^2}{k^2+2k+2}$, $\xi(F_{k-1})=\tfrac{k}{k^2+2k+2}$, $\xi(F_k)=\tfrac{k+2}{k^2+2k+2}$, yielding $\lambda_{\min}=\tfrac{1}{k^2+2k+2}$ (multiplicity $k(k+1)/2$) and extremal polynomial $d(x)=\tfrac{1}{k^2+2k+2}\{1-\tfrac{2(k+1)}{k}\|x\|_2^2(1-\|x\|_2^2)\}$. The paper also characterizes minimally supported symmetric E-optimal cube designs via barycenter depth sets and gives tables for k≤24.",None stated.,"The results are for approximate (continuous) design theory; converting to exact run sizes requires rounding and may lose exact optimality for small N. The work focuses on homoscedastic, independent normal-error linear regression and does not study robustness to model misspecification, nonnormality, heteroscedasticity, or correlated errors. Practical implementation guidance (software, construction algorithms for arbitrary k beyond the closed forms/sets) is limited, especially for choosing minimally supported designs for large k.",None stated.,"Develop computational tools/packages to generate the proposed E-optimal designs (including minimally supported cube designs) for arbitrary k and finite-run exact designs. Study robustness and extensions to correlated/heteroscedastic errors and to model misspecification (e.g., higher-order terms or active subset of predictors). Extend analogous explicit E-optimal design characterizations to other design regions (simplex/mixture constraints) and to constrained or multivariate responses.",1403.3805v2,https://arxiv.org/pdf/1403.3805v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:34:03Z
TRUE,Factorial (fractional)|Factorial (full)|Screening|Optimal design|Bayesian design|Sequential/adaptive,Screening|Model discrimination|Cost reduction,Other,Variable/General (examples: k=8 factors injection molding; k=5 factors reactor),Manufacturing (general),Simulation study|Case study (real dataset),TRUE,R|Fortran,Not provided,http://cran.r-project.org/web/packages/BsMD/vignettes/BsMD.pdf|http://lib.stat.cmu.edu/,"The paper develops an objective Bayesian methodology for choosing follow-up runs after an initial screening experiment yields ambiguous conclusions about active factors. It frames follow-up design selection as a Bayesian model discrimination problem over factorial regression models (main effects plus interactions up to a specified order), and proposes an objective model discrimination (OMD) criterion that is a posterior-weighted average of Kullback–Leibler divergences between posterior predictive distributions across all pairs of models. For model selection it uses multiplicity-adjusted priors on model space and the Bayarri et al. (2012) “robust” hierarchical g-prior for regression coefficients, while using a reference prior for prediction to obtain a closed-form OMD expression for fast design search. The method is applied to real screening/fractional factorial examples (injection molding and a reactor experiment), where the selected follow-up runs concentrate on previously untried factorial points and lead to substantially more concentrated posteriors over models/factor activity compared with prior conventional Bayesian approaches using tuned weakly-informative priors. The work advances follow-up design in screening by providing a principled, tuning-free Bayesian criterion and demonstrating improved ambiguity resolution in practical fractional factorial settings.","Models are normal linear regressions for factorial effects: $y\mid\beta_0,\beta_i,\sigma, M_i \sim N_n(X_0\beta_0+X_i\beta_i,\sigma^2 I_n)$. Follow-up designs are chosen by maximizing a model discrimination score $MD=\sum_{i\neq j} \Pr(M_i\mid y)\Pr(M_j\mid y)\,KL\big(m(\cdot\mid y,M_i),m(\cdot\mid y,M_j)\big)$ where $KL(f,g)=\int f\log(f/g)$. With a reference prior for prediction, the closed-form objective criterion (OMD) becomes $\sum_{i\neq j}\Pr(M_i\mid y)\Pr(M_j\mid y)\tfrac12\{\mathrm{tr}(V_j^{*-1}V_i^*)+\frac{n-t_i-t_0}{SSE_i}(\hat y_i^*-\hat y_j^*)'V_j^{*-1}(\hat y_i^*-\hat y_j^*)-n^*\}$ (log-determinant terms cancel in the symmetric sum).","In the injection molding example (originally 8 factors in a $2^{8-4}$ resolution IV fractional factorial, collapsed to 4 factors with a replicated $2^{4-1}$ design), the best OMD follow-up designs (for $n^*=4$) predominantly select runs from the untried half of the $2^4$ factorial (runs 9–16), indicating added discrimination from new design points; top OMD designs differ from CMD depending on whether 2FIs or 3FIs are allowed. In the reactor example (5 factors, starting from an 8-run $2^{5-2}$ resolution III fraction), OMD-selected 4-run follow-ups lead—when combined with screening runs and including a block effect—to strong posterior concentration on the model with factors $B,D,E$ (objective posterior probability 0.86) and high posterior inclusion probabilities (approximately $B\approx0.98$, $D\approx0.93$, $E\approx0.87$). Posterior heterogeneity over models (normalized Shannon index) drops from 0.74 to 0.21 under the objective approach after adding follow-up runs (a 71% reduction), compared with 0.79 to 0.32 (59% reduction) under the conventional tuned-prior approach.","The authors note that their objective Bayes approach requires the design matrix to be of full column rank, which can restrict the set of entertainable models (for a given interaction order) relative to “all possible” models; they suggest this can often be mitigated by omitting higher-order interactions or context variables such as blocking. They also acknowledge that using different priors for model selection (robust hierarchical g-prior) and for prediction (reference prior) means their discrimination criterion no longer has the exact theoretical properties of Box & Hill (1967), though they argue it holds approximately for moderate-to-large $n$.","The computational burden can still be substantial because OMD requires evaluating pairwise divergences across many candidate models (up to $2^k$) and searching over many candidate follow-up run combinations; the paper does not fully benchmark scalability for larger $k$ or larger candidate sets. The approach is tailored to Gaussian linear models with independent errors and categorical factorial factors; robustness to non-normality, heteroscedasticity, or autocorrelation (common in process data) is not explored. Practical guidance for choosing the interaction order (2FI vs 3FI) and for handling strong aliasing/common fractional-factorial complications beyond the full-rank constraint is limited, and software availability is not packaged for easy reuse.","The authors explicitly mention several possible extensions: (i) adopting a different prior on model space such as $(a=1,b=k+1)$ to encourage stronger sparsity (noting follow-up runs were broadly similar in their experiments); (ii) relaxing the “effect forcing” assumption by incorporating structural principles like effect sparsity, hierarchy, and heredity as in Bingham & Chipman (2007) and Wolters & Bingham (2011); and (iii) using alternative divergence measures (e.g., Hellinger distance) in place of KL divergence, noting that Hellinger has computational and interpretability advantages and admits closed forms in this setting.","Develop self-starting/Phase-I-to-Phase-II workflows where initial model/variance components and blocking effects are unknown and must be estimated while selecting follow-up runs, with performance assessed under parameter uncertainty. Extend the criterion to non-Gaussian responses and generalized linear models common in screening (binary defects, counts), and to correlated-error settings (time-ordered runs) where predictive distributions change. Provide open-source, reproducible implementations (e.g., an R package) with heuristics for large $k$ (stochastic search/greedy/sequential design) and systematic benchmarks against modern Bayesian variable-selection/design methods (e.g., spike-and-slab, horseshoe, Bayesian D-optimal or mutual-information designs).",1405.2818v1,https://arxiv.org/pdf/1405.2818v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:34:57Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Optimization|Prediction|Model discrimination|Cost reduction|Other,Bayesian D-optimal|Not applicable|Other,"Variable/General (supports continuous, discrete, categorical; includes high-dimensional settings)",Theoretical/simulation only,Simulation study|Other,TRUE,C/C++|Python|MATLAB|Other,Public repository (GitHub/GitLab),https://bitbucket.org/rmcantin/bayesopt/,"The paper introduces BayesOpt, a C++ library implementing state-of-the-art Bayesian optimization methods applicable to nonlinear optimization, stochastic multi-armed bandits, and sequential experimental design. The underlying methodology uses surrogate models such as Gaussian/Student-t/mixture processes with an acquisition function to choose the next evaluation point, updating the posterior sequentially to achieve sample-efficient optimization of expensive black-box functions. It emphasizes computational efficiency (e.g., incremental Cholesky updates for kernel matrix factorization and precomputation of acquisition-function terms independent of the query point) and flexibility via modular “factory-like” components (kernels, criteria, hyperpriors, and meta-criteria such as GP-Hedge). Performance is empirically benchmarked on standard test functions (e.g., Branin, Camelback, Hartmann) against SMAC, HyperOpt, Spearmint, and DiceOptim, reporting optimization gap and CPU time under different settings for kernel hyperparameter learning. The library provides multi-language interfaces (C, C++, Python, Matlab/Octave) and relies internally on NLopt for inner optimization routines.","The surrogate model is given as $f(x)=\phi(x)^\top w + \epsilon(x)$ with a nonparametric-process error term $\epsilon(x)\sim \mathrm{NP}(0,\sigma_s^2(K(\theta)+\sigma_n^2 I))$, where $K(\theta)$ is the kernel/covariance matrix parameterized by $\theta$. Bayesian optimization proceeds by iteratively updating the posterior $P(f\mid D)\propto P(D\mid f)P(f)$ and choosing the next point by maximizing an acquisition function: $x_i = \arg\max_x C\big(x\mid P(f\mid D)\big)$, then augmenting the dataset $D\leftarrow D\cup\{x_i,y_i\}$ with $y_i=f(x_i)$.","The paper reports benchmark results (mean and standard deviation over 10 runs) comparing BayesOpt configurations to other open-source Bayesian optimization tools on Branin (2D), Camelback (2D), and Hartmann (6D). BayesOpt1 achieves very low optimization gaps with substantially lower CPU time than several competitors in some settings (e.g., Branin at 200 samples shows gap 0.00000 with time about 8.6s; Camelback at 100 samples shows gap 0.00000 with time about 2.2s). A second configuration (BayesOpt2, using MCMC for hyperparameters) can also reach near-zero gaps but with much higher time (e.g., Branin 200 samples ~1802.7s; Hartmann 200 samples ~4093.3s), illustrating the computational tradeoff between MAP/ML learning and MCMC-based learning of kernel parameters.",None stated.,"As a library/technical report, the evaluation is limited to a small set of synthetic benchmark functions; results may not generalize to noisy, constrained, or high-dimensional real-world objectives where wall-clock cost is dominated by function evaluations rather than BO overhead. The paper does not provide a systematic study of robustness to model misspecification (e.g., nonstationarity, heteroscedasticity beyond the stated form), nor guidance on selecting acquisition functions/hyperpriors across problem classes. Comparisons across libraries can be sensitive to implementation details and tuning (including initial design size, optimization of the acquisition, and hyperparameter learning schedules), which are not exhaustively standardized beyond the brief configuration notes.",None stated.,"Provide a reproducible benchmarking suite and standardized configurations (including acquisition-optimizer settings and hyperparameter learning schedules) to enable fairer cross-library comparisons. Extend the library with stronger support for constraints and batch/parallel Bayesian optimization (e.g., q-EI or local penalization) and for nonstationary or structured kernels tailored to mixed continuous/categorical spaces. Add self-contained examples and packaged bindings (e.g., a pip/conda distribution) plus built-in diagnostics (posterior checks, sensitivity to priors) to improve practical adoption in experimental design workflows.",1405.7430v1,https://arxiv.org/pdf/1405.7430v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:35:28Z
FALSE,NA,NA,Not applicable,"Variable/General (stochastic dimension M; examples use M=10,20,30; polynomial order p up to 5; KLE truncation L)",Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/dolgov/tamen|https://github.com/oseledets/TT-Toolbox|http://github.com/ezander/sglib,"The paper develops a tensor-train (TT) low-rank framework to compute and store full Polynomial Chaos Expansion (PCE) coefficient tensors for random fields and to solve a stochastic elliptic diffusion PDE via stochastic Galerkin discretization. It contrasts sparse vs. full multi-index polynomial sets, arguing that full sets can be more accurate/flexible but are normally intractable; TT compression mitigates the curse of dimensionality. A key methodological contribution is a block TT-cross interpolation algorithm that reconstructs the PCE coefficient tensor from relatively few sampled entries, followed by TT-based assembly of the stochastic Galerkin stiffness operator and solution of the resulting large linear system using alternating minimal energy (AMEn). The approach enables post-processing (mean, variance/covariance, Sobol indices, level sets) directly in TT format. Numerical experiments for a 2D elliptic PDE with a Beta-distributed permeability field show that, while TT-based PCE construction can be slower than sparse methods for low orders, TT assembly of the Galerkin operator is dramatically faster and stable with respect to dimension/order, and TT can achieve comparable or better covariance accuracy for higher polynomial orders.","The random coefficient is expanded in Hermite PCE as $\kappa(x,\omega)\approx\sum_{\alpha\in\mathcal{J}}\kappa_\alpha(x)H_\alpha(\theta(\omega))$ with either a full index set $\mathcal{J}_{M,\mathbf{p}}=\{0..p_1\}\otimes\cdots\otimes\{0..p_M\}$ or a sparse set $\mathcal{J}^{sp}_{M,p}=\{\alpha:\sum_m \alpha_m\le p\}$. The stochastic Galerkin operator is assembled via Hermite triple products $\Delta_{\alpha,\beta,\nu}=\prod_m \int h_{\alpha_m}(z)h_{\beta_m}(z)h_{\nu_m}(z)\,dz$ and $K=K_0\otimes\Delta_0+\sum_{\ell=1}^L K_\ell\otimes\sum_{\nu\in\mathcal{J}}\Delta_\nu\,\tilde\kappa_\nu(\ell)$, but computed efficiently by exploiting TT structure and separability. The core algorithm reconstructs the high-dimensional tensor of reduced PCE coefficients $\tilde\kappa_\alpha(\ell)$ from sampled evaluations (based on a projected KLE/PCE formula) using block TT-cross, producing a TT representation $\tilde\kappa_\alpha(\ell)=\kappa^{(1)}_{\ell,s_1}(\alpha_1)\cdots\kappa^{(M)}_{s_{M-1}}(\alpha_M)$.","In the permeability-field experiments (MATLAB, TT-cross threshold $\varepsilon=10^{-4}$), TT-based PCE assembly is 10–50× slower than sparse evaluation for low orders, but scales roughly linearly in polynomial order while sparse costs grow sharply with order/dimension (e.g., for $p=3$, TT PCE assembly times reported are ~8.81 s (M=10), ~228.9 s (M=20), ~2422.9 s (M=30)). For Galerkin operator assembly, TT times remain ~0.11–1.18 s across $p=1..5$ and $M=10..30$, while sparse assembly becomes infeasible (e.g., at $p=3,M=20$ sparse ~735 s; at higher settings not run). In solving the stochastic linear system, TT becomes competitive for larger $p$; for example at $p=5,M=10$ sparse ~61.6 s vs TT ~45.5 s, though TT can be much slower at high $M$ (e.g., $p=5,M=30$ ~5362.8 s). Covariance errors relative to a TT reference show full-set TT can be more accurate than sparse at the same $p$ (e.g., at $M=10,p=2$: sparse $3.46\times10^{-3}$ vs TT $1.00\times10^{-4}$; at $M=20,p=1$: sparse $8.86\times10^{-2}$ vs TT $2.80\times10^{-2}$).","The authors note that the TT approach is not uniformly superior: for low polynomial order $p$ the sparse-set approach can be ""incredibly fast"" because TT methods involve expensive SVDs and cross iterations, making the comparison ""not so obvious."" They also report that computational time can grow rapidly with stochastic dimension due to slow decay of KL eigenvalues (here driven by small correlation length), which increases TT ranks substantially (e.g., ranks growing from ~70 at $M=10$ to ~200 at $M=30$). They further remark that characteristic/level-set style post-processing can cause rapid TT-rank growth, making some quantities challenging.","The numerical validation is centered on a single PDE type (elliptic diffusion) and one main random-field setup (Beta-shifted field with a specific covariance), so it is unclear how robust the performance claims are across different distributions, stronger nonlinearity, non-elliptic PDEs, or non-Gaussian measures requiring different polynomial families. The method relies heavily on independence/orthogonality structure (Hermite products, tensor-product index sets) and on low TT ranks; cases with strong variable coupling, anisotropy, or non-smooth response surfaces may yield prohibitive ranks and reduce benefits. Comparisons are primarily against a specific sparse PCE toolkit (sglib) and do not benchmark against other modern sparse/low-rank UQ strategies (adaptive sparse PCE, compressive sensing PCE, multi-element PCE, low-rank PGD) under matched accuracy/cost. Reproducibility may be limited because key algorithmic parameters (cross stopping criteria details, rank caps, solver tolerances) and complete scripts are not provided in the paper.","They explicitly ask whether the overall solution scheme can be endowed with more structure to obtain a more efficient algorithm, and whether there are better stochastic-field discretizations than the KLE–PCE approach. They also note that only basic statistics (mean/variance) were tested and pose as future work determining which additional quantities from their post-processing list (e.g., level sets, exceedance/frequency) are feasible in tensor formats and how to compute them efficiently. They mention potentially using QTT approximation to compress the response surface representation over $\theta$ further (logarithmic in degrees of freedom).","Develop self-adaptive anisotropic polynomial orders and variable ordering strategies (guided by estimated Sobol indices) to control TT ranks and reduce cross-sampling cost, especially for high $M$ with slow KL decay. Extend the approach to correlated/non-Gaussian inputs and non-Hermite polynomial bases with rigorous error control (including unknown input parameters and Phase-I-style calibration from data). Provide comprehensive benchmarks and open-source reproducible workflows (scripts, parameter files) comparing TT-PCE/SG to non-intrusive surrogates (sparse regression PCE, Gaussian processes, deep surrogates) under fixed accuracy targets. Investigate robustness to model misspecification (e.g., spatial discretization error, covariance misestimation) and devise hybrid sparse+TT or multi-level strategies to balance SVD cost at low orders with TT advantages at high orders.",1406.2816v1,https://arxiv.org/pdf/1406.2816v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:36:16Z
TRUE,Factorial (full)|Response surface,Parameter estimation|Prediction|Optimization|Cost reduction,Not applicable,"2 factors (crop spacing and fertilizer), each at 4 levels",Food/agriculture,Other,TRUE,R,Not provided,http://stat-athens.aueb.gr/~karlis/lefkada/boot.pdf|http://faostat.fao.org|http://www.Foreignpolicyblogs.Com/2011/04/12/Cassava|http://www.ats.ucla.edu/stat/mult_pkg/faq/general/citingats.htm|www.scholar.princeton.edu/tsearch/page|www.statease.com/rsm_simplified.html,"The paper applies design of experiments concepts and response surface methodology (RSM) to model cassava yield as a function of two controllable factors: crop spacing and inorganic fertilizer application. It describes desirable DOE properties (orthogonality, rotatability, uniform precision) and discusses first-order (factorial) and second-order (central composite design, CCD) response surface designs as standard approaches for fitting polynomial surfaces, although the analyzed dataset is stated to be a two-factor factorial experiment at four levels. The main modeling contribution is fitting an intrinsically nonlinear inverse polynomial model (IPM) to yield and cassava dimension responses using Gauss–Newton nonlinear least squares, with bootstrap resampling used to assess estimator stability and confidence intervals. Residual diagnostics (including Shapiro–Wilk) are used to argue model adequacy, and bootstrap intervals are reported to closely match observed-sample intervals, suggesting negligible bias. The paper concludes crop spacing is strongly significant and that the fitted nonlinear model provides an adequate surface for predicting cassava yield within the studied region.","The general response surface model is written as $y=f(x)\beta+\varepsilon$ and in linearized matrix form as $y=X\beta+\varepsilon$, with prediction variance $\mathrm{Var}[\hat y(x)]=\sigma^2 f(x)^{\top}(X^{\top}X)^{-1}f(x)$. The proposed nonlinear mean function is an inverse polynomial model (IPM); for two factors a second-order form is given (up to second-order terms) as $y^{-1}=\beta_{00}+\beta_{10}x_1+\beta_{01}x_2+\beta_{11}x_1x_2+\beta_{20}x_1^2+\beta_{02}x_2^2$ (presented in the paper’s inverse-parameterization form). Parameters are estimated by minimizing $S(\theta)=\sum_{i=1}^n\{y_i-f(x_i,\theta)\}^2$ via Gauss–Newton, yielding the update/estimator in linearized form $\hat\beta=(Z_0^{\top}Z_0)^{-1}Z_0^{\top}(Y-f^0)$.","The R nonlinear least-squares iterations reported for the IPM show, for Yield: 6 parameters, 10 iterations, SSE=2.25, convergence tolerance $7.47\times10^{-7}$, and Pr>F=0.001; for Dimension: 6 parameters, 12 iterations, SSE=349.2, tolerance $5.62\times10^{-7}$, Pr>F=0.001. Shapiro–Wilk tests on standardized residuals are reported as W=0.9386 (Yield) and W=0.9767 (Dim), interpreted as not violating normality. Bootstrap parameter estimates and 95% CIs for Yield are close to observed-sample estimates and CIs (e.g., $\beta_{11}\approx0.356$ bootstrap vs 0.349 observed; $\beta_{01}\approx-0.0092$ vs -0.0084; $\beta_{10}\approx-0.220$ vs -0.216), supporting the authors’ claim of negligible bias and estimator invariance. The discussion states crop spacing is strongly significant and that reusing estimated parameters as starting values reproduces the same estimates with unchanged SSE, suggesting convergence to a global minimum.",None stated.,"Although CCD and rotatability/orthogonality are discussed, the actual data analyzed are described as a two-factor, four-level factorial design; the paper does not clearly specify the run structure, replication, randomization, or blocking, making it hard to assess design validity and pure-error estimation. Optimization claims are qualitative; the work does not report an explicit estimated optimum (factor settings) or uncertainty on the optimum, which is typically central to RSM studies. The modeling and inference rely on independence/normality assumptions and appear to ignore potential field-trial issues (spatial correlation, heteroscedasticity, and nuisance factors such as soil gradients), which can materially affect conclusions in agricultural experiments.",None stated.,"Report the exact experimental run matrix (levels, coding, replication, and randomization) and consider adding center points/axial points if a true CCD/RSM workflow is intended. Extend the analysis to explicitly estimate and validate optimal factor settings (including confidence regions for the optimum) and to compare IPM against standard second-order polynomial RSM models using cross-validation or information criteria. Incorporate field-trial structure (blocking/spatial models) and robustness checks for nonconstant variance and outliers, and provide reproducible R code (or an R script/package) to enable replication.",1408.0251v2,https://arxiv.org/pdf/1408.0251v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:36:54Z
TRUE,Optimal design|Other,Parameter estimation|Cost reduction|Other,D-optimal,"Variable/General (design space size n; model dimension m; numerical example uses m=6 on a 101×101 grid, n=10201)",Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Personal website,http://www.iam.fmph.uniba.sk/design/,"The paper studies approximate D-optimal design on a finite design space when trials have unequal per-point costs and the experiment is constrained both by total number of trials (size) and by total cost. It formulates the constrained optimization for D-optimality, shows the inequality-constrained problem can be reduced to an equality-constrained version, and derives an equivalence theorem characterizing optimality under simultaneous size and cost constraints. The authors provide rules to delete design points that cannot appear in any optimal support (redundant points), enabling large-scale computations. They propose a simple multiplicative “barycentric” algorithm with monotonic convergence under mild conditions, and demonstrate its computational behavior on a quadratic regression grid example and a Monte Carlo study of random instances. A MATLAB implementation is provided online.","Design weights $w_x\ge0$ satisfy size and cost constraints: $\sum_x w_x\le 1$ and $\sum_x c_x w_x\le 1$ (reduced to equalities $\sum_x w_x=1$, $\sum_x c_x w_x=1$). The D-criterion is $\phi(w)=\det(M(w))^{1/m}$ with information matrix $M(w)=\sum_x w_x f(x)f(x)^\top$. The barycentric multiplicative update is $w^{(t+1)}=T_B(w^{(t)})=w^{(t)}\odot d^\pi(w^{(t)})$, where for $x_0\in X_0$, $d^\pi_{x_0}(w)=d_{x_0}(w)/m$ and for $x_\pm$ it averages weighted variances $\tilde d_{x^+x^-}(w)=(\delta_{x^+}d_{x^-}(w)+\delta_{x^-}d_{x^+}(w))/(\delta_{x^+}+\delta_{x^-})$ with $d_x(w)=f(x)^\top M(w)^{-1}f(x)$.","Theorem 1 gives a necessary-and-sufficient equivalence theorem for D-optimality under simultaneous size-and-cost constraints via bounds on $d_x(w)$ over the partitions $X_+,X_-,X_0$. Theorem 2 provides a computable efficiency lower bound $\mathrm{eff}(w|w^*)\ge m/(m+\epsilon)$ and deletion rules to remove redundant design points using a threshold function $h_m(\epsilon)$. Theorem 3 shows monotone convergence of the barycentric algorithm’s D-criterion to the optimum under a mild condition on $S(w(t))$. Numerically, in a quadratic regression on a 101×101 grid (n=10201, m=6) with costs $c_x=0.1+6r_1(x)+r_2(x)$, periodic deletion of redundant points substantially accelerates computation; a Monte Carlo study (1000 runs per setting) indicates deletion can reduce computation time by about an order of magnitude.","Convergence of the barycentric algorithm is proved under a mild technical condition $\liminf_{t\to\infty} S(w(t))>0$, which may fail when there are design points with exactly unit normalized cost ($X_0\neq\varnothing$); the authors note this case is expected to be rare and suggest perturbing such costs or using a lemma-based check. They also state that general-purpose SDP/SOCP methods become impractical in memory/time for large $n$ and that their MATLAB+SDP solver setup could only handle roughly $n<4000$ in their experiments.","The work is restricted to approximate (continuous) designs; translating results to exact integer-run designs under both constraints is discussed only via heuristics and is not solved algorithmically here. The core development assumes a finite design space; extensions to continuous design regions would require discretization or additional theory. The numerical evaluation focuses on a specific regression example and synthetic random regressors/costs; performance on structured real experimental settings (e.g., correlated regressors, model misspecification, heteroscedasticity) is not assessed.","The authors suggest that the fastest practical solvers may hybridize methods (e.g., use the barycentric algorithm with deletion to shrink the design space, then switch to SDP or SOCP, or combine barycentric with vertex-direction methods). They also note that while many ideas extend beyond D-optimality (e.g., to A-optimality), a barycentric algorithm for A-optimality has not been studied and would likely require new development plus adapted deletion rules.","Develop self-starting/robust versions that handle unknown/estimated model parameters (especially for nonlinear/local D-optimality) and quantify sensitivity to misspecification under cost constraints. Provide a principled rounding or mixed-integer optimization pipeline with guarantees for exact (integer-run) designs under simultaneous size and cost constraints. Expand empirical validation on real constrained experimentation problems (e.g., time-varying sampling costs, dose-finding) and release a reproducible open-source implementation beyond MATLAB (e.g., R/Python) with benchmarks.",1408.2698v1,https://arxiv.org/pdf/1408.2698v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:37:32Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal|A-optimal|Bayesian D-optimal|Bayesian A-optimal,Variable/General,Theoretical/simulation only|Environmental monitoring|Other,Exact distribution theory|Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper develops Bayesian A- and D-optimal experimental design criteria for Bayesian linear inverse problems posed in infinite-dimensional separable Hilbert spaces with Gaussian priors and additive Gaussian observation noise. It extends Bayesian D-optimality by defining the design objective as maximizing expected information gain (expected KL divergence from posterior to prior) and derives a rigorous infinite-dimensional expression based on the Fredholm determinant of an operator involving the prior-preconditioned misfit Hessian. It also studies Bayesian A-optimality and proves in the infinite-dimensional setting that minimizing the trace of the posterior covariance operator is equivalent to minimizing the Bayes risk (expected mean-square error) of the MAP estimator. The work is primarily theoretical/analytic, providing operator-theoretic formulas that justify standard finite-dimensional criteria when generalized to function-space inverse problems (e.g., PDE-governed parameters). It positions these results as a foundation for optimal sensor/measurement placement and related OED problems in infinite-dimensional Bayesian inverse problems.","Bayesian D-optimality is formulated as maximizing expected information gain: \(\mathbb{E}_{u\sim\mu_{pr}}\,\mathbb{E}_{y|u}[D_{kl}(\mu^{y}_{post}\|\mu_{pr})]=\tfrac12\log\det(I+\tilde H_m)\), where \(\tilde H_m=C_{pr}^{1/2}H_m C_{pr}^{1/2}\) and \(H_m=G^*G\). The KL divergence admits the infinite-dimensional form \(D_{kl}(\mu^{y}_{post}\|\mu_{pr})=\tfrac12\{\log\det(I+\tilde H_m)-\mathrm{tr}(H_m C_{post})+\langle u^{y}_{post},u^{y}_{post}\rangle_{C_{pr}^{-1}}\}\) with \(C_{post}=(H_m+C_{pr}^{-1})^{-1}\). Bayesian A-optimality corresponds to minimizing \(\mathrm{tr}(C_{post}(\xi))\), and the paper proves \(\mathbb{E}_{u\sim\mu_{pr}}\mathrm{MSE}(u^{y}_{post};u)=\mathrm{tr}(C_{post})\) (Bayes risk of MAP equals posterior covariance trace).","The expected information gain for the Gaussian linear inverse problem in Hilbert space is shown exactly to be \(\frac12\log\det(I+\tilde H_m)\), providing a mathematically rigorous infinite-dimensional analogue of Bayesian D-optimality using the Fredholm determinant. An explicit operator formula for \(D_{kl}(\mu^{y}_{post}\|\mu_{pr})\) is derived that avoids ill-defined finite-dimensional terms (e.g., explicit dependence on dimension and determinants of trace-class covariances). The Bayes risk (prior-averaged MSE) of the MAP estimator is proved to equal \(\mathrm{tr}(C_{post})\) in infinite dimensions, establishing the theoretical justification for A-optimality as MSE minimization. The paper notes that in many ill-posed inverse problems \(\tilde H_m\) is effectively low-rank, yielding the approximation \(\log\det(I+\tilde H_m)\approx\sum_{i=1}^r\log(1+\lambda_i)\) for dominant eigenvalues \(\lambda_i\).",None stated.,"Results are limited to Gaussian linear inverse problems (linear parameter-to-observable map and Gaussian prior/noise); extensions to nonlinear forward models, non-Gaussian priors (e.g., sparsity-promoting), or non-Gaussian noise are not developed. The paper suppresses the explicit dependence of the design vector \(\xi\) in most derivations and does not provide concrete algorithmic procedures or computational demonstrations for optimizing \(\xi\) under practical constraints (e.g., discrete sensor placement, costs, or feasibility). It largely assumes idealized observation models (often taking \(\Gamma_{noise}=I\)) and does not analyze robustness to model misspecification, correlated/heteroscedastic noise in detail, or finite-sample Phase I estimation of hyperparameters. No empirical case study is included to validate the operator criteria against real design tasks.","The paper suggests that many inverse problems yield a low-rank structure in the prior-preconditioned misfit Hessian \(\tilde H_m\), enabling efficient approximations of \(\log\det(I+\tilde H_m)\) via dominant eigenvalues, which points toward scalable computational approaches for D-optimal design in infinite dimensions. It also notes that generalizing beyond the simplifying assumptions (e.g., non-identity \(\Gamma_{noise}\), non-centered priors) is straightforward, implying extensions to more general Gaussian settings.","Develop computational algorithms (e.g., gradient-based optimization, randomized trace/log-det estimators, adjoint methods) for solving the infinite-dimensional A- and D-optimal design problems under realistic discrete/continuous design constraints and costs. Extend the criteria and theory to nonlinear Bayesian inverse problems (e.g., Laplace approximations, variational Bayes) and to non-Gaussian priors/noise, including robustness analyses under model and noise misspecification. Provide benchmarked numerical case studies (e.g., PDE sensor placement) comparing designs produced by the proposed infinite-dimensional objectives against finite-dimensional discretize-then-design baselines. Study sequential/adaptive experimental design in function space, where \(\xi\) is updated online using posterior information and computation-aware approximations.",1408.6323v2,https://arxiv.org/pdf/1408.6323v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:37:57Z
TRUE,Other,Model discrimination|Cost reduction|Other,Not applicable,"Variable/General (model variables/species; e.g., shuttle model has 19 species and 31 parameters; inference varies 3 parameters per model in ABC)",Healthcare/medical|Other,Simulation study|Other,TRUE,MATLAB|Other,Personal website,http://math.berkeley.edu/∼zhrosen/matroids.html,"The paper develops parameter-free, algebraic approaches to distinguish competing ordinary-differential-equation models of the canonical Wnt/β-catenin signaling pathway and to guide minimal experiments for model comparison. It combines (i) Bayesian parameter inference (approximate Bayesian computation) showing that multiple published Wnt models can all fit the same mammalian β-catenin time-course data, with (ii) parameter-free structural analyses from chemical reaction network theory (injectivity/multistability) and algebraic matroid theory. Injectivity/CRNT analysis indicates that previously published models are generally monostable, while a newly proposed “shuttle model” (with compartmental shuttling and degradation mechanisms) can exhibit multistationarity and multistability under conditions involving shuttling and degradation rates. Algebraic matroids of the steady-state ideals are computed to identify minimal sets of species whose measurements determine others and to generate steady-state invariants (circuit polynomials) that enable parameter-free model discrimination. The resulting matroid structure provides concrete experimental guidance on which groups of molecular species to measure (and how few) to discriminate between models without knowing kinetic parameters.","The proposed shuttle model is defined by mass-action ODEs for cytoplasmic and nuclear species, including β-catenin production/degradation, DC-dependent degradation, and shuttling, e.g. $X' = k_4 - k_5X + k_{24}X_n - k_{25}X - k_1Y_aX + k_2C_{XY}$ and $X_n' = -k_{24}X_n + k_{25}X - k_{15}X_n - k_{12}Y_{an}X_n + k_{13}C_{XY_n} - k_{30}X_nT + k_{31}C_{XT}$. Multistationarity conditions are discussed via Jacobian-sign conditions; two short necessary conditions reported are $k_3k_{15}+k_3k_{24}-k_{14}k_{24}>0$ and/or $k_5k_{14}-k_3k_{25}+k_{14}k_{25}>0$, tying bistability to degradation and shuttling rates. For matroid-based discrimination, an example circuit invariant for Schmitz et al. relating cytoplasmic and nuclear β-catenin is $I=h_1(\delta)X^2+h_2(\delta)X_n^2+h_3(\delta)XX_n$, tested with a coplanarity/SVD criterion on the transformed data matrix.","Bayesian ABC inference (with only three free parameters per model) fits the same mammalian β-catenin time-course data well for all five models studied, indicating that time-course data alone are insufficient for model selection at current data availability. CRNT/injectivity analysis finds only the Schmitz et al. model and the new shuttle model fail injectivity; further, Schmitz et al. admits at most two steady states with only one stable, whereas the shuttle model can have two stable steady states when three or more species shuttle between compartments. Matroid computations yield model ranks that translate into the number of independent species measurements required at steady state (e.g., the shuttle model matroid components have rank 5, so 5 appropriately independent measurements determine the rest). In a demonstrated discrimination test using β-catenin cytoplasm/nucleus data, the Schmitz et al. invariant-based coplanarity test strongly rejects Schmitz when data are generated from the shuttle model (reported $\Delta_{\text{Schmitz}}=64820$ vs cutoff 11.15 at 5% level), while Schmitz is compatible with its own simulated data ($\Delta_{\text{Schmitz}}=3.768$).","The authors note that disparity between model complexity and available data prevents choosing between models by model selection; even simplified (3-parameter) versions of each model fit the time-course data well. They also emphasize that unknown/variable parameter values (including differences between in vitro and in vivo contexts) limit mechanistic conclusions from parameter estimation and motivate parameter-free approaches. For bifurcation-style demonstrations, they caution that example parameter sets used to illustrate bistability are not biologically informed, so detailed quantitative predictions should not be over-interpreted.","The experimental-design guidance is primarily steady-state based (matroid invariants/coplanarity), so it may be less informative when only transient/non–steady-state measurements are feasible or when steady state is uncertain. The approach assumes correct model structure and mass-action/polynomial steady-state formulations; model misspecification, unmodeled feedbacks, or measurement biases could undermine discrimination. Practical feasibility of measuring the suggested species sets (e.g., multiple compartment-specific complexes) in real biological assays is not fully addressed, and comparisons largely emphasize a subset of models and observables (not a comprehensive benchmark across modern Wnt models or single-cell data types).","The paper suggests that matroid-derived dependencies can guide future experimentation by identifying which species to measure to discriminate between competing models with minimal experiments, helping narrow the gap between data and models. It also notes that multiple alternative bistable mechanisms/models could be explored within the proposed discrimination framework and that the tools demonstrated should be applicable across many systems-biology modeling problems where parameters are difficult to estimate. The authors highlight directions for further experimentation to test predicted roles of spatial localization/shuttling and degradation in governing bistable switching.","Developing explicit, quantitative optimal experiment design criteria (e.g., information-theoretic or Bayesian design) on top of matroid-identified candidate measurement sets could turn qualitative guidance into optimized sampling/timepoint/replicate plans. Extending the parameter-free discrimination framework to handle autocorrelated time-series data, partial observability, and non–steady-state invariants (dynamic invariants) would broaden applicability. Robustness studies under non-mass-action kinetics, stochastic effects, and single-cell heterogeneity (rather than averaged time courses) would better match modern experimental modalities and could reveal when matroid-based invariants remain reliable. Providing maintained software packages/workflows (beyond a personal webpage) and standardized benchmarks would improve reproducibility and uptake by practitioners.",1409.0269v3,https://arxiv.org/pdf/1409.0269v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:38:40Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,A-optimal,Variable/General (infinite-dimensional parameter field; design dimension = number of candidate sensor locations ns),Environmental monitoring|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://arxiv.org/abs/1410.5899v2|http://www.spe.org/web/csp/datasets/set02.htm,"The paper develops a scalable A-optimal Bayesian experimental design method for nonlinear PDE-constrained inverse problems with infinite-dimensional (function-space) parameters, focused on optimal sensor placement among candidate locations. The OED objective generalizes classical A-optimality by minimizing the expected trace of the posterior covariance; because the exact posterior covariance is unavailable in nonlinear problems, the authors use a Laplace/Gaussian approximation at the MAP point, giving an objective based on the trace of the inverse Hessian of the regularized misfit. The expectation over data is approximated by Monte Carlo sample averaging of synthetic datasets generated from prior/noise draws, and the trace is approximated by randomized trace estimators, yielding a bilevel PDE-constrained optimization problem with sparsity-promoting penalties on sensor weights. Adjoint-based derivatives (via Lagrangian formalism) enable gradient-based optimization, and the authors argue/verify scalability: PDE-solve complexity is essentially independent of parameter and candidate-sensor dimensions. Numerical studies for inferring a log-permeability field in subsurface flow (including an SPE10-based case) show optimal designs reduce posterior variance and improve MAP reconstruction compared with random designs using the same number of sensors.","The A-optimal objective is the expected average posterior variance, approximated as $\Psi(w)=\frac{1}{n_d n_{tr}}\sum_{i=1}^{n_d}\sum_{k=1}^{n_{tr}}\langle z_k, y_{ik}\rangle$ where $y_{ik}$ solves $H(m_{MAP}(w;d_i),w;d_i)\,y_{ik}=z_k$ and $H$ is the (Gauss-Newton or full) Hessian of the MAP objective. Data are generated via $d_i=f(m_i)+\eta_i$ with $m_i\sim\mu_{pr}$ and $\eta_i\sim\mathcal N(0,\Gamma_{noise})$, and the design enters via a weighted likelihood using $W=\mathrm{diag}(w)$ (sensor weights). The optimization problem is $\min_{w\in[0,1]^{n_s}}\ \Psi(w)+\gamma P(w)$ with sparsifying penalties $P(w)$ to encourage binary/low-cardinality sensor selections.","In the idealized subsurface flow example, the OED is computed with $n_d=5$ data samples and $n_{tr}=20$ trace vectors, producing optimal configurations with 10 sensors (for $\gamma=0.008$) and 20 sensors (for $\gamma=0.005$). Plots comparing many random designs against the computed A-optimal design show the A-optimal design achieves lower trace of the approximate posterior covariance and lower relative MAP error than randomly selected sensor sets with the same number of sensors, with stronger gains when fewer sensors are available (10 vs 20). Scalability experiments report that inner/outer CG iteration counts for objective/gradient evaluation are largely insensitive to increasing parameter dimension and only weakly dependent on candidate-sensor dimension; the number of quasi-Newton iterations for the outer OED optimization is also observed to be dimension-insensitive. In the SPE10-based case with 128 candidate locations, an A-optimal design with 22 sensors outperforms random designs in both average posterior variance and MAP relative error (as shown in the comparison plot).","The authors note that a key limitation is defining the OED objective using a Gaussian (Laplace) approximation of the posterior at the MAP point, which may be inaccurate for strongly nonlinear/non-Gaussian posteriors. They also state that results can be sensitive to the prior: if prior samples vary widely, the resulting design—optimized to accommodate that variability—may be suboptimal for the eventual true parameter. Finally, their sparsification provides only indirect control of the number of sensors; selecting a penalty parameter may require solving multiple OED problems to hit a desired sensor count.","The method requires repeated solution of MAP problems and many Hessian (inverse) applications, which can still be expensive for very large-scale PDEs without efficient preconditioning/low-rank approximations; performance may degrade if the effective rank of the prior-preconditioned Hessian is not small. The approach assumes a fixed finite set of candidate sensor locations and relaxes to continuous weights in $[0,1]$, which may yield near-binary solutions but does not guarantee globally optimal discrete (combinatorial) placements. Practical robustness to model mismatch, correlated/heteroscedastic noise beyond the diagonal model, and temporal/spatiotemporal data collection strategies is not established by the presented experiments.","They plan to study sensitivity of the optimal sensor placement to the number of data samples $n_d$ used in the OED expectation approximation, since each additional sample adds an inverse problem per OED iteration. They also suggest exploiting low-rank Hessian approximations to reduce the cost of multiple solves with the same Hessian and note coarse-grained parallelism across data samples as an avenue for computational speedup.","Extend the framework to handle non-Gaussian priors/noise or non-Laplace posterior approximations (e.g., variational Bayes or low-rank/nonlinear posterior covariance surrogates) to improve reliability in strongly nonlinear regimes. Develop principled calibration for the sparsity/penalty parameter (or constraints directly on sensor count) to better control the number of sensors without trial-and-error. Provide open-source implementations and standardized benchmarks (including real field datasets) and investigate robustness under model error, correlated observations, and spatiotemporal/online (sequential) sensor placement.",1410.5899v2,https://arxiv.org/pdf/1410.5899v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:39:23Z
TRUE,Other,Parameter estimation|Model discrimination|Other,Not applicable,2 factors (pressure and temperature); explored via oil pressure and heating power ramps,Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a new experimental cell/measurement setup (an indirect-heating configuration) for high-pressure/high-temperature electrical resistance measurements in a Paris–Edinburgh large-volume press. The key novelty is measuring voltage drop directly across a small sample using separate electrodes while heating indirectly with a graphite resistive heater, improving sensitivity to abrupt resistance changes at phase transitions and reducing pressure/temperature gradients due to small sample size. The method is demonstrated experimentally by mapping resistance discontinuities for Pb, Sn, and Bi over ~0–6 GPa and ~300–1000 K to identify solid–solid and solid–liquid transitions. Using known phase-diagram reference points (notably in Sn and Bi), the authors fit calibration relationships that map external controls (oil pressure and heating power) to sample pressure and temperature, enabling reconstruction of phase diagrams without direct P–T sensors. Reconstructed P–T phase boundaries agree closely with literature, supporting the setup’s accuracy and practicality for phase-diagram mapping under extreme conditions.","Sample pressure is calibrated from oil pressure using an exponential saturation form: $P = P_{\infty}\left(1-e^{-P_{\mathrm{oil}}/P_0}\right)$. Sample temperature is modeled as linear in heating power at fixed pressure: $T = a(P)\,W + T_0$, with pressure-dependent slope $a(P)=a_0+a_1P$ and $T_0\approx295\,\mathrm{K}$. Together these map the experimental control variables $(P_{\mathrm{oil}},W)$ to sample $(P,T)$ for identifying phase-transition loci.","The fitted calibration parameters are reported as $P_0=414\,\mathrm{bar}$ and $P_{\infty}=7.5\,\mathrm{GPa}$ for the pressure mapping, and $a_0=1.82\,\mathrm{K\,W^{-1}}$, $a_1=-0.027\,\mathrm{K\,W^{-1}\,GPa^{-1}}$ (with $T_0=295\,\mathrm{K}$) for the temperature mapping. The authors estimate uncertainties of about \u00b15% in sample pressure and \u00b120 K in sample temperature. Reconstructed phase diagrams for Pb, Sn, and Bi from measured resistance discontinuities show excellent agreement with previously published phase boundaries, validating both the indirect-heating design and the calibration approach. Experiments were repeated 2–4 times at multiple heating rates (3.75, 7.5, 11.25, 15 W/min) to check rate effects, and the isobaric decompression protocol mitigated melt percolation issues.","The authors note technical challenges in implementing the indirect-heating configuration, specifically (i) possible disconnection of metallic electrodes during compression and (ii) percolation of melt through the BN medium at low pressures (especially during first compression), which motivated their protocol of compressing first to high oil pressure and then measuring during decompression. They also state that the pressure/temperature calibration applies only to the specific experimental protocol used (compression to 760 bar followed by decompression), implying recalibration is required for different assemblies/protocols.","The approach relies on reference-point calibration from known phase diagrams (Sn/Bi) and assumes that calibration is sample-independent; this may not hold if different samples materially change thermal contact, heater coupling, or cell deformation behavior. Temperature is inferred from heating power via a linear model with mild pressure dependence; nonlinearity, hysteresis, and changing thermal conductivity with phase (solid/liquid) could bias inferred temperatures, particularly near transitions or across different materials. The method is demonstrated on simple elemental conductors; performance for poorly conducting samples, reactive melts, or materials with small resistance signatures (or strong contact resistance effects) is not established. No public implementation details are provided for the automated control/data acquisition and fitting, which may hinder reproducibility in other labs.","They state future experiments will target elemental systems and alloys and will pursue challenging studies of liquid–liquid phase transitions, leveraging the method’s high sensitivity to minor resistance changes to better understand liquid–liquid transitions.","A useful extension would be a systematic robustness study across materials with different resistivities and chemistries (including insulators/semiconductors and reactive systems), quantifying contact-resistance and electrode-stability effects. Providing an open workflow (data acquisition, calibration fitting, uncertainty propagation) and benchmarking against direct thermocouple/EOS-based calibrants would improve reproducibility and validation. Extending the design to multivariate sensing (simultaneous impedance spectroscopy, thermopower, or acoustic/structural probes) could improve phase identification and reduce reliance on external reference-point calibrations.",1412.0613v2,https://arxiv.org/pdf/1412.0613v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:39:49Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Model discrimination|Other,D-optimal|A-optimal|Bayesian D-optimal|Bayesian A-optimal|Compound criterion|Other,"Variable/General (design has v controllable variables, n runs; examples include 1 variable (Poisson), 3 parameters with sampling times (pharmacokinetics), 4 factors (logistic regression), and 1 dose factor (binomial regression follow-up))",Healthcare/medical|Pharmaceutical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Other,Not provided,http://www.bode2015.wordpress.com,"The paper develops a general decision-theoretic Bayesian design of experiments method for complex nonlinear and generalized linear/hierarchical models where expected utilities are analytically intractable and the design space is high-dimensional. It introduces the Approximate Coordinate Exchange (ACE) algorithm, which iteratively optimizes one design coordinate at a time by building one-dimensional Gaussian process emulators of a Monte Carlo approximation to the expected utility, combined with accept/reject steps based on a Bayesian t-test-style probability to guard against Monte Carlo/emulator error; a second phase optionally consolidates clustered points via point exchange to allow replication. The approach supports a wide range of Bayesian utilities, including Shannon information gain and negative squared error loss, and also enables pseudo-Bayesian (normal-approximation) D- and A-optimal criteria for comparison. Demonstrations include pharmacokinetic sampling-time design (compartmental model), multi-factor logistic regression (including hierarchical/random-effects structure), and follow-up dose selection under model uncertainty for a binomial regression case study; in these examples ACE yields higher expected utility than dimension-reduction approaches and shows meaningful gains over pseudo-Bayesian designs for small sample sizes. The method is positioned as overcoming the curse of dimensionality in Bayesian optimal design by replacing global high-dimensional emulation with a sequence of tractable one-dimensional emulators and coordinate-wise optimization.","The Bayesian expected utility is defined as $U(\delta)=\int\!\int u(\delta,\psi,y)\,\pi(y,\psi\mid\delta)\,dy\,d\psi$ and is approximated by Monte Carlo as $\tilde U(\delta)=\frac1B\sum_{l=1}^B u(\delta,y_l,\psi_l)$. In Phase I, ACE emulates the coordinate-conditional utility $\tilde U(\delta_i\mid\delta_{(i)})$ with a 1D Gaussian process; the emulator mean is $\hat U(\delta\mid\delta^C_{(i)})=\hat\mu_i+\hat\sigma_i\,a(\delta,\xi_i)^T A(\xi_i)^{-1} z(\xi_i)$ using a squared-exponential correlation with nugget. Candidate coordinate updates are accepted with probability $p_I^{\dagger}=1-T_{2B-2}\!\left(-\frac{B\tilde U(\delta^{C\dagger})-B\tilde U(\delta^C)}{\sqrt{2B\hat\nu_I}}\right)$ (analogously $p_{II}^{\dagger}$ in Phase II) based on a two-sample t distribution. Utilities used include Shannon information gain $u_S(\theta,y,\delta)=\log\pi(\theta\mid y,\delta)-\log\pi(\theta)=\log\pi(y\mid\theta,\delta)-\log\pi(y\mid\delta)$ and negative squared error loss $u_V(\theta,y,\delta)=-\sum_{w=1}^p[\theta_w-E(\theta_w\mid y,\delta)]^2$; pseudo-Bayesian criteria are $\phi_S(\delta)=E_\psi[\log|I(\theta;\delta,\gamma)|]$ (D-type) and $\phi_V(\delta)=-E_\psi[\mathrm{tr}\{I(\theta;\delta,\gamma)^{-1}\}]$ (A-type).","For the pharmacokinetic compartmental model, unrestricted SIG-optimal designs found by ACE yield up to about a 5% improvement in approximate expected Shannon information gain compared with beta-quantile dimension-reduction designs (including the Ryan et al. 2014 approach), and the pseudo-Bayesian D-optimal design closely approximates the SIG-optimal design. For homogeneous 4-factor logistic regression, fully Bayesian designs show substantial gains for small n: SIG-optimal designs have up to ~20% larger expected Shannon information gain than pseudo-Bayesian D-optimal designs, and NSEL-optimal designs have up to ~27% smaller expected posterior variance (trace) than pseudo-Bayesian A-optimal designs; differences shrink as n increases. Maximin Latin hypercube designs perform poorly relative to model-based Bayesian designs under both SIG and NSEL. In a beetle-mortality follow-up design under model uncertainty, ACE selects doses concentrated near ~1.77 mg/L (with replication for larger $n_0$), and the expected posterior variance of LD50 decreases rapidly as $n_0$ increases from 1 with diminishing returns thereafter; random design exploration plots confirm the ACE-selected designs attain the minimum (best) evaluated expected loss among large random samples of candidate designs for $n_0=1,2$.","The authors note that termination/convergence choice (NI and NII) is complicated by the stochastic nature of Monte Carlo expected-utility approximations and is assessed graphically via trace plots. They also state that the acceptance-test step assumes approximate normality and equal variances of the utility evaluations; if this assumption is severely violated, a more sophisticated (and more computationally costly) test would be required. They further remark that a fully Bayesian approach to GP emulation (e.g., MCMC over GP hyperparameters) would substantially increase computational cost, so they use MLE via Fisher scoring instead. For some problems (e.g., constrained sampling times), Phase II point exchange is omitted because replication is not permitted.","The method relies heavily on nested Monte Carlo for utilities like SIG/NSEL; despite emulator smoothing and acceptance tests, computational cost can still be very high for complex simulators or expensive likelihood evaluations, and the bias of nested Monte Carlo for log-marginal likelihood terms may affect optimization when inner-sample sizes are limited. The coordinate-wise optimization strategy can still be sensitive to multimodality and strong dependence between coordinates (potentially slow mixing or local optima), especially in highly constrained or discrete design spaces. GP emulators are fit repeatedly with fixed kernel form (squared-exponential) and MLE hyperparameters, which may be unstable for noisy utility evaluations or non-smooth utility surfaces, and emulator misfit could reduce efficiency even with acceptance testing. Practical implementation details (e.g., runtime scaling with n×v, robustness to correlated observations/autocorrelation, discrete/categorical factors) are not fully explored and may require additional adaptations.","The authors propose extending ACE to settings where the likelihood is only available numerically from expensive computer code (e.g., nonlinear differential equation models in uncertainty quantification). They suggest improving convergence via reparameterization to reduce dependencies between design coordinates, informed by pilot runs or pseudo-Bayesian designs. They also propose more efficient coordinate optimization using sequential strategies such as expected improvement adapted for stochastic responses. Finally, they mention variance-reduction ideas (e.g., antithetic/zero-variance Monte Carlo) and hybrid deterministic–Monte Carlo approximations (e.g., expectation propagation) to reduce or avoid nested simulation.","Developing self-tuning/default rules for Monte Carlo sample sizes (B and \tilde B) and emulator design size m that balance bias/variance and runtime, potentially adaptively during the run, would improve usability. Extending ACE to mixed discrete–continuous factor spaces and constrained/blocked/split-plot structures with hard randomization restrictions (beyond the examples) would broaden applicability in industrial DOE. Providing theoretical guarantees (or diagnostics) for convergence/near-optimality under stochastic utilities and emulator approximation would strengthen methodological assurance. Open-source implementations with reproducible benchmarks and standardized test suites across common Bayesian design problems (GLMs, PK/PD, hierarchical models) would facilitate adoption and fair comparison to alternative Bayesian design algorithms.",1501.00264v4,https://arxiv.org/pdf/1501.00264v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:40:44Z
TRUE,Computer experiment|Other,Prediction|Screening|Other,Space-filling|Other,"Variable/General (examples and simulations for d = 3, 4, 5; subspaces t = 2, 3, 4; with n = p^d such as n = 8 = 2^3 and n = 27 = 3^3)",Theoretical/simulation only,Simulation study,TRUE,MATLAB,Not provided,NA,"The paper studies how well Latin Hypercube Sampling (LHS) and Orthogonal Sampling (OS) cover lower-dimensional subspaces (t-way projections) of a high-dimensional parameter space when building populations of models or designing computer experiments. Using MATLAB simulations (averaged over hundreds of runs), the authors conjecture a simple formula for the expected coverage of a t-dimensional subspace after k independent LHS trials of size n: $P(k,n,d,t)=1-(1-1/n^{t-1})^k\approx 1-e^{-k/n^{t-1}}$, and argue this coverage is effectively independent of the ambient dimension d. They compare LHS versus OS and show OS yields more uniform coverage across sub-blocks of the projected t-dimensional space, while LHS can exhibit substantial non-uniformity at partial coverage levels (<100%). The work connects DOE concepts (orthogonal arrays / t-way coverage) to “populations of models” calibration workflows in computational science. The main contribution is the conjectured coverage law for t-way margins under repeated LHS trials and the demonstration that OS improves uniformity of marginal coverage.","The conjectured expected coverage of a $t$-dimensional subspace after $k$ Latin hypercube trials (each of size $n$) is $P(k,n,d,t)=1-(1-1/n^{t-1})^k$. For large $k$, they use the asymptotic approximation $P(k,n,d,t)\approx 1-e^{-k/n^{t-1}}$. They also note that achieving (near) 100% coverage implies $k\approx (t-1)\log(n)\,n^{t-1}$ (from $(1-p)^k>p$ with $p=1/n^{t-1}$).","From simulation plots for $d=3,4,5$ and projections with $t=2$ and $t=3$, the number of LHS trials required to reach a given partial coverage level (e.g., 25%, 50%, 75%) is similar across different $d$, supporting the claim that $P(k,n,d,t)$ is essentially independent of $d$. On log-log plots, the slope of trials-versus-$n$ for partial coverage matches $t-1$ (about 1 for $t=2$ and about 2 for $t=3$), consistent with $k$ scaling like $n^{t-1}$. For 100% coverage the slope is slightly larger than $t-1$ (reported ~1.25 for $t=2$ and ~2.3 for $t=3$), consistent with the additional $\log(n)$ factor. In a worked example ($d=3$, $n=27$), they report sub-block coverage variability under LHS at partial coverage (e.g., around 25% coverage at ~210 trials and 75% coverage at ~1010 trials), whereas OS is uniform by construction across sub-blocks.","The authors state their main coverage relationship is a conjecture supported by simulation evidence and note they plan to prove it analytically in subsequent work. They also emphasize that LHS does not ensure uniform coverage of lower-dimensional sub-blocks at partial coverage levels, motivating the use of orthogonal sampling when uniform marginal coverage is desired.","Results are based on specific simulation settings (selected $d$, $t$, and $n$ values; particular definitions of “coverage” and sub-blocking), so the conjecture’s accuracy and robustness across broader regimes (e.g., very large $d$, different $n$ structures not equal to $p^d$, correlated/constraint parameter spaces) is not fully validated. Comparisons focus primarily on LHS vs OS; other space-filling designs (e.g., maximin LHS, Sobol/low-discrepancy sequences) are not benchmarked. Practical guidance on choosing OS constructions when $n$ is not an exact power ($n\neq p^d$) or when factor ranges are continuous with transformations is not developed.",They explicitly state they will attempt to prove the coverage conjecture analytically in a subsequent paper.,"It would be valuable to extend the coverage theory and OS constructions to cases where $n$ is not of the form $p^d$, and to continuous-factor settings with common DOE transformations (e.g., stratification in quantile space). Additional benchmarks against modern space-filling and low-discrepancy designs (optimized/maximin LHS, Sobol, Halton, OA-LHS variants) could clarify when OS provides the best tradeoff between t-way uniformity and other criteria. Developing software implementations and diagnostics for practitioners (e.g., marginal coverage metrics and adaptive trial allocation to target desired t-way coverage) would improve usability in computer experiments and population-of-models workflows.",1502.06559v1,https://arxiv.org/pdf/1502.06559v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:41:13Z
TRUE,Optimal design|Bayesian design,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (examples: Nθ=7 in 2D, Nθ=11 in 3D; design variables include number and locations of receivers and measurement time)",Environmental monitoring|Other,Approximation methods|Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper develops a fast Bayesian optimal experimental design (OED) method for seismic source inversion, aiming to choose the optimal number and locations of seismic receivers (and implicitly the measurement time) to maximize the expected information gain. The design criterion is the expected Kullback–Leibler divergence between posterior and prior (expected information gain), and the main computational advance is reducing the standard double-loop estimator to a single-loop estimator via a Laplace approximation when the posterior is concentrated. The authors show posterior concentration arises because the Hessian of the negative log-posterior (cost functional) scales with the number of receivers and the number of time samples (measurement time), yielding diminishing approximation error as data volume increases. They propose parameter scaling to mitigate ill-conditioning of the Hessian due to vastly different parameter magnitudes, and compute Hessian terms using a second-order finite difference elastodynamics solver plus adjoint/dual equations; numerical integration over the prior is done with either sparse quadrature or Monte Carlo. Numerical examples for a 2D layered-earthquake model demonstrate efficiency and reveal nontrivial optimal receiver spacing (e.g., around d_R≈1000 in one scenario) with diminishing returns beyond ~20 receivers.","Experimental design objective is the expected information gain $I=\mathbb{E}_Y[D_{\mathrm{KL}}]$ where $D_{\mathrm{KL}}(y)=\int \log\frac{p(\theta\mid y)}{p(\theta)}\,p(\theta\mid y)\,d\theta$ and $I=\int\int \log\frac{p(\theta\mid y)}{p(\theta)}\,p(\theta\mid y)\,p(y)\,d\theta\,dy$ (Eqs. 12–13). Using Laplace approximation, $I$ is approximated in terms of the Hessian of the negative log-posterior, e.g., $\hat D_{\mathrm{KL}}(\theta^*)=-\tfrac12\log\big((2\pi)^{N_\theta}|H_1(\theta^*)^{-1}|\big)-\tfrac{N_\theta}{2}-h(\theta^*)$ (Eq. 24), where $H_1=\sum_{r,m} (\nabla_\theta g_r)^T C_\epsilon^{-1}(\nabla_\theta g_r)$ (Eq. 20) and $h(\theta)=\log p(\theta)$. The key scaling result is that the dominant Hessian term grows like $O(N)$ with $N=N_R\times N_t$ (number of receivers × number of time samples), supporting posterior concentration and decreasing Laplace error.","In the 2D example, scaling the Hessian dramatically reduces ill-conditioning: the unscaled Hessian has condition number $\mathrm{cond}(H_1)=3.88\times 10^{30}$, while diagonal scaling $S_{ii}=\sqrt{(H_1)_{ii}}$ yields $\mathrm{cond}(S^{-T}H_1S^{-1})\approx 12.16$. A convergence study with three uncertain parameters reports empirical error decay rates of about 0.40 for sparse quadrature and 0.49 for Monte Carlo (from log–log regression). In a comparison against nested Monte Carlo, the Laplace-based estimator converges much faster and the final discrepancy between methods is reported as <4% in the tested setup. Design sweeps show expected information gain increases sharply up to about 20 receivers and then shows marginal gains beyond that, and for fixed $N_R=5$ an intermediate receiver spacing (around $d_R\approx 1000$) maximizes information in their scenario III.","The paper notes sparse quadrature relies on regularity of the integrand, and that due to the singular source term the wave solution/information gain does not have high regularity with respect to source location parameters; in such cases sampling-based integration (e.g., Monte Carlo) may be needed. It also emphasizes that the method applies when assumptions on model smoothness and a uniformly bounded Jacobian singular value (Assumptions A1–A2) hold. Additionally, it remarks they sweep one-dimensional design scenarios; for higher-dimensional design spaces, more advanced optimization algorithms should be used.","The approach’s practical effectiveness depends on how well posterior concentration holds for real (not synthetic) earthquakes and modeling errors; the analysis assumes additive i.i.d. Gaussian measurement noise and a well-specified forward model, which may be violated with correlated noise, non-Gaussian errors, or significant model discrepancy. The Laplace approximation can be inaccurate for multimodal posteriors (common in waveform inversion) or when data are insufficient, yet the numerical demonstrations are limited to a simplified 2D layered model. The design space explored is restricted (mostly symmetric/equally spaced receiver arrays and limited scenarios), so global optimality in realistic deployment constraints is not established. Code/implementation details (beyond algorithm descriptions) are not provided, which may limit reproducibility and adoption.","The authors state that while they sweep one-dimensional design spaces in their scenarios, when more freedom is allowed in a higher-dimensional design space, more advanced optimization algorithms should be implemented. They also suggest the method can be generally applied to other non-repeatable, time-dependent experimental design problems provided the assumptions in Section 3 are satisfied.","Extending the method to account explicitly for model discrepancy and correlated/colored noise (e.g., spatiotemporal covariance in seismic recordings) would improve realism and robustness of the designs. Developing and testing gradient-based or Bayesian optimization methods for high-dimensional receiver-placement design (with constraints such as terrain, access, and cost) would make the approach more practically deployable than design sweeps. Evaluating performance under multimodal posteriors (e.g., with more complex earth models or limited bandwidth) and comparing against alternative design criteria (e.g., I-optimal prediction-focused criteria over regions of interest) would clarify when Laplace-based EIG remains reliable. Providing an open-source implementation and benchmarking on real seismic datasets would strengthen reproducibility and practical impact.",1502.07873v1,https://arxiv.org/pdf/1502.07873v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:41:55Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Prediction|Parameter estimation|Cost reduction|Other,I-optimal (IV-optimal)|V-optimal|Other,"Variable/General (examples include 1D, 2D, 5D, 10D inputs)",Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/GPEXP,"The paper develops and analyzes experimental design methods for building surrogate models of deterministic computer simulations, focusing on Gaussian process regression (GPR) and pseudospectral polynomial approximation. It proposes continuous-space optimization of an integrated posterior variance (IVAR, equivalent to IMSE) criterion for GPR using sample-average approximation and gradient-based constrained optimization, enabling batch (non-greedy) and greedy/batched point selection on complex (non-rectangular, non-convex) domains. Using Mercer kernel eigenfunction expansions, it derives conditions under which GPR matches pseudospectral approximation and explains discrepancies as kernel truncation/eigenfunction mismatch, nugget effects, or design-induced (loss of) discrete orthogonality. Empirically, IVAR-optimized designs produce well-spaced points with favorable interpolation stability (Lebesgue constants) and deliver lower L2 approximation errors than entropy (ALM) or mutual information (MI) greedy designs across several simulated examples. The work also studies adaptive schemes that interleave IVAR-based point selection with GP hyperparameter learning and compares them to adaptive Smolyak sparse pseudospectral approximations, finding GP+IVAR advantageous especially for non-additively coupled and higher-dimensional test functions.","The design criterion is the integrated posterior variance (IVAR): $x^*=\arg\min_{x\in U}\int_{\mathcal X} c(\hat x\mid x)\,d\mu(\hat x)$ where $c(x)=C(x,x)$ and $C(x,x')=K(x,x')-K(x,\mathbf x)^\top R\,K(\mathbf x,x')$ with $R^{-1}_{ij}=K(x_i,x_j)+\delta_{ij}\sigma^2$. IVAR is approximated via sample-average approximation: $\int c(\hat x\mid x)d\mu(\hat x)\approx \frac{1}{N_{mc}}\sum_{i=1}^{N_{mc}} c(\hat x_i\mid x)$, enabling gradient-based optimization using analytical derivatives of $c$ w.r.t. design points.","Across multiple simulated domains (e.g., 2D/5D balls, a periodic-kernel square, and a non-convex/non-simply connected 2D region), IVAR-optimized designs achieve lower relative $L_2$ surrogate error than greedy entropy (ALM) and greedy MI designs; even greedy/batched IVAR variants outperform ALM/MI in reported comparisons. IVAR designs avoid boundary clustering common in variance-maximization (ALM) for radial basis kernels and yield well-spaced point sets; interpolation stability is assessed via Lebesgue constants, which remain small until design size exceeds kernel complexity. The paper also shows that, under orthogonalizing quadrature designs and when the PSA basis matches kernel eigenfunctions, GP posterior mean and PSA coincide up to nugget/truncation effects (with an explicit bound). In adaptive comparisons (e.g., Ishigami in 3D and a 10D oscillatory Genz function), GP+IVAR with kernel hyperparameter learning reduces $L_2$ error more steadily and can outperform adaptive Smolyak PSA on strongly coupled/high-dimensional functions.",None stated.,"The IVAR objective depends on the chosen GP kernel and measure $\mu$; if these are misspecified relative to the true function or quantity of interest, IVAR-optimal designs may not be optimal in practice. Continuous, gradient-based IVAR optimization can become difficult for very large $N\times d$ (nonconvexity, local minima, scaling of $O(N^3)$ linear algebra and $O(N_{mc}N^2)$ variance evaluations) and may require careful initialization or batching/greedy approximations. Empirical comparisons are primarily on synthetic test functions and selected kernels; broader benchmarks (e.g., additional competing space-filling and optimal-design methods, noisy/heteroscedastic responses, correlated outputs) and more real-world case studies would strengthen generalizability.","The authors suggest comparing IVAR-optimal nodes to node-selection methods from the radial basis function interpolation literature and to nodes from Bayesian quadrature/average-case quadrature, and advocating closer connections between numerical analysis and statistics on these topics. They also propose more rigorous study of adaptive (closed-loop) designs that interleave point selection with kernel updates, including look-ahead strategies that balance information for hyperparameter learning with reduction of posterior variance.","Develop scalable implementations for large-$N$ IVAR optimization using low-rank kernel approximations, inducing points, or randomized trace/variance estimators, enabling higher-dimensional/high-budget designs. Extend IVAR design to settings with model discrepancy, nonstationary or heteroscedastic noise, and constrained/expensive-to-evaluate feasible regions, plus multi-fidelity or multi-output GPs. Provide open-source reproducible scripts/notebooks for all experiments and standardized benchmarks comparing against modern acquisition functions (e.g., Bayesian optimization criteria, integrated variance reduction variants) under consistent computational budgets.",1503.00021v4,https://arxiv.org/pdf/1503.00021v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:42:29Z
FALSE,NA,NA,Not applicable,Not specified,Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,http://www.fourierandwavelets.org/,"This paper studies sampling and recovery of smooth signals defined on graphs, contrasting uniform random sampling with an “experimentally designed” sampling strategy based on sampling scores (analogous to leverage scores). It introduces a new signal class, approximately bandlimited graph signals, and proposes two unbiased recovery algorithms that estimate low-frequency graph Fourier components using a chosen bandwidth parameter. The authors derive nonasymptotic mean-squared error bounds showing that the designed-sampling approach can converge faster on irregular graphs (where energy in the relevant spectral subspace is concentrated in a few nodes/columns). They further provide convergence-rate corollaries for two graph types (energy-spread vs. energy-concentrated) and validate claims via simulations on ring, Erdős–Rényi, and star graphs. Overall, the contribution is in graph-signal sampling theory and reconstruction rather than classical DOE for physical experiments.","Sampling model: $y=\Psi x+\epsilon$ with i.i.d. Gaussian noise $\epsilon\sim\mathcal N(0,\sigma^2 I)$ and sampling operator $\Psi$ selecting node indices. Random-sampling recovery (Alg. 1): $\hat x_b^{\,*}(k)=\frac{N}{|\mathcal M|}\sum_{i\in\mathcal M} U_{k i}y_i$, then reconstruct $x_i^*=\sum_{k<\kappa}V_{ik}\hat x_b^{\,*}(k)$. Designed-sampling recovery (Alg. 2) samples node $i$ with probability $w_i=\|u_i\|_2/\sum_j \|u_j\|_2$ (with $u_i$ the $i$th column of $U_{(\kappa)}$), and uses $\hat x_b^{\,*}(k)=\frac{1}{|\mathcal M|}\sum_{i\in\mathcal M}\frac{1}{w_i}U_{k i}y_i$ followed by the same reconstruction.","Both algorithms are shown to be unbiased for the first $\kappa$ graph-frequency components: $\mathbb E[x^*]=V_{(\kappa)}U_{(\kappa)}x$. For $x\in \mathrm{ABLA}(K,\beta,\mu)$ and $\kappa\ge K$, the MSE bounds have the same bias term $\alpha_2\mu\|x\|_2^2/\kappa^{2\beta}$ but different variance terms: random sampling scales with $\frac{\alpha_2(\max_j x_j^2+\sigma^2)}{|\mathcal M|}N\|U_{(\kappa)}\|_F^2$, while designed sampling scales with $\frac{\alpha_2(\max_j x_j^2+\sigma^2)}{|\mathcal M|}\|U_{(\kappa)}\|_{2,1}^2$. Simulations (50 trials, $N=10{,}000$, $K=10$, $\beta=1$, $\sigma^2=0.01$, $\kappa=10$) show similar performance on ring and Erdős–Rényi graphs (type-1) but markedly better recovery for the star graph (type-2) under designed sampling.",None stated.,"Despite using the term “experimentally designed sampling,” the work does not address DOE in the classical sense (factor settings, randomization/blocking, response surface/optimal designs); it focuses on probabilistic node-selection policies for graph sampling. Empirical validation is limited to synthetic signals and three canonical graph families, with no real-world datasets to test robustness to model mismatch. Implementation details (e.g., eigendecomposition cost for large graphs, numerical stability, and practical selection of bandwidth $\kappa$) are not fully developed, and code is not provided.",None stated.,"Extending the sampling-score approach to settings with unknown or streaming graphs (self-starting/adaptive sampling) and to correlated/non-Gaussian noise would broaden practical applicability. Providing scalable approximations to sampling scores (e.g., randomized methods) and releasing reference implementations would improve reproducibility. Testing on real graph-signal applications (sensor networks, social/traffic graphs) and comparing to additional contemporary graph sampling/reconstruction baselines would strengthen empirical support.",1504.05427v2,https://arxiv.org/pdf/1504.05427v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:43:02Z
TRUE,Optimal design|Other,Parameter estimation|Other,D-optimal|A-optimal|E-optimal|Other,"Variable/General (linear regression with parameter dimension p; also nonlinear regression examples; Example 2 uses q predictors, Example 3 uses polynomial degree d)",Theoretical/simulation only|Other,Other,TRUE,MATLAB,Not provided,NA,"The paper reformulates the computation of several classical optimal design criteria—D-, A-, and Ek-optimality, combinations of criteria, and Harman’s (2004) criterion-robust (maximin-efficiency) design—as infinite-dimensional linear programming problems for approximate designs on a finite design space. It derives min–max representations of these criteria of the form \(\phi(\xi)=\min_{\mu}\sum_{x\in\mathcal X} H(\mu,x)\,\xi(x)\), enabling solution by a modified cutting-plane method with an LP solver. The authors provide iterative LP algorithms for (i) D/A/Ek optimality, (ii) criterion-robust designs via normalized Ek constraints across \(k=1,\dots,p\), and (iii) D-optimality subject to a prescribed lower bound on the A-criterion, and demonstrate them on nonlinear and polynomial regression examples. They also discuss extensions of these reformulations to nonlinear models (including AVE/Bayesian-type averaged criteria) and present an appendix on “extended” criteria expressed via response-function differences. Computational demonstrations (implemented in MATLAB) show the algorithms recover known optimal/robust designs on large finite grids, at the cost of potentially substantial runtime for higher-dimensional cases.","For a finite design space \(\mathcal X\) and information matrix \(M(\xi)=\sum_{x\in\mathcal X} f(x)f(x)^\top\xi(x)\), the criteria are rewritten as LP-friendly min forms: \(\phi_D(\xi)=\min_{\mu\in\Xi^+}\sum_x \big(\det(M(\mu))^{1/p}\,\tfrac{1}{p} f(x)^\top M(\mu)^{-1}f(x)\big)\xi(x)\); \(\phi_A(\xi)=\min_{\mu\in\Xi^+}\sum_x \big(\|M(\mu)^{-1}f(x)\|^2 / \mathrm{tr}(M(\mu)^{-1})^2\big)\xi(x)\); \(\phi_{E_k}(\xi)=\min_{\mu\in\Xi}\sum_x \|P^{(k)}(\mu)f(x)\|^2\,\xi(x)\), where \(P^{(k)}(\mu)\) is the projector onto the span of the \(k\) smallest-eigenvalue eigenvectors of \(M(\mu)\). These lead to LPs maximizing \(t\) subject to \(\sum_x H(\mu,x)\xi(x)\ge t\) for all (iteratively accumulated) \(\mu\) in a cutting-plane scheme.","Using a modified cutting-plane LP method, the paper reports recovering known locally optimal designs for a nonlinear compartmental-type model on a 24,000-point grid, with convergence in tens of iterations (e.g., 64 iterations for local D-optimality and 49 for local \(E_1\)-optimality) and small equivalence-theorem discrepancies (on the order of \(10^{-5}\) to \(10^{-6}\)). For criterion-robust designs in quadratic regression on \([-1,1]^q\) (evaluated on the discrete set \(\{-1,0,1\}^q\)), the computed robust designs match prior literature for \(q=1,2,3\) and are reported for \(q=4\), with total computation time increasing sharply (e.g., up to ~23.5 hours to compute all \(E_k(\mathrm{opt})\) for \(q=4\)). The paper also demonstrates D-optimal designs under a prescribed A-optimality threshold in degree-4 polynomial regression on a 201-point grid, producing designs that interpolate between the unconstrained D-optimal and more A-efficient allocations as the A-threshold increases.","The authors note that the cutting-plane method can have bad convergence properties, and mention that a level method (adding a quadratic programming step) can be used when convergence is problematic. They also note numerical issues for D- and A-optimality when intermediate designs yield ill-conditioned or singular information matrices, suggesting regularization (e.g., \(M(\xi^{(n)})+\gamma I\)) or substituting other positive definite matrices. In extending formulations to nonlinear models (Appendix/Theorem 3), they state that the resulting LP problem is “too complex to be used for experimental design” and that, unlike earlier criteria (E-, c-, G-), a clear statistical interpretation is still missing for the extended D/A/Ek constructions.","The approach assumes a finite (discretized) design space; performance and guarantees can depend heavily on the discretization resolution, and continuous-region designs would require additional treatment. Computational cost can be substantial for higher dimensions and for computing criterion-robust designs (multiple \(E_k\) optimizations plus a larger LP), suggesting scalability limits beyond the reported examples. The paper does not provide reproducible code or detailed solver settings/tolerances beyond high-level descriptions, which may affect practical replication and performance comparisons across LP solvers. Comparisons are mainly against known optimal solutions rather than against alternative modern algorithms (e.g., multiplicative algorithms, coordinate-exchange, SDP formulations for some criteria), so relative efficiency and robustness versus other computational strategies is not fully established.","The authors indicate an interest in extending the response-function-based reformulations to nonlinear models (mirroring the aim in Pázman and Pronzato (2014)), but discuss that doing so for D-, A-, and Ek-criteria is difficult and left largely to the Appendix. They also mention alternative optimization strategies (e.g., the level method) as a remedy when cutting-plane convergence is poor. Beyond this, no detailed, explicit future-work agenda is clearly laid out.","Develop scalable variants of the LP/cutting-plane approach for large \(|\mathcal X|\) and high-dimensional parameterizations, e.g., via column generation, warm starts, or exploiting structure/symmetry to reduce constraints. Extend the framework to continuous design regions with provable discretization error bounds or adaptive refinement of \(\mathcal X\). Provide and benchmark open-source implementations across solvers (MATLAB/R/Python) and compare systematically with multiplicative algorithms, coordinate-exchange methods, and SDP-based formulations where applicable. Investigate robustness to model misspecification and practical settings with correlated/heteroskedastic errors, and develop clearer statistical interpretations and practical guidance for the nonlinear “extended” criteria formulations.",1504.06226v1,https://arxiv.org/pdf/1504.06226v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:43:46Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,Not applicable,"Variable/General (many factors discussed; examples include mpirun calls, synchronization method, window size, compiler/flags, DVFS level, cache state, pinning, message size, #processes)",Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.grid5000.fr|http://software.intel.com/en-us/articles/intel-mpi-benchmarks|http://mvapich.cse.ohio-state.edu/benchmarks/|https://asc.llnl.gov/sequoia/benchmarks/|http://hdl.handle.net/2440/37915|http://amd-dev.wpengine.netdna-cdn.com/wordpress/media/2012/10/24593_APM_v21.pdf|http://www.intel.com/content/www/us/en/architecture-and-technology/64-ia-32-architectures-software-developer-vol-2b-manual.html|http://www.itl.nist.gov/div898/handbook/eda/section3/autocopl.htm|http://doi.acm.org/10.1145/2807591.2807644|http://doi.acm.org/10.1145/2442776.2442781,"The paper revisits MPI benchmarking with an emphasis on sound experimental design and reproducibility of timing results for blocking collective operations. It identifies and empirically demonstrates several experimental factors that materially affect measured run-times (e.g., process/clock synchronization approach, multiple invocations of mpirun, DVFS settings, compiler flags, CPU pinning, and cache warmth), and recommends reporting them as part of benchmark metadata. Methodologically, it proposes a window-based benchmarking framework that requires globally synchronized clocks and introduces a new clock-synchronization algorithm (HCA) that models both clock offset and linear clock drift, trading accuracy against synchronization overhead via hierarchical aggregation. For statistical rigor, it proposes a design that replicates across multiple mpirun calls, randomizes the order of test cases, applies outlier filtering (Tukey rule), and compares MPI implementations using nonparametric hypothesis testing (Wilcoxon rank-sum) over per-mpirun summary statistics. Empirical evaluations across multiple clusters and MPI libraries show that HCA mitigates timing drift issues seen in offset-only synchronization and improves reproducibility compared with common benchmark suites/configurations (e.g., Intel MPI Benchmarks, SKaMPI).","Run-time for a collective is defined either (i) as the maximum local duration across ranks per observation, $t[i]=\max_r\{t_r[i]\}$ (typical with barrier-based synchronization), or (ii) using globally synchronized clocks as $t[i]=\max_r\{e_r[i]\}-\min_r\{s_r[i]\}$. HCA/JK model clock drift between processes as a linear function of time, e.g., $t_{2\to 1}(t_1)=t_1-t_2=s\,t_1+i$, and normalize local times via $t_{\text{norm}}=t-(t\cdot s+i)$. For hierarchical synchronization, linear drift models are merged transitively; for three processes the merged slope/intercept satisfy (Eq. 1) $s_{3\to1}=s_{2\to1}+s_{3\to2}-s_{2\to1}s_{3\to2}$ and $i_{3\to1}=i_{2\to1}+i_{3\to2}-s_{3\to2}i_{2\to1}$.","The authors show that offset-only window-based schemes (SKaMPI/Netgauge) exhibit substantial drift over time, while drift-aware methods (JK and HCA) maintain much smaller offsets; for example, on TUWien with 512 processes, maximum clock offsets for SKaMPI/Netgauge grow to hundreds of microseconds by 20 s, whereas HCA/JK remain far lower over the same horizon (Fig. 7). They demonstrate that MPI_Barrier can introduce large process skew depending on the MPI implementation (e.g., >40 µs skew between ranks for MVAPICH 2.0a-qlc in one test, Fig. 10), which can substantially bias local-time-based run-time estimates. Experimental factors like compiler optimization levels (-O1/-O2/-O3), DVFS settings (e.g., 2.3 GHz vs 0.8 GHz), pinning, and cache warmth can shift collective run-times enough to change which MPI library appears faster (Figs. 21–24). Their proposed multi-mpirun randomized design combined with outlier filtering and Wilcoxon testing yields more stable/reproducible benchmark outcomes than default configurations of Intel MPI Benchmarks and SKaMPI (Fig. 29).",None stated.,"The work focuses on benchmarking methodology for blocking collectives; conclusions may not transfer directly to nonblocking collectives or application-level codes with computation/communication overlap. Many recommendations assume access to dedicated nodes and the ability to pin processes and fix CPU frequency; these controls may be infeasible in shared production environments, limiting reproducibility. The proposed design emphasizes repeated measurements and multiple mpirun runs, which can be time-consuming and may not scale well for very large test matrices (many functions × message sizes × process counts) without an explicit budget/optimal allocation strategy.",None stated.,"Develop an explicit resource-allocation/replication planning strategy (e.g., adaptive stopping rules across mpirun replications) to minimize total benchmark time while achieving target precision, and formalize it as a sequential design. Extend the methodology to settings with autocorrelation, contention, and non-dedicated network/switch resources, including robust/self-starting variants that remain valid under uncontrolled noise. Provide an open-source reference implementation of HCA and the full benchmarking harness (randomization, outlier filtering, statistical tests) to encourage adoption and facilitate independent replication.",1505.07734v5,https://arxiv.org/pdf/1505.07734v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:44:29Z
TRUE,Sequential/adaptive|Bayesian design|Other,Optimization|Prediction,Other,"2 factors (audio complexity, visual complexity; discretized grids 10×10=100 and 19×19=361)",Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,MATLAB|Other,Not provided,http://www.mccauslandcenter.sc.edu/mricrogl/,"The paper proposes the “Automatic Neuroscientist,” a closed-loop framework that uses real-time fMRI feedback to automatically adapt experimental stimulus settings to evoke a predefined target brain state. Two proof-of-principle studies optimize an audio–visual stimulus pair (auditory and visual complexity levels) to differentially drive activity in two ROIs (lateral occipital cortex vs. superior temporal cortex). Study 1 uses a modified Simultaneous Perturbation Stochastic Approximation (SPSA) algorithm to search a discrete 10×10 stimulus grid and stop when the same optimum is selected three consecutive iterations (or after 10 minutes); it converges to the hypothesized optimum in 11/14 runs. Study 2 replaces SPSA with Bayesian optimization using a Gaussian process prior and an expected-improvement acquisition rule over a larger 19×19 grid, enabling rapid mapping of the full parameter space and identifying optima near the hypothesized [10,10] coordinate within short fixed-length runs (19 observations). The work advances experimental design in neuroimaging by shifting from fixed designs to adaptive, optimization-driven stimulus selection for efficient exploration and targeting of brain states.","Study 2 models the unknown objective (BOLD difference between target ROIs) with a Gaussian process prior using a squared-exponential covariance: $k(x,y)=\sigma^2\exp\left(-\|x-y\|^2/(2l^2)\right)$, with additive white-noise variance $\sigma_{\text{noise}}^2$. The next stimulus setting $x_{t+1}$ is chosen by maximizing the expected-improvement acquisition function: $x_{t+1}=\arg\max_x EI(x)$, where $EI(x)=(m(x)-x_{\max})\Phi(z)+\mathrm{var}(x)\phi(z)$ and $z=(m(x)-x_{\max})/\mathrm{var}(x)$ (notation as given in the paper). Study 1 uses SPSA to approximate gradients from two randomly perturbed evaluations per iteration in the discrete stimulus grid (details described verbally).","In Study 1 (10×10 grid), correct convergence to the hypothesized optimum was achieved in 11 of 14 runs, with non-converged/faulty runs capped at 50 iterations for conservative permutation testing; observed mean convergence rate was 16.86 iterations and median was 9, yielding permutation-test z-scores of −2.4035 (p=0.0061, one-tailed) for the mean and −2.1152 (p=0.0134) for the median. In Study 2 (19×19 grid), the mean±SD Euclidean distance between each subject’s estimated optimum and the hypothesized optimum [10,10] was 1.48±0.87 when aggregating available runs per subject. Using only the first run from each of five subjects, the group-level predicted optimum was near the hypothesized center and permutation testing on Euclidean distance gave z=−3.51 (p=0.00015, one-tailed). Each Bayesian-optimization run was fixed at 19 observations (≈5.27% of the 361-state space; 190 TRs ≈ 6.3 minutes).","The authors note Study 1’s SPSA approach provides only limited insight into the global relationship between stimuli and neural response across the parameter space, focusing instead on (possibly local) maxima. They also state SPSA’s per-iteration gradient estimation increases susceptibility to noisy outliers and reduces efficiency in low signal-to-noise scenarios. For Study 2/overall, they acknowledge that the time needed for Bayesian optimization to accurately map the space will vary with the signal-to-noise ratio and may increase for more subtle cognitive distinctions.","The demonstrations are limited to a low-dimensional (2D) discretized stimulus space with relatively strong sensory-evoked responses; performance and practicality in higher-dimensional cognitive task spaces (with longer temporal dependencies and weaker effects) remain uncertain. The Bayesian optimization setup fixes GP hyperparameters using prior data from Study 1, which may not generalize across subjects/scanners/tasks and may benefit from online re-estimation or hierarchical modeling. Real-time optimization depends on rapid, stable preprocessing/GLM estimation and assumes block-wise independence with canonical HRF; deviations (e.g., autocorrelation, motion artifacts, physiological noise, or HRF variability) could bias the objective feedback and acquisition decisions. Code and implementation details for real-time integration (latency, numerical optimization of EI over a discrete grid, failure modes) are not provided, limiting reproducibility.","They propose developing online stopping criteria that automatically end a run once posterior uncertainty over the parameter space is sufficiently small. They suggest extending the target brain state beyond simple ROI BOLD differences to richer objectives such as functional connectivity patterns, including combining Bayesian optimization with their real-time dynamic connectivity method (real-time SINGLE). They also note planned application to higher-level cognitive tasks and that required sampling time will depend on SNR and task complexity.","Extend the framework to continuous (non-discretized) stimulus parameters and higher-dimensional spaces with constraints, using scalable GP approximations or alternative surrogate models. Develop robustness to nonstationary responses (habituation/fatigue) via time-varying objectives or contextual Bayesian optimization that conditions on run/subject state. Provide standardized open-source real-time pipelines and benchmarks (latency, stability, reproducibility) and compare acquisition functions (UCB, Thompson sampling) and priors (Matérn, additive kernels) under realistic fMRI noise and HRF variability. Validate clinical/personalization claims with real patient cohorts and assess test–retest reliability of discovered optima and learned parameter-space maps.",1506.02088v1,https://arxiv.org/pdf/1506.02088v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:45:17Z
TRUE,Computer experiment|Optimal design|Sequential/adaptive|Other,Optimization|Prediction|Other,Other,"Variable/General (example shows 2 tuned parameters: diffusion-rate, evaporation-rate; plus population and stochastic seed/replications)",Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,https://github.com/openmole/gridscale|https://github.com/openmole/yapa|http://www.openmole.org/current/ants.nlogo|https://github.com/openmole/gridscale,"The paper presents OpenMOLE, a Scala-based scientific workflow engine aimed at distributing large-scale computational design of experiments and parameter exploration across heterogeneous HPC environments (SSH servers, clusters, and grids such as EGI). It demonstrates model exploration on a stochastic NetLogo ant foraging simulation, including replication over random seeds and aggregation of outputs via summary statistics (e.g., medians). For parameter tuning, it formulates a multi-objective optimization problem (three objectives: times to deplete three food sources) and solves it using evolutionary algorithms (NSGA-II), including an island-model variant to better exploit grid resources. The main contribution is not a new DOE construction but an executable workflow/DSL and execution middleware that makes large distributed parameter sweeps and optimization experiments reproducible and scalable (e.g., evaluating an initial GA population of 200,000 individuals in ~1 hour on EGI). The work advances practice for computer experiments by lowering deployment/friction (packaging via CARE, zero-deployment) and enabling transparent distributed execution of DOE/optimization workloads.","The experimental exploration is framed as multi-objective optimization: minimize three simulation outputs (ticks to empty food sources 1–3). A stochastic replication factor varies the RNG seed (e.g., 5 draws), and outputs are aggregated via a statistic task computing the median for each objective. The search uses NSGA-II with inputs bounded to ranges (e.g., $gDiffusionRate\in[0,99]$, $gEvaporationRate\in[0,99]$) and a small re-evaluation rate (e.g., 0.01) to reduce over-evaluated individuals; an island GA runs many islands in parallel with time-based termination (e.g., 1 hour per island).","In the showcased large-scale run on the European Grid Infrastructure (EGI), an NSGA-II island-model optimization is configured with 2,000 concurrent islands and a total of 200,000 island evaluations, each island running for about 1 hour (example configuration). The paper states that a GA initialization with a population of 200,000 individuals can be evaluated in approximately one hour on EGI using OpenMOLE’s distribution capabilities. The example produces a saved Pareto frontier (multi-objective trade-off set) for the ant-model calibration problem rather than a single optimum.","The paper notes that using only five replications for a stochastic model is generally unreliable and is chosen solely to reduce execution time in the toy example. It also implies that some distribution approaches (e.g., Docker) are impractical on heterogeneous grids because they assume a Docker engine on every target host, motivating alternative packaging (CARE).","The DOE content is largely about infrastructure/workflow rather than formal experimental design properties; there is no systematic comparison to classical DOE or space-filling designs (e.g., LHS) for exploration quality. The optimization results are demonstration-oriented and do not report robustness across alternative GA hyperparameters, budgets, or different stochastic noise levels/replication counts. The example focuses on a small continuous parameter space (2 main tuned parameters) and may not reflect challenges of higher-dimensional calibration or constrained/structured parameter spaces common in complex simulations.","The authors state that future OpenMOLE releases will integrate a fully functional web user interface for designing workflows, while keeping the DSL as a key component.","Add first-class support for principled computer-experiment design strategies (space-filling designs, Bayesian optimization, surrogate modeling) and guidance on selecting replication counts for stochastic simulations. Provide benchmarking studies comparing exploration efficiency/solution quality across DOE strategies and optimizers under equal computational budgets, and publish reusable example code/workflows as repositories or packages to improve reproducibility.",1506.04182v1,https://arxiv.org/pdf/1506.04182v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:45:46Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (N experimental units on a known pre-intervention network; binary treatment allocation vector Z),Network/cybersecurity|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.newton.ac.uk/programmes/DAE/seminars/090111301.pdf,"The paper proposes model-assisted experimental designs for randomized treatment assignment when outcomes are correlated according to a known pre-intervention network (without interference/SUTVA). It introduces restricted randomization strategies (balanced, unbiased in average degree, and network-optimal) that select allocations minimizing the conditional MSE of the difference-in-means estimator, yielding new network balance criteria based on degree and shared-neighbor structure. The authors provide analytic decompositions of conditional and marginal MSE under a normal-sum model (and a broader model class), showing how imbalance in group sizes, average degree, and within/between-group shared-neighbor overlap drives bias and variance. They prove that the difference-in-means estimator remains design-unbiased under several proposed restricted designs, even if the working model is misspecified. Extensive simulations on synthetic network families show reduced MSE and shorter Fisher randomization intervals compared to complete randomization, with robustness to network and prior misspecification; they also contrast with model-based MLE-optimal designs that can be sensitive to misspecification.","Treatment effect is estimated by the difference-in-means estimator $\hat\tau(Y\mid Z)=\frac{\sum_i Z_iY_i}{\sum_i Z_i}-\frac{\sum_i (1-Z_i)Y_i}{\sum_i (1-Z_i)}$. Under the normal-sum working model, the conditional MSE decomposes as $\mathrm{mse}(\hat\tau\mid Z)=\mu^2\{\delta_N(Z)\}^2+\gamma^2\,\omega(Z)^T\omega(Z)+\sigma^2\,\omega(Z)^TA^TA\omega(Z)$, where $\delta_N(Z)$ is the treated–control difference in average neighborhood size (degree) and $\omega_i(Z)=Z_i/N_1-(1-Z_i)/N_0$. The network variance term expands into within-treated, within-control, and between-group averages of shared neighbors $|\mathcal N_i\cap \mathcal N_j|$, motivating designs that reduce within-group overlap and increase between-group overlap.","Simulations on multiple synthetic network families (Erd\H{o}s–R\'enyi, power law, stochastic block model, and small-world; typically $N=500$ nodes) show that model-assisted restricted randomization reduces the mean squared error of the difference-in-means estimator relative to Bernoulli and balanced complete randomization. Fisher confidence intervals obtained by inverting Fisher exact tests under balanced optimal restricted randomization are shorter than those under balanced complete randomization; the paper reports distributions of percent reductions (example plots show reductions on the order of roughly 0–30% depending on network/setting). Robustness experiments with edge-rewiring perturbations (e.g., 5–10% edge modifications) indicate model-assisted designs remain comparatively stable, while MLE-based model-optimal strategies can degrade substantially under modest misspecification. Theoretical results (Theorem 2 and Corollary 3) establish design-unbiasedness for several restricted designs and show expected variance under balanced+optimal restrictions is no larger than under balanced complete randomization.","The authors limit attention to network-correlated outcomes under SUTVA (no interference), focusing primarily on the conditional MSE of the difference-in-means estimator under the normal-sum model (and a related normal-mean model), with additional results for a model family in the appendix. They note that practical use requires incorporating covariates and specifying/estimating model parameters $(\mu,\sigma^2,\gamma^2)$ (e.g., via priors calibrated from historical/pilot data). They also acknowledge that extending theory to broader classes of estimators/models is feasible but can yield complex, hard-to-interpret MSE expressions and balance criteria.","The proposed “optimal” restricted randomization relies on computing/approximating MSE over a vast allocation space, and the paper’s implementation uses stochastic optimization; performance and reproducibility may depend on tuning, initialization, and computational budget. The working models assume independent latent covariates and a specific way the network induces correlation; real networks may exhibit degree heterogeneity, community structure, and outcome distributions (e.g., heavy tails, zero inflation) not well captured by the Gaussian assumptions, potentially affecting which balance metrics matter most. The approach targets ATE estimation in two-arm trials; extensions to multi-arm, continuous treatments, or clustered/blocking constraints are not developed. Finally, while exact Fisher intervals are discussed, large-scale deployment may face computational constraints due to rerandomization/permutation inference under complex restrictions.","They propose extending the model-assisted design framework to broader classes of estimators satisfying symmetry conditions and to larger model families, despite increasing analytical complexity. They also note the need to incorporate observed covariates and to develop practical approaches for specifying/estimating model parameters (e.g., point priors or full priors and integrated MSE), using historical data or pilot studies. They state they are working on combining insights to design randomization strategies for settings with both network interference and confounding from network correlations.","Develop scalable, provably convergent algorithms (and diagnostics) for approximately sampling from the restricted design distributions with quantified approximation error, especially for large networks. Extend the balance criteria and restricted designs to settings with interference (violations of SUTVA), time-varying/dynamic networks, and weighted/directed networks, and study tradeoffs between bias from interference and variance from correlation. Provide software implementations (e.g., an R/Python package) and standardized benchmarks on real experimental network datasets to validate practical gains. Investigate alternative optimality targets such as minimax (worst-case) MSE over plausible network-correlation models, Bayesian integrated MSE, or objectives tailored to heterogeneous treatment effects and subgroup estimands.",1507.00803v4,https://arxiv.org/pdf/1507.00803v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:46:34Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Other,Not applicable,"Variable/General (units m, agents n; actions per agent depend on application; examples use 1–2 action parameters per agent such as (μ,σ²) or λ, or (λ,λ′))",Service industry|Other,Exact distribution theory|Approximation methods|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops an incentive-compatible experimental design framework for settings where treatments are administered by self-interested agents who can choose among multiple hidden “versions” of their treatment, inducing a non-cooperative game that can invalidate standard DOE assumptions (notably SUTVA’s no-hidden-versions and no-interference). An experiment is modeled as (i) a randomized assignment rule and (ii) a score function used to pick a winner; incentive compatibility is defined as agents having a dominant strategy to play their “natural action,” i.e., the action that maximizes their performance absent competition. The authors give sufficient conditions for incentive-compatible designs based on the existence of an identifying statistic for agent performance and on constructing score functions (often via variance-stabilizing transformations) so that an agent cannot profit by manipulating variance/risk rather than true performance. In the no-interference case they show how to build incentive-compatible—and in some cases more powerful—designs (higher probability the best agent wins), e.g., using reciprocal or square-root transformations tied to the outcome model (Normal high risk/reward; Poisson counts). Under strategic interference they show that standard designs may be non-identifiable and non-incentive-compatible, and propose a more elaborate randomized design (splitting units into groups with cross-exposure structure) that restores identifiability and enables incentive-compatible scoring in a Poisson interference model. The work advances DOE/SPC-style design thinking by integrating mechanism design/game-theoretic constraints directly into the experiment design and analysis.","The experimental design is formalized as D=(ψ,φ) with randomized assignment ψ and a winner-take-all score φ; the winner is \(\hat\tau(Y^{obs})=\arg\max_i \phi_i(Y^{obs})\). Agent performance (quality) is defined without competition as \(\chi(\alpha_i)=\mathbb{E}[Y_u(Z,A)\mid A=\alpha_i\mathbf{1}, Z_u=i]\), with natural action \(A_i^*=\arg\max_{\alpha_i\in A_i}\chi(\alpha_i)\). Incentive compatibility is dominant-strategy alignment: \(\arg\max_{\alpha_i\in A_i} P_i(\alpha_i,A_{-i}\mid D)=A_i^*\), where \(P_i\) is the probability of winning. A key construction uses an identifying statistic \(T\) satisfying \(\sqrt{k}(T-\chi(A))\Rightarrow N(0,\Sigma(A))\) and a transformed score \(\phi_i=f(T_i)\); in the no-interference case variance-stabilizing transformations (e.g., \(f(x)=1/x\) or \(f(x)=2\sqrt{x}\)) can make \(\mathrm{Var}(\phi_i)\) constant and yield incentive compatibility.","They show (Theorem 3.1) sufficient conditions for incentive compatibility using an identifying statistic and a score transformation that makes the (asymptotic) win-probability monotone in the target performance rather than exploitable through variance manipulation. In a Normal high risk/reward example with \(\sigma_i^2=\mu_i^4\), scoring by sample mean is not incentive-compatible, while scoring by negative reciprocal of the sample mean (a variance-stabilizing approach) is incentive-compatible. In Poisson-count settings without interference, the baseline sample-mean score is incentive-compatible, and a square-root (variance-stabilizing) score yields a more powerful design; they provide an inequality showing \(\Phi(\sqrt{2k}(\sqrt{\lambda_1}-\sqrt{\lambda_2})) > \Phi(\sqrt{k}(\lambda_1-\lambda_2)/\sqrt{\lambda_1+\lambda_2})\). A simulation with two blocks (10,000 repetitions) shows higher win probability for the better agent under \(\nu(x)=\sqrt{x}\) vs identity (e.g., with 50 units/block, 0.91 vs 0.85; with 100 units/block, 0.97 vs 0.93). Under a Poisson interference model, they show standard scoring fails (not incentive-compatible and can be non-identifiable), and then propose a redesigned experiment that enables constructing an identifying statistic \(T=BC^{-1}Y\) and an incentive-compatible score based directly on \(T\).","They note the approach often relies on parametric outcome/interference models to obtain identifying statistics, and that experimenters may be unwilling to make such assumptions; they suggest nonparametric or randomization-based approaches but highlight difficulties in their setting because agents strategically choose treatment versions. For interference settings, they acknowledge the need to correctly specify the interference structure and know (or otherwise handle) hyperparameters such as the interference discount \(\gamma\); they suggest treating such parameters as nuisance (e.g., profile likelihood) or using Bayesian priors, but do not fully develop these methods here. They also state they do not focus on general between-unit interference designs (outside the strategic interference focus) and mention it as future work.","The incentive-compatibility guarantees are largely asymptotic (large k) and depend on CLT/Delta-method approximations; finite-sample incentive properties and robustness to misspecification could be materially different in practice. Dominant-strategy incentive compatibility may be too strong or unrealistic in many applied settings (agents may have limited knowledge, bounded rationality, or equilibrium selection issues), and the paper does not empirically validate behavior of real strategic agents. The proposed interference-handling design is tailored to a specific linear/discounted spillover Poisson model; more complex network interference, heterogeneous effects, or unknown interference graphs could break identifiability and the proposed scoring. Practical deployment questions (sample size planning under strategic behavior, sensitivity to blocking/covariate imbalance, and operational constraints/cost) are not treated as a full DOE planning methodology.","They propose focusing in future work on broader forms of between-unit interference (beyond the strategic interference focus treated here). They also indicate plans to investigate nonparametric methods for performance comparisons when parametric modeling assumptions are undesirable. For interference hyperparameters (e.g., \(\gamma\)), they suggest extensions via nuisance-parameter methods (profile likelihood) or Bayesian approaches with priors and posterior predictive scoring.","Develop finite-sample (non-asymptotic) incentive-compatibility and power analyses, including guidance for sample size determination under strategic agents and for robustness to model misspecification. Extend the framework to richer interference settings (general networks, partial interference, cluster randomization) and to multivariate outcomes or multiple competing agents with complex action spaces. Provide implementable algorithms/software for constructing identifying statistics and score transformations in common models, plus benchmarking on real-world ad/marketplace experiments to validate behavioral assumptions and practical performance. Investigate alternative equilibrium notions (e.g., Bayes–Nash, trembling-hand robustness) and design mechanisms resilient to bounded rationality or collusion among agents.",1507.03063v1,https://arxiv.org/pdf/1507.03063v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:47:07Z
TRUE,Screening|Other,Screening|Parameter estimation|Other,Not applicable,7 factors,Manufacturing (general)|Other,Other,TRUE,Other,Not provided,NA,"The paper studies how pulsed-DC PECVD process parameters affect deposition rate, mechanical properties (hardness and Young’s modulus), and residual stress of hydrogenated diamond-like carbon (DLC) coatings deposited on martensitic steel with a Ti buffer layer. A Plackett–Burman (PB) screening design with 12 experimental trials is used to identify significant main effects among seven factors (deposition time, CH4 flux, chamber pressure, power, pulse frequency, substrate roughness, and Ti interlayer thickness), and effects are visualized using Pareto charts. The main findings are that PECVD power is the dominant factor increasing deposition rate and increasing hardness and Young’s modulus, while pulse frequency is the most critical factor for reducing residual stress. Adhesion/peeling behavior is mainly driven by substrate roughness once stress is reduced, and several coatings achieve stable thicknesses well above 0.25 µm (e.g., ~3 µm) with low stress when conditions are optimized. The work advances practical process understanding for DLC/Ti/steel coating deposition by providing a low-run statistical screening approach to prioritize control factors for subsequent optimization.","The PB main-effect estimate for a factor is computed as $E=\frac{\sum y_+ - \sum y_-}{N/2}$ (shown as Eq. (1)), where $y_+$ and $y_-$ are the responses at the high and low factor levels and $N$ is the total number of runs (here, 12). Residual stress is obtained from curvature measurements using Stoney’s equation (Eq. (2)), relating film stress $\sigma$ to substrate modulus, thicknesses, and the change in curvature radii before/after coating. Wear rate is computed from calotte geometry using Archard’s relation (Eq. (3)).","The 12-run PB screening over ranges time 30–60 min, CH4 flux 20–30 sccm, pressure 10–30 Pa, power 50–120 W, frequency 100–250 kHz, substrate RMS 2–6 nm, and Ti thickness 20–100 nm identifies power and pressure as significant drivers of deposition rate (positive effects), with the highest rate 54 nm/min (sample DLC-8) and the lowest 11 nm/min (DLC-1). Hardness ranges from about 11.7 to 18.6 GPa and Young’s modulus from about 83 to 137 GPa; Pareto charts indicate power (positive) and pressure (negative) as the primary contributors to hardness, and power as the strongest positive contributor to modulus. Residual compressive stress spans 0.13–0.64 GPa, and pulse frequency has the strongest effect in reducing stress (per Pareto chart). A thick, well-performing coating example is DLC-8 with ~3240 nm thickness, stress 0.13 GPa, hardness 16.3±0.4 GPa, and modulus 132±6 GPa.",None stated.,"Plackett–Burman designs confound main effects with two-factor interactions, so the reported factor ‘significance’ may partially reflect unmodeled interactions (e.g., power×pressure, frequency×duty cycle). The study uses only two levels per factor and focuses on room-temperature deposition, limiting conclusions about curvature/nonlinearity and temperature-dependent behavior known to affect sp2/sp3 ratio and hydrogen content. Statistical details (e.g., replicate runs, randomization order, estimated experimental error/dummy factors) are not clearly reported, which limits inference robustness and uncertainty quantification for effects.",None stated.,"Follow-on response-surface or factorial experimentation could optimize the key factors (power, pressure, frequency) and quantify interactions and nonlinearities, especially for jointly minimizing stress while maximizing mechanical performance. Extending the DOE to include duty cycle (kept ~20% here) and deposition temperature would broaden applicability and connect process settings to film structure (sp2/sp3, hydrogen content). Providing an implementation (e.g., LabVIEW control scripts and DOE/Pareto analysis code) and adding replicated/center-point runs would improve reproducibility and enable stronger statistical conclusions.",1507.04267v1,https://arxiv.org/pdf/1507.04267v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:47:36Z
TRUE,Response surface|Sequential/adaptive|Computer experiment|Bayesian design|Other,Model discrimination|Prediction|Cost reduction|Other,Other,Variable/General (input space X ⊂ R^d; examples include d=1 and d=2; also discrete 2-D state space in epidemics case study),Theoretical/simulation only|Healthcare/medical|Other,Simulation study|Other,TRUE,R,Not provided,NA,"The paper proposes Bayesian sequential design (adaptive sampling) methods to learn, over a continuous input space, which of L unknown noisy response surfaces is minimal at each x (a global ranking/classification problem). It models each surface with independent Gaussian process/kriging metamodels (including stochastic kriging for noisy simulators) and defines a loss that penalizes choosing a non-minimal surface proportionally to the regret integrated over x. To choose the next experiment (x, ℓ) in the joint design space X×{1,…,L}, it develops acquisition rules including Gap-UCB (bandit-inspired exploration–exploitation using posterior mean gaps and kriging variance) and Gap-SUR (stepwise uncertainty reduction targeting expected reduction in an empirical Bayesian loss/M-gap). The methods are evaluated via extensive Monte Carlo experiments on synthetic 1-D and 2-D examples and a case study on epidemic intervention in a stochastic SIR model, showing that adaptive designs concentrate samples near decision boundaries and can substantially reduce loss versus non-adaptive space-filling or uniform sampling. The work connects DOE/active learning and Bayesian optimization/contour-finding with multi-armed bandits by treating each (x,ℓ) query as an adaptive design decision under uncertainty.","Objective is to learn the classifier $C(x)=\arg\min_{\ell\in\{1,\dots,L\}}\mu_\ell(x)$ from noisy simulator outputs $Y_\ell(x)=\mu_\ell(x)+\varepsilon_\ell(x)$. The paper’s loss is an integrated regret $\mathcal{L}(\hat C,C)=\int_X \{\mu_{\hat C(x)}(x)-\mu_{C(x)}(x)\}\,F(dx)$, and under GP posteriors it uses the M-gap $M(x)=\hat\mu_{(1)}(x)-\mathbb{E}[\min_\ell M_\ell(x)\mid\mathcal{F}_k]$ with empirical loss $EL=\int_X M(x)F(dx)$. Key acquisition functions are Gap-UCB $E^{\text{Gap-UCB}}_k(x,\ell)=-\widehat\Delta_\ell(x)+\gamma_k\,\delta_\ell(x)$ and Gap-SUR $E^{\text{Gap-SUR}}_k(x,\ell)=\mathbb{E}[M^{(k)}(x)-M^{(k+1)}(x)\mid x_{k+1}=x,\ell_{k+1}=\ell,\mathcal{F}_k]$, with GP/kriging posterior mean/variance given by standard formulas (e.g., $(K+\Sigma)^{-1}$ updates).","In the 1-D toy example (L=2), Gap-SUR achieved substantially lower empirical loss than uniform sampling or non-adaptive LHS at a fixed budget (e.g., at K=200, empirical loss ≈ 0.96e−3 for Gap-SUR vs 2.89e−3 uniform and 2.16e−3 LHS), and it adaptively allocated more samples to the noisier surface (e.g., 146/200 from Y1). In the 2-D, L=5 example at K=500, adaptive methods (Gap-SUR/Gap-UCB) reduced both empirical and true loss relative to non-adaptive approaches, and the design concentrated samples along pairwise decision boundaries while allocating unevenly across surfaces (e.g., D≈(126,101,94,70,109) for Gap-SUR). In the epidemic management case study, the sequential design (with batching for variance estimation) learned an intervention/no-intervention boundary over a large discrete state space using K=200 design sites with r=100 replications per site, coping with strongly heteroscedastic noise. Across experiments, adaptive X×L designs demonstrated “double efficiency”: focusing spatially near classification boundaries and selectively sampling only the most relevant/uncertain surfaces at those locations.","The authors note that Gap-UCB performance is sensitive to the tuning schedule for the exploration parameter $\gamma_k$; poor choices can lead to overly aggressive exploitation or nearly space-filling behavior, and guidance on choosing $\gamma_k$ in black-box settings is left for future research. They also note that Gap-SUR requires knowing or estimating the observation noise variances $\sigma^2_\ell(x)$ when optimizing the acquisition function; in practice (e.g., the epidemic example) this necessitates batching/replication to estimate heteroscedastic noise. Finally, they remark that GP training/re-training can be computationally expensive and introduces extra variability, and stationary GP models may be misspecified for some problems.","The methodology assumes independent response surfaces across indices \ell (diagonal posterior covariance across arms at fixed x); in many multi-action simulators, cross-arm correlation (common random numbers, shared latent randomness) could be exploited for greater efficiency but is not modeled. The acquisition functions rely on GP/kriging adequacy (smoothness, kernel choice, approximate normality of Monte Carlo noise), so performance may degrade under strong nonstationarity, discontinuities, heavy tails, or dependent noise without robustification. Candidate-set optimization via LHS grids may become challenging in higher dimensions (large d) due to curse of dimensionality, potentially requiring more sophisticated optimizers or scalable GP approximations. Comparisons focus on a set of heuristics; broader benchmarking against modern multi-fidelity Bayesian optimization, correlated multi-output GPs, or best-arm identification methods with explicit confidence guarantees is limited.","They explicitly suggest developing better guidance and theory for selecting/tuning the UCB exploration schedule $\gamma_k$ and related randomized exploration strategies. They propose extending the framework to a fixed-confidence (adaptive stopping) setting with principled termination criteria tied to confidence/expected information gain rather than a fixed budget. They also discuss exploring alternative metamodels beyond stationary GPs (e.g., Loess, piecewise linear regression, treed GPs) and potentially using a cheaper surrogate during DOE and a richer model for the final classifier estimate.","A natural extension is to use multi-output/cokriging models to capture correlation across response indices \ell (including common random numbers), enabling more informative sampling and potentially reducing budget. Developing theoretical guarantees (consistency rates, regret bounds) for the proposed continuous X×L ranking objective under the specific loss (integrated regret) would strengthen the framework beyond heuristic justification. Scalability improvements—e.g., sparse/local GP updates, inducing points, or Bayesian neural surrogates—would help apply the approach to higher-dimensional state spaces common in stochastic control. More real-world case studies and open-source implementations would improve reproducibility and provide practical guidance on hyperparameter learning, batching choices for heteroscedastic noise, and candidate-set/optimizer design.",1509.00980v2,https://arxiv.org/pdf/1509.00980v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:48:30Z
TRUE,Computer experiment|Sequential/adaptive|Other,Prediction|Cost reduction|Other,Space-filling|Minimax/Maximin|Other,"Variable/General (examples in 1D, 2D, 3D, 5D state dimension; factors are state variables in the design space)",Finance/economics,Simulation study,TRUE,R,Not provided,https://arxiv.org/abs/1509.02179,"The paper proposes improving Regression Monte Carlo (RMC) for Bermudan/American option pricing by (i) replacing basis-function least squares regression with stochastic kriging (Gaussian process) metamodels for continuation value estimation, and (ii) treating the selection of simulation design points (stochastic grids) as a Design of Experiments (DOE) problem. It evaluates multiple DOE strategies—including space-filling designs (Latin hypercube sampling, Sobol and Halton low-discrepancy sequences, gridded designs), probabilistic density-based designs, and sequential/adaptive designs guided by expected-improvement/stepwise-uncertainty-reduction criteria—to concentrate simulation effort near the stopping boundary. The paper also introduces batching/replication at design sites to reduce simulation noise and enable estimation of heteroscedastic sampling variance for stochastic kriging. Through simulation case studies on Bermudan Puts and Max-Calls under GBM and stochastic volatility dynamics in dimensions up to 5, the methods reduce required simulation budgets (and associated memory) substantially relative to standard LSMC approaches, sometimes by up to an order of magnitude in path counts, though with increased regression overhead. Overall, it advances optimal stopping/RMC by integrating modern surrogate modeling and DOE principles to better allocate simulation effort where it matters for exercise decisions.","The DOE/metamodeling problem is formulated as observing noisy simulator outputs $Y(x)\sim f(x)+\varepsilon(x)$ with $\mathbb{E}[\varepsilon(x)^2]=\sigma^2(x)$ (Eq. 2.15), where in RMC $Y(x)$ is the simulated pathwise payoff and $f(x)=C(t,x)$ is the continuation value. Stochastic kriging gives posterior mean/variance $m(x)=k(x)^T(K+\Sigma)^{-1}y$ and $v(x,x')=K(x,x')-k(x)^T(K+\Sigma)^{-1}k(x')$ (Eqs. 3.5–3.6), with batching estimating $\bar y(x)$ and $\hat\sigma^2(x)$ (Eq. 3.7). Sequential DOE adds points by maximizing an acquisition/EI function (Eq. 5.1), including zero-contour/SUR-style criteria targeting the stopping boundary via the local loss based on $|m(x)-h(t,x)|$ and $v(x)$ (Eqs. 3.8, 5.6).","In benchmark studies, space-filling and sequential DOE combined with kriging achieved option value estimates comparable to standard LSMC using much larger path counts, e.g., for a 2D basket put with total budget $|D_t|=3000$, several DOE variants produced $\hat V(0,X_0)$ around 1.44–1.46 (Table 1), comparable to LSMC-BW11 with $N=50{,}000$ giving about 1.452. For a 3D max-call with $|D_t|=16{,}000$, DOE+kriging produced $\hat V(0,X_0)$ around 11.11–11.18 (Table 2), comparable to LSMC-BW11 with $N=300{,}000$ giving about 11.12. In higher dimensions (max-call d=2,3,5), kriging with Sobol or SUR used far fewer trajectories than LSMC (e.g., d=5: $N=32{,}000$ vs $N=640{,}000$) while producing similar prices (~16.30–16.32), but incurred substantial runtime overhead from GP fitting (Table 4). Kernel family choice (Matérn vs Gaussian) had negligible impact on accuracy once hyperparameters were fit (Table 3).","The authors note that kriging introduces substantial computational overhead, with GP training/prediction (scaling poorly with macro-design size) often dominating runtime and sometimes making time-savings negative even when simulation budgets drop. They emphasize sensitivity of overall performance to batching size $M$ and the resulting macro-design size $N_0=N/M$, and that sequential design can be expensive in overhead with only marginal additional savings over good static space-filling designs. They also acknowledge that selecting an appropriate bounded design region $\tilde X$ for space-filling designs can be difficult in unbounded state spaces and non-convex/disconnected stopping regions.","The DOE comparisons largely focus on pricing accuracy and simulation budget, but provide limited systematic sensitivity analysis over design-region specification $\tilde X$, acquisition-function parameters, and batching allocation across time steps (which can materially affect robustness in practice). The sequential DOE approach is evaluated with a candidate-set approximation (argmax over LHS candidates), but the impact of candidate-set size and optimization fidelity on convergence and reproducibility is not deeply explored. The method relies on independent-noise assumptions for kriging (diagonal $\Sigma$), which may be violated if common random numbers or correlated path generation is used; guidance for such cases is not developed.","The paper suggests exploring classification-style approaches that model the stopping set directly (e.g., converting payoffs into labels and fitting a probit/GP classifier) instead of regressing continuation values. It also proposes exploiting strong correlations across time steps for warm starts in both experimental design and kriging hyperparameter training, as well as varying design sizes $N_t$ and shrinking design domains $\tilde X_t$ over backward induction. Additional directions include integrating importance sampling ideas to concentrate designs near the stopping boundary and extending kriging-based derivative estimation (Greeks) for American-style options.","Develop scalable GP approximations (sparse/inducing-point, local GP, multi-resolution) tailored to batched RMC to reduce the cubic training cost and make sequential DOE practical in higher dimensions. Extend DOE policies to allocate batch sizes adaptively across design sites and time steps (e.g., targeted replication to equalize posterior misclassification risk near the boundary). Broaden empirical validation to autocorrelated simulators, alternative market models, and real calibration settings, and benchmark against modern machine-learning regressors (e.g., random forests, neural nets) paired with the same DOE strategies to isolate design vs model benefits.",1509.02179v3,https://arxiv.org/pdf/1509.02179v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:49:21Z
TRUE,Optimal design|Computer experiment|Bayesian design|Sequential/adaptive|Other,Model discrimination|Prediction|Parameter estimation|Cost reduction|Other,Other,Variable/General (examples include d=2 and d=6; batch sizes of 1 or 4 points; initial 4 or 64 points in examples),Energy/utilities|Transportation/logistics|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|None / Not applicable,Not provided,NA,"The paper proposes a Bayesian optimal experimental design approach for selecting expensive computer-model evaluation points to build Gaussian process (GP/kriging) surrogates targeted at failure detection and failure probability estimation. It recasts failure detection as Bayesian inference of the limit-state boundary (the contour g(x)=0) and chooses design points by maximizing expected information gain, using the Kullback–Leibler divergence from prior to posterior as the utility. The resulting expected-utility optimization is solved with a derivative-free stochastic optimizer (SPSA) and evaluated via Monte Carlo, with an entropy term approximated using a resubstitution density-estimation method (Gaussian mixtures) to avoid nested Monte Carlo. The method supports batch selection of multiple points per iteration, enabling parallel simulations, and is used in a greedy sequential (closed-loop) design where the prior is updated stage by stage. Numerical studies (Branin, four-branch reliability benchmark, and a clamped-beam dynamics example) show the approach concentrates samples near the failure boundary and yields accurate failure probability estimates with relatively few model evaluations, often outperforming stepwise uncertainty reduction (SUR) on the reported metrics.","Failure is defined by a limit-state function g(x) with event {g(x)<0} and failure probability P=\int I_{g(x)}p(x)dx, estimated by Monte Carlo \hat P=\frac{1}{n}\sum_{i=1}^n I_{g(x_i)}. The GP surrogate assumes g(x)=\mu(x)+\varepsilon(x) with kernel K, giving posterior predictions y|D,X^*,y^*\sim\mathcal N(u,C) where u_j=\mu(x_j)+r_j^T R^{-1}(y^*-\mu) and C_{jj'}=K(x_j,x_{j'})-r_j^T R^{-1}r_{j'}. Experimental design selects sampling locations D by maximizing expected utility U(D)=\mathbb E_{y|D}[\mathrm{KL}(p(\cdot|y,D)\|p(\cdot))], i.e., expected information gain based on Kullback–Leibler divergence.","On the rescaled Branin example (2D, uniform inputs), starting from 4 Latin-hypercube points and adding 9 sequentially, the proposed limit-state inference (LSI) design yields lower mis-detection probability and lower failure-probability estimation error than SUR in the plotted results. On the four-branch system (2D, standard normal inputs), a sequential batch design selecting 4 points/iteration terminates after 13 iterations (62 total points) and produces a final failure probability estimate 2.31\times10^{-3} versus 2.34\times10^{-3} from direct Monte Carlo with 10^5 samples, with 19/10,000 classification errors (11 false positives, 8 false negatives). On the clamped-beam dynamics problem (6 random inputs), using 64 initial points and then 4 points/iteration for 25 iterations (164 total points), the final failure probability estimate matches the reported Monte Carlo reference 3.35\times10^{-3}, with 22 misclassifications (12 false negatives and 10 false positives) on the evaluation sample set.","The authors note that, like other GP-based methods, the approach requires choices of covariance kernels, prior mean functions, and hyperparameters, and they do not provide a fully developed strategy specialized for failure probability estimation. They also acknowledge that their sequential strategy uses a greedy procedure that is generally only sub-optimal. They further state that a comprehensive comparison and detailed analysis versus SUR remains to be done.","The method’s performance depends on GP modeling assumptions (e.g., smoothness implied by the kernel and correct specification/estimation of hyperparameters), and robustness to model misspecification, discontinuities, or nonstationarity near failure boundaries is not systematically studied. The information-gain objective requires Monte Carlo and density/entropy estimation; the accuracy–cost tradeoff and potential bias/variance from Gaussian-mixture resubstitution entropy estimation are not thoroughly quantified. Practical guidance on constraints, scaling, and handling high-dimensional inputs (beyond using SPSA) is limited, and no public implementation is provided to assess computational overhead and reproducibility. Real-world validation is limited to a simulated mechanics example rather than industrial field data with measurement noise and model discrepancy.","They plan to develop effective ways to choose kernels, prior mean functions, and hyperparameters tailored to failure probability estimation. They propose more comprehensive comparisons with SUR, including detailed analysis of advantages and limitations. They also suggest replacing the greedy sequential strategy with a dynamic-programming approach and exploring hybrid surrogates combining GP with polynomial chaos. Finally, they propose applying the method to other tasks such as approximating feasible regions in constrained optimization.","Extending the design to account explicitly for observation noise, model discrepancy, and uncertain GP hyperparameters (fully Bayesian GP with integrated hyperparameters) would improve robustness for real engineering workflows. Developing scalable approximations (e.g., sparse GP, inducing points) and principled batch/parallel acquisition strategies with theoretical guarantees would help in higher dimensions and larger batch sizes. Benchmarking against a broader set of reliability-oriented acquisition functions (e.g., AK-MCS, expected feasibility/contour improvement, entropy-search variants) on standardized test suites would clarify when the KLD criterion is preferable. Providing open-source software and practical defaults (initial design size, stopping rules, computational budgets) would aid adoption and reproducibility.",1509.04613v1,https://arxiv.org/pdf/1509.04613v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:50:06Z
TRUE,Computer experiment|Optimal design|Screening|Other,Prediction|Screening|Cost reduction|Other,Space-filling|Not applicable,Variable/General (d-dimensional parameter space; examples include d=5),Theoretical/simulation only|Other,Exact distribution theory|Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper develops combinatorial counting arguments to derive the expected fraction of a discretized d-dimensional parameter space covered by the union of k Latin Hypercube Sampling (LHS) trials and (when n=p^d) Orthogonal Sampling (OS) trials. It proves that the expected coverage for LHS and OS is the same, asymptotically of the form P(k,n) \approx 1-\exp(-k\lambda) with \lambda=1/n^{d-1}, and extends the analysis to coverage after projecting onto lower-dimensional subspaces (motivated by experimental design and factor screening). For 2D projections (t=2), it shows analogous coverage with \lambda=1/n (and more generally \lambda=1/n^{t-1}), implying dependence on the projected dimension t rather than the full dimension d. Theoretical bounds are provided using inclusion–exclusion and inequalities to control approximation errors. Monte Carlo simulations (implemented in MATLAB) confirm the theoretical coverage relationships and illustrate scaling for d=5 with projections to t=2,3,4.","Coverage is computed by inclusion–exclusion: U(k,n)=\sum_{m=1}^k (-1)^{m+1}\binom{k}{m} x_m(n), with expected coverage fraction P(k,n)=U(k,n)/n^d. For LHS, the expected m-way intersection of d-tuples is x_m(n)=n^d \prod_{i=0}^{m-1} \frac{a+i}{b+i} with a=(n-1)!^{\,d-1}, b=(n!)^{\,d-1} (analogous form for OS when n=p^d). Asymptotically the expected coverage is P(k,n)\sim 1-\exp(-k\lambda), with \lambda=a/b=1/n^{d-1}; for 2D projection (t=2), \lambda=1/n (and more generally \lambda=1/n^{t-1}).","The expected percentage coverage of the full d-dimensional grid by k trials is asymptotically P(k,n)\approx 1-\exp(-k/n^{d-1}) for both LHS and OS (OS requiring n=p^d), proving they are equivalent in expected coverage. For projected subspaces of dimension t (explicitly t=2, with extension mentioned for general t), coverage follows P\approx 1-\exp(-k/n^{t-1}), and is independent of the ambient dimension d. The paper further derives exact/closed-form expressions for expected m-way intersection sizes (for LHS, OS, and for 2D edge/projection intersections) that plug into inclusion–exclusion to estimate coverage. MATLAB simulation studies (e.g., d=5 with projections to t=2,3,4 and multiple target coverage levels) support the theoretical scaling and approximations.",None stated.,"Results concern expected coverage of a discretized parameter grid (cells/tuples) under idealized random LHS/OS constructions; they do not address discrepancy, maximin distances, or other uniformity criteria beyond coverage, so two designs with equal expected coverage may differ materially in space-filling quality. Orthogonal-sampling theory is restricted to the case n=p^d (linked to orthogonal arrays), limiting direct applicability when n is arbitrary. Simulations are reported as confirmation but details such as number of Monte Carlo replications, variance/CI of estimates, and sensitivity to randomization choices are not fully specified in the excerpt, which limits reproducibility and strength of empirical comparisons.","The authors note (in discussion) that while they only present the analysis for t=2 projections, the results can be extended to arbitrary projected dimension t, yielding \lambda=1/n^{t-1}.","Provide practical guidance for choosing k and n under computational budgets, including uncertainty quantification on coverage estimates (e.g., variance and confidence intervals) and not just expectations. Extend the theory beyond grid-based coverage to continuous-space measures used in computer experiments (e.g., discrepancy, maximin distance) and study how OS vs LHS compares under those criteria. Develop and release reference implementations (e.g., MATLAB/R/Python) to reproduce simulations and enable practitioners to apply the coverage formulas in real experimental-design workflows, including cases with constraints or non-rectangular design regions.",1510.03502v1,https://arxiv.org/pdf/1510.03502v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:50:38Z
TRUE,Factorial (full)|Factorial (fractional)|Screening|Supersaturated|Definitive screening|Computer experiment|Sequential/adaptive|Other,Screening|Parameter estimation|Prediction|Cost reduction|Other,D-optimal|Bayesian D-optimal|Space-filling|Minimax/Maximin|Other,"Variable/General (examples use d=20; discusses very large d, including d>n for supersaturated designs)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Not provided,http://CRAN.R-project.org/package=SLHD|http://CRAN.R-project.org/package=MaxPro|http://CRAN.R-project.org/package=DiceDesign|http://CRAN.R-project.org/package=sensitivity|http://CRAN.R-project.org/package=flare,"This paper reviews and compares design-of-experiments strategies for screening influential factors, covering both physical experiments and computer experiments on numerical models. It surveys two-level factorial and fractional factorial designs (regular and nonregular), Plackett–Burman designs, supersaturated designs, systematic fractional replicate designs, and group-screening approaches (factorial group screening and sequential bifurcation), as well as space-filling designs such as Latin hypercube sampling and designs for Morris elementary effects. It also summarizes common analysis/modeling approaches for screening, including linear-model variable selection under complex aliasing and Gaussian-process-based variable selection. A comparative study demonstrates six screening methods on two 20-factor test functions, reporting sensitivity, false discovery rate, and type I error; results emphasize that very small experiments can be unreliable for nonlinear functions and that methods like elementary effects can be robust, while linear-model-focused designs (e.g., SSD/DSD) may underperform when the true response is nonlinear. Practical guidance is provided on tradeoffs among run size, aliasing, sparsity assumptions, and the intended fidelity of the surrogate model.","The paper frames screening via surrogate models $Y(x)=h^T(x)\beta+\varepsilon(x)$, with two-level main-effects model $\beta_0+\sum_{i=1}^d\beta_i x_i$ and an extended model including two-factor interactions $\sum_{i<j}\beta_{ij}x_ix_j$. It characterizes bias from omitted effects using the alias matrix $A=(H^TH)^{-1}H^T\tilde H$, yielding $\mathbb{E}(\hat\beta)=\beta+A\tilde\beta$. For Gaussian-process screening it uses product correlation $\mathrm{cor}(x,x')=\prod_{i=1}^d\exp(-\theta_i|x_i-x'_i|^{\alpha_i})$. Model-free screening is based on Morris elementary effects $EE_i(x)=\{Y(x+\Delta e_i)-Y(x)\}/\Delta$ and summary indices (mean/SD and mean absolute effect).","In the comparative study on two deterministic 20-factor test functions, Gaussian-process variable selection methods (SGPVS and RDVS) were ineffective at very small $n=16$ but achieved high sensitivity by $n\ge 84$ (often reaching sensitivity 1.0 with zero type I error/false discovery at $n=84$ or $200$, depending on method/example). The elementary-effects (Morris) approach improved with more trajectories/runs, achieving sensitivity 1.0 with zero type I error/false discovery for the more effect-strong example by $n=84$ and $n=210$. Linear-model-oriented screening with a supersaturated design (SSD, $n=16$) and a definitive screening design (DSD, $n=41$) could miss active variables for nonlinear responses and, in one example, SSD showed very high type I error (reported $\phi_I=0.90$) despite moderate sensitivity. The authors conclude that for $d=20$ and the six methods studied, $n\approx 40$ was a minimum for effective screening, with larger $n$ needed when factor sparsity is weaker.","The authors note that many reviewed factorial screening approaches rely on assumptions such as effect sparsity, effect hierarchy, and (for main-effects screening) strong effect heredity; if these do not hold, active variables may be missed. They also highlight that surrogate-model misspecification (e.g., using an overly simple linear model for highly nonlinear computer models) can be a primary source of screening errors. They further indicate that estimating Gaussian-process correlation parameters reliably for many variables can require large experiments and may be computationally impractical.","As a review plus illustrative comparison, the head-to-head study uses only two deterministic 20-factor test functions; conclusions about relative method performance may not generalize to noisy/stochastic settings, different nonlinearity structures, or correlated inputs. Several methods (e.g., EE vs GP vs SSD/DSD) are compared with different tuning/decision rules (visual thresholds, percentiles, AICc), which can affect fairness and reproducibility of performance claims. The paper cites R packages and acknowledges code providers, but without shared scripts it is difficult to replicate the exact designs (e.g., specific maximin/maxpro solutions) and analysis choices.","The paper explicitly notes that the choice of design for Gaussian-process-based screening is an area for further research, motivated by observed sensitivity of SGPVS performance to smoothness settings and design size. It also points to further research on applying screening to extensions of Gaussian-process models for high-dimensional inputs (citing work on high-dimensional GP extensions). The authors suggest that sequential screening strategies (small initial screening followed by targeted follow-up and GP modeling) are likely to be more effective when feasible.","Developing principled, unified tuning/threshold selection (across EE, group screening, shrinkage methods, and GP screening) that targets user-specified tradeoffs between sensitivity and false discoveries would improve comparability and practice. Extending the comparative evaluation to stochastic simulators/physical experiments with noise, including replication and autocorrelation, would clarify robustness of screening recommendations. Providing open-source reference implementations (including design generation and analysis pipelines) and benchmark suites beyond two functions would support reproducible, standardized screening-method comparisons.",1510.05248v1,https://arxiv.org/pdf/1510.05248v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:51:19Z
TRUE,Optimal design|Computer experiment|Other,Parameter estimation|Model discrimination|Prediction|Other,D-optimal|A-optimal|E-optimal|G-optimal|I-optimal (IV-optimal)|V-optimal|Compound criterion|Other,"Variable/General (design length N and HRF parameter count K; examples use K=9 and N=151/132; also two-stimulus designs u∈{0,1,2}^N)",Healthcare/medical,Exact distribution theory|Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper develops an analytical theory for constructing and proving optimal event-related fMRI stimulus sequences (experimental designs) for (i) estimating a hemodynamic response function (HRF) and (ii) comparing HRFs from two stimulus types. The authors connect the fMRI design matrices (formed from lagged stimulus indicators) to circulant biased weighing designs (spring/chemical balance weighing designs), which enables use of established optimal-design results. They prove that Hadamard sequences (including binary m-sequences as special cases) are optimal under broad classes of criteria—covering A- and D-optimality and extending to universal optimality/type-1 criteria—when the run length N is large enough relative to the number of HRF parameters K, and they provide explicit lower bounds (e.g., via a cubic root condition) ensuring A-optimality. For other run lengths (N=4t and N=4t+1), they derive new construction methods by inserting one or two zeros into runs of zeros in a Hadamard sequence to obtain universally optimal or type-1-optimal designs. The work expands the catalog of provably optimal fMRI designs across many practical experiment lengths and provides benchmark designs for evaluating computationally generated alternatives.","The single-stimulus fMRI model is $y=\gamma \mathbf{1}_N+X_d h+\varepsilon$ with $X_d=[d,Ud,\dots,U^{K-1}d]$ and shift matrix $U$ (Eq. 2.3). The (bias-adjusted) information matrix for $h$ is $M_b(X_d)=X_d^\top (I_N-N^{-1}J_N)X_d$, and optimality is studied under criteria such as $\Phi_A(M)=\mathrm{tr}(M^{-1})/K$ and $\Phi_D(M)=|M|^{-1/K}$. For two stimulus types, the contrast-of-HRFs model yields an information matrix for $\zeta=h_1-h_2$ given by $M_u=F_u^\top (I_N-\omega\{[\mathbf{1}_N,E_u]\})F_u$, with an upper bound $M_u\preceq X_{\bar d_u}^\top (I_N-N^{-1}J_N)X_{\bar d_u}/4$ where $\bar d_u\in\{-1,0,1\}^N$ recodes stimulus labels (Lemma 3.1).","For $N=4t-1$, designs achieving $M_b(X_{\tilde d^*})=(N+1)(I_K-N^{-1}J_K)$ are shown to be $\Phi_p$-optimal over $\tilde D=\{-1,1\}^N$ for $p\in[0,p_0]$ when $N$ exceeds an $N_0(K,p_0)$ (Theorem 2.1); an explicit sufficient bound for A-optimality ($p\le1$) is given by requiring $N\ge N_0(K,1)$, the largest real root of $c(x)=2x^3+(10-7K)x^2+2(2K-5)(K-1)x+4K^2-7K$ (Theorem 2.2). Hadamard sequences (and hence binary m-sequences for $N=2^r-1$) satisfy the above information-matrix form and are therefore A- and D-optimal for estimating HRF when $K\ge4$ and $N\ge N_0(K,1)$ (Corollary 2.1). For $N=4t$, certain circulant partial Hadamard constructions yield universal optimality (Theorem 2.3), and for $N=4t+1$ inserting two zeros into a run in a Hadamard sequence yields designs optimal for all type-1 criteria (Theorem 2.4/Lemma 2.5). Parallel optimality results are extended to comparing two HRFs (Theorems 3.1–3.4), including A-optimality over $\{-1,0,1\}^N$ under the same $N_0(K,1)$ condition (Theorem 3.2).","They note the analytical results primarily cover design lengths of the form $N=4t-1$, $4t$, and $4t+1$, and explicitly identify the absence of analogous optimal-design characterizations for $N=4t+2$ as a current gap. They also state that although results remain true for a specific expanded covariance form (a rank-1/mean component adjustment), other correlation structures such as autoregressive errors are not handled and are left for future study. They acknowledge that the analysis simplifies by not incorporating additional nuisance terms (e.g., trend/drift regressors) in the design derivations, even though such terms may be used at analysis time.","Optimality results rely heavily on the specific linear convolutional HRF model with fixed truncation K and on circulant/periodic assumptions (e.g., last K−1 elements repeated in a pre-scan period), which may not match common fMRI practices (e.g., variable ITIs, non-circulant boundaries). The theory largely treats single-subject/voxel-level inference with independent (or very restricted) noise; real fMRI data often exhibit strong temporal autocorrelation and spatial dependence that can materially change optimality rankings. The paper emphasizes theoretical optimality forms of the information matrix but provides limited empirical validation on realistic fMRI pipelines (prewhitening, high-pass filtering, nuisance regressors), so practical performance under standard preprocessing is not established. Implementation practicality is not fully addressed (e.g., constraints on stimulus timing, refractory periods, counterbalancing, multiple conditions beyond two) that can restrict admissible sequences.","They propose extending the theory to identify optimal fMRI designs when the design length is $N=4t+2$. They also suggest analytically studying optimal designs under other error correlation structures, specifically mentioning autoregressive processes. Finally, they note interest in extending results to experiments with a greater number of stimulus types beyond the one- and two-type settings treated in the paper.","Develop self-starting/robust variants that remain near-optimal when K is misspecified or when HRF shape varies across voxels/subjects, potentially via Bayesian or maximin criteria over HRF families. Extend the circulant-biased-weighing-design approach to constrained design spaces reflecting real fMRI protocol constraints (minimum ITI, jitter distributions, block/event hybrids) and quantify efficiency loss relative to the unconstrained optima. Provide software implementations and reproducible benchmarks (e.g., an R/Python package to generate Hadamard/insert-zero constructions and compute efficiencies under common prewhitening/filtering models). Validate designs on realistic simulation frameworks and real datasets incorporating autocorrelation, motion regressors, and multiple nuisance bases to assess robustness of theoretical optimality to standard preprocessing and model-misspecification.",1510.08661v1,https://arxiv.org/pdf/1510.08661v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:52:00Z
TRUE,Computer experiment|Other,Prediction|Other,Space-filling|Minimax/Maximin,11 factors,Energy/utilities|Other,Case study (real dataset)|Simulation study|Other,TRUE,Other,Not provided,http://www.salome-platform.org,"The paper studies how statistical metamodels can support large computer-based designs of experiments (DOE) in nuclear engineering by improving the robustness and reliability of automated code runs. Using the Germinal V1 thermomechanical fuel-pin simulation code, the authors build a DOE over an 11-dimensional hypercube and fit three surrogate models—Kriging (Gaussian process regression with a nugget), kernel regression (RKHS/Sobolev kernel with GCV regularization), and a neural network (MLP). The DOE inputs are generated by an LHS-Maximin space-filling design and filtered for infeasible configurations; the learning base has 3807 runs (later 3791 after postprocessor updates) and an independent test base has 1613 runs (later 1606). Beyond prediction accuracy comparisons, the metamodels are used to detect unflagged simulation failures (outliers) and to diagnose “code instabilities” arising from preprocessing/meshing artifacts; the Kriging nugget estimate is highlighted as a quantitative indicator of instability that decreases after preprocessing fixes. Overall, Kriging yields the best predictive accuracy and provides uncertainty quantification tools (predictive variance/confidence intervals) that help identify unreliable simulations and motivate improvements to the code manager (pre/post-processing).","DOE generation uses an LHS-Maximin space-filling design over a hypercubic domain for 11 inputs, then removes infeasible points. The Kriging surrogate is defined by the GP predictor $\hat f(x)=r(x)^\top R^{-1}y$ with predictive variance $\hat\sigma^2_{code}(x)=\hat\sigma^2\,(1+\hat\alpha-r(x)^\top R^{-1}r(x))$, using a Mat\'ern $3/2$ correlation and a nugget term to model small-scale variability/instability. Kernel regression uses the same linear-algebraic form $\hat f_\lambda(x)=r(x)^\top R_\lambda^{-1}y$ with $\lambda$ chosen by GCV, and neural networks are fit via backprop with architecture selection by RMSE minimization.","The DOE consists of 3807 training runs and 1613 test runs (original), with 11 input factors; updated preprocessing/postprocessing yields 3791 training and 1606 test runs. On the original computations, test RMSEs for predicting fusion margin are 38.5°C (NN), 36.1°C (Kriging), and 44.5°C (kernel), with $Q^2\approx0.98$–0.99; Kriging’s 90% predictive intervals achieve CIR ≈ 89.8%. The estimated Kriging nugget standard deviation is about 28.5°C for the original code, indicating substantial instability; after fixes it drops to about 19.8°C and RMSE improves to 31.3°C (NN), 27.6°C (Kriging), and 38.5°C (kernel), with CIR ≈ 91.2%. ROC/AUC for classifying viability (fusion margin > 300°C) is very high, with AUC ≈ 0.9977 (Kriging) on original and ≈ 0.9984 after updates.","The authors explicitly note they do not address code validation: they aim to emulate Germinal outputs without assessing whether those outputs represent physical reality. They also state that, while metamodels can quantify and indicate code instabilities (e.g., via Kriging’s nugget), developing more automatic tools to isolate and help experts correct instabilities is an open problem, and their segment-based diagnostic is only a practical heuristic.","The DOE is restricted to a hypercubic input region with feasibility filtering, so conclusions may not generalize outside this domain or under different operational constraints; feasibility filtering can also distort space-filling properties near constraints. The work focuses on a single scalar output (fusion margin), so it does not demonstrate DOE/metamodel performance for multivariate outputs, functional outputs, or multiple correlated quantities typical of reactor simulations. The LHS-Maximin design is one-shot rather than sequential/adaptive; there is no exploration of active learning, replication to separate noise/instability from model error, or sensitivity of results to DOE size/criterion. Software/implementation details for reproducing the DOE generation and surrogate fitting are not provided, limiting reproducibility.","The authors explicitly identify as an open problem the development of more automatic metamodel-based tools to help code experts isolate and correct code instabilities, beyond their proposed approach of running additional simulations along a line segment to visualize oscillations. They also indicate that the remaining non-negligible nugget after updates suggests the code manager could still be improved, motivating further investigation of residual preprocessing/postprocessing issues.","A natural extension is to use sequential/adaptive DOE (e.g., uncertainty-based sampling from Kriging predictive variance or instability indicators) to target regions near the viability threshold and regions with suspected instabilities. Replicated runs at identical or near-identical inputs could help quantify and separate numerical instability from deterministic model structure, enabling more principled nugget/regularization choices. Extending the approach to multiple outputs (multivariate GP/NN surrogates) and to constrained-design methods that maintain space-filling while respecting feasibility constraints would increase practical utility. Releasing code and standardized benchmarks for the Germinal DOE/metamodel pipeline would improve reproducibility and facilitate comparison with other modern surrogates (e.g., sparse GPs, deep ensembles, or Bayesian neural nets).",1511.03046v1,https://arxiv.org/pdf/1511.03046v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:52:34Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Cost reduction|Other,Not applicable,"Variable/General (candidate experiments defined by which variables are measured, time points, and external factors/initial conditions; HIV case uses 7 IFNα levels and multiple measured components; parameter space example: 7 parameters, 5 variables).",Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Not provided,http://sloppycell.sourceforge.net/,"The paper proposes “prediction deviation,” an optimization-based metric for uncertainty quantification in nonlinear dynamical systems that measures the maximum disagreement in specified predictions among all parameter sets that still fit observed data within a confidence bound. It then develops an a priori optimal experimental design strategy by estimating the worst-case impact of a candidate experiment on prediction deviation via a constrained max–max optimization problem (adding a constraint that forces model pairs to agree on the candidate experiment). The approach is demonstrated on a partially observed ODE model of interferon-alpha inhibition of HIV infection, where different IFNα-level/measurement combinations are treated as candidate experiments and selected sequentially to reduce uncertainty in the unobserved refractory-cell trajectory (CI). Theoretical results show that, under mild noise assumptions, prediction deviation bounds the trajectory of the true model with high probability, giving the metric an interpretable guarantee. Implementation uses numerical ODE integration with sensitivities and gradient-based constrained optimization with random restarts to solve the fitting, prediction-deviation, and experiment-impact problems.","The dynamical system is modeled as an ODE $\frac{dx}{dt}=f(x,t;\theta,\nu)$. Parameters are fit by weighted least squares $z_{\mathrm{fit}}(\theta;\mathcal P,\tilde x)=\sum_{j,i,t}\big(\frac{x_i(t;\theta,\nu^j)-\tilde x^j_i(t)}{\sigma_{ijt}}\big)^2$. Prediction deviation is defined by maximizing the prediction discrepancy $z_{\mathrm{dev}}(\theta_1,\theta_2;\mathcal Y)=\sum_{\ell,i,t}\big(\frac{x_i(t;\theta_1,\nu^\ell)-x_i(t;\theta_2,\nu^\ell)}{\sigma_{ilt}}\big)^2$ subject to both models fitting the observed data: $z_{\mathrm{fit}}(\theta_k;\mathcal P,\tilde x)\le z_u^*$ for $k=1,2$. Estimated experiment impact adds a candidate-experiment closeness constraint $z_{\mathrm{dev}}(\theta_1,\theta_2;\mathcal P')\le \eta$ (motivated by the bound $z_{\mathrm{dev}}(\bar\theta_1,\bar\theta_2;\mathcal P')\le 4\eta$ if both fit new data within $\eta$).","In the HIV/IFNα case study, using only one experiment (C+CI at IFNα=0.002 ng/mL) leaves the prediction of the unobserved CI trajectory essentially unconstrained: two distinct parameter sets can fit the observed totals yet predict CI ranging from nearly none to nearly all refractory cells. Solving the estimated experiment impact problem across 20 candidate experiments identifies C+CI at IFNα=0.0 ng/mL as the single best next experiment, and the observed reduction in prediction deviation after adding that experiment is reported as very close to the predicted worst-case reduction. Across candidates, most (17/20) experiments yield no reduction in prediction deviation for the CI prediction target, and some can even increase uncertainty due to added noise; candidate deviation alone does not predict actual impact. Sequentially selecting experiments by estimated experiment impact achieves nearly the same uncertainty reduction after only ~3–4 selected experiments as using all 20 candidates.","The authors note that estimated experiment impact is a worst-case analysis based on approximations: it cannot predict increases in uncertainty (it can only predict that uncertainty will not decrease), and the triangle-inequality step used to motivate the candidate constraint is generally loose. They also indicate that the usefulness of the approach depends on choosing an appropriate closeness/fit threshold parameter $\eta$, for which they provide heuristic choices (e.g., scaling by numbers of observations).","The design criterion is not cast in standard optimality terms (e.g., expected utility, Bayesian OED, Fisher-information criteria), which can make comparison to classical OED methods nontrivial and may limit interpretability of “optimality” across problems. The optimization is nonconvex and relies on random restarts; performance and reproducibility may be sensitive to initialization, local optima, and tuning, especially for higher-dimensional models common in systems biology. The worst-case framing may be overly conservative in settings where average-case or probabilistic guarantees would yield different (potentially more informative) experiment choices, and the method’s robustness to model misspecification is not deeply explored.","The authors state that estimated experiment impact can be extended from sequential to simultaneous experimental planning by combining multiple candidate experiments into a single candidate set within the same framework. They also provide additional supplemental demonstrations (e.g., Lorenz system) to illustrate settings where certain candidate measurements do or do not constrain the prediction task.","Developing a Bayesian or probabilistic version of prediction deviation/experiment impact (e.g., expected reduction under a posterior over parameters and outcomes) could reduce conservatism while retaining decision-theoretic grounding. Extending the approach to handle correlated/autocorrelated observation noise, discrete-time/partial-observation likelihoods, and explicit model discrepancy would improve applicability to real experimental data. Providing open-source reference implementations (e.g., a Python package) and benchmarks against standard OED criteria (FIM-based, mutual information, model discrimination) would aid adoption and clarify when worst-case design is preferable.",1511.03395v5,https://arxiv.org/pdf/1511.03395v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:53:08Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Optimization|Prediction|Cost reduction,Bayesian D-optimal|Not applicable,2 factors (auditory and visual stimulus parameters; each in 19 discrete steps; 19×19=361 combinations),Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper studies stopping criteria for closed-loop automatic experimental design in real-time fMRI using Bayesian optimization to select audio-visual stimulus settings that maximize a target brain activation pattern (difference between occipital and superior temporal cortex activity). The experiment is a sequential, adaptive design over a 2D discrete parameter space (19×19=361 stimulus combinations), modeled with a Gaussian process and optimized via the expected improvement (EI) acquisition function. Two stopping rules are proposed: (1) terminate when the Euclidean distance between consecutive proposed stimuli falls below a threshold, and (2) a hybrid EI/PI rule that terminates when the probability of improvement at the EI-selected next point drops below a significance level (e.g., α=0.05). The criteria are evaluated empirically using data from a prior real-time fMRI study (five subjects; multiple runs), showing that many runs could have ended earlier without degrading accuracy, while a few runs suggested additional iterations would help due to continued exploration. The work advances adaptive, closed-loop neuroimaging experiment design by addressing practical budget/attention constraints via principled stopping in Bayesian optimization.","The design-selection rule uses expected improvement: $EI(x)=(m(x)-f(x^+))\,\Phi(z)+\sigma(x)\,\phi(z)$ with $z=(m(x)-f(x^+))/\sigma(x)$, selecting $x_{new}=\arg\max_x EI(x)$. The Euclidean-distance stopping rule terminates when $\|x_{new}-x_{prev}\|_2$ falls below a threshold. The hybrid stopping rule computes $PI(x_{new})$ (based on the same $z$ as a Z-test pivot) and stops when $PI(x_{new})<\alpha$.","Empirically, in most runs the Euclidean distance between successive EI proposals dropped to 0 (repeatedly proposing the same stimulus), and the corresponding PI values were consistently below $\alpha=0.05$, indicating a natural stopping point. The authors report that stopping earlier in these runs would not have impaired accuracy, assessed via the Euclidean distance between the model-predicted optimum and the hypothesized optimum over iterations. In three runs, the Euclidean distance stayed above 0, suggesting that additional scanning iterations could have improved model accuracy (example given: run 2 of subject sub01 where final accuracy was worse than in run 1). Estimated potential savings are up to seven observations (about 2/3 minutes) in many runs.","The Euclidean-distance stopping rule cannot handle multimodal objective functions well (it may keep alternating between peaks and never terminate) and requires choosing a distance threshold that is difficult to interpret. For the PI-based stopping rule, the authors note that multiple-comparisons correction would be desirable but is challenging because the number of comparisons is unknown a priori. They also note inter-subject and inter-run variability, meaning some sessions may require more observations even when others can stop earlier.","The evaluation is based on a small dataset (five subjects, limited runs) and one specific 2D discrete stimulus grid, so generalization to higher-dimensional designs or different task spaces is uncertain. Stopping performance is assessed against a “hypothesized optimum” rather than a known ground-truth optimum of the latent function, which may confound conclusions about accuracy. The approach assumes a GP model with fixed kernel form/hyperparameter selection strategy; robustness of stopping behavior under model mis-specification or nonstationarity/noise typical in real-time fMRI is not fully explored.","The authors propose future work including formal analysis of the stopping criteria and empirical validation on a more complex task. They also highlight investigating correction for multiple comparisons for the PI-based stopping rule, noting the difficulty when the number of comparisons is not known in advance.","Extend stopping rules to settings with autocorrelated/nonstationary fMRI signals and unknown/noisy observation models (e.g., heteroscedastic GP or state-space models) to improve robustness. Study stopping behavior in higher-dimensional stimulus spaces and under multimodal objectives, potentially using batch/parallel BO or entropy-based criteria. Provide open-source implementation and standardized benchmarks across real-time neuroimaging datasets to assess practical reliability and reproducibility.",1511.07827v2,https://arxiv.org/pdf/1511.07827v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:53:41Z
TRUE,Split-plot|Optimal design|Other,Parameter estimation|Cost reduction|Other,D-optimal|A-optimal|Other,1 categorical treatment factor (single-factor); blocks of equal size (plus GLMM random effects),Healthcare/medical|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org,"The paper develops methods for constructing optimal block designs for single-factor experiments with count responses modeled by a Poisson generalized linear mixed model (GLMM) with a log link, explicitly addressing violations of unit–treatment additivity. It derives computationally efficient objective functions for DA-optimality (generalized D-optimality for contrasts) and C-optimality (trace-based for contrasts) using the marginal Fisher information for fixed effects, with simplifications that avoid costly matrix inversions. Locally optimal designs (given point priors for treatment means and variance components) are then searched using simulated annealing, and the resulting designs typically allocate replication inversely to the expected counts rather than balancing treatments across blocks. Examples in next-generation sequencing (mouse strain RNA-seq) and barn owl begging calls show that classical balanced block designs can be substantially inefficient when treatment effects are non-negligible under Poisson GLMM assumptions. The methods are implemented in the R package designGLMM available on CRAN, enabling practitioners to design efficient blocked count experiments under Poisson mixed models.","The Poisson GLMM for blocked count data is $\eta_{R(i,j)}=\alpha+\tau_{R(i,j)}+u_i+e_{ij}$ with $y_{ij}\mid u_i,e_{ij}\sim\text{Poisson}(\exp(\eta_{R(i,j)}))$, $u_i\sim N(0,\sigma_u^2)$, $e_{ij}\sim N(0,\sigma^2)$. The marginal information for fixed effects is $M^{\text{marg}}_{\beta}=M_{11}-M_{12}M_{22}^{-1}M_{21}$, yielding contrast criteria $\det\{B^T(M^{\text{marg}}_{\beta})^{-1}B\}$ (DA) and $\operatorname{tr}\{B^T(M^{\text{marg}}_{\beta})^{-1}B\}$ (C). For equal block size $k$, $M^{\text{marg}}_{\beta}$ is block-diagonal with per-block form $X_i^T\Omega_iX_i$, where $\Omega_i=\operatorname{diag}\!ig(1/(\sigma^2+\lambda_{R(i,j)}^{-1})\big)-\ell_i\ell_i^T\big/\{\sigma_u^2\,[1+(\ell_i^{1/2})^T\ell_i^{1/2}]\}$ and $\ell_i$ depends on $\sigma_u^2$ and the treatment means in block $i$.","Using simulated annealing to find locally optimal (point-prior) Poisson-GLMM block designs, the authors find optimal replication tends to be inversely proportional to treatment mean counts, contrasting with classical near-balanced allocation. In the toy setting with $t=3$, $b=2$, $k=3$, Table 1 shows that when treatment means differ greatly (e.g., $(\lambda_1,\lambda_2,\lambda_3)=(1,4,16)$) and block variance is small ($\sigma_b^2=0.016$), the C-optimal design becomes unbalanced (e.g., one block repeats low-mean treatments) and the classical BIBD efficiency can drop to about 0.851. For the RNA-seq mouse-strain example (21 samples in 3 flow cells), the C-optimal allocation changes markedly with effect size (e.g., for a gene with extreme means $\lambda_1\approx 1855.3$ vs $\lambda_2\approx 1.05$, the C-optimal design heavily replicates the low-mean strain), and classical near-balanced designs lose efficiency as effects grow. For the barn owl example (15 broods of size 10; four treatment combinations), the reported C-optimal design assigns each brood the same unbalanced allocation (denoted $1^3 2^3 3^2 4^2$ in their notation), whereas the classically optimal approach would diversify allocations to balance treatments across broods.","The authors note that optimality depends on the assumed count model (they use a Poisson–lognormal GLMM); alternative models for count data (e.g., negative binomial/Poisson–gamma mixtures, hurdle models, alternate mean–variance relationships) could yield different optimal designs. In the RNA-seq example they only model variability between flow cells (blocks) and not between lanes, and they acknowledge more complex structures (e.g., row–column designs, barcoding) may be needed. They also point out that offset terms for library-size normalization (common in RNA-seq) are not incorporated in their presented design framework.","The approach is locally optimal, relying on point priors for treatment means and variance components; if these are misspecified, the resulting design may perform poorly, and robustness is not systematically analyzed. The method and derivations are specialized to a single categorical treatment factor with equal-sized blocks; extensions to unequal block sizes, multiple blocking factors, or multiple treatment factors are not developed here. Practical guidance on selecting simulated annealing tuning parameters and verifying global optimality is limited, and comparisons against other modern search/optimization methods (e.g., coordinate-exchange, genetic algorithms) are not benchmarked in depth.","They state they are working on addressing additional RNA-seq design issues such as lane-to-lane variation, barcoding that suggests row–column structures, and inclusion of offset terms (e.g., $\log(c_{ij})$) for read-depth normalization. They also propose developing Bayesian optimal designs by incorporating prior distributions (rather than point priors) for model parameters. Finally, they suggest investigating alternative search algorithms that may be more efficient than simulated annealing for finding optimal designs.","Developing robust (minimax or Bayesian-expected) optimality criteria specifically targeted at prior misspecification for treatment means and variance components would make the designs more practical in early-stage studies. Extending the framework to multi-factor categorical experiments (including interactions) and to multilevel/unequal block sizes would broaden applicability to common laboratory and field settings. Providing open, reproducible benchmarking of design quality and runtime across multiple algorithms (SA vs coordinate-exchange vs integer programming heuristics) and delivering diagnostic tools (e.g., sensitivity of efficiency to priors, recommended default priors) would help adoption.",1601.00477v1,https://arxiv.org/pdf/1601.00477v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:54:19Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Other,Not applicable,"Variable/General (network-level design parameters such as treatment allocation proportion α, degree–treatment correlation; propagation parameters F and effect size λ)",Other|Theoretical/simulation only,Simulation study|Exact distribution theory|Other,TRUE,R,Not provided,NA,"This paper studies how to design randomized experiments on networks when interference/propagation is possible, showing that statistical power depends strongly on the assumed propagation mechanism and the network topology. Using simulation on a realistic network (the Ghana voter registration station road network with 868 nodes), the authors vary design parameters such as the initial treatment probability (α) and whether treatment assignment is correlated with node degree, under propagation dynamics modeled with a one-step variant of the Ising model (temperature parameter F) and outcome effect sizes (λ) with additive or multiplicative forms. Power is evaluated for two randomization-based inference frameworks: (i) Aronow & Samii’s exposure-condition estimators/tests (Horvitz–Thompson and Hajek variants) for contrasts like untreated-adjacent-to-treated vs untreated-not-adjacent, and (ii) Bowers et al.’s model-based randomization tests using distributional test statistics across exposure conditions. Key design conclusions are that allocating substantially less than 50% to treatment can maximize power for detecting propagated/indirect effects, and that biasing treatment toward higher-degree nodes can increase power for detecting interference effects in their setting. Overall, the paper advances DOE for network experiments by linking power to propagation models, exposure mappings, and topology-aware randomization schemes rather than default 50/50 allocation.","Treatment is initially assigned independently as $Z_i\sim\text{Bernoulli}(\alpha)$, and propagation is modeled via a one-step Ising-type infection probability $\Pr(Z_{i,t=1}=1\mid \cdot)=\{1+\exp(\tfrac{2}{F}(k_i-2m_i))\}^{-1}$ where $k_i$ is degree, $m_i$ the number of exposed neighbors, and $F$ a temperature parameter. Potential outcomes are generated from a baseline $Y(0,0)\sim U(0,1)$ with either multiplicative effects $Y(1,0)=Y(0,1)=\lambda Y(0,0)$ or additive effects $Y(1,0)=Y(0,1)=\lambda+Y(0,0)$. For Aronow–Samii inference, exposure conditions are defined (e.g., $d_1: Z_i=1$, $d_{(0,1)}: Z_i=0$ with $\ge1$ treated neighbor, $d_{(0,0)}: Z_i=0$ with 0 treated neighbors) and contrasted via a Horvitz–Thompson difference $\hat\tau=\hat\mu(d_{(0,1)})-\hat\mu(d_{(0,0)})$ with weights $1/\pi_i(d_k)$.","Across many simulated conditions (1,000 replicates per setting) on the 868-node Ghana network, power to detect indirect/propagated effects using the Aronow–Samii HT-based test is maximized at very low initial treatment proportions, with the best-performing design at $\alpha=0.05$ (and power often well below 0.5 for many other $\alpha$ values). Power increases as more nodes in the indirectly-exposed condition are truly exposed by propagation within one period (more propagation implied by the temperature parameter settings). When treatment assignment probability is positively correlated with node degree, power to detect indirect effects increases strongly at each $\alpha$ level (topology-aware randomization can outperform uniform assignment). For Bowers et al.’s model-based randomization tests, overall power is high, but there is a trade-off: larger $\alpha$ improves power to reject a global “no effects” null, while smaller $\alpha$ improves power for tests focused specifically on propagation effects (e.g., comparing $d_{(0,1)}$ vs $d_{(0,0)}$ while excluding directly treated nodes).","The authors note that their findings about optimal treatment proportions and degree-biased assignment are specific to their simulation setup (the Ghana road network, the chosen Ising propagation model, and the specified outcome/effect models) and should not be assumed universal. They also state they did not study performance of the Bowers et al. approach under incorrect propagation models, focusing instead on design/power under a known truth. Finally, they did not address how design and power change when the network structure itself is uncertain or only partially observed, nor how these methods interact with network-based sampling designs.","The design recommendations are evaluated primarily under a one-step propagation assumption; designs might differ under multi-step diffusion or time-varying exposure/outcomes, which are common in network interventions. The simulation uses a simplified outcome-generation mechanism (uniform baseline and homogeneous additive/multiplicative effects) and does not explore heterogeneous treatment effects, outcome noise beyond the baseline draw, or realistic measurement error, which could materially affect power and optimal allocation. Comparisons to other interference-aware design strategies (e.g., graph cluster randomization, two-stage saturation designs) are discussed conceptually but not benchmarked head-to-head in the simulation results presented, limiting practical guidance on which design family is best under comparable constraints.","They propose (1) developing approaches that can learn/estimate propagation models algorithmically or via nonparametric frameworks when strong a priori theory is lacking, (2) designing experiments to maximize power for combined/multiple hypotheses (e.g., direct and indirect effects jointly) rather than a single estimand/test, and (3) incorporating uncertainty about the network structure into inference and design (including integrating stochastic network models and considering network sampling designs such as respondent-driven sampling).","A useful extension would be to derive or approximate design-optimal rules (or heuristics) for choosing $\alpha$ and degree-biased assignment given a prior over propagation parameters, enabling Bayesian or robust design under propagation-model uncertainty. Another direction is to study constraint-aware designs (budget, ethics, minimum treated counts, clusterable units) and their effect on both estimands and variance, including comparisons to modern cluster/saturation designs on the same networks. Providing a public, reproducible software implementation (e.g., an R package with functions to compute exposure probabilities $\pi_i(d_k)$ under various assignment schemes and to run design-power simulations) would materially improve uptake and facilitate practitioner-driven design calibration.",1601.00992v3,https://arxiv.org/pdf/1601.00992v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:55:01Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (examples include 2 parameters for diffusion; 4 Manning’s n parameters for ADCIRC),Environmental monitoring|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/1601.06702,"This paper develops an optimal experimental design (OED) approach for stochastic inverse problems in which a chosen set of quantities of interest (QoI) is used to infer uncertain model input parameters in a measure-theoretic framework. It proposes two design criteria for selecting QoIs (e.g., sensor locations/times): (i) an average measure of the inverse image/support size (a proxy for parameter uncertainty/precision) and (ii) an average local “skewness” metric tied to geometric conditioning of the inverse map (a proxy for numerical accuracy/tractability of approximating the inverse solution with finite samples). These criteria are combined via a multicriteria (weighted) optimization that searches over candidate QoI subsets to balance precision vs. computational feasibility. The method is demonstrated on linear and nonlinear toy maps, a time-dependent diffusion PDE (choosing two temperature measurements in space-time from 400 candidates), and an ADCIRC storm-surge application (choosing four observation stations from 194 candidates) to reduce uncertainty in parameters such as thermal conductivities and Manning’s n coefficients. Numerical studies show that QoI choices can dramatically change inverse-solution support size and predicted-quantity uncertainty, and that minimizing support alone can yield highly skewed/ill-conditioned inverse images, motivating the multiobjective formulation.","The paper’s design criteria are based on local Jacobians of the QoI map $Q(\lambda)$. For a generalized-rectangle data event $B\subset D$ and local Jacobian $J_{\lambda^{(i)}}$, the local inverse-image size is approximated as $M_Q(\lambda^{(i)})\approx \mu_D(B)/\det(J_{\lambda^{(i)}})$ (square case) or more generally via the product of the nonzero singular values: $M_Q(\lambda^{(i)})=\mu_D(B)/\prod_{k=1}^m\sigma_{ik}$. Skewness is defined locally by $S_Q(J_{\lambda^{(i)}},j_k)=\|j_k\|/\|j_k^{\perp}\|$ and $S_Q(J_{\lambda^{(i)}})=\max_k S_Q(J_{\lambda^{(i)}},j_k)$, with computable forms using singular values of Jacobian submatrices. The multicriteria OED selects $Q^{(z)}$ by minimizing distance to an “ideal point” using $d_{Y_\omega}(x,y)=\omega d_S(x_1,y_1)+(1-\omega)d_M(x_2,y_2)$ and $\min_{Q^{(z)}} d_{Y_\omega}(p,(S_Q^{(z)},M_Q^{(z)}))$ with $p=(1,0)$.","In a linear $\mathbb{R}^2\to\mathbb{R}^3$ example (selecting 2 QoIs), the QoI pair that minimizes $M_Q$ yields $M_Q\approx 1.6\times 10^{-2}$ but higher skewness, while the pair that minimizes the combined objective (with $\omega=0.5$) achieves a better tradeoff (Table 2). In a nonlinear $\mathbb{R}^2\to\mathbb{R}^{10}$ polynomial example, different QoI pairs show large spread in $M_{Q,N}$ and $S_{Q,N}$ (e.g., a “worst” pair has $M_{Q,N}\approx 1.33\times 10^{-1}$ and $S_{Q,N}\approx 4.016$; Table 3). For the diffusion PDE OED (choosing 2 sensors from 400 candidates), the $M$-minimizing pair gives very small $M_{Q,N}\approx 1.72\times 10^{-5}$ but higher skewness ($\approx1.46$), while the skewness-minimizing pair yields essentially no parameter reduction (inverse image ~ whole space; Table 4/Fig. 7). In the ADCIRC example (choosing 4 stations), the optimal station set reduces relative support size to $7.904\times 10^{-3}$ versus $1.917\times 10^{-2}$ for a suboptimal set (about a 3× reduction).","The authors note that boundary effects (when $Q^{-1}(B)$ intersects the boundary of $\Lambda$) are neglected in measure approximations and may matter in certain cases. They also state that choosing the weight $\omega$ in the multiobjective metric is unresolved and is an active topic; guidance likely depends on available computational resources (number of model solves). The discrete exhaustive search over QoI subsets can become computationally expensive as the candidate set grows, motivating future work on mitigating optimization cost.","The design criteria depend on local Jacobians/singular values and Monte Carlo estimates, which can be noisy or biased for highly nonlinear maps unless sampling is sufficiently dense; the paper does not fully characterize the sensitivity of optimal QoI selection to sampling variability. The multicriteria scalarization (distance-to-ideal with a single $\omega$) may miss parts of the Pareto set and can be sensitive to the chosen metric form; alternative Pareto-search methods are not explored. Real-world constraints (sensor failure, correlated measurement errors, temporal/spatial correlation in model–data mismatch) are largely abstracted into generalized-rectangle uncertainty and may alter optimal designs. Practical scalability to very high-dimensional parameter fields (beyond the presented examples) is only partially addressed, and no complexity bounds or runtime comparisons for the full QoI-subset search are provided.","The paper explicitly calls for determining how to choose the weight $\omega$ as a function of available model solves/computational budget. It proposes extending from discrete sensor-choice problems to continuous optimization over sensor locations/times (infinitely many candidate QoIs) by studying local minima of a mapping from QoI configurations to the objective space. It also suggests investigating how skewness affects adaptive sampling efficiency in high dimensions, and studying how to determine the maximum number of useful QoIs (e.g., whether $m-1$ or $m-2$ QoIs yield nearly the same inverse solution).","Developing statistical uncertainty quantification for the selected “optimal” QoI set (e.g., bootstrap over Monte Carlo samples or Bayesian treatment over design criteria) would make the design recommendations more robust. Incorporating explicit sensor/noise models beyond axis-aligned rectangles (heteroscedastic, correlated, non-Gaussian, and model-discrepancy-aware) could materially change the inverse-image geometry and thus the design. Adding constraints and costs (deployment/maintenance, time-to-change, accessibility) would enable true cost-aware OED rather than treating cost only implicitly via computational budget. Providing open-source, reproducible implementations for the OED optimization loop (and benchmarking against classical OED criteria like D-/A-/I-optimality or mutual information) would strengthen adoption and clarify connections to broader DOE/OED literature.",1601.06702v1,https://arxiv.org/pdf/1601.06702v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:55:53Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Other,E-optimal|Other,"Variable/General (e.g., EGFR model: 48 vs 70 parameters; DNA repair model: 6 parameters; radiation design varied 4 experimental variables)",Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|C/C++|Fortran|Other,Not provided,http://sourceforge.net/projects/geodesiclm/,"The paper analyzes how optimal, model-based experimental design can fail in “sloppy systems,” where models are approximate and have many practically unidentifiable parameter combinations (sloppy Fisher Information Matrix eigen-spectra). It shows that selecting complementary experiments to improve parameter identifiability can unintentionally make previously omitted mechanisms relevant, increasing systematic model discrepancy and causing poor fits and reduced predictive power even when parameters appear tightly estimated. The authors introduce a simple hyper-model for model error (a multiplicative scale factor on experimental noise) to quantify discrepancy and propagate it into inflated parameter covariance estimates. They demonstrate the phenomenon on (i) EGFR signaling models by comparing an approximate Michaelis–Menten network (48 parameters) against a more mechanistic mass-action version (70 parameters) under optimally chosen experiments, and (ii) DNA repair/survival models where adding optimally chosen irradiation experiments increases inferred model discrepancy and degrades predictivity. The work argues for using hierarchies of models and designing experiments within regimes where the chosen model is valid, rather than focusing solely on parameter identifiability in a single approximate model.","Parameter identifiability is quantified via the Fisher Information Matrix (FIM), e.g. for least squares with Gaussian noise $I_{\mu\nu}=\sum_i \sigma_i^{-2}\,\partial_{\theta_\mu}y_i\,\partial_{\theta_\nu}y_i$. Model discrepancy is added as $d_i=y_i(\theta)+\sigma_i\xi_i+\delta_i$ with hyper-model $\delta_i=f\,\sigma_i\xi_i'$, leading to an estimator $f=\sqrt{\chi^2/(M-N)-1}$ and an inflated covariance $\mathrm{Cov}(\theta)=\frac{\chi^2}{M-N}I^{-1}$. For “optimal” EGFR experiments they also discuss selection by maximizing the smallest FIM eigenvalue (an E-optimal-style criterion).","For the EGFR example, five “optimal” experiments (chosen to maximize the smallest FIM eigenvalue) make the 48-parameter approximate model appear identifiable, but data generated from the 70-parameter mechanistic model cannot be fit well by the approximate model; with ~7000 data points, expected $\chi^2\approx7000\pm84$ but observed best-fit $\chi^2>100{,}000$ (never below 96,000), giving $f\approx3.7$ with $\delta f\approx0.03$ and inflating effective uncertainties by $\sqrt{1+f^2}\approx3.8$. The mechanistic model’s FIM under the expanded experiments indicates roughly ~60 eigen-directions above the noise level (suggesting a minimal model needs ~60 parameters), demonstrating missing relevant mechanisms in the 48-parameter model. For the DNA repair case, fitting the original 19 experiments yields $f=0.76$ ($\delta f=0.41$), but after augmenting to 35 experiments via optimal design, $f$ increases to 2.0 ($\delta f=0.33$), and the model loses the ability to predict previously observed dose-response asymmetry.","The authors note that the simple hyper-model for systematic error (uncorrelated, proportional to experimental noise) can break down when misfit has structure, as seen in the EGFR case (Figure 5). They state that more sophisticated discrepancy models (e.g., separate $f$ per time series, correlated/structured errors, or additional phenomenological parameters) might partially rescue predictivity but are beyond the paper’s scope. They also acknowledge practical complications in constructing increasingly detailed mechanistic models (e.g., needing parameters for the experimental apparatus), which they treat as beyond scope in their idealized “sloppy system” abstraction.","The optimal-design discussion centers on FIM-based local criteria (e.g., maximizing the minimum eigenvalue) that assume the model is correct and linearizable near nominal parameters; robustness to parameter uncertainty and structural uncertainty is not systematically incorporated into the design criterion. Comparisons are largely qualitative across two exemplars; there is no broad benchmark across multiple competing DOE criteria (e.g., D-, A-, I-optimal, Bayesian/robust designs) or explicit sensitivity to candidate-set constraints and feasibility/cost of experiments. Code is described as in-house and not shared, which limits reproducibility of the simulation and fitting pipelines (e.g., details of candidate experiment generation, noise models, and regularization choices).","They suggest that improved predictivity might be achieved with more sophisticated hyper-models of discrepancy (e.g., time-series-specific error scales or correlated systematic-error structures) as explored in the uncertainty-quantification literature. They also argue for developing better methods to quantify and account for model approximations so that optimal experimental design searches can be restricted to experimental regimes where the model is valid. Finally, they propose that mechanism identification should be approached via hierarchies of models of varying detail rather than parameter estimation in a single model.","Develop robust/Bayesian experimental design criteria that explicitly include structural model discrepancy (e.g., priors over discrepancy processes or model classes) and optimize predictive utility rather than identifiability alone. Create diagnostics that use patterns of misfit across designed experiments to suggest specific missing mechanisms (model criticism guided by DOE). Provide open-source implementations and standardized benchmarks (candidate experiment libraries, noise/discrepancy generators, and evaluation metrics) to compare DOE strategies under sloppiness and misspecification. Extend the analysis to autocorrelated/multivariate observations, mixed-effects/biological variability, and sequential design policies that update both parameter and discrepancy models online.",1602.05135v3,https://arxiv.org/pdf/1602.05135v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:56:35Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper designs an electronic speckle shearing phase-shifting pattern interferometer (ESSPPI) based on a Michelson interferometer, using a step-motor-driven rotatable mirror to produce adjustable shearing and a PZT-driven mirror to implement phase shifting. Experiments measure deformation of a rectangular aluminum plate under (i) different external force magnitudes applied at the same (center) position and (ii) the same external force applied at different positions. The authors use a four-step phase-shifting method to obtain phase distribution and a phase-unwrapped image for the center-load case. Results are presented as recorded speckle interferograms and reconstructed phase/unwrap figures, demonstrating increased fringe density with increasing force and fringe concentration near the load application point. The contribution is primarily an optical measurement system design and demonstration rather than a formal statistical design-of-experiments methodology.","The CCD optical intensity before deformation is modeled as $I_1=2A^2(1+\cos\phi)$. After deformation, $I_2=2A^2[1+\cos(\phi+\Delta\phi)]$, with the phase change related to deformation gradient via $\Delta\phi=\frac{4\pi}{\lambda}\frac{\partial w}{\partial x}\,dx$ (shear amount $dx$). The interferogram difference is expressed as $I=I_2-I_1=4A^2\sin(\phi+\Delta\phi/2)\sin(\Delta\phi/2)$, indicating sensitivity to deformation through $\sin(\Delta\phi/2)$.","Qualitatively, speckle interferogram fringes become denser as the applied external force at the plate center increases (shown across multiple recorded interferograms). With the same force applied at different plate positions, fringe concentration aligns with the force application point, interpreted as the region of maximum deformation. For the center-load case, the paper reports successful reconstruction of the phase distribution and a phase-unwrapped surface using a four-step phase-shifting method (figures provided), but does not report numerical error metrics, repeatability statistics, or quantified deformation values in the text excerpt.",None stated.,"The experimental study varies force magnitude and position but does not specify a structured DOE plan (factor levels, randomization, replication), so conclusions are largely qualitative and may be sensitive to uncontrolled conditions. The paper does not report measurement uncertainty, calibration/traceability of applied force, repeatability across trials, or robustness to environmental factors (e.g., residual vibration, speckle decorrelation), limiting practical generalization. Quantitative performance comparisons to alternative interferometry/phase-shifting implementations (e.g., sensitivity, resolution, SNR, error in unwrapping) are not provided.",None stated.,"A natural extension is a statistically designed experiment (with replication and randomized run order) to quantify the influence of controllable factors (shear amount, phase-step size, force magnitude, load position, exposure settings) on measurement accuracy and phase-unwrapping success. Adding calibration against an independent displacement/strain reference (e.g., DIC, strain gauges) and reporting uncertainty budgets would strengthen validation. Software/tooling for automated phase unwrapping and parameter tuning, along with robustness studies under noise/vibration and non-ideal surfaces, would improve deployability.",1602.06975v1,https://arxiv.org/pdf/1602.06975v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:56:58Z
TRUE,Optimal design|Other,Parameter estimation,Other,1 factor (gas/liquid volume ratio x),Environmental monitoring|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/kapelner/optDesignSlopeInt,"The paper develops optimal experimental designs for the phase ratio variation (PRV) method used with headspace gas chromatography to estimate Henry’s Law constants, where the target parameter is the slope-to-intercept ratio from a simple linear regression of inverse GC response on gas/liquid volume ratio. For homoskedastic measurement error, it derives an optimal approximate design that places all design points at the boundary values (xmin, xmax) and provides a closed-form formula for the optimal allocation proportion at each endpoint as a function of an a priori guess of the ratio parameter. For heteroskedastic errors (variance as a function of x), it proposes numerical search (Nelder–Mead) over the design space to minimize an approximate variance criterion for the ratio estimator. The paper also discusses multiple inference approaches for the ratio (delta-method normal approximation, Bayesian bootstrap, and parametric/nonparametric bootstraps), and evaluates design efficiency via Monte Carlo simulations and an applied example estimating kH for naphthalene in water. An open-source R package (optDesignSlopeInt) is provided to compute designs and conduct simulation “bakeoffs.”","The response model is $Y_i=\beta_0+\beta_1 x_i+E_i$ with parameter of interest $\theta=\beta_1/\beta_0$. Using a first-order Taylor (delta-method) approximation, the design criterion for homoskedastic errors yields $\mathrm{Var}(\hat\theta)\propto (\theta\bar x+1)^2/s_x^2$, which implies an optimal design supported only at $x\in\{x_{\min},x_{\max}\}$. The optimal endpoint allocation proportion is $\rho^*=\frac{1+\theta_0 x_{\max}}{2+\theta_0(x_{\min}+x_{\max})}$, with approximately $\mathrm{round}(\rho^* n)$ runs at $x_{\min}$ and the remainder at $x_{\max}$ (ensuring at least one at each endpoint).","In the naphthalene-in-water example (with domain roughly $x_{\min}=0.33$, $x_{\max}=14.44$, and $n=10$), the computed optimal homoskedastic design allocated 7 runs at $x_{\min}$ and 3 at $x_{\max}$. Simulations comparing equal spacing versus the optimal endpoint design showed substantially reduced estimator spread; the paper reports about a 61% reduction in error (intercentile range) and relative efficiency around 2.6 in that scenario, implying materially narrower confidence intervals. A robustness study varying the prior guess $\theta_0$ indicated limited degradation when $\theta_0$ is in the right ballpark (e.g., only modest percent increases in error over a wide range), while remaining far better than naive spacing. Simulations on designs from prior PRV studies suggested many published designs could have standard errors tens of percent higher than the proposed optimal designs under comparable assumptions.","For heteroskedastic designs, the authors do not derive closed-form optimal allocations in general and instead rely on heuristic numerical optimization (Nelder–Mead) that has no guarantee of finding a global optimum. They also note that normal-approximation (delta-method) confidence intervals perform poorly at the small sample sizes typical in laboratory PRV experiments, motivating bootstrap-based alternatives. The parametric bootstrap assumes normally distributed errors, which the authors acknowledge is not fully accurate for their measurement context (errors can be positively skewed due to physical constraints).","The “optimality” derivation for the homoskedastic case is based on a first-order Taylor approximation to the variance of a ratio estimator; when the intercept is near zero or noise is moderate, higher-order effects and heavy tails of the ratio can dominate, potentially changing what is practically optimal. The method also presumes the linear PRV relationship is valid over a chosen domain; in practice, domain selection is itself uncertain and can require exploratory runs, which may reduce the practical savings from the derived allocation. The heteroskedastic-design section depends on specifying/approximating the variance function $h(x)$; in many labs this function is unknown and may vary by compound, instrument state, and preparation error, making the computed “optimal” design sensitive to misspecification. Finally, the work focuses on a single-factor linear model and does not address correlated errors (e.g., batch effects, drift, or autocorrelation across vial runs) that could affect both design and inference.","The authors propose using higher-order Taylor approximations to improve variance approximations and confidence intervals, though they suspect limited gains at small n. They suggest exploring exact distribution-based methods for the ratio of two correlated normals (Fieller/Hinkley-type results) to potentially optimize designs and construct improved intervals, possibly using plug-in estimates. They also outline a Bayesian modeling approach with positivity-aware priors for $\theta$ and $\beta_0$ and using posterior variance as a design objective, expecting better behavior when $\beta_0$ is near zero or $\theta$ is small. They note further study of interval coverage and potential refinements (e.g., Efron-style bootstrap improvements), and mention a straightforward extension to the inverse ratio (intercept/slope) as a new parameter of interest.","A useful extension would be a fully sequential/adaptive PRV design: start with a small pilot to estimate $(\beta_0,\beta_1,\sigma^2)$ (and potentially $h(x)$), then update the endpoint allocation and rerun additional vials to target a desired precision. Developing minimax-robust (or Bayesian) designs that hedge against uncertainty in $\theta_0$ and the choice of domain $[x_{\min},x_{\max}]$ would strengthen practical applicability. Another direction is to incorporate errors-in-variables (uncertainty in volume ratio x) explicitly into the model and design criterion, since preparation/measurement error in volumes is a stated driver of heteroskedasticity. Finally, providing standardized benchmarks and open simulation scripts for a broader set of PRV contexts (different VOCs, detectors, and nonlinearity regimes) would help validate generalizability and guide practitioners in choosing between homoskedastic endpoint designs and numerically optimized heteroskedastic designs.",1604.03480v1,https://arxiv.org/pdf/1604.03480v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:57:28Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Parameter estimation|Model discrimination|Prediction|Optimization|Other,Bayesian D-optimal|Other,"Variable/General (continuous parameters, designs, and observations; examples include 1D parameter θ and 1D design d or displacement d_k)",Environmental monitoring|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper formulates sequential optimal experimental design (sOED) as a finite-horizon dynamic program, showing batch (open-loop) and greedy (myopic) designs as special, generally suboptimal cases. It focuses on Bayesian sOED for parameter inference using an information-theoretic terminal utility (expected KL divergence / mutual information) and develops tractable numerical methods for nonlinear models with continuous parameter, design, and observation spaces. The proposed solution uses approximate dynamic programming: one-step lookahead policies with value functions approximated via a linear feature architecture, trained by backward induction with regression and an exploration–exploitation sampling scheme to refine state visitation. Belief states (posteriors) are represented by adaptive grid discretizations in the paper’s demonstrations (with a companion paper proposed for scalable transport-map representations). Numerical experiments validate the approach against an analytic linear-Gaussian solution and demonstrate advantages over batch and greedy methods on a nonlinear contaminant source inversion (sequential sensing) example with movement costs and constraints.","The sOED objective is maximizing expected cumulative reward over N experiments: $U(\pi)=\mathbb{E}[\sum_{k=0}^{N-1} g_k(x_k,y_k,\mu_k(x_k)) + g_N(x_N)]$ subject to dynamics $x_{k+1}=F_k(x_k,y_k,d_k)$. The dynamic programming (Bellman) recursion is $J_k(x_k)=\max_{d_k\in\mathcal D_k}\,\mathbb{E}[g_k(x_k,y_k,d_k)+J_{k+1}(F_k(x_k,y_k,d_k))]$ with terminal $J_N=g_N$; a one-step lookahead policy uses an approximate value function $\tilde J_{k+1}$. The Bayesian information-based terminal reward is the KL divergence $g_N(x_N)=D_{\mathrm{KL}}(f(\theta\mid I_N)\|f(\theta\mid I_0))=\int f(\theta\mid I_N)\log\frac{f(\theta\mid I_N)}{f(\theta\mid I_0)}\,d\theta$ (equivalently expected mutual information), and value functions are approximated linearly as $\tilde J_k(x_k)=r_k^\top\phi_k(x_k)$.","In a linear-Gaussian example with $N=2$, the ADP-based sOED policy matches the analytically optimal expected reward $\approx 0.7833$, with Monte Carlo estimates around 0.74–0.78 depending on belief-state representation and iteration. In a nonlinear contaminant source inversion example, sOED outperforms greedy design in Case 1 (reported mean total reward about 0.15 vs 0.07 over 1000 trajectories) and outperforms batch design in Case 2 (about 0.26 vs 0.15). The policy-update (exploration/exploitation) mechanism substantially improves performance when the initial exploration distribution is poor (Case 3 shows a jump from about -1.20 at the first update to about 0.65 at the second, then stabilizing around 0.68–0.72).","The authors note that representing and propagating the Bayesian belief state (posterior) is challenging for continuous, non-Gaussian, multi-dimensional parameters, and that their adaptive grid discretization is only practical for one- or perhaps two-dimensional parameter spaces. They also state that a theoretical analysis of their iterative exploration/exploitation policy-update mechanism is deferred to future work, given complexities of mixed sampling and the induced state measure. They further acknowledge that identifying good value-function features is difficult and often relies on heuristics and expert knowledge.","The method’s performance likely depends strongly on the chosen feature/basis functions for value approximation and on tuning of exploration–exploitation and stochastic optimization settings, but systematic guidance and robustness analysis are limited. The demonstrations are simulation-based and small-scale; there is no real-data deployment, and the belief representation used in experiments (adaptive grids) does not scale to higher-dimensional inverse problems, limiting immediate applicability. The approach also relies on repeated Bayesian updates and Monte Carlo estimation inside nested optimizations, which can be computationally heavy; practical runtime/complexity comparisons to alternative scalable Bayesian OED methods are not fully characterized.","The paper points to future work on scalable belief-state representations and fast repeated Bayesian inference, specifically via transport map constructions enabling efficient conditioning in higher dimensions. It also suggests further exploration of alternative ADP approaches, including model-free methods such as Q-learning. The authors additionally mention that theoretical analysis of their policy-update (exploration/exploitation) iterative procedure is left for future work.","Developing principled feature construction/selection (or using modern nonlinear function approximators with stability guarantees) and quantifying approximation error impacts on design performance would strengthen the approach. Extending the framework to higher-dimensional designs/parameters with realistic constraints (e.g., multi-vehicle, spatiotemporal sensing, correlated noise, and model misspecification) and providing open-source implementations would aid adoption. Benchmarking against contemporary Bayesian OED methods (e.g., variational, Laplace/implicit, differentiable programming approaches) on standardized problems with computational budgets would clarify when ADP-based sOED is most advantageous.",1604.08320v1,https://arxiv.org/pdf/1604.08320v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:58:15Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (design variable: probe position; parameters estimated include beam position/radius, detector efficiency, and hole center/radius)",Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper presents a deterministic single-ion transmission microscope and applies Bayes experimental design to choose each next probe position to maximize expected information gain when imaging parametrized transmissive structures. The “design” variable is the controllable probing position (e.g., profiling edge position or 2D scan position), and the observations are binary transmission/detection events from single-ion extractions. They formulate a Bayesian sequential updating scheme for the parameter posterior and select the next design point by maximizing expected utility defined as the expected reduction in Shannon entropy (information gain). The approach is demonstrated experimentally for (i) 1D beam profiling to infer beam position, beam radius, and detector efficiency, and (ii) 2D localization of a circular hole in diamond to infer hole center (x,y) and radius (with beam radius and efficiency fixed from prior calibration). The work advances imaging/measurement efficiency by adaptively concentrating measurements where they are most informative, enabling nanometer-scale parameter estimation with very low particle dose.","Bayesian update for parameters $\theta$ given outcome $y$ and design (control) $\xi$: $p(\theta\mid y,\xi)=\frac{p(y\mid\theta,\xi)p(\theta)}{p(y\mid\xi)}$, with marginal likelihood $p(y\mid\xi)=\int p(y\mid\theta,\xi)p(\theta)\,d\theta$. Utility (information gain) is the entropy difference $U(y,\xi)=\int \ln p(\theta\mid y,\xi)\,p(\theta\mid y,\xi)\,d\theta-\int \ln p(\theta)\,p(\theta)\,d\theta$, and the expected utility is $U(\xi)=\sum_{y\in\{0,1\}} U(y,\xi)p(y\mid\xi)$. For 1D edge profiling with $\theta=(x_0,\sigma,a)$ and design $\xi$ (edge position), the binary detection model is $p(y=1\mid\theta,\xi)=\frac{a}{2}\,\mathrm{erfc}\!","For a representative comparison at mean 1 ion/pixel and assumed detector efficiency 0.96, the deterministic source yields SNR 4.90 versus 0.96 for an equivalent Poissonian source (Fig. 3b). In the 2D Bayesian-design hole-localization experiment on a diamond sample, using 572 ions total, they estimate the hole radius as $r=814.1\pm1.5$ nm and localize the hole center with accuracies $\Delta x=3.5$ nm and $\Delta y=2.0$ nm (with beam 1$\sigma$ radius fixed at 25 nm and detector efficiency fixed at 95% from prior calibration). The Bayes-design routine uses a recursive search over candidate design positions; five recursions are reported as sufficient to reach required accuracy without excessive computation.",They note that systematic errors from deviations of the actual structure from the parametrization (an ideal circle) are difficult to quantify because the precise extent of the deviation is unknown; the reported accuracy strictly applies to an ideal circular shape. They also indicate that their SNR comparison plot does not take dark count noise into account (though their gated detection suppresses dark counts experimentally).,"The Bayesian design/estimation assumes a correct forward model for transmission (e.g., Gaussian beam profile and idealized object transmission), so model misspecification (non-Gaussian tails, edge roughness, scattering) could bias inferred parameters and the chosen “optimal” probe locations. The computational procedure is described as grid-based with recursive search; scaling to higher-dimensional parameterizations or richer shape models may become computationally expensive without more advanced inference/optimization (e.g., particle filters, gradient-based or surrogate optimization). Results are demonstrated on specific structures and conditions; broader benchmarking against alternative adaptive sampling policies or non-Bayesian active learning baselines is limited.","They outline extending the apparatus beyond imaging to deterministic single-ion implantation at nanometer scale and discuss using transmissive markers for alignment to enable accurate dopant positioning free of parallax errors. They also mention that different ion species may be needed for imaging in some implantation contexts to avoid contamination, with attention to keeping beam energy consistent to avoid focus shifts.","A natural extension is to incorporate explicit model-mismatch robustness (e.g., heavier-tailed beam models, unknown background/dark counts, imperfect object transmission) and quantify sensitivity of the Bayes-optimal policy to these assumptions. Extending the Bayesian design to multiscale or nonparametric shape representations (splines/level sets) could broaden applicability beyond simple edges/circles. Providing open-source implementation and standardized benchmarks (dose vs. accuracy curves) would improve reproducibility and allow comparison to other adaptive experimental design/active sensing methods.",1605.05071v1,https://arxiv.org/pdf/1605.05071v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:58:55Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (examples in d=2,4,6; general d-dimensional setting)",Theoretical/simulation only,Simulation study|Other,TRUE,R,Not provided,https://arxiv.org/abs/1605.05524,"The paper proposes two new sequential Bayesian design strategies to estimate a fixed output percentile of an expensive deterministic black-box function with random inputs, using a Gaussian Process (GP) surrogate. The percentile estimator is taken as the percentile of the GP posterior mean evaluated over a Monte Carlo sample from the input distribution, and the authors derive an analytical update characterization of this estimator after adding a new run. Based on this, they introduce two Stepwise Uncertainty Reduction (SUR) infill criteria: one based on reducing a probability-of-exceedance discrepancy relative to the targeted percentile level, and one based on maximizing the (conditional) variance of the updated percentile estimator to target points that most affect the estimate. The methods are assessed on standard test functions (Branin in 2D, Hartman in 4D, Ackley in 6D) under tight evaluation budgets, showing accurate percentile estimates with relatively few model evaluations and generally outperforming random sampling. The work advances sequential design for uncertainty quantification tasks beyond optimization/failure-probability estimation by providing largely closed-form, GP-based SUR criteria that avoid expensive conditional simulations used in prior quantile-estimation SUR approaches.","Target percentile is defined as $q_\alpha(g(X))=F_Y^{-1}(\alpha)$ with $Y=g(X)$. The working estimator is the percentile of the GP posterior mean, approximated on an MC sample $X_{MC}$: $q_n\approx m_n(X_{MC})_{(\lfloor l\alpha\rfloor+1)}$. Given a candidate $x_{n+1}$ and observation $g_{n+1}$, the GP mean update is $m_{n+1}(x)=m_n(x)+\frac{k_n(x_{n+1},x)}{s_n^2(x_{n+1})}(g_{n+1}-m_n(x_{n+1}))$, and the updated percentile is piecewise determined by which interval of the standardized value $z=(g_{n+1}-m_n(x_{n+1}))/s_n^2(x_{n+1})$ it falls into. Two SUR criteria are proposed: $J_n^{prob}(x_{n+1})=\left|\mathbb{E}_{G_{n+1}}\big[\int_X \mathbb{P}(G(x)\ge q_{n+1}\mid\mathcal{A}_{n+1})dx\big]-(1-\alpha)\right|$ (with a derived bivariate-normal-CDF form), and $J_n^{Var}(x_{n+1})=\mathrm{Var}_{G_{n+1}}(q_{n+1}\mid\mathcal{A}_{n+1})$ (with a derived truncated-normal closed form over intervals).","In a 2D Branin example targeting the 85th percentile, the procedure starts from 7 LHS points and adds 11 sequential points; both SUR strategies concentrate samples near the estimated percentile contour and yield final estimates close to the true percentile computed from a large MC sample. In 4D and 6D test problems, experiments use 30-point LHS initialization plus 60 sequential evaluations, with performance summarized over 10 repeated runs and multiple percentile levels (e.g., 5%/97% in 4D; 15%/97% in 6D). Across most settings, both SUR strategies reduce absolute percentile-estimation error to below about 2% of the output range after roughly 30 iterations (total ~60 evaluations), generally outperforming random search; $J_n^{Var}$ is often more efficient, while $J_n^{prob}$ can be better in some cases but can also underperform for difficult high-percentile settings. The authors also note that for more central percentiles (e.g., 15% in 6D), gains over random sampling are more modest, consistent with the need to learn broader regions of the input space.","Both strategies rely on a Monte Carlo set $X_{MC}$ to estimate the percentile and (for $J_n^{prob}$) to approximate integrals, and in practice $|X_{MC}|$ is limited to a few thousand by computational resources. The authors state this can hinder extreme-percentile estimation and performance on highly multi-modal functions. They also note sensitivity to GP model error, including inaccurate hyperparameter estimation or poor kernel choice, which can reduce efficiency and robustness.","The approach is demonstrated only on deterministic simulators with GP surrogates; extension to stochastic simulators or observation noise would require modifying both the GP update and the quantile estimator/criteria. The method’s computational cost can still be substantial because each candidate evaluation requires computing interval structure (line intersections) over $X_{MC}$ and, for $J_n^{prob}$, many bivariate Gaussian CDF evaluations and numerical integration, which may not scale well to larger $d$ or larger candidate sets. The percentile estimator is based on the GP mean (plug-in) rather than fully propagating posterior uncertainty about $g$ and GP hyperparameters, which can lead to overconfident estimates and myopic sampling in misspecified settings.","The authors suggest combining adaptive sampling or subset-selection methods with their approach to better handle limited $X_{MC}$ size, particularly for extreme percentiles and highly multi-modal functions. They also propose accounting for GP model error (hyperparameter uncertainty and kernel misspecification), noting that a fully Bayesian GP approach could improve robustness but would add computational cost.","Develop self-adaptive schemes that vary $|X_{MC}|$ or use importance sampling targeted to the percentile region (with error control) to improve extreme-quantile accuracy under fixed compute. Extend the methodology to noisy/stochastic simulators (heteroscedastic noise, replication) and to constrained or mixed discrete-continuous inputs common in engineering applications. Provide scalable implementations (e.g., sparse/approximate GPs, parallel/batch acquisition, GPU-accelerated bivariate CDF/integration) and benchmark against modern Bayesian optimization/UQ acquisition functions for quantiles (e.g., knowledge-gradient variants). Release reproducible code and datasets to facilitate adoption and fair comparisons across competing sequential quantile-estimation methods.",1605.05524v2,https://arxiv.org/pdf/1605.05524v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T01:59:38Z
TRUE,Split-plot|Optimal design|Robust parameter design|Other,Parameter estimation|Robustness|Other,D-optimal|A-optimal|Minimax/Maximin|Other,"Not specified (t treatments arranged/run-ordered within b blocks; examples include t=3,7,10–18)",Food/agriculture|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/1605.08473,"The paper develops robust (minimax) designs for randomized complete block experiments when within-block errors are correlated (serial or spatial correlation) and the exact covariance matrix is unknown. It models covariance uncertainty via covariance “neighbourhoods” around a nominal block-diagonal covariance $R_0$ and constructs robust run orders/allocations by minimizing the worst-case loss in the covariance of estimated treatment means. Because GLS cannot be computed without the true covariance, the authors use a modified GLS estimator (MGLSE) based on $R_0$ and derive closed-form expressions for the maximized loss under two neighbourhood definitions (one scaled by $R_{j0}$ and one by $I_t$). Robust designs are computed via complete search for small problems and a simulated annealing algorithm for larger ones, with examples for time-ordered runs (1D) and field-plot layouts (2D). The paper also proves several theoretical properties, including that for certain neighbourhoods the LSE-based D-robust design repeats the same allocation across blocks, while MGLSE-based D-robust designs generally require differing allocations across blocks under DG/DE correlation.","Block effects model: $y_{ij}=\mu+\tau_r+\beta_j+\epsilon_{ij}$, and regression form $y=X\mu+U\beta+\epsilon$ with block-diagonal covariance $R=\sigma^2(V_1\oplus\cdots\oplus V_b)$. Estimators: LSE $\hat\theta_L=(Z^\top Z)^{-1}Z^\top y$ and MGLSE $\hat\theta_M=(Z^\top R_0^{-1}Z)^{-1}Z^\top R_0^{-1}y$. Robust (minimax) criterion: $\phi_L(\hat\mu,X)=\max_{R\in\mathcal R_{K,\alpha}} L\{\mathrm{Cov}(\hat\mu)\}$; for monotone $L$, the maximized loss has closed forms, e.g. for LSE: $\phi_L= L\big(\tfrac{1+\alpha}{b^2}X^\top R_0X\big)$ when $K=R_{j0}$ and $\phi_L=L\big(\tfrac{1}{b^2}(X^\top R_0X+\alpha X^\top X)\big)$ when $K=I_t$; analogous expressions are derived for MGLSE.","Closed-form worst-case loss expressions (Theorem 1) reduce the minimax design problem to minimizing explicit matrix criteria under two covariance neighbourhoods. For LSE with neighbourhood $\mathcal R_{R_{j0},\alpha}$ and common $R_{10}$ across blocks, the D-robust design repeats the same treatment allocation in every block (Theorem 2), and for A-criterion any design is robust (Theorem 3). For MGLSE with $b=2$, $n=1$, $t>3$ and DG/DE correlation, the D-robust design must differ across the two blocks (Theorem 4). Example D-robust designs (by complete search/annealing) are reported with objective values such as $(\phi_D)^{1/7}=0.60613$ (t=7,b=2) and $(\phi_D)^{1/3}=0.23165$ (t=3,b=5) for MGLSE under specified neighbourhood parameters.",None stated.,"The work is restricted to complete block designs with fixed block effects and (primarily) one replicate of each treatment per block; incomplete blocks and random block effects are not treated. Robustness is defined only with respect to covariance misspecification within specified neighbourhood classes (block-diagonal, independent between blocks), so performance under cross-block correlation, nonstationarity, or model misspecification in the mean structure is not established. The paper proposes simulated annealing but does not provide reproducible code or detailed tuning defaults, which can affect solution quality in combinatorial search. Practical guidance for selecting $R_0$ and neighbourhood size $\alpha$ is heuristic and not linked to formal elicitation or data-driven estimation procedures.","The authors note the methodology can be extended to (a) two or more replicates in each block (with dimensional changes to $X,U,V$) and (b) minimizing $\mathrm{Cov}(C\hat\mu)$ for user-chosen contrast matrices $C$ (e.g., comparing each treatment to a control). They also state a conjecture (suggested by Theorem 4) that the MGLSE-based D-robust design differing across blocks may hold more generally, proposing this as a future research topic.","Develop self-starting or adaptive robust designs that update $R_0$ (or $\alpha$) sequentially using Phase I data, reducing reliance on subjective prior covariance choices. Extend the framework to incomplete block designs, random block effects/mixed models, and settings with cross-block dependence or spatial nonstationarity. Provide open-source implementations (e.g., R/Python) with benchmarks and diagnostic tools to assess robustness sensitivity to neighbourhood specification. Study alternative optimality criteria (e.g., I-/G-optimality for prediction) and robustness to simultaneous mean-model misspecification and covariance uncertainty in blocked experiments.",1605.08473v1,https://arxiv.org/pdf/1605.08473v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:00:18Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Case study (real dataset),TRUE,Other,Personal website,http://www.geeksykings.eu/gila/,"The paper designs and implements GiLA, a distributed force-directed graph drawing algorithm in Apache Giraph using a Think-Like-A-Vertex (TLAV) paradigm, targeting large graphs on inexpensive cloud infrastructure. The method adapts the Fruchterman–Reingold model to distributed execution by approximating repulsive forces using a k-neighborhood (TTL-controlled flooding), plus preprocessing (pruning degree-1 vertices, partitioning with Spinner) and postprocessing (reinsertion of pruned vertices). It is experimentally evaluated on Amazon EC2 clusters (10/15/20 machines) using both real networks (up to ~1.5M edges) and synthetic random and scale-free graphs (up to 2M edges), reporting runtime/cost and drawing quality metrics (crossings per edge, edge-length SD, and a shape-based similarity metric). Results show good scalability for small k (especially k=2–3) and competitive or improved drawing quality versus a centralized OGDF Fruchterman–Reingold implementation; the paper also demonstrates an application to visual cluster detection using the LinLog model plus k-means clustering. The implementation code is made available online.","The force on a vertex $u$ is defined as $F(u)=\sum_{(u,v)\in E} f^a_{uv}(u)+\sum_{(u,v)\in V\times V} f^r_{uv}(u)$, where attractive and repulsive magnitudes follow the Fruchterman–Reingold form $\|f^a_{uv}\|=\delta(u,v)^p/d$ and $\|f^r_{uv}\|=d^2/\delta(u,v)^q$ (with $p=2, q=1$ in FR). In the distributed approximation, repulsion is computed only over the $k$-neighborhood $N_v(k)$, obtained by TTL-limited message flooding. The paper also gives a worst-case time bound of $O(\Delta^k\, s\, n/c)$ for maximum degree $\Delta$, supersteps per run $s$, and computing units $c$.","On the Real benchmark, with $k=2$ GiLA successfully drew all instances on 10 machines; graphs with about one million edges were drawn in under ~8 minutes, with reported cloud costs on the order of about $\$1 per drawing. Strong scalability improved notably for larger instances and higher $k$; e.g., for large graphs, moving from 10 to 20 machines often reduced runtime by >30%, and for amazon0302 the reduction with $k=3$ exceeded 50%. Increasing $k$ generally improved quality (fewer crossings per edge and higher similarity), but could cause failures/timeouts for small-diameter or scale-free graphs at $k\ge3$ on the largest sizes. GiLA at $k=3$–$4$ often produced fewer crossings than the centralized OGDF-FR baseline on the reported real instances, while also achieving competitive or better shape-based similarity.",None stated,"The method’s quality and feasibility depend heavily on the locality parameter $k$; for small-diameter and/or scale-free graphs, modest increases in $k$ can lead to very high communication and memory load (including out-of-memory/timeouts), limiting robustness across graph classes. The experiments use specific EC2 instance types and cluster sizes (10/15/20) and do not report variance across multiple random initializations, which could affect both runtime and quality for force-directed methods. Comparisons are primarily against a single centralized FR implementation (OGDF-FR), without broader benchmarking against other scalable layouts (e.g., multilevel/approximate FR variants, Barnes–Hut, fast multipole, or modern distributed layouts), which may limit conclusions about state-of-the-art performance.","The authors propose developing a distributed multi-level force-directed algorithm under the TLAV paradigm, noting that building the hierarchy efficiently in a distributed manner is challenging. They suggest using GiLA as the single-level refinement step at different hierarchy levels in such a multi-level approach.","Developing adaptive or self-tuning strategies for selecting $k$ based on observed diameter/degree distribution and online measurements of message volume could improve reliability and performance. Extending the approach to handle weighted/attributed graphs, dynamic/streaming graphs, or to incorporate more advanced repulsion approximations (e.g., distributed Barnes–Hut/quadtrees) could reduce the $k$-dependence while preserving quality. Providing a reproducible benchmark harness (scripts/configs) and releasing code via a standard repository/package would improve repeatability and adoption.",1606.02162v1,https://arxiv.org/pdf/1606.02162v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:00:57Z
TRUE,Split-plot|Factorial (full)|Other,Parameter estimation|Other,Not applicable,Variable/General (includes K-factor factorial structures; split-plot illustrated with 2 factors; examples include 3 treatments),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://arxiv.org/abs/1602.03915,"The paper develops a Neymanian (randomization-based) causal inference framework for estimating finite-population treatment contrasts under a very general assignment mechanism that allows randomization restrictions (e.g., stratification/blocking and split-plot structure) and unequal replications. It generalizes classical variance decompositions by introducing a class of positive semidefinite matrices Q that yields a decomposition var(\hat\tau)=V_Q(\hat\tau)-\tau^\top Q\tau, where V_Q(\hat\tau) is estimable from observed data and \tau^\top Q\tau captures bias due to non-additivity. The authors derive “generalized additivity” conditions (milder than strict additivity) under which sampling variances/covariances of linear unbiased estimators of treatment contrasts can be unbiasedly estimated, alongside assignment-probability conditions needed for those estimators to exist. They study consequences of violating additivity via a minimax-bias criterion, providing a justification for the conservative Neyman variance estimator (Q_strict) as minimax-optimal when second-order assignment probabilities are positive, and an analogous result for split-plot randomization. The framework applies to treatments with general factorial structure (K factors, possibly differing numbers of levels) and is illustrated with simulated numerical examples.","Defines unit-level and population-level treatment contrasts: \(\tau_i=\sum_{z\in Z} g(z)Y_i(z)\) and \(\bar\tau=\sum_{z\in Z} g(z)\bar Y(z)\). Uses linear unbiased estimators of treatment means, including Horvitz–Thompson \(\hat{\bar Y}_{HT}(z)=\sum_{i\in T(z)} Y_i(z)/(N\pi_i(z))\), and estimates contrasts by \(\hat{\bar\tau}=\sum_z g(z)\hat{\bar Y}(z)\). Central variance decomposition: \(\mathrm{var}(\hat{\bar\tau})=V_Q(\hat{\bar\tau})-\tau^\top Q\tau\), with an unbiased estimator \(\hat V_Q(\hat{\bar\tau})\) formed via first- and second-order assignment probabilities \(\pi_i(z),\pi_{ii^*}(z,z^*)\).","Unbiased estimation of sampling variance for any treatment-contrast estimator is possible under a generalized additivity condition \(Q\{Y(z)-Y(z^*)\}=0\) (milder than strict additivity when \(\mathrm{rank}(Q)<N-1\)) plus a second-order assignment-probability condition ensuring needed \(\pi_{ii^*}(z,z^*)>0\). When all second-order assignment probabilities are positive, the Neyman choice \(Q_{strict}\) uniquely minimizes worst-case (minimax) bias by minimizing \(\lambda_{max}(Q)\), giving a new justification for the conservative Neyman variance estimator. For split-plot assignment, admissible Q matrices have a restricted Kronecker form and the corresponding minimax-optimal choice is the between-whole-plot additivity matrix \(Q_{whole\text{-}plot}\). Simulations with N=50 and 3 treatments show bias patterns consistent with theory: under strict additivity both estimators are unbiased; under within-stratum additivity \(Q_{strat}\) removes bias; under stronger violations, \(Q_{strat}\) often reduces bias relative to \(Q_{strict}\) except in negatively correlated potential-outcome scenarios.","The paper notes that the generalized additivity (GA) condition involves unknown potential outcomes (many are unobserved), so its validity cannot be verified from experimental data alone. It also highlights that some assignment mechanisms (e.g., the unicluster assignment example) do not permit unbiased variance estimation under their framework because no Q in the proposed class can satisfy the required second-order assignment-probability condition. In discussing minimax optimality, it acknowledges that the minimax choice can be pessimistic in practice (e.g., when negative correlations among potential outcomes are unlikely).","The work is primarily theoretical and evaluates performance mainly through illustrative simulations; there is no real-data case study demonstrating implementation details or practitioner-facing guidance on choosing Q in applied settings. The framework focuses on linear unbiased estimators and finite-population (Neymanian) repeated-sampling inference; extensions to common complications such as interference between units, noncompliance, missing outcomes, or longitudinal/clustered outcomes are not developed. Computational aspects (e.g., efficient calculation of \(\hat V_Q\) for large N or complex Z) and software availability are not addressed, which may limit immediate uptake.","The authors propose studying selection of Q under a superpopulation perspective by minimizing average/expected bias \(E(\tau^\top Q\tau)=\mathrm{tr}(Q\Gamma)\) for \(\Gamma=E(\tau\tau^\top)\), suggesting semidefinite programming approaches. They also suggest exploring an optimal overall strategy that jointly chooses the assignment mechanism and the linear unbiased estimators to minimize model-expected total sampling variance of treatment-contrast estimators, borrowing ideas from finite-population sampling. They indicate work is in progress on these problems.","Developing practical, data-analytic diagnostics or sensitivity analyses for departures from GA (e.g., bounding \(\tau^\top Q\tau\) or reporting bias-aware intervals) would make the approach more usable when additivity is uncertain. Extending the results to common modern DOE settings—e.g., high-dimensional factorial screening, adaptive/sequential experimentation, and designs with interference/spillovers—would broaden applicability. Providing open-source implementations for computing \(\hat V_Q\) under stratified/split-plot and other restricted randomizations, plus templates for reporting, would materially improve adoption and reproducibility.",1606.05279v1,https://arxiv.org/pdf/1606.05279v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:01:42Z
TRUE,Optimal design|Bayesian design|Computer experiment|Sequential/adaptive|Other,Parameter estimation|Prediction|Model discrimination|Cost reduction|Other,D-optimal|V-optimal|Bayesian D-optimal|Other,Variable/General (examples include 3 factors for logistic regression; 5 factors for Poisson/log-linear; 3 controllable variables for helicopter DA example),Manufacturing (general)|Food/agriculture|Environmental monitoring|Other,Simulation study|Approximation methods|Other,TRUE,R,Not provided,http://www.tandfonline.com/doi/full/10.1080/08982112.2016.1246045|http://www.paperhelicopterexperiment.com,"The paper reviews and advances decision-theoretic Bayesian design of experiments, focusing on generalized linear models (GLMs) and models derived via dimensional analysis. It emphasizes computational challenges of evaluating expected loss functions (e.g., self-information/KL-based loss and squared-error prediction loss) and proposes using Gaussian process emulation to approximate expected loss, coupled with a cyclic descent/coordinate-exchange optimizer (the approximate coordinate exchange, ACE, algorithm) to scale Bayesian optimal design to higher-dimensional settings. The methodology is demonstrated on (i) logistic regression for binary responses (n=16, 3 variables) using SIL-optimal designs, (ii) Poisson/log-linear regression for count data (q=5, n=6) comparing SIL-optimal designs to pseudo-Bayesian D-optimal minimally supported designs, and (iii) a Gamma GLM for the paper helicopter experiment where dimensional analysis yields a parsimonious predictor and SEL-optimal designs improve average expected posterior variance versus V-optimal, fractional factorial, and Latin hypercube competitors. Overall, the work connects Bayesian optimal design, GLMs for discrete and continuous responses, and dimensional analysis, and argues that GP emulation enables previously infeasible Bayesian design problems to be solved practically.","Decision-theoretic Bayesian optimal design is defined by minimizing expected loss: $\xi^\star = \arg\min_{\xi\in\mathcal X^n}\int_{\mathcal Y}\int_{\Psi} l(\xi,y,\psi)\,p(\psi,y\mid\xi)\,d\psi\,dy$ (Eq. 1). Two main losses used are expected self-information loss (equivalently, negative expected KL divergence) $\Phi_{\mathrm{SIL}}(\xi)=\int\!\int[\log p(\psi)-\log p(\psi\mid y,\xi)]p(\psi,y\mid\xi)d\psi dy$ (Eq. 2) and expected squared-error loss for predicting the mean response $\mu(x)$, $\Phi_{\mathrm{SEL}}(\xi)=\int\!\int\!\int_{\mathcal X}(\mu(x)-\mathbb E[\mu(x)\mid y,\xi])^2 p(\psi,y\mid\xi)\,dx\,d\psi\,dy$ (Eq. 3). Pseudo-Bayesian D-optimality uses an information-matrix approximation $\Phi_D(\xi)=\int_{\Psi}-\log|M(\psi;\xi)|\,\pi(\psi)\,d\psi$ (Eq. 12), with $M(\beta;\xi)=X^T W X$ for GLMs (Eq. 4).","For the logistic regression example (3 variables, n=16), SIL-optimal designs achieved about a 10% lower negative expected KL divergence than pseudo-Bayesian D-optimal designs, and both strongly outperformed a central composite design. For the Poisson/log-linear example (q=5, n=6), SIL-optimal designs were very similar in performance to minimally supported pseudo-Bayesian D-optimal designs for both prior-diffuseness settings ($\alpha=0.5$ and $\alpha=0.75$). For the paper helicopter dimensional-analysis Gamma GLM (n=4), the SEL-optimal design reduced average expected posterior variance by about 8% vs a V-optimal design, 11–12% vs two regular fractional factorials, and 11–17% vs maximin Latin hypercube designs.",None stated.,"The paper’s empirical comparisons are based mainly on Monte Carlo approximations to expected loss for a small set of example problems; broader benchmarking across more GLM structures (e.g., random effects, overdispersion, autocorrelation) and wider design-region constraints would strengthen generalizability. The ACE accept/reject step relies on a normality assumption for simulated loss differences (a Bayesian t-test); performance/robustness when this assumption fails (and the impact of alternative nonparametric accept/reject rules) is not fully explored. Practical guidance on sensitivity to prior misspecification is limited, even though Bayesian designs can be highly prior-dependent for nonlinear/GLM settings.","The discussion notes that ACE is still computationally challenging for larger examples and suggests that methodological and practical remedies are needed to aid adoption of Bayesian design in practice, including leveraging ACE to provide stronger evidence through rigorous studies and enabling near-optimal designs under bespoke loss functions that incorporate practical considerations.","Develop and evaluate self-starting or robust Bayesian design variants that explicitly handle unknown dispersion/overdispersion, correlation, or model misspecification in GLMs, including sensitivity analysis tools for priors and model uncertainty. Provide open-source, reproducible implementations and benchmarks (standardized design problems with reference solutions) to accelerate adoption and allow fair comparisons with alternative Bayesian design optimizers. Extend the dimensional-analysis design framework to multivariate responses and constrained design spaces typical in engineering (e.g., manufacturability constraints), and study sequential/adaptive versions of ACE for staged experimentation.",1606.05892v3,https://arxiv.org/pdf/1606.05892v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:02:24Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Other,Optimization|Prediction|Model discrimination|Other,Other,"Variable/General (design space typically $X=[0,1]^\ell$; no fixed factor count)",Theoretical/simulation only,Exact distribution theory|Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper develops a probabilistic framework to prove consistency (residual uncertainty goes to zero) of a broad class of Gaussian-process (GP) based sequential design of experiments known as Stepwise Uncertainty Reduction (SUR) strategies. The key methodological observation is that many SUR residual-uncertainty sequences $H_n=H(P_n^\xi)$ form a supermartingale with respect to the filtration generated by sequential observations, enabling generic convergence theorems. Using this framework, the authors establish consistency results for several popular GP sequential design criteria, including integrated Bernoulli variance and excursion-volume variance criteria for excursion set/measure estimation, the knowledge gradient for global optimization with noise, and the expected improvement (EI) algorithm for noiseless optimization. A notable contribution is a new proof of EI consistency that applies to any GP with continuous sample paths, avoiding prior restrictive conditions (e.g., the no-empty-ball property). The work is primarily theoretical and advances foundations for GP-driven adaptive sampling in computer experiments and Bayesian optimization/active learning.","SUR is defined by an uncertainty functional $H_n = H(P_n^\xi)$ where $P_n^\xi$ is the GP posterior given data, and the one-step look-ahead sampling criterion $J_n(x)=\mathbb{E}_{n,x}[H_{n+1}]$ with greedy choice $X_{n+1}\in\arg\min_{x\in X} J_n(x)$. For excursion sets, a main example is integrated Bernoulli variance $H_n=\int_X p_n(u)(1-p_n(u))\,\mu(du)$ with $p_n(u)=\mathbb{P}_n(\xi(u)\ge T)$, yielding the supermartingale inequality $J_n(x)\le H_n$ via total variance. For optimization, uncertainty functionals underlying knowledge gradient and EI are written as risks involving $\mathbb{E}_n(\max \xi)-\max m_n$ or $\mathbb{E}_n(\max \xi)-M_n$ (current best), with gains $G_x(\nu)=H(\nu)-J_x(\nu)$ and $G(\nu)=\sup_x G_x(\nu)$ used in the consistency proofs.","Generic theorem: for (quasi-)SUR designs and suitable uncertainty functionals, $G(P_n^\xi)\to 0$ a.s., and under additional mild conditions $H(P_n^\xi)\to 0$ a.s. Applications: (i) for integrated Bernoulli variance, $H(P_n^\xi)\to 0$ a.s. and the soft/hard classifiers $p_n$ and $\mathbf{1}\{p_n\ge 1/2\}$ converge in $L^2(\mu)$ to the true excursion indicator $\mathbf{1}\{\xi\ge T\}$. (ii) for excursion volume variance, $\mathrm{Var}_n(\alpha(\xi))\to 0$ a.s. and $\mathbb{E}_n[\alpha(\xi)]\to \alpha(\xi)$ a.s. and in $L^1$. (iii) for knowledge gradient, the residual uncertainty $\mathbb{E}_n(\max \xi)-\max m_n\to 0$ a.s., implying decisions based on $\arg\max m_n$ become optimal. (iv) for EI (noiseless), $H_n\to 0$ and both $\max m_n\to \max \xi$ and $M_n\to \max \xi$ a.s. and in $L^1$, with a proof that does not require the no-empty-ball assumption and applies to any GP with continuous paths.","The authors note that consistency alone does not provide a strong theoretical justification for preferring SUR designs over non-sequential space-filling designs, since dense deterministic designs can also ensure consistency under weak assumptions. They also emphasize that their results do not address convergence rates, leaving a gap between theoretical guarantees and the strong empirical performance of SUR methods. They mention that further study of convergence rates is needed for full theoretical support and is left for future work.","The analysis assumes the GP prior is correctly specified and the target function is a sample path from that GP (well-specified Bayesian setting), which may not hold in practical Bayesian optimization or computer experiments with model mismatch. Hyperparameter estimation/adaptation (learning the kernel/mean/noise) is not treated; consistency may fail or require new arguments when parameters are inferred online. The results are largely asymptotic and do not quantify finite-budget performance or provide practical bounds on sample complexity, which is often the key operational concern in expensive experimentation.","They explicitly indicate that studying convergence rates of SUR sequential designs is needed to provide full theoretical support for their practical effectiveness, and state this will be the subject of future work. They also frame understanding consistency as a first step toward such rate analyses and deeper theory-practice alignment.","Extend the supermartingale/SUR consistency framework to settings with unknown GP hyperparameters (hierarchical Bayes or empirical Bayes) and analyze whether plug-in or fully Bayesian updating preserves consistency. Develop finite-time guarantees or rate results (e.g., bounds on regret or uncertainty reduction) under realistic smoothness/misspecification assumptions, and compare with modern GP-UCB/Thompson sampling theory. Provide implementation guidance and open-source reference implementations for SUR criteria (especially excursion-set criteria) to support reproducibility and practitioner adoption.",1608.01118v3,https://arxiv.org/pdf/1608.01118v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:02:51Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment,Parameter estimation|Cost reduction,D-optimal,"Variable/General (measurement plan factors include number of sensors N, sensor locations X, boundary-condition amplitudes A1/A2 and frequencies ω, plus choice of observing fields u and/or v)",Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.researchgate.net/profile/Julien_Berger3/|http://www.denys-dutykh.com/|https://www.researchgate.net/profile/Nathan_Mendes/,"The paper develops an Optimal Experiment Design (OED) framework to plan in-situ experiments for estimating porous-wall material parameters in heat transfer and coupled heat–moisture transfer models. The design variables are the measurement plan (number and location of sensors, and imposed boundary heat/vapour flux characteristics such as amplitude and frequency), and the criterion is the D-optimality objective defined as the determinant of a Fisher information matrix built from scaled local sensitivity functions. Two case studies are treated: (1) a 1D transient nonlinear conduction model with temperature-dependent conductivity, and (2) a coupled 1D heat and vapour transport model with several storage/transport coefficients. The method searches over grids of candidate plans by repeatedly solving the forward PDE and associated sensitivity PDEs; for limited prior parameter knowledge, it suggests an outer-loop sampling (Halton/LHS) over parameters. Performance is empirically verified via many simulated inverse problems (Levenberg–Marquardt), showing that OED-chosen periods/frequencies and sensor placement reduce estimation error; typically, placing sensors near the forced boundary and using low-frequency/high-amplitude excitation is favored, with more sensors needed when estimating multiple parameters simultaneously.","The measurement plan is π={N,X,A,ω} (or π={N,X,ω1,ω2} for the coupled case) and the D-optimal objective is Ψ=det(F(π)). The Fisher information matrix is F(π)=[Φij], with Φij=\sum_{n=1}^N \int_0^{\tau} \Theta_i(x_n,t)\,\Theta_j(x_n,t)\,dt, where the scaled sensitivity is \Theta_m(x,t)=(\sigma_p/\sigma_u)\,\partial u/\partial p_m (with \sigma_p=1 here). The OED is defined by π°=argmax_π Ψ, with sensitivities obtained by solving a sensitivity PDE coupled to the forward PDE for each parameter.","For the nonlinear heat-transfer case, the OED favors maximum tested heat-flux amplitude (A°=1 in dimensionless units, reported as 350 W/m²) and low frequencies (long periods), with one sensor at the forced boundary x=0 sufficient to reach ~95% of the maximal criterion when estimating a single parameter; estimating (c,k0,k1) used N°=3 at x=0. Reported optimal periods are 17.3 h (estimate c), 60.6 h (estimate k0), 53.5 h (estimate k1), and 25.2 h (estimate (c,k0,k1)). Verification via Ne=100 simulated inverse problems across 30 frequencies shows the empirical MSE is minimized near ω° and improves with added sensors, especially for multi-parameter estimation. For the coupled heat–moisture case, single-parameter OEDs typically use N°=1 at x=0 and specific optimal periods for heat/vapour forcing (e.g., IP(c10)[u]: 27.2 d/27.2 d; IP(d10)[u]: 78.1 d/20.9 d; IP(c21)[u,v]: 78.1 d/12.3 d), while full hygrothermal estimation has extremely small Ψ (~4.5×10⁻9) and requires more sensors (N°=3) and/or improved sensor precision/forcing amplitude.","The authors note that the OED depends on a priori knowledge of the unknown parameters; when no prior is available, they propose an outer loop sampling over parameters (e.g., Halton/LHS) to compute OEDs across plausible ranges. For the coupled case, they also state results were obtained assuming fixed, constant measurement errors for temperature and vapour pressure sensors, and that this hypothesis may be revisited in practical applications. They additionally remark that estimating all hygrothermal properties may be ill-conditioned (very small Ψ), suggesting increasing sensor precision and flux amplitudes as a remedy.","The study largely searches OEDs via gridded enumeration over candidate frequencies, amplitudes, and sensor locations; scalability to higher-dimensional design spaces (more sensors, richer boundary-condition parameterizations) and computational cost are not fully analyzed. The Fisher-information/D-optimal approach is local (sensitivity-based) and may be less reliable under strong nonlinearity or model mismatch; robustness to structural model error and non-Gaussian/heteroscedastic measurement noise is not assessed. Real-data validation is not provided, so practical issues such as sensor bias/drift, boundary-condition uncertainty, and spatial/temporal correlation in residuals remain unquantified.",They state that further work is expected using different design strategies (OED and others) and estimating properties using real observations.,"A natural extension is to develop robust or Bayesian OED formulations that explicitly integrate prior uncertainty, boundary-condition uncertainty, and model discrepancy, and to compare D-optimality with I-/G-optimal or multi-objective criteria (e.g., estimation accuracy vs. experiment duration/cost). Implementing gradient-based or surrogate-assisted optimization (rather than grid search) could scale the design to more complex excitations and multi-sensor layouts. Additional work could address autocorrelated measurement noise, unknown initial/boundary conditions, and provide open-source implementations and benchmark datasets for reproducibility.",1610.03688v1,https://arxiv.org/pdf/1610.03688v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:03:31Z
TRUE,Optimal design|Other,Parameter estimation,D-optimal|A-optimal|E-optimal|Other,Variable/General,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops theory for weighted optimality criteria in optimal experimental design when the experimental goal is to estimate a specified system of estimable linear functions of parameters (e.g., treatment contrasts), potentially with unequal importance weights. It proves that for any eigenvalue-based optimality criterion (including common criteria such as D-, A-, and E-optimality), “weighted optimality” based on a weight matrix and “standard” optimality for estimating the chosen system of functions (via the information matrix for that system) are equivalent in the sense that the relevant information matrices share the same nonzero eigenvalues. It also addresses the inverse problem: given a chosen weight matrix, it constructs an associated system of estimable functions whose optimal-design problem is equivalent under eigenvalue-based criteria. The work extends prior weighted-optimality theory to allow singular (positive semidefinite) weight matrices and systems of estimable functions whose span is smaller than the full estimation space, proposing a simpler corresponding weight matrix form. The paper further distinguishes between experimenter-specified (primary) weights on the functions of interest and the implied (secondary) weights induced for other functions through the weight matrix.","Model: $y=X(\xi)\tau+L\beta+\varepsilon$ with $\mathrm{Var}(\varepsilon)=I$. Information for parameters of interest: $C(\xi)=X^T(\xi)(I-P_L)X(\xi)$, with $P_L=L(L^TL)^{-}L^T$. For a system of estimable functions $Q^T\tau$, the information matrix is $N_Q(\xi)=(Q^T C^{-}(\xi)Q)^{+}$. Weighted optimality uses a (possibly singular, in the extension) weight matrix $W$ and a weighted information matrix; in the extended definition with $W=K_WK_W^T$, $C_W(\xi)=(K_W^T C^{-}(\xi)K_W)^{-1}$, and for a system $Q^T\tau$ the corresponding proposed weight matrix is $W_Q=QQ^T$ (or $W_{\tilde Q}=QBQ^T$ for function weights $B=\mathrm{diag}(b_i)$).","Main theoretical result: for eigenvalue-based criteria, $C_{W_Q}(\xi)$ and $N_Q(\xi)$ have the same nonzero eigenvalues (including multiplicities) when $W_Q$ corresponds to $Q$ (and similarly for a general $W$ via a constructed system). Consequently, maximizing any eigenvalue-based criterion (e.g., D-, A-, E-optimality) under weighted optimality is equivalent to maximizing the same criterion for the system of interest $Q^T\tau$. The paper also shows how to construct a system from a given weight matrix (e.g., using $R=(P_\tau W^{-1}P_\tau)^{+1/2}$ in the nonsingular setup, and $W^{1/2}$ under the extended singular-weight framework) to transfer existing algorithms/theory for standard optimality to the weighted setting. It provides interpretations of weighted E- and A-optimality in terms of worst-case and average weighted variances over functions in the weighted subspace.",None stated.,"The paper is primarily theoretical and does not provide computational experiments, empirical examples, or numerical benchmarks demonstrating algorithmic performance or design gains under weighted vs. unweighted objectives. The equivalence results are restricted to eigenvalue-based criteria; objectives not expressible purely via the spectrum (e.g., some tailored prediction criteria or non-spectral loss functions) are not covered. Practical guidance for choosing weight matrices (or translating practitioner preferences into $Q$ and $B$) is limited, and the behavior/interpretation of induced secondary weights can be non-intuitive in rank-deficient systems.",None stated.,"Develop and benchmark concrete algorithms (e.g., multiplicative methods, SOCP/MISOCP formulations) specialized to the proposed generalized/singular weighted framework, with open-source implementations. Extend the equivalence and weighting framework beyond eigenvalue-based criteria (e.g., I-/G-optimality for prediction, compound criteria, or Bayesian decision-theoretic losses) and study robustness to model misspecification. Provide practitioner-oriented elicitation methods for constructing $Q$ and primary weights $B$, and diagnostics/visualizations for secondary (implied) weights, especially in rank-deficient or highly correlated systems of contrasts.",1610.06427v1,https://arxiv.org/pdf/1610.06427v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:03:55Z
TRUE,Optimal design|Computer experiment|Other,Prediction|Cost reduction|Other,Minimax/Maximin|Other,"Variable/General (input dimension d; examples include d=1,2,3,6,11,52 reduced to 6)",Manufacturing (general)|Energy/utilities|Transportation/logistics|Theoretical/simulation only|Other,Exact distribution theory|Simulation study|Case study (real dataset),TRUE,None / Not applicable,Public repository (GitHub/GitLab),gitlab.com/JohnDoe1989/VariableFidelityData,"The paper develops a minimax-theoretic framework for designing experiments when observations come from two sources of different fidelity and cost (low-fidelity f and high-fidelity u), modeled via the standard additive cokriging relation $u(x)=\rho f(x)+g(x)$. For Gaussian process regression on (theoretically) infinite grids, it derives interpolation error expressions and then closed-form minimax interpolation errors over a smoothness class of spectral densities, both for single-fidelity and variable-fidelity settings. Using the minimax error for variable fidelity, it obtains an analytic optimal allocation (ratio) between numbers of low- and high-fidelity samples under a fixed computational budget and relative cost $c$ of high-fidelity runs. It proposes a practical procedure (Technique 1) that estimates the allocation using only the correlation between fidelities and then generates nested random designs with the computed sample sizes. Synthetic and multiple real-data studies (aerospace CFD/solvers, structural mechanics stress/displacement, ML hyperparameter evaluation, and astrophysics likelihood) show the proposed allocation often improves regression accuracy (RRMS) compared with common heuristics (all-high, all-low, equal-size, equal-budget).","Variable-fidelity model: $u(x)=\rho f(x)+g(x)$ with independent stationary GPs. The minimax variable-fidelity interpolation error for isotropic grid spacing $h$ and refinement $m$ is $R_{h,m}(L_f,L_g)=\rho^2\frac{L_f}{2}\left(\frac{h}{m\pi}\right)^2+\frac{L_g}{2}\left(\frac{h}{\pi}\right)^2$ (paper’s notation as in Theorem 4). Under budget constraint with high-fidelity cost $c$ and low/high sample-size ratio $\delta=m^d$, the optimal ratio is $\delta^* = \left(\frac{L_f}{L_g}c\rho^2\right)^{\frac{d}{d+2}}$ (Theorem 5), which determines optimal sample sizes for nested designs.","The paper gives a closed-form minimax interpolation error for single-fidelity GP interpolation on a grid: $R_H(L,\lambda)=\frac{L}{2\pi^2}\max_i\left(\frac{h_i}{\lambda_i}\right)^2$ (Theorem 2), and an additive decomposition for variable fidelity: $\sigma^2_{H,m}(\tilde u)=\sigma^2_H(\tilde g)+\rho^2\sigma^2_{H/m}(\tilde f)$ (Theorem 3). It then derives the minimax variable-fidelity error (Theorem 4) and the optimal budget allocation ratio $\delta^*$ (Theorem 5), showing when variable fidelity can outperform all-high-fidelity sampling at equal total cost. Empirically, synthetic experiments (e.g., $d=3$, total budget 300, $c\in\{5,10\}$) show the predicted optimal low-fidelity budget share is close to the share minimizing RRMS. Across several real datasets, the MinMinimax allocation typically yields lower RRMS than baseline heuristics, though not universally (e.g., Supernova can suffer due to too few high-fidelity points).","The authors note a gap between the theory and practice: the theory assumes an infinite grid design and requires knowledge of relative complexities/smoothness parameters to compute the optimal ratio. They also state the approach requires an accurate estimate of the correlation coefficient between fidelity levels and does not account for inaccuracies in estimating GP regression parameters. Finally, they only treat two fidelity levels, while practical problems may have multiple fidelities, and very small recommended high-fidelity sample sizes can make cokriging unreliable (suggesting a lower bound on $n_u$).","The optimal allocation is derived for a specific cokriging structure ($u=\rho f+g$ with independent stationary GPs) and minimax smoothness classes; if the low-fidelity model is biased in a non-additive/nonlinear way, the allocation may be suboptimal. The theoretical design analysis is grid-based and minimax, whereas the practical algorithm uses random nested designs and correlation-based plug-ins; there is no formal guarantee that the same optimality carries over to finite, space-filling designs. The method depends on a single global correlation estimate $r$; in many engineering problems correlation varies over the input space, which could motivate localized/adaptive allocation. No software package details or reproducible environment information (dependencies, versions) are provided beyond a repository link, which may hinder replication if the repo is incomplete or not maintained.","They suggest improving the approach by reducing sensitivity to correlation estimation error and by accounting for uncertainty/inaccuracy in regression model parameter estimates. They also explicitly note extending beyond two fidelity levels to settings where multiple fidelities are available in practice. Additionally, they imply practical safeguards such as imposing a lower bound on the high-fidelity sample size to avoid degenerate allocations.","A natural extension is to develop adaptive/sequential multi-fidelity DOE that updates the allocation and sampling locations as correlation and hyperparameters are learned (rather than fixing ratios a priori). Another direction is to generalize the theory beyond stationary kernels and global smoothness classes to anisotropic/nonstationary settings and to correlated $f$ and $g$ (or more flexible autoregressive multi-fidelity models). Providing finite-sample guarantees for random/nested space-filling designs (e.g., Latin hypercube) under the same budget model would strengthen practical relevance. Finally, releasing an implementation as an installable, documented package (with examples for common GP toolkits) would improve adoption and benchmarking.",1610.06731v3,https://arxiv.org/pdf/1610.06731v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:04:33Z
TRUE,Optimal design|Split-plot|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Other,Not applicable,"Variable/General (geographic clusters/regions as experimental units; cluster count examples: ~200 US, ~50 France)",Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies how to run randomized online experiments using geographic regions as the unit of randomization, where user travel induces interference between treatment and control and violates SUTVA. It proposes GeoCUTS, a distributed geographic clustering algorithm that constructs balanced clusters by building a movement graph from anonymized Google Search traffic and then solving a large-scale balanced graph partitioning problem (minimizing cut subject to near-equal cluster weights). The authors introduce a statistical evaluation framework for cluster-based randomized experiments under interference, including a new Q-metric that quantifies interference in a bipartite user–region setting and relates it to bias of the difference-in-means estimator under a linear dose-response model. Empirically, on US and France datasets, GeoCUTS achieves interference (Q-metric) comparable to hand-designed US Nielsen DMAs and better than grid baselines, while maintaining similar balance (B-metric). The work advances practical experimental design for geo-based cluster randomization by coupling scalable clustering methods with metrics tied to causal estimation bias/variance tradeoffs.","GeoCUTS builds a movement graph with node weights $w(A)=\sum_u \sqrt{\#\text{visits}_u(A)}$ and edge weights $w(AB)=\sum_u \sqrt{(\#\text{visits}_u(A))(\#\text{visits}_u(B))}$, followed by log/sqrt normalization. Interference is measured via a folded bipartite user–region structure: unnormalized $q_{kk'}=\sum_i a_{ik}a_{ik'}$ and the region quality $Q_k=\sum_i \frac{a_{ik}^2}{a_{:k}a_{i:}}$, with overall $\bar Q=\frac{1}{M}\sum_k Q_k$. Under a linear dose model $d_i=\frac{\sum_k Z_k a_{ik}}{a_{i:}}$ and $Y_i^t=Y_i^0(1+\beta d_i)$, they show $\mathbb{E}_Z[\hat\tau]=TE+\beta(\bar Q-1)$, connecting $\bar Q$ to bias. Balance is measured by a B-metric based on normalized region weights $w_k$ (using $f(x)$ such as $\log$, $\sqrt{\cdot}$, or identity).","Using ~200 clusters for the US and ~50 for France, GeoCUTS matches DMAs on Q-metric and consistently beats a grid baseline: e.g., US highly active query-weighted Q is 0.92 (GeoCUTS) vs 0.92 (DMA) vs 0.91 (Grid); US highly mobile 0.85 vs 0.85 vs 0.81 (Table 1a). For US highly active users, 100% of queries come from clusters with $Q\ge 0.8$ under GeoCUTS vs 99% under Grid; for highly mobile, 86% vs 60% (Table 1b). Balance (B-metric, scaled by 100) is similar to alternatives for US highly active (1.5 for GeoCUTS/DMA/Grid) and slightly better than DMA for US highly mobile (1.8 vs 1.7; Grid 1.3) (Table 2). Normalization affects the Q–B tradeoff: log normalization improves Q (e.g., 0.921 vs 0.881 vs 0.840 for highly active) but increases imbalance (B 1.65 vs 0.47 vs 0.06) (Table 5).",None stated.,"The empirical evaluation uses internal Google Search location/query logs and compares primarily against DMAs and simple baselines; broader validation on public datasets or additional industry clustering standards would strengthen generalizability. The causal link between Q-metric and estimation bias is derived for a simplified linear dose-response model and does not fully address other interference mechanisms, time-varying effects, or outcomes beyond query counts. Practical deployment issues (e.g., how clusters are maintained over time as mobility patterns shift, and how randomization/inference is performed with unknown cluster-level variances) are not fully detailed. No released implementation details or reproducible pipeline are provided, which limits external verification.",None stated.,"Extend the theoretical framework beyond the linear dose-response model to more realistic exposure models (carryover, saturation, lag effects) and derive design/inference guidance under those models. Develop self-updating or adaptive geo-clustering that can adjust as mobility patterns drift, while preserving experiment integrity across time. Provide methods for power analysis, variance estimation, and robust inference tailored to GeoCUTS clusters (e.g., randomization inference, cluster-robust variance, interference-robust estimators). Release an open-source implementation and benchmark suite (or synthetic generators) to enable reproducible comparisons across clustering and interference-mitigation methods.",1611.03780v2,https://arxiv.org/pdf/1611.03780v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:05:11Z
TRUE,Response surface|Optimal design|Sequential/adaptive|Computer experiment|Other,Optimization|Prediction|Cost reduction|Other,G-optimal|Space-filling|Not applicable,Variable/General (typically 2 factors for sinusoidal paths: amplitude and number of hemi-periods; also discusses 1-factor circles radius and 2-factor square waves).,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|C/C++|Other,Not provided,http://www.sut.org/,"The paper proposes Good Experimental Methodologies (GEMs) for repeatable path-following performance assessment of unmanned surface vehicles, emphasizing statistically designed experiments to select which target paths to run. It introduces a two-step adaptive DOE procedure: an initial space-filling design (e.g., Latin hypercube) followed by a second-stage design concentrated near suspected worst-performance regions (for finding maxima) or near high prediction-uncertainty regions (for reconstructing the response surface). Performance indices (responses) quantify geometric tracking accuracy and efficiency; the DOE is illustrated mainly with sinusoidal-path experiments parameterized by amplitude and number of hemi-periods. The response surface is modeled using a kriging/Gaussian-process framework with regression trend and Gaussian correlation, enabling prediction and MSE-driven adaptive sampling. The methodology is validated via extensive simulator experiments on the Charlie USV, arguing simulations should precede costly sea trials and can overturn engineering assumptions about which paths are “hardest” to follow.","Target paths are parameterized (e.g., sinusoid) as $\gamma_{(x_1,x_2)}(w)=x_1\sin\big(\pi x_2 w/W\big)$ with factors $x_1$ (amplitude) and $x_2$ (hemi-period count). The performance response is modeled as a Gaussian-process/kriging model $Y(x)=\sum_{j=1}^p \beta_j f_j(x)+Z(x)$ with covariance $\mathrm{Cov}(Z(w),Z(x))=\sigma^2 R(\theta,w,x)$ and (in simulations) Gaussian correlation $R_j=\exp\{-\theta_j(w_j-x_j)^2\}$. Prediction uses the BLUP $\hat y(x)=f(x)^T\hat\beta+r(x)^T R^{-1}(Y-F\hat\beta)$ and the associated kriging MSE is used to guide second-stage adaptive sampling (either near local maxima for worst-performance search or near high MSE for best-prediction designs).","In simulation on a dense grid (199 design points) with Gaussian correlation, the constant-trend kriging model achieved low mean MSE (reported as 0.0038 for $D_A$ and 0.0267 for $D_H$) and small maximum MSE (0.0173 for $D_A$ and 0.1218 for $D_H$) over a 100×100 evaluation grid. In a two-step adaptive worst-performance search using 18 total runs (10-point LHS + 8-point CCD), the estimated-maximum error versus a dense-grid reference was ABS = 0.0441 for $D_A$ (range about [0, 6.7]) and ABS = 2.1488 for $D_H$ (range about [0, 11.5]); using two local maxima for $D_H$ reduced ABS to 0.2540. For adaptive estimation (19 total points), reconstruction errors versus dense-grid values were reported as max/mean absolute error of 2.0180/0.3536 for $D_A$ and 3.8330/1.1212 for $D_H$. The simulator study also showed that worst tracking performance was not necessarily at the largest curvature/parameter extremes, contradicting an engineering prior and motivating space-filling first-stage designs.",None stated.,"Most of the demonstrated adaptive-design performance is based on simulator experiments for a single platform (Charlie USV), so external validity to other vehicles, controllers, and real sea conditions is not established. The factors explored (e.g., sinusoid amplitude and hemi-period count) cover only a narrow class of paths and do not include explicit environmental/noise factors (wind, current) as controllable design variables, limiting DOE scope for real trials. The adaptive second-stage construction is heuristic (e.g., CCD around observed maxima; random reallocation of infeasible points) and is not tied to a single formal optimality criterion, which can make reproducibility and theoretical guarantees weaker. Code/software for the DOE and kriging workflow is referenced (e.g., MATLAB DACE) but not provided, which may hinder replication.","The authors propose testing the reported GEMs on other vehicles and datasets (simulated and real) and conducting extensive sea campaigns to interpret performance indices and validate the adaptive methodology under uncontrollable external conditions. They also suggest implementing more sophisticated and automatic procedures for design selection and modeling within the DeepRuler framework, while keeping methods simple enough for online use and possible extension from path-following to path-tracking.","Add explicit DOE factors for environmental and operational conditions (e.g., current speed/direction, wind, sea state) and use split-plot or blocked designs to reflect hard-to-change conditions in sea trials. Develop principled sequential criteria (e.g., expected improvement for maxima search; integrated MSE/I-optimality for reconstruction) to replace or augment CCD heuristics and random reallocation. Extend the framework to multivariate responses and multi-objective designs that jointly model geometric accuracy and efficiency/stress metrics, with Pareto-front or utility-based design. Provide open-source implementations (e.g., Python/R) and standardized benchmarks/datasets to facilitate adoption and reproducibility across the marine robotics community.",1611.04330v1,https://arxiv.org/pdf/1611.04330v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:05:51Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Other,Prediction|Model discrimination|Optimization|Robustness|Cost reduction|Other,Other,"Variable/General (input dimension d; examples include 1D, 2D; reliability case uses 2 factors: PuO2 density and H2O thickness)",Energy/utilities|Other|Theoretical/simulation only,Simulation study|Case study (real dataset)|Other,TRUE,R,Package registry (CRAN/PyPI)|Public repository (GitHub/GitLab),https://cran.r-project.org/package=KrigInv,"The paper develops adaptive (batch-)sequential design of experiments strategies for estimating excursion sets of an expensive black-box function using Gaussian process (GP) emulation, with an emphasis on conservative set estimates that control false positives via a probabilistic inclusion constraint. It extends the conservative estimation framework (based on Vorob’ev quantiles/coverage probabilities) to sequential DOE using Stepwise Uncertainty Reduction (SUR) and proposes new acquisition criteria aimed at reducing uncertainty in conservative estimates, particularly by minimizing expected false negatives (Type II error) while maintaining conservativeness. The authors derive tractable, fast-to-evaluate closed-form expressions for the proposed SUR criteria (involving univariate/bivariate normal CDF terms), enabling practical optimization and batch selection. Performance is benchmarked on synthetic GP-generated functions under varying noise and batch-size scenarios and compared to IMSE/tIMSE and related SUR approaches, showing faster reduction of Type II error for conservative estimation-focused criteria. A reliability engineering case study (nuclear criticality safety) demonstrates that the proposed strategy yields smaller Type II error and improved volume estimation of the safe region versus baseline strategies under the same evaluation budget.","The excursion set is defined as $\Gamma(f)=\{x\in\mathcal X: f(x)\in T\}$, typically $T=[t,\infty)$ or $(-\infty,t]$. Conservative estimates are defined by $\mathrm{CE}_{\alpha,n}\in\arg\max_{C\in\mathcal C}\{\mu(C):\mathbb P_n(C\subset \Gamma)\ge \alpha\}$, with $\mathcal C$ chosen as Vorob’ev quantiles $Q_{n,\rho}=\{x: p_n(x)\ge \rho\}$ where $p_n(x)=\mathbb P_n(x\in\Gamma)$ from the GP posterior. The main sequential DOE criteria are SUR one-step lookahead objectives $J_n(x^{(q)};\rho)=\mathbb E_{n,x^{(q)}}[\mu(\Gamma\Delta Q_{n+q,\rho})]$ and a Type-II-focused variant $J^{t2}_n(x^{(q)};\rho)=\mathbb E_{n,x^{(q)}}[\mu(\Gamma\setminus Q_{n+q,\rho})]$, both expanded into integrals over $\mathcal X$ involving $\Phi$ and $\Phi_2$ (univariate/bivariate normal CDFs).","In synthetic benchmarks (2D GP excursion above a threshold) across multiple batch/noise scenarios with fixed total computational budget, the conservative-estimation-focused strategies (using $\rho=\rho_n^\alpha$ and especially the Type II criterion $J_n^{t2}$) reduce expected Type II error faster than IMSE and tIMSE, with clearer gains in fully batch settings (e.g., $q=8$ or $q=16$) than in the hybrid $q=1+7$ setting. In the nuclear criticality safety case study (2D input; safe set defined by $k\_\mathrm{eff}\le 0.92$), after 75 total evaluations, strategy C (minimizing the Type II criterion) achieves a median Type II error about 27% lower than IMSE; strategy B is about 25% lower; strategy A about 12% lower than IMSE. The study also notes that conservative criteria keep Type I error (false positives) very small by construction while most improvements come from reducing false negatives and improving safe-region volume estimation.","The authors note that conservativeness depends on the correctness of the underlying GP model: if the GP is misspecified, the resulting set may not truly be conservative, so practitioners should consider higher confidence levels (e.g., $\alpha=0.99, 0.995$) and perform model checking. They also state that computing conservative estimates requires approximating Gaussian process exceedance/inclusion probabilities via discretization, and suggest that continuous approximations might be more effective. They mention that fully Bayesian treatment of GP hyperparameter uncertainty could strengthen conservativeness but is computationally challenging for SUR criteria and left for future work.","The approach relies heavily on GP assumptions (e.g., smoothness/stationarity choices, approximate noise modeling); performance and conservativeness can degrade under strong nonstationarity, discontinuities, or complex heteroskedastic/noisy simulators unless the surrogate is enhanced. The sequential criteria involve numerical integration over the input space (Monte Carlo/space-filling grids); in higher dimensions excursion regions may be tiny and naive integration/optimization may become computationally difficult, potentially making the method less scalable without advanced SMC/active subspace ideas. The conservative estimate is restricted to a family of Vorob’ev quantiles, which can be suboptimal compared to optimizing over more general set classes and may bias shape/geometry of the estimate. Comparisons are primarily to IMSE/tIMSE and related SUR; broader comparisons to modern level-set active learning methods (e.g., classification-based or safe Bayesian optimization variants) are limited.","They propose extending the methodology toward a fully Bayesian approach that accounts for GP hyperparameter uncertainty, noting it would increase conservativeness but requires advanced Monte Carlo techniques. They suggest improving the exceedance probability computation used in conservative estimates by moving from discrete to continuous approximations. They also mention further study of expected Type I/II errors as stopping criteria and investigation of the sequential behavior of hyperparameter MLEs under SUR strategies.","Developing scalable implementations for higher-dimensional problems (e.g., via sequential Monte Carlo for integration, sparse/variational GP surrogates, or dimension reduction) would broaden applicability. Extending to model misspecification-robust conservativeness (e.g., Bayesian model averaging over kernels/means or conformal-type guarantees) could better protect against surrogate errors. Incorporating adaptive allocation of simulation effort (replications per point) jointly with point selection—especially under heteroskedastic Monte Carlo noise—could improve efficiency. Providing standardized software examples and benchmarks against modern safe/level-set active learning methods (e.g., SAFEOPT-style, GP classification, and calibrated classification uncertainty) would clarify comparative advantages.",1611.07256v6,https://arxiv.org/pdf/1611.07256v6.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:06:35Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General,Food/agriculture,Simulation study|Other,TRUE,R|Fortran|Other,Not provided,http://www.austatgen.org/files/software/downloads|http://www.r-project.org/|http://www.springer.com/mathematics/book/978-0-387-36895-5|http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=1303160,"The paper proposes a model-based optimal experimental design method for plant breeding field trials that optimizes (i) allocation of genotypes across locations (sparse multi-location designs) and (ii) placement of genotypes within each location (repchecks-type layouts). The design criterion is based on a linear mixed model with random genetic effects and residual spatial correlation; designs are optimized by minimizing prediction error variance (PEV) of genotype effects, optionally incorporating a kinship matrix to discourage placing closely related genotypes nearby and to balance families across locations. The optimization is formulated as an NP-hard permutation problem over the rows of an initial design matrix, and solved with a new adaptation of Differential Evolution (DE) to permutation spaces using Hamming-distance-driven interchange moves and several DE strategies (e.g., rand3, rand2best). Through case studies (e.g., 403 genotypes across five locations; 144-plot within-location layouts with replicated checks), the method achieves better objective-function values than random designs and improves upon or matches existing tools such as DiGGer, often with favorable convergence behavior. Practically, the approach enables kinship-aware, computationally efficient construction of sparse and repchecks designs under complex constraints for quantitative genetics and plant breeding trials.","Field-trial responses are modeled via a linear mixed model $y=X\beta+Zu+\varepsilon$, with $u\sim N(0,G)$ and $\varepsilon\sim N(0,R)$; kinship enters via $G=K\sigma_a^2$ (or $K=I$ to ignore relatedness). The design criterion is to minimize the prediction error variance $\mathrm{PEV}=\mathrm{var}(u-\hat u)=[Z' M Z+G^{-1}]^{-1}$ with $M=R^{-1}-R^{-1}X(X'R^{-1}X)^{-1}X'R^{-1}$, by choosing a permuted design matrix $Z^*=\pi^*(Z_0)$ subject to design constraints. Within-location spatial correlation is modeled with an $AR(1)\times AR(1)$ residual structure where $R_{i,i}=\psi$ and $R_{i,j}=\rho_r^{|a_i-a_j|}\rho_c^{|b_i-b_j|}$ (with a nugget via $\psi=1+\text{nugget}$). The DE-permutation step uses Hamming distance $\Delta_H(\pi,\pi')$ and forms a trial permutation as $\omega=\beta\oplus \lambda\Delta_H$, where $\oplus$ applies a sequence of interchange moves.","Between-locations example (400 experimental genotypes; 5 locations): the objective value improved from 0.04841361 (random design) to 0.04156354 after optimization (30 restarts, 2000 evaluations each), with improved family balance across locations. Within-location example without kinship (repchecks on 144 plots): the new algorithm achieved a better objective value than DiGGer (0.59188730 vs 0.59656328); reported convergence times were 24.4 s (new method) vs 9.9 s (DiGGer), noting the new method can exploit multi-threading whereas DiGGer is mono-threaded. With kinship included, DiGGer’s optimization (not kinship-aware) yielded a poorer criterion value on a structured start (1.16285241), while the proposed method improved it to 1.15030587 (convergence ~23.7 s); randomizing families before DiGGer improved it to 1.15371275 but remained worse than the proposed method. The authors also report that reducing evaluations to 2000 in the within-location case still produced 0.59340239, better than DiGGer’s value in that example.","The authors note that real experimental designs can involve many additional constraints and state that further work is ongoing to add constraints such as repetition/duplication of experimental genotypes within location. They also acknowledge that the paper concentrates on repchecks designs, implying limited coverage of other common breeding trial designs in this work. Comparisons are primarily against DiGGer, with broader benchmarking (e.g., against OD) described as ongoing rather than completed.","The design criterion is based on assumed mixed-model components (e.g., specified $R$ with fixed $\rho_r,\rho_c$ and nugget, and $G$ via kinship), so performance may be sensitive to misspecification of variance parameters and to departures from model assumptions (nonstationarity, anisotropy, edge effects, or non-Gaussian errors). The objective being minimized is presented generically as PEV; the paper does not fully clarify whether a scalar summary (e.g., trace/average/maximum) is optimized and how this choice affects robustness and interpretability across scenarios. Reported empirical results are limited to a few “extreme case” simulations/case studies, so generalizability across diverse field geometries, constraint sets, and larger networks is uncertain. No reproducible implementation or full benchmarking protocol is provided, limiting independent verification of speedups and solution quality.","They propose extending the approach beyond repchecks by defining new design matrices and incorporating additional complex constraints, including uniqueness/repetition constraints for experimental genotypes and fixed proportions of duplicated genotypes for p-reps/augmented p-reps designs. They also suggest applying the same approach to allocate entries to testers for topcross production/testing in hybrid crop breeding. The authors indicate ongoing work to add constraints and to compare more extensively with OD, and note that the method can be extended to more complex kinship matrices (e.g., marker-based) without loss of generality.","A useful extension would be a self-starting or robust design procedure that accounts for uncertainty in $R$ and $G$ (e.g., Bayesian or minimax-robust design over plausible spatial/heritability/kinship parameter ranges). Broader validation on real multi-environment trial datasets with downstream impacts on BLUP accuracy and selection decisions would strengthen practical relevance. Developing an open-source, reproducible implementation (with standardized benchmarks and reporting of computational budgets) would enable fair comparisons with DiGGer/OD and wider adoption. Methodologically, exploring alternative scalar optimality summaries of PEV (e.g., A-/D-/I-optimal surrogates for mixed models) and multi-objective trade-offs (PEV vs operational constraints like machinery passes) could improve interpretability and field usability.",1702.00815v2,https://arxiv.org/pdf/1702.00815v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:07:21Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,Variable/General (examples use p=100 in simulations; fingerprints of length 1024+ for molecules),Pharmaceutical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,http://www.rdkit.org,"The paper proposes a strategy for designing/learning predictive models by combining many inexpensive coarse (binary/categorical threshold) measurements with a much smaller number of expensive fine (quantitative) measurements. Coarse data are used to extract salient correlation features via eigendecompositions of within-category covariance matrices, and fine data are used to estimate the importance of those features through regression in an augmented quadratic model. The approach reduces the effective number of parameters by expressing the quadratic interaction matrix as a weighted sum of outer products of eigenvectors derived from coarse-measurement correlation structure, with weights learned from the quantitative measurements. The method is demonstrated on two molecular-property prediction problems (antimalarial potency and aqueous solubility) using molecular fingerprints, and on synthetic Ising/Hopfield-model simulations to explain why thresholded sampling reveals informative eigenvectors. Empirically, combining coarse and fine measurements achieves comparable predictive accuracy with an order of magnitude fewer quantitative measurements and outperforms linear-only and naive/random quadratic baselines under the reported splits.","The model assumes a quadratic predictor for quantitative outcomes: $y_i = h^T f_i + f_i^T J f_i + \varepsilon_i$. Coarse measurements define categories (e.g., above/below a threshold), from which sample covariance matrices $C_\pm = \frac{1}{N_\pm} R_\pm^T R_\pm$ are eigendecomposed to obtain eigenvectors $u_k^\pm$. The interaction matrix is constrained by an ansatz $J = \sum_{k=1}^{\hat p_+} c_k^+ u_k^+\otimes u_k^+ + \sum_{k=1}^{\hat p_-} c_k^- u_k^-\otimes u_k^-$, reducing dimensionality so $h, c^+, c^-$ can be fit using the limited fine measurements.","On an antimalarial dataset (1528 binary actives; 1189 quantitative pIC50 values), the combined coarse+fine approach achieves similar predictive accuracy with roughly an order of magnitude fewer quantitative measurements than using quantitative-only models. For aqueous solubility (1144 molecules), combining a coarse solubility threshold assay with fewer quantitative measurements yields an out-of-sample model with reported $r^2 = 0.85$ and MAE $=0.61$ (for a 90% training split). In Hopfield/Ising simulations (e.g., $p=100$, $m=3$, 5000 samples thresholded to $N=500$), the eigenvalue spectrum follows Marčenko–Pastur with a small number of outlier eigenvalues whose eigenvectors recover the planted patterns; cleaning using those outliers reconstructs $J$ well. Under stratified sampling, naive random-matrix cleaning fails, but incorporating additional quantitative measurements (example: 500) and fitting via the eigenvector ansatz improves coupling-matrix recovery (reported MAE 0.12 vs 0.31 using quantitative-only ridge).",None stated.,"The work frames “experiment design” primarily as a data-fusion strategy (how to combine coarse and fine measurements) rather than providing a formal DOE design/optimality procedure for choosing experimental settings or thresholds (no explicit D/A/I-optimal criterion or run-selection algorithm is specified). Reported empirical validations are limited to two molecular-property datasets plus synthetic simulations; broader benchmarking against modern semi-supervised/weak-supervision and probabilistic models for mixed-resolution labels is not shown. Practical guidance for selecting thresholds, category definitions, and the number of eigenvectors $\hat p_\pm$ in realistic, high-rank/noisy settings appears limited, and robustness to distribution shift/autocorrelation/measurement error beyond Gaussian noise is not thoroughly established in the excerpt.","The authors suggest integrating the eigenvector-derived overlaps (e.g., $f\cdot u_i$) as inputs to more complex nonlinear models, such as artificial neural networks, to extend beyond the Ising-type/quadratic model setting.","A natural extension is to formulate an explicit sequential DOE policy for deciding which samples to measure finely next (active learning) given current coarse labels and model uncertainty, potentially with cost-aware objectives. Another direction is to develop principled methods for choosing thresholds/categories and for selecting $\hat p_\pm$ (e.g., via cross-validation, stability selection, or Bayesian model selection) when the signal is not low-rank. It would also be valuable to evaluate the approach under realistic experimental complications (unknown/heteroscedastic noise, correlated samples, batch effects) and provide an open-source reference implementation and reproducible benchmarks across multiple domains beyond chemistry/drug discovery.",1702.06001v2,https://arxiv.org/pdf/1702.06001v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:07:51Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Other,Prediction|Parameter estimation|Cost reduction|Other,D-optimal|Other,"Variable/General (benchmarks include 3, 8, 10, and 62 inputs)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper studies how to choose experimental designs (sample locations) for building accurate sparse Polynomial Chaos Expansions (PCE) via regression under a limited model-evaluation budget. It introduces a novel sequential adaptive design strategy that iteratively (i) learns a sparse polynomial basis using LARS/hybrid-LAR and (ii) enriches the design by selecting new points from a large candidate set to optimize an optimality criterion computed from the updated model matrix. Two regression-optimality criteria are considered for point selection: D-optimality (maximize det(A^T A)) and the S-value criterion (combines determinant and column-orthogonality of the information matrix), alongside space-filling baselines (LHS variants, Sobol’ QMC, sequential maximin). Across four benchmark models of varying dimensionality (3, 8, 10, 62 inputs), the sequential S-optimal strategy yields the most accurate and stable sparse PCE (lower validation RMSE and better conditioning) compared with competing designs, while sequential D-optimal performs poorly in these tests. The study highlights that regression-optimal sequential designs need not be low-discrepancy/space-filling, and recommends S-value-based sequential enrichment for sparse PCE construction.","Sparse PCE coefficients are fit by (penalized) least squares over the experimental design, e.g. OLS $\hat{\mathbf y}=(\mathbf A^T\mathbf A)^{-1}\mathbf A^T\mathbf Y$ and LASSO/LAR form $\min_{\mathbf y}\frac{1}{N}\sum_{i=1}^N\left(M(\mathbf x^{(i)})-\sum_{\alpha\in\mathcal A}y_\alpha\Psi_\alpha(\mathbf x^{(i)})\right)^2+\lambda\|\mathbf y\|_1$. D-optimal designs maximize the information determinant: $\mathbf X_D=\arg\max_{\mathbf X\in\mathcal D_X}\det(\mathbf A^T\mathbf A)$. The S-value criterion is $S(\mathbf A)=\left(\frac{\sqrt{\det(\mathbf A^T\mathbf A)}}{\prod_{i=1}^{\mathrm{card}\,\mathcal A}\|\mathbf A^{(i)}\|}\right)^{1/\mathrm{card}\,\mathcal A}$, and sequential S-optimal enrichment selects points to maximize $S(\mathbf A(\mathbf X))$ given the current sparse basis.","In repeated (50×) numerical experiments on four benchmarks (Ishigami, Sobol’ g-function, 10D truss FE model, 62D diffusion problem), sequential S-optimal enrichment consistently achieves the lowest validation RMSE and the smallest variability across replications as the design size grows. Sequential maximin is competitive in low-to-moderate dimensions (e.g., ≤10) but degrades in high dimension (62D) because it does not exploit sparsity/low effective dimension. Sequential D-optimal designs tend to concentrate points near boundaries and show poor predictive performance (higher RMSE) despite optimizing det($A^T A$). Condition-number comparisons show Seq S-optimal yields substantially better-conditioned information matrices than alternatives, aligning with its improved stability.","The authors note there is no reliable rule of thumb for choosing the initial design size $N_{init}$ and the per-iteration augmentation size $N_{add}$; performance can depend on these choices and on the model’s sparsity. They also point out sequential maximin becomes erratic in high-dimensional settings, and that some sequential/optimal strategies can deviate from uniform/low-discrepancy space-filling behavior. The study is demonstrated on benchmark problems (including inexpensive ones) where large validation sets are feasible.","The proposed optimal-point search is implemented using specific algorithms (e.g., Fedorov exchange via MATLAB’s candexch and a greedy procedure for S-optimality); results may depend on optimizer settings, candidate-set size, and computational scalability when candidate sets are extremely large. The method selects points from a pre-generated candidate pool $X_L$; performance may degrade if $X_L$ is not sufficiently rich or if constraints/complex design regions are present. Comparisons are largely within the PCE/LARS regression paradigm; broader baselines (e.g., Bayesian adaptive designs, active learning with error indicators, or other surrogate types like GP/Kriging with uncertainty sampling) are not exhaustively benchmarked.","The authors suggest exploring alternative basis selection methods such as Orthogonal Matching Pursuit (OMP) and extending the sequential adaptive design ideas to other metamodel classes, notably Low Rank Approximations (LRA), to identify optimal experimental designs. They also propose investigating the use of the sequential adaptive designs within reliability analysis frameworks.","Developing principled, possibly adaptive rules for choosing $N_{init}$ and $N_{add}$ (e.g., based on online error/conditioning diagnostics) would improve practicality. Extending the approach to handle dependent inputs, constrained domains, and noisy model evaluations (heteroscedastic simulation noise) would broaden applicability. Providing open-source implementations (e.g., MATLAB/Python) and benchmarking on larger real-world engineering datasets would strengthen reproducibility and external validity.",1703.05312v1,https://arxiv.org/pdf/1703.05312v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:08:27Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Bayesian D-optimal|Other,"Variable/General (examples include 1–4 observation times for death model; 15 sampling times for PK; logistic regression with 4 factors and n=6,10,24,48 replicates giving up to 192-dimensional design spaces)",Healthcare/medical|Pharmaceutical|Other,Simulation study|Other,TRUE,R|MATLAB,Public repository (GitHub/GitLab),http://www.github.com/DJPrice10/INSH_Code|https://CRAN.R-project.org/package=acebayes,"The paper proposes the Induced Natural Selection Heuristic (INSH), a population-based stochastic search heuristic aimed at efficiently finding optimal Bayesian experimental designs in moderately large/high-dimensional design spaces without assuming smoothness of the utility surface. Designs are iteratively sampled, evaluated via an expected-utility criterion, and the best designs are “accepted”; new candidate designs are then generated by perturbing accepted designs, enabling parallel evaluation and exploration of multiple high-utility regions simultaneously. The Bayesian design criterion used is expected information gain (Kullback–Leibler divergence / Shannon information gain), and utilities are approximated via Monte Carlo methods (ABC-based utility evaluation for a discrete-data death model; nested Monte Carlo SIG estimation via acebayes for pharmacokinetic and logistic regression examples). Empirically, INSH matches prior gold-standard results for the Markovian death model, is substantially faster than ACE while achieving comparable utility for a 15-time-point pharmacokinetic sampling schedule, and is competitive up to ~40 design dimensions but underperforms ACE in very high-dimensional logistic regression settings where optimal designs lie on boundaries. The authors also show how INSH’s population of near-optimal designs can be leveraged to construct practical “sampling windows” rather than fixed sampling times.","Expected utility for design d is defined as $u(d)=\mathbb{E}_{\theta,y}[U(\theta,y,d)]=\int\!\int U(\theta,y,d)\,p(y\mid\theta,d)p(\theta)\,d\theta\,dy$. The utility is the Kullback–Leibler divergence from prior to posterior, $U(y,d)=\int \log\{p(\theta\mid y,d)/p(\theta)\}\,p(\theta\mid y,d)\,d\theta$, equivalently Shannon information gain $U(\theta,y,d)=\log p(\theta\mid y,d)-\log p(\theta)=\log p(y\mid\theta,d)-\log p(y\mid d)$. For continuous/high-dimensional cases they approximate SIG with nested Monte Carlo: $\tilde p(y\mid\theta,d)=\frac{1}{\tilde B}\sum_{b=1}^{\tilde B}p(y\mid\theta,\tilde\gamma_b,d)$ and $\tilde p(y\mid d)=\frac{1}{\tilde B}\sum_{b=1}^{\tilde B}p(y\mid\tilde\theta_b,\tilde\gamma_b,d)$, then $\tilde u(d)=\frac{1}{B}\sum_{l=1}^B[\log \tilde p(y_l\mid\theta_l,d)-\log \tilde p(y_l\mid d)]$.","For the pharmacokinetic example (15 sampling times in [0,24] with spacing constraint $t_{i+1}-t_i\ge 0.25$), one INSH run took 2.23 hours versus 15.53 hours for ACE on the stated hardware (~7× faster) while yielding designs with utilities comparable to ACE after post-processing (20 utility evaluations at $\tilde B=B=20{,}000$). INSH considered ~19k designs in that run (reported 19,428) versus ~120k utility evaluations in the ACE setup described. In the logistic regression example, INSH produced near-optimal designs comparable to ACE for n=6 and n=10 replicates, but lagged ACE for n=24 and n=48 where many optimal coordinates are at boundary values (ACE designs had 70/96 and 143/192 boundary values for n=24 and n=48, respectively). For sampling windows constructed from INSH’s top designs in the PK example, the average efficiency relative to the INSH-optimal design was reported as 99.07%.","The authors state they do not provide a proof of convergence of INSH to the global optimal design, emphasizing the practical goal of near-optimal designs in feasible computation time. They note INSH underperforms ACE in very high-dimensional settings (e.g., the logistic regression examples) especially when the optimum lies on the boundary, because perturbation/resampling makes it unlikely to maintain many boundary coordinates simultaneously. They also acknowledge that effective choices of INSH tuning parameters (acceptance counts, perturbation kernels, Monte Carlo effort) are problem-specific and may require trial-and-error.","The method’s reported performance relies heavily on problem-specific tuning (kernel width, retention/sampling schedules, Monte Carlo budgets), but there is limited systematic sensitivity analysis or robust default guidance, so reproducibility across new domains may be challenging. Comparisons to ACE are not always “apples-to-apples” because the authors change Monte Carlo effort levels and, for the logistic regression case, modify ACE settings to match runtime; a standardized comparison protocol (equal utility-evaluation budget, equal wall-clock with same variance reduction, etc.) is not fully established. The utility approximations (ABC and nested Monte Carlo) can be noisy and may bias acceptance decisions; the paper does not deeply quantify Monte Carlo error propagation into design quality or provide variance-reduction strategies beyond increasing B/\tilde B. Practical implementation for constrained designs (e.g., truncation + ordering constraints) may become difficult in more complex constrained design regions, but this is not explored beyond the presented examples.","The authors propose exploring improvements such as adapting the perturbation kernel using correlations/covariances among design parameters, and occasionally injecting samples from previously unvisited or earlier-discarded regions to increase exploration. They also call for general rules for choosing INSH inputs based on characteristics of the utility surface or design-space dimension/scale, potentially using initial samples to estimate such characteristics. They suggest increasing Monte Carlo effort (B and \tilde B) as the algorithm progresses so that utility estimates become more precise near the optimum, and they note that alternative high-dimensional examples with optima away from boundaries should be studied to better assess INSH versus ACE.","Developing principled adaptive schedules for (i) acceptance thresholds and (ii) perturbation scales (e.g., annealing/SMC-style tempering with effective sample size criteria) could make INSH more reliable and reduce manual tuning. Introducing boundary-aware or constrained sampling mechanisms (e.g., reflective kernels, hit-and-run, projected proposals, or mixed discrete-continuous moves that explicitly propose boundary points) could address INSH’s weakness when optima lie on the design-region boundary. A theoretical analysis of convergence and finite-time guarantees (even under simplifying assumptions) and a systematic study of Monte Carlo error effects on design selection would strengthen the methodology. Packaging the method as a robust, tested R/Python package with reproducible benchmarks and automated parallelization/backends (plus standard interfaces to common Bayesian design utilities) would improve adoption and facilitate fairer comparisons across algorithms.",1703.05511v2,https://arxiv.org/pdf/1703.05511v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:09:19Z
TRUE,Factorial (full)|Other,Screening|Parameter estimation|Model discrimination|Prediction|Other,Not applicable,"2 treatment factors (2×2 factorial) plus blocking factors (site; blocks within site); taxa dimension K≈2,662 OTUs",Food/agriculture|Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,Julia|R,Public repository (GitHub/GitLab),https://www.github.com/nsgrantham/mimix,"The paper proposes MIMIX, a Bayesian logistic-normal multinomial mixed-effects model for high-dimensional microbiome count data arising from designed experiments with blocking (e.g., randomized complete block designs across sites). MIMIX uses a multinomial likelihood for OTU counts with a log-ratio link, incorporates fixed treatment effects and random block/site effects, and captures cross-taxon dependence via Bayesian factor analysis with Dirichlet–Laplace shrinkage priors; spike-and-slab priors enable global and OTU-level (local) treatment-effect testing. The method is evaluated in a simulated blocked experiment (balanced two-level treatment within 5 blocks) and applied to a 2×2 factorial NutNet field experiment (nutrient supplement × herbivore exclusion) in an RCBD replicated across four sites. In simulations, MIMIX shows higher power than Bray–Curtis PERMANOVA and better detection/estimation of sparse OTU-level effects than a mixed model without factors, especially under higher error variance. In the real NutNet application, MIMIX finds strong global evidence for a nutrient-supplement effect (but not herbivore exclusion or interaction) and identifies a small subset of OTUs with nonzero nutrient effects while quantifying variance contributions from site and block random effects.","Counts are modeled as $\mathbf{Y}_i\sim\text{Multinomial}(m_i,\boldsymbol\phi_i)$ with inverse log-ratio link $\phi_{ik}=\exp(\theta_{ik})/\sum_{l=1}^K\exp(\theta_{il})$. The mixed-effects structure is $\boldsymbol\theta_i=\boldsymbol\mu+\mathbf{B}\mathbf{x}_i+\boldsymbol\gamma_{z_i}+\boldsymbol\epsilon_i$, reparameterized with factor loadings $\mathbf{B}=\Lambda\mathbf{b}$, $\boldsymbol\gamma_r=\Lambda\mathbf{g}_r$, and $\boldsymbol\epsilon_i=\Lambda\mathbf{e}_i+\boldsymbol\delta_i$, yielding $\boldsymbol\theta_i=\boldsymbol\mu+\Lambda\mathbf{f}_i+\boldsymbol\delta_i$. Dirichlet–Laplace priors shrink factor loadings and spike-and-slab priors on $\mathbf{b}$ support Bayesian variable selection/global and local (OTU) tests.","Simulation setup uses $K=100$ taxa, one binary treatment factor, $q=5$ blocks, $n=40$ observations (balanced within blocks), and sparse treatment effects (0%, 5%, 10% nonzero taxa) under varying block and error variances; MIMIX and a no-factor variant both outperform PERMANOVA on global detection, with MIMIX gaining advantage as error variance increases. In the NutNet 2×2 factorial RCBD across four sites (166 samples; 2,662 OTUs), PERMANOVA and MIMIX detect a nutrient-supplement effect (PERMANOVA $p=0.003$; MIMIX posterior probability 1.0) while no method detects herbivore exclusion or the interaction (PERMANOVA interaction $p=0.120$). MIMIX identifies 84/2,662 OTUs (3.2%) with 95% credible intervals excluding zero for the nutrient effect. Estimated random-effect variances reported are $\hat\sigma^2_{\text{Site}}=2.296$ and $\hat\sigma^2_{\text{Block}}=0.355$, and site+block explain over half of residual variation for about 66% of OTUs.","MIMIX is not currently suited for longitudinal studies with repeated measures over time. The authors note that as microbiome dimensionality grows (e.g., multiple domains of life beyond fungi), computation time and memory management will become more pressing and may require reconstructing the posterior sampling scheme.","The method assumes conditional independence across samples given fixed/random effects and uses a multinomial model conditional on sequencing depth; unmodeled zero-inflation, technical artifacts, or residual overdispersion beyond the logistic-normal+OTU-specific noise may affect robustness. Factor loadings are assumed common across fixed and random components, which the authors acknowledge as strong; misspecification could impact inference on treatment effects or latent clusters. Practical adoption may be limited by MCMC/HMC tuning burden and computational cost for very large $K$ and $n$, and results may be sensitive to prior choices (number of factors, shrinkage strength, spike-and-slab hyperparameters) without extensive prior sensitivity analysis.","The authors suggest extending MIMIX to handle longitudinal/repeated-measures microbiome studies. They also indicate the need to improve scalability (computation time and memory) as experiments become more complex and microbiome dimensionality increases, potentially requiring changes to the posterior sampling scheme.","Develop a self-starting/empirical-Bayes strategy for selecting the number of factors and priors, and provide systematic prior-sensitivity and convergence diagnostics guidance for practitioners. Extend the model to explicitly address zero inflation and other sequencing artifacts, and to accommodate autocorrelation (time/space) and more complex hierarchical structures (multiple random effects, crossed designs). Provide a packaged implementation (e.g., Julia/R) with standardized workflows and benchmarking against additional multivariate microbiome methods (e.g., DM regression, LNM alternatives, compositional approaches) on public datasets.",1703.07747v1,https://arxiv.org/pdf/1703.07747v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:09:58Z
TRUE,Optimal design|Other,Other|Prediction,Other|D-optimal,"Variable/General (univariate explanatory variable; focus on regression basis dimension n, e.g., n=3 analyzed in detail)",Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.math.wvu.edu/~gould/Vol.4.PDF,"The paper proposes a new optimal experimental design criterion aimed at minimizing the width of simultaneous confidence bands for curvilinear regression, using the volume-of-tube method to approximate the tail probability governing the band threshold. The resulting objective is the tube-volume (TV) criterion, defined as the length (1D volume) of a trajectory of a normalized regression basis vector on the unit sphere; TV-optimal designs minimize this length over the moment/information-matrix cone. The authors develop the criterion for Fourier regression and weighted polynomial regression, prove these problems are equivalent via a tangent transformation, and show the TV criterion is invariant under a Möbius group action on the moment cone (positive definite Hankel matrices). For the case n=3, they solve the nonconvex minimization by reducing it to a one-parameter problem over orbit cross-sections and show the TV-optimal designs form a Möbius orbit containing the D-optimal uniform designs; the minimum tube volume is achieved at a specific canonical matrix (v=1/3). A small Monte Carlo study illustrates that the TV-optimal design yields slightly narrower standardized simultaneous confidence bands than nearby three-point designs.","The regression model is $y_i=b^\top f(x_i)+\varepsilon_i$ with known variance $\sigma^2(x)$, information matrix $M=\sum_i f(x_i)f(x_i)^\top/\sigma^2(x_i)$ and $\Sigma=M^{-1}$. The simultaneous band has form $b^\top f(x)\in \hat b^\top f(x)\pm c_\alpha\sqrt{f(x)^\top\Sigma f(x)}$, where $c_\alpha$ is approximated using the volume-of-tube tail: $\Pr(\max_x |(\hat b-b)^\top f(x)|/\|\Sigma^{1/2}f(x)\|>c)\approx \mathrm{Vol}_1(\gamma_\Sigma)/(2\pi)\,\Pr(\chi_2^2>c^2)$. The proposed TV criterion is $\mathrm{Vol}_1(\gamma_\Sigma)=2\int_X \frac{\sqrt{(f^\top\Sigma f)(g^\top\Sigma g)-(f^\top\Sigma g)^2}}{f^\top\Sigma f}\,dx$ with $g=df/dx$, minimized over $\Sigma^{-1}\in\mathcal M$.","For $n=3$ in weighted polynomial regression (and equivalently Fourier regression with three bases), the TV criterion reduces (via Möbius invariance) to minimizing an elliptic-integral function $\mathrm{len}(v)$ over a single parameter $v$, achieving a unique minimum at $v=1/3$. The minimum tube volume is $4\pi\sqrt{2/3}$, and all minimizers are exactly the Möbius-group orbit of the canonical information matrix $M_{1/3}$. The resulting TV-optimal three-point designs include the D-optimal (uniform) designs as special cases. A Monte Carlo experiment (300,000 replications) comparing several three-point designs shows modest improvements in upper quantiles of the standardized maximum (e.g., at $\alpha=0.05$, about 2.6234 at $v=1/3$ vs 2.6328 at $v=1/12$).","The authors note that the general proof that TV-optimal designs form a Möbius orbit containing D-optimal designs for all $n$ remains a conjecture; they only verify local optimality for small $n\le 6$ by direct Hessian calculations. They also state that their main development treats only the univariate explanatory variable case, and that extending to multivariate predictors is a future topic. They further mention that on finite intervals $X=[A,B]$ the TV-optimal design can become an improper two-point design that may not coincide with the D-optimal design, raising questions about when proper TV-optimal designs exist and how to reconcile TV- and D-optimality when they differ.","The TV criterion is derived from an asymptotic/large-threshold approximation (volume-of-tube) for $c_\alpha$, so its accuracy may degrade for moderate $\alpha$ or for models where the tube approximation is less sharp; design optimality under the approximation may not translate to exact simultaneous-band optimality. The theory assumes independent errors and known variance function $\sigma^2(x)$ (or specific weighted forms), which may be unrealistic in many applications (unknown variance, heteroskedasticity misspecification, autocorrelation). The numerical evaluation and comparisons are limited (mainly $n=3$ and a small family of competing designs), and practical implementation guidance (algorithms/software) for general $n$ and general regression bases is not provided.","They propose proving the conjecture for general $n$ that TV-optimal designs are Möbius orbits containing D-optimal designs, potentially using isoperimetric-inequality ideas. They suggest extending the framework to multivariate explanatory variables, noting that a multivariate Möbius transform preserves the volume-invariance property, but the multivariate moment cone and optimization remain open. They also call for applying the TV criterion to other regression models and studying when proper TV-optimal designs exist, especially on finite design intervals, and for developing ways to combine TV- and D-optimal designs when they are incompatible (e.g., mixtures).","Developing self-contained computational methods (e.g., exchange/coordinate-descent or manifold/orbit-optimization algorithms) and releasing reference implementations would make the TV-optimal criterion usable for practitioners beyond the analytically tractable cases. It would be valuable to study robustness of TV-optimal designs under unknown/estimated variance functions (Phase I estimation) and under autocorrelation or non-Gaussian noise, including how the tube approximation behaves under these departures. Extending the criterion to multivariate response (multiple curves) or to model-misspecification settings (e.g., model discrimination or minimax band width over a class of models) could broaden applicability. Finally, benchmarking against other band-width-oriented design criteria (e.g., I-/G-optimality or L∞ variance minimization for curve comparisons) on common test problems would clarify when TV-optimality offers distinct advantages.",1704.03995v4,https://arxiv.org/pdf/1704.03995v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:10:32Z
TRUE,Optimal design|Sequential/adaptive|Other,Optimization|Prediction|Cost reduction,Not applicable,"Variable/General (demonstrated with 54, 56, and 22 input features; high-dimensional >50 emphasized)",Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/CitrineInformatics/lolo,"The paper proposes FUELS (Random Forests with Uncertainty Estimates for Learning Sequentially), a data-driven sequential experimental design framework to accelerate materials and process optimization in high-dimensional feature spaces (>50 dimensions). FUELS fits a random forest surrogate model and uses calibrated predictive uncertainty (bias-corrected infinitesimal jackknife + jackknife-after-bootstrap plus an explicit bias model) to choose the next experiment from a finite set of candidate materials/process settings. It evaluates three acquisition/selection strategies—Maximum Expected Improvement (MEI), Maximum Uncertainty (MU), and Maximum Likelihood of Improvement (MLI)—balancing exploration (uncertainty) and exploitation (predicted performance). Performance is benchmarked on four materials datasets (magnetocalorics, superconductors, thermoelectrics, steel fatigue strength) by repeated sequential runs; FUELS typically identifies the optimum with substantially fewer measurements than random guessing and is competitive with the COMBO Bayesian optimization method. The work advances DOE for materials discovery by providing a scalable, uncertainty-aware, sequential design approach using random forests and validated uncertainty calibration.","FUELS uses a random-forest predictive uncertainty estimate at candidate $x$: $\sigma(x)=\sqrt{\sum_{i=1}^{S}\max\left[\sigma_i^2(x),\omega\right]+\tilde\sigma^2(x)}$, where $\sigma_i^2(x)$ is the per-training-sample variance contribution, $\omega$ is a noise floor, and $\tilde\sigma(x)$ is an explicit bias model (a shallow decision tree). The sample-wise variance combines jackknife-after-bootstrap and infinitesimal jackknife ideas with a Monte Carlo correction: $\sigma_i^2(x)=\mathrm{Cov}_j[n_{i,j},t_j(x)]^2+\left(\bar t_{-i}(x)-\bar t(x)\right)^2-\frac{e\,v}{T}$, with tree predictions $t_j(x)$, counts $n_{i,j}$, variance $v$, and number of trees $T$. Candidate selection includes MEI (max predicted value), MU (max $\sigma(x)$), and MLI (max probability of improving over best observed, assuming Gaussian predictive uncertainty).","Across four test cases, FUELS reduced the average number of measurements needed to find the best candidate versus random guessing by roughly 3× on average. Reported mean steps (± standard error over 30 runs) include: Magnetocalorics (167 candidates, 54 inputs) FUELS-MLI 47±3 vs random 84; Superconductors (546, 54) FUELS-MU 52±5 and FUELS-MLI 73±9 vs random 273; Thermoelectrics (195, 56) FUELS-MU 29±3 and FUELS-MLI 32±3 vs random 98; Steel fatigue (437, 22) FUELS-MLI 24±2 and MEI 28±2 vs random 219. FUELS was competitive with COMBO in all cases and was reported to be ~10× faster per iteration than COMBO on the steel-fatigue test case (informal timing). Uncertainty estimates were assessed via 8-fold cross-validation; normalized residuals were approximately Gaussian with heavier tails, indicating reasonably calibrated but imperfect uncertainty.","The authors note potential sample bias in the benchmark datasets because candidates in public datasets tend to be “promising” and already measured, reducing the number of obviously poor candidates and making the task harder in a particular way. They also acknowledge that the uncertainty estimates have heavier-tailed residuals and cannot fully capture all uncertainty sources, including unmeasured variables/“unknown unknowns,” which can create outliers. They further state that rigorous computational-efficiency comparisons were beyond the scope of the study.","The sequential-design evaluation is performed on offline, previously measured candidate sets (a “replay” setting), so it does not capture practical constraints of real experimentation such as measurement noise heteroscedasticity, failed experiments, cost/throughput differences, or constraints on feasible candidate generation. The acquisition functions are heuristic and do not optimize a formal Bayesian/decision-theoretic utility; assuming Gaussian predictive errors for MLI may be mismatched for random-forest uncertainty and heavy-tailed behavior. Phase I initialization is fixed at 10 random points, and sensitivity to initialization, hyperparameters, and candidate-set size/structure is not comprehensively analyzed. Comparisons are limited mainly to random guessing and COMBO; other strong baselines (e.g., GP-UCB/Thompson sampling, TPE, SMAC, or modern batch BO) are not included.","The authors state they will investigate the optimal size of the initial randomly selected set (currently 10) and explore initialization/sampling strategies other than random selection. They also propose testing the sequential learning methodology on a real application where the target values are not previously available/known a priori, to better assess impact beyond retrospective benchmarks.","Extend FUELS to handle explicit experimental costs and constraints (economic/cost-aware sequential design, batch/parallel selection, and feasibility constraints common in materials processing). Develop and validate more robust predictive uncertainty (e.g., quantile regression forests, conformal prediction, or Bayesian RF variants) that better reflects heavy tails and non-Gaussian errors. Evaluate performance under autocorrelated/heteroscedastic measurement noise and with unknown/estimated nuisance variables, including self-starting or adaptive re-calibration of uncertainty. Provide standardized open-source benchmarking scripts and datasets for reproducibility, including comparisons to additional modern BO/active-learning baselines and multi-objective extensions (e.g., property tradeoffs).",1704.07423v2,https://arxiv.org/pdf/1704.07423v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:11:15Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Bayesian D-optimal|Other,"Variable/General (selecting up to K initial conditions; examples include K=7,9,40; initial-condition space dimension d=2 or d=4 depending on system)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies experimental design for learning a nonparametric correction term in misspecified dynamical systems modeled as ODEs with known dominant dynamics G(y) and unknown additive correction F(y). The correction is modeled with a Gaussian Process, and the experimental design problem is to choose a limited set of initial conditions (experiments) to maximize information gain about the GP (feature-space coefficients) under a budget of K experiments. They formulate a Bayesian D-optimality objective via mutual information and show the design objective (a proxy based on trajectories from the approximate model) is monotone submodular, enabling efficient greedy/lazy-greedy selection with a (1−1/e) approximation guarantee. The paper provides approximation bounds relating the proxy objective to the true (unknown-trajectory) objective when the approximate and true trajectories are close. Simulation experiments on a 2D nonlinear correction-to-linear system and a misspecified gravitational field demonstrate that designed initial conditions reduce correction-estimation error and improve trajectory prediction versus random seeding and fully data-driven learning.","The misspecified ODE is $\dot y(t)=G(y(t))+F(y(t))$, with an approximate model $\dot y_G(t)=G(y_G(t))$. Noisy samples of the correction along observed trajectories are obtained via $\tilde F(y(t_i))=\tfrac{d}{dt}\tilde y(t_i)-G(y(t_i))=F(y(t_i))+\varepsilon_i$. The design objective is Bayesian D-optimal: maximize mutual information $G(Y_0)=I(\Theta;\tilde F(Y_m(Y_0)))$, proxied by $\tilde G(Y_0)=I(\Theta;\tilde F(Y_g(Y_0)))$ where $Y_g$ comes from the approximate trajectories; the greedy algorithm evaluates $\tilde G$ via log-determinants of GP covariance matrices (entropy of Gaussians).","They prove $\tilde G(Y_0)$ is monotone and submodular, so greedy/lazy-greedy achieves at least a $(1-1/e)$ fraction of the optimal proxy objective under a cardinality (budget) constraint. They bound the discrepancy between the true and proxy mutual-information objectives when kernel covariances under true vs approximate trajectories differ by at most $\delta$ (and provide corollaries translating trajectory deviation $\Delta$ into objective error for Lipschitz/Polynomial kernels). In simulations, experimental-design-selected initial conditions yield visibly lower correction estimation error heatmaps and improved predicted trajectories compared with random seeding, and outperform a fully data-driven approach especially at small training-set sizes (e.g., for the 2D example they demonstrate designs on a 13×13 grid with K=9; for the gravitational field example they use |Y|=300 and show lower MSE curves as K increases).",None stated.,"The experimental-design theory relies on a proxy that uses trajectories from the approximate model; if misspecification is not small over the sampling horizon, the bounds can become loose and the selected design may be poor. The work assumes access to (noisy) derivative estimates along trajectories and i.i.d. Gaussian noise, which may be unrealistic with irregular sampling, state-estimation error, or autocorrelated/process noise. Practical deployment would require kernel and hyperparameter selection and scalable GP approximations for large K·T; while mentioned, the paper does not provide an implementation or runtime/scale study beyond simulations.","They propose extending to online/adaptive experimental design rather than a single batch design, and exploring less myopic sequential policies (e.g., dynamic programming ideas from Bayesian optimization). They suggest moving from a discrete lattice of candidate initial conditions to a continuous design space with continuous optimization. They note that the additive correction form may be insufficient for some applications and that other inference objectives besides mutual information/submodularity could be explored; they also mention explicitly modeling measurement error in state variables and using random-feature methods for scalability.","Develop self-starting designs that jointly estimate GP hyperparameters and the correction term (robust to hyperparameter misspecification), and study sensitivity of selected designs to kernel choice. Extend the framework to stochastic dynamics, partial observability, and autocorrelated measurement noise (e.g., state-space models), including designs optimized for downstream prediction error rather than parameter/feature uncertainty. Provide open-source software and benchmark suites, and evaluate on real experimental or observational dynamical-system datasets to validate gains beyond simulation settings.",1705.00956v3,https://arxiv.org/pdf/1705.00956v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:11:56Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal,"Variable/General (benchmarks shown for 3, 4, 8, 10, and 53 inputs)",Environmental monitoring|Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an adaptive (sequential) design of experiments method to more efficiently estimate first-order Sobol’ sensitivity indices when the indices are computed from a Polynomial Chaos Expansion (PCE) surrogate. Using asymptotic normality of Sobol-index estimators (via a delta-method applied to least-squares PCE coefficient estimates), the authors motivate a locally D-optimality criterion that targets minimizing the asymptotic covariance (confidence region volume) of the Sobol-index estimates rather than only optimizing estimation of PCE coefficients. The resulting algorithm adds points one-by-one from a finite candidate set by choosing the point that minimizes the determinant of an index-focused covariance proxy, updating PCE coefficients and the sensitivity-index gradient matrix at each step. Performance is benchmarked against random sampling, sequential D-optimal design for PCE coefficient estimation (maximizing det of the information matrix), and Latin Hypercube Sampling (LHS). Across several analytic test functions and two finite-element models, the adaptive-for-sensitivity design generally yields lower mean Sobol-index estimation error than competitors, with statistical significance assessed via Welch’s t-test; effectiveness is strongest for deterministic or low-noise outputs.","The PCE surrogate is $\hat{Y}=f_{PC}(\mathbf X)=\sum_{\alpha\in\mathcal L} c_\alpha\Psi_\alpha(\mathbf X)$ with coefficients estimated by least squares: $\hat c=\arg\min_c \frac1n\sum_{i=1}^n (y_i-c^T\Psi(x_i))^2$. First-order Sobol indices are computed analytically from PCE coefficients, e.g. (normalized basis) $S_i(c)=\frac{\sum_{\alpha\in\mathcal L_i} c_\alpha^2}{\sum_{\alpha\in\mathcal L^*} c_\alpha^2}$. The adaptive design selects the next point from candidates $\Xi$ by minimizing a locally D-optimal criterion: $x_i=\arg\min_{x\in\Xi}\det\big[B_{i-1}(A_{i-1}+\Psi(x)\Psi(x)^T)^{-1}B_{i-1}^T\big]$, where $A_n=\sum_{j=1}^n \Psi(x_j)\Psi(x_j)^T$ and $B$ is the Jacobian of $S(c)$ w.r.t. $c$.","In benchmarks on Sobol, Ishigami, Environmental, Borehole, and Wing Weight functions (3–10 inputs) and on two finite-element models (Truss: 10 inputs; Heat transfer: 53 inputs), the proposed “Adaptive for SI” method shows lower mean error in estimated Sobol indices than Random, sequential D-optimal-for-coefficients, and LHS designs over many runs (200–400) with different initial designs. The paper reports that the superiority is statistically significant based on Welch’s t-test p-values shown in the result figures. The method’s advantage is most pronounced in deterministic or low-noise settings; performance degrades as additive noise increases (illustrated on the Sobol function with noise std 0, 0.2, 1.4). For high dimension (53), they recommend generating a large candidate set (e.g., LHS) and selecting a subset adaptively with their criterion, due to optimization complexity.","The authors note that the method relies on having an appropriate, sufficiently accurate PCE specification (choice of degree $p$ and hyperbolic truncation parameter $q$) known a priori and an initial training sample that is sufficiently large and non-degenerate. They state that if the PCE model is misspecified (e.g., quadratic PCE for a cubic function), the approach can perform worse than model-free designs. They also mention that the locally D-optimal optimization can yield duplicate design points (a common issue), and that high-dimensional optimization can be computationally prohibitive, motivating the use of a finite candidate set.","The locally D-optimal criterion depends on current coefficient estimates, so early-stage estimation error can steer the sequential design toward suboptimal regions (path dependence) and may require safeguards beyond the non-degeneracy checks described. Results are based on specific candidate-set constructions (uniform grids or LHS pools), so performance may be sensitive to candidate-set size/quality and may not generalize to constrained or irregular design spaces. The work focuses on first-order Sobol indices only; extensions to total-effect and higher-order indices are not evaluated and may change the optimality structure and Jacobians.",None stated.,"Extend the adaptive D-optimal criterion to target total-effect and higher-order Sobol indices (or joint estimation of main + total effects), and study trade-offs when optimizing multiple sensitivity functionals. Develop robust/self-starting variants that mitigate path dependence (e.g., Bayesian/local-robust optimal designs, regularized updates, or exploration-exploitation schedules) and explicitly handle noisy stochastic simulators. Provide open-source implementations and systematic guidance on choosing candidate-set size and PCE specification (adaptive $p,q$ selection) with built-in model-error monitoring.",1705.03944v1,https://arxiv.org/pdf/1705.03944v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:12:39Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Model discrimination|Prediction|Optimization|Cost reduction|Other,D-optimal|A-optimal|E-optimal|Bayesian D-optimal|Bayesian A-optimal|Other,"Variable/General (examples include k=1, k=2, k=4; design dimensionality up to nk≈2 orders of magnitude larger than prior work)",Pharmaceutical|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|C/C++,Package registry (CRAN/PyPI),https://CRAN.r-project.org/package=acebayes,"This paper introduces the R package acebayes for constructing Bayesian optimal designs by maximizing an expected utility in a decision-theoretic framework. The core method is the Approximate Coordinate Exchange (ACE) algorithm, which converts a high-dimensional design optimization into a sequence of one-dimensional conditional optimizations using Gaussian-process emulators of the (typically intractable) expected utility. The package supports fully Bayesian utilities (e.g., Shannon information gain, negative squared error loss, 0–1 utilities) with Monte Carlo or normal-based approximations, and also pseudo-Bayesian design via quadrature for Fisher-information-based criteria. It provides general-purpose functions (ace/pace) for user-defined utilities and wrappers (aceglm/acenlm and repeated versions) for common GLM and nonlinear models. Examples demonstrate design construction for parameter estimation, model selection (chemical reaction order), and prediction/cost trade-offs (Gaussian process sensor placement), enabling substantially larger nk than prior general Bayesian design methods.","A Bayesian optimal design d* maximizes expected utility U(d)=E_{\gamma,y\mid d}[u(\gamma,y,d)]=\int u(\gamma,y,d)\,\pi(\gamma,y\mid d)\,d\gamma\,dy, approximated by \tilde U(d)=\sum_{b=1}^B w_b\,u(\gamma_b,y_b,d). ACE optimizes \tilde U(d) coordinate-wise: for each design coordinate x_{ij}, it fits a 1D GP emulator \hat U_{ij}(x) from Q evaluations \{x_{ij}^q,\tilde U(d_{ij}^q)\} and sets x_{ij}^*=\arg\max_x \hat U_{ij}(x), accepting proposals via a Bayesian test (stochastic MC) or improvement check (deterministic quadrature). For pseudo-Bayesian criteria, common approximations include \tilde u_{SIGD}(\theta,d)=\log|I(\theta;d)| (D-type) and \tilde u_{NSELA}(\theta,d)=-\mathrm{tr}(I(\theta;d)^{-1}) (A-type), with expectation over the prior by quadrature.","The package is positioned as enabling a “step-change” in solvable Bayesian design complexity, supporting much larger numbers of variables and runs than prior general Bayesian design approaches (previously reported nk as small as 4 in much of the literature reviewed). In a pharmacokinetic nonlinear example (n=18), Phase II consolidation improved the pseudo-Bayesian D-criterion and reduced unique sampling times from 18 to 13 with an estimated relative D-efficiency of about 103% versus the Phase I design. In a logistic regression example (n=6,k=4), repeating ACE from multiple starts substantially improved performance: the best-of-10 pseudo-Bayesian A-design had ~118.5% relative A-efficiency compared to the first repetition. In a model-selection example using a 0–1 utility (n=20), the approximate probability of selecting the true model increased from ~0.806 (start design) to ~0.879 (ACE design). In a GP prediction/cost example (n=10), the optimized design had higher utility and much lower placement cost (reported cost drop from ~6.65 to ~3.08).","The authors note that ACE is heuristic and can be sensitive to the starting design, recommending multiple runs from different initial designs (e.g., via pace and/or parallel computing). They also state that Bayesian optimal design can still require significant computational resources, and for complex problems recommend implementing utility calculations in a low-level language (e.g., C/C++) and running on a computational cluster. They emphasize that the paper’s examples are illustrative and intentionally small to limit computation time.","Although acceptance tests are used to mitigate Monte Carlo noise, performance may depend strongly on tuning choices (B, Q, grid resolution for coordinate maximization, GP emulator settings), and the paper does not fully characterize robustness of solutions to these hyperparameters across problem classes. The coordinate-wise approach may struggle on highly coupled design spaces with strong interaction among coordinates or with complex constraints, potentially increasing risk of local optima even with multiple restarts. Practical guidance on diagnosing emulator misfit, choosing candidate grids under constraints, and managing computational scaling with expensive likelihood/posterior computations could be more standardized (e.g., automated stopping rules, adaptive Q/B). The work is primarily demonstrated on independent-observation models and does not deeply address common complications like autocorrelation, nonstationarity (for GP examples), or misspecification impacts on utility-based designs.","They mention acebayes has already been used for more complex scenarios such as high-dimensional design spaces and ordinary differential equation models, and suggest for complex problems users should implement utilities in low-level languages and use clusters. They also highlight repeating runs from different starts (potentially in parallel) as a practical direction for overcoming local optima in applied use. No detailed methodological agenda is laid out beyond these extensions/applications.","Developing principled, adaptive tuning of Monte Carlo effort (B) and emulator design size (Q) during optimization—e.g., allocating more simulation where coordinate decisions are uncertain—could improve efficiency and reliability. Extending the framework with stronger support for constrained and discrete design spaces (including mixed discrete/continuous factors) and providing theoretical/empirical guarantees on convergence quality would broaden adoption. More out-of-the-box support for correlated/longitudinal data, multivariate responses, and robust designs under model misspecification (including heavy tails/outliers) would address common real-world SPC/engineering settings. Finally, richer software support (diagnostics, benchmarks, and reproducible vignettes for large-scale problems; optional GPU/parallel backends) would help practitioners apply ACE to computationally expensive simulators and modern Bayesian workflows.",1705.08096v3,https://arxiv.org/pdf/1705.08096v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:13:09Z
TRUE,Optimal design|Bayesian design|Computer experiment|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (examples include 1, 2, and 100 parameters; sensors/QoI selected as design variables)",Environmental monitoring|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an optimal experimental design (OED) framework built on the consistent Bayesian approach to stochastic inverse problems, where the posterior is defined so that its push-forward through the computational model matches a prescribed observed data density almost everywhere. Designs are represented by choices of quantities of interest/sensor locations, and the optimal design maximizes expected information gain from prior to posterior measured by Kullback–Leibler divergence, with the expectation taken over a restricted family of plausible observed densities (e.g., truncated Gaussians centered in the model’s data range and weighted by the prior push-forward measure). A key computational advantage is that once the push-forward of the prior is estimated (via Monte Carlo and KDE), posteriors and information gains for many candidate observed densities/designs can be computed without additional model evaluations, enabling efficient batch selection over a discrete candidate set. The authors address feasibility issues when candidate observed densities place mass outside the model’s attainable data set, proposing normalization/rescaling strategies that enforce absolute continuity assumptions required by the consistent Bayesian formulation. Numerical studies on representative PDE-based models (convection–diffusion, transient diffusion/source localization, elastic inclusion, and porous-media flow with KL permeability fields) demonstrate how the method ranks sensor placements and extends to greedy/multi-sensor selection, and provides comparisons/consistency with prior Bayesian OED findings in a benchmark example.","Posterior (consistent Bayes) density: $\pi^{\n\text{post}}_{\Lambda}(\lambda)=\pi^{\n\text{prior}}_{\Lambda}(\lambda)\,\frac{\pi^{\n\text{obs}}_{D}(Q(\lambda))}{\pi^{Q(\text{prior})}_{D}(Q(\lambda))}$, where $Q$ maps parameters to QoI and $\pi^{Q(\text{prior})}_{D}$ is the prior push-forward density. Information gain for design $Q$ is KL divergence $I_Q=\int_{\Lambda} \pi^{\n\text{post}}_{\Lambda}\log\!\left(\pi^{\n\text{post}}_{\Lambda}/\pi^{\n\text{prior}}_{\Lambda}\right)d\mu_{\Lambda}$, rewritten as an integral w.r.t. the prior using the ratio $\pi^{\n\text{obs}}_{D}(Q(\lambda))/\pi^{Q(\text{prior})}_{D}(Q(\lambda))$. Expected information gain is $\mathbb{E}(I_Q)=\int_{D} I_Q(q)\,dP^{Q(\text{prior})}_{D}(q)$ over a restricted family of observed densities (e.g., truncated Gaussians centered at $q\in D$), and the OED is $Q_{\text{opt}}=\arg\max_{Q_z} \mathbb{E}(I_{Q_z})$.","For a motivating nonlinear algebraic example, using QoI $Q_1$ gives information gain $I_{Q_1}\approx 2.015$, using $Q_2$ gives $I_{Q_2}\approx 0.466$, and using both yields $I_Q\approx 2.98$, illustrating how QoI choice changes posterior concentration. In the stationary convection–diffusion sensor-placement example (single parameter amplitude), the top design locations achieve expected information gain around $\mathbb{E}(I)\approx 2.83$ (top-ranked points reported near $(0.56,0.56)$), and the spatial pattern concentrates near the source and along the convection direction. In the transient diffusion/source-location example (two parameters), the highest expected gains occur near domain corners with top designs around $\mathbb{E}(I)\approx 0.74$ at $(0,0)$ and $(1,1)$, consistent with prior results in the referenced benchmark study. For the linear elastic inclusion problem (two parameters), the best sensor locations near the inclusion yield large gains (top reported about $\mathbb{E}(I)\approx 4.57$ for 1,000 samples). In the porous-media KL example (100 parameters), the best single-sensor designs achieve $\mathbb{E}(I)\approx 2.01$, and greedy placement up to 8 sensors shows diminishing incremental gains and symmetric optimal patterns.","The authors restrict attention to batch (open-loop) design over a discrete candidate set of experiments, noting that coupling the method with continuous/gradient-based design optimization is left for future work. They acknowledge computational bottlenecks from estimating the prior push-forward density (especially via KDE, which scales poorly with the dimension of the observation/QoI space) and from Monte Carlo forward propagation requiring many model runs for accurate statistics. They also note that OED results can depend on how observational uncertainty (e.g., Gaussian standard deviations) is specified, and that infeasible data issues can arise when observed densities place mass outside the model’s attainable data region, requiring normalization/rescaling.","The framework assumes a user-specified family of observed-data densities (e.g., truncated Gaussians centered at attainable outputs) and an expectation taken with respect to the prior push-forward; if this family is misspecified relative to actual measurement noise or bias, the resulting OED may be suboptimal in practice. The feasibility fix via truncation/normalization addresses support mismatch but does not explicitly model structural/model discrepancy; if model inadequacy shifts real observations outside $Q(\Lambda)$, the method can still fail or produce misleading designs. Comparisons to alternative OED criteria are limited and the approach’s robustness to correlated/non-Gaussian measurement noise, dependent data, or sequential/adaptive experimentation is not fully explored. Practical implementation for high-dimensional QoI spaces may be challenging because KDE-based push-forward estimation can deteriorate rapidly, motivating alternative density-estimation or surrogate approaches not developed here.","They state that developing a computationally efficient approach for coupling the expected-information-gain objective with continuous optimization techniques (e.g., gradient-based methods in continuous design spaces) is left for future work. They also mention pursuing more efficient strategies beyond the discrete/batch search used here, including gradient-based methods on continuous design spaces, in future work.","Develop scalable push-forward density estimation for moderate/high-dimensional observation spaces (e.g., transport maps, normalizing flows, Gaussian-mixture surrogates, or copula-based factorizations) to avoid KDE’s curse of dimensionality. Extend the consistent-Bayes OED to explicitly include model discrepancy and bias (e.g., hierarchical/embedded discrepancy models) so feasibility is handled probabilistically rather than by truncation/renormalization alone. Provide sequential/adaptive (closed-loop) consistent-Bayes OED with online updating of the push-forward and experiment selection, including stopping rules and multi-fidelity/surrogate acceleration for expensive PDE models. Release reference implementations and benchmarking suites to enable reproducibility and broader comparisons against established Bayesian OED methods (Laplace, MCMC-based, mutual information estimators) across common testbeds.",1705.09395v1,https://arxiv.org/pdf/1705.09395v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:14:17Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Cost reduction,A-optimal|D-optimal|Other,Variable/General (linear model with parameter dimension m; examples include m=30 (synthetic) and m=8 (real dataset)),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper introduces ESP-design, a family of optimal experimental design criteria for linear regression based on elementary symmetric polynomials of the information matrix inverse, which interpolates between A-optimality (ℓ=1, trace) and D-optimality (ℓ=m, determinant). It formulates a combinatorial subset-selection problem under a budget k, proposes a convex continuous relaxation in fractional weights z, and proves geodesic log-convexity of ESPs on the positive definite cone, yielding a tractable convex program. Two practical algorithms are developed: randomized rounding/sampling from the relaxed solution and a greedy removal algorithm with approximation guarantees via an extension of volume-sampling bounds from determinantal (D-opt) to general ESP objectives. Empirically on synthetic data and the UCI Concrete Compressive Strength dataset, the greedy method (initialized from the relaxation support) matches the quality of Fedorov exchange while running substantially faster; varying ℓ trades off A-like predictive performance with D-like sparsity/robustness properties. The work unifies A- and D-optimal design within a graded spectral-polynomial framework and adds new theory for ESPs (geodesic log-convexity and generalized volume-sampling identities).","Linear model: $y_i = x_i^\top\theta + \varepsilon_i$ with independent Gaussian noise. For a chosen subset $S$ (|S|\le k), the covariance of the unbiased estimator is $(\sum_{i\in S} x_ix_i^\top)^{-1} = (X_S^\top X_S)^{-1}$. ESP-design objective: $\min_{S\in\Gamma_k}\; f_\ell(S)=\frac{1}{\ell}\log E_\ell\big((X_S^\top X_S)^{-1}\big)$, where $E_\ell(M)=e_\ell(\lambda(M))$ is the order-\ell elementary symmetric polynomial of eigenvalues (with $\ell=1$ giving A-opt and $\ell=m$ giving D-opt). Continuous relaxation: $\min_{z\in[0,1]^n,\;\mathbf{1}^\top z\le k}\; \frac{1}{\ell}\log E_\ell\big((X^\top\mathrm{Diag}(z)X)^{-1}\big)$.","The authors prove $E_\ell$ is geodesically log-convex on $\mathbb{S}_{++}^m$, implying the continuous relaxation is convex/log-convex and solvable efficiently (e.g., projected gradient). They show the relaxed optimum saturates the budget ($\|z^*\|_1=k$) and provide a support-size bound under a generic-position condition: $\|z^*\|_0\le k+\tfrac{m(m+1)}{2}$. They extend volume-sampling results to ESP objectives (Lemma/Theorem 4.2/4.3), yielding a multiplicative approximation bound for the greedy removal algorithm: $E_\ell((X_{S^+}^\top X_{S^+})^{-1})\le \prod_{j=1}^\ell\frac{n_0-m+j}{k-m+j}\,E_\ell((X_{S_0}^\top X_{S_0})^{-1})$. Experiments show GREEDY (initialized from relaxation support) achieves designs comparable to Fedorov exchange with far lower runtime, and on concrete-strength data (n=1030, m=8) smaller ℓ gives best prediction while larger ℓ yields sparser selected designs.","The dual formulation for general ℓ requires introducing a matrix mapping $a(H)$ whose closed form is unknown except in the special cases ℓ=1 and ℓ=m; consequently, the general ESP-design dual is described as “a purely theoretical object” at present. The authors note it remains open whether a closed form for $a(H)$ can be derived or whether $E_\ell(a(H))$ can be computed from $H$ alone.","The work assumes the classical linear-Gaussian model with independent noise and focuses on subset selection for a fixed design matrix; robustness to model misspecification, heteroskedasticity, or correlated errors is not studied. Empirical validation is limited (one main UCI dataset plus synthetic settings), and comparisons emphasize Fedorov exchange and a few baselines; broader benchmarking against modern optimal-design heuristics/solvers (e.g., advanced integer/convex optimization or other spectral criteria) is not shown. Implementation details are not fully reproducible because no code is shared and some components rely on external projection routines, making it harder to assess practical performance across environments.","They highlight open questions around the general Lagrangian dual: determining a closed form for the mapping $a(H)$ for intermediate ℓ, or computing $E_\ell(a(H))$ using only knowledge of $H$. They suggest that better understanding the general dual—motivated by the importance of the D-optimal dual as the Minimum Volume Covering Ellipsoid—could provide valuable insights for both optimal design and optimization theory.","Developing scalable, reproducible software (e.g., an R/Python package) and benchmarking on larger real experimental-design problems would improve adoption. Extending ESP-design to settings with unknown/estimated noise variance, heteroskedasticity, autocorrelation, or generalized linear/nonlinear models would broaden applicability. Investigating adaptive/sequential variants of ESP-design (active learning/sensor placement) and establishing tighter or instance-dependent approximation guarantees for the greedy/sampling methods would further strengthen the framework.",1705.09677v1,https://arxiv.org/pdf/1705.09677v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:14:55Z
TRUE,Other,Parameter estimation|Other,Not applicable,Variable/General (binary treatment DR signal + covariates; examples include 1 covariate and multi-dimensional covariates),Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,http://www.pecanstreet.org,"The paper frames demand response (DR) signals as a binary treatment and uses an experimental-design/causal-inference perspective to estimate the average treatment effect (ATE), i.e., the average change in electricity consumption attributable to receiving a DR signal. It studies three linear-estimation approaches for ATE: simple linear regression/difference-in-means (SLR), multiple linear regression including covariates (MLR), and the modified covariate method (MCM) that models treatment–covariate interactions while leaving the baseline consumption model unrestricted. The authors derive estimators and analyze (approximate) variances, showing that adding covariates via a naive MLR can worsen ATE estimation when treatment is rare and/or when treatment effects interact with covariates in certain ways; MCM can be preferable when treatment is scarce. Performance is evaluated using synthetic simulations, EnergyPlus-generated building data, and Pecan Street observational data, illustrating regimes where each estimator performs best and providing practitioner guidance for DR program impact estimation.","Potential-outcome model: $Y_i=T_iY_i(1)+(1-T_i)Y_i(0)=f(x_i)+g(x_i)T_i$, with ATE $\bar g=\frac{1}{N}\sum_i g_i$. Least-squares regression uses $\hat\beta=(W^\top W)^{-1}W^\top Y$; in SLR with regressor $Z_i=T_i-p$, the ATE estimate is the difference-in-means $\hat{\bar g}_{\mathrm{SLR}}=\frac{\sum_i T_i Y_i}{\sum_i T_i}-\frac{\sum_i (1-T_i)Y_i(0)}{\sum_i (1-T_i)}$ (equivalently eq. (6) in the paper). MCM uses modified covariates $v_i=(T_i-p)x_i$ and estimates $\hat{\bar g}_{\mathrm{MCM}}=\frac{1}{N}\sum_i x_i^\top \hat\gamma$ under $g_i=x_i^\top\gamma$.","In an EnergyPlus building scenario with constant treatment effect and $p=0.5$, normalized variance ranks MLR best (0.100) vs SLR (1.000) and MCM (1.506) (Table II). When the treatment effect is linear in covariates and treatment is rare ($p=0.15$), MLR performs worst with normalized variance 3.191 vs SLR 1.000 and MCM 1.020 (Table III). For Pecan Street data, ATE estimates differ substantially: $\hat{\bar g}_{\mathrm{SLR}}=1.16$, $\hat{\bar g}_{\mathrm{MLR}}=0.59$, $\hat{\bar g}_{\mathrm{MCM}}=0.90$ (Table IV), and t-test p-values indicate MLR can make the treatment look insignificant (t-test p-value $1.4\times10^{-2}$) compared with SLR ($2.7\times10^{-7}$) and MCM ($2.9\times10^{-9}$) (Table V).",None stated.,"The key assumption is randomized treatment assignment independent of covariates (Bernoulli with probability $p$); this may not hold in operational DR where targeting/selection and time-varying confounding are common, potentially biasing all regression-based ATE estimates. Several variance comparisons rely on approximations (second-order Taylor/ratio approximations) and simplifying cases (often 1D covariates), so conclusions may shift under heavy-tailed noise, heteroskedasticity, autocorrelation, or high-dimensional collinearity. The paper emphasizes linear estimators; it does not benchmark against modern doubly robust/propensity-score or causal-ML methods that can mitigate model misspecification under weaker assumptions.",The authors state that the work provides a framework for further research in applying causal inference to analyzing consumption data and DR interventions.,"Extend the estimators to settings with non-random/targeted DR (selection on observables) using propensity scores, doubly robust estimators, or instrumental variables appropriate for grid-triggered events. Develop self-starting or online/sequential designs/estimators that adapt $p$ or targeting to reduce ATE variance under limited DR events, and quantify robustness under autocorrelated load/temperature time series. Provide open-source implementations and additional real-world validations across different DR program types (price vs event-based, residential vs commercial) and multiple covariates/high-dimensional side information.",1706.09835v2,https://arxiv.org/pdf/1706.09835v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:15:30Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,TRUE,MATLAB,Not provided,NA,"This paper designs and experimentally implements high-fidelity three-qubit Toffoli and Fredkin quantum gates on a three-qubit NMR quantum information processor using genetic programming to optimize sequences of hard RF pulses and inter-pulse delays. Candidate pulse-sequence “chromosomes” encode pulse widths, phases, and delay times; a fitness function based on normalized trace overlap between the target and realized unitary is maximized, with additional localized optimization to reach >0.99 theoretical fidelity. The resulting pulse sequences are implemented on iodotrifluoroethylene (three 19F spins), and state tomography reports experimental fidelities of about 0.96 (Fredkin) and 0.93 (Toffoli), with substantial reductions in gate time versus transition-selective shaped-pulse implementations. Robustness is assessed against RF offset and flip-angle (miscalibration) errors by mapping fidelity over specified error ranges. The work advances practical quantum-control pulse engineering, but it is not about statistical design of experiments (DOE) in the classical sense (factorial/RSM/optimal design).","The optimization objective uses a gate-fidelity/overlap fitness: $F=\frac{|\mathrm{Tr}(U_{\mathrm{tgt}}U_{\mathrm{opt}}^{\dagger})|}{\sqrt{\mathrm{Tr}(U_{\mathrm{tgt}}U_{\mathrm{tgt}}^{\dagger})\,\mathrm{Tr}(U_{\mathrm{opt}}U_{\mathrm{opt}}^{\dagger})}}$. The implemented unitary is parameterized as a product over $N$ blocks of hard RF pulses and free evolutions, e.g. $U_{\mathrm{opt}}=\prod_{l=1}^N e^{-i(H_{\mathrm{NMR}}+\Omega I_{\phi_l})\tau_l} e^{-iH_{\mathrm{NMR}}\delta_l}$ (and for three homonuclear spins, $I_{\phi_l}$ is replaced by the sum over spins). The system Hamiltonian in the rotating frame is $H_{\mathrm{NMR}}=-\pi\sum_i(\nu_i-\nu_i^{\mathrm{rf}})\sigma_z^{(i)}+\sum_{i<j}\frac{\pi}{2}J_{ij}\sigma_z^{(i)}\sigma_z^{(j)}$.","Numerically optimized sequences achieve theoretical fidelities of about 0.993 (CNOT, ~7 ms), 0.99 (Fredkin, 51 ms), and 0.995 (Toffoli, 27 ms), and a 90° selective pulse with theoretical fidelity 0.995 (~107 µs). Experimental state-tomography fidelities reported include ~0.97 for the CNOT output state, ~0.96 for the Fredkin output state, and ~0.93 for the Toffoli output state (starting from a |110⟩ pseudopure state). Compared with transition-selective shaped-pulse implementations, Fredkin improves from fidelity 0.72 (242 ms) to >0.95 (51 ms) and Toffoli from 0.76 (168 ms) to >0.95 (27 ms), implying both higher fidelity (~14% absolute) and ~5–6× shorter durations. Robustness is explored over offset ±20 Hz and flip-angle error ±14°, with regions where fidelity remains >0.9 reported for each gate.",None stated.,"The optimization is tailored to a specific NMR hardware model (fixed pulse amplitude, discretized phase/delay resolutions, and a particular three-spin Hamiltonian), so portability to other platforms likely requires re-optimization and may not preserve performance. Performance evaluation is largely based on reported fidelities and robustness maps over limited error ranges; it does not provide comprehensive statistical uncertainty, repeated experimental trials, or comparisons to modern gradient-based optimal control (e.g., GRAPE) under matched constraints. The approach can be computationally expensive (hours per iteration and long local-optimizer runs), which may limit scaling to larger qubit counts or more complex gates. The method assumes accurate Hamiltonian parameters and does not deeply analyze sensitivity to model mismatch beyond offset/flip-angle errors.",None stated.,"Extend the approach to larger qubit registers and other multiqubit gates while studying scaling of runtime and solution quality, potentially via hybrid GA + gradient-based refinements. Evaluate robustness to broader experimental nonidealities (Hamiltonian parameter drift, RF inhomogeneity, relaxation during sequences) and report confidence intervals from repeated experimental runs. Provide open-source implementations and benchmarks against alternative quantum-control optimizers under identical constraints (hard pulses only, fixed amplitude, discretization). Adapt the optimization to other quantum hardware (superconducting qubits, trapped ions) with hardware-specific constraints and include systematic transferability studies.",1707.00289v1,https://arxiv.org/pdf/1707.00289v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:16:06Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Other,Prediction|Cost reduction|Other,Other,Variable/General (illustration uses 2 physical inputs plus 1 fidelity parameter),Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian sequential design method for multi-fidelity stochastic computer simulators to estimate the probability that a scalar simulator output exceeds a critical threshold at the highest fidelity level. The simulator output at inputs (x,t) is modeled as Gaussian with mean given a Gaussian process prior, yielding closed-form expressions for the posterior mean and variance of the exceedance probability p(x). The authors introduce an L2-integrated posterior variance criterion (a SUR criterion) as an uncertainty measure for estimating p(x) (and indirectly the global exceedance probability P under an input distribution). They then propose a cost-aware multi-fidelity sampling rule, Maximum Speed of Uncertainty Reduction (MSUR), which selects both the next physical input and the fidelity level by maximizing expected uncertainty reduction per unit simulation cost. Performance is demonstrated on a stochastic damped harmonic oscillator example with multiple discretization time steps as fidelity levels, showing MSUR matches the best fixed-fidelity SUR strategy without requiring prior knowledge of the best cost–accuracy tradeoff.","The local exceedance probability at highest fidelity is defined as $p(x)=\mathbb{P}(Z_{x,t_{HF}}>z_{crit})$ and under the Gaussian output model becomes $p(x)=\Phi\!\left(\frac{\xi(x,t_{HF})-z_{crit}}{\sqrt{\lambda(x,t_{HF})}}\right)$. The uncertainty measure is $H_n=\int_X \mathrm{Var}_n(p(x))\,\mu(dx)$, i.e., integrated posterior variance of $p$. The MSUR acquisition chooses the next run by $(x_{n+1},t_{n+1})=\arg\max_{(x,t)} \frac{H_n-J_n(x,t)}{C(x,t)}$, where $J_n(x,t)=\mathbb{E}_n(H_{n+1}\mid X_{n+1}=x,T_{n+1}=t)$ is the expected post-sampling uncertainty and $C$ is the simulation cost.","On the 2D oscillator example with 10 fidelity levels ($\Delta\in\{1,0.5,0.33,0.25,0.2,0.17,0.1,0.05,0.02,0.01\}$ s) and an initial nested design (180/60/20/10/5 runs on the five lowest fidelities), MSUR is compared to single-level SUR strategies under a fixed time budget of 20 (12 repetitions). The results show the best fixed fidelity among single-level strategies is $\Delta=0.05$ s, while very low fidelities are too biased and very high fidelities too costly. MSUR achieves mean-square-error performance comparable to this best single-level choice for both estimating the global exceedance probability $P$ and the function $p(x)$, without knowing the optimal fidelity level in advance.",The authors note that assuming conditional normality of the simulator output is a simplifying hypothesis that must be verified for each particular simulator. They also mention that using other (possibly nonparametric) output distributions would forfeit the convenient conjugacy of the Gaussian process prior with the Gaussian likelihood.,"The approach assumes known covariance function and known noise/variance function $\lambda(x,t)$ (and fixed hyperparameters in the illustration), which may be unrealistic and can materially affect sequential decisions if misspecified. The method also relies on independence of simulator runs conditional on latent functions, and does not address correlated simulation noise or common random numbers. Empirical evaluation is limited to a single synthetic example and does not include broader benchmarks, sensitivity to candidate set size (500 points/level), or computational scaling of evaluating $J_n(x,t)$ and the integrals over $X$ in higher dimensions.",None stated.,"Extend MSUR/SUR to settings with unknown hyperparameters (full Bayesian or sequential empirical Bayes) and to non-Gaussian/non-normal simulator outputs (e.g., warped GPs or heteroscedastic likelihoods). Develop scalable approximations for evaluating $J_n(x,t)$ and the integrals defining $H_n$ for higher-dimensional inputs, including batch/parallel and constrained optimization variants. Add robustness to model misspecification (e.g., biased low-fidelity models, nonstationarity) and validate on additional real multi-fidelity simulators with publicly available implementations.",1707.08384v1,https://arxiv.org/pdf/1707.08384v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:16:41Z
TRUE,Optimal design|Computer experiment|Other,Parameter estimation|Prediction|Robustness|Other,D-optimal|I-optimal (IV-optimal)|Space-filling|Compound criterion,Variable/General (examples: pump has 2 dimensionless predictors; heat exchanger has 5 dimensionless predictors from 9 base variables),Manufacturing (general)|Energy/utilities|Other,Simulation study|Other,TRUE,R|Other,Personal website,https://ocw.mit.edu/resources/res-tll-004-stem-concept-videos-fall-2013/videos/problem￾solving/dimensional-analysis/,"The paper develops methodology for designing dimensional analysis (DA) experiments when there are multiple responses, extending the Buckingham \(\Pi\)-Theorem to the multivariate-response setting and clarifying the condition \(A\subseteq \text{span}(B)\) under which all responses can be made dimensionless. For multivariate DA regression, it formulates linear model approximations for each response, derives the block-diagonal information matrix under independent/constant-variance response errors, and proposes a multivariate integrated prediction variance criterion (an averaged I-optimality objective) for design construction. The authors provide practical guidance for constructing exact designs by searching in the original-factor space \(\chi\) with a coordinate-exchange algorithm coupled to continuous 1D optimization (e.g., L-BFGS-B), while evaluating criteria in the transformed \(\log \chi_\pi\) space to handle irregular DA regions. They also outline a nonparametric (space-filling/uniform) design procedure on \(\log \chi_\pi\) using rejection sampling plus Fast Flexible Filling (FFF), and discuss Robust-DA compound criteria that trade off efficiency in DA space and original-factor space. Methods are illustrated with a pump-design example (two \(\pi\) groups) and a heat-exchanger example (nine base variables reduced to five \(\pi\) groups, two responses with different predictor subsets).","The multivariate DA model reduces to dimensionless form \(\tilde\pi=h(\pi_1,\ldots,\pi_{p-\operatorname{rank}(B)})\), where the number of dimensionless predictors is \(p-\operatorname{rank}(B)\) and response dimensionless groups are obtained by solving \(B y_i=-a_i\). For multivariate linear approximations \(y_j(x_i)=g_j(x_i)^\top\beta_j+\varepsilon_{ij}\), the information matrix is \(M(\xi_n)=\sum_{i=1}^n F(x_i)W F(x_i)^\top\), which becomes block diagonal under independent constant response variances. The multivariate I-type objective is the averaged integrated prediction variance \(IMV(\xi_n)=r^{-1}v_\chi^{-1}\sum_{i=1}^r w_i^{-1}\,\mathrm{Trace}(D_i M_i)\); Robust-DA uses a compound criterion \(c(\xi_n,w)=wE_{\log\chi_\pi}(\xi_n)+(1-w)E_\chi(\xi_n)\).","For the heat-exchanger example, nine base variables are reduced to five dimensionless predictors, yielding responses \(\pi_0^{(1)}\) depending on \(\pi_1,\ldots,\pi_5\) and \(\pi_0^{(2)}\) depending only on \(\pi_1,\pi_3,\pi_4\). Using the multivariate I-type criterion for full third-order models with \(n=100\), the maximin design over response-variance weights occurs near \(w_1^*\approx0.35\), giving about 97% I-efficiency for both response models; designing for the larger model alone still yields about 92% efficiency for the smaller model. A uniform (space-filling) design on \(\log\chi_\pi\) has low I-efficiency (~29%) relative to the I-optimal parametric design in this example. For Robust-DA designs in the heat-exchanger example, the maximin tradeoff occurs at \(w\approx0.35\), with efficiencies about 83% (DA-space model) and 85% (original \(\chi\)-space quadratic empirical model).",None stated.,"The approach relies heavily on selecting polynomial surrogate models (e.g., full 3rd/4th order) in transformed \(\pi\)-spaces; if the true \(\phi\) is not well-approximated, optimality may not translate to good practical performance. The assumed response-error structure (uncorrelated responses and constant variance across the region) may be unrealistic in multiresponse engineering systems; correlated/heteroscedastic responses would change the information matrix and design criterion. Nonparametric uniform designs are evaluated mainly by I-efficiency against a chosen parametric model, which can disadvantage space-filling designs intended for broader model uncertainty; broader benchmarking (e.g., across multiple candidate models/kernels) is not developed. The paper indicates code is available but does not provide a persistent repository link in the text, limiting reproducibility.",None stated.,"Extend the multivariate DA design criteria to allow correlated responses and heteroscedasticity (non-constant \(W(x)\)), including estimation/robustification when \(W\) is unknown. Develop Bayesian or model-averaged DA design formulations that explicitly encode uncertainty about the functional form \(\phi\) and compare against uniform/space-filling alternatives on predictive loss. Provide scalable algorithms and software (with an archived repository) for high-dimensional DA problems where \(\log\chi_\pi\) rejection sampling becomes inefficient. Investigate sequential/adaptive DA experimentation, where early runs help refine the DA model, choose \(\pi\)-groups, or tune robust-DA tradeoff weights.",1708.01481v2,https://arxiv.org/pdf/1708.01481v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:17:20Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Prediction|Parameter estimation|Cost reduction|Other,Other,Variable/General (tomography examples use design vector p with ℓ candidate angles in Problem A; fixed ℓ<m angles in Problem B; examples include ℓ=2),Healthcare/medical|Other,Simulation study|Other,TRUE,Julia|MATLAB|Other,Not provided,http://arxiv.org/abs/1606.07399,"The paper develops optimal experimental design (OED) methods for linear ill-posed inverse problems with linear equality and inequality constraints on the unknown state (e.g., nonnegativity and box constraints in tomographic reconstruction). It proposes two OED formulations to reduce measurement cost: (A) a sparsity-promoting design over a finely discretized set of candidate measurements using an ℓ1 penalty to select a small subset, and (B) a low-dimensional parameterized design that optimizes the placement of a fixed number of measurements (e.g., projection angles). The authors study both Bayes risk minimization (leading to tractable expressions when the state problem is unconstrained) and empirical Bayes risk minimization using training images, enabling constrained state estimation without requiring explicit closed-form inner solutions. A key contribution is an efficient derivative-based bilevel optimization approach: for constrained state problems they use a relaxed primal-dual interior-point method to ensure differentiability and apply implicit differentiation of KKT conditions to obtain sensitivities, with matrix-free products and parallelization over training samples. Numerical tomography experiments on multiple synthetic and phantom datasets show that including state constraints materially changes the optimal designs and can achieve lower reconstruction MSE with fewer projections compared to unconstrained designs.","The OED problem minimizes Bayes risk plus measurement cost, e.g. $\min_{p\in\Omega} J(p)=\mathbb{E}\,\tfrac12\|\hat f(p)-f_{\text{true}}\|_2^2+R(p)$, where $\hat f(p)$ is the MAP estimator from a (possibly constrained) Tikhonov-regularized inverse problem. In OED Problem A, sparsity is enforced with $R(p)=\beta\|p\|_1$ and $p\ge 0$ to select a subset of candidate measurements; in Problem B, $p$ directly parameterizes a fixed number of measurement locations (angles). For unconstrained state problems the Bayes-risk objective simplifies to a Frobenius norm of a pseudoinverse, e.g. $J(p)=\tfrac{1}{2\sigma^2}\|M_\alpha(p)^\dagger\|_F^2$ with $M_\alpha(p)=[M(p);\,\alpha L]$, and constrained empirical Bayes OED uses KKT conditions of an interior-point QP to compute $\nabla_p J_N(p)$ via implicit differentiation.","For unconstrained state problems, the Bayes-risk OED objective admits a closed-form reduction to minimizing $\|M_\alpha(p)^\dagger\|_F^2$, enabling efficient derivative-based optimization and additional simplifications via (generalized) SVD. In empirical Bayes OED, the paper demonstrates via tomography simulations (multiple training datasets and constraint types) that inequality constraints (nonnegativity/box) can substantially reduce MSE compared with unconstrained/equality-only reconstructions and alter the globally optimal projection-angle designs. In sparse-design Problem A, increasing the sparsity weight $\beta$ reduces the number of selected projections, and constrained reconstructions can achieve lower MSE even with fewer projections. For binary-image datasets, the constrained OED recovers near-intuitive optimal angle sets (e.g., rectangles near $[0,90]^\top$; pentagons near five angles aligned with edges) more reliably than unconstrained designs.","The Bayes risk minimization development assumes no inequality constraints in the state problem (the paper focuses on the unconstrained case for simplicity, noting equality constraints could be handled but are not pursued). The empirical Bayes (training-data) formulation is highlighted as potentially infeasible for very large numbers of training samples because solving many lower-level problems can be computationally intensive (the authors note SAA may not be feasible for very large datasets).","The methodology is developed for linear forward models with Gaussian noise and (truncated) Gaussian priors; performance under strong model mismatch, non-Gaussian noise, or heavy-tailed errors is not addressed. The constrained sensitivity approach relies on interior-point relaxation and assumes an invertible KKT system/unique KKT point; numerical robustness under degeneracy, near-active constraint switching, or large-scale ill-conditioning is not deeply characterized. The design variables for Problem A are relaxed continuous weights with an ℓ1 penalty; resulting designs may require thresholding/post-processing and may not map directly to truly discrete measurement selection without additional combinatorial guarantees. Empirical comparisons are largely simulation-based in tomography; broader real-world validations and comparisons against alternative modern sparse/optimal design heuristics are limited.","The authors propose extending the framework to include integer or binary constraints on the design parameters, extending the approach to nonlinear inverse problems, and applying the methodology to other application domains beyond the tomography examples studied.","Developing self-starting/online (sequential) OED strategies that update designs as data arrive could improve practicality in adaptive imaging. Extending to correlated and/or unknown noise covariance (estimating $\Gamma_\varepsilon(p)$ jointly) and to model uncertainty in $M(p)$ would better match real acquisition systems. Providing open-source, fully reproducible code (e.g., a public jInv module and MATLAB phantom generator release) with benchmarks would strengthen adoption. Studying alternative optimality criteria (e.g., A-, D-, or I-optimality for prediction/estimation tradeoffs under constraints) and offering guidance for tuning hyperparameters like $\alpha$ and $\beta$ would improve usability.",1708.04740v1,https://arxiv.org/pdf/1708.04740v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:18:00Z
TRUE,Optimal design|Bayesian design|Other,Prediction|Parameter estimation|Other,A-optimal|Bayesian A-optimal|Compound criterion|Other,Variable/General (illustrated with 1D design variable x in straight-line regression with p=2 parameters),Healthcare/medical|Theoretical/simulation only|Other,Other,TRUE,R|Other,Not provided,https://cran.r-project.org/web/packages/OptimalDesign/index.html,"The paper develops computational methods for optimal experimental design on a finite design space under a compound Bayes risk criterion (CBRC) of the form \(\Phi(\xi)=\sum_{j=1}^s \operatorname{tr}((M(\xi)+B_j)^{-1}H_j)\), which includes linear prediction criteria in random coefficient regression (RCR) models. The main contribution is a reformulation that converts CBRC-optimal design into a constrained A-optimal design problem in an artificial/extended model with additional linear constraints, enabling the use of efficient second-order cone programming (SOCP) and mixed-integer SOCP (MISOCP) solvers for approximate and exact designs. The approach accommodates nonstandard linear design constraints (e.g., replication-free or spacing constraints) without bespoke CBRC optimization code. An RCR straight-line example (random intercept/slope) demonstrates computation of IMSE-prediction-optimal approximate and exact designs and shows how optimal allocations/support points change with the slope-variance parameter. The authors highlight that these computations can be carried out using existing A-optimality mathematical-programming toolchains (e.g., via the R package OptimalDesign).","Design information matrix: \(M(\xi)=\sum_{x\in\mathcal X}\xi(x) f(x)f(x)^\top\). Compound Bayes risk criterion (CBRC): \(\Phi(\xi)=\sum_{j=1}^s \mathrm{tr}((M(\xi)+B_j)^{-1}H_j)\). In the RCR prediction setting, the linear/IMSE prediction criteria reduce to a two-term CBRC, e.g. \(\mathrm{IMSE}_{\mathrm{pred}}(\xi)=\mathrm{tr}(M(\xi)^{-1}V) + (n-1)\mathrm{tr}((M(\xi)+D^{-1})^{-1}V)\). Theorem 1 shows CBRC minimization is equivalent to constrained A-optimality in an artificial model, minimizing \(\Phi_A(\tilde\xi)=\mathrm{tr}(\tilde M(\tilde\xi)^{-1})\) under linear coupling constraints across replicated design-space copies plus fixed auxiliary-point masses.","The key result is Theorem 1: any solution of a constrained A-optimal design problem on an extended design space yields a CBRC-optimal design when projected back to the original space. In the straight-line RCR example with \(n=100\), \(m=10\), \(d=51\) grid points and \(D=\mathrm{diag}(\delta_1,\delta_2)\) with \(\delta_1=0.01\), the unconstrained IMSE-optimal exact designs concentrate all trials at endpoints \(x=0\) and \(x=1\), with allocations shifting from (5,5) toward (1,9) as the rescaled slope variance \(\rho=\delta_2/(1+\delta_2)\) increases (Table 1). Under the additional spacing constraint \(\xi(x_k)+\xi(x_{k+1})+\xi(x_{k+2})\le 1\), optimal exact designs place one observation at each of 10 support points whose locations move with \(\rho\) (Table 2), and an example approximate design for \(\rho=0.1\) is provided (Table 3). The reported mean computing times per \(\rho\) value are ~0.30s (approximate) and ~4.79s (exact) without extra constraints and ~0.33s (approximate) and ~3.40s (exact) with the spacing constraint (on the stated Windows 7/i5/8GB machine).",None stated.,"The paper’s numerical evidence is primarily illustrative (focused on a low-dimensional straight-line RCR model on a discretized 1D grid), so empirical performance and scalability to higher-dimensional design spaces or larger parameter dimensions are not thoroughly benchmarked. Practical use may depend on access to robust SOCP/MISOCP solvers and careful tuning; solver choice and failure modes are not deeply discussed. The framework assumes a finite design space (discretization for continuous regions), which can introduce approximation error and computational burden as the grid grows.",None stated.,"Extending the computational study to larger-scale and higher-dimensional design spaces (including comparisons of solver performance and scalability across different conic/MIP solvers) would strengthen practical guidance. Developing and releasing reproducible scripts (or an extension to the OptimalDesign package) specifically for CBRC-to-A-optimality conversion would improve adoption. Methodological extensions to handle model misspecification, correlated errors/autocorrelation, or fully continuous design regions (with adaptive discretization/mesh refinement) would broaden applicability, especially in biomedical longitudinal studies.",1709.02317v1,https://arxiv.org/pdf/1709.02317v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:18:35Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Prediction|Other,Other,Variable/General (design vector $\xi\in\Xi$; examples include 1 design variable in scalar models; 2 design variables in EIT sensor placement; parameter dimension examples $d=1$ and $d=2$),Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Approximation methods|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a fast computational approach for optimal Bayesian experimental design based on maximizing expected information gain (expected KL divergence) between prior and posterior. It introduces a Laplace-based importance sampling strategy (DLMCIS) that replaces the inner-loop sampling distribution in the classical double-loop Monte Carlo (DLMC) estimator with a Gaussian proposal given by a Laplace approximation centered at the MAP estimate, reducing variance and mitigating arithmetic underflow without adding bias. The authors also analyze and optimize estimator parameters (outer/inner sample sizes and, for PDE models, mesh size) by minimizing expected computational work subject to an error tolerance and confidence constraint. Performance is demonstrated on (i) a linear scalar model (Laplace exact), (ii) a nonlinear scalar benchmark (Laplace introduces bias for MCLA), and (iii) optimal sensor placement in electrical impedance tomography for composite laminates (PDE forward model), showing orders-of-magnitude reductions in required inner samples relative to DLMC and improved robustness relative to Laplace-only (MCLA) when Laplace bias is limiting.","The design criterion is the expected information gain $I=\mathbb{E}[D_{\mathrm{kl}}]$ with $D_{\mathrm{kl}}(Y)=\int \pi(\theta\mid Y)\log\{\pi(\theta\mid Y)/\pi(\theta)\}\,d\theta$, equivalently $I=\int\!\int \log\{p(Y\mid\theta)/p(Y)\}\,p(Y\mid\theta)\,dY\,\pi(\theta)\,d\theta$ (Eq. 5). The proposed estimator is the double-loop Monte Carlo with Laplace-based importance sampling: $\hat I_{\mathrm{DLMCIS}}=\frac1N\sum_{n=1}^N\log\Big(\frac{p(Y_n\mid\theta_n)}{\frac1M\sum_{m=1}^M L(Y_n,\tilde\theta_{n,m})}\Big)$ where $L(Y_n,\tilde\theta)=p(Y_n\mid\tilde\theta)\,\pi(\tilde\theta)/\tilde\pi_n(\tilde\theta)$ and $\tilde\pi_n\approx\mathcal N(\hat\theta_n,\hat\Sigma(\hat\theta_n))$ is the Laplace approximation around the MAP (Eqs. 33–36).","DLMCIS reduces the required inner-loop sample size $M^*$ by several orders of magnitude relative to standard DLMC in the numerical examples (e.g., for the nonlinear scalar model, DLMCIS can achieve accuracy $10^{-3}$ with $M^*=5$ while DLMC requires $M^*=10^5$ as reported in Example 2). In the EIT sensor-placement example, to reach about $\mathrm{TOL}\approx 0.01$, DLMCIS uses $M^*=1$ whereas DLMC needs $M^*\sim 10^3$, and MCLA cannot achieve accuracies better than about $\mathrm{TOL}\sim 1$ due to Laplace bias. The method is shown to mitigate arithmetic underflow in the evidence (inner integral) computation by changing measure to a Laplace-based proposal, and it preserves asymptotic unbiasedness unlike Laplace-only approximations. In the EIT design study, the maximum expected information gain over the scanned design space occurs at $\xi=(2.0,0.6)$ (as reported with Fig. 19).","The paper notes that MCLA (Laplace-only single-loop) introduces an additional bias except when the Laplace approximation is exact, and that this bias limits achievable accuracy (scaling as $O(1/N_e)$ with the number of repeated experiments). For DLMCIS, the authors note an additional computational cost: constructing the Laplace proposal requires solving an optimization problem to find the MAP, which in their nonlinear example required about 30 extra forward solves per outer sample. They also remark the analysis/optimization depends on estimating constants (work and error model constants) in practice from pilot runs.","The approach assumes the MAP can be found reliably for each outer sample and that the Laplace Gaussian approximation is a good proposal; for multimodal or highly non-Gaussian posteriors, MAP-based Gaussian proposals can miss relevant mass and degrade importance sampling efficiency. The forward model is assumed deterministic with additive Gaussian observational noise and i.i.d. measurements; performance under model discrepancy, correlated errors, or time-series dependence is not addressed. The method’s practicality for large parameter dimensions (high-dimensional $\theta$) is unclear because repeated MAP optimization and Hessian/Jacobian calculations may become prohibitively expensive or unstable. No reusable open-source implementation is provided, which may hinder reproducibility and adoption.",None stated.,"Extend DLMCIS to handle non-Gaussian and/or correlated observation models (e.g., heavy-tailed noise, time-correlated data) and model discrepancy. Develop variants that reduce per-outer-sample optimization cost (e.g., warm starts, surrogate models, adjoint-based gradients/Hessians, or quasi-Newton updates) and investigate scalability to higher-dimensional parameter spaces. Explore robust/multimodal proposals (mixtures of Laplace approximations or tempering/sequential Monte Carlo) to improve performance when the posterior is not well-approximated by a single Gaussian. Provide an open-source implementation and standardized benchmarks for Bayesian OED (including EIG optimization over $\xi$) to facilitate reproducibility and practitioner use.",1710.03500v1,https://arxiv.org/pdf/1710.03500v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:19:23Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Case study (real dataset),TRUE,MATLAB,Not provided,https://youtu.be/jJWeI5ZvQPY,"The paper proposes two decentralized control methodologies for cooperative manipulation of a rigidly grasped object by N robotic agents without using force/torque measurements. The first is an adaptive nonlinear controller using quaternion feedback for object orientation to avoid representation singularities, with adaptation laws to compensate for unknown dynamic parameters and structured disturbances. The second is a model-free robust Prescribed Performance Control (PPC) approach that enforces user-specified transient and steady-state tracking bounds on the object pose and velocity errors. Both controllers incorporate a load distribution scheme based on a grasp-matrix pseudo-inverse that avoids undesired internal forces. The methods are validated via simulations with four UR5 manipulators and experiments with two WidowX arms, demonstrating bounded signals and successful trajectory tracking under uncertainties/disturbances (and within prescribed funnels for PPC).","Adaptive controller: reference velocity $v_f = v_d - K_f e$ with quaternion error $e_\zeta = \zeta_d \otimes \zeta_O^+$ and control per agent $u_i = Y_i(\cdot)\hat\vartheta_i + \delta_i(\cdot)\hat d_i + J_{M_i}(q)\big(-Y_O(\cdot)\hat\vartheta_O + \delta_O(\cdot)\hat d_O - K_v e_{v_f}\big)$, with adaptation laws $\dot{\hat\vartheta}_i \propto -Y_i^T J_{O_i} e_{v_f}$ and $\dot{\hat d}_i \propto -\delta_i^T J_{O_i} e_{v_f}$. PPC: normalized errors $\xi_s=\rho_s^{-1}e_s$, transformed $\varepsilon_s=\ln\frac{1+\xi_s}{1-\xi_s}$, reference velocity $v_r=-g_s J_O(\cdot)^{-1}\rho_s^{-1} r_s(\xi_s)\varepsilon_s$, and decentralized control $u_i=-g_v J_{M_i}(q)\rho_v^{-1} r_v(\xi_v)\varepsilon_v$ enforcing $|e_{s,k}(t)|<\rho_{s,k}(t)$.","Simulations: four UR5 arms track a 6D sinusoidal pose trajectory; for the adaptive controller, pose/velocity errors converge and adaptation errors remain bounded, with an artificial torque saturation limit $\bar\tau=150$ Nm respected. For PPC, pose and velocity errors remain strictly within the specified exponential funnels (e.g., $\rho_{s,k}(t)=(|e_{s,k}(0)|+0.09)e^{-0.5t}+0.01$, $\rho_{v,k}(t)=(|e_{v,k}(0)|+0.95)e^{-0.5t}+0.05$) and torques respect saturation. Experiments: two WidowX arms track a planar trajectory; both controllers achieve tracking with bounded errors, and PPC keeps errors within prespecified bounds; motor torque limits were set conservatively (MX-64: 3 Nm, MX-28: 1.25 Nm) and respected. A comparison to prior PPC work [46] shows that without input-bound calibration, large initial torque peaks can violate saturation despite good tracking performance.","The PPC approach is ill-posed near Euler-angle representation singularities (e.g., pitch $\theta=\pm\pi/2$), so it cannot handle trajectories that reach those configurations; the quaternion-based adaptive method can. The adaptive controller relies on known structure/parameterization of dynamics (regressor form) and is described as less robust to unmodeled dynamics than PPC in real-time scenarios. The paper assumes rigid grasping points and known constant object-to-grasp offsets for each agent, and assumes operation away from kinematic singularities.","The evaluation is limited to small team sizes (sim: 4 arms; exp: 2 arms) and specific robots/objects; scalability to larger N and diverse geometries/grasps is not empirically demonstrated. The decentralization relies on agents being able to reconstruct object pose/velocity from local kinematics and known offsets; sensitivity to calibration errors, compliance, and sensor noise is not thoroughly quantified. No open-source implementation is provided, limiting reproducibility, and comparisons to a broader set of modern decentralized cooperative manipulation controllers are limited.","Future work is aimed at extending the techniques to non-rigid (non-rigidly constrained) grasping/contact points and to uncertain object geometric characteristics (unknown offsets). The authors also mention pursuing approaches to avoid quaternion unwinding/global stabilization issues, including discontinuous/hybrid quaternion control combined with adaptation, and alternatives such as tracking directly on $SO(3)$.","Developing systematic gain-tuning/auto-tuning methods for PPC under unknown dynamic bounds (and guaranteeing actuator constraints) would improve practical deployment. Extending both controllers to handle measurement noise, time delays, and communication-limited partial information (e.g., when some agents cannot reconstruct object pose) with formal robustness guarantees would broaden applicability. Providing an open-source implementation and standardized benchmarks (including non-rigid grasps, compliant contacts, and varying payloads) would enable reproducible comparisons and adoption.",1710.11088v4,https://arxiv.org/pdf/1710.11088v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:20:05Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Other,A-optimal|E-optimal|D-optimal|Other,"Variable/General (parameter dimension $p$; examples use $p=20$; design size $k$ varies, e.g., $k=40$ and $k=10$ to $100$)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://pages.cs.wisc.edu/~jerryzhu/research/ssl/semireview.html|http://www.gatsby.ucl.ac.uk/~chuwei/data/EachMovie/,"The paper studies greedy algorithms for optimal experimental design under A- and E-optimality criteria, where standard (supermodularity-based) greedy guarantees do not apply because these objectives are generally not supermodular. It introduces two notions of approximate supermodularity—multiplicative $\alpha$-supermodularity and additive $\varepsilon$-supermodularity—and derives nonasymptotic worst-case greedy suboptimality bounds for each. It then proves explicit lower/upper bounds on the supermodularity-violation measures for A-optimality (trace/MSE of the Bayesian posterior covariance) and E-optimality (largest eigenvalue of that covariance) in terms of quantities such as experiment SNR, prior covariance, and conditioning of a task matrix $H$. The bounds imply that as SNR decreases (or the prior becomes more concentrated), the objectives behave increasingly like supermodular functions, so greedy selection approaches the classical $(1-1/e)$ guarantee. Numerical experiments on synthetic linear-Gaussian models and a recommender-system “cold-start” survey design (EachMovie dataset) illustrate that greedy designs perform competitively versus convex relaxations and far better than random selection, especially in low-SNR settings.","Experiments are linear with noise: $y_e = A_e\theta + v_e$, and the target is $z=H\theta$. For a design multiset $\mathcal D$, the Bayesian estimator is $\hat z_{\mathcal D}=H\left(R_\theta^{-1}+\sum_{e\in\mathcal D}M_e\right)^{-1}\left(\sum_{e\in\mathcal D}A_e^TR_e^{-1}y_e + R_\theta^{-1}\bar\theta\right)$ with $M_e=A_e^TR_e^{-1}A_e$, and posterior error covariance $K(\mathcal D)=H\left(R_\theta^{-1}+\sum_{e\in\mathcal D}M_e\right)^{-1}H^T$. The design criteria are A-optimality $\min_{|\mathcal D|\le k}\operatorname{Tr}(K(\mathcal D))$ (up to constants), E-optimality $\min_{|\mathcal D|\le k}\lambda_{\max}(K(\mathcal D))$, and (for comparison) D-optimality $\min_{|\mathcal D|\le k}\log\det(K(\mathcal D))$.","For A-optimality, the objective is shown to be $\alpha$-supermodular with a bound $\alpha(a,b)\ge \kappa(H)^{-2}\,\frac{\lambda_{\min}(R_\theta^{-1})}{\lambda_{\max}(R_\theta^{-1})+a\,\ell_{\max}}$, where $\ell_{\max}=\max_e \lambda_{\max}(M_e)$; this yields a greedy multiplicative guarantee that approaches $(1-e^{-1})$ as SNR decreases. Under simplified assumptions ($R_\theta=\sigma_\theta^2 I$, $H=I$, and per-experiment SNR bounded by $\gamma$), they obtain $\bar\alpha \ge 1/(1+2k\sigma_\theta^2\gamma)$ so $\bar\alpha\to 1$ as $\gamma\to 0$. For E-optimality, the objective is $\varepsilon$-supermodular with $\varepsilon(a,b)\le (b-a)\,\sigma_{\max}(H)^2\,\lambda_{\max}(R_\theta)^2\,\ell_{\max}$, yielding an additive greedy bound that tightens as SNR decreases (with a simplified bound $\bar\varepsilon\le 2k\sigma_\theta^4\gamma$ under the same assumptions). Simulations with $p=20$ and an experiment pool of size $|E|=200$ show the “equivalent” $\hat\alpha$ approaching 1 and greedy A/E-optimal designs matching or closely tracking convex-relaxation-based baselines, especially at low SNR; the EachMovie case study reports substantially better MAE and genre classification error for greedy selection versus random movie selection.","They note that removing the Bayesian prior (unregularized/non-Bayesian design using a pseudoinverse) breaks monotonicity of the covariance set function, and providing guarantees in that scenario is left as future work. They also remark that the derived bounds (e.g., for E-optimality) can degrade in high-SNR regimes and that tighter bounds may be possible when designs are constructed without replacement (no repeated experiments).","The main theoretical results are worst-case and can be quite loose; the practical tightness of the $\alpha$ and $\varepsilon$ bounds is not systematically characterized across structured design matrices (beyond brief discussion and synthetic tests). The framework assumes independent noise across experiments and relies on a Bayesian/regularized model; robustness to model misspecification, correlated noise, and non-Gaussian/heavy-tailed settings is not established. The paper does not provide implementation details or computational complexity comparisons for greedy versus convex methods on truly large-scale instances, nor packaged software for practitioners.","They explicitly identify extending guarantees to the unregularized (non-Bayesian) case where monotonicity may fail as future work. They also ask whether the results can be improved by exploiting structure in observation matrices (e.g., Fourier/random), how sparsity with unknown support (compressive sensing) might be incorporated, and what experiment properties beyond SNR might lead to smaller supermodularity violations; they suggest extending the approximate-supermodularity framework to other problems.","Develop self-starting/online variants where SNR or noise covariances are unknown and must be estimated, and analyze how this affects approximate supermodularity and greedy guarantees. Extend the analysis to correlated-across-experiment noise and to batched/parallel or adaptive (sequential) designs where measurements influence future selection. Provide sharper, instance-dependent certificates (e.g., data-dependent bounds computed from spectra of realized information matrices) and release reference implementations to support reproducible benchmarking on standard experimental-design and sensor-selection datasets.",1711.01501v2,https://arxiv.org/pdf/1711.01501v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:20:55Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This technical note outlines planned beam experiments and diagnostic measurements at Fermilab’s FAST electron linac and the IOTA storage ring using synchrotron light detected by a gated microchannel-plate photomultiplier (MCP-PMT). It describes the experimental setup (light extraction through a quartz window, mirror/lens transport, variable neutral-density filter, gated detection) and a data-acquisition architecture with analog integration and digital counting/timing chains. The paper estimates expected photon and photoelectron yields across several beam-energy/dipole configurations using synchrotron-radiation formulas, optical transmission, and detector quantum efficiency, and argues that single-electron detection in IOTA should be feasible if backgrounds are sufficiently low. It discusses likely background sources (thermionic emission, radiation, afterpulsing, light leaks) and mitigation via gating and possible optical-path delay. Overall, it is an instrumentation/beam-diagnostics design note rather than a statistical design-of-experiments (DOE) methodology paper.","Key calculations are based on classical synchrotron-radiation formulas (for long dipole magnets) to obtain spectral photon flux and total photons per electron per pass, then multiplied by wavelength/energy-dependent optical transmission and the MCP-PMT quantum efficiency to obtain the average collected photoelectrons per electron per pass $N_{pe}$. The note reports resulting yields (e.g., for IOTA 150 MeV, $N_{pe}\approx 1.8\times 10^{-4}$ photoelectrons per electron per pass) and uses the revolution period/frequency to translate this to expected counting rates for single-electron storage.","For a single electron circulating in IOTA (revolution period 133 ns, 7.52 MHz), the expected average counting rate is about 1.4 kHz, with negligible probability of two photons emitted in the same dipole on a given pass. The computed photoelectron yield for a representative case (IOTA 150 MeV) is about $1.8\times 10^{-4}$ photoelectrons per electron per pass; comparable yields are shown for multiple energies at the D600 dipole and IOTA dipoles (Table II). The similarity of spectra/fluxes between selected linac (D600) energies and IOTA enables commissioning tests at the linac prior to IOTA operations. The authors suggest single-electron intensity steps should be detectable with integration times on the order of 1 s provided backgrounds/fluctuations remain below the signal.","The authors note that feasibility depends on background levels and their fluctuations being low compared with the signal, motivating preliminary studies at the D600 location to assess backgrounds. They also state that the time-to-digital conversion/timing system is under development and that the effect of the magnetic field on photomultiplier response needs investigation.","The work does not present a formal experimental plan (factor settings, randomization, replication, blocking) or an explicit DOE framework for efficiently exploring backgrounds and systematics; thus practical tuning may rely on ad hoc iteration. The signal estimates depend on manufacturer-provided transmission/quantum-efficiency curves and idealized light-collection assumptions (e.g., collecting the full radiation cone), which may differ from in-situ alignment and acceptance losses. No quantitative uncertainty propagation or sensitivity analysis is provided for key assumptions (alignment, window/mirror degradation, stray light, discriminator thresholds, dead time). There is no demonstrated real-data validation in this note, so achievable SNR and detection limits under operational radiation conditions remain uncertain.",They propose preliminary experiments at the D600 dipole to characterize signal/backgrounds and test both current-mode and counting-mode readout (including using a neutral-density filter for low-intensity tests). They also plan to investigate additional background-reduction techniques such as delaying the synchrotron-light pulse via a longer optical path. The authors highlight the value of adding precise signal timing (<10 ps) to enable direct observation of the time structure of photon emission from single particles in a storage ring.,"A structured, quantitative measurement plan (e.g., factorial or sequential designs) to disentangle contributions from background sources (radiation, afterpulsing, light leaks, magnetic-field effects) could accelerate commissioning and improve reproducibility. Developing calibration procedures and uncertainty budgets for absolute intensity measurements across the full dynamic range (single-electron to nominal bunches) would strengthen diagnostic claims. Extending the approach to automated adaptive gating/threshold selection to maintain optimal detection performance under changing beam conditions would improve robustness. Public release of analysis scripts/DAQ configuration examples would aid adoption and reproducibility across similar accelerator diagnostics setups.",1711.03166v1,https://arxiv.org/pdf/1711.03166v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:21:27Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|E-optimal|V-optimal|G-optimal|Other,Variable/General (design points are p-dimensional vectors; p is arbitrary),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies the discrete optimal experimental design problem of selecting at most k points from a pool of n p-dimensional vectors to optimize standard alphabetical criteria (A, D, T, E, V, G) and Bayesian variants. It proposes a polynomial-time regret-minimization-based rounding framework that turns a near-optimal continuous (fractional) design into an integral subset/multiset and achieves a (1+ε)-approximation when k is at least on the order of p/ε^2. The approach uses a whitening reduction and then maintains the minimum eigenvalue of the selected covariance via an online learning game with a matrix ℓ1/2-regularized FTRL strategy and a swapping algorithm. It provides theoretical approximation guarantees that were previously unavailable for D/E/G-optimality in polynomial time, and improves sample-size requirements over prior work for A/V-optimality. Empirically, on synthetic data the swapping method outperforms common heuristics (uniform/weighted sampling and Fedorov exchange) and is competitive with greedy removal while being more scalable.","The design is chosen by minimizing f(\Sigma_S) over subsets/multisets: \(\min_{s\in\{0,1\}^n,\ \sum_i s_i\le k} f(\sum_i s_i x_i x_i^\top)\) (and multiplicity-b generalization \(s_i\in\{0,\dots,b\}\)). Optimality criteria include \(f_A(\Sigma)=\tfrac{1}{p}\mathrm{tr}(\Sigma^{-1})\), \(f_D(\Sigma)=(\det\Sigma)^{-1/p}\), \(f_E(\Sigma)=\|\Sigma^{-1}\|_2\), \(f_V(\Sigma)=\tfrac{1}{n}\mathrm{tr}(X\Sigma^{-1}X^\top)\), and \(f_G(\Sigma)=\max\mathrm{diag}(X\Sigma^{-1}X^\top)\). The rounding targets a PSD dominance condition (after whitening): find s with \(\sum_i s_i \tilde x_i\tilde x_i^\top \succeq (1-O(\varepsilon))I\), equivalently lower bounding \(\lambda_{\min}(\sum_i s_i \tilde x_i\tilde x_i^\top)\).","Main guarantee: for objectives satisfying monotonicity and reciprocal sub-linearity (covering A/D/T/E/V/G and Bayesian counterparts), there is a polynomial-time algorithm producing \(\hat s\in S_{k,b}\) with \(F(\hat s)\le (1+8\varepsilon)\min_{s\in S_{k,b}}F(s)\) when \(k\ge 5p/\varepsilon^2\) (Theorem 1.4). A rounding theorem shows that given a fractional solution \(\pi\), one can round to an integral design with \(F(\hat s)\le (1+6\varepsilon)F(\pi)\) in roughly \(\tilde O(np^2)\) per iteration and total \(\tilde O(nkp^2/\varepsilon)\) time for the swapping procedure. The paper emphasizes that prior to this work no polynomial-time (1+ε)-approximation was known for D/E/G-optimality, and prior A/V results needed k as large as \(\Omega(p^2/\varepsilon)\). Synthetic experiments (n up to 10,000, p=50) show the swapping method consistently improves over uniform/weighted sampling and Fedorov exchange and is close to greedy removal but with better scalability.","The authors note they did not try to optimize constants in the approximation bounds (e.g., in Theorem 1.4). They state the running time \(\tilde O(nkp^2/\varepsilon)\) is not optimal and mention awareness of a warm-restart variant improving it to \(\tilde O(nkp^2)\). They also remark that continuous optimization for the relaxation is the practical bottleneck and could be made more efficient.","The empirical evaluation is limited to synthetic design pools (block-structured Gaussian with prescribed eigenvalue decay) and does not include real datasets or application-specific constraints (e.g., costs, stratification, ethics in clinical selection). The algorithm’s practicality depends on repeatedly computing eigenvalues/inverses and scanning all n points per swap step, which may be heavy for very large n and high p without further acceleration or approximate linear algebra. The theoretical guarantee requires k \(\gtrsim p/\varepsilon^2\); performance when k is near p (where designs can be ill-conditioned) is demonstrated but not characterized theoretically beyond remarks. No reference implementation or reproducible code is provided, making it harder to validate runtime and tuning choices (e.g., the grid over α) across settings.","They highlight improving the efficiency of solvers for the continuous relaxation (the convex optimization step), noting it dominates runtime in practice and suggesting dual/approximate interior-point approaches as potentially helpful. They also propose pursuing negative (tightness) results to show the \(\Omega(p/\varepsilon^2)\) requirement on k is necessary (at least for E-optimality/general objectives), stating a conjectured one-sided construction and that only related two-sided constructions are known.","Developing and releasing an optimized open-source implementation (with randomized/approximate linear algebra and caching) would help assess scalability and adoption. Extending the method to structured experimental design constraints (e.g., group/partition constraints, budgets per stratum, or submodular side constraints) and to streaming/online pool settings would broaden applicability. More extensive benchmarks on real experimental-design applications (clinical cohort selection, sensor placement, active learning datasets) and comparisons against modern approximate optimal design heuristics would strengthen practical validation. Investigating robustness to model misspecification and correlated/noisy covariates (and integrating regularization/self-normalization) would improve reliability in applied settings.",1711.05174v1,https://arxiv.org/pdf/1711.05174v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:22:18Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Other,D-optimal|Bayesian D-optimal,"Variable/General (design variables are sensor-location weights over ns candidates; examples include ns=35 and ns=109 candidate locations, with nt=3 observation times)",Environmental monitoring|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper develops scalable algorithms for Bayesian D-optimal experimental design (OED) for PDE-based linear inverse problems with infinite-dimensional parameters, with a main focus on sensor placement (choosing measurement locations) under a formulation that remains well-defined in the infinite-dimensional limit. The D-optimal objective is the expected information gain, shown to equal \(\tfrac12\log\det(I+\mathcal H_m)\), where \(\mathcal H_m\) is the prior-preconditioned data misfit Hessian; the design enters through a diagonal weight matrix over candidate sensors. Because direct log-determinant and gradient evaluations are prohibitive, the authors propose three efficient approximations that exploit low-rank structure: truncated spectral decomposition, randomized subspace iteration/log-det estimation, and a “frozen” low-rank SVD of the prior-preconditioned forward operator. They provide error bounds for objective/gradient and KL-divergence estimators, and demonstrate effectiveness on a 2D time-dependent advection–diffusion initial-condition inversion problem, where optimized sensor placements outperform random configurations in both D-optimal objective and information gain.","D-optimal (expected information gain) objective: \(J(w)=\log\det(I+\mathcal H_m(w))\), with \(\mathcal H_m(w)=\mathcal F^* W_\sigma \mathcal F\) (after prior/mass preconditioning) and \(W_\sigma=\sum_{j=1}^{n_s} w_j E_j^\sigma\). The gradient is \(\partial_{w_j}J(w)=\mathrm{tr}((I+\mathcal H_m(w))^{-1}Z_j)\), where \(Z_j=\partial_{w_j}\mathcal H_m(w)=M^{1/2}\mathcal F^*E_j^\sigma\mathcal F M^{-1/2}\). Expected KL divergence link: \(\mathbb E_{\theta\sim\mu_{pr}}\,\mathbb E_{y|\theta}[D_{KL}(\mu_{post}^y\|\mu_{pr})]=\tfrac12\log\det(I+\mathcal H_m)\).","The paper derives that the Bayesian D-optimal criterion for the linear-Gaussian inverse problem equals half the log-determinant \(\tfrac12\log\det(I+\mathcal H_m)\), providing an infinite-dimensionally meaningful objective, and gives computable gradients with respect to sensor weights. It provides explicit expected-error bounds for randomized log-determinant/KL estimators (e.g., bounds involving the neglected spectrum \(\Lambda_2\), eigenvalue gap \(\gamma\), power iterations \(q\), and an oversampling-dependent constant \(C_{ge}\)). In numerical experiments (e.g., \(n=1018\), \(n_s=35\), \(n_t=3\)), the randomized estimators achieve accuracy comparable to using “exact” leading eigenvalues as the target rank increases, and accuracy does not degrade with mesh refinement. The computed D-optimal sensor configurations (e.g., designs with 15 active sensors in one setup; 31 active sensors in a higher-resolution setup with \(n=2605\), \(n_s=109\)) outperform hundreds of random sensor placements in the optimized objective and show higher information gain.",They note that the target rank \(k\) is chosen a priori and raise the open problem of adaptively determining \(k\) during optimization to meet a prescribed accuracy with minimal PDE solves. They also state as an open question whether suitable error estimators can be incorporated into optimization routines with theoretical convergence guarantees. They further identify extension to nonlinear inverse problems as future work (current work focuses on linear inverse problems with Gaussian models).,"The approach assumes linear forward models with additive Gaussian noise and a Gaussian prior; performance and theoretical guarantees may change under model nonlinearity, non-Gaussian priors/noise, or model error. The sensor placement is treated via continuous relaxation \(w\in[0,1]^{n_s}\) with sparsifying penalties and heuristic thresholding/continuation to obtain binary designs, which may yield designs sensitive to penalty parameters and threshold rules and may not ensure global optimality of the discrete (combinatorial) problem. Empirical validation is primarily on a single PDE model problem; broader benchmarking across different PDEs, noise structures (e.g., correlated noise), and real datasets is limited.","They propose developing adaptive strategies to select/adjust the target rank \(k\) during optimization to guarantee objective/gradient accuracy while minimizing PDE solves. They also suggest incorporating error estimators into optimization routines with theoretical convergence guarantees. Finally, they explicitly mention extending the strategy to OED for nonlinear inverse problems as future work.","Develop self-starting/online variants for sequential or adaptive sensor placement where data arrive over time and designs are updated, leveraging the randomized estimators. Extend the framework to correlated/heteroscedastic noise and robust design objectives (e.g., worst-case or risk-averse information gain) and to constraints reflecting deployment costs or spatial exclusion zones. Provide open-source implementations (e.g., MATLAB/Python) and standardized benchmarks to facilitate adoption and reproducibility, and study sensitivity of designs to prior misspecification and PDE discretization choices.",1711.05878v1,https://arxiv.org/pdf/1711.05878v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:22:58Z
TRUE,Factorial (fractional)|Supersaturated|Screening|Optimal design|Other,Screening|Parameter estimation|Cost reduction|Other,Not applicable|Other,"Variable/General (examples include 23 factors in 14 runs; general n-run, m-factor two-level designs)",Theoretical/simulation only,Other,TRUE,None / Not applicable,Not provided,NA,"This paper introduces the Summary of Effect Aliasing Structure (SEAS), a set of descriptive statistics to characterize aliasing in two-level nonregular fractional factorial designs, with emphasis on supersaturated designs (SSDs). SEAS comprises three multi-term patterns (MAP): the M-pattern (worst-case/maximum aliasing), A-pattern (average squared aliasing), and P-pattern (proportion of nonzero aliasing), built from aliasing indices computed over subsets of columns and their products (capturing main effects and interactions). The authors derive theoretical links showing that generalized resolution (GR), generalized wordlength pattern (GWLP), and the E(s^2) SSD criterion are all summaries/aggregations of SEAS components, so SEAS provides finer discrimination when traditional criteria tie. They demonstrate SEAS by comparing several 14-run/23-factor SSDs (including an E(s^2)-optimal design found by swarm intelligence) and show that designs optimal under E(s^2) or GWLP can differ substantially in interaction aliasing. They also propose Effect-SEAS, which applies SEAS patterns to individual columns to guide factor-to-column assignment based on each column’s aliasing with other effects.","The aliasing index for a subset S of k columns is $\rho_k(S)=\frac{1}{n}\left|\sum_{i=1}^n c_{i1}c_{i2}\cdots c_{ik}\right|$, and nonzero indices are grouped as $\alpha_k=\{\rho_k(S):\rho_k(S)\neq 0\}$. SEAS patterns are defined by: $e^M_k = k+\frac{\max \alpha_k}{10}$ (M-pattern), $e^A_k = k+\frac{\overline{\alpha_k^2}}{10}$ where $\overline{\alpha_k^2}=\sum_{\alpha_k}\rho_k(S)^2/|\alpha_k|$ (A-pattern), and $e^P_k = k+\frac{|\alpha_k|/\binom{m}{k}}{10}$ (P-pattern). Key relationships: $\mathrm{GR}(X)=r+1-10(e^M_r-r)$; $A_k(X)=100\binom{m}{k}(e^A_k-k)(e^P_k-k)$ (GWLP entry); and $E(s^2)=100n^2(e^A_2-2)(e^P_2-2)$.","In an illustrative comparison of five SSDs with 14 runs and 23 factors, SEAS differentiates designs that share the same traditional criteria: e.g., D1 and D2 both have $E(s^2)=7.921$ and GR = 2.29, but differ in interaction aliasing with $e^M_4=4.0714$ (D1) vs $4.1000$ (D2), indicating complete aliasing among some 4-factor combinations in D2. The swarm-intelligence E(s^2)-optimal design DSIB improves $E(s^2)$ to 7.415 (vs 7.921) and reduces GWLP $A_2$ to 9.571 (vs 10.224), yet still has $e^M_4=4.1000$ and comparatively worse interaction-aliasing summaries (e.g., higher $A_3$ in GWLP and slightly higher $e^A_3$ than some competitors). Effect-SEAS applied to DSIB shows substantial column-to-column differences; for example some columns have smaller $e^M_3(x_l)=3.0571$ vs 3.0857 for others, and the paper provides full rankings of columns under A- and P-patterns to guide factor assignment.","The authors note that SEAS patterns grow rapidly in length as the number of factors increases, making computation increasingly costly. They suggest a pragmatic workaround: invoking effect sparsity and hierarchy to compute only the first few pattern terms (e.g., up to the 4th or 5th), corresponding to aliasing among up to four or five factors.","SEAS is descriptive rather than directly tied to a formal decision-theoretic or power/estimation optimality criterion, so how to weight different pattern entries for a specific scientific goal is left to practitioner judgment. The approach assumes a two-level (\u00b11-coded) factorial design framework and focuses on aliasing/correlation structure; extensions to quantitative factors with model curvature or to mixed-level designs are not addressed. The paper presents illustrative design comparisons but does not provide a broad computational benchmark across many (n,m) settings or connect SEAS rankings to downstream model-selection performance metrics under specific effect sparsity/interactions scenarios.","The authors suggest applying SEAS to more general nonregular designs beyond the SSD examples studied. They also propose using Effect-SEAS to choose factor-to-column assignments specifically to minimize aliasing of interaction effects, extending beyond the main-effect-focused demonstration.","Developing efficient algorithms/approximations to compute SEAS for larger m (e.g., via recursion, pruning using hierarchy, or randomized subset sampling) would make the method scalable. Linking SEAS entries to predictive/model-selection performance (e.g., probability of correct effect identification under sparse interaction models) could turn SEAS from a descriptive tool into a prescriptive design-selection criterion. Providing open-source software (e.g., an R/Python package) and applying SEAS to real industrial/chemical screening case studies would improve adoption and validate practical value.",1711.11488v3,https://arxiv.org/pdf/1711.11488v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:23:43Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Screening|Optimization|Cost reduction|Other,Other,"Variable/General (K experimental treatment arms; J stages; parameters a,b,c,d and group size n)",Healthcare/medical|Pharmaceutical,Exact distribution theory|Approximation methods|Other,TRUE,R|Other,Upon request,https://cran.r-project.org/web/packages/mvtnorm/,"The paper develops methodology for designing optimized multi-arm multi-stage (MAMS) clinical trials with strong control of generalized type-I familywise error rates (rejecting at least a true nulls) and power defined as rejecting at least b out of c false hypotheses (generalized type-II considerations). It introduces an “abcd-MAMS” framework that also allows flexible trial termination when any d null hypotheses have been rejected, interpolating between simultaneous stopping (d=1) and separate stopping (d=K). The authors derive multivariate normal integral expressions for outcome probabilities, generalized FWER/FWP metrics, and expected sample size, and propose computational efficiencies by exploiting interchangeability of arms with equal allocation/variance/effect. An optimization formulation is given to choose group size and stopping boundaries to minimize a weighted objective involving expected sample size under null and alternative configurations, subject to generalized error-rate constraints, using a genetic algorithm in R. A clinical trial example (TAILoR) illustrates how a,b,c,d affect operating characteristics and sample size efficiency compared with conventional MAMS settings.","Test statistics are standardized Wald Z-statistics for each arm k and stage j: $Z_{k,j}=\hat\tau_{k,j}/\sqrt{\mathrm{Var}(\hat\tau_{k,j})}$ with $\hat\tau_{k,j}=(r_{k,jn})^{-1}\sum_{i=1}^{r_{k,jn}}X_{i,k}-(r_{0,jn})^{-1}\sum_{i=1}^{r_{0,jn}}X_{i,0}$ and $X\sim N(\mu,\sigma^2)$. The joint vector $Z$ is multivariate normal with mean $c(\tau,J)\circ I^{1/2}$ and covariance built from stagewise information, enabling exact computation of outcome probabilities via multivariate normal integrals with limits determined by futility/efficacy boundaries $f=(f_1,\dots,f_J)$ and $e=(e_1,\dots,e_J)$. Expected sample size is $E(N)=\sum_{(\psi,\omega)\in\Xi} n\left(\max_k \omega_k+\sum_{k=1}^K r_{k,\omega_k}\right)P(\psi,\omega)$, and generalized type-I FWER and b-out-of-c familywise power are sums of $P(\psi,\omega)$ over appropriate outcome sets (Eqs. 2.2–2.3).","A theorem establishes strong control of the a-generalized type-I FWER by showing the worst case occurs at the global null $\tau=0_K$, so ensuring $\Pr(\text{reject}\ge a\text{ true }H_k\mid 0_K)\le\alpha$ suffices. The paper reports that accounting for arm interchangeability greatly reduces the number of outcome scenarios/integrals; e.g., for d=2, K=4, J=4 the outcome set size drops from 3808 to 300. In the TAILoR-inspired example (K=3, J=2, \alpha=0.05, \beta=0.1), optimized designs meeting constraints can require substantially smaller first-stage group sizes than a conventional 1111-MAMS design (e.g., examples noted include group sizes 24 and 13 versus 40 for 1111). Runtime for 540 optimization runs using GA with mvtnorm on a 7-core laptop is reported as ~10–15 minutes per run.","The authors note that using a stochastic search (genetic algorithm) provides no guarantee of reaching the global optimum, motivating multiple random starts and careful tuning of GA control parameters. They also state that as the number of stages/arms grows, optimal design determination may become intractable, suggesting dimension-reduction strategies may be needed. They further note that extension to normally distributed outcomes with unknown variance is not straightforward in multi-arm settings.","The approach assumes normally distributed outcomes and relies on multivariate normal integral evaluation; performance and robustness under non-normality, heteroscedasticity, or model misspecification are not fully explored. It also appears primarily tailored to known variances and independent observations; correlated/clustered data or time trends common in platform trials could materially affect error control and operating characteristics. Practical implementation complexity (computing boundaries, handling deviations, and real-time interim monitoring logistics) and sensitivity to misspecified effect sizes (\delta, \delta_0) and allocation ratios may limit real-world generalizability without additional guidance or software tooling.","The authors suggest extending Wason (2015)-style dimensionality reduction to retain tractable optimization for larger J and K, and propose investigating a dynamic programming approach (analogous to Barber and Jennison (2002)) for potentially better optimization performance in multi-arm settings. They also highlight that adapting the methodology to unknown variance settings is challenging and point to related work considering quantile substitution and Monte Carlo approaches.","Developing a fully self-starting/variance-adaptive version (e.g., using t-statistics or robust variance estimation) with validated generalized error control would broaden applicability. Extending the framework to other endpoints (binary, time-to-event) with exact or efficient approximations for generalized error metrics and ESS optimization would be valuable for phase II/III platform trials. Providing an open-source implementation (beyond “upon request”) with reproducible benchmarks, plus guidance on choosing weights (w1,w2,w3) and sensitivity analyses for \delta/\delta_0, would improve uptake and standardization.",1712.00229v1,https://arxiv.org/pdf/1712.00229v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:24:27Z
TRUE,Sequential/adaptive|Computer experiment|Optimal design,Prediction|Cost reduction,I-optimal (IV-optimal),"Variable/General (examples include 1D analytical; hydrodynamic example has 4 total inputs: d1=2, d2=2, plus scalar intermediate output from code 1 feeding code 2)",Energy/utilities|Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops efficient sequential design strategies for surrogate modeling of two nested deterministic computer codes, where the output of code 1 becomes an input to code 2. Each code is modeled with a Gaussian process (universal kriging), and the authors derive ways to compute (or approximate) the nested predictor’s mean and variance despite the nested composition being non-Gaussian. They propose two sequential DOE criteria that minimize integrated prediction uncertainty (I-optimality): a “chained I-optimal” design for when the codes cannot be run separately, and a “best I-optimal” design that chooses whether to evaluate code 1 or code 2 next based on expected variance reduction per unit computational cost. To make the sequential optimization computationally feasible, they provide (i) an analytical moment computation under restrictive trend/covariance assumptions and (ii) a linearized (Gaussian) approximation enabling fast variance updates (with a kriging-believer style plug-in for prospective points). Demonstrations on an analytical example and a hydrodynamic projectile application show improved predictive accuracy versus a baseline space-filling/maximin LHS and versus treating the nested system as a single “black-box” code, with the cost-aware criterion tending to allocate early evaluations to the upstream (first) code.","The nested simulator is modeled as a composition $y_{\text{nest}}(x_1,x_2)=y_2(y_1(x_1),x_2)$, with GP posteriors $\hat y_i^c\sim GP(\mu_i^c,C_i^c)$. The key sequential DOE objectives are integrated posterior variance criteria: chained I-optimality $\arg\min_{x_3\in X_1\times X_2}\int_{X_{\text{nest}}}\mathrm{Var}(\hat y_{\text{nest}}^c(x)\mid x_3)\,dx$ and cost-weighted “best” I-optimality $\arg\max_{i\in\{1,2\},x_i}\frac{1}{\tau_i}\int_{X_{\text{nest}}}\{\mathrm{Var}(\hat y_{\text{nest}}^c(x)) - \mathrm{Var}(\hat y_{\text{nest}}^c(x)\mid x_i)\}\,dx$. For fast variance evaluation they propose a linearized nested GP with covariance $C_{\text{nest}}^c((x_1,x_2),(x_1',x_2'))=C_2^c((\mu_1^c(x_1),x_2),(\mu_1^c(x_1'),x_2'))+\partial_{\phi_1}\mu_2^c\,\partial_{\phi_1'}\mu_2^c\,C_1^c(x_1,x_1')$ (evaluated at the kriged means).","Across both examples, sequential designs based on integrated variance (I-optimality) improve prediction mean accuracy relative to adding points via space-filling/maximin LHS on the full nested input space. The cost-aware “best I-optimal” criterion is the most effective in the analytical example and is at least competitive (often slightly better early on) in the hydrodynamic example, particularly in the first few added points. The best I-optimal strategy allocates most early evaluations to code 1, indicating uncertainty propagation from the upstream code dominates initially; later evaluations are split across both codes. When code costs differ, prediction accuracy at a fixed total computational budget improves when the upstream (code 1) evaluations are cheaper, because the design can invest more runs where uncertainty reduction is most leveraged.","The exact “analytical” moment computation approach cannot be generalized to a coupling of more than two codes. The work focuses on two nested codes with a scalar intermediate variable; extending to a functional intermediate variable is left for future work. The linearized approach relies on the assumption that the posterior residual process for code 1 is “small enough” for the linearization to be valid, and uses a Kriging Believer-style plug-in for candidate points.","The sequential criteria require evaluating (and optimizing) integrated variance over $X_{\text{nest}}$, which in practice may need Monte Carlo/quadrature discretizations that can become expensive in higher dimensions and may be sensitive to the integration design. The linearization-based covariance depends on derivatives of the GP mean for code 2 with respect to the intermediate input, which can be unstable or inaccurate when code 2 is highly nonlinear in that input or when hyperparameter estimates are uncertain. Results are shown on two examples; broader benchmarking against other active learning / SUR criteria (e.g., entropy-based, IMSE/SUR variants, multi-fidelity/nested-structure competitors) and more real industrial case studies would strengthen generalizability.",The authors suggest extending the approach beyond the scalar-intermediate setting by considering the case of a functional intermediary variable (instead of a scalar output from the first code). They also indicate the linearized method could be generalized to the coupling of more than two codes (whereas the analytical method cannot).,"Develop and analyze designs for vector-valued or high-dimensional intermediate variables (and deeper code networks) with controlled approximation error, potentially via automatic differentiation and higher-order (nonlinear) uncertainty propagation. Incorporate hyperparameter uncertainty (full Bayesian treatment) into the sequential design objective rather than conditioning on point estimates, and study robustness under model misspecification (nonstationarity, heteroskedasticity, or weak smoothness). Provide open-source implementations and scalable optimization strategies (e.g., gradient-based acquisition optimization, batch/parallel selection, and sparse GP approximations) to support practical use in higher-dimensional nested simulators.",1712.01620v1,https://arxiv.org/pdf/1712.01620v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:25:15Z
TRUE,Sequential/adaptive|Other,Prediction|Parameter estimation|Cost reduction|Other,Not applicable,Variable/General (sampling schedule/temporal resolution; evaluated at 3–25 samples; pathway has 30 species with 34 reactions),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,https://arxiv.org/abs/1712.05453,"The paper is an in silico study of experimental design trade-offs for time-series sampling in gene regulatory network (GRN) inference, focusing on how temporal resolution (number of time points) affects inferred network quality. It simulates a stochastic chemical reaction network (Markovian jump process/CME) model of the Ras/cAMP/PKA pathway in the G1 phase of the yeast Saccharomyces cerevisiae cell cycle, then uniformly subsamples trajectories at varying numbers of samples. For each sampled time series, it fits a sparse linear autoregressive model using LASSO and converts regression coefficients into an inferred network topology via thresholding. Performance is evaluated by area under the precision–recall curve (AUPR) against the known ground-truth network derived from the reaction hypergraph, producing a trade-off curve (AUPR vs. number of samples) with an approximately sigmoid shape. The study also contrasts designs that include vs. exclude phosphoproteomic measurements (phosphorylated species), framing this as a fixed-cost vs. marginal-cost experimental design choice.","The simulated system is a chemical reaction network governed by the chemical master equation (CME): \(\dot P(X,t)=\sum_{k=1}^m w_k(X-S_k)P(X-S_k,t)-w_k(X)P(X,t)\), with propensities \(w_i(X)=c_i\prod_{l=1}^k \binom{X_{j_l}}{n_{j_l}}\) for reactant stoichiometries. Sampled trajectories are modeled by a discrete-time linear system \(\hat X_{k+1}=A(\Delta t)\hat X_k+\varepsilon_k\), and parameters are estimated via LASSO: \(\min_{B}\frac{1}{N}\sum_{k=0}^{N-1}\|Z_{k+1}-\Delta t\,B Z_k\|_2^2+\lambda\|B\|_1\). Network edges are obtained by thresholding: \(E(r)=\{(i,j):|B_{ij}|\ge r\}\), and quality is summarized by AUPR over thresholds \(r\).","Across 40 simulated ‘cells’, AUPR plotted against number of uniform samples (3–25) yields a roughly sigmoid trade-off curve: near-random performance for 3–6 samples, improved performance for about 7–15 samples, and diminishing returns beyond roughly 25 samples. The paper reports that for 3–6 samples the method performs on par with a random classifier, while for 7–15 samples performance is better than average with at least 95% certainty (pointwise by sample size). The best-performing shown model (with phosphoproteomics included) achieved an AUPR of about 0.41. When considering inclusion of phosphoproteomic species, some phosphorylated variables (e.g., Pde1p) contributed limited explanatory power (largely white-noise-like), while others (e.g., cAMP•Pde1p) were well explained and helped explain AMP.","The authors state the work is a proof-of-concept and that additional studies are needed before the trade-off graph/tool can be useful in a real laboratory setting. They emphasize results are limited to a particular in silico model of the Ras/cAMP/PKA pathway and a specific inference method (LASSO with a linear model), implying limited generality. They also note the linear model cannot capture all nonlinear interdependencies implied by CME propensities, so zero error is not expected even with infinite samples.","The study’s sampling “design” space is largely restricted to uniform sampling and varying only the number of samples, so it does not evaluate non-uniform/adaptive schedules that might be superior for transient dynamics. All conclusions are based on a single pathway model and simulated data; robustness to model mismatch, measurement noise typical of sequencing/proteomics pipelines, batch effects, and asynchronous cell populations is not demonstrated. The validation scheme uses a single type of condition shift (glucose-starved via altered initial GTP), and it is unclear how sensitive the selected \(\lambda\) and inferred trade-off are to other perturbations or to unknown parameters in realistic Phase I/II settings.",The authors state an ultimate goal of formulating quantitative guidelines and building decision support systems to help experimentalists navigate cost vs. sampling trade-offs to achieve desired inference quality at minimal cost. They also explicitly note that generality must be increased (via additional studies) before the approach becomes practically useful in the laboratory.,"Extending the design study to compare non-uniform, transient-focused, or adaptive sampling schedules (and possibly replicate-vs-dense strategies) would make the guidance more actionable. Evaluating additional inference methods beyond linear LASSO (including nonlinear or state-space approaches) and assessing sensitivity to realistic measurement noise, missing data, and asynchronous sampling would improve external validity. Providing open-source code and standardized benchmarks for the CME simulation + inference pipeline would facilitate reproducibility and broader adoption.",1712.05453v1,https://arxiv.org/pdf/1712.05453v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:25:57Z
TRUE,Computer experiment|Optimal design|Sequential/adaptive|Other,Prediction|Parameter estimation|Optimization|Other,Space-filling|Minimax/Maximin|Other,Variable/General (applications shown in 2 to 6 dimensions; general d-dimensional theory),Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a “spectral” framework for constructing high-quality space-filling designs for computer experiments, explicitly trading off coverage (minimum separation) and randomness. It introduces the pair correlation function (PCF) as a holistic space-filling metric and analytically links PCF (spatial domain) to the radially averaged power spectral density (PSD) (spectral domain) via Hankel/Fourier transform relationships, enabling principled design evaluation. Based on this analysis, it defines optimal target PCFs (Step PCF and a generalized Stair PCF) and derives realizability constraints (nonnegative PCF and PSD) plus bounds relating sample size N, dimension d, and achievable minimum distance r_min. To synthesize designs matching target PCFs, it develops an edge-corrected kernel density estimator for PCF in arbitrary dimensions and a weighted least-squares gradient-descent optimization algorithm (with one-sided PCF smoothing). Empirical studies in 2–6 dimensions for image reconstruction, regression on benchmark functions, and surrogate modeling for an inertial confinement fusion (ICF) simulator show the proposed Step/Stair spectral designs outperform common alternatives (LHS, Sobol/Halton QMC, and maximal Poisson-disk sampling), especially as dimension increases.","Space-filling is characterized via the pair correlation function $G(r)=\beta(r)/\lambda^2$ and the radially averaged PSD $P(k)=\frac{1}{N}|S(k)|^2=\frac{1}{N}\sum_{j,\ell}e^{-2\pi i k(x_\ell-x_j)}$. For isotropic designs, PCF and PSD are linked (Hankel/Fourier) as $G(r)=1+\frac{V}{2\pi N}\,\mathcal{H}[P(k)-1]$, yielding closed-form target PSDs for proposed designs. For the Step-PCF target ($G(r)=0$ for $r<r_{min}$, $1$ otherwise), the implied PSD is $P(k)=1-\frac{N}{V}\left(\frac{2\pi r_{min}}{k}\right)^{d/2}J_{d/2}(kr_{min})$; for the Stair-PCF target, $P(k)=1-\frac{N}{V}P_0\left(\frac{2\pi r_0}{k}\right)^{d/2}J_{d/2}(kr_0)-\frac{N}{V}(1-P_0)\left(\frac{2\pi r_1}{k}\right)^{d/2}J_{d/2}(kr_1)$.","The paper derives analytic bounds for realizable Step-PCF designs: for fixed $r_{min}$, the maximum sample size is $N=\frac{V\,\Gamma(d/2+1)}{\pi^{d/2} r_{min}^d}$; equivalently for fixed $N$, the maximum achievable separation is $r_{min}=\left(\frac{V\,\Gamma(d/2+1)}{\pi^{d/2}N}\right)^{1/d}$. It empirically shows that introducing a small single peak (Stair PCF, with constrained peak height e.g. $P_0\le 1.5$) can substantially increase achievable coverage ($r_{min}$ and relative radius) over Step PCF across dimensions 2–6. In image reconstruction (zone plate), Step/Stair designs reduce structured aliasing compared to QMC and improve PSNR relative to LHS, with performance depending on frequency content (complexity) and the coverage–randomness trade-off. In regression and ICF surrogate modeling experiments, the Stair PCF design is consistently best for $d\ge 3$, with performance gains increasing with dimension, while LHS/QMC degrade in higher dimensions.","The authors note that requiring both nonnegative PCF and PSD are necessary realizability conditions, but whether they are also sufficient is an open question (no counterexamples known). They also state that closed-form optimal parameters for the Stair-PCF family are difficult to obtain, so they explore Stair-PCF parameter configurations empirically under constraints (e.g., on peak height and widths). Additionally, they remark that matching Stair PCFs is more challenging for gradient descent and benefits from better initialization (e.g., MPDS or grid).","The approach focuses on isotropic, Euclidean hypercube-like domains (unit-volume regions) and radially averaged spectral properties; performance and theory may not transfer directly to anisotropic constraints, irregular domains, or strongly constrained design spaces without modification. The synthesis method uses an $O(N^2)$ pairwise-distance-based PCF estimator/gradient (implied by double sums), which may become computationally heavy for very large N or higher dimensions unless optimized/approximated. Empirical comparisons depend on specific implementations/parameterizations of competitors (e.g., MPDS initialization, LHS/QMC variants), and broader benchmarking (including modern space-filling/optimal designs like maxpro, kernel IMSE/I-optimal designs, and advanced LHS optimizers) is not explored. Practical guidance for choosing Stair-PCF parameters ($r_0,r_1,P_0$) for a given downstream task is limited beyond heuristics/constraints and empirical search.","The paper suggests extending the analysis and design methodology to non-linear manifolds, noting that some analytical tools used here may be exploitable for designing space-filling designs on manifolds. It also proposes investigating other PCF parameterizations corresponding to additional variants of space-filling designs and developing optimization approaches to synthesize those designs.","Developing scalable approximations (e.g., neighbor truncation, FFT-based spectral matching, or stochastic optimization) could make PCF-matching feasible for much larger N and higher d. Extending the framework to handle constraints common in practice (non-rectangular feasible regions, mixed/discrete variables, or black-box constraints) and to produce non-isotropic (directional) designs would broaden applicability. A task-adaptive or Bayesian variant that selects PCF/PSD targets based on prior information about f (e.g., lengthscales in Gaussian process surrogates) could yield stronger performance guarantees. Providing open-source implementations and standardized benchmarks would improve reproducibility and facilitate adoption by the DOE community.",1712.06028v1,https://arxiv.org/pdf/1712.06028v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:27:01Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Model discrimination|Robustness|Cost reduction|Other,D-optimal|A-optimal|I-optimal (IV-optimal)|E-optimal|Compound criterion|Bayesian D-optimal|Bayesian A-optimal|Other,Variable/General,Healthcare/medical|Service industry|Transportation/logistics|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper discusses how experimental design concepts are challenged by big data settings, emphasizing bias, confounding, and feedback that hinder causal identification when data are collected passively (e.g., social media). It frames causal inference using DAG/structural equation ideas and highlights intervention (setting/treatment) versus observation, including the principle that mixtures of observational and interventional data may be optimal. Methodologically, it reviews and extends “minimum-bias/robust” optimal design formulations by adding an explicit bias term to a regression model and examining how standard least-squares estimation of causal parameters degrades when the bias component is omitted. The authors connect bias-guarding design choices to a cooperative/game-theoretic viewpoint, proposing Nash equilibrium as an alternative to joint optimization and illustrating equilibria in a small two-parameter design-measure example. Randomization is presented as a mixed strategy/minimax device to protect against unknown or hidden bias mechanisms.","The central biased model augments a standard regression mean structure as $\mathbb{E}(Y_i)=\theta^\top f(x_i)+\phi^\top g(z_i)+\varepsilon_i$ with i.i.d. errors of variance $\sigma^2$. Using the full moment matrix $M=\int (f(x)^\top,g(z)^\top)^\top(f(x)^\top,g(z)^\top)\,\xi_{x,z}\,d(x,z)=\begin{pmatrix}M_{11}&M_{12}\\M_{21}&M_{22}\end{pmatrix}$, the MSE matrix for the reduced-model LS estimator of $\theta$ is $\mathbb{E}\{(\hat\theta-\theta)(\hat\theta-\theta)^\top\}=\sigma^2 N^{-1}R$ with $R=M_{11}^{-1}+M_{11}^{-1}M_{12}\psi\psi^\top M_{21}M_{11}^{-1}$ where $\psi=(N/\sigma)\,\phi$. Design criteria discussed include minimizing $\mathrm{tr}(R)$ (A-type) and $\det(R)$ (D-type), and worst-case (unknown $\psi$) criteria based on $\lambda_{\max}(Q_1)$ with $Q_1=M_{21}M_{11}^{-2}M_{12}$ (E-type/minimax flavor).","In a toy example with $\mathbb{E}(Y)=\theta_0+\theta_1 x+\phi z$ on four support points with weights parameterized by $(\alpha,\beta)$, the Nash-equilibrium conditions yield two equilibria: $(\alpha,\beta)=(1/2,1/2)$ and a numerically computed equilibrium $(\alpha^*,\beta^*)\approx(0.59306,0.08274)$. For $\psi=1$, joint minimization of $\mathrm{tr}(S_1)+\mathrm{tr}(S_2)$ attains minimum value 4 at $(1/2,1/2)$ with components $(\mathrm{tr}(S_1),\mathrm{tr}(S_2))=(3,1)$. At the alternative Nash equilibrium, the summed criterion is approximately 5.1735 with $(\mathrm{tr}(S_1),\mathrm{tr}(S_2))\approx(4.483,0.6905)$, illustrating that equilibrium play need not coincide with global optimality but can reduce bias impact relative to variance. The clinical-trial balance result is also shown algebraically: the bias contribution to trace is proportional to $\psi^2(\bar z_1-\bar z_2)^2/4$ and vanishes under mean balance.","The authors describe the work as preliminary and note that establishing specific model classes (e.g., choices of function classes $H$, randomization distributions $P_z$) and conditions on the design set $D$ under which the proposed minimax/randomization approaches become efficient algorithms is left for ongoing/current work.","The paper is largely conceptual and illustrative; beyond small analytic examples it does not provide a systematic simulation study or real-data case study to quantify performance or practical sensitivity. The bias-robust design framework relies on linear/additive structure and moment-matrix calculations, which may not transfer directly to common big-data causal settings with nonlinearity, high-dimensional confounding, interference, or strong dependence/feedback. The Nash-equilibrium framing depends on a stylized separation of roles/objectives and does not address how practitioners would specify Bob’s bias model/uncertainty set (or validate it) in practice.","The authors state that developing the framework into “efficient algorithms” is future work, including specifying appropriate model classes for hidden-bias functions (choices of $h$, $H$) and randomization distributions ($P_z$), and identifying conditions on the design space/feasible design set ($D$) under which the proposed minimax and randomization/game-theoretic approaches can be implemented effectively.","Natural extensions would include empirical validation via comprehensive Monte Carlo studies and at least one substantive big-data case study (e.g., observational platform data) comparing against standard robust/orthogonal design and modern causal design strategies. Extending the bias-robust criteria to nonlinear generalized linear models, high-dimensional settings (regularization), and dependent data (time series/feedback) would improve relevance to big-data contexts. Providing implementable software and scalable optimization routines for large finite design spaces (including approximate/greedy methods with complexity guarantees) would make the approach more actionable.",1712.06916v4,https://arxiv.org/pdf/1712.06916v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:27:27Z
FALSE,NA,NA,Not applicable,Not specified,Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper defines and documents the experimental setup for the Time Delay Lens Modeling Challenge (TDLMC), a community blind challenge built from simulated strong-lens datasets to test lens-modeling pipelines and quantify bias/precision in inferring the Hubble constant $H_0$. The authors describe how 50 mock HST-like imaging datasets (WFC3/IR F160W), along with associated time delays, external convergence priors, and stellar velocity dispersions, are generated with increasing realism across multiple “rungs.” Complexity is increased via more realistic source surface-brightness models, PSF information (true PSF provided vs. only a guess), and ultimately numerically simulated galaxy mass distributions. Performance is evaluated with challenge metrics (efficiency, reduced $\chi^2$, precision, and bias/accuracy) computed per rung and combined, rather than proposing a DOE methodology. The paper provides participation instructions and the public data access point for downloading the simulated datasets.","The key lensing relations define the time delay between images $i,j$ as $\Delta t_{ij}=\frac{D_{\Delta t}}{c}[\phi(\theta_i)-\phi(\theta_j)]$ with Fermat potential $\phi(\theta)=\frac{(\theta-\beta)^2}{2}-\psi(\theta)$ and $D_{\Delta t}=(1+z_d)\,\frac{D_d D_s}{D_{ds}}$. Line-of-sight effects are included via external convergence through $\Delta t_{\rm obs}=(1-\kappa_{\rm ext})\Delta t_{\rm true}$. Challenge scoring uses efficiency $f=N/N_{\rm total}$, reduced $\chi^2=\frac{1}{N}\sum_i\left(\frac{\tilde H_{0,i}-H_0}{\delta_i}\right)^2$, precision $P=\frac{1}{N}\sum_i \delta_i/H_0$, and accuracy/bias $A=\frac{1}{N}\sum_i (\tilde H_{0,i}-H_0)/H_0$.","The paper sets target performance ranges for submissions rather than reporting results: for the full challenge (48 systems across rungs 1–3), expected ranges are $0.6<\chi^2<1.5$, average precision $P<6\%$, and absolute bias $|A|<1\%$. For a single rung, the suggested targets are looser: $0.4<\chi^2<2$, $P<6\%$, and $|A|<2\%$. Dataset design details include 50 mock lenses total (2 in rung 0 training; 16 per rung for rungs 1–3), HST WFC3/F160W imaging simulated from eight 1200s exposures (9600s total) with drizzling, and external convergence drawn from $\mathcal{N}(0,0.025^2)$ with a modeling prior of $0\pm0.025$. Time-delay measurement noise is set to the larger of 1% or 0.25 days, and velocity-dispersion noise is Gaussian with 5% standard deviation.","The authors state the challenge is intentionally limited to isolate lens-modeling systematics: time delays are assumed to be known precisely and accurately (effectively zero bias with small random errors), and only a single-plane deflector is considered. They also simplify by holding all cosmological parameters fixed except $H_0$. They note that multi-band data and adaptive-optics ground-based imaging are not included and are left to future challenges.","Although the simulation-based challenge is realistic in several respects, results from it may not fully generalize to real survey heterogeneity (e.g., variable PSFs across epochs/fields, detector systematics, imperfect noise modeling, and astrophysical contaminants like dust or complex AGN/host offsets). The imposed priors/noise models (e.g., Gaussian $\kappa_{\rm ext}$ with fixed $\sigma=0.025$, fixed 5% kinematic error, and optimistic time-delay errors) can advantage methods tuned to these assumptions. The design also fixes the cosmology except $H_0$, which may under-represent parameter degeneracies faced in joint cosmological inference. Code generation is described (two independent simulation codes) but not released here, limiting reproducibility of the exact mock-generation pipeline.","They explicitly note that multi-band datasets or adaptive-optics assisted ground-based images are left for future challenges. They also mention a new time-delay measurement challenge (including multi-band data and microlensing-induced perturbations) is being developed by others (cited as in prep). The paper indicates that results of this lens-modeling challenge will be presented in a companion paper after deadlines, with broader community participation.","Future challenge designs could stress-test robustness to model misspecification by varying noise properties, PSF spatial/temporal variation, dust extinction, AGN–host centroid offsets, and correlated detector artifacts beyond drizzle correlations. Extending beyond single-plane lensing to explicit multi-plane line-of-sight structure (rather than a Gaussian $\kappa_{\rm ext}$ prior) would better probe external-convergence inference pipelines. Joint inference of additional cosmological parameters (or alternative cosmologies) would test degeneracies and calibration of posteriors more realistically. Publishing the simulation code and configuration files (or containerized pipelines) would improve reproducibility and enable systematic ablation studies.",1801.01506v4,https://arxiv.org/pdf/1801.01506v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:28:00Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|I-optimal (IV-optimal)|Other,"Variable/General (design points on finite set X={1,…,n}; regressor dimension m typically <10, but varies in experiments, e.g., m=6,15,28)",Theoretical/simulation only,Simulation study|Other,TRUE,R,Personal website,http://www.iam.fmph.uniba.sk/ospm/Harman/design/,"The paper develops a family of subspace ascent methods (SAM) for computing optimal approximate designs on finite design spaces for linear regression with uncorrelated errors, and introduces a randomized exchange algorithm (REX) as an efficient instance. REX repeatedly selects an active subset of design points combining the current support and a greedy set based on sensitivity/variance-type quantities, then performs many pairwise optimal weight exchanges (with an initial leading Bohning exchange) to increase the chosen optimality criterion. The main focus is D-optimality (maximizing det(M(w))^{1/m}); the authors prove convergence of REX to the D-optimal value and connect D-optimal design to the minimum-volume enclosing ellipsoid problem. They also derive closed-form optimal exchange step formulas for A-optimality and note these enable computing A- and I-optimal (IV/V-optimal) designs. Extensive numerical experiments in R on quadratic response-surface-type models and random Gaussian regressor models show REX is comparable or faster than state-of-the-art methods (cocktail algorithm and a Newton-type simplicial decomposition approach) across a wide range of n and m.","An approximate design is a weight vector w on a finite design space X with information matrix $M(w)=\sum_{x\in X} w_x f(x)f(x)^\top$. The paper considers criteria including D-optimality $\Phi_D(M)=(\det M)^{1/m}$ (often via $\Psi_D(M)=\log\det M$) and A-optimality $\Phi_A(M)=(\mathrm{tr}(M^{-1}))^{-1}$ (via $\Psi_A(M)=-\mathrm{tr}(M^{-1})$). REX/VEM updates designs by exchanging weight between two points $u,v$: $w\leftarrow w+\alpha(e_v-e_u)$ with $\alpha\in[-w_v,w_u]$ chosen to maximize the criterion; for D-optimality a closed form is given (Eq. (8)) using $d_x(w)=f(x)^\top M(w)^{-1}f(x)$ and $d_{u,v}(w)=f(u)^\top M(w)^{-1}f(v)$.","Simulation benchmarks (implemented in R) compare REX against the cocktail algorithm (CO) and a Newton-type method (YBT) on (i) discretized quadratic regression models on $[-1,1]^d$ and (ii) random Gaussian regressor models with large n; performance is summarized via log-efficiency curves over time. Across tested settings (e.g., m=6,15,28 with n ranging from 10,000 up to 1,000,000 in random models, and very large lattices for quadratic models), REX is reported as comparable or superior in most cases, with especially strong gains on unstructured/random regressor problems. The paper proves that under the D-optimal step-length rule, the sequence of D-criterion values produced by REX converges to the D-optimal value (Appendix A.1). Closed-form A-optimal exchange step formulas are provided (Appendix A.2), enabling use of REX for A-optimal and I-optimal design computation.","The authors note two main disadvantages: (i) REX can occasionally produce streaks of only slowly increasing criterion values before rapid improvement, and (ii) they could not prove convergence to the optimum for criteria beyond D-optimality (though they observed convergence in all tested problems). They also caution that numerical comparisons depend on implementation details and hardware, so relative results may vary.","The work focuses on approximate designs on finite design spaces with uncorrelated errors; practical issues such as exact (integer-run) designs, constraints (e.g., blocking, randomization restrictions), or correlated/heteroskedastic errors are not addressed in the main algorithmic development. Empirical evaluation is largely simulation-based with limited real-world case studies, so practical performance in domain-specific applications is not demonstrated. Code is provided as R scripts, but there is no indication of standardized packaging, extensive unit tests, or reproducibility artifacts (e.g., fixed seeds/benchmark suite) beyond the reported experiments.","They suggest studying convergence properties of REX for general concave criteria, improving/possibly adapting the tuning parameter $\gamma$ controlling batch size, and incorporating better initialization and deletion rules for non-support points to accelerate computation. They also propose extending REX to continuous (infinite) design spaces by sampling many random candidate points, running REX on the subsample, and then fine-tuning with constrained continuous optimization, noting that theoretical and numerical analysis of this approach is left for future work.","Useful extensions would include versions tailored to exact designs and designs with linear/nonlinear constraints (e.g., cost, replication limits, split-plot/randomization restrictions), and robust/self-starting variants handling unknown variance components or correlated errors. Broader benchmarking against modern convex-optimization/SDP/SOCP solvers and additional optimality criteria (e.g., E-, G-, V-optimality) with clear stopping rules would strengthen evidence. Providing a maintained open-source package (CRAN/GitHub) with reproducible benchmark scripts and standardized interfaces to common DOE problems would improve adoption.",1801.05661v1,https://arxiv.org/pdf/1801.05661v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:28:35Z
TRUE,Optimal design|Mixture|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal|A-optimal|I-optimal (IV-optimal)|Other,Variable/General (examples include 6 items in spring-balance weighing; 3 mixture components; subsampling example uses 2 predictors + intercept),Theoretical/simulation only|Manufacturing (general)|Food/agriculture|Other,Simulation study|Other,TRUE,R|Other,Personal website,http://www.iam.fmph.uniba.sk/ospm/Harman/design/,"The paper proposes AQuA (ascent with quadratic assistance), an integer-programming approach for constructing optimal or near-optimal exact experimental designs by maximizing a quadratic approximation of an optimality criterion around an (approximately) optimal approximate information matrix. It derives explicit second-order (Taylor) quadratic approximations for Kiefer’s Φp criteria (including D- and A-optimality) and shows how I-optimality can be handled via a model transformation to A-optimality. A key contribution is proving a low-rank structure of the resulting quadratic forms, enabling scalable mixed-integer conic quadratic programming (MICQP) formulations that avoid forming huge n×n matrices and work for large design spaces. Numerical studies demonstrate robustness to misspecified anchor matrices and strong performance under complex linear constraints, including size-constrained weighing designs, constrained symmetric mixture designs, and information-based subsampling of a large dataset under cost/quality/stratification constraints. The work advances practical computation of constrained exact optimal designs beyond rounding methods and standard discrete heuristics, especially for large n with small-to-moderate model dimension m.","Designs are integer weight vectors ξ∈N0^n with information matrix M(ξ)=∑_{i=1}^n ξ_i H_i, where H_i=A_i A_i^T. Kiefer criteria include Φ_p^+(M)=( (1/m) tr(M^{-p}) )^{-1/p} (A-optimality at p=1; D-optimality at p=0 via (det M)^{1/m}) and Φ_p^-(M)=-( (1/m) tr(M^{-p}) )^{1/p}. AQuA maximizes a quadratic approximation Φ_Q(M(ξ)) that becomes an integer quadratic program max_{ξ∈Ξ^E_{A,b}} h^T ξ − ξ^T Q ξ, with a low-rank factorization Q=SS^T leading to the conic reformulation using r≥||S^T ξ||_2.","Across benchmark size-constrained spring-balance weighing problems (m=6; n=64; N=6–30), AQuA often attains the exact optimal designs, with occasional degradation mainly for very small N (notably N=m), and remains robust even when the anchor matrix corresponds to a ~0.95-efficient random design. For constrained symmetric marginally restricted exact designs in a discrete 3-component quadratic Scheffé mixture model (n=861), AQuA-MICQP achieved similar efficiencies to exact MISOCP solutions but at dramatically lower runtime (e.g., D: efficiency ≈0.98374 with ~8.56s vs ~616.91s; I: efficiency ≈0.99647 with ~4.54s vs ~602.96s). In a large-scale subsampling example (n≈111,534 wine reviews) with budget/quality/stratification constraints and no-replication, iterative MICQP-based AQuA converged in about 4 iterations and seconds per iteration, producing feasible 42-point subsamples meeting the stated constraints.","The authors note that AQuA currently lacks general theoretical guarantees such as lower bounds on efficiency as a function of problem characteristics. They also report that AQuA can sometimes yield significantly suboptimal designs for small experiment sizes N≥m, particularly at N=m. Additionally, they state there is no theoretical proof of convergence for the proposed sequential/iterative AQuA scheme.","The approach relies on having (or computing) a good anchor (near-optimal) approximate information matrix; while robustness is demonstrated empirically, performance may degrade for harder models/constraints where good anchors are expensive or unstable to obtain. Comparisons are largely against specific solvers (Gurobi, MISOCP approach) and selected examples; broader benchmarking against modern specialized heuristics (e.g., coordinate-exchange variants, metaheuristics, or recent exact design MIP formulations for other criteria) is limited. Practical scalability also depends on availability/performance of commercial conic/MIP solvers and careful numerical linear algebra (e.g., factorization of the quadratic form), which may be challenging in ill-conditioned models.","The authors suggest AQuA can be extended to criteria beyond those treated in the paper, as long as suitable quadratic approximations (analytical or numerical) are available and the quadratic forms have low rank for the conic improvement. They also discuss the possibility of sequential/iterative application starting from a rough anchor matrix to handle situations where the optimal approximate design is hard to compute, though without a convergence theory.","Developing theoretical performance guarantees (approximation ratios or sufficient conditions for near-optimality) for AQuA-produced exact designs under common constraint families would strengthen adoption. Extending the framework to settings with correlated/autocorrelated errors, heteroscedasticity, or model uncertainty (robust/Bayesian exact design) could broaden applicability. Providing open-source, solver-agnostic implementations (e.g., using MOSEK/ECOS/HiGHS where possible) and standardized benchmarks would improve reproducibility and practical uptake, especially for very large design spaces.",1801.09124v3,https://arxiv.org/pdf/1801.09124v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:29:17Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Model discrimination|Cost reduction|Other,Not applicable,"Variable/General (design variables x∈R^d; case study 1: 2 design variables; case study 2: 3 design variables [t, c4(0), c9(0)])",Healthcare/medical|Pharmaceutical|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/cog-imperial/GPdoemd,"The paper develops a sequential design of experiments methodology for discriminating between multiple competing mechanistic models when the original model functions are non-analytical/black-box. It replaces each mechanistic model with a Gaussian process surrogate trained on model evaluations, enabling analytical-style design criteria (e.g., Box–Hill entropy-based, Buzzi-Ferraris, and Akaike-weight criteria) to be applied efficiently while still providing predictive uncertainty. Parameter uncertainty is propagated by approximating the posterior over parameters as Gaussian and then using first- or second-order Taylor expansions to obtain an approximate marginal predictive distribution used for both selecting the next experiment and computing model likelihoods. Performance is evaluated via two simulation case studies: (1) an analytical multi-response chemical kinetics benchmark to compare with classical analytical DOE methods, and (2) an ODE-based biochemical/pharmacokinetic-like benchmark with non-analytical models to demonstrate applicability and computational efficiency. Results show similar discrimination performance to classical analytical approaches on the analytical benchmark and practical runtime advantages (minutes vs days reported for MCMC-based approaches) on the ODE case study, while highlighting limits due to model indistinguishability and experimental noise.","Design selection is performed by maximizing an information-based discrimination criterion over the design variables, e.g. $x^*=\arg\max_x D(x)$ where $D(x)$ can be Box–Hill-type, Buzzi-Ferraris-type, or Akaike-weight-based criteria computed from differences in model predictions and predictive covariances. Each model $f_i(x,\theta_i)$ is replaced with a GP surrogate giving $p(f\mid z,D_{\mathrm{sim}})=\mathcal N(\mu(z),\Sigma(z))$ with $z=[x^\top,\theta^\top]$ and standard GP predictive equations $\mu(z^*)=m(z^*)+k_*^\top K^{-1}(\tilde f-m)$ and $\sigma^2(z^*)=k(z^*,z^*)-k_*^\top K^{-1}k_*$. Parameter uncertainty is incorporated by approximating $p(\theta\mid D_{\exp})\approx\mathcal N(\hat\theta,\Sigma_\theta)$ with $\Sigma_\theta^{-1}=\sum_n \nabla_\theta \mu_n^\top\Sigma_{\exp}^{-1}\nabla_\theta\mu_n$ and using Taylor expansions to approximate the marginal predictive mean/covariance $\dot\mu(x)$ and $\dot\Sigma(x)$.","In case study 1 (4 models, 2 responses, 2 design variables; 500 tests), the GP-surrogate DOE achieves similar performance to classical analytical DOE, with success rates up to 96.9% (second-order Taylor) and reduced failure rates as low as 3.1% for the $\pi$/DBH combination; average additional experiments ranged roughly 2–11 depending on criterion. In case study 2 (ODE models; 63 completed tests shown; max 100 additional experiments), discrimination is often inconclusive for 4 models due to near-indistinguishability of two models: inconclusive rates 58.7–90.5% and success rates 9.5–33.3% depending on criterion. When removing the confusable model (3-model setting), success rises substantially to 77.2–95.6% with low failure (0.9–6.1%) and much lower inconclusive rates (2.6–21.9%). The authors report DOE computation time on the order of minutes for case study 2, contrasting with days for prior MCMC-based methods on similar problems.","The authors note that if the Gaussian approximation to the parameter posterior $p(\theta\mid D_{\exp})\approx\mathcal N(\hat\theta,\Sigma_\theta)$ is inaccurate (given few/noisy experiments), then the Taylor-based marginalization may lose accuracy, though it is computationally tractable. They also acknowledge that model indiscriminability can occur when experimental noise exceeds differences in model predictions (as in case study 2 for models $M_1$ and $M_2$), requiring rethinking the experimental setup (lower noise or add measured states).","The approach requires generating a potentially large simulated dataset $D_{\mathrm{sim},i}$ for each candidate model and fitting separate GP surrogates; for high-dimensional design/parameter spaces this can become expensive and GP training scales poorly without sparse/approximate GP methods. Independence across output dimensions (diagonal predictive covariance in the basic surrogate description) may be restrictive for multi-response systems where outputs are correlated. The method’s performance depends on surrogate fidelity and kernel choice; misspecified kernels or insufficient model-evaluation coverage can bias the design criterion and discrimination decisions. The Taylor-based marginalization assumes local smoothness in parameters around $\hat\theta$ and may be unreliable under multimodal/non-Gaussian posteriors without the proposed mixture extension and an adequate way to obtain those mixture components.","The authors suggest extending the Taylor/marginalization approach from a single Gaussian approximation to a mixture of Gaussians for $p(\theta\mid D_{\exp})=\sum_j \omega_j\mathcal N(\hat\theta_j,\Sigma_{\theta,j})$ when such components are available, to improve accuracy while retaining tractability. They also note that in cases of apparent model indiscriminability, practitioners may need to redesign the experimental setup to reduce measurement noise or include additional measured states.","Developing scalable surrogate modeling (e.g., sparse/variational multi-output GPs) would improve applicability to higher-dimensional design/parameter spaces and correlated multi-response outputs common in pharmacometrics. A fully Bayesian treatment of GP hyperparameters and/or robustness checks for kernel misspecification could reduce overconfidence in surrogate-based predictive variances used by the design criteria. Incorporating constraints and practical experimental costs (time, dosing feasibility, ethics) into the sequential design optimization would make the DOE recommendations more directly actionable. Empirical validation on real pharmacokinetic/medical-device datasets and comparisons against modern Bayesian optimal design baselines (e.g., SMC-based, likelihood-free, or mutual-information estimators) would strengthen evidence for real-world effectiveness.",1802.04170v2,https://arxiv.org/pdf/1802.04170v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:30:07Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Optimization|Cost reduction|Other,A-optimal|D-optimal|Bayesian A-optimal|Bayesian D-optimal,"Variable/General (design vector w has Ns candidate sensor-location weights; example uses Ns = 22, selects k = 5)",Environmental monitoring|Other,Simulation study|Other,TRUE,Other,Not provided,http://hippylib.github.io|http://energy.gov/downloads/doe-public-access-plan,"The paper develops a goal-oriented optimal design of experiments (GOODE) framework for large-scale Bayesian linear inverse problems governed by PDEs, where the design minimizes posterior uncertainty in an end-goal quantity of interest (QoI) rather than in the full inferred parameter. It proposes two criteria—A-GOODE and D-GOODE—defined as the trace and log-determinant of the QoI posterior covariance \(\Sigma_{\text{post}}(w)=P\Gamma_{\text{post}}(w)P^*\), providing decision-theoretic interpretations via expected Bayes risk (A-GOODE) and expected information gain/KL divergence (D-GOODE). The authors derive scalable, matrix-free gradient formulas and present gradient-based optimization algorithms for sensor placement, including sparsity promotion using an \(\ell_1\)-penalty and bounded constraints \(0\le w_i\le 1\). Numerical experiments on contaminant source identification (advection–diffusion PDE) demonstrate that GOODE yields different sensor placements depending on the prediction goal (e.g., around different buildings) and that targeting low-dimensional QoIs can reduce computational cost relative to classical Bayesian OED. The implementation uses PDE solves (forward/adjoint) and supports parallelism across QoI dimensions and sensor candidates; penalty-parameter selection is studied via an L-curve strategy and brute-force comparison for small instances.","Design criteria are defined on the QoI posterior covariance \(\Sigma_{\text{post}}(w)=P\Gamma_{\text{post}}(w)P^*\) with \(\Gamma_{\text{post}}(w)=[F^*W_\Gamma F+\Gamma_{\text{pr}}^{-1}]^{-1}\). A-GOODE minimizes \(\Psi_{GA}(w)=\operatorname{tr}(\Sigma_{\text{post}}(w))\) and D-GOODE minimizes \(\Psi_{GD}(w)=\log\det(\Sigma_{\text{post}}(w))\). Sensor-placement weights enter the likelihood through \(W=I_{N_t}\otimes \mathrm{diag}(w)\) (relaxing binary selection), and optimization solves \(\min_w \Psi(w)+\alpha\|w\|_1\) subject to \(0\le w_i\le 1\).","For a contaminant transport inverse problem with \(N_s=22\) candidate sensor locations and observations at \(t\in\{0.4,0.6,0.8\}\), GOODE designs concentrate sensors near the region/building targeted by the prediction QoI, producing distinctly different placements for different goals (first building vs second building vs both). Vector-valued prediction operators have dimensions \(N_{\text{goal}}\in\{164,138,302\}\) (for three goals at \(t_{\text{pred}}=1.0\)), and the resulting A-GOODE vs D-GOODE designs differ even with the same prediction operator. The paper reports brute-force enumeration of all \(\binom{22}{5}=26{,}334\) 5-sensor designs for scalar-goal cases to validate the penalty-parameter selection and shows near-optimal \(\alpha\) values lie near the L-curve elbow. Gradient evaluations scale with PDE solves roughly proportional to \(O(N_{\text{goal}} r)\) (with r a numerical rank/iteration count) for A-GOODE and to either \(N_{\text{goal}}\) or \(N_s\) Hessian solves for the two D-GOODE gradient forms, enabling parallelization.","The authors note that solving the original binary sensor-selection problem is combinatorial/intractable, so they relax to continuous weights \(w_i\in[0,1]\) and then use a heuristic thresholding (pick the top-k weights) to obtain a binary design, yielding only a “near-optimal” solution to the original discrete problem. They also state that for non-binary weights the design is hard to interpret and implement, motivating thresholding. For D-optimality, they remark that using \(P=I\) makes \(\log\det(\Gamma_{\text{post}})\) meaningless in the infinite-dimensional limit, indicating limits of the naive determinant-based criterion without goal reduction.","The numerical validation is primarily on a single PDE-based application (advection–diffusion contaminant transport) with a fixed candidate-grid and mostly diagonal (uncorrelated) noise, so robustness to correlated observation errors and model mismatch is not fully assessed. The approach relies on linear-Gaussian assumptions (linear forward map, Gaussian prior/noise) and gradients of \(\Sigma_{\text{post}}\); performance and optimality under nonlinearity or non-Gaussian posteriors is not demonstrated. The \(\ell_1\) sparsity penalty plus thresholding can be sensitive to scaling and \(\alpha\), and may produce designs that depend on heuristic choices (k, threshold rule) without guarantees; comparisons to alternative sparse/binary optimization schemes (e.g., mixed-integer, reweighted \(\ell_1\), \(\ell_0\)-approximation) are limited.","They propose extending GOODE to nonlinear end-goal operators by linearization (using the Fréchet derivative) and note that developing practical GOODE strategies for nonlinear Bayesian inverse problems and nonlinear goal operators is future work. They also mention the challenge of choosing a data-dependent linearization point (e.g., MAP) because the data are unavailable a priori, referencing related strategies in nonlinear A-optimal design as a possible path.","Further work could develop fully discrete/binary sensor selection methods with optimality guarantees (e.g., mixed-integer or greedy/submodular approximations) that avoid post-hoc thresholding. Extending GOODE to correlated and temporally dependent noise (non-diagonal \(\Gamma_{\text{noise}}\)) and to model-error-aware designs would improve practical applicability. Creating open-source, reproducible implementations (e.g., a HippyLib example module) and benchmarking against alternative goal-oriented criteria (I-/G-optimality, risk-averse/robust Bayesian criteria) across multiple PDE applications would strengthen empirical evidence. Adaptive/sequential GOODE where sensor locations are updated as data arrive (online design) would also be a natural extension for monitoring problems.",1802.06517v2,https://arxiv.org/pdf/1802.06517v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:31:05Z
TRUE,Optimal design|Other,Parameter estimation|Other,A-optimal|Other,Variable/General (n experimental units on a graph; m treatments; also augmented block nodes for block/row-column/crossover designs),Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Other,Personal website,https://www.dropbox.com/s/fgfn2f2my4yqlzr/networkDesign_0.0.0.9001.tar.gz?dl=0,"The paper proposes a graph-theoretic framework for constructing exact optimal experimental designs by exploiting graph automorphisms to avoid redundant evaluations of candidate designs. Experiments are represented as networks of experimental units under a Linear Network Effects Model (LNEM), where responses depend on a unit’s assigned treatment and the treatments assigned to neighboring units (spillover/network effects). The main algorithm computes automorphisms (via VF2/igraph) and evaluates the optimality criterion only for lexicographically minimal representatives of each automorphism class, reducing criterion evaluations and wall-clock time in exhaustive and coordinate-exchange searches. The framework is extended by augmenting traditional block, row-column, and crossover designs with additional “block nodes” to induce a network structure so the same automorphism-based speedups apply. Empirical timing tables across several example networks and blocking structures show substantial reductions in evaluations and computation time, though the overhead can dominate when the number of automorphisms is extremely large.","The core model is the Linear Network Effects Model: $Y_i=\mu+\tau_{t(i)}+\sum_{k=1}^n A_{ik}\,\gamma_{t(k)}+\varepsilon_i$, where $A$ is the adjacency matrix and $t(i)$ is the treatment assigned to unit $i$. In matrix form, $\mathbb{E}(Y)=F\beta$ with design matrix columns for the intercept, direct-treatment indicators $u_j$, and network-spillover terms $Au_j$, giving Fisher information $I=F^\top F$. The paper focuses on minimizing the average variance of all pairwise treatment-effect differences (termed $A_s$-optimality for subject effects), e.g. proportional to $\frac{2}{m(m-1)}\sum_{j<\ell}\mathrm{Var}(\tau_j-\tau_\ell)$ (with an identifiability constraint such as $\tau_m=0$).","On a 10-node social network example with $m=2$, exhaustive search required 507 criterion evaluations without automorphism filtering versus 236 with filtering (time 0.04s vs 0.02s). For a 20-node social network, evaluations dropped from 524,287 to 221,183 (58.58s to 31.56s). For a 12-node “block design with neighbour effects,” evaluations dropped from 535,008 to 18,766 (108.52s to 33.68s). In induced block/row-column settings, reductions can be dramatic (e.g., 4×4 row-column with $m=4$: 170,863,644 to 1,610,909 evaluations; 141,456.6s to 14,123.94s), though some cases show increased time due to automorphism/lex-order overhead (e.g., 4×3 blocks). Coordinate-descent with automorphism filtering used far fewer evaluations than exhaustive search while achieving high efficiency (often near 1.0) on the provided examples.","The authors note computational overhead from (i) initially finding automorphisms and (ii) checking whether a candidate design is lexicographically minimal among its automorphisms. They state the overhead may be prohibitive for networks with a very large number of automorphisms (e.g., dense or highly regular graphs), and show in examples that runtime can increase in such cases despite fewer criterion evaluations.","The framework’s gains depend strongly on the ability of the automorphism software to compute (potentially large) automorphism groups reliably and quickly; performance may degrade for large-scale graphs or complex constraints beyond the paper’s examples. The paper largely benchmarks on relatively small/medium $n$ with unstructured treatments and specific criteria; it does not provide a systematic study across varying $n,m$, graph families, and constrained randomization rules often required in practice. The LNEM assumes i.i.d. errors and a specific linear spillover structure; robustness to misspecification (nonlinear/heterogeneous interference, autocorrelation in crossover, etc.) is not explored, which can affect the practical optimality of the resulting designs.",None stated.,"Extend the automorphism-filtering idea to additional design constraints (e.g., forced balance, exact replication counts, restricted randomization) and to broader optimality targets (e.g., $I$- or $G$-optimal prediction under interference). Develop scalable implementations for larger networks, possibly using group-theoretic orbit computations or incremental automorphism updates during search. Study robustness when the assumed network/spillover model is misspecified and investigate Bayesian or minimax formulations that hedge against uncertain adjacency/spillover parameters. Provide a maintained open-source package (e.g., CRAN/GitHub) with reproducible benchmarks and interfaces to common optimal-design algorithms.",1802.09582v1,https://arxiv.org/pdf/1802.09582v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:31:38Z
TRUE,Response surface|Optimal design|Other,Parameter estimation|Other,D-optimal,Variable/General (illustrated with 3-factor examples),Food/agriculture|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper develops methods for constructing exact locally D-optimal designs for multifactor nonlinear response surface models, motivated by chemical/biological experiments where mechanistic or hybrid nonlinear models are appropriate. It reviews standard discrete exchange algorithms (point exchange and coordinate exchange) for D-optimal design and shows their limitations when the design region is continuous and candidate sets are hard to predefine for nonlinear models. The authors propose a continuous-optimization exchange approach (using Nelder–Mead or quasi-Newton search over the full continuous region) and an additional multiphase optimization strategy that combines: (i) coarse discrete optimization to obtain good starting designs, (ii) continuous refinement, and (iii) rounding/replication refinement based on practical “closest distance” constraints. Two real experimental case studies (a 3-factor mechanistic chemical reactor yield model and a 3-factor hybrid enzymatic conversion model) demonstrate that the proposed methods yield substantially higher local D-criterion values (and thus higher information for parameter estimation) than the original central composite designs and than discrete-only optimization with fixed candidates. The work advances nonlinear DOE practice by providing a pragmatic, computationally efficient path to high-quality exact D-optimal designs without needing an overly dense prespecified candidate set.","The design criterion is the local D-optimality objective $\phi=\log|F^T F|$, where for a nonlinear model $Y_i=f(X_i,\theta)+\varepsilon_i$, the model matrix row is obtained by first-order linearization at a prior $\theta_0$: $f(X_i,\theta)\approx f(X_i,\theta_0)+\sum_{j=1}^p \left.\partial f(X_i,\theta)/\partial\theta_j\right|_{\theta_0}(\theta_j-\theta_{0j})$, giving $F_i=\left.\partial f(X_i,\theta)/\partial\theta\right|_{\theta_0}$. Exchange steps maximize a relative improvement $d_i=|F^T F|_{\text{new}}/|F^T F|$ over either a candidate set (discrete) or over the continuous region $\mathcal{X}$ (continuous optimization).","Example 1 (3 factors, 24 runs, mechanistic reactor model): reference face-centred CCD has $\phi=-52.7712$; discrete optimization over a 27-point candidate set gives $\phi=-49.7321$; continuous PEA improves to $\phi=-49.5528$; the multiphase method further improves to $\phi=-49.5116$ with only 8 distinct support points (unequal replication). Example 2 (3 factors, 18 runs, hybrid enzymatic model): reference face-centred CCD has $\phi=31.7538$; discrete optimization yields $\phi=38.8433$; continuous optimization yields $\phi=41.2246$ (reported for both continuous PEA/CEA), and the multiphase method matches $\phi=41.2246$ while simplifying to 9 distinct points (mostly replicated). Reported relative efficiencies include, for Example 2, discrete vs. continuous: $\exp(38.8433/6)/\exp(41.2246/6)\approx 67.24\%$; the empirical-model design and the CCD are far less efficient (about 32.49% and 20.63% respectively) under the nonlinear model.","The authors emphasize that local D-optimal designs depend on the assumed nonlinear model form and the chosen prior parameter values $\theta_0$; if the prior is inaccurate (large $|\theta-\theta_0|$), the design may be suboptimal though still valid. They also note that continuous optimization finds designs that are only guaranteed to be locally optimal, not globally optimal. Practical implementation may require rounding/replication adjustments (Phase 3) because continuous optimization tends to produce near-replicates rather than exact replicates.","The paper’s performance claims are demonstrated on two 3-factor case studies; broader evidence (more factors, different nonlinear structures, and diverse constraint types) would better establish robustness and scalability. The approach targets local D-optimality (parameter-estimation focused) and does not directly address robustness to prior misspecification via Bayesian/standardized maximin criteria, which are often important in nonlinear DOE. Implementation details (e.g., tuning of critical values, convergence diagnostics, and sensitivity to starting-point strategies) may affect reproducibility and outcomes, yet no public code is provided. The computational burden for higher-dimensional problems (large $vn$) is not fully characterized beyond qualitative discussion.","They suggest applying the proposed methodology to study and design experiments for different classes of nonlinear multifactor experiments beyond the two illustrative examples. They also highlight practical challenges of (i) identifying an appropriate nonlinear/hybrid model form and (ii) selecting realistic prior parameter values, implying further work on these aspects and on promoting such designs in application areas.","Develop Bayesian or robust (e.g., maximin/standardized) extensions of the multiphase continuous-exchange approach to explicitly handle parameter uncertainty rather than relying on a single local prior. Extend the methods to accommodate correlated errors, heteroscedasticity, constraints such as blocked/split-plot structures, and to generalized/non-Gaussian nonlinear models (e.g., nonlinear mixed effects) in a unified framework. Provide open-source implementations (e.g., MATLAB toolbox/R/Python) with benchmarks, and study computational scaling with factors/runs and different optimizers. Add guidance for practitioners on selecting closest-distance thresholds, rounding rules, and on diagnosing model misspecification and design sensitivity.",1803.06536v2,https://arxiv.org/pdf/1803.06536v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:32:18Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Model discrimination,Bayesian D-optimal|Compound criterion|Other,Variable/General (design dimension q = n·w; examples include up to q = 192; runs n up to 50; w up to 2 in examples),Healthcare/medical|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org/web/packages/acebayes/index.html,"The paper develops a general-purpose Bayesian optimal design methodology for models with intractable likelihoods, targeting realistic (high-dimensional) design spaces. It proposes an automatic auxiliary-modeling approach in which conditional and marginal likelihood components are approximated using multivariate Gaussian process emulators trained on simulation output from the mechanistic model. To enable likelihood-based utilities that require the marginal likelihood (evidence), it introduces a copula-based approximation that couples marginal auxiliary models with an auxiliary copula (typically a t-copula) to approximate dependence across runs. Designs are then optimized using the approximate coordinate exchange (ACE) algorithm, allowing large design dimensionality (reported up to q=192). The approach is demonstrated on several stochastic-process examples with aims of parameter estimation (e.g., aphid growth, parasite dynamics) and extended to model comparison using utilities such as Shannon information gain and 0–1 utility, showing improved expected utilities and major computational savings versus nested Monte Carlo.","Bayesian optimal design chooses $D$ to maximize expected utility $U(D)=\int u(\theta,y,D)\,dP_{\theta,y\mid D}$, approximated by Monte Carlo $\tilde U(D)=\frac{1}{B}\sum_{i=1}^B u(\theta_i,y_i,D)$. A key likelihood-based utility is Shannon information gain $u_S(\theta,y,D)=\log\pi(y\mid\theta,D)-\log\pi(y\mid D)$ with marginal likelihood $\pi(y\mid D)=\int \pi(y\mid\theta,D)\pi(\theta)d\theta$. For intractable likelihoods, the paper replaces $\pi(y\mid\theta,D)$ by an auxiliary likelihood $\pi_X(y\mid\theta,D)=\prod_{k=1}^n f_X(y_k\mid\theta,d_k)$ and approximates $\pi(y\mid D)$ via a copula factorization $\pi_X(y\mid D)=c_X(G_X(y_1\mid d_1),\ldots,G_X(y_n\mid d_n))\prod_{k=1}^n g_X(y_k\mid d_k)$.","In the compartmental-model check (tractable likelihood), the SIG design found via the proposed auxiliary Monte Carlo approach achieved an expected SIG of 4.28 (SE 0.003) versus 3.70 (SE 0.003) for an equally-spaced design, and was close to the nested Monte Carlo (exact likelihood) benchmark of 4.51 (SE 0.003). For the aphid model under SIG, auxiliary Monte Carlo design search scaled to n=50 with average compute time increasing from 0.5 hours (n=5) to 36.1 hours (n=50), while nested Monte Carlo was already 8.5 hours at n=5 and not attempted for larger n. In both the aphid and parasite examples, designs optimized under auxiliary Monte Carlo yielded higher estimated expected SIG/LR utilities than simple space-filling or equally-spaced designs, and matched nested Monte Carlo designs closely when nested Monte Carlo was feasible (n=2 or 5).","The authors state the methodology is not applicable to non-likelihood-based utility functions (e.g., utilities based on posterior variance/precision summaries), which may be preferred in some applications. They also emphasize that auxiliary model adequacy must be checked and may require iterative refinement (choice of auxiliary distribution and emulator settings), implying performance depends on the quality of the auxiliary approximation.","The approach relies on substantial offline simulation (e.g., large training sample sizes and MGP fitting) and on the assumption that the auxiliary emulator generalizes well across the design/prior space; poor coverage (or strong nonstationarity) can degrade design quality. Copula-based marginal likelihood approximation may struggle when the true dependence structure is complex, high-dimensional, or includes discrete components where copula factorization is not unique, potentially biasing evidence-based utilities. The design optimization is approximate and may be sensitive to ACE tuning, Monte Carlo noise, and local optima, especially for very large q and constrained design spaces.","The authors suggest exploring other deterministic approximations to posterior quantities (e.g., expectation propagation) as alternatives to normal-based approximations previously proposed in related work, motivated by potentially non-normal posteriors in intractable-likelihood settings.","Useful extensions include (i) principled uncertainty propagation from the emulator into the utility (fully Bayesian emulation rather than plug-in MLEs), (ii) methods for adaptive/sequential design where auxiliary models and copulas are updated as data accrue, and (iii) broader copula families or vine copulas to better capture complex dependencies. Additional empirical validation on real experimental datasets, plus user-friendly software beyond ACE (e.g., end-to-end pipelines and diagnostics), would strengthen practical adoption.",1803.07018v3,https://arxiv.org/pdf/1803.07018v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:32:55Z
TRUE,Optimal design|Other,Parameter estimation,D-optimal|A-optimal,"Variable/General (examples include 2 and 3 continuous design variables; parameter dimensions p=2,6,10)",Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Upon request,https://arxiv.org/abs/1801.05661,"The paper proposes simple iterative computational algorithms to compute optimal continuous (approximate) experimental designs for linear regression models directly on continuous design spaces, avoiding discretization of the design region. The main focus is on D-optimality (maximizing log-determinant of the information matrix) and A-optimality (minimizing the trace of the inverse information matrix). For the D-optimal algorithm, the authors provide an alternative proof of monotonic increase of the log-determinant along iterations and show convergence to the D-optimal design under a boundedness assumption. For A-optimality, they provide an analogous multiplicative-style update, report strong numerical evidence of convergence, and conjecture monotonic convergence. Numerical illustrations on circular-disk and hypercube design regions (2D and 3D polynomial regression examples) show the algorithm recovers known/support-point structures and yields weights close to theoretical values, comparing favorably to a randomized exchange (REX) method.","Model: $y(w)=x(w)^T\beta+\varepsilon(w)$ on a continuous design space $E$, with design density $f(w)\ge 0$ and $\int_E f(w)\,dw=1$. Information matrix: $I_E(f,x)=\int_E f(w)x(w)x(w)^T\,dw$ (and $\mathrm{Var}(\hat\beta)=I_E(f,x)^{-1}$ up to $\sigma^2$). D-optimal criterion: minimize $-\log|I_E(f,x)|$; update $f^{(n)}(w)=f^{(n-1)}(w)\,\frac{x(w)^T D^{(n-1)}x(w)}{p}$ where $D^{(n-1)}=\left[\int_E f^{(n-1)}(w)x(w)x(w)^T\,dw\right]^{-1}$. A-optimal criterion: minimize $\mathrm{tr}(I_E(f,x)^{-1})$; update $f^{(n)}(w)=f^{(n-1)}(w)\,\frac{p\left[(p-1)\,\mathrm{tr}(D^{(n-1)}x(w)x(w)^T D^{(n-1)})/\mathrm{tr}(D^{(n-1)})+1\right]}{p}$ with the same $D^{(n-1)}$ definition.","For D-optimality, the authors prove the log-determinant of the information matrix is nondecreasing across iterations (monotonicity) and show $\int_E |f^{(n)}(w)-f^{(n-1)}(w)|dw\to 0$ under a boundedness condition. In numerical examples, the computed optimal designs on the unit disk concentrate on the boundary (and include a central mass near $(0,0)$ when an intercept is present), matching theoretical proportions (e.g., for a quadratic model on the disk, approximately $1/6$ mass near the center and $5/6$ on the boundary ring). On $[-1,1]^2$ with quadratic terms, the method identifies 9 support points with weights close to theoretical values; on $[-1,1]^3$ it identifies 27 support points with weights close to theoretical values. Comparisons with the REX algorithm indicate the proposed method yields more stable/accurate weights in some settings (e.g., Settings 2 and 4) and avoids dependence on the number of discretization/design points.","For the A-optimality algorithm, the authors state that a theoretical justification (proof) of convergence is not available; they rely on simulation evidence and conjecture monotonic convergence on continuous design spaces. For D-optimality convergence, they assume $\log\left|\int_E x(w)x(w)^Tdw\right|$ is bounded.","The algorithms require evaluating integrals over the continuous design region at every iteration (e.g., $\int_E f(w)x(w)x(w)^Tdw$), which may be computationally challenging in higher dimensions or for complex regions without efficient quadrature/Monte Carlo schemes. The paper focuses on homoskedastic, independent errors and linear-in-parameters models; performance/robustness under model misspecification, heteroskedasticity, or correlated observations is not addressed. Implementation details (numerical integration strategy, stopping criteria, tuning, and complexity) are not fully standardized, and no public code is provided, which may limit reproducibility.","The authors suggest that while they focus on D- and A-optimality for linear models, the basic ideas are not limited to these criteria and that the continuous design space setting can be extended to high-dimensional situations. They also implicitly point to proving convergence/monotonicity for the A-optimal algorithm as an open direction (they conjecture monotonic convergence).","Provide rigorous convergence and (if true) monotonicity proofs for the A-optimal update, including conditions under which the algorithm converges to a globally optimal design. Develop scalable numerical integration/approximation schemes (e.g., adaptive quadrature, sparse grids, or stochastic approximations) and analyze computational complexity as dimension grows. Extend the approach to other optimality criteria (I-, G-, E-optimality), constrained designs, and settings with correlated/heteroskedastic errors or generalized linear models, and release reproducible open-source implementations with benchmarks.",1804.02655v1,https://arxiv.org/pdf/1804.02655v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:33:34Z
FALSE,NA,NA,Not applicable,Variable/General,Other,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper uses detailed Geant4 Monte Carlo simulations to study how detector-configuration choices affect the performance of plastic scintillator experiments coupled to a fixed-area photo-detector (SiPM-like). It varies detector “design parameters” such as scintillator size/volume-to-sensor-area ratio (scale factor F), presence and thickness of a reflective TiO2-based coating, surface roughness, coating reflectivity, and single vs. two-component scintillation decay. Using isotropic electron and proton primaries from 1 MeV to 1 GeV (plus some lower-energy checks for coating-threshold effects), it quantifies deposited energy, scintillation photon production, optical attenuation/technical attenuation length, light-collection fractions, and pulse-shape timing parameters at the photo-detector. Key findings include strong gains from reflective coating (substantially higher detected-photon fractions) and strong losses in collected signal as plastic volume grows due to more reflections and absorption/attenuation. The work is a simulation-based detector-optimization study rather than a formal DOE methodology contribution.","Key computed/fit relationships include: (i) scintillation light modeled via a scintillation yield (given as 8000 photons/MeV) with Birks quenching enabled; (ii) an exponential fit to the distribution of optical-photon path lengths before absorption, $A\,e^{-x/L_{\mathrm{abs}}}$, used to extract an effective/technical attenuation length $L_{\mathrm{abs}}$; and (iii) exponential fits to pulse tails, $N e^{-t/\tau}$ (or a two-component model $N_1 e^{-t/\tau_{\mathrm{slow}}}+N_2 e^{-t/\tau_{\mathrm{fast}}}$ when using dual decay components). Light-collection metrics are defined as fractions of produced photons reaching the sensor directly ($F_{\mathrm{geom}}$) or after reflections ($F_{\mathrm{ref}}$), and interface outcomes are summarized by fractions $R,RT,A,T$ (reflection, total internal reflection, absorption, transmission).","They simulate 200 events per configuration over electrons/protons, energies (1, 10, 30, 100 MeV, 1 GeV), scale factors F=1,3,10, and coating thicknesses (typically 0.25 and 0.50 mm; also 0.15 mm for threshold checks), totaling >18,000 simulations. With coating (0.25 mm), the fraction of detected scintillation photons is about 70%, 12%, and 0.8% for F=1, 3, and 10 respectively; without coating it drops to about 38%, 3.5%, and 0.3%. Coating thickness sets an energy threshold: with 0.25 mm, electrons below ~1 MeV and protons below ~10 MeV are largely stopped in the coating (e.g., ~50% of 10 MeV protons fail to reach the scintillator), while a thinner 0.15 mm coating allows detection of ~300 keV electrons. From optical-photon path-length fits, the extracted technical attenuation lengths are about 3.5 cm (F=1), 28.0 cm (F=3), and 81.8 cm (F=10), despite an input bulk absorption length of 250 cm. Increasing coating reflectivity (for F=3) raises detected-photon fraction from ~10% (reflectivity 0.85) to ~22% (reflectivity 0.96) and increases pulse timing parameters (wider pulses).","The authors note that a detailed simulation of the photo-detector response is complex and beyond the scope of the work; they model the photo-detector simply as a silicon block and terminate photons upon crossing into it. They also remark that coating thickness effects on optical behavior are treated as negligible in their optical model because the coating is implemented as an effectively non-transmitting reflective boundary with fixed reflectivity, so thickness mainly matters for the primary-particle energy threshold rather than optical transport.","The study is not a formal designed experiment/DOE: factors are explored via selected settings (e.g., discrete F values and coating thicknesses) without an explicit factorial/optimal design structure, making interaction effects and efficient exploration less systematic. Many simulation parameters (optical surface model choice, fixed reflectivity spectrum, roughness=0.5 default, isotropic flux assumption, and limited event count per cell) could materially affect results and uncertainty quantification, yet there is no sensitivity/uncertainty analysis beyond percentile error bars. The comparison is entirely simulation-based with limited validation against experimental measurements for the specific configurations considered, so real-world manufacturing tolerances (surface finish, coupling losses, wavelength-dependent reflectivity, sensor PDE, electronics shaping) may change absolute performance. The physics case is constrained to electrons/protons and a specific extruded scintillator/coating model; extension to heavier ions, gamma backgrounds, and realistic sensor/electronics models could alter thresholds and pulse-shape conclusions.","They suggest that each specific experiment should perform a full simulation of the complete assembly (including a more realistic photo-detector treatment) to understand the impact of primaries directly impinging on the photo-detector and other configuration-dependent effects. They also note that parameters like coating reflectivity may depend on coating thickness in practice, implying further study of that dependence beyond the simplified constant-reflectivity modeling.","A natural extension would be a structured multi-factor DOE (e.g., factorial or space-filling design) over geometry, coating thickness, reflectivity, and surface finish to quantify interactions and build surrogate models for optimization under mass/power constraints. Adding realistic SiPM/PMT models (PDE vs. wavelength, dark noise, gain fluctuations) and front-end electronics shaping would make pulse-parameter conclusions directly actionable. Broader validation against laboratory measurements for representative geometries would calibrate optical-surface parameters and attenuation lengths, improving predictive accuracy. Finally, expanding to heavier ions, mixed radiation fields, and orbital environment spectra (including angular distributions and shielding) would improve applicability to space-borne detector design.",1804.08975v2,https://arxiv.org/pdf/1804.08975v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:34:16Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design|Other,Optimization|Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (examples include 2 parameters per dopant + concentration; R Boolean uncertainties in gene networks; 20 discrete actions in a simulation),Semiconductor/electronics|Food/agriculture|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops a generalized formulation of the mean objective cost of uncertainty (MOCU) and derives an associated Bayesian experimental design policy that selects experiments to minimize the expected remaining MOCU. The framework is decision-theoretic (fits Lindley’s Bayesian experimental design paradigm) and accommodates flexible choices of uncertainty class, action/operator space, experiment space, and cost/utility tied directly to an operational objective (regret-style differential cost). The authors show that several existing objective-based experimental design formulations in materials discovery and dynamical gene networks (including settings with experimental error) arise as special cases. A central result is that under the modeling assumptions of offline ranking-and-selection with Gaussian beliefs and observation noise, the generalized MOCU policy coincides with the classical Knowledge Gradient (KG), and in the noiseless restricted-decision setting it reduces to Efficient Global Optimization (EGO)/Expected Improvement. A simulation study on a quadratic reward function illustrates improved opportunity cost for MOCU-based design versus a GP-based KG approximation when uncertainty is modeled at the parameter level rather than directly on action rewards.","MOCU is defined as $M_\Psi(\Theta)=\mathbb{E}_\theta[C(\theta,\psi^{\mathrm{IBR}}_{\Theta})-C(\theta,\psi_\theta)]$ with objective cost of uncertainty $U_\Psi(\Theta)=C(\theta,\psi^{\mathrm{IBR}}_{\Theta})-C(\theta,\psi_\theta)$. Experimental design selects $\xi^*=\arg\min_{\xi\in\Xi} D_\Psi(\Theta,\xi)$ where $D_\Psi(\Theta,\xi)=\mathbb{E}_\xi\big[M_\Psi(\Theta\mid\xi)\big]$ and $M_\Psi(\Theta\mid\xi)=\mathbb{E}_{\theta\mid\xi}[C(\theta,\psi^{\mathrm{IBR}}_{\Theta\mid\xi})-C(\theta,\psi_\theta)]$. Under KG assumptions with $C(\theta,\psi)=-\theta_\psi$, the IBR action is $\psi^{\mathrm{IBR}}_{\Theta\mid\xi^{:t}}=\arg\max_{\psi\in\Psi} m^t_\psi$ and the induced experiment policy is $\xi^{*,t}=\arg\max_{\xi_i}\mathbb{E}[\max_{\psi'} m^{t+1}_{\psi'}]-\max_{\psi'} m^t_{\psi'}$ (KG); in a noiseless restricted-choice setting it becomes expected improvement (EGO).","The paper provides an analytical equivalence result: generalized MOCU-based experimental design yields the same sequential sampling policy as Knowledge Gradient (KG) for ranking-and-selection under Gaussian belief models with additive Gaussian observation noise, and reduces to Efficient Global Optimization/Expected Improvement (EGO/EI) under noiseless observations with selection restricted to previously observed actions. In a Monte Carlo study (1,000 runs) on a 1D quadratic reward with unknown parameters and unknown noise variance, a MOCU-based parameter-uncertainty approach achieves lower average opportunity cost than a KG approach implemented via Gaussian process regression with estimated hyperparameters. The authors attribute the improvement to explicitly modeling uncertainty in underlying model parameters rather than directly placing approximate Gaussian beliefs on discrete action rewards.",None stated.,"The work is largely a conceptual unification and equivalence derivation; beyond one illustrative simulation, it does not provide broad empirical benchmarking across diverse objective functions, noise regimes, or misspecified priors. Practical computation of generalized MOCU (nested expectations and argmin/argmax over actions/experiments) can be expensive in high-dimensional continuous settings, but computational complexity/scalability guidance is not fully developed here. The claimed advantages over KG/EGO depend on having a useful parametric/physical model class for the underlying system; when such structure is unavailable, the framework may offer limited practical benefit over standard Bayesian optimization methods.",None stated.,"Develop scalable algorithms/approximations (e.g., variational, Monte Carlo, surrogate-based) for computing generalized MOCU and its acquisition function in high-dimensional continuous design spaces. Extend the framework to settings common in practice: unknown/estimated nuisance parameters (Phase I), model misspecification robustness, autocorrelated observations, and constraints/cost-aware or batch/parallel experimentation. Provide software and standardized benchmarks comparing MOCU-based acquisition to KG/EI/UCB/entropy-based criteria on real experimental campaigns (materials, genomics, signal processing) to validate practical gains and guide default modeling choices.",1805.01143v1,https://arxiv.org/pdf/1805.01143v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:34:41Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Screening|Optimization|Model discrimination|Prediction|Robustness|Other,Bayesian D-optimal|Not applicable|Other,"Variable/General (examples include 1D, 2D, and 3D action spaces; e.g., 3 variables in electrolyte design: Q, S, T)",Energy/utilities|Food/agriculture|Environmental monitoring|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper proposes a general Bayesian sequential design-of-experiments framework where the experimenter specifies (i) a probabilistic model for outcomes and (ii) a goal encoded as a penalty function over the unknown parameter and collected data. It introduces Myopic Posterior Sampling (MPS), a greedy/sequential policy that samples a parameter from the current posterior (Thompson/posterior sampling) and then chooses the next experiment minimizing a one-step look-ahead expected penalty under that sampled parameter. MPS is implemented via probabilistic programming with approximate Bayesian inference (variational inference; one example uses an analytical posterior), and look-ahead penalties are estimated by Monte Carlo sampling. Empirically, MPS is competitive with specialized methods across several DOE tasks including Bayesian active learning, posterior estimation (active regression), and a combined multi-objective electrolyte-design problem using a real dataset. Theoretically, the authors derive sublinear regret guarantees versus the myopically optimal policy (which knows the true parameter) under structural conditions (episodic penalties, recoverability, or “more data is better”), and relate performance to information gain; with adaptive submodularity/monotonicity they also compare to globally optimal non-myopic policies.","The DOE objective is expressed via either cumulative penalty $\Lambda(\theta^\star,D_n)=\sum_{t=1}^n \lambda(\theta^\star,D_t)$ or a terminal penalty $\lambda(\theta^\star,D_n)$. MPS uses a one-step look-ahead expected penalty $\lambda^+(\theta,D,x)=\mathbb{E}_{Y_x\sim P(\cdot\mid x,\theta)}[\lambda(\theta, D\uplus\{(x,Y_x)\})]$. At step $t$, it samples $\theta\sim P(\theta^\star\mid D_{t-1})$ and chooses $X_t=\arg\min_{x\in\mathcal X}\lambda^+(\theta,D_{t-1},x)$, then observes $Y_{X_t}$ and appends $(X_t,Y_{X_t})$ to the dataset.","Implementation-wise, for most experiments the posterior is approximated with variational inference in Edward and the look-ahead penalty is estimated with 50 Monte Carlo draws from $Y\mid X,\theta$ per candidate action; $\lambda^+$ is minimized by grid search (grid sizes 100/2500/27000 for 1D/2D/3D). Empirically, plots show MPS consistently outperforming random sampling and being close to the (unrealizable) myopically optimal policy that knows $\theta^\star$, while also being competitive with task-specific baselines (e.g., ActiveSelect for active learning and GP-EVR for posterior estimation). The main theoretical bound (finite action spaces) gives expected cumulative regret versus the myopic optimal policy: $\mathbb{E}[J_n(\theta^\star,\pi_{M}^{PS})-J_n(\theta^\star,\pi_M^\star)]\le B\sqrt{\tfrac{n|\mathcal X|\Psi_n}{2}}$, where $\Psi_n$ is the maximum mutual information gain and $B$ depends on the structural condition. Under monotonicity and adaptive submodularity, a guarantee is also provided comparing final reward $\mu=1-\lambda$ of MPS to that of the globally optimal policy run for fewer steps, up to an additive term on the order of $\sqrt{|\mathcal X|\Psi_n/n}$.","They note computational challenges: sampling from complex posteriors (Step 3) may be difficult and requires approximate inference (e.g., MCMC/variational inference), and minimizing the look-ahead penalty (Step 4) can be non-trivial because it may require empirical estimation of the expectation in $\lambda^+$. They also remark that extending the method beyond myopic (e.g., $k$-step look-ahead) quickly becomes impractical due to optimization over $\mathcal X^k$.","The experiments rely on approximate inference (variational approximations) and discretized/grid optimization for selecting actions, which can materially affect performance but are not systematically stress-tested (e.g., sensitivity to inference bias, grid resolution, and Monte Carlo sample size for $\lambda^+$). The strongest regret bound in Theorem 2 is stated for finite action spaces, while several showcased problems use continuous domains; bridging theory-to-practice would require additional assumptions (e.g., Lipschitz/smoothness) and an explicit optimization oracle or approximation analysis. Practical DOE settings often involve constraints, batch/parallel decisions, nonstationarity, and correlated/noisy time series outcomes; these are discussed only briefly (parallelism) and not empirically evaluated here.","They explicitly identify studying policies with $k$-step look-ahead (interpolating between myopic and fully optimal planning) as a natural theoretical question for future work. They also mention that their results could be generalized to very large or infinite action spaces under additional structure using known techniques, and that MPS can be applied in synchronous/asynchronous parallel settings with regret guarantees that depend mildly on the number of workers.","Provide end-to-end approximation guarantees that incorporate (i) posterior approximation error (VI/MCMC) and (ii) action-selection approximation error (grid or numerical optimization) to better match real implementations. Extend empirical validation to higher-dimensional continuous design spaces with realistic constraints and to settings with model misspecification/non-i.i.d. outcomes (e.g., temporal dependence). Release a reference implementation (e.g., in modern PPLs like Stan/Pyro/TensorFlow Probability) to facilitate adoption and benchmarking against contemporary Bayesian optimization and active learning baselines.",1805.09964v1,https://arxiv.org/pdf/1805.09964v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:35:27Z
TRUE,Sequential/adaptive|Computer experiment|Other,Prediction|Cost reduction|Other,Other,"Variable/General (examples: 2 inputs and 4 inputs; conductance-scaling factors in [0,1])",Healthcare/medical,Simulation study|Other,TRUE,MATLAB|C/C++|Other,Public repository (GitHub/GitLab),https://github.com/sanmitraghosh/ApPredict|http://www.gaussianprocess.org/gpml/code/matlab/doc/,"The paper proposes a two-step Gaussian-process (GP) emulation strategy for deterministic simulators with discontinuous response surfaces caused by bifurcations, using cardiac electrophysiology as the application. A GP classifier (one-vs-rest, multi-class) first detects and segments the input space into regions (no-depolarisation, valid action potential, no-repolarisation), and a GP regressor then emulates the biomarker surface (APD90) only within the valid region to avoid smoothing across discontinuities. Both steps use sequential/adaptive sampling (active learning): a new classifier ‘certainty metric’ guides boundary-focused sampling (via particle swarm optimisation), and entropy/conditional entropy guides regression sampling among candidate points filtered by the classifier. Performance is evaluated on 2D and 4D conductance-scaling input spaces using large simulated train/test sets (up to 100,000 points) and sparse GP approximations (FITC) to reduce computation. In a quinidine drug-block uncertainty propagation case study, the emulator reproduces APD90 distributions while greatly reducing the need for expensive ODE simulations and improving accuracy over a lookup-table interpolator.","Drug block inputs are conductance scalings $R_j\in[0,1]$ (from a Hill curve), and the emulator models APD90 as $y=f(\mathbf R)$. The boundary detector uses one-vs-rest GP classification with class probabilities $\pi_k(\mathbf R_*)$ and assigns $k_* = \arg\max_k \pi_k(\mathbf R_*)$; active learning targets low certainty $c(\mathbf R_*)=\max(\pi_k)-\max(\pi_{k^-})$. The surface GP regression uses posterior mean/variance (standard GP formulas) and chooses new points by maximizing conditional entropy $H(f(\mathbf R_*)|\mathbf y)=\tfrac12\log(\mathrm{Var}(f(\mathbf R_*)))+\mathrm{const}$ among candidate points. Sparse GP uses the FITC covariance approximation $\mathbf K\approx \mathbf Q + \mathrm{diag}(\mathbf K-\mathbf Q)$ with inducing points.","For a 4-input problem with a 100,000-point test set, the actively trained two-step emulator (training size 5000) achieved $E_{\text{boundary}}=1.577\%$ misclassification and surface MAE $E_{\text{surface}}=2.8742$ ms, versus a lookup-table interpolator trained on 5000 points with $E_{\text{boundary}}=13.967\%$ and $E_{\text{surface}}=17.9525$ ms. Two-step GP training took 5.5085 hours and prediction on 100,000 points took 68.898 s; the interpolator trained in 2.4320 hours and predicted in 1.8951 s, while full ODE simulation of the 100,000 test points took 44.2 hours. Active learning reduced errors faster than random sampling for both the boundary classifier and the surface regressor (learning curves shown for 2D and 4D cases). In the quinidine high-dose case study, the emulator closely matched simulator APD90 distributions; misclassification increased near the discontinuity (71/2000 points misclassified in that setup).",None stated.,"The work frames DOE as sequential design for emulator training but does not compare against common space-filling computer-experiment designs (e.g., Latin hypercube, maximin) or optimal design baselines for discontinuous surfaces, so the benefit over strong non-adaptive designs is not fully quantified. The active-learning rules (certainty metric, entropy maximization, PSO settings) introduce several tuning choices (e.g., threshold $\theta=0.5$, swarm size) whose robustness across models/dimensions is not systematically analyzed. The classifier/regressor are trained and assessed largely on simulator-generated datasets from the same model family; broader validation on other discontinuous simulators or real experimental data is limited. The approach assumes deterministic simulation with negligible noise; performance under stochastic simulators or numerical instability near bifurcations is not deeply explored.","The authors suggest extending the emulator beyond APD90 to other action-potential biomarkers, either via multiple separate GPs or a multi-output GP that captures correlations among biomarkers. They also discuss using the classifier certainty metric to gate when to run full simulations near discontinuities and potentially add those new simulation points back into the GP training sets (adaptive train-use-refine). They note that faster but less accurate GP-classifier approximations (e.g., Laplace) exist and imply continued investigation of accuracy–speed tradeoffs.","A natural extension is to benchmark the proposed adaptive sampling against established computer-experiment designs (space-filling LHS/maximin, treed GP design, Bayesian optimization criteria) under controlled discontinuity scenarios and higher dimensions. Incorporating explicit cost-aware or batch/sequential design strategies could better match parallel simulation settings (e.g., selecting multiple diverse points per iteration with theoretical guarantees). Developing a self-starting, noise-robust version (e.g., accounting for solver failure/instability as censored outcomes) would strengthen applicability near bifurcation boundaries. Providing a reusable software package (e.g., Python/R) with standardized interfaces to simulators and design criteria would improve practical adoption and reproducibility.",1805.10020v1,https://arxiv.org/pdf/1805.10020v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:36:12Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Other,Other,"Variable/General (M uncertain regulatory relations; examples include M = 2,3,5,7; mammalian network cases M = 2–6)",Healthcare/medical|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops sequential experimental design methods for reducing model uncertainty in gene regulatory networks (GRNs) in a way that is directly tied to a translational objective: optimal structural intervention to minimize steady-state probability mass of undesirable (e.g., cancerous) states. It uses the Mean Objective Cost of Uncertainty (MOCU) as the design criterion and introduces a finite-horizon dynamic programming (DP) approach (DP-MOCU) that plans over a specified horizon of experiments, contrasting it with a one-step greedy policy (Greedy-MOCU). As baselines, the paper also formulates greedy and DP strategies under entropy reduction (Greedy-Entropy and DP-Entropy), showing that entropy reduction does not necessarily translate into better intervention outcomes. Performance is evaluated via simulations on synthetic Boolean networks with perturbation (BNps) and on a mammalian cell-cycle Boolean network, demonstrating that MOCU-based strategies substantially outperform entropy-based strategies in reducing intervention cost, and that DP-MOCU generally improves on Greedy-MOCU when the experiment budget/horizon is fixed.","Uncertainty is represented by a belief vector over M unknown regulatory relations and induces posterior model probabilities $p^b_j$ over $2^M$ network models (Eq. 7). MOCU is defined as $M_\Psi(\Theta\mid b)=\mathbb{E}_{\Theta\mid b}[\xi_\theta(\psi^{\mathrm{IBR}}_{\theta\mid b})-\xi_\theta(\psi_\theta)]$ (Eq. 5), where the intrinsically Bayesian robust intervention is $\psi^{\mathrm{IBR}}_{\theta\mid b}=\arg\min_{\psi\in\Psi} \mathbb{E}_b[\xi_\theta(\psi)]$ (Eq. 6). Greedy-MOCU selects the next experiment by minimizing expected next-step MOCU (Eq. 8), while DP-MOCU solves a finite-horizon MDP with Bellman recursion $J_k(b)=\min_i \sum_{b'}\mathrm{Tr}_{bb'}(T_i)\big(g_k(b,b',T_i)+J_{k+1}(b')\big)$ (Eq. 10) using immediate cost $g_k=M_\Psi(\Theta\mid b')-M_\Psi(\Theta\mid b)$ (Eq. 9–10).","In synthetic BNp experiments (100 random 6-gene networks; perturbation probability $p=0.001$; priors drawn from symmetric Dirichlet with $\phi\in\{0.1,1,10\}$), MOCU-based strategies achieve substantially larger intervention gains than entropy-based strategies across multiple settings (e.g., $M\in\{2,3,5,7\}$ and varying horizons $N$), especially when the prior is close to uniform (high-entropy, e.g., $\phi=10$) where entropy-based selection behaves close to random. DP-MOCU consistently outperforms Greedy-MOCU in terms of end-of-horizon intervention cost when $1<N<M$, reflecting benefits of planning over the full horizon; Greedy-MOCU and DP-MOCU coincide when $N=1$. On the mammalian cell-cycle network (10 genes) with randomly selected unknown regulations ($M=2$ to $6$; $\phi=1$), DP-MOCU and Greedy-MOCU again yield the highest average intervention gain, while Greedy-Entropy and DP-Entropy perform poorly. A complexity table reports rapidly growing runtimes for MOCU-based methods (e.g., for $N=3$, Greedy-MOCU with $n=12$ takes ~32933s at $M=4$, ~65208s at $M=5$, ~127397s at $M=6$; DP-MOCU is slightly higher).","The authors explicitly note computational complexity as a key limitation: the running time of MOCU-based strategies grows exponentially with the number of genes and also increases with the number of unknown regulations, making scalability an issue. They point to prior work using network-reduction schemes to reduce computation at the expense of optimality (suboptimal experimental design), though still better than random selection. They also state DP-MOCU can be disadvantageous if one wants to stop early based on a threshold (unknown/variable number of experiments), because DP-MOCU is tuned to a fixed horizon.","The experimental design formulation assumes (in the main setup) that each experiment $T_i$ deterministically reveals the true value of a single uncertain regulation, which can be optimistic in wet-lab settings; while the paper mentions noisy outcomes are possible, the main DP/greedy comparisons appear centered on the perfect-information case. The method relies on a specified prior over $2^M$ network models and an independence structure to compute posteriors (Eq. 7), which may be misspecified in real GRN uncertainty. Empirical validation is primarily via simulations and a benchmark Boolean model of the mammalian cell cycle rather than demonstrations on real experimental intervention data. Practical guidance on selecting horizon $N$ and handling experiment costs/constraints beyond count (e.g., varying costs, feasibility, batch experiments) is limited.","The paper suggests that dynamic programming can be applied to other MOCU-based experimental design problems beyond GRNs, citing applications such as optimally performing materials development and optimal signal filtering, implying extension of DP-MOCU-style planning to those domains. It also highlights that network reduction schemes can address computational complexity (referencing prior work) as a way to enable practical use at the cost of optimality.","Extending DP-MOCU to explicitly handle experimental error (stochastic/incorrect experiment outcomes) within the DP formulation would improve realism and align with the briefly mentioned noisy experiment setting. Developing scalable approximations (e.g., point-based/value-function approximation, Monte Carlo tree search, or submodular/near-submodular guarantees) could mitigate the $3^{2M}$- and $2^M2^{3n}$-type complexity barriers for larger networks. Incorporating heterogeneous experiment costs and constraints (economic or budgeted design) and allowing batch/parallel experiments would broaden applicability. Providing an open-source implementation and benchmarking on real perturbation/knockout datasets would strengthen reproducibility and external validity.",1805.12253v1,https://arxiv.org/pdf/1805.12253v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:37:05Z
TRUE,Other|Sequential/adaptive,Parameter estimation|Model discrimination|Cost reduction|Other,Not applicable,"Variable/General (audience split ratio s, reach r, expected lift/effect size; extended to two-cell multi-cell settings)",Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/liuchbryan/fb_lift_study_design,"The paper develops statistical power and minimum sample size calculations tailored to Facebook lift studies, which differ from standard A/B tests due to control-group scaling and an unreached audience within the treated group. It derives the distribution of the lift test statistic under Poisson assumptions for conversions, and uses this distribution to compute critical values, power, and required sample sizes for one-tailed tests. The framework is generalized to multi-cell lift studies (e.g., comparing two campaigns/cells with potentially different audience compositions) by defining a difference-in-lift statistic and estimating its distribution via simulation to obtain significance, power, and sample size. Empirical checks (including Kolmogorov–Smirnov tests) show the simulated approach matches the derived distribution and is computationally faster for percentile/critical-value calculation. Practical guidance is provided in terms of practitioner-facing quantities (expected lift, reach percentage, and test/control split), with code released publicly.","Lift is defined as $L=(C_T-C_S)/R_S$, where scaled control conversions are $C_S=sC_C$ with $s=N_T/N_C$, and reached conversions in scaled control satisfy $R_S=rC_S$ (reach $r=N_{TR}/N_T$). This yields a computable form from reported quantities: $L=(C_T-sC_C)/(sC_C-C_T+R_T)$. Under Poisson modeling ($C_T\sim\text{Pois}(\lambda_T)$, $C_C\sim\text{Pois}(\lambda_C)$), power uses a critical value $c$ from $F_L(c\mid E(L)=0)=0.95$ and then $1-\beta=P(L>c\mid E(L)=L_m)=1-F_L(c\mid H_1)$. For two-cell studies, the test statistic is $D=L_B-L_A$, with $c$ set by $F_D(c\mid H_0)=1-\alpha$ and power $1-\beta=1-F_D(c\mid E(D)=D_m)$, estimated via simulation.","Using simulations (validated against the derived distribution via 500 K–S tests), the authors report the simulated method is over 30× faster for obtaining the 95th-percentile critical value than numerically inverting the analytic CMF. Minimum sample sizes (power 80%, reach 100%, 50:50 split, conversion rate assumption 5% for translating conversions to audience) for single-cell lifts require control conversions $C_C$ of 1,352 (10% lift), 5,107 (5%), 31,571 (2%), and 124,459 (1%). For a two-cell multi-cell lift study (with cell A lift taken as 5%), the required control conversions per cell increase to 2,745 (10% difference), 10,754 (5%), 67,453 (2%), and 264,745 (1%), implying over 4× total audience vs single-cell due to four groups (A/B × test/control). In an illustrative comparison holding total audience fixed, multi-cell power (about 78% at $D_m=5%$) is meaningfully lower than single-cell power (near 100% at $L_m=5%$).","They note the distribution of $C_T/R_S$ is not well defined when $R_S=0$ (division by zero) and proceed by approximation, arguing the issue is negligible when expected conversions are sufficiently large (e.g., $\lambda_C\gtrsim 30$); they mention a zero-truncated Poisson alternative would introduce complications. They also assume conversions can be modeled as a Poisson process, while suggesting the simulation framework can be adapted by swapping in other base distributions for non-Poisson metrics. For multi-cell studies, when cell B has not been tested previously they assume $C_{C,B}=C_{C,A}$ due to lack of a better estimate.","The approach depends on independence and stationarity assumptions (Poisson counts, no interference/contamination, stable conversion rates) that may be violated in ad platforms due to time-varying delivery, auctions, and user-level correlation; this could affect calibration of power and sample size. Reach $r$ is treated as an input/estimate, but in practice it is endogenous to budget, bidding, and auction dynamics, so misspecification of $r$ can materially bias power planning. The multi-cell framework is primarily described for two cells and equal splits; extensions to many cells and unequal allocations are not fully worked out, and variance inflation could be more complex. The method provides planning metrics but limited operational guidance for Phase I estimation of needed parameters (e.g., robust estimation of $\lambda_C$ under seasonality) beyond using prior results or a pre-study.",They state the simulation framework can still be applied to metrics that are not well described by a Poisson process by swapping in different base distributions for the underlying random variables. No other explicit future research directions are stated.,"Extend the framework to handle autocorrelation/seasonality and overdispersion (e.g., negative binomial or hierarchical Poisson models) common in conversion data, and study robustness of power under these deviations. Develop allocation/optimal-design guidance for multi-cell experiments (more than two cells, unequal splits) to minimize variance given budget/reach constraints. Provide self-starting or adaptive (sequential) planning procedures that update $\lambda_C$, $r$, and effect-size priors during the experiment while controlling error rates. Validate the methodology on real lift-study outcomes across multiple advertisers and verticals, and package the tooling for easier practitioner use (e.g., documented library with templates for common lift setups).",1806.02588v2,https://arxiv.org/pdf/1806.02588v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:37:49Z
TRUE,Computer experiment|Other,Prediction|Parameter estimation|Other,Space-filling|Other,7 simulation/model parameters (L=7) for sensitivity/sensor-placement study; 5 sampling points per parameter (N=5) with r=200 repeats (rN=1000 simulations).,Environmental monitoring|Other,Simulation study|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/openwfm/design|https://github.com/openwfm,"The paper uses coupled fire–atmosphere simulations (WRF-SFIRE / WRF-SFIRE-CHEM) to support the experimental design of prescribed-burn field campaigns (FASMEE), focusing on recommendations for burn size/duration and sensor placement. It develops a statistical method to select “typical” historical burn days via Mahalanobis distance in a transformed multivariate weather-state space, and uses those days to drive simulation ensembles. For sensor placement and parameter-identification support, it performs a global sensitivity/variance analysis using repeated Latin Hypercube Sampling (rLHS) combined with Sobol-style variance decomposition (variance of conditional expectation / sensitivity indices). The approach maps where predicted observables (e.g., vertical velocity, smoke concentration, plume-top height) exhibit the largest variance and strongest sensitivity to key model parameters, guiding where limited sensors should be deployed. MATLAB implementations for selecting typical days and for rLHS/validation are referenced as supplementary material, and code for the experimental-design components is provided via GitHub; WRF-SFIRE/WRFx tooling is also linked.","Typical-day selection is based on Mahalanobis distance of a weather-state vector $x_t$ from the sample mean: $\langle x\rangle = \frac{1}{N}\sum_{t=1}^N x_t$, covariance $C=\langle (x-\langle x\rangle)(x-\langle x\rangle)^T\rangle$, and distance $\|\delta_t\|=\|C^{-1/2}(x_t-\langle x\rangle)\|=\sqrt{(x_t-\langle x\rangle)^T C^{-1}(x_t-\langle x\rangle)}$. Sensor-placement sensitivity uses rLHS and variance decomposition: total output variance $\mathrm{var}(Y)$, variance of conditional expectation $\mathrm{var}(Y_i)$ for parameter $X_i$, and sensitivity index $\mathrm{eff}(Y_i)=\mathrm{var}(Y_i)/\mathrm{var}(Y)$. For linear models $Y=\sum_{i=1}^L a_i X_i$ with independent unit-variance inputs, the method yields $\mathrm{var}(Y_i)\to a_i^2$ as $r,N\to\infty$.","The sensor-placement study for the Fishlake case uses $L=7$ parameters, $N=5$ LHS points each, and $r=200$ repeats for $rN=1000$ simulations; instability at ignition for extreme parameters was mitigated by running the first 30 minutes with baseline settings before switching parameters. Mapped variances identify localized maxima for vertical velocity variance (e.g., at 1200 m AGL) suggesting optimal regions for Doppler lidar/radar/sodar deployment; example local variance reaches about $3\,\mathrm{m}^2/\mathrm{s}^2$ (≈1.7 m/s SD). First-order sensitivity maps show the heat-flux multiplier can explain up to ~50% of variance in vertical velocity, smoke concentration, and plume-top height, while heat-extinction depth can contribute up to ~40%, with spatially different influence patterns (core/plume vs flanks/upwind). Simulation comparisons indicate burn duration should be at least a couple of hours for plume evolution; Fishlake standard max updrafts reach ~5 m/s (and ~10 m/s with doubled heat flux), and Fort Stewart ignition-pattern effects on updraft/plume height can persist for hours (time series begin to converge ~7 hours after ignition).","The authors stress that simulations driven by “typical days” are not deterministic forecasts of actual burn conditions; results should be treated as planning guidelines. They note limitations in WRF-SFIRE’s ability to realistically resolve wildfire plume dynamics and near-surface fire-induced winds remain to be assessed, including potential issues from simplified canopy-flow representation. They also report model sensitivity to grid resolution (not shown) implying local updrafts may be underestimated when the mesh is too coarse, so simulated vertical velocities should be treated as lower limits. They further note that parameterized fire-spread models crudely simplify ignition, making interactions between simultaneously ignited lines hard to capture.","The rLHS/Sobol-style analysis is performed primarily on one base scenario/site (Fishlake) and a limited set of parameters; results may not generalize to other fuel types, terrains, atmospheric regimes, or alternative physics parameterizations. The sensitivity analysis largely focuses on first-order effects via VCE/sensitivity indices, which may miss important interaction (higher-order) effects in strongly nonlinear coupled fire–atmosphere dynamics. Practical sensor-placement recommendations are derived from model-based variance/sensitivity surfaces; without field validation, these “optimal” locations may be biased by model structural error (e.g., plume rise, turbulence, heat-flux vertical distribution). The paper references MATLAB tools and repositories but does not specify a fully reproducible pipeline (exact versioning, input datasets, run scripts, and parameter files) to replicate the 1000-simulation study end-to-end.","They suggest considering alternative definitions of “typical” conditions for non-Gaussian weather states, such as sampling from frequently occupied multivariate bins rather than using closeness to the mean. They propose running timely simulations (potentially a truncated sensitivity study) immediately before burns using forecast data to inform observation deployment. For the rLHS approach, they note additional parameters can be incorporated; while per-repetition cost does not increase with parameter count, more repetitions may be needed for statistical convergence. They also imply further work on direct measurements to better constrain parameters like heat-extinction depth, potentially requiring tower-based vertical heat-flux attenuation observations.","Extend the sensor-placement DOE to include interaction effects (second-order Sobol indices) and quantify uncertainty in sensitivity maps due to finite $r,N$ and due to stochastic atmospheric variability. Develop robust or Bayesian experimental design criteria that explicitly trade off sensor cost, logistical constraints, and expected information gain about prioritized parameters (e.g., heat flux vs extinction depth). Validate the proposed placement strategy against real burn datasets (Phase I/II), comparing predicted high-variance/high-sensitivity regions to observed plume and wind variability. Broaden the design to account for autocorrelated meteorology and model structural uncertainty by using multi-model ensembles or perturbed-physics ensembles beyond the 7-parameter set.",1806.06460v2,https://arxiv.org/pdf/1806.06460v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:38:47Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Computer experiment|Other,Parameter estimation|Prediction|Robustness|Other,A-optimal|D-optimal|Other,"Variable/General (inflation factors per state entry; localization radii per state entry or per observation; e.g., Nstate=40 in experiments)",Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://bitbucket.org/a_attia/dates_adaptive,"The paper proposes a gradient-based optimal experimental design (OED) framework to adaptively tune covariance inflation factors and covariance localization radii in ensemble Kalman filter (EnKF) data assimilation. The tuning is cast as constrained optimization problems that minimize posterior uncertainty, primarily using the Bayesian A-optimality criterion (trace of the posterior covariance), with D-optimality (determinant) discussed as an alternative. Inflation (multiplicative, potentially state-dependent) and localization (state-space B-localization and observation-space R-localization variants) are optimized independently each assimilation cycle using analytic gradients and an SLSQP optimizer. The approach is demonstrated on synthetic data from a two-layer Lorenz-96 system (with a single-layer model used for forecasting to induce model error), showing that adaptive inflation/localization can track the truth and achieve RMSE comparable to empirically tuned fixed parameters. The method is positioned as reducing costly trial-and-error tuning and as applicable even with correlated background/observation errors and dense or sparse observation networks.","Inflation is posed as minimizing $\Psi_{\mathrm{Infl}}(\lambda)=\mathrm{Tr}(A^e(\lambda))$ with bounds $\lambda_i\in[\lambda_i^l,\lambda_i^u]$, where multiplicative inflation uses $B^e=D^{1/2}BD^{1/2}$ (or $B^e=\lambda B$ in the scalar case) and $A^e=( (B^e)^{-1}+H^TR^{-1}H)^{-1}$. Localization is posed as minimizing $\Psi_{\mathrm{Loc}}(L)=\mathrm{Tr}(A^b(L))$ with bounds on radii, where $B^b=B\circ C(L)$ (Hadamard product) using kernels such as Gaussian or Gaspari–Cohn, and $A^b=((B^b)^{-1}+H^TR^{-1}H)^{-1}$. Both objectives are differentiated to yield explicit gradients used in SLSQP each assimilation cycle, with optional $\ell_1$ regularization on $\lambda$ and on $L$.","In Lorenz-96 experiments (K=40 observed variables, ensemble size typically $N_{ens}=25$), a benchmark from grid-search tuning suggested best fixed parameters near inflation $\lambda=1.5$ and localization radius $l=0.5$ (reported benchmark average RMSE about 0.170 over the last third of the time window). Adaptive inflation with bounded $\lambda_i\in[1,1.5]$ achieved stable tracking of the truth for suitable regularization (e.g., $\alpha$ around 0.0015–0.0035), with performance comparable to the benchmark and substantially better than a free run. Adaptive localization performed well even with no regularization ($\gamma=0$), while larger $\gamma$ increased computational cost and could degrade accuracy. Observation-space formulations of the localization objective produced acceptable RMSE but higher optimization iteration counts due to issues such as negative objective values, requiring additional constraints.","The authors note that inflation and localization are optimized independently; applying both simultaneously may require reformulating a joint/weighted objective and is left for future work. They also highlight practical difficulties for localization, including the need for a decorrelation kernel (ideally symmetric positive definite) and the computational impracticality of applying localization directly to the full state-space covariance in large-scale settings. Additionally, they report that projecting the OED localization objective into observation space can lead to negative objective values and higher optimization cost, reducing feasibility without further methodological improvements.","Despite claiming general applicability, the demonstrated validation is on a single synthetic Lorenz-96 setup; broader benchmarks on realistic geophysical models and observation operators (including strongly nonlinear/non-Gaussian cases) are not shown. The approach relies on repeated constrained optimization (SLSQP) each assimilation cycle, which may be computationally heavy for operational high-dimensional DA unless further approximations (e.g., low-rank/iterative trace estimators, scalable gradients) are developed and tested. The method’s sensitivity to choices of bounds and regularization (especially for inflation) suggests additional tuning is still required, and robustness to model/observation error misspecification beyond the tested ranges is unclear. No packaged, documented implementation is presented in the paper itself (beyond linking to a repo), and reproducibility may depend on external DA infrastructure (DATeS) and exact configuration details.","They propose extending the framework to tune inflation and localization simultaneously via a reformulated/weighted objective combining posterior traces. They suggest further investigation of localization kernels, including enforcing/ensuring symmetry and positive definiteness, and studying implications for prior vs posterior covariance definiteness. They also mention enforcing smoothness of space-time varying parameters (via regularization or moving averages) or using probabilistic prior knowledge for the tuning parameters, and exploring nonlinear observation operators and a probabilistic version of the framework.","A scalable variant could replace explicit traces with stochastic trace estimators (e.g., Hutchinson) and use matrix-free linear solvers to reduce per-cycle cost in large systems. Extensions to handle serially correlated model error and time-correlated observation errors (beyond allowing general R in principle) could be evaluated empirically, including self-starting/online estimation of hyperparameters (regularization weights, bounds). A more thorough comparison against state-of-the-art adaptive inflation/localization methods (e.g., hierarchical/Bayesian inflation, ECO-RAP variants, machine-learning localization) on multiple benchmark models would clarify when OED adds value. Providing a stable open-source reference implementation (e.g., pip-installable) with reproducibility scripts would materially improve adoption.",1806.10655v2,https://arxiv.org/pdf/1806.10655v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:39:29Z
FALSE,NA,NA,Not applicable,Not specified,Other,Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper tests General Relativity by estimating the PPN parameter $\gamma$ from a single, specially scheduled geodetic VLBI session (AUA020, 1–2 May 2017) in which two strong radio sources were observed at very small Sun elongations (about 1.15°–2.6°). The session was deliberately scheduled to maximize the number of near-Sun observations (over 1000 usable group delays) while managing solar thermal noise via strong sources, large antennas, and high recording rate. The authors process the same session with two independent VLBI analysis packages (OCCAM and Calc/Solve) and compare results across different subsets of scans (each source alone vs both). They find uncertainties on $\gamma$ of order $10^{-4}$ (down to ~$9\times 10^{-5}$ after downweighting one station in OCCAM), comparable to or better than global analyses using thousands of standard sessions, and argue that purpose-designed single sessions can deliver highly competitive relativistic constraints. The work advances VLBI-based GR tests by demonstrating the performance gains achievable through targeted observing schedules at very small solar elongation and by discussing solar-corona and troposphere error contributions.","The core model is the differential gravitational (Shapiro) delay used in VLBI: $\tau_{\rm grav} = \frac{(1+\gamma)GM}{c^3}\,\ln\!\left(\frac{|\mathbf r_1|+\mathbf r_1\cdot\mathbf s}{|\mathbf r_2|+\mathbf r_2\cdot\mathbf s}\right)$, where $\mathbf r_1,\mathbf r_2$ are station geocentric position vectors and $\mathbf s$ is the unit vector to the radio source. The parameter $\gamma$ is estimated by least squares (two software implementations) using group-delay observations, with ionosphere and solar corona mitigated primarily through dual-frequency (S/X) combinations.","Single-session solutions yield $\gamma-1$ uncertainties between about $1\times10^{-4}$ and $4\times10^{-4}$ depending on software and scan subset; using the closer source (0235+164) gives smaller formal error than the farther one (0229+131). OCCAM with all stations and all scans reports $\gamma-1=0.56\times10^{-4}$ with $\sigma_\gamma=1.15\times10^{-4}$; after downweighting Sejong, OCCAM reports $\sigma_\gamma=0.94\times10^{-4}$ (i.e., ~$9\times10^{-5}$). Calc/Solve with all sources reports $\gamma-1=-0.22\times10^{-4}$ with $\sigma_\gamma=1.10\times10^{-4}$. A global solution over 6301 sessions yields $\gamma-1=(2.72\pm0.92)\times10^{-4}$, and removing AUA020 increases the uncertainty by about $5\times10^{-5}$, indicating a detectable contribution from the special session.","The authors note that several potential sources of error must be considered even though no systematics are detected within $10^{-4}$. They discuss that solar-corona effects are corrected to first order by dual-frequency combination and that more sophisticated coronal calibration (regional electron density/magnetic field models) is possible, but likely unnecessary given their estimated small magnitude of higher-order terms. They also emphasize practical observing limitations: the minimum observable solar elongation is constrained by rapidly increasing solar thermal noise and unpredictable coronal activity near the limb.","The study is centered on a single specially designed session, so robustness to different networks, seasons, solar conditions, and scheduling choices is only indirectly supported. The analysis depends on complex nuisance-parameter modeling (troposphere, clocks, gradients) and shows some discrepancy (e.g., an unexplained ~2.7$\sigma$ deviation in one Calc/Solve subset), suggesting sensitivity to estimation choices that is not fully resolved. No reproducible pipeline, shared schedules, or analysis scripts are provided, limiting independent verification and reuse by practitioners. The work is not a DOE-methodology contribution; “design” refers to observation scheduling rather than a formal experimental design framework with optimality criteria.","They propose pushing to smaller solar elongations (e.g., 0.5°–1.0°) by observing a much stronger source (3C 279) to potentially reduce uncertainty on $\gamma$ by about a factor of two from elongation alone. They also suggest a future experiment with a larger network (e.g., ~15 radio telescopes) at high recording rates to collect thousands of near-Sun observations and anticipate an overall uncertainty improvement on $\gamma$ by about a factor of ten, potentially challenging the Cassini bound.","A systematic optimization study of scheduling (scan patterns, source selection, baseline geometry, and elevation constraints) could quantify which design choices most improve $\gamma$ precision and robustness. Additional real-data replications across different solar-cycle phases and during more disturbed coronal conditions would better characterize sensitivity to coronal variability and confirm generalizability. Developing and releasing open-source scheduling and analysis workflows (including station downweighting strategies and diagnostics for subset anomalies) would improve reproducibility and adoption. Extending the approach to joint estimation of related relativistic parameters or to multivariate modeling that explicitly accounts for correlated errors (e.g., time-correlated troposphere and coronal noise) could further stabilize inference.",1806.11299v1,https://arxiv.org/pdf/1806.11299v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:40:14Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Model discrimination|Cost reduction|Other,Other,Variable/General (application uses 2 experimental design parameters: A and π),Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),http://github.com/shakty/optimal-design,"The paper develops a Bayesian optimal experimental design procedure to efficiently discriminate among competing generative behavioral models in strategic interaction settings. It maximizes expected information gain—operationalized via a directed Kullback–Leibler divergence objective over models’ likelihoods of experimental outcomes—and introduces two computational innovations to make the approach tractable: adaptive search over the design space using a Gaussian-process bandit algorithm (GPUCB-PE) and a parameter-sampling approach that approximates the required likelihood integrals by simulating likely datasets rather than enumerating all datasets. The method is demonstrated on the two-player repeated Stop–Go imperfect-information game, optimizing two design parameters (maximum payoff A and state probability π) and comparing designs proposed by the algorithm versus expert-selected designs. Empirically, the algorithm’s suggested design is more data-efficient for distinguishing models than the modal “wisdom of experts” design; adding a reinforcement-learning model shows it explains observed behavior better than equilibrium-based alternatives. The approach is positioned as general-purpose and applicable to dynamically optimizing online experiments.","The design objective is expected information gain measured by KL divergence between competing models’ predicted outcome distributions. The paper uses $D_{\mathrm{KL}}[P\|Q]=\sum_{x\in\mathcal{X}} p(x)\log\frac{p(x)}{q(x)}$ and an asymmetric, one-sided multi-model criterion $I(1;\theta)=\sum_{x\in\mathcal{X}} \ell_1(x;\theta)\log\left(\frac{(1-p_1)\ell_1(x;\theta)}{\sum_{i=2}^n p_i\,\ell_i(x;\theta)}\right)$, where $\theta$ denotes experimental design parameters, $\ell_i(x;\theta)$ is the likelihood of dataset $x$ under model $i$, and $p_i$ are model priors. Computationally, $I(\theta)$ is optimized over $\theta$ via Gaussian-process UCB pure-exploration search, and $\ell_i$ are approximated by sampling model parameters and simulating datasets to form empirical histograms used in the KL calculation.","The improved procedure (Parameter-Sampled GPUCB-PE) reconstructs an information surface consistent with brute-force/grid approaches while avoiding exhaustive enumeration of datasets (which is computationally infeasible at scale). In the Stop–Go game application, the highest-information region occurs near $\pi\approx 0.5$ and minimal payout parameter $A\approx 2.0$, while experts most commonly propose $\pi=0.5, A=6.0$ (a lower-information design). The authors report that sampling about 1,000 simulated datasets per evaluation is sufficient to recover the optimal design location, and they use $n_s=10{,}000$ for searches. In online experiments (five designs, 79 participants), the algorithmically chosen high-information designs yield clearer separation in model likelihood odds than low-information regions; when adding a Roth–Erev reinforcement learning model, it fits observed behavior best across all five experimental designs.",None stated.,"The optimality criterion is asymmetric (one-sided KL) and depends on the choice of a reference model and model priors, which may materially affect the recommended design; sensitivity analyses to priors/reference choice are not emphasized. The parameter-sampling approximation introduces Monte Carlo noise and potential bias (especially with uniform parameter sampling), and the stopping rule/convergence of the GP search could be affected by heteroskedastic noise in the estimated information values. The demonstrated application optimizes only two design parameters in a discrete bounded region; scalability and robustness in higher-dimensional, constrained, or mixed discrete/continuous design spaces is not empirically benchmarked beyond the presented case.","The authors suggest extending the approach toward automated online experimentation platforms that iteratively assign experimental parameters, run experiments, update model comparisons, and repeat in near real time. They note that applying the method to higher-dimensional information landscapes and more complex models would likely require informative priors over model parameters, and they indicate future work will explore such approaches. They also point to potential applications such as optimizing experiments on social networks where network topology is the design variable.","A useful extension would be a systematic sensitivity analysis of the recommended optimal design to model priors, alternative information criteria (e.g., symmetric/average KL, Jensen–Shannon), and the choice of model set (misspecification). Developing variance-reduction and uncertainty-quantification for the sampled information estimates (e.g., control variates, Bayesian Monte Carlo, credible intervals over $I(\theta)$) would improve reliability and stopping rules. Packaging the method into a maintained software library with reproducible pipelines (including GP-search hyperparameter choices and noise models) and benchmarking on additional behavioral and non-behavioral DOE problems (including higher-dimensional and constrained spaces) would strengthen generalizability.",1807.07024v3,https://arxiv.org/pdf/1807.07024v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:40:57Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (validated on 1D, 3D, 5D synthetic functions; 8 factors in steel wire drawing case study)",Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/piyushpandita92/bode,"The paper proposes a Bayesian optimal design of experiments (BODE) method to efficiently estimate a specific quantity of interest (QoI): the statistical expectation (mean) of a black-box physical response surface under an assumed input distribution. The design criterion is the expected information gain in the QoI, quantified as the expected Kullback–Leibler divergence between the prior and posterior distributions of the QoI induced by adding a hypothetical observation. Using a Gaussian process surrogate with an RBF kernel and Bayesian hyperparameter inference (MCMC), the authors derive a semi-analytic, tractable expression for the expected information gain that avoids costly nested Monte Carlo over hypothetical observations. The resulting sequential design is validated on multiple synthetic functions of increasing dimensionality and compared against uncertainty sampling, showing similar performance in low dimensions and faster convergence/reduced epistemic uncertainty in higher dimensions. The method is demonstrated on an 8-variable steel wire drawing (finite-element simulation) problem to estimate the expected frictional work per tonne, and the authors provide a Python implementation.","The QoI is the statistical expectation $Q[f]=\int_{\mathcal X} f(x)p(x)\,dx$. The next design maximizes expected information gain $x_{n+1}=\arg\max_{\tilde x} G(\tilde x)$, where $G(\tilde x)$ is the expected KL divergence between $p(Q\mid\theta,D_n)$ and $p(Q\mid\theta,D_n,\tilde x,\tilde y)$. With GP surrogates, both QoI distributions are Gaussian, yielding a closed-form KL (and an analytic integral over $\tilde y$) leading to $G(\tilde x;\theta)$ and a posterior-average approximation $G(\tilde x)\approx \frac1S\sum_{s=1}^S G(\tilde x;\theta^{(s)})$.","Across 1D, 3D, and 5D synthetic examples, the posterior over $Q[f]$ concentrates toward the analytically known true expectation as sequential samples are added, while the predictive uncertainty decreases over iterations. In comparisons, EKLD and uncertainty sampling behave similarly in 1D, but EKLD converges faster in higher dimensions (e.g., the 3D example converges in about 20 collected samples vs. about 30 for uncertainty sampling; in the 5D example EKLD shows convergence around 45 added samples vs. about 65 for uncertainty sampling, per the paper’s discussion). In the steel wire drawing case study (8 design variables), starting from 20 initial points and adding 80 samples, the EKLD-based strategy drives the estimated expected frictional work per tonne toward an LHS-based reference estimate $\approx 0.2694$ with steadily shrinking uncertainty, and it outperforms uncertainty sampling in convergence speed.","The authors note that a key weakness is the assumption of a stationary GP covariance (kernel), suggesting that non-stationary GPs would yield more locally adapted designs but are non-trivial to implement. They also state that selecting the number of initial design points before starting sequential DOE is largely ad hoc in the literature and remains an open problem. Additionally, they mention that estimating EKLD via MCMC posterior averaging can be numerically noisy if too few/short MCMC chains are used, complicating optimization of the criterion.","The approach assumes the input distribution $p(x)$ is known (they take it uniform on a hypercube) and that GP modeling assumptions (e.g., smoothness/RBF kernel, near-deterministic noise fixed to $10^{-6}$) are adequate; performance may degrade under model misspecification, heteroscedastic noise, discontinuities, or strong nonstationarity. The acquisition optimization relies on an additional surrogate (GP over $G(\tilde x)$) and Bayesian global optimization settings (e.g., finite iterations, LHS candidates), which may affect robustness and scalability in higher dimensions. Empirical validation is limited to a small set of benchmarks and one manufacturing simulation case; broader comparisons to alternative information-based criteria and modern batch/parallel BOED methods are not explored.","They propose extending the methodology to non-stationary GP covariance functions to obtain more locally adapted sequential designs. They also highlight the need for more research on principled selection of the initial DOE size before starting sequential design. Finally, they state that the framework can be extended to infer more general statistics or QoIs (beyond the mean) of noisy black-box functions, which they plan to address in future work.","Extending the method to batch/parallel BOED (selecting multiple points per iteration) would improve practicality for modern parallel simulators/experiments. Developing self-starting/robust variants that handle unknown or misspecified input distributions, correlated inputs, and autocorrelated simulation outputs would broaden applicability. Providing open-source, reproducible experiment scripts and standardized benchmarks (including higher-dimensional cases) would strengthen evidence, as would integrating scalable GP approximations (sparse/variational, random features) to push beyond ~10 dimensions.",1807.09979v3,https://arxiv.org/pdf/1807.09979v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:41:42Z
TRUE,Optimal design|Response surface,Parameter estimation|Prediction|Cost reduction,E-optimal,"Variable/General (example: 2 factors, quadratic regression with 5 parameters; discretized grid 161×161 with constraint leaving 14,701 points)",Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper proposes a deletion (pruning) method for finite design spaces to remove design points that cannot support any E-optimal approximate design for linear regression-type models with uncorrelated observations. Using the E-optimality equivalence theorem and subgradient structure, it derives a necessary condition (via a computable function g_h(x,y)) that any support point of an E-optimal design must satisfy; points violating it can be safely deleted. The method computes a tight upper bound h on the smallest eigenvalue via a linear program and then performs per-point 1D convex minimizations in y to test deletability, enabling substantial reduction of large E-optimal SDP instances. An example in quadratic response-surface regression on a constrained discretized square shows that pruning thousands of points makes semidefinite programming feasible where it otherwise runs out of memory. The contribution complements earlier support-delimitation/deletion results for smoother Kiefer Φ_p criteria (notably D- and A-optimality) by addressing the non-differentiable, non-strictly concave E-criterion.","Approximate-design information matrix: $M(\xi)=\sum_{x\in\mathcal X}\xi(x)H(x)$ with (linear regression) $H(x)=f(x)f(x)^\top$. For chosen eigenvectors $v_i$ of $M$ and weights $\alpha_i\ge0,\ \sum_i\alpha_i=1$, define $Z=\sum_{i=1}^s \alpha_i v_iv_i^\top$ and $h=\max_{x\in\mathcal X}\mathrm{tr}(H(x)Z)$. If $h>\lambda_1(M)$ then any E-optimal support point must satisfy $g_h(x,y)=\sum_{i=1}^m \frac{u_i^\top H(x)u_i}{(\lambda_i(M)-h)y+\lambda_1(M)}\ge 1$ for all $y\in[0,\lambda_1/(h-\lambda_1))$, where $u_i$ are eigenvectors of $M$; in the regression case $g_h(x,y)=\sum_i \frac{(u_i^\top f(x))^2}{(\lambda_i(M)-h)y+\lambda_1}$. The bound $h$ is minimized over $\alpha$ by an LP: minimize $h$ s.t. $h\ge\sum_i\alpha_i v_i^\top H(x)v_i\ \forall x$, $\sum_i\alpha_i=1$, $\alpha\ge0$.","In Example 1 (quadratic regression on $[-1,1]^2$ with constraint $x_2\le ax_1+b$, grid 161×161), the feasible design space has 14,701 points and a direct SDP approach runs out of memory. Using an E-optimal design computed on a random 8,000-point subset, the method deletes 11,197 points, leaving 3,504 points on which the SDP solver easily computes an E-optimal design that is certified optimal for the original space. Using a coarser-grid E-optimal design (3,717 points) as the starting design, it deletes 12,895 points, leaving 1,806 points and making the final SDP solve very fast. Reported runtimes (MATLAB/SeDuMi via CVX on 4GB RAM) are minutes for pruning and seconds for the reduced SDP, versus failure on the full problem; the Elfving-set deletion took >2 hours and deleted none in this example.","The paper notes computational difficulty of E-optimal design due to non-differentiability and lack of strict concavity, and that standard SDP methods are limited to relatively small design spaces due to memory (working with $n\times n$ matrices). It also states that the amount of points removed depends on the model, and the size increase enabled by pruning may be small in some settings (e.g., when adding an interaction term fewer points are removed, sometimes not enough to make SDP feasible on the given hardware).","The method is presented for approximate designs on finite (or finitely supported) design spaces and relies on having a nonsingular starting design $\xi$; in practice, obtaining such a design and its eigen-decomposition may be nontrivial for very high-dimensional parameterizations. The pruning effectiveness depends on the quality of the provisional design and on choices for eigenvectors/weights used to form $Z$; suboptimal choices can reduce deletions, and the paper does not provide a principled strategy beyond heuristic recommendations when $\lambda_1$ has multiplicity >1. Empirical evaluation is limited to a small number of illustrative examples and does not benchmark against alternative modern E-optimal algorithms or pruning/screening heuristics across varied models/constraints. Implementation details for numerical stability (e.g., handling near-singular $M$, grid scaling, tolerance for the $g_h(x,y)\ge1$ test) are not discussed, which can matter in large SDP workflows.",None stated.,"Develop adaptive/iterative pruning strategies that update $\xi$ and re-prune during optimization, with theoretical guarantees on convergence and numerical tolerances. Extend the approach to correlated observations (generalized least squares), generalized linear/nonlinear models (local E-optimality), and constrained/blocked designs where $H(x)$ depends on nuisance parameters or random effects. Provide open-source implementations (e.g., MATLAB/Python) integrated with SDP and cutting-plane solvers, including robust defaults for multiplicity of $\lambda_1$, near-singular designs, and large-scale sparse $H(x)$. Conduct broader computational studies comparing runtime and memory savings versus competing methods (cutting-plane, bundle/nonsmooth optimization, specialized SDP formulations) across higher-dimensional response-surface and mixture regions.",1808.00731v1,https://arxiv.org/pdf/1808.00731v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:42:20Z
FALSE,NA,NA,Not applicable,"Not specified (task conditions include 0-back, 2-back, Fixation, Instruction; plus nuisance regressors such as motion)",Healthcare/medical|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Public repository (GitHub/GitLab)|Personal website,https://osf.io/4rvbz/|https://github.com/https://github.com/geebioso/VDGLM,"The paper proposes the Variance Design General Linear Model (VDGLM) for fMRI, extending the standard GLM by explicitly modeling both the mean and the variance of the BOLD time series as functions of an experimental design matrix. The method enables inference about variance (BOLD variability) effects while controlling for mean effects, and allows multiple task conditions and noise regressors to enter the variance model. The approach is demonstrated on Human Connectome Project working-memory (0-back/2-back) data, showing widespread decreases in BOLD variance during task engagement relative to fixation, with variance effects largely orthogonal to mean activation effects. Model comparison using 10-fold cross-validated out-of-sample log-likelihood indicates substantial preference for VDGLM over GLM in real data, while much lower preference appears in GLM-simulated data with matched autocorrelation, supporting that gains are not merely due to fitting autocorrelation. The authors implement estimation via constrained trust-region optimization and emphasize that VDGLM can be inserted into standard fMRI pipelines with minimal changes beyond model formulation and estimation.","The core VDGLM is a heteroskedastic Gaussian linear model: $y = X_m\beta + \eta$, with $\eta \sim \mathcal{N}(0,\mathrm{diag}(X_v v))$ and constraint $X_v v > 0$ (elementwise over time). A simple single-condition illustration is $y \sim \mathcal{N}(\beta_0 + x^T\beta_1,\,(v_0 + x^T v_1)I)$, where $v_1$ captures design-linked variance changes; the standard GLM is recovered by setting $v_1=0$ (constant variance). Prewhitening is performed using an AR(2) model on GLM residuals: $r_t = \phi_1 r_{t-1} + \phi_2 r_{t-2} + \epsilon$.","In the HCP working-memory application (875 subjects; 333 ROIs), both 2-back and 0-back engagement show predominantly negative variance effect sizes (reduced BOLD variance) relative to fixation across much of the brain. Mean and variance effects are largely uncorrelated for the 2-back–Fixation and 0-back–Fixation contrasts (reported $R^2 \approx -0.00294$ and $0.132$), indicating spatial orthogonality, while a moderate negative relationship appears for 2-back–0-back ($R^2=0.31$). Cross-validated model comparison shows 41% of subject/ROI series have higher out-of-sample log-likelihood under VDGLM than GLM in real data, versus 7% in GLM-simulated data with added autocorrelation; subject-level VDGLM preference ranges from 14% to 73% of ROIs. Computationally, fitting and comparison for 875 subjects reportedly took about half a day on a high-performance cluster using in-house MATLAB code.","The authors note that group-level inference for variance parameters is challenging: Wald/asymptotic tests were ill-behaved for some subjects due to high-condition-number matrix inversions, and they leave development of alternative statistics and a fully Bayesian framework to future work. They also state the VDGLM application was ROI-based to reduce computational load and improve signal reliability, with voxel-wise application left for future work. They acknowledge not correcting for physiological noise sources (heart rate/respiration), although they argue these are unlikely to explain task-linked variance decreases and may otherwise attenuate observed variance effects.","This is not a DOE paper; “experimental design” here refers to fMRI task design matrices rather than design-of-experiments methodology, and no design construction/optimization is studied. The positivity constraint on $X_v v$ can be restrictive and may complicate estimation/interpretation when variance regressors are highly collinear or when designs include many correlated nuisance regressors; the practical sensitivity to design-matrix conditioning could be more thoroughly benchmarked. The evaluation largely relies on one major dataset/task paradigm; broader validation across event-related designs, different TRs, and different preprocessing choices (including alternative prewhitening strategies) would strengthen generalizability. The referenced GitHub URL appears malformed in the citation (duplicated domain), which may hinder reproducibility unless corrected.","They propose future work on developing better-behaved inference procedures for variance effects (beyond the ill-conditioned Hessian-based approximate t-tests they tried) and on building a fully Bayesian VDGLM framework for more robust inference and principled model comparison. They also identify exploring variance transformations (e.g., log-variance) as a potential improvement, while noting it would change interpretability from additive to multiplicative effects. Finally, they explicitly state that extending VDGLM from ROI analyses to voxel-wise analyses is left to future work.","Provide a validated, user-friendly software release (e.g., a MATLAB toolbox and/or Python/R implementation) with end-to-end examples and corrected repository links to support adoption. Extend the framework to handle temporal dependence directly within the likelihood (rather than prewhitening-first), and assess robustness under misspecified autocorrelation and heteroskedasticity. Develop self-starting or regularized estimation for high-dimensional variance designs (many nuisance regressors) to mitigate conditioning problems, and evaluate sensitivity to design collinearity. Test VDGLM in additional fMRI contexts (resting-state with task regressors, pharmacological designs, clinical cohorts) and compare against alternative heteroskedastic GLM formulations used in neuroimaging.",1808.02157v1,https://arxiv.org/pdf/1808.02157v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:43:06Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Robustness|Cost reduction|Other,Other,Variable/General (K covariates; simulations vary K=10–90; N=100 with N_T=N_C=50),Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Not provided,NA,"The paper proposes ridge rerandomization, a treatment–control experimental design strategy that repeatedly randomizes until a covariate-balance criterion is met, replacing the usual Mahalanobis distance with a ridge-modified Mahalanobis distance $M_\lambda=(\bar x_T-\bar x_C)^\top(\Sigma+\lambda I)^{-1}(\bar x_T-\bar x_C)$. The ridge modification inflates covariance eigenvalues to handle collinearity or near-singularity and implicitly prioritizes balance along leading principal components without requiring user-defined covariate tiers. The authors prove unbiasedness of the difference-in-means estimator under the symmetric acceptance rule and derive the conditional covariance of covariate mean differences under Normal approximations, showing variance reduction relative to complete randomization. They provide guidance for choosing the acceptance threshold via a fixed acceptance probability $p_a$ and selecting $\lambda$ to improve average variance reduction while limiting distortion of correlation structure. Simulations (varying dimension and collinearity) show ridge rerandomization typically improves covariate balance and can yield more precise treatment-effect estimates and randomization-based confidence intervals in high-dimensional/high-collinearity settings compared to standard rerandomization.","The causal estimand is the average treatment effect $\tau=\frac{1}{N}\sum_{i=1}^N\{Y_i(1)-Y_i(0)\}$ with estimator $\hat\tau=\bar y_T-\bar y_C$. Standard rerandomization uses Mahalanobis balance $M=(\bar x_T-\bar x_C)^\top\Sigma^{-1}(\bar x_T-\bar x_C)$; ridge rerandomization replaces this with $M_\lambda=(\bar x_T-\bar x_C)^\top(\Sigma+\lambda I_K)^{-1}(\bar x_T-\bar x_C)$ and accepts assignments when $M_\lambda\le a_\lambda$ calibrated to an acceptance probability $P(M_\lambda\le a_\lambda\mid x)=p_a$. Under $(\bar x_T-\bar x_C)\mid x\sim N(0,\Sigma)$, $M_\lambda\mid x\sim\sum_{j=1}^K \frac{\lambda_j}{\lambda_j+\lambda}Z_j^2$ (weighted chi-squares), and the conditional covariance takes the eigen-basis form $\operatorname{Cov}(\bar x_T-\bar x_C\mid x,M_\lambda\le a_\lambda)=\Gamma\operatorname{Diag}(\lambda_k d_{k,\lambda})\Gamma^\top$.","Theory: ridge rerandomization preserves unbiasedness ($E[\hat\tau\mid x,M_\lambda\le a_\lambda]=\tau$ for $N_T=N_C$) and reduces each covariate mean-difference variance relative to complete randomization (all reduction factors $v_{k,\lambda}\in(0,1)$). Compared to standard Mahalanobis rerandomization, variance reduction across principal components is unequal, typically emphasizing leading PCs (small $d_{1,\lambda}$ relative to later components), which is advantageous under collinearity. Simulations with $N=100$, $p_a=0.1$, $K\in\{10,\ldots,90\}$ and equicorrelation $\rho\in\{0,0.1,\ldots,0.9\}$ show ridge rerandomization yields larger average variance reduction than standard rerandomization, with the advantage growing as $K$ and/or $\rho$ increase. In linear-outcome settings (e.g., $\beta=\mathbf{1}$), ridge rerandomization typically lowers relative MSE of $\hat\tau$ and shortens randomization-based confidence intervals versus standard rerandomization, while in constructed worst-case $\beta$ directions (aligned with last PCs) it can be slightly worse for small $K$.","The authors note that existing asymptotic inference theory developed for standard Mahalanobis rerandomization (e.g., Li et al. 2018) does not directly apply to ridge rerandomization because key distributional properties differ. They state that deriving asymptotic results (and related Bayesian inference) for ridge rerandomized experiments is beyond the scope of the paper. They also acknowledge that whether ridge rerandomization outperforms standard rerandomization for treatment-effect estimation depends on the (typically unknown pre-treatment) relationship between covariates and outcomes (the coefficient vector $\beta$).","The method relies on choosing tuning parameters ($p_a$, $\lambda$) and on Normal/CLT-style approximations for calibrating $a_\lambda$; performance may degrade with small samples, heavy tails, or strong dependence among units. The proposed $\lambda$-selection procedure targets average variance reduction of covariate mean differences, which may not align with the estimand-relevant prognostic strength of covariates and could be suboptimal if outcome models are nonlinear or include interactions not captured by mean balance. Computational cost can still be high when $p_a$ is small because rerandomization requires repeated random draws until acceptance, and the paper does not benchmark runtime or scalability for very large $N$ or constrained randomization settings. Extensions to multi-arm trials, clustered/blocked designs, interference, or other assignment mechanisms are not developed, which may limit direct applicability in many field experiments.","The authors propose deriving asymptotic (Neymanian) results for ridge rerandomized experiments to enable analytical variance approximations and related Bayesian inference, analogous to existing theory for standard rerandomization. They also suggest exploring broader classes of rerandomization criteria beyond Mahalanobis and ridge Mahalanobis distances, motivated by connections to principal components and Euclidean-distance-based schemes. Finally, they suggest investigating how prior information about the covariate–outcome relationship can guide the selection of rerandomization criteria when designing experiments.","Developing practical guidance for selecting $\lambda$ under explicit optimality targets (e.g., minimizing worst-case or Bayesian-expected MSE of $\hat\tau$ under plausible outcome models) would strengthen decision-making versus the current average-variance heuristic. Studying robustness to non-ideal conditions—unknown/estimated covariate covariance, missing covariates, autocorrelation/cluster dependence, and heavy-tailed covariates—would clarify when ridge rerandomization remains beneficial. Extending ridge rerandomization to clustered/blocked and multi-arm experiments (including constrained randomization used in field trials) and to factorial designs would broaden applicability. Providing open-source software implementations and computational benchmarks (acceptance rates, runtime, reproducibility) would aid adoption and facilitate comparisons with alternative balance methods such as regression adjustment, propensity-score based restrictions, or optimization-based assignment.",1808.04513v2,https://arxiv.org/pdf/1808.04513v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:43:57Z
TRUE,Optimal design|Other,Parameter estimation|Other,A-optimal,Variable/General (shape-variation basis dimension Nbasis=9 in 2D; Nbasis=17 in 3D; design variables are sensor activation weights over Nobs×Ntime potential measurements),Theoretical/simulation only,Other,TRUE,Other,Not provided,https://www.getfem.org|https://doi.org/10.1002/nme.2579,"The paper formulates an optimum experimental design (OED) framework for identifying an inclusion interface in a diffusion PDE, where the unknown parameter is a shape on a manifold rather than a finite-dimensional vector. Experimental conditions are the binary/relaxed activation weights of multiple spatial sensor regions over multiple measurement times, with spatially correlated measurement noise modeled via a covariance operator. The Fisher information is defined as a bilinear form on shape-variation tangent vectors (velocity fields) and discretized to a finite-dimensional subspace of shape perturbations; the combined Fisher information matrix is a weighted sum over sensor–time elementary information matrices. The OED problem minimizes an A-optimal criterion (trace of the inverse information, expressed via a generalized eigenproblem due to a non-orthonormal shape basis) under bounds and a total-activation budget, solved with a simplicial decomposition method plus Torsney’s algorithm. Numerical experiments in 2D and 3D (including a spatial-only variant) show sparse optimal activation patterns concentrated at times when diffusion fronts create informative gradients, and eigenmodes of best/worst identifiable shape deformations are analyzed.","Forward model: $\dot u-\nabla\cdot(k\nabla u)=0$ with mixed boundary conditions and piecewise-constant $k$. Sensitivity (material derivative) $\delta u$ satisfies a linearized PDE (Eq. 2.6). Elementary Fisher information as a bilinear form: $\Upsilon_{k,\ell}(V_1,V_2)=(C_k^{-1}E_{k,\ell}\delta u_1,E_{k,\ell}\delta u_2)$, implemented as $\int (A_k\delta u_1)(A_k\delta u_2)$ over the sensor region (or boundary in 3D). Combined FIM: $\Upsilon(w)=\sum_{k,\ell} w_{k,\ell}\,\Upsilon_{k,\ell}$; generalized eigenproblem $\Upsilon(w)v=\Lambda B v$ with Gram matrix $B_{ij}=b(V_i,V_j)$. A-optimal objective: $\Phi_A(\Upsilon)=\sum_i \Lambda_i^{-1}=\mathrm{trace}(B\Upsilon^{-1})$, minimized subject to $0\le w_{k,\ell}\le 1$ and $\sum w_{k,\ell}\le C_w$.","In the 2D example with $N_{\text{obs}}=8$, $N_{\text{time}}=22$ (176 potential activations) and budget $C_w=10$, the optimized design is sparse: 8 weights at 1, 4 fractional, 164 zero, and reaches $\Phi_A\approx 32.93\times10^{-4}$ for the main Robin-boundary case; uniform weights under the same budget yield $\Phi_A\approx 122.14\times10^{-4}$ (substantially worse). With homogeneous Neumann outflow ($\beta\equiv 0$), optimized $\Phi_A\approx 48.21\times10^{-4}$ versus uniform $\Phi_A\approx 252.17\times10^{-4}$. In 3D with $N_{\text{obs}}=45$, $N_{\text{time}}=22$ (990 activations) and $C_w=40$, the optimized pattern has 32 weights at 1, 20 fractional, 938 zero; a spatial-only variant with $C_w=12$ gives 4 weights at 1 and 16 fractional. Across examples, optimal activations concentrate around times when the diffusion front reaches sensor regions, and generalized-eigenmode analysis reveals best- and worst-identifiable shape-variation directions (often with a pronounced gap for the worst mode).","The authors note that the interface identification problem is structurally ill-posed in infinite dimensions (there exist bounded sequences of shape variations with vanishing Fisher information), so poorly identifiable subspaces cannot be cured by design alone. They therefore restrict attention to a finite-dimensional subspace of “reasonably well identifiable” low-frequency shape variations (regularization by discretization). They also state that extending the design to jointly optimize additional experimental conditions like the Robin parameter $\beta$ would remove the ability to precompute elementary FIMs and is left for future work.","Results are demonstrated on synthetic PDE simulations on unit-square/unit-cube geometries with specific sensor layouts and noise-model parameters; real experimental validation and robustness to model mismatch are not addressed. The design assumes independence across sensor regions and time instances (no inter-region/time correlation) and uses a specific spatial covariance operator form, which may not match practical sensing systems. The optimized solution relies on a relaxed continuous-weight design; converting to implementable binary schedules is only briefly handled via a heuristic rounding suggestion without quantifying performance loss. Computational cost/scalability is not fully characterized beyond iteration counts; for large 3D problems, precomputing all elementary sensitivities/FIM blocks can be expensive.","They propose extending the model problem to include additional experimental conditions such as optimizing the Robin parameter $\beta$ along the outflow boundary. In that case, elementary Fisher information matrices would depend on $\beta$ and could not be precomputed; developing efficient solution methods for this extended OED problem is left to future research.","Developing principled integer/binary design extraction (or mixed-integer OED) with guarantees on degradation relative to the relaxed optimum would improve implementability. Extending the framework to handle temporal correlations and cross-sensor correlations, unknown noise hyperparameters ($\alpha_0,\alpha_1$), and model-parameter uncertainty (Bayesian OED) would broaden applicability. Adding self-starting/online adaptive sensor activation that updates designs as measurements arrive could better exploit sequential information. More extensive benchmarking on varied geometries, sensor types, and real datasets, plus open-source implementations, would strengthen practical adoption.",1808.05776v1,https://arxiv.org/pdf/1808.05776v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:44:52Z
FALSE,NA,NA,Not applicable,Not specified,Semiconductor/electronics|Theoretical/simulation only,Case study (real dataset)|Simulation study,TRUE,Other,Not provided,http://www.lumerical.com/tcad-products/mode/|http://www.comsol.com,"The paper designs and fabricates hybrid silicon–chalcogenide (As2S3) photonic waveguides with controlled longitudinal width variations and demonstrates on-chip distributed Brillouin sensing using Brillouin optical correlation domain analysis (BOCDA). The method is used to localize geometry-dependent changes in the Brillouin gain spectrum and Brillouin frequency shift (BFS) along 6 mm waveguides, detecting longitudinal features as small as 200 µm. Experimentally, the authors vary ASE bandwidth (e.g., 60–80 GHz) to tune spatial resolution (about 1 mm down to 750 µm) and scan the correlation peak along the device using a delay line, showing the appearance/disappearance of a BFS peak (~7.59 GHz) at the narrow section. They also perform numerical simulations of optical mode transition (FDTD) and full-vectorial opto-acoustic overlap (COMSOL) to explain multi-peak Brillouin spectra due to multiple acoustic modes and to match measured spectra under fabrication-variation corner analysis. Overall, the work advances integrated SBS metrology by improving practical detection capability and providing design/analysis guidance for geometry-sensitive SBS in sub-wavelength waveguides.","Spatial resolution is described as approximately $\Delta z \approx \tfrac{1}{2} V_g \Delta t$, where the correlation-peak duration $\Delta t$ is inversely related to the ASE bandwidth. The Brillouin frequency shift is approximated by $\nu_B = \tfrac{2 n_{\mathrm{eff}} V_a}{\lambda_p}$ (Eq. 1), linking BFS to effective index, acoustic velocity, and pump wavelength; the paper notes that sub-wavelength waveguides require full-vectorial treatment beyond this approximation. Brillouin gain spectra are fit/reconstructed with Lorentzian line shapes based on the calculated opto-acoustic overlap strengths across acoustic modes in a target frequency span (about 7.50–7.90 GHz).","Integrated SBS measurements of width-varying waveguides show three peaks around 7.59, 7.72, and 7.81 GHz for a 1 mm feature, with the low-frequency peak (7.59 GHz) disappearing as the feature length is reduced to 200 µm. Distributed BOCDA measurements identify the narrow section by the emergence of the ~7.59 GHz peak localized near the mid-waveguide positions (around 3.8–4.2 mm for the 200 µm-feature device). The authors report spatial resolutions of about 1 mm (with 60 GHz ASE bandwidth) and about 750 µm (with 80 GHz bandwidth), enabling detection of a 200 µm feature though with degraded SNR. Simulations (COMSOL) reproduce the observed spectral shifts and the double-peak profile in wider waveguides as arising from multiple acoustic modes with strong overlap; corner analysis allows ~7% thickness/width variation to match experiment.","The authors state that the practical spatial-resolution/detection limit is constrained by SNR as ASE bandwidth increases (shorter correlation peak weakens local SBS signal relative to background spontaneous scattering). They also note that for ASE bandwidths larger than the BFS, separating the back-reflected pump from the amplified probe is difficult due to spectral overlap; pump back-reflection filtering removes part of the SBS signal. In the smallest-feature measurement, they explicitly report deterioration in signal quality due to lower SNR and spatial resolution being larger than the feature size, leaving residual artifacts away from the feature.","The demonstrated spatial resolution (750 µm) is still coarser than the smallest detected feature (200 µm), so “detection” relies on indirect localization rather than fully resolving the feature profile; quantitative localization error is not rigorously characterized. The study is limited to specific waveguide geometries/material stack and does not establish robustness to other fabrication variations, temperature/strain perturbations, or system drift that would affect BFS in practice. No open implementation details are provided for the BOCDA signal processing, fitting, or simulation workflow, making reproduction and fair benchmarking against other distributed SBS methods harder.","They suggest that improving fabrication (e.g., reducing grating-coupler back reflection such as via tilted grating couplers) would improve SNR and enable higher spatial resolution. They also state that further increasing spatial resolution could provide valuable information about the spatial limits of nonlinear opto-acoustic interaction in sub-wavelength regimes and allow study of more complex structures with very fine features.","A self-calibrating or statistically grounded localization framework (e.g., uncertainty estimates for feature position/BFS under low SNR) would strengthen claims about detectability below nominal spatial resolution. Extending the approach to handle non-idealities common in integrated platforms (temperature gradients, stress, polarization effects, and residual reflections) and demonstrating compensation methods would improve practical deployment. Public release of processing/simulation code or a reference implementation (e.g., scripts to reconstruct gain spectra and fit Lorentzians) would facilitate adoption and comparative evaluation across platforms.",1809.07160v1,https://arxiv.org/pdf/1809.07160v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:45:34Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Model discrimination|Cost reduction,Other,"Variable/General (examples include D=1–4 design variables; some cases include a binary design variable; model parameters |θ| vary e.g., 1–14)",Pharmaceutical|Other|Theoretical/simulation only,Simulation study,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/cog-imperial/GPdoemd,"The paper introduces GPdoemd, an open-source Python package for sequential optimal design of experiments aimed at discriminating among rival mechanistic models, including black-box models without accessible gradients. It proposes a new model-discrimination design criterion based on the quadratic Jensen–Rényi divergence (DJR), providing a closed-form divergence measure for Gaussian predictive distributions (including Gaussian mixtures). To handle black-box models, it replaces them with Gaussian process surrogate models and approximates parameter uncertainty propagation by marginalizing over parameter distributions using first- or second-order Taylor expansions, enabling use of classical discrimination criteria (HR, BH, BF, AW) and DJR. The approach is evaluated on multiple case studies (analytical and black-box, including biochemical/chemical process examples) via simulation, comparing required numbers of additional experiments, success/failure rates, and inconclusive outcomes against analytical baselines. Results show the surrogate-based approach can emulate classical gradient-based methods well and enables practical DOE for black-box model discrimination, with limitations mainly in scaling GP surrogates to high-dimensional settings.","Classical parameter-uncertainty propagation uses a Laplace approximation for the parameter covariance, $\Sigma_{\theta,i}=\left[\sum_{n=1}^N \nabla_\theta f_i(u_n)^\top \Sigma^{-1}\nabla_\theta f_i(u_n)\right]^{-1}$, and predictive covariance $\tilde\Sigma_i(u)=\nabla_\theta f_i(u)^\top\Sigma_{\theta,i}\nabla_\theta f_i(u)$. The new design criterion is the quadratic Jensen–Rényi divergence $D_{JR}(u)=\mathrm{Div}[H_2](u)$, where $\mathrm{Div}[H](u)=H(\sum_i \pi_i g_i(u)) - \sum_i \pi_i H(g_i(u))$ and $H_2(G)=-\log\int G(\gamma)^2 d\gamma$ yields closed-form expressions for Gaussian mixtures (Eqs. 12–15). For black-box models, GP surrogates provide predictive moments and the marginal predictive distribution under $\theta\sim\mathcal N(\theta^*,\Sigma_\theta)$ is approximated via Taylor expansion, e.g. first order: $\tilde\mu(u)\approx \mu(z^*)$ and $\tilde\Sigma(u)\approx \Sigma_f(z^*) + \nabla_\theta\mu\,\Sigma_\theta\,\nabla_\theta\mu^\top$ (Eq. 26), with second-order corrections in Eqs. 28–30.","Across multiple simulated case studies, the proposed DJR criterion performs comparably to classical model-discrimination criteria (BH/BF/AW), often with similar averages in additional experiments within reported standard errors (e.g., ammonia synthesis comparisons). The GP surrogate method with first-order Taylor marginalization (GP-T1) generally matches the analytical first-order baseline in both success rates and number of additional experiments, indicating effective emulation when gradients are unavailable. Second-order marginalization (GP-T2) can improve performance in some cases (e.g., ammonia synthesis) but can degrade in harder settings (e.g., a difficult mixing instance), highlighting sensitivity to approximation quality and discriminability. In a black-box biochemical network case, performance is constrained by model indistinguishability (high inconclusive rates); removing a nearly-indiscriminable rival model markedly increases success rates (Table 9). The software is provided as open source and includes implementations of criteria (HR/BH/BF/AW/JR), discrimination methods (Gaussian posteriors, $\chi^2$, Akaike weights), and GP modeling options (including sparse GP).","A key limitation noted is scalability: Gaussian-process surrogates can require large training data sets to accurately emulate models with high-dimensional design and parameter spaces, and GP training/inference can become computationally challenging despite sparse GP options. The authors also emphasize that they do not provide general guidelines for training-data generation because required data volume depends on model sensitivity and users’ computational budgets. They highlight model indistinguishability/indiscriminability as a major practical hurdle that can prevent successful discrimination even with many experiments.","The method’s practical effectiveness depends on the fidelity of GP surrogates across the joint design–parameter space; if the surrogate mis-specifies nonstationarity, discontinuities, or strong heteroskedastic noise, the resulting design recommendations may be biased. The approach assumes (approximately) Gaussian predictive distributions and often Gaussian parameter posteriors (via Laplace), which may be inaccurate for multimodal or constrained parameter posteriors common in mechanistic models. Comparisons emphasize a limited set of classical criteria and a few case studies; broader benchmarking against modern Bayesian optimal design methods (e.g., SMC/variational mutual information estimators) and robustness checks to misspecified noise models/autocorrelation are not fully explored.","The authors suggest extending GPdoemd with additional model discrimination criteria (e.g., Minimum Message Length, BIC, WAIC) and improving parameter estimation (e.g., incorporating global parameter estimation to avoid discarding the true model due to poor fits). They also discuss the need for better stopping criteria and approaches to deal with model indiscriminability, including modifying experimental setups (reducing measurement noise, adding inputs/outputs) or tightening parameter bounds. They note interest in designing parallel/batch experiments (batch optimization heuristics) and extending the approach beyond static models $y=f(u,\theta)+\epsilon$ to time-dependent/state-space models using GP-based dynamics and uncertainty propagation.","Developing adaptive, information-theoretic batch DOE that explicitly accounts for surrogate-model uncertainty and updates GP hyperparameters online could improve efficiency in lab settings with parallel runs. A self-starting/Phase-I procedure for calibrating noise models and handling autocorrelated or non-Gaussian measurement errors would broaden applicability to real process data. Extending to high-dimensional settings may benefit from active subspace discovery or sparse/additive GP structures to reduce training burden, plus rigorous guarantees (e.g., regret or consistency) linking surrogate error to discrimination outcomes. Providing a maintained pip/conda package and reproducible experiment scripts/benchmarks (beyond a repository link) would strengthen adoption and independent verification.",1810.02561v3,https://arxiv.org/pdf/1810.02561v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:46:19Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Optimization|Model discrimination|Prediction,Other,"Variable/General (design variable examples: 1D measurement time τ; non-myopic n-dimensional vector d=[τ1,...,τn] with n=8 demonstrated)",Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),http://github.com/SheffieldML/GPyOpt,"The paper proposes an efficient Bayesian experimental design framework for implicit (likelihood-intractable) simulator models. It uses mutual information between parameters and data as the expected utility, estimated via likelihood-free density-ratio estimation (LFIRE) so that posterior-to-prior ratios can be computed without evaluating likelihoods. The resulting expensive, noisy expected-utility surface is optimized with Bayesian optimization (Gaussian process surrogate with expected improvement acquisition) rather than grid search or sampling-based methods, enabling higher-dimensional design variables. The approach is demonstrated on two epidemiological simulators (a Death model with tractable likelihood for validation, and an intractable SIR model), showing LFIRE-based mutual information closely matches the analytic utility near the optimum for the Death model and that Bayesian optimization reaches good designs with far fewer utility evaluations, including non-myopic designs with n=8 measurement times.","The Bayesian optimal design is defined by $d^* = \arg\max_d U(d)$. The utility is mutual information $U(d)=\mathrm{MI}(\theta,y\mid d)=\mathbb{E}_{p(y\mid d)}\big[ D_{\mathrm{KL}}(p(\theta\mid d,y)\Vert p(\theta))\big]$, with a Monte Carlo estimator $U(d)\approx \tfrac{1}{N}\sum_{i=1}^N \log\tfrac{p(\theta^{(i)}\mid d,y^{(i)})}{p(\theta^{(i)})}$. Using LFIRE, the intractable ratio is replaced by $r(d,y,\theta)=\tfrac{p(y\mid\theta,d)}{p(y\mid d)}=\tfrac{p(\theta\mid d,y)}{p(\theta)}$, giving $U(d)\approx \tfrac{1}{N}\sum_{i=1}^N \log r(d,y^{(i)},\theta^{(i)})$.","For the Death model (1D time design, $0<\tau\le 4$), analytic grid search gives $(\tau^*,U(\tau^*))=(1.40,1.347)$, LFIRE grid search gives $(1.10,1.350)$, and LFIRE + Bayesian optimization gives $(1.06,1.359)$, indicating close agreement in achieved utility near the optimum. In non-myopic Death-model design with $n=8$ times, Bayesian optimization converges to an optimum in about 20 utility evaluations (contrasted with an illustrative grid size that would require 76,904,685 evaluations). For the SIR model (1D time design, $0<\tau\le 3$), optimal times are about $\tau^*=0.40$ (grid) and $0.44$ (Bayesian optimization); with $n=8$, Bayesian optimization converges in about 15 evaluations, and the utility improvement over 8 equidistant times is small (reported $U(d_{eq})=1.08$ vs $U(d^*)=1.10$).","The authors note that high-dimensional Bayesian optimization remains an active research area and that applicability in “hundreds of dimensions” still needs investigation. They also state that, as with all likelihood-free inference methods, posterior estimates are approximate; they observed this with LFIRE on the Death model. They further note it would be informative to characterize more generally how approximation error affects the mutual information utility.","The method assumes the simulator can be run cheaply enough to support repeated LFIRE ratio estimation (including generating samples from both $p(y\mid\theta,d)$ and the marginal $p(y\mid d)$), which may be prohibitive for expensive simulators. The mutual-information estimate depends on LFIRE classifier performance and chosen features/summaries (not detailed here), so misspecification can bias utilities and designs. The empirical evaluation is limited to two epidemiological examples and does not benchmark against other modern likelihood-free design utilities/estimators (e.g., neural MI estimators or alternative ratio-estimation schemes) under a standardized compute budget.","They suggest investigating more scalable approaches for very high-dimensional Bayesian optimization. They propose studying more generally how LFIRE approximation affects mutual information estimates. They also mention extending the framework to other implicit models (e.g., neurobiology, cell biology, physics, including temporal/spatial models) and incorporating experimental cost/time into the utility, not only information gain. Finally, they mention preliminary results indicating extension to sequential designs that update beliefs based on experimental outcomes.","Developing a fully self-contained implementation with open-source code for the complete LFIRE+Bayesian-optimization design loop (not just the BO library) would improve reproducibility and adoption. Extending the framework to handle model misspecification, observation noise models, and autocorrelated/partially observed data would broaden practical applicability. More systematic comparisons under equal simulation budgets (including sensitivity to priors, LFIRE training size, and BO hyperparameters) and real-data case studies would better establish robustness and guidance for practitioners.",1810.09912v2,https://arxiv.org/pdf/1810.09912v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:47:00Z
TRUE,Optimal design|Sequential/adaptive|Other,Cost reduction|Parameter estimation|Model discrimination|Other,Not applicable,Variable/General (n variables/vertices; interventions can target subsets; k-sparse interventions limit |I_i|≤k),Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies non-adaptive intervention (experiment) design for learning the directions of all undirected edges in a causal graph’s essential graph when each variable has an intervention cost. It formulates minimum-cost intervention design as finding a minimum-cost graph separating system (equivalently, a minimum-cost vertex coloring over binary vectors) on chordal components and proves the problem is NP-hard even on interval graphs (and even with unit costs). It proposes a modified greedy maximum-weight-independent-set coloring algorithm with a quantization step and proves a constant-factor cost approximation guarantee, achieving roughly a (2+ε)-approximation when the number of interventions is at least log(χ)+O(log log n). The paper also treats sparse experiments where each intervention includes at most k variables, giving lower bounds and near-optimal (in size) constructions for sparse graphs, and proposes a heuristic extension for weighted k-sparse designs using a penalty parameter. Empirically on simulated chordal graphs, the greedy method is close to optimal (via integer programming) and much faster, and the k-sparse method shows a trade-off between number of interventions and total cost.","Minimum-cost intervention design chooses subsets (interventions) \(\mathcal{I}=\{I_1,\dots,I_m\}\) that cut every edge at least once: \(\bigcup_{I\in\mathcal{I}}\delta(I)=E\), minimizing \(\mathrm{cost}(\mathcal{I})=\sum_{I\in\mathcal{I}}\sum_{v\in I} w_v\). In the equivalent coloring form with colors \(c(v)\in\{0,1\}^m\), minimize \(\mathrm{cost}(c)=\sum_{v\in V} \|c(v)\|_1\, w_v\). The k-sparse variant adds constraints \(|I_i|\le k\) for all interventions, and the paper proves a lower bound \(m_k^*\ge \tau/k\) where \(\tau\) is the minimum vertex cover size.","Main theoretical results are: (i) minimum-cost intervention design is NP-hard (even on interval graphs, even with unit vertex costs). (ii) A greedy coloring algorithm with weight quantization yields a graph separating system with \(\mathrm{cost}(\mathcal{I}_{\mathrm{greedy}})\le (2+\varepsilon)\,\mathrm{OPT}\) when \(m\ge \log\chi+\log\log n+5\), with \(\varepsilon=\exp(-\Omega(m))+n^{-1}\). (iii) For k-sparse designs, the minimum number of interventions satisfies \(m_k^*\ge \tau/k\), and a vertex-cover-plus-coloring algorithm achieves near-optimal size for sparse graphs (a \(1+o(1)\) approximation under stated sparsity/degree scaling). Empirically on simulated chordal graphs, the greedy method is close to the integer-programming optimum while running much faster (example reported: ~5 seconds vs 128 seconds for IP on n=10,000, max degree 20, m=5).",None stated.,"The work assumes the essential graph/chordal components are given (i.e., the observational phase has already recovered the correct Markov equivalence class), and focuses on worst-case guarantees for non-adaptive interventions; performance may differ under finite-sample conditional-independence testing errors. Interventions are modeled abstractly via graph separation (learning an edge direction whenever exactly one endpoint is intervened), without modeling statistical power, imperfect interventions, or outcome noise/cost of samples per intervention. The empirical evaluation is limited to simulated chordal graphs from a specific generator and does not include real-world causal discovery pipelines or non-chordal/latent-confounding settings.",They discuss extending the k-sparse intervention algorithm to the weighted case (via a penalty \(\lambda\) added to vertex weights to trade off intervention count vs cost) and suggest exploring the trade-off by running the algorithm for various \(\lambda\) values.,"Natural extensions include adaptive/sequential intervention policies that update designs based on interventional outcomes, and robustness to unknown/estimated costs or constraints beyond simple k-sparsity (e.g., grouped/forbidden interventions, batch costs). It would also be valuable to integrate finite-sample causal discovery uncertainty (errors in the estimated essential graph) and to evaluate end-to-end performance on realistic datasets, including settings with latent confounders or non-chordal essential graph components. Providing an open-source implementation would improve reproducibility and facilitate adoption in causal discovery toolchains.",1810.11867v1,https://arxiv.org/pdf/1810.11867v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:47:37Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper presents a deterministic methodology to generate extreme (rogue/freak) waves in a towing tank using an exact solution (Soliton on a Finite Background, SFB) of the spatial nonlinear Schrödinger equation (NLSE). It introduces and uses the Maximum Temporal Amplitude (MTA), defined as the maximum over time of the wave elevation at each spatial location, to link a desired extreme wave amplitude at a target position to the required wavemaker input and the resulting amplitude amplification factor. The approach leverages properties of SFB wave focusing and phase singularities to choose signal parameters (e.g., modulation length/frequency and background amplitude) that produce an extreme event at a prescribed tank location. The paper provides illustrative parameter sets in laboratory coordinates (e.g., 5 m depth, 200 m tank, target at 125 m, desired MTA 0.5 m) but does not present a formal design-of-experiments (DOE) framework (e.g., factorial/RSM/optimal designs) or statistical analysis. It concludes by noting future work toward numerical and experimental validation of the theoretical predictions.","The spatial NLSE model is given by $\partial_\xi \psi + i\beta\,\partial_{\tau\tau}\psi + i\gamma |\psi|^2\psi=0$, with $\xi=x$ and $\tau=t-x/\Omega_0(k_0)$. The laboratory maximum temporal amplitude is defined as $M(x_{lab})=\max_{t_{lab}}\eta_{lab}(x_{lab},t_{lab})$. The amplification factor is described as $\alpha=M(x_{ship})/(2a^{lab}_0)$, with an “actual” factor $\alpha_{actual}=M(x_{ship})/M(0)$ also reported.","For a reference case (depth 5 m, tank length 200 m, target position $x_{ship}=125$ m, desired $M(x_{ship})=0.5$ m, and $\omega_{lab}=3.5$), the paper tabulates example parameter choices yielding amplification factors $\alpha\approx 2.00$ to $2.94$. Corresponding far-field background amplitudes are $2a^{lab}_0\approx 0.17$ to $0.25$ m, modulation frequencies $\nu_{lab}\approx 0.118$ to $0.624$, and modulation periods $T_{lab}\approx 10.1$ to $53.3$. The reported $\alpha_{actual}$ values range from about 1.85 to 2.23 for the listed examples, and figures show the wavemaker vs. target-position time signals for selected cases.","The authors note that they do not describe the technical restrictions of the wavemaker in detail, stating only the intent that the required signal can be fed easily to wavemakers. They also state that their results are theoretical and that future work will provide numerical and experimental evidence to validate the predictions.","The work is not a DOE/statistical design study; it provides illustrative parameter sets rather than a systematic experimental design strategy (e.g., factors/levels, randomization, replication, uncertainty quantification). Results rely on the spatial NLSE and the exact SFB solution, so applicability may be limited when real tank physics deviate from NLSE assumptions (e.g., wave breaking limits, higher-order nonlinearities/dispersion, dissipation, reflections, wavemaker transfer functions). No robustness analysis is presented for parameter uncertainty or model mismatch, and there is no quantitative comparison against alternative generation strategies or empirical validation in the paper.","The paper states that future research will focus on numerical and experimental evidence to validate the theoretical prediction, including comparison with numerical wave-tank computations and laboratory measurements.","A useful next step would be to cast the parameter-selection problem as an explicit DOE/optimization task (e.g., constrained optimal design over wavemaker limits, robustness to uncertainty) and validate across a wider range of depths, target locations, and non-ideal tank effects. Developing a practical calibration workflow that incorporates wavemaker dynamics and measurement noise, plus open-source implementation scripts for generating the input signals, would improve reproducibility and adoption.",1811.01524v3,https://arxiv.org/pdf/1811.01524v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:48:06Z
TRUE,Optimal design|Sequential/adaptive,Parameter estimation|Cost reduction,A-optimal,"7 parameters (p, t+, De, Ds,p, Ds,n, kp, kn)",Energy/utilities|Theoretical/simulation only,Simulation study,TRUE,MATLAB|Other,Not provided,https://arxiv.org/abs/1811.08656|https://web.mathworks.com/matlabcentral/fileexchange/|https://web.casadi.org/,"The paper proposes a Fisher Information Matrix (FIM)-based optimal design of experiments (DoE) method to identify key parameters of the Single Particle Model with electrolyte dynamics (SPMe) for lithium-ion cells using voltage measurements under input current excitation. The DoE is formulated as a nonlinear constrained optimization that designs a piecewise-constant current profile to minimize an approximation of the parameter covariance matrix via the A-optimal criterion (minimize the trace of the inverse FIM). To make long-horizon design computationally feasible, the authors introduce a sub-optimal decomposition that splits the horizon into multiple shorter optimization subproblems whose solutions are concatenated. Performance is evaluated in simulation first with the SPMe as the “plant” (Gaussian voltage noise) and then in a more realistic setting where a high-fidelity P2D/DFN model (implemented in LIONSIMBA) generates data while SPMe parameters are estimated. Across both settings, the optimized input profiles reduce parameter variances and speed convergence compared with standard constant-current and multi-step discharge protocols, improving subsequent voltage prediction against the P2D model.","The parameter sensitivity matrix for an experiment ξ is defined as $S_{\xi}(\phi)=\nabla_{\phi} y_{\xi}(\phi)$ (computed numerically by finite differences), and the Fisher Information Matrix is $F_{\xi}(\phi)=S_{\xi}(\phi)^T C_y^{-1} S_{\xi}(\phi)$ with $C_y=\sigma_y^2 I$. Using the Cramér–Rao bound, the inverse FIM approximates a lower bound on the parameter covariance: $F_{\xi}(\phi)^{-1} \le C_{\phi}^{\xi}$. The DoE solves a constrained nonlinear program over the input current sequence to minimize the A-optimal objective $J=\mathrm{Tr}(C_{\phi})\approx \mathrm{Tr}(F_{\xi}(\hat\phi)^{-1})$ subject to SPMe dynamics and bounds on current and voltage.","With SPMe as the plant, the optimal DoE achieves very small scaled parameter variances after the first 1000 s experiment (e.g., $p$: $5\times10^{-5}$; $k_p$: $1.5\times10^{-3}$), while constant-current discharge retains much larger variances even after 10,000 s (e.g., $k_p$: 2.98). Ill-conditioning diagnostics after the full procedure show the CC approach yields much worse conditioning (condition number κ ≈ 31963; collinearity index γ ≈ 7548) than the optimal DoE (κ ≈ 577; γ ≈ 152) and multi-step (κ ≈ 260; γ ≈ 110). The sub-optimal horizon-splitting method substantially reduces computation time for a 200 s design example (≈18.3 s vs 68.5 s) with only a small degradation in the scaled trace of covariance (0.025 vs 0.020). When using P2D (LIONSIMBA) as the plant, the SPMe calibrated via optimal DoE yields the lowest RMS voltage prediction error on a validation multi-sinusoidal current profile compared with parameters obtained from CC or multi-step discharge.","The authors note that solving the full nonlinear constrained DoE over long horizons can be computationally burdensome, motivating the proposed sub-optimal decomposition approach. They also emphasize that practical effectiveness ultimately needs real experimental validation beyond simulation (explicitly suggested as future work).","The DoE is demonstrated only in simulation (SPMe and P2D/DFN simulator), so performance under real-world nonidealities (sensor drift, unmodeled dynamics, hysteresis, temperature variation, and aging) remains uncertain. The method assumes independent, identically distributed Gaussian voltage noise and relies on local sensitivity/FIM calculations around the current parameter estimate, which can be fragile under strong nonlinearities or poor initial guesses. Implementation details for the nonlinear solver (e.g., exact solver choice/tolerances and handling of potential nonconvexity/multiple local minima) are not fully specified, and reproducibility is limited because the DoE/estimation code is not shared.",Future work is stated as experimental validation of the proposed optimal DoE strategy on real battery hardware.,"Extending the DoE to include temperature-coupled dynamics and constraints (or jointly designing current and thermal conditions) would improve applicability in practical BMS settings. Robust/self-starting versions that account for parameter uncertainty explicitly (e.g., Bayesian/robust optimal design or multi-scenario design) could reduce dependence on the current estimate and improve reliability. Additional validation on different chemistries, aging states, and under input/output constraints typical of real test equipment (including current slew limits and SOC-dependent safety limits) would strengthen generalizability; releasing implementation code would also improve reproducibility and adoption.",1811.08656v2,https://arxiv.org/pdf/1811.08656v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:48:42Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction|Other,Bayesian D-optimal|Other,Variable/General (application example: 4 unknown parameters; design variable is electrode placement/current pattern in EIT),Manufacturing (general)|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops two multilevel estimators for the Expected Information Gain (EIG), an information-theoretic utility used in simulation-based Bayesian optimal experimental design, targeting settings where experiments are modeled by PDEs and require discretized numerical solves. It proposes Multilevel Double Loop Monte Carlo (MLDLMC) and Multilevel Double Loop Stochastic Collocation (MLDLSC), both using Laplace-based importance sampling to reduce the computational cost of the inner (evidence) expectation. For MLDLMC, the authors derive bias/variance/work bounds, provide an error-tolerance-driven parameter selection strategy (levels and sample sizes), and discuss work complexity relative to standard MLMC. MLDLSC replaces outer Monte Carlo with sparse-grid stochastic collocation (multi-index/combination technique) to exploit regularity in random inputs, while still using importance sampling for the inner integral. Numerical experiments on electrical impedance tomography for inferring fiber orientations in composite laminates show substantial computational gains over single-level DLMC with importance sampling, with MLDLSC outperforming MLDLMC when sufficient regularity is present.","The design criterion is the Expected Information Gain (EIG) $I=\mathbb{E}_{\theta}\,\mathbb{E}_{Y\mid\theta}\left[\log\frac{p(Y\mid\theta)}{p(Y)}\right]$, equivalently $I=\mathbb{E}[Z(\theta)]$ with nested evidence $p(Y)=\int p(Y\mid\theta')\pi(\theta')\,d\theta'$. The inner evidence is estimated via Laplace-based importance sampling: $p(Y)=\int p(Y\mid\theta')R(\theta';Y)\tilde\pi(\theta'\mid Y)\,d\theta'$ where $\tilde\pi(\cdot\mid Y)=\mathcal{N}(\hat\theta(Y),\Sigma(\hat\theta(Y)))$ is centered at the MAP $\hat\theta(Y)$ and $R=\pi/\tilde\pi$. MLDLMC uses the telescoping identity $I_L=\sum_{\ell=0}^L\mathbb{E}[\Delta Z_\ell]$ over discretization levels and estimates each term with an outer/inner loop (Eq. (25)); MLDLSC uses a multi-index sparse-grid combination estimator $I_{\mathrm{MLDLSC}}=\sum_{[\ell,\beta]\in\Lambda}\Delta[F_{\ell,\beta}]$ (Eq. (70)) with collocation quadrature in the stochastic space.","On an EIT Bayesian design example (inferring 4 ply fiber angles with 10 electrodes; noise $\epsilon_i\sim\mathcal{N}(0,10^{-4}I)$; $N_e=1$), the multilevel estimators achieve the requested error tolerances with reliable control: for MLDLMC with $\alpha=0.05$, only about 2% of 100 repeated runs exceeded the target tolerance, consistent with the 95% success requirement. Empirically, estimated work and weak-error rates were about $\gamma\approx 2$ and $\eta_w\approx 1.5$ for the PDE discretization hierarchy, and the chosen finest level scaled roughly like $L\approx 1.4\log(TOL^{-1})$. Timing comparisons show both MLDLMC and MLDLSC are markedly faster than the single-level DLMCIS baseline for the same tolerance; MLDLSC is faster than MLDLMC when the QoI has exploitable regularity in parameters/noise. The finest reported mesh level in the study reached $L=6$ (e.g., $N_x=640$, $N_y=256$) for small tolerances.","The paper explicitly states that efficiently maximizing the EIG over the design space (i.e., the optimization strategy to find the best experiment set-up) is beyond the scope; the focus is on efficient, error-controlled estimation of EIG for a fixed design. It also notes MLDLSC’s advantage depends on sufficient regularity with respect to uncertain parameters/noise, while MLDLMC may perform better when regularity is low. The authors remark that MLDLSC can perform worse than expected at large tolerances due to non-sharp error estimates used by the adaptive algorithm.","No implementation details (software/libraries, solver configurations, meshes, hardware) or reproducible code are provided, making it difficult to replicate timing and performance claims. The methods rely on Laplace-based importance sampling (Gaussian approximation around the MAP), which can degrade for multimodal, highly non-Gaussian, or weakly identifiable posteriors; robustness to such cases is not thoroughly explored. The numerical validation is centered on a single PDE-based EIT example; broader benchmarks (other PDEs, higher-dimensional designs, different noise levels, or competing modern Bayesian OED estimators) would strengthen generalizability. Design variables are discussed conceptually (e.g., electrode placement/current patterns), but the work does not demonstrate end-to-end design optimization or sensitivity of estimators to different design parameterizations.","The authors indicate that optimizing (maximizing) the expected information gain over the experimental design space is an important subsequent task but is outside the scope of this study, implicitly pointing to integrating these estimators with efficient optimization strategies. They also highlight that estimator choice depends on regularity, suggesting further work on when and how to exploit regularity (favoring MLDLSC) versus low-regularity regimes (favoring MLDLMC).","Develop and test end-to-end Bayesian OED workflows that couple MLDLMC/MLDLSC with scalable optimizers (stochastic gradients, coordinate exchange, Bayesian optimization) on realistic design spaces (e.g., electrode placement as continuous/discrete decisions). Extend the estimators to handle non-Gaussian/heteroskedastic noise, model discrepancy, and correlated observations, and evaluate robustness when the Laplace approximation is poor (e.g., multimodal posteriors). Provide open-source implementations with standardized benchmarks and profiling to clarify computational tradeoffs and practitioner guidance on choosing between MLDLMC and MLDLSC. Investigate multivariate/high-dimensional parameter settings and adaptive multi-index set construction tailored to nested EIG structure to further reduce cost and improve error estimates.",1811.11469v3,https://arxiv.org/pdf/1811.11469v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:49:35Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction,D-optimal|A-optimal|I-optimal (IV-optimal)|G-optimal|Other,Variable/General (examples include 4 predictors; general p predictors; plus bias terms m and confounders q),Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,R,Not provided,NA,"This paper reviews and extends model-based optimal design of experiments (optimal subdata selection) methods for choosing informative subsets from large datasets, with emphasis on mitigating model bias and protecting against confounders. It frames the general data-generating process as a linear model with three components: the main model term in observed covariates, an additional bias term in observed covariates, and a confounder term in unobserved covariates, then discusses special-case literature addressing each part. The review covers sequential, response-adaptive utility-based subsampling (Drovandi et al.) and the deterministic boundary-seeking IBOSS method for approximate D-optimal subdata selection, noting strengths and limitations for tall vs genuinely large (many predictors) datasets. New results include an equivalence-theorem-style statement for D-optimality in the combined (observed, unobserved) design space and derivations for the A-optimal trace criterion of the mean-square-error matrix when both bias and confounders are present. Simulations and worked examples (implemented in R) compare algorithms and show how ignoring bias/confounders changes selected subdesigns and downstream predictive performance.","The general model is $Y(x,z)=f(x)^\top\theta + h(x)^\top\psi + g(z)^\top\phi + \varepsilon$, with iid errors and design measure $\xi$ on $\mathcal X\times\mathcal Z$. For subdata selection, the (linear-model) information matrix is expressed as $M(\delta)=\sigma^{-2}\sum_{i=1}^N \delta_i x_i x_i^\top$ (IBOSS setting), and in the full bias+confounder setting as a block matrix $M(\xi)=\int (f,h,g)(f^\top,h^\top,g^\top)\,d\xi$. The mean-square-error matrix for the LSE of $\theta$ is $\sigma^2 N^{-1}R$ with $R=M_{11}^{-1}+(N/\sigma)^2 M_{11}^{-1}[M_{12}\;M_{13}][\psi\;\phi]^\top[\psi^\top\;\phi^\top]\begin{bmatrix}M_{21}\\M_{31}\end{bmatrix}M_{11}^{-1}$, and an A-optimality expansion is derived: $\mathrm{tr}(R)=\mathrm{tr}(M_{11}^{-1})+(N/\sigma)^2\{\psi^\top M_{21}M_{11}^{-2}M_{12}\psi+\phi^\top M_{31}M_{11}^{-2}M_{13}\phi+\phi^\top M_{31}M_{11}^{-2}M_{12}\psi\}$.","For IBOSS, the paper reports the D-optimality bound used to motivate boundary selection: $\det(M(\delta))\le 4\,(nd/(4\sigma^2))^{p+1}\prod_{j=1}^p (x_{(N)j}-x_{(1)j})^2$, leading to choosing $r=nd/(2p)$ extreme points per predictor. In Example 1 (mortgage default simulation with 1,000,000 points and 4 covariates), Algorithm 1 used an initial sample $n_t=5{,}000$ and final designed subset $n_d=6{,}200$ and produced parameter estimates consistent with the cited reference; predictive confusion matrices on a 10,010-point test set are reported, showing that RF/NN trained on the “best” subdata from Algorithm 1 outperform RF/NN with random training data. In Examples 2–3 (simulated bias+confounder settings), IBOSS tends to push selected points more strongly to the boundary in the $(x,z)$ space, and designs differ materially when utilities ignore confounders/bias versus when they include them. The paper also proves a D-optimal equivalence-theorem-style result for the combined model, linking $\max_{(x,z)} d((x,z),\xi)$ to $\det(M(\xi))$.","The authors note that current approaches are more suitable for tall datasets than for genuinely large datasets (many observations and many predictors), and that much work is needed to develop efficient algorithms for subsample selection in the presence of bias and confounders. They state that utility computation and distance calculations can suffer from the curse of dimensionality and become computationally hard or infeasible as predictors increase. For the robust-design algorithm based on QR decomposition (Wiens), they explicitly mention it requires QR decomposition of high-dimensional matrices, needs storing large matrices in memory, and that current implementations are not very performant.","As a review-plus-preliminary-results manuscript, empirical comparisons are limited to a few illustrative simulations/examples; results may not generalize to complex dependence structures common in big data (e.g., autocorrelation, clustering, non-iid sampling). The bias/confounder framework assumes linear additive structure and relies heavily on least-squares/information-matrix arguments; robustness to model misspecification beyond the specified bias terms (nonlinearities, heteroskedasticity, heavy tails) is not established. Practical guidance for tuning (e.g., grid/discretization choices, stopping criteria, utility choice, ν/τ parameters) is not fully operationalized for practitioners at scale, and no publicly available implementation is provided to assess real-world computational feasibility. Comparisons against other modern subsampling/sketching approaches (e.g., leverage-score sampling, coresets, randomized numerical linear algebra) are not developed.","They state they are integrating the bias/confounder-guarding ideas with the subdata-selection algorithms (Algorithm 1 and IBOSS) to create efficient subsample selection methods for settings with known and unknown confounders. They also state that results in the unified theory section need refinement, including linking loss functions of $R$ to those of $M(\xi)$ so that a General Equivalence Theorem can be leveraged for criteria based on $R$. More broadly, they indicate that significant work remains to turn the theory into an efficient algorithm for selecting subsamples from large/big datasets in the presence of selection bias, model bias, and confounding.","Develop scalable implementations (streaming/online and distributed) that avoid forming/storing large matrices (e.g., randomized QR/SVD, sketching) and benchmark runtime/memory on truly high-dimensional data. Extend the framework to non-iid data and complex sampling mechanisms typical of observational big data (clustered, longitudinal, networked) and study sensitivity to unmeasured confounding and partial identification. Provide self-starting/Phase-I-like procedures for estimating nuisance quantities (e.g., σ², bias neighborhood size τ/ν) and for uncertainty quantification after adaptive subsampling. Release reproducible software (R/Python) and standardized benchmarks to compare against leverage-score/coreset methods and modern active-learning/experimental-design approaches for generalized linear models.",1811.12682v1,https://arxiv.org/pdf/1811.12682v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:50:27Z
FALSE,Sequential/adaptive|Computer experiment|Other,Optimization|Other,Not applicable,Variable/General (genome encodes choice/arrangement of optical elements and their continuous parameters; typically 2 modes and up to m=3 operators; truncation T up to 170 photons),Other|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Public repository (GitHub/GitLab),https://github.com/paulk444/AdaQuantum,"The paper introduces AdaQuantum, a genetic-algorithm framework that automatically designs quantum optics experiments (choice/ordering/parameters of input states, operations, and heralding measurements) to engineer output states optimizing a user-defined fitness function. The authors demonstrate three fitness functions relevant to quantum metrology: (i) pure-state quantum Fisher information (QFI) scaled by mean photon number, (ii) mixed-state QFI under dominant optical noise (photon loss and imperfect heralding/detection), and (iii) Bayesian mean square error (BMSE) for phase estimation beyond the asymptotic regime where QFI-based bounds are reliable. Using a toolbox of experimentally feasible elements (squeezed/coherent/Fock states; displacement, squeezing, beam splitters, phase shifts; PNRD/bucket/multiplex/homodyne measurements), AdaQuantum discovers non-Gaussian, cat-like superpositions (vacuum plus ~80 photons) giving up to ~20× improvement over the optimal Gaussian state and ~100× over the best classical (coherent) state in the noiseless setting. Under realistic loss, the algorithm finds heralded-state schemes that still outperform the squeezed-vacuum benchmark, with practical heralding probabilities (~10–20% in the loss-optimized example). The work advances automated experiment design in quantum optics by providing a flexible, modifiable, simulation-driven search tool and releasing an implementation for researchers to adapt to new toolboxes and objectives.","The primary optimized metric in the noiseless case is the pure-state QFI for a phase shift: $F_Q = 4(\langle \psi'_\phi|\psi'_\phi\rangle - |\langle \psi'_\phi|\psi_\phi\rangle|^2)$ with $|\psi_\phi\rangle = e^{i\hat n\phi}|\psi\rangle$ and $|\psi'_\phi\rangle=\partial_\phi|\psi_\phi\rangle=i\hat n|\psi_\phi\rangle$, typically optimized as $F_Q/\bar n$. With loss/imperfect heralding the mixed-state QFI is computed from the spectral decomposition $\rho_\phi=\sum_m q_m|\psi_m\rangle\langle\psi_m|$ via $F_Q=\sum_i q_i F_Q^{(i)}-\sum_{i\neq j}\frac{8q_iq_j}{q_i+q_j}|\langle \psi'_i|\psi_j\rangle|^2$. For limited-data phase estimation the Bayesian mean square error uses $\epsilon(M)=\int d\theta\,p(\theta|M)\theta^2-\left(\int d\theta\,p(\theta|M)\theta\right)^2$ and $\bar\epsilon_{\rm mse}=\int dM\,p(M)\epsilon(M)$.","In the noiseless optimization of $F_Q/\bar n$, AdaQuantum finds states with up to ~20× larger $F_Q/\bar n$ than the optimal Gaussian (squeezed-vacuum) state and ~100× larger than the coherent-state benchmark, featuring cat-like number distributions with a dominant vacuum component and support around ~60–100 photons (≈80 highlighted). With photon loss applied both to heralding measurements and the output state, the optimized (loss-specific) heralded-state designs achieve $F_Q/\bar n$ values that remain above the squeezed-vacuum benchmark across transmission probabilities from ~0.65 to 1.0, with reported heralding probabilities on the order of ~10–20% for the showcased loss-robust family. For BMSE-based optimization (average photon number fixed to 1), the algorithm yields improvements over two squeezed vacuums and coherent+vacuum baselines; the relative improvement factors reported range from 2–7% vs squeezed-vacuum and 5–15% vs coherent+vacuum for $\mu\in\{4,8,12\}$ under the specified photon-counting-after-50:50-BS measurement scheme.","The authors restrict most demonstrations to $N=2$ optical modes because simulation cost grows rapidly with Hilbert-space dimension; they report that attempts at $N>2$ were not effective due to the exponential slow-down and inability to use very large populations. They model only the dominant noise sources (loss at the output and imperfect heralding/detectors), explicitly noting that noise in initial state preparation and operation implementation is omitted because it is typically smaller (but could be incorporated later). They also note that acting with squeezing operations on arbitrary states is extremely challenging experimentally, motivating reduced toolboxes without such operations.","The approach is an algorithmic search over a user-specified toolbox and parameter bounds, so performance and “optimality” are inherently conditional on those modeling choices and may miss superior schemes outside the encoding/genome/design space. Reported metrological gains rely on simulation assumptions (e.g., truncation convergence criteria, loss models, idealized elements/POVMs in some BMSE scenarios); real experimental imperfections such as mode mismatch, phase noise, dark counts, and calibration drift are not benchmarked and could materially reduce advantages. The paper emphasizes best-found designs but provides limited reproducibility details for all runs (random seeds, complete run logs), and comparative baselines for the metaheuristic (e.g., against modern Bayesian optimization or CMA-ES) are not exhaustively evaluated beyond a few MATLAB toolbox optimizers.","The authors propose extending AdaQuantum searches to more than two optical modes by significantly enhancing the global search (exploring other metaheuristics and improving genome encoding) to handle the increased simulation burden. They also plan to extend the set of fitness functions and targets, including producing specific states such as squeezed cat states and GKP states, and more broadly optimizing for non-Gaussianity, entanglement, and negative Wigner function. They further suggest incorporating additional experimental noise sources (state-preparation and operation noise) and investigating why the discovered arrangements are particularly robust to photon loss.","A valuable extension would be systematic robustness studies (sensitivity analyses) over broader experimental nonidealities (dark counts, finite mode overlap, phase drift, nonunitary squeezing/BS errors) and designing explicitly robust fitness functions or worst-case/Bayesian-robust objectives. Developing standardized benchmarks and open, fully reproducible experiment logs (seeds, configurations, and automated rerun scripts) would strengthen adoption and enable fair comparisons with alternative optimizers (CMA-ES, Bayesian optimization, RL methods). Providing a library of physically constrained, hardware-calibrated component models (with device-specific parameter priors) and adding multiobjective optimization (metrological performance vs heralding rate vs implementation complexity) would improve practical deployability.",1812.01032v2,https://arxiv.org/pdf/1812.01032v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:51:15Z
FALSE,Computer experiment|Sequential/adaptive|Other,Optimization|Other,Not applicable,Variable/General (experiment genome includes choice/sequence of optical elements and their parameters; toolbox includes states/operators/measurements with continuous and discrete parameters),Other|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB|Python|Other,Public repository (GitHub/GitLab),https://github.com/paulk444/AdaQuantum|https://github.com/lewis-od/Quantum-Optics,"The paper proposes AdaQuantum, a hybrid machine-learning approach to automatically design practical quantum optics experiments (state-engineering circuits) that generate specified target quantum states. The method encodes an experimental setup (inputs, a sequence of optical operations, and a heralding measurement) as a genome and uses a genetic algorithm to optimize the arrangement and continuous parameters, with state fidelity to the target as the ultimate fitness measure. To reduce evaluation cost during early search, a deep neural network surrogate/classifier is trained on photon-number distributions to rapidly classify candidate output states into target families (cat, squeezed cat, zombie, ON, cubic phase, or other), guiding the GA seeding. Using simulated quantum-optics models with increasing Hilbert-space truncation across three stages, the algorithm finds experimental schemes using an experimentally feasible toolbox and achieves >96% fidelity for five target state classes (two >99.7%). The work advances automated experiment-design in quantum optics by integrating evolutionary search with learned surrogates to speed exploration of a very large design space while maintaining final exact evaluation via fidelity.","The target-matching metric is the fidelity between pure states, defined as $F=|\langle\psi|\phi\rangle|^2$, which serves as the fitness in later GA stages. The DNN layer update is $\mathbf{x}_{i+1}=\sigma(M_i\mathbf{x}_i+\mathbf{b}_i)$, with a final softmax layer producing class probabilities. Example engineered-state expression (ON-state scheme) is $\mathcal{N}(\langle 8|\otimes I)\,\hat U_{T_4}\,|z_5\rangle_{12}$, where the bra denotes an 8-photon heralding measurement and $\mathcal{N}$ normalizes.","AdaQuantum produced experimental designs for five target state classes (cat, squeezed cat, zombie, ON, cubic phase) with fidelities 99.85%, 99.78%, 96.84%, 97.77%, and 96.11%, respectively (Table I). The neural network classifier achieved 99.3% accuracy on a 3,000-state test set with the shown confusion matrix; cubic phase states were the hardest class. Using the DNN in stage 1 is reported to be about two orders of magnitude faster than computing all discretized fidelities for multiple target families, enabling faster seeding of GA runs.","The authors note that the DNN provides only a “useful, albeit modest, speed-up” for the fidelity used here, framing it mainly as proof-of-principle for more demanding fitness functions. They also state that, unlike prior work, they do not include experimental noise in this paper and restrict to pure states and perfect operators/measurements. They acknowledge fidelity is lower than some related work because they constrain designs to be practical with currently available components.","The approach optimizes over simulated models with truncation heuristics; performance and fidelity may degrade when confronted with realistic imperfections (loss, mode mismatch, detector inefficiency/dark counts) and calibration constraints, and no robustness analysis is provided. The DNN uses only photon-number magnitudes (discarding phase), which may limit its effectiveness as a surrogate for broader classes of targets or for distinguishing states that share similar number distributions. The “design” is not DOE in the statistical sense: there is no treatment of uncertainty, replication, randomized run orders, or optimal allocation of physical experimental trials; it is primarily algorithmic circuit synthesis in simulation.","They state that future work will undertake a deeper analysis of the capabilities of AdaQuantum and the experiments and states it has produced. They also position the DNN-surrogate idea as paving the way to approximate more demanding fitness functions such as Bayesian mean squared error, with exact evaluation used afterward to validate and fine-tune.","A natural extension is to incorporate realistic noise models and hardware constraints directly into the fitness (including success probability/yield of heralded schemes) and to perform multi-objective optimization (fidelity vs. rate vs. resource cost). Another direction is a fully surrogate-assisted or active-learning loop where the DNN (or a regression surrogate) is updated online from GA evaluations and used to propose new candidates adaptively. Providing a self-contained, reproducible software environment (e.g., containers) and benchmarking against other experiment-synthesis methods would strengthen adoption and validation.",1812.03183v2,https://arxiv.org/pdf/1812.03183v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:51:48Z
FALSE,Computer experiment|Sequential/adaptive|Other,Prediction|Other,Not applicable,"Variable/General (examples include d=1,2,10 and up to d=50 in an application; also L response surfaces/actions)",Finance/economics|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,https://keras.io,"The paper proposes deep learning methods to rank multiple unknown response surfaces over a continuous input space by estimating the argmin surface index at each input, recasting the task as an image-segmentation/classification problem. Training data are generated by noisy Monte Carlo samples of each surface (or of continuation values in optimal stopping), and networks (feed-forward NNs and UNet-style architectures) output class probabilities via softmax/sigmoid. The authors empirically compare training data generation schemes—uniform grids, Latin hypercube sampling, and sequential design (Gap-SUR from prior work)—and study the impact of noisy labels induced by sampling noise. Across 1D, 2D, and 10D synthetic examples and Bermudan option pricing, they report that deep learning generalization accuracy is relatively insensitive to label noise and that uniform sampling can perform comparably to sequential design while being easier to parallelize. The method is applied to Bermudan max-call options by learning stop/continue decision maps backward in time, producing option prices close to benchmark values and scaling to higher dimensions (reported up to d=50).","The surface-ranking target is the classifier $C(x)=\arg\min_{\ell\in\{1,\dots,L\}} \mu_\ell(x)$ with noisy access $Y_\ell(x)=\mu_\ell(x)+\varepsilon_\ell(x)$. Performance is measured by a misclassification loss $\mathcal{L}(C,C_{dl})=\int_X \mathbf{1}\{C(x)\neq C_{dl}(x)\}\,\lambda(dx)$. In the Bermudan option application, the continuation value is $CV(t_i,x)=\mathbb{E}_{t_i,x}[V(t_{i+1},X_{t_{i+1}})]$ and the stopping rule is based on comparing $h(t_i,x)$ vs. $CV(t_i,x)$, with $CV$ estimated by Monte Carlo averaging along paths under the learned policy.","In the 1D example (binary classification), reported generalization accuracies are very high for uniform grids: about 99.7–99.9% with true labels and 98.5–99.5% with noisy labels for budgets $M=128$ to $512$. In the 2D example (5 surfaces), generalization accuracy improves with $M$ and is broadly similar between uniform and sequential designs; with noisy labels, UNet on a uniform grid at $M=576$ achieves about 96.44% generalization accuracy (vs. about 95.1% for a feed-forward NN under the same setting). For Bermudan max-call pricing (2D case), the estimated price is reported as 8.05 with standard deviation 0.029 (benchmark true value 8.075), using a 32×32 grid and $R=100$ replications per location. The approach is demonstrated for higher-dimensional max-call options with reported results up to $d=50$ and corresponding training/generalization runtimes.",None stated.,"This is not a DOE methodology paper; design choices (uniform grids, Latin hypercube, and a referenced sequential design) are evaluated empirically without formal optimal design criteria or guarantees. Reported comparisons are tied to specific synthetic functions and a Bermudan option setup, so conclusions about sampling-design insensitivity may not generalize to other response-surface classes, noise structures, or strongly anisotropic/highly localized decision boundaries. Practical deployment would likely require careful treatment of autocorrelation/time-series effects or non-i.i.d. simulation noise, which are not addressed in the presented experiments.","The authors propose studying replication/batching at locations near partition boundaries to reduce mislabeling from low signal-to-noise ratios, extending ideas from recent GP-based work. They also mention analyzing theoretical convergence for networks with more delicate architectures (e.g., UNet-like structures). Finally, they suggest extending the framework to continuum action spaces (continuous $L$), with discretization and potentially hybrid numerical methods (e.g., PDE solvers for pricing) to ensure convergence.","Develop principled active-learning/DOE criteria tailored to segmentation risk (e.g., boundary-focused acquisition with explicit noise/label uncertainty) and compare against space-filling designs under matched simulation budgets. Provide ablation studies on sampling designs (grid vs. LHS vs. adaptive) across a broader suite of benchmark functions and heteroscedastic/noisy simulators, and include calibration/uncertainty quantification for the learned classifier. Release reproducible implementations and benchmark protocols to enable fair comparisons across architectures and design strategies.",1901.03478v2,https://arxiv.org/pdf/1901.03478v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:52:23Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal,"Variable/General (feature dimension d; experiments vary d=20–400; real datasets have d=10–156, etc.)",Healthcare/medical|Service industry|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/neu-spiral/AcceleratedExperimentalDesign,"The paper studies experimental design for collecting a budgeted set of pairwise comparison labels (batch active learning) when comparisons are more informative/less noisy than absolute class labels but scale quadratically with dataset size. It formulates the selection problem as maximizing a monotone submodular D-optimality objective, i.e., a log-determinant criterion based on the information matrix formed from existing absolute labels and selected pairwise-difference features. While greedy selection gives a constant-factor approximation, naïve greedy is computationally prohibitive on O(N^2) candidate pairs; the authors derive accelerated greedy and lazy-greedy implementations that exploit pairwise structure, Cholesky factorization, and Sherman–Morrison updates to reduce complexity substantially. They evaluate runtime on synthetic data and multiple real datasets (including medical ROP) and show the accelerated methods can handle >10^8 candidate comparisons in under an hour, whereas naïve greedy would take >10 days. They also compare predictive performance of D-optimal selection to other submodular objectives (entropy, Fisher information, mutual information) and random selection, finding D-optimal is competitive while being far more tractable than MI/Fisher in large settings.","The design criterion is D-optimality: $f(S)=\log\det\big(\lambda I_d+\sum_{i\in A}x_ix_i^T+\sum_{(i,j)\in S}x_{i,j}x_{i,j}^T\big)$ with pairwise-difference covariates $x_{i,j}=x_i-x_j$. Greedy maximization uses marginal gains $\Delta(e\mid S)=\log(1+x_e^T A^{-1}x_e)$ (proxy gain $d_e=x_e^T A^{-1}x_e$) and updates $A^{-1}$ via Sherman–Morrison: $A^{-1}\leftarrow A^{-1}-\frac{A^{-1}x_ex_e^TA^{-1}}{1+x_e^TA^{-1}x_e}$. Accelerations rewrite $d_e=\|U x_e\|_2^2=\|U x_i-U x_j\|_2^2$ for $A^{-1}=U^TU$ and update gains using $d'_e=d_e-(z_i-z_j)^2$ where $z_i=v^Tx_i$ for a vector $v$ derived from the Sherman–Morrison update.","The main quantitative runtime claim is that selecting comparisons from a dataset with more than $1.125\times 10^8$ candidate pairs (e.g., synthetic $N=15000$, $d=400$) can be done in less than 1 hour using the accelerated lazy-greedy variants, whereas naïve greedy would require more than 10 days. Theoretical complexity is reduced from $O(N^2 d^2 K)$ (naïve greedy) to $O(N^2(K+d)+N(dK+d^2)+d^2K)$ (scalar greedy), with the dominant $O(N^2)$ per-iteration updates using only scalar operations. Empirically, factorization/scalar greedy provide up to ~2 orders of magnitude speedups over naïve greedy, and lazy versions can yield up to ~3 orders of magnitude improvement for smaller K, depending on feature dimension. In prediction experiments (e.g., ROP), D-optimal greedy selection improves over random selection and is competitive with Fisher/MI objectives, while MI and Fisher are often infeasible beyond small batch sizes due to much higher computational cost.","Mutual information and Fisher-information-based objectives are reported as computationally intractable at realistic scales in this setting, limiting comparisons to very small batch sizes (e.g., MI only for $K\le 8$–12 and Fisher only for $K\le 5$–10 in some datasets). The authors also note that the scalar lazy greedy variant can be disadvantaged when heap entries are “stale,” since updating a marginal gain may require applying multiple past updates, so it does not always outperform factorization lazy greedy in practice. They further observe that lazy greedy performance is sensitive to feature dimension (can perform poorly in low dimensions due to fewer early terminations).","The design criterion uses a log-det/Fisher-information approximation tied to linearized (Gaussian-noise) interpretations of a logistic/Bradley–Terry model; selection quality may degrade if this approximation is poor or if labels violate conditional independence/model assumptions. The approach presumes access to all candidate pairs (or their implicit enumeration) and relies on $O(N^2)$ memory/time patterns for maintaining gains over all pairs, which can still be challenging in extreme N without careful out-of-core/streaming implementations. The work focuses on batch (non-adaptive within-batch) selection; it does not analyze robustness to annotator noise variability, ties, missing comparisons, or settings with multiple annotators of differing reliability. Comparisons to alternative scalable active-learning heuristics (e.g., uncertainty sampling, clustering/coresets, approximate submodular maximization) are limited, and real-world validation beyond the included datasets is not extensive.","The authors suggest extending the acceleration ideas to other objectives that are structurally similar to D-optimality (e.g., A-optimality or E-optimality). They also propose investigating whether similar accelerations can be developed for objectives like mutual information, where even computing the function-value oracle is not tractable.","Developing approximate/streaming implementations that avoid explicit $O(N^2)$ storage of pairwise gains (e.g., using candidate pruning, sampling, or distributed computation) would improve scalability further. Extending the method to adaptive/sequential querying where comparisons are selected in multiple rounds with model refits could yield better label efficiency. It would also be valuable to study robustness under model misspecification (non-logistic noise, annotator bias), and to generalize to multi-class or multi-attribute comparisons and to settings with ties/partial orders. Providing a well-tested software package with reproducible benchmarks and memory-performance guidance would facilitate broader practical adoption.",1901.06080v1,https://arxiv.org/pdf/1901.06080v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:53:09Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Model discrimination|Cost reduction|Other,A-optimal|V-optimal|Not applicable|Other,Variable/General (linear regression in d dimensions; n≫d candidate experiments; subset size k),Theoretical/simulation only,Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a randomized experimental design framework for fixed-design least squares regression that unifies classical statistical A-optimal design (variance/MSE under homoscedastic noise) with worst-case/algorithmic regression guarantees. It introduces a minimax-optimality criterion that minimizes the worst-case (over arbitrary response distributions with finite second moment) excess MSE of an unbiased estimator relative to full-data least squares, normalized by the response noise energy. Methodologically, it proposes designs that combine (i) an initial size-d joint “volume sampling” (determinantal) step with (ii) additional i.i.d. samples drawn from a mixture distribution that includes leverage-score sampling and a new “inverse score sampling” distribution. The main results prove that with k = O(d log n + φ/ε) queries (φ = tr((XᵀX)⁻¹)), one can obtain an unbiased estimator with excess MSE at most ε·E||ξ||² for arbitrary response/noise, recovering near-tight A-optimal bounds as a special case and improving worst-case regression bounds. The paper also provides an improved algorithm for generating a volume-sampled set in O(d³ log d) time (given XᵀX and a sampling stream), strengthening the computational feasibility of such randomized designs.","Classical A-optimal subset objective: minimize tr((X_Sᵀ X_S)⁻¹) over |S|=k. Minimax design value: R*_k(X)=min_(S,ŵ) max_(y∉Sp(X)) (MSE(ŵ(y_S))−MSE(w_LS(y|X)))/E||ξ_{y|X}||², with unbiasedness E[ŵ(y_S)]=X†E[y]. The core randomized estimator is a rescaled-volume-sampling least squares estimator: draw π ~ VS^k_q(X) and set ŵ(y_S)=(S_π X)† S_π y, where Pr(π) ∝ det(Xᵀ S_πᵀ S_π X)/(det(XᵀX)∏_i q_{π_i}). Sampling uses leverage scores p^lev_i ∝ x_iᵀ(XᵀX)⁻¹x_i and inverse scores p^inv_i ∝ x_iᵀ(XᵀX)⁻²x_i mixed with uniform.","For any full-rank X with φ=tr((XᵀX)⁻¹), there exists a randomized design of size k=O(d log n + φ/ε) producing an unbiased estimator with excess MSE bounded by ε·E||ξ_{y|X}||² for any response distribution with finite second moment (including adversarial/fixed y). An analogous bound holds for mean squared prediction error (V-optimal-type objective) with sample size k=O(d log n + d/ε), removing dependence on φ. In the worst-case fixed-response setting, the result yields E||ŵ−w*||² ≤ ε·||Xw*−y||² in expectation with k scaling like O(φ/ε), improving prior leverage-score-based bounds that can be worse by up to a factor d. Computationally, the paper gives a new volume sampling algorithm that (under a mild domination condition q_i ≥ 0.5 p^lev_i) runs in O(d³ log d log(1/δ)) time with high probability, using O(d log d log(1/δ)) i.i.d. samples, improving earlier O(d⁴) sampling time.","The authors note that the logarithmic dependence on n in the sample size bounds (k = O(d log n + …)) arises from their tail-analysis technique; they state it may be possible to eliminate dependence on n altogether, but leave this as an open question. They also remark that their volume-sampling algorithm requires exact computation of XᵀX (typically O(nd²)), and while preprocessing improvements may be possible, they leave this as an open question.","The work is primarily theoretical and does not provide an empirical/real-data validation of the proposed minimax designs’ practical performance (e.g., robustness to model misspecification beyond second-moment assumptions, numerical stability, or sensitivity to approximate leverage/inverse-score computations). The design assumes access to the full candidate design matrix X (and often XᵀX) and focuses on linear least squares; extensions to generalized linear models or nonlinear responses are not addressed. The proposed randomized design depends on determinantal/volume sampling machinery that may be nontrivial to implement at scale, and the runtime guarantees rely on conditions like q dominating leverage scores and on preprocessing that can still be expensive for very large n.","They suggest as a natural direction extending Theorem 4 to biased estimators that exploit Bayesian prior information (since their main results focus on unbiased estimators). They also state it is an open question whether the log n dependence in the sample complexity can be removed, potentially even using the same sampling distribution. Finally, they note that preprocessing improvements for their faster volume sampling algorithm (reducing the need for exact XᵀX) may be possible and leave this open.","Develop practical algorithms and software that compute approximate leverage and inverse scores and implement the full randomized design pipeline with numerical safeguards, then benchmark against standard A-/V-optimal heuristics on real experimental design problems. Extend the minimax framework to settings with dependent/clustered errors, autocorrelation, or constraints typical in DOE (blocking, split-plot, random effects), and to non-Gaussian models (GLMs) where unbiasedness is not straightforward. Study exact/approximate optimization of the restricted minimax criterion over q (beyond the specific mixtures proposed) and characterize when inverse-score sampling provides the largest gains. Investigate robustness when X is only observed via streaming/sketches and when the candidate set is adaptively expanded (active learning/sequential design).",1902.00995v1,https://arxiv.org/pdf/1902.00995v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:53:41Z
TRUE,Sequential/adaptive|Computer experiment|Other,Prediction|Parameter estimation,Space-filling|Minimax/Maximin|Other,"Variable/General (examples with d=1,2,3,4; discussion for general d-dimensional simulators)",Theoretical/simulation only,Simulation study|Other,TRUE,R,Not provided,NA,"The paper proposes two new sequential design approaches for computer experiments aimed at improving global prediction accuracy (“global fitting”) of an expensive simulator response surface. The first method (MC) selects follow-up runs by maximizing an expected-improvement (EI) criterion that simultaneously targets estimation of multiple response contours by splitting the output range into k contour levels; the resulting EI is a sum of contour-estimation EI contributions across levels. The second method (SC var) adapts the contour level at each stage by choosing the predicted response at the candidate point with maximum predictive variance, then applying the single-contour EI to pick the next run. Both approaches are implemented with Gaussian process (GP) surrogates (constant mean, Gaussian correlation) and compared via simulation to maximin Latin hypercube (one-shot), sequential D-optimal (tgp), EI for global fit (Lam & Notz), and sequential minimum energy design (SMED). Across several test functions, the proposed methods generally reduce RMSPE and especially maximum prediction error relative to competitors under the same run budgets.","The GP surrogate uses $\hat y(x)=\mu + r(x)^\top R^{-1}(y-\mu 1)$ and predictive variance $s^2(x)=\sigma^2\{1-r(x)^\top R^{-1}r(x)\}$. For multiple contours at levels $a_1<\dots<a_k$, the improvement is $I(x)=\epsilon^2(x)-\min\{(y(x)-a_1)^2,\dots,(y(x)-a_k)^2,\epsilon^2(x)\}$ with $\epsilon(x)=\alpha s(x)$, and the expected improvement has closed form as a sum over contours (Eq. 16) using Normal pdf/cdf terms. The adaptive single-contour strategy sets the stage-specific contour level to $a_j=\hat y(x^*_{\text{opt}})$ where $x^*_{\text{opt}}=\arg\max s^2(x)$ over a candidate set, then maximizes the single-contour EI from Ranjan et al. (2008).","Performance is assessed by Monte Carlo simulation using RMSPE and maximum absolute error on a hold-out Latin hypercube test set (size $1000d$) across 50 replications per example. In the Branin function example (2D; budget 30 with $n_0=10$), the proposed MC (k=10) and SC var methods yield the lowest RMSPE and maximum error boxplots among the compared approaches. In a 3D multiplicative function (budget 60 with $n_0=20$), MC 10 and SC var are comparable to the best methods in RMSPE and are notably better in maximum error. In a 4D polynomial example (budget 80 with $n_0=27$), EIGF performs worst; MC 10 and SC var provide more accurate predictions in terms of maximum error while being competitive on RMSPE.","The authors note that if a surrogate other than a GP is used, the EI criteria would change and may not admit closed-form expressions for selecting follow-up points. They also caution (in discussing MC) that a substantial fraction of points can line up on pre-specified contours, which could yield biased designs if the contour levels $a_1,\dots,a_k$ are not chosen appropriately.","The methods rely heavily on GP modeling assumptions (e.g., smoothness/stationarity and near-normal predictive distributions) and independent deterministic simulator outputs; robustness to nonstationarity, discontinuities, heteroskedasticity, or noisy simulators is not fully explored. Optimization of the EI criteria is demonstrated on discrete candidate grids in examples, but scalability and optimization difficulty in higher dimensions (large d) may be substantial without specialized global optimization routines. Comparisons are limited to a small set of benchmark functions and particular budget/initial-design choices, and do not include other modern space-filling/sequential criteria (e.g., integrated variance reduction / IMSPE in sequential form, entropy-based criteria, or more recent acquisition functions).","The paper suggests extending the proposed contour-estimation-based sequential design ideas to computer experiments with both qualitative and quantitative factors, and to dynamic computer experiments. It also notes that using alternative surrogate models is possible, though the resulting EI criteria may not have closed-form expressions.","Develop self-adaptive strategies to choose k and contour levels (for MC) with theoretical or empirical guarantees, reducing sensitivity to arbitrary slicing of the response range. Extend the approach to noisy/stochastic simulators and to nonstationary or locally adaptive surrogates (e.g., treed/local GPs) with principled uncertainty quantification. Provide scalable implementations and optimization strategies for high-dimensional candidate spaces (e.g., gradient-based EI optimization, batch/parallel selection, and approximate GP methods). Validate on real, domain-specific simulators (beyond synthetic test functions) and release reproducible code to facilitate adoption.",1902.01011v1,https://arxiv.org/pdf/1902.01011v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:54:17Z
TRUE,Split-plot|Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Other,A-optimal|Other,Variable/General (m treatments; κ blocks; network adjacency defines interference),Network/cybersecurity|Food/agriculture|Pharmaceutical|Manufacturing (general)|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://snap.stanford.edu/data/egonets-Facebook.html,"The paper develops methodology to construct optimal block designs for experiments conducted on networks where units may interfere via connections (spillover/neighbor effects). It extends the linear network effects model to a Network Block Model (NBM) by including block effects alongside direct treatment effects and network (indirect) effects, and then constructs exact/near-optimal treatment allocations over network vertices. Designs are optimized using L-optimality criteria tailored to (i) minimizing average variance of all pairwise direct treatment differences (φ1) and (ii) minimizing average variance of all pairwise network-effect differences (φ2). Because exhaustive search can be infeasible, the authors adapt a modified Fedorov-style exchange procedure (Point Exchange on Networks, PEN) with multiple random starts to find near-optimal designs. They compare designs optimized under models with and without blocks/network effects and show that randomized balanced designs that ignore network structure can be highly inefficient (especially for φ2) and can yield biased treatment-effect estimates under model misspecification; they also propose defining blocks via spectral clustering and modularity to reflect network communities.","Network Block Model (NBM): $y_{ij}=\mu+\tau_{r(ij)}+b_i+\sum_{g=1}^{\kappa}\sum_{h=1}^{n(g)}A_{\{ij,gh\}}\,\gamma_{r(gh)}+\varepsilon_{ij}$, with iid errors. L-optimality objective: $\phi(\xi)=\mathrm{Tr}\big(S^T M(\xi)^{-1} S\big)=\mathrm{Tr}\big(M(\xi)^{-1}L\big)$ where $L=SS^T$ and $M$ is the information matrix from the NBM. Two specific criteria are defined: $\phi_1$ averages variances of all pairwise direct treatment contrasts, and $\phi_2$ averages variances of all pairwise network-effect contrasts, each expressible as sums of $s(v,q)^T M^{-1}s(v,q)$ over relevant contrast vectors.","Across illustrative networks (e.g., a 22-node coauthorship network and a 324-node Facebook ego-network), optimal designs accounting for network (and block) effects substantially outperform randomized balanced CRD/RBD designs, particularly for estimating network effects (criterion $\phi_2$). In Example 1 (22 nodes), under the NBM for $\phi_1$, the NBD achieves 0.1828 while the mean CRD and RBD values are 0.2500 and 0.2270, respectively; for $\phi_2$ under NBM, NBD is 0.0388 while mean CRD/RBD are 0.2354/0.2503. The paper also derives bias expressions under misspecification; e.g., ignoring network effects yields $E[\hat\tau_1]-\tau_1=-0.48\gamma_1-0.24\gamma_2$ for CRD under LNM and $-0.72\gamma_1-0.05\gamma_2$ for RBD under NBM in their example. For the Facebook example (324 nodes, κ=24 via modularity), the near-optimal values reported are $\phi_1^*=0.0124$ and $\phi_2^*=0.0002$, with the $\phi_2$-optimal design having notably unequal replication (117 vs 207 units for the two treatments).","The authors state their methodology is intended for small to moderate network sizes and note that extending/generalising the approach to large-scale networks with hundreds of thousands of nodes (e.g., commercial databases) remains an open research problem. They also note that certain network structures (e.g., regular graphs) do not allow estimation of network effects within their framework.","The optimality results depend on the correctness of the assumed linear interference model (additive neighbor effects via adjacency) and iid errors; performance under autocorrelation, heteroskedasticity, or misspecified interference mechanisms (directional/weighted edges, higher-order contagion) is not fully explored. The PEN exchange algorithm is heuristic and may converge to local optima; although multiple starts are used, there is no guarantee of global optimality nor complexity/runtime characterization across network sizes and treatment counts. Comparisons are largely based on selected example networks and randomization samples; broader benchmarking against alternative network-aware design strategies (e.g., graph cut/anti-cluster, Bayesian design, or robust criteria) is limited. Practical guidance on choosing κ (blocks) and handling uncertainty in community detection (clustering instability) is not developed beyond modularity maximization.","The authors explicitly identify scaling/generalising the methods to large-scale networks (hundreds of thousands of nodes) as an open research problem. They also frame broader adaptability to different adjacency specifications and block definitions as directions of interest, implying further methodological development for wider classes of networked experiments.","Develop robust or Bayesian design criteria that account for uncertainty in the network/interference model (e.g., weighted/directed edges, partial interference, higher-order spillovers) and in estimated community structure used for blocking. Extend the framework to correlated outcomes (temporal/spatial dependence on networks) and to unknown variance/Phase I estimation issues, including self-starting or sequential/adaptive redesign as data accrue. Provide theoretical guarantees or bounds for the PEN heuristic (or develop more scalable optimization such as coordinate descent with matrix updates, simulated annealing, or mixed-integer formulations) and publish reference software to facilitate adoption. Evaluate designs on larger and more diverse real datasets with standardized benchmarks and include diagnostics for checking interference/cluster assumptions post-experiment.",1902.01352v2,https://arxiv.org/pdf/1902.01352v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:55:00Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Model discrimination|Parameter estimation|Prediction|Other,Other,Variable/General (p variables/nodes in a causal DAG; interventions target subsets of nodes; many experiments use single-node interventions),Food/agriculture|Other|Theoretical/simulation only,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes ABCD-Strategy, a budgeted, batched Bayesian experimental design framework for selecting interventions to learn a targeted functional of an unknown causal DAG (e.g., descendants of a node) under constraints on samples and experimental rounds. The utility is based on mutual information (expected entropy reduction) between the target feature f(G) and outcomes from candidate interventions, generalized to multisets to handle repeated interventions and sample allocation within batches. Because exact optimization is intractable, the authors introduce tractable approximations (posterior sampling over DAGs via MCMC or DAG bootstrap, MLE plug-in for parameters, and importance sampling) and then apply greedy optimization with submodularity-based approximation guarantees. Empirically, ABCD outperforms random/baseline strategies in simulations on chain and Erdős–Rényi graphs, and shows gains on a real genomics benchmark (DREAM4) for targeted discovery (downstream genes).","The design selects a multiset of interventions $\xi$ to maximize expected utility $U_f(\xi;D)=\mathbb{E}_{y\sim P(y\mid D,\xi)}[U_f(y,\xi;D)]$ under constraints such as $|\xi|=N_b$ (samples per batch). The main utility is mutual information / entropy reduction: $U^{\text{M.I.}}_f(y,\xi;D)=H(f\mid D)-H(f\mid D,y,\xi)$, where $H$ is entropy under the Bayesian posterior over DAGs. For tractability they approximate the posterior with sampled DAGs and use importance weights $\hat w_i=P(y\mid G_i,\xi,\hat\theta^{\text{MLE}}_{G_i})$ to approximate the updated posterior after hypothetical outcomes.","Across simulated chain graphs and Erdős–Rényi DAGs, the ABCD strategy achieves substantially larger entropy reduction (posterior uncertainty reduction over graphs/features) than random or chordal-random intervention selection for the same sample/batch budgets. In ER graph simulations, with about 192 total samples over 3 batches and one unique intervention per batch, ABCD often learns graphs with near-complete certainty (entropy reduction near 1 in the plotted results). On the DREAM4 10-node gene network, ABCD shows improved performance over random for predicting downstream genes of several central nodes, though with high variability due to small sample sizes.","The paper notes that selecting an optimal design strategy is computationally intractable in general, motivating approximations (posterior sampling, MLE plug-in, importance sampling, greedy optimization). It also states that adaptive (non-uniform) batch sizes $N_b$ are not studied and are left for future work. For optimization guarantees, the authors prove the greedy/submodularity result only for the special case $f(G)=G$ (full graph recovery) and conjecture it extends to general target functionals.","The approach relies on model assumptions (e.g., correctness of the assumed parametric causal model such as linear Gaussian in places, independence, and correct intervention modeling), and performance may degrade under misspecification or hidden confounding. The empirical-Bayes/MLE plug-in approximation can underestimate uncertainty in small-sample regimes, which is precisely where experimental design is most needed. Practical deployment may be constrained by the need to repeatedly fit/simulate many candidate DAGs and compute utilities over large intervention families, which could be prohibitive for large p without strong sparsity/indegree bounds or engineered candidate sets.","They explicitly leave the study of adaptive batch sizes $N_b$ (optimizing how to partition samples across batches/rounds) for future work. They also conjecture that the submodularity/greedy-approximation guarantee (proved for $f(G)=G$) should hold for arbitrary target functionals $f$, indicating a theoretical extension direction.","Extending ABCD to handle latent confounders (e.g., MAGs/ADMGs) and model misspecification would improve applicability in real sciences. Developing self-starting/robust variants that do not require substantial initial observational data (or that account for uncertainty in the MEC more carefully) would strengthen practical use. Providing scalable open-source implementations and benchmarking against more modern active causal discovery methods (including continuous interventions, soft interventions, and constraint-based approaches) would clarify comparative benefits at larger scales.",1902.10347v1,https://arxiv.org/pdf/1902.10347v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:55:33Z
TRUE,Screening|Factorial (full)|Factorial (fractional)|Response surface|Computer experiment|Other,Screening|Parameter estimation|Model discrimination|Prediction|Cost reduction|Other,Space-filling|Not applicable,"Variable/General (example includes ~3 screening factors: $K_{aRCI}$, current limiting strategy, and $R_p$; mentions additional system variables like dispatch and fault/voltage profile)",Energy/utilities|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB|Other,Not provided,https://erigrid.eu/transnational-access/|https://erigrid.eu/dissemination/|https://webstore.iec.ch/publication/22349|http://sourceforge.net/projects/fmipp/,"The paper proposes a structured guideline/workflow for applying Design of Experiments (DoE) within the ERIGrid holistic testing procedure for cyber-physical energy systems (CPES) to improve rigor, comparability, and reproducibility of validation experiments. It explains core DoE concepts (factors/levels/treatments; treatment vs nuisance factors) and maps them to holistic testing artifacts (Purpose of Investigation, test criteria, test/experiment design) with emphasis on practices such as screening, randomization, blocking, and ANCOVA for nuisance factors. It provides a simulation-based example (fault ride-through testing of a wind power plant in an IEEE 9-bus system using co-simulation) illustrating a screening step using ANOVA to narrow treatment factors and set feasible level ranges. The example varies key control parameters (e.g., additional reactive current injection gain $K_{aRCI}$, converter current limiting strategy, and active power recovery rate $R_p$) and observes impacts on voltage and generator speed/frequency deviation, concluding that some factors should be treated as primary (e.g., $K_{aRCI}$, $R_p$) while others can be blocked (e.g., q-axis current prioritization). Overall, the contribution is methodological guidance rather than proposing a new optimal design; it synthesizes how to choose designs and analyses (ANOVA/regression) in CPES testing contexts and advocates iterative refinement via screening within a standardized documentation process.","Key method-defining relations appear in the example as converter current limiting and control parameters rather than a formal design construction. The current magnitude constraint is described as $i_d^{*2}+i_q^{*2}\ge |I|_{\mathrm{lim}}$, requiring curtailment along either the active d-axis or reactive q-axis (limiting strategy factor). Additional reactive current injection is governed by a proportional gain factor $K_{aRCI}$ (varied from 0 to 2.0), and active power recovery is governed by a ramp-rate limit parameter $R_p$; responses include terminal voltage $|U|_{PCC}$ and generator speed/frequency deviation $\Delta\omega_{G1}$.","In the screening simulations, increasing $K_{aRCI}$ slightly elevates retained voltage during fault ride-through, but the terminal voltage response is largely dominated by synchronous machine excitation systems; for frequency/rotor-speed deviation, $K_{aRCI}\ge 1.0$ clearly attenuates the amplitude of $\Delta\omega_{G1}$ (improved rotor angle stability). Comparing converter current limiting strategies at $K_{aRCI}=1.0$, prioritizing reactive current ($i_q^*$) is shown to be superior to d-axis priority for the frequency response, leading the authors to treat this factor as a block for further analysis. Varying active power recovery rates $R_p$ shows the frequency response is strongly impacted; very high values (>10 pu/s) are argued infeasible due to protection constraints, while very low values substantially impair frequency response and can interfere with conventional plant governing systems, motivating a limited feasible range and treatment-factor status for $R_p$.","The paper notes that its guidance cannot provide standardized application cases because both statistics and CPES testing contexts are too diverse. In the example screening, it explicitly states that remote faults causing shallow voltage dips were not considered, and that this could change the observed significance of $K_{aRCI}$ on voltage response. It also focuses on the screening part “for the sake of brevity,” implying the full experiment design step is not fully developed in the paper.","The work is primarily a high-level guideline plus a single illustrative co-simulation case, so it does not validate the proposed workflow across multiple real CPES laboratories or datasets where stochasticity, measurement noise, and operational constraints are substantial. It discusses many design families (e.g., Plackett–Burman, CCD, Latin hypercube) but does not provide concrete, end-to-end design construction choices (run size, alias structure, replication strategy, power calculations) for the example, limiting practical prescriptiveness. The example’s conclusions about factor importance may be sensitive to the chosen system (IEEE 9-bus), fault scenario, model fidelity, and deterministic simulation assumptions, and the paper does not quantify uncertainty or robustness of the screening outcomes.","The authors state that ERIGrid will continue improving international CPES validation and that more guideline material is expected to consolidate DoE application in the CPES domain. They specifically mention future guideline improvements including a discussion of threats to validity (internal vs external validity) and how DoE concepts like randomization or blocking mitigate some threats, while other threat definitions require further debate for lab-based and non-human testing contexts.","A useful extension would be to provide concrete design recipes for common CPES test classes (e.g., recommended screening designs, replication/randomization schemes, and blocking strategies under lab constraints), including power and sample-size guidance. Another direction is to evaluate the workflow empirically across multiple laboratories with the same test case to quantify reproducibility gains and to handle non-idealities such as unknown parameters, autocorrelation, and model-form uncertainty. Publishing reference implementations (templates plus analysis scripts) as open-source tooling would also improve adoption and enable benchmarking of design/analysis choices (ANOVA/regression/ANCOVA) on standardized CPES scenarios.",1903.02062v1,https://arxiv.org/pdf/1903.02062v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:56:18Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Optimization|Prediction,Bayesian D-optimal|Other,"Variable/General (design d can be continuous/discrete; examples include A/B group sizes, query points, survey questions, stimulus selection; some examples use 1D to 6D design spaces)",Healthcare/medical|Finance/economics|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Personal website,http://docs.pyro.ai/en/stable/contrib.oed.html|https://github.com/ae-foster/pyro/tree/vboed-reproduce,"This paper develops fast variational estimators of expected information gain (EIG) for Bayesian optimal experimental design (BOED), addressing the nested-expectation intractability that makes standard nested Monte Carlo (NMC) slow (best-case RMSE rate O(T^{-1/3})). It introduces four amortized variational EIG estimators—variational posterior (lower bound), variational marginal (upper bound), a variationally-assisted NMC estimator (upper bound but asymptotically consistent as inner sample size grows), and an implicit-likelihood compatible marginal+likelihood estimator. The authors provide theoretical convergence results showing O(T^{-1/2}) rates to the variational optimum for the bounds (and consistency for VNMC as the refinement budget increases), and empirically compare against NMC, Laplace, LFIRE, and DV baselines on several BOED benchmarks. End-to-end sequential BOED demonstrations include adaptive selection of stimuli in a mixed-effects human-subject experiment and a sequential economics (CES utility) design problem, showing faster posterior entropy reduction than random or NMC-based design. The methods are implemented in Pyro’s optimal experimental design (OED) module with documentation and reproduction code provided online.","The design utility is the expected information gain $\mathrm{EIG}(d)=\mathbb{E}_{p(y,\theta\mid d)}\big[\log p(y\mid\theta,d)-\log p(y\mid d)\big]$, and the optimal design is $d^*=\arg\max_d \mathrm{EIG}(d)$. Proposed estimators replace intractable terms with amortized variational approximations: (i) a posterior bound $\mathcal{L}_{\text{post}}(d)=\mathbb{E}[\log q_\phi(\theta\mid y,d)-\log p(\theta)]$ (lower bound); (ii) a marginal bound $\mathcal{U}_{\text{marg}}(d)=\mathbb{E}[\log p(y\mid\theta,d)-\log q_\phi(y\mid d)]$ (upper bound); and (iii) VNMC uses an importance-sampling estimate of $p(y\mid d)$ inside an NMC-style log term: $\hat\mu_{\text{VNMC}}(d)=\frac1N\sum_n\big(\log p(y_n\mid\theta_{n,0},d)-\log\frac1M\sum_m \frac{p(y_n,\theta_{n,m}\mid d)}{q_\phi(\theta_{n,m}\mid y_n,d)}\big)$.","The paper reports that the proposed variational estimators achieve substantially lower empirical MSE (bias^2 + variance) than standard NMC across multiple BOED benchmarks under fixed time budgets (Table 2), and that VNMC can continue improving beyond the bias plateaus of pure variational bounds. Theoretically, when the variational family contains the target distribution, the variational estimators converge at $O(T^{-1/2})$ (with $T=O(N+K)$ for MC samples and gradient steps), matching conventional Monte Carlo rates rather than NMC’s $O(T^{-1/3})$. In sequential design experiments, BOED using the proposed estimators reduces posterior entropy faster than random design and outperforms an NMC-based BOED baseline in the CES utility example (Figure 4). The methods also support implicit-likelihood models, demonstrated on mixed-effects settings where likelihood evaluation is intractable.","The authors note that variational bounds can converge to biased EIG estimates when the chosen variational family does not contain the true posterior or marginal, causing RMSE to plateau as MC variance is reduced. They also state that the $O(T^{-1/2})$ optimization result relies on strong/stardard stochastic optimization assumptions and that in practice optimization may converge to a local optimum rather than the global optimum, introducing additional asymptotic bias. They further highlight that some estimators require explicit likelihood evaluation (e.g., $\hat\mu_{\text{marg}}$ and $\hat\mu_{\text{VNMC}}$), whereas others are needed for implicit likelihood settings.","Most evaluations are on a limited set of benchmark models and do not fully characterize robustness to model misspecification, heavy-tailed noise, strong posterior multimodality, or temporal dependence/autocorrelation common in sequential/online experiments. The sequential design demonstrations optimize EIG via Bayesian optimization, but the overall real-time performance may depend heavily on BO settings, surrogate choices, and the cost of retraining amortized networks as posteriors evolve. Practical guidance for choosing variational families/architectures and diagnosing approximation failure (e.g., calibration checks for $q(\theta\mid y,d)$ or $q(y\mid d)$) is not deeply developed.",None stated.,"Natural extensions include broader robustness studies under misspecified priors/likelihoods and multimodal posteriors, and development of diagnostics/adaptive variational-family expansion to mitigate bound bias. Extending the framework to constrained, combinatorial, or high-dimensional design spaces with gradient-based design optimization (instead of Bayesian optimization) could improve scalability. Additional real-world case studies and packaged, reproducible implementations (e.g., PyPI/CRAN-style releases, standardized benchmarks) would help validate and operationalize the methods for practitioners.",1903.05480v3,https://arxiv.org/pdf/1903.05480v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:57:07Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Prediction|Other,Bayesian D-optimal|Other,"Variable/General (design variables denoted d∈R^{n_d}; examples include 1 design variable d∈[0,1] and 3 velocity points in the Mössbauer example)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a computational method for focused optimal Bayesian experimental design, where the utility is the expected information gain (EIG) in a targeted subset of parameters (parameters of interest) while marginalizing nuisance parameters. It proposes a layered multiple importance sampling (LMIS) estimator that reuses samples and cached forward-model evaluations across outer-loop data realizations to build increasingly tailored biasing distributions (via multiple importance sampling mixtures), reducing estimator bias and variance compared with prior-based nested Monte Carlo. The approach targets nonlinear forward models and non-Gaussian posteriors, and remains asymptotically exact/consistent for EIG estimation. Performance is demonstrated on linear-Gaussian examples (with known closed forms) and on a nonlinear Mössbauer spectroscopy design problem, showing orders-of-magnitude MSE reductions for the same number of model evaluations. The design criterion generalizes Bayesian D-optimal design via mutual information/KL divergence (prior-to-posterior) and supports “focused” learning trade-offs between parameter subsets.","The focused expected utility is the expected KL divergence from the prior marginal to the posterior marginal of parameters of interest: \(U(d)=\mathbb{E}_{y\mid d}[D_{\mathrm{KL}}(p_\theta(\theta\mid y,d)\,\|\,p_\theta(\theta))]\), equivalently mutual information between \(\theta\) and \(y\) given \(d\). A nested Monte Carlo estimator is \(\hat U(d)=\frac{1}{N}\sum_{i=1}^N \{\log \hat p(y^{(i)}\mid \theta^{(i)},d)-\log \hat p(y^{(i)}\mid d)\}\), where \(\hat p(y\mid d)\) and \(\hat p(y\mid\theta,d)\) are importance-sampling estimates of marginal and conditional likelihoods using biasing distributions; LMIS constructs these biasing distributions as multivariate-\(t\) approximations from reused-sample posterior moment estimates via multiple importance sampling mixtures.","In linear-Gaussian examples (including higher-dimensional settings), LMIS produces much smaller bias and variance than using the prior as the importance-sampling biasing distribution, and can achieve multiple orders-of-magnitude reductions in mean squared error for the same number of forward-model evaluations (figures show LMIS approaching performance of ‘exact posterior’ biasing). The paper also derives leading-order bias and variance scalings for the nested estimator, showing bias \(\sim C_1/M_1 - C_2/M_2\) and variance terms \(\sim D_3/N + D_1/(NM_1)+D_2/(NM_2)\), motivating improved biasing distributions and sample-allocation strategies. In the Mössbauer spectroscopy design (nonlinear, non-Gaussian posteriors), LMIS reduces MSE of EIG estimates by up to about two orders of magnitude versus prior-based estimation at comparable work, and yields different optimal designs depending on which parameter is targeted (e.g., \(d^*_{\text{center}}\approx(-1.3,0,1.3)\) vs \(d^*_{\text{offset}}\approx(-2,0,2)\)). Diagnostics based on customized effective sample size show substantially improved importance-sampling efficiency under LMIS.",None stated.,"The LMIS performance depends on how well the chosen parametric family for biasing distributions (multivariate t based on estimated posterior moments) approximates potentially complex, multimodal, or strongly non-elliptical posteriors; poor approximations can degrade inner-loop efficiency and bias reduction. The method is computationally tailored to settings where forward-model evaluations are expensive but likelihood evaluations for new y given cached model outputs are cheap; if this assumption fails, overhead may be higher. The sequential mixture construction can introduce nontrivial bookkeeping and (without pruning) quadratic growth in cached likelihood evaluations, and even with pruning may be sensitive to ordering/pruning heuristics in challenging tail regions.","The authors suggest extensions including using surrogate models to accelerate estimation (e.g., in the outer layer while preserving consistency), using Laplace approximations when gradients are available to build biasing distributions for marginal/conditional likelihood estimation, and combining surrogate/approximate estimators with full-model LMIS using control variates or multilevel Monte Carlo. They also suggest pairing LMIS with stochastic optimization methods to search the design space more effectively, and applying focused design to models with discrepancy terms (treating discrepancy parameters as nuisance or as the focus).","Developing robust/adaptive mechanisms to select the biasing family (e.g., mixtures, normalizing flows) and automatically tune degrees of freedom/mixture size could improve performance for highly non-Gaussian or multimodal posteriors. Providing a self-starting or online variant that updates designs sequentially as real data are collected (fully adaptive experimental design) would broaden applicability beyond offline optimization. Releasing a reference implementation and benchmarking against other modern EIG estimators (e.g., variational MI bounds, AIS/bridge sampling-based EIG, probabilistic numerics) on standardized testbeds would strengthen practical adoption.",1903.11187v1,https://arxiv.org/pdf/1903.11187v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:57:44Z
TRUE,Bayesian design|Sequential/adaptive|Other,Prediction|Parameter estimation|Other,Other,"Variable/General (design variable is measurement times; typical designs compare 3 vs 5 vs 9 time points; times chosen on a 15-minute grid over a 2-hour window, with utility integrating prediction error over 0–3 hours)",Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,http://arxiv.org/abs/1601.04753|http://arxiv.org/abs/1903.11697|http://dx.doi.org/10.1214/10-ba60,"The paper develops a Bayesian experimental design approach to improve Oral Glucose Tolerance Tests (OGTT) by choosing better blood-glucose sampling times. Using a mechanistic ODE model of glucose–insulin–glucagon dynamics with Gaussian measurement error, the design variable is the set of sampling times, and designs are ranked by expected utility under a predictive prior. The authors propose a Monte Carlo estimator of expected utility (nested simulation: simulate patients/data from the design prior, run MCMC for posterior inference under an inference prior, then estimate utility) and provide an asymptotic-normal framework to quantify uncertainty when comparing two designs (a z-test based on estimated variances). They search over a discrete grid of candidate time schedules (15-minute increments) and recommend a 5-point design (0:00, 0:45, 1:15, 1:45, 2:00 hours) that outperforms the conventional 3-point design (0, 1, 2 hours). Performance is validated via simulation against random schedules and via a real-data comparison on 17 patients with dense (15-minute) measurements, showing substantial accuracy gains for most patients.","The expected-utility objective is $U(d\mid\pi)=\int u(y\mid d)\,\pi(y\mid d)\,dy$ with predictive prior $\pi(y\mid d)=\int \pi(y\mid\theta,d)\,\pi(\theta)\,d\theta$, where the design $d=(t_1,\dots,t_n)$ are sampling times. Data model: $y_i=G(t_i)+\varepsilon_i$, $\varepsilon_i\sim\mathcal N(0,\sigma)$, with $G(t)$ generated by the ODE system in Section 2.1. Utility used in implementation is negative integrated squared error of the glucose curve over $t\in[0,3\,\text{hours}]$ (implemented as a Monte Carlo estimate using posterior samples).","A semi-brute-force search over schedules on a 15-minute grid (within a 2-hour test window) selected the proposed 5-time-point design: 0:00, 0:45, 1:15, 1:45, 2:00 hours, compared to the conventional 0, 1, 2 hours. Design comparison used Monte Carlo with up to $T_1=600$ simulated patients per design; posterior inference per simulated patient used t-walk MCMC with 1500 iterations. In a validation study over 17 real patients with full 15-minute data, the proposed design usually improved surrogate utility by a factor of about 2 or more versus the conventional design, with only 2 patients favoring the conventional design; the proposed design’s utility was never below 82% of the conventional design’s utility. Random-design simulations (100 random designs for each of 4/5/6 points) showed the proposed design was never significantly worse than the random alternatives and sometimes much better.","The authors note the approach is computationally intensive and the optimization is done over a coarse discrete grid (15-minute increments) rather than continuous time, partly due to practical constraints in clinical sampling. They also note their utility estimator is only asymptotically unbiased (not unbiased at finite posterior-sample size), motivating use of a large inner MCMC size $T_2$. They also highlight an unresolved issue: when pairwise design comparisons are inconclusive, an efficient sequential testing procedure for increasing $T_1$ to force a decision is not fully developed.","The recommended sampling schedule is optimized under a particular ODE model and specific prior choices (including the unusual separation of design prior $\pi_D$ and inference prior $\pi_I$); misspecification of the physiology model, error model (independent Gaussian with fixed $\sigma=5$), or population heterogeneity could change the optimal times. The design criterion focuses on reconstructing/predicting the entire glucose trajectory (integrated MSE) rather than directly optimizing diagnostic classification performance (e.g., diabetes vs non-diabetes sensitivity/specificity), which may be the real clinical endpoint. The search is restricted to a fixed 2-hour sampling horizon and discrete 15-minute grid, so potentially better continuous-time or longer-horizon designs are not explored. No publicly released code or fully reproducible pipeline is provided, which may hinder independent verification and adoption.","They propose developing an efficient algorithm for sequential testing/design comparison when an initial comparison is inconclusive and $T_1$ must be increased to force a decision. They also suggest more formal treatment of using different priors for design and inference, potentially framing prior selection within decision theory (analogous to work on reference priors).","Evaluate alternative utilities aligned with clinical goals (e.g., maximizing diagnostic accuracy or expected value of information for a diabetes decision threshold) and compare resulting optimal schedules. Extend the design to account for unknown/estimated measurement noise, correlated errors, or practical constraints such as missed/late draws and patient burden costs (economic/constraint-based design). Investigate robustness across model variants (e.g., different minimal models, covariates like BMI/sex) and consider adaptive designs where later sampling times depend on early readings. Provide an open-source implementation (Python/R) with standardized benchmarks to facilitate reproducibility and clinical translation.",1903.11697v1,https://arxiv.org/pdf/1903.11697v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:58:20Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Prediction|Cost reduction,D-optimal|Other,"Variable/General (design dimension $d$, parameter dimension $p$); examples include 2 parameters (Poisson), 3 parameters (pharmacokinetic), 2 parameters with 500 design points (geostatistical regression).",Healthcare/medical|Pharmaceutical|Environmental monitoring|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/dennisprangle/AdversarialDesignCode,"The paper proposes fast Bayesian optimal experimental design methods that avoid repeated posterior calculations by using Fisher-information-based objectives that are available in closed form for many models. It derives a “Fisher information gain” (FIG) objective, $J_{\mathrm{FIG}}(\tau)=\mathrm{tr}\,\bar I(\tau)$ with $\bar I(\tau)=\mathbb{E}_{\theta\sim\pi}[I(\theta;\tau)]$, via a decision-theoretic framework using the Hyv\""arinen scoring rule, and shows FIG can be optimized efficiently with stochastic gradients and automatic differentiation. The authors identify FIG’s drawbacks (non-invariance to reparameterization and designs that over-infer a single parameter combination, often via excessive replication) and introduce an adversarial/game-theoretic extension in which a critic chooses the least favorable linear reparameterization. The resulting subgame-perfect-equilibrium design criterion is equivalent to maximizing $J_{\mathrm{ADV}}(\tau)=\det\bar I(\tau)$ (a Bayesian D-optimality form) and is invariant to linear reparameterizations, encouraging balanced information across parameter combinations. Simulation studies on pharmacokinetic design and a high-dimensional geostatistical regression example demonstrate large speedups (reported as at least 10× faster than competing SIG-based methods) and scalability to hundreds of design variables; code is provided.","Bayesian design is posed as maximizing expected utility $J(\tau)=\mathbb{E}_{(\theta,y)\sim\pi(\theta,y;\tau)}[U(\tau,\theta,y)]$. Fisher information is $I(\theta;\tau)=\mathbb{E}_{y\sim f(\cdot|\theta;\tau)}[u(y,\theta;\tau)u(y,\theta;\tau)^T]$ with score $u=\nabla_\theta\log f(y|\theta;\tau)$. FIG uses $J_{\mathrm{FIG}}(\tau)=\mathbb{E}_{\theta}[\mathrm{tr}\,I(\theta;\tau)]=\mathrm{tr}\,\bar I(\tau)$, while the adversarial/game formulation yields $J_{\mathrm{ADV}}(\tau)=\det\bar I(\tau)$ and an equivalent minimax form $\min_{\tau}\max_{A: \det A=1}K(\tau,A)$ where $K(\tau,A)=-\mathbb{E}_{\theta}\,\mathrm{tr}(A^T I(\theta;\tau)A)$.","In the Poisson illustration, the FIG optimum is extreme (e.g., $\tau=1$), giving no information on one parameter, while SIG/ADV yields a balanced design (reported as $\tau=1/2$) and ADV is invariant to linear reparameterizations whereas FIG is not. In the pharmacokinetic simulation, the adversarial approach is reported as at least 10× faster than competing SIG-based methods, with mean runtimes around 1–2 seconds per design versus much larger times for ACE and tens of seconds for PCE (as tabulated). The ADV designs concentrate observations into a few time clusters and yield posteriors concentrated in multiple parameter directions, whereas FIG collapses to a single-time design producing diffuse marginals for some parameters. The geostatistical regression example reports optimizing 500 location choices in under a minute (about 19.4 seconds on average), illustrating scalability of the gradient-based minimax method.","The authors note their main methods assume the Fisher information matrix (and gradients) are tractable; when FIM is intractable they require alternative estimators (Appendix G) and discuss difficulties when even the likelihood/score is unavailable. They also state gradient-based methods require a continuous design space, so discrete design problems are not directly addressed. They further note the approach does not apply to discrete parameters because Hyv\""arinen score and Fisher information are defined for continuous parameters, and mention potential issues such as GDA dynamics converging to limit cycles and the presence of multiple local optima (mitigated by multiple starts and post-processing like point exchange).","Most empirical comparisons are based on a small number of benchmark models and selected competing algorithms; conclusions on general superiority may depend on implementation/tuning choices and convergence criteria across methods. The adversarial framework guarantees invariance only to linear reparameterizations (not general nonlinear transformations), so sensitivity may remain under broader reparameterizations. Practical use may be limited when prior expectations of Fisher information are expensive/high-variance to estimate in complex simulators or models with strong constraints, and the paper does not provide a standardized stopping/convergence test for minimax optimization beyond diagnostics and multiple restarts.","They propose extending the approach to settings with intractable Fisher information and especially when likelihood/score are unavailable due to latent variables or other intractabilities, as outlined in appendices. They suggest developing analogous methods for discrete design spaces, and investigating discrete-parameter analogues of Hyv\""arinen/FIM-based objectives. They also mention variance-reduction techniques for Monte Carlo gradients (e.g., randomized quasi-Monte Carlo), exploring alternative optimizers with better convergence guarantees (e.g., two-timescale updates), and modifying the game-theoretic framework (alternative scoring rules or allowing more general/nonlinear or data-dependent critic reparameterizations).","A useful extension would be systematic robustness studies under model misspecification (e.g., wrong noise model, autocorrelation) to see how FIG/ADV behave relative to SIG in practice. Providing packaged, well-tested software (e.g., a PyPI/CRAN package) and standardized benchmark suites would improve reproducibility and adoption. It would also be valuable to study theoretical and empirical behavior under constrained designs (hard-to-change factors, costs, feasibility regions) and to extend the adversarial criterion to multivariate responses and hierarchical/random-effects models common in biomedical applications.",1904.05703v5,https://arxiv.org/pdf/1904.05703v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:59:03Z
TRUE,Optimal design|Screening|Computer experiment|Other,Screening|Parameter estimation|Cost reduction|Other,D-optimal|Not applicable,Variable/General (big data solutions imply many factors; human-factor experiments note a maximum number of attributes shown to the experimentee; examples mention 2-level DoE/MSI combinations).,Network/cybersecurity|Environmental monitoring|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper proposes an ontology-based approach to formalize and support the Design of Experiments (DoE) for evaluating complex big data solutions, motivated by the combinatorial explosion of factors and factor levels arising from the “4Vs” (volume, velocity, variety, veracity). It introduces an ontology that (i) supports decomposition of a big data system into components and recombination of component-level results to system-level assessments, and (ii) translates DoE knowledge into terminological (T-Box) constraints to help select appropriate experiment designs and exclude inconsistent design choices. The approach distinguishes deterministic subsystems from non-deterministic (human-factor) subsystems and encodes implications such as treating any system with a non-deterministic component as non-deterministic for DoE selection. It leverages domain constraints (e.g., infeasible factor-level combinations and logical relations among Maritime Situational Indicators) to reduce the experimental design space, and discusses candidate designs such as full factorial, Plackett–Burman screening designs, and optimal designs (including D-optimal) under constraints like limited runs per experimentee. An application example is given for a maritime situational awareness big data solution for detecting/predicting suspicious vessel behavior using mobility analysis and MSIs.","The paper encodes DoE-selection logic and system properties as ontology axioms, e.g., a system is non-deterministic if it has any non-deterministic component: $\text{NonDeterministicSystem} \equiv \exists\,\text{hasComponent}.\text{NonDeterministic}$. It defines when replication, blocking, and randomization are required based on subject-of-experiment and nuisance factors: $\text{DoEWithReplication} \equiv \text{DoE} \sqcap \exists\,\text{hasSubjectOfExperiment}.\text{NonDeterministic}$; $\text{DoEWithBlocking} \equiv \text{DoE} \sqcap \exists\,\text{hasNuisanceFactor}.\text{Controllable}$; $\text{DoEWithRandomization} \equiv \text{DoE} \sqcap \exists\,\text{hasNuisanceFactor}.\text{Uncontrollable}$. It also gives example aggregation formulas for recombining component results, e.g. completeness as a conjunction/product over components and throughput as the minimum component throughput: $\text{Throughput}_{\text{system}}=\min(\text{Throughput}_{C_i})$.",NA,"The authors state the approach “is not claiming comprehensiveness” and position it as an initialization/enrichment of concept descriptions to make DoE knowledge accessible via semantic web technologies. They note that creating data with external validity for scenario-level (human-factor) experiments is not fully described and is deferred to an upcoming publication. They also indicate that the conclusion only sketches future work rather than completing all aspects (e.g., recombination for aggregating properties like latency/processing time).","The work does not provide a full empirical validation (e.g., simulation-based power/ARL-type metrics or statistical efficiency comparisons) showing that the ontology-driven exclusion of designs leads to better or more efficient experimental outcomes versus standard DOE workflows. The mapping from real experimental requirements to formal ontology constraints may be labor-intensive and sensitive to modeling choices (ontology completeness/consistency), which could limit adoption without tooling. The paper discusses candidate design families (factorial, Plackett–Burman, D-optimal) but does not specify a concrete end-to-end algorithm for design construction under constraints (beyond logical exclusion) nor demonstrate performance on a fully instantiated large-scale design problem. Treatment of stochasticity beyond “human-factor implies non-deterministic” (e.g., algorithmic randomness, autocorrelation in data streams) is not developed into specific DOE guidance.","They propose learning domain restrictions from data (augmenting “perfect-world” constraints with constraints reflecting imperfect real data and system behavior). They also suggest using learned/formalized confounding rules from domain data to support specification of fractional factorial designs (mandatory selection of aliased effects). They note that additional future work is needed to describe recombination of system properties that aggregate (e.g., latency or processing time) when synthesizing component-level evaluations to system-level results.","Develop a practical design-construction workflow that combines ontology constraints with optimization (e.g., constrained D-optimal/I-optimal design generation) and evaluate it on large, realistic component-factor graphs. Provide software tooling (e.g., a reasoner-driven DOE assistant) and benchmarks demonstrating run-count reductions and maintained statistical power/validity compared with baseline DOE practice. Extend the ontology to handle time-series/streaming effects common in big data (autocorrelation, concept drift) and to distinguish stochastic computational components from purely human-factor variability, with corresponding DOE recommendations. Add more real-world case studies beyond maritime surveillance to demonstrate generality and to validate that the suggested constraints improve reproducibility and interpretability in practice.",1904.08626v1,https://arxiv.org/pdf/1904.08626v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T02:59:37Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction|Other,Not applicable,2 factors (heating power $P_h$ and experiment duration $t_{QUB}$; with boundary/initial conditions treated as inputs),Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a design-of-experiments procedure for the Quick U-building (QUB) method, a short-duration (typically overnight) excitation test used to estimate a building’s overall heat transfer coefficient $H$ from the indoor temperature response to a rectangular heating/cooling power input. DOE is formulated as selecting the heating power $P_h$ and the heating/cooling duration $t_{QUB}$ to minimize total error in the estimated $H_{QUB}$, combining an intrinsic method error (difference between $H_{QUB}$ and a reference $H$ computed from a detailed state-space/thermal-network model) with propagated measurement error. The method computes building step responses from the state-space model, decomposes the response into sums of exponentials using eigenvalues/time constants, and produces contour maps of relative error versus $(P_h, t_{QUB})$ to guide experiment selection under practical constraints (max power/allowable temperatures, stable outdoor conditions). Two illustrative applications (a single-zone bungalow and a multi-zone typical house with multiple boundary temperatures including ground) show valleys of near-optimal designs and justify why QUB can be short relative to the largest building time constants due to dominance of medium time constants with significant amplitudes. Recommended designs in examples include heating times of several hours and powers on the order of 2–2.5 kW to keep total errors below ~10% under plausible sensor uncertainties.","QUB uses a 1st-order model $C\, d\theta_i/dt = P - H_{QUB}(\theta_i-T_o)$ and estimates $H_{QUB}$ from heating/cooling slopes: $H_{QUB} = \frac{P_h\alpha_c - P_c\alpha_h}{\Delta T_{0h}\alpha_c - \Delta T_{0c}\alpha_h}$. Total error combines intrinsic method error and measurement error propagation: $\varepsilon_H = \sqrt{(H_{QUB}-H)^2 + \varepsilon_{Hm}^2}$, with $\varepsilon_{Hm}$ computed via a Taylor-series uncertainty propagation over $(\alpha_h,\alpha_c,P_h,P_c,\Delta T_h,\Delta T_c)$. Step responses used for DOE are generated from the state-space model via $x(t)=e^{At}x(0)+A^{-1}(e^{At}-I)Bu$, with $e^{At}=V e^{\Lambda t} V^{-1}$ enabling a sum-of-exponentials/time-constant interpretation.","For the single-zone bungalow example, the intrinsic-error contour map indicates near-zero intrinsic error for heating times >~4 h across powers, while inclusion of measurement error shows robust regions requiring roughly $t_{QUB}\gtrsim 3$ h and $P_h\gtrsim 2000$ W. In that illustrated setting the QUB estimate reported is $H_{QUB}=61.2$ W/K, compared against a reference/model value of about 52.4 W/K, and the authors note prior studies typically find QUB vs. co-heating differences around 5–10%. For the typical two-floor house example (with outdoor temperature 19.5°C), the suggested optimal design is about $P_h\approx 2500$ W and $t_{QUB}\approx 6$ h, yielding about 7% error (dominated by ground-floor effects) and an equivalent indoor temperature of 20.3°C. Eigenvalue/time-constant decompositions (e.g., 149 time constants in the house model) support that medium time constants with significant amplitudes dominate the response over the experiment horizon, explaining why <12 h experiments can achieve <10% error when outdoor conditions are stable.","The method is stated to be valid only when outdoor conditions change insignificantly during the experiment; consequently, QUB experiments are recommended at night. The DOE relies on a priori knowledge of the building envelope (to build the thermal network/state-space model) and on assumed measurement uncertainty levels to predict total error. The treatment of multi-boundary conditions (e.g., ground temperature) is highlighted as a source of intrinsic error if not properly accounted for in defining/estimating the global conductance.","The DOE optimization is essentially a grid/contour search over $(P_h,t_{QUB})$ rather than an explicit optimality criterion (e.g., D-/I-optimal) tied to a statistical identification model; performance may depend on the fidelity of the detailed simulation model used as “truth.” Error propagation assumes small, independent, normally distributed measurement errors and uses linearized Taylor approximations, which may understate uncertainty when slopes are estimated from noisy, autocorrelated temperature series. Practical implementation issues (heater control accuracy, spatial temperature nonuniformity across rooms/zones, occupant effects, and wind-driven infiltration variability) are not fully incorporated into the DOE maps and could shift the recommended power/time settings in field deployments.",None stated.,"A natural extension is to formalize the DOE using an explicit statistical identification framework (e.g., Fisher information / D- or I-optimality) for $H$ and possibly additional parameters beyond a 1st-order approximation, enabling principled tradeoffs among power, duration, and sampling rate. Robust or adaptive versions could update $(P_h,t_{QUB})$ online based on early data (e.g., inferred dominant time constant) and explicitly handle nonstationary outdoor temperature or wind-driven infiltration. Providing open-source implementation to generate the error contour maps from common building models (and validating across many real buildings and climates) would improve reproducibility and practitioner uptake.",1904.08848v1,https://arxiv.org/pdf/1904.08848v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:00:12Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,A-optimal|D-optimal|E-optimal|Other,"Variable/General (examples include 1-, 2-, and 3-parameter qubit channel models; general n-parameter formulation)",Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper formulates quantum process (channel) tomography as an optimal design of experiments (DoE) problem, where the design consists of choosing input probe quantum states and output measurements, optionally mixed with optimized relative frequencies. It connects classical optimal design criteria (e.g., L","Design is a choice of input state and measurement, e=(c1,a0), inducing outcome probabilities p_b8(x|e)=Tr{T_b8(c1)a0_x}. For mixed/continuous designs e(m)=(bd,e_1..e_m), the Fisher information aggregates linearly: J_b8[e(m)]=a3_i bd_i J_b8[e_i] (and analogously for QFI: J^QM_b8[(bd,c1)]=a3_i bd_i J^QM_b8[c1_i]). Optimality criteria include A-optimality min Tr(J^{-1}), D-optimality max det(J), E-optimality max bb_min(J), and b3-optimality based on Tr(J^{-b3}). In nuisance-parameter problems, weighted A-optimality uses a8_W(J)=Tr(W J^{-1}) with W=diag(W_I,0) to focus on parameters of interest (equivalently on the partial Fisher information block).","For a 3-parameter linear-scaling qubit channel, the optimal design reduces to mixing three Pauli-eigenstate/Pauli-projective-measurement settings with optimized frequencies bd=(bd_1,bd_2,bd_3); for A-optimality (b3=1), bd_i  11-b8_i^2, improving over uniform Pauli-QPT unless the channel is isotropic. For the 3-parameter Pauli channel, A-optimal frequencies satisfy bd_i  11- be_i(b8)^2, while the D-optimal design coincides with uniform Pauli-QPT (bd_i=1/3). For a two-parameter Pauli-channel subfamily with a nuisance parameter, the authors derive an analytic A-optimal binary-mixture frequency via bb^*=(f_1-f_2)/(f_1+f_2) (with f_{1,2}=","They note that many optimal solutions are local (parameter-dependent), so cannot be implemented exactly without prior knowledge, motivating adaptive/sequential methods. In the noise-asymmetry example, they only investigate designs up to m=2 (and compare to a specific m=3 Pauli-QPT), so they do not rule out better m","The work is primarily methodological/theoretical with qubit case studies; general scalability to higher-dimensional systems (larger d) and many-parameter channels is not benchmarked computationally, and the complexity of optimizing over POVMs in realistic constrained labs is not quantitatively addressed. The adaptive scheme studied is heuristic and evaluated on one model family and one set of simulation settings (e.g., N, K, runway length), without a systematic tuning/robustness analysis or comparison to alternative Bayesian/adaptive optimal design algorithms. Practical issues such as state-preparation-and-measurement (SPAM) errors, drift, and correlated noise (violations of i.i.d. assumptions) are not incorporated, which can materially affect Fisher-information-based design optimality in experiments.","They propose further investigation of the effectiveness of adaptation for implementing local optimal designs, prompted by their observation that adaptation can hurt in low-asymmetry regimes. They also leave open whether larger-m designs (e.g., m","Develop and benchmark computational convex-optimization pipelines (with explicit experimental constraints on admissible states/measurements) and provide open-source implementations to facilitate adoption in quantum labs. Extend the optimal-design analysis to robust/Bayesian criteria that integrate prior uncertainty over b8, and to settings with SPAM errors, drift, or time correlations to assess design robustness. Study multi-parameter, higher-dimensional (qudit/multi-qubit) process models and compare optimal designs against modern tomography approaches (compressed sensing, shadow tomography) under realistic resource constraints.",1904.11849v1,https://arxiv.org/pdf/1904.11849v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:01:03Z
TRUE,Sequential/adaptive|Bayesian design|Other,Screening|Cost reduction|Robustness|Other,Not applicable,Variable/General (rollout stages/percentages; multiple metrics monitored in parallel),Network/cybersecurity|Service industry|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a staged rollout framework for safely deploying new software features by combining continuous sequential monitoring with adaptive ramp-up (staging) decisions. The monitoring component uses a mixture sequential probability ratio test (mSPRT) to continuously test treatment vs. control metrics during rollout, with a scalable delete-a-group jackknife variance estimator to handle correlated observations common in online products. The ramp-up component provides three rollout schedules—time-based, power-based (using derived power/sample-size approximations for the sequential test), and risk-based (Bayesian posterior risk control)—to decide how quickly to increase exposure. The methods are evaluated via Monte Carlo simulations (A/A and A/B scenarios) and on real rollout data, comparing false positive rate, power, time-to-detection, time-to-full-rollout, and user-exposure/loss metrics. Results show variance correction controls false positives under dependence, and adaptive ramp-up (especially risk-based) can reduce detection time and manage exposure compared with fixed time-based schedules.","The sequential monitoring uses mSPRT for testing the mean difference $\delta=\mu_{trt}-\mu_{ctrl}$ with $H_0:\delta=\delta_0$ vs $H_1:\delta\neq\delta_0$, integrating the likelihood ratio over a Normal prior $h(\delta)\sim N(\delta_0,\tau)$. A $(1-\alpha)$ sequential confidence interval is given as $\bar x_{trt}-\bar x_{ctrl}\pm\sqrt{\frac{V(V+\tau)}{\tau}\left(-2\log\alpha-\log\frac{V}{V+\tau}\right)}$, where $V$ is the variance of the mean difference. Dependence is handled with delete-a-group jackknife variance: $\mathrm{Var}(\bar X)=\frac{R-1}{R}\sum_{r=1}^R(\bar X_{(r)}-\bar X)^2$. The Bayesian risk-based ramp-up chooses next-stage sample size to satisfy $\Pr(N_{trt}\delta\le -C\mid D)\le R$ using the Normal posterior $p(\delta\mid D)\sim N(m_\delta,s_\delta^2)$ and yields $n^*_{trt,t+1}=\max\left(\frac{-C}{s_\delta\Phi^{-1}(R)+m_\delta}-\sum_{s=1}^t n_{trt,s},0\right)$.","Empirical comparisons (200 replications each) show all ramp-up algorithms achieve false positive rates below 0.05 in A/A tests and power above 0.9 in A/B tests on both synthetic and real data (e.g., real-data A/A FPR about 0.01–0.015; real-data A/B power about 0.97–0.99). In variance-estimation comparisons for sequential monitoring, assuming independence inflates false positives, while bootstrap, delta-method, and delete-a-group jackknife control FPR below 5% with similar power; jackknife is favored for scalability. On synthetic data, risk-based ramp-up achieves earlier average detection (about 52h) than time-based (about 55–63h) and power-based (about 57h), and earlier average full rollout (about 47h vs ~52–61h for alternatives). On real data, risk-based similarly reduces time-to-detection (~44h) and time-to-full-rollout (~52h) versus time-based (~63–67h) and power-based (~60h), while meeting the targeted risk probability threshold roughly (reported exceedance around 10–12% when target $R=0.1$).","The paper notes that predicting next-stage total population size $\hat n_{total,t+1}$ (used to convert recommended sample sizes into rollout percentages) requires a separate time-series model, and the discussion/choice of that model is beyond the scope of the paper. It also explicitly states that detailed discussion of data-quality monitoring/diagnosis (e.g., logging issues, outliers) is out of scope, even though data quality can materially affect false positives/negatives. The examples largely focus on illustrating the framework with a single metric, with multi-metric extensions discussed at a high level.","The framework depends on assumptions used in the sequential test and Bayesian risk model (e.g., approximate Normality/CLT for metric means, variance estimation quality, and stable logging/assignment), which may be fragile for heavy-tailed metrics or strong interference/network effects. The ramp-up evaluation is limited to a small set of parameter settings (e.g., particular priors, risk thresholds, stage caps), and performance ordering can change; a systematic sensitivity analysis and calibration guidance is limited. Practical deployment considerations such as multiple simultaneous rollouts with shared users (interference), metric multiplicity control in continuous monitoring at scale, and operational constraints (latency, delayed outcomes) are not fully analyzed theoretically.","The paper explicitly proposes using reinforcement learning to guide rollout as a Markov Decision Process, optimizing a reward that trades off feature value versus rollout cost, rather than focusing only on statistical power or posterior risk. It also notes extension to multiple metrics: monitoring multiple metrics in parallel with adjusted significance to control family-wise error, and adapting ramp-up by aggregating recommended sample sizes (max for power-based, min for risk-based).","Develop self-starting/unknown-parameter versions of the sequential monitoring that more formally account for variance estimation uncertainty and delayed outcomes, and extend beyond mean shifts to distributional changes (variance, tails, crash-rate spikes). Study interference/contamination and correlated assignment effects in staged rollouts with overlapping experiments (multi-layering) using causal inference frameworks. Provide open-source reference implementations and benchmarking suites for staged rollouts, including standardized scenarios (autocorrelation, nonstationarity, novelty effects) and robust default prior/threshold calibration procedures.",1905.10493v1,https://arxiv.org/pdf/1905.10493v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:01:46Z
FALSE,NA,NA,Not applicable,Not specified,Network/cybersecurity|Other,Simulation study|Case study (real dataset),TRUE,C/C++|Other,Not provided,http://www.ece.louisville.edu/hzeng/spectrum%20sharing.html,"This paper proposes a practical underlay spectrum-sharing scheme for a small cognitive radio network with two primary users and two secondary users, where the secondary side takes full responsibility for cross-network interference cancellation without knowing the primary waveform, frame structure, or protocol. The scheme combines two MIMO-based PHY techniques: blind beamforming (BBF) at the secondary transmitter (constructing precoders from overheard primary signals) and blind interference cancellation (BIC) at the secondary receiver (constructing spatial filters from pilot/preamble reference symbols) to suppress primary-to-secondary and secondary-to-primary interference. The authors build an SDR prototype (GNU Radio/USRP N210 plus host PCs) and evaluate in indoor office environments across 12 locations, demonstrating coexistence with commercial Wi‑Fi and with custom LTE-like and CDMA-like primary networks. Experiments report average interference-cancellation capabilities of about 25 dB (BBF) and 33 dB (BIC) and show the secondary link can achieve roughly 1.0–1.1 bits/s/Hz without harming primary performance (packet delivery rate/EVM). The work advances practical underlay CRN implementation by removing assumptions of cross-network CSI and primary-user cooperation while providing prototype-based validation.","BBF designs a spatial filter/precoder per OFDM subcarrier by minimizing the output power of the overheard primary signal: $\min \mathbb{E}[\mathbf{P}^*\mathbf{Y}\mathbf{Y}^*\mathbf{P}]$ s.t. $\mathbf{P}^*\mathbf{P}=1$, yielding $\mathbf{P}(k)$ in the minimum-eigenvalue subspace of $\sum_{l=1}^{L_p}\mathbf{Y}(l,k)\mathbf{Y}(l,k)^*$. BIC forms a linear spatial filter $\mathbf{G}(k)$ to minimize MSE of the desired secondary symbol, with solution $\mathbf{G}(k)=\mathbb{E}[\mathbf{Y}\mathbf{Y}^*]^+\,\mathbb{E}[\mathbf{Y}X_s^*]$ (implemented via averages over pilot/preamble symbols, potentially including neighboring subcarriers), and detection $\hat X_s(l,k)=\mathbf{G}(k)^*\mathbf{Y}(l,k)$.","Prototype experiments across 12 indoor locations show BBF achieves tx-side IC capability with a minimum around 20 dB and an average about 25 dB (e.g., 25.3 dB for $(M_p{=}1,M_s{=}2)$ and 25.1 dB for $(M_p{=}2,M_s{=}3)$). BIC achieves rx-side IC capability averaging about 33 dB (e.g., 32.8 dB for $(1,2)$ and 33.0 dB for $(2,3)$), enabling reliable decoding where ZF fails under strong cross-technology interference. In coexistence with commercial Wi‑Fi, primary packet delivery rate remains essentially unchanged when the secondary operates, while the secondary achieves approximately 1.0–1.1 bits/s/Hz spectral utilization. Secondary throughput extrapolated from EVM is about 3.0–6.7 Mbps (avg ~5.1 Mbps) in 5 MHz bandwidth for $(1,2)$ and about 3.0–7.5 Mbps (avg ~5.5 Mbps) for $(2,3)$. BBF performance is close to explicit/implicit beamforming baselines (reported average degradations ~2.1 dB vs EBF and ~1.0 dB vs IBF) while not requiring primary cooperation.","The authors note the scheme assumes bi-directional and consistent primary traffic patterns; if traffic direction/pattern is irregular, secondary users would need more sophisticated learning (e.g., RSS/AoA signatures) to align overhearing and transmission phases. Beamforming filters remain valid only within the channel coherence time; with high mobility, coherence time may be too short to exploit the full primary-transmission airtime, reducing secondary airtime utilization. The scheme can be limited by ill-conditioned MIMO channels and antenna correlation at the secondary devices, which reduces effective spatial DoF; a mitigation is adding more secondary antennas (massive MIMO). They also acknowledge that extending from a 2+2-user CRN to large-scale CRNs requires a holistic protocol beyond the scope of the paper.","The evaluation is limited to a controlled small topology (two primary and two secondary nodes) and an indoor office setting; it is unclear how robust performance is under diverse propagation environments, mobility, and varying interferer densities. Practical deployment would face synchronization/latency constraints and tight timing alignment between primary and secondary phases; the paper does not fully quantify sensitivity to timing misalignment, carrier-frequency offset, or hardware impairments beyond periodic calibration. The secondary PHY relies on using neighboring subcarriers’ pilots to increase effective training; this assumes strong frequency correlation and may degrade in channels with severe frequency selectivity. No public implementation artifacts are provided, and reproducibility of the SDR setup (GNU Radio flowgraphs, FPGA settings, calibration routines) cannot be verified from the paper alone.","The authors suggest developing a more sophisticated learning algorithm to infer primary traffic direction when primary traffic is not persistent/consistent, potentially using RSS and AoA signatures. They discuss studying operation when channel coherence time is short (high mobility), which would constrain secondary airtime utilization, and imply adapting the scheme to such dynamic conditions. They also indicate that scaling the approach to large-scale CRNs requires a holistic MAC/protocol design that can exploit BBF/BIC across many users. Finally, they mention massive MIMO as a potential avenue to mitigate secondary-side antenna correlation/DoF limitations.","A valuable extension is a self-starting/adaptive variant that tracks time-varying channels and interference with online updates to BBF/BIC filters and quantifies robustness to CFO, phase noise, and timing offsets. Broader benchmarking against additional practical baselines (e.g., interference temperature power control, iterative/null-space learning methods, and modern interference-aware receivers) under standardized scenarios would clarify relative gains. Extending the design to multi-secondary-user settings with contention, fairness, and aggregate interference constraints would move toward realistic CRN deployments. Providing an open-source reference implementation and experiment scripts (plus calibrated datasets) would materially improve reproducibility and facilitate adoption.",1905.10940v1,https://arxiv.org/pdf/1905.10940v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:02:32Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|V-optimal|Other,Variable/General (d-dimensional linear model; k selected from n candidate experiments),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable|Other,Not provided,NA,"The paper studies Bayesian optimal experimental design for linear regression when there are n candidate experiments (rows of X) and one must choose a subset S of size k to minimize a Bayesian optimality criterion of the posterior covariance $(X_S^\top X_S + A)^{-1}$, where A is a prior precision matrix. It introduces a new “regularized” determinantal point process (DPP), $\mathrm{DPP}^{p}_{\mathrm{reg}}(X,A)$, and proves a fundamental connection between sampling from this DPP and Bayesian design objectives. Using this connection, the authors give efficient algorithms that achieve $(1+\varepsilon)$-approximate designs for A-, C-, D-, and V-optimality provided the subset size satisfies $k=\Omega\big(d_A/\varepsilon + \log(1/\varepsilon)/\varepsilon^2\big)$, where $d_A$ is an A-effective dimension that can be much smaller than d. The main algorithm solves an SDP relaxation to obtain fractional weights p and then rounds by sampling from the regularized DPP; they also provide an efficient DPP sampling procedure that reduces cost to about $O(nd^2)$ preprocessing plus additional sampling overhead. Experiments on several LIBSVM real datasets show the DPP-based rounding improves over independent sampling baselines and, when combined with an SDP solution, matches or slightly exceeds a strong greedy baseline for A-optimal design.","Bayesian posterior covariance for subset S is $\Sigma_{w\mid y_S}=\sigma^2 (X_S^\top X_S + A)^{-1}$; the design objective is to minimize criteria such as A-optimality $\mathrm{tr}(X_S^\top X_S + A)^{-1}$, C-optimality $c^\top (X_S^\top X_S + A)^{-1} c$, D-optimality $\det(X_S^\top X_S + A)^{-1/d}$, and V-optimality $\tfrac{1}{n}\mathrm{tr}\, X(X_S^\top X_S + A)^{-1}X^\top$. The proposed rounding distribution is $\Pr(S)\propto \det(X_S^\top X_S + A)\,\prod_{i\in S} p_i\prod_{i\notin S}(1-p_i)$ (Definition 2), which yields expectation bounds like $\mathbb{E}[(X_S^\top X_S + A)^{-1}]\preceq (\sum_i p_i x_i x_i^\top + A)^{-1}$ and corresponding bounds for the design criteria.","They prove an upper bound (Theorem 1) showing that when $k\ge 4\,d_{\frac{n}{k}A}$ there exists a subset S of size k with objective within a small multiplicative factor of the baseline $f_A(\tfrac{k}{n}\Sigma_X)$. They give $(1+\varepsilon)$-approximation algorithms for A/C/D/V-optimal Bayesian design when $k=\Omega\big(d_A/\varepsilon + \log(1/\varepsilon)/\varepsilon^2\big)$ (Theorem 3), improving prior work by replacing d with $d_A$ and improving the dependence on $\varepsilon$ for these criteria. They provide an efficient sampling method for the regularized DPP that decomposes it into a low-rank correlation DPP plus independent Bernoulli sampling (Theorem 5), reducing sampling preprocessing to $O(nd^2)$ instead of $O(n^3)$. On LIBSVM datasets, their DPP sampling (even without solving the SDP) consistently outperforms uniform and predictive-length independent sampling for A-optimality, and with SDP-based weights it matches or slightly exceeds a strong greedy method.","The authors note that most of the runtime of the strongest variant (“with SDP”) is dominated by solving the SDP, and they suggest exploring alternative solvers and early termination with approximate SDP solutions. They also note (via comparison to prior lower bounds) that their main $(1+\varepsilon)$ result does not directly extend to E-optimality.","The main guarantees are for a linear-Gaussian model with independent noise and rely on the Bayesian prior precision A; robustness to model misspecification, heteroskedasticity, or correlated errors is not analyzed. Practical implementation depends on solving an SDP (or an approximate equivalent), which can still be expensive at large n and d; the paper does not provide open-source reference code for the full pipeline to assess scalability. Empirical evaluation is limited to a small set of low-to-moderate dimensional LIBSVM datasets (d up to 14) and focuses mainly on A-optimality, leaving performance for C/D/V criteria and higher-dimensional regimes less validated.","They explicitly suggest investigating alternative optimization approaches for the SDP step (e.g., interior point methods) and terminating solvers early once an approximate solution is reached, since the SDP dominates runtime in their experiments.","Extending the approach to settings with correlated observations, generalized linear models, or non-Gaussian noise would broaden applicability. Developing a fully self-contained, scalable implementation (e.g., exploiting faster approximate DPP sampling and first-order SDP/dual methods) and benchmarking on high-dimensional designs would strengthen practical impact. It would also be valuable to study robustness and sensitivity to the choice of prior precision A and to provide guidance for selecting/estimating A in practice.",1906.04133v1,https://arxiv.org/pdf/1906.04133v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:03:16Z
TRUE,Response surface|Optimal design,Parameter estimation|Prediction|Model discrimination|Other,I-optimal (IV-optimal)|D-optimal|A-optimal|Other,Variable/General (examples with 3 and 5 factors; general q factors),Food/agriculture|Theoretical/simulation only,Other,TRUE,R,Not provided,https://repositorio.unesp.br/bitstream/handle/11449/142916/000867062.pdf?sequence=1,"The paper develops and evaluates optimal response surface designs with an explicit focus on prediction performance across the experimental region. It extends a prior compound design criterion to jointly account for parameter estimation, point prediction and interval prediction of both responses and differences in response (relative to the region center), introducing new criteria including ID (point prediction of differences) and (IP)/(IDP) (interval prediction of responses/differences). It also proposes and extends graphical diagnostics for prediction capability—variance/standard-error dispersion graphs and fraction-of-design-space plots—including new variants for differences (DFDS) and for interval predictions, and recommends plotting against relative volume rather than distance. Methods are illustrated on a 3-factor cuboidal food experiment (cassava bread recipe) and a 5-factor spherical-region example; the results show compound-criterion designs can deliver high efficiency across multiple goals and that certain central composite designs can be ID-optimal for specific q and run-size ranges. The paper connects classical response surface designs (e.g., CCDs) with modern optimal-design criteria by showing when CCDs emerge as optimal under the new prediction-of-differences objective.","Key prediction criteria are trace forms of average prediction variance. For response prediction, I-optimality minimizes $\int_{\mathcal X} f(x)^\top (X^\top X)^{-1} f(x)\,dx \propto \mathrm{tr}\{M(X^\top X)^{-1}\}$, and its interval version (IP) multiplies by an $F$-quantile: $\mathrm{tr}\{M(X^\top X)^{-1}\}F_{1,d;1-\alpha_3}$. For differences relative to the center $x_0=0$, ID minimizes $\int_{\mathcal X} [f(x)-f(0)]^\top (X^\top X)^{-1}[f(x)-f(0)]\,dx \propto \mathrm{tr}\{M_0(X^\top X)^{-1}\}$, with interval version (IDP) using $F_{1,d;1-\alpha_4}$. A general compound criterion combines estimation and prediction goals via weighted products/powers of (adjusted) D/A-type terms and the trace-based prediction terms (Eq. 11).","In the 3-factor, $n=26$ cassava-bread example, a compound design with equal weights on (DP)S and ID (design 8) achieved high efficiencies across estimation and prediction measures and substantially outperformed a previously published multi-objective design for several interval-based criteria (reported gains: 40.63% for (DP)S, 28.20% for (AP)S, 27.62% for (IDP), 23.28% for (IP)). In the 5-factor, $n=30$ spherical-region example, the ID-optimal design coincided with a resolution-V half-fraction central composite design (CCD) with four center runs, an unusual equivalence for such a large candidate set. A computational exploration suggested CCDs can be ID-optimal for specific factor/run-size ranges (e.g., q=5 with half-replicate factorial portion is ID-optimal for $30\le n\le 33$; q=6 for $50\le n\le 55$), while outside these ranges the optimum is often “CCD-like”. Graphical diagnostics (SEDG/FDS and their difference/interval variants) highlighted that some designs that look poor on a single global metric may perform competitively over substantial fractions of the region, and that plotting against relative volume improves discrimination among designs.","Optimal designs are obtained using heuristic exchange algorithms, so equivalences and optimality claims come with the caveat that the algorithm might not have found the true global optimum. The CCD optimality findings are conditional on the candidate set used (based on an expanded full $3^q$ set mapped to the sphere), so optimality is not claimed over all possible continuous design measures. The authors also note they did not explore more than six factors in the CCD ID-optimality investigation.","The proposed criteria and plots assume the usual homoscedastic, independent error model with correct polynomial specification; robustness to model misspecification, heteroscedasticity, or correlated errors is not systematically studied. Interval-based criteria rely on pure-error degrees of freedom and F-quantile adjustments, which may be impractical in tightly budgeted experiments with limited replication. Empirical validation is limited to two illustrative examples; broader benchmarking across standard RSM scenarios (constraints, irregular regions, mixture/process constraints) would strengthen generalizability. Practical guidance on choosing compound weights $\kappa$ is limited and could affect reproducibility of “best” compromises across users.",None stated.,"Develop principled or data-driven methods for selecting compound-criterion weights (e.g., based on utility, Pareto-front exploration, or Bayesian decision criteria) and study sensitivity of chosen designs to these weights. Extend the prediction-of-differences framework to settings with autocorrelation, random effects/multi-stratum structures, or heteroscedasticity, and to alternative regions/constraints beyond cubes and spheres. Provide fully reproducible software (e.g., an R package/vignette) implementing the new criteria and the proposed relative-volume dispersion plots for broader adoption. Investigate theoretical conditions under which CCDs are provably ID-optimal (beyond candidate-set evidence) and characterize when such equivalences break down.",1906.07500v1,https://arxiv.org/pdf/1906.07500v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:03:56Z
TRUE,Sequential/adaptive|Optimal design|Bayesian design|Other,Model discrimination|Prediction|Cost reduction|Other,G-optimal|Other,"Variable/General (dimension d; sets X,Z ⊂ R^d)",Theoretical/simulation only|Healthcare/medical|Other,Simulation study|Exact distribution theory|Other,TRUE,Python|Other,Not provided,NA,"The paper introduces the transductive linear bandit problem: sequentially choose noisy linear measurements from a probe set X to identify the best item in a possibly different set Z with fixed confidence 1−δ. It derives an instance-dependent information-theoretic lower bound on the expected number of measurements for any δ-PAC algorithm, expressed via an optimal allocation over probes and a worst-case direction tied to the gaps between the optimal and suboptimal items. It proposes RAGE (Randomized Adaptive Gap Elimination), a round-based adaptive sampling algorithm that repeatedly solves an optimal design problem over pairwise difference directions Y(\hat Z_t) and uses rounding procedures to obtain near-optimal discrete allocations. The algorithm’s sample complexity matches the lower bound up to logarithmic factors, providing (as claimed) the first non-asymptotic nearly instance-optimal algorithm for linear bandit pure exploration. Empirical simulations compare RAGE to prior methods (XY-adaptive, LinGapE, ALBA, static XY, and an oracle allocation), showing RAGE is competitive in linear bandit benchmarks and can substantially outperform static allocations in transductive settings.","The lower bound (Gaussian noise) is $\mathbb{E}_{\theta^*}[\tau] \ge \log(1/(2.4\delta))\,\min_{\lambda\in\Delta(X)}\max_{z\neq z^*}\frac{\|z^*-z\|^2_{(\sum_{x\in X}\lambda_x xx^\top)^{-1}}}{((z^*-z)^\top\theta^*)^2}$. RAGE selects at round $t$ an allocation $\lambda_t^* = \arg\min_{\lambda\in\Delta(X)}\max_{y\in Y(\hat Z_t)} y^\top(\sum_x \lambda_x xx^\top)^{-1}y$ and sets $N_t \asymp (2^{t})^2\,\rho(Y(\hat Z_t))\log(|Z|^2/\delta_t)$ with $\rho(V)=\min_{\lambda}\max_{v\in V} v^\top(\sum_x \lambda_x xx^\top)^{-1}v$. Arms are eliminated when an empirical gap exceeds a threshold, using an OLS estimate $\hat\theta_t = A_t^{-1} b_t$ with $A_t=\sum_{j=1}^{N_t} x_j x_j^\top$.","Theorem 1 provides an instance-dependent lower bound scaling with the optimal allocation over X and the hardest comparison direction $(z^*-z)$ normalized by the squared gap. Theorem 2 shows RAGE identifies $z^*$ with probability at least $1-\delta$ and uses at most $N \le c\,\psi^*\,\log(1/\Delta_{\min})\,\log(|Z|^2\log^2(1/\Delta_{\min})/\delta)+ r(\epsilon)\log^2(1/\Delta_{\min})$ samples (for an absolute constant $c$), i.e., instance-optimal up to logarithmic factors. Simulations at $\delta=0.05$ report zero empirical failures and show RAGE is competitive with XY-adaptive and close to an oracle allocation in benchmark linear-bandit examples, while significantly outperforming static allocations in transductive examples. The implementation uses Frank–Wolfe to compute designs and rounding methods to produce discrete allocations; experiments are run in Python 3 with Gaussian noise.","They note a technical requirement that $\mathrm{span}(X)=\mathbb{R}^d$ (or more generally that relevant directions lie in the span of X) to obtain unbiased estimates of gaps, and they assume independent sub-Gaussian (often Gaussian for the lower bound) noise. They also discuss that naive rounding of fractional allocations can fail and that practical performance depends on using an efficient rounding procedure with enough samples. In the conclusion, they suggest the proven $\log(1/\Delta_{\min})$ factor in the upper bound may be unnecessary (i.e., analysis may be loose).","The method repeatedly solves convex optimal-design problems over a potentially large set of pairwise differences $Y(\hat Z_t)$ (size up to $|Z|^2$), which can be computationally heavy for large item sets; the paper relies on Frank–Wolfe heuristics and thresholding, but worst-case runtime/scalability is not fully characterized. The theory primarily targets the fixed-confidence, independent-noise setting; robustness to model misspecification (nonlinear rewards), heavy-tailed noise, or temporal dependence is not analyzed. Practical deployment would require careful numerical stability for $A_t^{-1}$ and design/rounding steps, especially in higher dimensions or near-collinear X, which is not deeply addressed. Comparisons are mainly to a subset of then-recent algorithms; additional baselines (e.g., modern elimination/optimal design hybrids or different confidence constructions) are not explored.",They propose developing anytime algorithms for the transductive setting that do not discard samples (analogous to anytime best-arm identification in multi-armed bandits). They suspect the extra $\log(1/\Delta_{\min})$ factor in their sample complexity bound may be removable and note this as an open direction. They also suggest extending ideas to regret minimization to obtain instance-based regret bounds for linear bandits and exploring connections to regret and fixed-budget formulations.,"Develop computationally scalable variants that avoid explicit enumeration of $Y(\hat Z_t)$ (e.g., constraint generation, coresets, or approximate worst-direction oracles) while preserving near-optimal sample complexity. Extend the framework to unknown/noisy features, generalized linear rewards, heteroskedastic noise, or autocorrelated observations with valid sequential confidence bounds. Provide self-starting/parameter-free implementations that reduce dependence on design-solver tolerances and rounding hyperparameters, and release a reference implementation for reproducibility. Validate the approach on real transductive applications (e.g., drug screening or recommendation systems) where probe constraints and costs are central, including cost-sensitive or batched measurement regimes.",1906.08399v1,https://arxiv.org/pdf/1906.08399v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:04:43Z
TRUE,Bayesian design|Other,Model discrimination|Other,Bayesian D-optimal|Other,Variable/General,Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper reframes quantum state discrimination as a Bayesian experimental design problem in which the “experiment” is the choice of quantum measurement (POVM) and the “design criterion” is an agent-chosen utility function. It shows that the two standard state-discrimination paradigms—minimal error and maximal confidence—arise from two different utility functions within the same Bayesian design framework, and it discusses how alternative utilities (e.g., conditional entropy / mutual information) yield different optimal measurements. The authors also introduce multi-objective (lexicographically ordered) utilities to break ties within families of optimal measurements (e.g., maximal confidence with minimum inconclusive probability) and to handle cases where minimal-error optimization is uninformative. In addition, they connect averaged utilities to resource monotones in the resource theory of quantum measurements, giving conditions on utilities that guarantee monotonicity under stochastic post-processing. The work is primarily conceptual/theoretical, providing general conditions and illustrative examples rather than a computational design algorithm implementation.","Bayesian experimental design objective specialized to quantum measurements: choose a POVM $\mathcal{E}$ to maximize averaged utility $U(\mathcal{E})=\max_{\Gamma}\sum_y\sum_\nu p(N_\nu,\mathcal{E}_y)\,U(\mathcal{E},y,\Gamma,\nu)$, where $\Gamma$ is a decision strategy mapping outcomes to decisions. Minimal-error discrimination corresponds to utility $U(\mathcal{E},y,\Gamma,\nu)=\delta_{\Gamma(y),\nu}$ (maximizing probability of success), while maximal-confidence corresponds to $U(\mathcal{E},y,\Gamma,\nu)=\delta_{\Gamma(y),\nu}/p(D_{\Gamma(y)})$ (maximizing summed posterior confidences). An alternative information-gain utility uses $U=\log p(N_\nu|\mathcal{E}_y)$, leading to optimization of $-H(N|\mathcal{E})$ or equivalently $I(N:\mathcal{E})$ (mutual information).","The paper establishes an equivalence between standard quantum state discrimination tasks and Bayesian experimental design via appropriate choices of utility functions, unifying minimal-error and maximal-confidence discrimination under one framework. It provides three sufficient conditions (C1–C3) on a utility function ensuring the induced averaged utility $U(\mathcal{E})$ is a resource monotone under stochastic post-processing of POVMs. It notes that utilities such as probability of success, total confidence, conditional entropy, and mutual information satisfy these conditions and thus define monotones. It also shows how multi-objective (lexicographic) utilities can select among families of optimal maximal-confidence measurements by adding a secondary objective like minimizing the inconclusive outcome probability.",None stated.,"The work is largely theoretical and does not provide numerical algorithms, computational complexity analysis, or software for actually computing optimal POVMs under general utilities, which limits immediate practical use for large Hilbert spaces or many candidate states. Empirical evaluation is limited to illustrative examples; there is no systematic simulation benchmark comparing different utilities or demonstrating scalability/robustness under noise, model mismatch, or experimental constraints on feasible measurements. The framing treats the design space as the set of all POVMs, but many real experiments restrict measurements (e.g., local/LOCC constraints), and the impact of such constraints is not developed.","The authors suggest that, now that state discrimination is cast as Bayesian experimental design, analytic or numerical methods from Bayesian experimental design optimization may be useful for finding optimal measurements for broader discrimination tasks. They also indicate that broader classes of utility functions satisfying their conditions could generate useful resource monotones, potentially leading to a finite family that characterizes the resource theory of quantum measurements.","Develop efficient computational methods (e.g., convex/semidefinite programming formulations, gradient-based optimization on POVM manifolds, or Bayesian/variational approaches) to compute optimal measurements for new utilities and multi-objective criteria at scale. Extend the framework to constrained measurement classes (e.g., LOCC, separable measurements, hardware-limited POVMs) and study how optimality/monotonicity conditions change. Validate the approach on realistic noisy experimental scenarios (finite data, calibration error, decoherence) and provide open-source implementations to enable adoption by quantum information and experimental groups.",1906.09737v2,https://arxiv.org/pdf/1906.09737v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:05:16Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Model discrimination|Screening|Cost reduction|Other,Bayesian D-optimal|Other,"Variable/General (design variable is initial prey density $N_0$; models include parameters $a$, $T_h$, and optionally $\lambda$)",Environmental monitoring|Food/agriculture|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/haydenmoffat/sequential_design_for_predator_prey_experiments,"The paper proposes the first Bayesian sequential optimal experimental design method for predator–prey functional response experiments, where the controllable design variable is the initial prey density $N_0$ and the response is the number of prey consumed. It considers candidate models formed by Holling type II vs type III mechanistic dynamics coupled with either binomial or beta-binomial observation models, and designs experiments sequentially to achieve parameter estimation, model discrimination, or both. Utilities are information-based (Kullback–Leibler divergence for parameter learning; mutual information/log posterior model probability for discrimination; and Borth’s total entropy as a dual-purpose utility) and are estimated efficiently using Sequential Monte Carlo (SMC) particle approximations. A simulation study (25 sequential runs, $N_0\in\{1,\dots,300\}$, four candidate models) shows sequential designs outperform near-optimal static Bayesian designs and random designs for parameter precision, model discrimination, and dual-purpose objectives. R code implementing the methodology is provided by the authors, supporting practical adoption in ecological experimental planning to reduce experimental runs, cost, and animal sacrifice.","Experiments choose a design point $d=N_0$ and observe $z=n\in\{0,\dots,N_0\}$ (prey eaten), modeled as $n\sim\text{Binom}(N_0,p_\tau)$ or $n\sim\text{BetaBinom}(N_0,p_\tau,\lambda)$ with $p_\tau=(N_0-N_\tau)/N_0$ where $N_\tau$ comes from Holling type II/III ODE solutions. The next design is chosen myopically as $d^*=\arg\max_{d\in D} U(d\mid y_{1:i},d_{1:i})$ where $U(d\mid\cdot)$ is the expectation of a utility over models and predictive responses. Utilities include (i) parameter-estimation KLD between current and updated posteriors, (ii) model-discrimination utility $\log \pi(m\mid y_{1:i},z,d_{1:i},d)$, and (iii) total-entropy utility as their sum; SMC provides particle-based estimates of posterior predictives and evidence ratios used inside these utilities.","In simulation experiments with $I=25$ sequential trials and design space $N_0\in\{1,\dots,300\}$ across four true-model scenarios (type II/III × binomial/beta-binomial), sequential optimal design consistently yields higher Bayesian D-posterior precision than both near-optimal static designs and random designs for all true models. For model discrimination objectives, sequential designs produce posterior model probabilities more concentrated on the true model than static or random strategies, again across all true models. Under the dual-purpose (total entropy) utility, sequential design improves both parameter precision and model probabilities relative to static design. The paper also reports an illustrative 4-step sequence where chosen $N_0$ values were 299, 38, 20, and 30 and the posterior probability of the true model increased from 0.25 (prior) to roughly 0.45–0.62 over the first few experiments.","The authors note computational issues when fitting an incorrect model: after a new observation, particle weights can become extremely skewed and the ESS can drop to ~1, leaving few unique particles even after resample/move steps, which can degrade utility estimation. They suggest this may be mitigated by introducing intermediate target distributions between consecutive posteriors (i and i+1). They also state a limitation of the myopic (one-step-ahead) design: looking only one observation ahead is not optimal in general.","The approach assumes conditional independence across experimental runs and relies on correct specification of the candidate model set; performance may degrade if real systems exhibit dependence, unmodeled heterogeneity, or functional response forms outside the candidate set. The design variable is effectively one-dimensional ($N_0$), so extensions to multi-factor ecological design choices (e.g., predator density, exposure time, temperature) are not evaluated. Although SMC is efficient, the method can still be computationally heavy because utilities require evaluating predictive distributions across a discrete response space for each candidate $N_0$, and the paper does not benchmark runtime/scalability for larger design spaces or more complex models.","They propose addressing ESS collapse by constructing a sequence of intermediate targets between posteriors at experiments i and i+1 (smoother annealing/bridging). They also suggest investigating multi-step-ahead design (e.g., a two-step dynamic programming approach) to move beyond myopic one-step-ahead optimization.","Extending the framework to additional controllable factors (e.g., exposure time $\tau$, predator density, environmental covariates) and to multivariate/structured responses could broaden applicability and test robustness. Developing robustness checks and misspecification diagnostics (e.g., posterior predictive checks guiding model-set expansion) would help practitioners when none of the candidate models fit well. Packaging the method as an R package with computational accelerations (parallelization, surrogate modeling of utilities) and providing practical guidance for Phase I prior elicitation would improve adoption.",1907.02179v2,https://arxiv.org/pdf/1907.02179v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:06:05Z
TRUE,Optimal design,Parameter estimation,D-optimal|A-optimal|E-optimal|Compound criterion|Other,Variable/General (treatments v1 and covariates v2; design space size v1×d),Healthcare/medical|Theoretical/simulation only,Other,TRUE,MATLAB|R,Not provided,NA,"The paper develops optimal approximate designs for linear models that include treatment effects and covariate effects when the response variances depend on treatment (heteroscedasticity), with interest focused on specified treatment contrasts (e.g., treatment-versus-control) and linear functions of covariate effects. It extends known D-optimal product-design results by proving that for any eigenvalue-based information criterion there exists an optimal product design, and it derives constructions for Kiefer’s a6p criteria including A- and E-optimality. For p<0, it provides an additive decomposition of the a6p objective for product designs, enabling the v1d-dimensional problem to be split into two smaller marginal optimizations (covariates and treatments). To address the large support sizes of optimal product designs, the paper characterizes a class of optimal (generally non-product) designs via linear constraints and proposes a linear-programming-based sparsification method to obtain optimal designs with smaller supports that are easier to round to exact designs. Examples illustrate substantial support reduction and improved efficiencies of rounded exact designs relative to rounded product designs in some settings.","Model: y(i,k)=c4_i+bc+g(k)^Tb2+b5(i,k) with Var(b5(i,k))=c3^2/bb_i (treatment-dependent variances). Approximate design be(i,k) on X={1,ef,v1}fef{1,ef,d}, moment matrix M(be)=a3_{i,k}be(i,k)bb_i f(i,k)f(i,k)^T where f(i,k)=(e_i^T,1,g(k)^T)^T; interest matrix A=diag(Q1,Q2) for treatment contrasts Q1^Tc4 and covariate functions K^Tb2. For a product design be=wf b1, the information matrix block-diagonalizes as N_A(wfb1)=diag(N_{Q1}(w), (a3_i bb_i w_i)N_K(b1)), and for pef(-1e,","The paper proves (Theorem 1) that for any eigenvalue-based information function, if an optimal design exists then there exists an optimal product design wfb1, so one can restrict attention to product designs without loss of optimality. For Kiefer a6p criteria (including A- and E-optimality), it shows how to compute the optimal product design by first optimizing the marginal covariate design and then optimizing the marginal treatment weights with a heteroscedasticity-dependent correction term (Theorem 2). It characterizes an entire class of optimal non-product designs through linear equalities and proposes a linear programming approach that tends to yield sparse-support optimal designs (Theorem 3). In Example 1 (v1=v2=3, bb=(9,1,1)), the A-optimal product design has support size 24 while an LP-derived A-optimal non-product design has support size 10; rounding to n=48 yields efficiency 0.9641 (rounded product) versus 0.9991 (rounded non-product). In Example 2, the E-optimal product design is supported on 45 points, while an E-optimal non-product design with support 28 can be rounded for n=40 (where product rounding is infeasible), achieving efficiency 0.8493.",None stated.,"The work is primarily methodological and demonstrates performance mainly through constructed examples; it does not provide a broad simulation study over many covariance structures, contrast systems, or variance patterns bb_i to quantify typical gains from sparsification. The approach assumes the treatment-specific variance weights bb_i (and the discretized covariate set T) are known and that errors are uncorrelated; robustness to misspecified bb_i, non-normality, or correlated/clustered outcomes common in clinical trials is not analyzed. Practical guidance on selecting discretizations for continuous covariates and on converting sparse approximate designs into implementable randomization schemes (beyond rounding) is limited. No implementation/code is provided despite reliance on LP solvers, which may affect reproducibility and uptake.","The paper notes that the linear-programming-based support reduction approach is not tied to the specific treatment-covariate model and can be applied more generally in linear regression models once an optimal design with large support is available, by solving constraints of the form M(be)=M(be*) or M(be)GA=A to obtain sparse vertex solutions.","Develop and release an open-source implementation (e.g., R/Python package) that automates computation of optimal product designs for a6p criteria and the LP sparsification step, including exact-design rounding and efficiency reporting. Extend the framework to settings common in trials such as unknown variance parameters, adaptive/sequential re-optimization of designs, and correlated outcomes (e.g., repeated measures or cluster randomization). Study robustness when bb_i are estimated rather than known, and when covariates are continuous without discretization (e.g., approximate designs over continuous regions). Add systematic simulation benchmarks comparing sparsified designs to alternative sparse constructions (e.g., orthogonal arrays, exchange algorithms) across a range of contrast systems and variance patterns.",1907.04044v1,https://arxiv.org/pdf/1907.04044v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:06:35Z
TRUE,Optimal design|Computer experiment,Parameter estimation|Prediction|Cost reduction,D-optimal,Variable/General (B-set of K b-tensors; each tensor has up to 6 DOF; experiments use K=60 tensors with 120 (C1) or 180 (C2) free parameters),Healthcare/medical,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an optimal experimental design framework for multidimensional diffusion MRI acquisitions aimed at improving estimation of biophysical tissue model parameters (the brain “Standard Model”). The design optimizes the choice of a set of diffusion-encoding b-tensors (“B-set”) by maximizing the determinant of the Fisher Information Matrix (D-optimality), using an analytically tractable Fisher matrix for a 4th-order cumulant signal expansion as a surrogate objective. To make the metric tissue-independent, the Fisher matrix is averaged over a broad, physically plausible parameter space derived from Monte Carlo sampling of Standard Model parameters. Optimization is performed under practical acquisition constraints (fixed eigenvectors and either fixed traces/shells or free traces), using a hybrid strategy combining stochastic SOMA search with local gradient-based refinement. Monte Carlo simulations with synthetic signals and Rician noise show the optimized protocol reduces estimation error versus a naïve LTE+PTE combination (reported ~14% improvement for cumulant tensor estimates and ~7% for Standard Model parameters) and yields designs dominated by linear and planar tensor encodings with two diffusion-weighting shells and no spherical tensor encoding.","b-tensors are defined by $\mathbf{B}=\gamma\int_0^\tau \mathbf{q}(t')\otimes\mathbf{q}(t')\,dt'$ with $b=\mathrm{Tr}(\mathbf{B})$ and $\mathbf{q}(t)=\gamma\int_0^t \mathbf{g}(t')\,dt'$. The surrogate signal model is the 4th-order cumulant expansion: $\log S=\log S_0- B_{ij}D_{ij}+\tfrac12 B_{ij}B_{k\ell}C_{ijk\ell}$. For a B-set $\{\mathbf{B}_k\}_{k=1}^K$, the Fisher information is $J_{ij}=\sigma^{-2}\sum_{k=1}^K \partial_{\Theta_i}S(\mathbf{B}_k;\Theta)\,\partial_{\Theta_j}S(\mathbf{B}_k;\Theta)$ for $\Theta=[D,C]$, and a tissue-independent objective uses the averaged Fisher matrix $\hat J(\{\mathbf{B}_k\})=\frac{1}{\mathrm{vol}(H)}\int_H J(\{\mathbf{B}_k\};\Theta)\,d\Theta$, maximizing $\det(\hat J)$ (D-optimality).","Under both constraint sets tested (C1: fixed eigenvectors and fixed trace per tensor with two shells; C2: fixed eigenvectors with trace/size free up to $b_{\max}$), the optimal B-sets use only linear and planar tensor encodings (LTE/PTE) and exclude spherical tensor encoding (STE). The optimal proportion of LTE is reported around 75%–85%, depending on whether $\mathrm{tr}(B)$ is fixed, and the C2 optimum naturally clusters into two shells with unequal numbers of measurements and only LTE on the lower shell. In Monte Carlo experiments (10,000 random voxels; Rician noise with SNR=100), the C2-optimized protocol improves accuracy versus a balanced LTE+PTE protocol by about 14% for estimating cumulant tensors (D and C) and about 7% for estimating Standard Model parameters (double-Watson ODF, no CSF fraction).","The authors note the Fisher information was computed using the 4th-order cumulant expansion rather than the full Standard Model signal, which is only a good approximation up to about $b_{\max}=2.5\,\mathrm{ms}/\mu\mathrm{m}^2$. They also acknowledge assuming optimistic SNR so that Rician noise can be approximated as Gaussian, and that the averaging/integration over the cumulant parameter space involves approximations. Finally, they state the high-dimensional, multi–local-minima optimization may be further improved and that relaxing constraints (e.g., allowing eigenvectors to vary or using b-dependent SNR) would likely improve results.","The optimized designs are evaluated primarily via synthetic Monte Carlo studies and may not reflect practical scanner constraints such as gradient waveform timing, TE/TR penalties, eddy current limits, SAR, peripheral nerve stimulation, and direction-dependent SNR—all of which can change the true optimality. The averaging over a broad parameter space can yield a design that is “globally” good but potentially suboptimal for specific clinical populations, anatomical regions, or targeted parameters (e.g., focusing on a subset of SM parameters). Also, D-optimality can overweight parameter-estimation precision at the expense of prediction error robustness or sensitivity to model mismatch, and the work does not report sensitivity of the optimum to mis-specified priors/parameter ranges beyond noting ranking stability for modified ranges.","They propose more comprehensive in silico experiments varying the number of measurements and using different fODFs, and note that tailoring the design to a specific parameter of interest may yield larger average improvements. They also plan to modify the information metric to better align with the global optimum for the Standard Model rather than the cumulant surrogate. Additionally, they explicitly suggest relaxing constraints (e.g., freeing eigenvectors and accounting for different SNRs across diffusion weightings) and improving the optimization strategy.","A natural extension is to incorporate explicit scanner/sequence constraints by optimizing over realizable gradient waveforms (including TE/TR and motion/eddy-current effects) rather than over abstract b-tensors, and to validate on real multidimensional dMRI datasets. A Bayesian design formulation using explicit priors over tissue parameters and fODFs could provide more clinically tailored protocols and quantify robustness to prior misspecification. It would also be valuable to compare against alternative criteria (A-/I-/G-optimality, prediction-focused metrics, or multi-objective trade-offs) and to study robustness under non-Gaussian noise at lower SNR and under spatial/temporal correlations typical in dMRI.",1907.06139v1,https://arxiv.org/pdf/1907.06139v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:07:16Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Optimization|Cost reduction|Other,Other,"Variable/General (examples include 1D, 2D, 5D, 7D; real applications use 5 factors for short polymer fiber; 1 factor for scaffold porosity; 1 factor for NN neurons)",Manufacturing (general)|Food/agriculture|Other,Simulation study|Case study (real dataset)|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab),https://bit.ly/2sDFQ35,"The paper proposes a Bayesian-optimization-based method to accelerate expensive experimental design by incorporating expert “hunches” in the form of per-variable monotonic trends in the underlying response function f(x). The target-seeking objective is formulated as g(x)=|f(x)-y_T|, which becomes unimodal along any variable where f(x) is monotone; the authors exploit this structure to speed convergence to a specified target value rather than an extremum. The core contribution is a two-stage Gaussian process approach: (1) fit a monotonic GP model for f(x) using derivative-sign constraints, then (2) generate “virtual” samples from this GP and use them (with an uncertainty correction) to build a GP surrogate for g(x) used by BO (GP-LCB acquisition). A theoretical adjustment to the exploration parameter is derived to mitigate overconfidence caused by adding virtual observations, preserving convergence guarantees (regret bounds) akin to GP-LCB. The method is validated on synthetic benchmarks, neural-network hyperparameter tuning to a target runtime, and two real experimental design tasks: short polymer fiber synthesis to target length (5 controllable parameters) and 3D-printed scaffold design to target porosity (smallest-detail parameter).","The target-seeking formulation is $x^* = \arg\min_{x\in\mathcal{X}} g(x)$ with $g(x)=|f(x)-y_T|$ (Eq. 1). Bayesian optimization uses GP-LCB for minimization: $a_t(x)=\mu_{t-1}(x)-\sqrt{\alpha_t}\,\sigma_{t-1}(x)$ (Eq. 8). In BO-MG, virtual samples from a monotone GP fit to $f$ are incorporated into a GP model for $g$ with predictive mean $\mu_g(x)=k^T K^{-1}[\mu_g(X_v);\,|y-y_T|]$ and variance $\sigma_g^2(x)=k(x,x)-k^T K^{-1}k$, where $K$ includes both observation noise and the virtual-sample predictive variances (Eqs. 9–11). To correct overconfidence from added virtual points, the exploration parameter is inflated, e.g., $\beta_t=\exp(2C)\alpha_t$ (or $\beta_t=(\max_x r_{t-1}(x))^2\eta\alpha_t$) with $r_{t-1}(x)=\sigma^{N_1}_{t-1}(x)/\sigma^{N_2}_{t-1}(x)$ (Eqs. 14–16, 22).","Across benchmark target-optimization problems (2D/5D/7D), BO-MG consistently reduces the “difference to target value” faster than standard BO, and improves over the derivative-sign baseline especially in higher dimensions. For neural-network hyperparameter tuning to a 2s target test time, BO-MG reached within 0.05s of target in 20/20 runs (100%), compared with 15/20 (75%) for standard BO and 6/20 (30%) for random search; the resulting neuron counts were similar (mean about 765 vs 768, with lower SD for BO-MG). In short polymer fiber synthesis (5 parameters; monotonicity in butanol speed), BO-MG reached target fiber lengths faster than standard BO in 3 of 4 target scenarios and similarly in 1 of 4, with a cited example of reducing iterations from 15 to 10 to reach 10µm error (reported as ~3 days saved). In scaffold porosity design, BO-MG produced recommendations closer to target porosities (e.g., for 50% target: 50.27% vs 49.22% for standard BO).","For BO-DS, the authors state a key drawback: derivative information about $g(x)$ is unavailable near the optimum (only away from it), leading to under-utilization of monotonicity information and ambiguous regions. For BO-MG, they note a potential issue: adding virtual observations can shrink predictive variance and cause overconfidence (insufficient exploration), requiring an adjustment to the exploration–exploitation trade-off parameter to maintain convergence guarantees.","The approach assumes reliable, correctly-specified monotonicity per variable; if the hunch is wrong or only locally monotone, the monotone GP and derived unimodality for $g(x)$ may bias the search. The method’s performance depends on choices for number/placement of derivative-sign points and virtual samples (N1/N2) and the correction factor $\eta$, which are set heuristically and may require tuning per problem. Computational cost can increase due to GP with derivative signs (noted as $O((t+m)^3)$) and repeated refitting/hyperparameter estimation each iteration, which may limit scalability in high dimensions or with many iterations. Comparisons are mainly against standard BO (and BO-DS/random search in one setting); broader baselines (e.g., constrained BO, alternative shape-constrained surrogates, or modern BO acquisition functions) are not extensively benchmarked.","The authors propose automatically detecting trends in the objective so that BO strategies can switch between different trend assumptions, and more broadly exploring other types of prior knowledge beyond monotonicity to improve efficiency in experimental design.","Develop robustness checks or probabilistic weighting for uncertain/possibly incorrect monotonicity hunches (e.g., Bayesian model averaging over monotone vs unconstrained surrogates). Extend the framework to handle non-i.i.d. noise, heteroscedastic measurements, and time-series/autocorrelated experimental outputs common in physical processes. Provide open-source, well-documented implementations (e.g., Python/R) and systematic guidance for selecting N1/N2, derivative-sign placements, and $\eta$ to reduce practitioner tuning. Generalize to multivariate responses and multi-objective target-seeking (e.g., hitting multiple property targets simultaneously) and evaluate on larger sets of real DOE applications with standardized benchmarks.",1907.09065v1,https://arxiv.org/pdf/1907.09065v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:07:58Z
TRUE,Sequential/adaptive|Computer experiment,Parameter estimation|Prediction|Other,Other,Variable/General (examples: 3 factors in short-column; d=8 inputs in floor system),Manufacturing (general)|Other,Simulation study|Other,TRUE,R|Other,Not provided,NA,"The paper develops and studies sequential computer experiment designs to estimate an extreme output tail probability $p_f=\Pr(y(X)>y_f)$ or an associated extreme quantile $y_f$ for expensive deterministic simulators with random inputs. A Gaussian process (GP) surrogate is fit to an initial space-filling design (e.g., Latin hypercube) and then augmented one run at a time using a sequential criterion that targets the failure/quantile contour. Two acquisition criteria are compared: expected improvement (EI) for contour estimation and a hypothesis-testing/standardized-discrepancy criterion (Roy & Notz) that prioritizes points with predictive mean near the contour and large predictive uncertainty. The work also addresses practical implementation choices: how to choose an initial design (recommend oversampling relevant input tails), how to model the input distribution (including tail-emphasizing empirical mixtures), and how to build an efficient Monte Carlo set via weighted stratified sampling to improve extreme-tail estimation. Case studies (a short-column limit-state function and a floor-system simulator) show the discrepancy criterion converges faster and yields substantially smaller RMSE than EI for extreme probabilities/quantiles under the same run budget, and the authors propose diagnostics to judge convergence when truth is unknown.","Sequential selection is framed as choosing the next simulator input $x^*$ to best learn the contour $y(x)=y_f$ while using a GP surrogate with predictive mean $\hat m(x)$ and variance $\hat v(x)$. The EI-based contour improvement is $I(x)=\frac{\alpha^2}{2}v(x)-(y(x)-y_f)^2$ when $|y(x)-y_f|<\alpha\sqrt{v(x)}$ (else 0), and $x^*=\arg\max_{x\in\mathcal X_{cand}}\mathbb E[I(x)]$. The discrepancy (hypothesis-testing) criterion uses the standardized distance and selects $x^*=\arg\min_{x\in\mathcal X_{cand}}\frac{|\hat m(x)-y_f|}{\sqrt{v(x)}}$, equivalently maximizing $\Pr\!","In the short-column example with total budget $n=40$ (initial $n_0=20$ plus 20 sequential points) and true failure probability 0.0025, the final probability-estimate RMSE over 10 repeats is 0.00439 (EI) vs 0.00014 (discrepancy) for random initial designs, and 0.00094 (EI) vs 0.00011 (discrepancy) for uniform/tail-oversampling initial designs. For estimating the extreme 0.0025-quantile (true value 0), final RMSE is 0.07592 (EI) vs 0.01287 (discrepancy) with random initials, and 0.04999 (EI) vs 0.00991 (discrepancy) with uniform initials. In the floor-system simulator with $d=8$, $n=30$ (20 initial + 10 sequential) and target $p_f=0.001$ / 0.999-quantile, the discrepancy criterion yields probability and quantile trajectories that converge tightly to the reference values (true quantile taken as 3.88 inches) across 10 repeats when paired with a stratified/weighted MC set of 12,800 points. Across examples, the discrepancy criterion converges faster and places more added runs near the failure boundary than EI, and uniform/tail-emphasizing initial designs improve performance.",None stated.,"The methods assume a deterministic simulator and rely on GP surrogate adequacy; performance may degrade with strong nonstationarity, discontinuities, or high effective dimension without additional structure (e.g., active subspaces). The sequential criteria are evaluated largely via two case studies and particular budget/settings; broader benchmarking versus other rare-event/surrogate approaches (e.g., subset simulation, importance sampling with surrogate refinement, Bayesian optimization variants, or advanced acquisition functions for reliability) is limited. Candidate-set optimization is performed over finite sampled sets (e.g., 10,000 candidates or reusing the MC set), which may miss better points in continuous spaces and can be sensitive to candidate-set size and sampling scheme.",None stated.,"Extend the approach to stochastic simulators (heteroscedastic noise) and to dependent/structured input distributions beyond independent marginals, including copula-based modeling and uncertainty in the input distribution itself. Develop principled acquisition functions specifically optimizing rare-event probability/quantile loss (e.g., variance of $\hat p_f$ under stratified estimators) and scalable optimizers that do not depend on large discrete candidate sets. Provide open-source implementations and larger comparative studies against modern reliability/rare-event methods (subset simulation, adaptive importance sampling, multifidelity surrogates) and investigate robustness under model misspecification and higher dimensions.",1908.05357v1,https://arxiv.org/pdf/1908.05357v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:08:37Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Other,Prediction|Cost reduction|Other,Other,"3 factors (main surrogate for Qext uses x=[Np, dp, σ]; later S11 model uses 4 inputs including θ)",Other,Simulation study|Other,TRUE,MATLAB|Fortran|Other,Not provided,NA,"The paper proposes a systematic surrogate-modeling workflow for optical properties of fractal (soot) aggregates using Gaussian Process (GP) regression combined with an adaptive sequential design of experiments (DOE) strategy to minimize expensive simulations. Training inputs are selected sequentially using a modified exploration–exploitation algorithm (Ajdari & Mahlooji) initialized with a Latin Hypercube Design plus boundary points; candidate regions are identified via Delaunay triangulation and new points are added at the highest-scoring simplex centroid. Surrogates for extinction efficiency Qext are trained using squared-exponential and Matérn covariance functions with ARD hyperparameters; points are first selected using fast RDG-FA predictions and then re-evaluated with high-fidelity DDA (DDSCAT) to build the final surrogate. The resulting GP surrogates are compared against several uniform-grid databases with cubic-spline interpolation, showing that GP-based sequential designs achieve comparable or better accuracy with far fewer sampled points, especially for extrapolation. A preliminary extension builds a GP surrogate for the error in RDG-FA predictions of the directional Mueller-matrix element S11 (with θ as an additional input) to correct RDG-FA and combine its speed with DDA-level accuracy.","Initial design uses Latin Hypercube Design (LHD) plus corner and edge-midpoint boundary points; sequential additions use a weighted exploration–exploitation score over Delaunay simplices: exploration score φi = (Ai/Amax)·10 and exploitation score Δi = (δi/δmax)·10 with δi computed from pairwise absolute response differences at simplex vertices, then total score Θi = α φi + (1−α) Δi and the next sample is the simplex centroid (with integer rounding for Np). Stopping is based on 5-fold cross-validation error using root relative square error (RRSE) and aggregated error E (Eqs. 32–34). GP regression uses standard predictive mean/variance µ* = K*^T Ky^{-1} y and Σ* = K** − K*^T Ky^{-1} K* with covariance choices including squared-exponential ARD k(xp,xq)=σf^2 exp(−(xp−xq)^TΛ^{-2}(xp−xq)/2) and Matérn ARD k(xp,xq)=σf^2 f_d(r_d) exp(−r_d).","For DDA-based Qext surrogate modeling at tolerance ε=10^{-4}, the squared-exponential ARD design used N=106 training points, while the Matérn( d=5 ) ARD design used N=192 (Table 3). In comparisons against four uniform-grid databases (N=150, 275, 486, 891) with cubic-spline interpolation/extrapolation, GP surrogates achieved similar interpolation performance but substantially better extrapolation behavior than databases (which showed particularly poor extrapolation), with the Matérn-based surrogate slightly outperforming the squared-exponential one in extrapolation accuracy (Figure 6). Orientation averaging used 216 directions, and configuration averaging indicated that Nr=200 aggregate realizations were sufficient to keep Qext averages within ±2% of the Nr=500 reference across tested dp values (Figure 4). For S11, GP-based error surrogates markedly improved RDG-FA angular predictions toward DDSCAT results, and outperformed database interpolation in extrapolation error (Figure 8).","The study limits the number of unknown input parameters to three (Np, dp, σ) for Qext, despite uncertainties in other influential parameters such as complex refractive index and fractal parameters. It also notes that particle overlapping and necking effects are ignored in the current modeling setup. For directional-property modeling (S11), the authors describe the presented S11 surrogate as preliminary and state that further improvements are needed (e.g., sampling based directly on S11 or improving the sampling algorithm).","Training-point selection is driven by RDG-FA trends rather than DDA responses, which can bias the sequential design toward regions RDG-FA deems informative and may miss DDA-specific nonlinearities (especially under extrapolation or near regime boundaries). The adaptive design relies on Delaunay triangulation, which can become computationally challenging or unstable in higher dimensions (the method is demonstrated mainly for 2–3D inputs). Comparisons are limited to uniform grids with cubic splines; stronger baselines for computer experiments (e.g., maximin Latin hypercube, Sobol sequences, Bayesian optimization/active learning acquisition functions like EI/UCB) are not included.","The authors state the methodology can be extended to include uncertainties in refractive index and fractal parameters, and to incorporate particle overlapping and necking effects that may substantially affect optical behavior. They also state that a more comprehensive study is required to better capture details in estimating directional properties (e.g., improving accuracy of S11 modeling).","A natural extension is to perform fully DDA-driven adaptive sampling (or multi-fidelity active learning that combines RDG-FA and DDA within one acquisition function) to reduce bias while controlling compute cost. The DOE could be reframed in a Bayesian active-learning framework (e.g., maximize integrated variance reduction/I-optimality or expected improvement in error metrics) and compared against modern space-filling designs (maximin LHD, Sobol) under fixed budgets. Publishing an implementation (e.g., MATLAB scripts integrating GPML with the sequential design) and providing reproducible benchmarks would enable broader adoption and facilitate sensitivity studies over tolerance ε, noise models, and hyperparameter priors.",1909.03280v1,https://arxiv.org/pdf/1909.03280v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:09:16Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Model discrimination,Other,Variable/General (examples include 1 design variable: temperature; 2 design variables: temperature and reactor volume; parameters inferred: 1–3),Other,Simulation study|Other,TRUE,Python|C/C++|Other,Public repository (GitHub/GitLab),https://bitbucket.org/ericawalk/datascience/src/master/,"The paper develops and demonstrates Bayesian design of experiments (BED) using expected information gain, computed as the Kullback–Leibler divergence between posterior and prior, to choose experimental observables and operating conditions that best reduce model uncertainty. Three examples are used: (1) a Bayesian linear regression benchmark where the “design” is which parameter is observable via the design matrix, enabling validation against an analytical posterior; (2) a nonlinear reversible-reaction equilibrium model where temperature is the design variable and information gain is evaluated on a grid of temperatures; and (3) a catalytic membrane reactor where temperature and reactor volume are jointly optimized for maximum expected information gain using both grid search and optimization (steepest ascent and differential evolution). Bayesian inference is performed with Metropolis–Hastings MCMC (with multilevel sampling in QUESO) at each candidate design point to estimate posteriors and information gain. Results show information gain decreasing with temperature in the equilibrium example, and identify a high-information region at higher volumes and lower temperatures in the membrane reactor example, with optimization algorithms leveraging grid-search initialization. The work introduces and validates the BuffaloROAM Python toolkit alongside QUESO and demonstrates practical workflows for BED in chemical kinetics settings.","BED objective is the expected information gain defined as a KL divergence between posterior and prior, appearing as the information-gain term in the evidence decomposition: \(\log p(D\mid M)=\mathbb{E}[\log p(D\mid\theta,M)]-\int p(\theta\mid D,M)\log\frac{p(\theta\mid D,M)}{p(\theta\mid M)}\,d\theta\). Bayesian inference is carried out via Metropolis–Hastings proposals \(\theta' = \theta + Q\), with acceptance based on likelihood–prior ratios. Example models include: linear regression \(D=X\theta+\epsilon\) with Gaussian prior/likelihood yielding closed-form posterior \(\Sigma_{post}=(\Sigma_{pri}^{-1}+\sigma^{-2}X^TX)^{-1}\), \(\mu_{post}=\Sigma_{post}(\Sigma_{pri}^{-1}\mu_{pri}+\sigma^{-2}X^TD)\); equilibrium reaction with \(K=\exp(-\Delta G/(k_BT))\) and \(C_A=1/(1+K)\); and a membrane reactor ODE model \(dF_A/dV=-r_A\), \(dF_B/dV=r_A-R_B\) with rate/transport terms \(r_A=-k_1 C_A + k_{-1} C_B\), \(R_B=k_m C_B\).","In the chemical equilibrium example, information gain monotonically decreases as temperature increases over five grid points from 298.15 K to 698.15 K, indicating lower temperatures are most informative for \(\Delta G\) given the assumed constant observation noise. In the catalytic membrane reactor grid search (temperature 298.15–1000 K; volume 100–1100 cm\u00b3), a reported peak information gain is 2.691 at 398.15 K and 1100 cm\u00b3, while the minimum reported is 2.492 at 498.15 K and 600 cm\u00b3. The differential evolution optimizer converges near the initial guess and reports a maximum around 439 K and 890 cm\u00b3. The Bayesian linear regression case matches analytical posteriors for both choices of design matrix (observing parameter 1 vs parameter 2), serving as validation for the MCMC-based implementations (QUESO and BuffaloROAM).","The steepest-ascent optimization can fail to converge when the information-gain surface is ill-conditioned or non-differentiable, and it requires an initial direction and learning rate. The authors note that both steepest ascent and differential evolution benefited from initial guesses informed by a brute-force grid search, implying optimization performance depends on initialization. In the chemical reaction example, they assume the model discrepancy/observation noise is constant across temperature (temperature-independent), which conditions the conclusion that the lowest temperature is optimal.","The BED evaluations rely on synthetic data generation (often from the prior model, and in one case with an imposed 10% parameter bias), so empirical validity with real experimental data and real-model discrepancy is not established. Information gain is estimated via repeated MCMC at each design point, which may be computationally prohibitive for higher-dimensional designs or more complex kinetics; the paper does not quantify computational cost, convergence diagnostics, or sensitivity to MCMC tuning. Competing BED estimators/approximations (e.g., Laplace/variational, nested Monte Carlo estimators with bias control) are not systematically compared, so robustness of EIG estimation accuracy is unclear. The designs treated are continuous but explored largely via coarse grids and local/global optimizers without formal guarantees, and constraints/practical experimental feasibility (e.g., temperature/volume limits, costs) are not incorporated into the optimization objective.","The paper suggests that, especially for nonlinear models where a rule-of-thumb is unavailable, a combination of brute-force grid search plus an optimization algorithm is advantageous for finding optimal designs, implying further development/application of such hybrid strategies. No other explicit future-work items are stated beyond this recommendation.","Apply the BED workflow to real experimental datasets in chemical kinetics to validate that predicted high-information conditions translate to improved parameter inference in practice, including explicit treatment of model discrepancy and bias. Develop faster and more scalable EIG estimators (e.g., Laplace, surrogate modeling/Gaussian processes, multi-fidelity) to reduce the need for full MCMC at every candidate design point. Extend the approach to multi-objective and cost-aware design (e.g., information gain per unit cost/time) and to sequential designs where experiments are chosen adaptively after observing data. Provide open, reproducible implementations (e.g., packaged code and documented workflows) and standardized comparisons against established BED libraries and baseline design strategies.",1909.03861v1,https://arxiv.org/pdf/1909.03861v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:10:01Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Optimization|Prediction|Parameter estimation|Other,Bayesian D-optimal|Space-filling|Other,Variable/General (applies to generic design spaces; examples include 1D–6D synthetic and ML hyperparameter tuning grids),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|MATLAB,Not provided,https://github.com/pytorch/botorch|http://gaussianprocess.org/gpml/code/matlab|https://www.sfu.ca/~ssurjano/optimization.html|https://www.sfu.ca/~ssurjano/integration.html,"The paper proposes BINOCULARS, a general framework for efficient nonmyopic finite-horizon sequential experimental design (SED) that approximates the intractable Bellman-optimal adaptive policy. At each iteration it computes an optimal remaining-horizon batch design under a chosen utility, then evaluates only a single point selected from that batch, effectively “reducing” sequential design to repeated batch-design subproblems. The framework is instantiated for Bayesian optimization using batch expected improvement (q-EI) and for Bayesian quadrature using a batch entropy/DPP objective, providing (to the authors’ knowledge) the first nonmyopic policy tailored for Bayesian quadrature. The authors justify the approach by relating the batch objective to a lower bound on the true lookahead value function and empirically show improved performance over myopic baselines (EI for BO, uncertainty sampling for BQ) and competitive performance versus prior nonmyopic baselines (rollout, GLASSES) at much lower computational cost. Experiments span synthetic benchmarks and real-world hyperparameter-tuning and model-evidence/quadrature tasks, demonstrating characteristic nonmyopic exploration-then-exploitation behavior.","They define the k-step optimal adaptive value via a Bellman recursion: $Q_k(x\mid D)=\mathbb{E}_y[u(y\mid x,D)]+\mathbb{E}_y[\max_{x'} Q_{k-1}(x'\mid D\cup\{(x,y)\})]$. BINOCULARS replaces the future adaptive term with a batch (non-adaptive) optimization leading to $B(x\mid D)=\mathbb{E}_y[u(y\mid x,D)]+\max_{X':|X'|=T-1}\mathbb{E}_y[Q(X'\mid D\cup\{(x,y)\})]$, a lower bound on the true lookahead value. For BO they use improvement utility $u(Y\mid X,D_0)=(\max_{x_j\in X} y_j-y_0)_+$ and optimize batch EI (q-EI); for BQ they use a GP-entropy utility $H(Y\mid X)=\tfrac12\log\det(2\pi e\,K(X,X))$, whose maximizer corresponds to a DPP mode.","On nine selected “hard” synthetic BO functions (100 repeats), nonmyopic q.EI.s variants outperform EI on average; e.g., EI average GAP 0.555 vs 12.EI.s average GAP 0.635 (Table 1). On seven real hyperparameter-tuning/robot-pushing objectives (50 repeats), the best BINOCULARS variant reported (6.EI.s) achieves average GAP 0.831 vs EI 0.779, and is statistically comparable to strong nonmyopic baselines such as 2.G (0.836) while being much faster per iteration (Figure 3). For BQ across 8 benchmark/real integrands (100 repeats), BINOCULARS (e.g., 2.DPP.s) improves median fractional error over UNCT on average (UNCT 0.068 vs 2.DPP.s 0.037; Table 3) and is markedly faster than rollout while only slightly slower than UNCT (Figure 4(c)). Results also suggest sampling from the optimized batch ("".s"") often outperforms choosing the best-immediate-reward point ("".b""), and increasing batch size q helps in BO up to a point but is not consistently beneficial in BQ.","The authors note that full lookahead (setting batch size $q$ equal to the remaining budget) is not necessarily best in practice because the model is always misspecified; planning too far ahead can hurt empirical performance and larger q increases computation. They also remark that the tightness of their lower-bound connection is an open question related to the adaptivity gap, and that better theory is needed to guide the choice of lookahead horizon q.","The approach depends on successfully solving a high-dimensional continuous batch optimization (q-EI/DPP mode finding), which can be sensitive to local optima, gradients, and initialization; performance may degrade in very high-dimensional or constrained/discrete spaces beyond the studied settings. Comparisons focus on EI/UNCT, rollout, and GLASSES; other modern nonmyopic BO/BQ strategies (e.g., entropy-search variants, Thompson sampling with lookahead approximations, or alternative batch BO criteria) are not systematically benchmarked. The paper uses GP models (with learned hyperparameters) and assumes conditional modeling choices that may be brittle under strong nonstationarity, heteroscedastic noise, or temporal dependence, which are not extensively stress-tested.","They propose deriving theory to guide selection of the lookahead horizon (batch size) q for BINOCULARS, echoing recent guidance on rollout horizons. They also suggest studying tighter theoretical characterization, including explicit bounds on the adaptivity gap for broader classes of sequential design problems.","Develop self-tuning/adaptive-q schemes that trade off computation and model misspecification online (e.g., selecting q based on posterior uncertainty, calibration, or marginal value of information). Extend BINOCULARS to settings with constraints and mixed discrete-continuous design spaces (common in hyperparameter tuning) using combinatorial/relaxed batch solvers. Provide robustness analyses and benchmarks under non-Gaussian noise, heteroscedasticity, and autocorrelation, and release a reusable implementation (e.g., BoTorch extension + MATLAB/Python parity) to facilitate adoption and reproducibility.",1909.04568v3,https://arxiv.org/pdf/1909.04568v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:10:41Z
TRUE,Response surface|Optimal design|Other,Prediction|Optimization|Cost reduction|Other,Not applicable,Variable/General (examples shown with 2 variables and 4 variables; general p-dimensional setting),Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper proposes an efficient interval uncertainty optimization framework for structural design that replaces the expensive inner-loop maximization over uncertain intervals with a surrogate-based evaluation. The surrogate is a Quasi-sparse Response Surface (QSRS) built from a large Chebyshev polynomial dictionary, where an elastic-net (combined l1 and l2 penalties) selects a sparse subset of basis functions so accurate models can be fit using relatively few samples. Sampling points are generated via a uniform design method, and global search in the outer loop is performed with a Multi-Island Genetic Algorithm (MIGA); interval arithmetic on the Chebyshev-based QSRS is used to compute objective/constraint bounds directly. Two test problems (a 2D “Three-Hump” function and a 4-variable pressure-vessel design) show substantially fewer required samples per MIGA iteration versus a Chebyshev surrogate interval-optimization baseline (e.g., 30 vs 49 samples; 100 vs 420 samples) with comparable validated objective bounds. The contribution advances surrogate-assisted robust/interval optimization by combining sparse polynomial surrogates with interval arithmetic to reduce computational cost in bounded-uncertainty design problems.","The QSRS surrogate is a polynomial expansion $\hat f(\mathbf{x})=\sum_{i=1}^{n} \phi_i(\mathbf{x})\,\beta_i$ using (tensor-product) Chebyshev polynomial basis functions, written in matrix form $\hat{\mathbf{f}}=\Phi\boldsymbol\beta$. Coefficients are estimated via elastic net: $\min_{\boldsymbol\beta}\ \|\mathbf{f}-\Phi\boldsymbol\beta\|_2^2+\lambda_1\|\boldsymbol\beta\|_1+\lambda_2\|\boldsymbol\beta\|_2^2$, with $\lambda_2$ chosen by cross-validation. For interval evaluation, since Chebyshev terms satisfy $\cos(\theta)\in[-1,1]$, bounds of the surrogate output are computed by interval arithmetic, e.g. $[\hat y]=[\beta_0-\sum_i |\beta_i|,\ \beta_0+\sum_i |\beta_i|]$ (as used to avoid an inner maximization loop).","In the 2-variable Three-Hump example (Chebyshev order 6 baseline), the interval-optimization method uses 49 sampling points and 49 basis functions per MIGA iteration, while QSRS uses 30 sampling points with 90 basis functions. The reported optimum objective values (validated by scanning) are $-4.3603$ for the baseline and $-4.3332$ for QSRS, with QSRS argued to avoid local-optimum issues caused by surrogate accuracy/wrapping effects at some iterates. In the 4-variable pressure-vessel example, the baseline uses 420 samples and 210 basis functions per iteration, while QSRS uses 100 samples with 300 basis functions; both methods report the same optimum and validated objective bound ($4.6315\times10^4$). Overall, QSRS achieves similar validated performance with markedly fewer samples per iteration (about 25% of the baseline sample size in the engineering example).",None stated.,"The work frames sampling as “uniform design” and basis selection via elastic net, but does not provide a clear DOE/optimal-design rationale (e.g., optimality criteria such as D-/I-optimality) or a sensitivity analysis of how the sampling plan affects surrogate error and interval bound tightness. Interval arithmetic bounds based on $\cos(\theta)\in[-1,1]$ can be conservative and may still suffer from wrapping/dependency effects for multivariate tensor-product expansions; the paper provides limited robustness checks for bound tightness in higher dimensions. Comparative evaluation is restricted to Chebyshev-surrogate interval optimization with fixed polynomial order and specific settings; broader comparisons (e.g., Kriging/GP surrogates, PCE, adaptive sampling) and real industrial datasets are not explored.",None stated.,"Extend the approach to adaptive/sequential sampling where new runs are added near candidate optima or where interval bounds are widest, to further reduce evaluations while controlling surrogate error. Study alternative sampling/design strategies (e.g., space-filling Latin hypercube, D-/I-optimal designs) and quantify surrogate accuracy/bound tightness tradeoffs as dimension grows. Develop and test robust versions for correlated/structured uncertainty, constraints with multiple responses, and noisy simulations/experiments, and provide open-source implementations to facilitate adoption.",1909.06279v1,https://arxiv.org/pdf/1909.06279v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:11:15Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Robustness|Other,Not applicable,"Variable/General (explicit factors include model architecture, optimizer, hyper-parameter configuration, PRNG seed, and rerun/nondeterminism).",Other,Simulation study|Other,TRUE,R|Other,Not provided,NA,"This paper proposes DECoVaC, a design-of-experiments methodology for machine learning experiments that explicitly controls and separates multiple sources of variability (model/architecture, optimizer, hyper-parameter configuration, PRNG seed, and unintended nondeterminism across reruns). The experimental plan uses a randomized re-seeding framework: for each model–optimizer combination, multiple hyper-parameter configurations are sampled and training is rerun multiple times per (seed, configuration) to generate clustered observations. The analysis fits a linear mixed-effects model with fixed effects for experiment groups (e.g., model–optimizer) and random effects for seeds and hyper-parameter configurations, enabling likelihood-ratio tests and ANOVA-style comparisons of mean performance across groups. In a few-shot learning case study on miniImagenet (Matching Networks, Prototypical Networks, TADAM; SGD vs Adam), the authors find statistically significant variability attributable to both seeds and hyper-parameter configurations, while reruns under identical settings are stable. The work advances ML reproducibility practice by providing a structured DOE + mixed-model testing framework to quantify which controlled variability components materially affect reported performance.","The core model is a linear mixed model: $y = X\beta + Zb + \alpha + \varepsilon$, with random effects $b \sim \mathcal N(0,\sigma^2 I)$ and $y\mid b \sim \mathcal N(X\beta+\alpha+Zb,\sigma^2 W^{-1})$. In their specific decomposition, observation $y_{ijk}$ uses fixed effects for experiment group ($\beta X_i + \alpha$) and an error split into random intercepts for seed and hyper-parameter configuration: $\varepsilon_{ijk}=b_{0j}+b_{1k}+\epsilon_i$, where $b_{uj}\sim \mathcal N(0,\sigma_u^2)$ and $\epsilon_i\sim \mathcal N(0,\sigma_\epsilon^2)$. Hypotheses are tested via likelihood-ratio tests on inclusion of random effects and via ANOVA-style tests (with small-sample df corrections) for differences among experiment means.","Using 10 PRNG seeds, 15 hyper-parameter configurations per seed, and 10 reruns per (model, optimizer, seed, configuration), they report 3,000 experiments per model (9,000 total across 3 models). Likelihood-ratio tests indicate significant random-effect contributions from both seeds (LRT=24.42, p=7.76e-07) and hyper-parameters (LRT=1302.28, p≈3.61e-285). Estimated random-effect variances are reported as: seed intercept variance 3.09e-05 (SD 0.00556), hyper-parameter intercept variance 0.001792 (SD 0.04233), residual variance 0.0004338 (SD 0.02083). Fixed-effect ANOVA over experiments shows significant mean differences (F=27.95, p≈5.90e-19), while pairwise rerun-mean comparisons within the same model–optimizer setting show no statistically significant differences (supporting rerun stability under identical settings).","They note that ideally PRNGs providing parallel pseudo-independent streams should be used across experiments, but such PRNGs are rarely available in common ML libraries; they instead use multiple seeds of a quality PRNG (e.g., Mersenne Twister) as an acceptable practical approximation. They also state an intent to release a ready-to-run notebook and Docker container, implying the current manuscript does not yet provide that tooling.","The DOE uses random sampling of hyper-parameters rather than an explicitly optimized design (e.g., D-/I-optimal) over a defined factor space, so efficiency and coverage of the hyper-parameter region are not guaranteed. The case study is restricted to a single dataset/task family (miniImagenet 5-way 5-shot) and three algorithms, which may limit generalizability of variance component magnitudes to other domains or training pipelines. The approach assumes the linear mixed model (Gaussian errors, additive random intercept structure, independence conditional on random effects) is adequate; heavy-tailed performance distributions or heteroskedasticity across models/optimizers could affect inference. Treatment of unintended nondeterminism is operationalized through reruns, but potential time-varying system effects (hardware load, library versions) and autocorrelation are not explicitly modeled.","They state they intend to release a ready-to-run notebook (with a Docker container) implementing the statistical tests and providing an easy-to-follow example, and to release the data gathered in their case study to exemplify the methodology.","A natural extension is to incorporate explicit optimal design criteria (e.g., D-/I-optimal or Bayesian optimal design) to allocate runs across seeds, hyper-parameter regions, and model variants for maximal power per compute budget. Robust mixed-model variants could address non-Gaussian/heteroskedastic outcomes and include random slopes or cross-classified effects (e.g., seed-by-optimizer interactions). Extending the framework to autocorrelated or drifting experimental environments (time/block effects) and to multivariate endpoints (accuracy, calibration, training time, energy) would improve practical utility. Providing open-source software with templates for common ML settings (distributed training, GPU nondeterminism controls) would facilitate adoption and benchmarking.",1909.09859v1,https://arxiv.org/pdf/1909.09859v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:12:00Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Robustness|Other,Bayesian D-optimal|Bayesian A-optimal|Compound criterion|Other,Variable/General (examples include k=3 variables with n=16 runs; k=1 variable with n=20; cubic spline example n=10),Healthcare/medical|Pharmaceutical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops an extended Bayesian decision-theoretic design (Bayesian optimal design) framework where the expected loss for choosing a design is taken under an alternative “designer” model, while inference after data collection is performed under a possibly different “fitted” model. It formalizes the resulting external expected loss and establishes key properties, including an inequality showing the external expected loss upper-bounds the internal expected loss under the designer model for compatible models and optimal actions. To understand behavior, the authors derive an asymptotic approximation to the external expected loss under model misspecification, highlighting that generator losses (e.g., self-information, squared error) and their composite counterparts (entropy, trace variance) are no longer equivalent and induce a bias–variance trade-off. The framework is illustrated with (i) linear regression design under a full-treatment designer model leading to criteria that promote replication (DE/AE-optimality), (ii) nonlinear Michaelis–Menten parameter estimation under Gaussian-process model discrepancy yielding designs that compromise between Bayesian-optimal and space-filling behavior, and (iii) (supplement) cubic spline models with unknown basis dimension using predictive loss. Computationally, expected losses are approximated using nested Monte Carlo and optimized via approximate coordinate exchange, demonstrating practical design construction under the proposed framework.","External expected loss is defined as $L_{DF}(\Delta)=\mathbb{E}_{\theta,y\mid D,\Delta}\{\mathbb{E}_{\gamma\mid \theta,y,F,\Delta}[\lambda(\beta,y,F,\Delta)]\}$ with $\beta=(\gamma^T,\theta^T)^T$. Internal expected loss under the fitted model is $L_{FF}(\Delta)=\mathbb{E}_{\beta,y\mid F,\Delta}[\lambda(\beta,y,F,\Delta)]$. An asymptotic approximation uses a misspecified-posterior normal approximation $\pi(\beta\mid y,F,\Delta)\approx N(\hat\beta,\tilde I^{-1})$ and leads to $L^*_{DF}(\Delta)=\mathbb{E}_{\theta\mid D,\Delta}\Big(\mathbb{E}_{\hat\beta\mid \theta,D,\Delta}\mathbb{E}_{\gamma\mid \hat\beta,\theta,F,\Delta}[\lambda^*(\beta,\hat\beta,\Delta)]\Big)$ where $\hat\beta\mid\theta,D,\Delta\sim N(\tilde\beta,\tilde K)$ with sandwich variance $\tilde K=\tilde I^{-1}\tilde J\tilde I^{-1}$.","For compatible models and optimal actions, the paper proves $L_{DF}(\Delta)\ge L_{DD}(\Delta)$, i.e., designing under an alternative fitted model yields an upper bound on the designer-model internal expected loss. In the normal linear regression vs full-treatment example with $n=16$, $k=3$, and $p=10$, the DE-optimal design had D-efficiency 85% while the D-optimal design had DE-efficiency only 7% (and similarly for squared error: 84% vs 10%), showing classical D/A-optimal designs can be highly non-robust under the alternative-model criterion and that the proposed criteria favor replication (DE/AE designs had $q=10$ unique treatments vs 16 and 14 for D/A). In the Michaelis–Menten example with $n=20$, the external squared-error-optimal design used 13 unique concentrations versus 12 for external trace-variance and internal squared-error designs; efficiencies reported include (external SE design) 100% under its own criterion, 67.0% under external TV, and 79.7% under internal SE, indicating a more space-filling compromise under generator loss when discrepancy is present. The asymptotic analysis explains these differences via extra bias/variance terms appearing for generator losses but not for composite losses under external expected loss.",None stated.,"The examples and performance claims rely heavily on Monte Carlo approximations (nested Monte Carlo with large $B$ and $\tilde B$) and a particular optimizer (approximate coordinate exchange), but the paper does not provide sensitivity analyses to Monte Carlo error, optimizer settings, or local minima, nor implementation details sufficient for reproducibility. The asymptotic approximation depends on regularity conditions and large-sample behavior; its practical accuracy for small-to-moderate run sizes typical in DOE is not systematically validated. Applications focus on relatively low-dimensional, continuous-factor settings; extensions to high-dimensional screening, constrained designs, discrete factors, or complex randomization restrictions are not empirically explored.","The authors state future work will focus on Bayesian decision-theoretic design when accounting for incompleteness of underlying mathematical models (robust Bayesian inference / M-open paradigms) as in the non-linear model discrepancy setting. They also note the framework could be generalized beyond parameter estimation to model selection and prediction aims, and that exploring this generalization is a further avenue for future research.","Developing open-source software implementing external expected loss design (including nested MC and approximate coordinate exchange) and benchmarking it against existing Bayesian OED toolkits would improve adoption and reproducibility. More systematic robustness studies could assess sensitivity to misspecification of the designer model (e.g., GP discrepancy hyperpriors) and to prior choices, including self-starting or empirical Bayes variants. Extending the approach to multivariate responses, correlated/longitudinal data, generalized linear models, and designs with practical constraints (blocking, split-plot, discrete levels) would broaden applicability and clarify computational scaling and optimality behavior in realistic industrial experiments.",1909.12570v4,https://arxiv.org/pdf/1909.12570v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:12:42Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Model discrimination|Parameter estimation|Prediction|Other,Other,"Variable/General (d observable variables; intervention target j∈{1,…,d} and continuous intervention value x)",Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian experimental design framework for active causal structure learning in structural causal models with continuous variables and nonlinear functional relationships modeled using Gaussian process (GP) priors (“GP networks”). At each step, the experiment is an intervention of the form do(Xj=x), and the next intervention is chosen to maximize the expected information gain about the causal graph G (an information-theoretic utility). Because the intervention space is continuous (uncountably many x values), the expected information gain objective is approximated via Monte Carlo sampling from interventional distributions and then optimized efficiently using Bayesian optimization (specifically GP-UCB), run separately for each discrete intervention target j. The approach leverages closed-form GP marginal likelihood and predictive posterior calculations (under Gaussian noise) to update posteriors over graphs and to evaluate the design objective. A small illustrative bivariate simulation example is provided, showing rapid posterior concentration on the correct causal direction after a limited number of interventions; the paper also discusses open problems such as scaling over the super-exponential number of DAGs and handling flexible root-node marginals.","The environment is an additive-noise SCM: $X_i=f_i(\mathrm{Pa}^G_i)+\epsilon_i$ with independent noise. The Bayesian design utility is information gain in the causal graph, e.g. $U=\sum_{G\in\mathcal G} P(G\mid x_{-j},do(X_j=x))\log P(G\mid x_{-j},do(X_j=x))$, and the optimal intervention solves $(j^*,x^*)=\arg\max_{j,x}\sum_G P(G)\int P(x_{-j}\mid G,do(X_j=x))\log P(G\mid x_{-j},do(X_j=x))\,dx_{-j}$. This expected information gain is approximated by Monte Carlo: maximize $\sum_G P(G)\frac{1}{M}\sum_{m=1}^M \log P(G\mid x^{(m)}_{-j},do(X_j=x))$, and Bayesian optimization (GP-UCB: $a_t(x)=\mu_t(x)+\beta\sigma_t(x)$) is used to optimize over continuous $x$.","The paper presents a prototype simulation in a bivariate nonlinear setting with $X\sim\mathcal N(0,1)$ and $Y=2\tanh(X)+\epsilon$, $\epsilon\sim\mathcal N(0,0.1)$. Using 15 samples total (5 observational plus 10 interventional, alternating intervention target between X and Y with random intervention values), the posterior over graphs is reported to converge to the correct direction $X\to Y$ with >99% confidence after only ten interventions. The main empirical evidence is illustrative rather than a broad benchmark; the core result is the feasibility of computing the objective using GP closed forms and optimizing interventions via Bayesian optimization in a continuous intervention space.","The authors note several practical and computational issues: the super-exponential number of DAGs makes summing over all graphs expensive, so they suggest focusing on small d where enumerating DAGs is possible and treating scaling as orthogonal. They also highlight challenges in modeling flexible root-node marginals (suggesting Dirichlet process Gaussian mixtures) and in selecting/updating GP hyperparameters online, where repeated type-II ML updates can be burdensome and change the model, while fully Bayesian hyperparameter treatment loses closed-form marginal likelihood.","The method assumes causal sufficiency, acyclicity, and an additive-noise SCM with independent noise; real systems often violate these (latent confounding, cycles/feedback, heteroskedastic or dependent noise), which can bias graph posteriors and design choices. The Monte Carlo estimator and BO loop can be computationally heavy even for small d, and the design objective depends on accurate sampling from interventional distributions under each candidate graph, which may be nontrivial as models become more complex (e.g., non-Gaussian root nodes). Empirical evaluation appears limited to a small illustrative example; without systematic comparisons (e.g., to alternative active intervention policies or acquisition strategies) it is hard to quantify robustness and sample-efficiency gains in broader settings.","They suggest extending root-node distributions beyond simple Gaussians by using Dirichlet process mixtures of Gaussians with efficient variational inference. They discuss alternative treatments for GP hyperparameters in an online setting, including hyperpriors and approximate inference or fitting hyperparameters once from a sufficiently large initial observational dataset and then fixing them. They also propose addressing scalability over DAGs via approximate inference (e.g., MCMC over graphs/orderings, minimal I-map MCMC) and combining the approach with constraint-based or independence-testing methods (e.g., RESIT) to improve tractability.","Developing a self-starting/robust version that handles unknown intervention noise, model misspecification, and mild violations of ANM assumptions (e.g., heteroskedasticity) would improve practical reliability. Extending the framework to richer intervention families (soft interventions, imperfect interventions, multiple simultaneous interventions, or constrained intervention sets) would align better with many laboratory and industrial settings. More comprehensive benchmarks—varying d, graph sparsity, functional complexity, and intervention costs—plus ablations comparing alternative acquisition functions and variance-reduction methods for the EIG estimator would clarify when BO-based intervention selection provides the biggest advantage.",1910.03962v1,https://arxiv.org/pdf/1910.03962v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:13:19Z
TRUE,Computer experiment|Bayesian design|Other,Prediction|Parameter estimation|Other,Space-filling|Not applicable,Variable/General (WRF application uses 5 inputs; benchmark uses 2 inputs; also multiple fidelity levels),Environmental monitoring|Other,Simulation study|Other,TRUE,MATLAB,Not provided,https://arxiv.org/abs/1910.08063,"The paper proposes Augmented Bayesian Treed Co-Kriging (ABTCK), a Bayesian multi-fidelity Gaussian-process emulator designed for computer experiments when simulation runs across fidelity levels are collected on non-nested designs. It augments the data by imputing missing runs to construct a hypothetical hierarchically nested design, enabling conditional conjugacy and efficient RJ-MCMC inference, and uses a binary treed partition to model nonstationarity/discontinuities in the response and in fidelity discrepancies. The method is demonstrated on a two-level benchmark with Latin hypercube designs that are not nested, and on a large-scale Weather Research and Forecasting (WRF) climate-model application with two grid resolutions and five Kain–Fritsch scheme parameters, where available runs are also non-nested. Empirically, ABTCK improves predictive accuracy and uncertainty quantification versus a stationary augmented co-kriging variant (ABCK) and a single-fidelity GP in the WRF study, and it captures local features via the treed partitioning. The work advances multi-fidelity DOE/emulation practice by making Bayesian co-kriging computationally feasible under non-nested experimental designs and by incorporating nonstationary structure through treed partitions.","Multi-fidelity autoregressive co-kriging is assumed: $y_t(x)=\xi_{t-1}(x)y_{t-1}(x)+\delta_t(x)$ for $t=2,\ldots,S$, with GP priors for $y_1(\cdot)$ and discrepancies $\delta_t(\cdot)$ and a basis expansion for $\xi_{t-1}(x)=w_t(x)^T\gamma_t$. Nonstationarity is handled by a binary tree partition $\{\mathcal{X}_k\}$ and applying the autoregressive model within each region, with a Chipman–George–McCulloch tree prior $\pi(T)$ on partitions. For non-nested designs, the method defines a completed nested design $\tilde X_{k,t}=\cup_{j=t}^S X_{k,j}$ and imputes missing outputs $\mathring y_{k,t}$ so the augmented likelihood factorizes over fidelities: $f(\tilde y|\cdot)=\prod_k f_k(\tilde y_{k,1}|\cdot)\prod_{t=2}^S f_k(\tilde y_{k,t}|\tilde y_{k,t-1},\cdot)$, enabling conjugate Normal–Inverse-Gamma updates and RJ-MCMC over $T$.","On a two-level benchmark with non-nested Latin hypercube designs ($n_1=120$, $n_2=30$), ABTCK achieved MSPE 0.0031 versus 0.0264 for the stationary augmented co-kriging (ABCK), indicating large gains from treed nonstationarity modeling. In the WRF application (two fidelities: 12.5 km and 25 km; total 240 runs; five inputs), repeated holdout evaluation (60 repetitions) reported mean MSPE 0.2118 (single-fidelity GP), 0.1205 (ABCK), and 0.0974 (ABTCK), with 95% credible-interval coverage improving from 0.613 (GP) to 0.840 (ABCK) and 0.945 (ABTCK). Reported average runtimes were 368 s (GP), 1804 s (ABCK), and 1240 s (ABTCK), showing ABTCK both more accurate and faster than ABCK due to smaller covariance inversions per tree region.","The authors note that providing a theoretical proof that overlapping designs across adjacent fidelity levels are preferable (for both computational and modeling reasons) is out of scope. They also state that using different partitions for the scale discrepancy $\xi_t(x)$, additive discrepancy $\delta_t(x)$, and baseline $y_1(x)$ could yield a more flexible model, but it is unclear whether conditional posteriors can still be marginalized to keep computation feasible.","The approach assumes deterministic simulators and conditional independence implied by the autoregressive co-kriging structure; if fidelity relationships are non-Markovian or non-monotone, performance may degrade. Treed partitions introduce discontinuities at region boundaries and can be sensitive to tree-prior settings and RJ-MCMC mixing; diagnostics and robustness to these choices are not extensively explored. The DOE aspect focuses on handling existing non-nested designs via imputation rather than proposing optimal acquisition rules; without careful design overlap across fidelities, imputation uncertainty may be large and predictions may rely heavily on GP extrapolation. No public code is provided, and computational feasibility for higher input dimension or more fidelity levels may still be challenging despite partitioning/augmentation.","They propose adding basis selection for mean and discrepancy bases via spike-and-slab priors with Gibbs updates, and using the treed prior to mitigate non-identifiability between scale and additive discrepancies. They mention a possible extension using different partitions for $\xi_t(x)$, $\delta_t(x)$, and $y_1(x)$ (with uncertainty about computational tractability). They also state they are working on a sequential design procedure for multifidelity simulations that accounts for non-hierarchically nested designs.","A natural extension is to develop explicit sequential/adaptive multi-fidelity acquisition criteria (e.g., Bayesian optimization or mutual-information designs) tailored to ABTCK’s augmented nested representation and treed nonstationarity. Robustness studies could relax assumptions of independence and stationarity within regions, incorporate heteroskedastic noise or stochastic simulators, and handle autocorrelated outputs common in climate models. Software packaging (e.g., MATLAB toolbox or R/Python implementation) with reproducible benchmarks would improve adoption, along with guidance on choosing tree priors, overlap requirements, and computational scaling. Extending to multivariate outputs (e.g., spatial fields) or calibration-with-observations settings would broaden applicability in climate and engineering UQ.",1910.08063v1,https://arxiv.org/pdf/1910.08063v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:14:05Z
TRUE,Optimal design|Other,Parameter estimation|Model discrimination|Cost reduction,A-optimal|D-optimal|E-optimal|Other,Variable/General (application estimates 2 parameters: k5 and k7; 3 candidate sensors with binary selection),Manufacturing (general),Other,TRUE,MATLAB,Not provided,NA,"The paper proposes a frequentist framework to detect model uncertainty by combining parameter identification (nonlinear least squares), optimal design of experiments (DOE) for sensor placement, and hypothesis testing on parameter consistency across data splits. DOE is formulated as a mixed-integer nonlinear program selecting an optimal subset of sensors (binary weights) to minimize the uncertainty of estimated model parameters, using standard optimality criteria (A-, D-, and E-criteria), with E-optimality chosen in the case study. Model inadequacy is detected by comparing parameter estimates from calibration vs. validation datasets using confidence ellipsoids derived from an approximate covariance matrix of the parameter estimator, with Bonferroni-corrected testing across multiple scenarios. The method is demonstrated on a mechanical forming machine (3D Servo Press) to discriminate among competing friction models: a no-friction model, a Coulomb friction model, and a learned (Preisach/neural-network-like) hysteresis model. Numerical results show that the DOE-selected sensor subset (2 of 3 sensors) yields robust parameter estimation and that the hypothesis tests reject the simpler models while retaining the learned hysteresis model as consistent with both loading and unloading data.","The parameter identification is posed as a weighted least-squares problem with binary sensor-selection weights $\omega\in\{0,1\}^{n_S}$: minimize $\sum_{k,j,i} \omega_k^2\big((z_{ijk}-h_k(y_j,p,q_j))/\sigma_k\big)^2$ subject to the state equation $E(y_j,p,q_j)=0$. After eliminating states, the objective is $f(p,z,\Omega)=\tfrac12 r(p,z)^\top \Omega r(p,z)$ with $r=\Sigma^{-1}(z-h(y(p),p,q))$ and diagonal $\Omega$ built from $\omega$. The (approximate) parameter covariance is derived via implicit differentiation as $C(p,\Omega)=H(\Omega)^{-1}J(\Omega)^\top \Omega^2 J(\Omega)H(\Omega)^{-\top}$ (with $H=\partial_{pp}^2 f$ and $J=\partial_p r$), yielding confidence ellipsoids $\{p:(p-p_0)^\top C^{-1}(p-p_0)\le \chi^2_{n_p}(1-\alpha)\}$. DOE chooses $\omega$ to minimize a scalar design criterion $\Psi(C)$ (A: $\mathrm{tr}(C)$, D: $\det(C)$, E: $\lambda_{\max}(C)$) under constraints such as selecting at least $n_p$ sensors.","In the 3D Servo Press study, three sensors (displacements at points D, F, and B0) are available and two stiffness parameters ($k5, k7$) are estimated, so at least two sensors must be selected. Evaluating candidate subsets for model $M3$, the E-criterion $\Psi_E$ is minimized by sensor combination 110 (measure at D and F): $\Psi_E=3.5062\times 10^9$ versus $4.9568\times 10^9$ for using all sensors (111), while omitting sensor 2 (101) makes the criteria explode (near-singular covariance). With $n_{tests}=4$ and Bonferroni-corrected level $1.25\%$, model $M1$ is rejected in all scenarios (e.g., $\alpha_{min}=0.02\%$ or $<0.01\%$), model $M2$ passes loading/unloading separately but is rejected for combined scenarios ($\alpha_{min}<0.01\%$), and model $M3$ passes all scenarios (e.g., $\alpha_{min}=24.59\%$ to $93.45\%$). The measurement-error normality assumption is supported by Shapiro–Wilk p-values (in %) of 60.11, 79.64, and 60.26 for the three sensors (at 5% level).","The authors state that the method requires (and explicitly checks for) Gaussian measurement errors; if errors are not Gaussian, the algorithm must repeat measurements or terminate. They also note that solving the DOE problem (a non-convex MINLP) can be computationally expensive and does not necessarily need to be solved to optimality; using a suboptimal sensor placement increases variance and thus enlarges confidence ellipsoids. In practical use, they caution that an inaccurate model may pass some tests and that success depends on having sufficient data across varied inputs and on an intelligent (potentially expert-guided) calibration/validation split to catch worst-case inadequacy.","The confidence ellipsoids rely on a local linearization/implicit-function-based covariance approximation and may be inaccurate for strongly nonlinear models or when the Gauss–Newton assumptions fail (e.g., multiple local minima, ill-conditioned Hessians). The DOE formulation uses binary sensor inclusion and does not consider continuous sensor placement optimization, correlated sensor noise, autocorrelation across force steps, or model/measurement bias; these could materially affect covariance estimates and test validity. The case study uses a small sensor set (3 candidates) and low-dimensional parameter vector (2 parameters), so scalability and robustness for larger $n_p$ and many candidate sensors/locations is not demonstrated. The hypothesis testing procedure depends on how validation splits are constructed and, while Bonferroni controls FWER, it can be conservative and does not address power/operating characteristics under realistic dependence structures beyond that bound.","The authors suggest testing the method on models with more than two parameters and with a larger number of possible sensor locations. They also propose extending the approach to iteratively (sequentially) re-solve the optimal experimental design problem using parameters identified from initial experiments to progressively improve parameter-estimation quality, similar to prior work on robust optimal experiment design.","A valuable extension would be to relax the Gaussian/independence noise assumptions (e.g., heavy-tailed noise, heteroskedasticity across force levels, and temporal correlation along loading/unloading trajectories) and study robustness of both covariance estimation and hypothesis tests. Developing a Bayesian or likelihood-ratio alternative that accounts for model discrepancy while retaining sensor-design optimization could improve uncertainty quantification and interpretability. Scaling the MINLP sensor-selection problem to large candidate sets could be addressed via convex relaxations, greedy/submodular approximations, or continuous relaxations with rounding, along with performance guarantees. Providing open-source implementations and benchmarking across standard DOE/SPC/UQ testbeds (including multivariate outputs and higher-dimensional parameter sets) would strengthen practical adoption and reproducibility.",1910.08408v3,https://arxiv.org/pdf/1910.08408v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:14:55Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design,Prediction|Model discrimination|Parameter estimation|Cost reduction|Other,Other,Variable/General (examples include 1D and 2D synthetic functions; real data has 4 inputs/features),Manufacturing (general)|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper formulates Input Uncertain Reliable Level Set Estimation (IU-rLSE), a Bayesian experimental design/active learning problem where the goal is to identify input settings whose probability of meeting a threshold requirement is at least α when inputs are subject to uncertainty. The unknown expensive black-box function is modeled with a Gaussian process prior, and reliability at a candidate input is defined as an integral probability $p_x^*=\int \mathbb{1}[f(s)\le h]g(s\mid\theta_x)ds$. The method constructs credible intervals for $p_x^*$ and sequentially selects new experiments via an acquisition function based on expected classification improvement (an input-uncertainty extension of MILE), with an approximation to avoid expensive nested integrations. The authors provide theoretical guarantees on final misclassification loss and almost-sure finite-time termination under regularity conditions. Empirically, on multiple synthetic benchmarks and a real Combined Cycle Power Plant dataset, the proposed IU-rLSE sampling strategy achieves higher F1/precision for identifying the reliable region than Straddle, MILE, or random sampling under input uncertainty.","Reliability probability at candidate input $x$ is $p_x^*=\int_D \mathbb{1}[f(s)<h]\,g(s\mid\theta_x)ds$ and the reliable set is $H=\{x\in X: p_x^*>\alpha\}$. Under a GP posterior for $f$, the paper defines $p_{t,x}=\int_D \mathbb{1}[f_t(s)<h]g(s\mid\theta_x)ds$ and constructs a credible interval $Q_t(x)=[\mu_t^{(p)}(x)\pm \beta^{1/2}\gamma_t(x)]$ where $\mu_t^{(p)}(x)=\int_D \Phi((h-\mu_t(s))/\sigma_t(s))g(s\mid\theta_x)ds$ and $\gamma_t^2(x)=\int_D \Phi_s(1-\Phi_s)g(s\mid\theta_x)ds$. The acquisition function targets expected classification improvement under input uncertainty (Eq. (5)), and an efficient approximation $\hat a_t(x)$ is derived (Eq. (10)) to reduce computation from triple integrals to a single integral.","In numerical studies, the true reliability $p_x^*$ (and hence true reliable set $H$) is approximated via 100,000 Monte Carlo simulations, and performance is reported via F1-score/precision over repeated Monte Carlo runs (typically 20; 100 for one unknown-distribution experiment). Across 1D polynomial, 2D sinusoidal, and 2D Himmelblau benchmarks under both Gamma and Normal input-noise models, IU-rLSE attains higher F1-score trajectories than Straddle, MILE, and random sampling. In a setting with unknown input distribution parameters (normal with one parameter unknown), IU-rLSE remains superior to baselines. On the Combined Cycle Power Plant dataset (4 inputs), IU-rLSE yields higher average F1-score and its precision trends toward 1, whereas baselines’ precision does not tend to 1 when they focus on classifying $f$ rather than reliability.",None stated.,"The method relies heavily on correct GP prior modeling assumptions (kernel choice, Gaussian observation noise, and conditional independence) and accurate specification/estimation of the input uncertainty model $g(s\mid\theta_x)$; misspecification could materially affect reliability estimates and sampling decisions. The acquisition function requires integration over the input uncertainty distribution and uses approximations (e.g., replacing integrals with evaluations at expected uncertain inputs), which may degrade performance in highly nonlinear settings or for multimodal/asymmetric uncertainty. Empirical validation is limited to a small set of benchmarks and one real dataset; robustness to higher dimensions, strong heteroscedasticity, and correlated input uncertainty is not demonstrated. No implementation details or released code are provided, which may hinder reproducibility and adoption.","The authors consider an extension where the input uncertainty distribution is unknown and propose estimating it by placing a prior over unknown parameters and updating a posterior to form $g_t(s\mid\theta_x)$ (Eq. (12)), indicating modeling/estimation of input uncertainty as an extension direction. Beyond that, no explicit future-work agenda is stated.","Develop robustness analyses and/or robustified acquisition functions under misspecified input-noise models (e.g., heavy-tailed or multimodal $g$, correlated uncertainties) and non-Gaussian/heteroscedastic observation noise. Extend IU-rLSE to higher-dimensional and constrained design spaces using scalable GP surrogates (sparse/variational GPs) and more efficient quadrature/Monte Carlo for the reliability integrals. Provide self-starting/online hyperparameter learning strategies (kernel and noise) with uncertainty propagation into $p_x^*$ credible intervals, and benchmark against more recent safe/robust BO and reliability-learning baselines. Release reproducible software (e.g., Python/R) with reference implementations of the acquisition approximation and posterior-updated input uncertainty model.",1910.12043v1,https://arxiv.org/pdf/1910.12043v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:15:33Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Cost reduction|Other,D-optimal|G-optimal|Other,Variable/General (general p-dimensional feature/regressor vector g(x); manifold dimension varies by application),Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper develops a theory and algorithm for optimal design of experiments when candidate inputs lie on a Riemannian manifold rather than a Euclidean region. Using the Laplacian Regularized Least Squares (LapRLS) / manifold regularization framework, it defines a manifold-adjusted information matrix and corresponding D-optimal (maximize determinant) and G-optimal (minimize maximum prediction variance) criteria. A new equivalence theorem is proved showing D- and G-optimal designs are equivalent on manifolds, along with a lower bound for the maximum prediction variance that is attained by the optimal design. The authors propose ODOEM, a sequential/convergent algorithm to construct continuous D/G-optimal designs on manifolds by iteratively adding the point with maximal prediction variance and updating design weights. Simulations on synthetic manifolds (torus, Möbius strip, Klein bottle variants) and an image-pose dataset (COIL-20) show ODOEM selects more informative/space-filling points along the manifold and yields lower MSE than Euclidean/kernel-regression D-optimal design, random sampling, several uniform design criteria, and prior manifold-aware active learning baselines (MAED/TED).","The LapRLS estimator leads to an approximate covariance for parameter estimates: $\mathrm{Cov}(\hat\beta)\approx \sigma^2\,(Z_k^\top Z_k+\lambda_A I_p+\lambda_I X^\top L X)^{-1}$. For a continuous design $\xi$ on manifold region $\mathcal X\subseteq \mathcal M$, the information matrix is $M_{\mathrm{Lap}}(\xi)=\int_{\mathcal X}\xi(z)g(z)g(z)^\top dz + C$ with $C=\lambda_A I_p+\lambda_I\int_{\mathcal M} g(x)\Delta_{\mathcal M} g(x)^\top d\mu$. The pointwise prediction variance is $d(z,\xi)=\sigma^2 g(z)^\top M_{\mathrm{Lap}}(\xi)^{-1} g(z)$; D-optimality maximizes $\det M_{\mathrm{Lap}}(\xi)$ and G-optimality minimizes $\max_{z\in\mathcal X} d(z,\xi)$.","Theorem 1 establishes a manifold analog of the Kiefer–Wolfowitz equivalence: the design maximizing $\det(M_{\mathrm{Lap}})$ (D-optimal) is the same as the design minimizing $\max_{z\in\mathcal X} d(z,\xi)$ (G-optimal), and at optimum $\max_{z\in\mathcal X} d(z,\xi^*) = p-\mathrm{Tr}(M_{\mathrm{Lap}}(\xi^*)^{-1}C)$. Theorem 2 proves the proposed sequential ODOEM update yields a monotone-increasing sequence of determinants and converges to the global D-optimal design in the continuous-design setting. Empirically, on multiple synthetic manifold datasets and COIL-20 pose estimation, ODOEM achieves lower MSE learning curves than kernel-regression D-optimal design, random sampling, and several uniform design criteria; it also outperforms MAED and TED baselines in the reported experiments (plots in Fig. 8).","The authors note that selecting regularization parameters $\lambda_A$ and $\lambda_I$ is not systematic in sequential settings; cross-validation is impractical early in the labeling sequence, so they use heuristic choices (e.g., fixed $\lambda_A$ and decreasing $\lambda_I=-\ln(k/n)$). They also state that for very large discrete candidate sets, exhaustively evaluating each point’s design criterion can be computationally exhausting, motivating potential approximations (e.g., clustering before evaluation).","The theory and algorithm rely on the correctness of the manifold-regularization/Laplacian approximation (graph Laplacian approximating the Laplace–Beltrami operator) and on choices such as kernel type and graph construction; performance may be sensitive to these modeling and tuning decisions. The main optimality and convergence results are for continuous (approximate) designs; practical DOE often requires exact designs with integer run sizes and may face constraints (batching, costs, hard-to-change factors) not addressed. Comparisons focus on a selected set of baselines and primarily MSE; broader benchmarking (different noise models, misspecified manifolds, varying graph sparsity, computational time/complexity) would strengthen practical guidance.","They propose developing a systematic procedure with theoretical guarantees for choosing the regularization parameters $\lambda_A$ and $\lambda_I$ in sequential learning/DOE settings. They also suggest extending the manifold DOE theory to other optimality criteria beyond D/G. Finally, they mention investigating scalable variants for extremely large discrete candidate sets, for example using unsupervised clustering to reduce the number of points that must be scored.","Provide exact-design (finite-run) counterparts to the continuous-design results, including rounding/optimal allocation strategies and guarantees under run-size constraints. Study robustness to manifold misspecification, graph-construction choices (kNN/\epsilon-graphs), and autocorrelated or heteroskedastic noise, and extend to settings with unknown variance/parameters (self-starting or Bayesian formulations). Develop open-source implementations (e.g., Python/R) and computational accelerations (incremental matrix updates, Nyström/Random Fourier features) with complexity analysis to support large-scale applications.",1911.02192v2,https://arxiv.org/pdf/1911.02192v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:16:15Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction,D-optimal|A-optimal|Compound criterion,Variable/General (design over N units and T time periods; treatment adoption time per unit; also considers lag length ℓ and covariates),Service industry|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/ruoxuanxiong/staggered_rollout_design,"The paper develops optimal experimental designs for panel experiments with staggered (stepped-wedge–style) treatment rollouts, where each unit adopts treatment at a chosen start time and then remains treated (irreversible adoption). For non-adaptive designs (all assignments fixed ex ante), it formulates precision-optimal assignment of adoption times to estimate both instantaneous and lagged (carryover) average treatment effects using (feasible) GLS under a two-way fixed effects plus latent factor error structure; the resulting optimization is generally NP-hard, so the authors derive optimality conditions and propose a near-optimal algorithm that yields an S-shaped rollout curve (slow–fast–slow) and covariate/latent-factor balancing via stratification. For adaptive experiments, it proposes the Precision-Guided Adaptive Experiment (PGAE) algorithm that updates rollout decisions over time via dynamic programming and stops early when an estimated precision threshold is met, while ensuring valid post-experiment inference through sample splitting. Synthetic experiments calibrated to several real datasets (flu rates, home medical visits, grocery spending, Lending Club loans) show large reductions in opportunity cost, reporting over 50% reductions versus static benchmark designs and additional gains from adaptivity. The work advances stepped-wedge/staggered-adoption design by explicitly optimizing precision for time-varying (lagged) effects and by providing an inference-valid adaptive rollout/stopping framework.","Treatment is encoded as $z_{it}\in\{-1,+1\}$ with irreversible adoption $z_{it}\le z_{i,t+1}$ and unit adoption time $A_i$ determining the entire treatment path. Outcomes are modeled as $Y_{it}=\alpha_i+\beta_t+X_i^\top\theta_t+\sum_{j=0}^{\ell}\tau_j z_{i,t-j}+u_i^\top v_t+\varepsilon_{it}$, and designs optimize a scalar precision objective such as T-optimality $\max_A\ \mathrm{tr}(\mathrm{Prec}(\hat\tau;A))$ where $\mathrm{Prec}(\hat\tau)=\mathrm{Var}(\hat\tau)^{-1}$. In the adaptive setting, the stopping rule compares estimated precision to a threshold, e.g., $\mathrm{Prec}(\hat\tau;\omega)=\frac{Nt}{\sigma_\varepsilon^2}\,g_\tau(\omega,t)\ge c$, with $g_\tau(\omega,t)$ a quadratic form in the treated fractions $\omega_{1:t}$.","For non-adaptive designs, the optimal treated fraction over time follows an S-shaped pattern (initially low, then high, then low) when optimizing joint precision for instantaneous and lagged effects, and the overall optimization is shown to be NP-hard in general; the proposed algorithm achieves near-optimality within a multiplicative factor $1+O(1/N^2)$. With covariates/latent factors, optimality conditions impose balancing within strata defined by observed and estimated latent covariates. In realistic synthetic experiments based on multiple real datasets, the proposed designs reduce opportunity cost by over 50% versus static benchmarks, and the adaptive PGAE design yields additional precision gains (reported as >20% improvement on top of non-adaptive improvements in their adaptive experiments).","The paper notes that optimizing alternative objectives such as A-optimality, D-optimality, or cumulative-effect variance is analytically infeasible in general due to non-convex, high-order polynomial structure, motivating its focus on the (trace) T-optimal objective for tractability. It also highlights a practical bias–variance tradeoff in specifying the lag length $\ell$: under-specifying $\ell$ can bias estimates, while over-specifying can reduce efficiency.","The main theoretical development relies on a specific linear model/GLS framework with assumptions like no interference, no anticipation, and (often) independence over time, which may be violated in many real panel experiments with temporal dependence, spillovers, or interference across clusters. The adaptive inference guarantees for PGAE depend on sample splitting and distributional regularity (e.g., bounded/symmetric errors in parts of the theory), which may limit robustness under heavy tails or heteroskedasticity; practical guidance on choosing split fractions and sensitivity to mis-specification could be further developed. The paper demonstrates performance primarily via synthetic experiments calibrated to real datasets; more evidence from real, executed staggered rollout experiments would strengthen external validity and operational adoption guidance (e.g., implementation constraints, compliance, missingness).",None stated.,"Extend the design and inference framework to settings with autocorrelated errors and seasonality beyond two-way fixed effects, and to settings with interference/spillovers using cluster-level or network-aware designs. Develop robust/self-starting variants that handle unknown $\sigma_\varepsilon^2$ and model mis-specification more explicitly, and compare against additional modern randomization-based approaches for staggered adoption. Provide open-source implementations for the full design and adaptive procedure (including dynamic programming components) in a standard statistical package with practical defaults and diagnostics for choosing $\ell$, stratification, and sample-splitting proportions.",1911.03764v6,https://arxiv.org/pdf/1911.03764v6.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:16:57Z
TRUE,Computer experiment|Other,Prediction,Space-filling|Minimax/Maximin,Variable/General (input dimension $d$),Theoretical/simulation only,Exact distribution theory|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies kriging (Gaussian process regression) prediction error when the imposed covariance kernel is misspecified relative to the true kernel, focusing on isotropic Matern-type spectral decay. It derives non-asymptotic, high-probability upper bounds and matching lower bounds for the kriging prediction error under both the uniform (supremum) metric and $L_p(\Omega)$ norms, with rates expressed in terms of the designs fill distance $h_{X,\Omega}$ and mesh ratio $\rho_{X,\Omega}$. A main finding is that when the imposed kernel is oversmoothed ($\nu>\nu_0$), prediction error becomes more sensitive to space-filling quality; quasi-uniform (space-filling) designs recover optimal rates, while random designs can lead to much worse behavior. The theory provides sharp rates (up to log factors in $L_\infty$) and interprets the role of design dispersion measures for robustness to kernel misspecification. A simulation study compares random sampling versus grid (quasi-uniform) designs and empirically confirms the predicted convergence-rate differences in oversmoothing scenarios.","The kriging interpolant with imposed kernel $\Phi$ is $I_{\Phi,X}Z(x)=r_{\Phi}(x)^\top K_{\Phi}^{-1}Y$, where $r_{\Phi}(x)=(\Phi(x-x_1),\ldots,\Phi(x-x_n))^\top$ and $[K_{\Phi}]_{jk}=\Phi(x_j-x_k)$. Design space-filling metrics are the fill distance $h_{X,\Omega}=\sup_{x\in\Omega}\min_{x_j\in X}\|x-x_j\|$ and mesh ratio $\rho_{X,\Omega}=h_{X,\Omega}/q_X$ with separation radius $q_X=\min_{j\neq k}\|x_j-x_k\|/2$. For Matern kernels, the correlation is $\Psi(x;\nu,\phi)=\frac{1}{\Gamma(\nu)2^{\nu-1}}(2\sqrt{\nu}\phi\|x\|)^\nu K_\nu(2\sqrt{\nu}\phi\|x\|)$ and its spectral density decays as $(4\nu\phi^2+\|\omega\|^2)^{-(\nu+d/2)}$.","For $L_p(\Omega)$ with $1\le p<\infty$, the upper rate is $\|Z-I_{\Phi,X}Z\|_{L_p}=O_P\big(\sigma h_{X,\Omega}^{\nu}\big)$ when $\nu\le\nu_0$, and $O_P\big(\sigma h_{X,\Omega}^{\nu_0}\rho_{X,\Omega}^{\nu-\nu_0}\big)$ when $\nu>\nu_0$; corresponding lower bounds scale as $\Omega_P(\sigma n^{-\nu_0/d})$. For the uniform error, the upper rates include a $\sqrt{\log(1/h_{X,\Omega})}$ factor and the lower bound includes $\sqrt{\log n}$: e.g., under quasi-uniform designs and $\nu\ge\nu_0$, $\sup_{x\in\Omega}|Z-I_{\Phi,X_n}Z|(x)\asymp n^{-\nu_0/d}\sqrt{\log n}$ and $\|Z-I_{\Phi,X_n}Z\|_{L_p}\asymp n^{-\nu_0/d}$. The simulations (1D example) regress log error on $\log(1/n)$ and show slopes matching theory: random sampling aligns with slope $2\nu_0-\nu$ (when consistent) while grid sampling aligns with slope $\nu_0$, and in some oversmoothing/random cases the predictor appears inconsistent (very low $R^2$ and near-zero slopes).",None stated.,"The design discussion centers on fill distance and mesh ratio (quasi-uniformity) and 1D illustrative simulations (random vs grid), so practical guidance for constructing quasi-uniform/space-filling designs in higher dimensions (e.g., LHD, maximin, low-discrepancy) is indirect and not benchmarked broadly. The results assume noiseless interpolation (no nugget/measurement error) and stationary isotropic kernels with algebraically decaying spectra, which limits direct applicability to noisy observations, anisotropy, nonstationarity, or correlated errors common in practice. Computational aspects (e.g., kernel parameter estimation, numerical stability, and scalability of $K^{-1}$) are outside scope, though they affect real DOE/kriging deployments.",None stated.,"Extend the robustness/design-dependent rates to noisy settings (nugget), unknown/estimated kernel hyperparameters, and anisotropic or nonstationary covariance structures. Develop and analyze sequential/adaptive space-filling strategies that explicitly control both fill distance and mesh ratio (or their practical proxies) in high dimensions, and validate on broader families of space-filling designs (maximin LHD, Sobol/Halton, energy designs). Provide software and reproducible implementations to compute/monitor $h_{X,\Omega}$ and $\rho_{X,\Omega}$ for common experimental regions and to translate the theory into actionable DOE diagnostics.",1911.05570v4,https://arxiv.org/pdf/1911.05570v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:17:40Z
FALSE,NA,NA,Not applicable,Not specified,Environmental monitoring,Case study (real dataset)|Other,TRUE,MATLAB,Public repository (GitHub/GitLab),https://github.com/HongFFIL/rihvr-matlab,"This paper presents the design and validation of a low-cost, miniature underwater digital in-line holographic microscope (DIHM) integrated with an amphibious robot (Aquapod) to perform in situ microscopic particle measurements over large spatial areas and varying depths. The authors describe the hardware design choices (Raspberry Pi camera/computer, 650 nm laser, sealed acrylic enclosure) and an associated hologram-processing pipeline based on regularized inverse holographic volume reconstruction (RIHVR) to reconstruct and count particles. Two experiments are reported: (1) a controlled pool study mapping spatial concentration fields of microbubbles from several source configurations using robot motion and external motion-capture tracking, and (2) a lake deployment profiling particle/algae concentration versus depth and comparing the profile qualitatively to a fixed monitoring sonde. Results demonstrate the system can produce concentration maps and depth profiles while also imaging microorganisms, emphasizing low cost and systematic spatial coverage rather than proposing new DOE methodology. The work advances practical in situ environmental monitoring by combining mobile robotics with DIHM and providing an open MATLAB implementation of the processing steps.",Not applicable,"In the lab pool experiment, the Aquapod-mounted DIHM produced spatial concentration maps that distinguished different bubble source configurations (single point, linear array, and bent/elbow arrangement) using interpolated concentration estimates from sampled locations. In the field experiment, the DIHM depth concentration profile (binned in 0.5 m increments) captured a decrease in concentration with depth consistent with the thermocline trend observed by a fixed sonde, while exhibiting higher variability due to instantaneous sampling versus the sonde’s 2-minute averaging. The system records 2048×2048 pixel holograms at 1 fps with 1 µs shutter speed over a 10 mm sample thickness (≈53 µL sample volume) and processes one hologram (40 planes) in ~21 s with a GPU-accelerated RIHVR method.","The authors note a trade-off between image quality and cost, relying on processing to compensate for increased noise from cheaper/smaller components. They state that images must be processed externally (no live/on-board processing), with throughput limited by data transfer and GPU memory bandwidth. They also note that low-cost in situ species identification has not yet been demonstrated and that their concentration measurements did not leverage the full 3D nature of holographic reconstructions.","The experiments do not constitute a statistically planned DOE (e.g., randomized/blocked designs), and performance claims rely largely on qualitative/illustrative mapping rather than quantified detection/estimation error (e.g., RMSE of concentration maps or uncertainty propagation). The concentration estimation depends on multiple heuristic processing choices (background subtraction window, 25% threshold, morphological closing, interpolation/smoothing) whose sensitivity/robustness is not systematically evaluated. Field validation compares against a different sensing modality (phycocyanin proxy from the sonde) with normalization assumptions (e.g., lake bed concentration assumed 0 for DIHM), which may limit interpretability and direct quantitative agreement.","The authors propose enabling on-board/live processing as single-board computers gain GPU capability, allowing closed-loop robot behaviors such as searching for peak concentration and triggering targeted water sampling. They suggest exploring automatic species classification with the low-cost DIHM (not yet demonstrated in situ) and leveraging full 3D reconstructions and higher frame rates to study micrometer-scale heterogeneity and swimming behavior. They also propose improving Aquapod mobility/control (including land–surface–water-column transitions and more robust depth control) and enhancing robot stability to improve imaging quality and competitiveness with existing field solutions.","A useful extension would be a formal calibration/validation study linking DIHM-derived counts/concentration to independent ground-truth measurements (e.g., filtered sample counts or known bead standards) across concentrations and particle types. Additional robustness studies could quantify performance under varying turbidity, ambient light leakage, vibration levels, and flow/entrainment near the robot, and could report repeatability metrics and uncertainty estimates. Publishing complete end-to-end field-processing scripts and a benchmark dataset (raw holograms + labels) would improve reproducibility and facilitate comparison with alternative reconstruction/counting methods.",1911.10231v1,https://arxiv.org/pdf/1911.10231v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:18:12Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"This paper designs and experimentally demonstrates an optical-fiber data transmission scheme based on the periodic nonlinear Fourier transform (PNFT), encoding information in the invariant nonlinear main spectrum. Using the exact inverse PNFT algorithm (developed in Part I), the authors construct a four-symbol (2 bits/symbol) constellation of genus-2 periodic/quasi-periodic waveforms constrained to share the same time period and matched group velocity, and transmitted with a cyclic prefix. Performance is validated in an optical recirculating-loop experiment and compared against NLSE split-step simulations including noise/attenuation, showing good agreement. The experiment achieves a bit-error ratio (BER) below 1e-3 up to 2000 km, and the observed reach exceeds a naive dispersion-plus-cyclic-prefix estimate due to a strong solitonic component in the waveform. The work advances PNFT-based communication by providing a practical waveform/constellation construction method tied to an exact inverse PNFT and by quantifying experimental feasibility and reach drivers (CP, phase continuity, soliton-like energy).","Finite-gap PNFT waveforms are expressed via a ratio of Riemann theta functions: $q(t,z)=K_0\,\frac{\theta((\omega t+k z+\delta^-)/(2\pi)|\tau)}{\theta((\omega t+k z+\delta^+)/(2\pi)|\tau)}e^{i\omega_0 t+i k_0 z}$. Period matching uses NLSE scaling: $\lambda_j\to s\lambda_j$ with $\omega\to s\omega$ and $k\to s^2 k$. Group-velocity matching and quasi-period handling use a frequency/phase shift property that induces a constant shift of the nonlinear spectrum (Eq. 7), and demapping uses the minimum squared-distance over eigenvalue permutations (Eq. 9).","A four-point genus-2 PNFT constellation with 50% cyclic prefix (total symbol duration 1 ns, bandwidth about 4.5 GHz, power 2.5 dBm, spectral efficiency 0.45 bits/s/Hz) is experimentally transmitted over a recirculating-loop link. The system achieves BER < 1e-3 up to 2000 km, with experimental BER curves in good agreement with NLSE simulations including amplifier noise and attenuation. Without cyclic prefix, BER exceeds 1e-3 after roughly 300 km (experiment/simulation), confirming strong ISI sensitivity. Enforcing continuous phase at symbol boundaries yields an additional reach of about 70 km (~1 span) in simulation.",None stated.,"The paper is not a DOE/experimental design contribution; the experiment is a proof-of-concept with a very small constellation (4 symbols) and low spectral efficiency (0.45 bits/s/Hz), so general conclusions about system-level throughput are limited. Results depend on specific lab conditions (recirculating loop, filtering, AWG/receiver constraints) and the integrable NLSE-based model with lumped amplification approximations; robustness to stronger non-integrable effects, polarization effects, and broader hardware impairments is not fully assessed. No publicly shared code or reproducibility package is provided for the PNFT waveform generation/DSP chain.","The authors state that making PNFT transmission attractive requires significantly increasing spectral efficiency by increasing genus and/or period (more bits per symbol at similar bandwidth/CP) and by modulating all available degrees of freedom, including auxiliary-spectrum degrees. They also highlight open problems in constellation design, the need for deeper theory including a channel/noise model for correlated non-Gaussian noise, and a nonlinear analogue of the Nyquist–Shannon sampling theorem for more efficient sampling. They further suggest polarization-division multiplexing (generalizing PNFT to two polarizations/Manakov system) and the development of nonlinear multiplexing methods, ideally all-optical.","Provide a standardized, open-source implementation (waveform synthesis, inverse/forward PNFT, DSP, and demapper) plus reproducible simulation scripts to enable benchmarking across labs. Extend evaluation to larger constellations and higher genus with systematic sensitivity studies (filter detuning, AWG quantization, clock jitter, laser phase noise, polarization effects) and fair comparisons against state-of-the-art NFDM/NFT modulation formats at matched SE and bandwidth. Develop self-adaptive or learning-based demapping/equalization strategies that explicitly model correlated, non-Gaussian spectral noise and mitigate ISI with shorter CPs, and validate on diverse link types (distributed Raman, different span lengths, dispersion maps).",1911.12615v2,https://arxiv.org/pdf/1911.12615v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:18:50Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,A-optimal|D-optimal|E-optimal,Variable/General (selecting n0 sensor locations/measurement directions from n candidates; examples shown with nd=nr=nx=30 and larger),Environmental monitoring|Energy/utilities|Other,Simulation study|Other,TRUE,Julia|Other,Not provided,NA,"The paper proposes a scalable algorithm for optimal sensor placement in Bayesian linear inverse problems, posed as an A-, D-, or E-optimal experimental design with binary selection variables over candidate measurement locations. It relaxes the integer program to a convex continuous problem and solves it using Sequential Quadratic Programming (SQP), where the gradient and Hessian of the design criterion are approximated via Chebyshev interpolation exploiting the continuously indexed (smooth-kernel) structure of the discretized integral operator. The approach yields per-iteration operations dominated by low-rank linear algebra and achieves overall complexity stated as O(n log^2 n), with an accompanying error analysis showing the integrality gap of a rounded integer design shrinks to zero as mesh size n→∞ under smoothness/analyticity conditions. The method is demonstrated on a two-dimensional advection–diffusion PDE “LIDAR” sensing-direction design problem, producing designs that concentrate sensing directions in the direction of the flow and showing substantial runtime improvements versus a generic nonlinear optimizer (Ipopt) while maintaining small integrality gaps.","Sensor selection is formulated as min_w \phi(\Gamma_{post}(w)) s.t. w_i\in\{0,1\}, \sum_i w_i=n_0 (or relaxed 0\le w_i\le 1). The posterior covariance is \(\Gamma_{post}(w)=(F^T W^{1/2}\Gamma_{noise}^{-1}W^{1/2}F+\Gamma_{prior}^{-1})^{-1}\) with \(W=\mathrm{diag}(w)\). For A-optimality, the gradient is \(\partial\,\mathrm{tr}(\Gamma_{post})/\partial w_i=-\|(F^TWF+I)^{-1}f_i\|_2^2\) and the Hessian entry is \(H_{ij}=2\,(f_i^T(F^TWF+I)^{-1}f_j)\,(f_i^T(F^TWF+I)^{-2}f_j)\); D-optimal derivatives are also provided in the appendix.","In the reported LIDAR experiments (e.g., nd=nr=nx=30), the SQP+Chebyshev approach computes a sufficiently accurate relaxed design in under a minute, whereas an “exact” solve with Ipopt is reported to take about 1.5 hours at that size. The paper shows integrality gaps (difference between relaxed objective and rounded integer design) shrinking with increasing discretization size and with more interpolation points (larger c in N=c log n), and it provides plots demonstrating convergence behavior for both full-F and low-rank approximations. Empirically, optimal sensing directions shift toward the wind/velocity direction as advection strength increases (larger c1,c2) and become more spread out when diffusivity µ is smaller; with large µ the relaxed weights become blurred.","The authors note that after the first several SQP iterations, objective decrease can become slow due to the low-rank structure of the approximated Hessian, and tightening the stopping tolerance can significantly increase iteration count with limited objective gain. They also state that choosing too few interpolation points (e.g., c=1) can yield poor approximations of F and uninformative designs (e.g., nearly uniform), even if rounding gives an integer solution. They remark that the PDE-derived kernel f(x,y,t) may not always be continuous (e.g., delta-function behavior in wave systems), which can affect applicability of interpolation-based assumptions.","The approach relies on smooth/analytic kernel structure to justify Chebyshev interpolation error bounds; performance and guarantees may degrade for nonsmooth forward maps, complex geometries, or strongly heterogeneous meshes common in PDE inverse problems. The empirical evaluation is centered on a specific LIDAR/advection–diffusion setup; broader benchmarking against state-of-the-art scalable OED methods (e.g., randomized trace/logdet estimators, stochastic Newton methods) and across multiple PDE models/noise structures is limited. Code is not shared, which hinders reproducibility and independent validation of claimed computational complexity and tuning choices (e.g., interpolation constant c, truncation thresholds).","They propose incorporating high-performance/parallel computing to accelerate the many matrix–vector and vector–vector products, including constructing and partitioning the large dense low-rank matrices across processors and coordinating inter-processor communication. They also discuss improving the current algorithm to address slow objective decay after initial SQP iterations, particularly linked to low-rank Hessian structure.","A valuable extension would be self-contained adaptive selection of interpolation order/points (adaptive c or local refinement) with error estimators tied directly to design-criterion accuracy. Robust variants for model misspecification, non-Gaussian noise, correlated spatial noise, and unknown hyperparameters (e.g., \(\sigma_{noise},\sigma_{prior}\)) could broaden practical applicability. Additional work could provide public implementations (e.g., a Julia package) and standardized comparisons on community PDE inverse-problem benchmarks, including large-scale 3D problems and non-rectangular domains, to assess scalability and robustness in realistic settings.",1912.06622v2,https://arxiv.org/pdf/1912.06622v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:19:27Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal|A-optimal|E-optimal|Other,Variable/General (application example uses 8 parameters),Environmental monitoring|Other,Simulation study|Other,TRUE,Python|MATLAB|Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher)|Not provided|In text/Appendix,https://arxiv.org/abs/1912.07412v1|https://www.mcs.anl.gov/petsc|https://doi.org/10.7289/V5NZ85MT|https://doi.org/10.7289/V5DF6P53|https://doi.org/10.5281/zenodo.3264781|https://doi.org/10.5281/zenodo.3558540|https://doi.org/10.5281/zenodo.3558700|https://doi.org/10.5281/zenodo.3558702|https://doi.org/10.5281/zenodo.3558698|http://www.python.org|http://www.numpy.org|http://www.scipy.org|http://metos3d.github.io|http://www.mathworks.de/help/releases/R2015a/pdf_doc/matlab/getstart.pdf|http://www.mathworks.de/help/releases/R2015a/pdf_doc/gads/gads_tb.pdf|http://www.mathworks.de/help/releases/R2015a/pdf_doc/optim/optim_tb.pdf,"The paper summarizes methods for parameter estimation, uncertainty quantification, and experimental design for a computationally expensive global marine biogeochemical model (phosphate and dissolved organic phosphorus). Parameters are estimated via generalized least squares (GLS), with uncertainty quantified using asymptotic normal approximations and three covariance approximations based on the Jacobian and Hessian, including a robust “sandwich” form for misspecification. For design of future measurements, it uses Fisher-information-based prediction of covariance reduction (requiring only the model derivatives at the current estimate) and evaluates candidate measurement designs by scalar criteria of the predicted covariance (trace/sum of diagonals, determinant, maximum eigenvalue) plus two custom relative-uncertainty criteria targeting parameters and outputs. The approach is applied to an 8-parameter marine phosphorus-cycle model using millions of PO4 observations and hundreds of DOP observations, showing improved fit and mapping where additional measurements would most reduce uncertainty (primarily near the surface). It also discusses sequential (iterative) experimental design, where new measurements are planned after updating parameter estimates.","Parameter estimation uses GLS: $\hat\theta_n=\arg\min_{\theta\in\Omega}(y_n-f_n(\theta))^T C_n^{-1}(y_n-f_n(\theta))$. The Fisher information matrix is $F_n(\hat\theta_n)=J_n(\hat\theta_n)^T C_n^{-1}J_n(\hat\theta_n)$; covariance approximations include $V_n^{(F)}=\sigma^2 F_n^{-1}$, $V_n^{(H)}=\sigma^2 H_n^{-1}$, and the quasi-ML “sandwich” $V_n^{(F,H)}=\sigma^2 H_n^{-1}F_n H_n^{-1}$. For design, predicted uncertainty of planned measurements is assessed using $V_n^{(F)}$ (and propagated output covariance $W_n=J_{\tilde f}V_n J_{\tilde f}^T$) and scalar criteria such as trace/sum of diagonals, determinant, maximum eigenvalue, plus custom relative-uncertainty criteria in Eqs. (15)–(16).","In the application, 8 biogeochemical parameters are estimated using GLS/WLS/OLS; the GLS optimum substantially changes several parameters from initial guesses (e.g., higher $\kappa_{re}$, $\alpha$, and $\kappa_I$) and improves model–data consistency. Using the sandwich covariance (Eq. 8) and 99% CIs, the largest relative parameter uncertainties are for $\kappa_{re}$ and $\kappa_{PO4}$ (~6–7%), followed by $\alpha$ and $\kappa_I$ (~3%); $p$ (mean P concentration) is ~0.1% uncertain. Output uncertainty at the surface averages about $6\times10^{-3}$ mmol m$^{-3}$ for PO4 (~1% relative) and $5\times10^{-3}$ mmol m$^{-3}$ for DOP (~2% relative), decreasing strongly with depth for PO4. Predicted information gain from new measurements is highest near the surface; a single additional measurement reduces average model uncertainty only on the order of ~1/20,000, implying many new measurements are needed for sizable reduction.","Only uncertainties resulting from measurement noise are considered; model discrepancy (structural/model error) and numerical/discretization errors are explicitly not captured. Monte Carlo–based uncertainty quantification is described as more accurate but deemed computationally infeasible for the expensive model, so the study relies on derivative-based covariance approximations. For experimental design, covariance approximations depending on realized measurements (e.g., Hessian-based and sandwich forms) cannot be used for predicting uncertainty reduction, so the design step relies on the Fisher-based approximation which is justified primarily under correct-model or regularity assumptions.","The design recommendations are “local” and depend on the current parameter estimate; if the estimate is biased (e.g., due to model discrepancy), the suggested measurement locations may not be globally optimal. The DOE is restricted to choosing additional measurement points (time/location/process) and does not address broader design constraints (logistics, sampling bias, nonstationary ocean dynamics) that can dominate real campaigns. The information-based criteria used are not linked to a clear optimality class (e.g., D-/A-/I-optimal) in the implemented objectives, making it harder to compare to standard optimal-design benchmarks or guarantee optimality properties. Measurement-error covariance estimation (including assumed zero correlations when data are sparse) may materially affect Fisher-information calculations, but sensitivity of design outcomes to these covariance modeling choices is not fully explored.","The paper suggests sequential optimal experimental design: iteratively plan new measurements, update parameter estimates using the new data, and redesign subsequent measurements based on the updated estimate. It also notes that assigning explicit costs to candidate measurements can support choosing designs under cost limits or optimizing information gain per cost.","Extend the experimental-design framework to explicitly handle model discrepancy (e.g., Bayesian calibration with discrepancy terms) so that design targets reducing predictive uncertainty rather than only parameter uncertainty under a potentially misspecified model. Develop and validate alternative design criteria focused on prediction in targeted regions/seasons (I-/G-optimal style) and compare against the custom relative-uncertainty criteria. Incorporate correlated/heteroskedastic observational error more robustly in the design step (e.g., sensitivity analysis over plausible $C_n$) and evaluate how designs change under alternative noise models. Provide a reproducible software package/workflow that couples the Fisher-based design computations to practical ocean-sampling constraints (ship tracks, floats, depth profiles) and validate the predicted gains with synthetic “add data and refit” experiments.",1912.07412v1,https://arxiv.org/pdf/1912.07412v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:20:19Z
TRUE,Factorial (full)|Split-plot|Other,Other,Not applicable,"Variable/General (illustrates 1–2 treatment factors; discusses crossed/nested factors; includes examples like 2-factor factorial, RCBD, Latin square, split-unit with 2 treatment factors)",Food/agriculture|Theoretical/simulation only|Other,Other,NA,R,Not applicable (No code used),https://orcid.org/0000-0003-2500-1221,"The paper argues for using Hasse diagrams as a central pedagogical and planning tool for teaching classical design of experiments by representing treatment structures, unit (experimental material) structures, and their combination through randomization. It shows how many standard balanced/orthogonal designs can be constructed and understood via crossing and nesting of factors, including completely randomized factorial designs, (generalized) randomized complete block designs, Latin squares/row–column designs, and split-unit (split-plot) designs. Using the diagrams, instructors and students can derive key properties such as degrees of freedom, error strata, experimental units, appropriate denominators for F-tests, and an “ANOVA skeleton table.” The paper also links diagrams directly to linear (mixed) model terms and software specification (illustrated with R formula syntax), helping non-specialists translate a design into an analyzable model. It highlights conceptual issues the diagrams clarify, such as pseudo-replication, randomization restrictions, unit–treatment vs treatment–treatment interactions, and complete confounding; limitations are noted for unbalanced and certain confounded designs.","The paper’s analysis examples use standard linear model forms derived from the Hasse diagram. For a two-factor balanced factorial with replication, the response model is $y_{ijk}=\mu+\alpha_i+\beta_j+(\alpha\beta)_{ij}+e_{ijk}$ with i.i.d. residuals $e_{ijk}$, matching the diagram terms for main effects, interaction, and the residual unit factor. Model specification is expressed using Wilkinson–Rogers notation (e.g., $A*B=A+B+A{:}B$) and in R using fixed-effect terms plus an Error() term for unit strata (e.g., $\texttt{Variety*Nitrogen + Error(Block/Plot)}$ for the split-unit example).",NA,"The exposition assumes balanced designs with equal numbers of replicates; while Hasse diagrams can describe unbalanced designs, the simple algorithms presented for degrees of freedom and deriving linear models generally do not work in the unbalanced case. The paper notes Hasse diagrams are not applicable (or not well suited) for designs with more complex confounding such as balanced incomplete block designs and Youden squares. It also states they are not well suited for explaining fractional factorial designs unless pseudo-factors are introduced.","The paper is primarily a pedagogical/methodological exposition and does not provide empirical evaluation of learning outcomes (e.g., controlled teaching studies) to quantify the instructional benefit of Hasse diagrams versus alternative approaches. It focuses on classical ANOVA-style reasoning for orthogonal designs; practical guidance for modern settings (missing data, constraint randomization, or strong heteroscedasticity/correlation structures) is limited beyond brief pointers to mixed models. The R-related discussion does not include implementable code or reproducible teaching materials to operationalize the approach.","The paper points to extensions of diagram-based ideas for more advanced settings, including diagram approaches tailored to linear mixed models with richer covariance structures and “tiered”/multi-randomization experiments (chains of experiments), suggesting these could stimulate similar uses in more advanced courses. It also notes that random treatment factors are straightforward in the diagrams but require linear mixed model analysis instead of classical ANOVA, implying an avenue for extending the teaching framework.","A natural next step is a formal educational evaluation (e.g., randomized or quasi-experimental comparison across course sections) measuring student ability to identify experimental units, error terms, and correct models. Extending the diagram-to-model translation to robust workflows for unbalanced data, missingness, and non-Gaussian responses (GLMMs) would improve practical relevance. Providing open-source teaching materials (R notebooks, diagram generators) and a standardized mapping from Hasse diagrams to software code (including lme4/brms) would make adoption easier and more reproducible.",1912.08567v1,https://arxiv.org/pdf/1912.08567v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:20:40Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Robustness|Cost reduction,A-optimal|Other,"Variable/General (design chooses subset of sensor locations from s candidates; example uses s=234 candidate locations, r=5 times, d=sr observations; OEDUU uses N=100 MC samples of uncertainty)",Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://www.spe.org/web/csp/datasets/set02.htm,"The paper develops an A-optimal sensor placement method for infinite-dimensional Bayesian linear inverse problems governed by PDEs when the forward model includes irreducible (non-reducible by data) uncertainties. It formulates “A-optimal design under uncertainty” as minimizing the expectation (over model uncertainty) of the trace of the posterior covariance, and discretizes the expectation with a sample-average/Monte Carlo approximation augmented with a sparsity/binary-inducing penalty to select a subset of candidate sensor locations. To make the resulting optimization under uncertainty computationally feasible, it derives a reformulation of the A-optimal objective in observation space (reducing trace computations from parameter space to measurement space) and constructs composite low-rank reduced bases for many forward-operator samples using randomized range-finder methods (optionally with clustering). The approach is demonstrated on a subsurface contaminant transport inverse problem (advection–diffusion PDE) with uncertainty in groundwater velocity (via uncertain permeability) and uncertainty in initial time, showing uncertainty-aware designs typically yield better expected posterior-variance reduction than designs computed for a single nominal realization.","The OED under uncertainty objective is $\phi(w)=\mathbb{E}_{\xi}\,[\operatorname{tr}(C_{\text{post}}(\xi,w))]$ with posterior covariance $C_{\text{post}}(\xi,w)=(F(\xi)^*W^{1/2}\Gamma_{\text{noise}}^{-1}W^{1/2}F(\xi)+C_{\text{pr}}^{-1})^{-1}$ (simplifying to $\Gamma_{\text{post}}(\xi,w)=(\sigma^{-2}F(\xi)^*WF(\xi)+\Gamma_{\text{pr}}^{-1})^{-1}$ for i.i.d. noise). The expectation is approximated by SAA/Monte Carlo: $\bar\phi_N(w)=\frac1N\sum_{i=1}^N \operatorname{tr}[(\sigma^{-2}F_i^*WF_i+\Gamma_{\text{pr}}^{-1})^{-1}]$. Using Sherman–Morrison–Woodbury, they rewrite traces in measurement space via $\Gamma_{\text{post}}=\Gamma_{\text{pr}}-\sigma^{-2}\Gamma_{\text{pr}}F^*(I+\sigma^{-2}WF\Gamma_{\text{pr}}F^*)^{-1}WF\Gamma_{\text{pr}}$ and thus $\operatorname{tr}(\Gamma_{\text{post}})=\operatorname{tr}(\Gamma_{\text{pr}})-\operatorname{tr}(\sigma^{-2}S^{-1}WF\Gamma_{\text{pr}}^2F^*)$ with $S=I+\sigma^{-2}WF\Gamma_{\text{pr}}F^*$. Sensor selection is posed as $\min_{w\in[0,1]^s}\phi_N(w)+\gamma\psi(w)$ with a continuation ‘regularized $\ell_0$’ sparsification penalty to promote binary designs.","In the subsurface-flow example, the candidate sensor set is $s=234$ locations with $r=5$ observation times (so $d=sr$ potential measurements) and the OED under uncertainty uses $N=100$ Monte Carlo samples of the irreducible uncertainty (velocity field and initial time). Reduced-order modeling via a composite randomized range finder yields a joint basis size of 410 for tolerance $\mu=0.002$ (and 985 for $\mu=10^{-4}$) for $N=100$ samples; reported relative errors for evaluating the all-sensors objective are about $6.876\times10^{-4}$ (1 cluster, $\mu=0.002$) and $2.335\times10^{-6}$ (1 cluster, $\mu=10^{-4}$). In performance comparisons, uncertainty-aware designs generally achieve larger expected trace reduction (better expected posterior-variance reduction) than deterministic designs computed for a single uncertainty realization, with the advantage most pronounced when using few sensors and/or fewer observation times per sensor. Clustering (e.g., 4 clusters) can reduce per-cluster basis sizes (e.g., 283–279 for $\mu=0.002$) with similar reported objective-approximation error on the all-sensors design.","The authors state the formulation is restricted to linear parameter-to-observation maps, Gaussian priors, and additive Gaussian noise, and that extending to nonlinear inverse problems would require additional work (e.g., Gaussian/Laplace posterior approximations). They also note reliance on Monte Carlo sampling for the irreducible uncertainty, which may require a large number of samples for high-accuracy expectation approximation.","The design criterion is risk-neutral (minimizes the mean of the posterior-variance trace), so it may not control worst-case performance under rare but consequential uncertainty realizations without a risk-averse extension. The method assumes a fixed discrete candidate sensor network (finite set) and uses relaxation plus nonconvex sparsification to obtain binary designs, which can be sensitive to tuning (e.g., $\gamma$, continuation schedule, and scaling $\alpha$) and may converge to local minima. Reported computational performance depends on the quality of low-rank structure and ROM accuracy; problems with weaker low-rankness or larger measurement dimension may reduce the benefit of the observation-space trace reformulation and joint bases.","They suggest exploring alternatives to Monte Carlo for approximating uncertainty (e.g., Taylor expansions, stochastic approximation with samples varying during optimization) and extending the approach to nonlinear inverse problems (e.g., via Gaussian approximations at the MAP point). They also mention investigating designs that account for reducible model uncertainties (secondary parameters) while prioritizing inference of primary parameters of interest.","Developing explicitly risk-averse OEDUU criteria (e.g., CVaR/worst-case trace or chance constraints) and comparing them empirically to risk-neutral designs would strengthen robustness claims. Extending the framework to correlated/non-Gaussian noise, unknown noise levels, and temporally/spatially correlated observation errors would improve practical applicability in environmental sensing. Providing an open-source implementation (e.g., scripts to reproduce ROM/OEDUU results with hIPPYlib) and standardized benchmarks would facilitate adoption and fair comparison with alternative sparse sensor placement methods.",1912.08915v2,https://arxiv.org/pdf/1912.08915v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:21:36Z
FALSE,NA,NA,Not applicable,Not specified,Energy/utilities|Other,Simulation study|Other,TRUE,Python|Other,Not provided,"https://doi.org/10.1016/j.aei.2007.08.012|http://arxiv.org/abs/1706.07068|https://doi.org/10.1016/j.rser.2013.02.004|http://generativedesign.eu/on/cic/ga2001_PDF/fischer.pdf|https://doi.org/10.1016/j.vetimm.2013.08.005|http://link.springer.com/chapter/10.1007/978-94-007-0510-4_17%5Cnhttp://link.springer.com/content/pdf/10.1007%2F978-94-007-0510-4_17.pdf|https://doi.org/10.1177/1478077118800982|https://doi.org/10.1016/j.rser.2018.04.080|https://doi.org/10.1080/19401493.2018.1507046|https://doi.org/10.1016/j.autcon.2017.03.017|https://doi.org/10.1146/annurev-statistics￾010814-020120.StanislasChaillou,|https://towardsdatascience.com/architecture-style￾ded3a2c3998f|https://doi.org/10.1016/S0034-4257(97)00083-7|https://doi.org/10.1016/j.aei.2011.07.009|https://ascelibrary.org/doi/abs/10.1061/9780784479681.024|http://arxiv.org/abs/1703.10847|https://doi.org/10.1016/j.enbuild.2011.10.006|http://arxiv.org/abs/1610.09585","The paper demonstrates deep-learning workflows that use synthetic, parametrically generated training datasets to predict and generate architectural design options. Three experiments are presented: (1) a CNN trained on 6,000 synthetic 2D shape images to classify unseen shapes; (2) an Auxiliary Classifier GAN (AC-GAN) trained to generate novel shapes conditioned on desired class labels; and (3) an AC-GAN trained to generate façade window/wall light–shadow patterns conditioned on daylight-performance labels derived from annual spatial daylight autonomy (sDA) simulations. Daylight labels are discretized into five categories (A–E) based on sDA ranges, and patterns are produced for a simple room/facade setup using DIVA4 simulations with Houston TMY weather. The work positions generative DL as an alternative to forward parametric optimization by learning a design “vocabulary” and producing options that can fall outside the original parametric search space.","The main “design rule” used for Experiment 3 labeling is a discretization of simulated daylight performance: assign label A if $0\le sDA<20\%$, B if $20\%\le sDA<40\%$, C if $40\%\le sDA<60\%$, D if $60\%\le sDA<80\%$, and E if $80\%\le sDA\le 100\%$. The AC-GAN is conditioned on class/performance labels to generate images (shapes or façade patterns), but the paper does not present explicit closed-form control-limit/DOE-style optimality equations.","Experiment 1 reports 96% validation accuracy after 10 training epochs on 6,000 synthetic 2D shape images and 93% accuracy on 45 manually drawn test shapes. Experiment 2 trains an AC-GAN for 5,000 epochs (batch size 32) on 2,616 shapes and produces label-conditioned shape generations that the authors describe as novel relative to the input set. Experiment 3 generates 572 façade patterns (4 seeds × 143 runs) labeled by sDA and trains an AC-GAN for 12,000 epochs (batch size 5); the discriminator loss approaches ~0.2 while generator loss rises to ~5.0, and generated patterns broadly track increasing window-to-wall ratio (WWR) with higher sDA labels. A single-sample check (Table 1) shows correct label agreement for A and E but mismatches for B–D when evaluated via re-simulated sDA.","For Experiment 3, the authors note that glare is not studied and sDA is used only for labeling daylighting performance. They also state that their re-drawing/post-processing workflow (erosion/dilation and re-drawing in Grasshopper) may not precisely represent the true sDA of the DL-generated images because window location affects daylight performance. They acknowledge reduced variety for some labels (notably D and E) and that the generator loss indicates results “are not as desired” even though WWR trends match labels.","The work is not a DOE contribution: the “experiments” are ML training/evaluation pipelines without formal design-of-experiments structures (randomization, replication for uncertainty quantification, factor-effect estimation, or optimal run selection). Performance evaluation in Experiment 3 relies heavily on discretized labels and limited verification (notably the table compares only one generated sample per label), so statistical confidence and generalization are unclear. Simulation settings (single room geometry, single orientation/south façade, one climate file) constrain external validity, and the approach may be sensitive to the chosen discretization thresholds and random-seed pattern generation scheme.","The authors propose generating multiple samples and re-evaluating Experiment 3 using a confusion matrix for a more accurate assessment. They suggest trying alternative network types (e.g., multilayer perceptron/DNN) for the generator/discriminator in Experiment 3 and exploring Style-GAN to combine performance-based results with designer-preferred patterns. They also intend to improve the DL/GAN generative system using synthesized big data and to integrate BIM, performance simulation, and optimization to further investigate DL/GAN-based generative design methods.","A useful extension would be to treat daylight performance as a continuous target (regression/conditional generation on continuous sDA) rather than coarse bins, and to validate generated patterns on multiple climates, orientations, room depths, and glazing properties. Incorporating constraints (constructability, minimum window sizes, adjacency/contiguity, and glare/ASE metrics) would make outputs more actionable. Releasing code and datasets (Grasshopper scripts, simulation automation, training notebooks) and providing ablation studies (dataset size, labeling scheme, architecture/hyperparameters) would improve reproducibility and clarify what drives performance.",2001.05849v2,https://arxiv.org/pdf/2001.05849v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:22:23Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Case study (real dataset),TRUE,MATLAB,Not provided,http://ieeexplore.ieee.org/document/4118466/|http://doi.wiley.com/10.1002/rob.20366|http://ieeexplore.ieee.org/xpls/abs{ }all.jsp?arnumber=6315688,"The paper presents the mechanical design and experimental characterization of “Modboat,” a low-cost aquatic robot propelled and steered using a single motor that rotates an internal mass and actuates passive flippers via conservation of angular momentum. A dynamic model is provided (adapted from prior work) and used to motivate an open-loop periodic motor input parameterization for producing forward motion and steering by making the two half-strokes asymmetric. Experiments in a water tank using an OptiTrack motion-capture system (120 Hz) evaluate trajectory curvature under nominally symmetric inputs and varying effective center-of-mass (COM) offset (via varying the input phase/zero orientation), revealing strong unwanted curvature and high sensitivity to COM/roll. The authors hypothesize that flush-closed flippers delay drag-induced opening and create asymmetric thrust under small roll, then test two design modifications (spacers preventing full closure and a redesigned bottom body removing the cylinder) that increase trajectory radii and reduce sensitivity. The work advances practical single-actuator aquatic robot design by identifying a key mechanical sensitivity and validating design changes that improve open-loop behavior as a precursor to future closed-loop docking/swarming control.","The robot dynamics are expressed as a coupled planar translation/rotation ODE (Eq. (1)) with generalized mass matrix $M=\mathrm{diag}(m,m,I)$ and forcing terms from flipper drag, body translational drag, body rotational drag, and the prescribed internal-mass acceleration term $-I_t\ddot{\phi}$. The commanded internal rotation is a piecewise sinusoid (Eq. (3)) parameterized by $(T_1,T_2,A,\phi_0)$, where differing half-stroke periods ($T_1\neq T_2$) bias thrust and produce turning. A simplified flipper-opening torque model is given as $\tau_{open}=\tfrac{1}{2} l_f F_f \sin(\alpha)$ (Eq. (5)), supporting the roll-sensitivity explanation.","In tank tests with symmetric input $(T_1,T_2,A,\phi_0)=(1,1,2,0)$ over 30 s, the robot exhibited strong curvature (small fitted-circle radii) rather than the near-straight trajectories predicted by simulation, and the turning direction/magnitude depended strongly on effective COM orientation $\phi_0$. Adding 5 mm spacers to prevent full flipper closure increased fitted radii overall and produced a region near $\phi_0\in[-15^\circ,-10^\circ]$ with trajectories closer to straight lines. A redesigned bottom body removing the bottom cylinder further increased achievable radii over a wider range of COM offsets and reduced roll/COM sensitivity sufficiently to make closed-loop stabilization plausible. Measurements note COM alignment within about $\pm1.25$ mm of the motor axis still yielded strong curvature, highlighting the mechanical sensitivity.","The authors note that the design has limited thrust and maneuverability due to the reciprocal motor motion and direction switching, which also slows response time (cycle-time limited). They report significant sensitivity to COM/roll that makes open-loop straight-line motion difficult without extremely precise balancing, which conflicts with carrying payloads or docking (both shift COM). They also acknowledge the improved designs are “still far from ideal,” though adequate to enable closed-loop control.","The experiments are largely open-loop and focus on curvature as the main performance metric; broader metrics (speed, energy consumption, repeatability across conditions, disturbance rejection) and statistical analysis of variability are limited. The modeling assumptions (instantaneous flipper opening/closing, only one flipper open, negligible translation vs rotation, no ambient flows) may not hold in practice and could materially affect predicted trajectories and control design. Results are from a controlled tank environment with motion capture and may not generalize to open-water conditions (waves, currents, biofouling) or to multi-robot hydrodynamic interactions that are central to swarming/modularity claims.","They propose implementing closed-loop orientation feedback to enable trajectory following and task execution, which is important for docking multiple boats and exploring swarming/group behavior. They also plan to study the impact of disturbances such as flows or vortices and to investigate alternative motion primitives that may better drive the system.","A natural next step is a systematic closed-loop control study with identification of hydrodynamic parameters and robustness analysis under sensor noise, actuator backlash, and varying payload/COM. Extending evaluation to real-world outdoor trials (with quantified current/wave conditions) and to multi-robot docking/swarming experiments would validate scalability claims. Providing open-source CAD, control firmware, and analysis scripts (or at least detailed build/parameter identification procedures) would improve reproducibility and accelerate adoption.",2002.01918v3,https://arxiv.org/pdf/2002.01918v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:22:55Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Cost reduction|Other,Other,"Variable/General (covariates X; binary treatment {0,1})",Healthcare/medical|Service industry|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/2002.05308v1,"The paper develops an efficient adaptive experimental design for estimating the average treatment effect (ATE) in sequential experiments where treatment assignment probabilities are updated over time using accrued data. It derives the semiparametric efficiency bound for regular ATE estimators as a function of the treatment-assignment policy and shows the bound is minimized by the “efficient” (Neyman) allocation, which assigns treatment with probability proportional to the covariate-conditional standard deviations of potential outcomes. The proposed design sequentially estimates these conditional variances (via nonparametric regression for conditional means and second moments) and uses the resulting estimated efficient assignment probability to randomize each new unit. For estimation/inference it introduces the Adaptive Augmented Inverse Probability Weighting (A2IPW) estimator whose score forms a martingale difference sequence, yielding asymptotic normality without a Donsker condition and achieving the minimized semiparametric efficiency bound under the proposed design. The paper also provides non-asymptotic, anytime-valid confidence sequences (LIL/Bernstein-type) enabling rate-optimal sequential hypothesis testing with early stopping, and demonstrates performance gains over fixed 50/50 RCTs in synthetic and semi-synthetic simulations (including IHDP).","ATE target: $\theta_0=\mathbb{E}[Y(1)]-\mathbb{E}[Y(0)]$. Semiparametric efficiency bound for assignment policy $\pi$: $V(\pi)=\mathbb{E}\left[\frac{\sigma_0^2(1,X)}{\pi(1\mid X)}+\frac{\sigma_0^2(0,X)}{\pi(0\mid X)}+(\theta_0(X)-\theta_0)^2\right]$. Efficient (Neyman) assignment: $\pi^*(a\mid x)=\frac{\sqrt{\sigma_0^2(a,x)}}{\sqrt{\sigma_0^2(1,x)}+\sqrt{\sigma_0^2(0,x)}}$. A2IPW estimator: $\hat\theta_T^{A2IPW}=\frac1T\sum_{t=1}^T\left(\frac{\mathbb{1}[A_t=1](Y_t-\hat f_{t-1}(1,X_t))}{\pi_t(1\mid X_t,H_{t-1})}-\frac{\mathbb{1}[A_t=0](Y_t-\hat f_{t-1}(0,X_t))}{\pi_t(0\mid X_t,H_{t-1})}+\hat f_{t-1}(1,X_t)-\hat f_{t-1}(0,X_t)\right)$.","The design’s optimal assignment policy is characterized in closed form (Proposition 2): the allocation that minimizes the semiparametric efficiency bound is the Neyman allocation proportional to conditional outcome standard deviations. Theorem 1 shows $\sqrt{T}(\hat\theta_T^{A2IPW}-\theta_0)\Rightarrow N(0,V)$ and the asymptotic variance equals the semiparametric efficiency bound under the limiting assignment policy, achieving the minimized bound when $\pi_t\to\pi^*$. The paper derives non-asymptotic anytime-valid confidence sequences using LIL/Bernstein-style martingale concentration, enabling sequential tests with early stopping and near-oracle sample complexity (expected stopping time proportional to the oracle fixed-sample requirement). In simulations, A2IPW under adaptive allocation attains lower MSE than a 50/50 RCT and improves rejection rates/power in settings with nonzero ATE (e.g., Dataset 1 at $T=300$: RCT MSE 0.073 vs A2IPW(NW) 0.025; IHDP surface A at $T=150$: RCT MSE 0.674 vs A2IPW(NW) 0.485).",None stated.,"The method relies on key causal/experiment assumptions (SUTVA, correct implementation of randomized assignment given $(X_t,H_{t-1})$, and bounded outcomes) and on the ability to consistently estimate conditional moments/variances; performance may degrade with high-dimensional covariates, limited overlap, or unstable propensity estimates (acknowledged indirectly via proposed stabilizations). Much of the empirical evidence is simulation-based (synthetic and semi-synthetic IHDP), so real-world operational constraints (delayed outcomes, noncompliance, interference, time trends) and robustness to model misspecification/autocorrelation are not fully validated. Implementation details for choosing nonparametric estimators, tuning parameters (bandwidth/k), and stabilization schedules ($\gamma_t,\zeta_T$) are not standardized, which can materially affect finite-sample behavior and reproducibility.","The conclusion suggests extensions to settings with network interference, clustered experimental designs, and heterogeneous treatment effects, and further work on finite-sample optimality guarantees and connections to best-arm identification. It also proposes incorporating reinforcement learning techniques into the treatment-assignment phase to improve adaptability in more dynamic environments.","Developing practical guidance and open-source implementations (with default tuning/stabilization choices) would improve adoption and allow benchmarking across domains. Extending the framework to handle delayed or censored outcomes and nonstationarity/autocorrelation (common in online experiments) would broaden applicability while preserving anytime-valid inference. Additional comparisons to modern bandit/CAIPW/DML baselines under realistic constraints (e.g., minimum allocation floors, multiple arms, multiple endpoints) and more real-world case studies would strengthen external validity.",2002.05308v7,https://arxiv.org/pdf/2002.05308v7.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:23:36Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Other,Not applicable,Variable/General (customers and listings randomized; treatment fractions a_C and a_L),Service industry|Other,Exact distribution theory|Simulation study|Other,TRUE,None / Not applicable,Not provided,https://assets.amazon.science/c1/94/0d6431bf46f7978295d245dd6e06/double-randomized-online-experiments.pdf|http://dx.doi.org/10.1080/01621459.2016.1241178|http://dx.doi.org/10.1145/2600057.2602837|https://web.stanford.edu/class/ee363/lectures/lyap.pdf|https://eng.lyft.com/experimentation-in-a-ridesharing-marketplace-f75a9c4fcf01|https://dspace.mit.edu/handle/1721.1/117999|https://doordash.engineering/2019/02/20/experiment-rigor-for-switchback-experiment-analysis/|https://arxiv.org/abs/1903.02124,"The paper develops a theoretical framework for experimental design in two-sided marketplaces where interference (violations of SUTVA) causes bias in standard A/B test estimators. It analyzes two canonical marketplace designs—customer-side randomization (CR) and listing-side randomization (LR)—and shows which is (asymptotically) unbiased depends on market balance: CR is unbiased in highly demand-constrained markets (λ/τ→0) while LR is unbiased in highly supply-constrained markets (λ/τ→∞). The authors introduce two-sided randomization (TSR), which jointly randomizes customers and listings, and propose naive and improved TSR estimators that can reduce bias across a wide range of market balance regimes. Methodologically, they build a stochastic continuous-time Markov chain model with inventory/occupancy dynamics and derive a mean-field (fluid) limit described by an ODE system; the steady state is characterized via an optimization/Lyapunov argument. They evaluate estimator bias (theory + mean-field analysis) and study bias–variance tradeoffs via finite-system simulations, also comparing TSR against cluster-randomized experiments.","The market dynamics are modeled by a mean-field ODE for available listing mass $s_t(\theta)$: $\frac{d}{dt}s_t(\theta)=(\rho(\theta)-s_t(\theta))\tau(\theta)-\lambda\sum_\gamma \phi_\gamma p_\gamma(\theta\mid s_t)$, where $p_\gamma$ is a multinomial-logit choice probability $p_\gamma(\theta\mid s)=\frac{\alpha_\gamma(\theta)v_\gamma(\theta)s(\theta)}{\varepsilon_\gamma+\sum_{\theta'}\alpha_\gamma(\theta')v_\gamma(\theta')s(\theta')}$. Naive estimators are defined from steady-state booking rates $Q_{ij}$: CR uses $\widehat{\mathrm{GTE}}_{CR}=Q_{11}/a_C - Q_{01}/(1-a_C)$, LR uses $\widehat{\mathrm{GTE}}_{LR}=Q_{11}/a_L - Q_{10}/(1-a_L)$, and TSR uses $\widehat{\mathrm{GTE}}_{TSRN}=Q_{11}/(a_Ca_L) - (Q_{01}+Q_{10}+Q_{00})/(1-a_Ca_L)$ (with further ‘improved’ TSR estimators adding interference correction terms).","The main theoretical result is a market-balance-dependent bias characterization: as $\lambda/\tau\to 0$ (demand-constrained), the naive CR estimator becomes unbiased for the global treatment effect while the naive LR estimator remains generically biased; as $\lambda/\tau\to\infty$ (supply-constrained), the naive LR estimator becomes (absolutely) unbiased while the naive CR estimator remains generically biased. The paper proposes TSR designs (choosing treatment fractions $a_C(\lambda/\tau)$ and $a_L(\lambda/\tau)$) that recover unbiasedness in both extreme regimes and reduce bias in intermediate regimes. Finite-market simulations (e.g., with $N=5000$ listings and repeated Monte Carlo runs) corroborate mean-field bias predictions and show TSR can substantially reduce bias but typically increases variance, revealing a bias–variance tradeoff. Simulations also indicate TSR estimators can outperform cluster randomization in highly interconnected markets where clustering cannot adequately remove interference.","The authors note several simplifying modeling assumptions: multinomial logit choice, independent consideration-set inclusion, exponential occupancy times, and interventions modeled primarily as shifts in choice/consideration parameters (not strategic long-run equilibrium responses). They also emphasize that the mean-field model is deterministic and cannot capture estimator variance; variance is instead studied via finite-system simulations. They further state they do not address valid inference (standard error estimation and confidence intervals) under interference, and that not all experimental designs are feasible for all intervention types (e.g., ranking interventions may be more suited to CR than LR/TSR).","The work characterizes bias largely through asymptotic regimes (mean-field steady state and extreme market-balance limits), so finite-sample guidance for realistic, short-horizon experiments may be less direct and may depend strongly on tuning choices (e.g., $a_C,a_L,\beta,k$) and burn-in. Comparisons are primarily against naive CR/LR and a stylized “best-case” cluster randomization setup; broader benchmarking against alternative interference-robust estimators (e.g., exposure mapping, reweighting/HT estimators on bipartite graphs, switchbacks) is limited. The proposed “improved” TSR estimators are heuristic and may require careful variance estimation and robustness checks under model misspecification (e.g., autocorrelated arrivals, non-exponential holding times, learning/re-ranking dynamics). Practical implementation details (power, sample size planning, and operational constraints for two-sided randomization) are not fully developed.","They propose optimizing TSR designs/estimators for bias–variance tradeoffs, including identifying designs that are optimal under different market conditions. They also suggest using TSR as a measurement tool to debias one-sided CR/LR experiments by estimating competition/interference effects. Additional directions include relaxing modeling assumptions (choice/booking processes), extending to other intervention types (e.g., ranking), and developing valid inference procedures (standard errors and confidence intervals) under interference and dependence.","Developing principled (e.g., semiparametric or design-based) estimators and variance estimators for TSR that yield valid confidence intervals under interference would make the approach operationally actionable. Extending the framework to time-varying platform algorithms (learning-to-rank), feedback loops, and strategic entry/exit could connect TSR to long-run marketplace equilibrium experimentation. Exploring robust TSR designs under partial compliance, heterogeneous treatment assignment constraints, and network/geo clustering simultaneously (hybrid TSR+cluster or TSR+switchback) could improve practicality. Providing open-source reference implementations and reproducible benchmarks across multiple marketplace simulators/datasets would accelerate adoption and comparative evaluation.",2002.05670v5,https://arxiv.org/pdf/2002.05670v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:24:39Z
FALSE,NA,NA,Not applicable,"Not specified (general parametric mechanical design parameters; control parameters p1,p2 and design parameters qI={κR,λR,υ}, qII={κD,λD,α,ρ,υ})",Other,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.ing.unitn.it/~dalcorsf/elastica_catastrophe_machine.html,"This paper develops the theory, design framework, and experimental validation of an “elastica catastrophe machine,” extending classical (discrete) catastrophe machines to a continuous elastic element governed by nonlinear elastica solutions. The method uses primary kinematical quantities and the concept of a universal snap surface to characterize snap-back (instability) conditions and to compute catastrophe loci as intersections/projections across control, kinematical, and physical spaces. Two parametric machine families (ECM-I and ECM-II, with two control parameters and multiple design parameters) are proposed and explored to show that catastrophe loci can take diverse shapes, with tunable convexity and a variable number of bifurcation points, enabling more efficient snapping devices. A physical prototype is built and tested with different rods; experimentally measured critical points along predicted catastrophe loci show excellent agreement with theory. The work is primarily nonlinear mechanics/instability modeling with design-parameter tuning rather than DOE methodology.","The elastica configuration is defined by boundary conditions at s=0 and s=l (Eq. 1) with inextensibility d=\sqrt{(X_l-X_0)^2+(Y_l-Y_0)^2}\le l (Eq. 2). Primary kinematical quantities are d and end-rotation combinations \theta_A=(\Theta_l+\Theta_0)/2-\arctan((Y_l-Y_0)/(X_l-X_0)) and \theta_S=(\Theta_l-\Theta_0)/2 (Eq. 12); bistability/monostability is determined by \mathrm{SK}(d,\theta_A,\theta_S)\gtrless 0, with snapping on the universal snap surface \mathrm{SK}(d,\theta_A,\theta_S)=0 (Eqs. 13–14). For ECM-I and ECM-II, the imposed end rotation is \Theta_l(p,q)=p_2+\upsilon (Eq. 37) and end position is parameterized by control/design relations (e.g., ECM-I: X_l=(\kappa_R+p_1\cos p_2)l,\;Y_l=(\lambda_R+p_1\sin p_2)l (Eq. 38); ECM-II: Eq. 42).","Parametric studies show catastrophe loci with a variable number of bifurcation points (reported examples include 1–5 depending on design parameters and machine subtype) and convexity measures approaching 1 for certain settings (e.g., C=0.9997 for one ECM-I locus; C=0.9998 for an ECM-II locus, and several cases with C≈1). The authors report specific convexity values for selected ECM-I settings (e.g., C={0.5707, 0.4475, 0.9402} for \upsilon=0 and \lambda_R={0.3,0.35,0.4}). Experimental validation overlays measured critical points (polikristal and carbon-fiber rods) on theoretically predicted catastrophe loci for multiple configurations (ECM-I and ECM-II variants) and shows excellent agreement, with higher measurement accuracy for carbon-fiber rods. Videos demonstrating ECM-I and ECM-IIb operation are provided as supplementary material via the linked webpage.","The authors note that during experimental tests, “very small portions of the catastrophe locus were not investigated because of some unavoidable physical limitations (for instance rod’s self-intersection).” They also state that accuracy is lower for polikristal rods, attributed to “intrinsic viscosity and weight-stiffness ratio” of the material.","The framework appears to assume quasi-static equilibrium paths and ideal boundary conditions; dynamic effects during snap-through (inertia, damping, rate dependence) are not integrated into the main predictive model, which may limit prediction under faster actuation. Practical reproducibility may depend sensitively on friction, clamp compliance, and manufacturing tolerances of the prototype, which are not systematically quantified. The numerical procedures used to compute catastrophe sets are referenced to supplementary material, but without shared code, independent replication and robustness checks across discretization/solver choices are harder.",None stated.,"Providing open-source code (and possibly CAD files for the prototype) for computing catastrophe loci and for reproducing the parametric studies would improve reproducibility and adoption. Extending the model to include dynamics (mass, damping, rate effects) and contact/self-intersection handling would broaden applicability to real devices. Further work could generalize to 3D rods, extensible rods, or multistable metamaterial architectures and validate performance in targeted applications (energy harvesting, wave mitigation) under operational loading.",2002.07872v1,https://arxiv.org/pdf/2002.07872v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:25:23Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Model discrimination|Other,Bayesian D-optimal|Not applicable,"Variable/General (design dimension examples: D=1,10,100 for linear model; D=1,10 for PK; D=1,5 for gas leak with 2D locations per measurement)",Healthcare/medical|Pharmaceutical|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/stevenkleinegesse/minebed,"The paper proposes MINEBED, a Bayesian experimental design method for implicit (likelihood-intractable) stochastic models that maximizes expected mutual information between parameters and data using neural mutual information estimation (MINE). By optimizing a variational lower bound on mutual information with respect to both the neural critic parameters and the design variables, the approach enables scalable gradient-based design optimization when a differentiable sampling path is available; when not, it falls back to Bayesian optimization over designs. A key byproduct is an implicit posterior estimator: once the MI bound is tight, the trained network yields an estimate of the posterior density via an exponential tilt of the prior, enabling amortized posterior sampling. Simulation studies on a noisy linear model (up to 100 design dimensions), a pharmacokinetic blood-sampling-time design problem, and a gas-leak localization fluid-simulator example show the method produces intuitive designs and reasonable posteriors, and extends MI-based design to higher-dimensional design spaces than prior likelihood-free approaches.","The Bayesian experimental design utility is the mutual information $I(\theta;y\,;d)=\mathbb{E}_{p(\theta,y\mid d)}\big[\log \tfrac{p(\theta\mid y,d)}{p(\theta)}\big]$. The method maximizes the MINE-f lower bound $I_b(d,\psi)=\mathbb{E}_{p(\theta,y\mid d)}[T_\psi(\theta,y)]-e^{-1}\,\mathbb{E}_{p(\theta)p(y\mid d)}[e^{T_\psi(\theta,y)}]$ jointly over $d$ and neural parameters $\psi$. With a reparameterized sampling path $y=h(\epsilon;\theta,d)$, gradients $\nabla_d I_b(d,\psi)$ are estimated via pathwise derivatives; when the bound is tight the posterior is given by $p(\theta\mid y,d)\propto e^{T_{\psi^*}(\theta,y)-1}p(\theta)$.","In the noisy linear model, the learned MI lower bound converges close to numerically-computed reference MI values for design dimensions $D=1,10,$ and $100$, demonstrating scalability to high-dimensional designs. For the pharmacokinetic model, the optimal single sampling time is reported as $t^*=0.551$ hours (with the MI lower bound close to a reference value), and for $D=10$ the optimal times form early/middle/late clusters consistent with prior PK design findings. For gas-leak localization where sampling-path gradients are unavailable, Bayesian optimization over designs yields an optimal single measurement location $d^*=[64,64]^\top$, and for $D=5$ the resulting posterior over leak location becomes substantially more concentrated (though can be bimodal due to wind-direction uncertainty). The paper also reports that MI-based designs can differ subtly from Bayesian D-optimal designs in an oscillatory toy example, reflecting MI’s sensitivity to posterior multimodality.","The authors note that the approach depends on being able to compute or approximate sampling-path derivatives with respect to design variables for gradient-based optimization; when these are unavailable, one must revert to gradient-free optimization (e.g., Bayesian optimization), reducing scalability. They also emphasize that they focus on one specific MI lower bound and do not claim it is superior to other MI bounds, implying performance may depend on the choice of bound and its tightness during training.","The method’s performance is likely sensitive to neural-network architecture, optimization hyperparameters, and stability of MI lower-bound training, but systematic robustness analyses and practical tuning guidance are limited. Empirical validation is primarily simulation-based; there is no real-world experimental deployment showing end-to-end gains under measurement constraints, model mismatch, or operational costs. Comparisons to alternative likelihood-free Bayesian design methods are limited and not standardized across benchmarks, making it hard to quantify relative advantages across scenarios (e.g., varying noise, priors, or misspecified simulators).",The authors suggest extending the method to sequential experimental design where the posterior is updated after each real-world experiment. They also propose investigating extensions to experimental design for model discrimination and for prediction of future observations.,"Develop self-starting or budget-aware variants that incorporate explicit experimental costs/constraints (e.g., travel/path planning for spatial sensing, patient burden in PK) into the design objective. Extend the framework to handle model misspecification and simulator/measurement noise robustness (e.g., heavy-tailed noise, autocorrelated observations) and provide diagnostics for bound tightness and design reliability. Provide reference software implementations and benchmarks across common simulator-based design tasks, including multivariate/high-dimensional parameter settings and discrete or constrained design spaces.",2002.08129v3,https://arxiv.org/pdf/2002.08129v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:26:06Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Optimization|Prediction|Cost reduction|Other,Not applicable,"Variable/General (examples include 1D toy, 2D synthetic, 6D Hartmann, classifier hyperparameters; real-world SPF has 5 parameters/162 discrete combinations)",Manufacturing (general)|Food/agriculture|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://tini.to/PJJH,"The paper proposes a Bayesian-optimization-based sequential experimental design method that incorporates expert prior knowledge about the location of the global optimum. Expert knowledge is represented as a prior distribution \(\pi(x^*)\) over the optimizer, and the authors derive an updated optimizer posterior \(p(x^*\mid D_n,\pi)\) that is proportional to the standard BO optimizer posterior times \(\pi\). They implement an efficient posterior-sampling (Thompson-style) algorithm by drawing candidate maximizers from the GP posterior (using random-feature GP sampling) and then reweighting/resampling them according to the expert prior. The method is evaluated on synthetic functions, Hartmann 6D, hyperparameter tuning tasks, and a costly real-world materials experiment (short polymer fiber synthesis), showing faster convergence (lower simple regret / higher achieved objective sooner) when the prior is informative and robustness when the prior is mildly misleading. The work advances the BO-for-experimental-design literature by providing a practical, general mechanism to inject prior beliefs specifically about the optimizer location, rather than about function shape constraints (e.g., monotonicity).","The experimental-design core is the derived optimizer posterior: \(p(x^*\mid D_n,\pi) \propto p(x^*\mid D_n)\,\pi(x^*)\), where \(p(x^*\mid D_n)=\int p(x^*\mid f)\,p(f\mid D_n)\,df\) is the (intractable) optimizer posterior under a GP surrogate. In practice, they draw \(\{x_i^*\}_{i=1}^N\) as maximizers of GP posterior function samples and then sample the next evaluation by reweighting: \(\Pr(x^*=x_i^*\mid D_n,\pi)\propto \pi(x_i^*)\) (normalized over \(i\)). GP posterior sampling is approximated with random Fourier features \(f(x)\approx \phi(x)^\top\theta\), sampling \(\theta\) from its Bayesian linear-model posterior.","Across synthetic objectives, PS-G (posterior sampling with an optimizer prior) achieves lower simple regret than standard Thompson sampling, EI, PES, and prior-only random search, and performance improves when the prior mean is closer to the true optimum and when its covariance appropriately reflects confidence. The method remains able to converge even with priors centered away from the optimum, though it may require more iterations when the prior is strongly misleading. For Hartmann 6D, PS-G outperforms TS, EI, and prior-based random search in simple regret over the evaluation budget shown. In hyperparameter tuning (OpenML100 datasets), PS-G with truncated Gamma priors on influential hyperparameters yields better (or competitive) maximal validation accuracy improvements over baselines during 30 iterations. In a real costly SPF synthesis task (5 parameters, 162 discrete combos), incorporating a truncated-Gaussian prior on butanol speed improves the maximal achieved desirable-fiber percentage over iterations compared with TS/EI/random search.","The authors note that choosing an appropriate prior \(\pi(x^*)\) is non-trivial, and that while the method can converge with a “gross” (misleading) prior, it is vulnerable when the prior is “extremely misleading.” They also highlight the practical challenge that expert knowledge must be represented in a tractable probabilistic form, even though humans may perceive it informally.","The method assumes the expert belief is well-captured by a prior directly on the argmax location; in many domains, experts may provide constraints or relative preferences that are easier to elicit than a distribution over \(x^*\). The approach relies on approximate GP posterior function sampling via random Fourier features and numerical maximization of sampled functions; performance can be sensitive to feature dimension, optimization of sampled functions, and scalability in high dimensions or with complex constraints. Comparisons focus on common BO baselines, but do not comprehensively benchmark against modern constrained BO, trust-region BO, multi-fidelity BO, or preference/ordinal-feedback BO that may be strong in expensive-experiment settings. Practical Phase I issues (hyperparameter learning of the GP kernel, handling nonstationarity, heteroscedastic noise, or correlated observations) are not deeply explored, yet these often matter in real experimental campaigns.",They suggest developing algorithms to detect a misleading prior quickly (to mitigate cases where \(\pi(x^*)\) is far from the true optimum).,"A natural extension is to formalize prior elicitation tools for \(\pi(x^*)\) (e.g., converting expert constraints, rankings, or “likely regions” into calibrated distributions) and to learn/temper the influence of \(\pi\) online. Extending the framework to constrained, multi-objective, and multi-fidelity experimental design would increase applicability to real labs, as would handling discrete/conditional search spaces more systematically. It would also be valuable to provide theoretical and empirical study on robustness under GP mis-specification, nonstationarity, and heteroscedastic noise, and to release reusable software (e.g., an R/Python package) with standardized benchmarks for reproducibility.",2002.11256v1,https://arxiv.org/pdf/2002.11256v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:26:44Z
TRUE,Optimal design|Bayesian design,Parameter estimation|Other,Other,"Variable/General (k controllable variables; examples with k=1,2,4)",Theoretical/simulation only|Pharmaceutical,Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies Bayesian decision-theoretic design of experiments using the Fisher information gain (FIG) utility, for which the expected utility simplifies to the prior expectation of the trace of the Fisher information matrix. For exponential family models, it proves that maximizing expected FIG reduces to independently maximizing a reduced-dimension objective function $\phi(d)$ over the design region, implying FIG-optimal designs can concentrate all runs at design points that maximize $\phi(d)$. It shows this simplification can be problematic: if $\phi(d)$ has fewer global maxima than the number of parameters $p$, then any FIG-optimal design is under-supported and induces parameter redundancy (non-identifiability). The paper illustrates these issues for normal linear regression (including response surface terms), Poisson regression, logistic regression, and a nonlinear compartmental model, often finding FIG-optimal designs place all runs at extreme points or even a single time point. It concludes FIG can be attractive computationally but should be used cautiously as a default utility, recommending checks for identifiability or alternative FIG-based approaches (e.g., adversarial/game-theoretic design).","FIG utility is $u_{\mathrm{FIG}}(\theta,y,D)=\|\nabla_\theta\log\pi(\theta\mid y,D)\|^2-\|\nabla_\theta\log\pi(\theta)\|^2$, with expected FIG $U_{\mathrm{FIG}}(D)=\mathbb{E}_\theta[\mathrm{tr}\{I(\theta,D)\}]$ where $I(\theta,D)=\mathbb{E}_{y\mid\theta,D}[\nabla_\theta\log\pi(y\mid\theta,D)\nabla_\theta\log\pi(y\mid\theta,D)^T]$. For exponential-family models with known scale, $\phi(d)=\sum_{j=1}^p \mathbb{E}_\theta\big[\mathrm{var}(y\mid\theta,d)^{-1}(\partial\mu(\theta,d)/\partial\theta_j)^2\big]$ and $U_{\mathrm{FIG}}(D)=\sum_{i=1}^n \phi(d_i)$ (up to constants in some cases). For normal with unknown variance integrated out under an inverse-gamma prior, $\phi(d)=\sum_{j=1}^p \mathbb{E}_\theta[(\partial\mu(\theta,d)/\partial\theta_j)^2]$.","For normal linear regression with monomial regression terms and symmetric bounded factor regions, the paper proves FIG-optimal designs place each factor at an extreme (endpoints), so the number of possible support points is at most $q=2^C$ (with $C$ the count of symmetric factor intervals), implying under-support when $p>2^C$ (e.g., a 2-factor second-order response surface with $p=6$ has $q=4$ so all FIG-optimal designs are under-supported). It further provides a sufficient condition showing that including an intercept and any squared term forces parameter redundancy under FIG-optimal designs even when the design is not under-supported. In a 4-factor logistic regression example with $p=5$, numerical maximization (via quadrature and multi-start optimization) finds only two maxima of $\hat\phi(d)$, hence any FIG-optimal design has $q=2<p$ and is parameter redundant. For a nonlinear pharmacokinetic compartmental model with design region $[0,24]$, $\hat\phi(d)$ has a single maximum at $d=24$, so the unique FIG-optimal design repeats the same sampling time and is under-supported.",None stated.,"The work focuses on FIG as the sole Bayesian utility and does not provide systematic empirical comparisons to SIG/NSEL-based Bayesian designs or pseudo-Bayesian D-/A-optimal designs in terms of downstream estimation/prediction accuracy under realistic finite-sample analyses. The main theoretical results are for exponential-family settings and, for unknown variance treated as nuisance, essentially specialize to the normal-inverse-gamma case; identifiability behavior under FIG with other nuisance-parameter marginalizations is not developed. The paper does not provide implementable algorithms/software for practitioners to detect multimodality/maxima of $\phi(d)$ or to enforce identifiability constraints during FIG optimization, beyond recommending checks and numerical searches.","It suggests that when FIG is used as a default utility, practitioners should verify that the FIG-optimal design does not induce parameter redundancy, or else use an alternative DOE approach that still leverages FIG but circumvents the disadvantages (citing Prangle et al. (2020)'s game-theoretic/adversarial approach).","Develop constrained or regularized FIG-optimal design methods that explicitly enforce identifiability (e.g., requiring at least $p$ support points and full-rank conditions or adding penalties for near-singular information). Extend the analysis beyond exponential families to correlated/heteroscedastic data, time-series/autocorrelated responses, and common mixed-effects settings where Fisher information is available but rank behavior is more complex. Provide practical diagnostics and open-source implementations to reliably locate all global maxima of $\phi(d)$ (including multimodal cases) and to quantify sensitivity of the maxima structure to the prior and model specification.",2003.07315v4,https://arxiv.org/pdf/2003.07315v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:27:08Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Other,Variable/General (network-connected units; treatment assignment and participant selection optimized; pilot size \bar m and main size \bar n),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,https://arxiv.org/abs/2003.08421|https://www.gurobi.com|https://www.midaco-solver.com,"The paper proposes a variance-optimal design framework for two-wave (pilot + main) experiments under network interference with local dependence, targeting precise inference on user-chosen linear estimators of direct, spillover, and overall effects. The pilot is selected via a min-cut style optimization to be well-separated from the rest of the network while retaining enough within-pilot edges to identify outcome covariances; pilot treatment is randomized (Bernoulli(1/2)) and used to estimate variance and covariance functions. The main experiment then jointly optimizes participant selection and treatment assignments to minimize a plug-in estimate of the conditional variance of the target estimator(s), subject to excluding pilot units and their neighbors to preserve unconfoundedness. The paper derives regret bounds showing the two-wave design’s variance approaches an oracle variance at a rate governed by pilot estimation error and the pilot-to-main size trade-off, and provides asymptotic normality and variance estimation results under network sparsity. Simulations on simulated graphs and real-world networks show substantial variance reductions relative to clustering and saturation-design competitors, especially under heteroskedasticity and nonzero neighbor covariances; extensions add variance-capped re-randomization for design-based inference and handle partially observed networks via imputation.","Pilot selection solves a cut-minimization MIQP: choose P to minimize cross-edges \(\sum_i\sum_{j\in N_i} p_i(1-p_j)\) s.t. \(\sum_i p_i=\bar m\) and enough within-pilot adjacency \(\sum_i p_i\sum_{j\in N_i}p_j\ge \delta\). The main design minimizes a plug-in variance \(\hat V_{\hat\sigma_p^2,\hat\eta_p}(R,D_R)\) combining estimated heteroskedastic variances and neighbor covariances: \(\frac{1}{n^2}\sum_{i:R_i=1} w(i)^2\hat\sigma_p^2(T_i,D_i,g_i(D_{N_i})) + \frac{1}{n^2}\sum_{i:R_i=1}\sum_{j\in N_i} R_j w(i)w(j)\hat\eta_p(\cdot)\). A consistent variance estimator for inference is \(\hat V = \frac{1}{n^2}\sum_i R_i w(i)(Y_i-\hat m_i)\sum_{j\in N_i\cup\{i\}} R_j w(j)(Y_j-\hat m_j)\).","A regret bound (Theorem 4.1) shows \(\bar n\,[\mathrm{Var}(\hat\Gamma\mid R,D_R,A,T)-V^\star_{\bar n}]\le O_p(N_{\max}\,\bar m^{-\xi} + N_{\max}^2\bar m/n)\), capturing the trade-off between pilot estimation accuracy and the loss from excluding pilot neighborhoods. With parametric pilot rates (\(\xi=1/2\)) and pilot size \(\bar m\asymp (n/N_{\max})^{2/3}\), the regret vanishes: \(\bar n\,[\mathrm{Var}(\hat\Gamma\mid\cdot)-V^\star_{\bar n}] = o_p(1)\) (Corollary 4.2). The estimator is asymptotically normal conditionally on realized assignments with a feasible variance estimator (Theorem 4.3). In simulations on real and synthetic networks, the proposed ELI design achieves the lowest sampling variance among compared designs and can imply large effective sample-size savings (reported as up to ~40 percentage points in their figure for certain settings), with gains increasing under heteroskedasticity and neighbor correlation.","The analysis focuses on sparse networks (maximum degree must grow slowly) and excludes dense or hub-dominated graphs (e.g., star networks). The core setup assumes the network is observed (with a partial-network extension placed in an appendix) and assumes the network does not change between pilot and main waves. The framework targets user-specified linear estimators, explicitly ruling out nonlinear outcome estimators such as feasible two-stage GLS.","The design relies on a known exposure mapping \(g_i\) with discrete support; misspecification of the exposure model or interference range could degrade both the variance model and causal interpretation. The main-wave optimization is computationally hard (nonlinear mixed-integer program) and the practical performance may depend on solver heuristics and tuning; finite-sample optimality is not guaranteed when solved approximately. The approach emphasizes variance-optimality for model-based estimands; if the underlying outcome model used to justify the estimand/estimator is incorrect, improved precision may not translate to better causal interpretability.","The paper calls for a comprehensive theoretical analysis of partially observed networks, including model selection for the edge-imputation step. It also suggests extending designs beyond local interference to more general interaction structures where interference propagates farther through the network. Another stated direction is understanding how network topology and alternative exposure mappings affect the performance of the proposed design mechanisms.",Developing robust or adaptive procedures that jointly learn/validate the exposure mapping (or interference radius) while designing assignments could improve reliability under misspecification. Extending the framework to multi-arm or continuous treatments and to cluster-level interventions would broaden applicability. Providing open-source reference implementations (and standardized benchmarks) would facilitate uptake and allow reproducible comparisons across network-experiment design methods.,2003.08421v5,https://arxiv.org/pdf/2003.08421v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:27:47Z
FALSE,NA,NA,Not applicable,Not specified,Transportation/logistics|Environmental monitoring|Other,Case study (real dataset)|Other,TRUE,Other,Not provided,https://loco-auv.github.io/|https://ardupilot.org/|https://github.com/neufieldrobotics/spinnaker_sdk_camera_driver|https://github.com/cra-ros-pkg/robot_localization,"This paper presents LoCO AUV, a low-cost, open-source autonomous underwater vehicle platform, detailing its hardware architecture (watertight enclosures, thrusters, power, computing, and sensors) and ROS-based software stack. It also describes a Gazebo-based simulator for prototyping and testing, including URDF modeling and simplified hydrodynamic force/drag approximation informed by experimental underwater trials. The authors implement and demonstrate several autonomy/HRI capabilities on the platform, including menu-driven control via ARTags and hand gestures, robot communication via motion, and diver-following using deep neural network perception with PID control. Field deployments are reported across pools, a lake, and ocean environments, focusing on practical deployment workflows and early operational experience. The work is primarily a systems/platform contribution rather than a designed-experiments (DOE) methodology paper.",NA,"Deployment count is reported: LoCO was deployed in pools seven times, in Wayzata Bay (Minnesota) once, and off the coast of Barbados five times, with deployments described as incident-free. Vehicle specifications are provided (e.g., 12.47 kg, max speed 1.5 m/s, battery life from ~30 min at max thrust to ~18.5 hrs idle). For simulation parameterization, the paper states that experimental underwater trials at various thruster inputs were used to estimate a cumulative forward/reverse drag coefficient (from IMU-derived velocity and known thruster-force curves), while other drag components (vertical/lateral/rotational) remain unevaluated. No ARL/power-style statistical results or formal DOE comparisons are presented.","The authors note that the Gazebo simulation does not directly simulate real-time fluid mechanics; instead, hydrodynamic forces are approximated. They state that drag parameters for vertical, lateral, and rotational motion remain to be evaluated because similar underwater data were not readily gathered. They also indicate the platform is still selecting final cameras, with ongoing comparison of candidate cameras for frame-rate and effectiveness. For diver-following, they explicitly note there is room for improvement in practice even though the existing algorithm is serviceable.","The experimental trials used to fit drag appear limited to calm-water, steady-state forward motion; this may not generalize to varying currents, turbulence, different payload configurations, or transient maneuvers, and uncertainty in IMU-derived velocity can bias drag estimates without external ground truth. The paper reports deployments and capabilities but provides limited quantitative benchmarking of autonomy performance (e.g., tracking accuracy, control error, robustness across visibility conditions) and limited statistical characterization across repeated trials. Because the platform is modular and still evolving (camera choice, sonar integration, battery monitoring circuit in development), reproducibility of reported performance may vary across builds unless a frozen reference configuration is maintained.","The authors state that all code, plans, 3D printed part models, assembly instructions, and documentation will be released under a permissive open-source license via their project website. They also mention ongoing development efforts including integrating a custom battery charge-reporting circuit, integrating a sonar altimeter, and improving state estimation (e.g., adding visual odometry and/or model-based odometry) and local motion control/HRI capabilities to enable more feasible single-person and tetherless deployments. They further describe upcoming collaborative trials with marine biologists using LoCO for data acquisition support.","A natural extension would be a standardized experimental evaluation protocol for autonomy/HRI modules (gesture/menu accuracy, diver-following tracking error, station-keeping performance) across controlled variations in visibility, lighting, turbidity, and current, with open datasets for benchmarking. Simulation fidelity could be improved by validating hydrodynamic parameters against external positioning systems (e.g., motion capture in pool, acoustic tracking) and extending drag/added-mass modeling beyond forward motion. Providing packaged, versioned software releases (e.g., Docker/ROS images) and full bill-of-materials variants would improve reproducibility across builders and reduce integration friction.",2003.09041v1,https://arxiv.org/pdf/2003.09041v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:28:20Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation,Other,Variable/General (design variable(s) depend on model; examples include 1D time/location designs and 2-parameter models),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a myopic sequential Bayesian experimental design framework for implicit (likelihood-intractable) models, targeting parameter estimation. The utility is the mutual information (MI) between parameters and prospective data; MI is estimated directly via likelihood-free density-ratio estimation using LFIRE, which simultaneously yields an approximate sequential posterior. At each iteration, the MI utility is optimized over the design space using Bayesian optimization with a Gaussian-process surrogate (Matérn 5/2 kernel, expected improvement). The method includes a weighted-particle belief update across sequential experiments and a resampling step (mixture-of-Gaussians smoothing) when effective sample size deteriorates. Empirical studies on a sinusoid toy problem (highlighting multimodal posteriors), epidemiological Death and SIR models, and a cell-spreading scratch-assay model show accurate parameter estimates in few iterations and illustrate that MI-based design avoids the multimodality bias seen with Bayesian D-optimality.","The sequential MI utility at iteration k is $U_k(d)=\int \log\frac{p(\theta\mid d,y,\mathcal D_{k-1})}{p(\theta\mid\mathcal D_{k-1})}\,p(y\mid\theta,d)\,p(\theta\mid\mathcal D_{k-1})\,d\theta\,dy$, approximated by Monte Carlo as $\hat U_k(d)=\frac1N\sum_{i=1}^N \log \hat r_k(d,y^{(i)},\theta^{(i)},\mathcal D_{k-1})$ with $y^{(i)}\sim p(y\mid\theta^{(i)},d)$ and $\theta^{(i)}\sim p(\theta\mid\mathcal D_{k-1})$. LFIRE estimates the density ratio $r(d,y,\theta)=\frac{p(y\mid\theta,d)}{p(y\mid d)}=\frac{p(\theta\mid d,y)}{p(\theta)}$, using a log-linear model $\hat r(d,y,\theta)=\exp(\beta(d,\theta)^\top\psi(y))$. Sequential belief updates are implemented via multiplicative weights $w_k(\theta)=\prod_{s=1}^k \hat r_s(d_s^*,y_s^*,\theta,\mathcal D_{s-1})$, with resampling triggered by effective sample size $\eta=(\sum_i w_i)^2/\sum_i w_i^2$.","On the oscillatory toy model, MI-based designs select later measurement times than Bayesian D-optimality in early iterations, preserving multimodal explanations; after sufficient data the two utilities yield similar designs. For the Death model after 4 sequential measurements, the posterior mean infection rate is reported as $\hat b=1.376$ with 95% credible interval $[1.128,1.621]$ (true $b=1.5$). For the SIR model after 4 iterations, the mean estimates are $\hat\beta=0.171$ with 95% CI $[0.112,0.233]$ and $\hat\gamma=0.045$ with 95% CI $[0.024,0.068]$ (true $(\beta,\gamma)=(0.15,0.05)$). For the cell-spreading model after 5 iterations using 300 particles, the reported means are $\hat P_m=0.394$ (95% CI $[0.166,0.642]$) and $\hat P_p=0.00150$ (95% CI $[0.00055,0.00265]$), containing the true values $(0.35,0.001)$.","The authors note that although the framework is general, extending to high-dimensional design spaces becomes substantially more computationally intensive and standard Bayesian optimization becomes expensive/less effective in high dimensions. They also state that MI focuses on information gain/estimation accuracy and does not account for differing experimental costs across designs unless one normalizes by cost. They also highlight that for expensive simulators (e.g., the cell model) they reduce particle counts (e.g., to 300), which may increase Monte Carlo and ratio-estimation error.","The approach depends on the quality of LFIRE ratio estimation with fixed, hand-crafted summary statistics (often low-order powers); misspecified summaries or classifier models can bias MI estimates and hence the chosen designs. Comparisons are limited primarily to Bayesian D-optimality in one toy setting; the paper does not benchmark against other modern likelihood-free sequential design approaches (e.g., neural ratio/likelihood methods, alternative MI estimators such as MINE-style bounds) or alternative acquisition strategies for BO under noisy objectives. The sequential design is explicitly myopic (one-step lookahead), which can be suboptimal when design decisions have long-term interactions or constraints. Practical issues such as model misspecification, unknown nuisance parameters, observation noise models, or correlated/temporally dependent experimental outcomes are not extensively stress-tested.","They suggest using high-dimensional Bayesian optimization advances or alternative gradient-free optimizers (e.g., approximate coordinate exchange) for high-dimensional design spaces. They propose incorporating experimental cost by optimizing a normalized information gain (e.g., MI divided by expected cost). They also state the framework could be adapted beyond parameter estimation to model discrimination or dual-purpose model discrimination and parameter estimation by conditioning on models and averaging over model space.","Developing self-tuning/learned summary statistics (e.g., neural embeddings) within LFIRE specifically optimized for MI-based design could improve robustness and reduce manual feature engineering. Extending the method to constrained or batch (multi-point) designs, and to non-myopic planning (multi-step lookahead) would broaden applicability in real experiments. More comprehensive robustness analyses under model misspecification, autocorrelated data, and unknown observation processes would clarify practical reliability. Providing open-source implementations and standardized benchmarks across implicit models would improve reproducibility and accelerate adoption.",2003.09379v1,https://arxiv.org/pdf/2003.09379v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:29:03Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Other,A-optimal|Bayesian A-optimal,"Variable/General (design variables are internal sensor locations; examples include mint=16, 18, 23, 26 internal sensors; boundary pixels up to mbdry=361; mt=20 time points)",Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper formulates iron-loss identification in electric-machine/transformer cores as an inverse heat-source problem for a linear parabolic heat equation with Robin/Neumann boundary conditions. It optimizes the placement of a limited number of internal temperature sensors (augmented by thermal-camera boundary measurements) using Bayesian optimal experimental design, specifically an A-optimality criterion that minimizes the trace of the posterior covariance of the discretized heat source. The forward model is discretized with finite elements in space and implicit midpoint time stepping, and Bayesian inversion assumes Gaussian priors and additive Gaussian measurement noise to obtain closed-form posterior mean/covariance expressions. To make repeated sensor-optimization feasible in large 3D problems, it introduces a low-rank (randomized SVD–style) approximation of the prior-conditioned boundary measurement operator so that posterior covariances can be updated efficiently during optimization. Numerical experiments with simulated data in 2D and 3D transformer-like geometries show that time-transient data and A-optimized internal sensor locations substantially improve reconstruction accuracy compared to regular-grid sensor placements.","Bayesian linear-Gaussian posterior covariance and mean are given by $\Gamma_{\text{post}}(p)=(\Gamma_{\text{pr}}^{-1}+F(p)^T\Gamma_{\text{noise}}^{-1}F(p))^{-1}$ and $\hat x(p)=\Gamma_{\text{post}}(p)(\Gamma_{\text{pr}}^{-1}x_{\text{pr}}+F(p)^T\Gamma_{\text{noise}}^{-1}y)$. The A-optimal design chooses sensor-location parameters $p^*$ to minimize $\Phi_A(p)=\mathrm{tr}(A\Gamma_{\text{post}}(p)A^T)$ (often with $A^TA=M$, the FE mass matrix, or $A=L$ where $\Gamma_{\text{pr}}^{-1}=L^TL$). For scalability, a low-rank approximation of the prior-conditioned boundary operator $F_{\text{pr,bdry}}=F_{\text{bdry}}L^{-1}\approx U_r\Sigma_rV_r^T$ is used to form an efficient approximate posterior covariance via a Woodbury-type expression (Eq. (31) in the paper).","In Experiment II (2D semirealistic transformer cross-section), A-optimized internal sensor placements yielded lower relative $L^2(\Omega)$ reconstruction errors than regular grids; e.g., with 18 inner sensors the regular grid gave $\mathrm{err}_{\mathrm{rel}}\approx 0.243$ while A-optimized gave $\approx 0.169$, and with 26 sensors regular grid gave $\approx 0.186$ while A-optimized gave $\approx 0.143$ (reported on figures). In Experiment III (3D), a low-rank reduction of boundary measurements from $m_{\text{bdry}}m_t=7220$ down to about $r=120$ produced negligible reconstruction discrepancy in a small test while enabling optimization in a large model with $n\approx 49{,}310$. For the large 3D case with $m_{\text{int}}=23$, A-optimized sensor locations improved error from $\mathrm{err}_{\mathrm{rel}}=0.317$ (initial) to $0.262$ (optimized). The study also demonstrates that transient (time-series) boundary data provides markedly more information than steady-state-only data for source reconstruction.","The authors emphasize the study is preliminary and uses a simplified linear heat model; in practice the forward model is nonlinear with temperature-dependent coefficients, and the boundary heat transfer coefficient $h$ is not precisely known. They assume the model parameters (except the source) are known and also assume the noise covariance is known and Gaussian, noting this is unrealistic but simplifies computations. They note the prior model is a simple combined $L^2$–$H^1$ regularization prior that may not capture generic properties of iron-loss fields. They also note that the A-optimal objective is not discretization invariant for their chosen prior (trace term blow-up as discretization is refined).","The optimization appears to be local (gradient/steepest-descent) and the objective is nonconvex with many local minima, so results may depend strongly on initialization and constraints; global optimality is not addressed. All main performance claims rely on simulated data generated from the same forward model class (no model discrepancy), so practical robustness to mismodeling, bias, and correlated/heteroscedastic measurement noise is not established. Comparisons are limited (primarily regular grids and one sparsification baseline in a simple case), with no broad benchmarking against alternative OED criteria (D-/G-/I-optimal) or sensor-placement heuristics. The approach treats sensor count as fixed (in the sliding-sensors method) and does not jointly optimize number of sensors vs. placement under explicit cost/feasibility constraints beyond a brief overlap penalty mention.","They suggest extending the work to more realistic settings by incorporating modeling errors either via an approximation-error approach (absorbing mismatch into the noise model) or by properly modeling nonlinearities and treating $h$ as an additional unknown, leading to a more complex nonlinear inverse problem. They also state that constructing a prior distribution that captures generic properties of iron-loss models is a research topic of its own. Another proposed direction is to design statistical tests from temperature measurements to confirm or reject proposed iron-loss models. They also discuss scalability/parallelization of forward solves as a path to larger problems.","Validate the sensor-placement methodology on real transformer/motor thermal datasets and assess sensitivity to calibration errors (sensor bias/drift) and spatial/temporal correlation in thermal-camera noise. Explore alternative Bayesian OED criteria (e.g., D-optimal/log-det, I-/G-optimal for prediction) and multi-objective designs that weight reconstruction quality in regions of interest (near windings) versus the whole domain. Extend to joint estimation of source and uncertain parameters (e.g., boundary transfer coefficient, anisotropic conductivity fields) with hierarchical Bayesian models and quantify the value of internal sensors under parameter uncertainty. Provide an open-source implementation and systematic guidance on selecting low-rank truncation $r$, prior hyperparameters, and optimization initialization/constraints for reproducible deployment.",2003.10395v2,https://arxiv.org/pdf/2003.10395v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:29:47Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical|Other,Other,TRUE,None / Not applicable,Not provided,https://api.pushshift.io|https://www.cdc.gov/tobacco/|https://taxfoundation.org/state-vapor-taxes-2019/,"This paper studies how social-media sentiment responds to tobacco-related policy changes using quasi-experimental designs (QEDs) rather than designed experiments (DOE). For a San Francisco flavored-tobacco ban, it uses an interrupted time series (segmented regression) to compare levels and trends across policy stages (proposal, approval, enforcement) for tobacco, e-cigarette, and flavored-tobacco tweet proportions and sentiment. For state e-cigarette taxes, it uses a natural-experiment before/after design comparing taxed states to adjacent non-tax states, using chi-square tests and regression with state-level covariates. Sentiment is labeled via VADER; topic/content corroboration uses Reddit data from the electronic cigarette subreddit and Empath topic analysis. Results emphasize increased negative sentiment and policy-focused discussion around implementation periods; limitations include low counts of geolocated tweets in some states and potential residual offline confounding.","The interrupted time series is described as segmented regression with separate level and trend parameters before and after each intervention point; the effect is assessed via changes in intercept (level) and slope (trend) across segments. For some comparisons, a two-sample t-test is used on the mean monthly proportion of tweets before vs. after policy events. In the natural experiment, the key estimand is the after-to-before ratio of e-cigarette tweet frequencies (by sentiment) in taxed states versus adjacent states, tested with a $\chi^2$ test; a regression then predicts post-tax tweet frequency using covariates including a tax indicator.","For the San Francisco ban, Table 2 reports statistically significant differences in slope/intercept for many outcomes across proposal/approval/enforcement, with a notable pattern of decreased positive and increased negative tweets around key events. For the tax natural experiment, Table 5 shows that adjacent-state after/before ratios often indicate relatively higher negative tweeting after taxes and generally decreased positive tweeting, though many $\chi^2$ tests are not significant and some states (Kansas, Delaware) have very low sample sizes. In regression (Table 6), the tax indicator is a significant predictor for negative tweets after the tax (p = 0.02), while “value before” is significant for predicting positive-tweet frequency after (p < 0.001); other covariates like overall tweet volume and user counts are also significant in places.","The authors note that geolocated e-cigarette tweet counts were low in some locations (e.g., Kansas and Delaware), limiting their ability to use desired denominators (e.g., proportion of all tweets that are e-cigarette-related by state). They also acknowledge that, despite using quasi-experimental designs, other offline confounders could still affect observed changes, though the designs aim to reduce such threats. They mention that access to larger Twitter samples (e.g., firehose) could remedy the low-count issue.","Because this is observational social-media analysis, the identifying assumptions behind the interrupted time series and natural experiment (e.g., no concurrent shocks at the same time, stable composition of users, no spillovers between states beyond adjacency) may be violated, especially given nationwide news cycles about vaping. Sentiment classification via VADER and keyword-based filtering may introduce systematic misclassification that varies over time (concept drift) and by region. The analysis relies on geolocated tweets, a highly selected subset of Twitter users, which may change in prevalence over time and bias estimates of policy effects. No reproducible pipeline or implementation details (code, exact model specifications, preprocessing) are provided, making replication difficult.","The authors suggest developing more nuanced topic modeling focused on policy to better unpack observed online discussion trends. They also suggest that obtaining larger Twitter datasets (e.g., firehose access) would address the low-volume limitation for geolocated tweets in some states. They discuss follow-up work such as recruiting/learning more about the drivers and backgrounds of individuals expressing sentiment online and aggregating multiple datasets/sources to obtain a fuller view of population sentiment.","A stronger causal design could incorporate explicit control series (e.g., synthetic control or multiple-baseline ITS across comparable cities/states) and sensitivity analyses for concurrent national events. Extending to multilevel/hierarchical models could better handle sparsity across states and quantify uncertainty under varying tweet volumes. Robustness checks using alternative sentiment models (supervised transformers) and validation against hand-labeled sentiment around policy events would strengthen conclusions. Providing an open, reproducible codebase and preregistered analysis plan would improve transparency and enable systematic benchmarking across policy domains.",2003.13783v1,https://arxiv.org/pdf/2003.13783v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:30:18Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Other,Other,"Variable/General (design parameter d; examples include 1D scalar d, choice of 2 summary statistics out of 13, and k sampling times with k=1–4)",Environmental monitoring|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/ziq-ao/LBKLD_estimator,"The paper proposes a Bayesian experimental design method for implicit (likelihood-intractable) stochastic models by maximizing a lower-bound approximation to the standard expected Kullback–Leibler divergence (KLD) information gain utility. The key contribution is a tractable lower bound (LB-KLD) that rewrites the intractable expected KLD in terms of a small number of output-space entropies, enabling likelihood-free evaluation via k-nearest-neighbor entropy estimation and a prior-partition (mixture) refinement using constrained k-means clustering. The resulting estimator requires only two (or a small number with partitioning) entropy estimations rather than one per prior sample, substantially reducing computational burden while remaining applicable to strongly non-Gaussian or multimodal posteriors. The method is demonstrated on a toy non-Gaussian example, the likelihood-free Ricker ecological model (design = selecting informative summary statistics), and an aphid population model (design = choosing sampling times), with comparisons to the ABC-based D-posterior-precision utility. Empirically, LB-KLD selects designs that better recover parameters when posteriors are non-Gaussian, while yielding similar designs to covariance-based utilities when posteriors are approximately Gaussian.","The Bayesian expected KLD utility is expressed as $U(d)=\mathbb{E}_{\theta,y\mid d}[\log p(y\mid\theta,d)-\log p(y\mid d)] =-\mathbb{E}_\theta[H(p(y\mid\theta,d))]+H(p(y\mid d))$. Using an entropy-power-inequality argument with $z=y-y'\ (y\perp y'\mid\theta)$ gives $\mathbb{E}_\theta[H(p(y\mid\theta,d))]\le H(\mathbb{E}_\theta[p(z\mid\theta,d)])-\tfrac{\mathrm{dim}(y)}{2}\log 2$, yielding a lower bound $U_L(d)=-H(\mathbb{E}_\theta[p(z\mid\theta,d)])+\tfrac{\mathrm{dim}(y)}{2}\log 2+H(p(y\mid d))$. A tighter partitioned form uses a mixture prior $p(\theta)=\sum_{l=1}^L\omega_l f_l(\theta)$ to obtain $U_L(d)= -\sum_{l=1}^L \omega_l\,H(\mathbb{E}_{\theta\sim f_l}[p(z\mid\theta,d)])+\tfrac{\mathrm{dim}(y)}{2}\log 2+H(p(y\mid d))$, with entropies estimated from samples via the Kraskov kNN estimator.","In the toy example with available likelihood, the partitioned LB-KLD approximation closely matches the exact expected KLD curve over $d\in[2,100]$, whereas the non-partitioned lower bound shows visible discrepancy; the KLD-optimal design is reported as $d=5$ while the D-posterior-precision (ABC covariance) utility selects $d=100$, leading to less informative posteriors. In the Ricker model (likelihood-free), LB-KLD selects summary statistics (1) average population and (2) zeros observed, while D-posterior precision selects (2) and (3) autocovariance lag0; over 1000 trials the LB-KLD design reduces MSE of posterior means for $\log r$ (0.01 vs 0.02) and $\phi$ (0.0046 vs 0.0073), with similar MSE for $\sigma$ (0.080 vs 0.079). In the aphid model sampling-time design, LB-KLD and D-posterior precision yield very similar optimal times; for k=4, LB-KLD gives (13.8, 19.1, 24.5, 30.6) vs D-posterior (15.8, 20.4, 25.2, 30.5), consistent with near-Gaussian posteriors.","The authors note that because LB-KLD is only a lower-bound approximation to the true KLD utility, it may in some circumstances yield different designs than exact KLD, making the approximation unsuitable in those cases. They also state that the prior-partition procedure is essential and currently empirical, and that the theoretically optimal partition strategy (including the number of partitions $L$) is unknown. They further acknowledge that comparisons are limited (primarily against D-posterior precision), and broader benchmarking against other likelihood-free design methods is left for future work.","The method relies on accurate kNN entropy estimation in the data/output space, which can degrade with higher-dimensional outputs/summary statistics, finite samples, or strong scaling/metric issues; performance may depend sensitively on choice of kNN hyperparameters, sample size n, and clustering/partition settings (L, nmin). The constrained k-means partitioning is heuristic and may be unstable across runs or miss structure when outputs are non-clusterable; the resulting lower bound can vary with clustering randomness and distance metric choices. Optimization over designs is treated with derivative-free/heuristic search (e.g., SPSA for k=3,4), so there is no guarantee of global optimality in complex design spaces. The approach assumes the ability to simulate from the model cheaply enough to support tens of thousands of prior-predictive simulations per design, which may be prohibitive for expensive simulators.","They plan a more comprehensive comparison against other likelihood-free experimental design approaches (e.g., Kleinegesse & Gutmann, 2019) considering both effectiveness and efficiency. They also aim to study when and why the LB-KLD approximation may fail to match KLD and to better characterize the method’s applicability and limitations. Finally, they propose investigating the optimal prior-partition strategy in theory, including principled selection of the number of partitions $L$.","Developing adaptive schemes to select $L$, n, and entropy-estimation hyperparameters (or to quantify estimator uncertainty) could improve robustness and reduce tuning burden. Extending the approach to handle dependent data, time series correlation, or model misspecification (robust/contaminated priors) would broaden applicability to real experiments. Providing gradient-based or surrogate-assisted optimization of $U_L(d)$, plus diagnostics for bound tightness, could make design search more reliable and efficient in high-dimensional design spaces. Packaging a reproducible software implementation with documented APIs (and possibly an R/Python package) and adding more real-data case studies would strengthen practical uptake.",2004.00715v2,https://arxiv.org/pdf/2004.00715v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:31:03Z
TRUE,Optimal design|Sequential/adaptive,Parameter estimation|Cost reduction,D-optimal|A-optimal|E-optimal|Other,"Variable/General (design space $\mathcal{X}\subset\mathbb{R}^d$, parameter dimension $p>1$)",Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies online “thinning” of experimental designs when candidate design points $X_i$ arrive sequentially as an i.i.d. stream from an unknown distribution $\mu$, and only a fixed proportion $\alpha\in(0,1)$ can be retained. The goal is to maximize a strictly concave design criterion $\Phi$ of the Fisher information matrix (covering D-optimality $\log\det$, and $\Phi_q$ criteria related to A/E-type criteria), under the bounded-design constraint $\xi_\alpha\le \mu/\alpha$. The authors propose a fully sequential subsampling algorithm that accepts a point if a directional-derivative score exceeds a threshold set by the $(1-\alpha)$-quantile of that score’s distribution, with the quantile estimated recursively (two-time-scale stochastic approximation). The method has constant memory (stores only current information matrix and scalar quantile/density estimates) and is suitable for very large $N$ and streaming. Almost sure convergence of the normalized information matrix of selected points to the optimal bounded-design solution is proved, and simulations across regression examples (including non-i.i.d. streams with buffer scrambling and settings with discrete components) illustrate fast convergence and good approximation to optimal designs; comparisons against exchange methods and IBOSS are also provided.","The information matrix is $M(\xi)=\int_{\mathcal X} M(x)\,\xi(dx)$ and the directional derivative is $F_\Phi(M,M_0)=\operatorname{tr}[\nabla\Phi(M)(M_0-M)]$. The optimal bounded-design update uses thresholding by a quantile: $M^+(M,\alpha)=\frac{1}{\alpha}\int I\{F_\Phi[M,M(x)]\ge C_{1-\alpha}(M)\} M(x)\,\mu(dx)$ with $\int I\{F_\Phi[M,M(x)]\ge C_{1-\alpha}(M)\}\,\mu(dx)=\alpha$. Algorithm 1 accepts $X_{k+1}$ if $Z_k(X_{k+1})=F_\Phi[M_{n_k},M(X_{k+1})]\ge \widehat C_k$ and updates $M_{n_{k+1}}=M_{n_k}+\frac{1}{n_k+1}(M(X_{k+1})-M_{n_k})$, while updating the quantile via $\widehat C_{k+1}=\widehat C_k+\frac{\beta_k}{(k+1)^q}(\mathbf{1}\{Z_k(X_{k+1})\ge \widehat C_k\}-\alpha)$.","A main theoretical result (Theorem 3.1) proves that under regularity assumptions on $\Phi$, $\mu$, and $M(x)$, the normalized information matrix from Algorithm 1 converges almost surely to the optimal bounded-design matrix $M^*_\alpha$ as $k\to\infty$. In simulations for quadratic regression with $X\sim\mathcal N(0,1)$ and D-optimality, the selected-point histogram matches the theoretically optimal truncated/three-interval selection and $\Phi(M_{n_k})$ rapidly approaches the optimal value for both $\alpha=1/2$ and $\alpha=1/10$. Additional examples show the need to adapt $\alpha$ when an exact final sample size $n$ is imposed, and demonstrate that buffer scrambling can partially recover performance on non-i.i.d. sequences. Comparative experiments indicate Algorithm 1 can outperform IBOSS outside its favorable settings and can be competitive (with higher compute cost) even when IBOSS assumptions hold.","The authors note that fixing the number selected $n$ (with $N\to\infty$ so $\alpha\to 0$) removes the convergence guarantee for Algorithm 1; performance then depends on $n$ being large enough, and exchange methods may be preferable in that regime. They state that cases where $\mu$ has atoms or where the directional-derivative distribution has discontinuities require additional technical developments; suggested remedies include regularization or modified step-size truncation. They also explicitly say they did not optimize initialization and tuning parameters and that model-robustness (misspecification of $M(x)$) is not treated.","The convergence proof assumes i.i.d. candidates (or effective randomization) and relies on smoothness/density conditions for the directional-derivative distribution; in many real streaming contexts (autocorrelation, concept drift, heavy tails), these conditions may fail and the quantile recursion may become unstable or biased. The method targets approximate-design optimality in terms of Fisher information; it does not directly address downstream estimation under finite-sample constraints, heteroskedasticity, or constraints such as factor level costs/feasibility regions beyond the bounded-design inequality. Practical implementation requires repeated evaluation of $F_\Phi$ and matrix updates/inversions (or low-rank updates), which may be computationally heavy in high-dimensional $p$ without careful numerical stabilization; the paper provides complexity discussion but no implementation guidance for numerical conditioning or distributed computation.","They suggest improving/optimizing initialization and tuning (notably $\beta_0$ and the initial quantile), using information like $C^*_{1-\alpha}\le 0$. They propose extending the method to nonlinear models with online parameter estimation by updating $\hat\theta$ and using $M(x,\hat\theta)$ in the selection and recursion. They also suggest connections and extensions to adaptive treatment allocation where the allocation proportion is adjusted online to a target, and explicitly mention that model-robustness under misspecification would require separate developments.","Develop self-starting and robust variants that handle unknown/estimated noise variance, non-normal errors, and autocorrelated or drifting streams (e.g., windowed or change-point-aware quantile updates). Extend to constrained and structured DOE settings (mixtures, split-plot/hard-to-change factors, blocked designs) where acceptance decisions must respect combinatorial constraints across time. Provide open-source implementations with numerically stable low-rank updates and benchmark suites comparing against modern core-set/subsampling methods (e.g., leverage-score sampling, kernel herding, Bayesian experimental design) under large-scale and high-$p$ regimes.",2004.00792v2,https://arxiv.org/pdf/2004.00792v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:31:51Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|E-optimal|Other,Variable/General (design space X ⊂ R^d; examples include d=3 and d=2; parameter dimension n=4 and n=25 in examples),Environmental monitoring|Other|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Not provided,https://www.tu-chemnitz.de/mathematik/part_dgl/people/herzog|https://www.tu-chemnitz.de/mathematik/part_dgl/people/legler,"The paper studies convex optimal experimental design (OED) problems over probability measures on a compact design space, focusing on additional bound (density) constraints that force the design measure to have an essentially bounded Lebesgue density with pointwise bounds 0 ≤ w ≤ 1 and a total-mass (budget) constraint. It formulates the continuous and discretized (piecewise-constant) OED problems in terms of minimizing standard information-based criteria applied to the Fisher information matrix (e.g., A-, log D-, and E-type criteria via the Kiefer φ_q family), optionally with an L2 regularization term to ensure uniqueness. Two first-order proximal methods (FISTA and an extrapolated proximal gradient method, PGMA) are analyzed and compared, with efficient projection onto the “restricted simplex” (mass constraint plus box constraints) being a key computational component; an explicit projection algorithm is provided and proved correct for the discrete case. Numerical experiments on a nonlinear Lotka–Volterra design problem and a PDE-based diffusion/sensor-placement problem show PGMA substantially outperforms FISTA, and further acceleration via simplicial decomposition and active-set based strategies (SDM/SDMH/PDSAS) yields large runtime improvements on high-dimensional discretizations. The work advances OED computation under practical density constraints by providing scalable first-order algorithms, stopping criteria tied to (relaxed) KKT conditions, and projection/active-set accelerations suitable for large design spaces.","The total Fisher information matrix is constructed as $\Lambda w=\int_X \Upsilon(x)w(x)\,dx$ (continuous) or $\Lambda w=\sum_{i=1}^m \Upsilon_i |E_i| w_i$ (discrete). The main criteria are the Kiefer $\phi_q$ family, e.g. $F_q(I)=\big(\tfrac1n\mathrm{tr}(I^{-q})\big)^{1/q}$ for $q>0$ (A-criterion at $q=1$) and $F_0(I)=\tfrac1n\ln\det(I^{-1})$ (log D-criterion), with $F_\infty(I)=\max\mathrm{eig}(I^{-1})$ (E-type) as $q\to\infty$. Bound-constrained optimality is expressed via a projection/fixed-point form: $w=\mathrm{proj}_{\Delta_C\cap B}(w-\gamma(\Lambda^*\nabla F_q(\Lambda w)+\alpha w))$, where $\Delta_C$ enforces $\int_X w=C$ and $B$ enforces $0\le w\le 1$; the projection can be written as $w=\mathrm{proj}_B(f-\zeta)$ with a shift $\zeta$ chosen to meet the mass constraint.","In the Lotka–Volterra experiment with a 27,000-point discretization and log D-criterion (q=0), PGMA reached the optimality-based stopping tolerance in about 5.6 CPU seconds and within ~300 iterations, while FISTA was stopped after 5000 iterations without approaching the desired tolerance. Step sizes in PGMA adaptively increased to values about five orders of magnitude larger than those used by FISTA, explaining much faster convergence. For a finer discretization (m=125,000) under the same criterion, active-set accelerations (SDM/SDMH) reduced runtime substantially (e.g., SDMH converged in <~1.5 s vs unaccelerated PGMA ~62 s in the reported setup). On a large PDE diffusion/sensor-placement case with m≈430,000, SDM and SDMH converged in about 2400 s and 1300 s respectively (excluding preprocessing of Fisher information matrices).","The authors note that the gradient of $F_q(\Lambda w)$ is only defined on the set where $\Lambda w\succ 0$ and is only locally (not globally) Lipschitz; this undermines standard global-Lipschitz convergence theory for FISTA, although they did not observe numerical issues because information-matrix eigenvalues stayed bounded away from zero. They also state that their simplicial decomposition inner solver (Torsney’s multiplicative algorithm) is only applicable for the unregularized case $\alpha=0$ and for $q\in[0,1]$, and that rounding/purging in simplicial decomposition yields only inexact solutions. They further remark that SDMH vs SDM performance is example-dependent and they make no claim SDMH always outperforms SDM. They acknowledge that precomputing elementary Fisher information matrices can dominate runtime for large problems.","The numerical comparisons rely on MATLAB implementations and specific hyperparameter choices (e.g., step-size/backtracking parameters, tolerances, iteration caps), and results may vary with different tuning or implementations; a more systematic sensitivity analysis is not provided. The design criteria treated are information-matrix based and assume a correct model and nominal parameter (local design); robustness to model misspecification or Bayesian/average designs is not explored. The bound constraint $0\le w\le 1$ is motivated by sensor-density limits, but the mapping from continuous densities to implementable discrete experiments/schedules may require additional rounding/exact-design steps beyond what is demonstrated. Real-world empirical validation beyond simulated/constructed models (Lotka–Volterra, PDE) is limited, and the effect of measurement noise structure, correlated observations, or constraints beyond simple box bounds is not studied.","They explicitly point out the opportunity to develop adaptive discretization strategies of the design space to reduce computational cost, since precomputation of the elementary Fisher information matrices can be a significant part of overall runtime for large-scale problems.","Extending the first-order/active-set framework to other constraint types common in OED (e.g., group/region constraints, minimum spacing, costs varying across X, or mixed-integer exact-design constraints) would broaden applicability. Developing self-starting or parameter-uncertainty-robust variants (Bayesian or minimax designs) and studying robustness when $\Lambda w$ becomes ill-conditioned would strengthen theoretical guarantees and practical reliability. Providing open-source implementations (e.g., MATLAB/Python) with reproducible experiment scripts and benchmarking against state-of-the-art SDP/MISOCP solvers would improve adoption and comparability. Incorporating autocorrelated/heteroscedastic noise models and multivariate observation operators in the Fisher information construction would make the approach closer to many real sensor-placement settings.",2004.08084v1,https://arxiv.org/pdf/2004.08084v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:32:44Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Model discrimination|Cost reduction,Bayesian D-optimal|Other,Variable/General (design variables include measurement time points and number of mice/replicates per time point; model has 5 ODE parameters),Healthcare/medical|Other,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper proposes a Bayesian optimal design framework for perturbation experiments aimed at learning parameters of mechanistic ODE models of hematopoiesis from cross-sectional (non-longitudinal) mouse data. A hierarchical Bayesian latent-variable model is introduced to handle the fact that each mouse is sacrificed at a single time point, treating individual initial cell counts as latent and using the ODE to propagate them to observation times. Optimal designs are chosen by maximizing expected utility defined as Kullback–Leibler divergence (information gain) between prior and posterior, with Monte Carlo estimation of expected utility across candidate designs that vary sampling times and numbers of mice per time point. The approach is evaluated on synthetic data (showing parameter recoverability under richer designs) and on real low-dose irradiation mouse data, illustrating that spreading a fixed mouse budget across more time points can improve parameter inference, and that early vs late sampling differentially informs division-rate vs feedback-gain parameters. Posterior inference is performed via MCMC, and marginal likelihoods needed for KL utility are approximated via bridge sampling.","The mechanistic mean process is a 2-compartment ODE for HSC and MPP counts (e.g., $x'_{HSC}=(2p_0^*-1)\eta_1 x_{HSC}$ and $x'_{MPP}=2(1-p_0^*)\eta_1 x_{HSC}-\eta_2^* x_{MPP}$), with feedback $p_0^*=\frac{p_0}{1+\gamma_1 x_{MPP}}$ and feedforward $\eta_2^*=\frac{\eta_2}{1+\gamma_2 x_{HSC}}$. The Bayesian design utility is KL information gain $U(y,d)=\int \log\frac{p(\Theta\mid y,d)}{p(\Theta)}\,p(\Theta\mid y,d)\,d\Theta$, and expected utility is $u(d)=\mathbb{E}_{\Theta,y}[U(y,d)]$ estimated by Monte Carlo over simulated datasets and posterior samples; marginal likelihood $p(y\mid d)$ is obtained via bridge sampling.","In a synthetic “7 days × 7 mice/day” design (49 mice), posterior inference recovers true parameter values within 95% credible intervals and shows high coverage (e.g., Table 1 reports coverage around 0.89–1.00 across parameters over 60 simulations). Grid search over 70 candidate designs (days between 0 and 6; 3–7 mice per day) shows expected utility increases with both more observation days and more replicates, and—given a fixed total number of mice—allocating fewer mice across more time points tends to yield higher information gain. Parameter-specific utilities indicate early-time observations contribute most to identifying division rates (e.g., $\eta_1$), while later-time observations improve identification of feedback gains (e.g., $\gamma_1$). For the preliminary real dataset (13 mice at days 0, 2, 6), the Bayes factor comparing feedback vs no-feedback is about 1.18, indicating weak evidence for feedback with that limited design.","The authors note the approach can be computationally expensive because it relies on repeated Monte Carlo simulation, MCMC posterior sampling, and bridge sampling for marginal likelihood estimation. They also acknowledge that more efficient sampling, ODE solvers, and more efficient exploration of the design space could substantially reduce computation time. They further note that extending to stochastic hematopoiesis models would be valuable but is challenging because likelihoods are often unavailable, requiring approximate Bayesian methods.","The design search is over a finite, hand-crafted grid of candidate schedules (e.g., days up to 6 and 3–7 mice/day), so optimality is only guaranteed within that restricted set; continuous-time optimization or broader constraints (e.g., animal availability, batching, lab capacity) are not fully modeled. The utility criterion is information gain on model parameters under the assumed ODE + noise model; if the mechanistic model is misspecified (unmodeled cell types, unmodeled downstream effects, non-lognormal variability), the chosen design may be suboptimal or misleading. Practical guidance on robustness to deviations from assumptions (e.g., overdispersion beyond lognormal/normal measurement error, time-varying perturbation effects) is limited.","They propose improving computational efficiency via faster sampling methods, more efficient numerical solvers for differential equations, and more efficient exploration/optimization of the design space. They suggest extending the framework to stochastic formulations (generalizing branching-process models) to directly model cellular stochasticity and heterogeneity, though recognizing likelihood challenges. They also propose extending the framework to include more differentiated cell types, additional perturbations (e.g., depletion of specific downstream cells), repeated measurements of covariates/cell counts allowing parameters to vary across individuals, and richer data sources such as serially sampled barcoded single cells for lineage tracking.","A natural extension is to develop a fully continuous (or adaptive/sequential) design strategy that updates sampling times as data accrue, rather than evaluating a fixed grid, especially under strict animal-budget constraints. Robust-design variants could explicitly account for model uncertainty (multiple plausible mechanistic models/feedback structures) by optimizing a model-averaged or worst-case utility. Packaging the workflow as open-source code (e.g., Stan models + utility/bridge-sampling pipeline) with reproducible design benchmarks would improve adoption and enable standardized comparisons across Bayesian ODE-design methods.",2004.09065v2,https://arxiv.org/pdf/2004.09065v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:33:27Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Optimization,D-optimal,9 parameters (np=9); control vector has 10 piecewise-constant current segments,Energy/utilities|Other,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper proposes a model-based design of experiments (MBDoE) method for lithium-ion battery parameter identification that replaces local (linearized) sensitivities in Fisher-information-based design with global parameter sensitivities (first-order Sobol’ indices). The approach targets nonlinear electrochemical models, demonstrated on the single-particle model with electrolyte and thermal dynamics (SPMeT), and designs an optimal applied-current profile to increase information content in measured voltage and temperature. Global sensitivities are computed efficiently using the Point Estimate Method (PEM), enabling tractable solution of the underlying dynamic optimization problem. The design criterion is the D-criterion, implemented as the determinant of a sensitivity-based information measure, and the resulting GSA-MBDoE input is compared against standard local-sensitivity MBDoE. Monte Carlo validation with noisy synthetic data shows the GSA-MBDoE produces more precise parameter estimates across all studied parameters, with especially large gains for some kinetic parameters.","The MBDoE is posed as a dynamic optimization over the input current profile $u(t)=I_{app}(t)$: $\max_{u(\cdot)}\, \Phi(S_{l/g}(p))$ subject to the SPMeT dynamics and constraints (Eq. 21). Local sensitivities are $S_l[j,i](t)=\left.\partial y[j](t)/\partial p[i]\right|_{\hat p}$ (Eq. 24), while global sensitivities use first-order Sobol’ indices $S_g[j,i](t)=\mathrm{Var}_{p[i]}(\mathbb{E}_{-i}[y[j](t)\mid p[i]])/\mathrm{Var}(y[j](t))$ (Eq. 26). The design objective uses the D-criterion $\Phi(S_{l/g}(p))=\det(S_{l/g}(p)^\top S_{l/g}(p))$ (Eq. 33), where for local sensitivities $S_l^\top S_l$ corresponds to a Fisher-information-like matrix.","Case study settings: experiment duration $t_{exp}=1000\,s$; piecewise-constant current with 10 segments of 100 s each, bounded in $[-15C,15C]$; measurements $y(t_k)=[V(t_k),T(t_k)]$ sampled every $t_s=5\,s$; soft constraints enforce $T\le 320\,K$ and $2.7\,V\le V\le 4.2\,V$. Nine uncertain parameters are optimized ($n_p=9$) with 10% prior standard deviation; PEM requires $n_{PEM}=2n_p^2+1=163$ simulations for global sensitivities. Monte Carlo validation uses 100 noisy datasets with additive white noise variances $\sigma_y^2(V)=10^{-2}$ and $\sigma_y^2(T)=0.3$. Reported efficiency ratios $\eta=\sigma_l^2[\hat p]/\sigma_g^2[\hat p]$ exceed 1 for all parameters, with particularly large improvement for $k_n^0$ (17.6513), and modest improvement for $\tau_s$ (1.0050).","The authors note that for non-globally identifiable parameter identification problems (e.g., parameters exhibiting multiple local minima), optimal experimental design must be augmented with rigorous parameter identifiability measures. They also state that faster global sensitivity analysis methods are needed when increasing the number of parameters studied or solving more complex optimization problems (e.g., higher-dimensional control input vectors or additional design degrees of freedom).","The study validates designs primarily via simulation (synthetic noisy data) rather than demonstrating improvement on real experimental datasets, so practical issues like model mismatch, unmodeled dynamics, and measurement artifacts are not fully assessed. Only first-order Sobol’ indices are used; if strong interaction effects exist, ignoring higher-order indices could yield designs that miss important coupled parameter influences. The D-criterion is applied to sensitivity matrices (including for the global-sensitivity case), but the statistical interpretation of $\det(S_g^\top S_g)$ as an information measure is less direct than for the classic FIM, and the impact of measurement noise covariance structure is not explored. Implementation details (discretization, optimizer settings, computational cost) and reproducible code are not provided, limiting reproducibility and independent benchmarking.","The authors suggest advancing experimental design with rigorous identifiability measures in cases where parameter estimation has multiple local minima (non-global identifiability). They also propose developing faster global sensitivity analysis approaches to handle larger parameter sets and more complex MBDoE problems, such as higher-dimensional input parametrizations or additional experimental design degrees of freedom.","Extending the framework to include total-effect (and possibly higher-order) Sobol’ indices could better account for parameter interactions in nonlinear electrochemical models. A robust/self-starting design that explicitly accounts for unknown parameters during Phase II (e.g., Bayesian MBDoE or min–max robust design) would improve practical applicability when priors are uncertain or biased. Applying the method to real laboratory experiments (with model mismatch and correlated noise) and reporting calibration improvements would strengthen external validity. Packaging the approach into an open-source implementation (e.g., MATLAB/Python toolbox integrated with battery models) would facilitate adoption and comparative benchmarking.",2004.09668v2,https://arxiv.org/pdf/2004.09668v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:34:04Z
TRUE,Optimal design|Bayesian design,Parameter estimation|Other,Other,"Variable/General (design variables are choice of observable, photon energy ω, scattering angle θ, and allocation across multiple kinematic points; parameters of interest are 6 polarizability combinations)",Other,Other,TRUE,Python,Public repository (GitHub/GitLab),https://buqeye.github.io/software/,"The paper develops a Bayesian experimental-design framework to choose optimal future Compton-scattering measurements (observable and kinematics) that most reduce uncertainty in nucleon electromagnetic polarizabilities inferred with chiral EFT (χEFT). The design utility is the expected information gain (KL divergence) from prior to posterior, which becomes an analytic expression (log-determinant shrinkage) under a linearized model and Gaussian assumptions. A key methodological component is a physics-informed Bayesian model for χEFT truncation errors using Gaussian processes to encode smooth, correlated theory uncertainty across energy and angle, including a power-counting rearrangement in the Δ(1232) region. The framework ranks candidate designs under realistic experimental precision scenarios and shows that incorporating theory uncertainty shifts optimal kinematics to lower energies than sensitivity-only approaches. The strongest expected gains are predicted for measurements of spin observables Σ2x and Σ2x0 around ω≈140–200 MeV and θ≈40°–120°, and for differential cross-section data between 100–200 MeV over wide angles to improve constraints on specific polarizability combinations.","The design criterion is the expected KL divergence between prior and posterior for parameters $\mathbf a$ (polarizabilities), which under Gaussian prior/posterior reduces to $U_{\mathrm{KL}}(d)=\tfrac12\ln\{|V_0|/|V(d)|\}=\ln S(d)$, where $V_0$ and $V(d)$ are prior and posterior covariance matrices and $S$ is the posterior shrinkage factor. The likelihood uses a linearized χEFT model $\mathbf y\mid \mathbf a \sim \mathcal N(B\mathbf a +\mathbf c,\,\Sigma)$ with $\Sigma=\bar c^2 R_{\delta k}+\Sigma_{\mathrm{exp}}$, combining correlated EFT truncation covariance (from a GP) with experimental error. The EFT truncation error is modeled via an expansion $y_k(x)=y_{\mathrm{ref}}(x)\sum_{n=0}^k c_n(x)Q^{\nu_n(\omega)}(x)$ and a GP prior on coefficient functions $c_n(x)$, yielding a GP for $\delta y_k(x)$ with covariance $R_{\delta k}(x,x')\propto y_{\mathrm{ref}}(x)y_{\mathrm{ref}}(x')\,Q^{\nu_{\delta k}}(x)Q^{\nu_{\delta k}}(x')\,r(x,x')/(1-Q(x)Q(x'))$.","Including χEFT truncation uncertainty substantially lowers the energies of optimal designs compared to truncation-free/sensitivity-only recommendations, preventing overly high-ω designs where theory is unreliable. The largest predicted uncertainty reductions for a 5-point design occur for Σ2x and Σ2x0, giving roughly ~50% shrinkage for the spin-polarizability combination $\gamma_{E-}$ in kinematics around ω≈140–200 MeV and θ≈40°–120°. Optimal 5-point differential cross-section measurements can yield sizable combined gains (reported up to ~40% for all polarizabilities collectively in their metric), and notably improve constraints on $\alpha_{E1}-\beta_{M1}$ (order ~10–25% depending on assumptions/priors) and on combinations including $\gamma_\pi$ and $\gamma_{M-}$ in the 100–200 MeV range over broad angles. Results are presented under three experimental precision scenarios (Standard/Doable/Aspirational), with the relative advantage of adding more angles versus increasing precision depending strongly on the observable.","The authors note the conclusions are conditional on using χEFT for parameter extraction and on specific prior assumptions; they do not perform a fully consistent Bayesian re-extraction of polarizabilities from all existing Compton data using their same theory-error model, so their design conclusions are described as indicative rather than definitive. They acknowledge their truncation-error estimates for spin observables near the pion-production threshold are less reliable (cusps/irregular convergence), and they exclude a threshold region when training GP hyperparameters. They also state they treat only point-to-point (“statistical”) errors and do not model common-mode/correlated systematic errors across kinematics, and they approximate facility constraints crudely (e.g., excluding hard detector angles rather than using facility-specific costs/feasibility).","The design search is restricted (e.g., often to multiple angles at a single energy, and to 1-point vs 5-point designs), so it may miss globally optimal designs spanning multiple energies or accounting for time-allocation as a continuous decision variable. The linearization of observables in polarizabilities, while empirically good in their checks, can still understate nonlinear posterior effects in regions of stronger parameter dependence or with larger prior widths. The utility focuses purely on parameter-information gain and does not include explicit experimental cost/beam-time optimization or multi-objective trade-offs (e.g., balancing theory validation vs parameter estimation). The GP truncation model assumes a specific kernel form and independence/i.i.d. structure for coefficient functions across orders, which may not capture all known EFT structural correlations or nonstationarities beyond the introduced cusp handling.","They propose performing a fully consistent Bayesian extraction of proton (and neutron) polarizabilities from the existing Compton database using the same error model used for design, to establish a better prior for future design calculations. They suggest extending the framework to facility-specific designs (e.g., MAMI or HIγS) with more realistic constraints and inclusion of correlated/common-mode experimental systematics. They also identify sequential/adaptive experimental design (campaigns updated based on early results) and full (nonlinear) Bayesian experimental design beyond linearization as future directions. Finally, they call for further study of χEFT convergence and truncation behavior for Compton observables, especially near the pion-production threshold.","Implement an explicit economic/operational design layer that optimizes expected information gain per unit beam time (or cost), including discrete detector-placement constraints and energy-bin widths, to make recommendations directly actionable for specific facilities. Develop robust design variants that hedge against model misspecification in the truncation-error GP (e.g., alternative kernels, nonstationary GPs, heavy-tailed residual models) and quantify sensitivity of design rankings to these choices. Extend the approach to multi-target settings (deuteron, 3He) with consistent few-body χEFT calculations and jointly optimized measurement plans across targets to disentangle proton/neutron polarizabilities. Provide a maintained, installable software package (beyond a notebook) with reproducible pipelines and benchmark datasets to standardize comparisons and encourage broader adoption.",2004.11307v3,https://arxiv.org/pdf/2004.11307v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:35:06Z
TRUE,Sequential/adaptive|Bayesian design|Other,Prediction|Parameter estimation|Optimization|Cost reduction|Other,Bayesian D-optimal|Other,Variable/General (n individuals/items; simulated examples include n=70 and n=96; groups/batches with k tests per stage),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/google-research/google-research/tree/master/grouptesting,"The paper proposes noisy adaptive group-testing algorithms formulated as a Bayesian sequential experimental design problem, where each stage selects a batch of pooled tests (groups) based on past outcomes. Groups are chosen myopically to maximize expected utility under the current posterior over infection vectors, using utilities such as mutual information (information gain) and the expected AUC of the ROC curve derived from marginal posteriors. Because the posterior over {0,1}^n is intractable for moderate n, the authors approximate it with sequential Monte Carlo (SMC) samplers and optimize the utility via greedy forward/backward combinatorial search. Simulation studies (e.g., n=70 with k=8 tests per cycle over T=5 cycles, with noisy sensitivity/specificity) show the proposed G-MIMAX and G-AUCMAX policies outperform adaptive (Dorfman, binary splitting) and non-adaptive (random, Origami OM3) baselines, especially at low prevalence. The paper also finds loopy belief propagation (LBP) decoding can be unstable/oscillatory under noise, and reports SMC-based decoding as more reliable, including a hybrid LBP-then-SMC fallback decoder.","Individuals have binary infection vector X\in\{0,1\}^n and a group g\subset\{1,\ldots,n\} has noiseless status [g,x]=\max(1,g^\top x)\in\{0,1\}. With sensitivity s_g and specificity \sigma_g (depending on group size), the noisy group test outcome Y_g satisfies P(Y_g=1\mid[g,X]=1)=s_g and P(Y_g=0\mid[g,X]=0)=\sigma_g, yielding P(Y_g=y\mid X=x)=(\sigma_g-\rho_g[g,x])^{1-y}(1-\sigma_g+\rho_g[g,x])^y with \rho_g=s_g+\sigma_g-1. The Bayesian design selects the next batch G to maximize expected utility U_\Phi(G,\pi_t)=\mathbb{E}[\Phi(\pi_t^G)] (including MI as a special case) where \pi_t is the posterior given past tests; MI admits an efficient form depending on f_{\pi_t}(g)=\sum_x \pi_t(x)[g,x].","In simulations with noisy tests (e.g., n=70, k=8 tests/cycle, T=5 cycles, group size cap n_max=10; typical parameters \sigma\approx0.97 and s\approx0.85; prevalence q=2% or 5%), the BOED policies G-MIMAX and G-AUCMAX achieve better sensitivity–specificity tradeoffs than Dorfman, Binary-Dorfman, random pooling, and Origami-based mixed policies for the same total number of tests (40). The paper reports that using SMC-based marginal decoding improves performance over LBP decoding; LBP can fail to converge and exhibit oscillations in a non-negligible fraction of runs (illustrated across cycles for the Random policy). The authors also report robustness experiments under misspecification of prevalence and sensitivity assumptions, where G-MIMAX/G-AUCMAX remain superior to baselines. Additional experiments scale to n=96 with size-dependent sensitivity (s_g decreasing with group size) and still show substantial gains for G-MIMAX over baselines.","The authors note their current approach scales exponentially with the number of tests per stage k because utility evaluation enumerates 2^k possible outcomes; this may require alternative strategies for larger k. They also point out performance depends on the quality of posterior sampling (SMC) and on the combinatorial solver used to optimize groups, suggesting both as avenues for improvement. They empirically show LBP decoding can be unreliable/oscillatory in noisy settings, motivating the need for SMC or hybrid decoders.","Most evaluation is simulation-based with assumed noise models and (often) correctly specified or controlled misspecification; real-world validation on clinical/lab pooled-testing datasets is limited/absent in the provided text. The method assumes conditional independence of tests given X and known (or assumed) sensitivity/specificity functions by pool size; in practice these may vary by lab process, dilution effects, and correlated errors. The greedy batch optimization may be sensitive to initialization/hyperparameters and may not approximate the global optimum, especially for larger n and more complex priors. Computational cost (SMC with many particles plus greedy search) may be challenging for time-critical, large-scale deployment compared with simpler heuristic group-testing designs.","The paper suggests improving the quality of posterior sampling, exploring alternative utility functions \Phi beyond MI and AUC, and improving the combinatorial solver used to construct groups from posterior samples. The authors also mention that because the method scales exponentially in k, extensions that perform resampling during group-optimization iterations (or otherwise address scaling to larger k) may be needed for larger batch sizes.","A practical next step is validating the BOED policies on real pooled-testing studies (e.g., RT-PCR pooling) with empirically estimated, dilution-dependent sensitivity/specificity and operational constraints (lab workflow, turn-around time). Methodologically, developing approximations that avoid explicit 2^k outcome enumeration (e.g., variational bounds, stochastic utility estimation, submodular relaxations) could make larger k feasible. Extensions to handle correlated infections via structured priors (households/contact networks) and temporal dynamics (changing prevalence) would improve realism. Providing a self-starting/online calibration of noise parameters and adaptive choice of particle count could increase robustness and reduce compute in deployment.",2004.12508v6,https://arxiv.org/pdf/2004.12508v6.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:35:52Z
TRUE,Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Robustness|Other,E-optimal|Minimax/Maximin|Other,"Variable/General (two-arm experiment; covariates of dimension d, typically d≥2; n units)",Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper studies optimal randomization schemes for two-arm controlled experiments under a minimax criterion for the variance of the difference-in-means estimator, where the unknown conditional mean outcomes vector lies in an assumed set (often permutation-symmetric or induced by a kernel/RKHS structure). It proves that when the admissible set is permutation symmetric (i.e., essentially “no structure”), complete randomization is minimax-optimal (“no free lunch”), and that designs that randomize only by flipping treatment labels on a single partition can be worse by a factor of n−1 in worst-case variance. For structured mean-outcome sets (e.g., MK defined via a positive semidefinite matrix/kernel K), the minimax-optimal design is the mixed-strategy optimal design (MSOD) from Kallus (2018), which generally requires randomization over multiple partitions unless K is rank-one. The paper then introduces an inference-constrained MSOD: a minimax-variance optimal design subject to uniformity constraints (upper bounds on assignment probabilities) intended to ensure Fisher randomization tests can have power at a chosen significance level α. It also argues rerandomization (Morgan–Rubin) is generally not minimax-optimal under this framework (except in essentially one-dimensional/rank-one cases) and can be computationally inefficient when acceptance probabilities are small.","The design-dependent component of the variance is written as B(σ,μ)=\sum_{W\in\mathcal W}\sigma(W)\langle W,\mu\rangle^2=\sup_{\mu\in M}\mu^\top Q(\sigma)\mu, where Q(\sigma)=\sum_W\sigma(W)WW^\top. For ellipsoidal mean sets M_K, the minimax objective becomes B(\sigma,M_K)=C\,\lambda_{\max}(K^{1/2}Q(\sigma)K^{1/2}). The inference-constrained MSOD solves \min_{\sigma\in S: \sigma(W)\le \alpha/2\,\forall W}\lambda_{\max}\Big(\sum_W\sigma(W)K^{1/2}WW^\top K^{1/2}\Big), with an SDP-based approximation using a finite set of candidate partitions and weights.","For permutation-symmetric admissible sets M, complete randomization is minimax-optimal. For the specific permutation-symmetric benchmark set M_CR (defined relative to complete randomization), any single-partition design (randomizing only between W0 and −W0) satisfies B(σ_single,M_CR)=C(n−1) while B(σ_CR,M_CR)=C, yielding a worst-case variance inflation factor of n−1. More generally for M_K, a single partition is minimax-optimal only in rank-one cases (K=vv^\top); otherwise MSOD requires randomization over multiple partitions. The paper proposes a constraint (\sigma(W)\le \alpha/2) to ensure randomization tests can in principle achieve p-values ≤α for two-sided statistics, and formulates/approximates the resulting constrained minimax design via semidefinite programming and iterative selection of top partitions.",None stated.,"The work is largely theoretical and does not provide broad simulation benchmarks or real-data case studies quantifying power/variance tradeoffs of inference-constrained MSOD versus alternatives across realistic finite-sample regimes. Practical implementation can be challenging because the exact MSOD/inference-constrained MSOD involves difficult optimizations over exponentially many assignments; the proposed heuristic relies on solving repeated integer quadratic programs and an SDP, which may be computationally heavy for large n. The uniformity constraint \sigma(W)\le \alpha/2 is motivated by enabling Fisher randomization tests for two-sided statistics, but it is not shown to be necessary/sufficient for good power across statistics or for other inferential goals (e.g., confidence intervals, weak nulls).",None stated.,"Develop scalable algorithms with provable approximation guarantees for (inference-constrained) MSOD at large n, potentially leveraging modern rounding/first-order SDP methods or exploiting structure in K (e.g., low-rank, sparsity, clustering). Provide systematic simulation and empirical evaluations comparing constrained MSOD, rerandomization, blocking/matching, and other restricted randomization designs in terms of both variance and randomization-test power under weak/strong nulls. Extend the framework to multi-arm, factorial, or cluster-randomized experiments and to settings with interference, covariate-adaptive randomization, or outcome/covariate model misspecification.",2005.03151v1,https://arxiv.org/pdf/2005.03151v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:36:16Z
TRUE,Factorial (full)|Other,Parameter estimation|Model discrimination|Other,Not applicable,"Variable/General (application includes 4 factors/variables; example uses a $2^4$ factorial + center point, 17 runs)",Manufacturing (general),Other,TRUE,Other,Not provided,https://opus4.kobv.de/opus4-unipassau/frontdoor/index/index/docId/207,"The paper develops the “numerical statistical fan,” a numerical-robust analogue of the algebraic-statistics statistical fan, to characterize all maximal identifiable hierarchical (polynomial) regression models when design points/covariates are noisy and only known within componentwise tolerances. A model (order ideal) is deemed numerically stable if its evaluation matrix has full column rank for every allowable perturbation of the design points, thereby excluding “numerical aliasing” (approximate linear dependence among design vectors). The authors propose recursive enumeration algorithms—adapted from border-basis enumeration—to compute all maximal stable order ideals; numerical independence is checked via the (heuristic) Fassino condition derived from tolerance-aware perturbation analysis. They apply the approach to a High Velocity Oxygen Fuel (HVOF) thermal spraying dataset where an underlying $2^4$ factorial design with a center point produces a measured (noisy) 4D covariate design; the resulting numerical statistical fan is much smaller and computationally feasible compared to the (generic) statistical fan, enabling more reliable model-selection over stable identifiable hierarchical models. Computations are reported as performed in MAGMA, and empirical results show fan size increases as tolerances shrink, converging toward the classical statistical fan as noise goes to zero.","A hierarchical model (order ideal) $\mathcal O$ is identifiable if its design/evaluation matrix $X_{\mathcal O}(D)$ is full rank; the statistical fan is the set of maximal identifiable order ideals with $|\mathcal O|=|D|$. For noisy designs $(D,\delta)$, numerical dependence is defined by existence of a permissible perturbation $\tilde D$ making the least-squares residual of $X_{\mathcal O}(\tilde D)\tilde a\approx t(\tilde D)$ equal to zero, and the heuristic Fassino condition declares numerical independence when the residual at some point exceeds a tolerance-weighted bound involving derivatives: inequality (2) built from $\rho(D)$, $(I-X_{\mathcal O}(D)X_{\mathcal O}^{\mathrm{inv}}(D))$, and $\sum_k \delta_k(\partial_k t(D)-X_{\partial_k\mathcal O}(D)a)$. The numerical statistical fan $S_{\mathrm{num}}(D,\delta)$ is the set of maximal stable order ideals, where stability requires $X_{\mathcal O}(\tilde D)$ full column rank for all $\delta$-perturbations $\tilde D$ of $D$.","In the HVOF thermal spraying application, the controllable-process design is a $2^4$ full factorial with one center point (17 runs), while the measured in-flight properties yield a noisy 4D design $D_Y$ with tolerance vector $\delta=(12.5,7,0.3,1.5)$. Using the proposed enumeration with the heuristic Fassino condition, the numerical statistical fan has 45 maximal stable order ideals (and 481 stable order ideals total) at scale $k=1$, growing to 1488 maximal stable order ideals at $k=0.5$ (with 16233 stable order ideals). Reported condition numbers for standardized designs are moderate (largest about 62.25 for a model with maximal elements $\{T^2V, V^5\}$; smallest about 5.65 for $\{TW, V^2, W^2\}$). For the full $|D_Y|=17$ case, the classical statistical fan size is reported as 416570 (vs 45 numerically stable maxima), demonstrating that the numerical fan is dramatically smaller and effectively computable.","The paper notes that using the heuristic Fassino condition to test numerical dependence is only sufficient for numerical independence and can yield false positives (declaring dependence when no exact dependence exists on any allowable perturbation). Because of this, the notion of stability becomes heuristic and the equivalence between weak maximality and maximality may fail under the heuristic test; the authors therefore propose a post-processing step to remove non-maximal order ideals by inclusion checks.","The main computational claims are based on a single real-data case study (thermal spraying) plus small illustrative examples; scalability to larger designs, higher dimension (more factors), or larger run sizes is not empirically benchmarked (time/memory complexity in practice is not reported). The practical impact on downstream model selection/prediction is asserted but not quantitatively validated (e.g., selected models’ predictive performance, robustness to noise mis-specification, or comparisons to alternative robust/regularized regression strategies). Also, the tolerance vector $\delta$ is estimated via ad hoc bounds (e.g., $2\sigma_{\max}$ per variable) and sensitivity to miscalibrated tolerances is not systematically studied beyond simple scaling $k\delta$.","The authors suggest extending the framework to other model classes such as quasi-order ideals. They also propose using the recursive algorithm to search for low-degree polynomials describing varieties close to empirical points, leveraging numerical dependencies, and—when the Fassino condition fails—searching explicitly for a perturbed design that induces exact dependence using root-finding methods as in Fassino and Torrente (2013).","A valuable extension would be a self-contained, open implementation (e.g., in R/Python) with reproducible benchmarks on synthetic and multiple industrial datasets, including runtime and memory profiling as design size and dimension grow. Another direction is to develop principled procedures for estimating/calibrating $\delta$ (possibly heteroscedastic and point-specific) and propagating uncertainty, as well as studying robustness when error distributions are not well captured by box tolerances. Finally, integrating fan enumeration with modern model-selection criteria (cross-validation, information criteria under noise, Bayesian model selection) and providing guidance on choosing among many maximal stable models would strengthen practical adoption.",2005.04051v2,https://arxiv.org/pdf/2005.04051v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:36:58Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Optimization|Other,Not applicable,Variable/General (case studies: HeartSteps 2 factors; SARA 4 factors; BariFit 4 factors; factors have 2–3 levels; both micro-randomized and baseline-randomized factors),Healthcare/medical|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This article is a methods-focused discussion of the micro-randomized trial (MRT), an optimization trial design used to develop just-in-time adaptive interventions (JITAIs) in digital health and related behavioral/education settings. It explains core experimental design elements specific to MRTs—factors (intervention components/options), decision points, repeated (sequential) randomization with possibly unequal randomization probabilities, and availability conditions that can suspend randomization in certain contexts. The paper uses three extended case studies (HeartSteps, SARA, BariFit) to illustrate how MRTs can evaluate time-varying causal effects of push components on proximal outcomes and how effects may vary by time and context (moderation), ultimately informing decision rules for optimized interventions. It also highlights practical design tradeoffs (participant burden/habituation vs. information yield), choice of proximal outcomes and their measurement windows, and when to use micro-randomization versus baseline randomization (including hybrid designs). The paper situates MRTs within the Multiphase Optimization Strategy (MOST) and discusses limitations and future directions such as optimizing the intervention as a whole and continual optimization in deployed systems.","Not applicable (the paper is largely conceptual/design-focused and does not center on a single new mathematical design criterion; it describes randomization at decision points with specified probabilities and availability conditions, e.g., randomize to options with probabilities like 0.3/0.3/0.4 when available).",Not applicable (this paper primarily provides experimental design considerations and illustrative case-study descriptions; it does not report a single primary quantitative performance result set in the provided excerpt).,"The authors note that more work is needed to integrate MRT results into the broader MOST goal of optimizing the intervention as a whole (not just individual components), because component effects/costs may not be additive and may involve interactions or economies of scale. They also note that behavioral theory is largely silent on behavioral dynamics (timing/duration of effects), making timing of proximal outcome measurement challenging and often reliant on domain expertise.","Because the paper is largely conceptual and case-study illustrative, it does not provide standardized empirical benchmarks (e.g., across many simulated scenarios) to quantify operating characteristics of different MRT design choices (decision-point frequency, unequal probabilities, availability rules) under realistic violations such as nonadherence, missingness, or time-series correlation. Guidance on choosing randomization probabilities and decision-point schedules is presented mainly heuristically (burden/habituation), without a formal optimization framework (e.g., constrained optimal design) or sensitivity analyses. Generalizability to settings with complex interference (spillover across time, carryover, multiple components interacting contemporaneously) may require more explicit modeling/design discussion than is provided.","The authors call for research on how best to use MRT results to optimize the intervention as a whole within MOST, given potential non-additivity and interactions among components. They suggest continual optimization via running MRTs on components in deployed digital interventions, iteratively updating versions. They also mention extending MRT ideas to pull components (randomizing content/order when users request support) and highlight personalized (person-specific) decision rules as an important future direction, leveraging reinforcement learning methods.","Developing formal, constrained optimal-design methods for MRTs (e.g., selecting decision-point schedules, randomization probabilities, and availability rules to maximize information subject to burden/habituation constraints) would strengthen design guidance. More work on robust MRT designs/analyses under autocorrelation, delayed effects, carryover, and missing-not-at-random availability/adherence would improve real-world applicability. Practical software tools (open-source) for planning MRTs (power, simulation, design selection) and reporting standards/checklists for MRT design decisions could facilitate broader adoption and reproducibility.",2005.05880v1,https://arxiv.org/pdf/2005.05880v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:37:24Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Optimization|Parameter estimation|Prediction|Other,Other,Variable/General (design parameter $\xi\in\mathbb{R}^d$; examples include 1 design parameter in a toy problem and 15 sampling times in a pharmacokinetic example),Pharmaceutical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/Goda-Research-Group/MLMC stochastic gradient,"The paper develops an efficient stochastic-gradient optimization approach for Bayesian experimental design, where the objective is to maximize the expected information gain (EIG) between prior and posterior on unknown parameters. It derives an analytic expression for the design gradient $\nabla_\xi U(\xi)$, which involves a ratio of inner expectations and is therefore a nested expectation; standard nested Monte Carlo with a fixed inner sample size yields biased gradient estimates. To remove this bias, the authors propose a randomized multilevel Monte Carlo (MLMC) debiasing scheme with an antithetic construction that produces an unbiased estimator of $\nabla_\xi U(\xi)$ with finite expected squared $\ell_2$-norm and finite expected computational cost under stated moment and bounded-score conditions. This unbiased gradient estimator is then plugged into generic stochastic gradient algorithms (e.g., Robbins–Monro/Polyak–Ruppert, AMSGrad) to search for optimal designs. Numerical experiments on a toy lognormal model and a pharmacokinetic blood-sampling design (15 time points) show stable optimization behavior and improved/qualitatively different designs versus biased fixed-inner-sample Monte Carlo, especially when using Laplace-approximation-based importance sampling for inner draws.","The expected information gain is written as a difference of two expectations, $U(\xi)=\mathbb{E}_{\theta,\epsilon}[\log \rho(f_\xi(\theta,\epsilon)\mid \theta,\xi)]-\mathbb{E}_{\theta,\epsilon}[\log \mathbb{E}_{\theta'}\rho(f_\xi(\theta,\epsilon)\mid \theta',\xi)]$. The gradient is $\nabla_\xi U(\xi)=\mathbb{E}_{\theta,\epsilon}\big[\nabla_\xi\rho/\rho-\mathbb{E}_{\theta'}[\nabla_\xi\rho]/\mathbb{E}_{\theta'}[\rho]\big]$ (all likelihood terms evaluated at $Y=f_\xi(\theta,\epsilon)$). A biased fixed-inner estimator replaces inner expectations by Monte Carlo averages, while the proposed unbiased estimator uses a randomized MLMC telescoping sum with antithetic corrections $\Delta\psi_{\xi,\ell}=\psi_{\xi,M_0 2^\ell}-\tfrac12(\psi^{(a)}_{\xi,M_0 2^{\ell-1}}+\psi^{(b)}_{\xi,M_0 2^{\ell-1}})$ and samples level $\ell$ with probability $w_\ell\propto 2^{-\tau\ell}$.","Theory: under a bounded score condition and a finite $u$th-moment condition, the antithetic correction satisfies $\mathbb{E}\|\Delta\psi_{\xi,\ell}\|_2^2=\mathcal{O}(2^{-\beta\ell})$ with $\beta=\min(u,4)/2$, enabling level probabilities $w_\ell\propto 2^{-\tau\ell}$ for any $1<\tau<\beta$ to yield finite expected cost and finite second moment. Toy example: fixed-inner Monte Carlo with small $M$ can optimize an upper bound (wrong objective) and fail to reach the true optimum, while the unbiased MLMC-based optimizer converges toward the true optimal design; empirically estimated decay exponents were about $\beta\approx1.63$–$1.64$. Pharmacokinetic example (15 sampling times): both methods converge to clustered replicate sampling schedules, but MLMC yields statistically different cluster centers and a slightly higher estimated converged EIG (about 4.544 vs 4.535) relative to biased $M=1$ Monte Carlo with Laplace-based importance sampling.","The authors note that in the pharmacokinetic example they sometimes observe an empirical decay rate estimate $\beta\le 1$, which is not covered by their Theorem 3.1 and would break the guarantee of simultaneously finite expected cost and finite second moment for the debiased estimator. They also state that choosing the level-sampling weights $w_\ell$ can be “a bit aggressive” and that a practical method to set $w_\ell$ appropriately for a given problem is left open. They explicitly defer detailed analysis of the Laplace approximation modification used for multiplicative+additive noise as beyond the scope of the paper.","The method assumes likelihoods and their design-gradients are available and cheap to compute; in many Bayesian design problems (implicit simulators, expensive PDE solvers) this may be unrealistic without adjoints/surrogates. The key theoretical conditions (bounded score in sup-norm and the stated moment condition under the chosen importance distribution) can be difficult to verify and may fail for heavy-tailed priors/likelihoods or near-singular designs, potentially leading to unstable weights or infinite variance. Optimization performance is sensitive to stochastic-optimizer hyperparameters (learning rates, AMSGrad settings) and constraints/projection choices, but systematic tuning guidance and robustness checks across problems are limited. Empirical comparisons focus mainly on biased fixed-$M$ nested MC; broader baselines (e.g., deterministic approximations, alternative unbiased nested estimators, or modern variational BOED methods) are not thoroughly benchmarked.","They explicitly call for further theoretical investigation of cases where the empirical MLMC decay rate appears to satisfy $\beta\le 1$ (not covered by their theorem), and for practical guidance on how to choose the level-probability sequence $w_\ell$ depending on the problem so as to balance finite cost and variance. They also mention that more detailed analysis of the Laplace approximation variant for models with multiplicative noise is beyond scope, implicitly motivating future work there.","Develop adaptive schemes to learn or tune $w_\ell$ (and/or $M_0$) online from pilot samples to ensure finite-variance debiasing while minimizing cost. Extend the unbiased-gradient construction to settings with intractable likelihoods (implicit models), approximate likelihoods, or expensive simulators, potentially via pseudo-marginal/ABC or surrogate-model gradients. Provide software and standardized benchmarks for Bayesian design optimization comparing against variational BOED and other stochastic-gradient estimators, including sensitivity analyses for optimizer hyperparameters and constraint handling. Investigate robustness to model misspecification, non-i.i.d./correlated observations, and design-dependent observation spaces, and derive practical diagnostics for detecting when the MLMC assumptions (moment/decay) are violated.",2005.08414v3,https://arxiv.org/pdf/2005.08414v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:38:06Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,A-optimal|D-optimal|Other,Variable/General (infinite-dimensional parameters; design dimension equals number of candidate sensors/observation times),Environmental monitoring|Healthcare/medical|Other|Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a review of optimal experimental design (OED) methods for Bayesian inverse problems governed by PDEs with infinite-dimensional (function-valued) parameters, with emphasis on sensor placement and optimal selection of observation times/locations. It formulates OED criteria in a function-space (Hilbert space) setting so that designs and criteria have meaningful infinite-dimensional limits prior to discretization, and then discusses discretization-aware computational strategies. The review covers common Bayesian optimality criteria—A-optimality (minimize posterior variance via trace of posterior covariance), c-optimality (minimize posterior variance of a linear functional/goal quantity), and D-optimality (maximize expected information gain via KL divergence / log-determinant criteria). For large-scale linear problems, it surveys scalable estimators for trace/log-determinant objectives (Monte Carlo trace estimators, randomized subspace iteration, and measurement-space reformulations), and optimization setups for sparse/binary sensor selection using relaxations and sparsity-promoting penalties. For nonlinear inverse problems, it reviews tractable OED approximations such as Bayes-risk minimization of the MAP estimator and Laplace-approximation-based criteria, and closes with future research directions (OED under model uncertainty, goal-oriented OED, mobile/switching sensors).","For Bayesian linear inverse problems with design weights w, the weighted likelihood is \(\pi_{\text{like}}(y\mid m;w)\propto \exp\{-\tfrac{1}{2\sigma^2}(Fm-y)^T W (Fm-y)\}\) with diagonal weight matrix \(W\). The posterior covariance depends on the design via \(C_{\text{post}}(w)=(\sigma^{-2}F^* W F + C_{\text{pr}}^{-1})^{-1}\). The reviewed optimality criteria include A-optimality \(\Phi_A(w)=\mathrm{tr}(C_{\text{post}}(w))\), c-optimality \(\Phi_c(w)=\langle C_{\text{post}}(w)c,c\rangle\), and D-optimality \(\Phi_D(w)=-\log\det(I+\sigma^{-2}C_{\text{pr}}^{1/2}F^*WF C_{\text{pr}}^{1/2})\) (equivalently expected information gain / KL-based log-det form).",Not applicable (review paper; no single set of new quantitative performance results is reported).,"The article is a review and is not exhaustive; it focuses primarily on OED for PDE-governed inverse problems with infinite-dimensional parameters and emphasizes sensor placement/measurement-point selection. It notes that several important practical/general settings are not treated in depth (e.g., mixed sensor types, correlated observation noise in full generality, and the broader range of experimental-design formulations beyond candidate-set sensor placement). It also emphasizes that nonlinear OED is fundamentally challenging because posterior uncertainty depends on (unknown) data and exact criteria are typically computationally infeasible, motivating approximations.","Because this is a review centered on PDE-constrained inverse problems, it does not provide a standardized benchmark suite or head-to-head empirical comparisons across all discussed methods, which can make practical method selection difficult. Many computational strategies surveyed rely on adjoint PDE solvers, low-rank structure, or Gaussian/Laplace approximations, which may degrade in accuracy for strongly nonlinear, non-Gaussian posteriors or weakly smoothing forward maps. The paper does not provide implementation details or software artifacts, so reproducibility and ease of adoption depend on consulting the cited primary sources.",It highlights several directions: scalable OED for infinite-dimensional nonlinear inverse problems (including exploring additional approximate criteria beyond Bayes risk and Laplace-based measures); OED under uncertainty in additional model components (optimization under uncertainty / reducible vs. irreducible uncertainty); goal-oriented OED tailored to prediction quantities; and extensions to time-dependent settings including switching/scanning sensor activation and mobile sensors.,"Develop open, reference implementations (e.g., in a PDE/adjoint framework) for the major linear and nonlinear OED pipelines, with common APIs for criteria/gradients and sparsity controls. Establish community benchmark problems (with meshes, priors, observation operators, and noise models) to compare A/c/D-optimal, Bayes-risk, and Laplace-based methods under consistent computational budgets. Extend robustness studies to autocorrelated model/measurement errors and to misspecification (e.g., wrong prior covariance structure), including diagnostics to detect when low-rank/Laplace assumptions are invalid.",2005.12998v3,https://arxiv.org/pdf/2005.12998v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:38:40Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/czimm79/MuControl-release,"The paper describes the design and validation of a low-cost, modular apparatus (hardware + open-source control software) for assembling and steering magnetic colloidal “microwheels” using 2D/3D rotating magnetic fields. It details a five-coil system with ferrite cores/plugs, heat-dissipating aluminum coil clamps, and interchangeable optics to image microrobot motion in both planar and 3D fluidic test sections. Performance is characterized via magnetic-field mapping and heating measurements, and supported by finite-element simulations (COMSOL Multiphysics) of magnetic fields and heat transfer. Demonstrations include microwheel translation on planar PDMS wells and in a 3D PDMS helical channel; speeds up to ~50 µm/s are reported under ~0.75–2.5 mT fields at 5–40 Hz while keeping sample temperature within ~3 °C of ambient. Overall, it is primarily an instrumentation/microrobotics methods paper rather than a designed-experiments (DOE) methodology contribution.","Key computed/controlled quantities are three phase-shifted sinusoidal drive signals (one per axis) whose superposition produces a 3D rotating magnetic field parameterized by magnitude, frequency, orientation, and camber angle $\theta_c$. For thermal modeling, natural convection heat loss is represented as $\dot{q}=h(T_c-T_a)$ at coil surfaces, with Joule heating from coupled electrical-thermal FEM modules in COMSOL. No DOE/optimal-design construction equations (e.g., information matrices or optimality criteria) are presented.","With toroid ferrite cores and aluminum heat-dissipating holders, sample temperature stayed within ~3 °C of ambient during operation; air-core coils without holders produced ~9.9 °C rise at the sample. In single-coil tests at 4 A peak and 40 Hz, peak coil temperatures were ~83.6 °C (air-core, no holder), ~56.6 °C (air-core with aluminum holder), and ~46.4 °C (toroid-core with holder). Field mapping at 4 A peak and 40 Hz showed ~2 mT near the center for x–y coil pairs (toroid cores, no plugs) and ~1 mT for the single z-coil; ferrite plugs increased field strength, with extended plugs giving the highest but more localized fields. Demonstrations achieved microwheel translation up to ~50 µm/s (reported range) and transport through a helical 3D channel (e.g., ~37 mm along the helix, ~5.7 mm vertical rise) under rotating fields up to 40 Hz.",None stated,"The study varies many operating parameters (frequency, field strength via coil configurations, camber angle) but does not use a formal DOE framework (randomization/blocking/replication plans, factorial modeling, uncertainty quantification) to attribute effects or interactions; results are therefore more demonstrative than statistically optimized. Performance evaluation is mostly specific to the presented coil geometries, bead type/size, and PDMS test sections, so generalization to other microrobot constructs or biological media (viscoelasticity, heterogeneity) is uncertain. Software is open-source, but the paper does not specify packaged reproducible experiment scripts/data for the reported plots, which may limit exact replication of benchmarks.",None stated,"A natural extension would be a formal, statistically planned experiment (e.g., factorial/response-surface) to model microwheel speed/temperature rise as functions of frequency, field magnitude, camber angle, bead concentration, and channel geometry, including interaction effects and uncertainty. Additional work could test robustness under non-ideal conditions (autocorrelated disturbances, conductive/ferromagnetic surroundings, biological fluids) and extend control to closed-loop feedback using image-based tracking. Providing a fully reproducible artifact (raw field/temperature datasets plus analysis scripts and COMSOL model files) would strengthen replicability and facilitate design transfer to other labs.",2006.02524v1,https://arxiv.org/pdf/2006.02524v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:39:09Z
TRUE,Other,Model discrimination,Not applicable,"Variable/General (design parameters include B-field strength, particle mass/charge, cavity geometry Rin/Rout/δ, and density ρ)",Other|Theoretical/simulation only,Approximation methods|Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a laboratory experimental concept to test chameleon-screened scalar fifth-force theories by observing how a designed scalar-field profile perturbs the cyclotron motion of a charged particle in a static, uniform magnetic field. It derives the relativistic and non-relativistic equations of motion for a charged particle under both Lorentz force and a scalar fifth force, showing that the fifth force generically induces a guiding-center drift of the orbit whose direction and timescale encode the transverse scalar-field profile. Using chameleon-field profiles motivated by geometries of nested cylinders (coaxial and axis-shifted) and parallel plates, the authors compare analytic drift estimates (guiding-center approximation) to numerical trajectory computations for multipole-like field configurations. They also identify macroscopic observables: the fifth-force-induced drift can generate measurable currents in a plasma between plates or annular currents between cylinders, potentially allowing inference of the scalar-field profile. Practical considerations include suppressing dominant gravitational drift by aligning the magnetic field with local gravity and assessing competing effects such as radiation reaction, concluding that the concept merits feasibility and noise studies for laboratory implementation.","In the non-relativistic limit with a uniform magnetic field $\mathbf{B}=B\,\mathbf{e}_z$, the motion is approximated by $\mathbf{a}=\omega_0\,\mathbf{v}\times\mathbf{e}_z-\beta\nabla\varphi$ with $\omega_0=qB/m$ and $\varphi\equiv \phi/M_P$. In the guiding-center approximation, the drift induced by an external force $\mathbf{F}$ is $\mathbf{v}_{\rm drift}=\mathbf{F}\times\mathbf{B}/(qB^2)$; for a radial fifth force this yields $\mathbf{v}_{\rm drift}=(\beta c^2\varphi'(r)/\omega_0)\,\mathbf{e}_\theta$ and an angular drift rate $\omega_{\rm drift}=\beta c^2\varphi'(r)/(r\omega_0)$. For a multipole scalar profile $\varphi(r,\theta)=\Phi_n(r)\cos(n\theta)$, the fifth-force-driven drift becomes $\mathbf{v}^{(n)}_{\rm drift}=\frac{\beta}{\omega_0}\left[\frac{n\Phi_n(r)}{r}\sin(n\theta)\,\mathbf{e}_r+\Phi_n'(r)\cos(n\theta)\,\mathbf{e}_\theta\right]$.","The authors argue that a chameleon-motivated fifth-force magnitude of order $F_0\sim 10^{-7}\,\mathrm{m/s^2}$ can produce an orbit guiding-center drift detectable on laboratory timescales for suitable choices of $B$, particle mass $m$, and charge $Z$; e.g., they state that a drift of $\sim 1\,\mu\mathrm{m}$ within $<1\,$hour is feasible for $B\sim 1\,$mT and $m\sim 100\,m_p$ (for $Z=1$). For a parallel-plate configuration, the fifth-force drift can induce macroscopic currents; with a gas-like number density $\eta_q\sim 3\times 10^{25}\,\mathrm{m^{-3}}$, they estimate currents at the nA scale, giving a scaling $I\approx 5\,\mathrm{nA}\,(\eta_q/\eta_0)(B/1\,\mathrm{T})^{-1}(m/m_p)(S_\perp/1\,\mathrm{m^2})(F_0/10^{-7}\,\mathrm{m/s^2})$. They note the induced magnetic field from annular currents is tiny (order $\sim 10^{-18}$ T) and negligible compared with the applied field. They also emphasize that gravitational drift would be orders of magnitude larger than the fifth-force signal but can be suppressed by aligning $\mathbf{B}$ with local gravity so that $\mathbf{g}\times\mathbf{B}=0$.","The authors state that their analysis provides only first elements and order-of-magnitude estimates, and that the technological feasibility and potential loopholes/noise sources require careful investigation beyond the scope of the work. They also note they used toy scalar-field profiles for illustration and that realistic chameleon profiles (and their optimization) must be implemented to assess detectability. They emphasize the crucial need for precise alignment of the magnetic field with local gravity and for careful study of gravity from the experimental surroundings.","The work does not provide a complete experimental uncertainty budget (e.g., magnetic-field inhomogeneity, patch potentials/stray electric fields, plasma instabilities, collisions/neutral drag, wall charging, or particle loss), any of which could mimic or obscure slow drifts/currents at the proposed sensitivity. Comparisons are largely against internal approximations (guiding-center theory) rather than benchmarking against alternative fifth-force detection modalities or established chameleon constraints to quantify incremental sensitivity. The numerical computations are described qualitatively without sufficient reproducibility details (discretization, convergence, parameter sweeps), and no implementation/code is shared.",They propose implementing and likely optimizing realistic scalar-field profiles (as in their prior nested-cylinder computations) and studying detectability of both drifts and induced currents including all unavoidable noise sources. They highlight that designing geometries that generate sharp angular gradients (large $\partial_\theta\varphi$) could enhance sensitivity and should be investigated. They also mention the freedom to allow time-varying magnetic fields and note that detailed feasibility studies are needed.,"A natural extension is a full experimental design optimization that jointly selects geometry (Rin/Rout/δ), plasma density/temperature, particle species, and magnetic-field configuration to maximize signal-to-noise under realistic constraints, possibly using Bayesian experimental design for model selection over $(\Lambda,n,\beta)$. Robust/self-calibrating strategies (e.g., periodic rotation/modulation of $\mathbf{B}$ relative to gravity, differential measurements with reversed charge sign/species, or null channels) could help separate fifth-force effects from systematics. Developing an open-source simulation/analysis pipeline (including chameleon field solvers coupled to particle/plasma dynamics) and validating on existing lab data would materially improve practicality and reproducibility.",2006.03359v1,https://arxiv.org/pdf/2006.03359v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:39:52Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (linear Bayesian inverse problems with parameters X∈R^n; examples include n=20, m=100 observations; and climate case reduced to 10 parameters with m=1642 candidate observations)",Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops and analyzes batch greedy, distributed batch greedy, and stochastic batch greedy algorithms for maximizing monotone (non-decreasing) set functions that may be non-submodular, providing approximation guarantees characterized by submodularity and supermodularity ratios. It applies these results to Bayesian optimal experimental design (OED) for linear–Gaussian inverse problems, where the design objective is mutual information (equivalently Bayesian D-optimality / expected information gain) under a cardinality constraint (subset/sensor selection). For this OED setting, the paper derives bounds on the submodularity and supermodularity ratios of the mutual-information set function, and proposes MM (minorize–maximize) inspired sequential algorithms using modular lower/upper bounds on mutual information. Empirically, the methods are demonstrated on synthetic linear inverse problems and on a real-world climate monitoring/sensor placement example (E3SM/sELM), showing how batch size trades off computation and design quality and that MM-based bounds can outperform standard batch greedy at larger batch sizes. Overall, the work advances computational approaches to subset-selection experimental design when classical submodularity assumptions do not hold (e.g., correlated observation errors).","The core OED problem is cardinality-constrained subset selection maximizing mutual information: \(\max_{P\in S(k)} I(X;P^\top Y)\), where \(P\) is a selection operator choosing \(k\) observations. In the linear–Gaussian case, mutual information has a log-determinant form \(I(X;Y_P)=\tfrac12\log\det(\Gamma_{Y_P})-\tfrac12\log\det(\Gamma_{Y_P|X})=\tfrac12\log\det(\Gamma_X)-\tfrac12\log\det(\Gamma_{X|Y_P})\), linking to Bayesian D-optimality. Batch greedy selects sets by incremental gains \(\rho_A(B)=F(A\cup B)-F(B)\), and the analysis uses submodularity and supermodularity ratios \(\gamma,\eta\) to bound performance (e.g., \(F(A)\ge (1-e^{-\eta_{V,q}\gamma_{V,k}})\max_{|B|\le k}F(B)\) for uniform batch size \(q\)).","The paper proves approximation guarantees for (i) standard batch greedy, (ii) distributed batch greedy, and (iii) stochastic batch greedy for maximizing monotone, potentially non-submodular objectives, with factors depending on both submodularity ratio \(\gamma\) and supermodularity ratio \(\eta\). In linear–Gaussian Bayesian OED with mutual-information objective, it shows both \(\gamma\) and \(\eta\) can be lower bounded by \(\log\zeta_{\min}/\log\zeta_{\max}\), where \(\zeta\) are generalized eigenvalues of \((\Gamma_Y,\Gamma_{Y|X})\). Numerical experiments (synthetic inverse problems and climate sensor placement) show smaller batches generally yield higher information gain, while MM-based modular bounds (MMGreedy/MMReverseGreedy) can match or exceed standard greedy, especially at larger batch sizes; the climate case shows relatively weak sensitivity to batch size and similar performance across heuristics due to information saturation/weak informativeness.","The authors note that their worst-case approximation analysis (e.g., the simplified bound \(1-e^{-\eta\gamma}\)) does not incorporate curvature and can exhibit a gap even for modular objectives; incorporating curvature is deferred to future work. They also acknowledge that bounds on sub/supermodularity ratios for mutual information can be loose (e.g., can become trivial in some dimensional regimes), contributing to a gap between worst-case guarantees and strong empirical performance. For MM-based algorithms, they emphasize computational challenges (e.g., estimating diagonals of matrix logarithms) and that their MMReverseGreedy approach lacks an approximation guarantee.","The experimental-design focus is largely limited to discrete subset selection under cardinality constraints in linear–Gaussian models; extensions to nonlinear/non-Gaussian Bayesian design are not developed despite motivating discussion. Empirical evaluations emphasize mutual information as the criterion; comparisons to alternative OED criteria (A-, E-, I-optimality) and to state-of-the-art optimal design solvers (e.g., continuous relaxations, exchange algorithms, DPP MAP solvers) are limited, making it harder to position practical competitiveness. Practical deployment issues such as model mismatch, nonstationary noise/covariance misspecification, and operational constraints beyond cardinality (costs, spatial constraints, matroids/knapsacks) are not deeply explored, though the framework could potentially extend. Software/implementation details are not provided, which limits reproducibility and adoption.",They propose extending the batch-greedy approximation analysis by incorporating curvature (classical or generalized) to tighten guarantees and remove gaps relative to modularity. They suggest developing adaptive strategies for choosing batch sizes across iterations to better trade off computation and performance. They also note potential for using the supermodularity ratio to analyze minimization problems for non-increasing set functions (mirroring work on greedy descent for supermodular minimization) and exploring deeper connections between operator inequalities/modular bounds and polyhedral combinatorics in submodular optimization contexts.,"A natural extension is to handle richer experimental-design constraints (e.g., knapsack budgets, matroid constraints, spatial coverage/dispersion constraints) and to quantify how guarantees change under these constraints. Developing scalable approximations for computing matrix-logarithm diagonals/log-determinants (e.g., stochastic trace estimators, randomized low-rank methods) with error-controlled impact on design quality would improve practicality for very large m. Robust/self-starting variants that account for unknown/estimated covariances (Phase I uncertainty) and correlated/time-dependent observations would align the methods with real sensor-network operations. Broader empirical benchmarking against modern OED and subset-selection methods (including DPP-based MAP/volume sampling, convex relaxations, and greedy variants with lazy evaluations) and releasing reference implementations would strengthen evidence and facilitate adoption.",2006.04554v3,https://arxiv.org/pdf/2006.04554v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:40:40Z
TRUE,Sequential/adaptive|Computer experiment|Optimal design|Other,Prediction|Optimization|Model discrimination,Space-filling|Minimax/Maximin|Other,"Variable/General (examples include p=2, p=4, p up to 20 in simulations; applications use p=45 for MoS2 and p=16 for Ni-Cr ReaxFF parameters)",Energy/utilities|Other,Simulation study|Other,TRUE,R|Other,Not provided,NA,"The paper proposes CLAIMED, a framework for exploring deterministic, complex multivariate response surfaces with unknown feasibility constraints, motivated by ReaxFF force-field parameter optimization via expensive computer simulations. It combines (i) an initial space-filling design (orthogonal-maximin Latin hypercube), (ii) a supervised feasibility classifier (e.g., logistic regression or random forest), and (iii) sequential sampling using Minimum Energy Designs (MED) to place new runs in low-loss (“good”) regions while keeping points well spread to discover multiple desirable regions rather than clustering at a single optimum. The multivariate responses are reduced to a weighted squared-error loss to a target vector, and infeasible points are handled by imputing a large loss and/or filtering MED-proposed points through the feasibility classifier; surrogate/emulator models (clustered penalized regression or deep learning) are used when simulator calls are expensive. Performance is illustrated on toy examples, modified DTLZ2 simulations (p=q up to 20), and two ReaxFF applications (MoS2 with 45 inputs/599 outputs; Ni-Cr with 16 inputs/90 outputs), where the approach finds multiple improved parameter sets and improves best loss relative to initial designs and a conventional one-factor-at-a-time method.","Responses are combined into a scalar loss using a weighted squared error: $L(Y,T,W)=\sum_{j=1}^q\{(Y_j-T_j)/w_j\}^2=(Y-T)^T W^{-1}(Y-T)$. MED selects points by minimizing an energy over design points $x_i$: $E=\sum_{i<j} q(x_i)q(x_j)/d(x_i,x_j)$, with weights chosen to represent a surface $r(x)$ via $q(x)=1/\{r(x)\}^{1/(2p)}$ (here $r(x)$ is taken as inverse loss). With surrogate and feasibility prediction, the predicted loss is $\hat L(x)=\sum_{j=1}^q \{(\hat f_j(x)-T_j)/w_j\}^2$ if predicted feasible, else set to a large constant $M$ (paper’s Eq. (2), and a clustered-response variant in Eq. (4)).","In simulations based on a modified DTLZ2 function, a 200-run strategy (100 initial maximin LHD + 100 MED points) outperformed a 200-run uniform design over 100 repetitions: in 71/100 cases MED found a lower minimum loss, and in 100% of cases MED achieved lower median loss with markedly smaller variance. In the MoS2 ReaxFF application (45 inputs, 599 responses), an initial OMLHD produced 1154 feasible runs out of 5000; iterative MED+classifier+surrogate cycles found a point with loss about 80,000, a reported 17% improvement over the best initial feasible point (~96,459). In the Ni-Cr ReaxFF application (16 inputs, 90 responses), starting from 79,635 initial design points (4,999 infeasible), the best initial loss was 2812 and the approach identified a best loss of 2316.17 (18% improvement) and also beat a conventional method (best loss 5614.26), producing multiple good parameter combinations in ~2 minutes versus ~2 weeks reported for manual tuning.","The authors note implementation drawbacks of the MED software they used (“mined”): generated points may fall outside the input range (especially in higher dimensions), points may not reduce total error well (sometimes the best MED point matches the best initial point), and computation time increases with both the number of initial points and dimensionality. They also state that avoiding infeasible regions is not guaranteed by MED alone, motivating post-filtering with a feasibility classifier.","The approach relies heavily on the quality of surrogate models and the feasibility classifier; miscalibration can cause MED to over-exploit incorrect low-loss regions or reject truly feasible high-value candidates (threshold choice such as 0.5 may be suboptimal and problem-dependent). The method’s performance comparisons are limited (e.g., EI/BO comparison is illustrative rather than a comprehensive benchmark against modern constrained BO and batch diversity methods), and the treatment of constraints via large-loss imputation can distort the loss landscape and MED weighting, especially when infeasible regions are large. Practical guidance on tuning (MED iterations, batch sizes, classifier selection, and surrogate uncertainty handling) is not fully formalized, and no publicly shared code hinders reproducibility.","The paper explicitly highlights as future work the study of Pareto-optimal formulations for multiresponse “goodness” (instead of a scalar loss), particularly for high-dimensional inputs/outputs and unknown constraints, noting that such Pareto optimization problems are rarely addressed for ReaxFF-scale settings. It also notes the framework could be extended to accommodate noisy (non-deterministic) responses.","A natural extension is to incorporate surrogate uncertainty directly into the MED weighting and constraint handling (e.g., Bayesian or probabilistic feasibility and robust acquisition), rather than using point predictions and hard thresholds. Additional work could develop principled stopping rules and batch/sequential design choices, and evaluate against state-of-the-art constrained/batch BO methods designed to return diverse near-optimal sets. Broader validation on more real-world constrained simulators and providing an open-source implementation (including MED+classifier+surrogate workflow) would improve adoption and reproducibility.",2006.05021v2,https://arxiv.org/pdf/2006.05021v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:41:22Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Other,Other,Not specified (two policies/arms; finite Markov chain state space),Service industry|Other|Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper studies optimal experimental design for comparing two system-wide policies (treatment vs control) when outcomes exhibit temporal interference, modeled as switching between two unknown irreducible Markov chains on a common finite state space. It proposes a nonparametric maximum-likelihood estimator (MLE) for the steady-state average reward difference (treatment effect) by estimating each chain’s transition matrix and stationary distribution from a single interleaved run. Within a broad class of time-average regular (TAR) adaptive designs, it proves consistency of the MLE and derives a central limit theorem for the estimator using martingale methods and Poisson’s equation for Markov chains. It characterizes asymptotically efficient designs by showing the minimum asymptotic variance is obtained by policy limits solving a succinct convex program, and then introduces an implementable adaptive online policy (OnlineETI) that learns these limits and achieves asymptotic efficiency. The work advances switchback/marketplace experimentation by providing a principled design that corrects bias from temporal interference while being sample-efficient via adaptive state-dependent switching.","Policies choose action $A_n\in\{1,2\}$ each step; rewards are generated as $R_n=F^{-1}(V_n\mid A_{n-1},X_{n-1},X_n)$. The nonparametric MLE estimates transitions by empirical frequencies $\hat P_n(\ell,x,y)=\frac{\sum_{j=0}^{n-1}\mathbf{1}(X_j=x,A_j=\ell,X_{j+1}=y)}{\max\{\Gamma_n(\ell,x),1\}}$ and then computes $\hat\pi_n(\ell)$ as the stationary distribution of $\hat P_n(\ell)$, with reward estimates $\hat r_n(\ell,x)=\frac{\sum_{j=0}^{n-1}\mathbf{1}(X_j=x,A_j=\ell)R_{j+1}}{\max\{\Gamma_n(\ell,x),1\}}$. The treatment-effect estimator is $\hat\alpha_n=\hat\pi_n(2)\hat r_n(2)-\hat\pi_n(1)\hat r_n(1)$, and efficient TAR limits $\kappa^*$ solve the convex program $\min_{\kappa\in K}\sum_{\ell=1}^2\sum_{x\in S}\frac{\pi(\ell,x)^2\sigma(\ell,x)^2}{\kappa(\ell,x)}$ with $K$ enforcing flow balance and normalization constraints.","Under TAR policies with positive limiting state-action frequencies, the MLE is consistent and satisfies a CLT: $\sqrt{n}(\hat\alpha_n-\alpha)$ converges to a mean-zero normal with variance determined by $\pi(\ell,\cdot)$, per-state quantities $\sigma(\ell,x)$ (defined via Poisson’s equation), and the policy limits $\gamma(\ell,x)$. The paper lower-bounds the scaled asymptotic variance for any TAR policy and shows that when the policy limits are constant, the exact limit is $\lim_{n\to\infty} n\,\mathrm{Var}(\hat\alpha_n-\alpha)=\sum_{\ell=1}^2\sum_{x\in S}\frac{\pi(\ell,x)^2\sigma(\ell,x)^2}{\gamma(\ell,x)}$. It proves the convex program over feasible limits $\kappa\in K$ has a unique positive optimizer $\kappa^*$, and any TAR policy with limits $\kappa^*$ is asymptotically efficient. It then proposes OnlineETI, an adaptive policy that estimates model primitives online, repeatedly solves the plug-in convex program, adds diminishing forced exploration, and is proven to have policy limits $\kappa^*$ (hence consistent and efficient). An illustrative example shows “cooperative exploration” can yield variance improvements unbounded in the size of the state space versus running each policy in isolation.","The authors focus on asymptotic efficiency rather than optimality at fixed finite horizons, and note the forced-exploration rate (e.g., $M_n(x)^{-\beta}$) likely trades off finite-horizon bias and variance; they leave optimal finite-horizon tuning/design open. They also note the approach is fully nonparametric and assumes full knowledge of a finite state space; in practice, full state-space knowledge may be infeasible, and adapting the insights to such applied domains remains future work. They mention an alternative estimation approach (sample-average with regenerative policies) but do not pursue it in the main development.","The model assumes a finite, irreducible Markov chain for each policy and bounded rewards; many real platforms have large/continuous state spaces, partial observability, nonstationarity, and heavy-tailed rewards, which may invalidate assumptions or complicate estimation. The method requires repeatedly computing stationary distributions, solving linear systems for Poisson’s equation, and solving a convex program over $\kappa\in K$, which could be computationally burdensome for large $|S|$ and may limit practicality without approximations. The optimal design characterization depends on unknown primitives (e.g., $\pi(\ell,\cdot)$ and $\sigma(\ell,\cdot)$), and while OnlineETI is consistent, its finite-sample behavior and sensitivity to estimation error/model misspecification are not fully characterized. The framework focuses on two policies; extensions to multiple treatments and constraints on switching (e.g., operational or fairness constraints) are not developed.","They propose studying optimal experimental design for fixed finite horizons and formally analyzing how the forced-exploration exponent $\beta$ trades off finite-horizon bias versus variance. They also suggest extending or adapting the approach to practical settings where full knowledge of the state space is infeasible, while still using the core insight of adaptive sampling to minimize variance under temporal interference. They point to related work on regenerative policies with sample-average estimation as another direction (referencing a prior presentation).","Develop scalable approximations for large or continuous state spaces (e.g., function approximation, state aggregation, or simulation-based Poisson-equation solvers) and quantify resulting efficiency loss. Extend the design and analysis to more than two policies (multi-arm switchback with interference), including constraints on switching frequency and operational costs. Study robustness to nonstationarity (policy drift, seasonality) and dependence beyond Markov (e.g., higher-order dynamics or exogenous shocks), including self-normalized or robust MLE variants. Provide software and benchmarking on real marketplace/queueing datasets to validate finite-sample performance and to compare against modern alternatives (e.g., Bayesian or doubly robust estimators under interference).",2006.05591v2,https://arxiv.org/pdf/2006.05591v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:42:09Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,TRUE,Python|Other,Not provided,https://jupyter.org/,"This paper compares two levels of inquiry in an undergraduate physics laboratory (guided inquiry vs. open inquiry) using transcribed audio recordings of student lab sessions during a CD diffraction experiment. In the guided version, students follow a recipe-like procedure; in the open version, students design and execute their own procedures using prompts and added degrees of freedom in the apparatus (e.g., changing distances and orientations, using different lasers). The authors conduct qualitative thematic analysis (open/axial/selective coding) augmented by quantitative counts of codes/categories/references to characterize differences in student learning experience and behaviour. They find open inquiry yields more concentrated, design-oriented talk and behaviours, richer instructor-student interactions, and less negative affect (e.g., less frustration/confusion), suggesting open inquiry better reinforces experimental design skills. The work contributes to physics education research on laboratory pedagogy rather than proposing statistical DOE methods or optimal experimental designs.",Not applicable,"Quantitatively, the guided-inquiry dataset (5 groups) produced 4 themes, 26 categories, 99 codes, and 1968 coded references, while the open-inquiry dataset (8 groups) produced 4 themes, 9 categories, 112 codes, and 1871 references. Average completion time was about 2 h 5 min (guided) versus 2 h 21 min (open), with both finishing within a 3-hour lab period. The open-inquiry condition showed more frequent design-related behaviours (e.g., identifying measurements, improvement ideas, decision making) and fewer/less intense negative affective codes (e.g., frustration) than guided inquiry.","The authors note several limitations: the study is based on one specific topic (CD diffraction) in a second-year lab at a single research institution, so results may not generalize to other topics or student populations. The guided- and open-inquiry groups were different students randomly selected from the class (not the same students under both conditions). They also acknowledge that although they categorize the labs as guided vs. open inquiry, each experiment is unique and the needed level/type of instructor guidance may shift with student familiarity and expertise.","The study is observational and not randomized at the individual level to conditions with tight control of timing/ordering; open inquiry was run mid-semester and guided inquiry later, so maturity/experience effects could confound comparisons. Outcomes focus on coded conversation/behaviour and affect rather than direct measures of experimental-design competence (e.g., validated assessments, rubric-scored artifacts, or longitudinal transfer). The analysis may be sensitive to coder interpretation and to the fact that “talkativeness” varies by group; fewer references could reflect less speech rather than better learning, and audio may miss nonverbal design work.",None stated.,"A stronger causal evaluation could randomize or counterbalance inquiry conditions (including order effects) and incorporate pre/post measures of experimental-design skills using validated instruments and artifact scoring. Extending to multiple lab topics, institutions, and student levels would test generalizability and identify which apparatus degrees-of-freedom and prompts most effectively support design learning without overwhelming students. Providing open-source analysis code and a coding manual would improve reproducibility and enable meta-analytic comparison across similar physics lab inquiry studies.",2006.06725v1,https://arxiv.org/pdf/2006.06725v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:42:33Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Model discrimination|Parameter estimation|Cost reduction|Other,Not applicable,Variable/General (n variables/nodes in a causal DAG; interventions on single nodes and sets of nodes under a budget),Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies experiment design for causal discovery when the causal structure is identifiable only up to a Markov Equivalence Class (MEC) from observational data. It introduces LazyIter, an algorithm that efficiently iterates over all possible interventional essential graphs obtainable from a single-node intervention by exploiting overlap between nearby candidate parent sets, avoiding repeated full recomputation of edge orientations. Using LazyIter, the authors develop LazyCount, a dynamic-programming approach to compute MEC sizes by partitioning into interventional Markov equivalence classes and recursively multiplying counts over chordal chain components. They then propose exact algorithms for worst-case experiment design in both active learning (greedy selection of the next single-node intervention) and passive learning (budgeted selection of multiple single-node interventions), with objectives based on minimizing worst-case remaining MEC size or maximizing worst-case number of oriented edges. Simulation experiments on synthetically generated graphs show substantial runtime improvements over prior MEC counting/iteration methods and better edge-orientation discovery under budgets compared with simple heuristics (Random, MaxDegree).","Two experiment-design objectives are defined on a UCCG essential graph $G$: (1) worst-case remaining interventional equivalence class size, $I_{\mathrm{opt}}=\arg\min_{I\subseteq V}\max_{R\in \mathrm{IR}_{\{I\}}(G)}|\mathrm{MEC}(R)|$; and (2) worst-case number of oriented edges after intervention, $I_{\mathrm{opt}}=\arg\max_{I\subseteq V}\min_{R\in \mathrm{IR}_{\{I\}}(G)}|\mathrm{Dir}(R)|$. MEC size is computed via partitioning over intervention result space and chain components: $|\mathrm{MEC}(G)|=\sum_{R\in \mathrm{IR}_{\mathcal I}(G)}\prod_{C\in \mathcal C(R)}|\mathrm{MEC}(C)|$. For passive (budgeted) design, a DP recursion is given for worst-case oriented edges: $DP[S][T]=\min_{R\in \mathrm{IR}_{\{v_1\}}(G[S])}\Big(|\mathrm{Dir}(R)|+\sum_{C\in \mathcal C(R)}DP[C][T\cap C]\Big)$ (analogously with $|\mathrm{MEC}(R)|$ for the other objective).","On synthetic graphs (100 generated per setting), LazyIter is consistently faster than the Hauser & B\u00fchlmann (2014) approach for enumerating single-node intervention outcomes, with the advantage growing markedly for denser graphs (Figure 2a, n=30, runtime up to ~2000s for baseline vs much smaller for LazyIter at high edge counts). LazyCount outperforms the MemoMAO MEC-size algorithm (Talvitie & Koivisto, 2019), especially on dense graphs (Figure 2b, n=30). For passive learning (budgeted interventions), the proposed exact worst-case DP method achieves higher edge discovery ratios than Random and MaxDegree across varying graph order, edge density, and budget (Figure 2c\u2013e), with the gap widening as graphs get larger/harder. Reported algorithmic complexity improvements include an $O(n)$ speedup factor for sparse graphs (n = number of variables) relative to prior MEC-size counting methods by leveraging sparsity through LazyIter-driven recursion.","The paper focuses heavily on chordal chain components (UCCGs) arising from essential graphs and builds its iteration/counting routines around single-node interventions as the core primitive. In the conclusion, the authors note that extending the approach to other experiment-design objectives (e.g., average number of oriented edges) and to passive settings where each experiment can intervene on multiple variables remains future work.","The experimental evaluation is limited to simulated graph families generated by a specific procedure (He et al., 2015), so it is unclear how performance and solution quality translate to real causal discovery problems with measurement noise, finite-sample errors in essential-graph estimation, or model misspecification. The design objectives are worst-case criteria; while appropriate for guarantees, they may be overly conservative in practice compared with expected/average-case utility, and the paper does not quantify this trade-off. Implementation details (language, optimization level) are not provided, making it hard to attribute observed wall-clock improvements solely to algorithmic advances and to reproduce benchmarks. The passive-learning DP scales as $3^n$ states (as stated), which may still be prohibitive beyond moderate n despite per-state speedups; practical cutoff sizes and memory constraints are not characterized.","The authors suggest extending the proposed experiment-design algorithms to additional objective functions, such as optimizing the average number of oriented edges rather than worst-case criteria. They also propose developing passive-learning algorithms that allow intervening on multiple variables in each experiment (multi-node interventions), beyond the paper\u2019s single-node-per-experiment model.","A valuable extension would be to incorporate uncertainty from finite-sample causal discovery (errors in estimated essential graphs) and study robust or Bayesian variants of the design criteria under estimation noise. Developing scalable approximations or anytime variants of the $3^n$ passive DP with quality guarantees (e.g., branch-and-bound, admissible heuristics, or submodular relaxations where applicable) would improve applicability to larger graphs. Providing open-source implementations and standardized benchmark suites would enable reproducible comparisons and broader adoption. Finally, extending LazyIter/LazyCount ideas to interventional equivalence under soft interventions, imperfect interventions, or partially observed/latent-variable settings would align the methods with many real experimental domains.",2006.09670v1,https://arxiv.org/pdf/2006.09670v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:43:13Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Prediction|Parameter estimation|Screening|Other,Other,"Variable/General (examples include 2D, 8D, 10D, and up to 30D inputs)",Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/ablancha/gpsearch,"The paper proposes output-weighted optimal sampling acquisition functions for sequential Bayesian experimental design (active learning) of expensive black-box functions, using Gaussian process regression surrogates. The key idea is to weight exploration by a likelihood ratio $w(x)=p_x(x)/p_{\mu}(\mu(x))$ so the design focuses on input regions that are both plausible under the input prior and produce unusually large (rare/extreme) outputs. Two likelihood-weighted criteria are emphasized: a likelihood-weighted uncertainty sampling rule and a likelihood-weighted integrated variance reduction (equivalently the Q-criterion/IVR-LW), with computational strategies that preserve tractability by estimating $p_{\mu}$ via KDE in 1D output space and approximating $w(x)$ by a Gaussian mixture model so required integrals/gradients become analytic (for an RBF-ARD kernel). The methods are demonstrated on simulated/design-test problems including rare-event quantification in a stochastic oscillator, uncertainty quantification for the 8D borehole (hydrological) model, and learning “danger maps” for extreme events in dynamical systems up to 30 dimensions. Across these examples, likelihood-weighted acquisition functions converge faster than unweighted US/IVR and LHS baselines for learning output distributions and tails/rare-event behavior, while remaining scalable to higher dimensions than prior rare-event active-learning approaches.","The sequential BED design selects $x_n=\arg\min_{x\in\mathcal X} a(x;\bar f,\mathcal D_{n-1})$ (or equivalently maximizes criteria depending on $a$). The core weighting is the likelihood ratio $w(x)=p_x(x)/p_{\mu}(\mu(x))$, where $p_{\mu}$ is the density of the GP posterior mean induced by $x\sim p_x$. Likelihood-weighted acquisition examples include US-LW: $a_{\mathrm{US-LW}}(x)=\sigma^2(x)\,w(x)$ and IVR-LW: $a_{\mathrm{IVR-LW}}(x)=\frac{1}{\sigma^2(x)}\int \mathrm{cov}^2(x,x')\,w(x')\,dx'$, computed efficiently by approximating $w(x)$ with a GMM so the integral and gradients are analytic for an RBF kernel.","In multiple numerical experiments, likelihood-weighted acquisition functions (US-LW and IVR-LW/Q) reduce the log-pdf error of the learned output distribution faster than standard uncertainty sampling (US), IVR/IVR-IW, and Latin hypercube sampling, particularly for heavy-tailed/rare-event regimes. For the stochastic oscillator example, US-LW and IVR-LW provide the best (nearly identical) convergence across noise levels tested, and performance is not very sensitive to the number of GMM components used to approximate $w(x)$. In the 8D borehole UQ problem, IVR-LW improves convergence over IVR and IVR-IW, while US-LW is roughly on par with US (attributed to mostly-uniform input priors). For dynamical-system extreme-event “danger map” learning, likelihood-weighted methods outperform unweighted baselines and retain benefits as dimension is augmented up to 30 with irrelevant (dummy) directions.","The authors note the method is not a universal solution (“no free lunch”): it is expected to help when sequential Bayesian design is appropriate and events of interest are sufficiently rare/extreme, and may provide no advantage otherwise. They also state performance can be limited by the topology of dangerous regions in dynamical systems (e.g., fractal/riddled sets) and by strong chaos/large Lyapunov exponents causing nearby inputs to yield very different outputs within the prediction horizon, which is challenging for GP regression as a smoother. They mention further work is needed to better alleviate the curse of dimensionality, e.g., by leveraging topology/curvature of the search space.","The approach depends on accurate estimation of $p_{\mu}$ (via KDE) and of the likelihood ratio, which can be sensitive to KDE bandwidth/finite-sample effects and may become unstable when $p_{\mu}(\mu(x))$ is very small (large weights) without explicit regularization. The need to optimize acquisition functions over potentially high-dimensional bounded domains can dominate cost when the black box is not extremely expensive, and the paper does not detail robust global-optimization strategies or failure modes for acquisition optimization. The analytic tractability claims hinge on the RBF kernel and the GMM approximation; performance/tractability for other kernels or strongly nonstationary behavior is not established.","They suggest improving high-dimensional performance further to alleviate the curse of dimensionality, including leveraging information about the topology or curvature of the search space. They also point to investigating whether similar gains carry over to Bayesian optimization (referencing related work) rather than experimental design/UQ.","Developing principled regularization/clipping or Bayesian treatment of the likelihood ratio (and uncertainty in $p_{\mu}$) could improve stability and robustness, especially in early iterations. Extending the method to handle strong input dependence/autocorrelation (time series), non-Gaussian observation noise, and fully Bayesian GP hyperparameter uncertainty (rather than MLE point estimates) could broaden applicability. Providing standardized benchmark suites, ablation studies on KDE/GMM choices, and a packaged implementation with reproducible scripts would strengthen practical adoption and clarify computational trade-offs at higher dimensions.",2006.12394v3,https://arxiv.org/pdf/2006.12394v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:44:03Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Robustness|Other,Not applicable,Variable/General,Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies heavily restricted randomization designs for randomized controlled trials (e.g., rerandomization and algorithmic “nearly random” designs) and formalizes the risk that overly strict restriction can yield very high mean squared error (MSE) for the difference-in-means treatment effect estimator. The authors develop a combinatorics-based framework (“veil of ignorance”) that evaluates designs independent of the realized covariate/outcome data and re-proves minimax-type results: complete randomization uniquely minimizes the worst-case MSE, and increasing restriction (smaller admissible assignment set size) increases both the maximum MSE and the variance of MSE. They propose a novel diagnostic, the assignment correlation (based on pairwise uniqueness among assignment vectors), and show that the variance of MSE is a linear function of this diagnostic for sets of designs satisfying symmetry conditions. Simulation studies (N=50 and N=100) illustrate that restricted/algorithmic designs can have modestly higher assignment correlation for well-behaved covariates but can produce extreme assignment correlation (and thus high MSE risk) when covariates are skewed or contain outliers, implying a practical need to monitor and possibly relax restrictions.","Design is a set of balanced assignment vectors $\mathcal W_H\subseteq\mathcal W$ satisfying the mirror property ($w\in\mathcal W_H\Rightarrow 1-w\in\mathcal W_H$). The estimator is the difference-in-means $\hat\tau_j=\frac{1}{N/2}\{w_j^\top Y(1)-(1-w_j)^\top Y(0)\}$ with design MSE $\mathrm{MSE}(\hat\tau\{\mathcal W_H\})=\frac{1}{H}\sum_{w_j\in\mathcal W_H}(\hat\tau_j-\tau)^2$. Pairwise uniqueness between assignments is $U_{j,j'}=\sum_{i=1}^N\mathbf 1[w_{ij}=1\wedge w_{ij'}=0]$, and the assignment correlation for a design is $\varphi(\mathcal W_H)=(\tfrac{4}{N})^2\sum_{u=1}^{N/2-1}\mu_u(\mathcal W_H)(u-N/4)^2$; under symmetry conditions, $\mathrm{Var}(
\mathrm{MSE})$ is linear in the average assignment correlation $\phi$ (Theorem 4).","Theorems 2–3 show monotone risk behavior: for smaller admissible set size $H$ (more restriction), the maximum attainable MSE over designs increases and the variance of design MSE decreases with $H$ only when moving toward larger $H$ (i.e., complete randomization yields zero variance of MSE across designs). Theorem 4 provides an explicit decomposition where $\mathrm{Var}({\mathrm{MSE}})$ is a linear function of the proposed assignment-correlation measure, separating data-dependent scaling ($\psi$) from purely combinatorial design features ($H$ and $\phi$). In simulations with $N=50$ and five covariates, rerandomization with $p_A\in\{0.1,0.01,0.001\}$ and a pair-switching algorithm yielded assignment correlations slightly above complete randomization for normal covariates (implying roughly 5–7% MSE increase per 1 SD in MSE), but for log-normal covariates/outliers, the pair-switching design produced extreme assignment correlation with tail behavior (reported maxima up to 0.1687 for $N=50$ and 0.2098 for $N=100$) corresponding to very large implied MSE variability (up to ~50–63% per 1 SD in their metric).","The key theoretical results are derived under a “veil of ignorance” framing where covariates carry no information about outcomes, so the expected MSE is the same across designs; the authors note that in practical applications covariates may be outcome-informative, creating a trade-off not captured directly by the veil-of-ignorance expectation. They also note that computing the assignment correlation exactly is $O(H^2)$ and may be infeasible for large $H$, requiring Monte Carlo approximation. They further acknowledge that a full decision-theoretic treatment of risk preferences and their implications is beyond the scope of the paper.","The main linear-variance result (Theorem 4) requires conditions on how designs are sampled/aggregated (conditions 1–2), which may not hold for specific deterministic or proprietary algorithmic design generators used in practice; the diagnostic’s calibration under such generators is not fully characterized. The simulations focus on a limited set of covariate distributions (normal vs. log-normal) and one algorithm (pair-switching), leaving uncertainty about behavior under autocorrelated covariates, high-dimensional covariates, or clustered/stratified populations common in field experiments. The work centers on the difference-in-means estimator under forced balance and does not directly address regression adjustment, heteroskedasticity-robust inference, or randomization-test power under the proposed diagnostic beyond citing related literature.","They suggest that a complete analysis of how experimenters’ risk preferences (risk-averse vs. risk-neutral vs. risk-loving) interact with assignment correlation and power—ideally in a decision-theoretic framework—is an interesting direction for future research. They also motivate practical algorithmic procedures for choosing how strict rerandomization should be by monitoring assignment correlation, implying further methodological development and guidance for implementation.","Develop formal guidelines for selecting rerandomization thresholds (e.g., $p_A$) that jointly optimize expected precision gains from covariate balance and control tail risk measured by assignment correlation, possibly via multi-objective or constrained optimization. Extend the combinatorial diagnostic to common complexities in experiments (unequal treatment fractions, multi-arm trials, clustered randomization, and stratified designs with many strata) and study how assignment correlation relates to randomization inference validity and power under those settings. Provide open-source software that computes/approximates assignment correlation for popular restricted-randomization algorithms and empirically benchmark it across datasets and design generators to establish practical cutoff values.",2006.14888v3,https://arxiv.org/pdf/2006.14888v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:44:51Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Computer experiment|Other,Prediction|Cost reduction|Other,Bayesian D-optimal|Other,"Variable/General (examples include 1D, 2D, and 7D input; hierarchical intermediate-variable settings)",Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python|https://gitlab.com/rozsasarpi/rprepo/,"The paper develops a sequential Bayesian optimal experimental design framework for structural reliability analysis, targeting efficient estimation of the failure probability $\mathbb{P}(g(X)\le 0)$ when evaluations of the performance function (often via expensive simulation/experiments) are costly. It formulates design decisions and experimental outcomes in a Bayesian decision-theoretic setting that explicitly separates aleatory uncertainty (in $X$) from epistemic uncertainty (in surrogate/hierarchical model components), and proposes myopic (one-step look-ahead) policies that minimize an expected residual-uncertainty loss. For tractable computation in hierarchical (non-Gaussian) settings, it introduces an approximation scheme combining importance sampling for rare-event (aleatory) estimation with the unscented transform for propagating epistemic uncertainty (UT-MCIS), and uses this to approximate acquisition functions. The method is demonstrated on several numerical experiments (including hierarchical benchmarks and a corroded pipeline case) showing that the sequential design can reach target uncertainty levels with relatively few expensive evaluations compared to baseline approaches. The work advances reliability DOE by extending sequential Bayesian design from standard GP surrogate settings to more general hierarchical reliability models and by providing a practical approximation strategy for acquisition optimization under rare-event probabilities.","Failure probability is defined as $\bar\alpha(g)=\mathbb{P}(g(X)\le 0)=\mathbb{E}[\mathbf{1}\{g(X)\le 0\}]$ and with epistemic uncertainty as $\alpha(\xi)=\mathbb{E}[\mathbf{1}\{\xi(X)\le 0\}\mid\mathcal{E}]$. Residual-uncertainty objectives include $H_{1,k}=\mathbb{E}[(\alpha-\bar\alpha)^2]$, $H_{2,k}=\mathbb{E}[\gamma_k(X)]$, and $H_{3,k}=(\mathbb{E}[\sqrt{\gamma_k(X)}])^2$ where $p_k(x)=\mathbb{P}_k(\xi(x)\le 0)$ and $\gamma_k(x)=p_k(x)(1-p_k(x))$. The myopic acquisition is $J_{i,k}(d)=\lambda(d)\,\mathbb{E}_{k,d}[H_{i,k+1}]$, approximated via UT over epistemic sigma points and (pruned) importance sampling over $X$ for rare-event estimation.","Across numerical experiments, the proposed sequential design reaches prescribed uncertainty thresholds (e.g., coefficient-of-variation criteria such as $\hat V_k\le 0.05$) with modest numbers of expensive evaluations. In the 1D illustrative example, convergence is reported after 3 iterations with an estimated true failure probability around 0.0234. In the 7D hierarchical benchmark (RP38), convergence is reached at iteration 10 using 2 evaluations of an expensive 1D subcomponent and 8 evaluations of an expensive 2D subcomponent, estimating a true failure probability around $8.1\times 10^{-3}$. On the 4-branch benchmark, the method reports intervals covering the Monte Carlo reference ($4.416\times 10^{-3}$) with 35–65 function calls depending on the stopping threshold, comparable to AK-MCS baselines reported in the literature. In the corroded pipeline case study, a decision-dependent stopping criterion relative to a target failure probability $10^{-3}$ is met after 25 iterations in the illustrated run.","The authors note that the myopic (one-step look-ahead) strategy can fail to reach certain stopping criteria in some settings (e.g., when an expensive experiment is required but has high cost), potentially leading to indefinite iterations. They also acknowledge that their unscented transform implementation uses a small, fixed set of sigma points and can tend to overestimate variance, which they observe in experiments. They emphasize that UT is used as a proxy to guide optimization rather than as an exact uncertainty estimator, and that final uncertainty can be re-estimated via more exact Monte Carlo once a strategy is chosen.","The acquisition optimization relies on several layered approximations (finite-dimensional epistemic representation, UT moment propagation, pruned IS for rare events), and the combined bias/variance impact on the selected design policy is not fully characterized theoretically. The paper’s reported computational burden (e.g., repeated model updates and acquisition evaluation over candidate decisions) may become substantial in high-dimensional decision spaces or with complex hierarchical surrogates, but scalability benchmarks are limited. Many demonstrations use GP surrogates with chosen priors and hyperparameter updates; practical sensitivity to prior misspecification, nonstationarity, or model discrepancy in surrogate modeling could affect robustness. Real-data validation is limited (pipeline example is synthetic), so field performance and implementation issues (e.g., constraints, non-Gaussian noise, model-form uncertainty) remain open.",The authors suggest exploring improved sigma-point selection methods for the unscented transform and possibly optimizing UT parameters. They propose studying multi-step look-ahead policies that include more terms from the dynamic programming objective (beyond myopic) and evaluating the tradeoff between improved decisions and increased computation time. They also mention extending the framework to buffered failure probabilities and developing heuristic objectives aligned with asymmetric risk preferences (penalizing underestimation of failure probability more than overestimation).,"Developing self-starting or online hyperparameter-learning strategies tightly integrated with the DOE objective (rather than ad hoc MLE after a few points) could improve stability and reduce dependence on conservative priors. Extending the framework to handle dependent/serially correlated aleatory inputs (time series loads) and constrained experimental regions (engineering feasibility constraints) would broaden applicability. Providing open-source reference implementations and standardized benchmark comparisons (runtime vs. accuracy) would facilitate adoption and clearer positioning against modern active-learning reliability methods. Theoretical guarantees (e.g., consistency of policy under approximation, or bounds relating UT-MCIS acquisition to true Bayes risk reduction) would strengthen methodological foundations.",2007.00402v1,https://arxiv.org/pdf/2007.00402v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:45:37Z
TRUE,Optimal design|Sequential/adaptive|Other,Robustness|Cost reduction|Other,Other,"Variable/General (unknown pairwise coupling strengths $a_{i,j}$ for all oscillator pairs; examples with $N=5$ and $N=9$ oscillators, i.e., $\binom{N}{2}$ uncertain couplings)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper proposes a general optimal experimental design (OED) strategy for uncertain dynamical systems modeled by coupled ODEs, instantiated on a non-homogeneous Kuramoto oscillator network with unknown, non-uniform pairwise coupling strengths. Uncertainty is represented as bounded intervals on couplings and a uniform prior over the resulting uncertainty class; the operational goal is robust frequency synchronization using an added control oscillator with minimal required coupling strength. The design criterion is objective-based uncertainty quantification via the mean objective cost of uncertainty (MOCU), and experiments are selected sequentially to minimize the expected remaining MOCU after each experiment. The experimental design space consists of pairwise synchronization tests whose binary outcomes shrink the feasible interval for a specific coupling parameter using a theoretical synchronization condition for two oscillators. Monte Carlo simulation (GPU-accelerated) shows the MOCU-based selection reduces uncertainty and control cost in fewer experiments than entropy-based or random selection, for examples with $N=5$ and $N=9$ oscillators.","The system is a coupled-ODE Kuramoto model $\dot\theta_i(t)=\omega_i+\sum_{j=1}^N a_{i,j}\sin(\theta_j-\theta_i)$ with uncertain couplings $a_{i,j}\in[a^{\ell}_{i,j},a^u_{i,j}]$ and a uniform prior over the uncertainty class. The robust control cost is defined via the minimum added-oscillator coupling $\xi(a)$ needed for synchronization and its robust counterpart $\xi^*(A)=\max_{a\in A}\xi(a)$; MOCU is $M(A)=\mathbb{E}_a[\xi^*(A)-\xi(a)]$. For an experiment on pair $(i,j)$ with binary outcome $B_{i,j}$, the expected remaining MOCU is $R(i,j)=\mathbb{E}_{B_{i,j}}[M(A\mid B_{i,j})]=\sum_{b\in\{0,1\}}\Pr(B_{i,j}=b)M(A\mid B_{i,j}=b)$ and the chosen experiment is $(i^*,j^*)=\arg\min_{i<j}R(i,j)$.","Using Monte Carlo estimation of MOCU with GPU acceleration (PyCUDA on an Nvidia GTX 1080-Ti), the MOCU-based sequential design reduced uncertainty faster than entropy-based and random selection in simulated Kuramoto networks. For $N=5$ oscillators, the MOCU-based scheme nearly reached the minimum attainable uncertainty level in about 3 experimental updates, while baselines required more updates. For $N=9$ oscillators (36 possible pairwise experiments), the method achieved a drastic reduction after a single update and reached the minimum uncertainty level in about 9 updates (considering up to 18 updates in the reported sequence). The paper also reports an example runtime of about 2.87 seconds per MOCU computation module for $N=5$, final time $T=4$, and Monte Carlo sample size $S=20{,}000$ (leveraging GPU parallelism).","The authors note that computing MOCU requires repeatedly solving the coupled ODEs with Monte Carlo sampling, making the approach computationally more expensive than simple measures like parameter entropy; they address this by GPU parallelization. They also restrict the experimental design space to pairwise synchronization tests (binary outcomes) rather than more general experiments, and use empirically determined correction constants $d_{i,j}$ in simulations to ensure nonsynchrony.","Performance is demonstrated only on simulated Kuramoto examples with a uniform prior over bounded parameter intervals; results may be sensitive to prior choice and to how the interval bounds/correction factors are set. The experimental outcome model is idealized (binary sync/no-sync based on asymptotic behavior), and practical measurement noise, finite observation windows, and model mismatch are not analyzed. The method’s scalability to much larger $N$ may be limited because evaluating $R(i,j)$ for many candidate experiments requires repeated MOCU computations (nested Monte Carlo/ODE solves), and the paper does not provide complexity or wall-time scaling for the full experiment-selection loop. No public code is provided, which may hinder reproducibility of the GPU implementation and parameter settings.","They propose expanding the experimental design space to include experiments whose outcomes are not directly usable to measure uncertain parameters or to shrink uncertainty via analytic theorems, and suggest that a Bayesian approach would be effective for this inverse problem. They state they are investigating Bayesian inversion strategies to map experimental outcomes into reductions of objective model uncertainty, and note the framework can be adapted beyond Kuramoto models to other uncertain ODE systems.","Developing noisy/finite-time versions of the pairwise synchronization experiment and robustness of the selection rule under observation error would improve practical applicability. Extending the approach to non-uniform or learned priors (e.g., hierarchical/Bayesian model averaging) and to partially observed or structurally uncertain networks (unknown edges) would broaden scope. More efficient surrogates or variance-reduction methods for estimating MOCU and $R(i,j)$ could reduce computation, enabling larger networks and richer experiment spaces. Providing an open-source implementation and standardized benchmarks against other OED criteria (e.g., Bayesian D-optimality, information gain) would strengthen reproducibility and comparative evaluation.",2007.06117v2,https://arxiv.org/pdf/2007.06117v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:46:15Z
TRUE,Split-plot|Other,Parameter estimation|Robustness|Cost reduction,Not applicable,Variable/General (examples include 1–2 treatment variables plus multiple control variables; batches/blocks sized by group counts and processing constraints),Healthcare/medical|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a methodological guidance piece on designing proteomics (LC–MS) experiments to avoid confounding by unanticipated and anticipated nuisance factors. It contrasts ordered allocation, complete randomisation, and block randomisation, showing (via illustrative/simulated examples) how block randomisation better distributes sequential effects such as machine drift and prevents severe imbalances where treatment becomes confounded with batch. It extends blocking to scenarios with unequal group sizes, multiple batches, and multiple treatment variables (including interactions), emphasizing that batches should be internally balanced so each batch behaves like a “small experiment.” The authors also discuss practical considerations relevant to proteomics workflows (multi-step processing constraints, keeping consistent batch membership across steps, and optional use of common reference samples) to reduce downstream modeling complexity and improve interpretability and reproducibility.",Not applicable (the article describes block randomisation procedures conceptually; no defining control-limit/optimality equations are central).,"No formal quantitative performance tables (e.g., power/variance/ARL) are reported; the main results are qualitative illustrations showing that (i) ordered allocation can mask or exaggerate treatment differences under machine drift, (ii) complete randomisation can still yield severely unbalanced allocations, and (iii) block randomisation maintains treatment representation over run order and across batches so treatment effects remain estimable within batches despite batch effects.","The authors state that power considerations are beyond the scope of the article and emphasize that an adequate number of samples is paramount for proper design and for answering the research questions. They also note that reference-sample strategies pose challenges (e.g., missing values and dynamic range) that are beyond the scope of the article.","The paper provides conceptual guidance and illustrative simulations but does not present a systematic empirical evaluation (e.g., across many drift/batch-effect scenarios) or formal optimality/efficiency comparisons against alternative constrained randomisation algorithms. It does not provide a concrete, reproducible algorithmic recipe (or software) for constructing block randomisations under complex real constraints (e.g., many covariates, limited batch sizes, hard-to-change factors), which can limit immediate adoption. Guidance is focused on balancing/estimability and interpretability; it does not address robustness to autocorrelation, missingness mechanisms, or downstream normalization/modeling choices that interact strongly with batch/blocking in proteomics.",None stated.,"Provide open-source tooling that generates constrained/block randomisation plans for proteomics workflows (multiple steps with different capacity constraints, partial nesting, and hard-to-change factors) while reporting balance diagnostics. Extend the discussion to formal design criteria (e.g., variance of treatment contrasts, estimability, and power under mixed models) and evaluate competing allocation strategies via large simulation studies and real multi-batch proteomics datasets. Develop practical recommendations for integrating block randomisation with reference-sample strategies (and with common proteomics normalization pipelines) under realistic missing-value and drift conditions.",2007.06336v1,https://arxiv.org/pdf/2007.06336v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:46:41Z
TRUE,Sequential/adaptive|Other,Prediction|Other,Not applicable,Not specified,Environmental monitoring,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper studies experimental design choices for evaluating machine-learning models in a large-scale computer-assisted labeling task for bathymetry (binary “good” vs “bad” echo-sounder measurements). It shows that the standard random i.i.d. train/test split can yield overly optimistic performance because sequential labeling creates dependence between neighboring measurements (violating independence), and because data from different geographic/organizational sources have markedly different distributions (violating identical distribution). The authors propose alternative evaluation designs that split data into larger, contiguous subsequences—either whole cruises or fixed-length segments of 100,000 measurements—assigned to train vs test, which reduces leakage and yields more realistic AUROC estimates. They also demonstrate strong domain shift across 17 regions: models trained on one region can perform near chance on another, and region-matched training improves AUROC over training on pooled global data. Overall, the work reframes “experimental design” here as how to partition/structure data for model training/testing under non-IID conditions, emphasizing sequence-aware and source-aware evaluation to better predict future deployment performance.","The method uses boosted decision trees (LightGBM) and a confidence score based on the boosting margin (normalized margin) as in Schapire et al. (1998), but the paper’s main design contribution is not a new statistical DOE formula; it is the definition of alternative data-partitioning schemes for evaluation: (i) random split by individual measurements, (ii) split by whole cruises, and (iii) split by non-overlapping contiguous segments of length 100,000 measurements assigned to train/test. Performance is summarized primarily by AUROC computed on the resulting test partitions.","With a random split by individual measurements, test AUROC is reported as 0.9975 (and ~0.997 in Figure 2), but performance drops when evaluated on unseen cruises. When splitting by whole cruises, AUROC drops to about 0.883, and when splitting by 100,000-measurement segments it is about 0.943 (Figure 2), indicating the random split was overly optimistic due to sequential dependence. Cross-region evaluation (17 regions) shows severe distribution shift: some train-region/test-region pairs achieve AUROC around 0.5 or lower (near chance). Training on region-matched data outperforms training on all pooled data; reported AUROC improvements for several regions range from about 0.73% up to 6.94% (Table 1).",None stated.,"The paper frames “experimental design” as train/test partitioning rather than classical DOE (factor-level run planning), so the design guidance may not transfer to controlled physical experiments. Evaluation focuses on AUROC without reporting other operational metrics (e.g., precision/recall at actionable thresholds, cost of false removals) that matter for assisted editing. Details needed for full reproducibility (exact feature set, hyperparameters, how margins are computed/normalized, and sampling strategy per region) are limited, and there is no shared code.",None stated.,"Develop principled source-aware or hierarchical modeling strategies (e.g., domain adaptation, mixture-of-experts by region/ship, or Bayesian hierarchical models) and compare them under the proposed split protocols. Extend evaluation to additional non-IID structures common in geospatial/temporal data (autocorrelation, spatial blocking, concept drift over time) and to multivariate/structured outputs (e.g., cruise-level quality flags). Provide an open, reproducible benchmark with standardized splits (cruise-blocked and region-held-out) plus released code to facilitate fair comparisons and ablations.",2007.07495v1,https://arxiv.org/pdf/2007.07495v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:47:12Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Parameter estimation|Cost reduction|Other,A-optimal|D-optimal|Other,"Variable/General (sensor-location design variables; examples include ns=43 candidate sensors, nt=6 observation times in experiments)",Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://doi.org/10.1137/130933381|https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.optimize.fmin_l_bfgs_b.html|http://hippylib.github.io|http://energy.gov/downloads/doe-public-access-plan,"The paper develops an optimal experimental design (OED) framework for Bayesian linear inverse problems when observation errors are correlated in space and/or time, targeting sensor placement/data acquisition under budget constraints. It proposes a pointwise covariance-weighting approach using a Hadamard (Schur) product between the observation-error covariance matrix and a symmetric weighting kernel built from the design variables, yielding a weighted likelihood and corresponding posterior covariance for goal-oriented quantities of interest. The work analyzes why common relaxations that weight the observation precision matrix can be inconsistent/discontinuous with correlated observations, and derives gradients for A- and D-optimal criteria under the new formulation, including versions for purely spatial correlations (block-diagonal in time) and full spatiotemporal correlations. It also discusses relaxation/regularization (e.g., sparsity via ℓ1) and interprets how relaxed designs affect variances/covariances, and shows how sigmoid parametrizations can convert the constrained design problem to an unconstrained one. Numerical experiments on an advection–diffusion contaminant-transport model demonstrate improved sensor-selection performance (better inverse-problem accuracy) when correlations are properly accounted for, compared with the traditional precision-weighting approach that ignores or mishandles correlations.","The forward/observation model is linear-Gaussian: $y=F\theta+\delta$, $\delta\sim\mathcal N(0,\Gamma_{\text{noise}})$, with posterior covariance $\Gamma_{\text{post}}=(F^*\Gamma_{\text{noise}}^{-1}F+\Gamma_{\text{pr}}^{-1})^{-1}$ and goal posterior $\Sigma_{\text{post}}=P\Gamma_{\text{post}}P^*$. The proposed design enters via a weighted precision built from a Schur product on the covariance: $W_\Gamma(\zeta):=(\Gamma_{\text{noise}}\,\circ\,W(\zeta))^{\dagger}$, leading to $\Sigma_{\text{post}}(\zeta)=P\,(F^*W_\Gamma(\zeta)F+\Gamma_{\text{pr}}^{-1})^{-1}P^*$. A- and D-optimal criteria are $\mathrm{Tr}(\Sigma_{\text{post}}(\zeta))$ and $\log\det(\Sigma_{\text{post}}(\zeta))$, with derived gradients using $\partial W_\Gamma/\partial \zeta_i$ for both spatial-only and spatiotemporal-correlation cases.","On a PDE advection–diffusion sensor-placement problem (ns=43 candidate sensors, nt=6 times) with synthetically correlated observation errors, the Schur-product OED produces different (more spatially spread) sensor selections than the standard precision-weighting relaxation and yields better inverse-solution accuracy when correlations are present. For spatial correlation length scale $\ell=1$, rounded designs with 8 sensors achieved RMSEs as low as about 0.016–0.024 for Schur-OED variants versus about 0.044–0.052 for the standard approach (depending on regularization). For stronger correlations ($\ell=3$), Schur-OED with 8 sensors achieved RMSEs down to about 0.0076–0.013 (best cases), while the standard approach could be as high as ~0.05 for 8 sensors; performance improves as more sensors are activated. The paper also reports using a Hutchinson trace estimator (nr=25) to approximate A-optimality efficiently and shows accurate trace estimation empirically.","The authors note that the approach relies on differentiability with respect to the design (including the regularization term), and they therefore use differentiable sparsity surrogates such as an \(\ell_1\) penalty rather than the nondifferentiable \(\ell_0\) penalty. They also state that numerical experiments focus on A-optimality with spatial correlations; empirical study of D-optimal designs and spatiotemporal correlation cases is left for future work. They mention that approaches avoiding relaxation/nondifferentiability (e.g., stochastic learning for binary optimization) are outside this paper’s scope.","The method is developed and evaluated primarily for linear-Gaussian inverse problems; performance and gradient validity for nonlinear forward models (where posterior covariances depend on data) are not addressed and may require additional approximations. Empirical validation is limited to a single PDE testbed with synthetic covariance models; real-world data or robustness to covariance misspecification (beyond optional localization ideas) is not extensively quantified. The approach uses pseudoinverses and kernels with terms that can blow up as weights approach zero, which may create numerical stability/conditioning challenges in large-scale implementations beyond the brief mitigation discussion. Comparison is mainly against a particular “standard” relaxation; broader benchmarking against alternative correlated-noise sensor-selection methods (e.g., greedy/exchange algorithms or other optimal design criteria/robust designs) is limited.","The authors call for extensions and empirical studies of D-optimal designs in the presence of correlations in both space and time, and further investigation of spatiotemporal-correlation scenarios. They also suggest comparing the quality and computational cost of various OED formulations and solution approaches (including greedy/exchange methods) in future work. They note that randomized approximation of D-optimality is deferred to future work.","Develop and test robust/self-starting variants that explicitly handle unknown noise covariance parameters (jointly estimating correlation structure and design) and quantify sensitivity to covariance misspecification. Extend the Schur-weighting framework to nonlinear and/or non-Gaussian inverse problems (e.g., Laplace approximations, ensemble-based approximations) and to adaptive/sequential design where sensors/times are selected online. Provide open-source reference implementations (e.g., HIPPYlib/Python examples) and scalability studies on larger 3D PDEs with realistic spatiotemporal noise models. Explore other criteria (I-, G-, or Bayesian risk-based prediction loss) and multi-objective designs (cost, robustness, detectability) for correlated-observation settings.",2007.14476v3,https://arxiv.org/pdf/2007.14476v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:48:10Z
TRUE,Optimal design|Computer experiment|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,D-optimal|Space-filling|Minimax/Maximin|Other,"Variable/General (physical inputs: p; calibration parameters: q; examples include p=2,q=1 and p=5,q=3; also discusses computer experiments in p+q dimensions)",Manufacturing (general)|Other,Simulation study|Other,TRUE,R,Not provided,https://cran.r-project.org/web/packages/MaxPro|https://cran.r-project.org/web/packages/randtoolbox|https://cran.r-project.org/src/contrib/Archive/support,"The paper proposes a robust optimal design strategy for planning physical experiments used to calibrate computer models with unknown calibration parameters, explicitly accounting for potential model discrepancy between the computer model and the real system. The design is constructed by combining (i) locally D-optimal points for estimating calibration parameters, (ii) additional points at the computer-model maximum and minimum to estimate a location–scale bias correction, and (iii) an augmenting space-filling design (illustrated with MaxPro) to support discrepancy estimation/validation. Parameter uncertainty is handled via (pseudo-)Bayesian design ideas: sampling calibration parameters from a prior and reducing the resulting set of local D-optimal designs using support points to obtain a robust set of runs. The approach is extended to the case where the computer model is expensive and replaced by a Gaussian-process surrogate, propagating emulator uncertainty into the design by sampling GP realizations and recomputing local designs. Performance is demonstrated via simulations on a toy example with increasing discrepancy and via an industrial case study (P&G diaper line), showing improved predictive accuracy under model discrepancy relative to “pure computer-model” D-optimal-only designs and standard factorial/fractional-factorial alternatives.","The physical system is modeled as $y=f(x;\eta)+\delta(x)+\epsilon$ with $\epsilon\sim N(0,\sigma^2)$. Calibration-only local D-optimal designs maximize $|J_0|$ where $J_0$ is the sensitivity matrix with entries $\partial f(x_i;\eta)/\partial \eta_j$ evaluated at a guessed (or sampled) $\eta$. Robustness to discrepancy is introduced by augmenting D-optimal calibration points with space-filling points (e.g., MaxPro) and optionally adding $x_{\max}=\arg\max_{x\in\mathcal X} f(x;\eta_0)$ and $x_{\min}=\arg\min_{x\in\mathcal X} f(x;\eta_0)$ to estimate location/scale bias in $y=f(x;\eta)+\beta_0+\beta_1 f(x;\eta)+\delta(x)+\epsilon$.","In the toy-example simulation study, as discrepancy magnitude increases (controlled by $\tau^2$ in $\delta(x)\sim GP(0,\tau^2R)$), the proposed hybrid (D-optimal + max/min + space-filling augmentation) design yields lower RMSPE than both a full-factorial alternative and a “pure computer model” design that allocates all runs to D-optimal calibration points; the gap widens with larger $\tau^2$ (Figure 8). In the P&G example with no added discrepancy, the pure computer-model design performs best (as expected), while the proposed design is comparable to a replicated fractional factorial. When a GP discrepancy with $\tau^2=6$ is added, the proposed design shows the smallest absolute prediction error distribution among the compared designs, while the pure computer-model design degrades substantially due to lack of space-filling discrepancy-estimation points.",None stated.,"The method relies on specifying priors (or guessed values) for calibration parameters and on choosing tuning parameters such as the replication/confidence parameter $r$, the number of prior samples/support points $m$, and the specific space-filling criterion; guidance is largely heuristic and performance may be sensitive to these choices. The approach for discrepancy robustness is tied to a particular sequential calibration-then-discrepancy modeling strategy (fixing/anchoring $\eta$) and may be less effective when calibration and discrepancy are strongly confounded or when discrepancy structure deviates from the assumed modeling (e.g., nonstationary, heteroskedastic, or correlated noise). Comparisons are limited to a small set of competing designs (e.g., factorial/FFD and pure D-optimal), and do not benchmark against other robust/Bayesian optimal design criteria (e.g., integrated MSPE/I-optimality) across a broad range of scenarios and constraints (e.g., discrete factor levels, hard-to-change factors).","The authors propose investigating the idea of extracting additional “features” from the computer model (e.g., local maxima/minima beyond global extremes) and adding them to the design even when the connection to an explicit discrepancy model is not evident.","Develop decision rules (or optimization) for selecting $r$ and allocating runs between calibration-focused points and discrepancy-focused space-filling points under explicit utility/cost criteria and practical constraints (e.g., discrete settings, split-plot). Extend the framework to autocorrelated/heteroskedastic measurement errors and to nonstationary or structured discrepancies, including multivariate outputs and multivariate calibration parameters. Provide open-source, end-to-end software implementing the full robust-design pipeline (including surrogate uncertainty propagation) and broader empirical benchmarks against alternative Bayesian and robust optimal design criteria.",2008.00547v1,https://arxiv.org/pdf/2008.00547v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:48:45Z
TRUE,Optimal design|Bayesian design|Computer experiment|Response surface|Sequential/adaptive,Parameter estimation|Prediction|Optimization|Cost reduction,Bayesian D-optimal,Variable/General (examples include 1 design variable in Test Case 1; 2ns design variables for well locations with ns=1 or 2 in Test Case 2; model-parameter dimension includes 5 KL coefficients in Test Case 2),Energy/utilities|Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://github.com/ahmed-h-elsheikh/polychaos-learn,"The paper develops an efficient optimal Bayesian experimental design (OBED) method for subsurface flow problems governed by expensive PDE simulations. The design criterion is expected information gain (expected KL divergence), corresponding to a Bayesian D-optimality objective for parameter inference and improved predictive accuracy. The key contribution is a polynomial chaos expansion (PCE) surrogate constructed directly for the utility function by exploiting PCE orthogonality to replace the integral over possible observations with a projection, drastically reducing the number of expensive utility evaluations. The approach is validated on two 1D toy models (smooth vs non-smooth) and then applied to a two-phase reservoir waterflooding setup where the design variables are locations of new measurement wells and the uncertain parameters are low-dimensional KL coefficients of a log-permeability perturbation field. Numerical results show that the PCE-based utility surface captures the maximizers of expected information gain and produces interpretable well-placement guidance (higher utility where permeability/pressure variance is larger), while reducing computational cost relative to nested-MCMC-based utility evaluation.","The Bayesian design utility is the expected information gain: $U(d)=\int D_{\mathrm{KL}}(m,d)\,p(m\mid d)\,dm$ with $D_{\mathrm{KL}}(m,d)=\int p(\theta\mid m,d)\log\frac{p(\theta\mid m,d)}{p(\theta\mid d)}\,d\theta$. The paper defines $G(\theta,\eta,d)=D_{\mathrm{KL}}(f(\theta,d)+\eta,d)$ and builds a PCE $G=\sum_{A,B,\Gamma} c_{AB\Gamma}\,p_A(\theta)q_B(\eta)r_\Gamma(d)$; by orthogonality the utility becomes the projection $U(d)=\sum_{\Gamma} c_{00\Gamma}\,r_\Gamma(d)$. The coefficients are estimated via regularized least squares (Elastic Net) over samples $(\theta_i,\eta_i,d_i)$: minimize $\frac1N\sum_i\left(G(\theta_i,\eta_i,d_i)-\sum_\Gamma h_\Gamma r_\Gamma(d_i)\right)^2+\lambda_1\|h\|_1+\lambda_2\|h\|_2^2$.","In Test Case 1 (1D design $d\in[0,1]$), the PCE surrogate closely matches reference utility curves for small noise levels and accurately recovers the design value maximizing $U(d)$ for both smooth and non-smooth forward models, though it smooths sharp derivative discontinuities in the non-smooth case. In the subsurface flow test case, a PCE surrogate for permeability perturbation and normalized pressure is trained on 4,000 PDE simulations (with 1,000 for validation), achieving about ~3% validation error for the surrogate used within the design loop. Utility response surfaces for placing 1 or 2 additional measurement wells show maxima near domain corners far from injector/producer locations, and minima when wells are colocated or near existing wells; adding more pressure measurement times changes the utility peak structure (from two peaks to one dominant peak) consistent with spatial variance patterns of pressure and permeability perturbations. The utility surface $U(d)$ is approximated using large sample sets (e.g., 200,000 design samples; KL divergence computed with MCMC chains of length ~50,000) and then fit with Legendre-polynomial PCE truncated to total degree 5 in the design variables.","The authors note that PCE-based OBED works best for problems where the utility function is smooth; for non-smooth cases (e.g., discontinuous derivatives), the PCE response surface may fail to reproduce derivative discontinuities due to smooth polynomial bases, though the maximizer of $U(d)$ can still be well approximated. They also emphasize the high computational cost of direct Bayesian design for PDE models due to nested high-dimensional integrals, motivating surrogates and projection-based reformulation.","The workflow still relies on expensive MCMC-based KL-divergence evaluations to generate training data for the utility surrogate (e.g., long chains and many sampled designs), which may remain prohibitive for higher-dimensional design spaces or more complex posterior geometries. The method assumes a Gaussian likelihood with a single shared noise standard deviation across heterogeneous observation types (permeability perturbation and pressure), which may be unrealistic and can affect optimal design recommendations. Results are primarily demonstrated on synthetic/numerical case studies; broader validation on real field datasets or robustness to model misspecification (e.g., wrong priors/correlation structure, model error beyond additive Gaussian noise) is not established.","The authors suggest further development of numerical integration techniques based on PCE in the context of Bayesian experimental design, noting that many practical design problems have moderate-dimensional design variables and commonly assume Gaussian noise, making PCE-based utility surrogates promising for wider engineering applications.","Extending the approach to handle non-Gaussian and heteroscedastic observation error models (and mixed data types) would improve realism for subsurface monitoring designs. Developing adaptive/sequential design strategies (e.g., active learning for choosing where to evaluate KL divergence next) could reduce the number of expensive posterior computations needed to fit the utility surrogate. Providing open-source, end-to-end reference implementations (including the reservoir simulator coupling and design optimization) and benchmarking against alternative OBED approximations (e.g., Laplace/variational, multilevel Monte Carlo EIG bounds) would strengthen reproducibility and clarify regimes where the method is most cost-effective.",2008.03989v1,https://arxiv.org/pdf/2008.03989v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:49:34Z
FALSE,Other,Prediction|Other,Not applicable,Variable/General (geometric parameters; examples with 1–2 parameters),Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper compares two reduced-order modeling strategies—an a priori Proper Generalized Decomposition (PGD) method and an a posteriori (least-squares) PGD method—for building separated response surfaces of incompressible Stokes flow solutions in geometrically parameterized domains. The full-order model uses a multidimensional hybridizable discontinuous Galerkin (HDG) solver formulated on a parameter-independent reference domain with a separable geometric mapping, enabling separated representations needed by PGD. Numerical experiments on a parameterized microswimmer geometry (radius and inter-sphere distance parameters, tested separately and jointly, with small and large parameter ranges) quantify accuracy for velocity, pressure, velocity-gradient and for a drag-force response surface. Performance is compared mainly via accuracy versus computational cost measured by the number of full-order HDG solves: a posteriori PGD requires many snapshots (parallelizable) while a priori PGD requires sequential spatial solves within an alternating-direction scheme. Results show similar performance for mild variability, but for larger parameter ranges and multiple geometric parameters the a priori PGD achieves comparable drag accuracy with substantially fewer full-order solves, while noting that improved sampling could reduce the snapshot burden in the a posteriori approach.","The PGD separated approximation expresses each unknown as a sum of rank-one modes, e.g. $u_{\text{PGD}}^{m}(x,\mu)=\sigma_u^{m} f_u^{m}(x)\,\psi^{m}(\mu)+u_{\text{PGD}}^{m-1}(x,\mu)$ with factorized parametric dependence $\psi^{m}(\mu)=\prod_{j=1}^{n_{pa}}\psi^{m}_{j}(\mu_j)$. The a posteriori PGD computes modes by solving a least-squares tensor approximation problem $\min\|G-G_{\text{PGD}}^{m-1}-\sigma_U^{m}\tilde f_U^{m}\otimes \tilde\psi^{m}_1\otimes\cdots\otimes\tilde\psi^{m}_{n_{pa}}\|^2$. Response surfaces for quantities of interest (e.g. drag) inherit separability, $F_D^{m}(\mu)=\sum_{j=1}^{m} D_j\,\psi^{j}(\mu)$, where $D_j$ is computed from the $j$-th spatial modes of pressure and velocity gradient.","For one-parameter cases with limited variability (e.g., radius variation), the a posteriori PGD attains very low errors with few modes but requires enough snapshots (e.g., tens) to push accuracy to $\sim 10^{-4}$, while the a priori method needs more modes and multiple AD iterations but can reach similar drag accuracy with a comparable number of full-order solves. For distance variation on a large interval ($I_2=[-3,2]$), the a priori PGD reaches drag error below $10^{-5}$ with roughly tens of spatial solves, whereas the a posteriori PGD needs on the order of a few hundred snapshots to reach similar accuracy (though snapshots are parallelizable). In two-parameter problems, to reach drag error below $10^{-3}$ the a priori approach required far fewer full-order solves (reported up to about 11× fewer calls to the HDG solver) than the basic a posteriori snapshot-based PGD at the tested snapshot grids. Error maps show that for the harder two-parameter/large-range case, the a priori approach can have more localized boundary-region errors even when global $L^2(I)$ errors are similar.","The authors note that the comparison is performed using “similar versions” of a priori and a posteriori ROMs without targeted sampling or error-control strategies, to keep the study unbiased; thus the a posteriori approach is not using advanced sampling that could reduce snapshots. They also state that the a priori PGD standard implementation is generally intrusive with respect to the spatial solver (requiring code access), while the a posteriori approach is non-intrusive. They further caution that extreme geometric mappings can degrade Jacobians and quadrature requirements, so mesh-quality studies are advisable for limiting geometric configurations.","The study’s a posteriori approach uses snapshot locations aligned with the a priori parametric discretization (Fekete nodes), which is not necessarily representative of best-practice adaptive/greedy sampling for RB/ROM and may disadvantage the a posteriori method beyond what is acknowledged. Results are demonstrated on a specific (Stokes, low-Re) microswimmer benchmark; conclusions may not generalize to nonlinear, time-dependent, or turbulent CFD where hyper-reduction and stability issues are central. Practical implementation effort and wall-clock time are not quantified (only number of full-order solves), which can obscure tradeoffs when parallel resources are available or when HDG solve costs vary with parameter-dependent mesh quality.","The paper suggests further comparisons for problems with more than two geometric parameters and for a posteriori algorithms equipped with tailored/advanced sampling techniques, to better understand applicability and limitations of PGD-based strategies in industrial parametric studies. It also points to ongoing research needs in a priori ROMs for advanced parameter-space treatment and embedded accuracy/error control.","A natural extension is to benchmark against other ROM families (e.g., reduced basis with certified error bounds, POD-Galerkin with hyper-reduction, neural operators) under the same geometric-parameter settings and computational budget. Developing and testing non-intrusive a priori PGD variants and robust error estimators for quantities of interest (e.g., drag) in multi-parameter geometric settings would improve industrial usability. Additional studies with non-affine/less easily separable geometric mappings and with uncertainty/noise in parameters would clarify robustness, and reporting parallel wall-clock scaling would make the cost comparison more operational.",2009.02176v2,https://arxiv.org/pdf/2009.02176v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:50:23Z
TRUE,Sequential/adaptive|Optimal design|Other,Parameter estimation|Prediction|Other,Other,Variable/General (simulations use d=10 covariates; theory covers d≥1 and d=1 cases),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/myphan9/Designing_Transportable_Experiments,"The paper studies how to design randomized experiments on a source population to estimate the Average Treatment Effect (ATE) for a different target population under covariate shift, assuming S-admissibility (equal outcome model conditional on covariates across source/target) and overlap. It proposes an unbiased importance-weighted ATE estimator for the target and a rerandomization-based design criterion, Target Balance, which accepts treatment assignments only when the importance-weighted covariate mean difference between treatment and control is small relative to the target distribution. Target Balance is implemented via a weighted Mahalanobis-distance criterion on the weighted covariates (wx), and the acceptance threshold is set to achieve a prespecified rejection probability α. The authors provide theory showing unbiasedness and variance reduction relative to complete randomization; for d=1 they prove optimal variance reduction among symmetric balance criteria with rejection probability ≤α, and for d≥1 they show asymptotic variance reduction and improvement over standard (source-based) balance. Simulations on linear and nonlinear outcome models with multivariate normal covariates demonstrate lower variance/MSE for the importance-weighted estimator paired with Target Balance, including for finite samples and with optional weight clipping when source/target differ substantially.","Target estimand: $\tau_Y^T = \mathbb{E}_T[Y^{A=1}-Y^{A=0}]$. Importance-weighted estimator: $\hat\tau_Y^T=\frac{1}{n_1}\sum_{A_i=1} W_i Y_i^*-\frac{1}{n_0}\sum_{A_i=0} W_i Y_i^*$ with $W_i=p_T(X_i)/p_S(X_i)$. Target Balance rerandomization accepts assignments when the weighted Mahalanobis distance of the importance-weighted covariate mean difference is small: $\phi_T(x,Z)=\mathbb{I}\{M(\frac{2}{n}(wx)^T Z) < a(x)\}$ where $M(U)=U^T\mathrm{Cov}(U)^{-1}U$ and $a(x)$ is chosen so $P(\phi_T=1\mid x)=1-\alpha$.","Theorem 1 shows the importance-weighted estimator remains unbiased for the target ATE even under rerandomization using Target Balance (with equal treatment/control sizes). For d=1 and finite n, Target Balance reduces conditional variance versus complete randomization, and Theorem 2 establishes optimal variance reduction among all symmetric balance rules with rejection probability at most α. For d≥1, asymptotic results (via rerandomization theory) show both Source Balance and Target Balance reduce variance, but Target Balance yields a lower asymptotic variance than Source Balance for the same rejection probability (Theorem 4). In simulations (linear and nonlinear models; typically d=10), the weighted estimator with Target Balance achieves the lowest variance/MSE across examined sample sizes and across source–target distances when weight clipping is not overly aggressive.","The authors assume the density ratio $p_T(X)/p_S(X)$ is known (Assumption 4) and note this is plausible in nested trial designs with known sampling probabilities; they conjecture similar results when importance weights are estimated with parametric rates and state this as an extension left for future work. They also note that weight clipping can reduce variance but induces bias, affecting MSE depending on the source–target distance and clipping threshold.","The approach relies on strong transportability assumptions (notably $p_S(Y\mid X)=p_T(Y\mid X)$) and sufficient overlap; violations (unmeasured effect modifiers, limited overlap) can lead to bias or extreme weights, and rerandomization cannot fix lack of support. Practical implementation depends on computing/estimating weights and covariance matrices for the weighted Mahalanobis metric; in high dimensions or with collinearity, covariance estimation and acceptance rates can become problematic. The paper’s experimental evaluation is simulation-based with Gaussian covariates/outcomes and does not include real-world case studies demonstrating operational feasibility (e.g., recruitment constraints, interference, noncompliance). Comparisons focus on complete randomization and rerandomization balance variants; other transportability estimators (e.g., doubly robust, outcome-model-based) and alternative design strategies (e.g., targeted recruitment to improve overlap) are not benchmarked end-to-end.","They propose extending the theory to the case where importance weights are estimated rather than known, specifically when estimated weights achieve parametric convergence rates. They also note, in the multivariate finite-sample discussion, that designing balance rules to minimize worst-case variance (e.g., by controlling the largest eigenvalue of the covariance term) is left for future work.","Develop self-starting/robust versions that handle unknown or estimated density ratios with regularization and uncertainty propagation, and study how rerandomization interacts with weight estimation error. Extend the design to high-dimensional settings (d large) using shrinkage covariance estimators or kernel/representation-based balance criteria tied to target prediction risk. Add empirical validation on real transportability problems (e.g., clinical trial generalization, online experiments with heavy-user bias) including sensitivity analyses for violations of S-admissibility and overlap. Explore joint design over both assignment and sampling/recruitment (actively selecting source units to maximize target overlap) and adaptive/sequential rerandomization policies under cost or recruitment constraints.",2009.03860v4,https://arxiv.org/pdf/2009.03860v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:51:01Z
TRUE,Optimal design|Other,Parameter estimation,Other,"1 controlled factor (outlet size φ); 2 unknown parameters (C, L)",Manufacturing (general)|Food/agriculture|Other,Simulation study|Other,TRUE,Python|Other,Supplementary material (Journal/Publisher),NA,"The paper develops local c-optimal experimental designs for a two-dimensional silo discharge problem, where the inter-jam time is modeled as exponential with mean η(φ;θ)=C^{-1}(exp(Lφ^2)−1) and θ=(C,L). The design goal is precise estimation of the minimum outlet size φ that ensures the expected time between jams exceeds a fixed threshold T0, which becomes estimating a nonlinear transformation g(θ)=sqrt(log(C(T0+1))/L). The authors obtain c-optimal designs using Elfving’s graphical method applied to a Fisher-information-based linearization of both the nonlinear model and the target transformation, yielding explicit two-point (and sometimes one-point) optimal designs supported at the domain extremes. They characterize when the gradient direction intersects different edges of the Elfving convex hull, partitioning T0 into intervals that determine the optimal support/weights, and provide a sensitivity analysis showing efficiency losses when nominal parameters are misspecified enough to change the intersected edge. A simulation study (implemented in Python) checks the accuracy of the Fisher information and linearization approximations by comparing approximated and empirical variances of the MLE of g(θ).","The mean model is η(φ;θ)=\frac{1}{C}(e^{Lφ^{2}}−1) with θ=(C,L), and the design target is g(θ)=\sqrt{\frac{\log(C(T_0+1))}{L}} (minimum outlet size meeting E[T|φ]≥T0). The Fisher information at a point is I(φ,θ)=\frac{1}{η(φ,θ)^2}\nabla η(φ,θ)\nabla η(φ,θ)^T, giving M(ξ,θ)=\int I(φ,θ)dξ(φ). A local c-optimal design minimizes c^T M(ξ,θ)^{-1} c with c=\nabla g(θ), and is found via Elfving’s method; for this model the optimal design is typically two-point at φ=a and φ=b with weight p determined by the intersection of the line through c with the Elfving convex hull.","For the domain X=[1.53,5.63] and nominal values C0=0.671741, L0=0.373098, the c-optimal design for T0=200 is two-point at {1.53, 5.63} with weights {0.5526, 0.4474}. The authors show the convex hull structure implies the optimal support is at the extremes, and T0 falls into intervals that determine whether the c-vector intersects edge A1A2, A2A3, or A3A4, with vertex intersections yielding one-point optimal designs. Sensitivity plots show substantial efficiency drops when (C*,L*) perturbations cause the intersection to move to a different hull edge; efficiency is more stable when C and L vary in the same direction. In simulations (e.g., n=1000 per run, m=1000 repetitions), approximated variances using the linearized/FIM approach closely match empirical MLE variances for moderate/large T0, while convergence is slower and discrepancies larger when T0 is near its lower feasibility bound.","The authors note that the derived c-optimal designs are local and depend on nominal parameter values because both the Fisher information and the gradient c=∇g(θ) depend on θ. They also emphasize that their solution relies on two linearization/approximation steps (Fisher information approximation and Taylor linearization of g), and therefore validate accuracy via simulation, observing slower convergence and poorer approximation when T0 is close to its lower bound.","The approach assumes independent exponential waiting times with a specific mean link η(φ;θ)=C^{-1}(e^{Lφ^2}−1); deviations such as overdispersion, non-exponential tails, or dependence between jams could materially affect optimality and required sample sizes. The design is restricted to a single controllable factor and a compact interval, and the resulting optimal support at endpoints may be impractical if extreme outlet sizes are unsafe, infeasible, or induce very long run times (cost constraints are not optimized). The paper’s validation focuses on variance approximation rather than robustness of design performance under model misspecification or under unknown-parameter (two-stage/self-starting) settings common in practice.",None stated.,"Develop robust or Bayesian c-optimal designs that integrate uncertainty in (C,L) rather than relying on a single nominal value, and quantify performance under misspecification of the exponential model. Extend the methodology to incorporate practical cost/time constraints (e.g., penalizing very large φ that yields very long experiments) via constrained or economic optimal design. Consider extensions to dependent or non-exponential inter-jam times, and provide a packaged software implementation to compute designs and efficiencies for general nonlinear transformations g(θ).",2009.06501v1,https://arxiv.org/pdf/2009.06501v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:51:35Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Model discrimination|Cost reduction|Other,Other,Variable/General (example focuses on 1 uncertain input: boundary perturbation δ; also varies viscosity v and ε range),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,In text/Appendix|Not provided,http://arxiv.org/pdf/1202.1055v1|http://arxiv.org/pdf/1202.1056|http://dx.doi.org/10.1137/S1052623401399903|http://dx.doi.org/10.1287/opre.1090.0741|http://dx.doi.org/10.1287/opre.1090.0795|http://dx.doi.org/10.4208/cicp.160210.300710a|http://dx.doi.org/10.1287/opre.43.5.807,"The paper proposes using Optimal Uncertainty Quantification (OUQ) as a rigorous, optimization-based framework for model certification/validation and for guiding experiment design under uncertainty, demonstrated on a PDE example (viscous Burgers’ equation with an uncertain boundary perturbation). The quantity of interest is a probability of “success” defined via the transition-layer (shock) location z* exceeding a target threshold scaled from the mean shock location. Monte Carlo sampling is used as a baseline to estimate distributions and bounds, while OUQ computes provably optimal upper and lower bounds on P(success) by optimizing over discrete probability measures subject to moment-type constraints (e.g., mean/variance constraints on the uncertain boundary condition and/or on z*). The study shows OUQ can produce more conservative (rigorous) extremal bounds than Monte Carlo, and can do so with substantially less computational effort than brute-force sampling aimed at tail events. The work emphasizes that in an experiment-design context, adding candidate information/constraints and observing how much bounds tighten can identify the most informative next experiment (i.e., what to measure next to reduce uncertainty).","The PDE example is viscous Burgers’ equation $u_t+u u_x-\nu u_{xx}=0$ on $x\in[-1,1]$ with boundary conditions $u(-1)=1+\delta$, $u(1)=-1$, leading at steady state to $u(x)=-A\tanh\!\left(\tfrac{A}{2\nu}(x-z^*)\right)$, where $(z^*,A)$ satisfy two nonlinear equations: $A\tanh\left(\tfrac{A}{2\nu}(1-z^*)\right)-1=0$ and $A\tanh\left(\tfrac{A}{2\nu}(1+z^*)\right)-1-\delta=0$. The “success” event is defined as $z^*>\tfrac{x\bar z^*}{100}$ with $x=100+dx$ and $dx\in[0,15]$, so $P(\text{success})$ is an exceedance probability. OUQ computes extremal bounds by optimizing $\inf_{(g,\mu)\in A} \mathbb{E}_\mu[q]$ and $\sup_{(g,\mu)\in A} \mathbb{E}_\mu[q]$, where $q$ is an indicator and $\mu$ is represented as a finite-support discrete measure $\mu=\sum_i w_i\,\delta_{x_i}$ subject to constraints (e.g., mean/variance constraints).","For the Burgers’ example, direct optimization (mystic lattice of Nelder–Mead) solves the nonlinear system for $(z^*,A)$ reliably and very quickly (reported about 0.15 s per solve on a laptop for given $(\nu,\delta)$). Monte Carlo experiments sample up to $N=100{,}000$ realizations (with $M\approx1.1N$ attempted) and discard rare non-convergent solves under a tolerance (e.g., only 2–5 “misses” out of 110,000 attempts in representative settings). The paper reports that OUQ produces rigorous optimal upper/lower bounds on $P(\text{success})$ that differ markedly from Monte Carlo min/max estimates, highlighting Monte Carlo’s difficulty in capturing rare-event tails. Adding additional information as constraints (e.g., adding a variance constraint in addition to a mean constraint) tightens the OUQ bounds, consistent with the experiment-design narrative that more informative measurements reduce uncertainty.","The authors note that OUQ can become significantly more computationally expensive as additional constraining information is added, especially when constraints require nested numerical optimizations to enforce feasibility. They also point out a trade-off between adding information (which tightens bounds) and the computational cost/complexity of solving the resulting OUQ optimization problem. Additionally, they remark that solver time depends strongly on the nature of the constraints and the objective landscape, and that differential evolution settings may require tuning for speed.","The demonstration is primarily a stylized PDE example with a low-dimensional uncertainty (effectively one uncertain input, $\delta$, with fixed $\nu$ per run), so scalability to higher-dimensional experimental design spaces (many controllable factors and measurement choices) is not empirically validated. The “experiment design” aspect is described conceptually (choose the next measurement that tightens bounds most) but is not implemented as a full sequential design loop with explicit design variables, costs, and measurement noise models. Results depend on the chosen constraint set (moment constraints and confidence-interval allowances); in practice, specifying defensible constraints from real data can be nontrivial and may dominate conclusions. Comparisons are mainly against Monte Carlo min/max estimates rather than against other formal robust design/optimal Bayesian experimental design approaches, so relative benefits in broader DOE settings remain uncertain.","The authors state they intend to follow this study with additional examples applying OUQ to more complex physical systems (including coupled PDE systems), including explicit examples of experiment design and calculation of an “optimal model” under uncertainty. They also suggest extending OUQ by incorporating other types of constraining information (e.g., legacy data with uncertainties, uniqueness, additional moments) while acknowledging potential computational trade-offs. They describe experiment design in OUQ as iteratively adding new information to the admissible set $A$ and selecting experiments that most tighten bounds, potentially with manual or optimizer-driven selection.","A natural next step is to cast experiment design explicitly as an outer optimization over candidate experimental settings (design variables), with realistic observation models (noise, bias, cost/beamtime constraints) and to compare against Bayesian optimal design utilities (e.g., expected information gain) and distributionally robust design baselines. Developing scalable algorithms and heuristics for high-dimensional OUQ designs (many uncertain inputs, correlated uncertainties, multiple outputs) would improve practical applicability, potentially using surrogate models, adjoints, or decomposition methods. Providing open-source, reproducible implementations (a repository/package) and standardized benchmarks on real experimental datasets (e.g., beamline experiments) would strengthen evidence for DOE impact and facilitate adoption. Robustness studies under model misspecification (e.g., when independence/product-measure assumptions fail or when constraints are only approximately satisfied) would clarify when OUQ-based design recommendations remain reliable.",2009.06626v1,https://arxiv.org/pdf/2009.06626v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:52:25Z
TRUE,Factorial (full)|Robust parameter design,Screening|Robustness|Parameter estimation,Not applicable,"3 factors (channel length L, channel width W, top oxide thickness Lt), each at 3 levels (min/nom/max); 3^3=27 runs",Semiconductor/electronics,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper applies Design of Experiments (DoE) to a numerical sensitivity/variability analysis of a dual-gate graphene field-effect transistor (GFET) to assess robustness to manufacturing tolerances. Three controllable fabrication factors are varied with a ±10% tolerance: channel length (L), channel width (W), and top oxide thickness (Lt), using a three-level full factorial design (3^3 = 27 simulated design points). Device output characteristics (I_D–V_DS curves) are generated using a Cadence Virtuoso Spectre simulation environment with an equivalent large-signal GFET model, while DoE post-processing (Dex Scatter Plot and Main factor Plot) is performed via custom MATLAB routines. Results show that channel width W is the dominant driver of drain current variability (largest main-effect slope), Lt has a smaller negative influence, and L has negligible influence. The combined tolerances can produce substantial drain-current variation (reported up to ~30% at high V_DS), guiding which fabrication parameters should be prioritized for process optimization and robustness.","The DoE uses a three-level full factorial grid over the factor space D* with points x=(L,W,L_t), yielding 3^3=27 simulations. For an output metric y_k, bounds are estimated as Y_k^*=[\min_{x\in D^*} y_k,\max_{x\in D^*} y_k], and sensitivity is summarized by S_{Y_k}=(\max_{x\in D^*} y_k-\min_{x\in D^*} y_k)/y_{k,nom}\times 100. The underlying GFET drain current is computed from a drift-diffusion model: I_{ds}=\mu \frac{W}{L}\int_{V_{CS}}^{V_{CD}} Q_{tot}(V_c)\,\frac{dV}{dV_c}\,dV_c, with \frac{dV}{dV_c}=1+\frac{C_q(V_c)}{C_t+C_b}.","Using the 3-level full factorial (27-run) tolerance study with ±10% variation in L, W, and L_t, the drain current variability band increases with V_DS: at V_DS=1 V the drain current varies up to ~25% relative to nominal, and at V_DS=18 V it reaches about ~30%. One-factor-at-a-time results indicate L has negligible effect on I_D, W increases I_D with larger width, and increasing L_t decreases I_D (via reduced oxide capacitance). DoE main-effect analysis reports slope coefficients approximately \alpha_L=0, \alpha_W=0.0275, and \alpha_{L_t}=-0.0122 (for V_GS=1 V, V_DS=10 V), identifying W as the most influential factor. The paper notes the ~30% combined variation is roughly consistent with contributions of ~0% (L), ~20% (W), and ~9% (L_t) at high polarization.",None stated.,"The DoE is limited to three factors and a fixed ±10% uniform tolerance assumption; real manufacturing variability may be non-uniform, correlated, or non-independent across parameters. Only a discrete 3-level grid is used, so nonlinearities and interaction effects are only coarsely resolved and true extrema over the continuous domain D may be missed (bounds are explicitly acknowledged as over/under-estimates). The study is entirely simulation-based using a particular equivalent GFET model and Cadence setup; robustness of conclusions to model-form uncertainty or to different GFET technologies/process stacks is not validated with new experimental data. No optimization of settings (beyond factor importance ranking) or uncertainty quantification of effect estimates is provided.","Future work will analyze additional performance functions (e.g., cut-off frequency and delay time) and explore other influential factors beyond L, W, and top-oxide thickness.","Extend the DoE to include interactions and curvature more explicitly via response-surface modeling (e.g., second-order models) and validate predicted sensitivity with additional simulation points or adaptive/sequential sampling. Incorporate realistic statistical distributions and correlations for process parameters and include other key variability sources (e.g., mobility variation, contact resistance, oxide permittivity, temperature) to better represent manufacturing and operating uncertainty. Perform multi-objective robust optimization (not just screening) across multiple performance metrics (I_D, f_T, noise figure, linearity) and provide confidence intervals for factor effects via replicated simulations or stochastic modeling. Release MATLAB post-processing scripts or provide a reproducible workflow (inputs, netlists, and scripts) to enable independent verification and reuse.",2009.07582v1,https://arxiv.org/pdf/2009.07582v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:52:58Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Parameter estimation|Prediction|Other,V-optimal|Other,"Variable/General (pool-based subset selection from m points in d-dimensional feature space; examples include d=100 synthetic, kernel/RKHS infinite-dimensional)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Not provided,http://yann.lecun.com/exdb/mnist,"This paper develops an optimal experimental design (OED) framework tailored to overparameterized regression/interpolation, where classical variance-only criteria (e.g., V-optimality) can perform poorly because prediction error can be bias-dominated, variance-dominated, or mixed depending on sample size relative to model complexity. The authors derive an expected risk decomposition for ridge/minimum-norm estimators that explicitly includes both bias and variance terms, and propose practical surrogate criteria that upper-bound the bias when the true parameter vector is unknown and approximate the data second-moment using an unlabeled pool. They formulate pool-based OED as selecting a subset of unlabeled points to minimize this overparameterized criterion, show connections to column subset selection, transductive experimental design, and coreset methods, and provide a kernelized objective enabling kernel ridge regression and NTK-based selection. A greedy subset-selection algorithm is proposed for minimizing the kernelized criterion, with computational optimizations. Empirically, on MNIST (kernel regression) and NTK-based single-shot deep active learning, the proposed bias-aware/bias-dominant selection often outperforms random selection and can improve over transductive experimental design baselines, especially in highly overparameterized settings.","For ridge/minimum-norm estimation with $M=X^TX$ and $M_\lambda=M+\lambda I$, the expected excess risk is decomposed as $\mathbb{E}[R(\hat w_\lambda)]=\|C_\rho^{1/2}(I-M_\lambda^+M)w\|_2^2+\sigma^2\,\mathrm{Tr}(C_\rho M_\lambda^{+2}M)$ (bias + variance). A practical design objective replaces the unknown $w$ by a Frobenius-norm bound and approximates $C_\rho$ using an unlabeled pool matrix $V$, yielding $\bar\psi_{\lambda,t}(M)=\|V(I-M_\lambda^+M)\|_F^2+t\,\mathrm{Tr}(V M_\lambda^{+2} M V^T)$, minimized over subset-selected designs $X=V_{S,:}$. Kernelization expresses the subset objective purely in terms of the kernel Gram matrix $K=VV^T$ (objective $J_{\lambda,t}(S)$) enabling use with kernel ridge regression and NTK.","On MNIST kernel ridge regression with an RBF kernel and greedy subset sizes 1–100, the bias-oriented choice $t=0$ yielded lower MSE than a mixed tradeoff (e.g., $t=0.5$) for $\lambda=0$, and also outperformed the transductive-equivalent choice $t=\lambda$ when $\lambda=0.75^2$. Across 112 UCI classification datasets (kernel ridge regression, selecting 50 samples), $\bar\psi_{\lambda,0}$ was more often better than $\bar\psi_{\lambda,\lambda}$ as $\lambda$ increased (e.g., at $\lambda=10$: 68 wins for $t=0$ vs 19 for $t=\lambda$; 25 ties under a 5% margin). In NTK-based single-shot deep active learning on MNIST with designs up to 800 points, the proposed selection ($\lambda=t=0$) improved test accuracy over random selection and was competitive with a k-centers coreset baseline; the paper notes that random selection needed roughly 600 samples to match accuracy achieved by the proposed design with about 400 samples in one reported Wide-LeNet5 experiment.","The authors state that no formal proof is provided for the empirical observation that their greedy ‘Overparameterized OED’ can mitigate double descent while outperforming classical OED. They emphasize that noise estimation (needed to set the bias–variance tradeoff parameter $t$ well) is difficult in the single-shot setting, and that selecting $t$ remains a practical challenge with only heuristic guidance (e.g., $t=0$ vs $t=\lambda$). For deep active learning, they present the NTK-based approach as preliminary and explicitly leave theoretical analysis for future work; they also mention simplifying choices (e.g., not enforcing $f_{\theta_0}(x)=0$) to keep experiments simple.","The method’s performance depends on the representativeness of the unlabeled pool used to approximate $C_\rho$ (or $V^TV/m$); dataset shift between pool and test distribution could degrade design quality, but robustness to shift is not systematically studied. The greedy subset selection optimizes a nonconvex combinatorial objective and can be computationally heavy for large pools due to kernel Gram matrix formation ($O(m^2)$ memory/time), which may limit scalability beyond moderate $m$ unless approximations are used. Comparisons are somewhat narrow (e.g., few active learning baselines and limited hyperparameter tuning protocols), and the deep-learning experiments rely on NTK computations that may not reflect practical finite-width, cross-entropy training pipelines common in modern active learning benchmarks. Sensitivity to kernel choice/feature scaling and to regularization $\lambda$ (beyond limited values) is not exhaustively characterized.","They explicitly leave sequential experimental design (incremental acquisition based on improved estimates of $w$ and/or noise) to future research, noting it could help set the tradeoff parameter $t$ dynamically. They also state that theoretical analysis of the proposed NTK-based single-shot deep active learning approach is left for future work. The discussion around parameter-choice (e.g., better defaults for $t$ than $t=\lambda$) also motivates future investigation into principled selection of the bias–variance tradeoff in single-shot regimes.","Developing scalable approximations (e.g., Nyström/Random Features, block/streaming kernels, sublinear screening) would improve applicability to large unlabeled pools typical in deep active learning. Extending the criterion and algorithm to handle label noise models, heteroskedasticity, and distribution shift (pool vs target) would strengthen robustness in real deployments. A systematic benchmark against modern batch active learning methods (e.g., BADGE, CoreSet variants, BALD-style methods) under standard classification losses would clarify practical competitiveness. Providing open-source implementations and studying theoretical approximation guarantees (e.g., approximate submodularity bounds or consistency in overparameterized regimes) would aid adoption and understanding.",2009.12820v3,https://arxiv.org/pdf/2009.12820v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:53:40Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Cost reduction,A-optimal,"Variable/General (select p sensors from n candidate locations; examples include n=1000, r=10, p≈10–50; NOAA SST uses r=10, p=10–30)",Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.esrl.noaa.gov/psd/data/gridded/data.noaa.oisst.v2.html,"The paper proposes a data-driven sparse sensor selection/placement method for nondynamical systems using an A-optimal design-of-experiments objective and a proximal-splitting optimization approach implemented via ADMM. Sensors are selected to minimize the A-optimality criterion, i.e., the trace of the inverse Fisher information matrix (equivalently, average parameter-estimation error variance) under linear latent-variable observation models built from reduced bases (e.g., POD/SVD). The method incorporates group sparsity on the gain matrix using block soft thresholding, block hard thresholding, and an L0-constrained block hard thresholding operator, followed by a “polishing” step that recomputes the estimator via pseudoinverse using the chosen sensor pattern. Performance is evaluated on randomized Gaussian sensing matrices and on NOAA OISST V2 sea surface temperature data, comparing against greedy A-optimal selection and a Joshi–Boyd-style convex relaxation. Results show ADMM with hard/L0-constrained thresholding achieves lower A-optimal objective than greedy and convex relaxation over much of the oversampling regime (p>r), with computational scaling O(n r^2) per iteration that is more favorable than convex relaxation for large n and yields improved reconstruction error on the SST dataset at p≥20.","The A-optimal objective is derived from the estimation error covariance: with i.i.d. Gaussian noise, $\mathrm{tr}(\sigma^2 (C^T C)^{-1})$, so the sensor selection aims to $\min \mathrm{tr}((C^T C)^{-1})$ (Eq. 11). An equivalent sparse-gain formulation minimizes $\mathrm{tr}(K K^T)$ subject to $K U=I$, augmented with group sparsity across columns/rows (Eqs. 18–21). ADMM updates solve a regularized least-squares step for $X$ and apply block soft/hard (and L0-constrained) thresholding proximal operators (Eqs. 23–33).","In random problems (e.g., $U\in\mathbb{R}^{1000\times 10}$), ADMM with block hard thresholding (BHT) and L0-constrained BHT achieves the lowest (best) normalized $\mathrm{tr}((\mathrm{FIM})^{-1})$ across most tested $p$ in the oversampling range (roughly $10\le p\le 50$), while ADMM with block soft thresholding performs worse than competitors. Computational complexity is reported as: greedy $O(p n r^2)$, convex relaxation $O(n^3)$ per iteration, and ADMM-based $O(n r^2)$ per iteration. On the NOAA OISST V2 application (with $r=10$ and five-fold CV), convex relaxation and ADMM improve the A-optimal criterion versus greedy at $p\ge 20$, and ADMM yields lower reconstruction error than greedy/convex for $p\ge 20$; reported runtimes for $p=20$ are 5.5 s (greedy), 320.4 s (ADMM), and 77430.2 s (convex relaxation).","Convergence is not guaranteed when using nonconvex sparsity terms (BHT and L0BHT), and performance depends on the ADMM step size $\gamma$, so $\gamma$ must be tuned (and was gradually decreased) to obtain convergence. Because the method is iterative, solutions could be obtained faster by adjusting the convergence threshold and the ADMM step size, indicating sensitivity of runtime to stopping criteria/parameter settings. The study focuses on the oversampling case $p>r$ and omits the $p<r$ formulation.","The formulation assumes i.i.d. Gaussian measurement noise and an accurate low-rank latent representation (e.g., POD from training data); performance may degrade under model mismatch, non-Gaussian noise, outliers, or temporal/spatial correlation (especially relevant for SST fields). Comparisons appear limited to greedy and one convex relaxation baseline; broader benchmarking against modern sparse sensor placement methods (e.g., submodular optimization variants, randomized/approximate Newton methods, or information-theoretic criteria beyond A-optimality) could change conclusions. Practical guidance for selecting $\lambda$, $\gamma$, and iteration limits is limited, and the hard-threshold variants can be sensitive to initialization and scaling, which may affect reproducibility and robustness across datasets.","The authors note that because the proposed method is iterative, faster solutions could be obtained by adjusting the convergence-judgment threshold and the ADMM step size, implying future work on improved parameter tuning and stopping criteria to accelerate convergence.","Develop self-tuning or adaptive schemes for $\gamma$ and sparsity control (e.g., continuation on $\lambda$ or explicit $p$-constraint handling) with convergence diagnostics, especially for the nonconvex thresholding variants. Extend the approach to correlated/heteroskedastic noise models and to temporally correlated measurements (common in geophysical data), potentially via generalized least squares or Bayesian A-optimal criteria. Provide open-source implementations and standardized benchmarks, and explore multiobjective designs trading off A-optimality with communication/energy/network constraints to better match real sensor-network deployment needs.",2010.09329v1,https://arxiv.org/pdf/2010.09329v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T03:59:23Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal|Bayesian D-optimal|Other,"Variable/General (design variable is selection of r sensors from d candidates; examples include d=9, 75, 81 and r ranging roughly 2–60)",Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a fast, scalable computational framework for Bayesian optimal experimental design (OED) in PDE-governed inverse problems, focusing on optimal observation sensor placement among many candidate locations. The design criterion is expected information gain (EIG), i.e., expected KL divergence from prior to posterior; for linear problems EIG reduces to a D-optimal log-determinant criterion, and for nonlinear problems EIG is approximated using Laplace (Gaussian) approximations plus additional approximations to avoid repeated PDE/inverse solves. Key ideas are (i) exploiting low-rank structure of the Jacobian/Hessian to identify a low-dimensional data-informed subspace, (ii) an offline–online decomposition so that the dominant cost (PDE solves) is paid offline and online design optimization requires no PDE solves, and (iii) a swapping greedy algorithm initialized via leverage scores to efficiently optimize sensor sets. Numerical demonstrations on an advection–diffusion initial-condition inversion (linear) and a log-normal diffusion coefficient inversion (nonlinear) show scalability as parameter and data dimensions grow from tens to thousands and improved designs versus standard greedy/random selection.","The Bayesian OED objective is the expected information gain $\Psi(W)=\mathbb{E}_y\big[ D_{\mathrm{KL}}(\mu_{\text{post}}(\cdot\mid y)\|\mu_{\text{pr}})\big]$, where $W$ encodes selecting $r$ sensors from $d$ candidates. For linear-Gaussian inverse problems, $\Psi(W)=\tfrac12\log\det(I+\tilde H)$ with prior-preconditioned misfit Hessian $\tilde H=\Gamma_{\text{pr}}^{1/2}F^T\Gamma_n^{-1}F\Gamma_{\text{pr}}^{1/2}$; for sensor placement $F=W F_d$ leads to an equivalent form $\Psi(W)=\tfrac12\log\det(I+W H_d W^T)$ with $H_d=\hat F_d\Gamma_{\text{pr}}\hat F_d^T$ and $\hat F_d=(\Gamma_n^d)^{-1/2}F_d$. For nonlinear problems, the posterior is approximated by Laplace at a MAP point $m_{\mathrm{map}}$ and a Gauss–Newton Hessian $H_m\approx J(m_{\mathrm{map}})^T\Gamma_n^{-1}J(m_{\mathrm{map}})$; eigenvalues of the prior-preconditioned Hessian (approximated via low-rank RSVD) are used to approximate log-determinant and trace terms in the KL expression.","For the linear advection–diffusion example with $d=9$ candidates, swapping greedy matches the brute-force optimal design for most budgets (all $r$ except $r=4$, where it is second-best), and standard greedy is sometimes suboptimal but close in EIG value. With $d=75$ candidates, both greedy methods outperform 200 random designs, and swapping greedy consistently achieves equal or higher approximate EIG than standard greedy (e.g., for $r=10$: 67.2216 vs 66.4687; for $r=30$: 114.0506 vs 112.6066). For the nonlinear log-normal diffusion example with $d=81$ candidates and $r=10$, correlations between EIG estimators are high: DLMC-EIG vs LA-EIG ≈ 0.9752; DLMC-EIG vs fixed-MAP LA-EIG ≈ 0.9453; fixed-MAP vs prior-sample-point approximation ≈ 0.999, suggesting the cheaper approximations preserve the design ranking well.","The authors note limitations due to assuming a Gaussian prior and additive Gaussian observation noise. For nonlinear problems, they acknowledge relying on a sequence of approximations (Laplace, low-rank covariance/Hessian, fixed-MAP and prior-sample-point substitutions) that trade accuracy for computational efficiency. They also point out the heuristic nature of greedy optimization for the combinatorial sensor selection problem.","The approach is tailored to sensor placement (subset selection) and does not directly address other experimental design types (e.g., continuous design variables, time-dependent adaptive sensing, or joint sensor-and-excitation design) without additional development. Performance claims rely heavily on low-rank structure and fast spectral decay; problems with slowly decaying spectra or strong nonlinearity/multimodality may degrade both Laplace accuracy and low-rank approximations. No reference implementation is provided, and practical deployment would require substantial PDE/adjoint and linear-algebra infrastructure, which may limit reproducibility and adoption.","They suggest extending the framework to nonlinear problems where Laplace approximation is inadequate (e.g., Helmholtz-type equations). They also propose developing theoretical guarantees for the swapping greedy algorithm. Another stated direction is to adaptively determine the number of sensors needed rather than fixing $r$ a priori.","Developing self-starting/robust variants that handle model misspecification (non-Gaussian priors/noise, outliers, correlated noise) would broaden applicability in real sensing systems. Extending the method to sequential/adaptive (online) experimental design where sensor locations or measurement times are updated based on observed data would align with Bayesian OED practice. Providing open-source software (e.g., built on existing PDE-inverse libraries) and benchmarking across standard OED testbeds would improve reproducibility and clarify when the approximations remain reliable.",2010.15196v1,https://arxiv.org/pdf/2010.15196v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:00:01Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a reflective/descriptive account of redesigning an 18-hour, 6-week graduate teaching assistant training course (“Teaching@SUTD”) from face-to-face delivery to synchronous online delivery during COVID-19. The redesign is framed using the SAMR model (Substitution, Augmentation, Modification, Redefinition) and a “Fit for Purpose” teaching and learning design framework that aligns learning outcomes, activities, assessment, and technology tools. The authors describe concrete choices of platforms and tools (e.g., Zoom, eDimension, Google Docs, Mentimeter, YouTube) for communication, content delivery, collaboration, and assessment. They discuss practical challenges (e.g., limitations of Zoom breakout rooms for a Jigsaw activity, reduced visibility of engagement when cameras are off) and report qualitative impressions from weekly student feedback. The work contributes practitioner-oriented guidance for online course design rather than proposing or evaluating formal experimental design (DOE) methods.",Not applicable,Not applicable (the paper reports qualitative reflections and lessons learned rather than DOE/SPC-style quantitative performance results).,"The authors note constraints from limited time/resources and the quick turnaround to move online, leading them to “augment and modify” rather than fully “redefine” the course. They also cite technical limitations of Zoom breakout rooms (only hosts can move between rooms) and streaming/lag issues that led to turning off video, reducing the ability to monitor engagement. They report that some collaborative activities (e.g., the Jigsaw activity) were less effective online due to uneven preparation/participation and reduced ‘energy’ compared with in-person settings.","Findings are largely anecdotal and based on a single course run (about 25 participants) without controlled comparison to face-to-face or alternative online designs, limiting causal claims about what changes improved learning. Student feedback is described qualitatively with no systematic measurement (e.g., validated surveys, learning outcomes, inter-rater assessment of microteaching), making effectiveness hard to quantify. Tool choices and outcomes may be context-dependent (SUTD’s infrastructure, student cohort, instructor expertise), so generalizability to other institutions or disciplines is uncertain.",None stated,"A natural extension would be to run comparative studies (e.g., quasi-experimental or randomized comparisons of alternative online activity designs for Jigsaw/peer learning) with measurable outcomes such as engagement, learning gains, and teaching performance. Developing and validating an instrumented evaluation framework (rubrics + surveys + analytics) could support evidence-based iteration of the “Fit for Purpose” framework. Packaging the redesign approach into reusable templates and sharing an implementation toolkit (including example lesson plans and activity protocols) would improve adoption and reproducibility across courses.",2010.15602v1,https://arxiv.org/pdf/2010.15602v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:00:22Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|E-optimal|Other,Variable/General (design vectors v_i in R^d; budget b ≥ d; n candidate vectors),Theoretical/simulation only|Other,Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a unifying local-search framework for discrete experimental design problems where one selects a subset/multiset of b design vectors v_i ∈ R^d to optimize classical optimal-design criteria. It provides new approximation analyses for combinatorial local search (notably Fedorov’s exchange method) for D-design in the without-repetition setting, and gives condition-number–dependent guarantees for A-design; it also proposes a new combinatorial local-search method for E-design based on a smoothed/potential objective derived from regret minimization. On the rounding side, it presents a unified randomized exchange (iterative randomized rounding) algorithm that matches or improves prior best-known guarantees for D/A/E-design and extends to multiple knapsack constraints (useful for weighted design and fairness constraints). Theoretical results include explicit approximation ratios and budget conditions (e.g., b relative to d, ε, and conditioning) plus probability bounds for satisfying knapsack constraints. The contribution advances the algorithmic theory of optimal experimental design by unifying combinatorial exchange and spectral/regret-minimization analyses and extending results to more general constraint settings.","The experimental design objective is to choose S (|S|=b) to optimize a function of the information matrix M(S)=∑_{i∈S} v_i v_i^⊤. D-design: maximize det(M(S))^{1/d}; A-design: minimize tr(M(S)^{-1}); E-design: maximize λ_min(M(S)). The convex relaxations use x∈[0,1]^n (and knapsack constraints ⟨c_j,x⟩≤b_j) with X(x)=∑_i x(i) v_i v_i^⊤; objectives are maximize log det(X(x)) or minimize tr(X(x)^{-1)). The randomized exchange algorithm uses action matrices A_t=(α Z_t−l_t I)^{-2} (trace 1) and samples swaps using probabilities proportional to (1−x(i))(1−2α⟨v_i v_i^⊤,A_t^{1/2}⟩) for removals and x(j)(1+2α⟨v_j v_j^⊤,A_t^{1/2}⟩) for additions.","For D-design (without repetition), the analyzed Fedorov exchange method achieves a (b−d−1)/b approximation, yielding a (1−ε)-approximation when b ≥ d+1 + d/ε. For A-design, Fedorov exchange attains (1+ε)-approximation under a budget condition depending on tr(X)tr(X^{-1}) (equivalently condition number κ of an optimal fractional solution), e.g., b ≥ Ω(((1+√κ)d)/ε). For E-design, a new combinatorial local-search algorithm achieves λ_min(Z) ≥ (1−ε)λ_min(X) when b ≥ Ω((d/ε^2)·√(λ_avg(X)/λ_min(X))) (≈ Ω(d√κ/ε^2) under conditioning). For D/A-design with multiple knapsack constraints, a randomized exchange/rounding algorithm returns an integral solution with det(·)^{1/d} ≥ (1−O(ε)) times the fractional optimum (D) or tr(·^{-1}) ≤ (1+ε) times the fractional optimum (A), under budgets b_j ≥ (2d||c_j||_∞)/ε, with stated high-probability feasibility/approximation bounds.",The authors note they do not have a preprocessing (“capping”) procedure in the without-repetition setting to reduce general A-design and E-design instances to the well-conditioned/short-vector regimes where their combinatorial guarantees apply. They leave as an open problem designing a fully combinatorial algorithm for A-design and E-design in the general without-repetition case.,"The work is primarily theoretical and its performance is established via approximation analyses and concentration bounds; practical runtime constants and empirical performance on real experimental design datasets are not demonstrated here. Several results rely on conditions such as sufficiently large budgets b (or b_j) and good conditioning of near-optimal fractional solutions; in poorly conditioned or small-budget regimes the guarantees may weaken. The algorithms also assume the ability to solve convex relaxations (for rounding results) and to perform linear-algebra operations (e.g., matrix inverses/square roots) which may be expensive or numerically sensitive for large d without specialized implementations.","They explicitly leave open the design of a fully combinatorial algorithm for A-design and E-design in the general without-repetition setting, i.e., without requiring a well-conditioned near-optimal solution or additional preprocessing akin to capping.","Developing practical implementations and benchmarks (including numerical stability considerations for computing X^{-1/2} and action matrices) would clarify real-world utility. Extending the unified framework to broader constraint families (e.g., matroid/general combinatorial constraints beyond knapsacks) with comparable tight guarantees would be valuable. Another direction is relaxing assumptions like independence/noise models implicit in linear measurement formulations, or adapting algorithms to streaming/online settings where candidate vectors arrive over time.",2010.15805v2,https://arxiv.org/pdf/2010.15805v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:00:56Z
TRUE,Other,Parameter estimation|Cost reduction|Other,Not applicable,Variable/General (covariates used for blocking/stratification; examples include ~29–30 covariates in simulations),Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes machine-learning-driven procedures for improving restricted randomization in experiments—especially blocking/stratification—so as to improve treatment–control balance on prognostic covariates and thereby improve precision of treatment-effect estimation. It formalizes two main strategies: a variable-selection-based blocking approach (optionally using Lasso for feature selection and CART for partitioning) and a future prognostic score (FPS) approach (using predictive ML such as random forests to form a one-dimensional score and then block by that score). The key design choice is how to choose blocking variables and the number/complexity of blocks, which the authors recommend tuning via cross-validation to balance improved mean-squared error (MSE) against degrees-of-freedom costs that can inflate standard errors when many blocks are used. The methods are evaluated via large-scale placebo simulations on real datasets (Mexico ENE and Sri Lanka microenterprise panel data) showing reductions in treatment-effect MSE and standard errors relative to a hand-crafted 48-block design. The paper also discusses how to adapt the same ML-driven ideas to pair-wise matching and rerandomization/minimization schemes.","The experimental outcome model is written as $y_{it}=\beta d_{it}+h_t(X_i)+u_{it}$, with blocking aimed at reducing variance/MSE of $\hat\beta$ by balancing prognostic covariates. The FPS strategy constructs a predictive score via $y_{\mathrm{pre2}}\approx g_{\mathrm{FPS}}(X,y_{\mathrm{pre1}})$ and then forms blocks based on predicted values (e.g., sequential allocation with minimum block size $c_B$). Feature selection is framed using Lasso: $\min_{\beta}\|y-X\beta\|_2^2+\lambda\|\beta\|_1$, with $\lambda$ chosen by cross-validation. The tradeoff in standard errors with additional blocks is illustrated via the OLS degrees-of-freedom term (e.g., $\sqrt{1/(n-b-1)}$), showing why overly many blocks can increase estimated standard errors.","Across 10,000 placebo simulations using two real datasets (Mexico ENE and Sri Lanka microenterprise panel) and sample sizes $n=100$ and $n=300$, all proposed ML-based blocking methods outperform the manual 48-block design. The reduction in treatment-effect MSE from the best ML method ranges from 16% to 34% in the reported comparisons (consistent with the abstract’s 14%–34% range). The reduction in the size of the standard error from the best ML method ranges from 6% to 16%. Performance leaders vary by context: FPS (random forest prognostic scores) is best for Mexico at $n=100$, while VS with Lasso+CART performs best for Mexico at $n=300$ and for both Sri Lanka samples in the reported tables.","The paper notes that guidance and performance depend on assumptions about temporal stability of the data-generating process (DGP): if the DGP changes over time, prognostic-score approaches can group units with similar predicted outcomes but dissimilar covariates, potentially weakening future balance. It also acknowledges practical issues such as “misfits” (odd-sized blocks preventing perfectly even treated/control counts) and that some improved partitioning ideas (e.g., jointly optimizing one-dimensional partitions beyond simple sequential allocation) may require custom tools beyond off-the-shelf methods. In the appendix, it notes that alternative splitting rules that directly incorporate expected out-of-sample performance (in the spirit of Athey & Imbens) would require custom software and are left for future work.","The evaluation is based on placebo simulations using observational panel datasets treated as if they were experimental baselines; performance may differ under genuine experimental assignment constraints, interference, attrition, or noncompliance. The procedures largely presume i.i.d. units (no network spillovers) and do not deeply assess robustness to heavy-tailed outcomes, outliers, or strong autocorrelation/clustering structures that can affect both blocking efficacy and standard-error estimation. Implementation details that matter in practice—e.g., handling high-cardinality categoricals, missing data, and operational constraints on block definitions—are not fully benchmarked, and no publicly released code is provided to verify reproducibility. The approach focuses on improving ATE estimation precision but does not study impacts on heterogeneous treatment effect estimation or multiple testing adjustments in subgroup analyses beyond noting how to incorporate prespecified subgroup partitions.","The authors explicitly point to opportunities for custom tools to improve performance beyond off-the-shelf ML, including better second-stage partitioning for prognostic-score blocking (to control the final number of blocks via cross-validation rather than creating too many blocks). They also highlight that incorporating out-of-sample considerations directly into tree split criteria (rather than via post-hoc cross-validation) would be desirable but is left for future work due to tooling limitations. They note feature learning (e.g., neural-network-based learned representations) as a potentially helpful direction for blocking when both FPS and VS are imperfect, though it may require larger samples and has not yet been applied to experimental blocking (to their knowledge).","A natural extension is to develop self-contained, reproducible software (e.g., an R/Python package) that implements the full pipeline, including misfit handling and operational constraints, and validates it across standard benchmark suites for randomized experiments. Methodologically, the approach could be extended to clustered/hierarchical experiments and settings with interference, where blocking and balance constraints must respect cluster boundaries and exposure mappings. Another direction is to integrate robust and distribution-free objectives (beyond MSE) for outcomes with skew/heavy tails and to study resulting impacts on valid inference under unknown/estimated nuisance structure. Finally, extending the selection criteria to explicitly optimize a user-specified tradeoff among MSE, standard errors, and subgroup-power objectives (possibly with Bayesian or decision-theoretic formulations) would make the design recommendations more directly aligned with practitioner goals.",2010.15966v1,https://arxiv.org/pdf/2010.15966v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:01:29Z
TRUE,Sequential/adaptive|Bayesian design|Other,Prediction|Optimization|Other,Not applicable,Variable/General (selecting k-space columns sequentially; action space size equals number of Cartesian k-space columns: 128 for Knee (128×128) and 256 for Brain (256×256)),Healthcare/medical,Simulation study,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/Timsey/pg_mri,"The paper frames accelerated MRI sampling (k-space subsampling mask construction) as an experimental design problem and learns adaptive, sequential Cartesian sampling policies using reinforcement learning (policy gradients). At each step, a policy network selects the next k-space column based on the current reconstructed image; the reward is the improvement in SSIM relative to ground truth, yielding a sequential design objective over a fixed acquisition budget. The authors compare a fully non-greedy return objective (long-horizon, γ=1) to a greedy objective (immediate reward, effectively γ=0) and find the greedy approximation performs nearly as well or better, largely due to lower gradient-variance and better adaptivity to individual images. Experiments on fastMRI single-coil Knee and Brain datasets show the learned greedy policy outperforms random/equispaced baselines and a non-adaptive oracle, and is competitive with an AlphaZero-style MCTS baseline, while being cheaper to train. They analyze adaptivity via mutual information between states and actions and explain non-greedy underperformance via lower signal-to-noise ratios of gradient estimates for longer horizons.","MRI design is posed as selecting the next measurement (k-space column) via a policy π(·|\hat{x}_t), where \hat{x}_t = G_\theta(U_t F x) is the reconstruction from the current mask. The objective maximizes cumulative reconstruction-quality gains: J(\phi)=\mathbb{E}_{x\sim D}\sum_{t=L}^{M-1}[\eta(x,G_{t+1})-\eta(x,G_t)] with \eta = SSIM. Policy-gradient estimators are derived for non-greedy returns (discount γ, summing future rewards) versus greedy updates using only immediate reward, both using multi-sample rollouts and a per-step baseline from the mean reward over q sampled actions/trajectories.","On fastMRI Knee, the greedy policy achieves SSIM 0.7230 (base horizon) and 0.7442 (long horizon), outperforming a random strategy (0.6948/0.6602) and slightly exceeding a non-adaptive oracle (0.7213/0.7421). On fastMRI Brain, greedy reaches 0.9106 (base) and 0.8917 (long), again above random (0.9020/0.5820) and comparable to the oracle (0.9099/0.8909). A discounted non-greedy model with γ=0.9 is marginally best overall (e.g., Knee long 0.7449; Brain long 0.8921), while fully non-greedy γ=1 underperforms. Gradient SNR analyses show non-greedy estimators have consistently lower SNR than greedy (e.g., Knee long epoch 20: greedy 3.49 vs non-greedy 1.04), supporting the variance explanation for reduced adaptivity.","Experiments and conclusions are based on single-coil fastMRI data; the authors note clinical multi-coil settings are more relevant and require confirmation. They caution that optimizing and evaluating with SSIM may not fully reflect clinical diagnostic utility, and that reconstruction methods can smooth subtle pathologies, so clinical use requires extreme care. They also state that the greedy-model reward baseline cannot be used directly in the non-greedy setting due to a combinatorial explosion in required trajectories.","The method assumes access to fully sampled ground-truth k-space during training to simulate counterfactual acquisitions and compute SSIM rewards, which may not match real acquisition noise/physics constraints and can limit transfer. Only Cartesian column sampling is considered; extensions to non-Cartesian trajectories or hardware constraints (slew rate, gradient limits) are not evaluated. Comparisons are limited (e.g., many learned-sampling methods are not fully benchmarked under identical reconstruction/backbones), and results may depend on the chosen fixed pretrained U-Net and SSIM reward rather than downstream diagnostic tasks.","They propose investigating value-function learning to estimate returns at intermediate nodes so that stronger, more local baselines can be used for non-greedy objectives without trajectory explosion. They suggest studying whether theoretical guarantees for adaptive greedy strategies (e.g., adaptive submodularity-style bounds) apply to MR subsampling. They also indicate validating conclusions in multi-coil MRI as an important next step.","Evaluate learned policies under more realistic acquisition models (measurement noise, coil sensitivities, trajectory timing constraints) and on prospective/real scanner studies. Extend the framework to multi-coil and multicontrast settings and to non-Cartesian or mixed trajectory design, potentially with constrained action spaces. Replace SSIM-only rewards with task-based or radiologist-aligned objectives (lesion detectability, segmentation performance) and assess robustness across anatomies and scanners; provide standardized benchmarks and an official implementation package for broader adoption.",2010.16262v2,https://arxiv.org/pdf/2010.16262v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:02:11Z
TRUE,Factorial (full)|Screening|Response surface,Screening|Optimization|Parameter estimation,Not applicable,7 factors (2^7 screening; then 3 factors in 3^3 follow-up),Energy/utilities,Simulation study,TRUE,MATLAB,Not provided,http://www.kios.ucy.ac.cy/testsystems/images/Documents/Data/IEEE%2014.pdf,"The paper proposes a DOE-based methodology to tune generator controller parameters (AVR amplifier gains and governor droop) in a power system subject to stochastic load variation. It first runs a full 2^7 factorial screening experiment (no repetitions) and uses a normal probability plot plus multifactor ANOVA (after a power transformation to address heteroscedasticity) to identify significant main effects and interactions on an objective function defined as a weighted average of integral absolute errors of voltage and frequency. Significant factors are then explored via a full 3^3 factorial experiment and a response-surface-style regression model is fit by least squares and checked for significance via ANOVA. The fitted model is minimized under box constraints (within the experimental region) to obtain tuned controller settings, which are validated by simulation on the IEEE 14-bus system and compared to benchmark parameters, showing a statistically significant reduction in the objective function. The work advances controller tuning under stochastic loads by combining classical screening DOE with response surface regression/optimization in a power-system transient simulation setting.","The response/quality metric is the objective function $f$, a weighted average of integral absolute errors across machines: $f=\frac{\sum_{i=1}^{n_g}\left(w_{1i}\int_0^{T_f}|V_{ti}(t)-V_{ti}(0)|dt+w_{2i}\int_0^{T_f}|\omega_i(t)-\omega_i(0)|dt\right)}{\sum_{i=1}^{n_g}(w_{1i}+w_{2i})}$. Screening uses a full $2^k$ factorial with $k=7$ factors; the response-surface step fits a polynomial regression (e.g., including linear terms and interactions/quadratic terms such as $K_{A1}^2$ and $K_{A2}K_{A1}$) estimated by least squares and then minimized over factor bounds.","A full $2^7$ factorial screening (no replications) identified significant effects D, E, F and interaction DE; ANOVA on transformed data reported very small p-values for D, E, F (all shown as 0.0000) and DE (p=0.0072). Follow-up with a full $3^3$ factorial on factors $K_{A2}$, $K_{A1}$ and $R_2$ fit a statistically significant regression model (overall model p=0.0005). Optimization within the experimental region yielded tuned values $K_{A2}=330.10$, $K_{A1}=373.26$, with other factors held at normal values and droop values at 0.05. Validation via a difference-of-means test on 20 samples per setting rejected $H_0$ (t=7.2545, p=5.58\times10^{-9}$), indicating the tuned parameters significantly reduce the mean objective function relative to the benchmark settings.","The paper notes the response-surface regression is only valid inside the experimental region and therefore the optimization is constrained to the specified factor bounds. It also acknowledges ANOVA validity requires assumptions (homoscedasticity, normality, independence) and shows a transformation was needed to satisfy homoscedasticity.","The screening design uses a single run per treatment combination (no replications), so pure error cannot be directly estimated and results may be sensitive to simulation noise from stochastic load realizations unless the random seed/realizations are controlled. The response surface model is not a standard second-order RSM in coded units (it uses selected terms and omits some potentially important curvature/interaction terms such as those involving droop), which may risk model misspecification. Results are demonstrated only on IEEE 14-bus with specific stochastic load settings (e.g., 0.5% WGN penetration and uniform slope range), so robustness/generalizability to other systems, disturbance types, and stronger stochasticity is unclear. Practical implementation details (computational cost, convergence, and reproducibility controls) and public code are not provided.",None stated.,"Extend the DOE/tuning approach to larger and more realistic networks (e.g., IEEE 39/118) and to additional controller parameters and controller types (e.g., PSS, excitation limits, turbine-governor dynamics). Incorporate replication or common-random-numbers strategies and/or stochastic kriging to better handle simulation noise and quantify uncertainty in estimated effects and optima. Develop a sequential/adaptive RSM (e.g., steepest ascent and local refinement) rather than a single fixed 3^k grid to reduce simulation budget. Provide an open-source implementation and study robustness to non-Gaussian/autocorrelated load processes and parameter uncertainty (Phase I/II self-starting or robust designs).",2011.00098v1,https://arxiv.org/pdf/2011.00098v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:02:48Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Cost reduction|Other,D-optimal|Other,"Variable/General (dimension $d$; includes combinatorial arms $X\subset\{0,1\}^d$ and linear bandits $X\subset\mathbb{R}^d$)",Theoretical/simulation only|Other,Simulation study,TRUE,None / Not applicable,Not provided,https://courses.cs.washington.edu/courses/cse599i/20wi/resources/bandit_notes.pdf,"The paper proposes RegretMED, an experimental-design-based algorithm for regret minimization in stochastic linear bandits and combinatorial bandits under both bandit and semi-bandit feedback. At each epoch it solves a constrained design/measurement-allocation optimization that trades off estimated regret cost against information gain, using geometry-dependent confidence bounds based on the Tsirelson–Ibragimov–Sudakov inequality and Gaussian width rather than naive union bounds. The authors derive finite-time high-probability and expected regret bounds that scale with the Gaussian width of (near-optimal) subsets of the action set, yielding state-of-the-art dependence in several regimes, and provide a polynomial-time oracle-based solver for the semi-bandit design subproblem. They also show explicit instances where optimism-based strategies (UCB/Thompson sampling) can be arbitrarily suboptimal in semi-bandits, while RegretMED succeeds. Experimental simulations on a resource allocation instance (semi-bandit) and a 2D example (bandit) show lower regret than LinUCB, Thompson sampling, and combinatorial UCB baselines.","The core design matrix is $A_{\text{band}}(\lambda)=\sum_{x\in X}\lambda_x xx^\top$ for bandit feedback and $A_{\text{semi}}(\lambda)=\mathrm{diag}(\sum_{x\in X}\lambda_x xx^\top)$ for semi-bandit feedback. RegretMED chooses an allocation $\tau$ each epoch by minimizing an estimated regret cost $\sum_{x\in X} 2(\varepsilon_\ell+\hat\Delta_x)\,\tau_x$ subject to a TIS/Gaussian-width-based confidence constraint of the form $\mathbb{E}_\eta\big[\max_{x\in X}\frac{(x_\ell-x)^\top A_f(\tau)^{-1/2}\eta}{\varepsilon_\ell+\hat\Delta_x}\big]+\sqrt{2\sup_{x\in X}\|x\|_{A_f(\tau)^{-1}}^2\log(2^\ell 3/\delta)}\le 1/128$ (bandit version), with a simplified constraint for the computationally efficient semi-bandit solver. The geometric complexity driving bounds is $\bar\gamma(A_f)=\sup_{\varepsilon>0}\inf_{\lambda\in\Delta_{X_\varepsilon}}\mathbb{E}_\eta\big[\sup_{x\in X_\varepsilon} x^\top A_f(\lambda)^{-1/2}\eta\big]^2$.","The main regret bounds (Theorem 1) give gap-dependent regret scaling essentially like $\tilde O\big((\bar\gamma(A_f)+d)\,\log^2(\cdot)/\Delta_{\min}\big)$ plus lower-order terms, and minimax regret scaling like $\tilde O\big(\sqrt{(\bar\gamma(A_f)+d)T}\big)$ up to polylog factors. In the semi-bandit regime the leading expected gap-dependent term matches the known lower bound order $\Theta(d\log T/\Delta_{\min})$, improving prior work that incurred extra $\log^2(k)$ factors for general combinatorial structures. The paper provides explicit constructions where optimistic algorithms have asymptotic regret larger by a dimension-dependent factor, while RegretMED achieves substantially smaller regret. Simulation results (50 trials; one-standard-error bars) show RegretMED outperforming CombUCB1 and CTS-Gaussian on a resource allocation instance and outperforming LinUCB and Thompson sampling on a classic 2D counterexample where optimism struggles.","The authors assume Gaussian noise for simplicity (noting the results extend to sub-Gaussian noise) and often assume the optimal arm is unique. They note Algorithm 1 takes $\Delta_{\max}$ as input only to simplify analysis and can use an upper bound in practice with only logarithmic impact. They also acknowledge the fully general algorithm is not computationally efficient when $|X|$ is exponentially large, motivating the modified semi-bandit optimization that can be solved with oracle calls.","The approach depends on repeatedly solving nontrivial design/optimization subproblems and on accurate estimation of gap quantities, which may be sensitive to model misspecification (e.g., heavy-tailed noise, adversarial corruption) or temporal dependence not covered by the theory. Practical performance may hinge on tuning/approximations in the solver (e.g., stochastic Frank–Wolfe, rounding/sparsification), yet no implementation details or reproducible code are provided, limiting verifiability. The method is developed for linear reward models; extensions to generalized linear or nonparametric reward functions are nontrivial and not analyzed.",They explicitly leave a rigorous proof of the asymptotic optimality properties of RegretMED to future work. They also state they leave a full investigation of the partial-monitoring/graph-bandit/side-observation applications suggested by their framework to future work.,"Providing open-source implementations and standardized benchmarks would enable reproducibility and clearer guidance on solver/parameter choices in practice. Extending the design-based planning approach to settings with unknown/estimated noise variance, heteroskedasticity, or dependent observations (e.g., time series) would broaden applicability. It would be valuable to develop robust/self-normalized versions with heavy-tailed noise and to study computationally efficient variants for general bandit feedback (not only semi-bandit) with large combinatorial action sets.",2011.00576v2,https://arxiv.org/pdf/2011.00576v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:03:32Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Other,A-optimal|D-optimal|Bayesian D-optimal|Bayesian A-optimal|Not applicable,"Variable/General (design space Ω ⊂ R^d or general Polish space; examples include d=2,3,7 with mixture constraints)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper extends proportional volume sampling (PVS)—a randomized rounding/sampling approach for near-optimal experimental designs—from finite design spaces to general bounded closed Polish spaces, covering both classical (Λ=0) and Bayesian/regularized (Λ≻0) linear regression design. Using point-process machinery, it defines a PVS point process via Janossy densities and proves unbiasedness identities for the inverse information matrix and determinant, then derives expectation guarantees for A- and D-optimality when conditioning PVS on a fixed design size k, tightening prior bounds (notably for D-optimality). It shows PVS can be sampled in polynomial time by decomposing it as a superposition of a determinantal point process (DPP) and a Poisson point process, and provides a rejection-free sampler for size-conditioned PVS. Empirically, simulations confirm PVS improves over i.i.d. random designs and that optimizing the reference measure helps, but gains can be modest in practical problems. To improve practical performance, it introduces DOGS, a global search heuristic that uses discrete PVS as a subroutine, and demonstrates via numerical examples that DOGS is robust for low-dimensional, irregularly shaped design spaces though it can lag discrete exchange methods in higher dimensions.","PVS on a general space Ω is defined as a point process with Janossy densities j_n(x_1,…,x_n) ∝ det(\phi(x)^T\phi(x)+\Lambda)\,d\nu(x_1)…d\nu(x_n), normalized by det(G_\nu(\phi)+\Lambda)\,e^{\nu(\Omega)} where G_\nu(\phi)=\int_\Omega \phi(x)\phi(x)^T d\nu(x). Key unbiasedness identities are E[(\phi(X)^T\phi(X)+\Lambda)^{-1}] = (G_\nu(\phi)+\Lambda)^{-1} and E[det(\phi(X)^T\phi(X)+\Lambda)^{-1}] = det(G_\nu(\phi)+\Lambda)^{-1}. The process admits a decomposition as DPP(K,\nu) ∪ Poisson(\nu), with kernel K(x,y)=\sum_{i=1}^p \lambda_i\psi_i(x)\psi_i(y) obtained from the spectrum of G_\nu(\phi)^{1/2}(G_\nu(\phi)+\Lambda)^{-1}G_\nu(\phi)^{1/2}.","For size-conditioned designs (|X|=k), the paper gives an explicit expectation bound for D-optimality (Prop. 3.3) implying an average D-efficiency at least \(\big(k!/((k-p)!k^p)\big)^{1/p} \gtrsim 1-(p-1)/k\), improving earlier \(1-O(\sqrt{\log k}/k)\)-type rates in prior work. For A-optimality (Prop. 3.5), it bounds E[Tr((\phi(X)^T\phi(X)+\Lambda)^{-1}) | |X|=k] by a multiplicative factor times Tr((G_\nu(\phi)+\Lambda)^{-1}); in the non-Bayesian case (Λ=0) this recovers the sharp worst-case A-efficiency bound (k-p+1)/k. It proves PVS can be sampled in polynomial time via a DPP+Poisson decomposition and provides a rejection-free sampler for PVS conditioned on fixed size k. Simulations show PVS beats i.i.d. sampling (larger gains as Λ decreases), and DOGS often outperforms local search/exchange methods on low-dimensional complicated domains, but may underperform discrete exchange methods in higher dimensions (>5).","The authors state that, despite the elegance and tractability of general PVS, its practical improvements can be modest and in simple examples even limited, with loose theoretical bounds and PVS samples (even selecting the best among many) not getting arbitrarily close to optimal designs. They also note that in higher ambient dimension (e.g., d>5) their DOGS heuristic fails to match a discrete exchange method over reasonable candidate sets, suggesting diminished effectiveness as dimension grows. They remark that computing the Gramian may require numerical integration/Monte Carlo depending on Ω and φ, making the approach heuristic if the Gramian is approximated.","The strongest approximation guarantees are in expectation and for size-conditioned PVS; practitioners may require high-probability guarantees or robustness to model misspecification, which are not developed. The framework assumes independent homoscedastic errors and a linear model; extensions to correlated noise, heteroscedasticity, or nonlinear models are not addressed. Practical deployment also hinges on the ability to solve the relaxed design problem (and, for DOGS, repeated convex programs), which may be costly for large p or complex φ, yet scalability studies beyond the reported examples are limited. Comparisons are primarily against a small set of heuristics (LSA/ExM and discretized ExM); broader benchmarking against modern optimal design solvers or Bayesian design algorithms could change conclusions.","They propose investigating hybrid strategies to overcome DOGS plateaus and high-dimensional shortcomings, such as alternating DOGS with local search episodes. They also suggest combining heuristics adaptively, for example using multi-armed bandit methods to choose among move types/algorithms. Additionally, they note that the Bayesian A-optimality bound (Prop. 3.5) appears improvable and indicate tightening it as a potential research direction.","Developing finite-sample/high-probability performance guarantees for size-conditioned PVS (and for DOGS-style iterative use) would make the methods more actionable for practitioners. Extending PVS-based design to dependent observations (time series/spatial), heteroscedastic models, generalized linear models, or nonlinear regression would broaden applicability. Providing open-source, optimized implementations (including fast Gramian approximation and scalable DPP sampling) and standardized benchmarks across canonical DOE problems would clarify practical tradeoffs. Finally, studying adaptive/sequential experimental design using PVS (updating ν as data arrive) could connect the point-process approach to active learning and Bayesian optimization workflows.",2011.04562v2,https://arxiv.org/pdf/2011.04562v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:04:11Z
TRUE,Split-plot|Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Model discrimination|Prediction|Cost reduction|Other,Not applicable,Variable/General (policy parameter vector $\beta\in\mathbb{R}^p$; examples include scalar treatment probability and discrete-type targeting),Food/agriculture|Other,Exact distribution theory|Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/2011.08174|https://www.socialscienceregistry.org/trials/9945|https://precisiondev.org/weather-forecasting-product-for-punjab-pakistan/,"The paper develops experimental designs for policy learning and welfare maximization when there is unknown within-cluster interference (spillovers) and only a finite number of large, approximately independent clusters. It proposes a single-wave paired-cluster “local perturbation” design: within each cluster pair, treatment is independently randomized with slightly different probabilities (or policy parameters) across the two clusters, enabling consistent estimation of the marginal policy effect (the derivative of welfare with respect to policy parameters) and permutation-based tests of policy optimality with finitely many clusters. The design also allows separate estimation of direct effects and marginal spillover effects by pooling treated/control observations within and across pairs. It then introduces a multi-wave adaptive experiment that uses sequential cross-fitting across cluster pairs and gradient-descent updates to learn welfare-maximizing policy parameters while controlling both out-of-sample and in-sample regret despite repeated sampling and interference. The methods are implemented in a large-scale field experiment in Pakistan (tehsil-level clusters; >250,000 farmers) delivering geo-localized weather forecasts, documenting spillovers and showing that increasing coverage from 50% to about 70% appears sufficient, generating large cost savings at scale; simulations calibrated to prior network experiments further illustrate performance gains over grid/saturation approaches.","Single-wave marginal policy effect estimator for a cluster pair $(k,k+1)$ uses a difference-in-differences slope: $\widehat M_{c}^{(k,k+1)}(\beta)=\frac{1}{2\eta_n}(\bar Y^{(k)}_{1}-\bar Y^{(k)}_{0})-\frac{1}{2\eta_n}(\bar Y^{(k+1)}_{1}-\bar Y^{(k+1)}_{0})$, where clusters are randomized with parameters $\beta\pm\eta_n e_1$. The marginal effect decomposes as $M(\beta)=\int[\pi(x;\beta)S(1,x,\beta)+(1-\pi(x;\beta))S(0,x,\beta)+\partial_\beta\pi(x;\beta)\Delta(x,\beta)]\,dF_X(x)$, combining marginal spillover and direct components. The multi-wave design updates policies via projected gradient steps using gradients estimated from a different (next) cluster pair (sequential cross-fitting) to avoid bias from repeated sampling.","The paper provides consistency and asymptotic normality results for the paired-cluster marginal policy effect estimator under weak within-cluster dependence and fixed (finite) number of clusters, and proposes sign-permutation randomization tests for $M(\beta)=0$ with valid asymptotic size. For the adaptive multi-wave design, it derives high-probability regret bounds: under strong concavity, out-of-sample regret scales on the order of $p/K$ (with $K$ clusters and $T$ iterations proportional to $K$) and in-sample regret scales like $p\log(T)/T$; under additional smoothness and larger samples, out-of-sample regret can decay exponentially in $K$. In the Pakistan field experiment, the estimated marginal policy effect is significant and negative at 50% coverage (implying welfare improves by increasing coverage), but small and not significant at 70% coverage, suggesting diminishing diffusion benefits and that ~70% treatment probability suffices. They report that using these marginal-effect insights can reduce program costs by about $\$1,000,000/year when scaled in Pakistan.","The authors note key assumptions for applicability: a finite number of large clusters with (approximate) independence between clusters, weak dependence within clusters (captured via restrictions on dependence/maximum degree growth), no carryover (dynamic) treatment effects in the main adaptive framework, and (quasi)concavity/strong concavity conditions for global optimality guarantees. They also acknowledge that if clusters are only approximately independent, between-cluster correlations must be asymptotically negligible. They emphasize that when concavity fails (e.g., spillovers only after enough treated), their method may converge to a local optimum and they discuss strict quasi-concavity extensions in the appendix.","The paired-cluster perturbation design requires implementers to vary treatment probabilities across large geographic clusters; in many policy settings this may be politically/ethically constrained or operationally infeasible, potentially limiting external applicability. The framework leans on homogeneity of the outcome model across clusters (common $m(\cdot)$ up to fixed effects); in practice, heterogeneous spillover mechanisms across clusters could make the estimated marginal policy effect less transportable even if covariate balance holds. The adaptive algorithm depends on repeated experimentation waves and timely outcome measurement; many outcomes of interest (e.g., long-run health/education) are slow-moving, making multi-wave learning difficult. Finally, while the paper provides strong theoretical guarantees, it does not provide ready-to-use open-source software, which can hinder uptake and faithful implementation of the randomization and inference procedures.","The authors propose studying settings where clusters are not fully disconnected (allowing cross-cluster interference), where clusters themselves must be estimated (e.g., via graph clustering), and where clusters have different covariate distributions (with improved matching/balancing). They also highlight interest in analyzing regimes where the degree of interference grows proportionally with sample size, building on their consistency results that allow weaker dependence conditions. They identify partially observed networks (and the costs/benefits of collecting network data) as an open direction, extending their value-of-network-data results.","Developing self-contained software packages (e.g., in R/Python) that implement paired-cluster perturbation randomization, sequential cross-fitting updates, and permutation-based inference would materially improve reproducibility and adoption. Empirically, more benchmarked comparisons against alternative interference-robust policy-learning approaches (e.g., saturation designs with model-based extrapolation, cluster-level Thompson sampling/bandits, or doubly robust estimators under partial interference) would clarify when the proposed approach is most advantageous. Extending the framework to settings with time-varying interference and policy-dependent network formation (where the network changes as treatment scales) would address important diffusion programs. Finally, studying robustness to misspecification of the functional form of $\pi(x;\beta)$ and to imperfect compliance/treatment take-up under interference would increase practical relevance.",2011.08174v9,https://arxiv.org/pdf/2011.08174v9.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:04:56Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Parameter estimation|Robustness,D-optimal|Not applicable,Variable/General (case studies: 2 decision variables in Case Study 1; 4 decision variables in Case Study 2),Manufacturing (general)|Pharmaceutical,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a safe, Gaussian process-based model-based design of experiments (GP‑MBDoE) framework for nonlinear dynamic systems subject to both parametric uncertainty and plant–model mismatch. One GP is used as a hybrid surrogate to learn model mismatch in safety/feasibility constraints (using the mechanistic model as a prior mean), while adaptive trust regions restrict exploration to regions where the local GP approximation is reliable. A second GP surrogate is trained on in-silico simulations of the mechanistic model to propagate parameter uncertainty into the information metric, enabling stochastic/robust information maximization. Chance constraints are enforced via a distributionally-robust reformulation (Chebyshev–Cantelli) using GP-predicted constraint mean/variance. Two in-silico flow reactor case studies (wrong mechanism; corrupted measurements/pump/HPLC-like bias) show GP‑MBDoE maintains feasibility across iterations while competing robust MBDoE approaches can violate constraints, with comparable or improved parameter-estimation performance metrics.","The experimental design problem maximizes an expected Fisher-information-based criterion, e.g. $\max_u\,\mathbb{E}_{\theta\sim\Theta}\,\psi\big( (\partial \hat x/\partial\theta)^\top\Sigma_{\exp}(\partial \hat x/\partial\theta)+M_0\big)$ subject to model dynamics and chance constraints. Constraint mismatch is modeled with a GP posterior mean/variance $m_{g_i}(u),\,\Sigma_{g_i}(u)$ and chance constraints are enforced conservatively via Chebyshev–Cantelli: $m_{g_i}(u)+r\sqrt{\Sigma_{g_i}(u)}\le 0$ with $r=\sqrt{(1-\varepsilon)/\varepsilon}$. Trust regions constrain successive designs: $(u-u_k)^\top(u-u_k)\le \min_i R_i$. The final optimization solved is $\min_u\,\hat m_J(u,\mu_\theta,\Sigma_\theta)-\alpha_J\sqrt{\hat\Sigma_J(u,\mu_\theta,\Sigma_\theta)}$ subject to the tightened GP constraints and trust region.","Across both in-silico case studies (flow reactors), GP‑MBDoE is reported to keep all experiments feasible with respect to the probabilistic constraints (target satisfaction probability 0.9), whereas Monte-Carlo robust MBDoE (MC‑MBDoE) and disturbance-estimation MBDoE (DE‑MBDoE) produce constraint violations under structural mismatch. In Case Study 1 the fitted-model chi-square statistics are similar across methods (reported 95% $\chi^2$: GP‑MBDoE 2018; MC‑MBDoE 1965; DE‑MBDoE 1888; reference 74.47) but only GP‑MBDoE remains feasible throughout. In Case Study 2 GP‑MBDoE improves the reported fit vs baselines (reported $\chi^2$: GP‑MBDoE 1805; MC‑MBDoE 3892; DE‑MBDoE 3079; reference 132.14) while maintaining feasibility. Reported average compute times: GP‑MBDoE 42s/121s (Case 1/2) vs MC‑MBDoE 250s/500s vs DE‑MBDoE 20s/62s.","The authors note feasibility guarantees rely on accurate GP mean/variance estimates, which can be unreliable with small datasets, motivating the use of local trust regions; they also state the trust-region conservatism can increase the experimental burden. They mention the main computational burden is training the second GP surrogate for the objective and that scalability can be problematic as the number of design variables and parameters increases (suggesting sparse/variational GP methods). They also note that global GPs can suffer similarly to confidence-region approaches because they require sufficient data, hence local approximations are used.","The chance-constraint tightening uses Chebyshev–Cantelli, which is distributionally robust but often very conservative and may lead to overly cautious designs or reduced information gain; practical tuning of $r$ can undermine formal guarantees. The method’s performance is demonstrated only in-silico on two case studies; real experimental validation (with true measurement/actuation constraints, delays, and nonstationary disturbances) is not shown. The approach assumes GP regression is an adequate surrogate for mismatch/constraints and that mismatch is learnable from limited data; in high-dimensional input spaces the trust-region approach may still require many experiments to maintain reliable constraint models. Comparisons are limited to two baselines (MC-backoff and constant disturbance estimation); other safe learning/control and safe BO approaches could be relevant benchmarks.","Future work is stated to focus on Bayesian calibration of the mechanistic model itself, estimating kinetic parameters through their posterior distribution to obtain non-asymptotic uncertainty quantification. The authors also suggest diagnosing structural mismatch and exploring alternative model structures, potentially using artificial neural networks to infer improved kinetic models from data.","Developing open-source implementations and benchmark suites would help adoption and reproducibility, especially for GP‑MBDoE with trust-region and chance-constraint tuning. Extending the method to handle autocorrelated noise/time-series residuals, non-Gaussian measurement error, and multi-output correlated GPs could improve realism for reactor data. More principled risk measures (e.g., CVaR constraints) or less conservative probabilistic bounds (when distributional assumptions are justified) could reduce conservatism relative to Chebyshev–Cantelli. Formal sample-complexity or regret-style guarantees for safety under iterative trust-region updates, plus experimental (non-simulated) case studies, would strengthen evidence of practical safety and efficiency.",2011.10009v2,https://arxiv.org/pdf/2011.10009v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:05:35Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Model discrimination|Prediction|Cost reduction|Other,Not applicable,Variable/General (demonstrations include 2 inputs; general method for D-dimensional input domain),Healthcare/medical|Other|Theoretical/simulation only,Simulation study,TRUE,Python|Other,Not provided,https://ml4molecules.github.io|http://ai4health.io,"The paper proposes a sequential design of experiments method to efficiently validate designed biomolecular networks by selecting experimental input settings that best reveal discrepancies between a mechanistic model’s predicted response surface and the (unknown) true response. It models the discrepancy function as a squared-exponential Gaussian process and uses Bayesian-optimization-style acquisition functions (maximum variance, expected improvement, and UCB variants) to choose the next experiment. A key contribution is a principled stopping criterion based on a distribution over an RMSE-like surface-wide discrepancy metric computed from GP posterior function draws, enabling accept/reject decisions at user-specified confidence thresholds. The approach is evaluated on simulated biochemical network/logic-gate surfaces (e.g., AND/OR/XOR), comparing acquisition strategies and examining sensitivity/specificity trade-offs versus the minimum number of experiments required before applying the stopping rule. Results indicate the stopping criterion can often accept/reject networks with relatively few runs, with termination time increasing as required certainty increases and ambiguous near-threshold cases sometimes remaining inconclusive within a budget.","The discrepancy is defined as $g(x)=y(x)-\eta(x,\theta_m)=\psi(x)+\epsilon(x)$, separating mechanistic model prediction from model inadequacy and experimental noise. A GP posterior over $g(x)$ is used to draw function samples $f_s$ on a uniform grid $X^*$, and for each draw an RMSE metric is computed as $M_h=\sqrt{\frac{1}{m}\sum_{j^*=0}^{m} f_{sh}(x_{j^*}^*)^2}$. The stopping rule computes $p(M_{\text{true}}<T_{\text{avg}})$ and accepts/rejects when this probability exceeds $p_{\text{accept}}$ or falls below $p_{\text{reject}}$ (after a minimum number of initial runs).","Across 12 simulated biochemical networks (run 10 times each), the stopping rule’s sensitivity/specificity depends on both the minimum number of experiments before applying the criterion ($n_{\min}$) and the certainty thresholds ($p_{\text{accept}},p_{\text{reject}}$), with a trade-off between false acceptances/rejections and required runs. The authors select $n_{\min}=8$ as a practical choice because improvements beyond this were small in their study. With $T_{\text{avg}}=0.2$ (domain-expert set), higher certainty settings (e.g., $p_{\text{accept}}=0.99$, $p_{\text{reject}}=0.01$) increased the number of experiments to termination. In acquisition-function comparisons on 8 test surfaces, no method dominated for both mean and maximum discrepancy, but maximum variance, high-variance-weighted UCB ($\kappa=3$), and Sobol sampling tended to perform best overall. The method could be inconclusive for surfaces with true RMSE very close to the threshold even after 50 observations under a fixed budget.","The study evaluates the approach on simulated (computer-model-generated) biochemical networks rather than real experimental data, and it assumes essentially noiseless observations by fixing a very small noise variance ($\sigma_n^2=10^{-4}$). The authors note that standard Bayesian optimization acquisition functions were used and that this component “could be improved.” They also point out that MAP hyperparameter inference can be over-confident for very small datasets, motivating a minimum-number-of-experiments safeguard before applying the stopping rule.","The stopping decision depends on several user/domain-expert choices (e.g., $T_{\text{avg}}$, uniform grid for $X^*$, number of GP function draws, and probability thresholds), and robustness of conclusions to these tuning choices is not systematically analyzed. The approach uses a squared-exponential kernel and MAP hyperparameter fitting, which can be brittle under model misspecification or heterogeneous/noisy wet-lab measurements; a fully Bayesian treatment or robustness checks would likely be needed in real deployments. The acquisition comparison appears limited (few surfaces, limited repeats for DOE criteria) and does not include other discrepancy-focused active learning criteria (e.g., integrated variance reduction/Bayesian quadrature criteria aligned with the RMSE/integral objective). Practical constraints common in wet-lab DOE (batching, hard-to-change factors, constraints on feasible concentration combinations, replication for noise estimation) are not incorporated.","The authors propose using acquisition functions tailored for Bayesian quadrature, since the goal is to maximize certainty about an integral-like quantity (surface-wide discrepancy), and they cite related work on modeling non-negative functions. They also aim to remove the need for the minimum number of experiments parameter by integrating over Gaussian process hyperparameters rather than relying on MAP point estimates.","Validate the method on real biomolecular network experiments with realistic measurement noise, replicates, and potential systematic biases, and study sensitivity of accept/reject outcomes to the choice of $T_{\text{avg}}$ and grid/discretization for $X^*$. Extend the framework to constrained/batched experimental settings and to mixed continuous/discrete inputs typical of synthetic biology designs. Explore discrepancy models beyond stationary SE kernels (e.g., nonstationary or monotone/shape-constrained GPs) and fully Bayesian hyperparameter inference to reduce overconfidence. Provide an open-source implementation and benchmarking suite to facilitate adoption and fair comparison against alternative active-validation and model-checking strategies.",2011.10575v2,https://arxiv.org/pdf/2011.10575v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:06:13Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/1902.05569|https://arxiv.org/abs/2004.05271|https://arxiv.org/abs/2004.04702|https://arxiv.org/abs/1503.03757|https://arxiv.org/abs/astro-ph/0108013|https://arxiv.org/abs/2012.01956|http://osc.edu/ark:/19495/f5s1ph73,"The paper forecasts statistical constraints on the matter clustering amplitude $\sigma_8(z)$ from combining galaxy cluster number counts above an observable-mass threshold with stacked cluster weak-lensing profiles $\Delta\Sigma(r_p)$, under a framework analogous to galaxy–galaxy lensing. The main nuisance parameters are the true mass corresponding to the selection threshold and the log-scatter $\sigma_{\ln M}$ between the mass proxy and true mass near that threshold; the authors quantify how priors on $\sigma_{\ln M}$ control degradation of $\sigma_8$ constraints. Forecasts are produced for survey scenarios similar to DES (Stage III wide), Roman HLS (Stage IV deep), and LSST (Stage IV wide), finding aggregate $\sigma_8$ precision of 0.26%, 0.24%, and 0.10% respectively if $\Delta\sigma_{\ln M}\le 0.01$, with ~20% degradation for $\Delta\sigma_{\ln M}=0.05$ (DES/HLS) or $\Delta\sigma_{\ln M}=0.016$ (LSST). The lensing covariance combines analytic Gaussian terms (shape noise + LSS) with non-Gaussian small-scale contributions calibrated from Abacus simulations and validated with ray-tracing simulations. The paper also provides analytic approximations to scale results to alternative survey assumptions and discusses targeted Roman observations of thousands of clusters as a cost-effective way to tighten constraints in selected redshift bins.","Cluster number density above an observable threshold is $\bar n=\int dM\,(dn/dM)\,\phi(M_{\rm obs}^{\rm th}|M)$, with selection function modeled as a complementary error function for lognormal proxy scatter: $\phi=\tfrac12\,\mathrm{erfc}\!\left[(\ln M_{\rm obs}^{\rm th}-\ln M)/\sqrt{2}\,\sigma_{\ln M}\right]$. The stacked weak-lensing observable is the excess surface density $\langle\Delta\Sigma(r_p)\rangle$ computed from the cluster–matter correlation function $\xi_{cm}(r)=\int dM\,(dn/dM)\,\xi_{hm}(r|M)\,\phi$, combining one-halo (NFW) and two-halo (bias $b(M)\,\xi_{\rm lin}$) terms. Cosmological and nuisance constraints are obtained via a Fisher matrix for data vector $D=(N,\langle\Delta\Sigma\rangle)$ and parameters $(\ln\sigma_8,\ln M_{\rm obs}^{\rm th},\sigma_{\ln M})$, with priors added on $\sigma_{\ln M}$.","Assuming $\Delta\sigma_{\ln M}\le 0.01$, the forecast aggregate precision on $\sigma_8$ is 0.26% (DES-like, 5000 deg$^2$), 0.24% (Roman HLS-like, 2000 deg$^2$ deep), and 0.10% (LSST-like, 18,000 deg$^2$). A 20% degradation of aggregate $\sigma_8$ precision occurs for $\Delta\sigma_{\ln M}=0.05$ in DES/HLS-like surveys and for $\Delta\sigma_{\ln M}=0.016$ in LSST-like surveys. For a single $\Delta z=0.1$ bin centered at $z=0.7$ with $M\ge 10^{14}\,h^{-1}M_\odot$ and $\Delta\sigma_{\ln M}=0.01$, the forecast errors on $\sigma_8(z=0.7)$ are 0.91% (DES-like), 0.83% (HLS-like), and 0.60% (LSST-like), with corresponding mean-mass errors of 1.9%, 1.3%, and 0.57%. Excluding small radii $r_p\lesssim 0.5\,h^{-1}{\rm Mpc}$ yields negligible information loss for mean-mass constraints, suggesting robustness to some small-scale systematics. A targeted ~1 month Roman program observing ~$\sim$2600 clusters can reach ~$\sim$0.5% on $\sigma_8(z=0.7)$ alone, or ~$\sim$0.33% combined with HLS under tight scatter priors.","The authors emphasize that realizing the forecast statistical precision requires stringent control of systematics, including multiplicative shear biases, photometric-redshift errors, cluster mis-centering, baryonic effects on mass profiles, boost-factor (source contamination) uncertainties, and incompleteness/contamination tails in $P(\ln M_{\rm obs}|\ln M)$. They highlight anisotropic selection/projection effects as “the single most challenging systematic,” potentially altering stacked lensing signals by 20–30% on some scales and requiring highly realistic end-to-end selection simulations. They also note their forecasts focus on statistical uncertainties and assume external knowledge (priors) of $\sigma_{\ln M}$ rather than fully self-calibrating it within the baseline $N+\Delta\Sigma$ framework.","The forecast assumes a simplified selection model (erfc/lognormal scatter near threshold) and treats $\sigma_{\ln M}$ as an external prior; real surveys may have mass-, redshift-, and observable-dependent non-Gaussian scatter and selection incompleteness that are harder to compress into a single parameter per bin. The Fisher-matrix approach and modeling choices (e.g., $\xi_{hm}=\max(\xi_{1h},\xi_{2h})$, fixed concentration–mass relation, and neglect of some cross-covariances except where tested) may understate modeling uncertainty or parameter degeneracies in full likelihood analyses. Survey assumptions about cluster detectability at a fixed mass threshold across “Stage III/IV” scenarios may be optimistic, since cluster-finding performance and photo-$z$ quality can vary with depth, blending, and observing conditions. No public implementation details are provided, which limits reproducibility of the exact covariance construction and forecast pipeline.","They state plans to explore (i) multi-observable strategies to constrain shear/photo-$z$ systematics by introducing nuisance parameters and combining cluster weak lensing with galaxy–galaxy lensing and cosmic shear, and (ii) using combinations of $w_{cg}$, $w_{gg}$, and $\Delta\Sigma$ to self-calibrate projection effects and better constrain $\sigma_{\ln M}$. They also indicate future exploration of leveraging targeted Roman observations to validate results and probe specific redshift/halo-mass ranges suggested by future data tensions.","Developing a fully self-starting or hierarchical Bayesian framework that jointly models selection, non-Gaussian proxy scatter, and anisotropic projection bias across observables (optical/X-ray/SZ) could reduce reliance on external priors and better propagate systematics into $\sigma_8(z)$ constraints. Extending the methodology to include realistic photometric-redshift error propagation, intrinsic alignments, and correlated shear calibration uncertainties across redshift bins would make the forecasts more directly comparable to full survey analyses. Providing open-source code (or a reproducible notebook) and benchmark datasets would improve transparency and enable community validation of covariance modeling choices (Gaussian + simulation-based non-Gaussian stitching). Additional validation on multiple simulation suites and with end-to-end mock cluster catalogs (including cluster finder pipelines) would help quantify sensitivity of results to the dominant anisotropic selection/projection systematic.",2012.01956v2,https://arxiv.org/pdf/2012.01956v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:07:02Z
TRUE,Optimal design|Other,Parameter estimation|Robustness,Not applicable,Variable/General (p covariates; simulations use p=2; theory highlights p=1 case),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/kapelner/GreedyExperimentalDesign/blob/master/hybrid_paper,"The paper proposes a new two-arm randomized experimental design that hybridizes optimal nonbipartite matching (to create covariate-similar pairs) with subsequent imbalance optimization across treatment arms. After forming matched pairs using an optimal nonbipartite matching algorithm (minimizing total within-pair distance, using Mahalanobis distance), the design keeps pairs fixed and then optimizes overall covariate balance by either rerandomization restricted to the matched-pair assignment space (MR) or a greedy pair-switching heuristic applied to within-pair covariate differences (MG). The goal is to reduce MSE for the difference-in-means estimator of an additive average treatment effect while retaining robustness to nonlinear response models (where distributional balance beyond first moments matters). The authors provide theory (notably for one covariate under uniformity) showing MG can achieve improved imbalance rates (reported as O_p(n^{-4}) in that setting) and argue the resulting restricted randomization retains high “degree of randomness” comparable to matching. Extensive simulations across linear, nonlinear, and mixed response models show MR/MG match or outperform pure imbalance-optimizing designs (rerandomization, greedy switching) in linear settings and match or outperform pure matching in nonlinear settings, yielding a robust “best-of-both” procedure.","Treatment effect is estimated by the difference-in-means written as $\hat\beta=\frac{w^\top y}{2n}=\tfrac{1}{2}(\bar Y_T-\bar Y_C)$. Covariate imbalance is measured by the Mahalanobis distance $d_M(w)=n(\bar x_T-\bar x_C)^\top \hat\Sigma_X^{-1}(\bar x_T-\bar x_C)$. Rerandomization defines a restricted design by accepting only allocations with $d_M(w)\le a$ (either from all balanced allocations, or from the matched-pair allocation space). The hybrid MG applies greedy switching to the vector of within-pair covariate differences (constructed after optimal nonbipartite matching) while preserving the constraint that within each matched pair, one unit is treated and the other is control.","In an illustrative simulation with $2n=40$ and one covariate, the hybrid matching+greedy (MG) achieved lower imbalance than greedy alone and substantially lower MSE under a nonlinear response compared to either greedy imbalance-optimization or matching alone (e.g., average squared error: G 0.04350, M 0.00614, MG 0.00273 under the nonlinear example). In broader simulations with $n=100$ and two covariates across five response models (null, linear, mostly-linear+slightly-nonlinear, linear+nonlinear, nonlinear), MR and MG were consistently best or tied for best MSE; pure imbalance-optimizing designs dominated in the purely linear model while matching-based designs dominated in the purely nonlinear model. Theoretical results for the univariate uniform-covariate case state that after matching, applying greedy switching to within-pair differences yields an imbalance rate of $O_p(n^{-4})$, combining the $O_p(n^{-3})$ greedy-switching rate with an additional $O_p(n^{-1})$ scaling from matched differences. The authors also argue (based on prior results for the greedy switching procedure) that the hybrid designs retain a high degree of randomness (important for protection against unobserved covariates) because the number of switches is small and the allocation space remains large.","The authors note that theoretical imbalance results for MG beyond the univariate case are not available; they explicitly focus theory on $p=1$ and state they are not aware of results for imbalance after optimal nonbipartite matching when $p>1$. They also state extending imbalance theory beyond uniformly distributed covariates is difficult; for normally distributed covariates, the distribution of within-pair gaps varies by tail/center, complicating arguments, and they rely on simulation evidence for that case.","The method’s practical performance may depend on the choice of distance metric (they emphasize Mahalanobis distance), and sensitivity to scaling/collinearity and mixed continuous/discrete covariates could be material without additional guidance. Computational cost could be substantial for large samples since optimal nonbipartite matching and repeated rerandomization/search may be heavy, yet runtime/complexity tradeoffs and scalability benchmarks are not fully characterized. The framework is limited to two-arm, forced-balance designs with a mirror property and focuses on additive treatment effects; extensions to unequal allocation ratios, clustered/split-plot constraints, or interference are not addressed. Real-world empirical validations (beyond simulations) are limited; results may vary under covariate measurement error, missingness, or strong outcome-covariate interactions that violate assumed robustness conditions.","They propose extensions to other matching types (e.g., ratio matching, multi-arm matching, matching with discrete covariates, differential covariate importance/weighting), including caliper matching that can leave some units unmatched and would require redesigning the greedy step. They suggest studying performance with alternative estimators such as OLS adjustment (noting potential asymptotic MSE gains and compatibility with matching) and analyzing robustness to imbalance in unobserved covariates via randomness considerations. They also propose adapting the hybrid approach to sequential/online settings by combining on-the-fly matching with sequential rerandomization ideas.","Developing formal finite-sample and asymptotic imbalance/MSE guarantees for $p>1$ (and for broader covariate distributions) would strengthen the method and guide parameter choices (e.g., rerandomization thresholds). Providing principled, data-driven tuning rules for the rerandomization acceptance rate and the greedy-switch stopping criteria—possibly optimizing predicted MSE under model uncertainty—would improve usability. Extending the approach to constrained randomization settings common in practice (cluster randomized trials, stratification with many strata, or split-plot/hard-to-change factors) would broaden applicability. Packaging and benchmarking scalable implementations (including parallelization and memory-efficient matching) and validating on multiple real experimental datasets would help adoption.",2012.03330v2,https://arxiv.org/pdf/2012.03330v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:07:42Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical|Other,Simulation study|Other,TRUE,Other,Not provided,http://upplysingaoflun.ecn.purdue.edu/~qobi/tpami2020|https://github.com/ptirupat/ThoughtViz|https://github.com/reedscot/icml2016|https://colab.research.google.com/drive/1Y6HyToZv6HkRKK48D663Fthv8-4n-nI|https://colab.research.google.com/drive/1aBrz3mbraekDqopFFXIWG1Wm4WiDe36R|https://colab.research.google.com/drive/1et3Pnlv9Iivtlcku8ck9-KEe2cKHijmv,"This paper responds to criticisms that block-design EEG classification results are primarily driven by temporal correlation, arguing that such claims are overstated and sometimes methodologically flawed. Using the previously published block-design visually evoked EEG dataset (40 classes, 6 subjects) plus two additional datasets (blank-screen intervals between blocks; and a rapid/interleaved presentation replication), the authors evaluate several deep EEG classifiers under proper preprocessing (bandpass + notch filtering and per-channel z-scoring). They report that, after appropriate filtering (especially high-gamma 55–95 Hz), pooled-subject classification on the block-design data remains significant (about 50% accuracy over 40 classes) while blank-screen and rapid-design-with-block-label tests are at or near chance, suggesting minimal temporal-correlation contribution. They further show that per-subject analysis and protocol choices that extend session duration and reduce breaks can amplify temporal-correlation artifacts and can reproduce the criticized high accuracies only when data are intentionally “contaminated.” Overall, the work emphasizes protocol hygiene (short sessions, breaks, proper filtering, subject pooling) for EEG decoding studies and reframes the block-design criticism as not generally applicable when studies are correctly designed and analyzed.",Not applicable (no DOE/design-construction equations; the paper centers on EEG experimental protocols and classification evaluation).,"On the block-design dataset (BDVE) with proper filtering, EEG-ChannelNet achieves ~48.8% accuracy in the 55–95 Hz band (chance 2.5%) and ~31.3% in 5–95 Hz; prior ~80% results are attributed to slow-drift contamination when using raw EEG. On blank-screen data (BDB), models classify preceding/following classes at ~3.8–9.1% (chance 5%), indicating minimal temporal-correlation signal. On rapid-design data (RDVE), accuracy with true labels is at chance (~2–11%), while with block-level (incorrect) labels inflation is modest (up to ~15.8% pooled for EEG-ChannelNet; roughly ≤9 percentage points above chance in their summarized claim), far below ~53% reported in the criticized work. They replicate high “temporal correlation” accuracies only by intentionally contaminating signals (e.g., incorrect filtering along the channel dimension), especially under per-subject evaluation.","The authors note that both their and Li et al.’s experiments are small-scale, and that determining the correctness/incorrectness of the findings requires further examination with larger-scale studies. They also state they did not have access to Li et al.’s code despite an agreement to share it, which limited exact replication of Li et al.’s analysis steps; they therefore used their own implementation.","The work’s conclusions depend on specific datasets, preprocessing choices (e.g., band definitions, Butterworth filtering, z-scoring), and selected deep models/hyperparameters; different pipelines (artifact rejection/ICA, baseline correction, different temporal windows) might change sensitivity to drift/correlation. Statistical uncertainty around reported accuracies (confidence intervals, significance testing across resamples/subjects) is not emphasized in the excerpt, which can matter with only six subjects. The study focuses on visual stimulus classification with fixed timing; results may not generalize to other EEG paradigms (e.g., motor imagery, continuous tasks) or to settings with stronger autocorrelation/nonstationarity.","They explicitly suggest that, given the small scale of both studies, the presence and impact of temporal correlation and the validity of conclusions should be further examined (i.e., more data and additional verification are needed). They also argue for deeper joint effort between machine learning and cognitive neuroscience communities to ensure appropriate cognitive experiment designs and avoid erroneous outcomes.","A valuable extension would be systematic robustness analysis across preprocessing alternatives (artifact removal, baseline correction, drift modeling), temporal-window lengths, and cross-validation schemes that respect time/order. Larger multi-site datasets with more subjects and multiple sessions per subject would help quantify generalization and separate subject-specific vs stimulus-specific signals. Providing standardized open-source implementations and reproducible pipelines (including metadata about sessions/breaks and stimulus order) would enable stronger replication and fairer comparisons across designs.",2012.03849v1,https://arxiv.org/pdf/2012.03849v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:08:12Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Computer experiment|Other,Parameter estimation|Optimization|Cost reduction|Other,Not applicable,Variable/General (discrete uncertainty class Θ and action set Ψ; examples use nθ=nψ=64; spring-mass-damper uses n=16 springs with nθ=64 and |Ψ|=n),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"This paper proposes a resource-constrained extension of Optimal Experimental Design using the Mean Objective Cost of Uncertainty (MOCU), targeting settings where evaluating the full cost matrix J(θ,ψ) over Θ×Ψ is computationally prohibitive. The authors replace expensive intermediate MOCU computations with a Gaussian Process (GP) surrogate trained on sparse samples of (θ,ψ), and refine the surrogate adaptively based on experimental measurements and a leave-one-out sensitivity criterion restricted to high-posterior-probability regions. The resulting procedure is a sequential (adaptive) OED scheme: MOCU selects experiments to reduce uncertainty about θ and improve decision quality, while posterior contraction triggers targeted additional model evaluations to improve the surrogate where it matters. Performance is demonstrated via Monte Carlo studies on a synthetic cost surface, a multifidelity variant (cheap vs expensive model), and a coupled spring–mass–damper control/design example. Results show adaptive refinement improves policy-quality relative to a static surrogate at similar evaluation budgets, while full MOCU achieves better performance with fewer experiments but requires orders-of-magnitude more model evaluations; the recommended approach depends on the computation-versus-experiment cost trade-off.","MOCU is defined for a discrete uncertainty class Θ and action set Ψ with cost J(θ,ψ). The robust action under a posterior is ψ_{ρ(θ|x,y)}=argmin_{ψ∈Ψ} E_{ρ(θ|x,y)}[J(θ,ψ)], and the mean objective cost of uncertainty is M_Ψ(θ|x,y)=E_{ρ(θ|x,y)}[J(θ,ψ_{ρ(θ|x,y)})−J(θ,ψ_θ)] where ψ_θ=argmin_{ψ∈Ψ}J(θ,ψ). The next experiment is chosen by x* = argmin_{x∈X} E_y[M_Ψ(θ|x,y)]. The paper’s computational contribution is to approximate the full cost matrix J(θ,ψ) with a GP surrogate \tilde{J}(θ,ψ) trained on sparse samples and refined adaptively via leave-one-out sensitivity within the posterior’s inner 68% region.","In the single-model synthetic example (nθ=nψ=64), adaptive surrogate-aided MOCU (32 initial + up to two refinements of 8 points; ≤48 total) outperforms the non-adaptive surrogate (48 static points) in average achieved cost after enough experiments for posterior collapse (reported as becoming apparent at roughly ~100 experiments). Full MOCU requires evaluating the entire matrix (64^2=4096 cost evaluations) and achieves comparable decision quality with roughly an order of magnitude fewer experiments than adaptive surrogate-aided MOCU in their comparisons. In a multifidelity example, using mostly cheap-model samples plus selective expensive-model refinement yields statistically significant improvement over non-adaptive sampling, with improvements becoming apparent after about ~128 experiments. In the spring–mass–damper application (n=16 springs; nθ=64; |Ψ|=n), adaptive sampling again improves over a static surrogate, while full MOCU reaches similar accuracy with fewer experiments at the expense of far more model evaluations.",None stated.,"The method is developed for discrete Θ and Ψ and relies on being able to sample and fit a GP surrogate for J(θ,ψ) (treated as a 2D input), so scalability to continuous/high-dimensional design/control spaces or very large discrete sets is not established. The adaptive refinement heuristic (triggering on posterior variance reduction and using an inner 68% posterior region plus leave-one-out sensitivity) lacks theoretical guarantees and may be sensitive to hyperparameter choices (e.g., trigger threshold, number of refinements, kernel choice). Empirical validation is limited to simulations/toy and small-scale dynamical examples; no real experimental dataset or wall-clock computational budget study is provided to quantify the compute/experiment trade-offs in practice.","The authors propose applying the approach to complex real-world design problems and developing an extension that explicitly accounts for the relative costs of experiments versus computations to recommend strategies (e.g., how many initial training points and adaptive refinements to use). They also suggest investigating alternative adaptive selection criteria beyond leave-one-out and tuning the scheme’s hyperparameters to improve performance.","Extending the approach to continuous Θ/Ψ with large-scale optimization (e.g., Bayesian optimization variants of MOCU) and to higher-dimensional cost surrogates (beyond 2D GP inputs) would broaden applicability. Robustness studies under model misspecification (non-GP-surrogate errors), noisy/heteroscedastic cost evaluations, and correlated/auto-correlated experimental outcomes would strengthen practical relevance. Providing open-source implementations and benchmarking on standardized OED testbeds (including real experimental design case studies) would improve reproducibility and clarify computational scaling.",2012.04067v1,https://arxiv.org/pdf/2012.04067v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:08:43Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal|A-optimal|E-optimal|Compound criterion|Other,Variable/General (state parameter dimension n; focuses on qubit models with n=2 or n=3; also discusses 1-parameter case),Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper formulates quantum-state estimation as a classical optimal design of experiments (DOE) problem where the “design” is the choice of quantum measurement (POVM/PVM) and the objective is to minimize functions of the inverse Fisher information (Cramér–Rao bound) under local unbiasedness. It derives analytic optimal designs for arbitrary qubit (two-level) state models under several optimality criteria, including A-, D-, E-, and a one-parameter family of γ-optimal criteria that interpolates among them. For qubits it characterizes the Fisher-information region and shows that randomization over projective measurements can achieve the same Fisher information region as general POVMs, enabling closed-form optimal designs via mixtures of PVMs aligned with eigenvectors of the SLD Fisher information. It proves a qubit-specific “quantum equivalence theorem” (a Kiefer–Wolfowitz-type result) showing the D-optimal design coincides with a weighted A-optimal design (weight matrix given by the SLD Fisher information). Finally, it compares efficiencies of A-, D-, E-optimal designs and standard Pauli tomography, showing that designs optimal for one criterion can be highly inefficient for others, especially near the pure-state (high-purity) regime.","Designs are defined by measurements Π (POVMs) with Fisher information Jθ[Π] computed from pθ(x|Π)=tr{ρθΠx}. For mixed/continuous designs, the Fisher information convexly combines as Jθ[e(m)]=∑i pi Jθ[ei] (or J(ξ)=∫ ξ(de)Jθ[e]). For qubits, the Fisher information region can be written as J(ξ)= (Jθ^SLD)^{1/2} J (Jθ^SLD)^{1/2} with J≥0 and Tr(J)≤1, enabling analytic solutions; e.g., γ-optimal designs satisfy Jθ[e*]= (Tr[(Jθ^SLD)^{-γ/(γ+1)}])^{-1}(Jθ^SLD)^{1/(γ+1)} with weights pi ∝ (λi^SLD)^{-γ/(γ+1)} along SLD eigenvectors.","For an n-parameter qubit model (n=2 or 3), the γ-optimal design has closed-form weights pi ∝ (λi^SLD)^{-γ/(γ+1)} on orthonormal measurement directions given by the eigenvectors of Jθ^SLD, yielding min Ψγ = n^{-γ}(Tr[(Jθ^SLD)^{-γ/(γ+1)}])^{(γ+1)/γ} (γ≠0). The D-optimal design uses uniform weights pi=1/n and yields ΨD(Jθ[e*])=n^{-n}Det[(Jθ^SLD)^{-1}], while the E-optimal design uses weights pi ∝ (λi^SLD)^{-1} and yields ΨE = Tr[(Jθ^SLD)^{-1}]. Theorem V.1 shows, for any qubit model, minimizing Det(J(ξ)^{-1}) is equivalent to minimizing Tr(Jθ^SLD J(ξ)^{-1}) (D-optimal equals weighted A-optimal). Efficiency comparisons in the standard 3-parameter Bloch model show that some optimal designs (and standard Pauli tomography) become very inefficient under D-optimality as purity |θ|^2→1, whereas A-optimality is comparatively more stable across criteria.","The paper restricts most analytic solutions to qubit (d=2) models and primarily to local optimal designs (optimal at a fixed θ), noting that optimal designs generally depend on the unknown true parameter. It also notes complications with singular (rank-deficient) Fisher information matrices and emphasizes avoiding singular optimal designs when the goal is to estimate all parameters, discussing the need for estimability constraints and generalized inverses in such cases.","The work is largely theoretical and does not provide empirical/experimental validation or numerical studies for higher-dimensional (d>2) systems where POVM vs randomized-PVM equivalence can fail and optimal designs may be harder to realize. Practical implementation details (robustness to model misspecification, finite-sample/phase-I estimation of θ for local designs, and constraints/costs of switching measurements) are not developed, which can materially affect real experimental DOE performance. Comparisons are primarily via Fisher-information-based criteria; alternative operational losses (e.g., fidelity-based risk under finite samples) are mentioned but not explored, so performance under realistic quantum tomography error metrics remains uncertain.","The paper explicitly calls for (i) developing efficient numerical optimization algorithms to find optimal designs in physically important problems, (ii) generalizing the equivalence theorem beyond qubits to higher-dimensional systems, and (iii) further study of singular design issues arising with nuisance parameters. It also lists additional DOE extensions as promising directions in quantum settings: sequential/adaptive design, block design, Bayesian design, minimax design, robust design, and model discrimination.","Providing open-source software implementing the derived qubit designs and generalized numerical solvers for d>2 would improve adoption and reproducibility. Studying robustness of the optimal designs to state-preparation/measurement errors, miscalibration, and drift (and incorporating such uncertainty into robust or Bayesian optimality criteria) would make the framework more practical. Extending the analysis to constrained design spaces reflecting laboratory-accessible measurements (e.g., limited bases, hardware switching costs) and to correlated/noisy measurement data would better match experimental conditions and could change optimality conclusions.",2012.11949v1,https://arxiv.org/pdf/2012.11949v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:09:21Z
TRUE,Optimal design|Sequential/adaptive,Parameter estimation|Cost reduction,A-optimal,2 factors (pressure p and temperature T; selecting informative isotherms with multiple pressure points),Other,Simulation study,TRUE,None / Not applicable,Supplementary material (Journal/Publisher),https://www.nist.gov/srd/refprop|http://formulize.nutonian.com/documentation/eureqa/,"The paper applies optimal experimental design (OED) to plan thermodynamic property measurement series (liquid-phase density) with the goal of reducing experimental effort while maintaining model accuracy. Using a Schilling-type correlation for density as a function of reduced temperature and pressure, the authors construct Fisher information matrices for candidate (p,T) measurements and select the most informative isotherms under an A-optimality criterion that minimizes trace(I^{-1}) (parameter-variance proxy). Numerical studies on published (p,ρ,T) data for ethylene glycol and propylene glycol show that fitting the fixed-exponent Schilling model using only the five most informative isotherms yields relative density errors close to those obtained using all measured isotherms. They also explore selecting isotherm temperatures freely on a 2 K grid (within the experimental temperature range) and discuss a sequential workflow where additional isotherms are chosen adaptively based on information gain. The work argues that computing optimal isotherms prior to new measurements can increase precision at no additional experimental effort and provides data and calculations as supporting information.","The density model is the Schilling-type correlation $\rho/\rho_0=\sum_{j=1}^{I_{Pol}} n_j\,\sigma^{t_j}\,\pi^{p_j}$ with reduced variables $\sigma=(T/T_0-1)$ and $\pi=(p/p_0+1)$. For fixed exponents $(t_j,p_j)$, each measurement contributes an elementary Jacobian row vector $j(\pi_i,\sigma_i)=[\sigma_i^{t_1}\pi_i^{p_1},\ldots,\sigma_i^{t_8}\pi_i^{p_8}]$ and an elementary Fisher information matrix $I_i=j^Tj$, summed over selected measurements to $I=\sum_i I_i$. Designs are chosen by minimizing the A-optimal criterion $\Psi_A(I)=\mathrm{trace}(I^{-1})=\sum_{j=1}^8 1/\lambda_j$, subject to selecting sets of isothermal experiments (multiple pressure points per isotherm).","For ethylene glycol, the fixed-exponent model fitted with all eight measured isotherms has greatest relative deviation about 0.022%, while fitting using only the five A-optimal isotherms increases the greatest deviation only to about 0.025% (difference ≈ 0.003%). For propylene glycol, the analogous comparison is about 0.014% (all isotherms) versus about 0.016% (best five), a difference of about 0.0015%. The A-optimal objective decreases with more included isotherms, with diminishing returns when moving from five to six isotherms. Free selection of isotherm temperatures on a 2 K grid suggests potentially better designs, but cannot be validated without new measurements; a free-exponent refit can reduce deviations on selected isotherms but may extrapolate poorly outside the selected range (e.g., large deviations on unselected isotherms for propylene glycol).","Because the study is limited to already measured data, the authors cannot build and compare a model fitted to the best five freely selected (unmeasured) isotherms; new measurements would be required to validate that advantage. They also note that fitted correlations are strictly limited to liquid-phase densities and do not reproduce the curvature along the vapor–liquid saturation line compared with a fundamental EOS, highlighting limitations in extrapolation beyond the measured (p,T) range. The free-exponent model is cautioned against use outside the measured range due to poor behavior on unselected isotherms (shown for propylene glycol).","The OED formulation assumes independent measurements and uses a linear-in-parameters approximation for the fixed-exponent case; correlated errors (e.g., along an isotherm) or heteroscedastic uncertainties could change the optimal selection. The work optimizes only the choice of isotherms given fixed within-isotherm pressure grids (taken from existing datasets), so it does not address full joint optimization of both temperature and pressure points under realistic costs (e.g., different costs for pressure changes). Comparisons focus on maximum relative deviation; additional metrics (prediction variance across the full region, robustness to model misspecification, and performance under unknown/estimated nuisance parameters like $\rho_0,T_0,p_0$) are not systematically assessed. Implementation details (software/language, reproducibility steps) are not specified in the main text even though computations are said to be in supporting information.","The authors plan to measure the freely computed optimal isotherms, fit a new model, and compare against existing models. They propose exploring additional objective functions beyond parameter-uncertainty criteria and incorporating thermodynamic constraints (e.g., extrapolation behavior) as boundary conditions in the design/modeling. They also intend to use sequential OED for nonlinear (free-exponent) models and to study the influence of the number of terms to address overfitting, aiming toward an OED setup specialized for thermodynamic property measurements.","Extend the design to explicitly handle correlated/heteroscedastic measurement errors (e.g., generalized least squares FIM) and include instrument stabilization time as a quantitative cost in the optimization (multi-objective or constrained design). Optimize the pressure grids jointly with isotherm selection (and allow adaptive pressure placement) to better exploit within-isotherm information. Develop Bayesian or robust OED variants to hedge against model-form uncertainty (fixed vs free exponents vs alternative EOS structures) and to propagate uncertainty in prior parameter estimates. Provide an open-source implementation and benchmark datasets to enable reproducible comparisons with other design criteria (D-, I-, and G-optimal) and alternative thermodynamic property targets (viscosity, speed of sound, heat capacity).",2012.12098v2,https://arxiv.org/pdf/2012.12098v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:09:56Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical|Finance/economics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies the causal impact of sovereign debt crises (SDC) on child mortality using a quasi-experimental observational design across 57 low- and middle-income countries with Demographic and Health Survey (DHS) birth histories (1990–2015), linked to country-year SDC events. The treatment is an indicator for whether an SDC occurs during the exposure window from in utero through each child-age mortality threshold (neonatal, under-1, under-2, under-3, under-4, under-5), and outcomes are binary mortality indicators by each threshold. Average and heterogeneous effects are estimated using generalized random forests with an adjustment set of household- and country-level covariates and propensity-score weighting to address limited overlap. Results show negligible/insignificant effects for neonatal mortality (noted as likely underpowered), but statistically significant adverse effects for older thresholds, increasing mortality probability by roughly 0.12–0.14. Treatment heterogeneity analyses indicate stronger adverse effects in low-income countries, with variable-importance highlighting country population size, economic development (GDP), and mother’s age as key moderators.","The estimand for the average treatment effect is written as $\tau = \mathbb{E}[Y(1)-Y(0)\mid X] = \mathbb{E}[Y\mid D=1,X]-\mathbb{E}[Y\mid D=0,X]$ under conditional ignorability $\{Y(1),Y(0)\}\perp D\mid X$ and consistency. The propensity score used for weighting is defined as $e(X)=\mathbb{E}[D\mid X]$. The conditional average treatment effect is defined as $\tau(x_i)=\mathbb{E}[Y(1)-Y(0)\mid X=x_i]$, estimated via generalized random forests.","Estimated SDC effects on mortality are adverse across most age thresholds: neonatal mortality increases by about 0.02 but is not statistically significant, with the treated neonatal sample noted as small (about 14,605 exposed vs 480,175 unexposed). For infant mortality and under-2 to under-5 mortality, the estimated increase in mortality probability is about 0.12–0.14 and statistically significant at the 95% level (e.g., infant +0.12 with reported std ≈ 0.059; under-2 +0.14 with std ≈ 0.06). CATE distributions show substantial heterogeneity, including a mass near zero for children in countries that rarely experience SDC. Variable-importance results for neonatal heterogeneity rank population size, GDP (rgdpna), and mother’s age as the most important moderators, and the discussion emphasizes stronger effects in low-income countries (e.g., neonatal effect around 0.09 in low-income vs ~0 in middle-income).","The authors state that, despite the quasi-experimental design and covariate adjustment, unobserved confounding may still bias results because key information on government behavior is lacking. They note that some governments may strategically seek default/restructuring to access loans from international financial institutions, and that exogenous natural or political shocks could both erode public finances (increasing SDC risk) and affect child mortality, creating confounding.","The identifying assumption that SDC exposure is “as-if random” conditional on the adjustment set may be tenuous because SDC timing and restructuring/default declarations can be endogenous to evolving crises that also directly affect health systems and mortality (e.g., pre-trends). The analysis appears to rely on large-scale observational linkage without explicit checks for parallel trends/event-study balance diagnostics typically used to support quasi-experimental claims. Measurement error/misalignment is possible because DHS birth histories are retrospective and SDC exposure is defined at the country-year level while conception/birth timing is monthly, which can induce exposure misclassification. Code and full implementation details (e.g., tuning choices, handling of survey weights, missingness, clustering implementation) are not provided, limiting reproducibility.","The authors suggest future studies should better handle the challenge of unobserved confounding, particularly related to government behavior and exogenous natural or political shocks that may push governments into default while also affecting child mortality.","Future work could add explicit causal-design diagnostics (e.g., event-study specifications around SDC onset, placebo tests, negative-control outcomes/exposures) to strengthen the quasi-experimental identification claim. Extensions could incorporate autocorrelation and staggered/multiple crisis events (dynamic treatment regimes) and explore alternative estimators (e.g., doubly robust learners, causal forests with honest splitting plus sensitivity analyses). Given DHS’s complex survey design, evaluating robustness to survey weights and clustering choices and providing open-source code would improve transparency and adoption. More domain-specific mechanisms (health spending, nutrition, service access) could be modeled to connect SDC shocks to mortality pathways and to guide policy interventions.",2012.14941v1,https://arxiv.org/pdf/2012.14941v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:10:29Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,E-optimal|G-optimal|Other,Variable/General (design points x in R^d; K candidate experiments; sample size n),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper studies randomized strategies for optimal experimental design in linear regression when the design is obtained from a convex relaxation that yields an optimal sampling distribution over a finite pool of candidate experiments. It provides finite-sample, high-probability performance bounds for randomized E-optimal and G-optimal designs, showing convergence toward the relaxed optimum at an expected O(1/\sqrt{n}) rate when sampling n experiments i.i.d. from the optimal distribution. Methodologically, it develops refined matrix concentration inequalities using new “upper” and “lower” intrinsic dimension notions to tighten spectral-norm and minimum-eigenvalue deviation bounds, improving constants/log factors in the G-optimal analysis. Empirically, it compares randomized sampling against greedy selection on synthetic data for E-optimality and in an application to best-arm identification in linear bandits for G-optimality, finding randomized approaches competitive and sometimes better as dimension increases. The work advances the theory of randomized rounding/sampling for optimal design criteria beyond the previously studied A-optimal case by giving explicit guarantees for E and G criteria and highlighting when dimension-refined concentration improves rates.","The setting is linear regression with candidate experiments x_k (rows of X\in\mathbb{R}^{K\times d}) and covariance proportional to \((\sum_{k=1}^K n_k x_k x_k^\top)^{-1}\); under the convex relaxation with proportions \(\mu\in\Delta_K\), define \(M(\mu)=\sum_{k=1}^K \mu_k x_k x_k^\top\) and the randomized design sum \(S_n=\sum_{i=1}^n X_i\) where \(X_i=x_{k(i)}x_{k(i)}^\top\), \(k(i)\sim\mu\). The E-optimal objective is \(f_E(\Sigma_D^{-1})=\|\Sigma_D^{-1}\|\) (equivalently minimizing the max eigenvalue of the estimator covariance), and the G-optimal objective is \(f_G(\Sigma_D^{-1})=\max_{x\in\mathcal{X}} x^\top \Sigma_D^{-1} x\) (worst prediction variance). Main bounds control \(f_E(S_n^{-1})\) and \(f_G(S_n^{-1})\) relative to the relaxed optima using matrix Chernoff/Hoeffding/Bennett-type inequalities, and introduce refined dimensions \(\mathrm{updim}(S)=\frac{\mathrm{Tr}(S-\lambda_{\min}(S)I)}{\|S\|-\lambda_{\min}(S)}\), \(\mathrm{lowdim}(S)=d-\mathrm{updim}(S)\) to replace the ambient dimension in log terms.","For randomized E-optimal sampling from the relaxed optimum \(\mu_E^\*\), if \(n \ge 2L\|M(\mu_E^\*)^{-1}\|\log(d/\delta)\) (with \(L=\max_{x\in\mathcal{X}}\|x\|^2\)), then with probability \(\ge 1-\delta\) the achieved E-objective satisfies a multiplicative error of order \(1+O\!\left(\sqrt{\tfrac{L\|M(\mu_E^\*)^{-1}\|\log(d/\delta)}{n}}\right)\) relative to the relaxed optimum. For randomized G-optimal sampling from \(\mu_G^\*\), with probability \(\ge 1-2\delta\) the G-objective satisfies \(f_G(S_{G,n}^{-1}) \le \left(1+\frac{L}{d}\|M(\mu_G^\*)^{-1}\|^2\sqrt{\tfrac{\text{const}\cdot\log(d/\delta)}{n}}+o(1/\sqrt{n})\right) f_{G,n}^\*\), yielding the expected \(O(1/\sqrt{n})\) convergence. A refined analysis replaces \(d\) in logarithmic factors by \(\tilde d=\mathrm{updim}(V)+\mathrm{lowdim}(V)e^{-n(1-\kappa^{-1})/16}<d\) (V is the covariance of the sampled matrices; \(\kappa\) its condition number), tightening the G-optimal bound. Simulations show randomized E-optimal can outperform greedy as n grows on Gaussian synthetic pools (K=500,d=10), and randomized G-optimal achieves similar or better sample complexity than greedy for best-arm identification as dimension increases.",None stated.,"The guarantees are primarily for a finite pool of candidate experiments and for i.i.d. sampling from the relaxed-optimal distribution; they do not directly address adaptive/sequential designs where sampling depends on observed responses (common in bandits and active learning). The analysis assumes well-specified linear regression with independent Gaussian noise and typically requires invertibility/spanning conditions (or projection), which may not hold in practice with collinearity or model misspecification. Empirical validation is limited (synthetic pools and a stylized linear-bandit instance), and there is no broad benchmark across diverse real datasets or practical constraints (costs, batching constraints, heteroscedasticity, correlation). Code is referenced via a solver choice (cvxopt) but no implementation artifacts are provided for reproduction.","The authors suggest extending the approach to batch or parallel bandits, using randomized sampling to select a batch of arms before observing rewards.","Develop self-normalized/adaptive versions of the randomized design guarantees for sequential decision-making where the sampling distribution updates online based on outcomes. Extend the theory and algorithms to settings with unknown/estimated noise variance, heteroscedastic errors, or correlated observations (time series/streaming), and to generalized linear models. Provide practical rounding/balancing schemes that enforce exact run counts, constraints, or multiple objectives (e.g., combined E/G or cost-aware criteria) while retaining similar concentration-based guarantees. Release open-source, end-to-end implementations and run broader empirical studies on real experimental design problems (e.g., biology, A/B testing, network tomography) to validate robustness and usability.",2012.15726v1,https://arxiv.org/pdf/2012.15726v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:11:16Z
FALSE,Computer experiment|Other,Prediction|Other,Not applicable,Variable/General (k input variables/features),Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Not provided,NA,"The paper proposes G-forse, a Gaussian-process/kriging-based metamodeling framework to globally interpret black-box machine learning models by estimating their response surface over a region of interest. The surrogate model is defined as $Y(x)=\mu(x)+Z(x)$, with a stationary GP residual and a mean function to capture trend, and uses a Gaussian product correlation function whose per-feature parameters $\theta_j$ are interpreted as variable-importance measures. Parameters are estimated by maximizing the likelihood, with optimization performed via differential evolution and L-BFGS-B. The method is evaluated on synthetic data (a known GLM with main effects and an interaction) and on multiple real datasets by fitting G-forse to the validated predictions of trained models (NN, GBM, AdaBoost, XGBoost) and assessing fidelity using RMSE and correlation. The framework also introduces “group explanation” by using the fitted correlation matrix to visualize sample-to-sample agreement patterns as heatmaps.","The surrogate/metamodel is $Y(x)=\mu(x)+Z(x)$ where $Z(x)$ is a mean-zero stationary Gaussian process. The covariance is $\text{Cov}(Z(x_i),Z(x_j))=\sigma^2\prod_{l=1}^k K(h_l;\theta_l)$ with $h_l=|x_i^{(l)}-x_j^{(l)}|$. The primary correlation form used is Gaussian product correlation $\psi(h)=\exp\left(-\sum_{j=1}^k \theta_j h_j^{p_j}\right)$; in experiments $p$ is fixed at 2 and $\theta$ is estimated by MLE and interpreted as feature importance.","Across several datasets, G-forse achieves high fidelity to black-box predictions, reporting correlations up to about $r\approx 0.99$ on multiple tasks (e.g., Housing and Cancer) with low RMSE in many settings (e.g., Crimes NN RMSE ≈ 0.10 with $r=0.88$; Cancer NN RMSE as low as 0.0001 with $r=0.99$ in one table). Differential evolution and L-BFGS-B produce broadly similar results, with L-BFGS-B improving performance on the Income dataset relative to DE. On synthetic GLM experiments, the paper reports G-forse estimates feature importance/coefficients more accurately and robustly than logistic regression, particularly where Hauck–Donner effect degrades logistic-regression inference. Heatmap visualizations based on the fitted correlation matrix are presented to show clusters of samples that agree/disagree in prediction behavior.",The authors note G-forse may not be scalable to ultra-high dimensional data. They also state that data smoothness is necessary for reliable estimation and that multicollinearity can negatively affect explanations.,"The work frames metamodeling as “computer experiments,” but it does not specify or compare alternative DOE/space-filling sampling strategies (e.g., Latin hypercube) for selecting query points to the black-box, which can materially affect surrogate quality. It also appears to rely on normalized inputs and a fixed smoothness exponent ($p=2$) without a robustness study for non-smooth response surfaces or strongly categorical/mixed inputs common in ML. Empirical evaluation focuses on fidelity metrics (RMSE, correlation) but provides limited benchmarking against established global explanation/surrogate baselines (e.g., global GAM/EBM, rule lists) under matched complexity constraints.",The authors suggest extending G-forse to image-classifier explanations by highlighting super-pixels using variogram-based relatedness measures. They also propose replacing the covariance function with other alternatives and improving scalability to ultra-high dimensional data via penalized likelihood combined with feature-selection techniques.,"A natural extension is to integrate explicit, DOE-style adaptive sampling (sequential design/active learning) to choose black-box query points that minimize surrogate uncertainty (e.g., via expected improvement or integrated variance) and improve explanation fidelity with fewer runs. Further work could address non-i.i.d. data and dependence (time series, grouped data), and develop self-tuning/robust versions that handle categorical/mixed features and non-smooth response surfaces. Packaging the approach with reproducible code and standardized benchmarks against strong interpretable models (EBM/GAM surrogates, rule-based surrogates, SHAP-based global summaries) would clarify practical tradeoffs.",2101.00772v1,https://arxiv.org/pdf/2101.00772v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:11:47Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical,Simulation study,TRUE,Other,Not provided,NA,"This paper develops an integrated real-time computer-aided system for colorectal polyp detection in real-world colonoscopy scenarios, motivated by domain shift between public datasets and operational clinical video frames (blur, artifacts, instruments, specularity, bubbles). The system includes a CNN-based blurry-frame detector to filter blurred frames, two independent polyp detectors (AFP-Net anchor-free detector and a dilated U-Net–based segmentation-to-box pipeline), and an ensemble rule that keeps detections only when both detectors agree (IoU > 0.1). The authors also contribute a private dataset of 7,313 annotated colonoscopy images from 224 complete procedures with pixel-level polyp and instrument annotations, intended to better represent clinical conditions. Performance is evaluated via extensive experiments on public datasets and the private dataset, including image-level precision/recall/F1 and video-level time-to-detection and false-positives-per-minute; the integrated system substantially reduces false positives while maintaining useful sensitivity, and runs in real time (~23 fps on clear frames).",Not applicable (no DOE/statistical experimental-design equations; the paper’s core method is a deep-learning pipeline with an ensemble rule based on IoU thresholding).,"On the private test set (trained on combined public+private), the integrated system achieves Precision 93.46%, Recall 70.49%, and F1-score 80.37% (vs. 70.45% F1 when trained only on public data). In video evaluation, the system detects >60% of polyps within 2 seconds of first appearance and >80% within 10 seconds. The ensemble plus blurry filtering reduces false positives from roughly 40–60 per minute for individual detectors to <6 per minute for the integrated system (and around ~5 per minute with blurry detection). Reported runtime is ~3 ms for blurry frames and ~43 ms per clear frame (~23 fps) using PyTorch 1.4.0 on an NVIDIA GeForce GTX 2080 Ti.",None stated.,"The study relies heavily on a single-site private dataset (Xiangya Hospital) collected over a limited time window, which may limit generalizability to other hospitals, patient populations, and endoscopy workflows. The approach uses heuristic thresholds (e.g., IoU > 0.1; size-based ensemble threshold) that may require re-tuning for different equipment, resolutions, or prevalence of artifacts. Evaluation emphasizes standard detection metrics and false-positives-per-minute, but does not quantify clinical endpoints (e.g., adenoma detection rate improvement) or prospective, randomized deployment impact. Code and trained models are not provided, limiting reproducibility and independent validation.",None stated.,"Validate the system prospectively in multi-center clinical studies and report clinical outcomes (e.g., ADR, miss rate reduction) and human–AI interaction effects. Extend robustness to distribution shifts such as different endoscope vendors, imaging modalities, and varying bowel prep quality, possibly via domain adaptation/self-supervised pretraining. Develop self-tuning or uncertainty-aware ensemble/thresholding to reduce reliance on fixed IoU/size heuristics and better control the sensitivity–false-positive trade-off. Release code/models and standardized benchmarks for the private dataset (as allowable) to enable reproducible comparisons.",2101.04034v1,https://arxiv.org/pdf/2101.04034v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:12:18Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,A-optimal|Not applicable,Variable/General (binary design variable per candidate sensor location; examples include ns=2 toy problem and ns=14 candidates in PDE sensor placement experiments),Environmental monitoring|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://gitlab.com/ahmedattia/doerl,"The paper proposes a stochastic learning method for solving binary optimal experimental design (OED) problems arising in Bayesian inverse problems (including PDE-constrained models), with an emphasis on sensor placement. The binary design vector is modeled as a multivariate Bernoulli random variable and the regularized OED objective is reformulated as an expectation over this distribution; the optimization is performed over the Bernoulli parameters using policy-gradient (REINFORCE-style) stochastic optimization. The approach enables using non-differentiable sparsity penalties such as the 0 norm directly and avoids continuous relaxation and subsequent rounding that are common in traditional OED formulations. The method is demonstrated on a 2D linear inverse-problem toy example and on an advectiondiffusion contaminant transport sensor-placement problem, including settings with no penalty, 0 sparsity regularization, and fixed-budget constraints. Variance reduction via a baseline (including an estimated optimal baseline) is analyzed and shown numerically to improve convergence and solution quality, often producing designs near the brute-force optimum for small candidate sets.","Binary Bayesian OED is posed as $\zeta_{\mathrm{opt}}=\arg\min_{\zeta\in\{0,1\}^{n_s}} J(\zeta)=\Psi(\zeta)+\alpha\Phi(\zeta)$ with $\Psi$ taken as A-optimality $\mathrm{Tr}(\Gamma_{\mathrm{post}}(\zeta))$ (and discussed D-optimality $\log\det(\Gamma_{\mathrm{post}}(\zeta))$). The stochastic reformulation sets $\zeta\sim\prod_{i=1}^{n_s}\mathrm{Bernoulli}(\theta_i)$ with PMF $P(\zeta\mid\theta)=\prod_i \theta_i^{\zeta_i}(1-\theta_i)^{1-\zeta_i}$ and optimizes $\Upsilon(\theta)=\mathbb{E}_{\zeta\sim P(\cdot\mid\theta)}[J(\zeta)]$. The policy-gradient identity yields $\nabla_\theta\Upsilon(\theta)=\mathbb{E}[J(\zeta)\nabla_\theta\log P(\zeta\mid\theta)]$ with score $\nabla_\theta\log P(\zeta\mid\theta)=\sum_i\left(\frac{\zeta_i}{\theta_i}+\frac{\zeta_i-1}{1-\theta_i}\right)e_i$ and the projected SGD update $\theta^{(n+1)}=\mathcal{P}(\theta^{(n)}-\eta^{(n)}\hat g^{(n)})$; a baseline version uses $J(\zeta)-b$ to reduce variance.","On the advectiondiffusion sensor-placement problem with $n_s=14$ candidate locations (enabling brute-force comparison over $2^{14}=16384$ designs), the baseline-enhanced algorithm (Algorithm 3.2 with the estimated optimal baseline) produces sampled designs whose objective values are reported to be within about 1% of the brute-force global optimum in the no-penalty case. With an \(\ell_0\) sparsity penalty (example shown with \(\alpha=1\)), Algorithm 3.2 attains the unique brute-force global optimum design (reported to use 3 active sensors), whereas the no-baseline variant can converge to suboptimal policies. Under a fixed-budget constraint implemented via a penalty $\Phi(\zeta)=|\|\zeta\|_0-\lambda|$ with example \(\lambda=8\), the optimal-baseline variant is shown to more reliably discover the brute-force optimum than the no-baseline method. The paper also reports learning-rate sensitivity experiments, indicating improved convergence for certain step sizes (example: \(\eta=0.5\) among tested values) but noting possible divergence with overly large rates.","The authors note that the main open issue is optimal selection of the learning-rate parameter: while RobbinsMonro step-size schedules guarantee convergence, they may require many iterations before reaching a degenerate (binary) optimal policy. They also indicate that additional analysis for nonlinear forward models is deferred, with the paper focusing primarily on linear forward operators for simplicity. The paper further notes that the Monte Carlo policy-gradient estimator can have high variance, motivating baseline/variance-reduction techniques.","The method assumes independent Bernoulli activations across candidate locations, which may limit the ability to encode structured design constraints (e.g., spatial spacing, mutual exclusion, or group selection) without additional modeling. Performance evidence is largely simulation-based and problem-specific (toy linear example and one PDE application); broader benchmarks against modern discrete/MI optimization or advanced Bayesian OED solvers (e.g., MIP with cutting planes, submodular methods where applicable, or more recent discrete relaxations) are not extensively covered. The algorithm returns designs by sampling from the learned distribution after optimization; solution quality can depend on sampling budget and may be variable when the policy is not fully degenerate. Practical guidance for choosing \(\alpha\), stopping criteria, and computational scaling to very large \(n_s\) (beyond moderate candidate sets) is limited, especially when each $J(\zeta)$ evaluation involves expensive PDE solves.","The authors state that optimal learning-rate selection and the explorationexploitation trade-off will be addressed in separate work. They also state that further analysis for nonlinear forward models will follow in separate works. Additionally, they note that other sample-based stochastic optimization approaches (e.g., sample average approximation, RobbinsMonro-type methods) could be used to solve the stochastic formulation and that comparative performance of these algorithms will be considered in future work.","Extending the probabilistic design model to handle dependent selections (e.g., via structured probabilistic models, constraints, or determinantal point processes) could better capture realistic sensor placement constraints. Developing self-tuning/adaptive step-size and variance-reduction schemes (control variates, actor-critic baselines) with theoretical guarantees specific to OED objectives could improve robustness and reduce tuning. More extensive real-world case studies (field sensor networks, lab experiments) and standardized computational benchmarks would strengthen empirical validation. Incorporating model/parameter uncertainty about \(\Gamma_{\mathrm{noise}}\) and relaxing assumptions such as linearity, Gaussianity, and independence of observation errors could broaden applicability.",2101.05958v1,https://arxiv.org/pdf/2101.05958v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:13:06Z
TRUE,Factorial (full)|Factorial (fractional)|Optimal design|Sequential/adaptive|Other,Parameter estimation|Other,D-optimal|A-optimal|E-optimal|Other,Variable/General (examples include 1–2 design variables; general n-dimensional domain),Manufacturing (general)|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://doi.org/10.1016/j.compchemeng.2020.107218|http://creativecommons.org/licenses/by/4.0/,"The paper proposes a two-phase methodology for optimal model-based design of experiments (MBDoE) aimed at answering (i) how many experiments are needed and (ii) which experiments (and repetitions/weights) should be run. Phase 1 generates a high-quality initialization either via problem-independent patterns (Sobol sequences, factorial/fractional factorial designs with multistart) or via problem-dependent discretization over a candidate set, using a convex Weighted Discretization Approach (WDA) and a new greedy MaxVol-variant (wMaxVol) that outputs both selected points and approximate weights (including for multivariate outputs). Phase 2 refines the design by solving the original nonconvex continuous-design optimization initialized from Phase 1, with designs assessed via the Kiefer–Wolfowitz equivalence theorem (sensitivity function) as an optimality certificate on discretized domains. The work focuses primarily on log-D optimality (maximize log det of Fisher information) for nonlinear models with possible constraints, and demonstrates the approach on an analytic exponential example and two chemical engineering case studies (flash distillation VLE and constrained tubular reactor kinetics). Numerically, discretization-based Phase-1 methods (especially wMaxVol) provide near-global initializations and reduce the need for costly multistart, often matching or outperforming pattern-based strategies at lower computational cost.","The MBDoE problem is posed over support points and weights as: minimize $\Psi(I(\xi))$ s.t. $\sum_i w_i=1$, $0\le w_i\le 1$, and design constraints, where the Fisher information matrix is $I(\xi)=\int \phi(x)\phi(x)^T\,\xi(dx)=\sum_i w_i\phi(x_i)\phi(x_i)^T$ (or $I(\xi)=\int \phi(x)\Sigma^{-1}\phi(x)^T\,\xi(dx)$ for multivariate outputs). The paper focuses on the log-D criterion $\Psi(I(\xi))=-\log\det(I(\xi))$ (equivalently maximize $\log\det I$). Optimality is checked via the Kiefer–Wolfowitz equivalence theorem using sensitivity $d(x,\xi)=\mathrm{tr}(I(\xi)^{-1}\mu(x))$ with the D-optimal bound $d(x,\xi^*)\le P$ for all $x$ and equality at support points.","On the exponential example (known analytic D-optimum), both WDA and wMaxVol recover near-optimal discrete designs and the phase-2 solve reaches the true optimum when the grid permits, with equivalence-theorem verification confirming optimality on the discrete candidate set. For flash distillation, single-start pattern initializations can converge to inferior local minima, while WDA(Grid) and wMaxVol(Grid) identify phase-1 solutions certified D-optimal on the candidate set and enable phase-2 to reach a better (putative global) solution; on a finer grid (90 candidates) they reach a design whose near-global optimality is supported by verification on a 966-point grid. Across case studies, wMaxVol is consistently much faster than WDA in phase 1 (e.g., flash distillation Grid(25): ~0.312s vs ~191s for phase-1 solve time reported, plus shared Jacobian cost), while yielding comparable designs and phase-2 performance.","The proof that wMaxVol approximates D-optimal design is provided only for the univariate-output case; the multivariate case proof is explicitly deferred to a separate article, with only a statistical convergence study included. Global optimality of the continuous (non-discretized) design generally cannot be guaranteed; equivalence-theorem checks certify optimality on discrete candidate sets and provide evidence (not proof) on finer grids. The paper also notes that parametric uncertainty is handled by fixing parameters at current estimates in an iterative workflow rather than being explicitly incorporated into the design optimization in this work.","Comparisons focus on Sobol/factorial patterns and the proposed discretization methods; the study does not benchmark against other common optimal-design algorithms (e.g., coordinate-exchange, Fedorov/Wynn-type exchange, multiplicative algorithms) or modern Bayesian/robust MBDoE variants, which could change conclusions about efficiency/quality. Practical reproducibility is limited because the main simulation environment (CHEMASIM) is proprietary and the implementations (Python wMaxVol, CVXPY setup) are not shared, making it hard to replicate timings and results. The approach depends on the quality and coverage of the candidate set/grid; in higher-dimensional design spaces, grid-based discretization may face scalability issues not deeply analyzed here.","The authors plan to extend the proposed methods to explicitly handle uncertain model parameters (parametric uncertainty) directly within the optimization problem, rather than fixing parameters to point estimates within the iterative workflow. They also state that a formal proof of wMaxVol for multivariate-output experimental design is beyond the scope of the paper and will be given in a separate article.","Develop scalable candidate-set generation and refinement strategies for higher-dimensional design spaces (e.g., adaptive space-filling plus local refinement) and quantify computational complexity as dimension grows. Extend the framework to robust/Bayesian MBDoE objectives (expected information gain, worst-case designs) and to correlated/serially dependent data, which are common in chemical process monitoring. Provide an open-source reference implementation (including equivalence-theorem verification tooling) and standardized benchmarks to enable broader adoption and fair comparisons with established exchange/multiplicative algorithms.",2101.09219v1,https://arxiv.org/pdf/2101.09219v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:13:50Z
TRUE,Optimal design|Sequential/adaptive|Other,Model discrimination|Parameter estimation|Screening|Cost reduction|Other,Other,Variable/General (examples include n=2–4 nodes; also an EGFR subnetwork with 3 variables used in an example),Healthcare/medical|Other,Simulation study|Other,TRUE,Python|C/C++|Other,Not provided,https://s2.smu.edu/doems|https://faculty.math.illinois.edu/Macaulay2/|http://home.imf.au.dk/jensen/software/gfan/gfan.html|https://www.sqlite.org/index.html|https://www.php.net|https://www.python.org|https://github.com/Macaulay2/M2/tree/master/M2/Macaulay2/packages,"The paper proposes an algebraic, data-driven framework that unifies design of experiments and model selection for discrete biological network data modeled by polynomial functions over finite fields. It introduces “linear shifts” (coordinatewise affine transformations) that partition candidate input data sets (experimental conditions) into equivalence classes, where all sets in a class yield the same identifiable model bases (standard monomial bases tied to Gröbner bases of the ideal of points). Representatives of equivalence classes are defined via a minimum-distance-to-origin criterion, providing “standard-position” experimental designs that tend to minimize the number of active nodes. The authors implement these results in a web-accessible database (DoEMS) that links equivalence classes of designs to associated model bases and Gröbner-basis metadata, enabling queries that answer (i) which experiments identify a known interaction and (ii) which interactions are identifiable from given experiments. An EGFR signaling/tumor growth example demonstrates how the database supports selecting initializations that uniquely identify interactions such as a specific monomial (e.g., x1x2) and how adding a point can introduce additional identifiable interactions.","Linear shift definition: for input sets Si,Sj \subset \mathbb{Z}_p^n with m points, Si\sim Sj if there exists \varphi=(\varphi_1,\dots,\varphi_n) with \varphi_k(x)=a_k x+b_k (a_k\in\mathbb{Z}_p^{\times}, b_k\in\mathbb{Z}_p) applied coordinatewise such that Si=\varphi(Sj). Identifiability condition recalled: a polynomial f is identifiable by a set S iff the evaluation matrix X(f,S) (rows=data points, columns=monomials in f) is full rank. Model bases are standard monomials SM_\prec(I(S)) (monomials not in the leading-term ideal LT_\prec(I(S))) for the ideal of points I(S).","DoEMS database is constructed for p=2 with 2\le n\le 4 and 1\le m\le p^n, and for p=3,n=2 with 1\le m\le p^n; it stores equivalence classes of input designs annotated with model bases/leading terms/Gröbner bases. In the worked example p=3,n=2,m=4, the 126 possible 4-point designs are partitioned into 7 linear-shift equivalence classes. In the EGFR subnetwork example (p=2,n=3,m=5), there are \binom{2^3}{5}=56 candidate 5-point designs, partitioned into 7 equivalence classes; requiring the interaction monomial x1x2 reduces this to 32 designs across 4 equivalence classes (each class size 8), with some designs uniquely identifying the target basis {1,x1,x2,x3,x1x2} while others admit additional alternative bases (implying additional interactions).",None stated.,"The “experimental design” notion is restricted to discrete, finite-field state spaces and polynomial-model identifiability; many practical biological experiments are continuous, noisy, and constrained in ways not represented by linear shifts. The database coverage is limited (explicitly to small p and n cases), so scalability to larger networks (higher n) may be computationally prohibitive without additional structure or approximations. The application example is illustrative rather than a full validation on real experimental datasets with measurement error, replicates, or temporal dependence.",The discussion notes several open theoretical/computational questions about the number and sizes of linear-shift equivalence classes and how these properties may further inform DOE/model selection. The authors also state that growing/expanding the DoEMS database and the information it provides would make the approach more accessible and broadly usable.,"Develop robust/noisy-data extensions (e.g., probabilistic identifiability or Bayesian criteria) so designs remain effective under measurement error and partial observability. Add explicit cost/feasibility constraints (e.g., inability to set many nodes to 0/1) and optimize over equivalence classes accordingly, turning the framework into a constrained optimal design tool. Provide packaged, reproducible software (e.g., a Python/R client library and containerized pipeline) and benchmark on larger, real biological datasets to assess practical gains in experimental efficiency and interaction recovery.",2101.09384v1,https://arxiv.org/pdf/2101.09384v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:14:26Z
FALSE,NA,NA,Not applicable,Not specified,Semiconductor/electronics|Other,Simulation study|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper presents a 60 GHz millimeter-wave near-field vector measurement bench using a subharmonic IQ mixer and lock-in detection to obtain complex (I/Q) reflected near-field voltages from a probe–sample interaction. It develops a signal-processing method to reconstruct the complex reflected voltage magnitude and phase from lock-in outputs, and then normalizes sample measurements by a gold-mirror reference to extract a “normalized vector reflection” \(\Gamma\) intended to represent near-field reflectivity. A key contribution is an analytical model of IQ mixer non-idealities (gain imbalance and quadrature phase error) that predicts the measured ratio traces a circle in the complex plane; fitting this circle yields \(\Gamma\) and an “ideality factor” \(\zeta\) that serves as a quality metric. The study experimentally examines how mechanical modulation waveform (sinusoid/square/triangle) and harmonic detection rank affect the extracted \(\Gamma\), and it characterizes \(\zeta\) and \(\Gamma\) versus frequency across 55–65 GHz using sliding-window processing. Results show \(\zeta\) varies strongly with frequency (best roughly 56–61 GHz) and that \(\Gamma\) depends on harmonic rank and probe details, indicating the extracted \(\Gamma\) is not a simple Fresnel reflection coefficient but can still be reproducible for material discrimination with calibration.","The reflected near-field voltage magnitude is computed from lock-in channel magnitudes as \(|v_e|=\sqrt{m_I^2+m_Q^2}\) with signs inferred from lock-in X/Y outputs to extend phase to \([-\pi,\pi]\), using \(\angle v_e=\arctan\big(\mathrm{sgn}(Q)m_Q/(\mathrm{sgn}(I)m_I)\big)\). IQ mixer imbalance is modeled as \(\tilde v_e=\Re(v e^{j\phi})+j\gamma\Im(v e^{j(\phi+\theta)})\), leading to a normalized ratio \(\tilde v_{eS}/\tilde v_{eR}\) that traces a circle; circle parameters map to \(X_c=\rho\cos\varphi\), \(Y_c=\rho(1+\gamma^2)\sin\varphi/(2\gamma\cos\theta)\), \(R=\zeta Y_c\) with \(\zeta=\sqrt{1-4\gamma^2\cos^2\theta/(1+\gamma^2)^2}\). The desired normalized vector reflection is then inferred by \(\rho=\sqrt{X_c^2+Y_c^2-R^2}\) and \(\varphi=\arccos(X_c/\rho)\), giving \(\Gamma=\rho e^{j\varphi}\).","From circle-fitting the ratio \(\tilde v_{eS}/\tilde v_{eR}\) (GaAs relative to gold) over a limited band around 60.7–60.8 GHz, the paper reports \(\Gamma=(0.482+j0.225)\pm0.004\) with ideality factor \(\zeta=0.436\pm0.005\) (rank-1, sinusoidal modulation). Across modulation waveforms, rank-1 \(\Gamma\) is very similar (e.g., square: \(0.489+j0.226\); triangle: \(0.487+j0.226\)) while \(\Gamma\) changes with harmonic rank and shows an even/odd dichotomy; measurements with \(\zeta\ge1\) are rejected (notably rank 5 due to 200 Hz power-line harmonic noise). Over 55–65 GHz, \(\zeta\) varies substantially and exceeds 1 near ~64 GHz, indicating unusable operation there; the authors recommend the 56–61 GHz range as most reliable based on \(\zeta\). After probe modification, at 60.75 GHz they obtain \(\Gamma=(0.400+j0.152)\pm0.014\), showing \(\Gamma\) is probe-dependent while \(\zeta\) remains similar.","The authors state that the extracted \(\Gamma\) values should not be regarded as true microwave reflection coefficients and that it is not realistic to infer a dielectric constant \(\varepsilon\) from them; the probe–sample interaction includes longer-range contributions beyond a simple near-field Fresnel model. They also note that some harmonic-rank measurements must be rejected when the inferred \(\zeta\) exceeds 1, which they attribute to practical issues such as noise (notably at 200 Hz), weak signals, or non-reproducible positioning. They conclude the values of \(\Gamma\) and its frequency dependence cannot be explained with a simple interaction model and require further study.","The method assumes the sample’s near-field response is constant over the chosen frequency sweep/window so that observed phase rotation is dominated by electrical path length; this may fail for dispersive/structured materials or resonant probe/sample behaviors. The extraction relies on a specific two-parameter IQ-imbalance model (gain mismatch \(\gamma\) and quadrature error \(\theta\)) and a circle fit; other non-idealities (nonlinearity, DC offsets, amplitude ripple coupled into I/Q, drift, cable reflections) could violate the circular model and bias \(\Gamma\)/\(\zeta\). Reported uncertainty treatment is briefly described but sensitivity to window size, frequency sampling, and outlier robustness is not systematically assessed. No shared implementation details make reproducibility and adoption harder for practitioners.","They propose to study and demonstrate the probe-dependence of \(\Gamma\) using other probes and materials, implying calibration/learning procedures for material discrimination in the near field. They state that linking \(\Gamma\) to intrinsic material parameters will require full 3D electromagnetic modeling of the coupled probe–sample system. They also suggest studying sub-wavelength spatial resolution in the context of vector (not just intensity) near-field measurements.","Develop a standardized calibration protocol (multi-reference standards and uncertainty budgets) to better separate instrument effects from probe–sample physics and to enable cross-probe comparability. Extend the method to account for correlated noise, drift, and additional IQ impairments (offsets, frequency-dependent imbalance) using more flexible models and robust fitting. Validate the approach on a wider set of real materials with known permittivity and on spatially varying samples (imaging) to assess discrimination performance and resolution versus harmonic rank. Provide open-source code for circle fitting, \(\Gamma\)/\(\zeta\) extraction, and quality-control rules to improve reproducibility.",2101.11616v1,https://arxiv.org/pdf/2101.11616v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:15:10Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Model discrimination|Other,Not applicable,"Variable/General (dynamic control inputs over time; includes uncertain initial state and model parameters; case study uses Du=2 controls, Dx=2 states, Dθ∈{3,4})",Pharmaceutical|Manufacturing (general)|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/scwolof/doepy,"The paper develops a framework for optimal design of dynamic experiments aimed at discriminating among rival mechanistic (state-space) models under multiple sources of uncertainty, including measurement noise, process noise, uncertain initial state, uncertain control inputs, and uncertain model parameters. It extends analytical (linear/Gaussian) design approaches and adapts a Gaussian-process (GP) surrogate strategy to handle black-box dynamic transition functions by placing GPs on the one-step state transition mapping, enabling tractable gradient-based optimization. The design problem optimizes initial conditions, time-discretized control inputs, and (for continuous-time settings) measurement time points to maximize a predictive-distribution divergence objective across candidate models, subject to safety/path constraints handled via mean constraints or second-order cone chance-constraint approximations. The method is demonstrated on a yeast fermentation case study with four rival models, showing comparable or improved objective values versus prior literature and illustrating how added uncertainty (e.g., process noise or control uncertainty) changes the optimal input profile and can increase required experiments for discrimination. An open-source Python package is used to generate results and support implementation.","The experimental design is posed as an optimal control/DOE problem maximizing a model-discrimination objective: $\arg\max_{\hat x_0,\hat u_{0:T-1},T_{\mathrm{meas}}}\sum_{k\in T_{\mathrm{meas}}} D\big(y_k^{(1)},\ldots,y_k^{(M)}\big)$ subject to each model’s state-space dynamics and constraints (Eq. 5). For black-box transitions, the one-step map $x_{k}=f(x_{k-1},u_{k-1},\theta)+w_{k-1}$ is replaced/augmented with a GP surrogate providing a predictive mean/covariance, enabling Gaussian uncertainty propagation via first-order linearization: $\mu_k\approx \mu_f(\tilde\mu_{k-1})$, $\Sigma_k\approx \nabla \mu_k\,\tilde\Sigma_{k-1}(\nabla \mu_k)^\top+\Sigma_x+\Sigma_f(\tilde\mu_k)$ (Eq. 9). Dependent-variable safety constraints are treated either on the predicted mean or via a second-order cone chance-constraint approximation using $\alpha=\sqrt{2}\,\mathrm{erf}^{-1}(1-\gamma)$.","On the Espie & Macchietto (1989) yeast fermentation case study, using the same formulation as Chen & Asprey (2003), the authors report a higher objective value (2805 vs. 2601), attributing the difference to improved solvers. When adding process noise ($\Sigma_x=0.01 I$), their optimized control achieves divergence 263 versus 244 for Chen & Asprey’s control (about 8% higher), and the optimal input profile changes noticeably. In simulations comparing constraint handling, cone (chance) constraints reduce both frequency and severity of constraint violations relative to mean constraints, though the average number of experiments required for discrimination is similar in their test setting. Under/over-estimating control-input uncertainty shows that underestimation increases inconclusive discrimination and can trigger constraint violations, whereas conservative (overestimated) uncertainty is less harmful. GP-surrogate (black-box) emulation yields marginally worse discrimination performance than the fully analytical approach due to added predictive uncertainty, but still correctly identifies the true model in their reported simulation runs.","The authors note that the framework relies on linear (first-order Taylor) and Gaussian approximations for uncertainty propagation for analytical tractability; approximation error can accumulate over long horizons, degrading predictions, reducing informativeness, and causing constraint violations. They also assume noise covariances $\Sigma_x$ and $\Sigma_y$ are known; in practice these must be estimated and require domain expertise, with results indicating conservative upper bounds are preferable. They further note the approach is open-loop (offline); a closed-loop (online) redesign approach would be computationally expensive with their method.","The divergence objective $D(\cdot)$ is treated abstractly and multiple choices exist; the practical sensitivity of optimal designs to the specific divergence/utility choice and weighting across models is not fully benchmarked. The GP surrogate uses independent-output GPs on state dimensions, which ignores cross-output correlations and may underrepresent joint predictive uncertainty in coupled dynamics. Optimization relies on gradients (exact or GP-approximated) and can be sensitive to solver settings, initialization, and nonconvexity; robustness across different optimizers and larger-scale/higher-dimensional systems is not fully established. Real experimental validation is not shown; results are primarily simulation/case-study based, so practical deployment issues (model mismatch, actuator limits, non-Gaussian noise, time delays, and data quality) remain open.","The conclusions discuss a spectrum of hybrid DOE approaches trading off accuracy and computational complexity, including using more advanced GP models whose posteriors may not be closed form (necessitating sampling-based predictive divergence approximations) and hybrid mechanistic/data-driven models with physically meaningful embeddings. They also remark that while closed-loop (online) redesign is theoretically possible for model discrimination, computational cost likely makes it impractical with the present approach, implying a direction toward more efficient methods if online operation is desired.","Extending the framework to handle autocorrelated/process-dependent noise and non-Gaussian uncertainties (e.g., heavy-tailed measurement noise) would improve robustness in real processes. A principled approach for jointly learning/estimating noise covariances and propagating that uncertainty into the design criterion could reduce reliance on expert-specified covariance upper bounds. Incorporating multi-output (coregionalized) GP surrogates or structured/physics-informed surrogates could better capture coupled state dynamics and reduce surrogate-induced uncertainty inflation. Scaling studies on higher-dimensional states/controls (and longer horizons) with sparse/approximate GP inference and benchmarking against modern Bayesian OED baselines would clarify computational limits and performance trade-offs.",2102.03782v2,https://arxiv.org/pdf/2102.03782v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:15:51Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|E-optimal|Other,Variable/General (subset of message events selected; cardinality constraint |H_T|=N_H),Other|Theoretical/simulation only,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://doi.org/10.1145/3449361|https://dev.twitter.com/rest/public,"The paper proposes CherryPickX, a suite of unsupervised methods to demarcate endogenous vs. exogenous opinion-bearing messages in online social networks, motivated by improving opinion-dynamics forecasting. The approach formulates message labeling as a cardinality-constrained subset selection (choose an endogenous set H_T) that minimizes parameter-estimation uncertainty of a temporal point-process opinion model (built on SLANT), using classical experimental design criteria. The resulting objectives (A-, D-, E-, and T-optimality-based) are shown to be NP-hard but (weakly) submodular/monotone (D submodular; T modular; A and E weakly submodular), enabling greedy approximation algorithms with guarantees. The selected endogenous subset is then used to fit model parameters via maximum likelihood and to forecast sentiments. Extensive experiments on multiple Twitter datasets and synthetic networks show improved forecasting error (MSE and polarity failure rate) over robust-regression/outlier-removal baselines and over using all events (SLANT), with D- and A-based variants typically strongest on real data and D/T strong on synthetic settings.","The design variable is the endogenous event subset $H_T\subseteq U_T$ with fixed size $|H_T|=N_H$, chosen to minimize a scalar summary of the parameter covariance $\Sigma(H_T)=\mathbb{E}[(\hat\theta-\theta)(\hat\theta-\theta)^T]$, where for regularized least squares $\Sigma(H_T)=\mathrm{diag}_{u\in V}\left(cI+\sigma^{-2}\sum_{e_i\in H_T}\phi_i^u(\phi_i^u)^T\right)^{-1}$. The paper defines experimental-design objectives $\Omega_A=\mathrm{tr}[\Sigma]$, $\Omega_D=\log\det(\Sigma)$, $\Omega_E=\lambda_{\max}(\Sigma)$, and $\Omega_T=-\mathrm{tr}[\Sigma^{-1}]$, and maximizes $f_X(H_T)=-\Omega_X(H_T)$ via greedy marginal gains.","On five Twitter datasets (e.g., Club, Elections, Verdict, Sports, Delhi) with a fixed outlier fraction $\gamma=0.2$ and prediction horizon history window $T=4$ hours, CherryPick variants generally reduce sentiment prediction MSE and polarity failure rate versus SLANT (no filtering) and robust baselines. For example, on Sports, CherryPickD achieves MSE 0.056 vs. SLANT 0.076, and failure rate 0.079 vs. 0.098; on Club, CherryPickD achieves MSE 0.037 vs. 0.040 and failure rate 0.113 vs. 0.126. Synthetic experiments on 512-node networks with 20% exogenous events show CherryPickD/CherryPickT improve parameter-estimation MSE relative to baselines across training sizes and degrade more gracefully as injected noise increases. Performance is sensitive to the assumed endogenous fraction; best results often occur when selecting roughly 70–90% of events as endogenous, with degradation when selecting too few or all events.","The authors note that true endogenous/exogenous labels are typically unavailable in public datasets, so they cannot report direct classification error and instead evaluate utility through downstream predictive performance. They also exclude intensity parameters $(\mu, B)$ from covariance-based subset selection because their MLE lacks a closed-form solution, making covariance estimation mathematically inconvenient. They mention computational inefficiency issues for some criteria, noting eigenvalue optimization (E-optimality) and trace-of-inverse operations (T-optimality) can lead to poorer training/performance in practice.","The method requires the user to pre-specify the endogenous subset size (or outlier fraction $\gamma$); in practice, this tuning may be difficult without labels and could materially affect results. The theoretical design criteria are derived under a specific linear/Gaussianized estimation setup for opinion parameters and assume the underlying SLANT-style model is correctly specified; robustness to model misspecification, non-Gaussian sentiment noise, temporal dependence beyond the Hawkes/SLANT assumptions, or missing network edges is not established. No implementation details (code, runtime/memory scaling) are provided, which may hinder reproducibility and assessing scalability on very large OSNs.",None stated.,"Develop data-driven procedures to estimate $\gamma$ (or $|H_T|$) automatically (e.g., via cross-validation on forecasting loss, empirical Bayes, or stability selection) to reduce reliance on manual tuning. Extend the covariance-based design to include intensity parameters $(\mu,B)$ (e.g., via approximations, observed information/Fisher information, or variational inference) and study robustness under model misspecification, heavy-tailed sentiment noise, and autocorrelated or missing data. Provide open-source implementations and benchmark computational performance; explore faster greedy/streaming methods for very large event streams and multivariate extensions (topics, communities, or multimodal signals) to improve practical adoption.",2102.05954v1,https://arxiv.org/pdf/2102.05954v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:16:28Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Prediction|Parameter estimation|Cost reduction|Other,D-optimal|Bayesian D-optimal|Other,Variable/General (design variables are candidate sensor locations; chooses r sensors out of d candidates),Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops an efficient computational framework for goal-oriented Bayesian optimal experimental design (GOOED) in large-scale linear inverse problems, motivated by optimal sensor placement. Instead of maximizing information gain about the model parameters, the method maximizes expected information gain (mutual information/KL-based) about a predicted linear quantity of interest (QoI). The authors derive an alternative closed-form EIG expression that enables an offline–online decomposition: expensive model-constrained operator constructions are performed once offline, while repeated design evaluations/optimization are done online without further PDE solves. Scalability is achieved by randomized low-rank approximations of two key data/goal-informed operators and by a swapping greedy algorithm (initialized via leverage scores) for the combinatorial sensor-selection problem. Numerical experiments on a contaminant transport PDE inverse problem show accurate EIG approximation with dimension-independent numbers of model solves and large speedups (reported as >1000× in PDE solves) compared to repeated direct EIG evaluations.","The Bayesian linear model is $y=Fm+\varepsilon$ with Gaussian prior and noise, giving a closed-form EIG for parameter inference $\Psi=\tfrac12\log\det(I+\Gamma_{pr}^{1/2}F^*\Gamma_n^{-1}F\Gamma_{pr}^{1/2})$. For goal-oriented design with QoI $\rho=Pm$, the goal EIG is $\Psi_\rho(W)=\tfrac12\log\det(I_\rho+\Sigma_{pr}^{1/2}H^\rho_m(W)\Sigma_{pr}^{1/2})$ where $H^\rho_m(W)=(F(W)P^\dagger)^*\Gamma_\eta(W)^{-1}F(W)P^\dagger$. A key offline–online reformulation is $\Psi_\rho(W)=\tfrac12\log\det(I_r + L^T W H_d^\rho W^T L)$ with $H_d^\rho=F_d\Gamma_{pr}P^*\Sigma_{pr}^{-1}P\Gamma_{pr}F_d^*$ and $\Gamma_\eta^{-1}=LL^T$; low-rank approximations $\hat H_d^\rho=U_kZ_kU_k^T$ and $\widehat{\Delta H}_d=V_l\Lambda_lV_l^T$ yield $\hat\Psi_\rho(W)=\tfrac12\log\det(I_r+\hat L^T W\hat H_d^\rho W^T\hat L)$.","On a contaminant transport (advection–diffusion) PDE inverse problem, the offline–online decomposition eliminates PDE solves during the many EIG evaluations required by sensor-selection optimization once low-rank operators are built. For $d=75$ candidate sensors and varying budgets (e.g., $r=5$ to $60$), the swapping greedy method yields consistently higher approximate EIG than 200 random sensor sets, and often improves over standard greedy for small $r$; for large $r$, both greedy methods become comparable as many designs achieve similar EIG. The authors report achieving over $1000\times$ speedup measured in PDE solves versus approaches that would require PDE solves for each EIG evaluation. Eigenvalue decay studies show the spectra of the key operators (notably $\Delta H_d$) decay rapidly and approximately independently of parameter dimension and candidate-sensor dimension, implying only $O(10^2)$ PDE solves are needed for accurate low-rank EIG approximation. They also provide an explicit EIG approximation error bound in terms of truncated eigenvalues (Theorem 3.4).",None stated.,"The method is developed for linear PtO and linear QoI maps with Gaussian prior/noise, so performance and guarantees may not carry over directly to nonlinear, non-Gaussian, or model-mismatch settings common in practice. The optimization is a heuristic combinatorial procedure (swapping greedy) without global optimality guarantees for the non-submodular objective, and results are shown on a single main application family (contaminant transport) with limited benchmarking against other modern BOED/MI estimators. Implementation details (software, reproducibility, runtime breakdowns) are not fully specified, and no public code is provided, which may hinder adoption. Practical deployment aspects such as sensor failure, correlated noise, temporal placement (when to measure), and constraints beyond a simple cardinality budget are not addressed.",The authors state future work will extend the approach to nonlinear Bayesian GOOED problems with nonlinear parameter-to-observable maps and nonlinear parameter-to-QoI maps.,"Extending the framework to handle correlated and/or heteroscedastic observation noise, time-dependent/trajectory designs, and richer constraints (e.g., spatial exclusion zones, costs varying by location) would broaden applicability in sensor placement. Developing theoretical or empirical guidance for selecting low-rank tolerances $(\varepsilon_\zeta,\varepsilon_\lambda)$ and for stopping criteria in swapping greedy could improve reliability. Providing open-source implementations (e.g., in Python/FEniCS or MATLAB) and testing on standardized PDE-inverse benchmarks would facilitate reproducibility and fair comparison to alternative EIG estimators (e.g., variational/neural MI methods). Robustness studies under model error and discretization error, and self-starting/online adaptive (sequential) sensor placement with partial data assimilation, are natural next steps.",2102.06627v2,https://arxiv.org/pdf/2102.06627v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:17:02Z
TRUE,Optimal design|Other,Prediction|Parameter estimation|Other,Other,Variable/General (examples with 1 stress variable; 2 stress variables; plus time as repeated-measures schedule fixed in advance),Other|Theoretical/simulation only,Exact distribution theory|Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper develops optimal experimental designs for repeated-measures accelerated degradation tests when degradation paths follow a linear mixed effects model with unit-to-unit random effects (random intercept and/or random slope in time) and potentially multiple stress variables. The design goal is to choose constant stress settings across units (and, in the broader framework, potentially measurement time plans) to minimize the asymptotic variance of the estimator of a quantile (especially the median) of the failure-time distribution under normal use conditions, defined via soft failure when the mean degradation path crosses a threshold. Using a delta-method approximation, the authors show that when measurement times are fixed in advance, minimizing the asymptotic variance of the estimated failure-time quantile reduces to a marginal c-optimal extrapolation problem in the stress-variable regression, independent of which quantile is targeted (provided the quantile exists). For product-type regression structures (stress × time interactions), the information matrix factorizes so stress and time designs can be optimized independently; for multiple stress variables with product structure, the c-optimal stress design factorizes into marginal c-optimal designs. The work provides analytic characterizations of optimal stress allocations (often concentrated on boundary stress levels) and discusses extensions, including cases with non-interacting stress variables where optimal designs may be non-unique (via Elfving’s theorem).","The mixed-effects degradation model is $Y_{ij}=f(x_i,t_j)^T\beta_i+\varepsilon_{ij}$ with product-type regressors $f(x,t)=f_1(x)\otimes f_2(t)$ and random effects entering via $\beta_i=\beta+\text{(unit deviations)}$, yielding per-unit covariance $V=F_2\Sigma_\gamma F_2^T+\Sigma_\varepsilon$. The Fisher information for fixed effects factorizes as $M_\beta=(F_1^TF_1)\otimes(F_2^TV^{-1}F_2)=M_1\otimes M_2$. The failure-time CDF under normal use satisfies $F_T(t)=\Phi(h(t))$ with $h(t)=(\mu(t)-y_0)/\sigma_u(t)$, and the asymptotic variance of the estimated quantile $\hat t_\alpha$ is approximated by $\mathrm{aVar}(\hat t_\alpha)=c^T M_\theta^{-1}c$ (delta method), which—when times are predetermined—reduces to minimizing the marginal c-criterion $f_1(x_u)^T M_1(\xi)^{-1} f_1(x_u)$.","With predetermined measurement times, the optimal stress design for estimating any failure-time quantile $t_\alpha$ (when $0<t_\alpha<\infty$) is the c-optimal extrapolation design at the normal-use stress $x_u$ in the marginal stress regression, so the optimal design does not depend on $\alpha$. For a single stress variable with $f_1(x)=(1,x)^T$ on $[0,1]$ and $x_u<0$, the c-optimal design places mass only at the endpoints $0$ and $1$ with weight at $1$ equal to $w^*=|x_u|/(1+2|x_u|)$ (and $1-w^*$ at $0$). For two stress variables with product structure and linear marginals, the c-optimal design is the product of the two marginal c-optimal endpoint designs, giving support on the four corners $(0,0),(0,1),(1,0),(1,1)$ with weights determined by the two $w_v^*$ values. The paper reports efficiencies showing common uniform/corner designs can require substantially more units than the optimal design (e.g., in the two-stress example, the equal-weight four-corner design has efficiency about 0.61 for the stated normal-use conditions).",None stated.,"The optimality results are local in the sense that quantile existence and (when optimizing time plans) some components depend on nominal model parameters; robustness to misspecification is not developed beyond discussion. The reduction to c-optimal stress designs relies on assumptions including correct linear mixed model specification, constant stress within unit, and the predetermined/common measurement-time plan; if stress-related random effects, autocorrelation/heteroscedasticity structures, or model misspecification are present, optimal designs may change. The work is primarily theoretical with illustrative numeric examples; there is limited empirical validation on real accelerated degradation datasets and no accompanying software implementation to ease adoption.","The authors propose studying cases where assumptions fail—e.g., models without full stress×time interactions (additive structures) and models where stress variables also have random effects—since then stress and time designs may not be optimized independently. They also suggest developing designs robust to misspecification of nominal parameters, such as maximin-efficient or weighted (“Bayesian”) optimal designs. Another stated direction is to consider criteria for simultaneous estimation of multiple characteristics of the failure-time distribution.","Develop practical, implementable algorithms and open-source software for constructing (approximate and exact) optimal stress allocations under linear mixed-effects ADT models with constraints (costs, discrete stress settings, safety limits). Extend the methodology to handle autocorrelated measurement errors and more realistic non-linear degradation paths (e.g., Arrhenius/Eyring transforms, nonlinear mixed models), and to multivariate degradation signals. Provide broader benchmarking against competing ADT planning criteria (e.g., D-, I-, or Bayesian prediction-optimal designs) and validate on multiple real industrial case studies to assess sensitivity to threshold choice and model misspecification.",2102.09446v2,https://arxiv.org/pdf/2102.09446v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:17:39Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction|Other,Other,Variable/General (K strata; design chooses per-stratum sample sizes and treatment/control allocations; example uses 30 strata),Healthcare/medical,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes a method for designing a more efficient randomized controlled trial (RCT) using information extracted from an observational study, without using the observational study for causal inference. The experimental design problem is a stratified RCT with binary outcomes, where the planner must allocate a fixed total sample size across strata and between treatment and control within each stratum to minimize weighted mean-squared error of stratum-specific treatment-effect estimates. Because observationally estimated stratum variances may be biased under unmeasured confounding, the authors construct sensitivity-aware confidence sets for stratum potential-outcome variances by adapting Zhao et al. (2019)’s bootstrap sensitivity analysis (indexed by a confounding parameter Γ). They then formulate a minimax regret design: choose allocations that minimize worst-case regret in risk relative to a default allocation, over variance values within the confidence sets; via von Neumann’s minimax theorem this becomes a tractable concave maximization solved numerically (e.g., projected gradient descent). In a Women’s Health Initiative example (designing an nr=1000 RCT using the observational component), the regret-minimizing allocation modestly improves average L2 loss versus equal or naive plug-in allocation, while guaranteeing not to underperform the default under the worst-case variance scenario.","Risk for stratified difference-in-means estimators is $R=\sum_k w_k\left(\sigma_k^2(1)/n_{rk,t}+\sigma_k^2(0)/n_{rk,c}\right)$, where $n_{rk,t},n_{rk,c}$ are per-stratum treatment/control counts. The naive optimal allocation (if variances known) is $n_{rk,t}\propto \sqrt{w_k}\,\sigma_k(1)$ and $n_{rk,c}\propto \sqrt{w_k}\,\sigma_k(0)$. The proposed design minimizes worst-case regret relative to a default $(\tilde n_{rk,t},\tilde n_{rk,c})$: $\min_{n}\max_{(\sigma_k^2(1),\sigma_k^2(0))\in A_k(\Gamma)}\sum_k w_k\left[\sigma_k^2(1)(1/n_{rk,t}-1/\tilde n_{rk,t})+\sigma_k^2(0)(1/n_{rk,c}-1/\tilde n_{rk,c})\right]$, subject to total sample size constraints. For binary outcomes, variances are linked to means by $\sigma_k^2(e)=\mu_k(e)(1-\mu_k(e))$, enabling sensitivity/bootstrap-based confidence sets for variances via bounds on $\mu_k(e)$.","In the detailed WHI example with Γ=1.5 and fine stratification (30 strata; equal weights; nr=1000), the regret-minimizing allocation reduced average L2 loss by 3.6% relative to the naive plug-in allocation and by 1.6% relative to equal allocation (over 1,000 resampled pseudo-experiments). Across multiple stratification choices and Γ values, regret-minimizing allocations were always non-worse than equal allocation by construction (loss relative to equal allocation was non-positive, sometimes exactly 0 when the method defaulted to equal allocation). The largest reported gains versus equal allocation were about 5–6% when stratifying on clinically relevant variables (age and cardiovascular disease history) at low Γ (near 1–1.1). Relative to naive allocation, the regret-minimizing design could underperform for some settings (e.g., stratifying only on age), reflecting its defensive/worst-case orientation, but tended to outperform naive allocation when the stratification was noisy/irrelevant or the number of strata was larger (e.g., all three variables / 30 strata showed ~2.8%–4.1% improvement depending on Γ).","The paper restricts the main tractable development to binary outcomes, noting that computations are tractable in that setting. In the general continuous-outcome case ($Y\in\mathbb R$), the variance-bounding step leads to a quadratic fractional programming problem with indefinite numerator and limited curvature guarantees, which is generally NP-hard; this is presented as a major challenge. The ellipsoidal confidence sets used for variances are acknowledged as not being the smallest valid sets (chosen for convexity and numerical convenience).","The approach assumes a fixed stratification scheme and relies on Assumptions 2–3 that stratum-specific potential-outcome means and variances match between observational and RCT populations; violations (transportability/generalizability failures) could mislead the design even if sensitivity analysis addresses confounding within the observational study. The method’s performance is evaluated primarily via resampling/pseudo-experiments from the WHI trial data rather than deployment in a truly prospective design setting, so operational constraints (recruitment feasibility by stratum, noncompliance, missing outcomes) are not stress-tested. The sensitivity model parameter Γ must be chosen by the user; practical guidance for selecting/calibrating Γ in real applications may be difficult, and results could be sensitive to this choice. Implementation details (software, optimization robustness, convergence diagnostics) are not provided, which may hinder reproducibility and practitioner uptake.","The authors propose extending the method to the general case of real-valued outcomes ($Y\in\mathbb R$), where bounding stratum variances under the sensitivity model becomes difficult. They suggest using Dinkelbach’s method to transform the quadratic fractional program into a sequence of quadratic programs, potentially leveraging recent advances in solving/heuristics for quadratically constrained quadratic programming. They also point to the need for new solution methods to handle the indefinite numerator structure that arises in this extension.","Develop self-starting or data-adaptive designs that update allocations sequentially as early trial outcomes accrue, while maintaining randomization and type-I error control, could make the approach more practical for staged RCTs. Extending the framework to handle clustered/correlated outcomes (e.g., longitudinal or site effects) and common RCT complications (noncompliance, attrition/censoring) would improve applicability. Providing an open-source implementation (with routines for building $A_k(\Gamma)$, solving the minimax program, and producing allocation tables) plus sensitivity analyses over Γ and stratification choices would aid reproducibility and adoption. Finally, studying robustness when Assumptions 2–3 fail (partial transportability) and integrating explicit recruitment/cost constraints per stratum would align the design optimization with real-world trial operations.",2102.10237v1,https://arxiv.org/pdf/2102.10237v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:18:22Z
TRUE,Sequential/adaptive|Bayesian design|Computer experiment|Other,Parameter estimation|Prediction|Other,Not applicable,"Variable/General (examples include 1D, 2D; engineering case uses 2 factors A and L)",Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/umbrellagong/HGPextreme,"The paper develops a sequential Bayesian experimental design (active learning) method to efficiently estimate an extreme-event (exceedance) probability when the input-to-response function is expensive and stochastic with heteroscedastic (input-dependent) noise. It uses variational heteroscedastic Gaussian process regression (VHGPR) to jointly model the response mean and a GP-driven log-variance, improving over standard GP regression that assumes constant noise. A new acquisition rule selects the next sample by maximizing a probability-weighted local uncertainty of the exceedance event, targeting reduction in the variance of the overall exceedance probability estimate. The method is validated on 1D and 2D synthetic examples and an engineering application estimating extreme ship roll probability in irregular waves, showing faster convergence than Latin hypercube baselines and large errors for SGPR-based approaches that ignore heteroscedasticity. The authors also provide a Python implementation package and example cases.","The exceedance probability is defined as $P_e=P(S(X,\omega)>\delta)=\int P(S(x,\omega)>\delta)\,p_X(x)\,dx$. The stochastic input-to-response model is decomposed as $S(x,\omega)=f(x)+R(x,\omega)$ with heteroscedastic noise $R\mid D\sim\mathcal N(0,\exp(g(x)))$ and $f(x),g(x)$ each given GP posteriors under VHGPR. The sequential design chooses the next input by maximizing a weighted uncertainty criterion: $x^*=\arg\max_x\,\mathrm{std}_{f,g}[P(S(x,\omega\mid f,g)>\delta)]\,p_X(x)$, approximating the mean/variance over $(f,g)$ via spherical cubature; the final estimate plugs in the posterior means $\mu_f,\mu_g$ into $P(S(X,\omega\mid f=\mu_f,g=\mu_g)>\delta)$.","Across 1D and 2D synthetic benchmarks, Seq-VHGPR reaches the Monte Carlo reference (within a reported 5% error band) within roughly the first ~20 sequential samples beyond the initial Latin-hypercube set, while LH-VHGPR converges much more slowly. In both synthetic problems, LH-SGPR converges to biased asymptotic values due to assuming constant noise (reported as about 3× the exact result in 1D and about 2.5× in 2D). In the ship-roll application (2D input: wave-group amplitude $A$ and length $L$), Seq-VHGPR matches the long-time simulation ‘exact’ exceedance probability within about ~30 sequential samples, whereas LH-VHGPR is slower and LH-SGPR remains appreciably biased. The sequential samples concentrate near regions that maximize the integrand $P(S(x,\omega)>\delta)p_X(x)$, explaining faster convergence.","The authors note that the acquisition function in Eq. (10) may be improvable for stochastic input-to-response systems, particularly by incorporating correlation across different $x$ rather than relying primarily on local standard deviation. They also state that transferring deterministic-ItR enhancements (e.g., hypothetical points or global sensitivity analysis) to the heteroscedastic VHGPR setting may incur significantly increased computational cost (e.g., retraining variational parameters). They remark that the Gaussian assumption for $S(x,\omega)$ (conditional on $x$) is standard but should be checked in applications and they provide such a check for the ship-roll case.","The proposed final estimator uses plug-in posterior means ($\mu_f,\mu_g$) to compute $P_e$, which may under-represent posterior uncertainty in the reported probability compared to full Bayesian marginalization over $f,g$. The acquisition maximizes a pointwise uncertainty quantity and does not directly optimize a global integrated risk/utility (e.g., expected reduction in variance of $P_e$), so it may be suboptimal in multimodal failure regions or when exploration–exploitation tradeoffs are delicate. Performance is demonstrated mostly via synthetic and simulation-based case studies; additional real experimental/field validations and robustness checks under model mismatch (non-Gaussian noise, autocorrelation, nonstationarity) would strengthen generality. Comparisons focus on SGPR/VHGPR with LH vs sequential sampling, but do not comprehensively benchmark against alternative heteroscedastic surrogate models or advanced reliability methods tailored to stochastic simulators.","They propose improving the acquisition function beyond Eq. (10) by accounting for correlations between different inputs and by adapting ideas from deterministic-ItR designs (e.g., using hypothetical points and global sensitivity analysis), while managing the added computational cost with VHGPR. They indicate these developments are left for future work.","Developing a fully Bayesian estimator of $P_e$ that integrates over the VHGPR posterior (with credible intervals for $P_e$) could better quantify uncertainty, especially with small data. Extending the method to handle non-Gaussian or heavy-tailed intrinsic noise (e.g., via warped GPs, Student-t likelihoods, or mixture models) and to correlated/temporal outputs would broaden applicability to realistic simulators. Multi-fidelity extensions (cheap/expensive simulators) and batch/parallel acquisition strategies would increase practicality in HPC environments. Providing a documented, reproducible benchmark suite and standardized comparisons against modern active learning reliability methods for stochastic simulators would clarify when Seq-VHGPR offers the greatest advantage.",2102.11108v4,https://arxiv.org/pdf/2102.11108v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:18:55Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (design variable(s) $\xi_t$ chosen sequentially; examples include 2D location coordinates, $(R,D)$, and observation time $\xi>0$)",Healthcare/medical|Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/ae-foster/dad,"The paper introduces Deep Adaptive Design (DAD), a method for amortizing sequential Bayesian optimal experimental design by learning a neural-network design policy $\pi_\phi$ offline and deploying it to choose each next design $\xi_t$ in milliseconds during live experiments. It reformulates sequential BOED as maximizing the total mutual information between parameters $\theta$ and the full experimental history $h_T$, enabling non-myopic adaptive strategies without repeatedly fitting posteriors at each step. To make training tractable, it derives sequential contrastive information bounds—especially a sequential Prior Contrastive Estimation (sPCE) lower bound—allowing end-to-end stochastic gradient optimization of the policy parameters. A permutation-invariant architecture pools encoded design–outcome pairs to respect symmetries of the history. Across simulated case studies (location finding, temporal discounting surveys, and an epidemiological death process), DAD achieves higher total expected information gain than fixed/random baselines and often outperforms slower non-amortized adaptive BOED methods while being orders of magnitude faster at deployment.","BOED objective for a single design is expected information gain / mutual information: $I(\xi)=\mathbb{E}_{p(\theta)p(y\mid\theta,\xi)}[\log p(y\mid\theta,\xi)-\log p(y\mid\xi)]$. For sequential design, DAD optimizes a policy objective: $I_T(\pi)=\mathbb{E}_{p(\theta)p(h_T\mid\theta,\pi)}[\log p(h_T\mid\theta,\pi)-\log p(h_T\mid\pi)]$, where the policy deterministically maps history to the next design $\xi_t=\pi(h_{t-1})$. Training uses the sequential PCE lower bound $L_T(\pi,L)=\mathbb{E}[\log \tfrac{p(h_T\mid\theta_0,\pi)}{\frac{1}{L+1}\sum_{\ell=0}^L p(h_T\mid\theta_\ell,\pi)}]$ with contrastive samples $\theta_{1:L}\sim p(\theta)$. The design network uses permutation-invariant pooling: $R(h_t)=\sum_{k=1}^t E_{\phi_1}(\xi_k,y_k)$ and $\pi_\phi(h_t)=F_{\phi_2}(R(h_t))$.","In the 2D location-finding task (T=30, K=2 sources), DAD achieved substantially higher total EIG bounds (e.g., lower bound 10.926±0.036 vs fixed 8.838±0.039; upper bound 12.382±0.095 vs fixed 8.914±0.038) and was far faster at deployment (about 0.047 s for 30 decisions vs ~8963 s for a variational iterative baseline). In hyperbolic temporal discounting (T=20), DAD again delivered the highest information bounds (lower 5.021±0.013; upper 5.123±0.015) while maintaining ~0.090 s total deployment time, compared with ~25.27 s for a non-amortized adaptive baseline (Badapted). In the epidemiological death-process example (T=4), DAD deployed in ~0.005 s and achieved higher estimated total EIG than fixed and variational baselines (2.113±0.008 vs 2.023±0.007 and 2.076±0.034, respectively). Overall, results support that amortized non-myopic policies can match or exceed conventional adaptive BOED performance with dramatically reduced online computation.","The authors note that DAD currently requires an explicit likelihood that can be evaluated, i.e., access to the density $p(y_t\mid\theta,\xi_t)$. They also assume conditional independence of observations given parameters, $p(y_{1:T}\mid\theta,\xi_{1:T})=\prod_{t=1}^T p(y_t\mid\theta,\xi_t)$, which may fail in time-series settings. Finally, they assume continuous design variables to support gradient-based optimization.","The method’s offline-trained policy can be sensitive to prior/model misspecification; if the deployed system differs from the simulator used for training, the amortized policy may choose systematically suboptimal designs. While contrastive bounds enable training, the resulting objective is a biased lower bound whose tightness depends on the number of contrastive samples $L$ and may affect performance if computational budgets constrain $L$. The paper provides code but does not establish broad real-data validations (beyond simulated case-study models), and practical guidance on choosing architectures/hyperparameters for new domains may still require substantial tuning.","They suggest extending DAD beyond settings with explicit likelihoods, relaxing the conditional-independence assumption to handle time-series models, and addressing the requirement that designs be continuous (to allow gradient-based optimization). They also propose further investigating the links between policy-based experimental design and model-based reinforcement learning as a potential avenue for new methods.","Developing robust/self-calibrating variants that adapt online to model mismatch (e.g., via posterior correction, domain adaptation, or meta-learning) would improve reliability in real deployments. Extending DAD to discrete or mixed discrete-continuous design spaces with principled gradient estimators (e.g., relaxations or combinatorial search hybrids) would broaden applicability. More extensive benchmarking on standardized BOED tasks and real experimental platforms, plus packaged tooling (e.g., a PyPI/conda library) and diagnostics for assessing bound tightness and policy failure modes, would accelerate practitioner adoption.",2103.02438v2,https://arxiv.org/pdf/2103.02438v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:19:35Z
TRUE,Optimal design|Other,Parameter estimation|Prediction,D-optimal|A-optimal|Other,Variable/General (finite discrete design space size N; regression parameter dimension p; exact design size n points),Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Other,Not provided,http://arxiv.org/ps/2103.02989v3,"The paper develops a convex formulation for optimal design of experiments in linear regression models with correlated observation errors on a finite (discrete) design space, motivated by Gaussian process/kriging settings. It introduces a new “virtual noise” variance structure that makes the approximate-design objective concave over a convex set of restricted design measures (with per-point upper bound 1/n), enabling convex optimization and a Kiefer–Wolfowitz–type equivalence theorem for correlated-data designs. Using this formulation, the authors propose a cutting-plane/modified linear programming algorithm that produces an optimal design measure and also yields an upper bound on the performance of any exact n-point design, plus natural procedures to generate exact designs (quantiles and random sampling from the measure). The approach supports standard optimality criteria (e.g., D- and A-optimality) and can be used to benchmark existing heuristic/approximate correlated-design algorithms. Numerical comparisons on classical literature examples and a real spatio-temporal sensor-placement/weather-station application show the bound is useful for calibration and that derived exact designs are competitive, while a Brimkulov–Krug–Savanov/Fedorov-type sensitivity heuristic often performs slightly better but can be evaluated against the convex upper bound.","Model with correlated errors: $y(x)=f^T(x)\theta+\varepsilon(x)$ on a finite design space $\mathcal X=\{x_1,\dots,x_N\}$ with known covariance matrix $C$. The method augments the model with independent “virtual noise” $w_\xi(x)$ having variance $\sigma^2_\xi(x)=\kappa\,(\frac{1}{n}-\xi(x))/\xi(x)$, producing an information matrix for a design measure $\xi$ of $M(\xi)=F^T\{C+W(\xi)\}^{-1}F$ (with $W(\xi)$ diagonal). Under $\kappa<\lambda_{\min}(C)$ and concave increasing criteria $\Phi(M)$ (e.g., $\log\det(M)$ for D, $-\mathrm{tr}(M^{-1})$ for A), $\xi\mapsto\Phi(M(\xi))$ is concave, enabling a linear-programming cutting-plane algorithm and an equivalence theorem characterized via a sensitivity-like function $h(x,\xi)$ and inequality over all n-tuples of points.","Theorem 1 establishes concavity of $\Phi\{M(\xi)\}$ over the convex restricted design set when $\kappa<\lambda_{\min}(C)$, enabling convex optimization for correlated-error designs. Theorem 2 provides an equivalence theorem and a computable performance upper bound for all exact n-point designs via the optimal measure $\bar\xi$. In Example 1 (n=4, D-opt), the BKSF heuristic achieves D-efficiency 0.9075 vs an exhaustive-search optimum 0.9158 (relative to the bound), while the quantile-from-measure method Q-VN gives 0.8316. In Example 2 (4-parameter polynomial, n=5, D-opt), Q-VN+EP achieves 0.9300 and BKSF 0.9270, close to exhaustive search 0.9308. In a real Upper Austria weather-station redesign (N=445, n=36, D-opt), the best of 100 random samples from the virtual-noise measure attains 0.9915 D-efficiency, and BKSF attains 0.9965 (all relative to the convex upper bound).","The paper restricts its theory to estimation problems on a finite discrete design space with a fixed number of candidate points and uses a tuning parameter $\kappa$ that must satisfy $\kappa<\lambda_{\min}(C)$ (chosen near this bound in examples). For computational reasons, the linear-programming algorithm is implemented over $\Xi_\varepsilon=\{\xi:\xi(x)\ge \varepsilon\}$ with small $\varepsilon>0$ to avoid numerical problems. The authors note that quantile-based conversion of measures to exact designs is not easily extended to higher-dimensional design spaces, motivating random sampling for multi-dimensional cases.","The approach assumes the covariance matrix/kernel (and thus $C$) is known; performance under covariance misspecification or when kernel parameters must be estimated (design for joint estimation) is not addressed. The restriction $0\le \xi(x)\le 1/n$ enforces unreplicated n-point support and excludes replicated designs, which may be desirable when measurement noise includes an i.i.d. nugget or when replication helps estimate variance components. The practical scalability of the cutting-plane LP method for very large N (beyond the presented N=445) and the sensitivity of solutions to the choice of $\kappa$ and $\varepsilon$ are not thoroughly characterized. Comparisons focus on a subset of competing correlated-design methods and primarily on D/A criteria, leaving out other objectives common in GP design (e.g., integrated MSE/I-optimality, entropy/mutual information) except indirectly.",None stated.,"Extend the convex virtual-noise framework to settings with unknown covariance/kernel parameters, enabling joint design for regression and covariance (or fully Bayesian) learning under correlation. Develop principled, scalable exact-design extraction methods in higher dimensions beyond random sampling/quantiles (e.g., deterministic rounding with guarantees or greedy methods guided by the equivalence theorem). Investigate robustness to covariance misspecification and incorporate autocorrelation/nonstationarity or heteroskedasticity, including nugget effects that permit replication. Provide open-source implementations and benchmarking suites to facilitate adoption and systematic comparison across modern GP/kriging design criteria (e.g., I-/G-optimality, information gain) and large N regimes.",2103.02989v3,https://arxiv.org/pdf/2103.02989v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:20:10Z
TRUE,Response surface|Computer experiment|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Space-filling,"Variable/General (benchmark functions range from 1D to 10D; examples include 2D, 3D, 4D, 8D, 10D)",Theoretical/simulation only,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/sjvrijn/multi-level-co-surrogates,"The paper studies how to split a fixed computational budget between low- and high-fidelity evaluations when building hierarchical (additive) multi-fidelity response surface models (e.g., co-kriging-style) for expensive simulation optimization. It constructs multi-fidelity Designs of Experiments using a nested Latin Hypercube Sampling (LHS) procedure (based on Le Gratiet, 2013) so that all high-fidelity points are included in the low-fidelity design, enabling training of a discrepancy model on paired points. The authors perform a large-scale empirical enumeration of many (n_h, n_l) sample-size combinations to create “error grids” of surrogate mean squared error (MSE) versus design sizes, and also propose a practical cross-validated subsampling method that approximates these grids from a single initial multi-fidelity DoE without new evaluations. They quantify the global trade-off via a linear fit to log10(MSE) over (n_h, n_l), yielding a gradient direction/angle that indicates whether additional budget should go primarily to high- or low-fidelity samples. Using this gradient and a cost ratio between fidelities, they derive a simple heuristic to choose the split of additional samples; it performs well on most benchmark problems but can fail when the local error-gradient behavior changes with sample size.","Multi-fidelity additive surrogate structure: $z_h(x)=\rho z_l(x)+\delta(x)$, with the paper using a simplified setting $\rho=1$ and separate models for $z_l$ and the discrepancy $\delta(x)\approx f_h(x)-f_l(x)$. Model error is measured as $\mathrm{MSE}(z,T)=\frac{1}{|T|}\sum_{x\in T}(z(x)-f_h(x))^2$. To summarize error-grid trends they fit $\log_{10}(\mathrm{MSE})=\alpha+\beta_h n_h+\beta_l n_l$ and define a global gradient angle $\theta=\arctan(\beta_h/\beta_l)$; with cost ratio constraint $\Delta n_h+\phi\Delta n_l=b$, they propose $\Delta n_l=\frac{b\beta_l}{\beta_h+\phi\beta_l}$ and $\Delta n_h=\frac{b\beta_h}{\beta_h+\phi\beta_l}$.","Full enumeration uses up to $I=50$ repetitions per (n_h,n_l) pair, with typical maxima $n_h^{\max}=50$ and $n_l^{\max}=125$ (noted as about 250,000 DoEs to sample/train). Test sets are LHS with size $|T|=500\cdot N$ (N = dimension). Example fitted global gradient angles reported include approximately 88° (2D Booth), 34° (2D Currin), 72° (4D Park91A), and 63° (8D Borehole), demonstrating very different marginal value of low-fidelity sampling across problems. Subsampling-based error grids correlate well with full-enumeration angles (reported spreads on the order of ~±5° to ±15°), and the budget-split heuristic often aligns with the minimum MSE along a fixed-budget line, though an example (Park91A) shows failure when the linear global fit does not match the local upper-right region of the error grid.","The authors note that their extrapolation/splitting heuristic can fail when the dominant gradient behavior of the error grid changes with the number of samples; a single global linear fit may not reflect the local direction of improvement at larger sample sizes (upper-right region). They also acknowledge they only considered a hierarchical surrogate model based on a simplified additive co-kriging design (including fixing $\rho=1$), and that other multi-fidelity models may yield different gradient angles and behavior.","The study is largely based on synthetic benchmark functions (mf2) and assumes the ability to define/obtain consistent low- and high-fidelity evaluations and a stable cost ratio, which may be difficult in real engineering workflows where costs vary with x or run conditions. The nested LHS construction (nearest-neighbor replacement) is heuristic and may not be optimal for discrepancy learning in higher dimensions or under constraints; alternative nested/optimal designs (e.g., D/I-optimal for GP hyperparameters or prediction) are not evaluated. The performance metric is primarily MSE against the high-fidelity function; implications for downstream optimization performance (e.g., regret, EI quality) are only suggested, not demonstrated as an end-to-end optimizer comparison.","They propose validating the approach on additional benchmark functions and real-world problems to confirm the utility of error grids for practical applications. They also suggest investigating other multi-fidelity surrogate models beyond their simplified additive co-kriging setup, expecting gradient angles to change with model quality. Finally, they indicate the need to explore the benefit of using error grids and the extrapolation scheme for online fidelity selection within optimization procedures.","Extend the methodology to more than two fidelity levels and to settings where fidelity costs are input-dependent or stochastic, requiring dynamic budget allocation. Develop localized or nonlinear trend models for the error grid (e.g., fitting only near current (n_h,n_l), or using nonparametric surfaces) to improve robustness of the extrapolation. Provide theoretical or empirical guidance on required initial DoE sizes for stable gradient estimation, and release a reusable implementation (e.g., a pip-installable tool) that automates nested design generation, subsampling, and budget-split recommendations.",2103.03280v4,https://arxiv.org/pdf/2103.03280v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:20:45Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Screening|Prediction|Parameter estimation|Cost reduction|Other,D-optimal|Bayesian D-optimal,Variable/General (population size N; examples include N=32 and N=10; pool membership variables define the design),Healthcare/medical,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper proposes DOPE (D-Optimal Pooling Experimental design), a Bayesian optimal experimental design method for constructing pooled RT-PCR tests for SARS-CoV-2 screening. A design is a set of pools, and DOPE selects pools by maximizing the mutual information (an information-theoretic D-optimal criterion) between the latent infection-state vector and the pooled test outcomes under a Bayesian prior and an RT-PCR error likelihood (false negatives per infected sample and a per-pool false positive). Mutual information is estimated via Monte Carlo sampling, posterior updating is performed via Gibbs sampling, and pool selection is solved with a discrete hill-climbing heuristic; the process can be sequential (adaptive retesting) or configured as nonsequential by setting the decision interval empty. Simulation studies compare DOPE with Dorfman, recursive, and matrix pooling under realistic error rates (e.g., Pfn=0.2, Pfp=0.01) and varying prevalence/connectivity, showing decision-interval settings where DOPE achieves lower false-negative rates and lower posterior entropy while using the same or fewer tests. The method’s practical advantage is producing posterior infection probabilities (not just binary calls) and incorporating prior risk information and test errors directly into the design objective.","The pooled-test likelihood for pool $T_k$ with latent infection indicators $\theta_h\in\{0,1\}$ is $P(d_k=0\mid T_k,\theta)=(1-P_{fp})\prod_{h\in T_k} P_{fn}^{\theta_h}$ and $P(d_k=1\mid T_k,\theta)=1-(1-P_{fp})\prod_{h\in T_k} P_{fn}^{\theta_h}$, with independence across pools giving $P(d\mid T,\theta)=\prod_k P(d_k\mid T_k,\theta)$. The D-optimal (information) design criterion is mutual information $\Psi(T)=I(\theta;d\mid T)=\sum_{\theta,d} P(\theta,d\mid T)\log\frac{P(\theta,d\mid T)}{P(\theta)P(d\mid T)}$, estimated by Monte Carlo via $\hat\Psi(T)=\frac{1}{L}\sum_{k=1}^L\Big(\log P(Y_k\mid \eta_k,T)-\log\frac{1}{L}\sum_{r=1}^L P(Y_k\mid\eta_r,T)\Big)$ where $(\eta_k,Y_k)\sim P(\theta)P(d\mid\theta,T)$ (or from the posterior in sequential steps).","In simulations with population size $N=32$ (maximal pool size assumed feasible without dilution effects) and error rates $P_{fn}=0.2$, $P_{fp}=0.01$, DOPE (with appropriately chosen decision intervals) dominates Dorfman, recursive, and matrix pooling in the sense of achieving lower false-negative rates and lower posterior entropy for the same or fewer average tests (Figure 2; example prevalence about 7.7%; mutual information estimated with 20,000 Monte Carlo samples; 123 simulated populations). Under varying prevalence (illustrated for $N=10$ and prevalence roughly 0.02–0.18, with 12,000 Monte Carlo samples), decision intervals can be selected so DOPE’s expected number of tests matches competitors, and DOPE typically yields lower false-negative rates across the prevalence range, with noted exceptions when matching matrix/separate-testing test counts is difficult (Figure 3). Reported false-positive rates across methods remain low (≤ about 1.5% per the discussion and supplement), so the main comparative gains are in reduced false negatives and fewer tests. Runtime examples are provided: a full DOPE run with $N=32$ and $L=20000$ takes <5 hours on seven Intel Xeon Gold 6252 CPUs; with $N=10$ and $L=12000$ takes <0.5 hour.","The authors note that epidemiological connectivity data needed for the clustered prior may be unavailable; in that case one can assume a disconnected population (with results in the supplement). They acknowledge that the iterative/sequential retesting steps may be difficult to implement operationally, suggesting a nonsequential variant (choose all pools a priori) as a potential direction. They also state DOPE is computationally demanding versus standard pooling and currently requires programming familiarity; additionally, the model neglects sample dilution effects and ignores temporal disease progression.","The approach depends on correctness/calibration of the Bayesian prior (cluster structure and parameters $P_p,P_s,P_b$) and the RT-PCR error model ($P_{fn},P_{fp}$); misspecification could degrade optimality and posterior probabilities, yet systematic robustness to prior/likelihood misspecification is not fully developed in the main text. The discrete hill-climbing search provides no guarantee of global optimality and may yield design-quality variability depending on initialization and heuristic settings. Practical deployment constraints (laboratory pooling logistics, maximum pool overlap per individual, pipetting/contamination risk, turnaround-time constraints) are not explicitly embedded as design constraints, which could limit real-world feasibility of some “optimal” pool structures.","They propose exploring/implementing a nonsequential version of DOPE (choose all pools in advance; set $K$ to total tests and decision interval $I=\emptyset$) when iterative retesting is impractical. They plan to create a GUI to enable easier use in facilities performing frequent testing, and they suggest runtime improvements could come from alternative approximations to $\Psi$ (e.g., variational/other estimators) or improved optimization (e.g., surrogate continuous optimization), although an attempted $\ell_0$-sparsification approach did not help much.","A valuable extension would be formal robustness/adaptive-Bayesian design that learns $P_{fn},P_{fp}$ and prior connectivity parameters online (hierarchical modeling) and optimizes designs under parameter uncertainty/misspecification. Incorporating operational constraints explicitly (limits on pool sizes, bounded number of pools per sample, batching/plate layouts, turnaround-time costs) could make the designs more deployable. Empirical validation on real pooled-testing datasets (not only simulations) and benchmarking against additional modern group-testing designs (e.g., nonadaptive combinatorial designs, near-constant column weight designs) would strengthen evidence for practice.",2103.03706v1,https://arxiv.org/pdf/2103.03706v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:21:25Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design|Computer experiment|Other,Parameter estimation|Model discrimination|Prediction|Optimization|Other,Bayesian D-optimal|Other,"Variable/General (design dimension examples: D=1,10,50,100,500; 2D design in quantum control; D sampling times in PK)",Pharmaceutical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://github.com/usnistgov/optbayesexpt,"The paper proposes SAGABED, a scalable gradient-free Bayesian experimental design method for implicit (likelihood-intractable) models that maximizes mutual information (expected information gain) between parameters and data. It replaces the need for pathwise gradients by combining a smoothed variational mutual-information lower bound (SMILE/ISMILE) with stochastic approximate gradient ascent using Guided Evolution Strategies (Gaussian-smoothing-based gradient estimates). The approach yields a unified, joint optimization over design variables and neural MI-estimator parameters, avoiding a two-stage “estimate MI then run Bayesian optimization” workflow that scales poorly in high dimensions. Experiments on (i) noisy linear regression, (ii) a pharmacokinetic blood-sampling-time design, and (iii) a quantum control tuning simulator show improved scalability and generally higher/lower-variance MI estimates and tighter posteriors than MINE-based baselines with SGD or Bayesian optimization, with competitive performance to gradient-based methods when gradients exist. The method also provides an approximate posterior density via the learned critic network and clipping, enabling posterior sampling by MCMC after design optimization.","The design utility is mutual information/expected information gain, e.g., $I(\xi)=\mathbb{E}_{p(\theta,y\mid\xi)}[\log p(\theta\mid y,\xi)-\log p(\theta)]$ and equivalently $I_{MI}(\xi)=\mathbb{E}_{p(\theta)p(y\mid\theta,\xi)}[\log\tfrac{p(y\mid\theta,\xi)}{p(y\mid\xi)}]$. MI is lower-bounded by a neural critic $T_\psi$: MINE $I_{\text{MINE}}(\xi,\psi)=\mathbb{E}_{p(\theta,y\mid\xi)}[T_\psi]-\log\mathbb{E}_{p(\theta)p(y\mid\xi)}[e^{T_\psi}]$, and the proposed smoothed version $I_{\text{SMILE}}(\xi,\psi)=\mathbb{E}[T_\psi]-\log\mathbb{E}[\text{clip}(e^{T_\psi},e^{-\tau},e^{\tau})]$ to reduce variance. The gradient w.r.t. design is approximated via Gaussian smoothing: $\nabla f_\sigma(\xi)=\frac{1}{\sigma}\mathbb{E}_\epsilon[f(\xi+\sigma\epsilon)\epsilon]$ estimated by Monte Carlo / (guided) evolution strategies, then used in stochastic gradient ascent updates for $\xi$ and $\psi$.","In the noisy linear regression example, for $D=1$ all methods converge to an MI lower bound around $\approx 2.6$ and recover boundary-optimal designs ($\xi^*=\pm 10$). In higher-dimensional cases (e.g., $D=50,100$), SAGABED attains MI lower bounds closer to reference nested-MC values with substantially lower variance than MINE+SGD and MINE+BO, while gradient-based training can become unstable (reported collapse at $D=100$). In the pharmacokinetic design, SAGABED achieves MI lower bounds close to reference at $D=10$ and maintains better performance and lower variance than BO-based outer optimization as $D$ increases to 100 and 500, producing noticeably narrower posterior marginals for $(V,k_a,k_e)$ than baselines. In quantum control (implicit simulator without gradients), SAGABED yields faster posterior narrowing and lower posterior variance than BO as the number of designed measurements increases (reported BO variance issues around design dimension 100).",None stated.,"The empirical evaluation is limited to three illustrative problems; broader benchmarks (different simulators, constraints, noise models, and misspecification) would better establish generality. The MI objective is optimized via a variational lower bound with clipping and smoothing hyperparameters (e.g., $\tau$, ES smoothing radius $\sigma$, subspace size $k$, mixing $\alpha$), and results may be sensitive to these choices without clear default-setting guidance. The approach optimizes a smoothed objective and uses approximate gradients, so convergence guarantees to globally optimal designs are not established; local optima and bias from the bound may affect design optimality, especially in very high dimensions. No implementation details/software are provided, which may hinder reproducibility and adoption.","The authors plan to extend the framework to sequential Bayesian experimental design (SBED) with iterative prior/posterior updates. They also aim to improve the computational efficiency of SAGABED, particularly important for SBED. Additionally, they propose improving stochastic approximate-gradient methods and enhancing global exploration to better escape suboptimal designs caused by local minima in current stochastic gradient ascent / evolution-strategy optimizers.","Develop self-tuning or theoretically guided strategies for key hyperparameters ($\tau$, ES smoothing radius, subspace dimension, exploration–exploitation tradeoff) and provide robustness diagnostics for practitioners. Extend to constrained and discrete design spaces (e.g., combinatorial designs, batch/sequential constraints, cost-aware designs) and evaluate on more real-world datasets with experimental validation. Provide uncertainty quantification for the MI estimate/bound during optimization (e.g., confidence intervals) to drive adaptive stopping and resource allocation. Release a reference implementation and standardized benchmarks to assess scalability, wall-clock performance, and reproducibility across hardware and simulators.",2103.08026v1,https://arxiv.org/pdf/2103.08026v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:22:05Z
TRUE,Optimal design|Sequential/adaptive|Other,Model discrimination|Prediction|Optimization|Other,Not applicable,"Variable/General (e.g., acquisition b-values/sampling scheme, diffusion model choice, fitting method, SNR/averaging/ROI size; tasks vary)",Healthcare/medical,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://doi.org/10.5281/zenodo.5510276,"The paper proposes a task-driven computational experimental design (CED) assessment framework for diffusion MRI that evaluates candidate experimental designs by end-to-end simulated task performance rather than parameter-estimation metrics (bias/variance, AIC/BIC). For a specified quantitative task (primarily clinical classification), tissue “ground-truth” characterisation, and candidate design choices (e.g., b-value protocol, diffusion model, fitting method, SNR/averaging assumptions), the pipeline synthesizes noisy signals (Rician noise), fits model parameters, and computes ROC curves and AUC as the design-performance metric. The framework is validated by comparing in-silico ROC/AUC predictions against two clinical spondyloarthritis datasets and shows qualitative and quantitative agreement, including when simulated cohorts are sub-sampled to match clinical sample sizes. In additional simulated tasks, the authors demonstrate that task-optimal choices of model (e.g., ADC vs. IVIM) and fitting method (segmented vs. bound-constrained NLLS) can be task-specific, illustrating that parameter-estimation-focused CED criteria can select suboptimal designs for clinical decision tasks. The approach is positioned as a general template for task-driven assessment/optimization in quantitative MRI beyond diffusion MRI.","The generative diffusion signal model for IVIM is $\frac{S(b)}{S_0}= f\,e^{-b(D_{\text{fast}}+D_{\text{slow}})}+(1-f)e^{-bD_{\text{slow}}}$, with parameters $f, D_{\text{fast}}, D_{\text{slow}}$. Rician-noise corruption is modeled as $S_{\text{noisy}}(b)=\sqrt{\mathcal{N}(S_{\text{noisefree}}(b),\sigma^2)^2+\mathcal{N}(0,\sigma^2)^2}$ with $\text{SNR}=S_0/\sigma$. An alternative (biased) ADC model used for fitting is $\frac{S(b)}{S_0}=e^{-b\,\text{ADC}}$ (fit via weighted least squares on the log-signal).","Against two clinical SpA datasets (28-patient in-house and a 41-patient external study), simulated ROC curves matched the qualitative shape of clinical ROC curves and reproduced the relative ranking of experimental settings and parameters; simulated AUCs agreed with clinical AUCs when accounting for clinical sample size via repeated sub-sampling of large simulated cohorts (1000× clinical size). In illustrative simulations, model choice was task-dependent: in one task IVIM outperformed ADC (higher AUC), while in another task ADC outperformed the IVIM generative model because lower estimation variance outweighed model bias for classification. Likewise, the preferred IVIM fitting method differed by task (sNLLS better for one task; bcNLLS better for another), reinforcing that task-agnostic parameter-estimation criteria can yield suboptimal experimental designs for classification outcomes.","The authors note that the pipeline requires clear knowledge/characterisation of how tissue diffusion properties relate to pathology (input I2), which may not be available for all clinical use-cases. They also acknowledge reliance on simulation: computational signal models are approximations of the true biophysical process, so accuracy depends on the adequacy of the chosen generative model (e.g., IVIM) to represent measured data.","The framework’s conclusions may be sensitive to misspecification of parameter distributions (e.g., assuming Gaussian variability) and independence assumptions (e.g., ignoring spatial correlation, motion artifacts, scanner drift, and other acquisition confounds that affect real ROC/AUC). The tasks considered are mainly threshold-based classification using mean ROI parameter estimates, which may not reflect modern multivariate classifiers or voxel-wise decision workflows used clinically. The work emphasizes assessment rather than providing a concrete optimization algorithm for selecting b-values or protocols under time/scan constraints, so translating assessment outputs into optimal protocol recommendations may require additional methodology. Software environment details (language, dependencies, reproducibility instructions beyond the repository link) are not described in the provided text, which can hinder practical adoption.","They suggest incorporating the assessment pipeline into an overarching task-driven CED framework that couples proposal of candidate designs with computational optimization, using task-specific performance as the optimization metric. They also state the framework generalizes beyond diffusion MRI to other model-based quantitative MRI applications and can support alternative task metrics beyond AUC (e.g., sensitivity/specificity-based measures), depending on the use-case.","Extending the approach to handle nuisance structure common in MRI (autocorrelation, heteroscedastic noise, motion/outliers, and scanner/site effects) would improve external validity of predicted task performance. Developing explicit, constraint-aware protocol optimization (e.g., optimizing b-value sets under scan-time and hardware constraints) using the pipeline as an objective would make the framework directly prescriptive. Broadening evaluation to multivariate and machine-learning classifiers (and calibration metrics) could align the framework with contemporary clinical decision support. Providing a self-starting/Phase-I style approach to estimate I2 tissue distributions from limited pilot data, with uncertainty propagation into predicted ROC/AUC, would make the method more robust when ground-truth characterisation is imperfect.",2103.08438v2,https://arxiv.org/pdf/2103.08438v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:22:43Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Model discrimination|Other,Other,"Variable/General (examples include D=1, 10, 50, 100 designs in noisy linear regression; N=50, 100, 500 measurements in quantum control example)",Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes SAGABED, a hybrid-gradient framework for Bayesian experimental design (BED) for implicit (likelihood-intractable) models, where optimal designs are found by maximizing a neural mutual-information (MI) lower bound. It replaces the need for pathwise/sampling-path gradients by using guided Evolution Strategies (zero-order gradient estimates) for the design variables while using standard gradient ascent for the neural MI-estimator parameters. To reduce estimator variance, it uses the SMILE objective (a clipped-density-ratio smoothed MI bound) rather than the standard MINE MI bound. Experiments on a noisy linear regression design problem with high-dimensional design vectors (D up to 100) and a quantum control tuning example (many designed measurements) show better scalability than Bayesian optimization and improved stability/variance relative to stochastic-gradient baselines. The method yields designs that produce tighter posterior samples (lower variance) while maintaining high MI lower-bound values, advancing scalable BED for simulator-based/implicit models.","The design criterion is mutual information between parameters and data, optimized over designs: $\xi^*=\arg\max_{\xi\in\Xi} I_{\mathrm{MI}}(\xi)$ with $I_{\mathrm{MI}}(\xi)=\mathbb{E}_{p(\theta)p(y\mid\theta,\xi)}[\log\tfrac{p(y\mid\theta,\xi)}{p(y\mid\xi)}]$. MI is lower-bounded via a neural critic $T_\psi(\theta,y)$ (MINE): $I_{\mathrm{MINE}}(\xi,\psi)=\mathbb{E}_{p(\theta,y\mid\xi)}[T_\psi]-\log\mathbb{E}_{p(\theta)p(y\mid\xi)}[e^{T_\psi}]$, and the paper uses the SMILE variant with clipped ratios: $I_{\mathrm{SMILE}}(\xi,\psi)=\mathbb{E}[T_\psi]-\log\mathbb{E}[\mathrm{clip}(e^{T_\psi},e^{-\tau},e^{\tau})]$. Design gradients are approximated by guided ES / Gaussian smoothing: $\nabla f_\sigma(\xi)=\tfrac{1}{\sigma}\mathbb{E}[f(\xi+\sigma\epsilon)\,\epsilon]\approx\tfrac{1}{M\sigma}\sum_i f(\xi+\sigma\epsilon_i)\epsilon_i$ with a guided covariance, while $\psi$ is updated by backpropagation.","Across noisy linear regression design problems, the proposed SMILE+Guided-ES (SAGABED) achieves higher MI lower bounds than MINE+BO as dimensionality grows (D=10, 50, 100), and shows more stable training than MINE+SGA. Posterior estimation using the learned ratio shows lower uncertainty: for example at D=100, SMILE+GES yields $\hat\theta_1=1.02\pm0.04$ and $\hat\theta_2=3.98\pm0.03$ (true $[1,4]$), compared with MINE+BO $1.35\pm0.11$ and $4.79\pm0.26$; at D=50, SMILE+GES gives $1.11\pm0.13$ and $4.25\pm0.19$. In the quantum control tuning example, SAGABED produces better measurement designs and markedly smaller posterior variance than Bayesian optimization for high numbers of designed measurements (e.g., N=50/100/500), as shown in the reported posterior-evolution plots.",None stated.,"The MI objective is optimized via variational lower bounds (MINE/SMILE), so the resulting design may depend strongly on critic architecture, training dynamics, and the clipping hyperparameter $\tau$ (bias–variance trade-off), with no guarantee of maximizing true MI. The empirical evaluation is limited to two examples and does not systematically test robustness to model misspecification, correlated/heteroskedastic noise, or constraints/costs on designs. The guided-ES gradient approximation can still be sample-inefficient at very high design dimensions and introduces additional hyperparameters (smoothing radius, subspace size k, mixing α) whose sensitivity is not fully characterized. No implementation details for reproducibility (code, exact settings, compute budget) are provided in the paper text excerpt.",None stated.,"Provide principled guidance or adaptive schemes for selecting $\tau$ (SMILE clipping), ES smoothing radius $\sigma$, and guided-ES subspace/mixing parameters, including sensitivity analyses. Extend the framework to constrained and cost-aware experimental design (e.g., discrete designs, batch/sequential constraints) and to settings with dependent observations (time series) common in physical experiments. Benchmark against additional likelihood-free/BED methods (e.g., SBI-based designs, alternative MI/entropy estimators) on standardized simulators and more real experimental datasets. Release an open-source implementation and add diagnostics for convergence and critic overfitting to improve practitioner adoption.",2103.08594v1,https://arxiv.org/pdf/2103.08594v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:23:22Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (experimental conditions such as measurement angle, measurement time, and contrast choice; model parameter dimension depends on number of layers/parameters)",Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/James-Durant/fisher-information,"The paper develops a Fisher information (FI) and Cramér–Rao bound-based framework to quantify maximum information gain and to optimize experimental design for neutron reflectometry. It derives an analytic FI expression for Poisson counting statistics, enabling sub-second predictions of parameter uncertainties and covariances for a specified reflectometry model, and shows uncertainty scaling as $\epsilon \propto 1/\sqrt{\tau}$ with measurement time. The method is positioned for real-time design decisions (e.g., choosing measurement angles, allocating time, selecting isotopic contrast) and for potential early stopping when additional counting would yield diminishing returns. The authors validate the approach using an instrument-aware experiment simulation framework (incident flux profiles, angle-dependent intensity scaling, Poisson noise) and benchmark FI uncertainty computation against Bayesian MCMC (refnx/emcee) and nested sampling (dynesty), demonstrating orders-of-magnitude faster uncertainty estimation. A soft-matter case study (DMPC bilayer with H2O/D2O contrasts) illustrates using FI to map information content versus contrast SLD and highlights the role of multi-contrast measurements in reducing parameter correlations beyond variance reduction alone.","For a reflectometry model with parameters $\xi$ and reflectivity points $r_i(\xi)$, the Fisher information matrix is $g_{\xi}=J^T M J$, where $J_{i,j}=\partial r_i/\partial \xi_j$ and $M$ is diagonal with entries $M_{i,i}=s_i/r_i$ (incident counts divided by model reflectance). Parameter uncertainty lower bounds follow from the Cramér–Rao relation, with per-parameter uncertainty approximated as $\epsilon_i=\sqrt{1/g_{\xi,ii}}$. The simulation model generates reflected counts via $N_i\sim\text{Poisson}((r_i+\alpha)\mu_i\tau)$ and converts to reflectivity by dividing by incident counts $s_i=\mu_i\tau$, implying $\epsilon\propto 1/\sqrt{\tau}$.","Benchmarking on simulated multilayer samples (1–6 layers; 3–18 parameters) shows FI uncertainty computation takes roughly 0.015–0.076 s, compared with MCMC at about 198–372 s and nested sampling at about 53–3862 s under the paper’s settings. Simulated reflectivity generated by the proposed experiment simulator closely matches measured OFFSPEC time-sliced data (Hotelling’s $t^2$ test after Anscombe transform: $p=0.874$, $t^2=0.159$), supporting realism of the Poisson/flux-based simulation. Log–log uncertainty vs. time exhibits the expected slope near $-1/2$, confirming the $1/\sqrt{\tau}$ dependence. In the DMPC bilayer contrast study, FI varies non-monotonically with contrast SLD (including near-zero information for contrast-matched interfaces) and indicates highest information often at D2O, while multi-contrast data improves identifiability by reducing parameter correlations (more Gaussian/less degenerate posteriors in corner plots).","The authors note the framework is frequentist and conditions on assumed/estimated parameter values; uncertainty bounds are valid if the chosen model and parameter estimates are correct, which is non-trivial in reflectometry where the globally correct model may be uncertain. They also state that FI quantifies maximum possible extractable information, which can differ from what Bayesian sampling actually extracts when parameters are strongly correlated or when the fitting estimator is biased. They report that their chosen fitter (differential evolution) can exhibit bias in some scenarios, affecting agreement between FI confidence ellipses and sampled posteriors.","The method relies on Poisson counting assumptions and (implicitly) independence across bins; real instruments may exhibit additional systematic errors (background drift, calibration uncertainty, resolution mis-specification, correlated errors) that would reduce realized information relative to FI. FI is computed via finite-difference gradients (0.5% perturbations), which may be sensitive to step size, parameter scaling, and non-smooth model behavior, potentially impacting stability for highly correlated or weakly identifiable parameters. The design guidance is primarily variance/CRB-driven; without an explicit scalar design criterion (e.g., D-optimality) or formal multi-objective treatment (variance–covariance trade-offs, operational costs like angle-change overhead), “optimal” decisions may be ambiguous in practice.","They propose extending the framework to include additional experimental factors such as time to change sample or angle. They also suggest quantifying and incorporating fitting biases and inter-parameter correlations into the FI-based analysis to better align with established Bayesian methods, and providing a metric (e.g., based on Pearson correlation coefficients from the FI matrix) indicating when FI predictions are expected to match sampling-based uncertainties. Finally, they plan to apply the approach to more complex real-world systems (e.g., magnetic structures) and to broader scattering techniques governed by counting statistics.","A natural extension is to formalize experimental design selection with established optimality criteria (e.g., D-/A-/I-optimality) over controllable settings (angles, time allocations, contrasts) and incorporate practical constraints (discrete angle sets, switching overhead, total beamtime budgets). Robust/self-starting variants could account for unknown nuisance parameters and model uncertainty by averaging FI over plausible parameter priors (Bayesian/expected FI) or using minimax robust design. Additional validation on diverse real datasets (including cases with detector dead time, background mis-modeling, and resolution uncertainty) and release of a reproducible workflow/package interface (e.g., refnx plugin) would strengthen adoption.",2103.08973v3,https://arxiv.org/pdf/2103.08973v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:24:14Z
TRUE,Response surface|Definitive screening|Screening|Other,Prediction|Screening|Optimization,Not applicable,"Variable/General (examples studied: DSD K=4,8; BBD K=3,5; case study K=5 DSD and K=5 CCD)",Manufacturing (general)|Pharmaceutical,Simulation study|Case study (real dataset)|Other,TRUE,Other,Not provided,https://arxiv.org/abs/2103.09303,"The paper proposes Self-Validated Ensemble Modeling (SVEM), a bagging-style framework for building predictive models from small-run designed experiments without holding out runs for validation. SVEM creates paired, anti-correlated fractional weights for each observation (training vs. “self-validation”) using a fractional random-weight bootstrap so that every run contributes to both fitting and validation in each bootstrap iteration. The method is evaluated on common small-sample three-level DOE structures—Definitive Screening Designs (DSDs) and Box–Behnken Designs (BBDs)—and compared to one-shot model selection methods such as forward/pruned forward selection, Lasso, and the Dantzig selector (plus fit definitive screening for DSDs). In Monte Carlo studies across multiple sparsity and noise settings, SVEM variants generally reduce prediction error (RMSPE) versus their non-SVEM counterparts, especially when the true model is complex/low-sparsity and designs are small or supersaturated. A plasmid manufacturing case study (DSD-built model tested on an independent CCD experiment) shows SVEM achieving the lowest test-set RMSPE among the compared approaches, supporting its practical value for predictive DOE modeling.","SVEM targets linear DOE models such as the full quadratic form $Y=\beta_0+\sum_{i=1}^K \beta_i x_i+\sum_{i=1}^K \beta_{ii}x_i^2+\sum_{i<j}\beta_{ij}x_ix_j+\varepsilon$. For each bootstrap iteration, it generates paired fractional weights using $u_i\sim U(0,1)$, then $w_{T,i}=F^{-1}(u_i)$ and $w_{V,i}=F^{-1}(1-u_i)$ where $F$ is the CDF of an Exp(1) distribution, producing anti-correlated training and self-validation weights. A model is fit on the training weights and chosen to minimize weighted self-validation error (e.g., $\sum_i w_{V,i}(y_i-\hat f(X_i))^2$); the final SVEM model averages coefficients across $nBoot$ selected models (unselected terms get coefficient 0 in that iteration).","Simulation studies (1,000 replicates per scenario) on DSDs (K=4,8) and BBDs (K=3,5) show SVEM-based methods typically lower (log)RMSPE than the corresponding one-shot selection procedures, with the largest gains when the true model has low sparsity (many active effects) and in smaller/supersaturated settings (e.g., DSD K=8 where $P=45$ and $N=21$). SVEM often yields larger final models (many terms selected at least once), aligning with improved prediction when the underlying relationship is complex. A tuning study found little improvement beyond $nBoot\approx 200$ (recommended 200; upper limit 500). In the plasmid manufacturing case study, SVEM+forward selection achieved the best independent test performance on CCD data (RMSPE 53.5, $R^2$ 0.55), outperforming non-SVEM alternatives (all reported CCD RMSPE > 60 in Table 3).","The authors note that SVEM’s improved predictive accuracy comes at increased computational cost, since it repeats fitting/tuning for many bootstrap iterations (e.g., 200), and this can be demanding when the base learner is computationally heavy (e.g., best subsets, SVMs). They also state it is not yet known how well SVEM works under randomization restrictions such as blocking/whole-plot factors, or whether it needs generalization to accommodate these structures.","The method is demonstrated mainly for linear-model-based response surfaces (full quadratic candidate set) and its performance for other DOE objectives (e.g., effect estimation/interpretation with controlled Type I error) is not established; large ensemble-selected models may be harder to interpret for scientific insight. Practical guidance on selecting/tuning base learners and hyperparameters (beyond $nBoot$) and on diagnosing convergence/stability of the ensemble is limited. The paper relies on JMP Pro implementation; without shared code, reproducibility and independent benchmarking across software ecosystems are constrained.","The authors propose studying how to quantify uncertainty around SVEM predictions. They also suggest investigating which design construction methods are best suited for use with SVEM, and extending SVEM to broader ML model classes (e.g., neural nets, SVMs, tree-based methods) and to non-normal response distributions given SVEM’s minimal distributional assumptions. They further note the need to assess or generalize SVEM for experiments with randomization restrictions (blocking or whole-plot factors).","Develop self-starting or computationally efficient variants (e.g., warm starts, early stopping, subsampling) to reduce runtime for expensive base learners. Provide theoretical results on prediction error/consistency or connections to cross-validation and stability selection under supersaturated DOE settings. Add guidance and tooling for interpretability (e.g., variable importance, effect plots with uncertainty) and release open-source implementations (R/Python) plus benchmark datasets to enable broader adoption and fair comparisons.",2103.09303v4,https://arxiv.org/pdf/2103.09303v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:24:50Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Model discrimination|Parameter estimation|Cost reduction|Other,Other,"Variable/General (nodes/variables in a causal network; simulations include 8, 10, and 11 nodes; method noted as limited to <25 nodes)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Case study (real dataset),TRUE,MATLAB,Public repository (GitHub/GitLab),https://github.com/mzemplenyi/OED-graphical-models,"The paper proposes a computationally tractable Bayesian optimal experimental design (OED) method for sequentially choosing interventions to learn causal structure in directed acyclic graphs (causal networks). The design criterion selects the next intervention to maximize the entropy of a graph-space partition induced by that intervention, motivated by an asymptotic result showing that (with many replicates) expected posterior entropy reduction can be approximated using only summaries of the current posterior rather than repeated posterior updates on hypothetical future datasets. The method is specialized to discrete (categorical) Bayesian network models with Dirichlet (BDeu) priors, and uses MCMC with dynamic-programming-based proposals to sample from the posterior over graphs. The authors evaluate multiple partition schemes (e.g., interventional Markov equivalence class, child/parent/descendant sets, and pairwise-child edge indicators) via simulations on networks of varying topology and compare against existing active-learning baselines and random intervention selection. They also apply the approach to protein-signaling network data (Sachs et al.) and to simulated data based on that benchmark network, demonstrating strong performance in simulations and highlighting challenges under likely model misspecification in real data.","The experimental design score for a candidate intervention/experiment $e$ is the posterior entropy of the induced partition: $\hat h_e = -\sum_y \hat p_e(y)\log \hat p_e(y)$ with $\hat p_e(y)=\frac1T\sum_{t=1}^T \mathbf{1}\{f_e(G_t)=y\}$ computed from posterior samples $G_{1:T}\sim p(G\mid D)$. This is motivated by the asymptotic approximation $H(\theta\mid X_{1:N}^e)\approx H(\theta)-H(f_e(\theta))$ as $N\to\infty$, so minimizing expected posterior entropy is approximated by maximizing $H(f_e(\theta))$ under the current posterior. For the categorical Bayesian network model with BDeu prior, the marginal likelihood used for posterior sampling includes the closed-form Dirichlet-multinomial term (Eq. 11).","Across simulation studies (e.g., 50 simulations per setting), entropy-based sequential intervention selection consistently outperformed random intervention selection in mean Hamming distance and true positive rate as experiments accumulated. On the 8-node Asia network (with settings $n_{obs}=300$, $n_{intv}=300$), the proposed method using the interventional Markov equivalence class (MEC) partition and a DP-based alternative both outperformed bninfo and random selection after early experiments. In topology tests, performance differences between methods using similar entropy criteria were attributed to differences in priors and posterior computation (e.g., DP’s implicit modular prior harming performance on chain structures). On real Sachs protein-signaling data (limited to five available interventions), all methods struggled, while on simulated data generated from the Sachs benchmark network the MEC approach achieved near-perfect recovery after several optimally chosen interventions.","The authors state the method is currently limited to networks with fewer than 25 nodes due to the super-exponential growth of DAGs and computational limits of the DP-based MCMC proposals. They also acknowledge difficulties on the real Sachs dataset, attributing this to possible model misspecification (true biology may involve cycles; interventions may not match the assumed edge-breaking mechanism), limited available interventions (only 5 of 11 proteins), and dependence on discretization choices.","The asymptotic justification relies on the “infinitely many replicates” proxy; performance may degrade when each intervention has small sample size or low signal-to-noise, and the approximation quality is not formally characterized for finite N. The approach assumes i.i.d. samples and a correctly specified discrete CPD family with conjugate priors; robustness to measurement error, latent confounding, selection bias, or dependent/temporal data is not addressed. Practical scalability is constrained not only by node count but also by the cost of repeated long MCMC runs (e.g., hundreds of thousands of samples) and the need to evaluate partition functions $f_e(G)$ over many posterior draws for each candidate intervention.","They propose scaling the method to larger networks beyond ~25 nodes, motivated by the computational bottleneck from DP-based MCMC proposals. They also suggest developing approaches that relax the acyclicity assumption and improve robustness to model misspecification, and note the need for more benchmark datasets with mixed observational and interventional data for evaluating OED/active learning methods.","Developing a finite-sample (replicate-limited) version of the design criterion—e.g., explicitly modeling expected information gain for a specified N per intervention—could improve reliability when experiments are small. Extending the framework to continuous data/heterogeneous node types, time-series/feedback (cyclic) models, and settings with hidden variables or imperfect interventions would broaden applicability in biology. Providing an open-source implementation beyond MATLAB (e.g., Python/R) and adding diagnostics for MCMC mixing/uncertainty in the design scores would improve accessibility and practical deployment.",2103.15229v1,https://arxiv.org/pdf/2103.15229v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:25:26Z
TRUE,Optimal design|Other,Parameter estimation|Cost reduction|Other,A-optimal|D-optimal|Other,Variable/General (matrix $X\in\mathbb{R}^{n\times p}$; examples include $p=4$ or $5$ with $n=20$ or $30$),Manufacturing (general)|Food/agriculture|Energy/utilities|Pharmaceutical|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a framework for model-based Design of Experiments when the candidate experiment-feature matrix contains missing (imputable/designable) entries, which makes classic optimal design methods (that assume a fully known $X$) suboptimal. It formulates a coupled Simultaneous Selection–Imputation Optimization (SSIO) problem that selects $r$ experiments (rows) and imputes missing feature values to minimize an A-optimality criterion, $\mathrm{trace}((X(m)^\top\Lambda(s)X(m))^{-1})$. The main method uses a maximum-entropy principle with deterministic annealing (homotopy from a convex entropy-dominated objective to the nonconvex A-optimal objective) and derives iterative updates for selection weights and imputed values, with discussion of incorporating additional constraints (e.g., budgets). Simulation experiments on synthetic incomplete matrices show substantially lower A-optimal costs than sequential baselines (mean imputation followed by Fedorov exchange or random selection), and also outperform direct constrained optimization that tends to get stuck in poorer local minima. The authors also outline how the approach can be adapted to other optimality criteria such as D-optimality.","The SSIO objective is $\min_{s\in S,\,m\in M}\;\mathrm{trace}((X(m)^\top\Lambda(s)X(m))^{-1})$, where $s$ selects $r$ rows and $m$ are missing/designable entries bounded in intervals. A relaxed MEP formulation replaces the discrete selection with probabilities and (under an independence assumption) uses row weights $q_i\in[0,1]$ to minimize a free-energy Lagrangian: $L=\mathrm{trace}(\sum_i q_i x_i x_i^\top)^{-1}-a_0 + T\sum_i[q_i\log q_i+(1-q_i)\log(1-q_i)] + \mu(\sum_i q_i-r)$. The resulting implicit fixed-point/gradient-like updates include $q_i=\big(1+\exp\{-(x_i^\top R^{-2}x_i-\mu)/T\}\big)^{-1}$ with $R=\sum_i q_i x_i x_i^\top$, plus corresponding updates for $\mu$ and for missing entries via first-order conditions.","Across six simulated scenarios (e.g., $X\in\mathbb{R}^{20\times 4}$ or $20\times 5$ or $30\times 5$ with 10–24% missing entries and selecting $r\in\{6,11,12\}$ rows), Algorithm 1 consistently yields lower A-optimal costs than sequential baselines. For example E1, its cost is reported as 0.52× that of mean-imputation + Fedorov exchange and 0.35× that of mean-imputation + uniform sampling; for E4 it is 0.42× vs mean+Fedorov and 0.20× vs mean+random. Against direct interior-point optimization of the coupled objective on E1, Algorithm 1 achieves a cost about 0.57× as large (illustrating annealing helps avoid poor local minima). The abstract also reports over 60% improvement in cost value compared with sequential benchmark approaches.",None stated.,"The method relies on an independence factorization of the selection distribution ($p(s)=\prod_i p_i(s_i)$), which is a strong approximation for fixed-size subset selection and may affect optimality/feasibility (the exact combinatorial constraint is only enforced via a Lagrange term). Results are demonstrated only on synthetic simulation matrices with randomly missing entries and simple mean-imputation baselines; comparisons to stronger missing-data models (e.g., low-rank matrix completion, EM/multiple imputation) or modern optimal design solvers are not shown. Practical guidance on choosing the annealing schedule, stopping criteria, and robustness to non-iid noise/model misspecification is limited, and no implementation details or runtime scaling results are provided.","The authors state they are working toward (a) a proof of convergence for the constrained-augmented iterations (e.g., when adding resource/budget constraints) and (b) extending the framework to incorporate inequality constraints in the SSIO formulation.","Validate SSIO on real incomplete-design datasets (e.g., sensor networks or manufacturing experiments) and under structured missingness (MAR/MNAR) rather than purely random missing entries. Provide systematic annealing/step-size selection rules, runtime/scalability analysis for large $n,p$, and ablations on the independence assumption. Extend to unknown/noisy model settings (Phase I estimation of error variance, correlated errors, or Bayesian/robust optimal design) and release reference software for reproducibility and broader adoption.",2103.15628v1,https://arxiv.org/pdf/2103.15628v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:26:00Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Other,Not applicable,Variable/General (design parameter is experiment scaling factor ε; input sequence length T and order n+1 persistency-of-excitation requirement),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper addresses how to design (data-collection) experiments that enable data-driven state-feedback control synthesis for unknown nonlinear discrete-time systems via LMIs/semidefinite programming. Prior results require two data conditions: (i) full row-rank of the stacked data matrix [U0,T; X0,T] and X1,T, and (ii) a quadratic inequality bounding the nonlinear remainder term D0,T relative to X1,T. The authors show by counterexample that, unlike the linear case, persistently exciting inputs alone do not guarantee these conditions for nonlinear systems even arbitrarily close to equilibrium. They propose an experiment-design method based on scaling both the initial condition and input sequence by a factor ε, proving that for sufficiently small ε the required rank and inequality conditions hold, the SDP is feasible, and the resulting controller stabilizes the nonlinear system in the first approximation. A simulation on an inverted pendulum illustrates that the SDP parameter α and the computed gain K converge rapidly as ε decreases, supporting the practical use of scaling to obtain informative data for controller learning.","The nonlinear system is written as x(k+1)=Ax(k)+Bu(k)+d(k), where A=∂f/∂x|0 and B=∂f/∂u|0 and d(k) collects higher-order terms. The data conditions are: Assumption 1 requires full row rank of [U0,T; X0,T] and X1,T; Assumption 2 requires D0,T D0,T^T ⪯ γ X1,T X1,T^T for some γ>0. Controller synthesis solves the SDP (7) maximizing α subject to LMIs involving X0,T, X1,T, Q (and IT) and returns K = U0,T Q (X0,T Q)^{-1}, with a sufficient stability condition γ < α^2/(4+2α). The experiment-design mechanism scales an existing persistently exciting experiment as (εx0, εu[0,T−1]); the resulting data satisfy [Ũ0,T; X̃0,T] = ε[U0,T^l; X0,T^l] + [0; Ξ̃] and X̃1,T = εX1,T^l + Ψ̃, where Ξ̃,Ψ̃ vanish faster than ε, preserving rank for small ε.","A constructive counterexample (x(k+1)=x(k)^2+u(k)) shows [U0,T; X0,T] can lose rank (rank 1) despite a persistently exciting input of order n+1 and arbitrarily small excitation amplitude. Theorem 4 proves that for any persistently exciting input sequence of order n+1 (assuming controllability of the linearization), there exists ε̄ such that for all ε∈(0,ε̄) the scaled experiment (εx0, εu[0,T−1]) satisfies Assumptions 1 and 2 (for any prescribed γ>0). Theorem 5 further shows that for sufficiently small ε these scaled experiments also make the SDP feasible and the solution yields a stabilizing controller. In the inverted pendulum simulation, stability is achieved for ε=1,0.5,0.1,0.01, with reported convergence metrics ||K−K̄|| decreasing from 8.2e−3 (ε=1) to 1e−5 (ε=0.01), and |α−ᾱ| from 2e−7 to 3e−10, while the inequality condition γ<α^2/(4+2α) is marked as fulfilled in all cases shown.","The authors note that practical aspects are neglected: scaling experiments may make collected data more sensitive to noise. Although their earlier framework allows robust analysis with bounded noise, the noise sensitivity induced by scaling is not studied in this paper and is identified as needing attention.","The experiment-design method hinges on operating sufficiently close to an equilibrium and on controllability of the linearized system; in many applications actuator limits, quantization, and measurement noise may prevent using very small ε while keeping matrices well-conditioned (rank tests can become numerically fragile). The approach is largely local (stabilization “in the first approximation”) and does not quantify the region of attraction or performance away from equilibrium, so practical closed-loop guarantees may be limited. The method also presumes access to state measurements (state-feedback gain K from state data matrices), while many nonlinear systems have only output measurements; extensions to output-feedback/nonlinear observer settings are not covered here.",They state they are currently investigating the impact of these results on the nonlocal design of stabilizers for nonlinear systems. They also explicitly point out that the effect of noise under experiment scaling has not been studied in the current paper and deserves attention.,"Developing a noise-aware (or optimal) choice of ε that trades off informativeness/conditioning against noise amplification would make the procedure more actionable. Providing practical stopping/verification rules (e.g., statistical tests or confidence bounds for Assumptions 1–2 and for stability when γ is unknown) would help implementers. Extending the experiment-design and synthesis approach to output-only data, constrained inputs, and systems with process disturbances/autocorrelation would broaden applicability, along with software that automates persistent excitation design and ε-scaling selection.",2103.16509v1,https://arxiv.org/pdf/2103.16509v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:26:38Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Prediction|Parameter estimation|Cost reduction|Other,A-optimal|Bayesian A-optimal,"Variable/General (design parameter p defines projection geometry; e.g., 2D p∈R^2: angle + lateral offset; 3D uses central angles (θ,φ) plus detector subset/quadrant, with fixed d, δ, M in examples)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a greedy sequential Bayesian optimal experimental design method for linear inverse problems with an edge-promoting total variation (TV) prior, demonstrated in X-ray tomography. It interprets lagged diffusivity TV reconstruction as iteratively constructing Gaussian approximations to the non-Gaussian TV prior/posterior near the MAP estimate, yielding an approximate Gaussian posterior covariance that encodes edge information. The next measurement (projection geometry) is selected by Bayesian A-optimality, i.e., minimizing the trace of the updated posterior covariance after adding a candidate projection. The approach is tested in 2D parallel-beam and 3D cone-beam simulated tomography, comparing adaptive optimized narrow/partial-aperture projections against equiangular or “equally spaced” full-width reference projections at matched radiation dose. Simulations show faster error reduction at low-to-moderate dose for adaptive designs on piecewise-constant phantoms (edge-rich targets), with diminishing advantage as more projections are collected and with occasional exploration/exploitation issues.","Measurement model: stacked linear-Gaussian observations $Y_j = R(p_j)U + N_j$ with $N_j\sim\mathcal N(0,\Gamma^{(j)}_{\text{noise}})$, leading to posterior $\pi(u\mid y_k)\propto \exp\{-\tfrac12(y_k-R_ku)^T\Gamma_{\text{noise}}^{-1}(y_k-R_ku) - \gamma\Phi(u)\}$. TV-type prior uses $\Phi(u)=\int_D\varphi(|\nabla u|)\,dx$ with smoothed TV $\varphi(t)=\sqrt{t^2+T^2}$ and lagged-diffusivity quadratic surrogate inducing $\Gamma^{(j)}_{k-1}=H(u^{(j-1)}_{k-1})^{-1}$; Gaussian posterior mean and covariance are given by (3.12)–(3.13). A-optimal design chooses next projection via $p_{k+1}=\arg\min_p\operatorname{tr}(A\,\Gamma^{(k+1)}_{\text{post}}(p)\,A^T)$, where $\Gamma^{(k+1)}_{\text{post}}(p)$ is the Gaussian covariance update (4.1).","Across multiple simulated 2D tests (simple shapes, random ellipse phantoms, Shepp–Logan) and a 3D geometric-shapes test, sequential A-optimal adaptive narrow/partial-aperture projections typically reduce relative $L^2(D)$ reconstruction error faster than equiangular/“equally spaced” full-aperture reference projections at equivalent radiation dose, especially early in the sequence. In 2D random-target experiments (100 targets), mean relative $L^2$ error curves show optimized quarter- and half-width beams outperforming full-width equiangular references for low-to-moderate numbers of projections, with overlapping variability indicating target dependence. In the Shepp–Logan experiment (100 noise realizations), the TV-edge-promoting adaptive design ultimately outperforms sequential designs optimized under a fixed Gaussian prior after enough projections, while differences vs equiangular full-width references diminish at higher dose. In 3D, optimized quarter-aperture designs eventually achieve lower relative $L^2$ error than the reference sequence after an initial region where references can be better, and the gap narrows as the number of projections increases.","The authors note that the greedy sequentially chosen projection geometries may be locally optimal but not globally optimal after many rounds; this can leave parts of the target insufficiently investigated and can cause occasional jumps in reconstruction error when previously unexplored objects are detected. They also state the method is only tested for a linear inverse problem with a TV prior (though it may generalize to other Gibbs priors), and that the optimization step relies on an exhaustive search routine, which is computationally demanding. They emphasize that reconstruction quality and optimality can be target-dependent and that a narrower beam can increase computational cost due to a larger search space.","Performance is demonstrated only on simulated data with idealized linearized log-intensity modeling and independent Gaussian noise; real CT data issues (model mismatch, beam hardening, scatter, calibration errors, Poisson counting noise) are not assessed. The A-optimal criterion uses trace of covariance (often with $A=I$), which may not align with task-specific imaging objectives (e.g., edge localization accuracy, ROI detectability) and may be sensitive to discretization and prior tuning ($\gamma,T,\tau$). The design search is effectively discrete/exhaustive over a predefined candidate set; scalability to richer continuous geometry spaces or clinical constraints is unclear, and no released implementation limits reproducibility. The Gaussian approximation via lagged diffusivity is local (near MAP) and may be unreliable for multimodal or strongly non-Gaussian posteriors, potentially affecting design decisions.","They propose studying how to balance exploration of new areas vs refinement of already detected edges in the sequential feedback design, motivated by observed behavior where the algorithm oversamples detected features. They suggest extending the approach to nonlinear inverse problems and integrating it with more efficient optimization procedures than exhaustive search. They also identify as future work investigating whether the sequential designs produced by their approximation approach approximate (asymptotically or otherwise) those obtained under the exact TV prior.","Develop task-based utilities (e.g., edge localization metrics, detectability indices, ROI-weighted criteria with adaptive $A$) and compare against other Bayesian OED criteria (e.g., D-optimal/EIG) under the same TV prior approximation. Extend the method to more realistic noise and physics models (Poisson + electronic noise, polychromatic spectra, scatter) and to correlated/noisy-projection settings, including robustness analysis under model mismatch. Replace exhaustive search with gradient-based or surrogate-assisted optimization over continuous geometry parameters, and provide open-source software for reproducible benchmarking. Evaluate on real CT datasets and assess practical constraints such as motion, dose-rate limits, and hardware geometry restrictions, potentially adding constraints/regularization directly in the design optimization.",2104.00301v1,https://arxiv.org/pdf/2104.00301v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:27:21Z
TRUE,Computer experiment|Optimal design|Screening|Supersaturated|Other,Screening|Prediction|Parameter estimation|Other,Space-filling|Minimax/Maximin|Other,"Variable/General (examples include p=48, 96, 192 predictors; p can exceed n)",Theoretical/simulation only,Simulation study,TRUE,R|Other,Not provided,NA,"The paper proposes an experimental design approach for collecting data (constructing the regression matrix X) for regularized linear models such as the Lasso, with the explicit goal of reducing variable-selection errors. The authors advocate using nearly orthogonal Latin hypercube designs (NOLHDs), combining space-filling stratification with low columnwise correlations, to improve sparse variable selection relative to random Latin hypercubes, i.i.d. sampling, and two-level (including supersaturated) designs. They introduce a new near-orthogonality assessment metric, the proportion correlation vector δ_t(X), and give two systematic construction methods: (i) an orthogonal-array-based construction (after Lin, Mukerjee, and Tang, 2009) and (ii) a generalized Kronecker-product construction improving projection properties over earlier work. Performance is demonstrated via Monte Carlo examples comparing the Lasso’s number of false selections γ across competing designs, showing NOLHDs yield markedly smaller γ, with advantages increasing as p/n grows. Computation of the Lasso uses 5-fold cross-validation and the R package lars; some designs are generated using the Gendex DOE software, but no implementation for the proposed design method is shared in the paper (an R package is stated to be under development).","The modeling/selection target is the sparse linear model $y=x^T\beta+\varepsilon$ with Lasso estimator $\hat\beta=\arg\min_\beta\{(y-X\beta)^T(y-X\beta)+\lambda\|\beta\|_1\}$. Variable-selection accuracy is measured by the false selection count $\gamma=\#\{j\in A(\hat\beta)\setminus A(\beta)\}+\#\{j\in A(\beta)\setminus A(\hat\beta)\}$, and design orthogonality is summarized by the column correlation matrix $\rho$ with entries $\rho_{ij}$ and by the proposed proportion-correlation vector $\delta_t(X)$ where $\delta_{t_k}(X)=[p(p-1)]^{-1}\sum_i\sum_{j\neq i}\mathbf{1}(|\rho_{ij}|\le t_k)$. A random Latin hypercube is generated via $z_{ij}=(d_{ij}-u_{ij})/n$ and rescaled to $[a,b]^p$ by $z_{ij}\leftarrow (b-a)z_{ij}+a$; a generalized Kronecker-based construction forms blocks $M$ via $M_{\text{block}}=bA+r\,dC$ as in equation (10).","Across simulation studies (50 replications each) with Lasso tuning by 5-fold CV, NOLHD-based designs consistently reduce the median false selection count $\gamma$ versus two-level (including supersaturated) designs, random Latin hypercubes, and i.i.d. sampling. Example 4 (n=50, p=48) shows median $\gamma$ of 13 (NOLHD) vs 18 (two-level), 18 (RLHD), and 20 (i.i.d.). Example 5 (n=49, p=96) shows median $\gamma$ of 17.5 (NOLHD) vs 27 (supersaturated two-level), 25 (RLHD), and 27 (i.i.d.). Example 6 (n=64, p=192) shows median $\gamma$ of 27 (NOLHD) vs 42.5 (supersaturated two-level), 43 (RLHD), and 41 (i.i.d.), indicating the NOLHD advantage grows with higher p/n.","The paper notes that other (nearly) orthogonal Latin hypercube construction methods exist but are not considered due to run-size constraints. It also states that developing a model-based optimal design criterion specifically for the Lasso is difficult because the Lasso solution does not admit an analytic form, making it challenging to define a sensible, computationally efficient design criterion for that problem.","The empirical evaluation focuses on Gaussian independent errors and a centered linear model; robustness to non-normality, heteroskedasticity, autocorrelation, and model misspecification is not investigated. Comparisons are limited to a few competing design classes and a single Lasso implementation/tuning scheme (5-fold CV with lars), without sensitivity analysis over alternative penalties (elastic net), selection metrics (FDR/TPR), or alternative tuning/selection rules. Practical constraints common in physical experiments (hard-to-change factors, factor ranges/constraints, discrete levels) are not addressed, even though the proposed designs assume continuous controllable factors and Latin-hypercube level structure.",The authors suggest studying the proposed NOLHD-based design strategy for variants of the Lasso in subsequent work. They also state that an R package implementing the proposed method is under development and will be released in the future.,"Extend the approach to settings with unknown/estimated noise variance and Phase I/II considerations, including robust/self-starting variants when distributional assumptions fail. Develop and benchmark design strategies tailored to other regularizers (elastic net, group lasso) and to generalized linear models, along with theory linking NOLHD correlation/projection properties to selection consistency metrics. Provide open-source software and reproducible benchmarks, and study constrained/industrial variants (split-plot, discrete levels, bounded/irregular regions) where strict Latin-hypercube structures may be infeasible.",2104.01673v1,https://arxiv.org/pdf/2104.01673v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:27:43Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Other,Other,Variable/General (examples include 1D design domain; 10-dimensional design vector for toy models; measurement-time vectors up to 10+ measurements in epidemiology example),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/stevenkleinegesse/GradBED,"The paper proposes a gradient-based Bayesian experimental design (BED) framework for implicit (likelihood-intractable) models, using neural-network–parameterized lower bounds on mutual information as the design utility. Optimal designs are obtained by jointly training the critic network and optimizing designs via stochastic gradient ascent, using pathwise (reparameterization) gradients through the simulator/sampling path, with a special treatment when an observation model is analytically tractable. The framework supports multiple MI lower bounds (notably NWJ, InfoNCE, and a JSD-based objective) and targets multiple BED aims by changing the variable of interest: parameter estimation, model discrimination, joint MD/PE, and improving future predictions. Empirically, it compares bounds on intractable toy models and demonstrates an epidemiology example involving SDE latent dynamics with discrete (Poisson) observations, showing the method can learn informative measurement-time designs and provide amortized posterior approximations as a by-product. Overall, it advances BED for simulation-based/likelihood-free settings by enabling scalable, gradient-driven optimization of information-gain utilities without explicit likelihoods or variational distributions.","The BED problem is posed as maximizing a mutual-information lower bound over both design and critic parameters: $d^* = \arg\max_{d\in\mathcal D}\max_{\psi} L(\psi,d)$. Mutual information is $I(v;y\mid d)=\iint p(y\mid v,d)p(v)\log\frac{p(v\mid y,d)}{p(v)}\,dy\,dv$, with task-specific $v$ (e.g., $v=\theta$ for PE, $v=m$ for MD, $v=y_T\mid d_T$ for prediction). Prominent bounds include NWJ: $L_{\text{NWJ}}=\mathbb E_{p(v,y\mid d)}[T_\psi]-e^{-1}\mathbb E_{p(v)p(y\mid d)}[e^{T_\psi}]$, InfoNCE (contrastive): $L_{\text{NCE}}=\mathbb E\big[\frac1K\sum_i \log \frac{e^{T_\psi(v_i,y_i)}}{\frac1K\sum_j e^{T_\psi(v_j,y_i)}}\big]$, and JSD-based: $L_{\text{JSD}}=\mathbb E_{p(v,y\mid d)}[-\mathrm{sp}(-T_\psi)]-\mathbb E_{p(v)p(y\mid d)}[\mathrm{sp}(T_\psi)]$ with $\mathrm{sp}(z)=\log(1+e^z)$.","Across toy-model experiments, the framework finds sensible boundary/clustered optimal designs and achieves MI estimates close to reference MI values for PE, MD, joint MD/PE, and future-prediction objectives; posterior approximations from trained critics closely match reference posteriors (reported via KL divergences). In the epidemiology case study (SDE-based SIR/SEIR with Poisson observations), the learned optimal measurement times concentrate in early-to-mid windows (roughly $t\in[10,40]$) and MI gains saturate as the measurement budget increases (illustrated up to 10 measurements for PE). For model discrimination between SIR and SEIR with 10 measurements, reported F1-scores are approximately 0.996 (SIR as truth) and 0.983 (SEIR as truth), indicating strong discriminative performance with the learned designs. The paper also notes variance issues for MI estimation with the NWJ bound (due to the exponential term), while JSD-based training is reported as stable and competitive in their comparisons.","The approach requires access to gradients of the simulator sampling path with respect to designs, either explicitly or via automatic differentiation; applicability is reduced when such gradients are unknown or undefined (e.g., fully discrete/categorical simulators). The authors note their framework can handle discrete observations when an analytically tractable observation model is available (separating latent differentiable dynamics from discrete observation), but not otherwise. They also state they do not pursue gradient-free design optimization in this work (despite it being possible), citing scalability limitations with design dimension.","The paper’s empirical validation is largely simulation-based (toy implicit models and an SDE epidemiology simulator) with no real experimental/industrial dataset, so practical robustness to model misspecification and real-world constraints (e.g., costs, feasibility, constraints on repeated measurements) is unclear. Performance can depend on neural critic architecture, training stability, batch size (notably for InfoNCE bias via the $\log K$ ceiling), and optimization hyperparameters; these sensitivities may affect reproducibility across problems. The method assumes the design does not affect the prior over the variable of interest ($p(v\mid d)=p(v)$), which may not hold in some adaptive/selection settings. Computational cost may be high for expensive simulators, since design optimization requires repeated simulation and backpropagation through (or alongside) the simulator.","They suggest exploring scalable adaptations when sampling-path gradients are unavailable/undefined, including investigating alternative gradient approximations (e.g., finite differences or simultaneous perturbation methods) and ways to relax the gradient requirement. They propose extending the framework from static to sequential Bayesian experimental design, potentially learning design networks for amortized sequential BED. They also point to further work on neural network/critic architecture choices (e.g., residual connections, separable critics, pooling/convolutions, and conditioning the critic on the design) and on handling imbalanced dimensionalities between variables of interest and data via summary-statistic learning.","A valuable extension would be benchmarking against additional state-of-the-art likelihood-free BED methods (including recent amortized/sequential approaches) on standardized simulator suites and measuring wall-clock/sample-efficiency tradeoffs. Incorporating explicit experimental constraints and costs (e.g., constrained optimization, multi-objective utilities, budgeted design) would improve real-world applicability. Developing uncertainty quantification for the learned design optimum (e.g., variability across random seeds/training runs) and diagnostics for bound tightness near the optimum would help practitioners trust the outputs. Providing packaged implementations (e.g., a pip-installable library) and broader support for differentiating through common simulator types (including hybrid discrete-continuous systems) would facilitate adoption.",2105.04379v1,https://arxiv.org/pdf/2105.04379v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:28:33Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Prediction|Robustness|Cost reduction|Other,Not applicable|Other,"Variable/General (design of injection-well/source locations; number of sources varies, e.g., up to 6 in examples)",Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://developers.google.com/optimization/|https://github.com/scikit-fmm/scikit-fmm,"The paper proposes a new experimental design framework that integrates Bayesian Evidential Learning (BEL) with machine-learning-based posterior prediction to optimize the placement (location/configuration) of tracer-injection wells for delineating a wellhead protection area (WHPA). Instead of repeatedly solving inverse problems for each candidate design, BEL learns a direct probabilistic relationship between simulated tracer-test breakthrough curves (predictor) and WHPA shapes (target) from a prior ensemble of hydraulic conductivity fields and forward simulations. After dimensionality reduction (PCA on predictors and targets, then CCA to learn correlated canonical variates), the posterior distribution of WHPA given observed data is computed analytically under multivariate-Gaussian assumptions, enabling rapid evaluation of many candidate well-location designs. Experimental design is formulated as minimizing posterior uncertainty (utility) measured via image/shape discrepancy metrics (Modified Hausdorff Distance and Structural Similarity Index) across prospective datasets, with k-fold cross-validation to assess robustness. The approach is demonstrated on a synthetic single-layer aquifer using MODFLOW/MT3D-USGS/MODPATH forward models, showing that adding and appropriately locating injection wells reduces posterior WHPA uncertainty and that downstream wells were most informative in their setup.","BEL inference is performed in canonical space after PCA+CCA: assuming joint Gaussianity, the conditional posterior $p(\mathbf{h}_c\mid \mathbf{d}^*_c)$ is multivariate normal with covariance $\Sigma_{\mathbf{h}_c\mid \mathbf{d}^*_c}=\Lambda_{11}^{-1}$ and mean $\mu_{\mathbf{h}_c\mid \mathbf{d}^*_c}=\Sigma_{\mathbf{h}_c\mid \mathbf{d}^*_c}(\Lambda_{11}\mu_{\mathbf{h}_c}-\Lambda_{12}(\mathbf{d}^*_c-\mu_{\mathbf{d}_c^*}))$ (Eqs. 3–4), where $\Lambda=\Sigma^{-1}$ is the precision matrix of the joint Gaussian (Eqs. 1–2). The experimental-design utility uses discrepancy between true and posterior-sampled WHPA images/contours; e.g., Modified Hausdorff Distance computed from pairwise Euclidean distances between contour point sets: $\mathrm{MHD}=\max(\overline{\min_i D_{ij}},\overline{\min_j D_{ij}})$ (Eq. 17), and SSIM is used as an alternative similarity metric (Eq. 18).","Using a dataset of $n=500$ realizations, the authors use $n_{train}=400$ (80%) to train BEL and show posterior WHPA predictions that envelop the true WHPA while reducing uncertainty relative to the prior, with visible tightening near injection wells. Predictor preprocessing interpolates each breakthrough curve to 200 time steps and retains $\delta=50$ PCs explaining 99.87% variance; the target WHPA (signed-distance image) uses $\upsilon=30$ PCs explaining 98.81% variance. Training-size sensitivity indicates average SSIM stabilizes around $n_{train}\approx 400$ (training sizes tested from 125 to 900). For design with single-source subsets (6 candidate wells), boxplots of standardized MHD/SSIM over a 100-sample test set suggested downstream wells (4–6) were more informative, but 5-fold cross-validation showed 100 test samples were insufficiently consistent; increasing to $n=1250$ (1000 train / 250 test) yielded consistent rankings, with wells 4–6 best and well 1 worst, and well 6 appearing most informative overall in their example.","The authors note BEL performance depends on finding an adequate predictor–target relationship; their learning step (CCA) is linear in reduced space and may bias predictions for highly non-linear relationships. They highlight subjectivity in choosing training-set size and other practitioner-set hyperparameters (numbers of PCs for predictor/target, number of posterior samples, and the choice of data-utility function). They also mention that their prior hydraulic conductivity fields come from sequential Gaussian simulation and are relatively smooth; more geologically complex media (e.g., channelized) may require more advanced priors (e.g., multiple-point statistics), though they cite prior BEL applications in complex settings.","The experimental-design objective is evaluated with discrepancy metrics (MHD/SSIM) that require access to the “true” WHPA for synthetic test cases; in real deployments, design utility would need a preposterior/expected-utility formulation that does not condition on an unknown truth or would rely on simulated truths only. The method assumes prospective observations are consistent with the prior/training distribution; prior misspecification (e.g., boundary conditions, dispersivity, conceptual model error) could lead to misleading posterior uncertainty estimates and hence suboptimal designs. The MG inference relies on approximate joint normality and sufficiently linear canonical relationships; although transformations are used, departures could affect posterior calibration, especially for extreme or multimodal cases.","They suggest handling highly non-linear relationships via adaptations such as iterative prior updating (citing Hermans et al., Michel et al., Park and Caers), though they acknowledge this would increase computation and reduce BEL’s adaptability for experimental design. They also note the framework could be extended to more complex/uncertain prior components (e.g., boundary conditions, spatial correlation parameters, 3D layered cases) and to more geologically realistic priors beyond smooth SGS fields.","Developing a fully decision-theoretic preposterior expected-utility design (integrating over unknown future data) that can be computed efficiently with BEL would better match real-world DOE usage where the true WHPA is unknown. Extending the design space from a finite set of predefined well locations to continuous or combinatorial multi-well configurations with explicit cost/constraints (e.g., mixed-integer optimization) would strengthen practical applicability. Providing open-source, end-to-end implementation (including workflows for prior-checking and posterior predictive checking) and benchmarking against standard Bayesian experimental design methods would improve reproducibility and adoption.",2105.05539v1,https://arxiv.org/pdf/2105.05539v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:29:16Z
TRUE,Optimal design|Computer experiment|Bayesian design|Other,Prediction|Parameter estimation|Other,G-optimal|Bayesian D-optimal|Other,Variable/General (includes effectively infinite-dimensional RKHS features; also discusses regimes where sample size T (or N) can be much less than ambient dimension d),Theoretical/simulation only,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper studies high-dimensional (including RKHS/infinite-dimensional) experimental design for linear/kernel bandits, focusing on the practical problem of rounding a continuous optimal design (a distribution over measurement vectors) into a discrete allocation when the sample budget T can be much smaller than the ambient dimension d. It proposes a new rounding-and-estimation approach based on a Robust Inverse Propensity Score (RIPS) estimator that avoids dimension-dependent minimum-sample requirements and removes the large-deviation term present in standard IPS bounds, while requiring only logarithmic dependence on |V|. The method extends to regularized designs in an RKHS and to misspecified models (bounded approximation error h), enabling kernel-bandit algorithms that match state-of-the-art regret in the well-specified case and yield linear-in-h robustness under misspecification. Empirical simulations compare RIPS-based designs against classical rounding baselines (Carathéodory/ceiling rounding, Allen-Zhu et al. rounding, random sampling/IPS, and a Project-Then-Round RKHS baseline) on finite-dimensional G-optimal design, RKHS G-optimal-like design with RBF kernels, and a constructed instance where IPS exhibits large deviations. The main practical implication is that near-continuous-design performance can be achieved with discrete allocations even when T≪d (or d is infinite), making optimal-design-style planning feasible for kernelized bandits and high-dimensional settings.","The continuous (relaxed) G-optimal-like design is posed as minimizing worst-case variance: $\bar\lambda=\arg\min_{\lambda\in\Delta_\mathcal{X}}\max_{v\in\mathcal{V}} v^\top(\sum_{x\in\mathcal{X}}\lambda_x x x^\top)^{-1}v$ (finite-dimensional form). In the RKHS with regularization, the design matrix is $A^{(\gamma)}(\lambda)=\sum_{x\in\mathcal{X}}\lambda_x\,\phi(x)\phi(x)^\top+\gamma I$, and the allocation is drawn i.i.d. from the optimizer of $\min_{\lambda\in\Delta_\mathcal{X}}\max_{v\in\mathcal{V}}\|v\|^2_{(A^{(\gamma)}(\lambda))^{-1}}$. RIPS estimates each linear functional via robust mean estimation of $v^\top (A^{(\gamma)}(\lambda))^{-1}\phi(x_t) y_t$, yielding bounds of order $\|v\|_{(A^{(\gamma)}(\lambda))^{-1}}\big(\sqrt{\gamma}\|\theta^*\|+h+\sqrt{c(\sigma^2+B^2)\log(|\mathcal{V}|/\delta)/T}\big)$ without an extra large-deviation term.","Theorem 1 provides a high-probability uniform error bound for the RIPS procedure over all $v\in\mathcal{V}$ in an RKHS, with error scaling like $\sqrt{\gamma}\|\theta^*\|+h+\tilde O(\sqrt{(\sigma^2+B^2)/T})$ times the appropriate design-induced norm, and crucially with no restriction that $T\gtrsim d$ (dimension-free). In contrast, the IPS bound includes an additional term of order $\max_{x\in\mathcal{X}}|v^\top A(\lambda)^{-1}x|/T$ that can dominate until $T\ge d$; the paper constructs an example where this behaves like $d/T$ while the main variance term behaves like $\sqrt{d/T}$. The PTR RKHS baseline (Algorithm 2) achieves constant-factor approximation when $T=\Omega(d_e(\lambda,\gamma))$ where $d_e$ is an effective dimension; RIPS avoids this requirement. Simulation results show RIPS is competitive with strong finite-dimensional rounding methods on G-optimal design, performs similarly to PTR/IPS in an RKHS example when $T<m$, and dramatically outperforms IPS on a constructed instance exhibiting IPS large deviations. The paper also derives kernel-bandit regret and pure-exploration guarantees using RIPS that match known well-specified rates and give linear dependence on misspecification level $h$.","The paper notes that RIPS requires the number of measurements to exceed a logarithmic factor in $|\mathcal{V}|$ (e.g., $T\ge c\log(|\mathcal{V}|/\delta)$ for the robust mean estimates). It also acknowledges that computational considerations are mostly ignored; specifically, the computational cost of the RIPS estimator scales linearly in $|\mathcal{V}|$, which can be problematic when $|\mathcal{V}|$ is very large (e.g., combinatorial bandits where $|\mathcal{V}|$ may be exponential).","The empirical evaluation is simulation-based and limited to a few illustrative settings; broader benchmarking (different kernels, noise models, misspecification patterns, and larger-scale pools) would strengthen claims about practical performance. The approach relies on specifying a finite set $\mathcal{V}$ of directions over which worst-case error is controlled; in many design problems choosing/approximating an appropriate (possibly huge or continuous) $\mathcal{V}$ is itself nontrivial and may affect guarantees and runtime. Although the method is described as dimension-free, implementation still involves kernel matrix operations and solving a convex design optimization over $\lambda$; scalability with large $|\mathcal{X}|$ and numerical stability (especially in RKHS with small $\gamma$) could be challenging without additional approximations.","The paper proposes designing a multidimensional robust mean estimation method with the same favorable properties as RIPS but without dependence on $|\mathcal{V}|$, to address settings where $|\mathcal{V}|$ is potentially exponential (e.g., combinatorial bandits).","Developing scalable solvers for the continuous design optimization in large pools (e.g., stochastic/Frank–Wolfe variants with kernel approximations) and providing end-to-end complexity/accuracy tradeoffs would improve deployability. Extending the methodology to handle structured data issues common in sequential experimentation—such as temporal dependence/autocorrelation in rewards, heteroskedastic noise, or adaptive/online updates of the design distribution—would broaden applicability. Providing open-source reference implementations and standardized benchmarks for RKHS experimental design and kernel bandits would facilitate adoption and reproducibility.",2105.05806v1,https://arxiv.org/pdf/2105.05806v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:29:54Z
TRUE,Optimal design|Sequential/adaptive|Other,Model discrimination|Parameter estimation|Prediction|Other,Other,Variable/General (p variables/nodes; each intervention targets up to q variables; batch size up to m interventions),Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/ssethz/multi-perturbation-ed,"The paper studies experimental design for causal structure learning in directed acyclic graphs when each experiment can intervene on multiple variables (multi-perturbation) and experiments are chosen in a batch under constraints: at most m interventions, each targeting at most q nodes. It proposes efficient near-optimal algorithms for two infinite-sample objectives: (i) an edge-orientation objective that rewards the number (or weighted number) of edges whose direction becomes identifiable after interventions and Meek-rule propagation, and (ii) a mutual-information-like objective that minimizes the expected log size of the remaining interventional Markov equivalence class. For the edge-orientation objective, it proves monotonicity/submodularity over the ground set of interventions and uses a stochastic continuous-greedy approach (NMSCG + rounding) to approximately greedily select multi-node interventions with constant-factor approximation guarantees. For the mutual-information infinite-sample objective, it optimizes a sampled approximation and uses separating-system constructions to restrict the search space, yielding a provable lower bound and practical greedy selection. Experiments on synthetic graphs and gene regulatory network benchmarks show multi-perturbation designs outperform random and single-node-intervention baselines in identifiability and related metrics (and in finite-sample settings using an MI objective approximation).","Interventions are sets I ⊂ [p] with |I| ≤ q, and a batch ξ is a (multi)set with |ξ| ≤ m. The infinite-sample mutual-information objective is $F_\infty(\xi)= -\frac{1}{|\mathcal G|}\sum_{G\in\mathcal G}\log_2|\mathrm{Ess}_{\xi\cup\xi_0}(G)|$, i.e., it favors interventions that reduce the essential-graph (equivalence-class) size. The edge-orientation objective is $F_{EO}(\xi)=\frac{1}{|\mathcal G|}\sum_{G\in\mathcal G}|R(\xi,G)|$ (or a weighted coverage variant), where $R$ is the set of edges oriented by intervention-implied directions plus closure under Meek rules. For selecting a q-node intervention via continuous relaxation, it uses a multilinear extension $f(x)=\sum_I F^{\xi}_{EO}(I)\prod_{i\in I}x_i\prod_{i\notin I}(1-x_i)$ with constraints $\sum_i x_i\le q,\ 0\le x_i\le 1$, and stochastic gradient estimates based on sampled DAGs/interventions.","The paper proves that the edge-orientation objective is monotone submodular over the (large) ground set of multi-node interventions, enabling approximation guarantees for greedy-style selection. Using NMSCG with rounding, the intervention chosen at each step achieves an expected $(1/e)$-approximation (up to $\epsilon$) to the best marginal intervention, and the resulting batch algorithm (DGC) achieves a constant-factor guarantee $\mathbb{E}[F_{EO}(\xi)]\ge (1-e^{-1/e})F_{EO}(\xi^*)-\epsilon$. For the MI-infinite objective approximation with separating systems, it provides a lower bound of the form $\tilde F_\infty(\xi)\ge (1- m/|S|)\tilde F_\infty(\emptyset)$ (with $|S|$ bounded by a separating-system construction), and reports empirical improvements over random and single-node greedy baselines on synthetic graphs and GRN benchmarks. A public GitHub repository is provided to reproduce experiments.","The authors note that multi-perturbation design has a doubly-exponential search space, so the methods rely on submodularity/relaxations and, for MI-based objectives, on tractable approximations using a sampled multiset of DAGs rather than summing over all DAGs in the equivalence class. They also state that their polynomial-time algorithms assume access to a uniform sampler over DAGs in the current essential graph, which exists for MECs but not generally for essential graphs under interventions; in practice they use an efficient non-uniform sampler. For finite-sample settings, they explicitly note their methods do not have theoretical guarantees (they are adapted as practical algorithms).","The intervention model assumes hard interventions (with only limited discussion of soft interventions), no hidden confounding, and faithfulness; performance may degrade under latent variables, selection bias, or violations of these assumptions. The computational burden can still be significant in large graphs because objective evaluation/gradient estimation requires repeated application of Meek rules and repeated sampling over candidate DAGs, and the quality depends on the representativeness of the sampled DAG set (especially for the MI approximation). The objectives focus on identifiability/orientation and equivalence-class reduction; they may not align with downstream causal effect estimation goals or experimental feasibility constraints beyond simple (m,q) budgets (e.g., costs varying by target, intervention failure, toxicity/viability constraints). Comparisons are primarily against random and single-node-intervention baselines; broader benchmarking against other active causal discovery heuristics or model-based Bayesian design approaches could strengthen conclusions.","They suggest extensions to soft interventions (noting some results extend easily and discussing an equivalence result in the appendix) and note an open possibility that the MI-infinite objective might be directly optimizable without sampling approximations if submodularity could be established for the full essential-graph objective. They also indicate that better accounting for finite-sample effects (e.g., noise structure, heteroscedasticity, and weight variability) could improve performance beyond infinite-sample approximations. The paper also highlights the practical need for effective sampling over DAGs in interventional essential graphs, implying algorithmic improvements there would benefit the approach.","Develop self-normalized or Bayesian experimental design variants that integrate uncertainty over both graph structure and functional parameters (beyond sampled DAG bags) and handle non-uniform intervention costs and feasibility constraints. Extend the guarantees and methods to settings with latent confounding (e.g., MAG/PAG representations), autocorrelated/time-series data, and imperfect or stochastic interventions. Provide scalable software implementations (e.g., in Python/R) with optimized Meek-rule closure and faster approximate marginal-gain evaluation, and benchmark on larger real interventional datasets (e.g., Perturb-seq/CRISPR screens) with experimentally measured outcomes. Study robustness of the proposed objectives/algorithms to model misspecification and faithfulness violations, and connect design choices to downstream causal effect estimation accuracy and decision-making performance.",2105.14024v2,https://arxiv.org/pdf/2105.14024v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:30:42Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Optimization|Parameter estimation|Cost reduction|Other,Other,"Variable/General (examples: 5-oscillator and 7-oscillator Kuramoto networks; factors are uncertain coupling strengths a_{i,j} and choice of experiment (i,j))",Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable|Other,Public repository (GitHub/GitLab),https://github.com/bjyoontamu/Kuramoto-Model-OED-acceleration,"The paper proposes a machine-learning acceleration scheme for MOCU-based optimal experimental design (OED) in uncertain Kuramoto oscillator networks, where experiments are selected to most reduce objective-relevant uncertainty for robust synchronization control. OED candidates are pairwise “isolation” tests of oscillator pairs (i,j) with a binary outcome (synchronized vs not), which updates bounds on uncertain coupling parameters and thus the posterior uncertainty class. The computational bottleneck in MOCU computation—repeated ODE solves within a binary search to find the minimum control coupling that guarantees synchronization—is replaced by a trained binary classifier (a shallow fully connected neural network) that predicts synchronization from engineered features. Through simulation studies on 5- and 7-oscillator settings, the ML-based expected-remaining-MOCU values are highly correlated with the original sampling/ODE-based approach, and the resulting experiment rankings yield essentially the same OED performance. The method achieves large speedups (reported up to ~154× for OED in the abstract; much larger for individual MOCU computations in experiments) while maintaining comparable uncertainty reduction versus the baseline MOCU OED, and outperforming entropy-based or random experiment selection baselines.","Uncertain Kuramoto dynamics: $\dot\theta_i(t)=\omega_i+\sum_{j=1}^N a_{i,j}\sin(\theta_j(t)-\theta_i(t))$ and with a global control oscillator: $\dot\theta_i(t)=\omega_i+\sum_{j=1}^N a_{i,j}\sin(\theta_j-\theta_i)+a_{N+1}\sin(\theta_{N+1}-\theta_i)$. The robust control cost is $\xi^*(\mathcal A)=\max_{a\in\mathcal A}\xi(a)$ and MOCU is $M(\mathcal A)=\mathbb E_{A}[\xi^*(\mathcal A)-\xi(a)]$; expected remaining MOCU for an experiment is $R(i,j)=\mathbb E_{O_{i,j}}[M(\mathcal A\mid O_{i,j})]=\sum_{o\in\{0,1\}}P(O_{i,j}=o)M(\mathcal A\mid O_{i,j}=o)$, selecting $(i^*,j^*)=\arg\min_{(i,j)\in\mathcal E}R(i,j)$.","For the 5-oscillator case, expected remaining MOCU estimates from the ML-based method and the original sampling/ODE-based method have Pearson correlation 0.9849 (p=1.9017e-76), while average time per expected-remaining-MOCU computation drops from 818.7 s to 0.1110 s. For the 7-oscillator case, the correlation is 0.9606 (p=2.6206e-56), with time per computation dropping from 3684.9 s to 0.6953 s. In OED simulations (both 5- and 7-oscillator settings), ML-based MOCU OED achieves essentially identical uncertainty reduction curves (MOCU vs number of experiment updates) to sampling-based MOCU OED and consistently outperforms entropy-based and random experiment selection; the ML method matches the first optimal experiment predicted by the sampling-based method in reported repeated trials. The abstract reports overall OED acceleration up to 154× without degradation of OED performance.",None stated.,"The OED “experiment” model is a stylized pairwise isolation test with a binary outcome and an update rule derived from a two-oscillator synchronization theorem; real experimental systems may not permit isolation, may yield noisy/graded outcomes, or violate the theorem’s assumptions. The ML surrogate is trained using labels generated by long, high-fidelity ODE simulations; the paper does not quantify how performance degrades under distribution shift (different frequency distributions, different uncertainty classes, different N) or if retraining is required. The approach is demonstrated on small N (5 and 7) and on a specific control architecture (single global synchronizer), leaving scalability/generalization to larger, more heterogeneous networks uncertain.","The paper suggests comparing the ML-based method and the sampling-based method from a “data efficiency” perspective (training-data generation cost) and notes broader applicability of ML-accelerated MOCU/OED to other problems. It also proposes future work on using deeper neural networks to discover scientific/relational knowledge in non-homogeneous Kuramoto models (beyond the available two-oscillator theorem), potentially expanding the experimental design space and enabling more effective uncertainty reduction.","Develop a self-starting/online-learning variant that updates or fine-tunes the classifier as experimental outcomes accumulate, with uncertainty-aware acquisition to reduce labeling cost. Extend the experiment model to handle noisy, partial, or continuous-valued measurements and incorporate measurement error models into Bayesian/MOCU updates. Provide theoretical or empirical robustness analyses for classifier errors on experiment ranking and downstream OED performance, and test transfer across different network sizes/topologies to reduce retraining needs.",2106.00332v2,https://arxiv.org/pdf/2106.00332v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:31:19Z
FALSE,NA,NA,Not applicable,"Variable/General (image dimensions treated as input variables; e.g., MNIST 784 dims, CIFAR10 3072 dims)",Network/cybersecurity|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://github.com/bethgelab/foolbox|https://pytorch.org/tutorials/beginner/fgsm_tutorial.html|https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html|http://yann.lecun.com/exdb/mnist/,"The paper investigates why deep neural networks (DNNs) admit adversarial examples by empirically examining the DNN “response surface” (classification boundary) in the neighborhood of a clean image. Using multiple trained models with identical architectures but different random initial seeds, it defines DNN “uncertainty regions” as inputs where at least two such models disagree on the predicted class, and shows transferability of adversarial examples is not universal. The authors construct lower-dimensional hyper-rectangles around a clean image by aggregating adversarial examples produced by standard attacks (e.g., CW-L2, FGSM, BIM, NewtonFool, Momentum Iterative, Pointwise) and then sampling within the inferred pixel-interval subspace; this yields infinitely many adversarial samples within small L2 neighborhoods and often with fewer perturbed pixels than the original attacks. Experiments on MNIST with LeNet CNNs and MLPs, and on CIFAR10 with re-trained MobileNet, demonstrate fractured/“cracked” decision boundaries featuring both uncertainty regions (model disagreement) and transferable adversarial regions (broad agreement in misclassification). The paper further argues that standard generalization-error theory cannot capture adversarial vulnerabilities because these can concentrate on lower-dimensional (measure-zero) subsets that do not affect expected risk under continuous input distributions.","Uncertainty region is defined as $U := \{W: \exists\, M_i, M_j \in \mathcal{M}\ \text{s.t.}\ M_i(W) \neq M_j(W)\}$ where models share architecture but differ by training seed. A $\delta$-neighborhood around a clean image $W$ is $B(\delta,W)=\{W_a: \|W_a-W\|_2\le \delta\}$. For attack $k$ and target class $t$, with adversarial set $I_k(t)=\{W^a_k: M_1(W^a_k)=t\}$, pixel-wise intervals $s^{t,k}_i=\max_{I_k(t)}(W^a_{k,i})-\min_{I_k(t)}(W^a_{k,i})$ are ranked and a hyper-rectangle is built from the top-$b$ intervals: $R_k(t)=\bigotimes_{j=1}^b[\min(W^a_{k,(j)}),\max(W^a_{k,(j)})]$.","Across MNIST LeNet models, many hyper-rectangles sampled from attack-derived subspaces yield high misclassification for the attacked model $M_1$ while some other seeds’ models classify correctly, evidencing uncertainty regions; conversely some hyper-rectangles are highly transferable with many models misclassifying. The smallest reported interval sizes used for hyper-rectangles are very small (e.g., up to about 0.036 on the [0,1] pixel scale in Table 2), indicating near-constant perturbations in selected dimensions. Sampling within these hyper-rectangles often reduced L2 perturbation compared to the original attack examples (e.g., MNIST PW 1→2 mean L2 about 9.58 for sampled vs 17.39 for attack; CIFAR10 airplane→deer sampled mean L2 about 65.9 vs attack mean about 256 in Table 10). On CIFAR10 MobileNet, BIM-L2 perturbs nearly all dimensions (reported 3071/3072), while a ~2000-dimensional hyper-rectangle can still produce strong misclassification for some seeds and near-zero for others, again showing seed-dependent uncertainty regions.","The authors state that their method relies on existing adversarial-attack algorithms to identify the hyper-rectangles/subspaces where adversarial examples lie. They also note the current approach is demonstrated on low-resolution images, and conjecture that for high-resolution images adversarial examples may lie on bounded regions of lower-dimensional manifolds, which they leave to future work.","The empirical findings depend heavily on specific model families (LeNet/MobileNet/MLP), training setups, and chosen attacks/thresholds (e.g., requiring 80–100 adversarial samples per clean image), which may limit reproducibility and generality across modern architectures and training regimes. The “response surface” analysis is largely based on sampling-induced hyper-rectangles rather than a principled geometric characterization of the true decision boundary; results may be sensitive to how $b$ (number of intervals) and $\delta$ are selected. Comparisons are mostly seed-variants of the same architecture; transferability across substantially different architectures/training objectives (and real-world distribution shifts) is only partially addressed.","They explicitly leave several directions to future work: analyzing more complex response surfaces when models output soft labels (including top-5 accuracy settings), capturing the DNN classification boundary in much higher-dimensional feature spaces (high-resolution images), and measuring/understanding the size of DNN uncertainty regions versus transferable adversarial regions.","Developing a more formal, attack-independent procedure to estimate uncertainty/transferable regions (e.g., via certified robustness bounds or geometric/topological summaries) would strengthen the claims beyond attack-derived subspaces. Extending the approach to correlated/structured perturbations (spatial, color, and semantic transformations) and to distributional shifts would improve practical relevance. Providing open-source implementations and standardized benchmarks (multiple datasets, architectures, defenses) would enable broader validation and facilitate adoption by practitioners.",2107.00003v1,https://arxiv.org/pdf/2107.00003v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:31:59Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Other,D-optimal|Other,"Variable/General (examples include p=1,k=3 toy; p=50,k=50 simulations; p=11,k=14 microbiome)",Food/agriculture|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies whether and how experimental design (the predictor design matrix X) can improve Bayesian inference of the response precision matrix Ω in Gaussian chain graph models (multivariate regression with correlated errors). Using the Laplace approximation to the marginal posterior precision of Ω as the design optimality criterion, it compares four priors (flat, conjugate Normal–Wishart, Normal–MGIG, and a general independent prior) and derives when the criterion depends on X. It shows that under the flat and conjugate Normal–Wishart priors the Laplace-approximated marginal posterior precision of Ω is independent of X, implying that optimal experimental design for learning Ω is impossible under that criterion. In contrast, under the Normal–MGIG and general independent priors the criterion does depend on X, enabling optimal design, but the authors derive a sharp upper bound on the information gain achievable from experiments that depends on prior knowledge about regression effects. Theoretical claims are supported by simulations (comparing KL divergence prior-to-posterior and Stein’s loss of MAP estimators under random/specific vs null designs) and illustrated on human gut microbiome data, where MGIG priors show uncertainty reduction from “experiment” while Wishart does not.","Model: $Y_i\mid X_i,B,\Omega\sim\mathcal N(\Omega^{-1}B^T X_i,\Omega^{-1})$. The DOE optimality criterion is the Laplace-approximated marginal posterior precision of $\Omega$, given by the Schur complement $(A-GD^{-1}C)$ of the joint negative Hessian of the log posterior (with blocks for $\Omega$ and $B$). Under Normal–MGIG, the resulting expression includes an $X$-dependent term involving $\Lambda^{-1}-\Lambda^{-1}(X^TX+\Lambda^{-1})^{-1}\Lambda^{-1}$, enabling (e.g.) D-optimal design via maximizing $|X^TX+\Lambda^{-1}|$.","For flat and conjugate Normal–Wishart priors, the Laplace-approximated marginal posterior precision of $\Omega$ simplifies to a form independent of the design matrix $X$, so optimizing $X$ cannot improve inference of $\Omega$ under this criterion. For Normal–MGIG and general independent priors, the criterion depends on $X$ but the experiment-driven information term is bounded above (sharply, approached as $X^TX\to\infty$) by a quantity determined by prior mean effects (e.g., $B_0\Omega^{-1}$) and prior certainty (e.g., $\Lambda^{-1}$). Simulations with $k=p=50$ across multiple covariance structures show that experiments (random or “specific”) provide sustained information gain and improved MAP accuracy versus a null design primarily when prior knowledge on $B$ is both accurate and certain (notably for MGIG with small $\Lambda$); otherwise differences tend toward zero with increasing sample size. In a gut microbiome example (about 13–14 responses and 11 predictors), only the MGIG prior shows reduced posterior uncertainty in partial correlations when comparing experiment vs null settings.","The authors note that they focus on the Laplace approximation of the marginal posterior precision of $\Omega$ (rather than exact posterior precision) because exact posterior precision is often intractable, and that Laplace approximation requires log-concave posteriors (they verify log concavity conditions for the priors studied). They also state that a thorough comparison of multiple priors for chain graph models is beyond the paper’s scope, focusing instead on priors already adopted in chain graphs plus the proposed Normal–MGIG. For the real microbiome data, they cannot truly design an experiment and instead compare the observed design to a hypothetical null experiment via simulated samples.","The optimal-design conclusions hinge on using the Laplace-approximated marginal posterior precision of $\Omega$ as the sole design criterion; other Bayesian utilities (e.g., expected information gain about edges, posterior loss on partial correlations, or design targeting specific substructures) might yield different dependencies on $X$ even under conjugate priors. Many derivations assume model correctness (Gaussianity, conditional independence structure, and typically i.i.d. samples); robustness to autocorrelation, compositional effects in microbiome abundances, or heavy tails may affect practical DOE recommendations. The work discusses bounds as $X^TX\to\infty$, but operational constraints (costs, feasible treatment levels, randomization/replication limits) are not incorporated into a constrained or economic design framework.","They propose extending the question (whether effective experimental design for learning network structure requires prior knowledge of treatment effects) to more general Gibbs measures with two-body interactions, specifically mentioning the auto-logistic/Ising model and the challenge posed by intractable partition functions. They suggest that approximations to the partition function (e.g., log-determinant relaxations) might help connect the Gaussian-case insights to discrete network models.","Developing constrained Bayesian optimal designs (e.g., with fixed budgets, discrete treatment sets, replication constraints, or ethical limits) would make the results more directly actionable for experiments in biology. Extending the framework to handle unknown/estimated hyperparameters (empirical Bayes), self-starting priors, and Phase I/II separation could clarify how much prior knowledge on $B$ is realistically obtainable. Additional work could target design criteria aligned with graphical goals (edge selection accuracy, FDR control, or posterior inclusion probabilities) and provide open-source implementations to facilitate adoption.",2107.01306v5,https://arxiv.org/pdf/2107.01306v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:32:43Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Optimization|Other,Not applicable,Variable/General (components repeatedly randomized at decision points; examples include 2–4 components; decision points up to 5/day; total randomizations up to hundreds/thousands),Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|SAS,Public repository (GitHub/GitLab)|Not provided,http://people.seas.harvard.edu/~samurphy/JITAI_MRT/mrts4.html|https://statisticalreinforcementlearninglab.shinyapps.io/mrt_ss_continuous/|https://tqian.shinyapps.io/mrt_ss_binary/|https://github.com/StatisticalReinforcementLearningLab/HeartstepsV1Code/blob/master/xgeepack.R|https://github.com/tqian/paper_mrt_PsychMethods|https://github.com/tqian/paper_mrt_PsychMethods/tree/main/synthetic_data,"This paper is a methodological tutorial/review on the micro-randomized trial (MRT), an optimization experimental design used to develop just-in-time adaptive interventions (JITAIs) for digital health and related behavioral interventions. It explains key MRT design elements (components/options, decision points, randomization probabilities that may be unequal to manage burden, and context-based availability restrictions) and illustrates them via three real MRTs (HeartSteps, SARA, BariFit), including hybrid designs with both baseline randomization and micro-randomization. On the analysis side, it defines the causal excursion effect for time-varying treatments using the potential outcomes framework and shows how it links to estimands identifiable under sequential randomization. It reviews the weighted and centered least squares (WCLS) estimator for consistent estimation of causal excursion effects in the presence of endogenous time-varying covariates, and provides guidance for implementation using standard GEE software (e.g., R/SAS). An illustrative analysis of HeartSteps reports an estimated average (log-scale) proximal effect of activity suggestions versus none and shows evidence the effect decays over study days, with moderation by location for walking suggestions.","The primary causal estimand is the causal excursion effect at decision time t: 
$\beta(t,s)=\mathbb{E}[Y_{t+1}(\bar A_{t-1},1)-Y_{t+1}(\bar A_{t-1},0)\mid I_t(\bar A_{t-1})=1,\ S_t(\bar A_{t-1})=s]$ (Eq. 1), which conditions on availability ($I_t=1$) and moderators $S_t$. Under sequential randomization it can be written using observed-data regressions as an average of conditional mean differences (Eq. 2). Estimation uses WCLS via an estimating equation equivalent to a weighted GEE with centered treatment: solve for $(\hat\alpha,\hat\beta)$ in $\sum_{i,t} I_{it}[Y_{i,t+1}-Z_{it}^T\alpha-(A_{it}-p)S_{it}^T\beta]\,[Z_{it},(A_{it}-p)S_{it}]^T=0$ (Eq. 5), where $p$ is the known randomization probability.","In the HeartSteps 42-day MRT illustrative analysis (n=37), the estimated average causal excursion effect of delivering an activity suggestion (vs none) on log 30-minute step count was $\hat\beta_0=0.131$ (p=0.060; 95% CI −0.006 to 0.268), interpreted as roughly a 14% increase on the original step-count scale. When allowing effect to vary linearly with study day, the interaction estimate was $\hat\beta_1=-0.018$ per day (p=0.005; 95% CI −0.031 to −0.006), indicating decay over time. Location moderation for walking suggestions was significant: $\hat\beta_1=0.377$ (p=0.049; 95% CI 0.001 to 0.753) for home/work vs other, while moderation for anti-sedentary suggestions was not significant (p=0.472).","The authors note that more work is needed to integrate MRT results into the broader MOST goal of optimizing the intervention as a whole (not just individual components), because component costs/effects may be non-additive (interactions or economies of scale). They also discuss limitations to generalizability caused by design-time availability restrictions (e.g., HeartSteps not randomizing while “driving”), meaning the resulting data cannot inform effects in those contexts. They further highlight practical limitations due to evolving sensor capabilities, which can change what contexts are detectable and thus what can be safely randomized.","As a design/analysis tutorial, empirical validation is limited to a small number of case studies and one detailed illustrative analysis (HeartSteps with n=37), so performance and robustness across broader settings (e.g., heavy missingness, strong autocorrelation, nonstationarity beyond linear day trends) is not comprehensively benchmarked. The paper emphasizes WCLS for marginal excursion effects; guidance for high-dimensional moderator selection and multiplicity control in exploratory moderation (common in JITAI development) is not a primary focus. Implementation guidance is partly fragmented across appendices and external repositories; reproducibility for the real HeartSteps dataset is limited because the original data are not public.","They call for research on how best to use MRT results to optimize the entire intervention within MOST when component effects/costs are non-additive. They suggest continual optimization in deployed digital interventions, potentially embedding MRT-like experimentation in routine delivery (analogous to including experimental items in standardized tests). They also propose extending MRT use to “pull” components (user-initiated decision points) and emphasize development of personalized, person-specific decision rules, pointing to reinforcement learning as a key direction.","Developing standardized reporting guidelines and default design diagnostics for MRTs (e.g., balance checks under availability constraints, monitoring burden/engagement trajectories) would improve comparability across studies. Methods that jointly handle availability, missing not at random due to disengagement, and interference/social-network effects would broaden applicability in real-world digital platforms. User-friendly software packages (e.g., an R package wrapping WCLS with small-sample corrections, diagnostics, and plotting) and open benchmark datasets would also accelerate adoption and reproducibility.",2107.03544v3,https://arxiv.org/pdf/2107.03544v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:33:27Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Parameter estimation|Optimization,Other,"Variable/General (feature dimension $d$; examples: $d=20$ synthetic, $d=300$ Yahoo LTR after subsampling from 700)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,https://webscope.sandbox.yahoo.com/catalog.php?datatype=c,"The paper proposes a design-of-experiments approach for stochastic linear contextual bandits that produces a single non-reactive (static) exploration policy, motivated by settings where policy updates are costly or impractical. Using an offline batch of contexts only (no rewards), a “planner” runs a reward-free LINUCB-style procedure to construct a mixture of policies; an online “sampler” then collects reward data by randomly selecting one of these precomputed policies per round, yielding an i.i.d. dataset. The learned dataset is used for regularized least squares estimation of the reward parameter and a greedy policy is extracted. Theoretically, the method achieves near-minimax online sample complexity (up to logs) to learn an $\varepsilon$-optimal policy in expectation over contexts, while requiring only polynomially many offline contexts, with tradeoffs controlled by the regularization level. Empirically on synthetic data and the Yahoo! Learning-to-Rank dataset, the planner-sampler strategy learns better policies with fewer online samples than standard baselines (e.g., random exploration), and remains competitive under model misspecification.","Policy extraction uses ridge/regularized least squares: $\hat\theta=(\Sigma_N^0)^{-1}\sum_{n=1}^N \phi(s_n^0,a_n^0)r_n^0$, with $\Sigma_N^0=\sum_{n=1}^N \phi(s_n^0,a_n^0)\phi(s_n^0,a_n^0)^\top+\lambda_{\mathrm{reg}}I$, and greedy policy $\pi_b(s)=\arg\max_{a\in A_s} \phi(s,a)^\top\hat\theta$. The exploratory policy is a mixture over planner-generated policies $\pi_{\text{mix}}$: on each online round sample index $m$ uniformly and play $a_n^0=\pi_m(s_n^0)$. Performance is controlled by the average max predictive uncertainty $\mathbb{E}_{s\sim\mu}\max_a\|\phi(s,a)\|_{(\Sigma_N^0)^{-1}}$, yielding bounds such as $\mathbb{E}_{s\sim\mu}\max_a|\phi(s,a)^\top(\theta^*-\hat\theta)|\le \varepsilon$ and policy suboptimality $\le 2\varepsilon$ under stated sample sizes.","The main guarantee (Theorem 1) shows that with probability at least $1-\delta$, the expected maximum prediction error is at most $\varepsilon$ and the greedy policy’s expected suboptimality is at most $2\varepsilon$, after running the offline planner for $M=\Omega(\tilde d^2\beta/(\lambda_{\mathrm{reg}}\varepsilon^2))$ iterations and the online sampler for $N=\Omega(\tilde d\beta/\varepsilon^2)$ (logs/consts suppressed). Instantiated bounds (Table 1) indicate online data scales like $(d\log|S\times A|)/\varepsilon^2$ in small finite $|S\times A|$ settings and like $d^2/\varepsilon^2$ in large-action regimes, with offline data requirements between $d^2/\varepsilon^2$ and $d^3/\varepsilon^2$ depending on $\lambda_{\mathrm{reg}}$. In experiments, the planner-sampler (S-P) consistently outperforms random exploration on both synthetic and Yahoo LTR setups, particularly in low-sample regimes and for larger regularization values.","The authors note that method performance may degrade if the online context distribution differs from the offline one, explicitly raising distribution shift as an open concern. They also acknowledge that the Yahoo! learning-to-rank setting is reward-model misspecified (not exactly linear), though experiments suggest the approach still performs well. They emphasize a bias–variance tradeoff in choosing regularization $\lambda_{\mathrm{reg}}$, with optimal choices being problem dependent.","The DOE objective/criterion is expressed via bandit uncertainty and covariance growth rather than a standard, clearly named optimality criterion (e.g., explicit D/A/I-optimal design), which can make it harder for DOE practitioners to map the approach to classical design catalogs. The method relies on a known feature map $\phi(s,a)$ for all candidate actions in offline contexts; in many applications, action features may be partially unknown, costly to compute, or depend on unavailable information. Comparisons are limited (e.g., primarily random and heuristic baselines); broader benchmarks against other nonadaptive contextual-bandit exploration or optimal-design-for-bandits methods could better establish relative performance across regimes (e.g., heavy-tailed noise, correlated contexts, or varying action-set sizes). Code is not shared, which limits reproducibility for the real-data pipeline and hyperparameter choices.","They ask whether one can devise a procedure that works well across all values of regularization without requiring substantially more offline data. They ask how performance degrades under mismatch between offline and online context distributions. They also propose incorporating prior reward data to reduce sample complexity, and studying how benefits depend on state-action space size and the (possibly adversarial) policy that collected that prior reward data.","Develop an explicit connection to classical optimal design criteria (e.g., D-/G-/I-optimality) for contextual settings, potentially yielding clearer design interpretations and alternative solvers. Extend the approach to handle autocorrelated or non-i.i.d. contexts/rewards, and to robust/heavy-tailed noise via robust regression or concentration tools. Provide practical guidance or adaptive tuning rules for $\alpha$ and $\lambda_{\mathrm{reg}}$ (including self-starting or data-driven selection) and implement an open-source reference implementation to standardize evaluation. Explore generalizations beyond linear models (generalized linear bandits, kernelized/RKHS, or representation learning) while retaining non-reactive data-collection constraints.",2107.09912v2,https://arxiv.org/pdf/2107.09912v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:34:08Z
TRUE,Sequential/adaptive|Optimal design|Bayesian design|Computer experiment|Other,Optimization|Prediction|Cost reduction|Other,Other,Variable/General (examples include 1D; 3 factors; 4 factors; up to 13 variables in reviewed AM studies),Manufacturing (general)|Other,Case study (real dataset)|Simulation study|Other,TRUE,R|Python|Other,Public repository (GitHub/GitLab),http://www.optomec.com/|https://botorch.org/api/acquisition.html|https://github.com/HIPS/Spearmint|https://github.com/SheffieldML/GPyOpt|https://github.com/wujian16/Cornell-MOE|https://github.com/GPflow/GPflowOpt|https://github.com/josejimenezluna/pyGPGO|https://github.com/amzn/emukit|https://github.com/dragonfly/dragonfly|https://github.com/secondmind-labs/trieste|https://github.com/pytorch/botorch|https://github.com/benavoli/SkewGP|https://github.com/GuangchenW/BayesianOptimization,"The paper is a tutorial-style review on using Bayesian optimization (BO) as a sequential design of experiments framework for expensive black-box objectives, with emphasis on additive manufacturing (AM). It explains BO’s core ingredients (probabilistic surrogate modeling, mainly Gaussian processes with Matérn kernels, and acquisition functions such as EI, UCB, entropy search, and knowledge gradient) and frames experimental design as iterative selection of the next run(s) to maximize information and/or performance under limited budgets. It surveys reported BO applications in AM (e.g., lattice/metamaterial design, melt-pool control, autonomous printing systems) and highlights design-space characteristics including constraints, discrete/categorical variables, batch/parallel runs, multi-objective and multi-fidelity settings. It compares prominent open-source BO software in R and Python and provides implementation examples for batch BO, multi-objective BO, and BO with black-box constraints, plus an extension to preference (pairwise comparison) outputs via Skew Gaussian processes. Practical takeaway: BO is positioned as a sample-efficient, adaptive DOE strategy for AM process/structure optimization, with guidance on software selection and code templates for common DOE scenarios.","The experimental-design problem is posed as black-box global optimization $x^* = \arg\max_{x\in \mathcal{X}} f(x)$ with noisy observations $y=f(x)+\varepsilon$ (typically $\varepsilon\sim\mathcal{N}(0,\sigma^2)$). A Gaussian process surrogate yields posterior mean $m_n(x)=m(x)+k(x,X)[K(X,X)+\sigma^2 I]^{-1}(y-m(X))$ and variance $\sigma_n^2(x)=k(x,x)-k(x,X)[K(X,X)+\sigma^2 I]^{-1}k(X,x)$. A key acquisition for sequential design is expected improvement $\alpha(x)=\mathbb{E}[\max(0,f(x)-\hat f)\mid \mathcal{D}_{1:n}]$, with standard closed form using $\Phi$ and $\phi$; UCB is given by $\alpha(x)=m_n(x)+\beta\,\sigma_n(x)$.","The review reports that only eight BO-in-AM papers were found in their systematic search and that all were published within the prior five years, suggesting BO adoption in AM is recent. In an autonomous AM example (crossed-barrel toughness), simulations suggested BO could reduce required experiments by nearly a factor of 60 relative to a large grid campaign. Another prototype autonomous system for extrusion parameters achieved near-ideal geometry within about 100 experiments. The paper also provides example “next-batch” recommendations from multiple BO libraries on a Taguchi L27 AM dataset, illustrating that different packages produce plausible but different suggested settings under the same data and batch size.","The authors note that there is “no clear winner” among BO software packages because performance depends on domain/problem characteristics and default configurations; comparative studies should not be interpreted as universally ranking packages. They also caution that some package comparisons are limited (e.g., acquisition lists not complete for some libraries, default methods compared rather than optimal configurations) and that certain libraries have installation/compatibility limitations (e.g., Trieste incompatibilities with GPy-based stacks at the time of writing, Cornell-MOE Linux-only).","Because this is primarily a tutorial/review, many empirical comparisons are illustrative rather than a controlled benchmark: the suggested “next points” across libraries are not evaluated on a common true objective, so it’s hard to infer real performance differences. The GP/BO exposition largely assumes i.i.d. noise and standard GP regression; practical AM data often have heteroskedastic noise, constraints/feasibility failures, and temporal drift that may require more robust/self-starting or online calibration approaches. The AM DOE examples rely on existing datasets (often Taguchi/factorial) rather than demonstrating full end-to-end BO-driven physical experimentation with measurement costs, warm-up/Phase I modeling choices, and stopping rules.","They point to a broader trend toward autonomous/digitalized additive manufacturing and argue BO will be a key enabling technology, implying further integration of BO into closed-loop AM systems. They also mention maintaining and updating the provided code as upstream BO packages evolve (with an updated code repository link).","A useful extension would be more systematic, reproducible benchmarking of BO-for-DOE in AM across noise regimes, constraints, discrete/categorical factors, and multi-fidelity simulation-to-experiment transfer, including standardized metrics (simple regret, feasibility rate, cost-adjusted performance). Another direction is developing AM-focused BO workflows that explicitly handle failed builds, heteroskedastic measurement noise, and process drift (e.g., robust acquisition functions, feasibility modeling, and change-point-aware surrogates). Finally, packaging the tutorials into maintained, task-oriented templates (e.g., for constrained multi-objective AM process optimization) with validated defaults could lower adoption barriers for practitioners.",2107.12809v4,https://arxiv.org/pdf/2107.12809v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:34:49Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Other,Optimization|Prediction|Cost reduction|Other,Other,"Variable/General (survey strategy parameters such as footprint size, cadence/rolling cadence, filter allocation, visit pairing/triplets, exposure time, dithering patterns, minisurvey allocations)",Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://project.lsst.org/meetings/ocw/sites/lsst.org.meetings.ocw/files/OpSim%20Rolling%20Cadence%20Stratgey-ver1.3.pdf|https://ls.st/srd|https://community.lsst.org/t/from-opsim-v4-to-fbs-1-2/3856|https://community.lsst.org/tag/opsim|https://github.com/LSST-nonproject/sims_maf_contrib|http://astro-lsst-01.astro.washington.edu:8081|https://ls.st/Doc-26952|https://www.lsst.org/scientists/survey-design/ddf|https://github.com/LSSTScienceCollaborations/ObservingStrategy|http://ls.st/doc-28382|http://ls.st/Document-28453|https://www.lsst.org/submitted-whitepaper-2018|http://ls.st/doc-32816|https://pstn-051.lsst.io|https://www.lsst.org/content/charge-survey-cadence-optimization-committee-scoc|https://ls.st/scoc|https://www.lsst.org/sites/default/files/SCOC%20Handout.pdf|https://ls.st/cadencenotes|https://lsst-tvssc.github.io/metricshackathon2020/|https://www.lsst.org/content/survey-cadence-notes-2021|https://www.sciserver.org/about/|https://datalab.noirlab.edu/|https://www.lsstcorporation.org/,"This paper documents and frames the community-driven process used to optimize the Rubin Observatory LSST observing cadence within formal survey requirements, using large-scale survey simulations and metric-based evaluation. It describes how alternative cadences (e.g., baseline versus rolling cadence) and other strategy parameters (footprint, filter allocations, exposure definitions, visit pairing/triplets, DDF/minisurveys, dithers) are explored via the OpSim survey simulator and evaluated via the Metric Analysis Framework (MAF). The work is primarily a design-and-governance/process paper: it explains how simulation families are constructed by varying strategy parameters along axes and how standardized metrics are used to compare science performance across many domains. The evaluation relies on simulated 10-year pointing histories with realistic constraints (weather, seeing, sky brightness) and community-developed metrics, culminating in structured calls for input (COSEP, White Papers, Cadence Notes) and an optimization committee (SCOC) for final recommendations. Overall, it advances “experimental design” for survey strategy as an iterative, community-engaged, simulation-and-metrics optimization workflow rather than a classical physical DOE.",Not applicable (process/simulation-and-metrics optimization paper; no single DOE statistic or optimality criterion is formalized as an equation in the provided text excerpt).,"The paper reports that Rubin released large suites of simulated cadences for evaluation (e.g., 173 OpSim runs in releases 1.5–1.7.1 for the post–White Paper phase) and illustrates that core cadence properties (e.g., internight gap distributions) differ substantially between baseline and rolling strategies while still meeting SRD constraints. It summarizes substantial community participation outputs (COSEP with 104 unique authors; 46 White Papers with 467 unique authors; 39 Cadence Notes with 218 unique authors) and institutionalizes the optimization via the SCOC. Quantitative performance comparisons are presented mainly as illustrative metric visualizations (e.g., radar plots across science metrics) rather than a single optimized design point or numeric optimality gain.","The authors note vulnerabilities and challenges in the process: many community white papers only outlined appropriate metrics and required substantial additional effort to implement, raising concerns about resourcing for coding and evaluation. They also emphasize that community engagement is time- and labor-intensive and can be selective without participation support; the learning curve and unfunded service workload can bias contributions toward well-resourced scientists and fields.","The optimization approach, as described, lacks a unified formal objective/utility function; combining heterogeneous science metrics into a single decision criterion can be subjective and may depend heavily on committee judgment and chosen weights. Results may be sensitive to simulator fidelity (weather/seeing models, scheduler assumptions) and to the completeness/robustness of the metric suite; unmodeled operational realities could shift the preferred cadence. Reproducibility is limited if the exact simulation configurations and analysis code used for figures/decisions are not fully packaged and version-pinned in an accessible repository alongside the paper.","The paper indicates the cadence is not finalized at the time of writing and that remaining phases (fine-tuning and final recommendations) will be described in a closing Focus Issue paper. It also states that cadence optimization will continue during Rubin operations, with ongoing monitoring and periodic re-optimization supported by additional simulations and yearly evaluations to adjust strategy as needed.","A natural next step would be to formalize a multi-objective optimization framework (e.g., explicit Pareto fronts or an agreed-upon utility/weighting scheme) to make trade-offs more transparent and repeatable. Developing and releasing a fully reproducible pipeline (containerized environments plus versioned OpSim inputs and metric code) would improve auditability of cadence decisions. Robustness studies that stress-test cadence choices under alternative assumptions (e.g., different weather/seeing distributions, downtime scenarios, or evolving system performance) would clarify sensitivity and reduce decision risk.",2108.01683v2,https://arxiv.org/pdf/2108.01683v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:35:30Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (units are aggregate entities; examples include J=45 stores, J=15 simulated units; treated units constrained by m)",Service industry|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com,"The paper develops ""synthetic control designs"" for experiments where units are large aggregates (e.g., markets) and only one or a few units can be treated, making pure randomization prone to large post-randomization imbalance and bias. It formulates non-randomized design optimization problems that jointly select treated units and donor pools and compute synthetic weights to match population (or treated-group) predictors using pre-intervention outcomes and covariates, with variants including constrained cardinality, targeted ATE vs ATT tradeoffs, unit-level synthetic controls, clustered designs, and penalized objectives. Under linear factor models for potential outcomes, it derives finite-sample bias bounds and proposes inference using permutations over blank (held-out pre-treatment) periods plus post-treatment periods, enabling dependence and nonstationarity in time series. An empirical illustration using weekly sales for 45 Walmart stores shows close pre-fit and non-rejection for a placebo intervention, and simulations show synthetic control designs can substantially reduce RMSE relative to randomized assignment with common adjustments (stratification, regression, nearest-neighbor matching). The work positions synthetic controls as a design tool (not only an analysis method) for low-sample aggregate experiments, emphasizing representativeness of treated units and matchability by donor pools.","The core designs choose nonnegative weights w (treated) and v (controls) with sums to 1 and disjoint support (w_j v_j = 0), by minimizing predictor discrepancies such as (i) ||X-\sum_j w_j X_j||^2 + ||X-\sum_j v_j X_j||^2 (Eq. 6), or (ii) ||X-\sum_j w_j X_j||^2 + \beta ||\sum_j w_j X_j-\sum_j v_j X_j||^2 (Eq. 8), with optional cardinality constraints m \le ||w||_0 \le \bar m and penalization terms (Eq. 11). The resulting estimator is the synthetic difference-in-averages \hat\tau_t=\sum_j w_j^* Y_{jt}-\sum_j v_j^* Y_{jt} (Eq. 7), with unit-level controls aggregated via v_j^*=\sum_i w_i^* v_{ij}^* (Eq. 10), and inference based on permutation p-values over blank/post periods (Eq. 19) and split-conformal intervals using blank-period placebo effects (Eqs. 20–21).","In the Walmart placebo illustration (J=45 stores, T=143 weeks, T0=128, TE=100 fitting, TB=28 blank, post=15), the constrained design with m=2 achieves close pre-fit and yields a permutation p-value of 0.933 with confidence intervals covering zero throughout the post period. The same application reports markedly lower normalized post-period RMSE for synthetic control design versus randomized/stratified/matching estimators (e.g., m=2: SC 0.018 vs RND 0.312, STR 0.299, 1-NN 0.070, 5-NN 0.063). In simulations (baseline: J=15, T=30, T0=25, TE=20, post=5), the unconstrained/appropriately tuned designs show low MAE/RMSE and strong power (average p-values around 0.01–0.03 with rejection rates around 0.83–0.95 when effects are nonzero), while under the null the p-values are near 0.5 with rejection rates near 0.05. Comparative simulation RMSEs favor synthetic control design across treated-unit counts (e.g., m=1: SC 3.45 vs RND 6.35; m=5: SC 1.09 vs RND 3.22).",None stated.,"The proposed designs are non-randomized, so validity relies on strong pre-treatment fit and structural assumptions (e.g., linear factor models) and may be sensitive to misspecification, interference across aggregate units, or unmeasured time-varying confounding not captured by predictors. Practical implementation requires solving non-convex/MIQP-style optimization (cardinality/disjointness) and choosing tuning parameters (\beta, \xi, \lambda) and clustering decisions, which can materially affect results and may require additional guidance for routine use. The code link is described generically as GitHub without a specific repository URL in the provided text, which reduces reproducibility from the manuscript alone.",None stated.,"Develop principled, data-driven selection rules for tuning parameters (\beta, \xi, \lambda), treated-unit budget constraints, and clustering, with theoretical guarantees on bias/power trade-offs. Extend designs and inference to settings with unknown/estimated pre-treatment parameters, serial correlation and cross-unit dependence beyond the blank-period framework, and explicit modeling of interference/spillovers across aggregate units. Provide robust software implementations and benchmarks (including sensitivity analyses and diagnostics for extrapolation/interpolation risk) and validate on additional real-world multi-market experimentation datasets.",2108.02196v5,https://arxiv.org/pdf/2108.02196v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:36:10Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (input dimension $d$ for black-box function $f(x):\mathbb{R}^d\to\mathbb{R}$),Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This discussion paper analyzes and simplifies a sequential Bayesian optimal design framework for estimating the expectation $q=\int f(x)p(x)\,dx$ of an expensive black-box function using a Gaussian process surrogate with noisy evaluations. It shows that the information-gain acquisition originally expressed with multiple terms simplifies exactly to $G(\tilde x)=\log(\sigma_1/\sigma_2(\tilde x))$, i.e., choosing the next point is equivalent to minimizing the (hypothetical) posterior variance of the integral quantity $q$. The paper provides an alternative interpretation of the criterion as maximizing an overall correlation-weighted measure between the prospective observation at $\tilde x$ and the function over the input distribution. It further generalizes the original paper’s analytical computations (previously derived for uniform input distributions) to arbitrary input distributions by approximating $p(x)$ with a Gaussian mixture model, yielding closed-form expressions for required kernel integrals. Overall, it reframes the acquisition in a more interpretable variance-reduction form and broadens practical applicability beyond uniform $p(x)$.","The target quantity is $q=\int f(x)p(x)\,dx$ with noisy samples $y=f(x)+\varepsilon$, $\varepsilon\sim\mathcal N(0,\sigma^2)$ and GP surrogate $f\mid D_n\sim\mathrm{GP}(m_n,k_n)$. The acquisition is expected information gain $G(\tilde x)=\mathbb E_{\tilde y}[\mathrm{KL}(p(q\mid D_n,\tilde x,\tilde y)\|p(q\mid D_n))]$, which the discussion proves simplifies to $G(\tilde x)=\log(\sigma_1/\sigma_2(\tilde x))$ where $\sigma_1^2=\iint k_n(x,x')p(x)p(x')\,dx\,dx'$ and $\sigma_2^2(\tilde x)=\sigma_1^2-\frac{(\int k_n(\tilde x,x)p(x)\,dx)^2}{k_n(\tilde x,\tilde x)+\sigma^2}$. For arbitrary $p(x)$, it proposes $p(x)\approx\sum_{i=1}^{n_{\mathrm{GMM}}}\alpha_i\mathcal N(x;w_i,\Sigma_i)$ to obtain closed forms for kernel integrals like $K(x)=\int k(x,x')p(x')\,dx'$ under an RBF kernel $k(x,x')=s^2\exp\{-\tfrac12(x-x')^T\Lambda^{-1}(x-x')\}$.","The paper’s main result is an exact algebraic simplification: the multi-term form of the acquisition (as presented in the discussed paper) reduces to $G(\tilde x)=\log(\sigma_1/\sigma_2(\tilde x))$, because the additional terms cancel using the update expression for $\sigma_2^2(\tilde x)$. Consequently, maximizing information gain is equivalent to minimizing the next-step variance of the estimator of $q$, i.e., $x_{n+1}=\arg\min_{\tilde x}\sigma_2^2(\tilde x)$. It also provides a correlation-based interpretation showing that the criterion can be written in terms of the overall (input-distribution-weighted) correlation between the prospective observation at $\tilde x$ and outputs across $x$. Finally, it derives analytic computation formulas for the key kernel integrals for general $p(x)$ via Gaussian-mixture approximation, extending beyond the uniform-distribution case.",None stated.,"As a discussion note, the work focuses on algebraic simplification and analytic integral formulas rather than demonstrating end-to-end performance (e.g., ARL/efficiency curves, wall-clock scaling, or empirical comparisons) on benchmark black-box problems. The proposed generalization relies on approximating $p(x)$ with a Gaussian mixture model; accuracy and computational cost may degrade in high dimensions or for complex/multimodal $p(x)$ unless the number of mixture components grows substantially. The derivations assume a GP with specified kernel (illustrated with an RBF form) and additive Gaussian noise; robustness to model misspecification, heteroscedastic noise, and strongly nonstationary functions is not addressed. Implementation details (e.g., how to fit the GMM, choose $n_{\mathrm{GMM}}$, and maintain numerical stability in matrix inversions) are not provided.",None stated.,Empirically validating the simplified acquisition and the GMM-based analytic computations on standard Bayesian quadrature/active learning benchmarks (including high-dimensional cases) would clarify practical gains in accuracy and runtime. Studying approximation error introduced by the GMM surrogate for $p(x)$—and how it propagates to $G(\tilde x)$ and integral-variance reduction—would help guide mixture-size selection and error control. Extending the approach to non-Gaussian/heteroscedastic observation noise and to more flexible/nonstationary kernels could broaden applicability to real experimental/simulation settings. Providing open-source reference code (including stable numerical routines for $K(x)$ and $\sigma_1^2$) would improve adoption and reproducibility.,2108.03732v1,https://arxiv.org/pdf/2108.03732v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:36:38Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,E-optimal|Minimax/Maximin|Other,"Variable/General (optimizes angles, contrast SLDs, time splits, underlayer SLD/thickness; magnetic layer thicknesses)",Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/James-Durant/experimental-design,"The paper develops and applies a Fisher-information (FI) based framework to optimize neutron reflectometry experiment design under Poisson counting statistics. Experimental “factors” include measurement angles, contrast (bulk liquid) scattering length density, allocation of total counting time across conditions, and sample modifications such as underlayer SLD/thickness and magnetic layer thickness. The optimization objective is to reduce overall parameter uncertainty by maximizing the minimum eigenvalue of a scaled FI matrix (a maximin / E-optimal-style criterion), and solutions are found via constrained global optimization (differential evolution) and visualized in low-dimensional cases. The framework is demonstrated on multiple reflectometry scenarios (lipid bilayers with contrast variation and underlayers, a one-shot kinetic monolayer measurement, and a polarized magnetic thin-film problem), and improvements are validated with simulated experiments and Bayesian sampling (nested sampling). The authors provide open-source code and Jupyter notebooks to reproduce the examples and adapt the workflow to other instruments given an incident flux profile.","Reflectometry kinematics uses $Q=\frac{4\pi\sin\theta}{\lambda}$. The design criterion is based on the Fisher information matrix under Poisson counting statistics, after scaling parameters into comparable “importance” units; the scalar objective is to maximize the minimum eigenvalue of the (scaled) FI matrix (maximin), equivalently minimizing the negative of that eigenvalue in differential evolution. For magnetic-model discrimination they also use a Gaussian log-likelihood, $\ln L=-\tfrac12\sum_{i=1}^N\left(\frac{r_i-r_i^{(m)}}{\delta r_i}\right)^2+\ln\big(2\pi(\delta r_i)^2\big)$, and compare log-likelihood ratios versus counting time.","For lipid bilayers, the optimal first two solvent contrasts are the extremes (pure D2O and pure H2O), and adding a second contrast yields large reductions in uncertainty (illustrated by much tighter posteriors; the text cites median parameter error dropping from ~50% to ~10% in an example). A third contrast provides only small additional gains; silicon-matched water (SMW, ~2.07×10⁻⁶ Å⁻²) is a practical near-optimal compromise when the model is not known. Optimized angle allocation tends to favor one low angle (~0.7–0.8°) and one high angle (near the upper bound 4.0°) with most time spent at the high angle in their OFFSPEC-like simulations. Adding a single underlayer can increase the minimum-eigenvalue objective by more than an order of magnitude in the demonstrated bilayer cases, and for magnetic films the framework indicates feasibility of detecting moments as small as 0.01 μB/atom in ~20 Å layers when thickness/design is optimized, reducing required counting time compared with a sub-optimal thickness choice.","The authors note several simplifying/limiting assumptions that can affect optimal settings, including: ignoring time penalties for changing experimental conditions (e.g., changing angle/contrast), assuming backgrounds are modeled (and that the Poisson-statistics FI derivation does not directly apply to background-subtracted data), and using instrument-specific bounds and flux profiles so optima are beamline dependent. They also assume molecular volumes are invariant across measurement conditions for lipid systems, and simplify multi-contrast optimization by assuming all contrasts share the same angles and within-angle time ratio to reduce dimensionality.","The approach is model-based, so designs may be sensitive to model misspecification (wrong layering/constraints) and to uncertainty in assumed “nominal” parameter values used to compute FI; robustness to these uncertainties is not systematically characterized. The maximin/min-eigenvalue objective can overweight poorly identifiable parameter combinations without incorporating practical interpretability or explicit costs for switching conditions, and may recommend extreme angles (e.g., at bounds) that could be impractical due to footprint, background, or resolution effects beyond the simplified simulation. Comparisons focus on internal validation (simulation + nested sampling) rather than benchmarking against alternative DOE criteria (e.g., D- or I-optimality) across standardized testbeds or extensive real-beamtime case studies.","The paper states that the framework is not specific to neutron reflectometry and could be applied to any technique that can be accurately simulated with Poisson error bars, implying extension to other instruments/measurement modalities. It also highlights potential utility for more complex designs such as functionalized and magnetic underlayer systems with polarized contrasts, though these are not explored in depth here.","A valuable extension would be robust/Bayesian design that integrates prior uncertainty over model parameters and model forms (e.g., averaging FI over priors) to reduce sensitivity to assumed nominal values and to enable principled model discrimination design. Incorporating explicit operational costs/constraints (time to switch angles/contrasts, alignment overhead, footprint limits, background growth at high Q) into the objective would improve practical deployability. Additional work could provide standardized software tooling (a packaged library) and broader validation on real experimental datasets across multiple beamlines, including autocorrelated counts or non-Poisson error structures (e.g., after background subtraction).",2108.05605v2,https://arxiv.org/pdf/2108.05605v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:37:35Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a qualitative HCI/visualization design study examining creativity in data visualization practice through the lens of design fixation. The authors conduct semi-structured remote interviews with 15 professional data visualization practitioners and analyze transcripts using a hybrid inductive/deductive thematic analysis. Findings characterize how practitioners experience fixation (e.g., getting stuck on initial ideas, details, or story framing) and identify factors that encourage fixation (e.g., chart recommendations/templates in tools, focus on details, sunk cost/attachment, habits/best practices, precedent examples, and client influence). The paper also describes strategies that discourage or help overcome fixation, including incubation (breaks), experimenting/creative exercises (sketching, “10 solutions,” intentionally bad designs), seeking inspiration (including outside datavis), external feedback, and deliberate perspective shifts. It concludes with research opportunities around recommendation systems, inspiration tools, and techniques to prompt perspective shifts without increasing fixation.",Not applicable,"Not applicable (qualitative thematic findings; no DOE-style factors, optimality criteria, or quantitative experimental design results are presented).",None stated,"The study relies on a small, self-selected sample (15 practitioners) recruited via networks and online communities, which may limit generalizability across industries and experience levels. As an interview-based study, results are subject to recall bias and participants’ ability to introspect about their own fixation. The work identifies factors and strategies but does not experimentally test causal effects or compare interventions for reducing fixation. Domain-specific variation (e.g., types of visualization tasks, toolchains, organizational contexts) is not systematically controlled or stratified.","The authors identify opportunities for future research on how chart recommendations/templates can be designed to support creativity without promoting premature adherence, and how to provide inspiration without increasing fixation (e.g., curated inspiration catalogs/tools, partial visualizations/partial data as stimuli, and mashup/‘bad outcome’ generators). They also point to research on methods that help designers step back to see the big picture and support perspective shifts to overcome fixation.","Evaluate proposed anti-fixation interventions with controlled experiments or longitudinal field deployments in real design teams, measuring downstream diversity/novelty of designs and time-to-solution. Develop and validate operational metrics of “fixation” for visualization design workflows (e.g., idea diversity over iterations, switching costs between chart families) and study tool-mediated causes. Extend the work to different contexts such as collaborative/team design, high-stakes regulated domains, and AI-assisted visualization generation where recommendation bias may be stronger.",2108.06451v1,https://arxiv.org/pdf/2108.06451v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:37:51Z
TRUE,Other,Model discrimination|Prediction|Other,Not applicable,Variable/General (illustrations use S=3 SNPs and S=10 sites; empirical analyses over regions with S segregating sites),Other|Theoretical/simulation only,Exact distribution theory|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an experimental-design (DOE) framing for population genetic variation, arguing that evolution may promote population-wide genome patterns that increase the external validity (EV) of adaptations across genetic backgrounds while controlling confounding (CF). It models pairwise SNP differences as permutations (especially derangements) and connects ideal population structures to Latin-square-like designs, yielding predicted edge probabilities analogous to Erdős–Rényi–Gilbert random graphs but derived from design concepts. A generating function is introduced to quantify how many pairwise differences/intersections of each Hamming distance are needed to achieve specified EV/CF properties under random sampling, leading to approximate constants such as derangement-related probabilities near 1/e and (e−1)/e^2. Empirically, the paper tests these predictions against genome-wide SNP intersection patterns across five species (humans, Arabidopsis, Drosophila, sheep, and brown rats) using binomial-style comparisons over many genomic windows, finding observed intersection/transition patterns close to the proposed derangement-based expectations. A small simulation study illustrates how maintaining ‘derangement paths’ can stabilize hierarchical effect estimates (ANOVA-style) under missingness/imputation compared with alternative imputation schemes.","The core DOE-linked construct is a generating function for required sample sizes by confounding and external validity levels: 
$$\Theta(k,t)=\sum_{d=1}^{\infty}\Big[\sum_{i=1}^{t}\binom{t}{i}\Big(\frac{d!}{k!}\sum_{j=1}^{d}(-1)^j\frac{1}{j!}\Big)^{2-i}\Big]\,\times \mathrm{CF}^{k}\,\mathrm{EV}^{(d-k)t}.$$
From the derangement counts, the paper derives approximate success probabilities for ‘deranged’ pairwise structures such as $$p_{DR}=\frac{d!}{d!\sum_{i=1}^{d}(-1)^i/i!}\approx \frac{1}{e}$$ and a limiting form $$p_{NDR}=p_{DR}(1-1/e)\approx (e-1)/e^2,$$ which are used to predict expected SNP intersection counts via binomial/ERG-style expectations (e.g., $E[z]=\hat n\,p_{cf}$ and $E[\Delta]=S\,p_{ev}$ in their notation).","Across genome-wide analyses in five species (humans N=2504; Arabidopsis N=1135; flies N=1170; sheep N=453; brown rats N=1223), observed SNP intersection-size distributions over many genomic windows are reported to align more closely with derangement-based probabilities (near 1/e and (e−1)/e^2) than with random alternatives (including Wright–Fisher-inspired baselines). The paper reports binomial-test style 95% credible intervals over 500 randomly sampled windows (with window length tied to $M=N H_{N-1}$) and states that empirical observations diverge from alternatives (e.g., p_PRE, p_RND) while matching the proposed p_DR/p_NDR regimes. Sequential intersection increments between adjacent difference sizes (\Delta between d and d+1) rapidly plateau to near-constant levels consistent with the two predicted limits, with human data described as particularly consistent with the large-t (near p_NDR) regime. A simulation with 1000 individuals and 10 sites suggests effect estimates remain accurate under ‘random derangement-path’ imputations even when a large fraction of genomes are unobserved, compared with bottom-up or random-genome imputation.",None stated.,"The work maps population-genetic patterns to idealized DOE structures (e.g., Latin squares/derangements) and compares to simple nulls, but many realistic genomic forces (recombination, linkage disequilibrium, population structure/admixture, sequencing/variant-calling artifacts, and autocorrelation along the genome) could also affect intersection patterns and may confound attribution to the proposed EV/CF mechanism. Empirical validation relies heavily on window-based summaries and binomial-style comparisons with fixed probabilities; sensitivity to window choice (e.g., the $M=N H_{N-1}$ rule), ascertainment bias, and dependence among pairwise comparisons is not fully addressed. The paper does not provide an implementation pathway (software/code) for practitioners to reproduce the full pipeline or apply it to new datasets, limiting reproducibility and adoption.","The authors note that other relationships to Erdős–Rényi–Gilbert network models could be explored in future work, and that many consequences of the EV–CF duality remain to be studied, including genome, temporal, and phylogenic limits of the proposed patterns. They also point to the need to investigate additional evolutionary regimes (e.g., balancing selection versus selective sweeps) and broader theoretical implications of statistically grounded concepts like external validity and confounding for population structures.","A natural extension is to develop a fully specified statistical model that accounts for linkage/recombination and population structure while testing derangement/Latin-square predictions, ideally with likelihood-based or Bayesian inference rather than window-wise binomial approximations. Another direction is to operationalize the DOE analogy into concrete design/diagnostic tools: e.g., estimating ‘EV/CF indices’ with uncertainty, performing regime classification (sweep vs equilibrium vs balancing) with calibrated error rates, and validating against independent selection scans. Providing open-source software and reproducible workflows (including data preprocessing, QC, and windowing) would substantially improve reproducibility and enable broader empirical benchmarking across more taxa and sequencing platforms.",2108.06580v1,https://arxiv.org/pdf/2108.06580v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:38:31Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Not applicable|Other,Variable/General (examples include 1 design variable ξ; sensor-network measurement selection up to 14 distances),Network/cybersecurity|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops two multimodal methods to estimate expected information gain (EIG) for Bayesian optimal design of experiments when the posterior is multimodal, a setting where standard (single-mode) Laplace approximations and Laplace-based importance sampling can be biased or numerically unstable. The first method (Multimodal Laplace Approximation, MLA) approximates the posterior as a Gaussian mixture built from multiple mode-finding optimization runs with randomized initializations and local Laplace (Hessian) approximations at each discovered mode, yielding an analytic approximation to the KL divergence/EIG. The second method (Multimodal Nested Importance Sampling, MNIS) uses that Gaussian-mixture approximation as an importance-sampling proposal to remove the Laplace approximation bias when data are not in the large-sample regime. The paper provides a dimension-independent lower bound on the number of independent optimization restarts needed to capture all modes with a specified probability. Numerical studies (polynomial toy model and sensor-network localization/measurement-selection problems, including calibrated and uncalibrated noise) show MLA and MNIS can be orders of magnitude more efficient than double-loop Monte Carlo while maintaining accuracy, especially as the number of modes grows.","The EIG is defined as an expected KL divergence: $I=\int\!\int \log\frac{p(\theta\mid\bar y,\xi)}{p(\theta)}\,p(\theta\mid\bar y,\xi)\,p(\bar y\mid\xi)\,d\theta d\bar y$ (Eq. 2). For a multimodal posterior with modes $\hat\theta_k$ and local Laplace covariances $\Sigma_k$, the multimodal Laplace approximation yields (Eq. 11/14) $I\approx \mathbb E_{\theta_t,\bar y}\big[\sum_{k=1}^K (w_k\log w_k -\tfrac12 w_k\log|\Sigma_k| - w_k\log p(\hat\theta_k)) -\tfrac d2\log(2\pi)-\tfrac d2\big] + O(1/m)$. MNIS estimates EIG with an outer Monte Carlo over $(\theta_i,\bar y_i)$ and an inner importance-sampling estimate of the evidence using a Gaussian-mixture proposal $p_{gm}(\theta)=\sum_{k=1}^K w_k\,\mathcal N(\hat\theta_{ik},\Sigma_{ik})$ and weights $\beta_{ij}=p(\theta_j)/p^i_{gm}(\theta_j)$ (Eq. 18).","In the toy multimodal example, MLA shows negligible bias with as few as $M_5=10$ outer samples, while DLMC requires very large inner-loop sizes (reported at least $N_1\ge 10^4$) to reduce relative error below 10%. For the polynomial model at $\xi=1$, MLA with $n=20$ optimization restarts and $M_5=10^3$ took about 390s versus about 2000s for DLMC with $M_1=10^3$ and $N_1=10^5$ to get comparable accuracy; MNIS with $M_6=10^3$ and $N_6=10^3$ achieved similar accuracy at comparable cost to MLA. In the four-sensor localization design, MLA with $n=20$ and $M_5=10^2$ took about 74.28s, while DLMC with $M_1=10^3$ and $N_1=10^7$ was reported to take $2.25\times 10^6$s for comparable accuracy, illustrating several orders-of-magnitude savings. In the sensor-network design study, the maximum EIG increased with the number of distance measurements but exhibited diminishing returns after about 8 measurements (out of 14 possible distances).","The authors note MLA has an $O(1/m)$ approximation error term that may be non-negligible when the number of observations $m$ is not large, motivating MNIS to remove Laplace bias. They also restrict the derivations to an additive Gaussian-noise observation model for the presented Laplace-based development (while remarking Laplace ideas can extend beyond this).","The approach depends on repeatedly finding (and correctly de-duplicating) all relevant posterior modes via multi-start local optimization; in higher-dimensional or rugged posteriors, the basin-of-attraction probability $p$ may be very small, making the required number of restarts large and the method less practical. The method assumes sufficient smoothness (bounded third derivatives) and uses local quadratic approximations; strongly non-Gaussian local structure (skewness/heavy tails) can degrade mixture-Laplace fidelity and importance-sampling efficiency. Comparisons are primarily against DLMC; broader benchmarking against other multimodal evidence/EIG estimators (e.g., SMC, nested sampling, annealed IS, variational mixtures) is not provided. No implementation details (software, stopping criteria, mode-clustering thresholds) are given, which can materially affect robustness and reproducibility.",They propose applying the multimodal experimental design approach to models based on complex numerical PDEs as a future research direction.,"Developing adaptive restart strategies (e.g., sequentially increasing restarts until mode-discovery stabilizes) and principled mode-clustering/weight-normalization diagnostics would improve reliability in practice. Extending the methodology to autocorrelated/non-i.i.d. data, non-Gaussian observation models, and high-dimensional designs (including gradient-based design optimization with differentiable EIG estimates) would broaden applicability. Providing open-source implementations and benchmarking against modern multimodal Bayesian design/EIG estimators (SMC/nested sampling/thermodynamic integration/variational mixture approaches) would clarify tradeoffs and best-use regimes.",2108.07224v1,https://arxiv.org/pdf/2108.07224v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:39:14Z
TRUE,Optimal design|Sequential/adaptive|Other,Optimization|Prediction|Other,Other,"Variable/General (N layer plasma frequencies $\{\omega_p^{(i)}\}_{i=1}^N$; examples shown include N=2, 10, 60, 110 and scans over N up to ~250)",Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an automated experimental-design approach for axion dark-matter haloscope detectors using gradient descent to optimize detector properties. A 1D layered metamaterial “plasma haloscope” model is used, where the design variables are the effective plasma frequencies (and associated permittivities) of each layer subject to fixed detector length and assumed loss model. The design objective is to maximize the stored electromagnetic energy $U$ at a target axion mass/frequency, which is proportional to detectable power under critical coupling, and the optimization is performed via gradient descent with Armijo backtracking. In a proof-of-principle study at $m_a=40\,\mu$eV, the optimized periodic/structured layer configurations modestly outperform a baseline human-designed homogeneous plasma haloscope tuned to $\omega_p=\omega_a$. The work positions gradient-based optimization as a general tool to search a broader landscape of detector geometries/materials (2D/3D, boundary shapes, permeability, coupling) in future studies.","Design variables are the layer plasma frequencies $X=\{\omega_p^{(1)},\ldots,\omega_p^{(N)}\}$ entering a Drude-like permittivity per layer $\varepsilon^{(i)}=1-\dfrac{\omega_p^{(i)}}{\omega^2+i\Gamma^{(i)}\omega}$. The 1D field equation is $-\dfrac{d^2 E_k}{dx^2}+\omega^2\varepsilon(x)E_k=g_{a\gamma\gamma}\,\omega^2 a_0 B_0$ with boundary conditions at $x=0,L$. The objective is to maximize stored energy $U=\int dV\,[\partial_\omega(\omega\,\mathrm{Re}[\varepsilon])E^2+B^2]$; equivalently minimize $F[X]\equiv -U[X]$ and update designs by gradient descent $X_{n+1}=X_n-\gamma_n\nabla_X F(X_n)$ using Armijo backtracking.","For a fixed detector length $L=2.7\,$m and uniform loss choice $\Gamma=0.1\,\omega_p$, gradient descent produces non-uniform/periodic permittivity profiles that increase stored energy (power proxy) relative to the homogeneous tuned design $\omega_p=\omega_a$ from Ref. [38]. Example optimized designs are shown for $N=10,60,110$ layers at $m_a=40\,\mu$eV, with the optimized electric-field profile exhibiting enhanced structure compared to the initial near-uniform seeded design ($\omega_p^{(i)}\approx\omega_a$). A scan over number of layers shows the optimized stored energy generally increases with $N$, and for sufficiently large $N$ can slightly exceed the homogeneous baseline. (The paper reports results largely in relative/arbitrary units via plots rather than tabulated numeric ARL-style metrics.)","The study is explicitly a proof of principle and is restricted to an idealized 1D model; the authors note that extending to 2D/3D will increase computational overhead and requires more careful treatment. They also note that tunability and practical/material constraints are not fully incorporated yet, though they could be added as constraints in the optimization. Additionally, optimizing detector length $L$ naively leads to a trivial solution (increasing volume without bound), so $L$ is fixed in the presented results.","The optimization criterion (maximizing stored energy at a single target axion mass/frequency) may not translate directly to minimized scan time over a mass range, broadband performance, or robustness to model uncertainties. The method appears susceptible to local minima and dependence on initialization, but systematic multi-start or global optimization comparisons are not presented. Practical implementability is unclear because the design variables are effective plasma frequencies/permttivities per layer without demonstrating realizable metamaterial parameter ranges, fabrication tolerances, or sensitivity to losses beyond the fixed fiducial model. No reproducible implementation/benchmark implementation details (e.g., discretization, gradient computation method, numerical solver accuracy) are provided, limiting independent verification.","The authors propose extending the approach to more complex 2D and 3D detector designs, including treating boundary geometry as optimizable parameters and exploring a wider variety of materials and boundary geometries. They suggest expanding the design space to include additional material properties such as magnetic permeability $\mu$, and incorporating practical constraints (e.g., maximum layers, minimum component sizes). They also highlight optimizing antenna/readout coupling to maximize scan speed (e.g., assuming quantum-limited amplification) and using more sophisticated gradient-descent/ML algorithms for a more exhaustive landscape search that could escape local minima.","Develop a multi-objective or robust-design formulation that optimizes scan time over a frequency band, incorporates tunability constraints, and penalizes sensitivity to fabrication tolerances and loss-model uncertainty. Provide open-source reference code and standardized benchmarks (including comparisons to global optimizers such as CMA-ES, Bayesian optimization, or genetic algorithms) to quantify when gradient methods are advantageous. Extend to stochastic/uncertain axion signals and include realistic readout noise models to optimize end-to-end sensitivity rather than stored energy alone. Explore self-consistent electromagnetic solvers and adjoint-method gradients in 2D/3D to scale optimization while maintaining numerical accuracy.",2108.13894v1,https://arxiv.org/pdf/2108.13894v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:39:49Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design,Prediction|Optimization|Other,Other,Not specified,Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops scalable Bayesian uncertainty quantification and sequential experimental design methods for large-scale linear inverse problems under Gaussian process (GP) priors with linear (integral) operator observations. Its core contribution is an implicit, low-memory representation of the GP posterior covariance that supports on-the-fly access and fast sequential updates without ever forming the full (potentially larger-than-memory) covariance matrix, enabled by sequential disintegration/kriging update formulas generalized to operator data. This representation is used to compute sequential data acquisition plans for excursion set recovery (learning high-density regions) in a gravimetric inverse problem for the Stromboli volcano, extending the weighted integrated variance reduction (wIVR) criterion to inverse problems. The methodology is demonstrated via simulations and a realistic geophysics case study, showing substantial uncertainty reduction in excursion volume and competitive excursion detection performance with hundreds of sequentially chosen measurements. The work connects modern Bayesian sequential design/active learning with large inverse problems by removing key computational/memory barriers that typically prevent using posterior covariance-dependent criteria at scale.","Posterior mean/covariance under linear operator observations are given by $\tilde m_X = m_X + K_{XW}G^\top (G K_{WW} G^\top + \tau^2 I)^{-1}(y-G m_W)$ and $\tilde K_{XX'} = K_{XX'} - K_{XW}G^\top (G K_{WW} G^\top + \tau^2 I)^{-1}G K_{WX'}$. In sequential assimilation, the update is $m^{(n)} = m^{(n-1)} + \lambda_n^\top (y_n - G_n m^{(n-1)})$ and $K^{(n)}=K^{(n-1)}-\lambda_n^\top S_n \lambda_n$, with $\lambda_n(X)=S_n^{-1}G_n K^{(n-1)}_{W_n X}$ and $S_n=G_n K^{(n-1)}_{W_nW_n}G_n^\top+\tau^2 I$. The sequential design criterion is the weighted integrated variance reduction (wIVR): $\mathrm{wIVR}_n(s)=\int_D (K^{(n)}_{xx}-K^{(n+1)}_{xx}[G_s])\,p_n(x)\,dx$, discretized as a weighted variance-reduction expression involving $K^{(n)}_{XX}$ and $G_s$.","For the Stromboli case study on a 50 m 3D grid (about $2\times 10^5$ cells), the paper notes that an explicit posterior covariance would require on the order of 160 GB (32-bit) storage, motivating the implicit approach. In sequential excursion-set experiments, a myopic wIVR strategy with around 450 sequential gravimetric observations achieves roughly 70–80% true-positive excursion volume detection across several simulated ground truths, with false-positive rates typically about 5–15% (smaller excursion sets yielding fewer false positives). The authors report that wIVR can outperform a representative static design by about 10% for some ground truths and that the criterion substantially reduces posterior uncertainty on excursion volume, approaching near-minimal residual uncertainty as defined by a “limiting distribution” (all feasible surface locations measured). Computationally, the sequential/implicit approach reduces the cubic $O(p^3)$ cost associated with inverting all data at once to about $O(p^2 p_c)$ when assimilating in batches of size $p_c$, and supports incremental updates at marginal cost tied to the batch size rather than total $p$.","The authors state that the GP priors used are intended as a proof of concept and acknowledge pitfalls such as lack of positiveness of realizations and limited expressivity of the prior for the physical density field. They also note results depend on the chosen excursion-set estimator (Vorob’ev expectation) and could change with different estimators, and that only a simple (myopic) wIVR criterion/optimization is considered while more refined criteria exist but are more computationally challenging.","The sequential design objective (wIVR) is tailored to GP posterior variance reduction weighted by excursion probability; its effectiveness may degrade under model misspecification (e.g., incorrect kernel stationarity, misestimated noise, or forward-model error) common in geophysics. The design is constrained to local candidate moves (within 150 m of last location), which couples performance to path/trajectory constraints and may confound comparisons to globally optimized designs. The paper does not provide an openly available reference implementation, which can limit reproducibility and adoption given the algorithmic complexity (implicit covariance multiplication, chunking, and operator handling).","The authors propose extending to more sophisticated excursion estimators (e.g., conservative estimates) and to multivariate excursions. They also suggest moving beyond myopic optimization to finite-horizon (dynamic programming) trajectory optimization, and incorporating location-dependent observation costs (e.g., accessibility) into the design criterion. They further point to applying more refined SUR (sequential uncertainty reduction) strategies—especially those targeting excursion volume uncertainty—as a promising direction.","Developing self-tuning/robust sequential design that accounts for hyperparameter uncertainty (Bayesian treatment of $\theta$) and forward-model discrepancy could improve reliability in field settings. Extending the implicit-covariance framework to non-Gaussian priors or positivity-constrained models (e.g., log-GP, transformed GPs) would better match physical density constraints. Benchmarking against alternative large-scale GP approximations (sparse/variational, state-space, SKI) on comparable inverse/design tasks would clarify trade-offs in accuracy vs. compute. Providing optimized open-source software (e.g., GPU-accelerated chunking and operator products) and standardized test problems would materially aid reproducibility and practical uptake.",2109.03457v4,https://arxiv.org/pdf/2109.03457v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:40:34Z
TRUE,Factorial (full)|Other,Parameter estimation|Other,Not applicable,Variable/General (k treatment arms; includes 2-arm and 2×2 factorial examples),Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops a unified design-based inference framework for bounding and estimating the variance of any linear estimator under virtually any randomized experimental design. It represents many common estimators—Horvitz–Thompson, difference-in-means/contrast-of-means, Hájek, OLS, and WLS—as linear functions of observed outcomes and derives first-order Taylor linearizations whose variances can be expressed as quadratic forms involving a “first-order design matrix” that depends only on the assignment mechanism. Because exact variances are generally unidentified under the Neyman causal model, it formalizes variance bounds via bounding matrices and shows how classic bounds (a generalized Neyman bound) and the Aronow–Samii bound arise, providing a new proof of Aronow–Samii using the Gershgorin circle theorem. It proposes a general eigenvalue-based algorithm to construct identified variance bounds for arbitrary designs and shows that the resulting plug-in bound estimators reproduce Eicker–Huber–White (“robust”) and cluster-robust standard errors as special cases under appropriate designs. The work is primarily theoretical and aims to enable systematic comparison of designs and variance bounds using matrix/spectral analysis rather than proposing a new randomization scheme.","Linear estimators are written as $\hat\delta_c=c'WRy$ with an associated Taylor approximation that can be expressed as a Horvitz–Thompson form (Theorem 2.8). The key design object is the first-order design matrix $d:=\operatorname{Var}(\mathbf 1'_{kn}\pi^{-1}R)$, yielding $\operatorname{Var}(\hat\delta^{HT}_c)= z_c' d z_c$ and for Taylor approximations $\operatorname{Var}(\hat\delta^T_c)= z^T_c{}' d z^T_c$. Variance bounds replace $d$ by a bounding matrix $\tilde d$ with $\tilde d-d\succeq 0$, e.g., Aronow–Samii $\tilde d^{AS}=d+\mathbb I(d=-1)+\operatorname{diag}(\mathbb I(d=-1)\mathbf 1_{kn})$, and the bound estimator uses inverse-probability weighting: $\widehat{\operatorname{Var}}_{\tilde d}(\hat\delta)= \hat z_c' R(\tilde d/p)R\hat z_c$ where $p=\mathbb E[R\mathbf 1\mathbf 1'R]$.","The paper shows that first-order Taylor linearizations of a broad class of linear estimators can be rewritten as Horvitz–Thompson estimators for transformed outcomes, making variance analysis reducible to studying $d=\operatorname{Var}(\mathbf 1'\pi^{-1}R)$. It establishes general conditions under which Horvitz–Thompson estimators and the proposed variance-bound estimators are root-$n$ consistent (Conditions 1–3,7) and verifies these conditions for complete randomization. It proves that the Aronow–Samii bound is an identified variance bound for arbitrary (identified) designs via Gershgorin’s circle theorem and generalizes Neyman’s bound to $k$-arm settings under stated design symmetries. It provides an iterative eigen-decomposition algorithm that modifies an initial matrix into a positive-semidefinite adjustment while preserving identification constraints (entries corresponding to unobservable covariance terms). As a special case, for OLS under Bernoulli assignment, the plug-in bound estimator reduces to White’s HC0 sandwich form, and for independent cluster assignment it reduces to CR0 cluster-robust SEs.",None stated.,"The paper is primarily a theoretical unification of variance (bound) estimation; it does not provide a systematic simulation benchmark across many realistic finite-sample scenarios to quantify how conservative different bounds/algorithms are in practice. Many results rely on first-order Taylor linearization and boundedness/stability conditions; finite-sample performance when these approximations are poor (e.g., small samples, extreme propensities, high leverage covariates) is not assessed. Practical guidance on choosing among bounding matrices/algorithms (e.g., tradeoffs between tightness and computational cost) is limited, and the eigenvalue-based bounding algorithm may be computationally heavy for large $kn$ without exploiting structure/sparsity.","The paper indicates that systematic study/comparison of designs and bounds using spectral analysis (and further discussion of covariate-adjusted/generalized regression estimators) will be pursued, including in later papers in the series (notably paper 3 of 4).","Provide comprehensive simulation studies and real experimental case studies comparing the generalized Neyman, Aronow–Samii, and algorithmic bounds across common designs (complete randomization, blocking, clustering, rerandomization) and estimators (DIM/OLS/WLS) to evaluate conservativeness and coverage. Develop scalable implementations that exploit block/sparse structure of $d$ and $\tilde d$ and provide software for practitioners. Extend the framework to settings with interference, network randomization, or time-series/clustered dependence where identifying assumptions and $p$ structures become more complex, and study robustness to estimated/unknown assignment probabilities.",2109.09220v1,https://arxiv.org/pdf/2109.09220v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:41:08Z
TRUE,Factorial (full)|Factorial (fractional)|Screening|Response surface|Robust parameter design|Computer experiment|Other,Screening|Optimization|Parameter estimation|Prediction|Cost reduction|Other,Not applicable,Variable/General (examples: 4 factors in a 2^4 full factorial; 9 factors/hyperparameters in CNN tuning example),Other,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper proposes a machine-learning-assisted approach for designing pragmatic experiments in complex settings (many factors, mixed levels, and feasibility constraints) by embedding an initial constrained full-factorial candidate set into a low-dimensional latent space using a generative β-variational autoencoder (β-VAE). After learning a 2D latent representation with approximate orthogonality, isotropic normality, and factor-level disentanglement, the practitioner specifies a structured sampling pattern (e.g., square or polar grids) on the latent space; decoding these points yields a reduced-run “generated DOE” (G-DOE). The method is demonstrated on a classical 2^4 factorial and on a constrained mixed-level hyperparameter-tuning problem for a CNN (initial 7,200 full factorial trials reduced to 1,920 after constraints), showing that systematic latent-space grids produce more balanced coverage than random subsampling and can achieve similar best observed/validated objective values with far fewer runs. The paper also explores using clustering (Ward’s, k-means) and an aggregated gradient metric on decoded factor maps to guide grid placement and improve coverage/balance under constraints. Overall, it advances DOE practice by providing a pragmatic, interactive design-construction workflow for constrained, mixed-level, high-factor experiments where standard catalog designs are hard to apply directly.","The β-VAE objective is described as a reconstruction loss plus a KL-regularization term: Loss = binary_crossentropy + β·KL, where KL is the Kullback–Leibler divergence between the approximate posterior over latent variables and an isotropic standard normal prior. For grid guidance, a latent-space gradient metric is defined on a decoded high-resolution grid: \(\text{Gradient\_Metric}(x,y)=\sum_{i=1}^{q}\sum_{j=1}^{2}\left|\frac{\Delta F_i(x,y)}{\Delta L_j}\right|\), where \(F_i\) is the decoded (normalized to [0,1]) level of factor i and \(L_j\) are the uniformed latent axes.","On the constrained CNN hyperparameter example, the initial full factorial candidate set has 7,200 trials and becomes 1,920 after applying two feasibility constraints; pragmatic designs were generated with square grid N=64 (≈30× reduction) and polar grid N=40 (≈48× reduction), plus random N=64 baselines. Reported best/validated LCL accuracy metrics are close across systematic designs and the benchmark full design (e.g., Initial Full validated 98.6%, Square Grid validated 98.4%, Polar Grid validated 98.8%), while some random designs show larger interpolation-to-validation discrepancies (e.g., validated 97.8% and 97.9% for two random replicates). Decoding a 100×100 latent grid shows constraints largely preserved in generation: only 6 out of 10,000 decoded points violated the imposed constraints. Clustering-based grids (N=64 centroids) were slightly worse than the square grid in the example (Ward’s clustering LCL ≈98.4% vs square grid ≈98.5%).","The paper notes that visualization/interpretability is limited to 2D latent-space representations. It emphasizes that β-VAE fitting is stochastic, with representations varying from run to run, so hyperparameter tuning or replicated runs may be needed. It also cautions that decoded pragmatic designs may fail desired properties (e.g., balance/orthogonality/constraints/uniqueness) and therefore require interactive checking and grid modification; additionally, for small-run designs, supervised models used for factor-importance assessment risk overfitting.","The approach does not provide DOE optimality guarantees (e.g., D-/I-optimality) for the decoded grids, and the resulting G-DOE can be non-orthogonal/unbalanced unless iteratively adjusted, so performance may depend strongly on subjective grid choices and rotations. The method’s behavior under typical industrial complications (measurement noise models, randomization/blocking, nuisance factors, correlated errors, missing runs, and practical run-order constraints) is not developed, even though it targets “complex experiments.” Results are demonstrated on limited examples (one classical factorial and one CNN tuning case), and it is unclear how sensitive conclusions are to β-VAE architecture, preprocessing (one-hot/log transforms), and rounding of discrete factors. No reference implementation is shared, which limits reproducibility and practitioner adoption.","The paper suggests that some steps require additional validation, such as whether duplicating the DOE matrix is unnecessary for designs with many factor levels, and implies extending analysis beyond the presented settings (e.g., exploring different grids and tuning strategies). It also notes the possibility of using alternative interpolation methods (polynomial, splines) for mapping response surfaces on the latent grid, beyond the conservative linear interpolation used in the examples.","Developing formal criteria and automated algorithms to choose/optimize latent-space grids (and rotations) with explicit objectives (e.g., prediction variance, screening power, constraint satisfaction) would make G-DOE less ad hoc and more comparable to optimal-design methods. Extending the framework to handle blocking/split-plot/run-order constraints and correlated or nonstationary responses would better match real experimental operations. A rigorous benchmarking study against modern constrained/optimal mixed-level design generators (including Bayesian/space-filling and discrete D-optimal methods) would clarify when β-VAE-based generation is superior. Providing open-source software and guidance on hyperparameter robustness, discrete-factor handling, and uncertainty quantification for decoded trials would materially improve reproducibility and adoption.",2110.01458v2,https://arxiv.org/pdf/2110.01458v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:41:48Z
FALSE,Other,Other,Not applicable,Not specified,Transportation/logistics|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper proposes a human-subjects experimental study to examine how automated-vehicle (AV) explanation timing affects two distinct trust constructs: cognitive (rational) trust and affective (emotional) trust. The planned methodology is a within-subjects design with three conditions: no explanation, explanation before the AV acts, and explanation after the AV acts. Condition order is to be counterbalanced using a Latin square, and each condition includes multiple unexpected driving events to provide realistic scenarios. Trust outcomes are to be measured via 7-point Likert items adapted from McAllister (1995), with a pilot study and PCA planned for instrument validation. The manuscript is a study design/proposal in HCI/human factors rather than a Design of Experiments (DOE) methods paper; it does not develop new DOE theory or optimal design methods.",Not applicable,NA,None stated,"As presented, the study is a proposal and reports no empirical results, so effectiveness claims about explanation timing on cognitive vs affective trust remain unvalidated. The design appears to confound (or at least not fully specify) how multiple “unexpected events” within each condition are analyzed (e.g., aggregation vs modeling event-level effects), which can introduce learning/carryover and context effects in within-subjects comparisons. Measurement validity may be limited if McAllister (1995) items are adapted to AVs without confirmatory validation (e.g., CFA/measurement invariance) beyond PCA, and common-method bias is likely given self-report trust outcomes in an online survey. External validity may be limited because scenarios are described rather than experienced in a simulator/real vehicle, and participants’ prior AV familiarity may moderate effects but is not clearly modeled as a factor.","The authors suggest that the expected results would direct future research to explore factors that lead to different types of trust—i.e., factors influencing rational assessments that drive cognitive trust versus factors that inspire emotional investment that drive affective trust.","Follow-on studies could test the same manipulation in higher-fidelity settings (driving simulator, VR, or field studies) and under different operational design domains (urban/highway/rural) while explicitly modeling event-level variability. Extending to mixed-effects models that account for repeated measures, order/carryover, and participant covariates (e.g., AV experience, risk propensity) would strengthen inference. Future work could also evaluate behavioral/physiological trust proxies (takeover behavior, eye tracking, workload) alongside self-report, and examine additional explanation dimensions (content, modality, uncertainty communication) beyond timing.",2110.03433v1,https://arxiv.org/pdf/2110.03433v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:42:06Z
TRUE,Response surface|Optimal design|Other,Prediction|Parameter estimation|Other,Not applicable,"Variable/General (examples include 2, 4, and 9+ factors; ANSYS table lists up to 12 variables)",Transportation/logistics|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper proposes a Response Surface Single-Loop (RSSL) approach for reliability-based design optimization (RBDO) that replaces the traditional MPP/FORM inner-loop reliability analysis with closed-form higher-order probability-of-failure formulas for quadratic limit-state models. Deterministic constraints are represented (or locally approximated) by general quadratic response surface models, and probabilistic constraints are computed directly from these quadratic forms without searching for a most probable point. The method leverages standard DOE for fitting quadratic response surfaces, emphasizing Box–Behnken designs (BBD) and (when needed) central composite designs (CCD), as commonly implemented in ANSYS for quadratic regression metamodels. RSSL is demonstrated on benchmark problems and a vehicle crashworthiness application, showing improved constraint satisfaction versus FORM-based single-loop approaches, and far fewer function evaluations than double-loop RBDO (PMA/RIA). Practical guidance includes selecting DOE region size in standardized space (e.g., scaling with target reliability index and a constant factor) and handling constant or mean-proportional (varying) standard deviations of design variables.","Deterministic constraints are modeled by a quadratic response surface $Q(z)=z^T A z + k^T z + c$ (with $z=[d^T\;x^T\;p^T]^T$), and transformed to standard normal space leading to a quadratic form $z_N^T A' z_N + k'^T z_N + c'$. The probabilistic constraint is evaluated as $P_f=\Pr[z_N^T A' z_N + k'^T z_N + c' < 0]$ using closed-form higher-order probability expressions (from Mansour & Olsson 2014) depending on the eigenvalue signs of $A'$, avoiding MPP search. DOE region sizing is tied to the target reliability index (e.g., using $z_N\in\pm C_R\,\beta_d$ and mapped back to original space for sampling).","For the 2-variable mathematical benchmark, RSSL used 24 total function evaluations (15 to solve the deterministic problem + 9 DOE runs to fit quadratic models) and achieved Monte Carlo reliability close to the target (e.g., $\beta_{MC2}\approx 3.00$), outperforming competing methods in constraint satisfaction at comparable or lower objective. In a 4-variable quadratic test problem, RSSL produced solutions closer to the true optimum than FORM-based methods, especially at tighter failure probabilities (e.g., $P_{f,all}=0.135\%$), with much smaller constraint violations in Monte Carlo checks. In the vehicle crashworthiness case (10 constraints, 7 design variables), RSSL achieved feasibility where several FORM-based single-loop/decoupled methods violated key constraints, and required far fewer function evaluations than double-loop PMA/RIA (e.g., 41 evaluations for RSSL vs. thousands for double-loop methods for $r_d=0.9$ as reported).","The authors note that fitting quadratic response surface models around the deterministic solution can introduce errors if active-constraint MPPs lie far from the deterministic solution or if constraints are highly nonlinear between these regions. They suggest that, in such cases, a surrogate-model update (sequential/iterative refitting) could be used, even though the presented RSSL is positioned as a single-loop method. They also state that achieving higher accuracy for critical problems may require a sequential MPP-based approach with a trust-region/zoom-pan strategy and analytical sensitivities for the higher-order failure probability.","DOE choices (BBD/CCD) and region-sizing heuristics (e.g., fixed $C_R$ values) may not be robust across strongly anisotropic, constrained, or highly correlated design spaces; performance could depend materially on these tuning parameters. The approach is fundamentally tied to quadratic surrogate adequacy; when the underlying response is non-quadratic, error control and model validation (e.g., cross-validation, lack-of-fit tests, sequential enrichment) are not developed in depth. Comparisons focus on selected RBDO competitors and benchmarks; broader evaluation against modern adaptive surrogate RBDO (e.g., active learning kriging with reliability bounds) is not provided, and practical software implementation details (automation, diagnostics) are limited.",The authors suggest developing an alternative sequential MPP-based approach with a trust-region (zoom-pan) strategy and analytical sensitivities for the higher-order probability-of-failure expressions to further increase accuracy when needed. They also point to using surrogate-model update schemes (sequential refitting) for cases where the deterministic-solution-centered quadratic approximation is insufficient.,"A natural extension is a fully adaptive DOE strategy (sequential design) that enriches samples near estimated probabilistic constraint boundaries, with explicit surrogate error metrics and stopping rules. Another direction is integrating robust model selection/validation for the quadratic response surfaces (e.g., lack-of-fit tests, regularization, and diagnostic plots) and extending the framework to non-quadratic or piecewise/quasi-quadratic surrogates. Providing open-source implementations (e.g., MATLAB/Python toolboxes) and benchmarking on standardized RBDO test suites with correlated non-normal variables would improve reproducibility and adoption.",2110.04898v1,https://arxiv.org/pdf/2110.04898v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:42:41Z
TRUE,Sequential/adaptive|Bayesian design|Computer experiment|Other,Parameter estimation|Optimization|Prediction|Other,Not applicable,"Variable/General (design variable is the experiment/functional; examples include 2D input locations and line-integral parameters; Bayesian optimisation example uses 2 parameters α,β with gradient data)",Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/MatthewAlexanderFisher/GaussED,"The paper introduces GaussED, a probabilistic programming language coupled with an automated sequential experimental design (SED) engine for Gaussian process (GP) models with general (possibly nonlinear) quantities of interest and continuous linear functional observations (e.g., function values, gradients, Laplacians, and integrals). Instead of requiring users to hand-design an acquisition function, GaussED adopts a Bayesian decision-theoretic approach: users specify only a quantity of interest and a loss function, and the system constructs a Bayes-risk acquisition objective. To make optimisation of this acquisition objective practical, the backend uses a finite-dimensional spectral GP approximation and a reparameterisation trick, enabling stochastic gradient-based optimisation over the design space; GP hyperparameters are updated online via maximum likelihood with automatic differentiation. Demonstrations include probabilistic solution of PDEs via Laplacian observations, adaptive tomographic reconstruction from line integrals with a nonlinear output warp, and gradient-enhanced Bayesian optimisation for maximum-likelihood parameter inference in a Lotka–Volterra model. The work advances SED/PPL tooling by unifying several bespoke pipelines under a single automated framework for GP-based sequential design with linear-functional data.","SED chooses the next experiment as $\delta_n \in \arg\max_{\delta\in\mathcal D} A(\delta;P_f,\delta_{n-1}(f))$. GaussED’s default acquisition is a loss-based Bayes-risk utility leading to the nested-expectation form $A(\delta;P_f,\delta_{n-1}(f)) = -\tfrac12\iint L(g,g')\,dP_f(g'\mid \delta_{n-1}(f),\delta(g))\,dP_f(g\mid \delta_{n-1}(f))$, typically with squared-error loss on a quantity of interest $L(f,g)=\|q(f)-q(g)\|^2$. The backend uses a spectral GP approximation $f(\cdot)=\mu+\sum_{i=1}^m \omega_i\phi_i(\cdot)$ with independent $\omega_i\sim\mathcal N(0,s(\sqrt{\lambda_i}))$ to enable a reparameterisation trick for stochastic gradient optimisation over design parameters.","On the Poisson PDE example on $[-1,1]^2$ with Laplacian observations, GaussED produces space-filling designs and empirically achieves the optimal fill-distance rate with slope $-\tfrac12$ (shown in Fig. 2c) over $n=150$ SED iterations. For tomographic reconstruction from batches of 9 parallel line integrals, SED with a nonlinear quantity of interest $q(f)=\exp(3f)$ yields visibly improved reconstructions compared with a random design at the same budget (e.g., $n=270$), while using linear $q(f)=f$ reverts to a space-filling design. In gradient-based Bayesian optimisation for Lotka–Volterra likelihood maximisation, the method outperforms first-order gradient ascent and L-BFGS in terms of best likelihood reached under a small evaluation budget (plots in Fig. 4c–d). Reported computational budgets include ~9 CPU hours for the PDE run (150 iterations), ~2.5 CPU hours for tomography, and ~1.5 CPU hours for the Bayesian optimisation demo, using spectral basis sizes around $m\approx 28^2$–$35^2$ depending on the example.","GaussED is restricted to Gaussian process models and to observations that are continuous linear functionals, which limits modelling flexibility compared with more general probabilistic programming or emulation toolkits. The authors also note that automating acquisition construction (removing bespoke, task-specific acquisition functions) may incur a performance loss relative to specialised Bayesian optimisation software, though their experiments suggest any gap may be modest.","The approach assumes an accurately specified GP prior class; online maximum-likelihood hyperparameter fitting can be brittle and overconfident in practice, especially under model misspecification or when the spectral truncation $m$ is too small, and the paper does not deeply quantify robustness to these issues beyond appendices. Many demonstrations rely on noise-free (exact) observations; while the paper mentions that Gaussian noise can be folded into the covariance, performance and stability under realistic observation noise and numerical error are not comprehensively benchmarked. The acquisition optimisation is nonconvex and handled by stochastic optimisation with heuristic initialisation; global optimality is not guaranteed and sensitivity to random seeds/optimisers suggests potential variability in practice. Comparisons are illustrative and limited; broader benchmarking against established active learning/SED baselines (e.g., mutual information lower bounds, integrated variance reduction variants, or modern BO packages with gradients) would clarify where GaussED is competitive.","The paper positions GaussED as an off-the-shelf benchmark for SED against which more sophisticated or bespoke methods can be compared, and highlights that its applicability is limited by the restriction to GPs and continuous linear functional data. It suggests that removing the need for hand-specified acquisition functions trades off some performance versus specialised methods, implying further work could investigate and reduce any such performance gap while retaining automation.","Extending GaussED beyond GPs (e.g., to Bayesian neural operators or deep kernel learning) and beyond continuous linear functionals (e.g., nonlinear or inequality observations) would broaden practical applicability. Developing principled treatments for noisy/approximate functionals (including numerical PDE/ODE solver error) and self-starting/fully Bayesian hyperparameter learning (instead of repeated MLE) would improve robustness. Adding theoretical or empirical guarantees for acquisition optimisation (multi-start strategies, global optimisers, or trust-region methods) and providing calibrated uncertainty about $q(f)$ under truncation error from finite $m$ would strengthen reliability. Finally, packaging standardized benchmarks and metrics across domains (PDEs, tomography, BO with derivatives) and releasing reproducible scripts would enable fairer comparisons to the growing SED literature.",2110.08072v1,https://arxiv.org/pdf/2110.08072v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:43:26Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Screening|Cost reduction|Other,Other,"Variable/General (up to ~20,000 possible genetic interventions; treatment descriptors q=420/799/808; covariates p=0 in curated assays)",Pharmaceutical|Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://www.biorxiv.org/content/early/2019/07/31/720243|https://www.biorxiv.org/content/early/2021/05/12/2021.05.11.443701|https://www.sciencedirect.com/science/article/pii/S0092867418313333|https://www.frontiersin.org/article/10.3389/fimmu.2019.02879,"This paper introduces GeneDisco, a benchmark suite for batch active learning aimed at experimental design in early-stage drug discovery for in vitro genetic perturbation experiments (e.g., genome-wide CRISPR knockout/CRISPRi). The benchmark curates four real interventional assay datasets (T-cell proliferation, IL-2 production, IFN-γ production, and K562 sensitivity to NK cells) and standardizes three sets of treatment descriptors (Achilles dependency profiles, STRING PPI network embeddings, and CCLE proteomics), enabling acquisition of batches of interventions from a finite pool. It provides implementations of multiple batch acquisition policies (e.g., random, k-means/core-set diversity, BADGE, BALD/top-uncertain and soft-uncertain, margin-style, and adversarial BIM variants) and evaluates them with counterfactual predictors (BNN and random forest) under iterative acquisition cycles. Performance is assessed primarily by predictive accuracy (test MSE of counterfactual outcome prediction) and a “hit ratio” measuring how quickly methods recover top-5% high-absolute-effect genes. Across assays and batch sizes, the results highlight trade-offs between diversity-driven acquisition improving model MSE and uncertainty-driven acquisition improving hit discovery, and show that rankings depend on batch size, descriptor choice, and model class.","The experimental-design component is formalized as batch active learning on a finite intervention pool: at cycle k an acquisition function selects a batch $D_k=\alpha(g_{k-1},D^{k}_{\text{avail}})$ of size $b$ from remaining interventions. Example acquisitions include Random sampling (Eq. 1), diversity/core-set style selection based on distances (Eq. 8/14), and information-theoretic uncertainty via BALD with mutual information $I(Y;\Omega\mid t, D^{k-1}_{\text{cum}})$ (Eqs. 3–7), estimated from an ensemble/posterior over predictors (BNN/forest). The learning objective used for benchmark optimization is counterfactual prediction error, $L_{\text{MSE}}=\text{MSE}(y_t,\hat y_t)$, and “hits” are defined as the top 5% of genes by $|y_t|$ discovered over cycles.","The paper reports large-scale baseline evaluations over 4 assays, 9 acquisition functions, and multiple batch sizes (16–512), repeating runs over 5 random seeds and consuming >20,000 CPU-hours. It finds that method rankings depend strongly on batch size and descriptor/model choice, with diversity-focused, model-independent policies (e.g., random, kmeansdata) often yielding better MSE improvements in smaller-batch regimes, while uncertainty-aware methods can improve discovery of high-effect “hit” genes. The authors observe a consistent trade-off: policies that perform well on predictive MSE can underperform on hit ratio, and vice versa, suggesting multi-objective considerations for practical experimental exploration. No single acquisition strategy dominates across all datasets and settings.","The authors note that the ranking of acquisition functions depends on confounding factors such as evaluation metric, dataset characteristics, and model/hyperparameter choices, so results may not extrapolate to significantly different experimental conditions. They state that GeneDisco assumes an initial labeled set sufficiently representative for training/validation across active-learning cycles, which may be challenging early when data are scarce. They also acknowledge that interventional biological experiments are noisy and that GeneDisco does not model budget trade-offs between collecting more interventions versus running technical/biological replicates to mitigate label noise.","The benchmark uses fixed train/test splits and evaluates acquisition in an offline “pool-based” setting on existing CRISPR screens, which may not capture real lab constraints (plate layout, batching constraints, reagent availability, toxicity/viability failures) that affect feasible designs. Outcomes are treated as single continuous targets per assay and the acquisition objectives are largely heuristic (MSE and top-|y| hits), which may not align with downstream decision-making (e.g., causal validation, multi-phenotype desirability, safety/druggability constraints). Dependence on specific treatment descriptors (Achilles/STRING/CCLE) may bias conclusions, and the robustness of acquisition policies to distribution shift across cell types/donors is not directly evaluated (notably p=0 for curated assays).",The authors state an intention to expand GeneDisco to enable multi-modal learning and to support simultaneous optimization across multiple output phenotypes of interest. They also suggest that the observed trade-off between model improvement and hit discovery warrants research into methods that manage this trade-off to maximize long-term discovery rates under noisy experimental conditions.,"A natural extension is to incorporate explicit experimental-cost and constraint models (e.g., plate/parallelization constraints, replicates vs. exploration, failed interventions) and to evaluate economically optimal or constrained batch designs. It would be valuable to benchmark robustness under realistic noise, unknown/estimated controls, and distribution shift (different donors/cell lines) and to add multi-task/multi-output acquisition strategies (Pareto or constrained optimization) aligned with druggability and safety criteria. Providing reference implementations as a maintained package and adding standardized baselines for Bayesian optimization/Thompson sampling or causal-utility objectives (beyond MSE) would improve comparability and practical uptake.",2110.11875v1,https://arxiv.org/pdf/2110.11875v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:44:13Z
TRUE,Response surface|Sequential/adaptive|Computer experiment|Other,Optimization|Prediction|Robustness|Cost reduction|Other,Space-filling|Not applicable,"Variable/General (examples use k=10 fisheries; decision variables are patrol allocations across k fisheries; additional DOE used over (F_{other}, P_B,i, P_R,i) and over (P_B, P_R))",Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a response-surface-methodology (RSM) sampling/optimization approach to analyze a large, computationally expensive two-player fisheries patrol-allocation game motivated by the South China Sea fishing dispute. The key methodological contribution is an iterative, two-stage RSM algorithm that (i) generates initial design points using a modified Latin hypercube plus heuristic-generated points on a simplex (budget-constrained patrol allocations), (ii) fits a surrogate (boosted regression trees) to the ‘robust utility’ objective, and (iii) adaptively selects new points by optimizing the surrogate under randomized additional constraints to force exploration. The game incorporates a robust (worst-case) behavioral-uncertainty model where the adversary may deviate from the model-defined optimum within an epsilon suboptimality band, and the algorithm must solve nested expensive optimizations (using derivative-free COBYLA with random restarts). Empirically, across 1000 randomized parameter instantiations (k=10 fisheries, epsilon=10%), the robust solution often yields materially higher worst-case (robust) utility than a non-robust benchmark that assumes perfect best-response play; an illustrated case shows ~27% improvement in robust utility with <1% loss under non-robust conditions. The work advances DOE/RSM usage in game-theoretic computational settings by combining space-filling initial designs, surrogate modeling, and adaptive sampling tailored to simplex-constrained decision variables and robustness considerations.","The core decision problem is a sequential robust optimization: $P_B^* = \arg\max_{P_B}\, \pi_B(P_B, P_R')$ subject to (i) $P_R^*=\arg\max_{P_R}\pi_R(P_B,P_R)$ and (ii) $P_R' = \arg\min_{P_R}\pi_B(P_B,P_R)$ with patrol-budget constraints $\sum_i P_{B,i}=P_{B,t}$, $\sum_i P_{R,i}=P_{R,t}$ and an epsilon-suboptimality constraint on Red: $\pi_R(P_B,P_R^*)/\pi_R(P_B,P_R') - 1 < \varepsilon$. The adaptive DOE/RSM step selects the next sample by solving $P_B^{\text{sample}}=\arg\max_{P_B}\hat\pi_B^{\text{rob}}(P_B)$ with the simplex constraint and a randomized additional bound on one component (e.g., $P_{B,i}/P_{B,t}<\alpha$) to encourage exploration. Initial sampling includes a modified Latin hypercube over $[0,1]^k$ followed by rescaling to satisfy the simplex (budget) constraint.","Across 1000 randomized examples (with k=10 fisheries and $\varepsilon=10\%$), the robust solution frequently improves robust utility over the non-robust benchmark; the paper reports the 95th percentile improvement $v=\pi_B^{\text{rob}}(P_B^*)/\pi_B^{\text{rob}}(P_B^{NR})-1$ as 17.52% and a penalized metric $w$ (accounting for possible loss when Red plays the model-defined optimum) with 95th percentile 9.62%. In a highlighted Example 1, robust utility increases from $\$130,857,172.92 (non-robust solution evaluated robustly) to $\$166,663,708.36 (robust solution), a 27.36% gain. Under the non-robust assumption (Red plays the model-defined best response), utilities are $\$196,247,780.94 (robust solution) vs. $\$197,344,455.69 (non-robust solution), i.e., <1% difference. The optimization subproblems can be expensive: solving Red’s best-response for k=10 averages >10 minutes on a laptop, and for k=60 can exceed 2 hours, motivating the surrogate-based DOE strategy.","The author notes the work does not derive traditional RSM theoretical guarantees such as convergence, arguing that sufficiently complex problems are better handled with complex response surfaces that may not admit such analysis. The paper is described as experimental, using multiple randomized parameter instantiations rather than an empirically calibrated South China Sea model. It also assumes fisheries are dispersed enough that each patrol vessel can be allocated to one and only one fishery at a time, and acknowledges that the true effect of patrols on costs in disputed fisheries remains an open empirical question.","The DOE/RSM procedure’s performance is demonstrated mostly via synthetic parameter draws and one motivating scenario; there is limited evidence of robustness to alternative surrogate choices, hyperparameter settings, or different initial-design sizes beyond the reported configuration. The adaptive sampling constraint (randomly bounding one component) is heuristic; there is no comparison to other exploration strategies (e.g., expected improvement/UCB Bayesian optimization on the simplex, trust-region methods) or sensitivity analysis showing how it affects finding global optima. Computational reproducibility is constrained because implementation details (software stack, data-generation scripts) are not provided, and the nested optimizations plus heuristic-generated initial points may be hard to replicate exactly. Finally, the ‘computer experiment’ is on a constrained simplex; the paper does not discuss design diagnostics (space-filling quality after rescaling) or potential bias introduced by rescaling LHS points to satisfy the sum constraint.","The paper suggests extensive experiments across a wider variety of problems to identify which classes of response surfaces (e.g., tree-based vs. others) perform best for different game/optimization features, especially where optimal behavior changes discontinuously. It also proposes operationalizing the methodology for the real South China Sea by engaging fisheries experts to build a more realistic model and parameterization. Additionally, it calls for empirical research to estimate how patrols impose costs on fishermen in disputed fisheries (as existing studies focus on illegal fishing in undisputed EEZs).","Developing a principled adaptive-design framework on the simplex—e.g., Bayesian optimization with acquisition functions and constraint handling—could replace the randomized single-coordinate bound and provide clearer exploration–exploitation control and stopping criteria. Providing an open-source implementation (and possibly a benchmark suite of instances) would enable replication and facilitate comparisons against alternative DOE/surrogate-optimization methods (Gaussian processes, random forests, neural surrogates, trust-region surrogate optimization). Extending the approach to autocorrelated/temporal dynamics (multi-period patrol decisions) or multivariate/many-player games would improve applicability to real maritime settings. Finally, adding uncertainty quantification around surrogate predictions (and propagating it into decision-making) would better align the method with robust decision analysis.",2110.12568v2,https://arxiv.org/pdf/2110.12568v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:44:58Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,Bayesian D-optimal|Other,"Variable/General (design dimension $N_d$; examples include $N_d=1$ for scalar design $d_k\in[0.1,3]$ and $N_d=2$ for sensor displacement $d_k\in[-0.25,0.25]^2$)",Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/wgshen/sOED,"The paper develops a Bayesian sequential optimal experimental design (sOED) framework for a finite number of experiments by formulating sOED as a finite-horizon POMDP with continuous states/actions and information-theoretic utilities. It proposes a computational solution using policy-gradient reinforcement learning (actor–critic) with deep neural networks to parameterize both the deterministic design policy and the value/Q functions, and derives/proves the finite-horizon policy-gradient expression along with a Monte Carlo estimator. The design utility is primarily expected information gain, operationalized via KL divergence from prior to posterior (equivalently expressed as terminal or incremental information gain), enabling Bayesian D-optimal-style criteria for nonlinear models. The method is validated on a linear-Gaussian benchmark (matching analytic optimal performance) and demonstrated on a contaminant source inversion problem governed by convection–diffusion dynamics, showing advantages over batch (static) and greedy (myopic) designs. The authors release implementation code publicly on GitHub.","The Bayesian update uses Bayes’ rule $p(\theta\mid d_k,y_k,I_k)=\frac{p(y_k\mid\theta,d_k,I_k)p(\theta\mid I_k)}{p(y_k\mid d_k,I_k)}$. The sOED objective is to find a policy $\pi=\{\mu_k\}$ maximizing expected total utility $U(\pi)=\mathbb{E}[\sum_{k=0}^{N-1} g_k(x_k,d_k,y_k)+g_N(x_N)]$, with information-gain rewards using KL divergence either as a terminal reward $g_N=D_{KL}(p(\cdot\mid I_N)\Vert p(\cdot\mid I_0))$ or incrementally $g_k=D_{KL}(p(\cdot\mid I_{k+1})\Vert p(\cdot\mid I_k))$ (proved equivalent). The key methodological result is the deterministic policy-gradient for finite-horizon sOED: $\nabla_w U(w)=\sum_{k=0}^{N-1} \mathbb{E}[\nabla_w \mu_k(x_k)\, \nabla_{d_k} Q_k^{\pi}(x_k,d_k)\vert_{d_k=\mu_k(x_k)}]$, estimated via Monte Carlo episodes.","On a linear-Gaussian $N=2$ benchmark, both ADP-sOED and PG-sOED achieve mean total reward about $0.775\pm0.006$ (via $10^4$ simulated episodes), close to the analytic optimum $U(\pi^*)\approx0.783$. PG-sOED shows large computational advantages over ADP-sOED: reported training time 24 s vs 837 s, and testing time 4 s vs 24,396 s (for the stated episode counts), with far fewer forward-model evaluations ($3.1\times10^6$ vs $5.3\times10^8$). In contaminant source inversion examples, PG-sOED yields higher mean total reward than greedy and batch baselines (e.g., case 1: $0.615\pm0.007$ vs greedy $0.552\pm0.005$; case 2: $1.344\pm0.008$ vs greedy $1.178\pm0.010$ and batch $1.264\pm0.007$; case 3: $3.435\pm0.016$ vs greedy $3.057\pm0.015$ and batch $2.856\pm0.012$).","The authors state that the main limitation is scalability to high-dimensional settings: estimating the KL-based terminal reward requires Bayesian inference, and their demonstrated implementation uses grid-based posterior evaluation that suffers from exponential growth with parameter dimension. They note that higher-dimensional problems would need alternative inference/estimation approaches (e.g., MCMC, variational inference, likelihood-free ratio estimation, transport maps) and possibly dimension reduction.","Performance and stability likely depend on reinforcement-learning hyperparameters (network architecture, exploration noise schedule, learning rates, episode count) and could be sensitive across problems, yet systematic tuning/robustness studies are limited. Comparisons are primarily against batch/greedy and one ADP-sOED baseline; broader benchmarking against other modern Bayesian OED estimators (e.g., variational EIG bounds, nested Monte Carlo/MLMC EIG estimators, gradient-based Laplace/adjoint OED) is not extensive. The approach assumes an accurate likelihood/forward model (or surrogate) and i.i.d. noise structure as specified; model misspecification or correlated/heteroscedastic errors beyond their form may degrade performance.","They propose improving scalability for high-dimensional inference/utility estimation beyond gridding, via methods such as MCMC, variational inference, approximate Bayesian computation, transport maps, and dimension reduction. They also suggest exploring more advanced RL techniques (e.g., replay buffers, off-policy/target networks, parameter sharing) to improve sample efficiency and convergence. Finally, they point to adopting richer utility measures capturing goal-oriented design, robustness, and risk.","Developing end-to-end differentiable estimators of expected information gain (e.g., variational lower bounds, amortized ratio estimation) could reduce reliance on expensive posterior computations and improve scaling. Extending the framework to constrained/unsafe experimentation (chance constraints, risk-sensitive RL) and to partially observed/controlled physical-state dynamics with real-time constraints would broaden applicability. Providing a reusable software package with standardized benchmarks and ablation studies (exploration strategy, belief representation choice, surrogate error impact) would strengthen reproducibility and practitioner adoption.",2110.15335v2,https://arxiv.org/pdf/2110.15335v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:45:40Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Model discrimination,Other,"Variable/General (design vector is reward probabilities per arm per block; examples shown: 3 arms with 2 blocks for MD and 3 blocks for PE, i.e., 6 or 9 design parameters)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a Bayesian optimal experimental design (BOED) approach for simulator-based (likelihood-intractable) cognitive models by combining mutual-information-based utilities with neural estimation (MINEBED). Experimental designs are optimized to maximize expected information gain about a variable of interest, targeting both model discrimination (model indicator) and parameter estimation (model parameters) in multi-armed bandit tasks. A bespoke neural architecture learns approximate sufficient summary statistics per task block and amortized posteriors, while Bayesian optimization (Gaussian process surrogate with Matérn-5/2 kernel and Expected Improvement) handles gradient-free design optimization for discrete behavioral data. In simulation studies with three-armed bandits (30 trials per block), the resulting optimal reward-probability designs are often extreme (e.g., [0,0,0.6] then [1,1,0] for model discrimination) and substantially improve model recovery (confusion matrices) and sharpen posterior distributions relative to baseline designs sampled from Beta(2,2). The work extends prior BOED in cognitive science from tractable-likelihood models to realistic simulator models using modern ML-based MI lower bounds and amortized inference.","The experimental design utility is mutual information (expected information gain): $U(d)=\mathrm{MI}(v;y\mid d)=\mathbb{E}_{p(y\mid v,d)p(v)}\left[\log \frac{p(v\mid y,d)}{p(v)}\right]$. For implicit models they maximize a tractable NWJ lower bound parameterized by a neural network $T_\psi(v,y)$: $U(d;\psi)=\mathbb{E}_{p(y\mid v,d)p(v)}[T_\psi(v,y)]-e^{-1}\mathbb{E}_{p(y\mid d)p(v)}[e^{T_\psi(v,y)}]$, and use $U(d)=\max_\psi U(d;\psi)$. The trained network yields an (unnormalized) posterior estimate $p(v\mid y,d^*)\propto p(v)\exp(T_{\psi^*}(v,y)-1)$.","For model discrimination (three competing models) with two blocks, the BOED-optimized reward probabilities are reported as [0, 0, 0.6] (block 1) and [1, 1, 0] (block 2), producing markedly better confusion matrices than a literature-inspired baseline design sampling probabilities from Beta(2,2). For parameter estimation of the WSLTS model with three blocks, the optimized designs are [0,1,0], [0,1,1], and [1,0,1], yielding noticeably sharper marginal posteriors than baseline designs when averaged over 1,000 observations. Experiments use three-armed bandits with 30 trials per block; training uses 50,000 prior samples/simulations per design and a Bayesian-optimization budget of 400 utility evaluations (80 initial). Overall, optimal designs tend toward extreme reward probabilities and improve both model recovery and parameter recovery compared with common practice.","The authors note that their experiments are purely synthetic (no real participant data), so empirical effectiveness in real behavioral studies is unvalidated. They also highlight that it would be interesting to adapt the approach to sequential BOED settings with intractable models, implying the current work focuses on fixed (non-sequential) design optimization within their setup.","The approach appears computationally heavy (e.g., tens of thousands of simulations per design and hundreds of BO evaluations), which may be prohibitive for more complex simulators or larger design spaces. Results are demonstrated on a relatively constrained task family (three-armed Bernoulli bandits with short blocks); performance and robustness for more realistic/nonstationary reward structures, larger numbers of arms, or richer observation models are unclear. The MI lower bound optimization and neural estimator can suffer from bias/variance and training instability; sensitivity to network architecture, hyperparameters, and BO settings is not systematically explored. No publicly available implementation is provided, which may hinder reproducibility and adoption.",They suggest adapting the approach to the sequential BOED setting for intractable models. They also propose applying the method to real participants (real-world behavioral data) since the current results are based on synthetic simulations only.,"Extend the method to handle common behavioral-data complications such as misspecification, nonstationarity, and correlated/heterogeneous participants (hierarchical priors) and evaluate robustness under these conditions. Provide scalability studies and efficient simulation strategies (e.g., multi-fidelity surrogates, reuse of simulations, or active learning) to reduce computational cost for larger design spaces. Develop and release a reference software implementation to enable reproducible BOED pipelines in cognitive experiments. Explore alternative BOED objectives (e.g., task-specific losses, constrained designs, or multi-objective trade-offs between MD and PE) and compare against other modern implicit-likelihood BOED approaches.",2110.15632v1,https://arxiv.org/pdf/2110.15632v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:46:13Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Parameter estimation|Model discrimination|Other,Other,"Variable/General (design variable(s) \(\xi_t\) chosen over T sequential steps; examples include 1D time points, 2D locations; parameter dimension examples 2–20D)",Environmental monitoring|Healthcare/medical|Pharmaceutical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/desi-ivanova/idad,"The paper proposes implicit Deep Adaptive Design (iDAD), a policy-based method for real-time sequential Bayesian optimal experimental design with implicit (likelihood-intractable) simulators. It learns a design policy network offline by maximizing likelihood-free lower bounds on the total expected information gain (EIG) across a sequence of experiments, using InfoNCE and NWJ mutual-information bounds with an auxiliary critic network. The approach removes prior DAD requirements of tractable likelihoods and conditional independence, and introduces architectures (attention for exchangeable settings; LSTM for non-exchangeable/time-series settings) for encoding experiment histories. Experiments on location finding, pharmacokinetics sampling-time design, and an SDE-based SIR model show iDAD achieves higher EIG than likelihood-free baselines and near-matches likelihood-based DAD where applicable, while proposing new designs in milliseconds at deployment. The trained critic can also be used for likelihood-free posterior inference via density-ratio estimation after data collection.","Designs are chosen by a deterministic policy network, $\xi_t=\pi_\phi(h_{t-1})$, with history $h_t=\{(\xi_i,y_i)\}_{i=1}^t$. The total information objective is written as $I_T(\pi)=\mathbb{E}_{p(\theta)p(h_T\mid \theta,\pi)}[\log p(h_T\mid \theta,\pi)]-\mathbb{E}_{p(h_T\mid \pi)}[\log p(h_T\mid \pi)]$, and optimized via likelihood-free lower bounds: NWJ $\mathcal{L}^{\mathrm{NWJ}}_T=\mathbb{E}_{p(\theta)p(h_T\mid\theta,\pi)}[U(h_T,\theta)]-e^{-1}\mathbb{E}_{p(\theta)p(h_T\mid\pi)}[\exp U(h_T,\theta)]$, and InfoNCE $\mathcal{L}^{\mathrm{NCE}}_T=\mathbb{E}[\log \frac{\exp U(h_T,\theta_0)}{\frac{1}{L+1}\sum_{i=0}^L \exp U(h_T,\theta_i)}]$ with $\theta_{1:L}\sim p(\theta)$ contrastive samples.","On the location-finding benchmark (T=10), iDAD substantially improves information gained over static and heuristic baselines; e.g., for 4D parameters, iDAD achieves MI lower bounds around 7.69–7.75 vs ~5.52–5.55 for MINEBED/SG-BOED and ~4.79 for random, and remains close to likelihood-based DAD (7.97). Deployment time on CPU is ~0.0167–0.0168 seconds per adaptive design for iDAD versus ~2256 seconds for a traditional variational myopic approach (and ~0.007 seconds for DAD). In the SDE-based SIR model (true implicit), iDAD yields MI lower bounds ~3.87–3.92 versus ~3.40 (MINEBED), ~3.75 (SG-BOED), and ~2.67 (equal-interval). Pharmacokinetics experiments similarly show iDAD as best among implicit-capable methods and statistically close to DAD when likelihood is available.","The authors note iDAD’s practical benefit for live experiments comes at the cost of substantial offline training that can be computationally expensive, though amortization mitigates this when reusing the trained policy across many experiment instances. They also state the reliance on gradients restricts the method to continuous design settings, leaving discrete design spaces as an open challenge.","Performance depends on the tightness/stability of variational MI lower bounds (InfoNCE/NWJ) and the capacity/optimization of the critic; poor critics can bias the training signal and underestimate true EIG, especially in harder implicit models. The approach assumes differentiable simulators with access to $\partial y/\partial \xi$ (often via autodiff) and continuous design variables, which can be restrictive for simulators with discontinuities, stochastic control flow, or non-differentiable measurement processes. Empirical validation is largely simulation-based; broader real-world deployments and robustness to model misspecification, observation constraints, and experimental failures are not established.",They suggest extending the approach beyond continuous design settings to handle discrete designs. They also imply exploring architecture/training trade-offs to better control the cost–performance balance.,"Develop self-starting/online-updatable versions that adapt the policy during deployment under simulator mismatch, and add robustness to model misspecification and nonstationarity. Extend iDAD to constrained, mixed discrete-continuous, or combinatorial design spaces (e.g., via relaxations or reinforcement-learning style estimators), and to batched/parallel experimentation with resource constraints. Provide standardized software benchmarks and more real-world case studies, plus diagnostics for critic quality/tightness (e.g., calibration checks) and uncertainty-aware design policies.",2111.02329v1,https://arxiv.org/pdf/2111.02329v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:46:51Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Cost reduction|Other,Other,"Variable/General (design variable is experimental condition/dose; examples use 1 design variable: BMP ligand concentration titration; 2 unknown parameters in onestep model, 3 in twostep model)",Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://arxiv.org/abs/2002.08129|http://arxiv.org/abs/1906.04032,"The paper proposes SBIDOEMAN, a sequential Bayesian optimal design-of-experiments algorithm for simulation-based inference of mechanistic biological (BMP signaling) models with implicit/intractable likelihoods. It alternates between (i) selecting the next experimental design by maximizing an information-gain utility (mutual information between parameters and outcomes) estimated via MINE and optimized using Bayesian optimization with a Gaussian process, and (ii) updating parameter posteriors using likelihood-free inference (SNPE) with normalizing flows. The approach is evaluated on simulated “onestep” and “twostep” BMP pathway models with held-out true parameters, comparing SBIDOEMAN against random and log-equidistant ligand titration designs over a budget of 5 sequential experiments. Across ensembles of runs under an 8-hour compute budget, SBIDOEMAN yields lower RMSE of MAP parameter estimates than both controls, showing faster/more stable convergence especially in the simpler onestep model. The work advances SBI-focused Bayesian experimental design for perturbation-response systems biology by integrating mutual-information estimation, Bayesian optimization, and neural posterior estimation in a closed loop.","The optimal design is chosen by maximizing a utility: $d^* = \arg\max_{d\in D} U(d)$ with $U(d)=I(\theta,y\mid d)=\mathbb{E}_{p(\theta)p(y\mid\theta,d)}\left[\log\frac{p(y\mid\theta,d)}{p(y\mid d)}\right]$. The mutual information is approximated via a Donsker–Varadhan lower bound estimated by a MINE network and then optimized over designs with Bayesian optimization (Gaussian process). Posterior updating uses SNPE with a normalizing-flow density estimator (Eq. 2 in the paper) to approximate $p(\theta\mid x_o)$ sequentially after each chosen design.","After 5 sequential design rounds, SBIDOEMAN achieved lower MAP-estimate RMSE than random and equidistant controls on both simulated BMP models. Onestep model: SBIDOEMAN $0.004\pm0.007$ (n=48) vs Random $0.013\pm0.035$ (n=38) vs Equidistant $0.023\pm0.051$ (n=50). Twostep model: SBIDOEMAN $0.149\pm0.153$ (n=48) vs Random $0.242\pm0.146$ (n=40) vs Equidistant $0.249\pm0.173$ (n=50). Qualitatively, SBIDOEMAN reduced variance and converged faster than random search, with clearer gains in the simpler two-parameter onestep setting and more gradual improvement in the three-parameter twostep setting.","The authors note that neural network-based SBI methods, while improving over ABC, remain computationally expensive and involve many hyperparameters to tune/assess. They also point out that their evaluation uses a simulator as a surrogate for collecting experimental data, and that real experimental validation would require replacing simulation with an iterative lab process. They mention that more complex models (e.g., twostep) may require more designs to converge.","The work benchmarks only on two simplified BMP pathway models and only in simulation, so performance and robustness under real experimental noise, batch effects, and model misspecification are unclear. The paper does not clearly define practical constraints on the design space (e.g., dosing bounds, discrete plate layouts), which can materially affect optimal designs in perturbation experiments. Comparisons are limited to random and log-equidistant titrations; other common Bayesian OED/SBI baselines (e.g., expected posterior entropy reduction with alternative MI estimators, or acquisition functions without MINE) are not included. Reproducibility is limited because implementation details (software stack, code release) are not provided in the manuscript excerpt.","They plan to evaluate SBIDOEMAN on experimental (real) data rather than simulator-only studies. They suggest incorporating symmetries/equivariance in normalizing-flow architectures to reduce computational burden and improve utility for experimental design. They also propose extending toward settings with multiple candidate mechanistic models (model selection), such as distinguishing different BMP ligand interaction models (e.g., homodimeric vs heterodimeric behavior).","A useful next step would be to add explicit resource and feasibility constraints (e.g., discrete dose grids, replicate allocation, plate/well constraints) and study how constrained OED changes the selected designs. Extending SBIDOEMAN to handle autocorrelated/time-course readouts, nuisance parameters, and unknown observation noise models would improve applicability to real perturbation assays. Providing a self-starting or amortized variant (training across related pathway models) and releasing a reference implementation/package would improve practical adoption and reproducibility. More extensive benchmarking against modern Bayesian OED for implicit models (different MI bounds/estimators and acquisition functions) would clarify when MINE-based objectives offer the biggest advantage.",2111.13612v1,https://arxiv.org/pdf/2111.13612v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:47:27Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Optimization|Other,Not applicable,"Variable/General (N units, panel over T pre-treatment periods; examples use N=25, T=2; simulations sample N=10 from 50 states, with K=3 or 7 treated units)",Finance/economics,Simulation study,TRUE,Other,Not provided,https://github.com/synth-inference/synthdid/blob/master/experiments/bdm/data/urate_cps.csv,"The paper studies optimal experimental design in panel-data settings with pre-treatment outcomes, where researchers can choose which aggregate units to treat and then estimate effects using difference-in-means or synthetic-control-style weighted comparisons. It formulates joint treatment assignment and weighting as optimization problems (per-unit synthetic control and two global weighted-mean formulations), and shows the combined problem is NP-hard; it proposes mixed-integer programming (MIP) formulations to select treated/control sets and weights under linear constraints. Using simulations based on US state unemployment data (BLS), the proposed MIP-based designs reduce RMSE and improve statistical power relative to randomized assignment with either difference-in-means or synthetic control, across homogeneous and heterogeneous effect scenarios. The paper also proposes permutation-based inference that permutes time periods (following Chernozhukov et al., 2021) and reports correct/conservative size with improved power in simulations. Overall, it contributes a design-and-analysis framework for experiments with synthetic controls, emphasizing optimal unit selection when pre-treatment panels are available.","The core estimators compare treated outcomes to weighted controls: for treated unit $i$, $\hat Y_{i,T+1}(0)=\sum_{j:D_j=0} w^i_j Y_{j,T+1}$ and $\hat\tau_i=Y_{i,T+1}-\hat Y_{i,T+1}(0)$. The per-unit design objective jointly chooses $D_i\in\{0,1\}$ and weights by minimizing pre-treatment fit plus a weight-regularization penalty: $\min \frac{1}{KT}\sum_i\sum_{t\le T} D_i\big(Y_{it}-\sum_j w^i_j(1-D_j)Y_{jt}\big)^2 + \frac{\lambda}{K}\sum_i\sum_j D_i (w^i_j)^2$, with constraints like $\sum_i D_i=K$, $w^i_j\ge0$, and $\sum_j w^i_j(1-D_j)=1$ for treated $i$. Two global variants minimize $\frac1T\sum_{t\le T}(\sum_i w_i D_i Y_{it}-\sum_i w_i(1-D_i)Y_{it})^2+\lambda\sum_i w_i^2$ with normalization constraints on treated/control weights; these are solved via MIP reformulations.","In simulations using BLS unemployment data (sampling 10 units over 10 periods, with treatment in the last 3 periods and $K\in\{3,7\}$), MIP-designed procedures reduce RMSE versus randomized designs. For ATET estimation, one-way and two-way global objectives improve over randomized synthetic control and randomized difference-in-means by roughly 11%–31% depending on homogeneous vs. heterogeneous effects and $K$. For heterogeneous unit-level effects, the per-unit MIP approach improves unit-level RMSE over randomized synthetic control by over 13% (when $K=3$) and 16% (when $K=7$). Permutation-based (time-period) inference yields correct or conservative test sizes and higher power than randomized baselines in additional simulated power-curve experiments.","The authors note that solving the mixed-integer formulations becomes computationally burdensome as the number of units increases, and they prove the underlying problem is NP-hard, making exact solutions for substantially larger problems unlikely. They also state that their theoretical guarantees for inference rely on rather strong assumptions. They emphasize that the per-unit formulation is harder to solve than global formulations due to many more decision variables (unit-specific weights).","The proposed designs rely heavily on the informativeness/stability of pre-treatment outcomes; if pre-treatment dynamics shift (nonstationarity, structural breaks) or interference/spillovers occur across units, the optimized donor fits and MSE-based objectives may not translate into better post-treatment counterfactuals. The paper’s empirical evidence is simulation-based using one main public dataset and relatively small sampled panels (e.g., 10 units), which may not generalize to higher-dimensional settings (many units, longer horizons, multiple outcomes) without scalable approximations. Practical deployment would also depend on robust hyperparameter choice for $\lambda$ and on solver behavior (time limits, suboptimality gaps), which can affect reproducibility and performance but are not benchmarked extensively.","The conclusion states that future research should further investigate how assumptions about treatment-effect heterogeneity, the estimand of interest, and computational considerations affect the choice among the proposed approaches. The paper also points to extending practical guidance and addressing current limitations of the methodology, including computational tractability and inference under weaker assumptions.","Develop scalable approximations or heuristics (e.g., greedy/sequential selection, continuous relaxations, or decomposition methods) with performance guarantees to handle much larger $N$ than exact MIP. Extend the framework to settings with autocorrelation and nonstationary panels (e.g., explicit time-series error models) and to interference/network spillovers across units. Provide open-source reference implementations and benchmarking across multiple real applications (policy, geo-experiments, marketing) to assess robustness and sensitivity to $\lambda$, pre-period length $T$, and solver suboptimality.",2112.00278v1,https://arxiv.org/pdf/2112.00278v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:48:03Z
TRUE,Sequential/adaptive|Other,Optimization|Prediction|Other,Not applicable,"Not specified (process parameters include tool feed rate, rotation rate, heat treatment, and time-series measurements such as power/torque/force/temperature)",Manufacturing (general),Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper proposes Differential Property Classification (DPC), a machine-learning framework intended to support data-driven experimental design in advanced manufacturing when physics-based models are unavailable and data are scarce. Instead of predicting an absolute property value from process parameters (regression), DPC compares two candidate process-parameter settings and predicts which will yield a larger target property (or whether they are approximately equal), turning experiment selection into a 3-class classification problem. The method constructs pairwise examples using a threshold $t$ (set to 1% of the property’s standard deviation) to define “meaningful” differences and trains models (XGBoost or an MLP) under different training regimes: direct regression backbone, difference regression, or direct classification. The approach is demonstrated on 20 ShAPE AA7075 tube experiments with properties including ultimate tensile strength, yield strength, and max load, showing that XGBoost-based DPC achieves strong accuracy and remains reasonably effective in very low-data regimes (e.g., training on only a handful of experiments). Overall, DPC reframes experiment planning as choosing between candidate parameter sets using relative property predictions, aiming to reduce trial-and-error experimentation cost and time.","The DPC dataset is formed from pairs of test points: $D_e^{\mathrm{DPC}}=\{(x^e_{i_1},x^e_{i_2},z_{i_1,i_2})\}$, with labels $z\in\{0,1,2\}$ determined by a threshold $t$ on property differences. Specifically, $z_{i_1,i_2}=1$ if $y^e_{i_1}-y^e_{i_2}>t$, $z_{i_1,i_2}=2$ if $y^e_{i_2}-y^e_{i_1}>t$, and $z_{i_1,i_2}=0$ if $|y^e_{i_1}-y^e_{i_2}|<t$. Predictions can be produced either by (i) a regression backbone $f:X\to\mathbb{R}$ whose outputs are compared via the same threshold rule, (ii) a difference-regression model $f:X\times X\to\mathbb{R}$ predicting $y_1-y_2$, or (iii) a direct classifier predicting $z$ from concatenated pairs.","Using a direct-regression backbone, XGBoost outperformed an MLP on DPC accuracy for all evaluated properties (e.g., Max Load: 87.81% vs 77.00%±3.0; UTS: 89.00% vs 88.00%±1.0; Yield Strength: 82.94% vs 79.00%±1.0). Comparing XGBoost backbone types, direct regression and direct classification were similar, while difference regression was consistently worse (e.g., UTS: direct reg 90.00%, difference reg 86.50%, classification 89.00%). In a low-data study (fixed test set, varying training experiments), the XGBoost direct-regression DPC model achieved about 80% accuracy with 5 training experiments and improved to about 90% with 15 experiments. The test-pair set size reported for the held-out experiments was 1600 paired comparisons.",None stated.,"The work does not present a formal DOE strategy (e.g., optimal design criteria, acquisition functions, or explicit selection policies) for choosing the next experiment; it mainly reframes prediction as pairwise classification and reports accuracy. Evaluation is limited to a single manufacturing dataset (20 experiments) and does not address robustness to distribution shift (new tooling/material lots) or temporal correlation/structure in the time-series process measurements beyond the chosen representation. The threshold choice ($t=1\%$ of the standard deviation) is heuristic and may materially affect class balance and conclusions, but sensitivity analysis and practitioner guidance are limited.",The authors suggest that a model more robust than direct regression or direct classification could be developed by designing a loss function that incorporates raw property magnitudes while still directly optimizing DPC accuracy (motivated by their difference-regression attempt).,"Develop an explicit sequential DOE/active-learning loop that uses DPC predictions to pick the next experiment under constraints (cost, feasibility, safety) and quantify expected improvement. Provide principled threshold-selection/calibration (e.g., based on measurement noise, indifference regions, or utility) and study sensitivity to $t$ and class imbalance. Validate across additional processes/material systems and include uncertainty quantification (e.g., calibrated probabilities) to support risk-aware experiment selection.",2112.01687v1,https://arxiv.org/pdf/2112.01687v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:48:35Z
FALSE,Other,Other,Not applicable,Not specified,Healthcare/medical|Other,Simulation study|Other,TRUE,Julia|Other,Not provided,NA,"The paper studies design-based variance estimation for linear causal effect estimators under arbitrary interference (via exposure mappings) and arbitrary experimental designs, where unbiased/consistent variance estimation is generally impossible due to unobserved potential-outcome products. It reframes the construction of minimally conservative (yet always conservative) variance estimators as choosing an estimable quadratic-form upper bound on the true variance. The authors characterize admissible variance bounds within the class of positive semidefinite quadratic forms, showing common bounding approaches (including an Aronow–Samii/Young’s-inequality style bound) can be inadmissible. They propose OPT-VB, a convex optimization (often an SDP) over slack matrices that enforce design compatibility while minimizing a user-chosen strictly monotone objective (e.g., Schatten norms, targeted linear objectives encoding prior outcome structure), guaranteeing the resulting bound is conservative, estimable, and admissible. A simulation based on a real network experiment (Paluck et al. 2016) finds optimized bounds can be far less conservative than existing bounds, yielding narrower confidence intervals while retaining over-coverage.","A linear estimator is written as \(\hat\tau = n^{-1} V^\top \theta\), yielding \(\mathrm{Var}(\hat\tau)=n^{-2}\theta^\top A\theta\) with known \(A=\mathrm{Cov}(V)\). Variance bounds are restricted to quadratic forms \(VB(\theta)=n^{-2}\theta^\top B\theta\) with \(B\succeq A\) and design-compatibility constraints \(b_{k\ell}=0\) for unobservable pairs \((k,\ell)\in\Omega\); equivalently \(B=A+S\) with slack \(S\succeq 0\) and fixed entries \(s_{k\ell}=-a_{k\ell}\) on \(\Omega\). The bound is chosen by solving \(S^*\in\arg\min_{S\in\mathcal S} g(S)\) (OPT-VB), where strictly monotone objectives ensure admissible bounds.","In the Paluck et al. (2016)-based simulation (2,170 seed students; variance matrix computed via 5 million Monte Carlo draws), the existing Aronow–Samii bound was extremely conservative: relative bias about 349% for synthetic outcomes and 275% for real-data outcomes. Optimized bounds (trace/Frobenius/targeted/composite objectives) reduced relative bias to <15% for synthetic outcomes and roughly 80%–102% for real-data outcomes, with confidence-interval widths about half as large (synthetic) and ~30% narrower (real data) compared to Aronow–Samii. All methods remained conservative (over-coverage), but Aronow–Samii produced ~100% coverage, indicating excessive conservativeness, whereas optimized bounds had coverage closer to 95% (e.g., ~0.96–0.99 depending on setting). The paper also proves theoretical guarantees: any strictly monotone objective in OPT-VB yields conservative, design-compatible, admissible bounds; Schatten p-norm objectives (p in [1,∞)) are strictly monotone; and every admissible quadratic bound corresponds to some targeted linear objective with a positive definite targeting matrix.","The authors note several open limitations: their results are developed for the class of linear point estimators and may not extend to modern ML-based estimators whose variances are not quadratic forms in the potential outcomes. They restrict attention to quadratic-form bounds and acknowledge tighter bounds may exist outside this class (e.g., Fréchet–Hoeffding-type ideas), with admissibility/sharpness properties unknown in general. Their mean-squared-error discussion for selecting bounds relies on an upper bound for estimator precision and is presented as a heuristic that can be loose, rather than direct MSE minimization. They also assume the exposure mapping is correctly specified and flag extension to misspecified/unrestricted exposures as an open question.","The approach requires forming (or accurately approximating) the covariance matrix \(A=\mathrm{Cov}(V)\), which can be computationally heavy for large K (e.g., many unit-exposure potential outcomes) and may not scale without exploiting structure/sparsity. The optimized bound depends on choices like the definition of the “unobservable” set \(\Omega\) (in simulations defined via a probability cutoff), which introduces a tuning/robustness issue that may affect finite-sample performance and comparability across studies. While admissibility is ensured within quadratic bounds, practical performance can depend on solver tolerances and numerical stability of SDP solutions, potentially yielding near-feasible/near-PSD matrices in implementation. The paper’s simulation evidence is based on a single empirical design family (a specific network/two-stage randomization), so general empirical performance across other interference structures/designs (e.g., cluster randomized with many clusters, rerandomization) is not fully demonstrated.","The authors suggest extending the framework beyond linear estimators, potentially via linearization of ML-based estimators to obtain asymptotically valid quadratic bounds, though finite-sample extension appears difficult. They also propose exploring broader classes of variance bounds beyond quadratic forms (e.g., generalizations of Fréchet–Hoeffding-type sharp bounds) and studying whether such bounds are admissible. Another stated direction is to move beyond heuristic bound-selection for MSE and investigate whether one can choose bounds to directly minimize MSE. Finally, they highlight extending the methods to settings with misspecified exposure mappings as an open research question.","Develop scalable algorithms exploiting sparsity/graph structure in \(\Omega\) and block structure in designs (e.g., cluster/stratified designs) to handle very large K without full SDPs. Provide guidance and diagnostics for selecting probability cutoffs/compatibility relaxations (e.g., enforcing \(\Pr(k,\ell\in S)\ge c\)) and for sensitivity analysis over \(\Omega\) definitions. Extend evaluation to additional real datasets and design types, and compare against alternative conservative variance approaches (e.g., randomization-based/permutation SEs under interference) when applicable. Release robust software implementations (with reproducible examples and solver settings) and study numerical stability/regularization choices for practitioner-facing workflows.",2112.01709v2,https://arxiv.org/pdf/2112.01709v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:49:15Z
TRUE,Factorial (full)|Factorial (fractional)|Response surface|Computer experiment|Sequential/adaptive|Optimal design|Other,Prediction|Cost reduction|Other,Space-filling|Not applicable,"Variable/General (k factors; examples shown with k=3; discusses general k and 2^k, 2^(k-p) run sizes)",Energy/utilities|Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),https://ec.europa.eu/energy/sites/default/files/methodology_for_the_european_resource_adequacy_assessment.pdf|https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html|https://spacefillingdesigns.nl/,"This paper is a review and strategic outlook that links three areas: security of electricity supply (resource adequacy assessment), artificial intelligence (AI) methods, and design of experiments (DOE) as an efficient way to sample simulation model input spaces. It argues that probabilistic resource adequacy simulations are computationally expensive and that AI-based metamodels (surrogate models/response surfaces) can approximate model input–output behavior quickly, enabling broader scenario analysis under uncertainty. The DOE component is presented as the key tool for selecting informative simulation runs, discussing classical designs (full and fractional factorial, central composite designs) and space-filling computer experiment designs (Latin hypercube), plus adaptive/optimal DOE via active learning (e.g., uncertainty sampling, query-by-committee). The paper provides a conceptual pipeline and literature synthesis indicating that combining DOE with AI metamodeling is promising for accelerating security-of-supply studies and highlights research gaps (notably AI forecasting of storage dispatch and (non-)availabilities at system level).","For factorial designs, the paper states the run sizes: a full factorial design uses $2^k$ combinations for $k$ factors, and a fractional factorial design uses $2^{k-p}$ combinations where $p$ reflects the degree of fractionation/aliasing (commingling). For autoencoders (as a dimensionality-reduction method discussed in the broader AI section), it gives the encoder/decoder mapping $h(x)=f(W_1x+b_1)$ and $\tilde{x}=f(W_2h(x)+b_2)$ with training objective $\vartheta(\theta)=\sum_{x\in X}\lVert x-\tilde{x}\rVert^2$. DOE-specific control-limit/optimality equations (e.g., D-, A-optimal criteria formulas) are not provided; Latin hypercube is described conceptually as a space-filling sampling scheme.","No original DOE simulation study or quantitative DOE performance comparison is reported; the work is a literature review and methodological synthesis. Quantitative values mainly summarize publication counts from Scopus-based searches (e.g., metamodeling+DOE publications per year and shares using AI-based methods) rather than reporting design efficiency metrics. The paper cites external studies as examples (e.g., reported $R^2$ values in cited metamodeling applications), but these are not new results derived by this paper.","The authors note that the presented AI-method overviews are not comprehensive (they are intended as guides) and that the list of application fields is not exhaustive. They also emphasize that the optimal choice of metamodeling method and experimental design is application-specific and typically cannot be determined a priori, requiring testing/validation.","Because this is a review, the DOE recommendations are not backed by a unified empirical benchmark on resource-adequacy models; there is no controlled comparison of designs (e.g., LHD vs. CCD vs. factorial) under common simulation budgets and error metrics. The discussion of “optimal/adaptive DOE” remains conceptual (active learning taxonomy) without prescribing concrete acquisition functions, stopping rules, or robustness to stochastic simulation noise common in Monte Carlo adequacy models. Practical implementation issues (high-dimensional constraints, correlated inputs, feasibility regions, and mixed discrete/continuous factors typical in energy scenarios) are not deeply developed into actionable design construction guidance.","The paper calls for future research on (1) efficient AI-based metamodeling of complex security-of-supply models (supported by DOE) to expand scenario scope under uncertainty, and (2) applying AI-based forecasting methods for storage dispatch and for (non-)availabilities of power plants at the system level, which the review identifies as under-covered areas.","A useful next step would be to establish an open benchmark suite for resource adequacy metamodeling that fixes several representative simulation models, factor spaces (including constraints), and evaluation metrics (e.g., integrated prediction error over the design space and cost-aware accuracy) to compare DOE strategies fairly. Research could also develop self-starting/adaptive DOE procedures tailored to stochastic simulators (accounting for Monte Carlo error and heteroskedastic noise) and mixed-integer factor spaces, and provide reproducible software implementations (e.g., Python/R packages) for energy-model metamodeling pipelines including design generation, sequential sampling, and uncertainty quantification.",2112.04889v2,https://arxiv.org/pdf/2112.04889v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:49:50Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Computer experiment|Other,Optimization|Prediction|Parameter estimation|Cost reduction|Other,Other,"Variable/General (state-action dimensions; examples include 4D state/1D action Cartpole, 8D state/2D action Reacher, etc.)",Theoretical/simulation only|Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper frames data collection in model-based reinforcement learning as a Bayesian optimal experimental design (BOED) problem in a setting where the learner can actively choose any state–action pair to query (“transition-query RL” / generative model access). It proposes a BOED-style acquisition function defined as the expected information gain (mutual information) about the optimal-policy trajectory $\tau^*$ obtained by querying a candidate state–action pair, thereby prioritizing data that is relevant to control rather than uniformly reducing model uncertainty. A practical algorithm (BARL) approximates the acquisition via posterior function sampling of the transition dynamics (Gaussian-process dynamics model with approximate posterior sampling) and solves sampled MDPs via MPC to generate trajectory samples. The method is evaluated on simulated continuous control tasks (pendulum, cartpole, lava path, reacher) and a simplified nuclear-fusion beta-tracking simulator, showing large improvements in sample efficiency versus model-based (e.g., PETS, MPC with passive data) and model-free baselines (e.g., SAC/TD3/PPO). The main practical takeaway is that BOED-guided selection of state–action queries can reduce the number of expensive transition queries by orders of magnitude in settings with costly simulators.","The design criterion is an expected information gain acquisition over candidate query points $(s,a)$: $\mathrm{EIG}_{\tau^*}(s,a)=\mathbb{E}_{s'\sim T(s,a\mid D)}\big[H(\tau^*\mid D)-H(\tau^*\mid D\cup\{(s,a,s')\})\big]$, i.e., mutual information between the prospective transition outcome and the optimal trajectory. Using MI symmetry, it is rewritten as a difference of predictive entropies and approximated by Monte Carlo with posterior dynamics samples and corresponding MPC-induced trajectory samples: $\mathrm{EIG}_{\tau^*}(s,a)\approx H[s'\mid s,a,D]-\frac{1}{mn}\sum_{i,j} H[s'\mid s,a,\tau^*_{ij},D]$. The sequential design rule is to select $(s,a)$ that maximizes this acquisition at each iteration and then query the simulator/transition function to obtain $s'$.","Across five tasks, the proposed BARL method is reported to learn an optimal policy with roughly 5–1,000× fewer transition samples than model-based RL baselines and about $10^3$–$10^5\times$ fewer than model-free RL baselines (as stated in the abstract). The paper reports median samples-to-solve (Table 1), e.g., Lava Path: BARL 11 vs MPC/EIGT 41 and PETS 600; Pendulum: BARL 16 vs PETS 5200 and SAC 6000; Cartpole: BARL 91 vs PETS 1625 and SAC 31000; Reacher: BARL 251 vs PETS 700 and SAC 23000; Beta Tracking: BARL 96 vs PETS 300 and SAC 9000. Ablations indicate the acquisition remains effective even when the MPC used to generate posterior-sample trajectories is somewhat suboptimal (Appendix B.1). Runtime per BARL iteration is nontrivial (Table 3), reflecting a compute–sample tradeoff suitable for expensive-simulator regimes.","The authors state that computing the acquisition function is computationally expensive because it requires executing the full control algorithm over multiple posterior function samples, leading to high computational requirements. They also note that their current Gaussian-process dynamics model is expensive and “does not scale well to higher dimensions and large datasets.”","The approach assumes access to a generative model / ability to query arbitrary state–action pairs, which is often unrealistic in real physical systems where only on-policy/trajectory data are available and resets to arbitrary states are impossible. The acquisition relies on approximating $\pi^*$ with an MPC controller and on posterior sampling fidelity; performance may degrade if the Bayesian dynamics model is misspecified, if dynamics are highly nonstationary, or under significant partial observability or temporal correlation not captured by the GP model. Comparisons are largely simulation-based, and there is limited evidence of robustness under real-world constraints (e.g., action/state bounds, safety constraints during querying, simulator–reality gap).",The authors propose extending the method beyond Gaussian processes to other Bayesian models such as Bayesian neural networks and leveraging GPU compute to improve scalability to higher dimensions and larger datasets. They also indicate an intention to extend and apply BARL to more realistic nuclear-fusion control settings beyond the simplified beta-tracking setup.,"A natural extension is to incorporate safety-aware or constrained BOED so that queried state–action pairs satisfy safety constraints (especially relevant for robotics and plasma control). Another direction is adapting the acquisition to settings without arbitrary transition queries (rollout/trajectory-only data), possibly via hybrid designs that choose interventions available through reset distributions or policy-conditioned exploration. Developing theoretically grounded approximations (e.g., variational bounds for EIG, low-rank/posterior approximations) and benchmarking against more modern active model-based RL methods would clarify scalability and generality. Providing open-source implementations and standardized task suites would also improve reproducibility and adoption.",2112.05244v2,https://arxiv.org/pdf/2112.05244v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:50:29Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive,Parameter estimation|Model discrimination|Prediction|Other,Bayesian D-optimal|Other,"Variable/General (design vector ξ ∈ R^s; examples include 1 design factor ξ in linear Gaussian example and 2 design factors (ξ1, ξ2) in EIT)",Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Approximation methods|Simulation study,TRUE,None / Not applicable,Not provided,NA,"This paper develops computationally efficient estimators for Bayesian optimal experimental design (BOED) when the forward model includes nuisance parameters that must be marginalized, which would otherwise add an extra inner loop to expected information gain (EIG) estimation. The authors derive a small-noise approximation that analytically approximates the nuisance-parameter marginalization by modifying the likelihood via a correction to the noise covariance; this removes the additional inner Monte Carlo loop under a positive-definiteness condition tied to the nuisance covariance magnitude. Building on this, they present two practical EIG estimators: a small-noise double-loop Monte Carlo formulation and a small-noise Monte Carlo–Laplace method (plus a Laplace-based importance sampling variant) to reduce computational cost and mitigate nested-sampling burden. They provide complexity/error-splitting analyses showing that, under their approximation, the total asymptotic work remains comparable to the nuisance-free case. Numerical examples include a linear-Gaussian test problem and an electrical impedance tomography (EIT) design problem for composite laminate materials, demonstrating accuracy when nuisance uncertainty is sufficiently small and showing breakdown when the corrected covariance loses positive definiteness.","The data model is $y_i(\xi)=g(\xi,\theta,\phi)+\varepsilon_i$ with Gaussian noise and nuisance parameters $\phi$ marginalized inside the likelihood. The EIG is expressed as an expected KL divergence and estimated via nested Monte Carlo; with nuisance parameters this yields a two-inner-loop DLMC estimator (Eq. (9)). The key DOE contribution is the small-noise approximation: linearizing the residual in $\phi$ leads to an approximate marginalized likelihood $\tilde p(Y\mid\theta)$ with an updated precision $\Sigma_\varepsilon^{-1}-\Sigma_\varepsilon^{-1}MM^\top\Sigma_\varepsilon^{-1}$ (Eqs. (20)–(21)), where $M=\nabla_\phi g(\theta,0)\Sigma_\phi^{1/2}$; this enables one-inner-loop DLMC (Eq. (26)) and a Laplace-based EIG estimator (Eq. (57)).","The small-noise approximation eliminates the extra nuisance-parameter inner loop, keeping overall complexity comparable to nuisance-free BOED; for DLMC with optimal tuning the work scales as $\mathrm{TOL}^{-(3+\gamma/\eta)}$ (Eq. (40)), while the Monte Carlo–Laplace estimator scales as $\mathrm{TOL}^{-(2+\gamma/\eta)}$ but has an inherent bias floor controlled by $N_e$ (Eq. (67) and feasibility condition (63)). In Example 1, the approach breaks down when nuisance variance exceeds a threshold (reported around $\sigma_\phi^2\approx 1.44\times 10^{-8}$) because the corrected covariance loses positive definiteness. In the EIT example, a similar threshold is observed (around $\sigma_\phi^2\approx 9.03\times 10^{-7}$), and EIG surfaces indicate the design is improved by larger electrode shift and distance, with shift having stronger influence. The DLMCIS (importance sampling) variant typically needs far fewer inner samples than plain DLMC, often $M^*=1$ except at very small tolerances.","The authors state the small-noise approximation is only valid when nuisance uncertainty is sufficiently small so that the corrected covariance (metric tensor) remains positive definite; otherwise the method deteriorates and can fail completely (Remarks 2–3 and Examples 1 and 3). They also note they do not account for the bias introduced by the first-order likelihood approximation used in the small-noise derivation because computing it is very demanding (Example 1 discussion). Additionally, the Monte Carlo–Laplace estimator introduces a fixed/inherent bias (controlled by $N_e$) that can make small tolerances infeasible (Eq. (63) and Figures 8–9 discussion).","The method relies on differentiability of the forward model and availability/accuracy of sensitivities (e.g., $\nabla_\phi g$, $\nabla_\theta g$ and Hessian-related terms), which can be expensive or unstable for complex PDE models; performance may degrade if adjoints or reliable automatic differentiation are unavailable. The positive-definiteness condition depends on $\nabla_\phi g(\theta,0)$ (which varies with $\theta$), so global validity over the prior/design space may be hard to certify in practice without conservative bounds, potentially limiting applicability. The paper emphasizes small-noise nuisance uncertainty; scenarios with moderate/large nuisance effects (or highly nonlinear nuisance influence) are not addressed beyond demonstrating breakdown, and robustness to model misspecification (non-Gaussian noise, correlated errors, model discrepancy) is not studied. No publicly available implementation is provided, which may hinder reproducibility and adoption.",None stated.,"Develop adaptive/diagnostic procedures to detect when the corrected covariance is near loss of positive definiteness and to switch to safer estimators (e.g., fallback to explicit nuisance marginalization or tempered approximations). Extend the approach to handle larger nuisance uncertainty via higher-order (second-order) expansions in $\phi$ or alternative marginal-likelihood approximations that preserve positive definiteness by construction. Investigate robustness under non-Gaussian or correlated observation noise and under model discrepancy, especially for PDE-based applications like EIT. Provide open-source implementations (e.g., with adjoint-based gradients) and benchmarking on additional real experimental datasets to validate practical performance and tuning guidance.",2112.06794v4,https://arxiv.org/pdf/2112.06794v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:51:11Z
TRUE,Split-plot|Other,Parameter estimation|Model discrimination|Prediction|Other,Not applicable,Variable/General (p treatments; examples include 5 treatments in simulations and 4 treatments in pesticide study; blocks n vary),Food/agriculture|Theoretical/simulation only|Other,Simulation study|Approximation methods|Other,TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=melt,"The paper develops an empirical likelihood (EL) framework for inference in general block designs, including highly unbalanced and incomplete block designs, with a focus on simultaneous inference for multiple treatment comparisons under blocking. It derives a joint asymptotic limiting distribution for a vector of constrained EL test statistics, showing convergence to a generalized Wishart-type multivariate chi-square distribution, enabling single-step multiple testing with generalized family-wise error rate (gFWER) control. Two calibration procedures are proposed: an asymptotic Monte Carlo (AMC) method that simulates from an estimated asymptotic Gaussian representation, and a nonparametric bootstrap (NB) method based on null-transformed/resampled block data. Simulation studies on a balanced incomplete block design show NB better maintains nominal FWER/coverage in small samples and under violations of linear mixed-model assumptions, while AMC becomes comparable as sample size increases. An application to pesticide (clothianidin) concentration experiments illustrates that EL-based simultaneous comparisons can yield different conclusions than mixed-model-based methods when assumptions like normality, equal variances, and additivity are violated.","For block designs, the estimating function is $g(X_i,\theta)=(X_i-\theta)\circ c_i$ (Hadamard product with the block-incidence row). The profile empirical likelihood ratio leads to the statistic $\ell_n(\theta)=2\sum_{i=1}^n\log\{1+\lambda(\theta)^\top g(X_i,\theta)\}$, where $\lambda(\theta)$ solves $\frac{1}{n}\sum_{i=1}^n \frac{g(X_i,\theta)}{1+\lambda^\top g(X_i,\theta)}=0$. For hypothesis $H_j: h_j(\theta)=0$, the multiple-testing statistic is $T_{nj}=\frac{a_n^2}{n}\inf_{\theta\in H_j\cap K_n}\ell_n(\theta)$, and jointly $T_n\Rightarrow \chi^2(m,\mathbf q, R)$ (multivariate chi-square) under the complete null.","In simulations for a balanced incomplete block design with 5 treatments (10 pairwise tests) and $n\in\{50,100,200\}$ blocks at $\alpha=0.05$, AMC can be anti-conservative in small samples (e.g., FWER around 0.078–0.108 and SCI CP around 90–92% in some scenarios at $n=50$), especially under skew/heavy-tailed distributions or variance heterogeneity. The NB procedure typically achieves FWER and simultaneous coverage closer to nominal even at $n=50$ (FWER roughly 0.033–0.049; CP about 95–97%), but produces longer intervals due to larger cutoffs. In the pesticide application (32 derived blocks; 4 treatments; 6 pairwise comparisons), AMC and NB produce similar adjusted p-values and identify most comparisons as significant except Fungicide vs Low, while the mixed-model-based HBW approach yields different conclusions for some contrasts under strong assumption violations.","The authors note that convergence to the asymptotic multivariate chi-square distribution may be slow, particularly for incomplete block designs and small samples, making asymptotic calibration prone to inflated error rates. They also state that the nonparametric bootstrap can be challenging when the convex hull constraint fails in resamples, and suggest that alternative bootstrap schemes (e.g., block bootstrap) or empirical likelihood variants without the convex hull constraint may be needed. They further mention that specifying an appropriate null transformation for the bootstrap can be difficult in more general settings beyond their mean/block-design formulation.","The work is primarily about analysis/inference for block designs rather than constructing or optimizing experimental designs; it does not address design selection, power/cost tradeoffs, or optimal allocation of runs/blocks. The proposed single-step procedures control gFWER but may be less powerful than modern stepwise or hierarchical multiple-comparison procedures in large-scale settings; power is not deeply optimized beyond cutoff calibration. Practical implementation depends on solving constrained EL optimizations for each hypothesis (and many times in NB), which can become computationally heavy for large numbers of treatments/comparisons or complex constraints, and the paper does not provide broad benchmarking of runtime/scalability or numerical stability across design structures.","The authors suggest extending AMC/NB to deliver common quantile procedures (instead of common cutoffs) when marginal degrees of freedom differ across tests, and adapting both approaches to stepwise procedures to improve power while maintaining gFWER control. They also propose extending the framework to other parameters and estimating functions once suitable multivariate asymptotics are established, noting challenges in defining a bootstrap null transformation and in computation when the constrained EL objective becomes nonconvex. Finally, they point to high-dimensional settings where the number of parameters grows with $n$, and note that controlling alternative error rates (e.g., false discovery rate) would be relevant but is outside the paper’s scope.","Developing software that automates incidence-matrix handling, hypothesis specification (contrasts), and robust null-transformations for bootstrap in complex multi-layer experimental structures (e.g., repeated measures within blocks) would improve adoption. Extending the approach to explicitly model and accommodate autocorrelation/temporal structure common in repeated sampling designs (e.g., days post-planting) and to multivariate responses could broaden applicability. A systematic comparison against resampling-based multiple-comparison methods tailored for mixed models (e.g., permutation with restricted exchangeability) and against modern robust mixed-model alternatives would clarify when EL-based SCIs/tests are most advantageous. Finally, investigating computational acceleration (e.g., warm starts, parallelization, or approximations to the EL optimization) would be valuable for large $m$ pairwise-comparison problems.",2112.09206v2,https://arxiv.org/pdf/2112.09206v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:51:57Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Other,A-optimal,"Variable/General (pulse-feed amounts as decision variables for 8 parallel reactors; parameters include 18 global kinetic parameters plus local reactor parameters {kLa, kp} per reactor)",Food/agriculture|Other,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper proposes a hierarchical, online framework that couples model-based optimal experimental design (OED) with auxiliary model predictive control (MPC) to enable tractable OED for eight parallel, pulse-fed mini-bioreactor cultivations. OED computes pulse-feed profiles that maximize information for parameter identification of a mechanistic macro-kinetic growth model, using the Fisher Information Matrix and an A-optimality (trace) criterion. Because solving a fully constrained parallel OED becomes computationally intractable and tends to push operation toward active dissolved-oxygen (DOT) constraints, the authors relax constraints in the OED and enforce the key DOT lower-bound via per-reactor auxiliary MPC tracking controllers. An in-silico study of glucose-limited bacterial fed-batch cultivation with pulsed feeding shows the approach maintains feasibility (avoids oxygen limitation) while retaining near-optimal information compared with a constrained single-reactor OED benchmark. The framework advances adaptive/online OED for high-throughput parallel bioprocessing by separating information maximization from constraint handling to make closed-loop operation practical under tight decision intervals.","The OED uses the Fisher information matrix (FIM) built from output sensitivities: \(FIM \approx \sum_{r\in R}\sum_{t_i\in M} s_r(t_i)^T\Sigma_i^{-1}s_r(t_i)\), with sensitivities \(s_r(t_i)=\partial y_r/\partial\theta\) and sensitivity dynamics \(\dot s_r(t)= (\partial f/\partial x)s_r(t)+\partial f/\partial\theta\), plus jump updates at pulse times from differentiating the mass-balance map \(x(t^+)=f_d(x(t),u(t))\). The scalar design objective is A-optimality: \(\psi_A = \tfrac{1}{n_\theta}\mathrm{Tr}(C[0,t_p](\hat\theta))\), where \(C\) approximates the parameter covariance via the Cramér–Rao bound. The auxiliary MPC per reactor solves a tracking problem \(\min_{u_r}\sum_{t\in U}\|u_r(t)-u^{oed}_r(t)\|^2\) subject to reactor dynamics, bounds, and a DOT lower-bound constraint over the prediction horizon.","In a single-reactor comparison used to quantify information loss, the reported A-criterion values (lower is better) are 1.014 for the proposed hierarchical OED+auxiliary MPC, 1.106 for the fully constrained OED, and 3.386 for unconstrained OED (which can become infeasible due to oxygen limitation). In the 8-reactor in-silico study, the parallel OED is re-solved every 60 minutes due to computational cost, while each auxiliary MPC is solved every 10 minutes to enforce the DOT constraint. Sensitivities increase strongly after pulse feeds, so unconstrained OED tends to increase feed and activate/violate the oxygen constraint; the auxiliary MPC reduces pulse sizes when violation is predicted, restoring feasibility with limited information-content degradation. Measurement noise is simulated with 5% Gaussian variance (scaled) and parameter uncertainty is introduced via 10% uniform perturbations.","The authors note that shifting constraints from the OED to auxiliary MPC introduces an approximation, making some reduction in information content unavoidable relative to a (fully) constrained OED. They also indicate that full parallel constrained OED under full discretization is computationally intractable online at the required decision interval, motivating the hierarchical relaxation approach.","Performance is demonstrated only in silico; real HT bioreactor data could introduce unmodeled effects (sensor drift, calibration errors, actuation delays, mixing heterogeneity) that may degrade both identifiability and constraint satisfaction. The approach relies on local linearized sensitivity/FIM approximations around prior parameter estimates and assumes independent, Gaussian measurement noise; robustness to misspecified noise, correlated errors, or structural model mismatch is not established. Comparisons are limited (e.g., no systematic benchmarking against alternative OED criteria such as D- or I-optimality, Bayesian/robust OED formulations, or other tractable parallel-design heuristics), and computational timing/scaling metrics are not fully quantified for larger reactor counts or longer horizons.",The authors state future work will extend the strategy to full high-throughput experiments involving different experimental conditions. They also state it is important to account for parametric uncertainty by formulating robust OED.,"A natural next step is a Bayesian/robust OED formulation for the parallel setting (e.g., expected A- or D-optimality under parameter uncertainty) combined with chance-constrained or tube-based MPC to better guarantee DOT feasibility under uncertainty. Experimental validation on a real parallel cultivation platform, including actuator/sampling constraints and timing jitter, would test practical deployability. Extending the framework to handle correlated measurement noise, unknown variance parameters, or self-starting/online noise estimation could improve reliability early in batch when parameter guesses are poor. Software release (e.g., scripts and model files for do-mpc/CasADi) and reporting computation times per optimization would aid reproducibility and adoption.",2112.10548v1,https://arxiv.org/pdf/2112.10548v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:52:27Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Optimization|Prediction|Cost reduction|Other,Other,Variable/General (examples include 2 factors for Ackley/Booth; 12 parameters for airfoil case study),Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2112.10944,"The paper proposes Bayesian Sequential Sampling via Reinforcement Learning (BSSRL), a deep reinforcement-learning policy-gradient approach for sequential batch sampling in Bayesian optimal experimental design / Bayesian optimization settings with expensive black-box experiments or simulators. Unlike myopic one-step-ahead Bayesian SDOE, BSSRL selects a batch of inputs at each step while optimizing a long-horizon reward over a known finite evaluation budget, using a Gaussian process surrogate to summarize belief and a neural-network policy trained with REINFORCE. The method can be applied both to continuous action spaces and to discrete candidate sets via a nearest-neighbor mapping to a predefined grid. Empirical evaluation includes synthetic optimization (train on Ackley, test on Booth) and a high-dimensional turbine airfoil efficiency problem (12 design parameters) using a pre-generated CFD dataset, comparing against a batch Bayesian optimization baseline. Results show competitive or improved convergence, especially for larger batch sizes in the synthetic example, and modest gains over batch BO in the airfoil retrofit study for some batch sizes.","A GP surrogate is used with prior $f\sim\mathcal{GP}(0,k(x,x'))$ and an RBF kernel $k(x,x')=s^2\exp\{-\tfrac12\sum_{j=1}^d (x_j-x'_j)^2/\ell_j^2\}$. Posterior GP mean and covariance follow standard conditioning: $m_n(x)=k_n(x)^T(K_n+\sigma^2 I)^{-1}Y_n$ and $k_n(x,x')=k(x,x')-k_n(x)^T(K_n+\sigma^2 I)^{-1}k_n(x')$. The RL reward at step $i$ is defined from changes in GP minimum statistics: $r_i=-\big(m_n^{(i)}(\tilde x_{\min})-m_n^{(i-1)}(\tilde x_{\min})+\alpha(\sigma_n^{(i)}(\tilde x_{\min})-\sigma_n^{(i-1)}(\tilde x_{\min}))\big)$, with discounted return $R=\sum_{i=1}^N \gamma^i r_i$.","On the Booth test function (policy trained on Ackley), BSSRL matches batch Bayesian optimization for small batch sizes and converges faster for larger batch sizes, reportedly reaching the global minimum in fewer than ~20 function evaluations for the larger-batch settings while the baseline can require roughly 3× more evaluations in that regime. In trajectory visualizations (e.g., batch size 3), BSSRL’s selected batches appear more space-filling near the optimum earlier than batch BO, which tends to cluster and initially sample more suboptimal regions. For the 2D airfoil efficiency retrofit problem (12 parameters; 600 pre-generated CFD simulations; initial dataset of 6 points), BSSRL is competitive and for batch sizes 1 and 5 reaches the best-known efficiency using about ~10 fewer queried points than batch BO, while performance can degrade for larger batches due to the high-dimensional policy input and discrete-grid nearest-neighbor approximation. The paper notes BSSRL can underperform batch BO for large batch sizes because the policy input dimension grows as $d\times n + 2$, making training harder with fixed episodes.","The authors state they do not use a fully Bayesian treatment of GP hyperparameters, instead estimating them by maximizing likelihood, and they acknowledge known drawbacks of this approximation. They also note that for discrete/mesh-grid objective functions (e.g., selecting from a pre-generated simulation set), BSSRL’s continuous-action formulation requires approximations (averaging sampled batches and mapping to nearest neighbors), which can delay convergence and makes the retrofit airfoil setup not fully compatible with the original formulation. Additionally, they mention BSSRL can perform worse than batch BO for large batch sizes because the policy’s input dimension grows linearly with $d\times n + 2$, making it difficult to train well under a fixed training budget.","The comparison baseline appears limited primarily to a particular batch BO strategy (“believer”); broader comparisons to common parallel BO methods (e.g., qEI/qKG variants, local penalization, Thompson sampling, determinantal point processes) and to batch Bayesian experimental design utilities could change conclusions. The RL state uses only partial GP summary statistics (minimum mean and std plus selected points), so the learned policy may be sensitive to this state representation and may not generalize across kernels/noise regimes or constraint types without retraining. Computational cost is a potential barrier: training a policy with many episodes can be expensive relative to standard batch BO, but wall-clock comparisons are not clearly quantified. Practical robustness aspects (observation noise misspecification, model mismatch, constraints, multiobjective trade-offs) and uncertainty calibration are not thoroughly stress-tested in extensive benchmarks.","The paper indicates future opportunities in improving the computational demands of training the optimal policy and in developing/choosing better analytic formulations of the utility (reward) function for different tasks. It also suggests the method has potential to outshine prior methods in scenarios with strong logistical constraints and limited ability to gather an initial surrogate-training dataset, motivating further application to challenging engineering problems.","Developing principled batch utility criteria (e.g., expected information gain, qEI/qKG-style objectives) within the RL reward and studying their impact on stability/generalization would strengthen the connection to Bayesian OED. Extending BSSRL to handle constraints, multiobjective design/optimization, and noisy/heteroscedastic observations—common in physical experiments—would improve practical applicability. More rigorous benchmarking on standard BO/OED test suites with equal compute budgets (including wall-clock time) and ablations on state design, kernel choice, and training horizon would clarify when RL provides net benefit. Providing an open-source implementation and exploring offline-RL variants (learning policies directly from historical experiment logs) would facilitate adoption and reproducibility.",2112.10944v2,https://arxiv.org/pdf/2112.10944v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:53:08Z
TRUE,Sequential/adaptive|Other,Optimization|Prediction|Cost reduction|Other,Other,"3 factors (Cell type, Molecule type, Molecule concentration); concentration restricted to 4 levels in experiments",Healthcare/medical|Pharmaceutical,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an active learning (AL) scheme to optimize scientific experimental design by adaptively selecting which experiments to run, demonstrated retrospectively on a drug-response dataset of breast cancer cell lines and small-molecule perturbagens. The predictive component combines matrix factorization via Alternating Least Squares (ALS) to learn latent cell/molecule features with a fully connected deep neural network (ALSDL) trained using an RMSE-based loss augmented with a penalty for incorrect sign (classification) relative to a decision boundary. For experiment selection, it introduces an Expected Loss Minimization (ELM) query strategy that chooses the next unlabeled experiment(s) expected to minimize overall loss when added to the training set (using model-predicted labels to approximate the expectation). In comparative retrospective AL runs, ELM sampling is reported to converge faster and achieve lower RMSE and higher accuracy than orderly (manual-like), random, and uncertainty sampling strategies for both GR and IFD endpoints. The work positions AL as a practical approach to reduce physical experimental burden while improving predictive model quality in automated drug-response experimentation.","The AL query strategy is defined by expected loss minimization: select $x_{\mathrm{ELM}}=\arg\min_{x^+\in U}\,\mathrm{loss}(\hat M^+, D\cup \hat U)$, where $\hat M^+$ is retrained after adding $(x^+,\hat y^+)$ using the current model’s prediction to approximate unknown labels in $U$. The ALSDL training objective augments RMSE with a sign-based classification penalty: $\mathrm{loss}=\mathrm{RMSE}-\beta\frac{1}{n}\sum_{i=1}^n \mathrm{sign}(\hat y_i y_i)$ (generalized to multi-class via boundaries $c_j$). The response metrics include $\mathrm{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n (\hat y_i-y_i)^2}$ and the domain definitions $\mathrm{GR}=\frac{2^{\log_2(x(c)/x_0)}}{\log_2(x_{ctrl}/x_0)}$ and $\mathrm{IFD}=f_d(c)-f_{d,ctrl}$.","On 10-fold cross-validation, ALSDL improves over ALS for both endpoints: for GR, average test RMSE decreases from 0.250860068 (ALS) to 0.160126785 (ALSDL) and accuracy increases from 0.854432773 to 0.872478992. For IFD, the reported table shows average test RMSE 0.0097235223 (ALS) vs 0.054959413 (ALSDL) with accuracy 0.748781513 vs 0.818277311 (note the IFD RMSE value appears inconsistent with the text/curves claiming lower RMSE around 0.02 for ALSDL). In the active learning study (initialized with 40 labeled samples, adding 40 per query for 8 queries; 360 total), ELM sampling achieves the lowest loss and highest accuracy after the query budget, outperforming orderly, random, and uncertainty sampling on both GR and IFD in the shown learning curves.","The author notes that due to limited computational power, hyperparameters were not heuristically tuned and were manually set based on the optima of several trials. They also state the FCNN architecture is not necessarily optimal and requires further investigation. They suggest performance should not vary significantly with hyperparameter shifts because the scheme is built on mature/robust algorithms.","All evaluations are retrospective on a fully observed dataset; real-world AL deployment would require actual lab execution, measurement noise, batching constraints, and delayed outcomes, which are not modeled. The ELM strategy approximates expected improvement by using pseudo-labels from the current model, which can amplify model bias/confirmation and lacks uncertainty integration (e.g., Bayesian treatment) or guarantees. Comparisons are limited to a few baselines (orderly/random/uncertainty) and do not include common DOE/optimal design or Bayesian optimization baselines (e.g., D-/I-optimal, GP-based BO, Thompson sampling), making it hard to position performance relative to established design methods. Software/implementation details are insufficient for exact replication (no code, unspecified frameworks), and one reported IFD RMSE result appears internally inconsistent with the narrative/plots.",The paper states that more datasets will be tested and more experiments will be conducted on the proposed AL scheme to improve its generality. It also indicates the FCNN setting and hyperparameters require further investigation/optimization.,"Evaluate the approach against classical optimal experimental design and Bayesian optimization baselines under comparable budgets, including batch/parallel query settings typical of high-throughput biology. Incorporate uncertainty-aware expected loss (e.g., Bayesian neural nets, ensembles) rather than point pseudo-labels to better approximate expectation and reduce bias. Extend to settings with unknown/estimated noise, replicate measurements, and temporal/operational constraints (plate layouts, blocking, randomization, hard-to-change factors). Provide an open-source, reproducible implementation and conduct prospective wet-lab validation to confirm resource savings and predictive gains hold beyond retrospective analyses.",2112.14811v1,https://arxiv.org/pdf/2112.14811v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:53:44Z
TRUE,Sequential/adaptive|Bayesian design|Computer experiment|Optimal design|Other,Prediction|Cost reduction|Other,Other,"Variable/General (demonstrated with d = 1, 2, 8; and a 2D CFD case with inputs {A, L})",Transportation/logistics|Energy/utilities|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/umbrellagong/MFGPextreme,"The paper proposes a multi-fidelity sequential Bayesian experimental design (BED) method to efficiently estimate the full response probability density function (PDF) of an expensive input-to-response system, with emphasis on extreme (tail) events. A multi-fidelity Gaussian process surrogate (Kennedy–O’Hagan autoregressive model) is combined with a new acquisition that greedily selects both the next input location and the fidelity level by maximizing expected uncertainty reduction per unit computational cost. The acquisition targets tail accuracy by using an output-weighted criterion and is further made practical in higher dimensions by deriving an analytic approximation (via Gaussian-mixture approximation of the weight function) and its gradient, enabling gradient-based optimization. The method is validated on synthetic 1D/2D/8D problems and on an engineering case estimating extreme ship roll statistics using CFD with two grid resolutions as low/high fidelity, showing faster convergence than single-fidelity and fixed-fidelity-hierarchy baselines. The authors release Python code (MFGPextreme) implementing the algorithm.","The design objective is to sequentially choose $(i^*,\tilde x)$ maximizing benefit per cost: $(i^*,\tilde x)=\arg\max_{i,\tilde x} B(i,\tilde x)/c_i$ (Eq. 14). The benefit is the reduction in an output-weighted integrated posterior variance of the high-fidelity response, yielding $B(i,\tilde x)=\frac{1}{\mathrm{var}(y_i(\tilde x)|D)}\int \mathrm{cov}^2\big(f(x),f_i(\tilde x)|D\big)\,w(x)\,dx$ with $w(x)=p_x(x)/p_f(f(x))$ (Eqs. 13, 12). Multi-fidelity coupling uses the autoregressive model $f_i(x)=\rho_{i-1} f_{i-1}(x)+d_i(x)$ with GP priors (Eq. 3) and RBF kernels (Eq. 4), and analytic evaluation uses a GMM approximation to $w(x)$ to avoid numerical integration (Eqs. 17–19).","Across synthetic testbeds (dimensions 1, 2, and 8), the adaptive bi-fidelity method (BF-O) consistently achieves the lowest or near-lowest error in the estimated response PDF tail compared with single-fidelity optimal sampling (SF) and bi-fidelity methods with fixed low/high sampling ratios (BF-F$n$). In the 1D Forrester example, at total cost $c=6$ BF-O attains an error nearly two orders of magnitude smaller than SF (Fig. 1b). In the 2D oscillator studies spanning multiple low-fidelity accuracies and cost ratios, BF-O ranks 1st or 2nd at cost 80 across all 10 cases (Table 1). For the 8D borehole model, BF-O again outperforms SF and all tested BF-F$n$ baselines (Fig. 5), and the analytic acquisition drastically reduces per-iteration optimization time relative to criteria requiring retraining or numerical integration (Fig. 6).","The authors note the method is a greedy, one-step-ahead strategy and “there is no guarantee that successively applying (14) provides a global optimal solution,” so its validity must be tested empirically against fixed-hierarchy approaches. They also indicate that without their analytic approximation, numerical integration in the acquisition can become prohibitively expensive for high-dimensional problems, motivating the GMM-based analytic evaluation. For the CFD ship-motion case, they evaluate convergence using an uncertainty measure because the true PDF is unknown, limiting direct error quantification against ground truth.","The approach relies on Gaussian-process modeling assumptions (e.g., kernel choice, stationarity, and Gaussian observation noise) and the Kennedy–O’Hagan autoregressive structure, which may be misspecified for some multi-fidelity relationships (e.g., strong nonlinearity/discontinuities between fidelities). Tail-focused weighting uses $w(x)=p_x(x)/p_f(f(x))$ and then approximates it with a Gaussian mixture; if $p_f$ is poorly estimated early on or the tail structure is highly non-Gaussian/multimodal, the acquisition may be biased. The paper focuses primarily on bi-fidelity demonstrations; extension behavior with many fidelity levels (s>2) and robustness to hyperparameter uncertainty/overfitting are not deeply examined. Comparisons are limited to SF and fixed-ratio BF hierarchies, omitting other modern multi-fidelity active learning strategies (e.g., information gain/entropy-based, mutual information, or Bayesian quadrature variants) that could be competitive for tail estimation.","They suggest improving the framework by incorporating reinforcement learning to further enhance decision-making in multi-fidelity sampling, which they state will be a topic of future study.","Developing a fully Bayesian treatment of GP hyperparameters (and fidelity correlation parameters) could improve robustness of the acquisition, especially early in sequential design when data are scarce. Extending the method to handle non-i.i.d. inputs or temporal/spatial correlations (e.g., stochastic process inputs) and to incorporate constraints (feasible regions) would broaden applicability. Additional work could explore alternative tail-targeted utilities (e.g., direct estimation of exceedance probabilities, quantiles, CVaR) and theoretical guarantees or bounds on regret/cost-efficiency in the multi-fidelity setting. Packaging the method into a maintained Python library (with reproducible benchmarks and OpenFOAM coupling scripts) would improve practical adoption.",2201.00222v1,https://arxiv.org/pdf/2201.00222v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:54:21Z
TRUE,Optimal design,Parameter estimation,D-optimal|G-optimal,Variable/General (design points on a finite set X; model dimension N and candidate set size M),Theoretical/simulation only,Simulation study,TRUE,MATLAB,Personal website,https://www.math.unipd.it/~fpiazzon/Software/OptimalDesignComputation/,"The paper proposes an algorithm to compute D-optimal experimental designs on a finite candidate set by following the long-time asymptotics of the gradient flow of the log-determinant of the information (Gram) matrix. The design problem is reformulated from a constrained maximization over nonnegative weights summing to one into an unconstrained real-analytic minimization in squared coordinates, enabling use of gradient-flow theory and a backward-Euler/Newton numerical scheme. The author proves global existence and convergence of the continuous-time gradient flow to a minimizer (yielding a D-optimal design), and proves convergence and provides rate estimates for the proposed discrete algorithm under step-size conditions, including an adaptive step-size variant. Ill-posed/non-unique cases are handled via a strongly convex regularization and (optionally) Carathéodory–Tchakaloff compression to recover sparse designs with the same moments. Numerical experiments (polynomial regression on grids/point clouds/meshes) demonstrate convergence behavior and compare against the Titterington multiplicative algorithm, using an accompanying MATLAB package.","The information (Gram) matrix for weights $w\in\mathbb{R}^M_{\ge 0}$ is $G(w)=V^T\,\mathrm{diag}(w)\,V=\sum_{i=1}^M w_i\,V_{i,:}^T V_{i,:}$, and D-optimality maximizes $\det G(w)$ subject to $\|w\|_1=1$. An equivalent unconstrained objective is $F(z)= -\frac{1}{N}\log\det G(z^2)+\|z\|_2^2$ with $w=z^2$, whose gradient flow $\dot z=-\nabla F(z)$ converges to a minimizer. The gradient has components $\partial_i F(z)=2 z_i\left(1-\frac{B(x_i;z^2)}{N}\right)$, where $B$ is the Bergman function built from the $w$-orthonormalized basis, and the discrete scheme minimizes $g(z;z_k,\tau)=F(z)+\frac{\|z-z_k\|_2^2}{2\tau}$ (backward Euler) solved by Newton iterations.","The paper proves equivalence between the standard constrained D-optimal design problem, a convex unconstrained problem in weights, and a real-analytic unconstrained problem in squared coordinates (Theorem 1). It proves global real-analytic well-posedness of the gradient flow and convergence of trajectories to a minimizer (Theorem 2), and establishes convergence of the backward-Euler/Newton algorithm (Algorithms 1–2) to an optimal design under a suitable time-step bound, with rate estimates via the Łojasiewicz inequality (Theorem 4, Proposition 8). In numerical experiments, the algorithm reaches KKT residuals around $10^{-15}$ on several polynomial-design examples and shows markedly faster KKT-residual decrease versus CPU time than the Titterington multiplicative algorithm in a large random-point test (Experiment 5). In ill-posed cases, the regularized approach plus Carathéodory–Tchakaloff compression can dramatically reduce support size (e.g., from 321 to 15 points in one reported restart setting).",None stated.,"The method is developed for finite candidate sets; continuous design regions require a separate discretization step (and performance depends on that discretization). The experimental section focuses on polynomial regression examples and does not provide broad benchmarking across different model types, noise structures, or constraints (e.g., exact integer run counts). While a MATLAB package is provided, reproducibility depends on external implementation details (e.g., numerical orthogonalization choices), and no standardized test-suite/results table is included to compare settings across problems.",The paper suggests (in the ill-posed setting) using a sequence of regularized problems with decreasing regularization parameter and then applying Carathéodory–Tchakaloff compression to obtain sparse designs; it also notes analogous convergence-rate results can be proven for the regularized objective $F_\eta$ (Remark 5).,"Extending the approach to common practical constraints (exact/rounded run counts, cost-weighted candidate points, or additional linear constraints on weights) would increase applicability. A systematic study of discretization strategies for continuous regions (e.g., adaptive candidate-set refinement guided by the Bergman function) could connect the finite-set method to continuous optimal design more directly. Providing an open benchmark suite and additional comparisons with modern convex/Frank–Wolfe and interior-point D-optimal design solvers would clarify computational tradeoffs, especially for large M and higher N.",2201.03042v1,https://arxiv.org/pdf/2201.03042v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:54:55Z
TRUE,Optimal design|Other,Prediction|Parameter estimation|Other,D-optimal,Variable/General (Bayesian linear regression feature dimension d; experiments indexed by feature vectors x ∈ X; examples use d=100 and |X|=20),Network/cybersecurity|Other|Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/neuspiral/Networked-Learning,"The paper proposes “experimental design networks,” a framework where multiple geographically distributed learners train (possibly different) Bayesian linear regression models by consuming Poisson data streams transmitted over a capacity-constrained multi-hop network. Each learner’s utility is the expected D-optimal design objective (log-determinant of the Bayesian information/precision matrix, equivalently minimizing estimator covariance volume), and the network’s goal is to choose edge/flow rate allocations to maximize aggregate learner welfare under link-capacity and flow-conservation/flow-bounding constraints. The authors show the resulting objective is monotone and continuous DR-submodular via a Poisson-based continuous relaxation of the classic (integer-lattice) DR-submodular D-optimality objective. They develop a Frank–Wolfe-type algorithm using a novel gradient estimator based on truncation of infinite Poisson sums and Monte Carlo sampling, and prove a (1−1/e)-approximation guarantee (up to additive error) in polynomial time. Extensive simulations across synthetic and real backbone topologies show the method outperforms baselines (max-sum-rate, α-fair rate utility, and projected gradient ascent) in both aggregate utility and downstream model estimation error.","Learner utility uses the D-optimal design criterion for Bayesian linear regression: for counts n = [n_x]_{x∈X}, G(n)=\log\det\left(\sum_{x\in X}\frac{n_x}{\sigma_t^2}xx^\top+\Sigma_0^{-1}\right). With Poisson arrivals n_x\sim\text{Poisson}(\lambda_x T), learner utility is U_\ell(\lambda_\ell)=\mathbb{E}[G_\ell(n_\ell)], and the network objective maximizes \sum_\ell (U_\ell(\lambda_\ell)-U_\ell(0)) subject to link capacities \sum_{x,t}\lambda^e_{x,t}\le\mu_e and flow constraints. The gradient is expressed as \partial U/\partial\lambda_{\ell,x}=T\sum_{n\ge0}\Delta_{\ell,x}(\lambda_\ell,n)\Pr[n_{\ell,x}=n], with \Delta defined by conditional one-step marginal gains; it is estimated by truncation and sampling.","The authors prove the aggregate network utility is monotone and continuous DR-submodular under Poisson streaming arrivals, enabling DR-submodular maximization tools. They provide a Frank–Wolfe variant with a Poisson-tail-bound-based truncated-and-sampled gradient estimator, and show the algorithm achieves U(\lambda_K)\ge(1-e^{\epsilon_1-1})\max_{\lambda\in D}U(\lambda)-\epsilon_2 with probability 1-\epsilon_0 (i.e., approaching the classic 1−1/e factor). In simulations on multiple synthetic topologies and real backbones (GEANT, Abilene, Deutsche Telekom), the proposed FW method yields the highest normalized aggregate utility and the lowest average norm of estimation error per learner compared to MaxSum, MaxAlpha, and projected gradient ascent. Sensitivity experiments varying source rates and bottleneck link capacities show FW remains best across settings and can achieve better model quality with fewer effective source rates than non-experimental-design baselines.",None stated.,"The experimental-design component is restricted to Bayesian linear regression with (largely) Gaussian noise assumptions and relies on Poisson arrival/stability assumptions (e.g., Kelly-network/queueing conditions) to obtain independent Poisson arrivals at learners; performance and guarantees may not extend to correlated traffic, non-Poisson arrivals, or significant temporal dependence. The numerical evaluation is simulation-based with synthetic feature generation and priors; there is no real-world deployment/trace-driven validation of traffic, queueing, or learning performance. Practical implementation may be challenging because flows are indexed by both type t and feature x (routing/labeling overhead), and the gradient estimation uses Monte Carlo sampling and truncation choices that may be computationally heavy at scale.","The authors suggest exploring other experimental design objectives beyond D-optimality (e.g., A-optimality and E-optimality) and incorporating data source generation costs. They also point to distributed and adaptive implementations of the proposed rate allocation schemes. They propose extending to non-linear learning tasks (e.g., deep neural networks) using approximate Bayesian inference such as variational inference, and studying a multi-stage setting where learners update posteriors/priors across stages to analyze resulting system dynamics under network constraints.","Developing robust/self-starting variants that handle unknown noise variances, model misspecification, and non-Gaussian/noisy labels would improve applicability. Extending the framework to autocorrelated or bursty arrivals (beyond Poisson) and to time-varying capacities with online regret guarantees would better match operational networks. Providing open-source implementations with reproducible experiment scripts (including exact environment requirements) and adding trace-driven or real testbed evaluations would strengthen empirical evidence and adoption.",2201.04830v1,https://arxiv.org/pdf/2201.04830v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:55:33Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal,2 factors (convection parameters RH and τ); design point k indexes latitude and (in seasonal case) season/time window,Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Julia|Other,Public repository (GitHub/GitLab),https://doi.org/10.5281/zenodo.6679974,"The paper proposes an ensemble-based, embarrassingly parallel Bayesian experimental design algorithm to target where/when to acquire limited-area climate data (e.g., observations or high-resolution simulations) to maximally reduce uncertainty in parameters of a global climate model (GCM). Design points k correspond to spatial locations (latitudes) and, in cyclostationary simulations, to both latitude and season; utility is defined via D-optimality as the inverse determinant of the posterior covariance of parameters given restricted data. The method is integrated into the calibrate–emulate–sample (CES) framework (with a CEES variant) using ensemble Kalman inversion to generate training runs, Gaussian-process emulation of time-averaged climate statistics, and MCMC to sample posteriors needed for utility evaluation, requiring only O(10^2) forward GCM evaluations. Demonstrations with an idealized aquaplanet GCM target data for calibrating two convection parameters (RH and relaxation timescale τ) using latitude-dependent statistics (humidity, mean precipitation, and extreme-precipitation frequency), in both stationary and seasonal-cycle settings. Results show highest information gain typically near the ITCZ (especially for seasonal cases and wider latitude stencils), but for narrow single-latitude designs the subtropical precipitation-minimum region can be most informative; the predicted utility generally ranks site informativeness correctly though it may misestimate absolute gains due to sampling variability and noise inflation.","The local-data inverse problem is posed as $z_k = S_\infty(\theta;k) + \delta_k + \sigma_k$ with internal-variability noise $\sigma_k\sim\mathcal N(0,\,W_k\Sigma(\theta)W_k^T)$ and restriction operator $W_k$ selecting the design point (Eqns. (2),(5),(7)). Design utility uses D-optimality: $U(W_k)=\big(\det\,\mathrm{Cov}(\theta\mid W_k y)\big)^{-1}$ (Eq. (6)), where the posterior covariance is estimated from posterior samples obtained via CES/GP-emulator + MCMC.","In stationary single-latitude designs, the highest-utility latitude is around ±19° (subtropical precipitation minima), outperforming equatorial designs; posterior-utility values reported for four sites are 26.4 (−19°), 13.9 (−3°), 4.4, and 1.7 (high latitude). With wider stationary stencils (ℓ=3), optimal utility shifts closer to the equator/ITCZ region (e.g., centered near −8° with reported utility 126.0 vs 35.3 at −3° and 97.5 at −19°). In seasonal-cycle designs, optimal sites are near the migrating ITCZ (±3°) in solstice seasons with much larger utilities (reported 131.9 and 154.7 at optimal solstice designs) than secondary subtropical peaks (47.9 and 39.5 near ±30°). Shorter averaging (T=30 days) increases noise and can yield broader/multimodal posteriors, reducing targeting effectiveness.","The authors note that the targeting algorithm can become less effective when available data are very noisy (e.g., shorter averaging windows), and posterior distributions can become more multimodal/non-identifiable. They also state that although the algorithm often predicts the ordering of informativeness across sites, it does not necessarily accurately predict absolute information content because of sampling variability and additional model-error inflation. They further note that naive evaluation of utilities at all design points is inefficient for very large candidate design spaces and that practical site selection also involves considerations beyond optimal experimental design.","The demonstrations are in a perfect-model/synthetic-data setting using an idealized zonally symmetric aquaplanet GCM with only two uncertain parameters, so performance and optimal locations may not transfer directly to comprehensive GCMs with many interacting parameters and structural discrepancies. The approach relies on approximating internal-variability covariance as fixed at a control parameter (e.g., $\Sigma(\theta)\approx\Sigma(\theta^*)$), which may bias utilities when variability is strongly parameter-dependent. Utility is computed from posteriors obtained via GP emulation and MCMC; emulator misspecification, limited training coverage from EKI samples, and MCMC convergence/mixing could affect covariance/determinant estimates, especially in higher dimensions or multimodal settings.","The authors suggest moving beyond naive exhaustive evaluation of utilities over all design points for large design spaces, highlighting the need for more sophisticated optimization methods and faster determinant evaluation/approximation techniques (e.g., Laplace approximations, polynomial chaos surrogates, bounds optimization, randomized determinant approximations, and Gaussian-process surrogate optimization). They also discuss applying the framework to comprehensive climate models and to practical targeting of embedded high-resolution simulations or observational campaigns, where additional constraints and considerations arise.","Extend the method to multi-point (K>1) joint designs using scalable combinatorial or submodular-optimization strategies with posterior-covariance updates, and assess tradeoffs between sequential/adaptive versus batch selection. Develop robust/self-starting versions that account for unknown or parameter-dependent variability and model discrepancy (including non-Gaussian/internal-variability structure and autocorrelation), and quantify sensitivity of utilities to covariance estimation error. Provide open-source reference implementations/workflows (e.g., Julia packages) with diagnostics for emulator error and MCMC convergence, and validate on real observational/high-resolution-simulation datasets and on higher-dimensional parameter calibrations in more realistic GCMs.",2201.06998v2,https://arxiv.org/pdf/2201.06998v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:56:16Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (examples include up to 16,641 uncertain parameters; up to 100 design variables; chooses r sensors from d candidates, e.g., 15 out of 50 or 100)",Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://doi.org/10.5281/zenodo.4608729|https://doi.org/10.11588/ans.2015.100.20553,"The paper develops a scalable Bayesian optimal experimental design (OED) method for PDE-governed inverse problems with very high-/infinite-dimensional parameters, focusing on optimal sensor placement. The design criterion is expected information gain (EIG; mutual information / expected KL divergence) and the design problem is combinatorial (select r sensors from d candidate locations), solved via a greedy selection algorithm. To make repeated EIG evaluation tractable, the authors replace expensive PDE solves inside a double-loop Monte Carlo (DLMC) EIG estimator with a derivative-informed projected neural network (DIPNet) surrogate of the parameter-to-observable map, using active-subspace (input) and POD (output) projections plus an adaptively built low-rank ResNet. They provide bounds showing EIG and evidence/normalization-constant approximation errors are of the same order as the DIPNet generalization (RMS) error. Numerical studies on inverse Helmholtz scattering and nonlinear reactive transport (up to 16,641 parameters; up to 100 candidate sensors/design variables) show near-DLMC accuracy with up to ~three orders of magnitude speedup compared with a reference double-loop Monte Carlo approach.","Bayesian OED is posed as maximizing EIG over sensor-selection designs: $W^*=\arg\max_{W\in\mathcal W}\Psi(W)$ with $\Psi(W)=\int \int \log\!\left(\frac{\pi_{\mathrm{like}}(y\mid m,W)}{\pi(y,W)}\right)\,\pi_{\mathrm{like}}(y\mid m,W)\,dy\,d\mu_{\mathrm{pr}}(m)$. EIG is estimated by DLMC: $\Psi_{\mathrm{dl}}(W)=\frac1{n_{\mathrm{out}}}\sum_{i=1}^{n_{\mathrm{out}}}\log\left(\frac{\pi_{\mathrm{like}}(y_i\mid m_i,W)}{\hat\pi(y_i,W)}\right)$ with inner-loop evidence estimator $\hat\pi(y_i,W)=\frac1{n_{\mathrm{in}}}\sum_{j=1}^{n_{\mathrm{in}}}\pi_{\mathrm{like}}(y_i\mid m_{i,j},W)$. The DIPNet surrogate replaces $F_d$ by $\tilde F_d(m)=\Phi_r\,f_r(V_r^\top m;w)+b$, yielding a surrogate evidence $\tilde\pi(y_i,W)=\frac1{n_{\mathrm{in}}}\sum_j\exp\big(-\tfrac12\|y_i-W\tilde F_d(m_{i,j})\|^2_{\Gamma_n^{-1}}\big)$ and approximate EIG $\Psi_{\mathrm{nn}}(W)$.","Across two PDE testbeds (Helmholtz inverse scattering and nonlinear advection–diffusion–reaction inverse transport), DIPNet surrogates achieved high test accuracy for observables (reported $\ell^2$ accuracy of 81.56% for Helmholtz and 97.13% for the nonlinear transport example). For evidence and EIG estimation, DIPNet-enabled DLMC closely matches a high-sample Monte Carlo reference (e.g., inner-loop $n_{\mathrm{in}}=60000$), while a simple MC/DLMC run at comparable PDE-solve cost shows much larger bias and error. The method is demonstrated on problems with 16,641 uncertain parameters and up to 100 candidate sensors/design variables, and the authors report up to three orders of magnitude speedup relative to a reference double-loop Monte Carlo computation. Greedy sensor-selection using the surrogate EIG consistently finds designs with higher true DLMC EIG than randomly sampled designs in their experiments.",None stated.,"The approach relies on the existence of strong low-dimensional structure (rapid spectral decay in active-subspace and POD operators); if the parameter-to-observable map is not sufficiently compressible, DIPNet accuracy and thus EIG accuracy may degrade. The EIG estimator itself is based on DLMC and still uses many inner-loop samples for accurate evidence estimation; while PDE solves are avoided via the surrogate, the Monte Carlo variance/bias tradeoffs and sensitivity to likelihood concentration remain. The paper demonstrates performance on two PDE examples but provides limited guidance on robustness under model misspecification, non-Gaussian/non-additive noise, correlated observations, or strong model discrepancy common in field experiments. Practical deployment would benefit from publicly released implementations and clearer end-to-end computational cost breakdowns (including training, basis construction, and optimization wall times).",Future work will focus on gradient-based optimization using derivative information of DIPNet with respect to both parameters and design variables; exploring alternative OED criteria such as A-optimality and D-optimality; and developing/assessing new neural network architectures for intrinsically high-dimensional Bayesian OED problems.,"Extending the method to handle non-Gaussian and/or correlated noise models, model discrepancy, and temporally/spatially correlated data would broaden applicability in real sensing systems. Providing adaptive stopping/accuracy control for EIG estimation (e.g., multilevel or randomized estimators) coupled to surrogate error estimates could make the method more reliable and cost-efficient. Developing self-starting or online/sequential design workflows (updating the surrogate and design after collecting data) would align with practical experimental campaigns. Releasing reproducible code and standardized benchmarks would enable broader validation and comparison against other modern Bayesian OED methods (e.g., variational MI estimators, Laplace-based EIG, and low-rank Hessian approaches).",2201.07925v2,https://arxiv.org/pdf/2201.07925v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:56:55Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Case study (real dataset),TRUE,MATLAB|Other,Not provided,https://astrojax.com,"The paper designs and builds an experimental setup to study 3D trajectories of an Astrojax toy modeled as a nontraditional double-pendulum system, combining Lagrangian-mechanics modeling with 3D motion capture measurements. Using a binocular 3D capture system (infrared illumination and reflective markers), the authors obtain 3D positions of the string free end and two balls and identify two operating modes: a “lagging pendulum” driven by horizontal circular motion of the free end, and a “celestial pendulum” produced by manual vertical closed-loop excitation. For the lagging mode, the stable-motion assumptions simplify the five-DOF Lagrange equations and yield relations including a phase-lag condition and an evaluative expression for an internal coordinate (e.g., λ) that is compared to measured values. For the celestial mode, the system is simplified to a planar model and simulated numerically (Mathematica/Mathematics) via coupled ODEs to predict trajectory shape, amplitude ratios, and period relationships, which are then compared with experiments; reported fits show good agreement (e.g., R^2≈0.9066 and mean deviation ≈5.5% for one period comparison). Overall, this is an experimental physics/mechanics study with an “experimental design” in the pedagogical/engineering sense, but it does not present DOE (designed experiments) methodology such as factorial/optimal designs or design optimality criteria.","Key formulas define the mechanics models rather than a designed-experiments plan. The full model writes kinetic energy $T=\tfrac12 m_1(\dot x_1^2+\dot y_1^2+\dot z_1^2)+\tfrac12 m_2(\dot x_2^2+\dot y_2^2+\dot z_2^2)$ and potential $U=m_1gz_1+m_2gz_2$, then uses $L=T-U$ and five Euler–Lagrange equations over generalized coordinates (e.g., $\lambda,\theta_1,\theta_2,\phi_1,\phi_2$). For the lagging mode the driven free-end trajectory is set as $\varepsilon_x(t)=\gamma\cos(\beta t)$ and $\varepsilon_y(t)=r\sin(\beta t)$ with stability constraints (constant $\omega$, fixed angles) to derive phase relations and an evaluative expression for $\lambda$. For the celestial mode a reduced planar Lagrangian $L=\tfrac12(m_1+m_2)\dot x^2+\tfrac12 m_2(l^2\dot\varphi^2+2l\dot x\dot\varphi\cos\varphi)+m_2gl\cos\varphi$ leads to coupled ODEs (Eqs. 11–12) solved numerically.","For the lagging mode, experimental measurements of the internal length-like parameter $\lambda$ (from 3D capture) agree with the theoretical expression, with reported theory–experiment error within about 10% across tested driving speeds. For the celestial mode, simulations and experiments indicate ball-2 trajectories form ellipses with major/minor axis ratio close to 2:1, and the z-direction amplitude is about twice the x-direction amplitude. A period comparison after an energy correction using 22 datasets yields small discrepancies: 11 groups with <5% deviation and 17 groups with <10% deviation; linear fit reports $R^2\approx0.9066$ and mean deviation about 5.5%. The study further reports that orbit aspect ratio and amplitude depend on the mass ratio $m_2/m_1$ (and not on string length in certain relationships), while period depends jointly on string length and mass ratio.",None stated.,"The study varies parameters (e.g., fan speed/drive rate, mass ratio, string length) but does not use a formal DOE plan (randomization, replication strategy, blocking, or factorial structure), so uncertainty quantification and factor-effect attribution are limited. Manual closed-loop excitation (human feedback) introduces uncontrolled variability and potential bias, making repeatability and comparability across trials harder without standardized protocols. The paper reports fit quality and percent deviations but provides limited details on measurement uncertainty propagation (beyond camera error bounds) and on how energy corrections/generalized forces are specified. Generalization to different excitation patterns, damping/friction, and environmental disturbances is not systematically tested.",None stated.,"A structured DOE (e.g., factorial or response-surface design) over controllable factors such as drive frequency/amplitude, mass ratio, and string length could quantify main effects/interactions on trajectory features and period with confidence intervals. Replacing manual excitation with an instrumented actuator plus feedback control would improve repeatability and allow controlled comparisons between open-loop and closed-loop forcing. Extending the model/experiments to include damping, parameter uncertainty, and robustness to disturbances (and reporting calibrated uncertainty budgets) would strengthen practical applicability. Publishing analysis scripts and a reproducible workflow (MATLAB/Mathematica notebooks) would enable independent verification and classroom adoption.",2201.08572v2,https://arxiv.org/pdf/2201.08572v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:57:34Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Model discrimination|Other,Bayesian D-optimal|Other,Variable/General,Finance/economics|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/csiro-mlai/RL-BOED,"The paper proposes a deep reinforcement learning (RL) approach to Bayesian optimal sequential experimental design by reformulating sequential experimental design (SED) as a (hidden-parameter) Markov decision process and training a stochastic policy to maximize a sequential prior-contrastive-estimation (SPCE) lower bound on expected information gain (EIG). The main contribution is an MDP/HIP-MDP construction with a dense, factorized reward that decomposes the terminal SPCE objective into per-step rewards, improving credit assignment and exploration while avoiding repeated posterior inference. Unlike amortized deep adaptive design (DAD), the RL formulation does not require a differentiable likelihood and can optimize both continuous and discrete design spaces, including black-box models. Empirical studies on three BOED benchmarks (source location, constant elasticity of substitution, and a discrete prey-population/predator–prey functional response task) show RL achieves state-of-the-art or comparable EIG while retaining fast deployment via a neural policy forward pass. The work advances BOED by enabling generic off-the-shelf deep RL algorithms (e.g., REDQ) to learn non-myopic experimental design policies with improved exploration compared to deterministic amortized baselines.","The BOED objective is expected information gain: $d^*=\arg\max_{d\in\mathcal D}\,\mathbb E_{p(y\mid d)}[H(p(\theta)) - H(p(\theta\mid y,d))]$. For non-myopic SED, they optimize the SPCE lower bound $\mathrm{sPCE}(\pi,L,T)=\mathbb E[g(\theta,h_T)]$ with $g(\theta,h_T)=\log \frac{p(h_T\mid \theta_0,\pi)}{\frac{1}{L+1}\sum_{l=0}^L p(h_T\mid \theta_l,\pi)}$. Their dense reward factorization is $R_t=\log p(y_t\mid \theta_0,d_t) - \log(C_t\cdot \mathbf 1)+\log(C_{t-1}\cdot \mathbf 1)$ where $C_t=[\prod_{k=1}^t p(y_k\mid \theta_l,d_k)]_{l=0}^L$, yielding $\sum_{t=1}^T R_t=g(\theta,h_T)$ (with $\gamma=1$) and thus $J(\pi)=\mathrm{sPCE}(\pi,L,T)$.","On the source-location task at $t=30$, RL achieves an SPCE lower bound of $11.73\pm0.040$ versus DAD $10.965\pm0.041$ and VPCE $7.766\pm0.069$; the SNMC upper bound for RL is $12.362\pm0.062$ (similar to DAD $12.380\pm0.086$). On the CES task at $t=10$, RL’s SPCE lower bound is $13.965\pm0.064$, exceeding DAD’s SNMC upper bound ($13.374\pm0.150$), with RL upper bound $17.794\pm0.226$. On the discrete prey-population task at $t=10$, RL attains $4.456\pm0.032$ (lower) and $4.459\pm0.033$ (upper), comparable to SMC ($4.521\pm0.065$ lower). Deployment-time benchmarks show RL proposes designs in about $\sim1.35\times10^{-3}$ s (CES) and $\sim1.50\times10^{-3}$ s (prey), orders of magnitude faster than VPCE/SMC baselines.","The authors note that in the discrete prey-population problem, RL does not outperform strong myopic baselines, possibly because those baselines compute an explicit posterior each step while RL must encode this information implicitly in policy/critic networks. They also suggest the learned state representation may degrade performance and that improved representation learning might close the gap. They further acknowledge that the importance of exploration (a key advantage of RL over DAD) varies by problem, so gains may not be uniform across SED tasks.","The method optimizes a variational lower bound (SPCE) rather than EIG directly; improving the bound may not always translate to improved true EIG, and bound tightness depends on the number of contrastive samples $L$. The HIP-MDP formulation assumes the ability to sample outcomes from $p(y\mid\theta,d)$ during training; in many real experimental settings (especially physical experiments), such a simulator may be unavailable or mismatched, raising sim-to-real concerns. Performance and stability may be sensitive to RL hyperparameters/seed choice and reward/representation choices; the paper reports multiple seeds for evaluation but does not provide a systematic robustness analysis or guarantees. The approach is demonstrated on benchmark tasks; broader real-world validations (with experimental constraints, costs, and noise processes beyond the simulator) are limited.","The authors suggest that their formulation enables applying a wide range of RL techniques to SED problems, particularly methods designed for difficult policy learning settings such as sparse rewards and high dimensionality. They also imply that improved representation learning for the state/history could potentially improve RL performance on tasks where it currently matches but does not beat myopic baselines. They indicate that “smartly transforming” design problems to MDPs is a path to leveraging additional RL advances.","Develop self-starting/online variants that adapt to model mismatch and reduce reliance on an accurate simulator, including sim-to-real transfer or robust RL formulations. Extend the approach to constrained and cost-aware experimental design (e.g., penalties for switching costs, safety constraints, batch/parallel experiments, or multi-fidelity settings) with explicit economic or utility trade-offs. Explore alternative information objectives (e.g., mutual information for specific parameters, predictive/utility-focused criteria, or Bayesian decision-theoretic losses) and analyze when optimizing SPCE correlates with true EIG. Provide standardized software artifacts (reproducible training scripts, config sweeps) and broader benchmarks, including multivariate/high-dimensional designs and real experimental case studies.",2202.00821v3,https://arxiv.org/pdf/2202.00821v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:58:16Z
TRUE,Optimal design|Sequential/adaptive,Parameter estimation|Optimization,G-optimal|Other,Variable/General (dimension $d$; finite arm set $\mathcal{X}\subset\mathbb{R}^d$),Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Supplementary material (Journal/Publisher),NA,"The paper studies regret minimization for finite-armed logistic bandits and introduces an experimental-design-driven algorithm, HOMER, to improve regret bounds by reducing dependence on the hard problem constant $\kappa$. The core idea is to use a novel ‘H-optimal’ design objective that minimizes the worst-case confidence width for estimating mean rewards $\mu(x^\top\theta^*)$ under a fixed-design logistic regression concentration result. HOMER alternates between (i) an H-optimal design for efficient gap estimation and arm elimination and (ii) a G-optimal design to ensure a warmup condition needed for tight fixed-design confidence intervals, combining them via a mixture and rounding procedure. The paper also proposes WAR (Warmup by Accepts and Rejects), an adaptive warmup sampling scheme that can require far fewer samples than a naive worst-case warmup and is proved never worse (in sample complexity) than the naive warmup. Empirically, WAR substantially reduces the warmup sample count versus naive warmup in simulated 3D problems, approaching an oracle warmup that knows $\theta^*$.","The paper defines a new H-optimal design criterion for logistic models: $h^*=\min_{\lambda\in\Delta_{\mathcal{X}}}\max_{x\in\mathcal{X}} \mu'(x^\top\theta^*)^2\,\|x\|^2\, H_\lambda(\theta^*)^{-1}$ where $H_\lambda(\theta)=\sum_{x\in\mathcal{X}}\lambda_x\mu'(x^\top\theta)xx^\top$. It contrasts this with the (logistic) G-optimal objective $g^*=\min_{\lambda\in\Delta_{\mathcal{X}}}\max_{x\in\mathcal{X}} \|x\|^2\, H_\lambda(\theta^*)^{-1}$. A key requirement for tight fixed-design confidence bounds is the warmup condition $\xi_t^2=\max_{1\le s\le t}\|x_s\|^2 H_t(\theta^*)^{-1}\le 1/\gamma(d)$, and the MLE is $\hat\theta=\arg\max_\theta \sum_{s=1}^t y_s\log\mu(x_s^\top\theta)+(1-y_s)\log(1-\mu(x_s^\top\theta))$.","HOMER achieves a leading regret term scaling as $\tilde O\big(\sqrt{d\,\mu'(x_*^\top\theta^*)\,T\,\log|\mathcal{X}|}\big)$ in the fixed-arm setting (and an instance-dependent bound $O\big(\frac{d\,\mu'(x_*^\top\theta^*)\log(|\mathcal{X}|T)}{\Delta}\big)$), improving prior leading terms that include extra $d$ and/or $\log^2(\kappa)$ factors. With naive warmup, the lower-order term is $O(d^2\kappa\log(|\mathcal{X}|T))$, matching the state-of-the-art worst-case lower-order dependence. WAR is proved $\delta$-valid and its pessimistic-planning sample complexity is never worse than naive warmup (up to additive rounding terms), while often much smaller in practice. In simulations with 20 arms in $d=3$, WAR reduced warmup samples vs naive (e.g., for $S=8$: naive $2{,}623{,}477\pm3.0$ vs WAR $122{,}405\pm30{,}815.5$), and was closer to an oracle warmup ($50{,}258\pm4{,}052.5$).","The authors note that proving the overall sample complexity of WAR in multi-dimensional cases likely requires analyzing how the volume of the confidence set evolves over optimistic probing iterations, and they leave this as future work. They also note that avoiding the rounding step (as can be done in linear bandits with robust mean estimators) remains an open question for logistic bandits. In the impacts/limitations discussion, they warn about potential societal harms when deploying myopic optimization (e.g., recommender systems) that may introduce or reinforce biases.","The proposed experimental design objectives depend on (plug-in) estimates of $\theta^*$ through $\mu'(x^\top\hat\theta)$; performance may be sensitive to early estimation errors, especially when rewards are near-deterministic (small variance) causing unstable inverse-information behavior. The work focuses on finite, fixed arm sets and conditional independence; extensions to nonstationary settings, drifting parameters, or strongly dependent observations are not addressed and could materially affect both design optimality and confidence validity. Practical computation of optimal designs and repeated MLE fitting plus rounding can be heavy for large $|\mathcal{X}|$ or high $d$, and the paper does not provide complexity benchmarks beyond hardware/software notes. Empirical evaluation is limited (small synthetic experiments); broader benchmarks or real-world case studies would be needed to validate robustness and tuning choices (e.g., thresholds $L,U,r$ in WAR).","They explicitly suggest analyzing the multi-dimensional sample complexity of WAR by studying how the confidence set volume evolves during optimistic probing. They also point to broader open questions on the fundamental limits of fixed-design-type inequalities (when Gaussian-like tails are achievable) and on bias–variance tradeoffs for MLE without distributional assumptions on covariates. They propose that bias-corrected/alternative estimators (e.g., extensions of the KT estimator / Jeffreys-prior regularization) could reduce the warmup dimension dependence and leave developing tight fixed-design concentration for such estimators to future work.","Provide efficient, scalable solvers and approximation guarantees for the proposed H-optimal objective in large discrete arm sets (and study sensitivity to approximation/rounding error). Extend the approach to contextual (changing-arm-set) logistic bandits and to generalized linear models beyond logistic with theory and practical heuristics for warmup/design. Develop software artifacts (reproducible code package) and standardized experimental benchmarks (including real logged bandit feedback) to assess robustness under model misspecification, class imbalance, and correlated rewards. Explore self-normalized or robust alternatives that avoid strict warmup conditions and reduce dependence on MLE bias, potentially enabling ‘self-starting’ versions of HOMER/WAR.",2202.02407v1,https://arxiv.org/pdf/2202.02407v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:59:06Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Robustness|Cost reduction|Other,Other,"Variable/General (examples include 1D sampling in Zermelo; 3 variables M, α, β for X-43 aerodynamics; discretized parameter vector θ of size q, e.g., 30 basis functions in Zermelo and q general)",Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Not provided,http://www.gurobi.com,"The paper proposes a sensitivity-driven optimal experimental design (OED) framework for nonlinear dynamical systems in which data acquisition is targeted to reduce model errors that most strain a feedback controller. The key idea is to use hyper-differential sensitivity analysis (HDSA) to quantify how perturbations in an uncertain model component $g$ change the optimal reference-tracking control effort, then embed these sensitivities into a deterministic OED criterion. The resulting OED problem selects a budget-limited set of experiment/sampling points (e.g., state–control configurations) by solving a mixed-integer quadratic program, rather than using classical alphabetic (A-, D-, etc.) criteria. The approach is demonstrated on the Zermelo navigation problem (sampling an uncertain current model) and a hypersonic trajectory control problem (sampling aerodynamic coefficients based on NASA X-43 data), showing reduced feedback effort and improved tracking after updating surrogates with the selected samples. Overall, it advances OED for control by coupling trajectory optimization, feedback tracking, and experiment selection through post-optimality sensitivities computed once at a nominal model.","HDSA defines an operator mapping model to optimal tracking controller via the implicit function theorem, with derivative $F_g(g)=-H^{-1}B$ where $H=J_{vv}$ and $B=J_{vg}$, and sensitivity for a perturbation direction $\delta g$ is $S(\delta g)=\|F_g(g)\,\delta g\|/\|\delta g\|$. After discretizing model perturbations with parameters $\theta$ (e.g., via splines/RBFs), $F_g(g)$ is approximated by a matrix $D$ and the design-dependent “updated sensitivity” is $S(w)=DAB(w)$. The OED criterion minimizes a Frobenius-norm surrogate under a budget: $\min_{w\in\{0,1\}^d}\mathrm{Tr}(S(w)^TS(w))$ s.t. $\sum_j\kappa_j w_j\le\kappa_B$, yielding an MIQP form such as $\min_{w,s}\sum_i c_i^2 s_i^2$ with linear constraints linking $s_i$ to information-gain terms $1-w^Tr_i$.","In the Zermelo example, the OED budget is set to $\kappa_B=3$ sampling points on a 30-node mesh for the uncertain current model; the selected points concentrate near peaks of the HDSA sensitivity curve, and the updated surrogate reduces the required LQR feedback effort and improves reference tracking compared with the original model. The paper further benchmarks the proposed design against 100 random 3-point designs across 100 randomized “true” models, reporting that the HDSA-driven design outperforms the majority of random designs in the joint space of tracking error and feedback effort. In the X-43 hypersonic case, an OED with $\kappa_B=8$ points in the $(M,\alpha,\beta)$ configuration space is computed; after updating aerodynamics with those samples, the closed-loop trajectory and control signals show substantially reduced feedback demand and avoid violating actuator bounds observed under the initial model. Quantitative ARL-style metrics are not used; performance is assessed through tracking-error and feedback-effort comparisons and constraint-violation behavior in simulation.","The authors note that the design from the proposed optimization is not expected to be globally optimal for true closed-loop performance because it is optimal only with respect to the specified error model $A$, information-gain assumptions $r_{i,j}$, and the linearization/Taylor approximation implicit in HDSA. They explicitly state that the error and information-gain quantities will “never be fully known in practice,” which limits achievable optimality, though the framework is intended to pragmatically incorporate best estimates and domain expertise when available. They also motivate that recomputing trajectories/sensitivities after each candidate design would be ideal but is “computationally intractable,” so the method relies on sensitivities computed once at the nominal model.","The OED is deterministic and depends heavily on user-chosen discretization/basis functions for perturbing $g$ (e.g., RBF placement/widths) and on the heuristic correlation/error-reduction model $r_{i,j}$; results may be sensitive to these choices without a systematic calibration procedure. The method is demonstrated with LQR feedback around nominal trajectories; it is not shown how performance transfers to other feedback architectures, strong nonlinearity far from the nominal, or significant state-estimation noise. The approach updates surrogates by interpolation at design points; in real experiments with noise/bias, a robust regression/Bayesian update and corresponding design criterion may be necessary. No publicly available implementation details are provided, and scalability to very high-dimensional design spaces or expensive MIQP instances is not empirically characterized beyond noting low-rank/SVD approximations for $D$.","The conclusion states there is opportunity to leverage HDSA beyond data acquisition, including other aspects of trajectory planning and feedback controller design, motivated by interpreting HDSA as the feedback effort required to overcome uncertainty. The authors also emphasize the broader applicability of coupling uncertainty calibration, planning, and control, suggesting extensions of the framework to other engineering applications where these elements must be integrated. No additional specific methodological extensions (e.g., stochastic/noisy settings, unknown parameters, alternative criteria) are explicitly detailed.","Extend the framework to handle measurement noise and statistical uncertainty explicitly (e.g., Bayesian or robust OED variants) while retaining the control-effort-driven objective, and study how designs change under noisy high-fidelity evaluations. Develop a fully sequential/adaptive loop that recomputes trajectories and sensitivities after surrogate updates (possibly with warm-started MIQP or surrogate-assisted optimization) to quantify gains from iteration versus the single-pass approximation. Provide theoretical or empirical robustness analysis to autocorrelated dynamics, model-mismatch beyond the local HDSA linearization regime, and alternative feedback controllers (MPC, nonlinear feedback). Release reference implementations and benchmarking suites (including MIQP scalability and low-rank $D$ approximations) to support reproducibility and broader adoption.",2202.03312v1,https://arxiv.org/pdf/2202.03312v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T04:59:53Z
TRUE,Factorial (full)|Factorial (fractional)|Response surface|Split-plot|Optimal design|Computer experiment|Other,Prediction|Cost reduction|Other,D-optimal|A-optimal|I-optimal (IV-optimal)|G-optimal|E-optimal|V-optimal|Space-filling|Minimax/Maximin|Other,"Variable/General (examples include 2–8 factors in deterministic design comparisons; PDE examples are 2D inputs (x,t))",Other,Simulation study,TRUE,Python|MATLAB|Other,Not provided,NA,"This paper is a state-of-the-art review of Design of Experiments (DoE) strategies used to generate training data for surrogate models, with emphasis on physics-informed neural networks (PINNs). It surveys classical deterministic DoE (full/fractional factorial, central composite, Box–Behnken, Doehlert, optimal design), space-filling designs (simple grids, Latin hypercube, orthogonal arrays, uniform design), and modern/random and quasi-random strategies (MCMC/Gibbs/Metropolis–Hastings; CVT, maximin LHS, Sobol, Halton, Hammersley, Faure), plus grid-based methods (full grid, sparse grid, rotational sparse grid). The paper then runs numerical experiments on five PDEs (viscous Burgers, Schrödinger, 1D heat, Allen–Cahn, KdV) to compare DoE sampling choices for PINN training, assessing prediction accuracy via mean squared error and absolute error distributions. Across the studied problems, quasi-random sampling—especially Hammersley—tends to yield lower errors than classical deterministic designs, and the authors report needing roughly 350–400 training samples to stabilize MSE for most examples. The work positions DoE choice as a key driver of PINN accuracy and computational efficiency in PDE surrogate modeling.","PINN training is formulated by minimizing a composite loss that combines data/constraint mismatch and PDE residual error, e.g. $L = L_u + L_f$ (or $L=L_0+L_b+L_f$ for periodic BC cases). For Burgers’ equation, the physics residual is $f(t,x)=u_t + u u_x - \nu u_{xx}$ and losses are mean-squared: $L_u=\frac{1}{N_u}\sum_{i=1}^{N_u}|u(t_i^u,x_i^u)-u_i|^2$ and $L_f=\frac{1}{N_f}\sum_{i=1}^{N_f}|f(t_i^f,x_i^f)|^2$. Similar residual constructions are used for Schrödinger ($f=i u_t+0.5u_{xx}+u^2\bar u$), heat ($f=u_t-u_{xx}$), Allen–Cahn ($f=u_t-0.0001u_{xx}+5u^3-5u$), and KdV ($f=u_t+u u_x+u_{xxx}$).","In the numerical comparisons, the authors report that errors (MSE and absolute error boxplots) decrease as training-set size increases, with MSE typically stabilizing around 350–400 DoE samples (KdV around ~350; others around ~400). Across all five PDE case studies, classical deterministic schemes such as factorial design (FD) and central composite design (CCD) frequently show worse (higher-median) absolute errors than quasi-random/space-filling methods. Hammersley sampling is consistently reported as the best-performing or among the best-performing DoE strategies, with Sobol and sparse grid often the next-best depending on the PDE. The paper’s main empirical takeaway is that quasi-random sequences (especially Hammersley) cover the input domain more effectively for PINN training, improving predictive accuracy at a fixed sample budget.","The authors explicitly note that the present study does not consider any sequential (adaptive) method where samples are added iteratively to the DoE set based on performance measures. They also emphasize that accuracy depends on DoE set size, implying a practical tradeoff with computational budget, but do not provide a sequential strategy to optimize this tradeoff.","Although many DoE schemes are reviewed, the numerical evaluation focuses on a fixed set of five PDE benchmarks with specific network architectures and hyperparameters, so conclusions (e.g., Hammersley best) may not generalize to other PDEs, higher-dimensional inputs, noisy observations, or different PINN formulations. The comparison does not appear to control for potential interactions between sampling strategy and optimization/training dynamics (e.g., random seeds, optimizer settings), which can materially affect PINN performance. Optimal design is discussed theoretically, but the empirical section mainly compares sampling strategies rather than demonstrating criterion-optimized experimental designs (e.g., D-/I-optimal designs built for a specified surrogate model). Domain/application variety is broad in discussion, but real-world case studies with measured data are not shown in the provided content, limiting evidence for practical deployment impact.","They suggest extending the work to sequential/adaptive DoE, where one sample is added to the previous DoE set in each iteration based on performance measures, since this was not considered in the present study.","A natural extension is to evaluate robustness of DoE conclusions under observation noise, parameter uncertainty, and autocorrelated sampling in time-dependent PDE data, including self-adaptive PINN weighting between data and physics losses. Another direction is to benchmark additional high-dimensional PDE settings (more input dimensions/parameters) and compare against modern space-filling and Bayesian experimental design criteria tailored to PINNs. Providing open-source implementations and standardized benchmarking protocols (fixed seeds, matched budgets, multiple optimizers) would strengthen reproducibility and the practical guidance for selecting DoE strategies in PINN workflows.",2202.06416v1,https://arxiv.org/pdf/2202.06416v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:00:35Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (examples include 3 source locations; source location + wind; infection-rate parameter with 4 observation times),Environmental monitoring|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/2202.07472,"This bachelor thesis proposes a method for sequential Bayesian experimental design (BED) using reinforcement learning, termed RL-SED, aiming to maximize expected information gain (EIG) while also accounting for experimental cost and sample efficiency. Because direct EIG computation is expensive, the method uses contrastive information bounds (CID) as a computationally cheaper surrogate objective, evaluated at the end of an episode to support holistic (non-myopic) planning. The sequential policy is learned with the Soft Actor-Critic (SAC) algorithm, treating experimental planning as an agent-environment interaction where actions update the next design point. The approach is evaluated via numerical simulations on three benchmark sequential design environments (location finding, contaminant source inversion, and an epidemiological death process), showing higher CID reward and markedly fewer training episodes than a deep adaptive design (DAD) baseline and a random policy. The work advances the practical BED literature by integrating modern off-policy RL to handle continuous actions, sequential constraints, and explicit movement/time costs in experimental planning.","The batch EIG objective is expressed as $U(\pi)=\mathbb{E}_{\theta}\mathbb{E}_{y\mid\theta,\pi}[\log p(y\mid\theta,\pi)-\log p(y\mid\pi)]$ (Eq. 3.5), and extended to sequential observations $y_{0:t}$ (Eq. 3.8). The paper optimizes a contrastive lower bound (CID): $I_L=\log\frac{p(y\mid\theta_0,\pi)}{\frac{1}{L+1}\sum_{l=0}^L p(y\mid\theta_l,\pi)}$ (Eq. 3.6) and its sequential form $I_L=\log\frac{p(y_{0:t}\mid\theta_0,\xi_{0:t})}{\frac{1}{L+1}\sum_{l=0}^L p(y_{0:t}\mid\theta_l,\xi_{0:t})}$ (Eq. 3.9), with conditional factorization $p(y_{0:t}\mid\theta,\xi_{0:t})=\prod_{i=0}^t p(y_i\mid\theta,\xi_i)$ when applicable (Eq. 3.10). The sequential design is updated by actions via $\xi_t=\xi_{t-1}+a_t$ (Eq. 4.5), and the RL reward is sparse: $r=0$ during the episode and $r=I_L$ at the end (Eq. 4.6).","Across three simulated environments (CID inner sample size $L=2000$), RL-SED achieves higher final mean reward and drastically fewer training episodes than DAD. Location finding: RL-SED reward 5.955 vs DAD 4.164, with 15,000 vs 750,000 episodes. Source inversion: RL-SED reward 5.510 vs DAD 5.051, with 3,000 vs 40,000 episodes. Death process: RL-SED reward 2.042 vs DAD 1.506 (below random 1.630), with 15,000 vs 1,000,000 episodes. Additional generalization tests (varying prior distribution parameters) generally show RL-SED maintaining competitive or better rewards than DAD and random across shifted environments.","The thesis notes that using the full history $s_{0:t}$ for sequential decision making leads to a huge state space, which becomes problematic when assuming more complex experimental environments. It suggests that state-space size is a key bottleneck that must be addressed for scaling to richer, real-world settings. No other explicit limitations are clearly highlighted beyond computational considerations motivating CID and end-of-episode reward computation.","All evaluations are numerical experiments in modified benchmark simulators; the work does not demonstrate performance on real experimental datasets or online deployments, so practical robustness (sensor errors, model misspecification, safety constraints) remains untested. The objective optimized is CID (a lower bound proxy) rather than true EIG, and the impact of bound looseness (choice of $L$, variance) on policy quality is not fully characterized. Comparisons focus on DAD and random baselines; broader BED baselines (e.g., mutual-information neural estimation approaches, approximate dynamic programming variants, Bayesian optimization/active learning heuristics) are not systematically benchmarked under matched compute budgets. Implementation details (architecture, hyperparameters, compute/time) and reproducibility are limited by lack of shared code.","The thesis suggests transferring more state-of-the-art reinforcement learning techniques to the construction of sequential experimental designs. It also explicitly proposes investigating methods to reduce/compress the state space input to the policy, as the current use of $s_{0:t}$ leads to an impractically large state representation in more complex environments.","A natural extension is to study robustness to model misspecification and non-idealities (non-Gaussian noise, constraints, partial observability) and to develop self-normalizing or risk-sensitive objectives that trade off information gain with safety/cost under uncertainty. It would be valuable to evaluate RL-SED under equal wall-clock/compute budgets and to add stronger baselines from sequential mutual-information estimation, Bayesian optimization, and approximate DP methods. Developing open-source implementations and ablation studies (end-of-episode reward vs shaped rewards; SAC vs other off-policy methods; effect of $L$ in CID) would clarify what drives performance gains. Finally, extending to higher-dimensional, multi-agent, or batched-sequential hybrid settings (e.g., parallel experiments) would broaden applicability.",2202.07472v1,https://arxiv.org/pdf/2202.07472v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:01:23Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Model discrimination|Parameter estimation|Cost reduction|Other,Bayesian D-optimal|Other,"Variable/General (graphs with 10, 20, 50 nodes/variables; single-target interventions; batch size B used)",Healthcare/medical|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|R|Other,Public repository (GitHub/GitLab),https://github.com/yannadani/cbed,"The paper proposes CBED, a Bayesian optimal experimental design framework for active causal discovery that selects both (i) which variable to intervene on and (ii) the intervention value, addressing a gap in prior methods that typically choose only targets or assume linear SCMs. The utility is mutual information between intervention outcomes and the unknown SCM/DAG posterior, estimated via a BALD-style Monte Carlo estimator, and intervention values are optimized with Bayesian optimization using GP-UCB per candidate node. The method is extended to batch (static) design via two strategies: Greedy-CBED (submodular greedy maximization with a (1−1/e) guarantee) and Soft-CBED (soft top-k sampling from candidate target–value pairs for efficiency). Empirical evaluation on synthetic Erdos–Rényi and scale-free graphs (linear and nonlinear additive-noise SCMs) and on the in-silico DREAM gene-regulatory benchmark shows faster recovery of causal structure than baselines, with GP-UCB value selection outperforming fixed or data-support sampling of intervention values. The work advances BOED for causal discovery by scaling Bayesian design to larger nonlinear SCMs while jointly optimizing intervention targets and continuous intervention values in batched settings.","Interventions are designs of the form $\xi=\{(j,v)\}:=do(X_j=v)$ and are chosen by maximizing mutual information $\arg\max_{j,v} I(Y;\Phi\mid\{(j,v)\},D)$, where $\Phi=(g,\theta)$ denotes the SCM/DAG and parameters. The MI is estimated in BALD form as $I(Y;\Phi\mid\{(j,v)\},D)=H(Y\mid\{(j,v)\},D)-H(Y\mid\Phi,\{(j,v)\},D)$ with Monte Carlo likelihood evaluations. Intervention values are optimized per node via GP-UCB: $v^{(t+1)*}_j=\arg\max_v\,\mu^t_j(v)+\sqrt{\beta^{t+1}_j}\,\sigma^t_j(v)$ over a domain $[-k,k]$.","On 50-node nonlinear SCMs (Erdos–Rényi and scale-free), Soft-CBED with GP-UCB value optimization achieves the best E-SID trajectories among compared acquisition functions, indicating faster approach to the true graph than Random and AIT-based baselines. Ablations show that GP-UCB value selection yields substantially better causal discovery than intervening at a fixed value (e.g., 0) or sampling values from the observational support. Batch selection via Soft-CBED performs comparably to Greedy-CBED while being much faster: for D=50, runtime drops from 284.98s (Greedy+GP-UCB) to 24.17s (Soft+GP-UCB), and from 32.56s (Greedy+Fixed) to 6.42s (Soft+Fixed). On DREAM (in-silico single-cell regulatory networks), Soft-CBED improves E-SID over Random and softAIT on several datasets (e.g., Ecoli1/Ecoli2) under a gene-knockout (fixed-value) setting.","The authors state that their conclusions rely on standard causal discovery assumptions being satisfied, including causal sufficiency (no hidden confounders), finite samples, nonlinear additive-noise SCM assumptions, and atomic single-target interventions. They also note that these assumptions must be carefully verified for the application of interest. In DREAM knockout-like experiments, they restrict to a fixed intervention value (0.0), limiting demonstration of value-optimization benefits in that specific setting.","The method’s BO and MI estimation require repeated likelihood evaluations and posterior sampling, which may be expensive or unstable for very high-dimensional graphs or more complex SCM parameterizations, and sensitivity to BO/GP hyperparameters (kernel choice, bounds, $\beta_t$ schedule) is not deeply explored. The framework assumes perfect interventions and single-target (atomic) interventions; many practical domains need imperfect, soft, or multi-target interventions with constraints. Robustness to model misspecification (non-additive noise, non-Gaussian noise, latent confounding, selection bias, and temporal/autocorrelated data) is not evaluated, which can materially affect causal discovery and design. Empirical comparisons focus on selected baselines; broader benchmarking against other modern active causal discovery or Bayesian design methods (e.g., constraint-based or score-based with interventional planning under different assumptions) could strengthen generality claims.","The paper suggests that the methodology and conclusions hold under the stated assumptions and emphasizes careful verification for applications; no concrete, specific future-work agenda is otherwise clearly enumerated in the provided text beyond these caveats. It also notes planned inclusion of societal impact discussion in the camera-ready, implying further documentation/analysis work rather than a technical extension.","Extending CBED to handle latent confounders (violations of causal sufficiency) and imperfect/soft interventions would broaden applicability to real experimental settings. Incorporating multi-target interventions and explicit experimental constraints/costs (e.g., limited feasible dosage ranges per variable, intervention coupling, or safety constraints) would make the design policy more realistic. Developing self-starting or robust MI estimators under misspecified likelihoods (implicit models) and evaluating performance under non-Gaussian noise and autocorrelation would improve reliability. Providing a well-maintained software package with reproducible pipelines and scalable approximations (e.g., surrogate models for MI, amortized design policies) could enable adoption on larger-than-50-node systems.",2203.02016v3,https://arxiv.org/pdf/2203.02016v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:02:05Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Robustness|Cost reduction|Other,Not applicable,Variable/General (d covariates; also k treatments; treatment probability q; robustness parameter φ),Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online (sequential, single-pass) covariate-balancing experimental design for randomized experiments, aimed at reducing variance of causal effect (ATE) estimates by minimizing linear covariate imbalance as units arrive. The core method (Balancing Walk Design / BWD) adapts online discrepancy-minimization techniques to handle arbitrary marginal treatment probabilities (q) and extends from binary to multiple treatments via a binary-tree reduction. The authors derive worst-case bounds on imbalance and on the mean-squared error of the difference-in-means/Horvitz–Thompson-style estimator, showing the resulting estimator is no worse than an implicit ridge regression (regularization tied to a robustness parameter φ) and within a logarithmic factor of best-known offline discrepancy design bounds. A restart variant ensures the procedure never fails while retaining guarantees (up to constants). Simulation studies across multiple data-generating processes (including non-i.i.d./drift/seasonality) show favorable MSE/MISE and runtime compared with Bernoulli/complete randomization, Efron/Smith biased-coin designs, and offline methods like QuickBlock.","Binary treatment assignment uses online weighted discrepancy minimization with signed weights \(\eta_i\in\{-2q,\,2(1-q)\}\) and running sum \(w_i=w_{i-1}+\eta_i x_i\), with randomized assignment probabilities depending on \(w_{i-1}^\top x_i\) and threshold \(c\propto \log(2n/\delta)\). The ATE estimation error is expressed as \(\hat\tau-\tau=\tfrac{2}{n}\eta^\top \mu\), yielding \(\mathrm{Var}(\hat\tau)=\tfrac{4}{n^2}\mu^\top \mathrm{Cov}(z)\mu\) with \(z-E[z]=\eta\). Robustness is introduced by augmenting covariates with \(\sqrt{\phi}e_i\) (equivalently scaling inner products by \(\sqrt{1-\phi}\)), leading to matrix \(Q=(\phi I+(1-\phi)XX^\top)^{-1}\) and concentration bound \(|\hat\tau-\tau|\le \tfrac{2c}{n}\sqrt{\mu^\top Q\mu}\) (w.p. \(1-\delta\)).","They prove high-probability bounds that the online design controls the per-step inner product \(|w_i^\top x_i|\le c\) (w.p. \(1-\delta\)) and yields an overall imbalance bound \(\|\sum_i \eta_i x_i\|_2\) scaling on the order of \(\min(1/q,9.3)\sqrt{d\log(d/\delta)\log(n/\delta)}/\sqrt{(1-\phi)\phi}\). The worst-case MSE of the ATE estimator is upper bounded by \(\mathbb{E}(\hat\tau-\tau)^2\le \tfrac{4\sigma^2}{\phi n^2}\sum_{i=1}^n \mu_i^2\) (high probability) and also by an implicit ridge-regression objective \(L=\min_\beta \{\tfrac{1}{\phi n}\|\mu-X\beta\|^2+\tfrac{1}{(1-\phi)n}\|\beta\|^2\}\) via \(\mathbb{E}(\hat\tau-\tau)^2\le \tfrac{4\sigma^2 L}{n}\). For multiple treatments, the tree-based reduction achieves multi-treatment discrepancy scaling as \(O(\log k\cdot D(\delta/k))\) (Theorem 2), with an explicit bound \(O(\tfrac{\log k}{q(1-\phi)}\, d\,\tfrac{\log(dk/\delta)\log(nk/\delta)}{\phi})\) up to constants. Simulations show BWD improves MSE/MISE over simple/complete randomization and biased-coin baselines, often approaching or matching offline QuickBlock while running in linear time.","They note the method targets linear measures of balance and will be most effective when the covariate–outcome relationship is (near) linear; highly non-linear settings can favor offline blocking methods (e.g., QuickBlock) in simulations. They also discuss that the base algorithm can “fail” with probability \(\delta\) (when \(|w_{i-1}^\top x_i|>c\)), motivating a restart modification to guarantee assignments with probability one (at the cost of constants).","Theoretical guarantees are framed largely in worst-case imbalance/MSE terms and rely on the chosen feature representation (linear feature map); performance may degrade if important nonlinearities or interactions are omitted or if covariates are high-dimensional with weak signal. Practical deployment details—e.g., handling missing covariates, delayed/streaming feature computation, interference, or clustered/serially correlated outcomes typical in online A/B testing—are not directly addressed. The paper emphasizes design-based variance reduction but does not provide software artifacts or implementation guidance for production experiment platforms (monitoring, guardrails, diagnostics), which can be material for adoption.",None stated.,"Extending the approach to richer balance criteria (nonlinear feature expansions with controlled complexity, kernel/learned representations) while keeping strict latency constraints would broaden applicability beyond near-linear outcome models. Developing self-starting/Phase-I-free tuning rules for \(\phi\), \(q\), and \(\delta\), plus practical diagnostics for accidental bias and robustness under covariate/outcome drift, would aid practitioners. Open-source reference implementations and benchmarking on real online experimentation logs (including attrition and non-i.i.d. arrivals) would strengthen empirical validation and adoption.",2203.02025v1,https://arxiv.org/pdf/2203.02025v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:02:49Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (benchmarks include θ with 2 parameters in SIR and 2 in Cartpole; Location Finding uses 2 sources in N∈{2,5,10,15} dimensions)",Healthcare/medical|Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper introduces RL-DAD (Reinforcement Learning for Deep Adaptive Design), a simulation-based, sequential Bayesian optimal experimental design method for implicit models where the likelihood is intractable and the simulator may be non-differentiable. BOED is reformulated as a POMDP and a design policy is learned offline via deep reinforcement learning (TD3), using an InfoNCE-style, likelihood-free information lower bound as the reward signal. A dense reward shaping is proposed (difference of InfoNCE lower-bound terms across steps) to mitigate sparse-reward training issues while preserving the same total episodic objective. The method is evaluated on three simulated benchmarks—location finding, an epidemiological SIR model, and a non-differentiable cartpole simulator—showing RL-DAD is competitive with policy-based BOED baselines and can be applied where differentiability/analytic likelihood assumptions fail. Overall, RL-DAD advances adaptive DOE by amortizing sequential design computation into an offline-trained policy for fast online deployment.","The sequential BOED objective is to maximize total mutual information $I(\theta; y_{1:T}\mid \xi_{1:T})$, which decomposes as $\sum_{t=1}^T I(\theta; y_t\mid h_{t-1},\xi_t)$. RL-DAD uses an InfoNCE-style lower-bound term $g(h_t,U;L)=\log\frac{\exp(U(h_t,\theta_0))}{\frac{1}{L+1}\sum_{\ell=0}^L \exp(U(h_t,\theta_\ell))}$ with contrastive samples $\theta_{0:L}\sim p(\theta)$ and learned critic $U$. The dense reward at step $t$ is $r_t=g(h_t,U;L)-g(h_{t-1},U;L)$ so that $\sum_{t=1}^T r_t=g(h_T,U;L)$, encouraging multi-step policies that maximize (a lower bound on) information gain in likelihood-free settings.","Across benchmarks, RL-DAD outperforms Random and the static MINEBED-BO baseline and is competitive with stronger-assumption policy-based BOED methods when applicable. In Location Finding (T=10, two sources, N∈{2,5,10,15}), RL-DAD achieves EIG upper bounds of 7.100 (N=2), 2.455 (N=5), 0.766 (N=10), 0.407 (N=15), compared with Random 4.862/1.899/0.570/0.221 and MINEBED-BO 5.105/2.139/0.194/0.109. In SIR, RL-DAD’s InfoNCE EIG lower bound is 3.715 vs Random 1.915 and MINEBED-BO 2.539 (iDAD: 3.843). In Cartpole (non-differentiable), RL-DAD achieves 4.802 vs Random 3.434 and MINEBED-BO 3.628, while iDAD is not applicable.","The authors state that RL-DAD requires substantial initial training time. Because it depends on deep reinforcement learning, it inherits RL’s sample inefficiency and brittleness to hyperparameters and implementation details. They also note the high computational cost during training and the desire to reduce the required number of simulated samples.","Comparisons largely emphasize methods avoiding heavy online computation, which may exclude strong sequential (but online-intensive) BOED baselines; this can limit conclusions about best-achievable EIG without deployment constraints. The optimization target is a learned InfoNCE lower bound using a critic that can be biased/miscalibrated, so higher bound values do not necessarily guarantee better true information gain or posterior quality. Practical deployment still requires a simulator that matches the real system; domain shift (sim-to-real mismatch) could degrade design quality, but robustness to simulator misspecification is not analyzed. The work does not provide implementation/code, which may hinder reproducibility for the RL training pipeline and benchmark details.",They propose applying RL-DAD to real-world tasks such as robot manipulation and Real2Sim transfer. They also aim to address the high computational cost during training and reduce the number of simulated samples required.,"Developing self-normalized posterior diagnostic checks and calibration metrics (beyond InfoNCE) would help validate whether improved bounds translate to improved inference accuracy. Extending RL-DAD to handle constraints and costs on designs (e.g., safety, budget, switching costs) and to risk-sensitive or robust BOED objectives would improve real-world applicability. Investigating more sample-efficient RL or model-based RL variants (and ablations across RL algorithms) could reduce training cost and improve stability. Providing a standardized open-source implementation and benchmark suite would enable reproducible comparisons and faster adoption.",2203.04272v1,https://arxiv.org/pdf/2203.04272v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:03:21Z
TRUE,Optimal design|Sequential/adaptive|Other,Optimization|Parameter estimation|Other,Other,"Variable/General (pairwise experiments over oscillator pairs; parameters are coupling strengths a_{i,j} for 1≤i<j≤N; examples N=5 and N=7)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/Levishery/AccelerateOED,"The paper proposes a deep-learning surrogate to accelerate MOCU (mean objective cost of uncertainty)-based objective uncertainty quantification and optimal experimental design (OED) for uncertain dynamical systems, demonstrated on uncertain Kuramoto oscillator networks. Experiments correspond to selecting an oscillator pair (i,j), isolating it, and observing a binary synchronization outcome that tightens the uncertainty interval on coupling strength a_{i,j}; candidate experiments are ranked by expected remaining MOCU and the minimum is selected. The surrogate is a message-passing neural network (MPNN) that directly predicts (expected remaining) MOCU from graph-structured inputs (node frequencies and edge uncertainty bounds), removing the need for Monte Carlo sampling and repeated ODE solves used in prior sampling-based MOCU estimation. A novel axiomatic constraint loss enforces monotonicity of MOCU with respect to uncertainty bounds, improving ranking preservation needed for OED. Simulation results on N=5 and N=7 oscillators show 4–5 orders of magnitude speedup versus sampling-based OED with virtually identical MOCU-reduction performance across sequential experiment updates.","The Kuramoto dynamics are defined by coupled ODEs and a controlled extension (Eqs. (1) and (3)), with the robust control cost defined as $\xi^*(\mathcal{A})=\max_{a\in\mathcal{A}}\xi(a)$ and MOCU as $M(\mathcal{A})=\mathbb{E}_{a\in\mathcal{A}}[\xi^*(\mathcal{A})-\xi(a)]$ (Eqs. (4)–(5)). For each pairwise experiment (i,j), expected remaining MOCU is $R(i,j)=\sum_{b\in\{0,1\}}P(B_{i,j}=b)\,M(\mathcal{A}\mid B_{i,j}=b)$ and the OED selects $(i^*,j^*)=\arg\min_{i<j}R(i,j)$ (Eqs. (7) and (9)). The learning objective combines MSE with an axiomatic monotonicity constraint on predicted MOCU, using $L=\ell_{\mathrm{MSE}}+\lambda\ell_{\mathrm{AC}}$ and penalizing violations of $\partial \widehat{y}/\partial a^{\ell}_{i,j}\le 0$ and $\partial \widehat{y}/\partial a^{u}_{i,j}\ge 0$ (Eqs. (13)–(15)).","Across Kuramoto models with N=5 and N=7 oscillators, the MPNN surrogate accelerates MOCU-based OED by about 4–5 orders of magnitude relative to the Monte Carlo + ODE-solver baseline, with no visible loss in OED performance (MOCU reduction over sequential updates closely matches the sampling-based method). For N=7, estimating MOCU for a single model via sampling takes about 19 seconds on average, while the MPNN computes MOCU for an entire 2,000-point test set in 1.160 seconds (≈32,758× speedup). In an OED update step for N=7, computing expected remaining MOCU for all experiments takes ~2,995.8 s (sampling-based) versus ~0.0340 s (MPNN), and for N=5 it is ~1,345.4 s versus ~0.0321 s. The axiomatic constraint improves monotonic/ranking preservation: MP+ achieves 0.9254/0.9178 (N=5) and 0.8660/0.8475 (N=7) success rates under bounds-tightening tests (vs. MP 0.8553/0.8466 and 0.7540/0.7600; sampling-based ~0.66 and ~0.57).","The authors note that training the neural surrogate requires generating sufficient labeled training data, where ground-truth MOCU values are computed using the computationally expensive sampling-based approach (solving many ODEs), making dataset generation costly. They also remark that an iterative OED scheme (re-estimating expected remaining MOCU after each update) did not improve performance in their simulations, suggesting benefits may only appear in more complex scenarios.","The method is validated only in simulation on Kuramoto models with small N (5 and 7) and a specific experiment type (pairwise isolation with binary synchronization outcome); it is unclear how well performance and calibration transfer to larger N, different network structures, or other experimental modalities/outcomes. The surrogate is trained on MOCU values produced by a Monte Carlo estimator (K=20,480) that itself has variance; the learned model may inherit bias/noise and may require retraining if priors or uncertainty-class generation differs from the training distribution. Practical deployment would need guidance on out-of-distribution detection and uncertainty of surrogate predictions to avoid mis-ranking experiments when the surrogate is wrong.","They suggest reducing dependence on labeled training data by using the axiomatic constraint loss in a self-supervised or constructive-learning style pretraining procedure that learns correct rankings from paired uncertainty classes without ground-truth MOCU labels. They also propose adapting the approach beyond Kuramoto models and beyond robust synchronization objectives, including accelerating MOCU-based Bayesian active learning (e.g., SMOCU/WMOCU acquisition functions) and exploring real-world applications such as optimal therapeutic stimulation of brain networks under uncertainty.","Develop a principled uncertainty-aware surrogate (e.g., Bayesian neural nets or conformal prediction) that provides confidence intervals for predicted (remaining) MOCU and propagates this uncertainty into experiment selection to reduce risk from mis-ranking. Extend to richer experimental outcomes (multi-class/continuous measurements) and to constraints/cost-aware OED where experiments have heterogeneous costs or feasibility constraints. Provide scaling studies for larger oscillator counts and compare against other OED criteria (e.g., EIG) or hybrid strategies, plus release a reproducible pipeline for dataset generation and benchmarking across multiple dynamical systems.",2203.07120v4,https://arxiv.org/pdf/2203.07120v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:04:12Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction,A-optimal,"Variable/General (estimates line parameters $y=(g_{k,l},b_{k,l})$ for all $(k,l)\in\mathcal{L}$; dimension $2|\mathcal{L}|$)",Energy/utilities,Simulation study|Other,TRUE,MATLAB|Other,Not provided,https://arxiv.org/abs/2203.14011,"The paper develops computationally tractable approximations for optimal experimental design (OED) in AC power-grid admittance (line conductance/susceptance) parameter estimation. It formulates OED using an approximate Fisher Information Matrix (FIM) for a Gaussian-noise maximum-likelihood estimation setup and uses an A-optimal criterion (minimize $\mathrm{Tr}(F^{-1})$) to choose generator reactive power setpoints to maximize estimation information while keeping operating costs low. To address scalability issues from Jacobian and FIM inversions, it proposes (i) an inner Newton-type iteration with a fixed Jacobian to approximate power-flow solutions and sensitivities, and (ii) two outer approximations to avoid repeated symbolic matrix inversions of the FIM inverse (a Fisher-linearized objective and a quadratic-program approximation), plus an inner-linearized alternative. The methods are benchmarked on 5-bus and 14-bus case studies (MATPOWER data) showing that the quadratic approximation (QA-OED) can be significantly faster and is the only tractable approach among the proposed variants on the modified 14-bus case, while maintaining comparable variance reduction to classical OED. Overall, the work advances practical OED for larger power systems by introducing implementable approximations that reduce computational burden while preserving most of the information gains over random excitation/RLS baselines.","The OED objective uses an A-optimal criterion: minimize $\mathrm{Tr}(F^{-1})$ where the Fisher information matrix is approximated as $F(x,y,u)=\Sigma_0^{-1}+T(x,y,u)^\top\Sigma^{-1}T(x,y,u)$. The sensitivity term is $T=\partial_y M(x,y)+\partial_x M(x,y)\,\partial_y x^*(y,u)$ with $\partial_y x^*(y,u)=-(\partial_x P(x,y))^{-1}\partial_y P(x,y)$ from the implicit function theorem. Proposed approximations replace $x^*$ by a Newton-type iterate with a fixed Jacobian and replace $\mathrm{Tr}(\tilde F^{-1})$ via a first-order (Fisher-linearized) expansion leading to objectives like $-\mathrm{Tr}(\tilde F(\hat u)^{-1}\tilde F(u)\tilde F(\hat u)^{-1})$ and, further, to a QP form $\tfrac12 u^\top H u + J u$ with linearized state constraints.","For the 5-bus case with 200 batches, reported times are: OED 19.7926 s (sum variance 0.0582), FLA-OED 36.1696 s (0.0281), QA-OED 3.9811 s (0.0358), and ILA-OED 8.6328 s (0.0583). For reaching a target sum variance of 0.1, reported times are: OED 11.1998 s, FLA 7.7718 s, QA 1.7076 s, and ILA 4.9239 s (all achieving ~0.098–0.100). Relative loss of optimality at the first iteration (vs. classical OED optimum) is 8.45 for random input, 0.36 for FLA, 0.36 for QA, and 4.94 for ILA. On the modified 14-bus case, QA-OED computes 200 estimation steps in 15.2507 s, while FLA-OED and ILA-OED are reported as not computationally tractable.","The authors note that solving OED becomes computationally expensive/intractable for larger grids due to the need to invert the Fisher information matrix and the nonlinearity of AC power flow equations. They also remark that their measurement set is not minimal and that identifiability depends on topology and measurement placement. In the outlook, they state that further improving scalability of OED is crucial for practical application.","The approach assumes additive Gaussian measurement noise with known covariance and (largely) full-state/line-flow measurements, which may not match real PMU/SCADA availability, missing data, or gross errors; performance under non-Gaussian noise/outliers is not assessed. The OED is largely local (uses $\hat y$ in place of true $y$ and Taylor/linearization steps), so robustness to poor initial parameter guesses, multiple power-flow solutions, and constraint activity changes is unclear. Comparisons are limited to small/medium test cases (5- and modified 14-bus) and do not explore larger IEEE cases, correlated noise, or computational scaling laws (time vs. buses/lines). Code is not provided, which makes reproduction of the reported runtimes and solver settings difficult.",They propose further improving scalability for practical application and specifically mention considering approaches based on semidefinite programming in future work.,"Evaluate robustness to limited/realistic measurement configurations (e.g., PMU placement constraints), missing data, and non-Gaussian/outlier-contaminated measurements, potentially via robust/Bayesian OED criteria. Extend and benchmark scalability on substantially larger grids (e.g., 118-, 300-bus) with detailed profiling and memory analysis, and explore structure-exploiting linear algebra (sparsity, low-rank, iterative solvers) for FIM-related computations. Incorporate explicit operating-cost or security constraints (N-1, voltage stability margins) into the OED formulations and assess closed-loop impacts on system operation; provide an open-source implementation to support adoption and comparison.",2203.14011v2,https://arxiv.org/pdf/2203.14011v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:04:51Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Other,A-optimal|Other,Variable/General (optimizes 3D light-source directions/positions; experiments use 3 light sources and also evaluate many random configurations),Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Not provided,https://www.blendswap.com/blends/view/89954|https://www.blendswap.com/blends/view/90518,"The paper proposes an optimum experimental design (OED) method to compute optimal light-source configurations for calibrated photometric stereo (PS) of Lambertian surfaces, aiming to improve surface-normal estimation compared with heuristic light placement. The authors model image intensities under multiple lighting directions, assume additive independent Gaussian noise (motivated by Poisson-to-Gaussian approximation), and propagate measurement noise to the covariance of the estimated normal vector via a first-order Taylor expansion. Light positions/directions are then chosen by minimizing a scalar function of the covariance/confidence region (A-criterion), yielding an optimization objective based on traces involving $(S^TS)^{-1}$ and a shape-dependent matrix $B$ (from normal-vector normalization). The method is evaluated on synthetic data rendered in Blender and on real images captured with a robot-arm-mounted light source; results show visually sharper normal maps and substantially reduced angular error versus classic PS and a prior light-configuration method. Quantitatively on synthetic objects, the reported angular-error distribution mean/median improves to 2.010°/0.593° versus 9.712°/8.733° for classic PS and 6.078°/3.313° for Drbohlav & Chantler (2005).","Photometric stereo forward model: $I=\rho S N$ (Eq. 4) with Lambertian reflectance, and inverse recovery via least squares $\hat N=(S^TS)^{-1}S^T I$ (Eq. 11), then normalization $N=\hat N/\|\hat N\|$ (Eq. 9). Noise propagation yields a covariance for $\hat N$ (and confidence region) using a first-order Taylor expansion (Eqs. 13–16). The OED objective is based on an A-type criterion minimizing the trace of the covariance: initially $\min_S \mathrm{trace}((S^TS)^{-1})$ (Eq. 20), extended with a shape-dependent normalization Jacobian $B$ to $\min_S \Phi(S)=\mathrm{trace}(B (S^TS)^{-1} B^T)$ (Eq. 23).","On synthetic Blender-rendered objects (Teddy, Vase), the proposed OED-selected lighting yields normal maps with visibly sharper surface structures than classic PS. The reported angular-error distribution versus ground truth has mean/median 2.010°/0.593° for the proposed method, compared with 9.712°/8.733° for classic PS and 6.078°/3.313° for Drbohlav & Chantler (2005). When comparing the confidence-region size measure $\Phi(S)$ against 10^6 random light configurations (including a heuristic maximizing inter-light distance), the proposed design achieves $\Phi(S)=0.0211$ versus best heuristic-random value 1.8657 (as reported in Fig. 9 caption). Real-object experiments (flower pot, kettlebell, engine part) show qualitative improvements and greater apparent noise stability, though no ground truth is available.","The study is limited to calibrated photometric stereo under the Lambertian (diffuse) reflectance assumption; uncalibrated PS and non-Lambertian surfaces are explicitly out of scope. The approach is stated to be inoperative in the presence of diffuse (ambient) light, cast shadows, and inter-reflections. For real datasets, no ground truth is available, so evaluation there is qualitative/visual.","The optimal-design derivation relies on a first-order (linear) Taylor approximation for uncertainty propagation, which may be inaccurate under higher noise levels or strongly nonlinear effects from normalization and imaging geometry. The method assumes independent pixelwise Gaussian noise with known variances; real cameras often show spatially varying, correlated, intensity-dependent noise and additional effects (e.g., saturation, vignetting), which may change the optimal lighting. The optimization treats light directions/positions in continuous 3D space but practical constraints (occlusions, mounting limits, safety, power constraints) are not fully integrated into the design problem as presented. Comparisons are mainly to classic PS and one prior light-configuration method; broader benchmarks against modern robust/non-Lambertian PS pipelines or alternative design criteria (e.g., D- or I-optimality) are not explored.","Future research is stated to focus on broadening the approach to handle cases where it is currently inoperative—specifically to deal with cast shadows, inter-reflections, and diffuse/ambient light effects.","Extend the OED formulation to non-Lambertian BRDFs (or mixtures of diffuse/specular) and to uncalibrated/self-calibrating PS where light directions are uncertain and part of the estimation. Incorporate practical experimental constraints (robot reachability, collision avoidance, exposure/saturation limits, energy budgets) directly into the optimization and study robustness to model mismatch. Evaluate alternative optimality criteria (D-, I-, G-optimal) and multi-objective designs (e.g., balancing parameter variance and prediction error over a region) with larger-scale comparative benchmarks. Provide an open-source implementation and reproducible experiment scripts to facilitate adoption and independent verification.",2204.05218v1,https://arxiv.org/pdf/2204.05218v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:05:27Z
TRUE,Split-plot|Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,Other,Variable/General (stratified/blocked RCT with K strata; simulation uses K=12 strata; treatment is binary with treated/control allocations within each stratum),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops methods to design a prospective stratified randomized controlled trial (RCT) when the final analysis will use an Empirical Bayes shrinkage estimator to combine RCT estimates with stratum-level causal effect estimates from an existing observational study. The authors focus on the κ2 (and positive-part κ2+) shrinkage estimator, which shrinks each stratum’s unbiased RCT estimate toward its observational counterpart with a strength tied to the RCT estimator’s variance, and they show how to compute its risk efficiently via numerical integration of expressions involving ratios of quadratic forms. Using this risk as the objective, they propose practical design heuristics/algorithms for allocating total sample size across strata and treatment arms: (i) Neyman allocation, (ii) a “naïve” heuristic that optimizes risk assuming zero observational error (ξ=0), and (iii) a “defensive/robust” heuristic that optimizes worst-case risk under Γ-level unmeasured confounding via Tan’s marginal sensitivity model (with intervals from Zhao et al., 2019). They additionally propose “guardrails” (minimum cell sizes, detachability constraints so the RCT remains analyzable alone, and dominance/risk-reduction constraints) to avoid extreme allocations. Simulation studies with a rare binary outcome demonstrate that designs tailored to shrinkage can materially reduce mean-squared error relative to equal or standard Neyman allocations, with robust designs performing best when unmeasured confounding is present.","The shrinkage estimator is defined as $\kappa_2 = \hat\tau_o + \Big(I_K - \frac{\operatorname{tr}(\Sigma_r^2)\Sigma_r}{(\hat\tau_o-\hat\tau_r)^T\Sigma_r^2(\hat\tau_o-\hat\tau_r)}\Big)(\hat\tau_r-\hat\tau_o)$ (and $\kappa_2^+$ applies a positive-part truncation to prevent negative shrinkage weights). Its conditional risk is expressed (up to scaling) as $R(\kappa_2)=\frac{1}{K}\big(\operatorname{tr}(\Sigma_r)+\operatorname{tr}(\Sigma_r^2)\,\mathbb{E}_r[\,4(\hat\tau_r-\hat\tau_o)^T\Sigma_r^4(\hat\tau_r-\hat\tau_o)/((\hat\tau_r-\hat\tau_o)^T\Sigma_r^2(\hat\tau_r-\hat\tau_o))^2-\operatorname{tr}(\Sigma_r^2)/((\hat\tau_r-\hat\tau_o)^T\Sigma_r^2(\hat\tau_r-\hat\tau_o))\,]\big)$, with expectations computed via one-dimensional numerical integration formulas (Bao & Kan, 2013). The RCT design variables are the per-stratum treated/control sample sizes $(n_{rkt},n_{rkc})$ under a fixed total budget $\sum_k(n_{rkt}+n_{rkc})=n_r$, with $\sigma_{rk}^2=\sigma_{kt}^2/n_{rkt}+\sigma_{kc}^2/n_{rkc}$ entering $\Sigma_r=\operatorname{diag}(\sigma_{rk}^2)$.","In simulations patterned after a rare binary outcome setting (observational study size 20,000; prospective RCT size 1,000; 12 strata), designing for shrinkage improved performance beyond equal allocation and standard Neyman allocation. Under no unmeasured confounding, using shrinkage (κ2/κ2+) with a Neyman-designed RCT reduced risk substantially (often on the order of ~45–60% relative to using the RCT estimator $\hat\tau_r$ under equal allocation), and the “naïve” ξ=0-optimized design typically achieved a few additional percentage points of improvement. When unmeasured confounding was induced, the Γ-robust (defensive) designs (e.g., Γ around 1.0–1.2 in their examples) produced the best non-oracle performance for κ2/κ2+, while the naïve design remained close behind and still strongly outperformed equal and Neyman allocations when paired with shrinkage. The paper also reports that Neyman allocation is best if one plans to use the RCT estimator alone ($\hat\tau_r$), but is not generally optimal once the shrinkage-risk term is incorporated.","The authors note that the exact risk expressions rely on approximating the RCT estimator $\hat\tau_r$ as normal, and they recommend enforcing minimum per-cell sample sizes to ensure the CLT is reasonable. They also emphasize that observational estimates may be biased due to unmeasured confounding and therefore propose sensitivity-model-based (Γ-robust) designs and “guardrails” (detachability and risk-reduction constraints) to protect the RCT’s standalone validity. They acknowledge treatment-effect heterogeneity is assumed to follow a fixed, known stratification on observed covariates, which is a rigid structure compared with modern heterogeneous treatment effect modeling.","The proposed design optimization is heuristic (a greedy local search over unit swaps) for a generally non-convex objective, so it may converge to local optima and performance may depend on initialization and constraint choices. The approach assumes accurate estimation (or sensitivity-bounded estimation) of stratum-level potential outcome moments from the observational data; in practice, propensity score model misspecification, limited overlap/positivity, and finite-sample instability of IPW/SIPW can materially affect both the observational estimates and the design inputs. The robust design relies on a particular sensitivity model (Tan’s marginal sensitivity model) and bootstrap-based intervals; if the confounding structure violates this model or the bootstrap is unstable in small strata/rare outcomes, the worst-case ξ and variance bounds may be misleading. The paper does not provide an implemented software package, which can limit adoption and reproducibility.","They propose extending the framework beyond a fixed, known stratification by incorporating more flexible heterogeneous treatment effect methods (e.g., learning strata or effect functions from data). They suggest using observational data in a first stage to estimate a stratification scheme and then using remaining data for shrinkage in a second stage, or alternatively modeling treatment effects continuously as a function of covariates (estimating $\tau(X)$) and shrinking across “nearby” covariate values rather than within predefined strata.","Developing globally optimizing or provably convergent design algorithms (or tighter relaxations) for the non-convex shrinkage-risk objective could improve reliability versus greedy swaps, especially with many strata. Extending the design and shrinkage framework to clustered/longitudinal or interference settings (common in policy and healthcare), and to outcomes beyond binary with appropriate variance models, would broaden applicability. Creating a self-starting/Phase-I version that jointly estimates required design quantities while adaptively recruiting in stages (with guarantees under unknown variances/bias) would align with practical trial operations. Providing open-source software and benchmark suites (including overlap violations and misspecified propensity models) would enable clearer guidance for practitioners on when the proposed designs are robust.",2204.06687v1,https://arxiv.org/pdf/2204.06687v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:06:15Z
TRUE,Other,Parameter estimation|Model discrimination|Other,Not applicable,Variable/General,Transportation/logistics|Other,Other,TRUE,None / Not applicable,Supplementary material (Journal/Publisher),https://davidissamattos.github.io/ease-2022-causal/,"The paper describes how Volvo Cars uses causal graphical models (DAGs) to design randomized field experiments (A/B tests) for automotive software changes when full randomization is constrained by deployment conditions, environment interactions, and limited sample sizes. The method emphasizes making causal assumptions explicit, identifying confounders and valid adjustment sets (blocking backdoor paths) to achieve conditional exchangeability, and validating the causal map via expert review plus checks against observed conditional independences in collected data. The approach also supports assessing experiment validity (including DAG-derived checks akin to A/A tests and sample-ratio mismatch diagnostics), decomposing total effects into direct and indirect effects, and reasoning about transportability to other populations. An illustrative climate-software example shows how to adjust for variables such as vehicle variant, city, and weather/temperature to estimate unbiased total or direct causal effects. Overall, the contribution is a practical DOE/experimentation design workflow grounded in causal inference rather than classical factorial/RSM DOE.","Key identification relies on DAG/backdoor reasoning: choose an adjustment set $Z$ that blocks all backdoor paths from treatment (software change) $T$ to outcome $Y$ to satisfy conditional exchangeability, enabling estimation of causal effects via $E[Y\mid do(T)]$ using adjusted estimators. The paper notes implementation options such as stratification (estimating conditional causal effects by strata of $Z$), inverse probability weighting using propensity scores $P(T\mid Z)$, or including $Z$ in a (linear) regression model; under linearity, an indirect effect can be computed as (total effect) minus (direct effect).","In the illustrative example, the DAG implies that to identify the unbiased total effect of the climate software change on energy consumption, adjustment for vehicle variant is sufficient, while identifying the direct effect requires adjusting for city, temperature (weather), and vehicle variant. The paper reports qualitative process benefits (clearer communication of assumptions, validity checks, and support for transportability reasoning) but does not provide numerical power/ARL-style performance metrics or quantified effect-size improvements.",The authors state that the main disadvantage is the need to construct a correct causal graphical model; the true causal structure may be hard or impossible to obtain in some cases.,"The paper provides a high-level workflow and an illustrative example but limited empirical evaluation of how often DAG-based adjustment changes conclusions versus simpler designs, and no quantitative assessment of sensitivity to DAG misspecification. It also does not detail operational constraints such as how restricted randomization is implemented in practice (e.g., blocking, rerandomization, compliance issues) or how uncertainty in the learned/validated graph propagates into effect estimates and decisions. The approach may be challenging when key confounders are latent/non-measurable or when outcomes and covariates are strongly time-dependent/autocorrelated, which is common in cyber-physical/vehicle telemetry.",None stated.,"Future work could formalize design procedures for restricted randomization informed by the DAG (e.g., blocked or constrained assignment, rerandomization) and evaluate their bias/variance tradeoffs under realistic automotive telemetry constraints. Sensitivity analyses for unmeasured confounding and graph uncertainty (e.g., partial identification, Bayesian model averaging over DAGs) would strengthen robustness. Additional real-world case studies with quantified impacts on decision quality, required sample sizes, and generalization/transportability across markets, climates, and vehicle variants would improve external validity, along with open-source tooling templates for practitioners.",2204.08743v3,https://arxiv.org/pdf/2204.08743v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:06:38Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|E-optimal|Compound criterion|Other,"Variable/General (sensor selection: choose p sensors from n candidates; examples include n=100, 1000, 10000; latent dimension r=10; p up to 20 in experiments)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Not provided,https://creativecommons.org/licenses/by-nc-nd/4.0/|http://papers.nips.cc/paper/8351-rsn-randomized-subspace-newton.pdf|https://www.sciencedirect.com/science/article/pii/S0888327021000145|http://www.techscience.com/CMES/v129n1/44194|https://doi.org/10.1063/5.0049071|https://doi.org/10.1260/1756-8293.6.1.29|https://www.sciencedirect.com/science/article/pii/S0965997820303045,"The paper proposes a nondominated-solution-based multi-objective greedy (NMG) algorithm for sensor selection formulated as an optimal design of experiments problem using Fisher information matrix criteria. The method simultaneously optimizes multiple DOE criteria (D-, A-, and E-optimality) by iteratively expanding candidate sensor sets and retaining up to Lmax nondominated (Pareto-ranked) sets each step using an efficient nondominated sorting procedure (ENS-SS) with crowding-distance-based selection. The authors theoretically relate NMG to a broader class of “relaxed greedy methods” and discuss approximation guarantees for monotone submodular objectives, motivating why combining D/A/E criteria can improve practical performance even when some criteria (notably E-optimality) lack submodularity. Empirical evaluation on randomly generated Gaussian sensor-candidate matrices (Monte Carlo over 100 trials) shows NMG yields sensor sets with better D-, A-, and E-optimality indices than single-objective pure greedy methods and generally better than single-objective group-greedy baselines, at the cost of increased computation. The study also analyzes the Pareto fronts and reports trade-offs (notably between D- and E-optimality) and weaker association between A- and E-optimality.","Sensor model: $y = HUz = Cz$ with $C=HU$. DOE objectives for selecting $p$ sensors use the Fisher information matrix: D-optimality maximizes $f_D=\det(CC^\top)$ (if $p\le r$) or $\det(C^\top C)$ (if $p>r$); A-optimality minimizes $f_A=\operatorname{tr}[(CC^\top)^{-1}]$ (or $\operatorname{tr}[(C^\top C)^{-1}]$); E-optimality maximizes $f_E=\lambda_{\min}(CC^\top)$ (or $\lambda_{\min}(C^\top C)$). The NMG algorithm builds sets sequentially, evaluating these objectives for candidate augmentations of retained sets and selecting up to $L_{\max}$ nondominated sets each step via nondominated sorting (ENS-SS) and crowding distance.","Across random Gaussian test problems (100 Monte Carlo trials) with $n\in\{100,1000,10000\}$ sensor candidates, $r=10$, and $L_{\max}\in\{5,10,20,50\}$, the NMG method consistently improves all three criteria (higher D and E, lower A) relative to pure greedy methods optimized for a single criterion. With $L_{\max}=50$, NMG also generally outperforms single-objective group greedy methods (DGG/AGG/EGG) for A- and E-optimality, while being comparable to DGG for D-optimality when $p\le r$ and slightly worse for $p>r$. Computational time is higher for NMG than both pure greedy and group greedy due to repeated nondominated sorting and evaluation of three criteria each step, but the paper argues complexity is moderated by early termination in sorting. Correlation analysis for $p=20$ indicates a strong negative association between D- and E-optimality (e.g., correlations around -0.84 to -0.88), suggesting a pronounced trade-off on the Pareto front.","The authors note increased computational time relative to pure and group greedy methods, mainly due to nondominated sorting (ENS-SS with crowding distance) and evaluating all three optimality indices at each step. They also note that formal submodularity guarantees do not directly apply to the unmodified division-into-cases D/A/E objectives, motivating discussion of modified (unified) submodular forms for D and A (via adding $\epsilon I$) and highlighting that E-optimality lacks submodularity.","Experiments are primarily on synthetic random Gaussian candidate matrices; performance on structured, application-specific sensor libraries (e.g., fluid dynamics POD modes with constraints) is not demonstrated, limiting external validity. Comparisons focus on greedy and group-greedy baselines; broader multi-objective optimization baselines (e.g., NSGA-II, multi-objective convex relaxations, or Pareto-front approximations with submodular multiobjective methods) are not benchmarked. Implementation details that affect reproducibility (e.g., exact parameter settings, random seeds, numerical stabilization for inverses/determinants, handling of near-singularity) are not fully captured in the provided text excerpt, and code is not shared. The method’s behavior under model mismatch (noise, correlated noise, nonlinearity, or parameter uncertainty) is not evaluated, though these are common in sensor placement practice.","The paper notes that incorporating sensor arrangement cost is important in practical problems and suggests the proposed multi-objective framework makes it easier to introduce costs as additional objectives. It also remarks that combining NMG with a more efficient alternative E-optimality-related pseudo-greedy criterion (maximal projection on minimum eigenspace) could further accelerate or improve E-optimality, but this is left out of scope and deferred to future work. The authors also emphasize applicability to actuator selection via sensor/actuator duality.","Provide open-source reference implementations (e.g., MATLAB/Python) and standardized benchmarks to enable reproducible comparison across sensor selection methods. Extend evaluation to realistic physical datasets (e.g., CFD/PIV/PSP) with spatial/installation constraints, correlated noise, and non-Gaussian disturbances, and to robust/self-starting formulations when parameters are unknown. Investigate adaptive/sequential placement (online updating of $U$ or $C$) and constraint handling (budgeted costs, minimum spacing, coverage) directly within the Pareto-greedy selection. Develop theoretical guarantees or empirical robustness studies for the true (unmodified) E-optimality objective and for multi-objective submodular optimization variants, including sensitivity to $L_{\max}$ and nondominated-sorting truncation rules.",2204.12695v2,https://arxiv.org/pdf/2204.12695v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:07:22Z
TRUE,Other,Parameter estimation|Other,Not applicable,"Variable/General (crossover designs with S sequences, P periods, repeated measures within period L; examples include 3 treatments/3 periods and 2 treatments/2 periods)",Healthcare/medical|Food/agriculture,Simulation study|Other,TRUE,R,Not provided,https://CRAN.R-project.org/package=gee|https://journal.r-project.org/archive/2013-1/mcdaniel-henderson-rathouz.pdf|https://www.R-project.org/,"The paper develops a methodology for analyzing crossover experimental designs with repeated measures for Gaussian and non-Gaussian outcomes using generalized estimating equations (GEE). Its key contribution is a family of working correlation structures that separates within-period and between-period dependence and combines them through a Kronecker product, R(α)=Ψ⊗R1(α1). The authors propose an estimation procedure for the Kronecker correlation parameters (estimating α1 via a residual-based estimating equation and estimating Ψ via a trace-based residual moment estimator) and prove asymptotic unbiasedness and consistency. In simulations (P=3 periods, L=5 repeated measures per period, S=2 sequences; n from 2 to 100 replicates per sequence), the Kronecker-based models more often achieve the best quasi-likelihood information criterion (QIC) and yield better confidence-interval coverage than independence, with performance closest to nominal when the working structure matches the truth. Applications to arterial blood pressure (3 treatments, 3 periods, 6 sequences, 10 repeated measures per period) and dairy cattle diet data (2 treatments, 2 periods, 3 repeated measures per period) show the Kronecker structures achieve the smallest QIC and support inference including carryover effects when no washout period exists.","The mean model is specified via a GEE-compatible linear predictor with carryover: g(μ_ijk)=x_ijk^Tβ=μ+γ_j+τ_{d[i,j]}+θ_{d[i,j-1]}+… (higher-order carryover terms). The working covariance is V(μ_i)=D(V(μ_ijk)^{1/2}) R(α) D(V(μ_ijk)^{1/2}), with the proposed correlation matrix R(α)=Ψ⊗R1(α1). Within-period parameters α1 are estimated using an estimating equation built from products of Pearson residuals, while between-period correlations are estimated by \hat{ψ}_{jj'}=(1/n)\sum_i tr(R1(\hat{α}_1)(r_i^{(j)}-\bar r^{(j)})(r_i^{(j')}-\bar r^{(j')})^T). Model comparison uses QIC = −2 QL(\hat μ; I) + 2 tr(\hat Ω_I^{-1} \hat V_R).","In the simulation study (100 Monte Carlo repetitions per scenario; n from 2 to 100 replicates per sequence), the model with the true Kronecker correlation structure most frequently attains the smallest QIC, even at low replication. Confidence-interval coverage for period and sequence effects is poor under an independence working correlation, while Kronecker and standard correlated structures provide more appropriate coverage; the true (Kronecker) structure is closest to the nominal 95% overall. In real data, QIC favored Kronecker structures: for arterial pressure the smallest QIC is for Ψ⊗Exch_10 (43393) and for dairy cattle the smallest QIC is for Ψ⊗Exch_3 (138.22). Estimated between-period Ψ shows small-to-moderate positive correlations (e.g., arterial: ψ_13≈0.4486; dairy: ψ_12≈0.1073), while within-period exchangeable correlations were ≈0.4958 (arterial) and ≈0.5610 (dairy).",None stated.,"The work targets analysis/working-correlation specification for crossover designs rather than constructing or optimizing the experimental design itself; practical guidance on how design choices (e.g., number of periods, sequences, repeat times) affect identifiability/power is limited. The proposed estimation relies on large-sample asymptotics and independence across experimental units; performance under small samples, missingness, dropout, or subject-level autocorrelation/nonstationarity beyond the chosen R1 forms may be less reliable. Comparisons are primarily to standard single-matrix working correlations via QIC; broader benchmarking against alternative longitudinal/crossover approaches (e.g., GLMMs with flexible random effects, Bayesian hierarchical models) is limited. No publicly shared code is provided, which may hinder reproducibility despite clear mention of R packages used.",None stated.,"Develop design-stage recommendations for crossover studies with repeated measures (e.g., selecting number/timing of within-period measurements and number of periods/sequences) that best support estimation of carryover and the Kronecker correlation parameters. Extend the correlation family and estimation to settings with missing data/dropout typical in crossover/longitudinal studies and assess robustness to misspecified link/variance functions for non-Gaussian outcomes. Provide software (e.g., an R package or reproducible scripts) implementing the proposed Kronecker GEE fitting, estimation of Ψ and α1, and QIC-based selection. Investigate multivariate outcomes and more complex crossover structures (cluster-randomized crossover, unequal period lengths, variable numbers of repeats per period) where separable Kronecker assumptions may need adaptation or diagnostics.",2205.01281v1,https://arxiv.org/pdf/2205.01281v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:07:58Z
TRUE,Optimal design|Sequential/adaptive|Other,Cost reduction|Model discrimination|Other,Not applicable,Variable/General (graph nodes/variables; interventions on subsets of variables),Theoretical/simulation only,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/SinaAkbarii/min_cost_intervention/tree/main,"The paper studies how to design a minimum-cost set (or collection) of interventions in a known causal graph to make a target causal effect identifiable when observational data alone is insufficient due to latent confounding. It proves the minimum-cost intervention design problem is NP-complete and hard to approximate below a logarithmic factor by reductions to weighted vertex cover and weighted minimum hitting set. The authors reformulate identifiability obstacles as “hedges” and show the problem reduces to hitting all hedges; they propose an exact algorithm that iteratively discovers (minimal) hedges and solves a (weighted) hitting set, with a logarithmic-factor approximation variant using greedy hitting set. They also propose polynomial-time heuristics based on minimum-weight vertex cuts (via max-flow) and provide regret bounds under Erdős–Rényi random graph models. Extensive simulations on random graphs (and benchmark structures with added latent confounding in the appendix) show the heuristics achieve low regret with substantially lower runtime than the exact method.","The intervention design is posed as minimizing total intervention cost over collections of intervention sets that render a causal query identifiable: $\mathcal{A}^*\in\arg\min_{\mathcal{A}\in \mathrm{ID}_G(S,T)}\sum_{A\in\mathcal{A}} C(A)$, with additive costs $C(A)=\sum_{x\in A}C(x)$. For single c-component targets, it reduces to a single intervention set optimization $A^*_S\in\arg\min_{A\in \mathrm{ID}_1(S)}\sum_{a\in A} C(a)$, and is shown equivalent to a weighted minimum hitting set over hedge sets (hit $F_i\setminus S$ for all hedges $F_i$). The anytime version returns $A\cup \mathrm{pa}_{\leftrightarrow}(S)\cup (\mathrm{Hhull}(S,G[V\setminus(A\cup \mathrm{pa}_{\leftrightarrow}(S))])\setminus S)$ with regret bounded by the added hedge-hull cost.","The problem is NP-complete, and approximating within a factor better than about $(1-o(1))\ln|V|$ is NP-hard (via reduction from weighted minimum hitting set). An exact algorithm (Algorithm 2) is provided that returns an optimal minimum-cost intervention set by iteratively discovering hedges and solving a hitting set subproblem; a greedy hitting-set variant yields a logarithmic-factor approximation when only polynomially many hedges are discovered. Two max-flow/vertex-cut-based heuristics run in $O(|V|^3)$ and empirically achieve small normalized regret on Erdős–Rényi random graphs; average-case bounds under equal costs give $\mathbb{E}[c_1]\le q^{-1}\mathbb{E}[c^*]$ and $\mathbb{E}[c_2]\le p^{-1}\mathbb{E}[c^*]$ for the two heuristics. Simulations (e.g., with $p=0.35,q=0.25$ and costs in {1,2,3,4}) show heuristic methods are much faster than the exact approach while often remaining near-optimal.","The authors note the exact algorithm has exponential worst-case runtime because it may require discovering exponentially many hedges before termination. They also state the heuristic algorithms can “potentially stumble on sub-optimal solutions,” offering no worst-case optimality guarantee beyond the provided average-case/regret analyses in certain random-graph models. They further discuss that relaxing additive/linear intervention costs changes approximation guarantees (e.g., greedy hitting set no longer ensures logarithmic-factor approximation under general non-linear costs).","The approach assumes the causal graph (including latent-confounding structure encoded by bidirected edges) is known and correct; misspecification would directly affect identifiability and chosen interventions. Costs are modeled largely as additive over variables (with extensions discussed), but real experimental costs often involve complex logistics, constraints, and interference that may be hard to encode as simple set functions. Empirical evaluation is mostly on simulated/random graphs (and benchmark structures with synthetic confounding), so evidence on real experimental decision-making workflows and practical identifiability verification costs is limited. The paper focuses on identifiability (learnability in principle) rather than statistical efficiency (sample sizes, variance) once interventions are selected, which can matter for practical experimental design.",The paper suggests extending and interpreting results under more general non-linear (non-additive) cost models and discusses that most results remain applicable under milder assumptions like non-decreasing costs. It also points to studying special cases where the problem can be solved in polynomial time (Appendix D) and leveraging the algorithmic connections to other NP-class problems via hitting set/set cover formulations. The anytime version is presented as a practical direction: run until a desired computable regret bound is achieved.,"A valuable extension would integrate statistical efficiency into the objective (e.g., sample size/cost trade-offs and uncertainty-aware or Bayesian criteria), not just identifiability. Another direction is robustness to graph uncertainty—design interventions when the causal graph is partially known (equivalence classes) or estimated with error, possibly via minimax or Bayesian experimental design over graphs. Developing scalable implementations and benchmarks for larger graphs (thousands of nodes) using modern MILP/CP-SAT solvers or specialized approximation schemes could broaden applicability. Finally, validating the method on real intervention planning problems (e.g., online A/B testing with constraints, biomedical perturbation experiments) would test practical assumptions about costs and permitted interventions.",2205.02232v3,https://arxiv.org/pdf/2205.02232v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:08:42Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Other,Other,Variable/General,Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper develops general asymptotic efficiency bounds for experiments where units are drawn sequentially from an infinite population and treatment assignment can be any (possibly randomized) function of the full covariate sample and past observed outcomes, covering stratification, matched pairs, and adaptive/sequential designs. It proves a likelihood expansion and a local asymptotic normality (LAN) result for such dependent, design-adaptive data generating processes, enabling Le Cam–style efficiency bounds beyond the iid setting. For binary-treatment average treatment effect (ATE) estimation, it shows that allowing stratification or other dependence-inducing assignment rules cannot improve the first-order asymptotic efficiency beyond the optimized Hahn (1998) semiparametric bound achieved under an independently assigned propensity score chosen via the Neyman allocation. The results extend to multiple treatments, including designs with constraints on treatment/sampling via cost constraints, yielding analogous optimized semiparametric efficiency bounds. Overall, the contribution is theoretical: it characterizes fundamental efficiency limits for a broad class of experimental designs and shows the optimized iid bound remains a lower bound even under adaptive/stratified designs.","For binary treatment with independent assignment propensity score $e(x)$, the Hahn (1998) variance bound is $v_{e(\cdot)}=\mathrm{var}(\mu(X,1)-\mu(X,0))+\mathbb{E}[\sigma^2(X,0)/(1-e(X))]+\mathbb{E}[\sigma^2(X,1)/e(X)]$. Minimizing this yields the Neyman allocation condition $\sigma^2(x,0)/[1-e^*(x)]^2=\sigma^2(x,1)/[e^*(x)]^2$. The main LAN expansion shows the log-likelihood ratio admits a first-order score term plus a quadratic information term even when $W_{n,i}$ depends on $(X_1,\ldots,X_n)$ and past outcomes, with information involving $I_X+\frac1n\sum_i\sum_w \mathbf{1}\{W_{n,i}=w\}I_{Y(w)\mid X}(X_i)$.","Theorem 3.1 provides a general likelihood ratio expansion for arbitrary covariate- and history-dependent treatment rules; Corollary 3.1 gives LAN under convergence (or upper-bounding) of the implied information matrix. For binary ATE, Theorem 4.1 shows that for any treatment rule (including stratified/adaptive rules), the experiment is LAN with information equal to the optimized Hahn bound under the Neyman allocation, implying no first-order asymptotic efficiency gains beyond that bound are possible. Corollary 4.1 gives a local asymptotic minimax lower bound for any ATE estimator under any such design, with limiting risk at least that of $N(0,v_{e^*})$. For multiple treatments with cost constraints, Theorem 5.1 yields an analogous LAN result and lower bound with optimized sampling probabilities $p^*(x,w)$ satisfying first-order/KKT conditions (Eq. 10–11) under constraints (Eq. 9,12).",None stated.,"The results are first-order (root-$n$) asymptotic bounds; they do not address potential finite-sample or higher-order efficiency gains that stratification/matching can deliver in practice. The framework assumes iid sampling from an infinite population (or sampling with replacement approximation), which may be imperfect for finite populations or settings with interference/network effects, and it largely abstracts from outcome autocorrelation or time-series dependence beyond the design-induced dependence. Practical attainability requires estimating nuisance quantities (e.g., conditional variances for Neyman allocation, or KKT multipliers under constraints), but the paper focuses on bounds rather than implementable procedures with guaranteed performance under misspecification.",None stated.,"Develop constructive, fully implementable adaptive/stratified design-and-estimation procedures that attain the optimized bounds under realistic nuisance estimation (including uniform validity with estimated $e^*(x)$ or $p^*(x,w)$ and cost constraints). Extend the LAN and efficiency results to settings with interference/spillovers, clustered sampling, or serially dependent outcomes where iid population sampling is violated. Study second-order/fixed-$n$ approximations to quantify when dependence-inducing designs (matched pairs, fine stratification) improve finite-sample MSE/power despite identical first-order bounds, and provide guidance on tradeoffs under practical constraints.",2205.02726v2,https://arxiv.org/pdf/2205.02726v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:09:03Z
TRUE,Optimal design|Bayesian design|Robust parameter design,Optimization|Model discrimination,Bayesian D-optimal|Not applicable,"Variable/General (design variable(s) \(\xi\); examples include A/B group size, 1D location in Preference model, and sampling time(s) in pharmacokinetics)",Healthcare/medical|Service industry|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Robust Expected Information Gain (REIG) for Bayesian experimental design, addressing sensitivity of expected information gain (EIG) to prior misspecification and sampling error in EIG estimators. REIG is defined by minimizing an affine (tangent) relaxation of EIG over a KL-divergence ambiguity set around the reference prior, yielding a conservative, risk-averse design criterion. Using convex duality, REIG reduces to a 1D convex optimization that acts as a log-sum-exp stabilization/post-processing of per-sample KL-divergence (or information) estimates, making it easy to integrate with sampling-based EIG estimators. Numerical experiments on benchmark design problems (A/B testing, preference learning, pharmacokinetic sampling-time design) show that REIG can provide lower-bounding/worst-case behavior under prior perturbations and can mitigate variability from under-sampled estimators (VNMC/ACE/MINE) by downweighting extreme/inflated contributions.","EIG is defined as \(I(p,\xi)=\mathbb{E}_{p(\theta,y\mid \xi)}[\log \tfrac{p(y\mid\theta,\xi)}{p(y\mid\xi)}]\). The robust objective uses an ambiguity set \(\mathcal{A}(\varepsilon,p)=\{q: D_{KL}(q(\theta)\|p(\theta))\le \varepsilon\}\) and defines \(I_\varepsilon(p,\xi)=\inf_{q\in\mathcal{A}} I_{\text{aff}}(q,\xi;p)\), where \(I_{\text{aff}}(q,\xi;p)=\mathbb{E}_{q(\theta,y\mid\xi)}[\log \tfrac{p(y\mid\theta,\xi)}{p(y\mid\xi)}]\) is the affine/tangent approximation. Duality yields \(I_\varepsilon(p,\xi)= -\inf_{\lambda\ge 0}\big\{\lambda\varepsilon + \lambda\log \mathbb{E}_{p(\theta)}[\exp(- D_{KL}(p(y\mid\theta,\xi)\|p(y\mid\xi))/\lambda)]\big\}\), implemented via a sample-average log-sum-exp over estimated divergences.","In a preference-model robustness test, the authors set a perturbed prior with KL divergence \(\varepsilon=0.2\) and show empirically that \(I_\varepsilon(p,\xi)\) acts as a lower bound for both \(I(p,\xi)\) and \(I(q,\xi)\) across designs while changing the optimal design compared to maximizing EIG under the reference prior. In stabilization experiments, using small-sample settings (e.g., VNMC/ACE with \(M=30\) posterior samples) they report that increasing \(\varepsilon\) (e.g., from 0.001 up to 0.1 in A/B tests) reduces overestimation for problematic designs without harming well-estimated designs. For Preference and Pharmacokinetic benchmarks, they use radii like \(\varepsilon=0.2\) and \(\varepsilon=0.1\) (based on KL between alternative and original priors) and show that REIG-based post-processing can tighten/shift lower/upper-biased estimators toward the high-sample “true” EIG curves, though large \(\varepsilon\) can invert the usual upper-/lower-bound behavior of VNMC vs. ACE.","The authors state that their understanding of how to apply the method is incomplete, with most open questions related to choosing the ambiguity-set radius \(\varepsilon\); they note that an a priori estimate for \(\varepsilon\) seems unlikely in most cases. They also emphasize that results are initial numerical experiments and that more investigation is needed into principled selection of \(\varepsilon\), potentially linked to sampling error in EIG estimators.","The robustness is only with respect to prior uncertainty; the likelihood/model (and observation model mismatch) is treated as certain, which may limit usefulness under model misspecification common in practice. The empirical evaluation is benchmark-focused and largely simulation-based with neural estimation components, but the paper does not provide implementation details sufficient for full reproducibility (no shared code), nor a broad sensitivity analysis over estimator hyperparameters/training stability. The affine relaxation introduces conservatism whose impact on true design optimality (gap to \(I^{\text{true}}_\varepsilon\)) may be scenario-dependent, and large \(\varepsilon\) can alter estimator bound properties (VNMC/ACE) in non-intuitive ways, potentially complicating practitioner interpretation.","They propose developing a principled, practical choice of the ambiguity radius \(\varepsilon\), analogizing to Morozov’s discrepancy principle by matching \(\varepsilon\) to the sampling error of estimators like VNMC/ACE. They suggest leveraging the fact that VNMC and ACE provide (in expectation) upper and lower bounds on EIG to form an a posteriori estimate of a suitable \(\varepsilon\), and state that investigating this is future research.","Extend the ambiguity-set robustness beyond the prior to include likelihood/model uncertainty (e.g., ambiguity in \(p(y\mid\theta,\xi)\) or the joint \(p(\theta,y\mid\xi)\)) and evaluate the tradeoffs empirically. Provide self-contained software and systematic benchmarks comparing REIG against other robust BED criteria (e.g., worst-case mutual information under alternative divergences, Wasserstein balls) and assess computational overhead in high-dimensional designs. Study theoretical and empirical guidance for \(\varepsilon\) selection under finite-sample nested estimators (e.g., concentration bounds/CI-driven calibration) and investigate behavior under dependent/autocorrelated data or sequential/adaptive experimental design loops.",2205.09914v1,https://arxiv.org/pdf/2205.09914v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:09:45Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,A-optimal|E-optimal|Not applicable,Variable/General (design over query points/allocations; functional dimension p emphasized),Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops optimal experimental design (OED) methods for estimating known linear functionals $C\theta$ when the unknown element $\theta$ lies in a reproducing kernel Hilbert space (RKHS), covering both finite- and infinite-dimensional settings. Unlike classical estimability-focused linear regression OED, it explicitly accounts for unavoidable approximation bias and proposes bias-aware design objectives via information matrices for (i) an interpolation/least-norm estimator and (ii) a ridge-regularized estimator. It studies fixed and adaptive (sequential) designs, deriving non-asymptotic high-probability confidence sets under sub-Gaussian noise whose width scales with the functional dimension $p$ (range of $C$) rather than $\dim(\mathcal H_\kappa)$. Design criteria are scalarizations of the information matrix, focusing on E-optimality (maximize $\lambda_{\min}$ for confidence sets) and A-optimality (minimize MSE via trace), and it discusses robust (min–max over uncertain functionals/parameters) formulations. Algorithms for constructing designs include greedy/sequential allocation updates and convex-optimization-based fractional designs with rounding, and the approach is demonstrated on applications including gradient estimation, ODE learning (pharmacokinetics), linear bandits, and Lyapunov stability certification.","Observation model: $y=\langle \theta,\Phi(x)\rangle+\varepsilon$ with sub-Gaussian noise. Interpolation estimator for a fixed design set $X$: $\widehat{C\theta}=L^\dagger y = C V_0^{-1}X^\top K^{-1}y$ with $K=XV_0^{-1}X^\top$, and associated (interpolation) information matrix $W^\dagger(X)=\big(CV_0^{-1}X^\top K^{-2}X V_0^{-1}C^\top\big)^{-1}$. Ridge estimator: $\widehat{C\theta}_\lambda=L_\lambda y=C V_0^{-1}X^\top(\lambda\sigma^2 I+K)^{-1}y$ with information matrix $W_\lambda(X)=\sigma^{-2}\big(C(\sigma^2\lambda V_0+X^\top X)^{-1}C^\top\big)^{-1}$; design scalarizations include $f_E(W)=\lambda_{\min}(W)$ and $f_A(W)=1/\mathrm{Tr}(W^{-1})$.","For fixed designs, the paper gives non-asymptotic confidence bounds for both interpolation and ridge estimators; for interpolation it requires a relative bias condition $\|(C-L^\dagger X)V_0^{-1/2}\|_F^2\le \nu^2\|L^\dagger\|_F^2$ and yields $\|C(\widehat\theta-\theta)\|_{W^\dagger}\le \sigma\sqrt{\xi(\delta)}+\nu/\sqrt{\lambda}$ (probability $\ge 1-\delta$) with $\xi(\delta)=p+2(\sqrt{p\log(1/\delta)}+\log(1/\delta))$. For ridge with fixed design it provides $\|C(\widehat\theta_\lambda-\theta)\|_{W_\lambda}\le \sqrt{\xi(\delta)}+1$ (probability $\ge 1-\delta$). For adaptive designs, it derives confidence sets based on projected features $z(x)$ and an additive information matrix $\Omega_{\lambda,t}=\sigma^{-2}Z_t^\top Z_t+\lambda S$ that yields widths scaling with $p$ and supports sequential stopping rules; it also notes improved linear bandit regret bounds when the action set is finite.","The paper notes that it focuses on E- and A-design scalarizations and does not treat other common criteria (e.g., D-, V-, G-design) “due to space considerations.” It also states that providing a general recipe for checking the relative bias condition is not possible because it depends strongly on the form of the linear operator $C$, though it is always possible in principle via representer arguments and often via truncation/projection. It remarks that an exhaustive treatment of rounding techniques and design-construction algorithms is out of scope.","The practical computation of RKHS operators (e.g., $V_0$, projections defining $z(x)$, and inverses/pseudoinverses of kernel matrices) can be numerically delicate for large $T$ or ill-conditioned kernels, but systematic numerical stability guidance is limited. The empirical evaluation appears application-driven rather than a broad benchmark across many kernels/design spaces/noise levels; comparative baselines may not cover the full set of modern OED approaches (e.g., fully Bayesian OED, mutual-information criteria, or recent approximate G-optimal designs) in the RKHS setting. The theoretical guarantees rely on correct kernel/model specification and sub-Gaussian noise; robustness to kernel misspecification, heavy-tailed noise, or temporal dependence is not deeply analyzed.",None stated.,"Extending the bias-aware design and confidence-set results to settings with dependent/heteroscedastic noise (common in time-series and control) and to heavy-tailed noise via robust/self-normalized methods would broaden applicability. Developing scalable implementations (e.g., low-rank kernel approximations with certified error, streaming updates for $\Omega_{\lambda,t}$, and automated bias-condition checks) and releasing software would help adoption. Additional theory for other optimality criteria (D-, G-, I-optimality) and more extensive empirical benchmarks across kernels and high-dimensional domains would clarify trade-offs and best practices.",2205.13627v3,https://arxiv.org/pdf/2205.13627v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:10:30Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Other,Bayesian D-optimal|D-optimal|Other,Variable/General (examples include 1D design variable $x$; polynomial degree $k$; preference-learning over a discrete set of 200 gambles),Theoretical/simulation only|Other,Simulation study,TRUE,None / Not applicable,Not provided,https://doi.org/10.5281/zenodo.5523272,"The paper studies Bayesian adaptive experimental design (a form of information-theoretic active learning) and shows it can suffer from active learning bias (ALB) when the assumed model class is misspecified. The adaptive design chooses each next design point by maximizing expected information gain (mutual information between the next observation and parameters), which in linear-Gaussian regression reduces to a Bayesian D-optimality criterion. Through extensive simulations in polynomial regression and a simulated preference-learning task, the authors show (i) ALB appears under misspecification and can lead to worse generalization than random (passive) sampling, (ii) the magnitude of ALB increases with a KL-divergence-based measure of model misspecification ($D_{model}$), and (iii) specifying larger observation noise in the hypothesized model mitigates or eliminates ALB. They further note that in linear regression the optimal adaptive design is effectively batch D-optimal and independent of realized outcomes, while nonlinear settings (e.g., preference learning) still exhibit the same qualitative robustness patterns.","Bayesian updating: $p_t(\theta)=p_{t-1}(\theta)\,\frac{m(y_t\mid x_t,\theta)}{\int m(y_t\mid x_t,\theta)p_{t-1}(\theta)d\theta}$. Adaptive design objective uses expected information gain with utility $u(x,y,\theta)=\log\frac{p_t(\theta\mid x,y)}{p_t(\theta)}$, so $\mathbb{E}[u_{t+1}(x)]$ is mutual information. In linear-Gaussian polynomial regression, $\mathbb{E}[u_{t+1}(x)]=\tfrac12\log(\sigma^2+\phi_k(x)^\top S_t\phi_k(x)) -\log\sigma$, leading to choosing $x$ maximizing $\phi_k(x)^\top S_t\phi_k(x)$ (equivalent to Bayesian D-optimality). Misspecification is quantified by $D_{model}=\int D_{KL}(f(x)\|m(x,\theta^*))\,g(x)\,dx$, and ALB by $\mathrm{ALB}=\frac{\mathbb{E}[L(m(x,\hat\theta_{adaptive}),y)]}{\mathbb{E}[L(m(x,\theta^*),y)]}-1$.","Simulation results show that under misspecification (e.g., fitting a linear model to quadratic data), Bayesian adaptive design yields worse out-of-distribution negative log-likelihood risk than random sampling, whereas in well-specified settings adaptive sampling improves convergence without harming asymptotic risk. Across 1,000 simulated experiments, ALB increases monotonically with the KL-based misspecification measure $D_{model}$ (shown for linear-vs-quadratic and quadratic-vs-cubic mismatches). Increasing the assumed observation noise in the hypothesized model (e.g., using $\sigma=1000$ instead of $\sigma=100$) largely eliminates ALB in the reported regression simulations and similarly reduces ALB in simulated preference learning when the choice model is made noisier (smaller sensitivity parameter). The paper also reports that in linear regression the adaptive-optimal design sequence is independent of observed $y$ and matches the (asymptotic) batch D-optimal design, so the observed ALB arises from the interaction of sampling bias with misspecification rather than feedback from outcomes.","The authors note their conclusions hinge on simulation experiments and call for theoretical results to delineate conditions under which ALB should occur as a function of misspecification. They also point out linear regression has idiosyncratic properties (e.g., equivalence of batch and adaptive solutions; optimal designs independent of outcomes) that may limit generalizability, motivating their additional nonlinear examples and appendices. In the appendix classification replication, they acknowledge fewer experiments due to higher computational cost and limited variation in $D_{model}$, preventing a strong test of the misspecification-magnitude result there.","The work does not provide empirical validation on real adaptive experiments with human/industrial data, so practical frequency and severity of ALB in deployed Bayesian adaptive design remains uncertain. The mitigation strategy of inflating noise is demonstrated but not cast as a principled robust-design objective (e.g., minimax/Bayesian model averaging over misspecification), and its impact on efficiency/power is not quantified in detail beyond qualitative trade-offs. Comparisons focus mainly on random sampling baselines rather than a broader set of robust/adversarial active-learning or covariate-shift-corrected Bayesian design methods.","They propose developing theoretical characterizations of when active learning bias should be expected (potentially as a function of $D_{model}$), drawing connections to misspecified bandit theory. They suggest studying why decreasing $D_{model}$ and increasing assumed noise enhance robustness, and how these effects relate to robustness under covariate shift more generally. They also suggest examining settings where the model includes an explicit noise parameter with a non-degenerate prior (rather than fixing it) and exploring sampling strategies that incorporate more exploratory trials to protect against biased design distributions.","Develop robust Bayesian adaptive design criteria that explicitly hedge against model misspecification (e.g., model averaging over candidate classes, worst-case KL neighborhoods, or generalized Bayesian/tempered posteriors) and evaluate them against EIG/D-optimal designs. Extend the analysis to dependent data (time series), multivariate/high-dimensional design spaces, and constraints/cost-aware designs where adaptivity is limited. Provide open-source implementations and reproducible benchmarks (including real-world adaptive preference-learning or psychophysics datasets) to quantify ALB and mitigation trade-offs across domains.",2205.13698v2,https://arxiv.org/pdf/2205.13698v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:11:09Z
TRUE,Factorial (full)|Factorial (fractional)|Response surface|Split-plot|Optimal design|Screening|Sequential/adaptive|Computer experiment|Robust parameter design|Other,Parameter estimation|Screening|Optimization|Model discrimination|Prediction|Robustness|Other,D-optimal|Space-filling|Minimax/Maximin|Other,"Variable/General (often specified as number of factors p and/or runs n, depending on package/function)",Theoretical/simulation only|Other,Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/emitanaka/paper-DoE-review,"This preprint reviews the current state and future prospects of R packages for design of experiments (DoE), focusing on CRAN’s ExperimentalDesign task view. It combines exploratory analyses of RStudio CRAN download logs (2013–2021), CRAN package metadata, and task-view package networks to characterize which DoE tools are used in practice and how the ecosystem evolves. The paper finds downloads are concentrated in a small set of long-established packages while the overall set remains diverse, and it documents low collaboration/intra-dependency among DoE packages (development “in silos”). It also qualitatively compares interface styles for generating designs—factorial design generators (full/fractional/orthogonal arrays), recipe-style functions for classical designs (e.g., CRD/RCBD/split-plot), optimal-design search functions (e.g., Fedorov/Monte Carlo/blocking), space-filling designs for computer experiments (e.g., Latin hypercubes), and augmenting/sequential designs. The authors argue for more unified, cognitively aligned interfaces to improve extensibility and practical adoption across the DoE software ecosystem.",Not applicable (the paper is primarily a software-ecosystem review/EDA; it describes design-generation functions and criteria conceptually rather than introducing a new charting/statistic or deriving new DOE formulas).,"The exploratory analysis reports substantial inequality in downloads across DoE packages (Gini index ranging roughly 32.7% to 69.1% from 2013–2021), with about 68% of 2021 downloads attributable to the top ~10% of packages. Most top-10 packages by downloads remain consistently top-ranked over the nine-year window, and earlier-release packages tend to have higher download counts (moderate negative correlations between first release date and log downloads). Text-mining of package descriptions highlights frequent emphasis on “optimal design”, “sequential design”, “latin hypercube”, and “computer experiment,” suggesting strong practical interest in optimal/space-filling/adaptive approaches. Network summaries show ExperimentalDesign has among the lowest intra-connectivity and average contributors across CRAN task views, consistent with siloed development.","The authors note several limitations: CRAN task views are volunteer-maintained so relevant DoE packages may be missing; only RStudio’s CRAN mirror downloads were analyzed, which may bias usage estimates; the study is limited to R packages even though practitioners may use other software; and the analyses are observational/EDA, so conclusions are speculative and not necessarily generalizable.","Download counts are an imperfect proxy for real-world usage because they conflate automated CI installs, dependencies, mirrors, and repeated installs, and they do not measure whether functions are actually used for DOE generation versus analysis. The qualitative interface discussion focuses largely on a handful of highly downloaded packages, which may under-represent modern or niche interface innovations and may not capture practitioner workflows outside CRAN (e.g., Bioconductor, proprietary tools, internal company packages). The paper does not provide a systematic taxonomy mapping packages to design classes/criteria with standardized benchmarks, so “coverage” of DOE capabilities remains somewhat anecdotal.","The authors suggest future work including: further exploratory data analysis, expanding beyond R packages to other software ecosystems used for experimental design, and additional discussion of interface design aspects to support more consistent, extensible, practitioner-aligned DOE software development.","A useful extension would be to build a structured, machine-readable ontology of DOE capabilities (design classes, constraints, criteria, outputs) across packages, enabling gap analysis and interoperability testing. Another direction is standardized computational benchmarks comparing design generators on common tasks (e.g., D-optimal under constraints, space-filling in high dimensions, sequential design under model misspecification) with reproducible code and performance metrics. Developing and evaluating unified interfaces (e.g., consistent formula/data semantics, constraint specification, and augmentation workflows) across packages—possibly via a shared core library—could directly address the silo problem identified.",2206.07532v2,https://arxiv.org/pdf/2206.07532v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:11:45Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Other,Other,"Variable/General (design variables are pyruvate and lactate flip angles per scan; total dimension 2N, with N=30 in examples)",Healthcare/medical,Simulation study|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/prashjha/HyperpolarizedMRI,"The paper proposes an information-theoretic optimal experimental design (OED) framework for hyperpolarized 13C-pyruvate MRI to reduce uncertainty in the inferred pyruvate-to-lactate apparent exchange rate, kPL. Mutual information (MI) between measured signal data and uncertain kinetic/physiologic parameters is used as the design criterion, and the flip angles for pyruvate and lactate are optimized (either constant across time or time-varying across acquisitions). A standard spatially invariant two-compartment pharmacokinetic ODE model is used for design and inference, and a spatially varying reaction–diffusion PDE (Block-Torrey/continuum mixture theory) “high-fidelity” model is introduced to generate synthetic data as a control to test robustness to spatial effects like diffusion and vascular source strength. MI is approximated via Gauss–Hermite quadrature under Gaussian priors and Gaussian additive noise assumptions; gradients for optimization and inversion are computed using MATLAB automatic differentiation. Numerical experiments show that while time-varying flip angles can yield higher MI values, constant flip-angle designs can provide better accuracy/precision for recovering kPL from noise-corrupted data; for the studied setting, constant angles around 35° (pyruvate) and 28° (lactate) performed best at low SNR, and results also highlight potential bias when fitting spatially varying (HF) data with the spatially invariant (LF) model.","The low-fidelity (LF) model propagates compartment-averaged magnetization via $\bar\phi(t_{k+1})=\exp(T\!R_{k+1}A)\,C_k\bar\phi(t_k)+\frac{k_{ve}}{\nu_e}\int_{t_k}^{t_{k+1}}\exp((t_{k+1}-\tau)A)\,VIF(\tau)\,d\tau$, where $C_k=\mathrm{diag}(\cos\theta_P^k,\cos\theta_L^k)$ and $A$ encodes relaxation and exchange (including $k_{PL}$). The measured signals at scan $k$ are $s^k=\mathrm{diag}(\sin\theta_P^k,\sin\theta_L^k)\,(\nu_e\bar\phi^k+(1-\nu_e)VIF(t_k))$, and the scalar “total signal” data used for MI is $G(K,P)=\sum_{k=1}^N(s_P^k+s_L^k)$. The OED criterion is mutual information $I(K)=\iint p(z,P)\log\frac{p(z,P)}{p_0(P)p(z)}\,dP\,dz=H(z;K)-H(z\mid P;K)$, with Gaussian prior $P\sim\mathcal N(\hat P,\Sigma_P)$ and additive Gaussian noise model $z=G(K,P)+\varepsilon$, $\varepsilon\sim\mathcal N(0,\sigma_z^2)$, leading to the optimization $\max_{K\in D} I(K)$ (equivalently $\max H(z;K)$ under their assumptions).","Across tested SNR settings, MI-optimal flip angles depend on the assumed noise level; time-varying flip-angle schedules can increase the MI objective compared with constant flip angles. However, when validating by repeated inference from noise-corrupted synthetic data, the constant flip-angle scheme was generally more accurate and precise (lower variance and bias) for recovering $k_{PL}$ than the time-varying scheme, which was more noise-sensitive. For the presented data and setup, a constant design with pyruvate and lactate flip angles of approximately 35° and 28° (respectively) was recommended as best for accuracy/precision of $k_{PL}$ recovery at low SNR and was comparable to current clinical choices (e.g., 20°/30°) while improving performance at very low SNR. High-fidelity (spatial) simulations showed that inferred $k_{PL}$ can deviate from the value used in HF “ground truth,” with stronger discrepancies and higher uncertainty in regions with stronger vascular pyruvate input, illustrating model mismatch effects when fitting HF-generated data with an LF model.","The authors note that optimized design parameters may not be fully optimal in a general 3D setting because commonly used pharmacokinetic (LF) models do not account for spatial variation (diffusion/perfusion heterogeneity). They state that adding more uncertain parameters is limited by the curse of dimensionality when using quadrature for MI integration and suggest that alternative integration approaches (e.g., MCMC) may be needed. They also acknowledge that the work assumes Gaussian noise on the real component/readout and that Rician noise is more appropriate at low SNR, especially late in the acquisition as signal decays. They further acknowledge simplifications in the high-fidelity model (e.g., diffusion-dominated transport without convection, limited constituents/compartments) as a first step.","The OED criterion is computed using a scalar “total signal” $G=\sum_k(s_P^k+s_L^k)$ rather than the full vector time series, which may discard temporal information and potentially change the optimal flip-angle policy compared with designing on the full observation model. The optimization appears tailored to a specific prior/parameter set and fixed TR/number of scans; robustness to different priors, TR schedules, constraints (hardware/SAR), and physiological variability beyond the assumed Gaussian prior is not fully characterized. Comparisons are mainly within MI-based designs (constant vs time-varying) and against a representative clinical scheme, but broader benchmarking versus Fisher-information/CRB-based OED, Bayesian D-optimality, or other established HP-MRI flip-angle design strategies is limited. Practical implementation considerations (e.g., constraints on rapid flip-angle changes, calibration errors, B1 inhomogeneity, slice-profile effects) are not deeply evaluated and could affect real-world performance.","The authors propose extending uncertainty modeling beyond the current uncertain parameters, while addressing computational scaling issues of quadrature (potentially via MCMC or other integration schemes). They state that Rician noise models should be incorporated for low-SNR optimization and/or when using complex-valued data. They suggest exploring alternative and more realistic high-fidelity models that better reflect HP-MRI physics, including additional constituents and endogenous/hyperpolarized species. They also mention adding model fidelity via convection/blood-flow effects and more sophisticated vascular coupling (e.g., porous media convection, Dirichlet inflow, or 3D–1D vascular network coupling).","A natural extension is to formulate MI (or other Bayesian OED) on the full multichannel time-series observation model (pyruvate and lactate signals over scans) rather than a scalar aggregate, and to design under explicit hardware/clinical constraints (bounds, smoothness, SAR, slice timing) to improve deployability. Developing a self-consistent OED/inference pipeline that uses the same (or hierarchically linked) spatial model for both design and estimation could reduce model-mismatch bias seen with HF-generated data fit by an LF model. Robust OED that accounts for B1 inhomogeneity, calibration errors, TR variability, and motion/autocorrelation could improve clinical reliability, and multivariate designs that include additional controllable parameters (e.g., TR, spectral encoding, k-space trajectory, timing of injection/acquisition) would broaden impact. Finally, releasing validated open-source code and performing prospective validation on real patient/phantom datasets would strengthen evidence for clinical translation.",2206.12509v2,https://arxiv.org/pdf/2206.12509v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:12:35Z
FALSE,NA,NA,Not applicable,Not specified,Finance/economics|Service industry,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.1145/3377325.3377498|https://doi.org/10.1145/3511299|https://doi.org/10.1145/3287560.3287563|https://arxiv.org/abs/2005.01831|https://doi.org/10.1145/3442188.3445941|http://blog.datadive.net/interpreting-random-forests/|https://doi.org/10.1145/2939672.2939778|https://doi.org/10.1109/ICCV.2017.74|https://doi.org/10.1145/3308532.3329441,"This paper argues that evaluations of explainable ML methods often draw misleading conclusions due to experimental setups that deviate from real deployment contexts, and demonstrates this via an application-grounded user study in e-commerce fraud review. The authors replicate and modify a prior fraud-analyst study design to better match production conditions: real base rate (15% fraud), an operational utility metric (Percent Dollar Regret, PDR) instead of accuracy, a realistic third action (escalate), and additional placebo arms (random and irrelevant explanations). Seven experimental arms compare raw transaction data only, data+model score, and data+model score plus explanations from TreeSHAP, TreeInterpreter, LIME, as well as random/irrelevant feature-attribution explanations. Results show model scores significantly reduce decision time and escalation rate, but post-hoc explanations provide no measurable incremental improvement on the operational PDR metric and instead slow analysts down; random/irrelevant explanations perform similarly to ‘real’ explanation methods. The paper concludes that seemingly small evaluation-design choices (task, users, metrics, base rates, and inference strategy) can reverse conclusions about explanation utility and provides guidance for designing application-grounded evaluations.","The primary operational metric is Percent Dollar Regret (PDR):
$$\mathrm{PDR}=1-\frac{\mathrm{Realized\ Revenue}}{\mathrm{Possible\ Revenue}}=\frac{\sum_i\Big(\mathbb{1}(y_i=0,\hat y_i=1)(\beta+\delta\lambda)+\mathbb{1}(y_i=1,\hat y_i=0)\alpha\Big)v_i}{\sum_i \mathbb{1}(y_i=0)(1+\lambda)v_i}.$$
Here $y_i$ is the true fraud label (1=fraud, 0=legit), $\hat y_i$ is the analyst decision, $v_i$ is transaction value, and parameters encode business costs/benefits: $\alpha$ (FN penalty), $\beta$ (prob. losing sale on FP), $\delta$ (prob. losing long-term customer value on FP), and $\lambda$ (long-term customer value multiplier). Escalations are assumed to yield the correct decision with a time penalty $\tau$ (set to 600s in analysis).","Using 500 transactions per arm with three professional fraud analysts, introducing the model score reduced mean decision time from 79.2s (Data) to 51.3s (Model) (reported $p=.0001) and reduced escalation from 5.9% to 2.2% ($p=.003). Operational performance (PDR) showed no significant improvement: 9.5% (Data) vs 8.9% (Model), and explanation arms were similar or worse (TreeSHAP 9.7%, TreeInterpreter 10.0%, LIME 11.6%). Explanations increased decision time relative to model-only (e.g., TreeSHAP 67.3s, TreeInterpreter 65.6s; significant vs Model with $p=.01$ and $p=.03$, respectively), without significant differences in PDR or confusion-matrix metrics. Random/irrelevant explanations had similar PDR to real explainers (Random 9.0%, Irrelevant 9.7%) and also slowed decisions (Irrelevant 62.2s), supporting the claim that the mere presence of explanation content did not improve the operational objective.","The authors note that they could not randomize at the user level because only three expert analysts were available; randomization was at the transaction (instance) level, limiting user-level hypotheses and interaction analyses. They also note the relatively short exposure per experimental arm and the lack of ongoing feedback/training that exists in real operations, which may reduce the chance for analysts to learn to use explanations effectively over time. Finally, they acknowledge the study focuses only on post-hoc, feature-attribution explainers (TreeSHAP, TreeInterpreter, LIME), so findings may not generalize to other explanation modalities.","The study is effectively a within-organization, small-expert-sample experiment; even with 500 transactions/arm, conclusions about human behavior may be sensitive to analyst-specific strategies and to the fixed ordering of stages (data → model → explanations), which may confound treatment effects with learning/fatigue despite transaction randomization. The PDR metric depends on several calibrated parameters ($\alpha,\beta,\delta,\lambda,\tau$); while a sensitivity sweep is reported, the metric may still omit other operational costs (e.g., downstream customer service, chargeback processes, SLA constraints) and may be organization-specific. Also, the experimental UI and offline setting may not fully replicate production pressures (queue dynamics, time-of-day load, incentives), potentially attenuating or altering explanation effects.","The authors suggest future studies in contexts with more expert users to enable user-level randomization and to study interactions between explainer utility and individual characteristics (e.g., experience). They also propose longer-term experiments that include training periods and periodic performance feedback to test whether analysts can learn to use explanations effectively over time. Finally, they recommend exploring other explanation modalities beyond post-hoc feature attributions, including inherently interpretable models, counterfactual explanations, and example-based methods, as well as designing explanation+UI approaches tailored to the domain and to model-uncertainty regimes (e.g., the review band).","A natural extension is a counterbalanced or crossover design that randomizes the ordering of conditions (or uses Latin-square scheduling) to separate explanation effects from learning and fatigue, while still using expert analysts. Another direction is to integrate incentive-compatible or cost-constrained objectives directly into the experimental protocol (e.g., explicit time/cost budgets, escalation capacity limits) and evaluate system-level outcomes via simulation of full queue operations. Finally, releasing standardized, privacy-preserving benchmarks (or synthetic but behaviorally validated transaction sets) and open-source experiment tooling would make application-grounded evaluation more reproducible and allow head-to-head comparison of explanation methods under matched operational metrics.",2206.13503v4,https://arxiv.org/pdf/2206.13503v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:13:19Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,NA,None / Not applicable,Not applicable (No code used),https://doi.org/10.1145/3486622.3494026|https://www.mturk.com/|https://www.prolific.co/|https://github.com/manununhez/dreamtv-app,"This paper proposes AFFORCE, an actionable design framework for creating attractive and engaging crowdsourcing systems tailored to older adults (65+). The authors synthesize barriers to older adults’ participation (e.g., accessibility, motivation, communication, task/platform complexity, lack of support) and map them onto a generalized “user journey” to identify dropout points. AFFORCE then integrates and extends prior crowdsourcing framework elements into a set of design components (e.g., platform choice, task decomposition into pausable “mezzo-tasks,” task evaluation/tagging, personalized task recommendations, training/sandboxing, feedback, impact communication, user empowerment, and promotion). The work is primarily conceptual and grounded in the authors’ prior exploratory studies and a literature review, offering practitioner-oriented guidance rather than proposing a statistical or experimental design method. A stated limitation is that no existing system incorporates all proposed elements, motivating future work to evolve the framework into a more comprehensive guide as additional studies emerge.",Not applicable,NA,The authors state that a limitation of the current crowdsourcing-focused framework is that there are no systems which incorporate all of the proposed elements (noting this is also part of the motivation for writing the paper).,"The framework is not validated via a controlled evaluation (e.g., field experiment or comparative user study) demonstrating that AFFORCE elements causally reduce dropout or improve engagement for older adults. Operational guidance is high-level, leaving open how to prioritize elements under real budget/engineering constraints, and how to quantify tradeoffs (e.g., personalization vs. privacy, longer mezzo-tasks vs. completion rates). Generalizability may vary across subgroups of older adults (health, cognition, culture, ICT literacy) and across different crowdsourcing genres (paid microtasks vs. citizen science), but these boundary conditions are not formally tested.","The authors express hope that the framework will be used by researchers and practitioners and, based on future emerging studies, that they can develop the framework into a more comprehensive guide to better include older adults in ICT-mediated activities.","Empirically test AFFORCE components using randomized or quasi-experimental studies (e.g., A/B tests of onboarding/empowerment, task framing, sandbox training, and impact messaging) with engagement and quality outcomes. Develop measurable design heuristics and instruments (e.g., checklists, scoring rubrics) and implement reference UI patterns/components to support adoption by platform builders. Investigate personalization methods that respect privacy (e.g., minimal profiling, on-device models) and assess robustness under constraints common in real crowdsourcing settings (sparse data per user, cold-start, multilingual interfaces, accessibility needs).",2207.03170v1,https://arxiv.org/pdf/2207.03170v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:13:46Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Prediction|Optimization|Model discrimination|Cost reduction|Other,Other,Variable/General (batch size D; examples include 10 experimental contexts for discrete actions and D=40 for continuous actions; actions can be discrete with K options or continuous),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Not provided,https://proceedings.neurips.cc/paper/2021/file/d811406316b669ad3d370d78b51b1d2e-Paper.pdf,"The paper proposes a model-agnostic Bayesian experimental design framework for efficiently testing and improving context-dependent (personalized) treatment assignment decisions from causal ML models, focusing on evaluating expected regret of past actions. The design objective is the expected information gain about the maximum achievable reward at a set of evaluation contexts (""max-value"" EIG), rather than information about all model parameters as in standard BED. Because the objective is doubly intractable with implicit likelihoods, the authors use an InfoNCE-based lower bound to estimate the mutual-information objective and optimize it end-to-end with gradient ascent over a large-batch design; discrete actions are handled via a Gumbel–Softmax-relaxed stochastic policy. Experiments on synthetic discrete- and continuous-treatment simulators compare against random assignment (A/B-style) and UCB baselines, showing higher estimated EIG and lower MSE in estimating the max-reward vector m* (and often better parameter estimation and downstream action quality). The method is positioned as practical for real-world testing where randomization is costly, since it avoids allocating obviously suboptimal treatments while still exploring to gain information relevant to regret evaluation and future improvement.","Expected regret is defined as $R(a,c^*) = m(c^*) - \mathbb{E}[y\mid do(a),c^*]$, where $m(c^*)=\max_{a'}\mathbb{E}[y\mid do(a'),c^*]$. The experimental design selects a batch of actions $A$ to maximize mutual information between outcomes $y$ and the max-reward vector $m^*$: $I(A;C,C^*)=\mathbb{E}[\log p(y\mid m^*,do(A),C) - \log p(y\mid do(A),C)]$. They optimize an InfoNCE lower bound $\mathcal{L}(A,U;C,C^*,L)=\mathbb{E}\left[\log \frac{\exp(U(y,m_0^*))}{\frac{1}{L+1}\sum_{\ell=0}^L \exp(U(y,m_\ell^*))}\right]$ jointly over the critic $U$ and design/actions $A$; for discrete actions they use a Gumbel–Softmax policy $\pi_{d,i}=\frac{\exp((\log\alpha_{d,i}+g_{d,i})/\tau)}{\sum_j\exp((\log\alpha_{d,j}+g_{d,j})/\tau)}$.","In the discrete-treatment synthetic study (K=4 actions), the learned design achieves higher EIG and substantially lower error in estimating $m^*$ than UCB and random; e.g., Table 2 reports EIG $4.729\pm0.009$ and $\mathrm{MSE}(m^*)=0.594\pm0.025$ for the proposed method versus Random EIG $3.573\pm0.006$ with $\mathrm{MSE}(m^*)=1.953\pm0.070$ (UCB1 has $\mathrm{MSE}(m^*)=1.003\pm0.043$). In the continuous-treatment example with a 40-context batch (Table 1), the proposed method attains EIG $6.527\pm0.003$ and $\mathrm{MSE}(m^*)=0.0014\pm0.0001$, outperforming UCB variants (best UCB $\mathrm{MSE}(m^*)\approx0.0030$) and random baselines ($\mathrm{MSE}(m^*)\approx0.0024$–$0.0042$). Across additional batch sizes (D=20 and D=60), the proposed method similarly yields the lowest $\mathrm{MSE}(m^*)$ and improved EIG relative to baselines (Tables 5–6).","The authors note their current approach does not incorporate constraints in the experimental design (e.g., operational or safety constraints on which treatments can be tested). They also state that scaling to high-dimensional treatments and contexts remains to be explored, for both continuous and discrete settings.","Results are demonstrated only on synthetic simulators; there is no real-world case study to validate robustness under practical issues such as nonstationarity, interference between units, or imperfect compliance. The approach assumes access to a differentiable Bayesian simulator for interventional outcomes (or a relaxation for discrete actions), which may be difficult to specify or fit credibly in many applied causal settings (model misspecification could negate the claimed efficiency). The method optimizes an InfoNCE lower bound whose tightness depends on critic capacity and contrastive-sample settings, but the sensitivity of performance to these choices is not systematically characterized.","Future work is suggested in three directions: (i) extending the method to handle constraints in the experimental design, (ii) studying scalability to high-dimensional treatments and contexts for both continuous and discrete actions, and (iii) extending the framework from large-batch (offline) designs to sequential/adaptive experimentation.","A valuable extension would be evaluation under model misspecification and partial identification (e.g., sensitivity to unobserved confounding or incorrect causal assumptions), since the method’s utility hinges on the Bayesian interventional model. Another direction is to provide practical guidance/diagnostics for critic/bound quality (e.g., monitoring tightness, variance, and failure modes of InfoNCE-based EIG) and to release a reference implementation for reproducibility. Finally, empirical validation on real-world contextual decision problems (e.g., online platforms, healthcare) with constraints and delayed/biased feedback would clarify practical gains versus simpler bandit or off-policy evaluation approaches.",2207.05250v1,https://arxiv.org/pdf/2207.05250v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:14:27Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction|Other,Bayesian D-optimal|Other,"Variable/General (CT projection angles; experiments use 200 candidate angles, pilot 5 angles, then +35 sequentially selected)",Healthcare/medical|Manufacturing (general)|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),github.com/educating-dip/bayesian experimental design,"The paper proposes an adaptive Bayesian experimental design method for computed tomography (CT) angle selection that starts from a sparse pilot scan and then sequentially chooses additional scan angles. The key contribution is using the linearised Deep Image Prior (DIP) to build a data-dependent Gaussian prior covariance that incorporates information from pilot measurements while retaining conjugate linear-Gaussian tractability. Angle selection is performed greedily using two Bayesian utility criteria—expected information gain (EIG; a D-optimal log-determinant objective) and expected squared error (ESE; a trace/A-optimal-in-measurement-space objective)—with efficient Monte Carlo estimation via Matheron’s rule. On synthetic CT data with image-specific preferential edge directions, the linearised DIP design reduces the number of required scans by about 30% versus equidistant-angle baselines in the 10–20 angle regime, and improvements transfer to both DIP and classical TV-regularized reconstruction. The work argues adaptivity (dependence on measured values) is critical for outperforming standard equidistant acquisition in CT, and reports practical runtimes (minutes for a full design) enabled by the linearised model.","The CT measurement model is linear-Gaussian: $y = Ax + \varepsilon$, with $\varepsilon \sim \mathcal{N}(0,\sigma_y^2 I)$ and a Gaussian prior $x \sim \mathcal{N}(0,\Sigma_{xx})$, yielding posterior covariance $\Sigma_{x\mid y^{(t)}} = \Sigma_{xx} - \Sigma_{xx}A^{(t)\top}(\Sigma_{yy}^{(t)})^{-1}A^{(t)}\Sigma_{xx}$. The Bayesian design utilities for adding angle $\beta$ include EIG $=\log\det(\sigma_y^2 I + A_\beta \Sigma_{x\mid y^{(t)}} A_\beta^\top)+C$ (D-optimal) and ESE $=\mathrm{Tr}(A_\beta \Sigma_{x\mid y^{(t)}} A_\beta^\top)+C$ (A/trace in measurement space). The linearised DIP prior sets $x=J\theta$ with $\theta\sim\mathcal{N}(0,\Sigma_\theta)$ so $\Sigma_{xx}=J\Sigma_\theta J^\top$, where $J$ is the U-net Jacobian at the pilot-trained optimum; a neural g-prior uses $\Sigma_\theta=g\,s^{-1}I$ with $s$ based on the (diagonal) Fisher proxy.","On synthetic 128×128 CT images with preferential directions and 200 candidate angles, using a 5-angle equidistant pilot scan followed by 35 sequential selections, linearised DIP-based design improves reconstruction PSNR over equidistant-angle selection by >1.5 dB in the 10–15 total-angle regime (5% noise). In this early regime, the method achieves similar quality with roughly 30% fewer scanned angles than the equidistant baseline. Retraining the DIP/Jacobian every 5 acquired angles yields additional gains, reported as consistently >0.5 dB after 20 angles in their experiments. In a 10% noise setting, gains are smaller but still present per the reported curves, and ESE generally outperforms EIG except that EIG improves when combined with the neural g-prior.","The authors note that fully online adaptive CT design can be impractical due to added computation between scans, motivating their two-phase (pilot + adaptive) procedure rather than continuous online adaptation. They also state that model misspecification and hyperparameter overfitting can degrade measurement covariance estimates and thus harm EIG performance, and they observe overfitting issues for simpler priors (e.g., Matérn-1/2) in their experiments. Finally, they present results primarily on synthetic data and explicitly indicate the need to test on real measurements.","The empirical evaluation is limited to a specific synthetic rectangle dataset with engineered preferential directions, so it is unclear how robust the gains are on diverse anatomies/materials, 3D CT geometries, or different detector/physics models (e.g., Poisson noise, beam hardening). The design criterion optimizes linear-Gaussian utilities using an approximate (linearised) DIP prior; performance may depend strongly on the pilot-scan quality, Jacobian stability, and chosen retraining schedule, with no systematic sensitivity analysis reported. The greedy submodular selection guarantee applies to the linear design objective but does not directly guarantee end-to-end performance under non-linear reconstruction (DIP/TV), so the link between utility gains and reconstruction gains could break in other settings. No open-source package details or reproducibility instructions beyond the repository link are summarized in the paper excerpt (e.g., exact environment, scripts, and data generation seeds).",They state that future work will apply linearised DIP designs to real CT measurements. They also indicate interest in further investigating alternative methods for setting/regularizing model hyperparameters due to observed overfitting effects (discussed in the appendices).,"A valuable extension is to handle more realistic CT observation models (e.g., Poisson noise with log transforms, scatter/beam hardening) and study robustness under model mismatch. Extending the approach to 3D CT and cone-beam geometries, and to constraints such as limited-angle or hardware-imposed angular trajectories, would improve practical relevance. Developing a self-tuning, computationally bounded adaptive policy (e.g., stopping rules, adaptive K for Monte Carlo, or approximate updates) could further reduce inter-scan latency. Finally, benchmarking against modern learned scanning policies (e.g., reinforcement learning-based approaches) on standardized datasets and releasing a well-documented implementation would strengthen comparative evidence and adoption.",2207.05714v1,https://arxiv.org/pdf/2207.05714v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:15:10Z
TRUE,Optimal design|Other,Parameter estimation|Other,Other,Variable/General (examples include stepped-wedge cluster trials with treatment + 5 period indicators; geospatial example with 3 mean parameters),Healthcare/medical|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|C/C++,Package registry (CRAN/PyPI),https://cran.r-project.org/package=glmmrOptim,"The paper develops and evaluates combinatorial optimisation algorithms to compute exact (integer) c-optimal experimental designs for correlated observations modeled via generalized linear mixed models (GLMMs). It shows the c-optimality criterion g(d)=c^T M_d^{-1} c (with M_d based on GLS information under correlation) is a monotone supermodular set function, enabling the use of local search, greedy search, and reverse greedy search algorithms over discrete experimental units. The methods handle correlation both within and between experimental units, and include extensions to model-robust (prior-weighted) c-optimal designs over a class of candidate GLMMs. Empirical comparisons on cluster randomized trial and geospatial sampling examples find local search and reverse greedy yield near-identical best designs and consistently near-optimal variance, while greedy search performs worse; reverse greedy is typically faster and more reliable. The paper also compares against multiplicative (weight-based) approaches (Elfving/generalized Elfving via SOCP) and shows combinatorial designs are comparable or slightly better after rounding, especially at small sample sizes.","The design objective is c-optimality: g(d)=c^T M_d^{-1} c (infinite if M_d is not PSD), where under GLS for a GLMM the information matrix is M_d = X_d^T \Sigma_d^{-1} X_d (and for a nonlinear mean they also use a first-order derivative-based F matrix: M_d = F^T \Sigma^{-1} F). They derive a marginal information update with correlation via a Schur complement: M_d = M_{d'} + \delta M_{d',d''} where \delta M involves [X_2 - \Sigma_{12}^T \Sigma_1^{-1} X_1]^T S^{-1}[X_2 - \Sigma_{12}^T \Sigma_1^{-1} X_1], establishing monotone supermodularity. For non-Gaussian GLMMs they approximate \Sigma \approx W^{-1} + Z D Z^T (Breslow–Clayton MQL approximation) and discuss optional ‘attenuation’ of the linear predictor for improved mean approximation.","Across multiple Gaussian and non-Gaussian example settings with different covariance structures, the reverse greedy and local search algorithms produced best designs that were essentially indistinguishable; the worst local-search solutions were typically <1% above best for cross-sectional CRT examples but could be >10% worse for cohort CRT examples, indicating the need for multiple random restarts. Greedy search was consistently worse, with reported relative efficiencies often ~103–110% (and in some cohort cases much larger) versus the best design. For model-robust (prior-weighted) criteria over classes of models, reverse greedy and local search again achieved ~100% relative efficiency, while greedy was ~104% in the shown examples. In a comparison to multiplicative (SOCP-based weight) methods plus rounding for cluster-sequence experimental units, the best multiplicative-rounded designs were within about 2% variance of the best combinatorial designs, with combinatorial performing better particularly at smaller/odd sample sizes.","The authors note that combinatorial algorithms cannot guarantee finding the global minimum; local search converges only to a local optimum and greedy/reverse greedy lack applicable theoretical approximation guarantees in their setup (e.g., g(\emptyset)=\infty implies unbounded curvature so reverse greedy has no bound). They also highlight computational limitations: evaluating the information matrix requires estimating and inverting \Sigma, and for non-Gaussian models they rely on approximations (e.g., \Sigma \approx W^{-1}+ZDZ^T) rather than exact likelihood-based information. They further note practical implementability issues: some optimal designs can be impractical or unlikely to be implemented in real studies.","Performance is demonstrated on a limited set of stylized examples; broader benchmarking across more varied design regions, parameter settings, and stronger between-unit correlation patterns would better establish generality. The approach assumes covariance structure (and often parameters) are known/plausibly specified; misspecification of \Sigma (or link/mean) could materially change optimal designs, and robustness is only explored via a weighted average over a small discrete model set. The method focuses on c-optimality for a specified contrast vector c; practical studies often need simultaneous precision for multiple contrasts/endpoints, which may require compound/multiobjective criteria or constraints not fully explored here. While an R package is referenced, the paper does not state whether the exact scripts for reproducing all simulation tables/figures and Monte Carlo evaluation are archived with a fixed DOI or repository for full reproducibility.","They suggest further research on Bayesian optimal design: approximating Bayesian c-optimality (integrating c^T [M_d+V_0]^{-1} c over priors on parameters) via discretization/weights and assessing whether the simple combinatorial algorithms here can be a viable alternative to more advanced Bayesian design algorithms (e.g., approximate coordinate exchange). They also indicate more work is needed to evaluate the usefulness of these approximations compared with existing Bayesian optimal design approaches for nonlinear models.","Extend the framework to handle nuisance-parameter uncertainty more systematically (e.g., adaptive/sequential updating of designs as covariance/mean parameters are learned in a Phase I/II workflow). Develop constrained versions (e.g., balance constraints, minimum/maximum per cluster-period/location, logistical constraints) with proven approximation bounds or effective heuristics tailored to common trial/sampling restrictions. Provide dedicated methods for multivariate or high-dimensional outcomes (multiple c vectors; compound criteria) and for autocorrelated/non-exchangeable residual structures beyond the examples (e.g., spatio-temporal separable covariance). Add large-scale empirical validation on real datasets and publish a fully reproducible benchmark suite (data + code + fixed versions) to compare against alternative optimal design solvers and modern metaheuristics.",2207.09183v2,https://arxiv.org/pdf/2207.09183v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:15:51Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical,Simulation study,TRUE,R,Public repository (GitHub/GitLab),https://github.com/ptredondo/mxfar,"The paper proposes the mixed-effects functional-coefficient autoregressive (MXFAR) model for multivariate time series collected on multiple subjects in designed experiments, capturing nonlinear cross-channel dependence driven by an observed reference signal and accommodating between-subject heterogeneity via random effects. Estimation is done with a local-linear mixed-effects formulation (Henderson’s mixed model equations), with bandwidth and model/reference-signal selection guided by a multi-fold cross-validation accumulated prediction error (APE) criterion; a bootstrap test is also developed to assess nonlinearity versus a mixed-effects VAR null. A new nonlinear frequency-domain directional connectivity measure, functional partial directed coherence (fPDC), is defined by Fourier transforming the MXFAR coefficient functions and normalizing analogously to classical PDC, reducing to standard PDC when coefficients are constant in the reference signal. The framework is applied to an EEG visual-cognitive task with ADHD vs control children, using sliding windows and selecting Cz (lag 6) as a reference signal and MXFAR order p=4, revealing group differences in nonlinear connectivity not captured by linear PDC or spectral conditional Geweke–Granger causality. An R package (mxfar) is provided to implement the methodology and reproduce key EEG analysis results.","The MXFAR(p) model is $\mathbf Y_t^{(n)} = f^{(n)}(U_t^{(n)})\mathbf X_t^{(n)} + \boldsymbol\varepsilon_t^{(n)}$, where $\mathbf X_t^{(n)}=(\mathbf Y_{t-1}^{(n)\prime},\ldots,\mathbf Y_{t-p}^{(n)\prime})^\prime$ and the entries $f^{(n)}_{j,g{:}\ell}(U_t)$ are smooth coefficient functions of the reference signal. Locally at $u$, coefficients are approximated as $f^{(n)}_{j,g{:}\ell}(U_t)\approx (\alpha_{j,g{:}\ell}+a^{(n)}_{j,g{:}\ell})+(\beta_{j,g{:}\ell}+b^{(n)}_{j,g{:}\ell})(U_t-u)$ and estimated via mixed-model equations. The fPDC is defined using $f_{j,g}(\omega,u)=\mathbf 1_{j=g}-\sum_{\ell=1}^p f_{j,g{:}\ell}(u)e^{-i2\pi\omega\ell}$ and $\mathrm{fPDC}_{j,g}(\omega,u)= f_{j,g}(\omega,u) / \sqrt{\sum_{j'=1}^k |f_{j',g}(\omega,u)|^2}$.","In the EEG application (104 participants; 51 ADHD, 53 controls; 6 modeled channels with Cz as reference), model selection via APE chose an MXFAR model with lag order $p=4$ and reference signal Cz at delay $d=6$, fitted over 17 overlapping 5-second windows. The MXFAR–fPDC networks showed amplitude-dependent (reference-signal-dependent) nonlinear connectivity patterns and clearer group differences than linear VAR-based PDC and spectral conditional Geweke–Granger causality, including asymmetry and discontinuities in low-frequency prefrontal–occipital loops for ADHD and stronger symmetry/stability for controls. At high frequencies, the ADHD group exhibited more approximately linear connectivity while controls showed reference-amplitude-driven nonlinear patterns, including temporal-channel involvement consistent with memory-related processing. The paper also reports computational scaling considerations and provides indicative run-time curves (e.g., estimating MXFAR over $M=50$ discretized reference-signal points) showing increasing cost with dimension $k$, lag $p$, and subjects $N$.","The authors note that assuming white-noise errors may be inadequate; autocorrelated errors would require a functional-coefficient ARMA-type extension (FARMA), which they state is outside the paper’s scope. They also acknowledge computational limitations as model fitting scales poorly with the number of channels, leading them to restrict the EEG application to six channels driven by a single reference channel. They caution that their connectivity interpretations are exploratory/speculative and require verification by neuroscientists, and they note that prominence of links does not imply statistical significance, with fPDC significance testing requiring further theoretical work.","Although motivated by “designed experiments,” the paper does not contribute to DOE design construction (randomization, blocking, optimal run allocation, factorial/response-surface planning); the experimental component is treated as an observational/analysis setting with replicated subjects. The fPDC depends on local stationarity/second-order stationarity assumptions (Assumption 1) and independence conditions that can be fragile for EEG (nonstationarity, artifacts, and temporal dependence), and robustness to violations is not fully characterized. The group-level aggregation and thresholding choices (e.g., 0.8-quantile, 10% region rule, 50% window consistency) introduce subjective tuning that may affect network conclusions, yet sensitivity analyses for these thresholds are limited in the main text. Comparisons are restricted to linear PDC and spectral cGGC; other nonlinear or state-space connectivity methods (e.g., time-varying VAR, nonlinear Granger variants, transfer entropy, or dynamic graphical models) are not benchmarked.","They propose developing a functional-coefficient autoregressive moving average (FARMA) extension to address autocorrelated errors. They also indicate that statistical significance testing for fPDC links needs further theoretical development, and they outline a possible direction via adapting frequency-specific thresholds akin to those used for PDC. More broadly, they suggest extending the general MXFAR framework to incorporate various covariates/factors and applying it to other domains beyond EEG (e.g., macroeconomic GDP interactions or stock markets).","Develop self-starting/online and time-varying versions of MXFAR–fPDC to better handle nonstationary EEG dynamics without relying on sliding-window segmentation and ad hoc discretization of the reference-signal support. Provide rigorous uncertainty quantification for fPDC surfaces (simultaneous bands or multiplicity-controlled edge selection) via bootstrap/Bayesian methods and evaluate how edge-selection stability changes with threshold rules. Extend the model to multivariate reference signals (or learned low-dimensional latent drivers) with regularization to avoid the curse of dimensionality, enabling richer experimental covariates and stimulus-related drivers in designed studies. Expand empirical benchmarking to include modern nonlinear connectivity estimators and evaluate performance under common EEG complications (autocorrelation in errors, heavy tails, artifacts, and cross-subject parameter drift).",2208.00292v3,https://arxiv.org/pdf/2208.00292v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:17:09Z
TRUE,Optimal design|Sequential/adaptive|Other,Model discrimination|Parameter estimation|Cost reduction|Other,Not applicable,Variable/General (examples include n=3 nodes; simulations use n=10 with |supp(f)|≤4),Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops algebraic-geometry-based theory and algorithms for designing experiments (choosing input perturbation sets V over finite fields) that guarantee a unique minimal wiring diagram (unique minset) for network inference, independent of the unknown experimental outputs. Using polynomial dynamical systems and Stanley–Reisner theory, it derives necessary and sufficient conditions on V for uniqueness, including a linear feasibility test (Theorem 3.1) based on monomials encoding coordinate differences between pairs of input points. It connects uniqueness of minsets/wiring diagrams to uniqueness of reduced Gröbner bases of the ideal of points I(V), providing conditions such as diagonal-freeness as necessary for unique reduced Gröbner bases. The approach is demonstrated on an EGFR-mediated tumor-suppression network example showing how additional carefully chosen experiments can eliminate ambiguous wiring diagrams. Computational experiments (Boolean functions on F2^10 with 20 sampled input points) support a heuristic that smaller average Hamming distance among design points tends to yield fewer candidate minsets, and a “small-distance” sampling scheme outperforms random sampling in reducing ambiguity.","Design points are inputs V\subset \mathbb{F}_p^n and pairwise differences are encoded by square-free monomials m(s,s')=\prod_{s_i\neq s'_i} x_i, forming a multiset M of all pairwise monomials and its multivariate subset. Theorem 3.1 tests whether there exists an output assignment T yielding multiple minsets by checking consistency of linear constraints ta\neq tb together with equalities for outputs corresponding to all single-variable divisors of a multivariate monomial (system (3.2)). Uniqueness links to Gröbner theory via ideals of points I(V)\subset \mathbb{F}_p[x_1,\ldots,x_n] and conditions for a unique reduced Gröbner basis; uniqueness of normal form across Gröbner bases implies a unique minset (Theorem 3.4).","Theorem 3.1 gives a computationally simple necessary-and-sufficient condition (via sparse linear feasibility) to decide whether an input set V can ever lead to multiple minsets for some outputs, enabling identification of V that guarantee uniqueness for all outputs. Theorem 3.12 shows that if V contains a “diagonal” point (differs from all others in at least two coordinates), then some output assignment produces multiple minsets; hence diagonal-freeness is necessary for I(V) to have a unique reduced Gröbner basis (Corollary 3.13). In simulations (10,000 trials per function) with n=10, m=20 and fanout-free Boolean functions with support size ≤4, designs generated by a small-Hamming-distance scheme exhibited smaller internal distance d(V) and consistently fewer minsets than randomly sampled designs. A worked EGFR network example illustrates how adding specific extra experiments can reduce three candidate minsets to two, or to a unique wiring diagram depending on the chosen additional input-output pair.",None stated.,"The proposed guarantees are primarily for uniqueness of minimal wiring diagrams/minsets under the polynomial dynamical system framework over finite fields; real experimental data may violate assumptions such as determinism, noise-free measurements, and exact finite-field discretization. The computational results emphasize heuristics based on Hamming distance but do not provide formal optimality guarantees (e.g., global minimization of number of minsets) or robustness analysis under noisy/uncertain outputs. Software, implementation details, and computational complexity in realistic high-dimensional biological settings are not fully specified, which may limit immediate reproducibility and practical uptake.","The authors propose extending the results to signed minimal wiring diagrams and addressing the question of existence in that setting. They also highlight as an open problem the need for a complete geometric/combinatorial characterization of input sets V\subset \mathbb{F}_p^n for which I(V) has a unique reduced Gröbner basis, motivated by its connection to unique minsets and unique interpolating normal forms.","Developing noise-robust or probabilistic versions of the uniqueness guarantees (e.g., accounting for measurement error, stochastic dynamics, or uncertain discretization) would improve applicability to experimental biology. Extending the design criteria to multivariate outputs (simultaneously designing for multiple nodes) and to sequential/active-learning experiment selection that updates V based on observed outcomes could further reduce experimental burden. Providing open-source implementations (e.g., in SageMath/Macaulay2/Singular) and benchmarking against alternative network-inference DOE strategies would strengthen reproducibility and practical guidance.",2208.02726v1,https://arxiv.org/pdf/2208.02726v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:17:39Z
TRUE,Response surface|Optimal design|Factorial (fractional)|Split-plot|Other,Parameter estimation|Model discrimination|Robustness|Other,D-optimal|A-optimal|Other,Variable/General (examples include 5 factors; 3 factors in a blocked case study),Food/agriculture|Theoretical/simulation only|Other,Other,TRUE,R,Supplementary material (Journal/Publisher),NA,"The paper develops compound optimality criteria for response surface (polynomial regression) designs that explicitly account for possible model misspecification (“model contamination”) via additional polynomial terms nested within an extended model. The compound criterion combines (i) pure-error based DP/LP inference criteria for the primary model, (ii) a newly developed lack-of-fit (LoF) criterion targeting detectability of contamination terms via a posterior credible-region volume, and (iii) newly developed MSE-based criteria that jointly address variance and bias of primary-model parameter estimators under contamination. The framework is extended to blocked experiments by incorporating block effects and using projection matrices (e.g., Q) to adjust information matrices and LoF/bias components. Nearly optimal exact designs are constructed using a point-exchange algorithm; the MSE(D) component is evaluated with Monte Carlo integration over a prior on contamination coefficients. Theoretical development is illustrated with a 5-factor 3-level example (n=40) and a real blocked case study in animal supplement dosing (3 factors, 2 blocks, n=36), yielding practical guidance on weight selection and trade-offs among competing objectives.","Primary model: $Y=X\beta+\varepsilon$ with pure-error variance estimated from the full treatment model. Pure-error DP-optimality minimizes $F_{p,d;1-\alpha_{DP}}\,\lvert(X'X)^{-1}\rvert$ (and LP-optimality minimizes $F_{1,d;1-\alpha_{LP}}\,\mathrm{tr}\{W(X'X)^{-1}\}$). Model contamination uses an extended model $Y=X_p\beta_p+X_q\beta_q+\varepsilon$; LoF DP-criterion minimizes $\lvert L+I_q/\tau^2\rvert^{-1/q}F_{q,d;1-\alpha_{LoF}}$ where $L=X_q'X_q-X_q'X_p(X_p'X_p)^{-1}X_p'X_q$. The MSE(D) component is based on $\mathrm{MSE}(\hat\beta_p)=\sigma^2(X_p'X_p)^{-1}+A\beta_q\beta_q'A'$ with $A=(X_p'X_p)^{-1}X_p'X_q$, leading to criterion (9) and the combined compound criterion as a weighted product of efficiencies (eq. (3)); blocked-experiment versions replace $X_p'X_p$ by $X_p'QX_p$ and use $\tilde L$ and $d_B$ (e.g., eqs. (12), (18)).","In the 5-factor, 3-level example (n=40; primary second-order model with p=21; contamination with q=30 third-order terms), the authors show strong trade-offs: LoF(DP)-optimal designs can have low DP-efficiency (around 40%), while DP-optimal designs can lose roughly half their LoF(DP)-efficiency when contamination scale increases (from $\tau^2=1/q$ to $\tau^2=1$). For $\tau^2=1/q$, several compound-weight choices yield designs with DP-efficiency near 100% and LoF(DP)-efficiency around 94%, indicating feasible compromises when contamination is expected to be small. In the blocked real case study (3 factors, n=36, b=2 blocks; p=9; q=10), compound designs with two center points per block retained high individual efficiencies (roughly 93–101% under the constrained search, as reported) and relative efficiency losses due to forced center points were small (≤ about 5.05% depending on $\tau^2$). Computation time for the case study designs was reported as typically 13–15 hours (sometimes up to 20–24 hours) using the proposed search and Monte Carlo evaluation.","The authors note that evaluating the MSE(D) component via Monte Carlo is computationally expensive and can be time-consuming (hours per design in the case study). They suggest that when computation time is prohibitive, a point-prior approximation for contamination coefficients can replace Monte Carlo with only small efficiency losses. They also acknowledge that the point-exchange algorithm can produce nearly optimal designs and may miss the exact optimum in some instances.","The approach requires specifying a contamination structure (choice of potential terms $X_q$) and a contamination scale parameter $\tau^2$, which may be difficult to elicit reliably and can materially affect the resulting design. Comparisons are largely internal (efficiency trade-offs under the proposed criteria) and do not comprehensively benchmark against standard RSM designs (e.g., CCD/Box–Behnken) across many scenarios, so practical gains versus common defaults may be unclear. The methodology assumes independent homoscedastic normal errors and relies on polynomial regression structure; robustness to non-normality, heteroscedasticity, or correlated responses is not addressed. Implementation depends on exchange algorithms and Monte Carlo approximations, which may be sensitive to candidate sets, starting points, and tuning choices; reproducibility may require careful reporting and software support.","The authors suggest extending the robust pure-error compound-criterion approach to other objectives (e.g., prediction-focused I-type criteria) and to more complex experimental-unit and treatment structures. They specifically mention interest in expanding the MSE-criteria to more complex structures such as crossed structures, experiments on networks, and complex interventions including sequential designs. They also discuss multi-objective optimization via Pareto fronts as an alternative to weighted-compound criteria, pointing to related algorithmic developments in the literature.","Developing guidance or elicitation tools for choosing $X_q$ and calibrating $\tau^2$ (e.g., sensitivity analysis workflows or Bayesian hierarchical priors over contamination) would improve usability. Creating faster approximations (variance-reduction, quasi-Monte Carlo, analytic approximations) or surrogate-based search could reduce the heavy computation in MSE(D)-based criteria. Extending the criteria to handle autocorrelation/longitudinal outcomes, heteroscedasticity, and multivariate responses jointly (rather than separately) would broaden applicability in industrial and biological settings. Providing an open-source R package with documented examples and reproducible benchmarks against standard RSM designs would facilitate adoption.",2208.05366v2,https://arxiv.org/pdf/2208.05366v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:18:25Z
TRUE,Computer experiment|Optimal design|Mixture|Other,Prediction|Optimization|Parameter estimation|Robustness|Other,Space-filling|Minimax/Maximin|Other,"14 factors total in main study: 3 unconstrained continuous (z1,z2,z3) + 10 mixture components with sum-to-1 constraint (x1..x10) + 1 categorical/binary factor (z4 data type).",Network/cybersecurity|Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Do-AIQ, a design-of-experiments framework for systematic quality evaluation of AI algorithms, demonstrated on a mislabel detection (MLD) algorithm under data poisoning. The DOE includes hyperparameters (loss weight ratio and deviation threshold), data quality factors (mislabel rate and class-imbalance proportions on a simplex), and a categorical data-type factor (MNIST vs. FashionMNIST), with responses being detection accuracy and classification (prediction) accuracy. Methodologically, it develops an efficient space-filling design for a high-dimensional constrained (mixture/simplex) space using a criterion related to maximum projection/modified maximin behavior and an optimization algorithm combining coordinate exchange/descent, then crosses this with a Latin hypercube for unconstrained factors and a cross-array for the categorical factor. An additive Gaussian process surrogate model is fit to emulate performance over the mixed continuous/categorical factor space and to quantify uncertainty. Numerical experiments (2,000 design settings with 5 replicates each; 10,000 runs) show mislabel proportion strongly degrades detection accuracy, class imbalance has a smaller negative effect, and data type affects prediction accuracy more than detection accuracy; AGP yields far lower test MSE than linear regression for both responses.","The constrained space-filling design for class proportions $x_1,\dots,x_m$ (with $\sum_{l=1}^m x_l=1$ and $0\le x_l\le 1$) is obtained by minimizing a product-distance criterion: $\min_X \sum_{i<j} \big[\prod_{l=1}^m (x_{il}-x_{jl})^2\big]^{-1}$ subject to the simplex constraints. They relate this to a modified maximin / maximum projection criterion $f(X\mid\theta)=\sum_{i<j}\{1/(\sum_{l=1}^m \theta_l(x_{il}-x_{jl})^2)\}^{p/2}$ (with $p=2m$) and show the adopted criterion arises in expectation over $\theta$. The surrogate is an additive Gaussian process $\tilde y(\tilde w)=\mu+\sum_{h=1}^{\tilde q} G_h(\tilde z_h,\tilde x)$ with covariance components $\Sigma_h=\tau^2\psi_h(\tilde x_i,\tilde x_j)\,\phi_h(\tilde z_{hi},\tilde z_{hj})$ and GP prediction mean/variance $\mu_* = \Sigma_{*y}\Sigma_{yy}^{-1}y$, $\sigma_*^2=\Sigma_{**}-\Sigma_{*y}\Sigma_{yy}^{-1}\Sigma_{y*}$.","Design-validation on the simplex with $m=10$ compares the proposed constrained space-filling design to Kennard–Stone: for Coverage (PM1), Proposed beats K–S at N=50 (0.1509 vs 0.0287), 100 (0.2149 vs 0.0440), 150 (0.1852 vs 0.1579), and 200 (0.2099 vs 0.2022); for Maxmin (PM2) Proposed is better for larger N (e.g., N=100: 0.4490 vs 0.3403; N=150: 0.3926 vs 0.3768; N=200: 0.3795 vs 0.3672). In the main study, factor ranges include $z_1\in[1/500,500]$, $z_2\in[1,3]$, and $z_3\in[10\%,50\%]$ with datasets MNIST and FashionMNIST; total design settings are 2,000 with 5 replicates each (10,000 total runs). AGP surrogate modeling substantially outperforms linear regression on held-out test data: for prediction accuracy $y_1$, MSE is $8.7147\times10^{-5}$ (AGP) vs 0.0028 (linear regression); for detection accuracy $y_2$, MSE is $6.1186\times10^{-5}$ (AGP) vs 0.0006. Empirically, detection accuracy decreases roughly linearly as mislabel proportion $z_3$ increases, while data type has little effect on detection accuracy but a pronounced effect on prediction accuracy.","The authors note that the framework is demonstrated using a particular mislabel detection algorithm, although Do-AIQ is intended to extend to other AI algorithms. They also state that the study uses about ten classes; for datasets with many classes (e.g., CIFAR100 with 100 classes), the proposed design construction algorithm may face computational difficulties. They further point out that sequential design may be needed to address sensitivity concerns and limits on feasible numbers of design runs.","The empirical evaluation uses only two image datasets (MNIST and FashionMNIST) and one MLD method structure (Pulastya et al., 2021), so conclusions about robustness to “data type” may not generalize to other modalities or substantially different datasets. The DOE is largely framed around space-filling exploration; it does not fully address how to choose/weight the design region when practical operating conditions are non-uniform or when rare but high-risk regions matter most. Implementation details (software, training protocol, compute budget) and reproducibility artifacts are not provided, which makes it hard to replicate the 10,000-run experiment and verify sensitivity to random seeds/optimization settings.","They propose sequential design based on additive Gaussian processes to address sensitivity and limitations on the number of design runs. They also suggest extending Do-AIQ to multiple datasets with different numbers of classes. Finally, they identify ensuring fairness in the design of experiments as an additional future direction.","Develop a fully self-starting/online Do-AIQ workflow that updates the surrogate and adaptively allocates additional runs to regions with high uncertainty or high risk (multi-objective active learning for detection vs prediction accuracy). Extend the constrained space-filling design and AGP surrogate to additional realistic data issues (correlated labels, structured noise, covariate shift, temporal dependence) and to higher-dimensional categorical settings (many datasets/types) without binary expansion blow-up. Provide open-source implementations and standardized benchmarks to enable reproducible comparison across AI assurance studies.",2208.09953v1,https://arxiv.org/pdf/2208.09953v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:19:14Z
FALSE,Other,Prediction|Other,Not applicable,Not specified,Healthcare/medical,Simulation study|Other,TRUE,Python|MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/GuangyaoDou/EEG4Students,"The paper presents EEG4Students, a practical protocol and workflow for collecting scalp EEG data (in-person and remotely during COVID-19) using affordable consumer-grade devices (notably Muse headsets) and analyzing the resulting data with standard machine-learning classifiers. The experimental task paradigm replicates the Think–Count–Recall (TCR) setup, labeling five cognitive tasks (Think, Count, Recall, Breathe, Draw) across multiple sessions per subject. A preprocessing and cleaning pipeline is described (MATLAB preprocessing to flag noise, then Python notebooks for cleaning and model training), followed by time-wise cross-validation to evaluate classification performance. Empirically, Random Forest and RBF-kernel SVM achieved the best average accuracies on the retained subjects, while KNN was fastest; the work emphasizes feasibility and guidance for non-expert remote EEG data collection rather than proposing new DOE methodology.",Not applicable,"Across the retained dataset (12 subjects after denoising/exclusions), Random Forest achieved the highest reported average accuracy (0.63; average runtime 37.1 s) and RBF SVM was second (0.61; 15.2 s). Other reported averages include GradientBoost 0.58 (119.0 s) and KNN 0.55 (2.3 s). In the remote-collection attempt, 6 participants were scheduled, 3 attended, and only 1 yielded a usable complete dataset after denoising. Data cleaning excluded the first 30% (18 seconds) of each 1-minute task segment and removed anomaly runs where electrode values were constant for 1.4 seconds; subjects/sessions/folds losing >65% of data were excluded.","The authors note that remote/virtual data collection was substantially less efficient than in-person sessions due to communication barriers and idiosyncratic device/software issues that are hard to troubleshoot remotely. They highlight headset/software incompatibilities (e.g., newer Muse models not working with MuseLab, requiring an alternate app and protocol) and that multiple remote participants failed to submit usable data because of noise/contact problems. They also acknowledge relatively low classification accuracy typical of EEG due to low SNR, outliers, and mind-wandering, and state they did not implement deep learning models because the dataset was small.","The study is not framed as a formal designed experiment (no randomization/blocking/DOE optimality discussion), so causal claims about protocol components (e.g., rubber bands, coaching style, app choice) are not isolated. The evaluation is limited to a small, convenience sample with substantial post-hoc exclusion based on data-loss thresholds, which can bias performance estimates toward easier/cleaner subjects and sessions. Comparisons focus on a set of standard classifiers and a single paradigm (TCR tasks) with limited exploration of feature engineering, subject-independent generalization, or statistical uncertainty (e.g., confidence intervals) around accuracies. Reproducibility may depend on hardware/firmware/app versions and undocumented preprocessing hyperparameters beyond the described heuristics.","They propose improving user/participant training and exploring strategies to make virtual data collection more efficient and robust. They plan to recruit more subjects and EEG coaches to expand the dataset, including developing a study/work community for students to share troubleshooting knowledge for non-clinical devices. They also state they will implement deep learning models (e.g., CNN/RNN/LSTM) once more data are collected.","A useful extension would be to formalize protocol evaluation using a factorial or randomized study (e.g., in-person vs. remote, MuseLab vs. Mind Monitor, coaching intensity, headset stabilization) to quantify which design choices most improve signal quality and downstream accuracy. Reporting subject-independent and cross-device generalization (train on one cohort/device/app, test on another) would strengthen claims about broad usability. Adding robust/statistical performance reporting (confidence intervals, paired tests, calibration metrics, learning curves vs. data retained) would improve interpretability under heavy missingness/exclusion. Packaging the pipeline into a reproducible software release (pinned environments/containers) and documenting all preprocessing thresholds would reduce implementation variability.",2208.11743v1,https://arxiv.org/pdf/2208.11743v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:19:45Z
TRUE,Split-plot|Optimal design|Other,Parameter estimation|Robustness|Cost reduction,Minimax/Maximin|Other,Variable/General (subjects matched/blocked using d covariates; two-arm treatment vs control),Healthcare/medical|Theoretical/simulation only,Simulation study|Other,TRUE,R,Not provided,NA,"The paper studies experimental design choices for two-arm randomized controlled trials with an incidence (binary) outcome under a nonparametric response model, focusing on estimation of the sample average treatment effect (risk difference) via the difference-in-means estimator. It evaluates restricted randomization designs (balanced complete randomization, blocking, and a priori pairwise matching) through the mean squared error (MSE) of the estimator, showing the design enters only through the assignment covariance matrix term $v^\top\Sigma v$. The authors prove that optimal a priori pairwise matching (constructed by sorting subjects by $v_i=p_{T,i}+p_{C,i}$ and pairing adjacent subjects) is optimal among all block designs and is also minimax over an adversarial class of sorted $v$ vectors. They support the theory with large Monte Carlo simulations under logistic response models and with a parametric-bootstrap study using clinical trial covariates, showing meaningful MSE reductions (and potential sample-size savings) when matching is informative. Practical guidance is given for implementing approximate matching when $v$ is unknown, including optimal nonbipartite matching using Mahalanobis distance.","The estimand is the sample average treatment effect (risk difference) $\tau=\frac{1}{2n}(p_T-p_C)^\top\mathbf{1}$ and the estimator is the difference-in-means $\hat\tau=\frac{1}{n}W^\top Y$. The MSE equals the variance and is $\mathrm{MSE}(\hat\tau)=\frac{1}{4n^2}\big(v^\top\Sigma v+2(p_T^\top(1-p_T)+p_C^\top(1-p_C))\big)$ where $v=p_T+p_C$ and $\Sigma=\mathrm{Var}(W)$ depends on the restricted randomization design. For block designs, off-diagonal assignment covariances within a block of size $n_b$ are $-1/(n_b-1)$ (with BCRD as one block and pair matching as $n$ blocks of size 2, giving within-pair covariance $-1$). The minimax result is stated as $\max_{v\in\mathcal V}\mathrm{MSE}(\hat\tau_{PM})=\min_{W\in\mathcal W}\max_{v\in\mathcal V}\mathrm{MSE}(\hat\tau_W)$ for $\mathcal V=\{v:0\le v_1\le\cdots\le v_{2n}\le 2\}$.","Theorem 3.1 shows that (with optimal ordering by $v_i$ and even block sizes) pairwise matching (PM; $n$ blocks of size 2) yields strictly lower MSE for $\hat\tau$ than any coarser block design, including balanced complete randomization (BCRD). Theorem 3.2 establishes PM as minimax over an adversarial class of sorted $v$ vectors, i.e., it minimizes worst-case MSE among all designs satisfying balance and marginal $P(W_i=\pm1)=1/2$. A robustness result (Remark 3.1) shows that if matching is effectively random, PM’s MSE equals BCRD’s; conversely, sufficiently poor/adversarial matching can make PM worse than BCRD (Remark 3.2 gives an explicit inequality condition). Simulations under logistic models and a parametric-bootstrap clinical-trial study show PM and blocking can substantially reduce MSE versus BCRD when covariates are informative; in the clinical bootstrap, the efficiency ratio (BCRD MSE / PM MSE) at sample size about 200 is reported as approximately 2 for low-dimensional matching, implying roughly 50% sample-size savings in that setting.","The authors note that their main theoretical results assume subjects can be sorted by the unknown $v_i=p_{T,i}+p_{C,i}$ to create the optimal match structure, which is infeasible in practice when $d>1$ without strong modeling assumptions; their practical approach relies on approximate matching (e.g., Mahalanobis distance) instead. They also explicitly restrict their theoretical analysis to the difference-in-means estimator for the risk difference (SATE) and do not provide theory for other estimands/estimators such as (log) odds ratios or covariate-adjusted logistic regression coefficients. They mention that getting matches “wrong” can lead to performance worse than BCRD and that a risk measure to decide when to revert to BCRD is not yet developed. In simulations with more covariates (e.g., $d=5$ in the clinical bootstrap), PM can lose its advantage when many covariates are uninformative, producing nearly random matches.","The optimality/minimax results hinge on assumptions of independent Bernoulli outcomes conditional on covariates and assignment and on a non-sequential (one-shot) allocation; many incidence endpoints in practice exhibit clustering, overdispersion, site effects, or time trends that violate conditional independence. The minimax class $\mathcal V$ requires $v$ to be sorted in subject index order, which effectively presumes access to (or perfect approximation of) the latent ordering; performance under realistic imperfect matching is studied by simulation but lacks general theoretical guarantees. Comparisons focus on a limited set of restricted randomization schemes (BCRD, blocking, PM) and do not benchmark against rerandomization, biased-coin/covariate-adaptive randomization, or modern optimization-based assignment rules that might perform well for binary outcomes. Practical implementation details for Phase I estimation of distances/standardization (e.g., robust Mahalanobis under mixed covariate types, missing data) are not fully addressed, and no reproducible code is provided to validate the simulation/clinical bootstrap pipeline.","The authors propose extending the theoretical analysis beyond the risk-difference difference-in-means estimator to other common estimators/estimands for binary outcomes, including risk ratio, odds ratio, log-odds ratio, and covariate-adjusted estimators (e.g., the logistic regression treatment coefficient). They suggest investigating paired estimators tailored to pair-matched designs and developing theory analogous to their main theorems for these settings. They also state a desire to establish theory for $d>1$ when matching is performed approximately in practice via optimal nonbipartite matching with Mahalanobis distance, including developing a measure of ‘risk’ to decide when to revert to BCRD when matching quality is poor. Finally, they suggest exploring sequential allocation settings, including on-the-fly pairing methods, since many clinical trials are sequential.","Developing finite-sample guarantees (or bounds) for MSE/ARL-type criteria under approximate matching quality measures (e.g., balance metrics, prognostic score distance) would bridge the gap between the ideal $v$-sorted theory and practice. Extending the framework to clustered/stratified trials (multi-center), correlated binary outcomes, and covariate-dependent missingness would improve applicability to real incidence endpoints. It would also be valuable to compare PM against rerandomization and covariate-adaptive randomization schemes using the same objective (MSE of a prespecified estimand) and to study tradeoffs under model misspecification of the outcome mechanism. Providing open-source, audited software to generate assignments (PM/BL/BCRD) and reproduce the simulations/bootstraps would facilitate adoption and independent verification. Lastly, investigating Bayesian or decision-theoretic formulations that incorporate uncertainty about prognostic relevance of covariates (to avoid degradation when many covariates are uninformative) could improve robustness in high-dimensional matching.",2209.00490v1,https://arxiv.org/pdf/2209.00490v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:20:33Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,R,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher)|Public repository (GitHub/GitLab)|Not provided|In text/Appendix|Personal website|Upon request,https://doi.org/10.6084/m9.figshare.20754028.v1|https://www.cancer.gov/tcga|https://dreamchallenges.org/,"This paper investigates why newly proposed data analysis methods often look better in their introductory papers than in later comparison studies. The authors propose and run a systematic “cross-design validation of methods” experiment: for two tasks (multi-omic cancer subtyping and differential gene expression), they reproduce each method’s original evaluation and then re-evaluate each method under the other paper’s study design (datasets/simulation settings, competitors, and evaluation criteria). Across four methods (PINSPlus, NEMO, SFMEB, MBCdeg), three perform worse under the crossed design, with the largest driver being differences in datasets (including different preprocessing for the TCGA cancer subtyping datasets) rather than competitors or metrics. The work also highlights reproducibility issues (code does not always reproduce exact published figures) and degrees of freedom in benchmarking (parameter settings, preprocessing, metrics, aggregation). The authors discuss mechanisms behind optimistic original evaluations (design overfitting to the method, method overfitting to design, expertise differences, and field-of-application mismatch) and recommend open code/data and more robust, transparent benchmarking practices.",Not applicable,"In the cross-design validation, 3 of 4 methods (PINSPlus, SFMEB, MBCdeg) showed worse relative performance when evaluated under the other paper’s design, while NEMO was comparatively robust across designs. For cancer subtyping, PINSPlus appeared clearly superior under its original design (e.g., many significant survival p-values) but did not outperform competitors under Rappoport & Shamir’s design; differences were largely attributable to dataset/preprocessing differences even on overlapping cancer types. For differential expression, SFMEB and MBCdeg2 both showed poorer relative ranks when moved to the other simulation framework, with performance differences driven mainly by the simulated data-generating setups rather than the set of competitors. The study also found reproducibility gaps: PINSPlus results were almost fully reproducible; R19 results could not be exactly reproduced (likely due to permutation tests and code/version differences), and Z21 used an AUC computation that could yield 1−AUC for some repeats due to a package default.","The authors note the experiment is small-scale (only four methods and one crossed design per task), so results cannot be generalized to the methods’ overall quality or to methodological research broadly. They also state they include only a subset of results from each original paper (main-paper figures/tables, limited settings), giving an incomplete picture of each method’s strengths/weaknesses. They further acknowledge outcomes may depend on their own expertise and on which alternative design is used for re-evaluation (a larger K×K set of designs could yield different conclusions).","The work frames “study design” broadly (datasets, competitors, metrics) but does not formalize a statistical model for separating design effects from implementation/parameter-tuning effects, so attribution (e.g., to datasets) remains largely empirical. The cross-design re-implementations necessarily involve subjective choices (e.g., manual cluster-number selection, parameter carryover across simulation frameworks) that could systematically disadvantage certain methods, and the sensitivity analyses are limited. Because only two tasks in biostatistics/bioinformatics are examined, the findings may not transfer to other domains where data generation, evaluation metrics, and tooling differ substantially. Finally, code availability is treated as a selection criterion, which could introduce selection bias toward papers with better engineering practices or particular research communities.",None stated,"A natural extension is to scale cross-design validation to many more method pairs and tasks, producing a larger “design transfer” matrix (K×K) to quantify typical optimism/transfer degradation. Developing standardized, preregistered benchmarking templates (including fixed preprocessing, parameter-tuning budgets, and reporting checklists) would help reduce degrees of freedom and make cross-design comparisons more interpretable. More systematic sensitivity analyses (e.g., parameter sweeps under a neutral tuning protocol and multiple alternative preprocessing pipelines) could disentangle dataset effects from tuning/expertise effects. Providing reusable software tooling to automate cross-design validation (e.g., containers, workflow managers, and benchmark harnesses) would also improve reproducibility and lower the cost of neutral evaluations.",2209.01885v1,https://arxiv.org/pdf/2209.01885v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:21:07Z
TRUE,Other,Other,Not applicable,Variable/General,Other|Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This position paper discusses evaluation and experimental design challenges for Visual Text Analytics (VTA) from an interdisciplinary NLP and visualization perspective. Rather than proposing a classical DOE (factorial/RSM/optimal) design, it outlines conceptual dimensions that should structure human-centered experimental design for VTA evaluations: target audience (user groups), tasks, linguistic scale/granularity, and underlying NLP models/representations. It identifies four major challenge groups impacting evaluation design—data ambiguity (lack of single ground truth), experimental design, user trust, and “big picture” issues like guidelines, repositories of study designs, and reproducibility/long-term adoption. The paper proposes a workflow sketch linking data/annotation/model evaluation with visualization usability studies, expert reviews, case studies, trust measurement, and longitudinal adoption, arguing for more standardized instruments and reporting practices. Overall, it advances SPC-like rigor in VTA evaluation by organizing concerns and suggesting research opportunities, but does not provide a concrete experimental design construction algorithm or empirical results.",Not applicable,Not applicable,None stated,"As a position paper, it provides conceptual frameworks and suggested dimensions but no concrete experimental protocols, statistical power guidance, or validated instruments specific to the proposed taxonomy. There is no empirical evaluation demonstrating that the proposed dimensions improve study quality or reproducibility, and no worked examples mapping real VTA studies into the framework. The scope is broad and largely qualitative, so actionable guidance (e.g., recommended designs, sample sizes, counterbalancing schemes, or analysis plans) remains underspecified.","The authors call for systematic analyses of prior VTA evaluation efforts and the development of guidelines for conducting and reporting VTA evaluations, including a repository of standard experimental designs. They also suggest developing standardized, easily applicable metrics/instruments (e.g., questionnaires) to measure user trust in VTA over time, and emphasize reproducibility and long-term adoption studies in real-world scenarios.","Operationalize the proposed audience–task–scale–model dimensions into reusable study templates (e.g., within-/between-subjects variants, counterbalancing and blocking recommendations) and validate them through comparative meta-research. Develop and psychometrically validate VTA-specific trust and calibration instruments that link subjective trust to objective model/visualization error characteristics. Provide open-source tooling/checklists for preregistration, reporting, and reproducibility (datasets, annotation protocols, analysis scripts) tailored to VTA user studies and case studies.",2209.11534v2,https://arxiv.org/pdf/2209.11534v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:21:27Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (examples include 2 parameters for isotropic elasticity; 4 parameters for von Mises plasticity example; 6 parameters for modified Hill anisotropic elastoplasticity),Other,Simulation study|Other,TRUE,None / Not applicable,Upon request,https://www.energy.gov/downloads/doe-public-access-plan,"The paper proposes a sequential design-of-experiments framework for calibrating complex history-dependent material models by casting experiment planning as a Markov decision process solved with deep reinforcement learning. The RL agent selects discrete experimental control actions (e.g., strain increments over many loading steps) to maximize an information-gain reward computed from the Kullback–Leibler divergence between prior and posterior parameter distributions estimated online with an enhanced Kalman filter (extended KF plus a switching KF for elastic/plastic regimes). The KF provides both parameter mean and covariance updates, which are also used to augment the RL state representation together with the full loading history, enabling rapid online experiment design without expensive forward-prediction resampling. Numerical experiments on linear elasticity and elastoplasticity (von Mises and modified Hill anisotropic yield models) demonstrate that the learned policies recover known-optimal designs in benchmark cases and produce high-performing designs when the optimal protocol is not known a priori. The approach is positioned as a cost-saving alternative/augment to reward definitions based on Nash–Sutcliffe efficiency that require additional test sampling.","The calibration model is written as $y=m(x,z;\theta)$ with history evolution $\dot z=f(x,z;\theta)$, and the EKF linearizes the observation with parameter sensitivities $A_k=\partial_\theta m\,|_{\theta_{k-1},x,z}$ so that $m(\theta_k)\approx m(\theta_{k-1})+A_k(\theta_k-\theta_{k-1})$. EKF updates use residual/covariance/gain: $r_k=d_k-m(x_k,z_k;\mu_{k-1})$, $S_k=A_k\Sigma_{k-1}A_k^T+R$, $K_k=\Sigma_{k-1}A_k^TS_k^{-1}$, and $\mu_k=\mu_{k-1}+K_kr_k$, $\Sigma_k=\Sigma_{k-1}-K_kS_kK_k^T$. The DOE reward is cumulative information gain via KL divergence for Gaussians: $KL_k=\tfrac12\{\log\tfrac{\det\Sigma_0}{\det\Sigma_k}+\mathrm{tr}(\Sigma_0^{-1}\Sigma_k)+(\mu_k-\mu_0)^T\Sigma_0^{-1}(\mu_k-\mu_0)-n_\theta\}$ and $R_{KL}=\sum_{k=1}^n (KL_k-KL_{k-1})$, optionally combined with an under-sampled NSE reward $R_{total}=w_{NSE}R_{NSE}+w_{KL}R_{KL}$.","In the linear isotropic elasticity verification (2 parameters), training over 10 RL iterations (10 episodes per iteration) learns a near-binary policy favoring two-step experiments that include one volumetric and one shear increment (in either order), while assigning effectively zero value to repeated identical actions that cannot identify both moduli. For von Mises plasticity on the $\pi$-plane (calibrating $Y_0$ and $H$ with known elastic parameters), the RL policy converges by 20 iterations to a monotonic radial loading design with final state $[1,1,1,1,1,1]$ (six +$\Delta\epsilon_1$ steps), yielding calibrated values $Y_0=0.30261$ vs. benchmark $0.3$ and $H=0.9578$ vs. benchmark $1.0$. For the modified Hill anisotropic elastoplasticity example (6 parameters), RL over 30 iterations identifies short 5-step strain protocols (e.g., $[1,1,4,1,1]$ for $B=0.5$ and $[1,1,1,4,1]$ for $B=2.0$) that calibrate parameters close to benchmarks, and a third experiment starting from isotropic/von Mises data recovers $\nu_\perp\approx\nu$ and $B\approx 1$ consistent with isotropy/von Mises behavior.","The authors note that KL rewards derived from the EKF can be unreliable because EKF linearization may diverge without a sufficiently close initial guess and can be inconsistent due to underestimation of the true covariance, which may inflate KL-based rewards. To mitigate this, they introduce a mixed reward that augments KL with an under-sampled NSE term used as a regularizer during training. They also state that the decision points for the MDP may need larger step sizes than the KF (sub-cycling the KF) for stability and accuracy, implying practical tuning/complexity in deployment.","The empirical results are based on numerical/synthetic experiments with simulators; the performance and robustness under real laboratory conditions (model mismatch, actuator constraints, drift, unmodeled rate/temperature effects) is not demonstrated. The action space is discretized and relatively small in the showcased trees; scalability to richer continuous control spaces or very long horizons may be limited by MCTS sample efficiency and computational cost. The reward relies on Gaussian parameter posteriors from KF-style updates, which may be a poor approximation for strongly nonlinear, non-identifiable, or multi-modal parameter landscapes common in complex constitutive models.","The authors propose extending the action space to include variable step sizes to improve decisions near yield onset, potentially requiring reformulating RL in continuous time. They also suggest multi-agent exploration and replay to accelerate training. For deployment on real experiments, they explicitly discuss the need for transfer learning from simulated to physical tests and investigating more effective state representations; they further note that model selection (and/or ML/data-driven models) is generally needed because the correct material model may not be known a priori.","A natural extension is to replace/augment EKF with more robust Bayesian filters (e.g., unscented/ensemble KF or particle filters) and study how posterior non-Gaussianity affects KL-based rewards and policy learning. Benchmarking against classical Bayesian optimal design methods (e.g., D-/I-optimal sequential designs) on comparable discrete action spaces would help isolate when RL+MCTS offers clear advantages. Providing an open-source reference implementation and reproducible benchmark suite (including real experimental datasets) would materially improve adoption and allow systematic ablation studies on reward choice, state encoding, and tree-search hyperparameters.",2209.13126v1,https://arxiv.org/pdf/2209.13126v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:22:25Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Other,Other,Variable/General (GLM predictors Np varied 1–5; designs use NE=5 experimental units; design variables are entries of design matrix D with dimension Np+1 including intercept),Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper proposes “design amortization” for Bayesian optimal experimental design (BOED): a single learned variational model can estimate expected information gain (EIG) across (potentially infinitely many) designs, rather than refitting a variational model per candidate design. The approach uses a permutation-invariant set encoder (Deep Sets-style) to embed design–outcome pairs into a context vector and a conditional normalizing flow to represent the design-conditional variational posterior qφ(θ|y,d). Training is done using a cheaper variational lower bound (the Barber–Agakov/posterior-estimator bound) and the resulting qφ is then reused to compute tighter but more expensive variational nested Monte Carlo (VNMC) upper/lower bounds on EIG. Empirically across several GLMs (normal with known/unknown noise, logistic, binomial, categorical, multinomial) with 5 experimental units, the amortized method yields substantially tighter EIG bounds and much better sample efficiency than standard nested Monte Carlo (NMC). The work advances BOED computation by combining amortized inference, set-invariant architectures, and flow-based variational posteriors to enable scalable design evaluation over large/continuous design spaces.","EIG is defined as the expected reduction in entropy: $\mathrm{EIG}(d)=\mathbb{E}_{p(y|d)}[H[p(\theta)]-H[p(\theta|y,d)]]$ and equivalently as an expected log-ratio involving posterior/prior or likelihood/marginal likelihood (Eq. 3). The posterior-estimator (Barber–Agakov) lower bound is $\mathrm{EIG}(d)\ge \mathbb{E}_{p(\theta,y|d)}\big[\log \frac{q_\phi(\theta|y,d)}{p(\theta)}\big]$ (Eq. 5), while VNMC provides an upper bound (Eq. 6) and a contrastive lower bound (Eq. 7) using importance sampling with $q_\phi(\theta|y,d)$. The amortized variational posterior is parameterized as a design-conditioned normalizing flow: $q_\phi(\theta|c_{y,d})=p_{\phi_0}(x_0|c_{y,d})\prod_{i=1}^K|\det J_{T_{\phi_i}}(x_i|c_{y,d})|^{-1}$ (Eq. 10), where the context $c_{y,d}$ is computed by a permutation-invariant set function (Eq. 8).","In an amortization comparison on a normal/linear GLM, the proposed amortized approach was reported as >3× faster than a non-amortized variational form (293s vs 920s) while producing a tighter and more accurate EIG lower bound across 50 candidate designs. Across GLM model experiments, VNMC bounds computed using the learned amortized posterior were consistently much tighter than NMC bounds, and for cases with computable/near-ground-truth (linear known/unknown noise) the VNMC bounds were described as nearly exact. The paper reports that VNMC achieved these improvements with far fewer samples than NMC (e.g., “167× fewer samples”). Architectural ablations indicate attention-based set encoders and affine coupling flow layers materially improve variational-posterior training loss and downstream EIG estimation quality.","The paper notes that while it focuses on evaluating/estimating EIG efficiently (via amortization and variational bounds), it does not directly tackle full design optimization in this work; optimization is left for future work. It also emphasizes that in several non-linear GLM settings ground-truth EIG is not available, so tightness of bounds is used as evidence of accuracy rather than direct error to truth.","The empirical study is primarily simulation-based on GLMs with relatively small numbers of predictors (up to 5) and a fixed small number of experimental units (NE=5), so scalability to larger, higher-dimensional, or more structured design spaces (constraints, discrete choices, costs) is not established. The approach assumes the ability to simulate from the model $p(y,\theta|d)$; performance may degrade for expensive simulators, model misspecification, or when likelihoods/simulators are not readily available. Comparisons are largely against NMC (and a limited non-amortized variational baseline in one case), leaving out other modern BOED approximations/optimizers (e.g., Laplace/EP-based utilities, mutual-information neural estimators, surrogate modeling, or other amortized SBI/BOED methods), which may affect conclusions about relative performance. Practical guidance for real Phase-I/Phase-II experimental workflows (unknown priors, constraints, sequential stopping, robustness to non-exchangeable units/autocorrelation) is limited.","The authors state plans to extend the approach from design evaluation to design optimization tasks. They highlight that the amortized variational form $q_\phi(\theta|y,d)$ is differentiable with respect to the design variables $d$, suggesting it could support and potentially improve gradient-based BOED optimization objectives (e.g., as in Foster et al. 2020).","A natural extension is to demonstrate end-to-end BOED where designs are optimized (not just evaluated) under realistic constraints (bounded regions, integer/discrete factors, cost/ethical constraints, batch/sequential policies). Further work could study robustness to model misspecification and non-i.i.d. data (autocorrelation, heteroscedasticity, non-exchangeable units) and develop diagnostics for when amortized posteriors are insufficient for tight VNMC bounds. Scaling experiments to higher-dimensional predictors and larger numbers of experimental units, along with benchmarks on real application datasets, would strengthen evidence for practical impact. Providing an open-source implementation and standardized evaluation protocol would improve reproducibility and adoption.",2210.03283v2,https://arxiv.org/pdf/2210.03283v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:23:09Z
TRUE,Optimal design|Screening|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General,Healthcare/medical|Environmental monitoring|Other,Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/sbb-gh/experimental-design-multichannel,"The paper proposes a task-driven experimental design paradigm for multi-channel imaging in which the design (a subset of image channels/measurements) is optimized jointly with training a machine-learning model to perform a user-specified task. The proposed method, TADRED, treats channel selection as supervised feature selection and uses a coupled scoring network and task network within a recursive feature elimination/backward-selection style outer loop that progressively reduces an initially densely sampled design to a smaller subset. The method targets both classical model-based tasks (e.g., quantitative MRI parameter estimation) and “model-free” tasks such as reconstructing a densely sampled measurement set from a sparse subset, aiming to reduce acquisition time/cost by selecting a fixed number of channels. Empirical evaluations across simulated qMRI (VERDICT, NODDI), real qMRI datasets (MUDI challenge, HCP test-retest), and hyperspectral imaging (AVIRIS remote sensing, simulated oxygenation estimation) show TADRED consistently outperforming Fisher-information-based baselines and strong supervised feature selection methods (SSEFS, FIRDL), often substantially at small subset sizes. The work advances DOE for imaging by reframing high-dimensional continuous design search as discrete channel subset selection driven directly by downstream task loss, enabling scalable optimization in real-world high-dimensional measurement spaces.","The experimental design is posed as selecting a channel set $D$ of fixed size $C$ minimizing task loss: $D^*=\arg\min_D\, L(T(X_D),Y)\ \text{s.t. }|D|=C$ (Eq. 1), and in the new paradigm jointly optimizing subset and task model with $D\subset \bar D$: $(D^*,T^*)=\arg\min_{D,T} L(T(X_D),Y)\ \text{s.t. } D\subset \bar D$ (Eq. 2). For a fixed outer-loop step, TADRED optimizes a mask $m_t$ (with $\|m_t\|_0=C_t$) and task network using filled-in removed features: $X_{D_t}= m_t\odot X_{\bar D}+(\mathbf{1}-m_t)\odot X^{\text{fill}}_{\bar D}$ and minimizes $L(T_t(X_{D_t}\odot \bar s_t),Y)$ (Eq. 3). Feature scores mix sample-dependent and global scores: $s=\alpha\odot \tilde s+(1-\alpha)\odot \bar s_t$ (Eq. 4), with iterative pruning of lowest-scored channels across steps.","On VERDICT-MRI parameter estimation with $\bar C=220$, TADRED yields lower MSE than SSEFS/FIRDL/random across subset sizes (e.g., at $C=14$: 2.64 vs 4.58 (SSEFS) and 4.05 (FIRDL)). For a Fisher-matrix baseline design with $C=20$, the paper reports RMSE of $15.0\times 10^{-2}$, while TADRED achieves $2.04\times 10^{-2}$ under the same experiment. On the MUDI reconstruction task ($\bar C=1344$), TADRED improves over the prior PROSUB method (e.g., $C=500$: 0.22 vs 0.49 MSE; $C=10$: 2.88 vs 3.48). For AVIRIS hyperspectral reconstruction ($\bar C=220$), TADRED strongly outperforms baselines (e.g., $C=110$: 0.87 vs 1.81 random and 2.03 SSEFS), and for simulated oxygenation estimation it reduces RMSE relative to the published baseline (e.g., $C=6$: 2.80 vs 4.54 for HbO2/Hb; similar gains for SO2).","The authors note TADRED may underperform state-of-the-art feature selection methods (SSEFS/FIRDL) in typical feature selection settings where a small discriminative subset exists among many uninformative features, because TADRED is tuned for scenarios where most channels are informative. They also state that iterative subsampling in an RFE/backward-selection framework can reduce the upper bound on achievable performance because optimal feature sets for sizes $C_t$ and $C_{t-1}$ may not be nested. They further acknowledge increased computational time compared to random selection due to iterative subsampling, though they report runtime comparable to SSEFS and FIRDL.","The method relies on having an initial densely sampled dataset $X_{\bar D}$ (often expensive to acquire) that adequately covers the measurement space; if this dataset is biased or not representative of deployment conditions, selected channels may not generalize. The approach selects a fixed-size subset without explicitly modeling uncertainty in downstream task performance (e.g., no Bayesian treatment of score uncertainty), which may matter when training data are small. Comparisons to classical DOE focus primarily on Fisher-matrix baselines; broader DOE baselines (e.g., modern approximate/Bayesian optimal designs with scalable surrogates or active-learning variants tailored to discrete sets) are not fully benchmarked at similar computational budgets. Practical constraints like unequal per-channel acquisition costs or hardware feasibility constraints are discussed as adaptable but not demonstrated experimentally.","The authors propose exploring alternative strategies to iterative subsampling/backward selection to address the non-nestedness of optimal feature subsets across sizes. They also suggest extending the framework to more complex cost functions where channels have unequal costs. Additionally, they note the method can naturally extend beyond per-pixel independent tasks by using architectures such as CNNs for the task network, and anticipate broader applications beyond imaging (e.g., autofocus and cell population studies).","A valuable extension would be a principled cost-aware or constrained subset selection (e.g., knapsack or matroid constraints) with explicit hardware/physics feasibility constraints for channel combinations. Robust/self-starting variants that handle distribution shift (scanner/site differences, atmospheric conditions in remote sensing) and nuisance variability could improve deployment reliability. Incorporating uncertainty estimates (e.g., Bayesian neural nets or ensembles) into the selection criterion could yield designs that are more robust when training data are limited. Providing standardized benchmarks and open evaluation protocols for task-driven imaging DOE would also strengthen comparability and accelerate adoption, along with releasing a reusable software package for TADRED.",2210.06891v4,https://arxiv.org/pdf/2210.06891v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:23:56Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Screening|Optimization|Prediction|Cost reduction|Other,Not applicable,"Variable/General (subsystems, components, parameters; not enumerated)",Transportation/logistics|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper proposes common terminology and a six-stage taxonomy for Decision Support Systems (DSSs) aimed at adaptive, human-in-the-loop experimental design for field robotics. The taxonomy ranges from Stage 0 (no support) through Stage 5 (sequential recommendations), progressively shifting responsibility from the human experimenter to the DSS, including prompting, monitoring/alerts, partial recommendations, single fully-defined recommendations, and multi-step experimental sequences. The focus is on improving the informativeness of tests while reducing experimental cost and risk in unstructured, stochastic environments, and positioning AI/ML-enabled Intelligent DSSs (IDSSs) as key enablers for higher stages. The paper synthesizes themes from prior DSS/IDSS literature across domains and maps them to field-robot experimentation needs rather than presenting a new statistical DOE construction or optimal design algorithm. It also identifies technical gaps for future work, including system models, information/value-of-information models, experimenter preference/risk models, and sequential decision-making models.",Not applicable (taxonomy/conceptual framework; no DOE optimality formulas or design-construction equations are provided).,"No quantitative DOE results (e.g., efficiencies, ARL, power, sample-size, or optimality gains) are reported. The main deliverable is a qualitative six-stage taxonomy with stage-specific DSS outputs, system requirements, and experimenter responsibilities, plus a set of identified technical gaps and research opportunities.","The authors note that automating experimentation is out of scope; the DSS is intended to support (not replace) the human decision maker, and autonomous/threshold-based stage transitioning is argued to be likely intractable and less flexible than human-selected stages. They also emphasize that higher-stage DSS/IDSS capabilities are not yet realized and would require significant advances in modeling and data-efficient learning given the limited number of costly field experiments.","Because the work is a conceptual taxonomy, it does not validate the stages with user studies, simulation experiments, or real field-robot deployments, so practical effectiveness and human-factors impacts (workload, trust, overreliance) remain unquantified. The taxonomy does not specify concrete DOE/BO/active-learning algorithms, optimality criteria, or how to operationalize “information gain,” “risk,” and “cost” across heterogeneous robotic tasks, which can limit implementability and comparability across systems. It also does not address statistical issues common in field data (autocorrelation, nonstationarity, confounding from environment changes) that affect experimental conclusions and the reliability of recommendations.","They identify several critical research gaps: developing scalable mathematical system models of the autonomy-under-test (including subsystem interactions and failure), formal information models (value of information/experiments and measures of decision effectiveness) that balance information gain vs. cost and risk under sparse data, experimenter models to infer/represent preferences and risk tolerance (including dynamic changes), and decision-making/planning models for orchestrating sequential experimental design across varying time horizons (especially to enable Stage 5).","Develop and release an open-source reference implementation (even for lower stages like prompts/monitoring) plus benchmark datasets and standardized evaluation protocols to enable reproducible comparison of DSS approaches. Empirically evaluate the taxonomy via controlled human-subject studies (e.g., comparing stage selections, decision quality, and safety outcomes) and real field-robot case studies, including robustness to distribution shift across environments. Connect the DSS stages to concrete adaptive-design methods (Bayesian optimization, bandits, Bayesian experimental design) with explicit utility functions that incorporate safety/risk constraints and communication limitations.",2210.08397v1,https://arxiv.org/pdf/2210.08397v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:24:21Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,Variable/General (selecting subsets of network variables/nodes under a budget constraint),Healthcare/medical|Pharmaceutical|Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/srtaheri/Simplified_LVM,"This paper proposes a simulation-based experimental design algorithm for planning observational experiments to estimate causal queries in partially observed biomolecular networks. Given a weighted ADMG (nodes weighted by measurement cost), a causal query (e.g., ATE or E[Y|do(T=t)]), a budget constraint, and a choice of estimator, the method enumerates cost-feasible sub-networks (sub-ADMGs) that maintain (asymptotic) unbiasedness and ranks them by the empirical variance of the resulting causal query estimates. Synthetic datasets used for evaluation are generated either from historical data (e.g., Bayesian-network forward simulation or Tabular GAN) or from mechanistic/assumed models (e.g., Hill-function models, SDE/Gillespie simulation), enabling variance estimation under planned sample size and replication. Across three biomolecular case studies, the ranked sub-networks often achieve substantially lower estimator variance and/or lower measurement cost than using only minimal adjustment sets or measuring the full network, and the best design depends on estimator choice and sample size.","The design objective is to select sub-ADMGs (subsets of variables/nodes) that support (asymptotically) unbiased causal query estimation under a cost budget and minimize estimator variance. The algorithm searches node subsets containing required variables (treatment T, outcome Y, and a valid adjustment set A) with total weight ≤ W and ranks each sub-ADMG G′ by var(\hat{Q}_{G′}) computed over K simulated datasets of size N (Algorithm 1: for each subset s, build G′=generateSubADMG(G,s), estimate \hat{Q}_{G′}, store var(\hat{Q}_{G′})). Example query/estimands discussed include ATE = E[Y|do(T=1)]−E[Y|do(T=0)] and E[Y|do(T=t)], with linear regression estimators given in equations such as Y=β0+βT T+γZ+ε and mediator-factorizations where ATE is a product of path coefficients (e.g., \hat{ATE}=\hat{α}\hat{ρ}\hat{η}).","In the case studies, including mediators in the measured sub-network frequently reduced variance by more than 50% relative to using only treatment+outcome+adjustment variables (e.g., Case study 1 reports a large variance drop when adding a single mediator). Measuring the full network was sometimes suboptimal and could increase variance and even introduce bias when “bad controls” were included (illustrative example and Case study 3). Case study 1 reports variance values (Table 1) showing that the lowest-variance sub-ADMG differs by estimator and sample size (e.g., for linear regression at N=100, the best shown sub-ADMG has variance 0.236 vs ~2.25–2.41 for others). Case studies 2–3 show that estimator constraints (e.g., IPW/AIPW/g-formula using only limited covariates) can eliminate benefits of adding extra measured nodes, whereas flexible estimators (linear regression/causal generative) can exploit mediators and additional nodes.","The authors note that the approach requires a known biomolecular network topology; they mitigate this partially by using ADMGs that can misspecify latent-variable structure while preserving observed-variable topology. They also state that the accuracy of generated synthetic data is a limitation, suggesting sensitivity analysis to address it. Finally, although supported estimators are (asymptotically) unbiased, they may be biased in practice due to violated assumptions and relatively small sample sizes; simulating interventional data is suggested as a diagnostic.","The sub-ADMG search (Algorithm 1) is exponential in the number of optional nodes, and while budgets and heuristics can reduce runtime, scalability to very large networks may still be challenging without more principled approximation/optimization. The variance ranking depends strongly on the fidelity of the synthetic data generator; if the simulator misrepresents dependencies or measurement noise, the selected sub-network may not be optimal (or even unbiased) on real data. The method focuses on variance/cost trade-offs under (asymptotic) unbiasedness but does not formalize uncertainty in the estimated variance itself (finite K) or provide statistical guarantees for selecting among close designs.","The authors propose closer integration with simulated interventional data to expand the scope of supported causal queries, to check identifiability (e.g., using the G-ID algorithm), and to improve practical utility of the approach.","Developing more scalable design-search strategies (e.g., greedy/branch-and-bound, Bayesian optimization over node subsets, or submodular approximations when applicable) could broaden applicability to larger biomolecular networks. Extending the framework to handle autocorrelated/time-series measurements, measurement error models, and multivariate/high-dimensional outcomes could improve realism for omics experiments. Providing a formal decision-theoretic objective (e.g., expected MSE including finite-sample bias) and uncertainty quantification for variance estimates used in ranking would make design recommendations more robust.",2210.13423v2,https://arxiv.org/pdf/2210.13423v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:24:58Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Screening|Optimization|Prediction|Cost reduction|Other,Not applicable,Variable/General (k ≥ 2 arms/treatments; multi-armed bandit style adaptive allocation),Service industry|Other,Case study (real dataset)|Other,NA,None / Not applicable,Not provided,NA,"The paper discusses practical pitfalls of deploying adaptive experimental design (AED) systems (e.g., Thompson-sampling bandits) in industry settings with non-stationarity, highlighting issues like biased estimates, reduced power from imbalanced allocations, and Simpson’s paradox under time-varying means. It proposes an AED framework for counterfactual inference that targets identifying the arm with the highest counterfactual cumulative gain (the reward that would have been obtained if all traffic had been routed to that arm). The method estimates cumulative gain via inverse probability weighting and uses always-valid (time-uniform) confidence intervals to enable sequential monitoring without inflating Type I error. An elimination-style algorithm allocates traffic uniformly over active arms and drops arms deemed inferior based on the cumulative-gain confidence bounds after a settling period. The approach is motivated and tested in a commercial experimentation environment, and is positioned as balancing inference validity, opportunity cost (regret), and robustness to time variation.","Cumulative gain for arm i after t days is defined as $G_{i,t}=\sum_{s=1}^t n_s\mu_{i,s}$, where $n_s$ is total traffic on day s and $\mu_{i,s}$ is the (potentially time-varying) success rate. An unbiased estimator uses inverse probability weighting: $\hat G_{i,t}=\sum_{s=1}^t r_{i,s}/p_{i,s}$ where $r_{i,s}$ is observed reward and $p_{i,s}$ is the allocation probability. Elimination uses an always-valid confidence radius $\phi(i,j,t,\delta):=\sqrt{(V_t(i,j)+\rho)\log\big((V_t(i,j)+\rho)/(\rho\delta^2)\big)}$ with $V_t(i,j)=\sum_{s=1}^t n_s\left(\hat\mu_{i,s}(1-\hat\mu_{i,s})/p_{i,s}+\hat\mu_{j,s}(1-\hat\mu_{j,s})/p_{j,s}\right)$; arm j is removed when $\hat G_{i,t}-\hat G_{j,t}-\phi(i,j,t,\delta/k)>0$ for some i.","The paper states theoretical guarantees in the stochastic stationary/constant-gap setting: the proposed elimination algorithm returns the best arm with probability at least $1-\delta$, using at most $\mathcal{O}(\log(k/\delta)\sum_{i=1}^k \Delta_i^{-2})$ samples and incurring instance-dependent regret no more than $\mathcal{O}(\log(k/\delta)\sum_{i=1}^k \Delta_i^{-1})$, characterized as near-optimal. In the general non-stationary setting, it does not provide strong worst-case guarantees beyond the property that an eliminated arm is dominated (in cumulative gain) by some arm remaining active at that time. It includes a commercial case study illustrating that naive bandit allocation plus standard t-tests can fail (low power/Simpson’s paradox), motivating cumulative-gain-based inference, but does not report detailed numeric lift/ARL-style performance tables in the provided excerpt.",The authors note that in the general non-stationary setting it is difficult to make strong performance statements/guarantees for the elimination algorithm beyond a weaker dominance property at the time of elimination. They also explicitly point out a downside of elimination under time variation: an arm eliminated because it is sub-optimal today could become the best-performing arm in the future.,"The proposed approach relies on sufficiently large daily traffic to invoke CLT-based approximations for the always-valid confidence interval, which may be inaccurate for low counts, rare events, or highly imbalanced/volatile traffic. Uniform allocation over the active set can be costly when many arms remain and may underperform more sophisticated constrained-allocation strategies that trade off regret and information gain. The paper does not specify how to handle practical complications such as delayed outcomes, interference/network effects, covariate imbalance, or autocorrelation/seasonality beyond the daily-batch framing. Code, reproducible simulation benchmarks, and sensitivity analyses for hyperparameters (e.g., settling time $\tau$, constant $\rho$) are not provided, limiting reproducibility and guidance for deployment tuning.",The authors suggest exploring combinatorial settings for multivariate testing. They also propose studying experimentation dynamics that leverage priors on the probability of launching successful treatments to better balance identification and regret objectives.,"Develop self-starting/robust variants that do not depend on CLT approximations (e.g., nonparametric or martingale-based bounds valid at small samples and for heavy-tailed rewards). Extend the framework to contextual and stratified experimentation (covariate-adjusted cumulative gain) and to delayed feedback settings common in online commerce. Provide systematic simulation studies and real-world benchmarks comparing against state-of-the-art best-arm identification and bandit algorithms under realistic non-stationarity (seasonality, drift, shocks). Release an implementation (library) and deployment guidance for choosing $\tau$, $\rho$, and stopping/elimination rules under business constraints (minimum traffic floors, fairness, risk controls).",2210.14369v1,https://arxiv.org/pdf/2210.14369v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:25:33Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Screening|Other,Other,"Variable/General (dimension $d$, sparsity level $s$; designs over $A\subset\mathbb{R}^d$)",Theoretical/simulation only,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/jajajang/sparse,"The paper proposes POPART, a computationally efficient sparse linear regression estimator that leverages a known population covariance matrix and combines coordinate-wise robust mean estimation (Catoni) with hard thresholding, yielding finite-sample $\ell_\infty$, support-recovery (no false positives), and $\ell_1$ error guarantees. These bounds motivate a new experimental design objective: choose a sampling distribution $\mu$ over the action/measurement set to minimize $H_2(Q(\mu))=\max_i (Q(\mu)^{-1})_{ii}$, which the authors show is a convex optimization problem. They define $H_*^2=\min_{\mu\in\mathcal{P}(A)}\max_i (Q(\mu)^{-1})_{ii}$ and prove it can be strictly better than the surrogate design constant $C_{\min}^{-1}$ used for Lasso-based designs, leading to improved estimation guarantees. Using this estimator and design, they develop explore-then-commit sparse linear bandit algorithms with improved regret bounds that depend on $H_*^2$, and they provide matching lower bounds in the data-poor regime. Empirical experiments compare POPART with Lasso under different design choices and show faster $\ell_1$ error decay and lower cumulative regret when using the proposed design/estimator combination.","The DOE object is based on the population covariance matrix induced by a sampling distribution $\mu$: $Q(\mu)=\int_{a\in A} aa^\top\, d\mu(a)$. The proposed experimental design criterion is to minimize the worst coordinate variance proxy $H_2(Q(\mu))=\max_{i\in[d]}(Q(\mu)^{-1})_{ii}$, defining the optimal value $H_*^2=\min_{\mu\in\mathcal{P}(A)}\max_i (Q(\mu)^{-1})_{ii}$. This is contrasted with the prior surrogate for Lasso design $C_{\min}(A)=\max_{\mu\in\mathcal{P}(A)}\lambda_{\min}(Q(\mu))$.","The POPART/WARM-POPART bounds yield $\ell_1$ estimation error scaling like $\|\hat\theta-\theta^*\|_1=O\big(s\,\sigma\,\sqrt{H_2(Q(\mu))\,\log(2d/\delta)/n}\big)$ (e.g., Corollary 1 gives $\|\hat\theta-\theta^*\|_1\le 8s\sigma\sqrt{H_2(Q(\mu))\ln(2d/\delta)/n_0}$ under its conditions). They prove the geometric constants satisfy $H_*^2\le C_{\min}^{-1}\le d\,H_*^2$, and give arm sets where either inequality is tight, implying the POPART-motivated design can improve over $C_{\min}$-based designs. For sparse linear bandits, Explore-then-Commit with WARM-POPART achieves regret $\tilde O\big((R_{\max}s^2\sigma^2 H_*^2 n^2)^{1/3}\big)$ (Theorem 3) and they prove a matching lower bound in the data-poor regime of order $\Omega(\kappa^{-2/3}s^{2/3}n^{2/3})$ for a family of instances (Theorem 5). Simulations (two cases) show POPART with the $H_*^2$-optimized design converges faster in $\ell_1$ error than Cmin-Lasso and yields lower cumulative regret than the prior ESTC baseline.","POPART assumes access to the population covariance matrix $Q(\mu)$ as an input, which may not be available in practice. The authors note that direct comparison to Lasso is nontrivial because Lasso’s key constant (compatibility constant) is defined via an intractable optimization and depends on the empirical covariance. They also point out POPART’s bound can scale with $\sqrt{R_0^2+\sigma^2}$ when the pilot estimator is poor, motivating the warm-up variant WARM-POPART.","The design objective $\min_\mu \max_i (Q(\mu)^{-1})_{ii}$ is stated as convex, but the paper does not fully address computational scaling and implementation details for large/continuous action sets (e.g., support size of optimal designs, solver complexity, or discretization errors). The estimator and design rely heavily on i.i.d. sampling from a chosen distribution and a correctly specified linear model with sub-Gaussian noise; robustness to model misspecification, heteroskedasticity, dependence, or heavy tails beyond what Catoni handles coordinate-wise is not fully explored. The bandit algorithms are primarily explore-then-commit; practical performance in adaptive/fully sequential designs beyond this framework (and in data-rich regimes where optimal regret is $\tilde O(\sqrt{sdn})$) is not demonstrated beyond stated theory.","The authors suggest investigating whether $(Q(\mu)^{-1})_{ii}$ is the statistical limit for testing whether $\theta_i^*=0$ and whether dependence on $H_*^2$ in regret is unimprovable beyond the particular lower-bound family. They propose studying how to use POPART without relying on known population covariance, e.g., estimating covariance from extra unlabeled data or using empirical covariance directly. For sparse linear bandits, they suggest developing algorithms that simultaneously achieve data-poor optimal regret and data-rich optimal regret $\tilde O(\sqrt{sdn})$, and extending results to changing arm sets.","A natural extension is to study alternative optimality criteria aligned with other loss functions (e.g., prediction risk/I-optimality over a target set, or minimizing worst-case regret constants directly) and compare them empirically to the $\max_i (Q^{-1})_{ii}$ criterion. It would be valuable to develop open-source implementations of the convex design solver (continuous/discrete) and benchmark scalability and design support size, especially for large $d$ and complex arm sets. Further work could generalize the design and estimator to settings with unknown/estimated $Q$, correlated/auto-correlated covariates, and heteroskedastic noise, and to multivariate or generalized linear models where sparse structure remains but linear-Gaussian assumptions fail.",2210.15345v3,https://arxiv.org/pdf/2210.15345v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:26:21Z
TRUE,Optimal design|Bayesian design|Other,Optimization|Parameter estimation|Prediction|Other,A-optimal|Other,Variable/General (examples include 2 parameters in pricing logistic model; multi-parameter contact matrix in pandemic model),Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://anonymous.4open.science/r/ExpDesign4ETO-DEB3/,"The paper studies the common “estimate-then-optimize” workflow, where parameters of a structural model are estimated from data and then decisions are optimized as if those estimates were true, and analyzes the resulting decision regret. It derives a novel upper bound on regret for smooth objectives under unconstrained optimization with local strong convexity, showing regret depends primarily on the cross-derivative matrix $D=\partial^2 f/\partial x\partial \theta$ and the estimation error. Under an assumption that the estimator error is a linear transformation of a sub-Gaussian vector, it obtains a high-probability bound with leading term proportional to $\mathrm{Tr}(D\Sigma D^\top)/n$. This leads to an experimental design procedure: choose an experiment (which determines the estimator covariance $\Sigma$) to minimize $\mathrm{Tr}(D\Sigma D^\top)$, which corresponds to A-optimal design for the transformed parameter system $K\theta$ (and reduces to C-optimal design when the decision is one-dimensional). Because $D$ depends on unknown $\theta^*$, they propose a Bayesian design variant that minimizes the prior expectation of $\mathrm{Tr}(D_0\Sigma D_0^\top)/n$, and demonstrate via simulations on a normal-parameter linear model, a pricing/logistic conversion experiment, and a pandemic control application.","Estimate-then-optimize chooses $\hat x\in\arg\max_x f(x,\hat\theta)$. The core regret bound (Theorem 3.4) gives $f(\hat x,\theta^*)-f(x^*,\theta^*)\le \frac{4\beta_1}{\rho^2}\left\|\left.\frac{\partial^2 f}{\partial x\partial \theta}\right|_{(x^*,\theta^*)}(\hat\theta-\theta^*)\right\|^2 + \frac{4\beta_1}{\rho^2}\cdot\frac{\beta_2^2}{4}\|\hat\theta-\theta^*\|^4$. Under $\hat\theta-\theta^*=\Sigma^{1/2}X/\sqrt n$ with sub-Gaussian $X$, the leading term in the high-probability bound is proportional to $\mathrm{Tr}(D\Sigma D^\top)/n$, motivating the design objective $\min\ \mathrm{Tr}(D\Sigma D^\top)/n$ (and a Bayesian version $\min\ \mathbb E_{\theta\sim\pi}[\mathrm{Tr}(D_0\Sigma D_0^\top)/n]$).","The main experimental-design implication is that minimizing the leading regret-bound term reduces to minimizing $\mathrm{Tr}(D\Sigma D^\top)/n$, which coincides with A-optimal design (and with C-optimal design when $x$ is one-dimensional). In the pricing/logistic conversion simulation (e.g., $n=100$ samples across $m=10$ prices), the optimized allocation produced consistently lower regret than uniform allocation across several priors (Table 1). In the pandemic-control example, optimized contact-tracing allocations also reduced regret vs. uniform across budgets (e.g., for $C=100$, optimized regret $6.5\pm1.0$ vs. uniform $9.3\pm1.3$; Table 2), with similar improvements under increased transmissibility (Table 3). Additional simulations show regret decays approximately on the order of $1/n$ in both pricing and pandemic examples, aligning with the bound’s $O(\log n/n)$ behavior.","The conclusion notes that the method could be extended to decide whether only a subset of uncertain parameters should be used/estimated, implying the current approach does not address parameter subset selection. It also notes that extending to model selection (possibly infinite-dimensional/nonparametric $\theta$) would require additional technical machinery, which is not handled here. The authors also caution that improving data-driven decision-making can be misused by actors whose goals are not aligned with societal welfare.","The regret bounds rely on smoothness and (local) strong convexity and emphasize unconstrained optimization, whereas many practical decision problems have hard constraints, nonconvexity, or discrete decisions, which may invalidate the theory or require nontrivial adaptation. The DOE objective depends on $D=\partial^2 f/\partial x\partial \theta$ evaluated near $(x^*,\theta^*)$, but computing/approximating $D$ can be expensive (especially when $f$ is defined implicitly via simulation, as in the pandemic example) and sensitive to finite-difference choices. The experimental-design step largely treats the mapping from design choices to estimator covariance $\Sigma$ as known/tractable, but in real systems $\Sigma$ may be misspecified (model/estimation errors, non-i.i.d. data, interference in pricing experiments, etc.), potentially weakening guarantees and design optimality.","The authors suggest extending the approach to decide whether to use/estimate only a subset of uncertain parameters $\theta$, potentially dropping insensitive components while maintaining good decisions. They also suggest extending the ideas to model selection, potentially with infinite-dimensional/nonparametric parameters, which would require additional technical machinery.","Extend the regret-bound-driven design approach to constrained optimization (e.g., capacity/feasibility constraints) and to nonconvex or discrete/combinatorial decisions, where local minima and rounding can change regret behavior. Develop robust/self-normalizing or misspecification-resistant designs when the assumed estimator distribution (sub-Gaussian/asymptotic normal) or covariance mapping $\Sigma$ is uncertain. Provide practical algorithms and software for efficiently estimating/approximating $D$ (e.g., automatic differentiation through simulators, adjoint methods) and for solving the resulting design optimization at scale, along with broader real-world validations beyond simulation case studies.",2210.15576v1,https://arxiv.org/pdf/2210.15576v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:27:06Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Other,Other,Variable/General (treatment T is binary; covariates X are d-dimensional: d=10 in synthetic experiment; 25 covariates in IHDP semi-synthetic experiment),Healthcare/medical|Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian sequential experimental design (active learning) algorithm for estimating the parametric component (treatment effect coefficient $\theta$) in a partially linear model $Y=\theta T + f(X)+\varepsilon$, where the nuisance function $f$ has a Gaussian process prior. The design selects the next covariate–treatment pair $(x,t)$ from a candidate pool to maximize an information-gain objective defined as the expected reduction in posterior entropy of $\theta$, yielding an equivalent criterion based on minimizing the next-step posterior variance/entropy of $\theta$. The method is positioned as a “targeted” sequential design tailored to causal-parameter estimation (ATE/ACE) rather than learning the entire outcome surface. Performance is evaluated via Monte Carlo experiments on synthetic data (with an RBF kernel GP prior) and semi-synthetic IHDP benchmark data (with a dot-product kernel), comparing the proposed optimal design to random sampling. Across both settings, the proposed design achieves substantially lower absolute estimation error for $\theta$/ATE, particularly at small sample sizes.","Model: $Y_i=\theta T_i + f(X_i)+\varepsilon_i$, with $f\sim\mathrm{GP}(0,C(\cdot,\cdot;\omega))$ and Gaussian noise. The posterior for $\theta$ is Gaussian, $p(\theta\mid t,y,X)=\mathcal N(\mu_{\theta,n},s_{\theta,n}^{-1})$, where $s_{\theta,n}=s_{\theta,0}+t^\top(s_\varepsilon^{-1}I_n+\Phi)^{-1}t$ and $\mu_{\theta,n}=s_{\theta,n}(s_{\theta,0}\mu_{\theta,0}+t^\top(s_\varepsilon^{-1}I_n+\Phi)^{-1}y)$. The sequential design maximizes information gain $g(x,t)=H[p(\theta\mid t,y,X)]-\mathbb E[H[p(\theta\mid t,y,X,t,y,x)]]$, which reduces to choosing $(x_{n+1},t_{n+1})$ that minimizes the next-step posterior entropy/variance via $s_{\theta,n+1}(x,t)=s_{\theta,0}+[t^\top,t](s_\varepsilon^{-1}I_{n+1}+\Phi_0(x))^{-1}[t^\top,t]^\top$.","In Monte Carlo experiments (1000 repetitions), the proposed information-gain (optimal) sequential design yields lower absolute error $|\mu_{\theta,n}-\theta|$ on synthetic data and $|\mu_{\theta,n}-\mathrm{ATE}|$ on semi-synthetic IHDP data than a random design, with the gap largest at smaller sample sizes. Synthetic setup: $d=10$, $|X_{\text{pool}}|=400$, first 10 points random then adaptive; GP uses an RBF kernel with $\omega=1$ and noise precision $s_\varepsilon=1$. IHDP setup: 747 observations with 25 covariates; $X_{\text{pool}}$ is 400 sampled observations, first 50 points random then adaptive; the assumed true ATE is 4.021 and the GP uses a dot-product kernel with $\omega=1$. The figures show consistently faster error reduction under the optimal design than under random sampling.","The paper notes that if one places priors on hyperparameters (e.g., $\mu_\theta,s_\theta,\omega,s_\varepsilon$), the posterior of $\theta$ is no longer available in analytic form and then numerical methods such as MCMC are required (handled in prior work rather than this paper). It also emphasizes evaluation primarily through synthetic and semi-synthetic experiments rather than fully real-world deployments where ground-truth causal effects are unavailable.","The method assumes the partially linear outcome model and Gaussian/noise and GP-prior structure; performance may degrade under model misspecification (e.g., treatment effect heterogeneity, non-Gaussian errors, or GP kernel mismatch). The design operates over a finite candidate pool $X_{\text{pool}}$ and binary treatment assignment, so it may not directly address continuous treatments or continuous design regions without additional optimization machinery. Practical implementation details (computational complexity of repeatedly computing $(s_\varepsilon^{-1}I+\Phi)^{-1}$ updates, scalability to large pools/large $n$) and sensitivity to hyperparameter estimation are not thoroughly benchmarked. Comparisons are limited to random design; stronger baselines (e.g., uncertainty sampling variants, Bayesian D-optimal/variance-reduction designs, or causal-specific adaptive policies) are not reported.","The conclusion suggests extending the approach to other causal parameters beyond ATE/ACE (e.g., CATE or effect modification) and to other modeling approaches for the data-generating mechanism beyond the particular partially linear GP-prior model studied here. It frames applying the proposed targeted sequential design idea to other combinations of estimands and data-generating mechanisms as future work.","Develop self-starting/robust variants that account for unknown hyperparameters online (jointly adapting design while estimating $\omega$ and $s_\varepsilon$) and assess sensitivity to misspecification. Extend to continuous treatments, multi-armed/continuous interventions, and constrained experimentation (e.g., budgeted, ethical, or balance constraints) common in causal studies. Provide scalability improvements (e.g., sparse GP approximations, incremental matrix updates) and an open-source implementation to enable broader adoption. Validate on additional real-world datasets and compare against established adaptive causal designs (e.g., bandit-style policies or targeted learning/adaptive trial designs) using standardized metrics.",2211.02230v1,https://arxiv.org/pdf/2211.02230v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:27:48Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (design variable d in a compact metric space; examples include 2D sensor/location designs and a 5-timepoint sensor setting),Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper studies stability of the expected utility (expected information gain) used in Bayesian optimal experimental design (OED) under perturbations/approximations of the likelihood, formulated in a non-parametric (infinite-dimensional) Bayesian inverse-problem setting. It introduces general KL-divergence-based assumptions on surrogate likelihoods and proves a uniform-in-design convergence bound: the expected utility error decays at rate proportional to the square root of the likelihood KL error bound, and shows this 1/2 exponent is sharp via a lower-bound example. Using Γ-convergence arguments, it proves that maximizers of the surrogate expected utility converge (up to subsequences) to maximizers of the true expected utility, establishing stability of the optimal design itself. For Gaussian-noise inverse problems, it connects KL perturbations to weighted L2(μ0) errors of the observation map and shows the assumptions hold when the forward map is approximated by a surrogate (e.g., interpolation or polynomial chaos). Numerical experiments (including sensor placement for an inverse heat equation) empirically confirm the predicted convergence rates.","Bayesian OED objective is the expected information gain: $U(d)=\iint \log\!\big(\pi(y\mid x;d)/\pi(y;d)\big)\,\pi(y\mid x;d)\,d\mu_0(x)\,dy=\int D_{\mathrm{KL}}(\pi(\cdot\mid x;d)\,\|\,\pi(\cdot;d))\,d\mu_0(x)$. Surrogate objective uses an approximate likelihood $\pi_N$: $U_N(d)=\iint \log\!\big(\pi_N(y\mid x;d)/\pi_N(y;d)\big)\,\pi_N(y\mid x;d)\,d\mu_0(x)\,dy$. Main stability bound (uniform over $d\in D$): $\sup_{d\in D}|U(d)-U_N(d)|\le C\sqrt{\psi(N)}$ when $\mathbb{E}_{\mu_0}[D_{\mathrm{KL}}(\pi_N(\cdot\mid X;d)\|\pi(\cdot\mid X;d))]\le C\psi(N)$; for Gaussian noise with $y=G(x;d)+\varepsilon$, $\varepsilon\sim\mathcal N(0,\Gamma)$, $\mathbb{E}_{\mu_0}D_{\mathrm{KL}}(\pi_N\|\pi)=\tfrac12\mathbb{E}_{\mu_0}\|G(X;d)-G_N(X;d)\|^2_{\Gamma}$.","Under KL-based approximation assumptions, the paper proves a uniform stability rate: $\sup_{d\in D}|U(d)-U_N(d)|=\mathcal O(\sqrt{\psi(N)})$, i.e., the expected-utility convergence rate is one-half the likelihood KL convergence rate (Theorem 3.4). It further proves convergence (up to subsequences) of surrogate maximizers $d_N^*\in\arg\max_d U_N(d)$ to a true maximizer $d^*\in\arg\max_d U(d)$ (Theorem 3.5). For Gaussian likelihoods, it shows the KL perturbation equals $\tfrac12\mathbb{E}_{\mu_0}\|G-G_N\|^2_{\Gamma}$, so the utility error rate matches the $L^2(\mu_0)$ convergence of the forward-map surrogate (Theorem 4.4). A scalar linear example demonstrates the $1/2$ exponent is asymptotically sharp: $|U_N-U|$ scales like $|a_N-a|$ while $\mathbb{E}D_{\mathrm{KL}}$ scales like $|a_N-a|^2$. Three numerical examples (including inverse heat-equation sensor placement with polynomial chaos surrogates) show empirical rates aligned with the theoretical predictions.",None stated.,"The work focuses on stability to deterministic surrogate likelihood/forward-map perturbations; it does not provide uniform-in-design error bounds when the expected utility is approximated by nested Monte Carlo (which is typically required in practice), and explicitly notes the uniform MC case is not addressed. The main results rely on KL-based closeness and bounded log-ratio second moments, which may be difficult to verify or satisfy for heavy-tailed noise models, misspecification, or practical surrogate errors that are not uniformly controlled over the entire design space. The paper establishes convergence of maximizers only up to subsequences and does not give explicit rates for convergence of the optimal designs themselves (beyond objective convergence).","The authors suggest extending the stability analysis to other utilities (e.g., negative least squares loss) and utilities related to Bayesian optimization. They note that further numerical approximation (such as Monte Carlo) is usually needed for expected utility, and that while fixed-design MC convergence is studied in prior work, the uniform-in-design case remains open. They also point out that Γ-convergence does not directly yield convergence rates for the optimal designs and identify obtaining such rates as an open problem.","Develop explicit (non-asymptotic) rates for design-argmax convergence under additional regularity/strong concavity or identifiable-maximizer conditions, enabling practical stopping rules for surrogate refinement. Extend the stability theory to dependent/temporal data (autocorrelated noise) and to non-Gaussian likelihoods where KL/Hellinger control may require different techniques (e.g., robust divergences). Provide algorithmic guidance for adaptive surrogate refinement focused on regions near the design optimum (goal-oriented error indicators for $U(d)$). Release reference implementations for the three numerical examples to facilitate benchmarking and reproducibility across Bayesian OED software stacks.",2211.04399v2,https://arxiv.org/pdf/2211.04399v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:28:36Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Model discrimination|Cost reduction|Other,Other,Variable/General (faces parameterized by BFM latents; shape 199 + texture 199 dimensions; expression off; optimization noted as R^398),Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://github.com/kriegeskorte-lab/controversial_stimuli_tutorial|https://proceedings.mlr.press/v97/kornblith19a.html|https://dspace.mit.edu/handle/1721.1/143617|https://openreview.net/forum?id=a3YPu2-Mf2h&noteId=Kc9ZgrRCKS|http://arxiv.org/abs/1412.6980|https://github.com/pytorch/vision/tree/main/references/classification,"The paper proposes a Bayesian optimal experimental design approach for synthesizing ""controversial"" stimulus sets that efficiently discriminate among competing representational models by maximizing an expected model-recovery utility. Unlike classic BOED that optimizes protocol parameters, the design variable here is the stimulus set itself, generated by optimizing latent parameters of a differentiable face generator (Basel Face Model) with gradients backpropagated through a differentiable renderer and candidate neural-network models. The utility compares how well each candidate model predicts data generated by a held-out instance of itself versus how well the best alternative model predicts those data, with model layers treated as nuisance parameters via maximization. The approach is evaluated via in-silico model-recovery simulations under added noise and via a human behavioral face-pair dissimilarity arrangement experiment comparing six VGG-16-based models trained on different tasks/datasets. Results show that controversial (optimized) stimuli markedly improve model discriminability, and in human data a network trained for inverse rendering of BFM latents is significantly more aligned with human dissimilarity judgments than identification, classification, or autoencoding variants.","The design objective is a BOED-style utility over stimulus set ξ that rewards separability of models’ predicted dissimilarity vectors: for each model m (and layer l as a nuisance parameter), maximize the difference between within-model predictability and the best competing model’s predictability, using a saturating transform f (e.g., f(x) = -e^{-10x}). Model performance ψ is instantiated as mean across trials of Pearson correlation between observed dissimilarities y_t and model-predicted dissimilarities \hat{y}_m(\xi_t): \psi(y,m\mid\xi)=\frac{1}{T}\sum_t \mathrm{corr}(y_t,\hat{y}_m(\xi_t)). The controversial stimulus set is obtained by gradient ascent on this utility by adjusting BFM shape/texture latents with differentiable rendering and network forward passes.","In simulations of model recovery across six candidate models (and 16 layers) with additive Gaussian noise, the controversial (optimized) stimulus sets yield substantially higher model-recovery accuracy than randomly sampled or systematically distance-spread stimulus sets, especially as noise increases (Fig. 1). In human behavioral data (90 participants; 30 per stimulus-set condition), random and systematic stimulus sets did not produce a single model significantly better than all others, whereas the controversial stimulus set did. Specifically, the VGG-16 trained on inverse rendering (predicting BFM latents from images) achieved significantly higher cross-validated Spearman correlation with human dissimilarity judgments than each of the other five models (Fig. 2), and this result replicated on a second independently optimized controversial set (Fig. S4).","The authors note that gradient-based stimulus optimization requires differentiability of the utility/objective, the stimulus renderer, and the models, which restricts applicability. They also acknowledge that the inverse-rendering model was tested under favorable conditions because it was trained to invert the same generative model (BFM) used to parameterize the experimental stimuli, potentially biasing the comparison. They suggest using alternative synthesis methods (evolutionary algorithms) and alternative stimulus generators (e.g., GANs) to address these constraints.","The utility relies on correlations of model-predicted dissimilarities and treats layer selection via maxima, which can introduce selection bias and may make results sensitive to the set of layers/instances considered and to the specific ψ choice (Pearson for optimization vs. Spearman for analysis). The design is locally optimized (SGD/Adam) and may depend on initialization and hyperparameters; while a replication set is shown, broader robustness across many seeds and alternative priors p(m), p(l|m) is not fully characterized. Human-task specifics (pair arrangement with each face appearing only once) may limit generalization to other RSA paradigms (e.g., full RDM estimation with repeated items) and to real-world photographic face variability beyond BFM-rendered faces. No publicly released code for the specific BOED stimulus-synthesis pipeline limits reproducibility and adoption.","The authors propose exploring evolutionary-algorithm-based stimulus synthesis as a complementary approach that does not require differentiability. They also plan to evaluate models using controversial stimuli parameterized by alternative generators (e.g., generative adversarial networks) to mitigate the favorable match between the inverse-rendering model’s training generator and the experiment’s stimulus generator.","Developing and validating self-contained software (with released code and pretrained models) would facilitate replication and broader use in neuroscience/psychophysics labs. Extending the BOED objective to explicitly model human measurement noise and inter-subject variability (hierarchical Bayesian design) could yield designs optimized for statistical power at fixed sample sizes. Testing robustness across different representational comparison metrics (whitened-RDM measures, distance metrics beyond squared Euclidean) and across multivariate neural data (fMRI/MEG) would strengthen generality. Incorporating constraints for ecological validity and fairness (e.g., broader demographic variation via calibrated generators) could improve external validity of controversial stimuli in face perception studies.",2211.15053v1,https://arxiv.org/pdf/2211.15053v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:29:21Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Parameter estimation|Model discrimination|Prediction,Other,Variable/General (example shown: 4 inputs; 3 candidate models; 5 parameters in ground-truth model),Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper formulates Bayesian optimal experimental design (OED) for symbolic discovery/symbolic regression, where both the functional form (model identity) and continuous parameters are treated as unknown. It studies several design selection criteria—mutual information between observation and model, posterior model entropy reduction, Jensen–Shannon divergence, and response entropy—and shows they are equivalent under the assumed i.i.d. Gaussian measurement noise setting. The authors implement sequential design: iteratively choose the next design point x to maximize informativeness, observe y, and update posteriors over models and parameters. Computation uses Hamiltonian Monte Carlo to sample parameter priors/posteriors and constrained gradient-based optimization (SQP) to optimize the design criterion, with predictive distributions computed via numerical integration or fast convolution/FFT methods. A simulation on a Feynman-equation example with three candidate models demonstrates that the correct model’s posterior probability concentrates quickly under low noise and more slowly under higher noise, while parameter uncertainty (mean variance) decreases over design iterations.","Observation model: y(x)=m_true(x,θ_true)+ε with ε~N(0,σ^2), giving p(y|m,θ,x)=φ(y; m(x,θ), σ^2). Bayesian predictive for each model and overall mixture: p(y|m,x)=E_{θ_m}[φ(y; m(x,θ_m),σ^2)] and p(y|x)=E_m[p(y|m,x)]=∑_m p(m)p(y|m,x). Main design criteria include maximizing conditional mutual information I(y;m|x)=E_m[ D_KL(p(y|m,x) || p(y|x)) ] (equivalently maximizing Jensen–Shannon divergence or minimizing posterior model entropy), and—under constant noise entropy—maximizing response entropy H(y|x). An alternative ‘logdet’ criterion builds a KL-divergence matrix D(x) over sampled (m,θ) pairs with entries D_KL(N(m(x,θ),σ^2)||N(m'(x,θ'),σ^2))=((m(x,θ)-m'(x,θ'))^2)/(2σ^2) and selects x by minimizing −log det D(x).","In the numerical study based on a Feynman lecture-notes equation, the authors use 3 candidate models, 4 input variables, and a 5-parameter ground-truth model, with parameter priors p(θ_m)~N(μ_m, I). Using 4,000 HMC samples and averaging over 20 trials, the correct model’s mean posterior probability reaches ~1 after about 12 sequentially chosen design points when noise variance is small (σ^2=0.01). Under larger noise (σ^2=1), the correct model probability increases steadily but does not fully reach 1 within 18 design points in the reported plots. Parameter uncertainty (tracked via mean per-parameter variance) drops rapidly in early design points and continues decreasing with more acquisitions, with slower improvement under higher noise.","The study is presented as a preliminary validation on a small example and the authors note the need to test the formulation on more settings and larger problems. They highlight computational challenges around computing derivatives for optimization/sampling, addressed here via symbolic differentiation. They also note HMC’s requirement of unbounded support, which can conflict with naturally constrained parameters (e.g., nonnegative exponents), and discuss that this can cause extreme responses near zero inputs; they mitigate this via an additional prior term and mention alternative constraint-handling approaches.","The empirical evaluation is limited to a single synthetic/scientific benchmark with a small set of candidate models, so it is unclear how robust performance is across broader symbolic regression libraries and more severe model misspecification. The approach assumes i.i.d. Gaussian noise with known variance σ^2; in practice σ^2 is often unknown, noise can be heteroskedastic, and data can be correlated, which can change optimality and criterion equivalences. Design optimization appears to be for a single next point x (greedy/sequential myopic design); without lookahead, it may be suboptimal in multi-step settings. No public code is provided, and implementation details (e.g., SQP settings, discretization/grid choices for convolution) could materially affect results and reproducibility.","The authors state they need to experiment with more settings and larger problems and to explore acceleration techniques for Bayesian OED. They list potential directions including importance sampling/particle methods to reuse samples under posterior updates, Laplace approximations (to be evaluated for suitability), approximate Bayesian methods for expensive models, and formulations where the design point is itself sampled via MCMC to obtain an optimum as an MCMC output. They also mention exploring alternative sampling strategies beyond MCMC, including Hamiltonian Monte Carlo (already used) as more efficient sampling.","Extending the framework to handle unknown noise variance (and more generally nuisance parameters) with hierarchical priors would improve realism and could change the optimal design criterion. Robust/self-starting variants that tolerate autocorrelation, heteroskedasticity, and outliers would be valuable for real measurements. Scaling to high-dimensional design spaces and large candidate model sets may require amortized surrogate models (e.g., neural density estimators for p(y|x)) or efficient gradient estimators; benchmarking runtime/accuracy tradeoffs would clarify practicality. Providing open-source implementations and standardized benchmarks for symbolic discovery OED would improve reproducibility and enable fair comparison with alternative active learning and Bayesian optimization approaches.",2211.15860v1,https://arxiv.org/pdf/2211.15860v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:30:00Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Optimization|Prediction|Robustness|Cost reduction,A-optimal|D-optimal|Bayesian D-optimal|Compound criterion|Other,"Variable/General (e.g., sensor-placement designs with 5 candidate sensors in toy example and 10 candidates in advection–diffusion example)",Environmental monitoring|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Personal website,https://gitlab.com/ahmedattia/pyoed|https://web.cels.anl.gov/~aattia/pyoed/index.html,"The paper presents PyOED, an extensible Python toolkit for model-constrained optimal experimental design (OED) in inverse problems and data assimilation (DA), with modular components for simulation models (ODE/PDE), observation operators, error models, DA solvers, and OED optimizers. It targets Bayesian OED and sensor-placement style designs where the design variables control observational configurations (e.g., selecting/weighting sensor locations) and can be optimized under sparsity/budget penalties. The package supports common OED utilities including alphabetic criteria (A- and D-optimality via Fisher information or posterior covariance/precision) and information-theoretic criteria such as expected information gain (EIG), and it includes both relaxed continuous designs and stochastic learning approaches for binary designs. The paper demonstrates workflows on two prototypical numerical test cases: a linear-Gaussian toy time-dependent model and a PDE-constrained 2D advection–diffusion problem, illustrating solving inverse problems and optimizing sensor subsets under budget constraints. Practical emphasis is on rapid prototyping/benchmarking and recombining components rather than scalability, with documentation and a public code repository provided.","The core OED formulation is posed as $\zeta_{\mathrm{opt}}=\arg\max_\zeta U(\zeta)$ with optional sparsity penalty $U(\zeta)=\Psi(\zeta)-\alpha\Phi(\zeta)$, often for sensor placement with $\zeta\in\{0,1\}^{n_s}$ (or relaxed to $[0,1]^{n_s}$). In linear-Gaussian settings, the design enters through a weighted likelihood using a design-dependent precision/covariance $W_\Gamma(\zeta)$, yielding a design-dependent posterior covariance $\Gamma_{\text{post}}(\zeta)=\big(F^*W_\Gamma(\zeta)F+\Gamma_{\text{pr}}^{-1}\big)^{-1}$ and Fisher information $\mathrm{FIM}(\zeta)=\Gamma_{\text{post}}(\zeta)^{-1}$. Utility examples include D-optimality via maximizing $\log\det(\mathrm{FIM}(\zeta))$ (e.g., Eq. (17)) and A-optimality via trace-based criteria; nonlinear cases are handled via Laplace linearization and/or information gain criteria such as EIG.","The paper reports qualitative benchmarking outcomes from two tutorial-style experiments rather than a broad performance study: (i) in a 5-state linear-Gaussian toy problem, the 4D-Var solution recovers the posterior covariance in close agreement with the closed-form expression, with covariance-matrix mismatch RMSE on the order of $\sim 10^{-14}$ (near machine precision). (ii) In sensor-placement OED examples, the relaxed D-optimal formulation is demonstrated for selecting a best pair among 5 candidate sensors, and a stochastic learning binary OED formulation is demonstrated for selecting 4 sensors out of 10 in an advection–diffusion PDE case; in the latter, the stochastic approach yields a solution close to the brute-force global optimum while using far fewer evaluations than enumerating all $2^{10}=1024$ designs. Numeric plots show objective improvement over optimization iterations in both cases, but detailed ARL/efficiency tables are not the focus of the article.","The authors state that the current version of PyOED is intended to be extensible rather than scalable, and that the main limitation of the initial version is scalability due to the lack of parallelization support. They note that future versions aim to add MPI support (e.g., via mpi4py) and support PETSc, and potentially rewrite performance-critical components in Cython.","Because the paper is primarily a software description with selected tutorials, it provides limited systematic comparison against other OED/DA software in terms of computational cost, scalability, or numerical accuracy across a diverse benchmark suite. The showcased OED results are largely illustrative; there is no comprehensive sensitivity analysis on hyperparameters (e.g., sparsity penalty weights, baselines in stochastic learning) or robustness to model misspecification, correlated/non-Gaussian noise, or strong nonlinearity beyond Laplace approximations. Practical adoption may also depend on interfaces to large external PDE solvers and adjoint/AD toolchains; while extensibility is emphasized, end-to-end performance on truly large-scale PDE inverse problems is not demonstrated in this paper.","The authors propose extending PyOED with additional DA methods (e.g., variants of Kalman filtering/smoothing, particle filtering, and hybrid variational-ensemble methods) and adding relevant optimization methods such as branch-and-bound for mixed-integer OED. They also state plans to address scalability by adding MPI support (e.g., mpi4py) and PETSc support, and improving performance by converting/rewriting suitable portions in Cython. They additionally note interest in incorporating advances in automatic differentiation and faster evaluation of previously intractable utility criteria.","A natural next step is to provide standardized, reproducible benchmark suites (models, priors/noise, candidate designs) with quantitative metrics (runtime, memory, gradient/adjoint counts, objective gaps) to compare PyOED methods and competing libraries fairly. More work could also focus on rigorous treatment of nonlinearity and non-Gaussian posteriors in OED (beyond Laplace), including scalable EIG estimators, nested Monte Carlo control variates, and posterior sampling-based design for strongly nonlinear PDEs. Additional practical enhancements would include turn-key interfaces to common PDE toolchains (adjoint generation, checkpointing) and robust handling of correlated/heteroskedastic observation errors and model discrepancy within the OED loop.",2301.08336v3,https://arxiv.org/pdf/2301.08336v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:30:42Z
FALSE,NA,NA,Not applicable,Not specified,Network/cybersecurity|Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,MATLAB,Not provided,https://www.mathworks.com/help/pdf_doc/supportpkg/xilinxzynqbasedradio/xilinxzynqbasedradio_ug.pdf|https://wiki.analog.com/resources/eval/user-guides/ad-fmcomms3-ebz|https://www.xilinx.com/support/documentation/boards_and_kits/zc706/ug954-zc706-eval-board-xc7z045-ap-soc.pdf|https://www.avnet.com/wps/portal/us/products/avnet-boards/avnet-board-families/zedboard|https://www5.epsondevice.com/en/products/crystal_unit/tsx3225.html|https://www.analog.com/media/en/technical-documentation/data-sheets/AD9361.pdf|https://www.ti.com/lit/an/swra554a/swra554a.pdf|https://ditu.amap.com/,"This paper develops a practical frequency-hopping MIMO dual-function radar-communication (DFRC/JRC) scheme that explicitly models and mitigates real transceiver imperfections: sampling timing offset (STO), carrier frequency offset (CFO), and frequency-dependent front-end errors (FEE). It proposes radar-centric waveform modifications (two design constraints) and a low-complexity receiver algorithm to jointly estimate and compensate these impairments at an unsynchronized communication receiver while preserving radar sensing performance. The approach is validated via Monte Carlo simulations (radar estimation RMSE and communications BER) and over-the-air experiments using low-cost SDR hardware (Xilinx Zynq + ADI FMCOMMS3), demonstrating that the modified waveforms have minimal impact on radar imaging while materially improving communications constellation quality when front-end frequency dependence is accounted for. Additional indoor SDR experiments measure BER versus receive gain/SNR under controlled conditions, showing trends consistent with simulation. Overall, the work advances practical FH-MIMO DFRC by moving beyond ideal synchronization assumptions and providing an experimentally validated impairment-aware design.","The FH-MIMO hop signal is modeled as complex exponentials per hop/antenna $s_m(t)=e^{-j2\pi f_{hm} t}$ (Eq. 1), with orthogonality constraints across antennas per hop (Eq. 2). With STO/CFO, the received hop signal includes a phase rotation $e^{j(\omega_{ihm}+\Delta\omega)(t+\Delta t_{ih})}$ and front-end gain $\beta_{ihm}(\omega_{ihm})$ (Eq. 5), leading to a Fourier-domain expression (Eq. 6) that separates contributions of CFO, STO accumulation, and front-end/channel terms. CFO is estimated from ratios of zero-frequency pilots across consecutive PRTs (Eq. 11), STO from estimated clock stability (Eqs. 12–13), and PSK symbols from compensated ratios using $D$ (Eq. 16) under waveform Designs 1–2.","Simulations with 50 random targets show the modified DFRC waveforms yield essentially the same radar estimation RMSE as a traditional FH-MIMO radar across SNR (Fig. 5), supporting the claim of minimal sensing degradation. Communications simulations provide BER curves for FHCS and PSK (8PSK/16PSK) and show longer hop duration improves BER, while FHCS BER is insensitive to modulation order (Fig. 6). Over-the-air results show non-negligible CFO on the order of about 9–10 kHz that varies slowly over a CPI (Fig. 8), and constellation plots demonstrate that neglecting frequency-dependent transceiver gains substantially worsens demodulation, while the proposed method improves clustering and further averaging improves it again (Fig. 9). Indoor experiments collect 800 CPIs (1,024,000 demodulations) and produce BER-versus-gain trends consistent with simulation (Fig. 12).",None stated.,"The work is not a DOE/experimental-design contribution; experiments are demonstrations without formal design-of-experiments planning (e.g., randomized run order, factor screening, replication strategy, or uncertainty quantification across environmental conditions). Over-the-air experiments appear limited to specific hardware (ZC706/ZedBoard + FMCOMMS3) and a small set of scenarios (one outdoor setup plus an indoor controlled link), which may limit generalizability to other RF front-ends, mobility, or stronger multipath/interference. Performance reporting relies heavily on constellation plots and BER curves without detailed confidence intervals or sensitivity analysis to parameter choices (e.g., $K,H,T_p$, averaging windows).",None stated.,"A useful extension would be a self-starting/online version that tracks time-varying CFO/STO/front-end drift across longer durations and under mobility, with robustness to nonstationary channels. More extensive real-world validation across diverse propagation environments (NLoS, multipath-rich, interference) and with multi-antenna UEs would clarify operational limits and scaling. Releasing reference MATLAB/FPGA implementation code (or an SDR reproducibility package) would improve adoption and enable benchmarking against alternative DFRC synchronization/impairment compensation methods.",2301.11501v1,https://arxiv.org/pdf/2301.11501v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:31:20Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Other,Variable/General (linear bandit feature dimension $d$; actions have features $x(a)\in\mathbb{R}^d$),Theoretical/simulation only|Other,Simulation study,TRUE,None / Not applicable,Supplementary material (Journal/Publisher),NA,"The paper studies optimal data collection (experimental design) for online policy evaluation in linear bandits when reward noise is heteroscedastic with variance $\sigma^2(a)=x(a)^\top\Sigma^*x(a)$. It derives an exact MSE expression for a weighted-least-squares (WLS) value estimator and defines a novel convex “PE-Optimal” design (behavior policy) that minimizes the policy-value MSE, which differs from standard A/D/E/G-optimal criteria. Because $\Sigma^*$ is typically unknown, the authors propose SPEED (Structured Policy Evaluation Experimental Design), an explore-then-commit method that first estimates $\Sigma^*$ (via OLS residuals and a covariance regression) and then tracks the oracle PE-optimal design. They provide finite-sample regret bounds of SPEED relative to the oracle design, showing $\tilde O(d^3 n^{-3/2})$ regret and a matching-in-$n$ lower bound $\Omega(d^2 n^{-3/2})$. Simulations and experiments on synthetic setups and several real datasets (e.g., MovieLens, UCI Red Wine Quality, UCI Air Quality) show SPEED achieves MSE close to the oracle and substantially better than on-policy sampling and A-/G-optimal baselines.","The WLS estimator is defined by $\hat\theta_n=\arg\min_\theta\sum_{t=1}^n \frac{1}{\sigma^2(a_t)}(r_t-x(a_t)^\top\theta)^2$. The (heteroscedastic) design/information matrix for a behavior policy $b$ is $A_{b,\Sigma^*}=\sum_{a\in\mathcal A} b(a)\,\frac{x(a)}{\sigma(a)}\frac{x(a)^\top}{\sigma(a)}$. The policy-evaluation MSE (loss) is characterized as $L_n(\pi,b,\Sigma^*)=\frac{1}{n}\sum_{a,a'} w(a)^\top A_{b,\Sigma^*}^{-1} w(a')$ with $w(a)=\pi(a)x(a)$, and the PE-optimal design is $b^*=\arg\min_b L_n(\pi,b,\Sigma^*)$.","The oracle strategy that knows $\Sigma^*$ and samples according to the PE-optimal design achieves loss scaling on the order of $\tilde O(d/n)$ (up to problem-dependent factors). SPEED, which estimates $\Sigma^*$ after an exploration phase of length $\Gamma=\sqrt n$ and then optimizes the empirical PE-optimal design, has regret relative to the oracle of order $\tilde O(d^3/n^{3/2})$ (informally stated as $O(d^3\log n/n^{3/2})$ with additional dependence on $\sigma_{\min},\sigma_{\max}$). A lower bound is proved for a related regret notion: any algorithm must incur at least $\Omega(d^2\log n/n^{3/2})$ in a specified hard instance, matching the $n^{-3/2}$ rate. Empirically, SPEED’s MSE decreases faster than on-policy sampling and standard A-/G-optimal design baselines and is close to an oracle that knows $\Sigma^*$.",None stated.,"The method relies on a specific quadratic-in-features variance model $\sigma^2(a)=x(a)^\top\Sigma^*x(a)$ with bounded eigenvalues and sub-Gaussian noise; performance may degrade under model misspecification or heavy-tailed noise. SPEED is explore-then-commit with a fixed exploration length (e.g., $\Gamma=\sqrt n$) and depends on an optimization “approximation oracle” to solve the PE-optimal design, which may be nontrivial for very large action sets. The paper’s empirical section mentions code in supplementary material but does not specify a standard software stack in the main text, which can hinder reproducibility without the supplement.","The conclusion states an intent to extend the results to harder settings, specifically data collection to minimize the MSE for multiple target policies (multi-policy evaluation).","Extend the design and SPEED to contextual or sequential decision processes beyond bandits (e.g., linear MDPs) and to settings with autocorrelated/nonstationary rewards. Develop robust/self-normalized versions that handle unknown feature norms, heavy-tailed noise, or misspecified heteroscedastic structure. Provide efficient, scalable solvers (and open-source implementations) for computing PE-optimal designs when $|\mathcal A|$ is very large or continuous, potentially via stochastic/online convex optimization or Bayesian design variants.",2301.12357v3,https://arxiv.org/pdf/2301.12357v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:31:56Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Other,Other,Variable/General (protocol designs d0–d5; models with 9 parameters (Beattie) and 15 parameters (Wang)),Healthcare/medical|Other,Simulation study,TRUE,Python|Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher),https://github.com/CardiacModelling/empirical%20quantification%0Aof%20model%20discrepancy|https://doi.org/10.5281/zenodo.8409925,"The paper proposes an empirical uncertainty-quantification approach for predictive uncertainty arising from model discrepancy by training the same mechanistic model on data generated under multiple experimental designs (electrophysiology voltage-clamp protocols) and forming an ensemble of fitted parameter sets. For a validation protocol, the spread of predictions across the ensemble is used as a heuristic prediction band intended to reflect model-discrepancy-driven uncertainty, even for unseen protocols. The method is demonstrated on synthetic voltage-clamp data for hERG/IKr ion channel kinetics using two Markov models (Beattie et al. and Wang et al.), under two discrepancy scenarios: a misspecified maximal conductance and misspecified model dynamics (wrong model structure). Results show that when discrepancy is present, parameter estimates become strongly protocol-dependent despite very low within-protocol variability under repeated noise realizations, and the ensemble spread tends to widen and highlight regions of complex dynamics where discrepancies are largest. The approach is positioned as a practical tool to compare candidate models and to motivate discrepancy-aware experimental design criteria (e.g., selecting new protocols that maximize ensemble prediction spread).","Training per protocol is defined by least-squares/RMSE minimization: \(\hat\theta_d = \arg\min_{\theta\in\Theta} \mathrm{RMSE}(y(\theta;d), z(d))\) with additive IID Gaussian observation noise. Given a set of training protocols \(D_{train}\), an ensemble of predictions for a validation protocol \(\tilde d\) is \(\{y(\hat\theta_d;\tilde d): d\in D_{train}\}\); the proposed prediction band is the pointwise min–max envelope \(B^{(i)}=[\min_{d\in D_{train}} y_i(\hat\theta_d;\tilde d),\, \max_{d\in D_{train}} y_i(\hat\theta_d;\tilde d)]\), with midpoint \(B^{(i)}_{mid}=(B^{(i)}_{lower}+B^{(i)}_{upper})/2\). In Case I, discrepancy is induced by fixing maximal conductance to \(g=\lambda g^*\) and optimizing over the restricted space \(\Theta_\lambda\): \(\hat\theta_{\lambda}(d)=\arg\min_{\theta\in\Theta_\lambda}\mathrm{RMSE}(y(\theta;d),z(d))\).","In synthetic studies with 10 kHz sampled current data and IID Gaussian noise (e.g., \(\sigma=0.03\) nA), within-protocol parameter variability under repeated noise draws is reported to be negligible compared with between-protocol differences under discrepancy. For Case I (misspecified conductance), the summary table shows the ensemble band mean width increasing from about \(2.1\times10^{-4}\) nA at \(\lambda=1\) to \(\sim 4.3\times10^{-2}\)–\(7.4\times10^{-2}\) nA when \(\lambda\) moves to 4 or 0.25, and the midpoint RMSE rising from \(\sim 3.0\times10^{-2}\) nA at \(\lambda=1\) to \(\sim 5.9\times10^{-2}\) nA (\(\lambda=4\)) or \(\sim 1.6\times10^{-1}\) nA (\(\lambda=0.25\)). For Case II (misspecified dynamics), the discrepant Beattie model yields a much wider band (mean width \(\sim 7.5\times10^{-2}\) nA) and lower coverage (\(\sim 34\%\)) than the correctly specified Wang model (mean width \(\sim 7.0\times10^{-4}\) nA; coverage \(\sim 87\%\)), with midpoint RMSE about \(1.1\times10^{-1}\) nA vs \(3.0\times10^{-2}\) nA, respectively. The prediction band is shown to be narrow in near-zero-current regions and widest around sharp current transients (e.g., action-potential-like spikes), aligning with where model structural differences most affect dynamics.","The authors state that the min–max prediction band is only a heuristic for discrepancy-driven uncertainty and provides no guarantee that the true (noise-free) data-generating process lies within the interval; in their examples, the DGP often lies outside the band. They note that using more training protocols may improve coverage, but practical constraints limit how many protocols can be run on a single cell. They also acknowledge potential mismatch in assumptions, including that the true system may not be well-described by deterministic ODEs (stochastic channel effects may require SDEs) and that IID Gaussian observation noise may be inaccurate (autocorrelation and experimental artifacts may occur).","The proposed band uses an extreme-envelope (min–max) across a small set of protocols, which can be sensitive to outlier fits or optimization failures and does not provide calibrated frequentist coverage or Bayesian credible levels. The demonstrations are largely on synthetic data with known noise level and high sampling rate, so performance under realistic phase I uncertainty (unknown noise parameters, parameter drift, cell-to-cell variability, and experimental nonstationarities) is not established. The method’s dependence on the chosen protocol library (Dtrain) and on protocol similarity to intended contexts of use could limit generalizability; without principled selection of Dtrain, the band may under- or over-state uncertainty. Comparisons to alternative discrepancy/UQ approaches (e.g., GP discrepancy models, ABC) are discussed conceptually but not benchmarked quantitatively on the same tasks.","The authors suggest using cross-protocol predictive error and ensemble spread to compare and select among candidate models for a given context of use. They propose developing discrepancy-aware experimental design criteria by choosing new protocols that maximize the ensemble prediction spread (Equation 31), rather than relying on optimal design methods that assume a correctly specified model. They also propose that in settings with fewer observations or higher noise, it may be preferable to use a distribution-based approach by propagating Bayesian posteriors per protocol instead of point estimates.","A natural extension is to replace the min–max envelope with calibrated statistical summaries (e.g., percentile bands from posterior predictive distributions across protocols) and to study coverage properties under protocol set size and optimization uncertainty. Incorporating explicit protocol-distance metrics or clustering could help choose representative training protocol subsets and avoid redundancy, improving computational efficiency and interpretability. Testing on real hERG patch-clamp datasets with realistic artifacts (leak, capacitance transients, drift) and autocorrelated noise would better validate practical utility. Finally, integrating multi-objective DOE (information gain + discrepancy detection) could formalize the proposed ‘maximize spread’ idea while controlling experimental cost and identifiability.",2302.02942v2,https://arxiv.org/pdf/2302.02942v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:33:05Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Prediction|Parameter estimation|Cost reduction|Other,Other|Not applicable,Variable/General (parameter dimension $d_u$; examples include $d_u=2$ and $d_u=10$; also a 1D illustrative example),Environmental monitoring|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper introduces Gaussian process (GP) regression as a surrogate modeling tool for computationally expensive Bayesian inverse problems, focusing on approximating either the forward map $G(u)$ or the negative log-likelihood $\Phi(u)$. It derives new bounds showing that the discrepancy between the true posterior and the GP-accelerated approximate posterior can be controlled by the GP approximation error measured in a posterior-weighted $L^2_{\mu_y}$ norm, rather than a prior-weighted norm. This leads to new experimental design guidance: training (design) points for the GP surrogate should be concentrated in regions of high posterior mass, since errors elsewhere have limited impact on the posterior approximation. The paper further provides convergence results linking posterior-weighted surrogate error to local fill distance, supporting sampling-based and sequential/adaptive design strategies (including approaches that update design points using approximate posteriors). Simple numerical experiments illustrate how matching (or slightly inflating) the design measure relative to the posterior reduces error for fixed budgets of expensive model evaluations.","Bayesian posterior is $d\mu_y/d\mu_0(u) \propto \exp(-\Phi(u))$, with GP surrogate either for $\Phi$ or for $G$ leading to $\Phi_N(u)=\tfrac12\|y-G_N(u)\|^2_{\Gamma}$. Mean-based GP posterior uses $\exp(-\mathbb{E}[\Phi_N(u)])$; marginal GP posterior uses $\mathbb{E}[\exp(-\Phi_N(u))]$. New bounds control Hellinger error by posterior-weighted surrogate error, e.g. (specialized in Corollary 1) $d_{Hell}(\mu_y,\mu^{mean}_{y,N})\lesssim \|\Phi-m^{\Phi}_N\|_{L^2_{\mu_y}}$ (and marginal adds a term involving GP predictive variance $k_N(u,u)$), motivating posterior-targeted design points.","Key theoretical result: Hellinger distance between the true posterior and GP-accelerated posteriors is bounded by posterior-weighted GP approximation errors (Theorems 1–2; Corollaries 1–2), implying training accuracy is primarily needed where $\mu_y$ places mass. Theorem 3 links expected posterior-weighted surrogate error to local fill distance and shows near-optimal convergence rates in $N$ can be retained even if design points ignore low-posterior-mass regions, provided points are sufficiently dense where posterior density is high. Numerical experiments (1D) show the posterior-weighted error $e(N,\nu)=\int \mathbb{E}_\nu|m^f_N(u)-f(u)|^2\,\mu(du)$ decreases with $N$ and is minimized when the design distribution resembles (or slightly inflates the tails of) the posterior. A computational-cost illustration reports large speedups: for a subsurface PDE example, one forward solve is about $2.6\times 10^{-1}$s while GP mean evaluation is about $3.6$–$5.6\times 10^{-5}$s, and GP coefficient computation is about $3.2\times10^{-3}$–$1.2\times10^{-2}$s; the GCM example notes reducing required forward evaluations from $O(10^5)$ to $O(10^2)$ using GP emulation.","The paper notes a practical circularity: the theory suggests choosing GP training points based on the (true) posterior, but the posterior is the object being approximated, so the design must be implemented via approximate/sequential strategies. In the numerical illustration of Theorem 3–type behavior, the authors explicitly acknowledge that the example does not satisfy all theorem assumptions (e.g., the parameter space $U$ is unbounded and there is no a priori truncation via $U_N$), though the observed behavior matches predictions. They also state that assumptions in the main convergence theorems need to be checked case-by-case for a given surrogate/noise model.","Most design guidance is theoretical and depends on regularity/RKHS assumptions (e.g., $f\in H_k(U)$ / Sobolev smoothness and fill-distance behavior) that may not hold for nonsmooth or chaotic forward models, potentially weakening practical guarantees. The work focuses on scalar emulation or component-wise emulation of vector outputs and notes (without developing) that modeling cross-output correlations can materially improve posterior approximations; the DOE implications for multi-output GP design are therefore incomplete. Practical design criteria are framed in terms of posterior-weighted $L^2$ errors and fill distance rather than implementable acquisition functions under realistic constraints (e.g., expensive hyperparameter learning, model discrepancy, simulator stochasticity, and high-dimensional $d_u$ where fill-distance rates degrade).","The introduction points to practical methods to resolve the circular design issue, referencing sequential design strategies requiring only an approximate posterior and schemes that update training points while exploring the posterior via MCMC; this positions sequential/posterior-adaptive design as a natural direction for implementation. The paper also highlights motivation to place calibrate-emulate-sample methodologies on firmer theoretical foundations, indicating further development/analysis of posterior-targeted design strategies for GP surrogates in Bayesian inverse problems as a continuing research direction.","Develop explicit, implementable acquisition functions that directly approximate the paper’s posterior-weighted error objectives (e.g., criteria targeting $\|\Phi-m_N\|_{L^2_{\mu_y}}$ or Hellinger/KL proxies) under unknown posterior and learned GP hyperparameters. Extend the DOE theory to correlated multi-output GPs (co-kriging) and to settings with simulator noise/model discrepancy, where design must trade off observation noise, emulator uncertainty, and bias. Provide scalable design strategies for high-dimensional parameter spaces (e.g., active subspaces, gradient-informed designs, or sparse/inducing-point GPs) and validate on more realistic large-scale inverse problems with publicly reproducible code/software.",2302.04518v1,https://arxiv.org/pdf/2302.04518v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:33:42Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction|Other,Other,"Variable/General (m items, n buyers; design variables are assignment probabilities x_{ij} for item–buyer pairs)",Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://doi.org/10.1145/3543507.3583528,"The paper proposes a near-optimal experimental design for A/B testing on two-sided online platforms when buyers have hard budget constraints that make standard Bernoulli randomization infeasible. The design is formulated as an optimization over an “experiment matrix” X of independent item-level assignment probabilities, with feasibility enforced via budget constraints in expectation and a post-processing “throttling” modification (sequential or random) to ensure realized allocations satisfy budgets. The estimator targets the total treatment effect (difference in total utility between two allocation algorithms) using an inverse-probability-weighted form that replaces intractable inclusion probabilities p_{ij} with design probabilities x_{ij}, yielding small bias under random throttling and asymptotically vanishing average bias under stated conditions. The offline design is obtained by convex optimization to minimize an upper bound on MSE (equivalently controlling variance with limited bias), and an online variant recomputes proportional subproblems as items arrive. Simulations and a Tencent advertising dataset show the optimized design improves over a throttled Bernoulli baseline, with roughly ~20% lower variability/MSE in large-scale regimes and favorable behavior as budget-to-cost and supply-demand ratios increase.","Total treatment effect: $\tau = \sum_{i,j} u_{ij} w^1_{ij} - \sum_{i,j} u_{ij} w^0_{ij}$. Observation matrix under realized allocation $W$: $o_{ij}=w_{ij}u_{ij}$. Proposed practical IPW estimator uses design probabilities: $\hat\tau(O)=\sum_{i,j} \frac{o_{ij} w^1_{ij}}{x_{ij}}-\sum_{i,j} \frac{o_{ij} w^0_{ij}}{x_{ij}}$ (motivated by Horvitz–Thompson with true inclusion probs $p_{ij}$, but replacing $p_{ij}$ by $x_{ij}$ due to throttling). Offline optimal design solves a convex program: minimize $\sum_{i,j} (\mu_{ij}^2+\sigma_{ij}^2)(w^1_{ij}+w^0_{ij})/x_{ij}$ subject to $\sum_j x_{ij}\le 1$, $\sum_i x_{ij}c_{ij}\le b_j$, and $0<x_{ij}\le 1$.","The paper proves that when budgets are sufficiently large (no throttling), the estimator $\hat\tau$ is unbiased (Proposition 4.1) and provides an explicit variance expression (Proposition 4.4). Under bounded costs and random throttling, for expected-budget-satisfying designs the average bias vanishes as the number of items per buyer grows, with an explicit rate upper bound of order $O(m_k^{-1/3})$ (Theorem 4.3). In synthetic experiments (many trials across randomly generated instances), the optimized design achieves about a 20% reduction in standard deviation (and thus improved MSE) versus a throttled Bernoulli randomization baseline in large supply-demand regimes. On a Tencent ads dataset (~14k items, ~6k buyers), the optimized experiment shows substantially lower relative standard deviation than the Bernoulli baseline while incurring only small relative bias, yielding lower overall MSE in practice.","Computing the true inclusion probabilities $p_{ij}$ under throttling is intractable, so the practical estimator replaces $p_{ij}$ with $x_{ij}$, introducing bias when budgets bind. The online algorithm has time complexity $O(m)$ times the offline solver cost, which can be prohibitive at large scale; in the real-data study they therefore use the no-budget approximation solution ($X_2^*$) instead of running the full online algorithm. The paper also notes that conventional Bernoulli randomization is infeasible under budget constraints and thus uses a modified (throttled) Bernoulli as a benchmark rather than the standard design.","The design assumes independent item-level assignment (product-form distribution over allocations) before throttling; this restricts the design space and may be suboptimal compared to correlated/randomized designs that directly enforce budgets without post-hoc throttling. The method relies on accurate prior estimates of $\mu_{ij}$ and $\sigma_{ij}^2$ (or a model for them) and fully known costs $c_{ij}$; misspecification could degrade optimality and bias/variance guarantees. Interference, dynamics (learning effects), and feedback loops common in two-sided marketplaces are largely abstracted away (utilities are treated as fixed draws observed only when allocated), which may limit causal validity in real platforms with strategic/competitive responses.","The authors suggest extending the model to settings where budgets can increase/recover over time (e.g., room inventory returning to vacancy), requiring more advanced online algorithms for variable budgets. They also propose extending beyond the one-to-many model to many-to-many two-sided markets, potentially imposing budget constraints on both sides and/or conducting two-sided experiments. Another stated direction is to study experimental design when users are strategic and can affect costs and allocations, connecting to interference effects.","Developing designs that incorporate correlated randomization or direct constrained sampling (rather than independent $x_{ij}$ plus throttling) could reduce bias and improve efficiency under tight budgets. A self-starting or adaptive approach to learn/update $\mu_{ij}$ and $\sigma_{ij}^2$ online (bandit-style or Bayesian experimental design) would make the optimization more robust when prior moments are uncertain. Providing open-source reference implementations and scalable solvers (e.g., decomposition, dual methods) would help adoption on very large platforms, especially for the online recomputation setting.",2302.05005v1,https://arxiv.org/pdf/2302.05005v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:34:18Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Optimal design|Other,Prediction|Parameter estimation|Optimization|Other,Other,Variable/General (examples: 2 factors; PDE example uses d=4 KL terms),Energy/utilities|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper formulates failure probability estimation with expensive computer models as a sequential design of computer experiments aimed at learning the limit-state boundary g(x)=0 efficiently. A deep neural network surrogate is trained for the limit-state function, and a Bayesian-style “posterior distribution of the limit state” is defined as p(\tilde z\mid D) \propto \exp(-|G(x)|/\lambda), where G is the surrogate. Normalizing flows are used to learn a transport map from a standard Gaussian to this posterior, enabling efficient sampling of candidate design locations without solving a difficult global optimization problem. Three adaptive DOE criteria are proposed: NFBD (sample from the learned limit-state posterior), NFBD-FG (adds a minimum-distance/space-filling acceptance rule with a fixed generalization threshold), and NFBD-AG (uses an adaptive threshold based on the input density p(x) to vary generalization tolerance across the space). Performance is demonstrated on two 2D benchmark problems and a 4D Darcy-flow PDE example, showing convergence of estimated failure probabilities and improved efficiency compared with Latin hypercube sampling under the same simulation budget.","Failure probability is defined as $P=\mathbb P(g(x)\le 0)=\int I_{g(x)}p_X(x)\,dx$ with indicator $I_{g(x)}=\mathbf 1\{g(x)\le 0\}$, and estimated by Monte Carlo $\hat P=\frac{1}{n}\sum_{i=1}^n I_{g(x_i)}$. The method defines a surrogate-based limit-state posterior $p(\tilde z\mid D)=p(G(x)=0\mid D)\propto \exp(-|G(x)|/\lambda)$ and uses normalizing flows with change-of-variables $q(\tilde z)=q(z_0)\left|\det \frac{\partial f}{\partial z_0}\right|^{-1}$ to map $z_0\sim\mathcal N(0,I)$ to $\tilde z=f(z_0)$. The NFBD-FG/AG criteria use a spacing distance $\rho(z,D)=\min_i\|z-x_i\|_{\infty}$ with acceptance based on a threshold $\epsilon$ (fixed for FG, adaptive for AG via $\epsilon(x)\propto \beta\,p(x)^{-2/d}$).","In the four-branch (2D) benchmark with a budget $N_{\max}=95$, the ground-truth MC estimate is about $2.05\times 10^{-3}$ and the final DNF estimates are $2.13\times 10^{-3}$ (NFBD), $2.18\times 10^{-3}$ (NFBD-FG), and $2.15\times 10^{-3}$ (NFBD-AG), all within <10% relative error and better than LHS ($2.86\times 10^{-3}$) at the same evaluations. For the iso-probability-lines (2D) example, ground truth is about $3.01\times 10^{-3}$ and final estimates are $2.76\times 10^{-3}$ (NFBD), $2.84\times 10^{-3}$ (NFBD-FG), $2.83\times 10^{-3}$ (NFBD-AG), again <10% error and slightly better than LHS ($2.69\times 10^{-3}$) under the same budget. For the Darcy-flow PDE (d=4) case, the reported true value is $4.24\times 10^{-3}$ and the final estimates are $4.22\times 10^{-3}$ (NFBD), $4.88\times 10^{-3}$ (NFBD-FG), and $4.88\times 10^{-3}$ (NFBD-AG), stated to be more accurate than LHS ($5.42\times 10^{-3}$) and within <10% relative error. Qualitatively, NFBD-AG is reported to converge faster and more stably than NFBD, while NFBD-FG stabilizes quickly but can plateau due to its fixed generalization/spacing assumption.",None stated.,"The proposed criteria depend on training a DNN surrogate and a normalizing flow model; performance may be sensitive to architecture choices, training stability, and hyperparameters (e.g., flow capacity, $\lambda$, and threshold rules), but systematic robustness analysis is not reported. The stopping rule is based on relative change in the estimated failure probability and may be unreliable for very rare events or when surrogate bias dominates (apparent in the PDE example where NFBD-AG and NFBD-FG end at the same biased value). Comparisons are mainly against Latin hypercube sampling; broader baselines common in reliability DOE (e.g., GP/SUR, AK-MCS, subset simulation with surrogates) and computational cost accounting (wall-clock, parallelism) are limited or absent.",None stated.,"Provide ablation and sensitivity studies for key design parameters (e.g., $\lambda$, $\epsilon_0$, flow depth) and establish more principled calibration/selection rules. Extend the method to settings with model discrepancy/noisy simulators and to autocorrelated or constrained input spaces, including explicit handling of unknown distribution parameters. Benchmark against established active-learning reliability methods (AK-MCS, SUR variants, GP information gain approaches) under matched compute budgets and add publicly available reference implementations for reproducibility.",2302.06837v1,https://arxiv.org/pdf/2302.06837v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:34:53Z
TRUE,Factorial (full)|Sequential/adaptive|Other,Parameter estimation|Screening|Optimization|Model discrimination|Prediction|Cost reduction|Other,Not applicable,"Variable/General (illustrative examples typically 3 factors; e.g., App, Coaching, Meal; plus time-varying Prompt in MRT examples)",Healthcare/medical,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/d3center-isr/repository-hybrid-trials-QKWHF6125W,"The paper introduces the Hybrid Experimental Design (HED), an experimental approach for developing multi-component interventions where components are delivered/adapted on different timescales (e.g., slow human-delivered coaching and fast mobile prompts). It conceptualizes HEDs as factorial designs with sequential randomizations at multiple timescales and presents three core variants: hybrid factorial-SMART, hybrid factorial-MRT, and hybrid SMART-MRT. For each variant, the authors lay out estimands (main effects and interactions) and corresponding regression models, including handling restricted re-randomization among non-responders (SMART) and repeated micro-randomization with proximal outcomes (MRT). They discuss estimation approaches such as Weight-and-Replicate (W&R) with inverse-probability weighting for SMART components and WCLS for proximal effects in MRT settings with time-varying covariates. An illustrative analysis based on a weight-loss hybrid SMART-MRT uses simulated data mimicking a completed study and demonstrates how HEDs can reveal synergies/burdens across components and message-delivery rates.","For hybrid factorial-SMART, a marginal mean model is specified with coded factors $Z_{11}$ (App), $Z_{12}$ (Coaching), and $Z_{21}$ (Meal for non-responders), including all interactions: $E(Y|X_0,Z_{11},Z_{12},Z_{21})=X_0\gamma_0+\theta_1 Z_{11}+\theta_2 Z_{12}+\theta_3 Z_{11}Z_{12}+\theta_4 Z_{21}+\theta_5 Z_{11}Z_{21}+\theta_6 Z_{12}Z_{21}+\theta_7 Z_{11}Z_{12}Z_{21}$. For hybrid factorial-MRT, distal outcome models incorporate the subject-specific average prompt rate $\bar A_i$ (Model (2)), while proximal outcome models regress $Y_{i,t+\Delta}$ on the micro-randomized treatment $A_{it}$ and interactions with baseline-assigned factors (Model (3)). For hybrid SMART-MRT, analogous distal and proximal models are provided (Models (4)–(5)), including an indicator $C_{it}$ to ensure factors introduced later only affect outcomes after their assignment time.","Using simulated data patterned after a completed weight-loss hybrid SMART-MRT, the proximal (12-hour) self-monitoring model found no statistically significant main effect of message delivery nor significant interactions (all 95% CIs included 0). In the distal (6-month weight loss) model among non-responders, a significant three-way interaction was reported between initial coaching, step-up intensity, and message-delivery rate (Estimate = -14.85; 95% CI [-24.39, -5.32]). The interaction pattern suggested higher message rates could be beneficial when support intensity was lower (coaching + modest step-up) but potentially burdensome when support intensity was higher (coaching + vigorous step-up). The paper emphasizes that HEDs enable estimation of both main effects and cross-timescale interactions by pooling data appropriately across sequential randomizations and decision points.","For simplicity, the paper does not address missing-data issues in detail and notes that no HED-specific guidelines yet exist; it suggests inflation of sample size for expected missingness and considers multiple imputation likely preferable to listwise deletion. It also notes that many presented models are linear with identity link and that other outcome types (binary/categorical) may require different link functions with implications for interpretation and power. The authors caution that more work is needed to understand implications of applying the proposed methods across outcome types and design variations. They also mention that power/sample-size planning for interactions in HEDs is not directly covered by existing tools and may require simulation or new derivations.","The paper frames HEDs broadly but provides limited head-to-head empirical evaluation of alternative analysis methods (e.g., W&R vs other causal estimators) under realistic violations such as nonadherence, delayed effects, or interference; robustness to these issues remains unclear. Practical implementation burdens (operational complexity of multi-timescale randomization, data pipelines, and treatment fidelity in real-world digital interventions) are discussed conceptually but not quantified or systematically evaluated. The illustrative analysis is based on simulated data mimicking a study rather than a full reproducible re-analysis of the actual trial dataset, which limits external validation of the claimed interaction patterns. Finally, while HEDs are described as factorial-like, guidance on choosing factor coding/contrast structures and guarding against multiple-testing/false discoveries in the many interaction queries is not developed.","The authors call for development of sample-size/power planning resources specifically for HEDs, especially for interaction effects that span components/timescales. They identify missing-data handling as an open area, including how to leverage HED structure in imputation models and how to study attrition as an outcome. They propose extensions to accommodate non-continuous outcomes (e.g., binary/count via log/logit links) and to understand resulting implications for power and interpretation. They also suggest methodological extensions for design variants such as varying micro-randomization probabilities across participants and establishing guidelines for adapting analysis methods to these variations.","Developing standardized software (e.g., an R package) that automates design specification, estimation (W&R/WCLS/GEE variants), and reporting for common HED variants would accelerate adoption and reduce implementation errors. Systematic simulation benchmarks under realistic complexities (time-varying availability, treatment noncompliance, delayed/carryover effects, autocorrelation, and informative missingness) would clarify when each estimator is reliable. Extending HED methodology to multivariate outcomes and high-frequency sensor-based measures (with appropriate temporal dependence models) would better match many digital health settings. Finally, formal multiplicity-control strategies and decision-analytic criteria for prioritizing among many possible cross-timescale interactions would strengthen practical use in intervention optimization.",2302.09046v1,https://arxiv.org/pdf/2302.09046v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:35:32Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Model discrimination|Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (intervention targets among d variables; examples include d=2,5,10,20,40,50; constrained multi-target q=k and unconstrained q≤d; batch size B=1,2,5)",Theoretical/simulation only|Food/agriculture|Other,Simulation study|Other,TRUE,Python|R|Other,Public repository (GitHub/GitLab),https://github.com/yannadani/DiffCBED,"The paper proposes DiffCBED, an end-to-end differentiable Bayesian optimal experimental design method for causal discovery that selects a batch of interventions jointly, including both discrete intervention targets (single- or multi-target) and continuous intervention states. It replaces prior greedy batch selection and black-box optimization over intervention states with gradient-based optimization via a learned design policy using continuous relaxations (Gumbel-Softmax for single-target; relaxed Bernoulli/Binary Concrete and subset relaxations for multi-target) and straight-through gradients. The utility optimized is expected information gain (mutual information) for batch designs; the authors develop nested Monte Carlo (NMC) and an importance-weighted variant (IWNMC) that can avoid explicit posterior estimation by using self-normalized importance sampling from a prior/proposal. Experiments on synthetic Erdős–Rényi SCMs and a semi-synthetic DREAM E. coli gene network simulator show improved causal graph recovery metrics (e.g., Expected SHD, expected F1, interventional MMD) compared with random baselines and prior BOED approaches such as SoftCBED and SSGb, in both single-target and multi-target settings. The method is positioned as enabling previously unexplored multi-target-state batch intervention design in causal Bayesian experimental design.","The BOED utility is expected information gain (mutual information): $U(\xi)=I(Y;\Theta\mid\xi)=\mathbb{E}_{p(\theta)p(y\mid\theta,\xi)}[\log p(y\mid\theta,\xi)-\log p(y\mid\xi)]$. For batch design, the objective is the joint MI $I(Y_{1:B};\Theta\mid \xi_{1:B},h_{t-1})$ (Eq. 2). The paper uses a Nested Monte Carlo estimator (Eq. 3) and an importance-weighted/self-normalized variant IWNMC (Eq. 4) to approximate and differentiate the MI objective for optimizing a policy over discrete targets and continuous states.","Across synthetic Erdős–Rényi graphs, DiffCBED consistently outperforms random-fixed/random-random baselines and prior acquisition strategies (e.g., SoftCBED, SSGb) in single-target-state (e.g., d=50, B=5) and multi-target-state (e.g., d=20, B=2) settings, improving Expected SHD, expected F1, and i-MMD over multiple acquisition batches (figures reported with 95% CIs over multiple seeds). In a higher-dimensional constrained multi-target experiment (d=40, q=5, B=2) using IWNMC with a proposal distribution, the reported table shows Random ESHD 43.78±46.67 vs SSGb 15.59±29.66 vs DiffCBED 0.44±0.21, with F1 0.91±0.08 vs 0.97±0.05 vs 0.99±0.00 and i-MMD 0.16±0.07 vs 0.10±0.06 vs 0.07±0.01. On a semi-synthetic DREAM E. coli gene interaction network (d=10), DiffCBED also outperforms the baselines on the same metrics over acquisitions. The bivariate qualitative study shows gradient optimization concentrates design samples in high-EIG regions after training.","The authors state that a primary limitation is the need to estimate a posterior after every acquisition when using the NMC-based approach. They also note that while the IWNMC estimator can avoid posterior estimation, the resulting designs are still non-adaptive (static/batch) rather than adaptive over time.","Performance and stability likely depend on the quality of the approximate posterior/proposal (e.g., DAG-bootstrap/GIES samples); bias or misspecification there could mislead EIG optimization. The importance-weighted approach can suffer from weight degeneracy in higher dimensions (they observe effective sample size issues), which may limit scalability without carefully engineered proposals. The evaluation is largely simulation/semi-simulation; broader real-world validation with actual intervention constraints/costs and model misspecification (non-additive noise, latent confounding, cycles, measurement error, or autocorrelation) is not demonstrated. Implementation complexity (policy optimization, relaxed discrete choices, nested MC) and hyperparameter sensitivity (e.g., temperature schedules, L, batch size) may make deployment nontrivial despite reported robustness in an ablation.","They suggest training a policy to design adaptive experiments (sequential/adaptive BOED) as an interesting direction, since current designs (including those from IWNMC) are non-adaptive. They point to prior work on adaptive experimentation/policy-based design as potential avenues for such extensions.","Extend the framework to handle latent confounders, cyclic causal models, or partial observability, which are common in practical causal discovery. Develop self-starting/online variants that jointly learn/refresh posterior approximations with differentiable structure learning to reduce reliance on non-differentiable external causal discovery (e.g., GIES) and the posterior-sample transfer step. Provide stronger theoretical guidance on variance/bias trade-offs and computational scaling for NMC/IWNMC (e.g., adaptive choice of L, variance reduction, control variates) and on constraints/cost-aware intervention design. Release standardized benchmarks and additional real-domain case studies (e.g., genomics perturbation data with realistic intervention ranges) plus packaged software to improve reproducibility and adoption.",2302.10607v2,https://arxiv.org/pdf/2302.10607v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:36:10Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Optimization|Prediction|Model discrimination|Cost reduction|Other,Bayesian D-optimal|Compound criterion|Other,Variable/General (contexts and actions; includes discrete actions with K categories; examples include up to 5000-dimensional design space),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/microsoft/co-bed,"The paper formulates contextual optimization as a Bayesian experimental design (BED) problem and proposes CO-BED, a model-agnostic framework for designing batches of contextual experiments using an information-theoretic objective. The key design criterion is the contextual max-value expected information gain (CMV-EIG), defined as the mutual information between experimental outcomes and the max achievable rewards in specified evaluation contexts, enabling transductive design (learning about optima in deployment contexts from experiments run in possibly different contexts). Because CMV-EIG is intractable, the authors derive an InfoNCE-based variational lower bound and optimize it jointly with a learned critic network and the experimental designs via stochastic gradients; discrete actions are handled using a Gumbel–Softmax relaxation to enable pathwise gradients. Empirical studies across parametric models, Gaussian process contextual BO (including confounded observational data), contextual linear bandits, and a structural equation model show CO-BED matches or outperforms random, UCB, Thompson sampling, and bespoke model-specific baselines, while scaling to very high-dimensional design spaces (e.g., 5000D).","The proposed design objective is CMV-EIG: $\mathrm{CMV\text{-}EIG}(A;C,C^*)=\mathbb{E}\left[\log \frac{p(y\mid m^*,C,A)}{p(y\mid C,A)}\right]=I(m^*;y\mid C,C^*,A)$, where $A$ are actions for experimental contexts $C$ and $m^*$ are max-values in evaluation contexts $C^*$. They optimize an InfoNCE lower bound $\mathcal{L}(A,U;L)=\mathbb{E}\left[\log\frac{\exp(U(y,m^*_0))}{\frac{1}{L+1}\sum_\ell \exp(U(y,m^*_\ell))}\right] \le \mathrm{CMV\text{-}EIG}$ with critic $U_\phi$. For discrete actions, a Gumbel–Softmax relaxation samples soft one-hot actions with $\pi_{d,k}=\frac{\exp((\log\alpha_{d,k}+g_{d,k})/\tau)}{\sum_j\exp((\log\alpha_{d,j}+g_{d,j})/\tau)}$ to permit gradient-based design optimization.","Across experiments, CO-BED consistently achieves higher estimated CMV-EIG (via the trained InfoNCE bound) and lower deployment regret than generic baselines (random/UCB/Thompson) and is competitive with bespoke alternatives (e.g., LEVI+LP in GP contextual BO; Sampler-Planner in linear contextual bandits). In a continuous-action parametric benchmark (40 experimental contexts), CO-BED reports CMV-EIG $6.527\pm0.003$ versus Thompson $6.184\pm0.004$, and lower regret $0.044\pm0.001$ versus $0.051\pm0.001$. In the linear contextual bandit benchmark, CO-BED outperforms the bespoke Sampler-Planner at small batch sizes and performs on par for larger batches, with learned designs avoiding uninformative actions. The method scales to an unknown-causal-graph setting with designs up to 5000 dimensions and maintains the best regret among compared methods as action dimension increases.","The authors note that CO-BED’s generality increases computational cost because it optimizes a variational MI bound and trains a (small) neural critic network. They also mention an assumption that rewards $y$ are continuous (common in BO/bandits) and suggest future work to lift it, and that computing/approximating conditional max-values $m^*\mid(C^*,\psi)$ efficiently can be challenging. Finally, while experiments focus on one experimentation stage followed by one deployment stage, they state there is no conceptual barrier to multiple adaptive rounds.","Performance depends on the quality/optimization stability of the learned critic and the tightness of the InfoNCE lower bound; a loose bound can yield suboptimal designs even if optimization converges. The Gumbel–Softmax approach can introduce a train–test mismatch (soft actions during training vs. argmax at deployment) and may require careful temperature/annealing choices, especially in large discrete action spaces. Computing $m^*$ relies on analytic solutions, enumeration, or finite action grids; for complex continuous action spaces this approximation may become expensive or biased, potentially limiting real-world scalability. Comparisons are largely simulation-based and may not reflect issues like nonstationarity, interference between units in batch experiments, or constraints/ethics common in real deployments (e.g., marketing/medical).","They propose investigating ways to lift the assumption that outcomes $y$ are continuous and exploring more efficient methods to compute or approximate conditional max-values $m^*\mid(C^*,\psi)$. They also suggest extending beyond the one-shot (single experimentation round followed by deployment) setup to allow multiple, adaptive rounds of experimentation and deployment.","Develop self-normalizing or bias-corrected estimators to better align the optimized InfoNCE bound with true CMV-EIG, especially under soft/hard action mismatches and finite-$L$ contrastive sampling. Extend CO-BED to settings with dependent outcomes (e.g., time series, clustered units, network interference) and incorporate realistic operational constraints (budgets, fairness/ethics, safety) directly into the design objective. Provide theoretical guarantees or diagnostics for critic adequacy/bound tightness and its impact on design optimality, plus ablations on critic architecture sensitivity. Release/maintain reference implementations and benchmarks (e.g., as a PyTorch/Pyro package) and validate on real-world datasets or live experiments to assess robustness under model misspecification.",2302.14015v2,https://arxiv.org/pdf/2302.14015v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:36:54Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Optimization|Other,Other,"Variable/General (general design variable(s) \(\xi\); also sequential designs \(\xi_1,\ldots,\xi_T\))",Healthcare/medical|Finance/economics|Pharmaceutical|Semiconductor/electronics|Environmental monitoring|Theoretical/simulation only|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a Statistical Science review of modern Bayesian experimental design (BED), focusing on information-theoretic utility functions—especially expected information gain (EIG), i.e., mutual information between outcomes and quantities of interest—used to choose experimental designs optimally. It synthesizes recent computational advances that make BED practical, including multilevel Monte Carlo debiasing/unbiased EIG (and gradient) estimators, variational/functional approximations that yield lower/upper bounds on EIG (including settings with implicit likelihoods), and stochastic-gradient optimization schemes for design search. For sequential settings (Bayesian adaptive design), it highlights policy-based approaches—especially Deep Adaptive Design (DAD)—that learn a design policy offline to enable fast, non-myopic adaptive experimentation at deployment time. The review also discusses links to active learning, Bayesian optimization, and reinforcement learning, and emphasizes open problems such as handling discrete design variables and robustness to model misspecification. Overall, it positions BED as a unifying framework for adaptive and optimal data acquisition while clarifying the main bottlenecks (nested estimation and optimization) and modern strategies to overcome them.","The core design objective is the expected information gain (EIG) about a target quantity \(\theta\): \(\mathrm{EIG}_\theta(\xi)=\mathbb{E}_{p(\theta)p(y\mid\theta,\xi)}[\log p(y\mid\theta,\xi)-\log p(y\mid\xi)]\), equivalently the mutual information between \(y\) and \(\theta\) given design \(\xi\). For sequential/adaptive design with history \(h_{t-1}\), the incremental objective is \(\mathrm{EIG}_\theta(\xi_t\mid h_{t-1})=\mathbb{E}_{p(\theta\mid h_{t-1})p(y_t\mid\theta,\xi_t,h_{t-1})}[\log p(y_t\mid\theta,\xi_t,h_{t-1})-\log p(y_t\mid\xi_t,h_{t-1})]\). The policy-based objective for a design policy \(\pi_\phi\) is total EIG over \(T\) steps: \(\mathrm{TEIG}_\theta(\pi_\phi)=\mathbb{E}[\log p(y_{1:T}\mid\theta,\pi_\phi)-\log p(y_{1:T}\mid\pi_\phi)]\).","Not applicable (review paper; no single set of benchmark ARL/efficiency numbers is reported). The paper’s main results are conceptual and methodological: it explains why naive nested Monte Carlo EIG estimators are biased and can converge slowly in total computational cost, and summarizes how multilevel/debiasing methods can recover unbiased estimation with standard Monte Carlo cost–error scaling under stated conditions. It also compiles variational bound formulations that enable consistent/efficient EIG optimization via stochastic gradients (including in implicit-likelihood settings) and summarizes evidence from the cited literature that policy-based BAD/DAD can improve both speed (amortized, near-instant deployment) and design quality (non-myopic behavior) compared with greedy BAD.","As a review, the paper does not present a single implemented method with an explicit experimental limitations section; instead it notes general limitations of BED practice. It emphasizes that BED/BAD can be computationally demanding (especially nested estimation and repeated inference in sequential settings), that gradient-based optimization is challenging with discrete/non-differentiable design variables, and that performance depends strongly on the adequacy of the assumed model/simulator—making model misspecification a potentially severe source of pathology for adaptive design.","Because the article is a broad survey, several factual fields needed for a “factsheet” (e.g., concrete factor counts, specific optimality criteria beyond information gain, and empirical runtime/accuracy comparisons under a common benchmark) are inherently not pinned down and therefore cannot be extracted as definitive values. The review’s coverage of practical software/tooling is limited: there is no consolidated implementation guidance or reproducible codebase, which can hinder translation for practitioners. Additionally, while it discusses discrete-design challenges, it does not provide a unified, practically validated recipe for mixed discrete/continuous, constrained, or safety-critical experimental design settings (common in clinical trials/engineering).","The paper highlights future directions including: scaling policy-based Bayesian adaptive design (e.g., improved architectures/objectives/training to handle higher-dimensional design spaces and longer horizons \(T\)), better handling of discrete design components, and deeper integration/cross-fertilization with Bayesian active learning and reinforcement learning. It calls for more theoretical and empirical understanding of model misspecification in BED/BAD, including identifying failure modes and developing mitigation strategies. It also emphasizes expanding BED to richer models and simulators (implicit likelihoods), leveraging modern computational advances to support larger, more realistic applications.","Develop standardized benchmark suites (with agreed simulators, design constraints, and evaluation metrics) for comparing EIG estimators, optimizers, and policy-based methods across domains, including mixed discrete/continuous designs and safety/ethics constraints. Provide open-source reference implementations (e.g., in Python/JAX/PyTorch) for unbiased MLMC estimators, variational bounds, and DAD/iDAD training pipelines with reproducible experiments. Extend robustness tooling beyond qualitative discussion—e.g., principled distributionally robust BED objectives, posterior predictive checks tailored to adaptive design, and guardrails to prevent “design collapse” under misspecification. Explore hybrid online–offline approaches that fine-tune policies during deployment when the simulator is imperfect or drifting (nonstationary environments).",2302.14545v2,https://arxiv.org/pdf/2302.14545v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:37:18Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Optimization|Prediction|Cost reduction,Bayesian D-optimal|Other,"Variable/General (examples include 2D, 4D, 5D, 6D benchmark functions; and 2D hyperparameter tuning tasks; spacecraft shield design with multiple categorical/continuous variables)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable|Other,Not provided,http://archive.ics.uci.edu/ml,"The paper proposes BO-Muse, a human–AI teaming framework for accelerated experimental design/optimization of expensive black-box objectives. In each iteration (batch), a human expert proposes one experiment and an AI proposes one experiment using Bayesian Optimization (GP-UCB), with the AI’s exploration parameter adaptively boosted to counter human over-exploitation from cognitive entrenchment. The authors provide a theoretical sub-linear regret bound showing BO-Muse converges at a rate faster than either the human or AI alone under mild assumptions, while retaining worst-case GP-UCB-style convergence even if the human behaves arbitrarily. Empirical validation includes simulations on standard optimization benchmarks and real-world human-in-the-loop experiments for SVM and Random Forest hyperparameter tuning, plus an applied spacecraft debris shield design study using expensive LS-DYNA simulations. Overall, the work contributes a practical DOE/active-learning workflow and analysis for combining expert intuition with BO-driven exploration to improve sample efficiency under limited experimental budgets.","BO uses a GP surrogate with posterior mean/variance $\mu_{t-1}(x)$ and $\sigma^2_{t-1}(x)$, and selects the next evaluation by maximizing the GP-UCB acquisition $a_t(x)=\mu_{t-1}(x)+\sqrt{\beta_t}\,\sigma_{t-1}(x)$ (Eq. 1). For misspecification (used to model humans), EC-GP-UCB augments exploration: $a_t(x)=\mu_{t-1}(x)+\big(\sqrt{\beta_t}+\epsilon_t/(\sqrt{t}\,\sigma)\big)\sigma_{t-1}(x)$ (Eq. 3). BO-Muse runs batched recommendations $(\hat x_s,\breve x_s)$ where the human is modeled as EC-GP-UCB and the AI as GP-UCB (Eq. 4), with AI exploration set via $\breve\beta_s=\breve\zeta_s\,\breve\chi_s$ and a heuristic lower bound $\breve\zeta_s\approx 7$ (Eq. 9) to ensure sufficient exploration in teaming.","On synthetic benchmark functions (Matyas-2D, Ackley-4D, Rastrigin-5D, Levy-6D), BO-Muse achieves lower simple regret across iterations than Generic BO (GP-UCB) and simulated-human baselines, averaged over 10 random runs. In human-subject studies on the UCI Biodeg dataset, teams using BO-Muse + Human achieve lower test classification error (simple regret) over 30 iterations than Human-alone and AI-alone (Generic BO) for both SVM (tuning $C$ and $\gamma$ in exponent range $[-3,3]$) and Random Forests (tuning max depth and samples per split). In the spacecraft shield design application (LS-DYNA simulations taking 20–40 CPU hours each), BO-Muse-inspired designs led the expert to a best successful shield at 13.8 kg/m$^2$ (Table 5, ID 29) after identifying earlier feasible solutions around 14.3–14.6 kg/m$^2$ (IDs 9 and 11). Theoretical analysis provides a sub-linear regret bound indicating faster asymptotic convergence than either agent alone under assumptions on human model gap decay.","The authors note that the assumed human “cognitive entrenchment” (over-exploitation) may not hold in all cases; if not, BO-Muse may over-compensate by increasing AI exploration and reduce sample efficiency. They also acknowledge that some experts may fail to improve their internal model with more observations, making expert suggestions less useful, and that human behavior may deviate from the GP-UCB-style model used for analysis. In the worst case, every second (human) experiment could be effectively wasted, though convergence remains sub-linear because the AI GP-UCB component still drives optimization and additional observations can only help the AI model.","The method’s empirical validation is limited in scale for human-in-the-loop studies (small participant pool) and includes an application case study with only one domain expert, making generalization uncertain. The approach presumes the human can reliably propose feasible experiments within constraints and that running both human and AI suggestions per batch is affordable; in some real laboratories, doubling evaluations per iteration may be prohibitive. Performance may be sensitive to kernel choice/hyperparameter estimation and to the heuristic exploration multiplier (e.g., $\breve\zeta\approx 7$), yet robustness across noise levels, constraints, and higher-dimensional mixed-variable spaces is not comprehensively benchmarked. The paper does not provide implementation code, which limits reproducibility of the interactive workflow and parameter-setting details.",None stated (no explicit future work section beyond general remarks about scope and limitations).,"Develop and release a reference implementation (including the expert interface) to enable reproducible studies and broader adoption. Extend BO-Muse to constrained and safety-critical experimental design (e.g., feasibility constraints, safe BO), multi-objective settings (performance vs. cost/weight), and multi-expert/multi-agent teaming where experts disagree. Study robustness to nonstationarity, correlated/heteroscedastic noise, and discrete/mixed decision variables common in real DOE, and evaluate alternative exploration-control schemes learned online rather than using a fixed heuristic multiplier. Conduct larger controlled user studies to quantify when and how AI ‘muse’ suggestions influence expert decisions and outcomes across domains.",2303.01684v2,https://arxiv.org/pdf/2303.01684v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:37:56Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Model discrimination|Parameter estimation|Cost reduction|Other,Other,Variable/General (design variables depend on task; examples include 1 continuous factor in toy example; 1 factor (lag time) in memory retention; 2 factors (signal strength and number of observations) in signal detection; 4 factors (lottery probabilities) in risky choice),Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/AaltoPML/BOSMOS,"The paper proposes BOSMOS, an online (sequential), simulator-based Bayesian optimal experimental design method for selecting among competing cognitive models with intractable likelihoods. BOSMOS adaptively chooses the next experiment/design by Bayesian optimization of a new simulator-based utility objective that targets model discrimination by maximizing behavioral variation across model configurations under current posterior beliefs. For inference, it combines likelihood-free parameter updating (via BOLFI with a GP surrogate of a discrepancy) with a new likelihood-free approximation of each model’s marginal likelihood to enable Bayesian model selection. The approach is evaluated in simulated experiments on three cognitive-science paradigms (memory retention, sequential signal detection, and risky choice) and compared to ADO and LFI baselines such as MINEBED, showing similar selection accuracy while requiring orders of magnitude less computation time and substantially fewer simulations.","Design selection uses a simulator-based utility (Eq. 5): choose $d_t$ to minimize $\mathbb{E}_{q_t(\theta_m,m|D_{1:t-1})}[\hat H(x'_t\mid d_t,\theta_m,m)]-\hat H(x_t\mid D_{1:t-1},d_t)$, where $\hat H$ is a kernel Monte Carlo entropy estimate and $q_t$ is a particle posterior. Likelihood-free parameter inference approximates $p(\theta_m\mid x_t)\propto L_{\epsilon_m}(x_t\mid\theta_m)p(\theta_m)$ with $L_{\epsilon_m}(x_t\mid\theta_m)\approx\mathbb{E}[\kappa_{\epsilon_m}(\rho(x_{\theta_m},x_t))]$ (Eqs. 6–7) using a GP surrogate of discrepancy $\rho$. Model selection uses a separate marginal-likelihood approximation (Eq. 9): $L(x_t\mid m,D_{t-1})\propto \mathbb{E}\,\kappa_\eta(\hat\rho(x_\theta,x_t))$ with Gaussian kernel bandwidth $\eta$, followed by Bayesian updates for $q(m\mid D_t)$ and $q(\theta_m\mid m,D_t)$ (Eqs. 10–12).","In simulated studies across memory retention, sequential signal detection, and risky choice, BOSMOS achieves model-selection performance comparable to likelihood-based ADO where tractable likelihoods exist, while operating in a likelihood-free setting. The paper reports that BOSMOS can reach similar accuracy to existing methods while taking up to two orders of magnitude less time than LFI alternatives (e.g., MINEBED) in the presented case studies. Reported runtime summaries (Table 5) show BOSMOS completing 100 design trials in tens of minutes (e.g., ~35.6 min for memory retention; ~73.4 min for signal detection; ~88.3 min for risky choice), while MINEBED takes thousands of minutes in the same tasks. BOSMOS also uses far fewer simulations per design trial than MINEBED (e.g., BOSMOS uses ~1500 for design selection plus ~100 for inference per iteration, vs. MINEBED using 5000 simulations per training per trial).","The authors note that improved behavioral imitation does not necessarily imply convergence to the ground-truth model/parameters, often due to poor identifiability in the model–parameter space. They also state a potential consistency issue: selecting only the most informative designs can bias posterior estimates and yield overconfident posteriors, especially with poorly chosen priors or summary statistics for high-dimensional data. They further mention the implementation is a proof of concept and not fully optimized (e.g., parallelization could speed it up).","The proposed utility depends on kernel-based entropy estimates and GP surrogates for discrepancies; performance may be sensitive to kernel/bandwidth choices ($\eta$, $\epsilon_m$), GP kernel/hyperparameters, and discrepancy definitions, but robustness to these choices is not systematically explored. Empirical validation is entirely simulation-based; there is no demonstration on real human-subject experimental data where simulator mismatch, nonstationarity, or measurement artifacts could degrade performance. The approach assumes trials are conditionally independent given current design and parameters and analyzes single subjects with fixed parameters, which may be unrealistic in longitudinal/adaptive cognitive tasks where learning/adaptation and autocorrelation occur. Comparisons may be affected by implementation/compute budgets (e.g., different simulation counts and tuning for baselines), and broader benchmarks against additional modern SBI/OED approaches are limited.",The authors propose extending the approach to hierarchical models to move from single-subject inference to population-level modeling and adaptive priors. They suggest removing the stationarity assumption by introducing dynamic models where responses depend on the history of prior responses/designs. They also recommend incorporating amortized non-myopic design selection to further reduce between-trial computation time and improve long-horizon exploration/planning of experiments.,"Develop a self-starting/robust version that explicitly handles simulator misspecification and model inadequacy, including posterior predictive checks and diagnostics for design-induced overconfidence. Extend BOSMOS to handle correlated/noisy real-world data streams (e.g., autocorrelation, lapses, missing responses) and evaluate on real online experiments with humans to quantify practical gains and user burden. Provide principled or automated methods for choosing discrepancy functions and kernel bandwidths (possibly via cross-validation or Bayesian treatment) and study sensitivity/robustness across tasks. Release a reusable software package/API with standardized interfaces to simulators and design spaces, plus computational scaling studies (parallel/batched BO, multi-fidelity simulators) for larger model sets and higher-dimensional designs.",2303.02227v1,https://arxiv.org/pdf/2303.02227v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:38:35Z
TRUE,Supersaturated|Screening|Other,Screening|Parameter estimation|Prediction|Other,E-optimal|Minimax/Maximin|Other,Variable/General (examples include m=21–29 factors/columns with N=18–24 runs/rows; also N=16 with m=25–27; N=20 with m=21; N=22 with m=22–23; N=24 with m=24–25),Theoretical/simulation only|Other,Simulation study|Other,TRUE,C/C++|Other,Upon request,https://arxiv.org/abs/2303.09104,"The paper develops a bit-parallel tabu search algorithm to construct two-level (±1) supersaturated designs (SSDs) that are E(s^2)-optimal and minimax-optimal with small maximum column correlation (s_max ∈ {2,4,6}). A key methodological contribution is an equivalence between such SSDs (with even N rows) and resolvable incomplete block designs (RIBDs) whose parallel classes have bounded intersection structure; this turns SSD construction into a constrained combinatorial optimization problem over blocks. The authors exploit bit-string representations of blocks and CPU popcount/SIMD instructions to accelerate the repeated block-intersection computations that dominate the objective evaluation. Using the resulting bit-parallel tabu search, they report fifteen previously unknown optimal SSDs achieving the Ryan–Bulutoglu E(s^2) lower bound for several (N,m) sizes (e.g., N=16, m=25–27; N=18, m=23–29; N=20, m=21; N=22, m=22–23; N=24, m=24–25). The work advances the SSD construction literature by combining a structural equivalence (SSD↔RIBD) with an efficient heuristic search that outperforms prior NOA_p procedures on the unsolved instances considered.","The E(s^2) criterion for a ±1 array H(N,m) is defined as E(s^2)=\frac{1}{\binom{m}{2}}\sum_{i<j} s_{ij}^2, where s_{ij} is the (i,j) entry of H^\top H (column inner products measuring non-orthogonality). Using the RIBD equivalence, |s_{\ell j}|=|4|B_{\ell,p}\cap B_{j,h}|-N| and thus E(s^2) can be written in terms of block intersection sizes: \frac{1}{\binom{m}{2}}\sum_{\ell<j}(4|B_{\ell}\cap B_j|-N)^2. The optimization uses a weighted objective f(B)=\frac{1}{\binom{m}{2}}\sum_{\ell<j} w(\ell,j)(4|B_{\ell}\cap B_j|-N)^2, where weights penalize violations of intersection bounds that enforce s_{max}\in\{2,4,6\}.","The tabu search algorithm constructs 15 new E(s^2)-optimal and minimax-optimal SSDs achieving the sharpest known Ryan–Bulutoglu E(s^2) lower bound with s_max∈{2,4,6}, for sizes (N,m)=(16,25),(16,26),(16,27),(18,23)–(18,29),(20,21),(22,22),(22,23),(24,24),(24,25). For example, they report the hardest case (N=16,m=27) required about 3,954,798 runs and 24.4 CPU-days (but ~16.1 hours on 40 threads via independent parallel runs). They also provide, for each found SSD, the design matrices and Gram matrices (H^\top H) in an appendix, documenting achieved s_max and frequencies f_{s_max}. Across the searched instances (N=16–24, m up to 30), the bit-parallel TS succeeds on cases where the prior NOA_p algorithms failed despite very long runs.","The authors note the method is heuristic: there is no guarantee the tabu search reaches the lower bound b(N,m), and they terminate runs when a preset iteration limit without improvement is exceeded. They also emphasize that CPU time is highly variable because the search is stochastic, making performance comparisons based on time less informative for very difficult instances. Code is not publicly released; they state the source code can be requested by emailing the first author.","The constructions and optimality claims are restricted to two-level (±1) SSDs with balanced columns and focus on small s_max values (2,4,6); the approach may not generalize straightforwardly to multi-level SSDs or other correlation constraints. Empirical evaluation emphasizes whether the algorithm finds designs meeting known lower bounds on selected sizes, but does not provide a systematic benchmark suite or sensitivity analysis over tabu parameters (e.g., tabu tenure M, nitmax) beyond limited tuning remarks. Practical guidance for applying these SSDs in real screening studies (analysis methods under aliasing, robustness to model misspecification, or handling of constraints like blocking/randomization) is not developed, since the paper is primarily about combinatorial construction.",None stated.,"A natural extension is to generalize the SSD↔RIBD equivalence and the bit-parallel tabu framework to multi-level supersaturated designs or mixed-level designs, where intersections/correlations have more complex structure. Further work could provide reproducible software (public repository) and a standardized benchmarking protocol comparing TS, NOA_p, simulated annealing, and other metaheuristics across a broader range of (N,m) and objective variants. It would also be valuable to study robustness of the found designs under common analysis strategies for SSDs (e.g., Bayesian variable selection, lasso) and under deviations from independence or equal-variance assumptions, connecting construction optimality to downstream screening performance.",2303.09104v3,https://arxiv.org/pdf/2303.09104v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:38:57Z
TRUE,Optimal design|Sequential/adaptive,Model discrimination|Parameter estimation,Not applicable,"Variable/General (genome-scale networks; examples include 10, 500, 1,000, 1,146, 2,000 nodes/genes)",Food/agriculture|Healthcare/medical|Theoretical/simulation only|Other,Simulation study,TRUE,R|Python|MATLAB|C/C++|Other,Public repository (GitHub/GitLab),https://github.com/shahashka/SP-GIES,"The paper proposes SP-GIES (Skeleton-Primed Greedy Interventional Equivalence Search), a hybrid causal structure learner that scales joint learning from observational and interventional gene data by first estimating an observational skeleton (e.g., via PC/CLR/ARACNE) and then restricting GIES’ edge search space to that skeleton. It targets genome-scale biological network recovery (gene regulatory networks) under hard interventions (e.g., gene knockouts) and shows improved runtime and competitive or better accuracy versus GIES/IGSP on synthetic random graphs, DREAM4 in-silico benchmarks, and a large RegulonDB E. coli dataset. The work also integrates SP-GIES into an optimal experimental design (OED) loop that selects which gene interventions to perform using expected-utility criteria (information gain/entropy reduction and edge-orientation/oriented-edges criteria). Empirically, SP-GIES achieves up to ~3.87× speedup over GIES and reaches AUC-PR ≈ 0.91 on a 1,000-node small-world setting, scaling to 2,000 nodes; in OED experiments on DREAM4, SP-GIES improves downstream OED performance compared with GIES-based loops though it can stagnate due to skeleton restriction. Overall, it advances scalable causal discovery pipelines that combine interventional data with adaptive experiment selection for biological network recovery.","The OED selection is posed as maximizing an expected utility over candidate interventions: \(\hat I = \arg\max_I \mathbb{E}_{G\mid D}[U]\), where \(U\) may be mutual information/entropy decrease or a function of oriented edges. The Bayesian formulation samples graphs from \(P(G\mid D) \propto P(D\mid G)P(G)\) with likelihood \(P(D\mid G)=\int_\theta P(D\mid \theta,G)P(\theta\mid G)\,d\theta\). Hard interventions are modeled via an interventional distribution \(P_{do(X_i=x)}\) that removes incoming edges to intervened nodes and factorizes over non-intervened conditionals, matching gene knockout-style interventions.","On large-scale evaluation, SP-GIES reaches AUC-PR 0.91 on a 1,000-node small-world network and scales to 2,000 nodes, with reported speedups up to about 3.87× versus GIES in their scaling study. On DREAM4 in-silico 10-node networks, SP-GIES often attains the best (or among best) SHD/SID/AUC-PR across the five networks (e.g., AUC-PR up to 0.63 reported). On the RegulonDB (1,146 nodes) real dataset, SP-GIES improves over some baselines but can underperform CLR because GIES’ linear-Gaussian scoring may be mismatched to nonlinear real biology; they observe hub detection issues with real data that improve when synthetic linear-Gaussian data is generated from the same topology. In OED loops on DREAM4 (10 rounds), edge-orientation utility outperforms information-gain utility, and SP-GIES-based OED performs better than GIES-based OED but may plateau as additional interventions are added.","They note SP-GIES remains bottlenecked by the GIES step and still fails to complete very large problems (e.g., \(10^4\) nodes did not finish within 24 hours), indicating joint learners need distributed/parallel breakthroughs. They report degraded behavior on the biologically relevant RegulonDB dataset, attributing this partly to mismatch between GIES’ linear-Gaussian likelihood scoring and potential nonlinear effects in real gene regulation. They also observe that fixing the skeleton can overly restrict the search space, leading to stagnation/local minima in iterative OED settings.","The OED evaluation is limited to very small networks (DREAM4 size 10), so it is unclear whether the proposed autonomous design loop remains computationally and statistically effective at the genome scale where intervention choices are massive and posterior sampling is costly. The paper treats environmental interventions in RegulonDB as observational due to model limitations, which may bias conclusions about real-world interventional benefit. Practical deployment details (e.g., noise models for perturbations, batch constraints, experimental costs, and feasibility of gene interventions) are not fully incorporated into the design objective, which can limit real lab applicability.","They propose adding uncertainty to the skeleton (e.g., randomly including edges outside the skeleton) to reduce stagnation and local minima effects. They call for distributed parallel implementations of joint learners (including SP-GIES/GIES-like methods) to reach 10^3–10^4 node scales and to make OED tractable in autonomous loops. They also suggest incorporating nonlinearity into joint learners (e.g., via modified scoring or alternative nonlinear methods) and explicitly modeling environmental condition variables/interventions in datasets like RegulonDB.","Developing a principled Bayesian treatment of skeleton uncertainty (e.g., probabilistic priors over candidate edges) could integrate the skeleton stage with the GIES search rather than hard-restricting it. More extensive, standardized OED benchmarks at larger graph sizes (including batch interventions, costs, and constraints) would clarify when OED beats random choice in practice. Providing end-to-end, reproducible pipelines (data preprocessing + learner + OED loop) and scalable implementations (e.g., GPU/distributed) for the joint scoring step would likely be necessary for real robotic-lab integration.",2304.03210v1,https://arxiv.org/pdf/2304.03210v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:39:38Z
TRUE,Optimal design|Bayesian design|Factorial (full)|Factorial (fractional),Parameter estimation|Other,Bayesian D-optimal,Variable/General (examples include 1D example; 5 factors in etching case; artificial example with 5 factors: three 2-level + one 3-level categorical + one 3-level quantitative),Semiconductor/electronics|Manufacturing (general)|Theoretical/simulation only,Simulation study|Other,TRUE,R|Other,Not provided,http://CRAN.R-project.org/package=lhs|http://CRAN.R-project.org/package=AlgDesign|https://doi.org/10.1007/s42519-019-0063-6|https://doi.org/10.1080/00224065.2018.1489042|https://www.sciencedirect.com/science/article/pii/S0047259X22000045|https://doi.org/10.1080/00949655.2021.1926459,"The paper develops a Bayesian D-optimal design methodology for experiments with mixed response types: one continuous response and one binary response that are dependent through a conditional normal model for $Y\mid Z$ and a logistic regression model for $Z\mid x$. It derives Bayesian D-optimal criteria under both noninformative priors (frequentist-equivalent) and conjugate informative priors, and provides tractable upper-bound objective functions that avoid intractable integration over unobserved binary outcomes. The authors propose an efficient point-exchange algorithm to construct local D-optimal designs for fixed parameter values and a global Bayesian design obtained by aggregating local designs over parameter draws from the prior. Performance is illustrated in an artificial factorial example and a wafer-etching case study, showing improved D-efficiency over designs optimized for only one response type and over naive combined designs that ignore response dependence. The approach is positioned as filling a gap in DOE for quantitative–qualitative (QQ) systems where classical single-response designs can fail to ensure estimability for conditional models.","The QQ model combines a logistic regression for the binary response $Z$ and conditional linear models for the continuous response: $\pi(x,\eta)=\exp(f(x)^T\eta)/(1+\exp(f(x)^T\eta))$ and $Y\mid Z=z\sim N(\mu_0+z f(x)^T\beta^{(1)}+(1-z)f(x)^T\beta^{(2)},\sigma^2)$. The (upper-bound) Bayesian D-optimal design objective used for construction is $Q(X)=\mathbb{E}_\eta\Big[\log\det(F^TW_0F)+\tfrac12\sum_{i=1}^2\log\det(F^TW_iF+\rho R_i^{-1})\Big]$ with $W_0=\mathrm{diag}(\pi(1-\pi))$, $W_1=\mathrm{diag}(\pi)$, $W_2=I-W_1$, and $\rho=\sigma^2/\tau^2$ (set $\rho=0$ for noninformative priors on $\beta^{(1)},\beta^{(2)}$). Global designs are formed by sampling $\eta$ from its prior, computing local optimal designs, and aggregating design-point frequencies.","In the artificial example (5 factors; quadratic model with $q=22$ effects; run size $n=66$), the proposed local QQ design (DQQ) achieved higher D-efficiency than linear-only (DL), logistic-only (DG), and naive combined (DC) designs; reported efficiencies include eff(DQQ,DL)=1.08 and eff(DQQ,DG)=1.11 for $\rho=0$, and eff(DQQ,DL)=1.10 and eff(DQQ,DG)=1.14 for $\rho=0.3$, with eff(DQQ,DC)=1.05 and 1.07 respectively. Across 500 sampled $\eta$ values, histograms show eff(DQQ,DC|$\eta$) consistently greater than 1, indicating systematic improvement over the combined strategy. In the etching-process case (5 factors, $q=21$, $n=126$, $\rho=0.5$), local and global designs from the proposed method are almost uniformly more efficient than the global combined design and also outperform a replicated $3^{5-2}$ minimum-aberration fractional factorial benchmark in the reported efficiency comparisons.","The paper relies on a normal approximation to the posterior for the logistic-regression parameters $p(\eta\mid z,X)$ to obtain tractable D-optimal criteria, acknowledging that the exact integration is not tractable. It also focuses on the specific QQ joint modeling framework (logistic for $Z$ and conditional normal linear models for $Y\mid Z$) and treats $\mu_0$ and $\sigma^2$ as nuisance parameters excluded from the criterion. The global-design construction via frequency aggregation of local designs is described as heuristic for approximating continuous optimal designs.","The method’s performance and validity depend on the correctness of the assumed QQ model (logit link, conditional normality, homoscedastic variance across $Z$ levels, and independent observations); robustness to misspecification (e.g., overdispersion, heteroscedasticity, dependence/autocorrelation) is not thoroughly studied. The point-exchange search can be sensitive to initialization and may converge to local optima; while multiple restarts are suggested, no guarantees or systematic convergence diagnostics are provided. The candidate-set filtering rule that drops points with $\pi(x,\eta)\notin[0.15,0.85]$ may exclude potentially valuable design points for estimating effects near boundaries, especially when true probabilities are extreme, and its impact is not deeply analyzed.","The authors suggest extending the framework to non-conjugate priors for $\eta$ (e.g., normal priors with corresponding modified criteria), to multiple quantitative and/or multiple qualitative responses (including handling multi-level qualitative outcomes via dummy binaries or alternative QQ models), and to more rigorous continuous-design formulations beyond the heuristic frequency-aggregation approach. They also discuss extending the approach to sequential (adaptive) design settings and adapting the criterion to other binary-response links such as probit. Additionally, they note interest in exploring different QQ joint-model structures (e.g., latent-variable QQ models or mixed graphical models) and deriving corresponding D-optimality criteria and algorithms.","Developing and releasing open-source software implementing the criterion and point-exchange/global aggregation algorithms (including reproducible examples) would materially improve adoption and benchmarking. A comprehensive robustness study (misspecified link, heteroscedastic $Y\mid Z$, correlated errors, or mis-modeled dependence between $Y$ and $Z$) and comparisons against alternative multiresponse/Bayesian design approaches (e.g., fully utility-based expected information gain via simulation) would clarify practical reliability. Extending to constrained or irregular design regions and to cost-weighted settings where measuring one response is more expensive than the other would broaden applicability in industrial experiments.",2304.08701v2,https://arxiv.org/pdf/2304.08701v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:40:26Z
TRUE,Optimal design|Other,Parameter estimation|Other,D-optimal|A-optimal|E-optimal|Other,"Variable/General (given vectors $v_1,\ldots,v_n\in\mathbb{R}^d$; select $k\ge d$)",Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper studies discrete experimental design (selecting a subset of $k$ vectors) under a unified “$p$-norm of inverse eigenvalues” objective $\Phi_p$, which interpolates classical E-, A-, and D-design criteria as special cases ($p=\infty,1,0$ up to inversion). It gives the first approximation algorithm for the general $\Phi_p$-design problem for all $p\ge 1$, based on a randomized local search / randomized exchange framework combined with a convex relaxation. The main result shows a $(1+\varepsilon)$-approximation when the budget satisfies $k\gtrsim \min\{dp/\varepsilon,\, d/\varepsilon^2\}$, interpolating known bounds for D/A-design and E-design. Technically, the analysis bounds objective change under rank-two updates using Sherman–Morrison and Lieb–Thirring inequalities, introduces a new restriction parameter $\kappa$ to control higher-order terms for large $p$, and proves progress and concentration via martingale inequalities.","The objective is $\Phi_p(A)=\left(\tfrac1d\operatorname{tr}(A^{-p})\right)^{1/p}=\left(\tfrac1d\sum_{i=1}^d \lambda_i^{-p}\right)^{1/p}$ for PSD $A$ with eigenvalues $\lambda_i>0$, with limits $p\to 0^+$ and $p\to\infty$ recovering (inverted) D- and E-design and $p=1$ giving A-design. The convex relaxation is $\min_{x\in[0,1]^n}\;\Phi_p(\sum_i x(i)v_iv_i^\top)\;\text{s.t. }\sum_i x(i)\le k$. The rounding algorithm performs randomized exchanges with action matrix $A_t=(\alpha Z_t-c_t I)^{-2}$ and samples swap-in/out indices with probabilities proportional to $x(j)(1+\alpha\langle v_jv_j^\top, A_t^{1/2}\rangle)$ and $(1-x(i))(1-\alpha\langle v_iv_i^\top, A_t^{1/2}\rangle)$, restricting removals by $\langle v_iv_i^\top,Z_t^{-1}\rangle\le\kappa$.","Main theorem: for integer $p\ge 1$ (and extendable to real $p\ge 1$), given an optimal fractional solution to the convex relaxation, there is a randomized polynomial-time algorithm returning an integral design $z\in\{0,1\}^n$ with $\sum_i z(i)\le k$ and $\Phi_p(\sum_i z(i)v_iv_i^\top)\le (1+\varepsilon)\,\Phi_p(\sum_i x(i)v_iv_i^\top)$ provided $k\gtrsim d/(\gamma\varepsilon)$ where $\gamma=\max\{\varepsilon,1/p\}$. This yields the stated budget requirement $k\gtrsim \min\{dp/\varepsilon, d/\varepsilon^2\}$, matching known special-case behavior and interpolating between the $d/\varepsilon$ and $d/\varepsilon^2$ regimes. A bicriteria intermediate guarantee bounds size as $|S|\le (1+\varepsilon)k+O(d/\gamma+d/\kappa)$ with high probability.","The authors note they omit details for extensions to weighted/multiple budget (knapsack) constraints, stating the generalization is possible but left out for cleaner presentation. They also state that while the main theorem is presented for integer $p\ge 1$, the extension to real $p\ge 1$ requires using a generalized binomial theorem and ensuring a stronger condition (e.g., $v^\top Y^{-1}v\le 1/2$) for convergence, which their algorithm enforces via the restricted removal set.","The work is primarily theoretical and does not provide empirical benchmarks or real-world case studies demonstrating runtime/constant-factor performance on practical design instances. It assumes access to (and efficient solvability of) the convex relaxation and relies on properties like positive definiteness and eigenvalue bounds that may be delicate under noisy or ill-conditioned data; practical numerical stability is not discussed. The algorithm is randomized with high-probability guarantees that involve dimension- and parameter-dependent exponents, and the constants hidden in $\gtrsim$ and big-O terms may affect practical feasibility.","They suggest it would be interesting to see whether the randomized local search approach used here (and in prior work) can be applied to other generalizations of D/A/E-design, including Bayesian experimental design variants (with prior matrix $B$) and objectives based on elementary symmetric polynomials, potentially yielding better results. They also remark that the approach can be generalized to weighted settings with multiple budget/knapsack constraints, though details are omitted.","Developing implementations and empirical evaluations (including comparisons to state-of-the-art rounding/sampling methods) would clarify practical performance and parameter tuning. Extending the guarantees to settings with correlated/streaming data, robustness to model misspecification, or adaptive/sequential design where vectors arrive over time would broaden applicability. Providing tighter bounds (or lower bounds) on constants and exploring deterministic alternatives or variance-reduction techniques could improve reliability in practice.",2305.01942v1,https://arxiv.org/pdf/2305.01942v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:41:07Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Robustness|Cost reduction,A-optimal,"Variable/General (binary sensor-activation variables; examples include ns=2,5,10 and up to 150 candidate sensors)",Environmental monitoring|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://gitlab.com/ahmedattia/pyoed,"The paper develops robust A-optimal Bayesian optimal experimental design (OED) methods for inverse problems, focusing on binary sensor placement where prior/noise model elements may be misspecified. Robustness is formulated via a worst-case (max–min) objective over an admissible set of uncertainty parameters, and the authors show that a naive relaxation or naive stochastic reformulation can yield nonbinary/non-smooth optimization surfaces. They propose a stochastic learning formulation that maximizes an expectation over Bernoulli design policies of the inner worst-case objective, producing a smooth objective in the policy parameters and avoiding differentiability requirements with respect to the binary design. An efficient algorithm (a sampling-based outer policy optimization with an inner optimization over the uncertainty parameter) is presented, with variance reduction (baseline) and reuse of evaluated (design, uncertainty) pairs to cut computational cost. Numerical experiments on a 2D advection–diffusion source-identification problem demonstrate that the approach recovers or closely approximates robust optimal sensor configurations under uncertain observation-noise variances and can enforce sparsity/budget constraints via penalties.","Binary OED is posed as $\zeta\in\{0,1\}^{n_s}$ maximizing a utility minus penalty, robustified to $\max_{\zeta}\min_{\lambda\in\Lambda} J(\zeta,\lambda)$. For A-optimality the utility uses the trace of the Fisher information matrix: $U(\zeta)=\mathrm{Tr}(\mathrm{FIM}(\zeta))$ with $\mathrm{FIM}(\zeta,\lambda)=F^*\big(\mathrm{diag}(\zeta)\,\Gamma_{\text{noise}}(\lambda)\,\mathrm{diag}(\zeta)\big)^{\dagger}F+\Gamma_{\text{pr}}^{-1}$. The proposed robust stochastic formulation optimizes a Bernoulli policy $p\in[0,1]^{n_s}$ via $\max_p\,\mathbb{E}_{\zeta\sim P(\zeta|p)}[\min_{\lambda\in\Lambda} J(\zeta,\lambda)]$, using score-function gradients $\nabla_p\log P(\zeta|p)$ and an inner-gradient for $\lambda$ (e.g., Eq. (3.8)).","On the advection–diffusion sensor-placement problem with uncertain diagonal observation-noise standard deviations (each $\lambda_i\in[0.02,0.04]$), the method recovers the global robust optimum in the $n_s=2$ case after one iteration (degenerate optimal policy $p=(1,1)^T$). For $n_s=5$ with sparsity penalty (using $\|\zeta\|_0$ and $\alpha=10$), the recommended sampled design matches the global max–min optimum over the explored uncertainty samples and returns multiple near-optimal sparse candidates. For $n_s=10$ with a budget of $\tau=3$ active sensors enforced by $\Phi(\zeta)=|\|\zeta\|_0-3|$ and $\alpha=50$, the returned designs achieve objective values nearly identical to the global optimum while respecting sparsity. The experiments also show high reuse of previously evaluated $(\zeta,\lambda)$ pairs (redundancy rising as the policy becomes degenerate), reducing the number of new objective evaluations per iteration to near zero after a few iterations.","The authors note three main challenges: (i) choosing/tuning the outer-loop learning rate is nontrivial and currently done empirically; automatic tuning is needed for large problems. (ii) The method can get trapped in local optima, especially if the outer optimization quickly converges to a degenerate (binary) policy or due to nonconvexity of the expectation surface; they suggest using fewer steps/smaller step sizes and adding random perturbations to escape. (iii) There are no global optimality guarantees; establishing such guarantees is challenging and left for future investigation.","Robustness is primarily demonstrated for misspecified observation-noise covariance (diagonal variance parameterization in experiments); broader misspecification structures (e.g., correlated noise, model error, uncertain forward model/operator mismatch) are not empirically validated here. The robust objective is worst-case over a prescribed admissible set $\Lambda$, so performance depends strongly on how $\Lambda$ is chosen; guidance for selecting/validating $\Lambda$ from data or domain knowledge is limited. Comparisons against alternative robust OED solvers (e.g., deterministic cutting-plane/bundle methods for max–min, robust convex relaxations, or other robust Bayesian design utilities like KL-based criteria) are not a major part of the evaluation, making it harder to benchmark efficiency/quality. Real-world field data case studies are not included; the main validation is simulation-based on a PDE model problem.","They explicitly propose (i) automatic tuning of stochastic optimization hyperparameters such as the learning rate, (ii) addressing/local-optima issues (including perturbation strategies and controlling convergence to degenerate policies), and (iii) investigating global optimization strategies and guarantees, for example by treating activation probabilities as random variables and using sampling methods such as Hamiltonian Monte Carlo for global optimization.","Extending the robust framework beyond A-optimality to other Bayesian utilities (e.g., D-optimality, KL information gain) with similarly scalable gradients and robustness to multiple simultaneous misspecifications (prior + noise + model error) would broaden applicability. Developing self-starting/online variants that update designs sequentially as data arrive (adaptive robust OED) could better match operational sensing scenarios. Providing principled, data-driven ways to set the admissible uncertainty set $\Lambda$ (or using distributionally robust formulations) would make robustness less dependent on user-chosen bounds. More comprehensive empirical benchmarking on additional PDE inverse problems and real sensor data (including correlated and non-Gaussian noise, and autocorrelated observations) would strengthen evidence of practical performance and robustness.",2305.03855v1,https://arxiv.org/pdf/2305.03855v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:41:55Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Model discrimination|Cost reduction|Other,Other,"Variable/General (design variable is the next measurement point x chosen from a finite grid X; examples use N=400 candidate points; model parameters vary by application: e.g., spectral deconvolution has {a_k, μ_k, σ_k, B} for K peaks; Hamiltonian selection has {Δ, V, Γ, U_fc, (U_ff), b}).",Semiconductor/electronics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper proposes a sequential (adaptive) experimental design for spectral measurements that selects future measurement points via active learning using general parametric models, rather than a Gaussian process surrogate. Bayesian inference is used to fit model parameters and perform model selection, with posterior sampling carried out by the exchange Monte Carlo method, enabling uncertainty quantification even when analytic forms are intractable. The next measurement points are chosen from a finite set of allowable energy points to maximize an expected discrepancy (based on KL divergence) between the best-model MAP predictive distribution and posterior predictive distributions, split between improving parameter estimation of the best model and discriminating it from the second-best model. The method is validated in numerical (artificial-data) experiments for X-ray photoelectron spectroscopy (XPS): Bayesian spectral deconvolution (selecting number of peaks and estimating peak positions) and Bayesian Hamiltonian selection (choosing between 2-state vs 3-state Hamiltonians and estimating parameters). Across these simulations, the sequential design concentrates measurement time near informative spectral features (peaks) and achieves model-selection/parameter-estimation accuracy comparable to static designs with roughly three times larger total measurement time; an appendix shows Gaussian-process-based active learning performs poorly under the same high-noise settings.","Observed counts are modeled as $y\sim p(y\mid f_M(x;\theta_M))$, with independent likelihood $p(D\mid\theta_M,M)=\prod_i p(y_i\mid f_M(x_i;\theta_M))$ and posteriors $p(\theta_M\mid D,M)$ and $p(M\mid D)$ computed via Bayesian inference (exchange Monte Carlo). For active learning, the pointwise utility is an expected KL divergence between the best-model MAP predictive distribution $p(y\mid \hat f(x))$ and posterior predictive distributions: $G(x;M)=\int \mathrm{KL}(p(y\mid\hat f(x)),p(y\mid f(x;\theta_M)))\,p(\theta_M\mid D,M)\,d\theta_M$ (computed for best and second-best models). For Poisson counting with measurement time $T$, $p(y\mid f\times T)=\mathrm{Poisson}(y;f(x;\theta)T)$ and $\mathrm{KL}=T\left[-\hat f(x)+f(x;\theta)+\hat f(x)\log\frac{\hat f(x)}{f(x;\theta)}\right]$, so $T$ scales the KL but does not change the argmax selection.","In Bayesian spectral deconvolution (XPS, artificial data), with $N=400$ grid points and total time $T_{\mathrm{sum}}=2000$ (initial scan plus $k=160$ rounds selecting $n=10$ points), the sequential design yields peak-position posterior widths similar to a static design with $T_{\mathrm{sum}}=6000$, indicating about a 3× reduction in required measurement time for comparable accuracy. Model-selection posteriors over $\{M_2,M_3,M_4\}$ across 10 trials show higher probability mass on the true model under the sequential design than the static $T_{\mathrm{sum}}=2000$ baseline, again comparable to static $T_{\mathrm{sum}}=6000$. In Bayesian Hamiltonian selection (between $M_2$ and $M_3$), with $T_{\mathrm{sum}}=12000$ and the same $k=160,n=10$ structure, the sequential design achieves parameter-estimation widths for $(\Delta,U_{fc})$ and model-selection accuracy similar to a static design with $T_{\mathrm{sum}}=36000$ (about 3× time saving). The appendix demonstrates that Gaussian-process-uncertainty sampling instead concentrates measurements at spectrum edges (uninformative regions) under these high-noise settings and yields worse parameter estimates than the proposed parametric-model-based design.","The authors note that computation speed is a key limitation because Bayesian inference is run after each measurement using exchange Monte Carlo, which can be time-consuming depending on the modeling function. They also state that applying the method to actual experiments remains future work, since systematic errors (model–data mismatch) may be significant and robustness to such errors must be assessed and potentially improved.","The method’s acquisition rule relies on the current best/second-best models and MAP predictor; if early posterior mass is misled (model misspecification, poor priors), exploration may be limited and the design could over-focus on incorrect regions. Evaluations are primarily on artificial/simulated XPS data; real experimental complications (drift, background changes, instrument constraints beyond monotone energy scanning, non-Poisson noise, correlations) are not quantitatively studied. The design is restricted to a fixed discrete grid of candidate points and a specific batch selection heuristic (split between $M_b$ and $M_\circ$), without comparison to alternative Bayesian design criteria (e.g., expected information gain on parameters/models) that might provide stronger theoretical guarantees.","They propose accelerating computation by using Monte Carlo methods suited for parallel computation and by reusing previous inference results between sequential steps. They also propose applying the approach to real experiments and studying/improving robustness to systematic errors (deviations between assumed models and measured data). Additionally, they suggest using the same parametric-model active learning idea to select a subset of already-collected data points to reduce fitting cost when the forward model/simulator is expensive (e.g., SESSA, DFT, quantum-chemistry-based NMR models).","Develop a fully Bayesian experimental design objective that directly maximizes expected information gain about the model index and key parameters (rather than a MAP-referenced KL), and study its robustness vs. the current heuristic. Extend the framework to handle autocorrelated measurements, non-Poisson/overdispersed count noise, and instrument drift via hierarchical/noise models and online calibration. Provide open-source, reproducible implementations (including exchange Monte Carlo sampling and acquisition optimization) and benchmark on real XPS or other spectroscopy datasets with wall-clock runtime analysis and ablations (batch size n, prior sensitivity, model-set updating strategy).",2305.07040v1,https://arxiv.org/pdf/2305.07040v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:42:44Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (examples include p=1,2,3,6,10; nuclear physics case p=3)",Energy/utilities|Theoretical/simulation only|Other,Simulation study|Case study (real dataset),TRUE,Python|R|Other,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/parallelUQ/PUQ|https://CRAN.R-project.org/package=mined,"The paper proposes a sequential Bayesian experimental design framework for calibrating expensive deterministic simulation models by adaptively selecting simulator parameter settings to evaluate. It builds a Gaussian-process emulator (using a basis/PCA approach for high-dimensional simulator outputs) and introduces a novel acquisition function, EIVAR (expected integrated variance), that targets reducing uncertainty in the estimated unnormalized posterior density of calibration parameters rather than global emulator accuracy or point optimization. The EIVAR criterion has a closed-form expression derived from the emulator’s predictive mean/covariance and the assumed Gaussian observation-error model, and it naturally balances exploration of uncertain posterior regions with exploitation of high-posterior regions. The method is extended to settings with unknown observation/discrepancy covariance via plug-in estimation and to parallel computing via a batch-sequential (synchronous) strategy using a kriging-believer update within batches. Empirically, EIVAR yields lower posterior estimation error than random sampling and alternative acquisitions on synthetic test problems and performs well on a nuclear physics reaction-model calibration case study.","Bayesian calibration uses the unnormalized posterior $\tilde p(\theta\mid y)=p(y\mid\theta)p(\theta)$ with Gaussian likelihood $p(y\mid\theta)\propto\exp\{-(1/2)(y-\eta(\theta))^\top\Sigma^{-1}(y-\eta(\theta))\}$. With emulator predictive distribution $\eta(\theta)\mid D_t\sim\mathcal N(\mu_t(\theta),S_t(\theta))$, the paper derives $\mathbb E[\tilde p(\theta\mid y)\mid D_t]=\phi(y;\mu_t(\theta),\Sigma+S_t(\theta))\,p(\theta)$ and a closed-form variance $\mathbb V[\tilde p(\theta\mid y)\mid D_t]$ (Lemma 3.1). The acquisition is EIVAR: $A_t(\theta^*)=\int_{\Theta}\mathbb E_{\eta^*\mid D_t}\{\mathbb V[p(y\mid\theta)\mid (\theta^*,\eta^*)\cup D_t]\}\,p(\theta)^2\,d\theta$, approximated by a Monte Carlo/reference-set sum; minimizing this selects the next simulator run(s).","Across multiple synthetic calibration test problems (including 2D banana/bimodal/unimodal/unidentifiable posteriors and higher-dimensional cases with $p=d\in\{3,6,10\}$), EIVAR achieved the lowest mean absolute difference (MAD) between estimated and true unnormalized posterior compared with random sampling (RND) and alternative sequential acquisitions (MAXVAR and MAXEXP) over 50 replicates. In a nuclear physics reaction-model calibration (3 parameters, 15-angle observations), EIVAR reduced MAD substantially faster than RND and outperformed MAXVAR/MAXEXP, with acquired runs concentrated in the non-negligible posterior region rather than near-zero posterior regions. Batch-sequential versions (batch sizes 2–32) reached similar final accuracy, but larger batches showed degraded early-stage accuracy due to within-batch imputation (kriging believer), illustrating a wall-clock-time vs. statistical-efficiency trade-off. The paper also demonstrates EIVAR’s ability to target the high-posterior region when the observation/discrepancy covariance is unknown by estimating covariance parameters at each stage and plugging them into the criterion.","The authors note that EIVAR can be computationally expensive in higher-dimensional parameter spaces because it requires evaluating an integral (approximated via a reference set), and they suggest sparse grids or other quadrature/sampling schemes to mitigate this. They also point out that their derivations assume positive definiteness conditions (e.g., for $S_t(\theta)$ and related matrices), which can be violated when using a low-rank basis (q<d), though they argue the final expressions remain computable if $\Sigma$ is positive definite (or after adding a small jitter). For batch design, they acknowledge that increasing batch size can degrade emulator quality early in sampling when using the kriging-believer strategy, affecting accuracy.","The approach relies heavily on GP emulator adequacy (stationarity, kernel choice, hyperparameter fitting) and on accurate uncertainty quantification; misspecification can distort posterior-variance estimates and therefore the acquisition behavior. Acquisition optimization is performed over a random candidate set rather than continuous/global optimization, which may lead to suboptimal point selection, especially in higher dimensions or multimodal posteriors. The plug-in treatment for unknown discrepancy/noise covariance (estimating $\theta_e$ via likelihood maximization each stage) ignores uncertainty in $\theta_e$ and may understate posterior uncertainty or bias acquisition toward overly confident regions. Comparisons do not fully benchmark against modern Bayesian optimization / Bayesian quadrature active-learning baselines under identical compute budgets (e.g., entropy search for posterior approximation), which could affect conclusions about relative efficiency.","The authors propose extending EIVAR to stochastic simulation models, including strategies to decide between replication and exploration when simulator noise is present. They suggest replacing the emulator with more expressive surrogates such as deep Gaussian processes and using EIVAR to guide calibration in complex response surfaces. They also mention developing hybrid active-learning criteria to better handle non-identifiability and multimodal high-posterior regions, and extending the framework to jointly acquire physical design points $x$ and calibration parameters $\theta$ when sequential field data collection is possible. Additional suggested directions include deriving EIVAR under non-Gaussian residual error distributions and studying asynchronous batch strategies and guidelines for choosing batch size when simulator runtimes vary.","A valuable extension would be a fully Bayesian treatment of unknown discrepancy/noise covariance (integrating over $\theta_e$) within EIVAR to avoid plug-in bias and better quantify uncertainty-driven acquisition. Developing gradient-based or surrogate-assisted optimization of the EIVAR criterion (beyond finite candidate sets) could improve point selection efficiency, especially in moderate-to-high dimensions. Robust variants of EIVAR that remain reliable under emulator misspecification (e.g., heavy-tailed observation models, nonstationary kernels, or model checking/adaptive kernel selection) would increase practical applicability. Finally, providing standardized benchmarks with equal wall-clock budgets (including parallel overhead) and releasing reproducible scripts for all experiments would strengthen empirical validation and practitioner adoption.",2305.16506v1,https://arxiv.org/pdf/2305.16506v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:43:31Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Optimization|Cost reduction,Not applicable,Not applicable (treatment allocation over time; 2 arms),Healthcare/medical|Finance/economics|Theoretical/simulation only|Other,Simulation study,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/crharshaw/Clip-OGD-sims,"The paper studies Adaptive Neyman Allocation in a design-based potential outcomes framework for two-arm sequential randomized experiments, where the goal is to match the (infeasible) variance-optimal non-adaptive Neyman allocation that depends on all potential outcomes. It introduces two equivalent design performance measures—Neyman Ratio and Neyman Regret—and shows the Neyman Ratio is proportional to the expected Neyman Regret scaled by 1/T, so any sublinear regret implies convergence to the Neyman variance. The authors propose Clip-OGD, an adaptive allocation algorithm based on clipped/projection-growing online stochastic projected gradient descent, and prove it achieves O(√T) expected Neyman regret (up to subpolynomial factors), implying asymptotic recovery of the optimal Neyman variance. For inference, they construct a conservative variance estimator (bounding the unidentifiable correlation term) that yields asymptotically valid Chebyshev-type confidence intervals, and they discuss conditions under which Wald-type intervals may be justified. Simulations based on a microeconomic field experiment dataset compare Clip-OGD to Bernoulli, Neyman, and Explore-then-Commit designs and show Clip-OGD converges toward Neyman variance while ETC can incur linear regret under nonstationarity/order effects.","Treatment assignment is adaptive with P(Z_t=1|F_t)=P_t, and the adaptive Horvitz–Thompson ATE estimator is \(\hat\tau=\frac1T\sum_{t=1}^T Y_t\big(\mathbf{1}\{Z_t=1\}/P_t-\mathbf{1}\{Z_t=0\}/(1-P_t)\big)\). The per-round convex cost used for regret is \(f_t(p)=y_t(1)^2/p + y_t(0)^2/(1-p)\), so \(T\,\mathrm{Var}(\hat\tau)=\mathbb{E}[\frac1T\sum_t f_t(P_t)]-\frac1T\sum_t \tau_t^2\). Neyman regret is \(R_T=\sum_{t=1}^T f_t(P_t)-\min_{p\in[0,1]}\sum_{t=1}^T f_t(p)\), and Clip-OGD updates \(P_t=\mathcal{P}_{\delta_t}(P_{t-1}-\eta G_{t-1})\) with growing clip \(\delta_t=\tfrac12 t^{-1/\alpha}\) and gradient estimate \(G_t=Y_t^2\big(-\mathbf{1}\{Z_t=1\}/P_t^3+\mathbf{1}\{Z_t=0\}/(1-P_t)^3\big)\).","They prove (Theorem 4.1) that the Neyman Ratio \(\kappa_T=(V-V_N)/V_N\) satisfies \(\kappa_T=\Theta((1/T)\,\mathbb{E}[R_T])\) under mild moment/correlation assumptions, linking variance optimality directly to regret. With parameters \(\eta=1/\sqrt{T}\) and \(\alpha=\sqrt{5\log T}\), Clip-OGD achieves \(\mathbb{E}[R_T]\le \tilde O(\sqrt{T})\) (Theorem 4.2), implying the adaptive estimator variance converges to the Neyman variance in large samples. They propose a conservative bound \(T V_B=4S_0S_1\) and an estimator \(T\hat V_B=4\sqrt{\frac1T\sum y_t^2\mathbf{1}\{Z_t=1\}/p_t}\,\sqrt{\frac1T\sum y_t^2\mathbf{1}\{Z_t=0\}/(1-p_t)}\) with error \(T\hat V_B-TV_B=O_p(T^{-1/2})\) (Theorem 5.1), yielding asymptotically valid Chebyshev intervals. In simulations on imputed potential outcomes based on Groh & McKenzie (2016), Clip-OGD eventually approaches Neyman variance, while Explore-then-Commit can remain far from Neyman variance and may even underperform Bernoulli when early units are atypical; simulation code is provided in a public GitHub repository.","The authors note their theoretical guarantees are most relevant for moderate and large sample sizes and do not resolve whether adaptive designs are always beneficial in small samples. They also state that the conservative variance estimator may be loose because the potential-outcome correlation term is not identifiable, leading to potentially overly conservative intervals. They conjecture (but do not prove) a CLT for the adaptive Horvitz–Thompson estimator under Clip-OGD, so they recommend caution when using Wald-type normal intervals without a formal CLT.","The core method and theory are developed for a two-arm setting with Bernoulli-type randomization probabilities; extensions to multi-arm settings, constrained randomization (e.g., blocking/matching), or cluster randomization are nontrivial and not addressed. The design-based framework assumes no interference and focuses on one unit arriving per round with immediate outcome observation; many practical sequential experiments involve delayed outcomes, batching, or time-series dependence that could affect both regret behavior and inference. The simulations rely on imputed potential outcomes (model-based imputation and normalization), so empirical performance may differ under real outcome-generating processes or heavy-tailed/heteroskedastic outcomes beyond the assumed moment bounds. Implementation details such as software environment and reproducibility beyond the linked repo are not described in the excerpt, and computational cost/scalability for very large T is not systematically evaluated.","They propose proving a central limit theorem for the adaptive Horvitz–Thompson estimator under Clip-OGD to justify narrower Wald-type confidence intervals. They also suggest extending the approach to batched treatment allocations and settings with delayed outcome observations. Finally, they mention investigating Adaptive Neyman Allocation variants under interference to enable more realistic inference in network and marketplace experiments.","Extending Clip-OGD to multi-arm trials and to constrained randomization schemes (e.g., stratification, covariate-adaptive randomization, matched pairs) would broaden applicability in clinical and social science experiments. Developing self-tuning or data-driven rules for choosing \(\eta\) and \(\alpha\) (with finite-sample guarantees) could reduce the “overhead of adaptivity” without requiring prior moment bounds. Robustifying the method and inference to heavy tails, outliers, or autocorrelated outcomes (e.g., via robust gradient estimates or truncation) could improve practical stability. Providing packaged implementations (e.g., an R/Python library) with diagnostics and guidance on positivity/clip behavior would make the design easier for practitioners to adopt.",2305.17187v2,https://arxiv.org/pdf/2305.17187v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:44:24Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Cost reduction|Other,A-optimal|I-optimal (IV-optimal)|G-optimal|Minimax/Maximin|Other,"Variable/General (regression with m parameters; examples include m=2,3,5; design points n and runs N vary)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Other,Personal website,http://www.iam.fmph.uniba.sk/ospm/Harman/design/,"The paper develops a mixed-integer linear programming (MILP) formulation for computing exact (discrete) optimal experimental designs for linear regression models on finite design spaces. It covers a broad minimax family of criteria Φ_B(M)=max_ℓ tr(B′_ℓ M^{-1} B_ℓ), yielding MILP formulations for A-, I(IV)-, G-, and MV-optimality (among others), and improves on prior mixed-integer second-order cone programming (MISOCP) approaches. The key technical step is using McCormick linearization with auxiliary variables z_{ijk}=d_i c_{jk} and requiring finite a priori bounds on covariance-matrix elements (Σ=M^{-1}), for which the authors provide both SDP-based (computational) and analytic bounding methods. A major practical advantage is the ability to incorporate multiple linear design constraints (e.g., resource/space-filling constraints) and linear constraints directly on variances/covariances of the least-squares estimator while retaining MILP structure. Numerical studies (quadratic regression, nonlinear/local design via discretized candidate supports, and random Gaussian regressors) compare runtime behavior with MISOCP and illustrate constrained and replication-allowed extensions.","The exact design is encoded by d∈{0,1}^n with 1′d=N and information matrix M(d)=∑_{i=1}^n d_i f_i f_i′; the covariance matrix is Σ=M^{-1}. The minimax criterion is Φ_B(M)=max_{ℓ=1..K} tr(B′_ℓ M^{-1} B_ℓ)=max_ℓ tr(B′_ℓ Σ B_ℓ). The MILP introduces z_{ijk}=d_i c_{jk} (c_{jk}=(Σ)_{jk}) and replaces bilinearities with McCormick constraints using bounds L_{jk}≤c_{jk}≤U_{jk}, yielding linear constraints equivalent to z_{ijk}=d_i c_{jk} for binary d and a linearized version of MΣ=I_m.","They show that for the Φ_B minimax class (including A-, I-, G-, and MV-optimality) the exact optimal design problem can be written as an MILP, enabling branch-and-cut MILP solvers and additional linear constraints on designs and on Σ entries. For A-optimality they derive tight simple covariance bounds c^*_{jj}≤α and |c^*_{jk}|≤α/2 (j≠k), and for MV-optimality |c^*_{jk}|≤α; for general Φ_B they provide analytic and SDP-based bounds and prove finiteness of the SDP bound problems. In quadratic regression (n=31, N=5) reported mean MILP runtimes (s) were about 3.99 (A), 11.75 (I), 2.59 (MV), and 36.62 (G) for binary designs, and substantially larger when replications are allowed. They demonstrate constrained designs (space-filling and cost constraints) and show MILP can match designs found by exchange methods while being convenient for constrained exact optimization on candidate supports.","They note the approach does not extend to correlated observations because the information matrix then lacks the additive structure needed for their formulation. They also emphasize that exact methods like MILP/MISOCP are generally limited to small-to-medium problem sizes due to time complexity, and their MILP can be slower than MISOCP as n and m grow. For continuous design spaces, MILP cannot be applied directly and requires discretization or restricting to a candidate support set.","The MILP relies on having reasonably tight covariance bounds L_{jk},U_{jk}; weak bounds can lead to large MILPs and poor solver performance, and computing SDP-based bounds may itself become expensive for larger m (requiring many SDPs). The method is specialized to criteria linear in Σ via the Φ_B minimax trace form and explicitly does not handle D-optimality (log-det) within the same linearization framework. Practical use may depend heavily on solver availability/performance (they use Gurobi) and on good initial feasible designs d0 satisfying any added constraints.",They highlight avenues including developing better analytical/computational covariance bounds and constructing suitable initial feasible designs d0 under arbitrary design/covariance constraints. They also mention plans to implement the MILP algorithm in the R OptimalDesign package to provide a more user-friendly interface.,"Extending the approach to handle autocorrelated or spatially/temporally correlated errors (non-additive information structures) would broaden applicability, potentially via alternative linearizations or decomposition methods. Developing self-starting or robust versions for unknown variance and model misspecification, and benchmarking against modern exact-design MINLP/branch-and-bound methods across standardized test suites, would strengthen practical guidance. Providing open-source implementations not tied to commercial solvers (or solver-agnostic formulations with performance profiling) and adding diagnostic tools to choose between analytic vs SDP bounds could improve adoption.",2305.17562v2,https://arxiv.org/pdf/2305.17562v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:44:55Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Other,Not applicable,Variable/General (binary treatment; binary moderator; includes randomization of moderator measurement timing),Other,Exact distribution theory|Simulation study|Other,TRUE,R,Package registry (CRAN/PyPI),https://cran.r-project.org/package=prepost,"The paper analyzes a design trade-off in experiments with moderation: measuring moderators pre-treatment can induce priming bias, while measuring them post-treatment can induce post-treatment bias when the moderator is affected by treatment. It formalizes three designs—pre-test, post-test, and a randomized placement design that randomizes whether the moderator is measured before or after treatment—and studies identification of conditional average treatment effects and treatment–moderator interactions under each. Using a nonparametric potential-outcomes framework, it derives sharp (nonparametric) bounds for interactions under randomization alone and shows these are typically wide and may not identify even the sign of the interaction. It then introduces substantive assumptions (priming monotonicity, moderator monotonicity, and stability of the moderator under control) that can tighten bounds, and provides sensitivity analyses parameterizing the fraction of units subject to priming or moderator shifts. The methods are illustrated with a survey experiment on electoral messaging and land insecurity from Horowitz and Klaus (2020), and the authors provide an R package (“prepost”) to implement the procedures.","Defines the post-test CATE and interaction as $\tau(m)=\mathbb{E}(Y_i^*(1)-Y_i^*(0)\mid M_i^*=m)$ and $\delta=\tau(1)-\tau(0)$, where $Y_i^*(t)\equiv Y_i(t,1)$ is the unprimed (post-test) potential outcome and $M_i^*$ is the pre-test (true) moderator. Under pre-test measurement the identified estimand is $\tau_{pre}(m)=\mathbb{E}(Y_i\mid T_i=1,M_i=m,Z_i=0)-\mathbb{E}(Y_i\mid T_i=0,M_i=m,Z_i=0)=\mathbb{E}(Y_i(1)-Y_i(0)\mid M_i^*=m)$, which can differ from $\tau(m)$ due to priming. Sensitivity restrictions bound the fraction primed: $\Pr(Y_i^*(t)\neq Y_i(t)\mid M_i^*=m)\le \theta$, yielding bounds like $\tau(m)\in[\tau_{pre}(m)-\theta,\tau_{pre}(m)+\theta]$ and $\delta\in[\delta_{pre}-2\theta,\delta_{pre}+2\theta]$.","Without substantive assumptions, the pre-test design yields only logical bounds ($\tau(m)\in[-1,1]$, $\delta\in[-2,2]$) and the post-test design yields wide sharp bounds that generally cannot determine the sign of the interaction. Adding priming monotonicity (pre-test) or moderator monotonicity and stability-under-control (post-test) can substantially tighten bounds; under the strongest combination (moderator monotonicity + stability, and optionally priming monotonicity in randomized placement), bounds can approach the naive OLS interaction in their empirical example but may still include zero once sampling uncertainty is incorporated. The sensitivity analyses show that even small allowed proportions of affected moderators/outcomes (parameters $\gamma$ and $\theta$) can rapidly widen bounds, and they provide thresholds (e.g., when bounds cross zero) for assessing robustness. An R implementation is provided via the prepost package.","The authors note their main exposition focuses on binary outcomes and moderators “to ease exposition,” with extensions to more general cases left open. They also note that randomization-only bounds can be very wide and that inference for bounds is challenging due to nondifferentiability, making standard bootstrap theoretically problematic (they use a conservative bootstrap-based approach in practice).","The framework’s informative conclusions can depend heavily on untestable monotonicity/stability assumptions whose plausibility may vary widely across applications; guidance on diagnosing these assumptions beyond indirect checks (e.g., small average effects) is limited. The primary empirical illustration relies on a single applied setting and dichotomization of an outcome, so the practical performance for common continuous moderators/outcomes and richer heterogeneity structures (e.g., multi-category moderators, multiple moderators) is not demonstrated in the main text. The randomized placement design is analyzed for identification/bounds but the paper does not provide a formal design-optimization criterion (e.g., how to allocate sample sizes to pre vs post arms) beyond qualitative discussion.",They suggest optimizing the randomized placement design to better balance priming versus post-treatment bias concerns. They also propose examining how separate pre-test surveys conducted weeks or months before treatment might justify different assumptions and yield improved identification.,"Developing optimal allocation rules for randomized placement (sample split between pre/post measurement) under explicit loss functions (e.g., expected bound width or decision error) would make the design guidance operational. Extending the nonparametric bounds and sensitivity analysis to non-binary moderators/outcomes (ordinal/continuous), multiple moderators, and settings with clustered assignment or interference would broaden applicability. Providing vetted, end-to-end software workflows (including inference options beyond bootstrap and recommended reporting templates) and additional real-world replications across domains would strengthen adoption and external validity.",2306.01211v5,https://arxiv.org/pdf/2306.01211v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:45:32Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Parameter estimation|Optimization,Other,"Variable/General (demonstration uses 1 design variable: delay time t; parameter vector estimated includes 5 parameters: J, D, Γ, h_ext, w_ext)",Other,Simulation study,TRUE,Python|Other,Not provided,NA,"The paper proposes a machine-learning-enabled Bayesian optimal experimental design (BOED) workflow to adaptively choose x-ray photon fluctuation spectroscopy (XPFS) delay times that maximize information gain for ultrafast spin dynamics studies. A fully connected neural network serves as a differentiable surrogate for linear spin-wave theory (LSWT), enabling rapid forward predictions of the intermediate scattering function-derived signal s(t) and efficient BOED utility evaluation. The BOED approach uses a particle-filter representation of the posterior over model parameters and selects the next delay time sequentially using a utility based on predicted signal variance (a simplification of a KL-divergence expected information gain), with an additional spacing cost to avoid redundant measurements. Automatic differentiation is leveraged to run intermittent gradient-descent updates that correct poor priors and improve robustness of parameter estimation. Benchmarks on simulated data (with Poisson noise, finite pulse-width convolution, and extraneous low-energy peaks) show BOED methods outperform random and sequential time sampling, with the BOED+GD hybrid achieving the lowest online parameter estimation errors and selecting more informative delay times.","The surrogate-to-measurement mapping computes the ISF via a Fourier-cosine transform with decay and pulse-width convolution: $S(q,t)=\int_0^\infty e^{-\Gamma \tau}\sum_i S(q,\omega_i)\cos(\omega_i\tau)\,\Pi(t-\tau;a)\,d\tau$ (Eq. 4), and the measured quantity is $s(t)=|S(q,t)/S(q,0)|^2$. The BOED utility is simplified to the predicted variance over the current particle posterior, penalized by a spacing cost: $U_n(t)=\sigma_s^2(t;P_n(x))/c(t)$ (Eq. 6), and the next design point is $t_{n+1}=\arg\max_t U_n(t)$ (Eq. 7). Posterior updates use a Gaussian approximation to Poisson noise likelihood $P(s_{n+1}|x,t_{n+1})\propto\exp\{-\tfrac12[(s_{n+1}-\hat s_{n+1})/\sigma]^2\}$ with $\sigma=\sqrt{\eta\max(s_{n+1},1)}$ (Eq. 8) and particle-filter Bayes update $P_{n+1}(x)\propto P(s_{n+1}|x,t_{n+1})P_n(x)$ (Eq. 10).","Across simulated benchmarks (100 test samples, 40 sequential measurements, 6 conditions from pulse widths $a\in\{0.1,0.2\}$ ps and noise levels $\eta\in\{0.5,1.0,2.0\}$), BOED-based strategies yield lower mean absolute error (MAE) in estimating key Hamiltonian parameters (J and D) than sequential sweeping or random time selection (Fig. 7). The hybrid BOED+GD approach provides the lowest online learning errors among compared strategies and successfully corrects failures caused by mis-specified/narrow priors (Fig. 5). BOED-selected delay times concentrate early measurements near the most informative temporal regions (around ~1 ps in their examples) and then diversify across the full domain, while also producing higher average informativeness as measured by larger mean $|\partial s(t)/\partial t|$ over chosen times (Fig. 7e–f). The method remains effective under added experimental complications such as finite pulse-duration convolution and extraneous near-zero-energy scattering peaks (Eqs. 11–13; Fig. 6).","The authors note that the current framework is developed only for suggesting delay-time measurements (a single experimental “tuning knob”), and extending it to guide additional experimental parameters would broaden applicability. They also state that their effective utility function is a simplified surrogate for the original KL-divergence expected information gain and “may not be a faithful representation” of the full expression, especially under Poisson noise assumptions. They further indicate that improvements are possible in refining utility functions and broadening application scenarios.","The approach depends on the fidelity and coverage of the NN surrogate trained on simulated LSWT/SpinW data; if the true experiment deviates (model mismatch, unmodeled physics, different q-regions), BOED decisions and parameter posteriors may be biased. The likelihood uses a Gaussian approximation to Poisson noise with ad hoc clamping, which can be inaccurate at low counts and may affect posterior calibration and design choices. The chosen utility (variance-only with a spacing cost) is heuristic and not invariant to nonlinear parameter identifiability; it may overemphasize regions with high variance that do not optimally reduce posterior uncertainty. No real experimental case study is provided, so robustness to detector artifacts, drift, and correlated noise remains unverified.","They propose extending the method beyond delay-time design to additional experimental “tuning knobs,” and suggest using alternative NN surrogate models (e.g., implicit neural representations taking continuous q and ω) to guide measurements in momentum space and capture more excitation modes. They also suggest exploring policy optimization and reinforcement learning to develop faster and more powerful NN-based utility functions instead of the simplified variance utility. Finally, they mention extending the Hamiltonian to include additional interactions (e.g., Kitaev terms) to estimate competing parameters with nearly indistinguishable dispersion features.","Validating the workflow on real XPFS datasets (including instrument drift, background subtraction errors, and correlated/heteroscedastic noise) would be a critical next step for practical adoption. Developing principled expected-information-gain utilities for Poisson (or compound Poisson) photon-counting models, and comparing them against variance heuristics in terms of calibration and efficiency, would strengthen the design component. A fully self-starting or online-surrogate-updating variant (active learning of the surrogate as data arrive) could reduce reliance on precomputed training sets and mitigate model mismatch. Providing an open-source reference implementation and standardized benchmarks would improve reproducibility and enable broader SPC/DOE-style comparisons across adaptive design strategies.",2306.02015v1,https://arxiv.org/pdf/2306.02015v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:46:17Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Optimization|Cost reduction|Other,Not applicable,Variable/General (discretized feature bins/cells; examples include 20 discrete x-values in synthetic; VAE latent grid default 20x20),Healthcare/medical|Service industry|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://www.uplift-modeling.com/en/v0.3.1/api/datasets/fetch_x5.html|https://tinyurl.com/RetailHero|https://github.com/raddanki/Sample-Constrained-Treatment-Effect-Estimation,"The paper proposes a task-specific experimental design method for treatment effect estimation that jointly selects trial participants (cohort selection) and assigns treatments to maximize expected performance on a chosen downstream metric, rather than relying on uniform randomization (RCT). The approach discretizes the covariate space (via a VAE latent representation and a rectangular grid) and models outcome probabilities per cell and treatment arm with Beta-Bernoulli posteriors; at each step it uses Thompson sampling to choose the next (cell, treatment) that maximizes an analytically derived expected target-metric value. The authors derive closed-form/approximate expressions for the expected downstream objective for several metrics (MSE of uplift, squared ATE error, AUQ/area under the Qini curve, and ERUPT), yielding different sampling policies depending on the intended application (clinical ATE vs. marketing uplift ranking/targeting). Experiments on synthetic data and large real-world RCT datasets (International Stroke Trial subset; Criteo uplift; RetailHero) show substantial sample-efficiency gains, particularly for AUQ in marketing where the method can match RCT performance with roughly an order-of-magnitude fewer samples. The paper positions the method as a bandit-style, task-aware alternative to covariate balancing/active learning baselines, emphasizing scalability via latent-space discretization and sequential/batch operation.","Individual treatment effect/uplift is defined as $u(x)=\mathbb{E}[Y\mid X=x,T=1]-\mathbb{E}[Y\mid X=x,T=0]$. With a Beta prior $\theta_{x t}\sim\mathrm{Beta}(\alpha_{x t},\beta_{x t})$ for Bernoulli outcomes, the representative uplift model is the difference of posterior means: $\hat U(x)=\sum_{t=0}^1(-1)^{t+1}\frac{\alpha_{xt}+\sum_{i=1}^{n(x,t)}Y_i^{(x,t)}}{\alpha_{xt}+\beta_{xt}+n(x,t)}$. The design policy evaluates (by hand-derived formulas) $f^{R}_{\alpha}(\{n(x,t)\};\theta)$, an analytic expression for $\mathbb{E}[R[\hat U]]$, and uses Thompson sampling to sequentially choose the next arm $(x,t)$ (or latent grid cell $b$ and $t$) maximizing the one-step-improved objective; AUQ is defined as $\mathrm{AUQ}[\hat u]=\int_0^1 Q(f)\,df-\mathrm{ATE}[u]/2$ with $Q(f)$ based on ranking by predicted uplift.","Across targeted marketing tasks evaluated by AUQ on large real-world datasets (CriteoVisit, CriteoConversion, RetailHero), AUQ-optimized sampling substantially outperforms RCT for all tested sample sizes and typically matches RCT performance with roughly an order-of-magnitude fewer samples. For clinical-trial-style objectives (squared ATE error), the ATE-optimized sampler generally improves over RCT on 3 of 4 datasets (including Stroke), while the Recursive GSW baseline often does not beat RCT and is infeasible on the largest Criteo datasets due to scaling. Uniform sampling in VAE latent space performs surprisingly close to AUQ-optimized sampling on AUQ tasks, but performs poorly for ATE estimation, highlighting metric dependence. Sensitivity analyses show AUQ performance is relatively insensitive to latent dimensionality (e.g., 2 vs 4) and grid resolution (e.g., 10x10 vs 20x20 vs 30x30), while an informed prior improves AUQ versus a uniform prior; batching the sequential updates preserves much of the performance, especially for AUQ.","The authors state that the primary limitation is the need for by-hand/analytic calculations of the expected-performance objective for each target metric, which makes extending to new downstream metrics labor-intensive. They also note that future work could explore alternatives to Thompson sampling as the backbone of the approach.","The method’s theoretical development relies on discretizing the covariate space (via a VAE and a fixed grid), which can introduce sensitivity to representation learning quality and binning choices in more complex/high-dimensional settings, despite reported robustness in the studied datasets. The analytic objectives (especially AUQ/ERUPT) use approximations (e.g., CLT/normal approximations in large-sample limits) whose accuracy at small per-cell counts may affect early-stage decisions. The approach is presented for binary treatments and largely Bernoulli outcomes (with only brief notes on continuous outcomes), and does not directly address interference, noncompliance, time-varying treatments, or outcome/covariate shift between trial and deployment. Practical deployment also presumes access to a large pool and the ability to selectively enroll/assign treatments, which may be constrained by ethics, regulations, or operational limits in real trials.",They propose exploring data-driven alternatives to deriving metric-specific formulas by hand for each downstream objective. They also suggest investigating alternatives to Thompson sampling as the core optimization mechanism for the sequential design policy.,"Extending the framework to multi-treatment (more than two arms), continuous or time-to-event outcomes, and settings with nonstationarity or covariate shift would broaden applicability to real clinical and online experimentation contexts. Developing self-starting/robust versions that handle unknown/estimated nuisance components (e.g., propensity imbalances, clustering, autocorrelation) and providing finite-sample guarantees for the approximated objectives would strengthen reliability. Adding constraints for ethical/operational requirements (minimum exploration rates, per-group quotas, safety constraints) could make the policy more deployable. Releasing a reference implementation and benchmarking on additional public causal design suites (including observational-data active learning settings) would improve reproducibility and comparative evaluation.",2306.05484v1,https://arxiv.org/pdf/2306.05484v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:46:59Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Parameter estimation|Model discrimination|Prediction|Other,Other,"Variable/General (design vector $\xi_k\in\mathbb{R}^{N_\xi}$ over a finite horizon of N sequential experiments; examples include 1D, 2D, and 6D designs in case studies)",Manufacturing (general)|Healthcare/medical|Finance/economics|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/wgshen/vsOED,"The paper proposes variational sequential optimal experimental design (vsOED), a Bayesian finite-horizon sequential OED method that frames design as an MDP and solves for an adaptive design policy via actor–critic reinforcement learning. It maximizes information-theoretic utilities (expected information gain) using terminal or incremental information-gain rewards and introduces a one-point reward formulation; replacing true posteriors with variational posteriors yields a provable lower bound on EIG (Barber–Agakov style). The method supports multiple objectives—model discrimination, parameter inference, and goal-oriented prediction (QoIs)—and can incorporate nuisance parameters and implicit likelihoods without requiring model derivatives. Numerically, it derives/estimates policy gradients and variational gradients, and approximates posteriors with Gaussian mixture models and normalizing flows. Across several simulation-based case studies (source localization, CES preference model, SIR epidemic model, and a convection–diffusion–reaction PDE with surrogates), vsOED shows improved sample efficiency versus prior sequential design/RL baselines, especially under limited training budgets.","Sequential observation model and Bayesian update are central: $Y_k = G_k(\Theta_m,\Psi_m,\xi_k; m, I_k)+E_k$ and $p(m,\theta_m,\psi_m\mid i_{k+1}) \propto p(y_k\mid m,\theta_m,\psi_m,\xi_k,i_k)\,p(m,\theta_m,\psi_m\mid i_k)$. The design objective is maximizing expected cumulative utility over a policy $\pi$: $\max_{\pi}\,\mathbb{E}[\sum_{k=0}^{N-1} r_k(I_k,\xi_k,Y_k)+r_N(I_N)]$, where rewards are (terminal or incremental) KL information gains on model indicator, parameters, and/or QoIs. vsOED replaces posterior terms with variational posteriors $q(\cdot\mid i_k;\phi)$ to form a BA-style lower bound, and uses an actor–critic policy-gradient: $\nabla_w \tilde U(w;\phi)=\mathbb{E}[\sum_{k=0}^{N-1} \nabla_w \mu_{k,w}(I_k)\,\nabla_{\xi_k}\tilde Q^{\pi_w}_k(I_k,\xi_k;\phi)]$.","The paper reports that vsOED achieves higher expected-utility/EIG estimates than competing sequential design methods in multiple numerical studies, with the largest gains under limited training budgets (e.g., 10 million trajectories, and 1 million for the SIR case). For source localization (single- and multi-model), incremental-information-gain rewards (IIG) generally outperform terminal-information-gain (TIG) at longer horizons, and vsOED policies tailored to different objectives (model discrimination vs parameter inference vs QoI prediction) exhibit distinct, interpretable sampling patterns. For implicit-likelihood SIR design, vsOED performs comparably to iDAD while not requiring model derivatives. In the convection–diffusion–reaction example, surrogate models achieve low test MSE (reported for end-time concentration and flux QoI surrogates) and vsOED successfully handles nuisance parameters and multiple candidate models.","The authors state that vsOED performance is sensitive to inaccurate posterior approximations, especially when posteriors have compact support, multiple modes, or highly non-Gaussian structure, and that improving posterior representations (particularly in high dimensions) would be valuable. They also state that vsOED currently does not handle discrete designs or stochastic policies, limiting applicability in some settings.","Most demonstrations are simulation-based and evaluation relies heavily on Monte Carlo estimators and learned surrogates (in the PDE case), so real-world experimental constraints and model mismatch are not fully tested. The approach requires substantial offline training (many simulated trajectories) and careful hyperparameter tuning, which may be burdensome for expensive simulators despite claimed sample-efficiency gains. Comparisons are not always on identical objective estimators (e.g., iDAD trained on different MI bounds than the BA-based bound used for vsOED evaluation), which can complicate fairness of some conclusions.","The authors propose developing more accurate/adaptive variational posterior representations for challenging posteriors (compact support, multimodality, high-dimensional/non-Gaussian). They also suggest extending vsOED to discrete designs and stochastic policies, and incorporating more advanced reinforcement learning algorithms such as proximal policy optimization, trust-region policy optimization, and soft actor–critic.","A self-contained benchmarking suite with standardized utilities/estimators (and matched compute budgets) would strengthen cross-method comparisons, especially across different MI bounds and derivative requirements. Extending vsOED to handle model misspecification, constraints typical of lab/field experimentation (batching, delayed outcomes, safety/ethics constraints), and robust or risk-sensitive utilities would broaden practical impact. Providing a mature, documented software package (beyond a research codebase) and guidance for tuning/diagnostics (e.g., posterior-approximation quality checks) would aid adoption by practitioners.",2306.10430v2,https://arxiv.org/pdf/2306.10430v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:47:41Z
FALSE,NA,NA,Not applicable,Not specified,Other,Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper proposes an optical integrated sensing and communication (OISAC) scheme for cooperative mobile robotics that integrates camera sensing with screen-camera communication (SCC) so a follower robot can both estimate relative pose and receive the leader’s state (e.g., velocity) from the same RGB images. The SCC link is designed around a displayed image containing four feature markers plus stripes (for detection) and a data-carrying pixel matrix; receiver-side algorithms include marker detection, perspective correction, and demodulation/decoding. To address mobile-robot issues, the authors introduce velocity smoothing to reduce motion-induced image blur and set ROS subscriber queue size to 1 to reduce screen display staleness. A leader–follower formation controller is designed to exploit the directly received leader velocity and visually estimated relative pose, with Lyapunov-based stability analysis. Real-world Turtlebot2 experiments (constant-velocity circular motion, sharp braking, and U-shaped trajectory with velocity changes) show improved accuracy/stability/robustness compared with a benchmark that estimates the leader’s states via EKF, including substantially shorter braking distances.","A velocity-smoothing (acceleration-limiting) constraint is applied to commanded velocities: for each component $u_i\in\{v,\omega\}$, $u_{c,i}(t)=\min\{u_{t,i}(t),u_{c,i}(t-\delta t)+a_i\delta t\}$ if increasing, else $\max\{u_{t,i}(t),u_{c,i}(t-\delta t)-a_i\delta t\}$. Relative pose is estimated from four detected screen feature points using pinhole-camera geometry, e.g., $x_l^f=z_O^c+d_f+d_l\cos\gamma$ and $y_l^f=-x_O^c+d_l\sin\gamma$, with $z_O^c,x_O^c$ computed from pixel coordinates and known marker spacings. The follower control law is $u_f=\sigma H(K\varepsilon + F\hat u_l)$, where $\varepsilon$ is the formation error, $\hat u_l$ is SCC-received leader velocity, and stability is shown via a Lyapunov function $V=\varepsilon^T\varepsilon/2$.","SCC packet loss rate is reported as <3% within 1.2 m distance and 50° view angle (e.g., at 120 cm loss rate 0.028; at 50° loss rate 1.3%). ROS display delay increases sharply with subscriber queue size (average delay 0.06 s at queue size 1 vs 3.21 s at queue size 50 when publishing at 20 Hz), motivating $N_q=1$. In formation with constant circular motion, formation errors settle within approximately $[\pm0.02,\pm0.02,\pm0.02]^T$ (meters, meters, radians). In the braking experiment, the proposed OISAC-based follower achieves braking distances not exceeding 0.08 m and is about 3×–5× shorter than the EKF-based benchmark across tested leader speeds (0.1–0.6 m/s).","Trade-offs are acknowledged between screen size and OISAC performance (larger screens improve detection/decoding and reduce packet loss at longer distances/angles but consume more power). With fixed screen size, enlarging the sensing/marker area improves sensing accuracy but compresses the data (pixel matrix) area, reducing throughput and increasing packet loss; an application-dependent balance is needed. The design also assumes leader visibility and bounded relative orientation to ensure marker identification under perspective distortion.","The evaluation is confined to a specific hardware setup (Turtlebot2, Kinect camera, 15.6-inch LCD) and controlled indoor conditions; performance under varying ambient lighting, sunlight, glare, screen refresh artifacts, or outdoor operation is not established. The method relies on persistent line-of-sight and successful marker detection; occlusions or clutter that mimics markers could degrade both sensing and communication simultaneously (single point of failure). Comparisons are primarily against an EKF-based estimator; broader baselines (e.g., AprilTag/ArUco-based tracking, optical flow, VIO, or alternative SCC/OCC schemes) and ablations isolating each proposed fix (blur mitigation, queue sizing) are limited. No public implementation details are provided, which may hinder reproducibility and deployment assessment (CPU load/latency budgets, decoding rates, failure modes).","The authors propose exploring other optical communication technologies (e.g., visible light communication) and adapting the OISAC scheme to more complex cooperative robotics tasks such as obstacle avoidance and cooperative object transportation.","Extend the approach to handle non-line-of-sight interruptions and partial occlusions via redundancy (multiple markers/screens, temporal coding) or sensor fusion (IMU/VIO) while preserving the ‘radio-free’ premise. Study robustness to lighting dynamics and screen/camera timing effects (rolling shutter, refresh synchronization) with explicit modeling and compensation. Provide systematic design guidelines/optimization for marker–data-area allocation and coding rate under motion/blur constraints, potentially with adaptive coding based on estimated link quality. Release an open-source ROS package and benchmarks to enable reproducible comparisons and facilitate adoption.",2306.10584v1,https://arxiv.org/pdf/2306.10584v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:48:23Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Model discrimination|Cost reduction|Other,Other,Variable/General (examples: 6 cavity frequencies; 4 qubit frequencies; measurement setting x is 1D),Semiconductor/electronics|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,https://www.tensorflow.org/,"The paper develops a deep-learning-based Bayesian optimal experimental design framework to adaptively choose measurement settings that maximize expected information gain (mutual information) for parameter estimation in quantum many-body systems. The method uses a variational mutual-information bound (Barber–Agakov) and a conditional normalizing flow to approximate the intractable Bayesian posterior, enabling greedy sequential selection of the next experiment. Two simulated applications are presented: (i) estimating onsite resonance frequencies in a 6-cavity coupled resonator chain from noisy complex transmission measurements as a function of drive frequency, and (ii) estimating qubit frequencies in a 4-qubit chain from pulsed dynamics with projective (multi-shot) measurements as a function of evolution time. In both examples, the adaptive (active) strategy learns parameters faster (fewer measurements for a target information gain / posterior concentration) than random, fixed, or uniform-sweep baselines. The work advances DOE/active learning for quantum platform characterization by pairing Bayesian design criteria with scalable deep posterior approximations for higher-dimensional, non-Gaussian posteriors.","Bayesian update: $P_{n+1}(\lambda\mid M_{n+1})=\dfrac{P(y_{n+1}\mid\lambda,x_{n+1})P_n(\lambda\mid M_n)}{P_n(y_{n+1}\mid x_{n+1})}$. The design criterion is expected information gain (mutual information): $I(x)=\int dy\,d\lambda\,P_n(\lambda)P(y\mid\lambda,x)\log\frac{P_{n+1}(\lambda\mid y,x)}{P_n(\lambda)}$, optimized over the next setting $x$. Because $P_{n+1}$ is intractable, they maximize the Barber–Agakov lower bound by introducing a variational posterior surrogate $Q(\lambda\mid y,x)$ (implemented as a conditional normalizing flow) and maximizing $\int dy\,d\lambda\,P_0(y\mid x)P_1(\lambda\mid y,x)\log\frac{Q(\lambda\mid y,x)}{P_0(\lambda)}$ w.r.t. both $Q$ and $x$.","For the coupled-cavity example (6 unknown cavity frequencies), greedy information-gain maximization selects measurement frequencies near high-slope regions of the transmission response and yields faster convergence of the posterior than random or fixed-frequency sampling; both active and random converge asymptotically, but active requires fewer measurements. For the 4-qubit example (4 unknown qubit frequencies), with 100 sequentially chosen pulse durations and each measurement repeated 100 times (multi-shot), the active strategy achieves a given total information gain in about half as many measurements as random (e.g., total information gain \~10 reached in \~10 measurements active vs \~20 random, as reported). Fixed and uniform-sweep strategies are less efficient, with sweep sometimes requiring more measurements than random. The posterior is shown to be non-Gaussian early on, motivating the normalizing-flow approximation.","The authors note that the main current drawback is computational time: updating the neural-network posterior and optimizing over measurement settings takes on the order of hundreds of seconds per measurement step in their unoptimized implementation, which is too slow for scenarios where experiments occur on microsecond–millisecond timescales. They also remark that different applications require different tradeoffs between approximation precision and time efficiency. They mention that the method currently uses a greedy (one-step lookahead) strategy, which may be suboptimal compared with budgeted multi-step planning.","The demonstrations are simulation-only and rely on correct, tractable likelihood models (e.g., Gaussian noise for cavity transmission; effective binomial/multi-shot statistics for qubit readout); performance under realistic model mismatch (drifts, calibration errors, non-Gaussian/heteroscedastic noise, SPAM/readout errors, decoherence modeling errors) is not established. The measurement-setting space in the examples is essentially one-dimensional (drive frequency or evolution time); scalability to genuinely high-dimensional control spaces (many pulse parameters, multi-port settings) is not empirically validated here. Comparisons are limited to simple baselines (random/fixed/sweep) rather than stronger Bayesian design baselines (e.g., Laplace/Gaussian posterior OED, GP surrogates, entropy search variants), so relative gains vs state-of-the-art OED approximations remain unclear.","They suggest mitigating runtime by pre-optimizing across many simulated scenarios and training a supervised model to output the next measurement setting as a deployable policy conditioned on past settings and outcomes. They propose incorporating explicit cost functions so that more expensive measurement settings are penalized. They also mention extending toward likelihood-free active inference without discretizing the input space, and exploring non-greedy sequential strategies (e.g., budgeted planning) including reinforcement-learning agents to outperform greedy information-gain maximization. Finally, they envision generalizing the measurement setting $x$ to represent an entire experimental setup and searching over a broader class of experiments as a step toward an “artificial scientist.”","A practical next step is a robust/self-calibrating version that jointly accounts for nuisance parameters and model mismatch (e.g., unknown noise level, drift, decoherence/readout/SPAM errors) and evaluates robustness of the learned policy under these deviations. Extending the approach to multidimensional control (multi-tone/multi-qubit pulse shaping, simultaneous multi-port probing) with constrained optimization and/or batch (parallel) Bayesian design would better match real quantum-hardware workflows. Providing open-source implementations and standardized benchmarks (common quantum models, noise models, and evaluation metrics like parameter RMSE vs measurement budget) would enable reproducibility and fair comparison against alternative OED methods.",2306.14510v1,https://arxiv.org/pdf/2306.14510v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:49:10Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Prediction|Cost reduction|Other,Other,"Variable/General (design vector dimension $D$; examples shown: $D=1,10,100$; designs initialized in $[-10,10]$)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a stochastic-gradient Bayesian optimal experimental design (BOED) method tailored to simulation-based inference (SBI) with non-differentiable, black-box simulators. It introduces a likelihood-free prior-contrastive-estimation mutual-information lower bound (LF-PCE) by replacing the intractable likelihood in PCE with a normalized neural likelihood estimator (normalizing flow), enabling differentiable optimization of designs $\xi$ jointly with the amortized inference model parameters $\phi$. The objective targets maximizing expected information gain (EIG), i.e., mutual information between parameters $\theta$ and observations $y$ under a design, and adds a regularization term to stabilize training as designs change. Empirically, the approach is validated on a noisy linear regression simulator with non-Gaussian noise, showing EIG improvements across increasing design dimensions and demonstrating that regularization improves stability in high-dimensional design spaces. The resulting learned likelihood surrogate supports posterior inference (via MCMC on $p_\phi(y_o\mid\theta,\xi^*)p(\theta)$) without additional sequential refinement steps in the toy example.","Design criterion is expected information gain / mutual information: $I(\xi)=\mathrm{MI}_\xi(\theta;y)=\mathbb{E}_{p(\theta)p(y\mid\theta,\xi)}[\log \tfrac{p(y\mid\theta,\xi)}{p(y\mid\xi)}]$. The proposed likelihood-free PCE lower bound replaces $p(y\mid\theta,\xi)$ with a neural likelihood $p_\phi(y\mid\theta,\xi)$: $\mathbb{E}[\log \tfrac{p_\phi(y\mid\theta_0,\xi)}{\frac{1}{L+1}\sum_{\ell=0}^L p_\phi(y\mid\theta_\ell,\xi)}]$, with an added stabilization term $\lambda\,\log p_\phi(y\mid\theta_0,\xi)$. Design gradients are obtained by differentiating through the normalizing flow log-density with respect to $\xi$.","On the noisy linear model, LF-PCE’s EIG lower bound increases with design dimension (shown for $D=1,10,100$) and exhibits diminishing returns when scaling from 10D to 100D. In high-dimensional design optimization (notably $D=100$), setting $\lambda=0$ can lead to instability/divergence in optimizing the density estimator, while $\lambda>0$ stabilizes training at the cost of slightly lower EIG. Posterior summaries after optimizing designs (held-out truth $\theta_{\text{true}}=[2,5]$) improve with more design points: for $D=10$, reported posterior mean\,$\pm$\,68% IQR is $\theta_0\approx 0.07\pm1.40$ and $\theta_1\approx 4.87\pm0.16$; for $D=100$, $\theta_0\approx 1.35\pm0.52$ and $\theta_1\approx 4.81\pm0.20$.","The authors note LF-PCE can be unstable because the normalizing-flow data distribution changes as experimental designs $\xi$ are updated during optimization, and they introduce a regularization term $\lambda$ to improve stability. They also discuss an exploration–exploitation-like tradeoff where larger $\lambda$ can reduce the MI lower-bound estimate and yield more homogeneous designs.","Results are demonstrated primarily on a toy noisy linear model; effectiveness on complex SBI simulators (e.g., with high-dimensional observations, strong simulator stochasticity, or severe model misspecification) is not established. The method optimizes a lower bound on MI that can be biased (and depends on the number of contrastive samples $L$ and training dynamics), but sensitivity analyses over $L$, architecture choices, and optimizer settings are limited. Practical guidance for constraints, discreteness, or cost/feasibility of real experimental designs (beyond simple box constraints $[-10,10]$) is not addressed, which can be critical in real BOED deployments.","Future work will examine the tradeoff between design diversity (for better entropy reduction) and robustness of the neural density estimator, analogous to the exploration–exploitation tradeoff in Bayesian optimization.","Extending LF-PCE to constrained, discrete, or mixed design spaces (and incorporating explicit experimental costs) would increase practical applicability. Further evaluation under autocorrelated data, misspecified simulators, and higher-dimensional observation models would clarify robustness in realistic SBI settings. Providing software and reproducible benchmarks (and comparing against additional gradient-based BOED MI estimators such as MINE/InfoNCE variants and alternative SBI surrogates) would strengthen empirical evidence and adoption.",2306.15731v1,https://arxiv.org/pdf/2306.15731v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:49:50Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Optimization,A-optimal|Bayesian A-optimal,Variable/General (design parameter dimension δ; EIT example uses 9 design parameters: electrode currents I1..I9 with I10 constrained),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/vinh-tr-hoang/DOEviaPACE,"The paper proposes a scalable, likelihood-free method for A-optimal Bayesian design of experiments that avoids sampling or integrating over the posterior distribution, targeting settings where the observation map is expensive (e.g., PDE-based). The key idea uses the law of total variance to rewrite the expected conditional variance (ECV) in terms of the variance of a conditional expectation, then approximates the conditional expectation via its L2 orthogonal projection property (PACE), implemented with regression/ANNs. The authors derive asymptotic error bounds for their Monte Carlo estimator of the total ECV (tECV) and argue that posterior intractability does not degrade estimator performance because neither likelihood evaluation nor posterior sampling is required. For continuous design domains, they integrate ANN training with stochastic gradient-based optimization of the design parameters, using a nonlocal conditional-expectation approximation and transfer learning to reduce observation-model evaluations. Numerical experiments (linear-Gaussian tests and an electrical impedance tomography design problem) show substantially fewer forward-model evaluations than importance-sampling-based nested (double-loop) estimators for comparable accuracy.","Observations are modeled as $Y_d = h(Q,d) + \Xi$ (Eq. 1). The A-optimal objective is the total expected conditional variance $V(d)=\sum_{i=1}^n \mathbb{E}[\mathrm{Var}(Q_i\mid Y_d)] = \mathrm{tr}\,\mathbb{E}[\mathrm{Cov}(Q\mid Y_d)]$ (Eq. 6), and the optimal design is $d_A=\arg\min_d V(d)$ (Eq. 7). Using the orthogonal projection characterization of conditional expectation, $\mathbb{E}[Q\mid Y_d]=\phi_d(Y_d)$ with $\phi_d=\arg\min_f \mathbb{E}\|Q-f(Y_d)\|_2^2$ (Eq. 11), they express the tECV as $V(d)=\mathbb{E}\|Q-\mathbb{E}[Q\mid Y_d]\|_2^2$ (Eq. 12) and estimate it via a train/test MC scheme (Eqs. 16–17).","They derive an MAE bound for the PACE-based tECV estimator: $\mathbb{E}|\hat V- V| = O\big((2/\sqrt{\pi N}+2/\sqrt{\pi M})\,\mathbb{E}\|Q-f^*(Y_d)\|_2^2\big)+\epsilon_{S'}$ (Eq. 24), implying relative MAE $O(2/\sqrt{\pi N}+2/\sqrt{\pi M})$ when approximation bias is negligible (Eq. 27). In the 1D linear-Gaussian example, compared to importance sampling (IS), the PACE estimator required ~30× fewer samples for $\sigma_\xi^2=0.012$ and ~200× fewer for $\sigma_\xi^2=0.0012$ at similar accuracy; data augmentation reduced required samples by nearly two additional orders of magnitude. In high-dimensional linear-Gaussian tests (up to $n=20$), IS often failed to converge (or produced NaNs), while PACE remained effective. In the EIT example, to reach ~0.9% relative MAE, IS needed >4000 samples versus ~1000 for PACE with data augmentation, and their continuous-domain optimizer converged in ~15 iterations to an A-optimal current pattern close to $[1,1,1,-1,-1,1,1,-1,-1]^\top$.",None stated.,"The approach relies on learning/approximating the conditional expectation accurately; performance may degrade if the regression/ANN is misspecified, undertrained, or lacks sufficient coverage of the joint $(Y_d,d)$ space, introducing nontrivial approximation bias $\epsilon_{S'}$. Continuous-domain optimization assumes access to (or computability of) $\nabla_d h$ and uses stochastic gradient procedures whose convergence and robustness may depend sensitively on tuning (kernel width for nonlocal sampling, learning rates, augmentation factor, transfer-learning stability). Comparisons focus mainly on IS-based nested estimators; broader baselines (e.g., Laplace-based A-optimality with adjoint/Hessian methods, variational approximations, or alternative amortized design methods) are not systematically benchmarked across multiple nonlinear PDE problems.",None stated.,"Extend the likelihood-free PACE framework to handle model misspecification and non-i.i.d./correlated observation noise (e.g., time-series or spatially correlated errors) and to provide robustness guarantees under distribution shift between training and deployment designs. Develop theory and practical guidance for selecting the nonlocal kernel/coverage strategy and ANN architecture to control approximation bias $\epsilon_{S'}$ in high-dimensional designs, possibly with adaptive sampling/active learning of $(Q,d)$ pairs. Broaden validation to additional nonlinear PDE-constrained inverse problems and include head-to-head comparisons with state-of-the-art gradient/Hessian-based A-optimal design (adjoint methods) and modern amortized Bayesian design approaches, with runtime/accuracy trade-offs and open-source reproducible benchmarks.",2306.17615v2,https://arxiv.org/pdf/2306.17615v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:50:34Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Model discrimination|Cost reduction|Other,Bayesian D-optimal|Other,"Variable/General (design variables include sensor/receiver locations and acquisition/processing choices; examples include 1–10 receivers and 2-receiver AVO networks; model parameter dimensions include 1D (AVO velocity), 2D (source location), and higher with nuisance parameters)",Environmental monitoring|Energy/utilities|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/dominik-strutz/VarBEDfGP,"The paper develops Bayesian optimal experimental design (OED) methods for geophysical experiments using expected information gain (EIG, mutual information) as the design utility, focusing on variational approximations that are novel in geophysics. It presents data-space and parameter-space EIG estimators, including nested Monte Carlo (NMC), a covariance/Gaussian evidence approximation (DN), and variational methods that learn analytic approximations to the evidence p(d|ξ) (variational marginal) or the posterior p(m|d,ξ) (variational posterior), plus neural mutual-information lower bounds (InfoNCE). The key practical contribution is showing that variational methods can compute and optimize EIG far more efficiently than naive NMC, enabling scalable design and enabling “interrogation” (goal-oriented) designs targeting specific questions rather than all parameters. Methods are demonstrated on three schematic applications: seismic source location via arrival-time differences, amplitude-versus-offset (AVO) design for velocity contrast estimation, and survey design for constraining CO2 saturation in subsurface storage, including threshold (classification-style) questions. Benchmarks show DN can be extremely cheap but may fail under multimodal evidence, while variational posterior/MI-bound approaches support stochastic-gradient (one-step) optimization of designs when design gradients are available.","Design utility is the expected information gain: EIG(ξ)=E_{p(d|ξ)}[I(p(m|d,ξ))−I(p(m))], equivalently EIG(ξ)=E_{p(d,m|ξ)}[log p(d|m,ξ)−log p(d|ξ)] (mutual information between m and d). Naive nested Monte Carlo estimates p(d|ξ) with an inner Monte Carlo average and plugs into EIGNMC(ξ)=\frac1N\sum_i \log \frac{p(d_i|m_{i,0},ξ)}{\frac1M\sum_j p(d_i|m_{i,j},ξ)}. Variational marginal replaces p(d|ξ) with a learned q_m(d|ξ) giving EIG_marg≈\frac1N\sum_n \log \frac{p(d_n|m_n,ξ)}{q_m(d_n|ξ)}, while variational posterior learns q_p(m|d,ξ) giving EIG_post≈\frac1N\sum_n \log \frac{q_p(m_n|d_n,ξ)}{p(m_n)}; DN assumes Gaussian evidence so EIG_DN depends on log-determinants of evidence/data covariances.","Across seismic source-location and AVO examples, naive NMC is computationally impractical at moderate sample sizes, while NMC with reused inner-loop samples (NMCre) and variational methods converge to near-optimal designs with far fewer forward evaluations. The DN method can yield strong designs with as few as ~10–100 forward samples but can produce poor designs when the evidence in data space is strongly multimodal; the paper constructs a source-location scenario where DN underperforms many random designs due to covariance inflation from mode separation. For AVO, variational posterior and InfoNCE (lower-bound methods) perform particularly well relative to NMCre and enable stochastic-gradient design optimization; the paper reports large reductions in forward evaluations for SGD-based optimization compared with grid search and sequential construction. In the CO2 saturation survey example, the optimal receiver offsets change substantially depending on whether the goal is estimating all parameters, estimating CO2 saturation specifically, or classifying whether saturation exceeds thresholds (e.g., 10% or 90%), illustrating question-dependent optimal designs.","The authors note that variational approaches depend on the expressiveness and convergence of the chosen variational family (e.g., GMM/MDN/flows) and thus can yield biased EIG estimates if misspecified or poorly trained. They explicitly discuss that the DN method can fail when the Gaussian-evidence assumption is strongly violated (notably under multimodality), and that identifying a priori when DN will fail can be difficult. They also state that one-step stochastic-gradient design optimization requires gradients of the likelihood (or forward model) with respect to design parameters, which may be unavailable or hard to compute for some geophysical forward solvers.","Comparisons are largely on synthetic, schematic problems; broader validation on real geophysical field data and operational constraints (logistics, deployment failures, missing data) is not demonstrated, which may affect practical generalizability. Several methods (variational marginal/posterior, InfoNCE) involve significant tuning choices (network architectures, mixture components, optimization steps) and the paper does not provide standardized guidance or robustness checks across these hyperparameters. The sequential construction heuristic used for multi-receiver designs can converge to local maxima, so reported “optimal” designs may not be globally optimal in higher-dimensional design spaces. The work focuses on EIG as the sole utility; other utilities common in practice (risk-sensitive, cost models with explicit budgets, multi-objective tradeoffs) are not deeply integrated beyond qualitative discussion.","They state that their Python package is in an early stage and will be updated, with user-friendly documentation and tutorials added over coming years. They also point to future potential of extending focused/interrogation designs and of using neural mutual-information bounds and deep/adaptive (amortized) sequential design to scale to larger problems and autonomous settings. The discussion suggests further development and application of these methods for more complex, high-dimensional geophysical scenarios and for likelihood-free settings where explicit likelihood evaluation is not possible.","A valuable next step would be systematic real-data case studies (e.g., operational seismic networks or time-lapse CO2 monitoring) to quantify end-to-end gains and sensitivity to model mismatch, correlated noise, and nonstationarity. Developing principled diagnostics for when DN (Gaussian-evidence) approximations are safe—e.g., multimodality detection in evidence samples—would help practitioners choose methods adaptively. Creating standardized, reproducible benchmarks (common priors, forward models, cost constraints, and compute budgets) would enable fairer comparisons across EIG estimators and optimizers. Packaging the methods with turn-key implementations (including GPU training defaults and hyperparameter heuristics) and integrating with differentiable geophysical solvers would improve adoption and scalability.",2307.01039v2,https://arxiv.org/pdf/2307.01039v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:51:25Z
TRUE,Sequential/adaptive|Optimal design|Other,Prediction|Parameter estimation|Optimization|Other,Other,"Variable/General (tabular MDP with |S| states, |A| actions, horizon H; design targets state-action coverage/uncertainty rather than fixed-factor levels)",Theoretical/simulation only|Other,Exact distribution theory|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Reward-Free Non-reactive Policy Design (RF-NPD), a design-of-experiments procedure for reinforcement learning that uses an offline dataset to design a single non-reactive (memoryless) exploration policy to collect additional online data in one deployment. The key methodological idea is the “sparsified MDP,” which keeps only state-action-to-next-state edges that are sufficiently covered by the offline data and redirects poorly covered edges to an absorbing zero-reward state, combining pessimism (avoid unknown regions) with optimism (explore where uncertainty is high). The exploratory policy is computed entirely offline by simulating a reward-free UCB-style exploration algorithm (RF-UCB) on the empirical sparsified MDP and then using a uniform mixture over the simulated greedy policies as the deployed exploration policy. The main theoretical result gives a high-probability sample-complexity guarantee showing that, after collecting on the order of \(\tilde O(H^2 |S|^2 |A| / \varepsilon^2)\) online episodes, the final policy is \(\varepsilon\)-optimal on the sparsified MDP for any reward revealed later (reward-free setting). The paper further relates the guarantees back to the original MDP under coverage/concentrability conditions, highlighting that sufficient offline coverage can reduce the offline-data dependence on \(\varepsilon\) compared to purely offline RL while keeping the online exploration policy non-reactive.","Sparsified dynamics: \(P^{\dagger}(s'\mid s,a)=P(s'\mid s,a)\) if \(N(s,a,s')\ge \Phi\), otherwise \(P^{\dagger}(s'\mid s,a)=0\) and missing mass is sent to an absorbing state \(s^{\dagger}\) (Eq. 4.2). The RF-UCB exploration bonus is \(\phi(x,\delta)=\tfrac{H}{x}\big[\log(6H|S||A|/\delta)+|S|\log(e(1+x/|S|))\big]\) (Eq. 4.3), and the uncertainty/backward recursion is \(U_h^k(s,a)=H\min\{1,\phi(n_k(s,a))\}+ \hat P^{\dagger}(\cdot\mid s,a)^\top \max_{a'} U_{h+1}^k(\cdot,a')\) (Eq. 4.4). The sparsification threshold is chosen as \(\Phi=6H^2\log(12H|S|^2|A|/\delta)\) (Eq. 5.1), yielding the guarantee \(\max_{\pi} V_1(s_1;P^{\dagger},r^{\dagger},\pi)-V_1(s_1;P^{\dagger},r^{\dagger},\pi_{\mathrm{final}})\le \varepsilon\) (Eq. 5.2).","Theorem 5.1 shows that with probability at least \(1-\delta\), after \(K=\tilde O(H^2|S|^2|A|/\varepsilon^2)\) online episodes collected under a single non-reactive exploration policy, the returned policy is \(\varepsilon\)-optimal on the sparsified MDP for any reward function revealed in the planning phase. When the offline data has full edge coverage (so \(M^{\dagger}=M\)), this implies \(\varepsilon\)-optimality on the original MDP with the same order \(\tilde O(H^2|S|^2|A|/\varepsilon^2)\) trajectories, matching known reward-free lower bounds up to logs. Under partial coverage, the guarantees compete with the best policy restricted to the covered subgraph, and Corollary 5.2 provides conditions under which \(\varepsilon\)-suboptimality on the original MDP holds given sufficient offline samples scaling with a concentrability term and additional \(\tilde O(H^3|S|^2|A|/\varepsilon^2)\) online samples. The work emphasizes that the required offline sample size for global optimality can scale like \(\tilde O(1/\varepsilon)\) (under certain coverage assumptions) rather than \(\tilde O(1/\varepsilon^2)\) typical of purely offline RL, provided additional online data can be collected non-reactively.","The paper notes that fully non-reactive exploration is information-theoretically impossible from tabula rasa with polynomial samples (citing Xiao et al., 2022), so their guarantees necessarily compete on the sparsified MDP defined by offline coverage rather than always on the original MDP. It also emphasizes that the sparsified MDP (and thus performance) depends on the offline dataset’s local/partial coverage through the thresholded counts \(N(s,a,s')\), meaning uncovered regions are effectively treated as absorbing and cannot be improved without additional coverage. The offline-data model assumes i.i.d. sampling of state-actions from a distribution \(\mu\) with next states from \(P(\cdot\mid s,a)\), which constrains applicability to settings matching this logging assumption.","The method is developed for finite tabular episodic MDPs; scalability to large/continuous state spaces would require function approximation and may change both the algorithm and guarantees. The offline “i.i.d. (s,a) from \(\mu\)” dataset assumption is strong compared with typical logged trajectories, and performance/feasibility under trajectory (Markovian) logging, confounding, or behavior-policy drift is not addressed. Practical deployment may be sensitive to the choice of threshold \(\Phi\) (sparsification aggressiveness), but there is limited guidance on tuning beyond the theory, and overly pessimistic sparsification could discard useful low-probability transitions. The paper is primarily theoretical; without extensive empirical benchmarks on standard RL environments, it is unclear how the approach compares in practice to other single-deployment/batched exploration heuristics or to limited-switching algorithms under realistic constraints.",None stated.,"Extend the framework to trajectory-based offline datasets (logged episodes) and to settings with dependence/autocorrelation, aligning assumptions with standard offline RL logs. Develop versions for function approximation (linear/general) and large-scale problems, including practical space/compute-efficient implementations of the virtual exploration on the empirical sparsified model. Provide principled, data-driven methods to select or adapt the sparsification threshold \(\Phi\) and to quantify/diagnose when sparsification is too conservative or too permissive. Validate the approach empirically on benchmark RL suites and real-world domains (e.g., recommender systems/healthcare) with deployment constraints, comparing against other batched/non-reactive exploration and offline-to-online finetuning baselines.",2307.04354v1,https://arxiv.org/pdf/2307.04354v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:52:13Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|E-optimal|Compound criterion|Other,Variable/General (focus on two-parameter models; examples include simple linear regression with 1 regressor and Michaelis–Menten with 1 design variable and 2 parameters),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper reviews and clarifies conflicting claims in the optimal design literature about whether D-optimality can simultaneously reduce estimator correlation and shrink the confidence ellipse for two-parameter models. It develops an analytical criterion for two-parameter models based on R-optimality (product of the variances of the parameter estimators) and shows the identity \(\Phi_R^2 = \Phi_D^2/(1-\Phi_{r^2})\), linking precision (D-optimality) and squared correlation. The authors prove convexity and differentiability of the proposed R-optimal criterion (for two parameters), derive its gradient/directional derivative, and use equivalence-theorem arguments to characterize optimal approximate designs. Performance is illustrated analytically for simple linear regression over general intervals and numerically for the Michaelis–Menten nonlinear model (with design-region truncation near 0), including comparisons to D-, (standardized) A-, EM- (modified E-), and correlation-focused criteria and to compound/Pareto-front selections. Across these studies, R-optimality (and sometimes standardized A-optimality) yields lower estimator correlation than D- or EM-optimality while maintaining high efficiency for precision-oriented criteria.","The model uses the Fisher information matrix \(M(\xi)=\int f(x)^T f(x)\,\xi(dx)\) with \(\Sigma \propto M(\xi)^{-1}\) for two parameters. For two parameters, D-optimality and R-optimality are related to the covariance entries via \(|\Sigma|=\sigma_1^2\sigma_2^2-\sigma_{12}^2\), \(\Phi_R^2 = (M^{-1})_{11}(M^{-1})_{22}\), and squared-correlation criterion \(\Phi_{r^2}=\sigma_{12}^2/(\sigma_1^2\sigma_2^2)\). The key identity is \(\Phi_R^2 = \Phi_D^2/(1-\Phi_{r^2})\), implying R simultaneously trades off determinant-based precision and estimator correlation; a compound criterion is also used: \(\Phi_\lambda(\xi)=(1-\lambda)/\mathrm{Eff}_D(\xi)+\lambda/\mathrm{Eff}_R(\xi)\).","For simple linear regression on \([a,b]\), the D-optimal design is the two-point design at the endpoints with equal weights, while the R-optimal design also uses endpoints but with analytically derived unequal weights \(p_R\) (special cases: \(p_R=1/3\) if \(a=0\), \(2/3\) if \(b=0\), and \(1/2\) if \(b=-a\)). The minimum D-efficiency of the R-optimal design is reported as \(\ge 2\sqrt{2}/3=0.943\), and the minimum R-efficiency of the D-optimal design as \(\ge 3\sqrt[3]{3}/4=0.919\) over the analyzed intervals. In the Michaelis–Menten study (e.g., with \(b=5\) and truncation \([\epsilon,bK]\)), EM- and \(r^2\)-focused criteria can become near-singular (placing essentially all weight at the lower bound when \(\epsilon\) is near 0), whereas D-, standardized A-, and R-optimal designs remain stable two-point designs with similar efficiencies; reported squared correlations for optimal designs remain substantial (e.g., around \(r^2\approx 0.66\)–0.75 depending on \(\epsilon\)), with R and standardized A typically providing the lower correlations among compared criteria.","The authors note that directly minimizing correlation can lead to singular designs, motivating the use of compound criteria rather than correlation-only optimization. For the Michaelis–Menten example, they restrict the design space away from the origin (use \([\epsilon,bK]\) for \(\epsilon>0\)) due to “typical convergence problems near \(x=0\)” and show that some criteria yield degenerate one-point designs when \(\epsilon\) is very small. They also acknowledge that R-optimality uses only the diagonal of the inverse information matrix (not the covariance), which in principle could limit its ability to optimize correlation, even though empirically it performs well here.","Results are developed specifically for two-parameter models; the central identity and convexity/differentiability proofs for R-optimality are not established for general \(m>2\), limiting direct applicability to higher-dimensional problems where correlation structure is more complex. The nonlinear Michaelis–Menten analysis relies on nominal parameter values (local optimal design), and performance under parameter misspecification (robust/Bayesian criteria) is not assessed. “Simulation of two-point designs” for Pareto-front construction is described, but details on sampling scheme, reproducibility, and numerical optimization settings are not provided here, and no software/code is shared, which may hinder replication and broader adoption.",None stated.,"Extend the proposed R-optimality-based decorrelation approach beyond two parameters (e.g., define and analyze convex, differentiable analogs that control overall correlation structure) and compare against established multi-parameter correlation/conditioning criteria. Develop robust (Bayesian/minimax) versions for nonlinear models with uncertain nominal parameters and study sensitivity of correlation reduction to misspecification. Provide open-source implementations and benchmark studies across standard nonlinear kinetics and growth models (including autocorrelated or heteroscedastic errors) to validate practical gains and guide default recommendations for practitioners.",2307.05159v1,https://arxiv.org/pdf/2307.05159v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:52:55Z
TRUE,Sequential/adaptive|Optimal design|Bayesian design|Other,Prediction|Optimization|Cost reduction|Other,D-optimal|A-optimal|Other,"1 factor (viewing angle), selected sequentially from 180 discrete candidates; number of acquired angles varies (e.g., <10; experiments use 3–19)",Manufacturing (general)|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher),https://github.com/tianyuan1wang/SeqAngleRL|https://ieeexplore.ieee.org/document/10572344,"The paper proposes a sequential (adaptive) optimal experimental design method for X-ray CT angle selection, framing the design problem as a Bayesian POMDP and learning a non-greedy policy via deep reinforcement learning (Actor–Critic). The design variable is the next projection angle, chosen online from a discrete set of 180 candidate angles, with the state represented by the current reconstruction (belief state) rather than the raw measurement history. The reward is defined using PSNR relative to ground truth, with two A-optimality-motivated variants: an end-to-end terminal PSNR reward and an incremental PSNR improvement reward. Performance is evaluated via extensive synthetic 2D parallel-beam CT simulations across multiple phantom families (simple shapes through modified Shepp–Logan), including rotations and noise, comparing against an equidistant-angle benchmark. Results show learned policies generally outperform equidistant sampling and exhibit a-posteriori adaptation to object-dependent informative angles; end-to-end rewards yield higher final performance while incremental rewards converge faster during training.","Measurements at angle $\theta$ follow $y(\theta)=A(\theta)\bar{x}+\epsilon$ with Gaussian noise $\epsilon\sim\mathcal{N}(0,\sigma^2 I)$. Reconstructions are obtained by regularized least squares: $\hat{x}(\theta)=\arg\min_x \tfrac12\sum_{k=1}^M \|A(\theta_k)x-y(\theta_k)\|_2^2+\alpha L(x)$ (implemented with SIRT + box constraints). The sequential design is a POMDP with actions as angles and rewards based on PSNR: terminal reward $R=\mathrm{PSNR}(\hat{x}_{M+1},\bar{x})$ (end-to-end) or incremental reward $R=\mathrm{PSNR}(\hat{x}_{k+1},\bar{x})-\mathrm{PSNR}(\hat{x}_k,\bar{x})$.","On ellipses (unseen-rotation test, 300 phantoms), learned adaptive policies outperform equidistant for 3–7 angles; e.g., at 3 angles PSNR is 23.16±1.02 (end-to-end) vs 22.40±0.74 (equidistant), and at 7 angles 26.87±1.06 vs 26.73±0.62. On triangles (unseen-rotation test, 300 phantoms, 5 angles), PSNR improves substantially: 24.07±2.07 (end-to-end) vs 20.64±1.05 (equidistant). On mixed phantoms (unseen-rotation test, 900 phantoms, 7 angles), PSNR is 24.85±1.64 (end-to-end) vs 22.94±0.76 (equidistant). For complex phantoms d7/d8 (unseen-rotation tests, 300 phantoms), end-to-end policies beat equidistant across 3–19 angles (e.g., d7 at 3 angles: 20.09±0.95 vs 18.67±0.33; d8 at 19 angles: 28.01±0.39 vs 27.66±0.16).","The authors note limited generalization when phantoms differ substantially in shape and intensity values compared to training, and emphasize that a robust training dataset is crucial. They observe sub-optimal behavior where the agent sometimes repeats or clusters angle selections, suggesting the policy network capacity/structure may be insufficient to avoid repetition. They also restrict experiments to synthetic 2D parallel-beam CT to keep scenarios interpretable and with known strategies, acknowledging this is not fully realistic. They point out performance drops under measurement noise, attributing much of the difficulty to reconstructing from noisy measurements.","The method assumes access to ground-truth images during training to compute PSNR-based rewards; in real industrial CT, ground truth is typically unavailable, so deploying the same reward formulation requires surrogates or self-supervised/unsupervised objectives. The action space is discretized to 180 angles in [0°,180°), which may limit applicability to continuous trajectories and physical constraints (mechanics, dose/time) unless additional constraints are incorporated. The evaluation is primarily against an equidistant baseline; comparisons to strong adaptive OED heuristics (e.g., information-gain approximations, DEIM-based subsampling, Bayesian linearized methods) are limited in the reported experiments. Computational cost and latency of reconstruction-in-the-loop decision making (SIRT iterations per step) are not fully characterized for real-time in-line deployment.","The authors propose replacing SIRT with deep learning-based reconstruction methods trained end-to-end to accelerate and improve reconstruction quality. They plan to test against datasets containing defects and incorporate defect detection into the cost/reward function. They intend to improve the policy network to mitigate repeated angle selections. They also plan to extend from 2D parallel-beam to more realistic 3D geometries with more degrees of freedom (e.g., tilting, zooming), strengthen generalization, and validate on actual CT scans.","Develop reward surrogates that do not require ground truth (e.g., posterior uncertainty, self-consistency, task-based defect detectability, or learned reward models) to enable training on real scans. Incorporate explicit constraints and multi-objective costs (scan time, dose, motion limits) via constrained RL or safe RL, and allow continuous-angle/trajectory parameterizations. Benchmark against additional adaptive sampling/OED baselines under matched computational budgets, and report wall-clock decision time to substantiate in-line feasibility. Extend to robustness under model mismatch (calibration errors, beam hardening, scatter, misregistration) and to domain adaptation from simulation/CAD to real data.",2307.06343v2,https://arxiv.org/pdf/2307.06343v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:56:35Z
TRUE,Computer experiment|Bayesian design|Sequential/adaptive|Optimal design|Other,Prediction|Cost reduction|Other,Other,"Variable/General (application varies melt parameters incl. average melt rate and melting exponent m; fidelity is spatial resolution, e.g., 5 and 10 km; mentions 1–10 km in general)",Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Not provided,http://github.com/SheffieldML/GPy,"The paper develops a multi-fidelity experimental design (MFED) approach for expensive computer simulations, targeting ice-sheet modeling where high-resolution runs can take weeks. It models simulator outputs across both physical input parameters and a continuous fidelity axis (spatial resolution) using Gaussian process surrogates, and selects new runs by maximizing a cost-adjusted reduction in conditional integrated variance (CIV) at the highest fidelity. To address uncertainty in GP hyperparameters (notably the fidelity-axis lengthscale), it proposes an MF-UCB algorithm that chooses an optimistic lengthscale quantile (via a confidence parameter ν) to balance exploration across fidelities and exploitation for uncertainty reduction. The method is analyzed through a submodular maximization/knapsack framing, yielding regret-style bounds that separate greedy approximation error from lengthscale-identification error. A case study using the WAVI ice-sheet simulator (with precomputed ensembles at 5 and 10 km resolution) illustrates when low-fidelity runs are informative and how cost-adjustment can favor cheaper resolutions depending on model configuration (e.g., partial melting setting).","The simulator is modeled as $f:(\theta,\tau)\mapsto \mathbb{R}^D$ with budget constraint $\sum_i c(\theta_i,\tau_i)\le C$, and a GP surrogate over joint input–fidelity space. The per-point utility is the reduction in conditional integrated variance at highest fidelity: $U(x,X)=\int \sigma_X(\theta,\tau=0)\,d\theta-\int \sigma_{X\cup x}(\theta,\tau=0)\,d\theta$, and sampling uses a greedy cost-adjusted rule $\arg\max_x U(x,X)/c(x)$. MF-UCB selects a lengthscale $\beta_i$ from an upper-confidence quantile defined by $p_e(\beta\ge \beta_i)\le \nu$, then chooses $x_i=\arg\max_x U_{\beta_i}(x)/c(x)$ under remaining budget.","On the WAVI ice-sheet example (using pre-defined ensembles at 5 km and 10 km), the learned GP lengthscale over the fidelity axis determines whether low-fidelity (10 km) simulations transfer information to high fidelity: with partial melting enabled, the inferred relationship across fidelities is strong enough that 10 km is often the best cost-adjusted choice. With partial melting disabled, the model learns low cross-fidelity correlation, and the CIV-based utility surface effectively discards the 10 km fidelity as uninformative for reducing uncertainty at highest fidelity. The paper reports that a 5 km simulation is approximately 10× more expensive than a 10 km simulation and can take about a week, motivating substantial potential savings via MFED when low fidelities are informative. Theoretical analysis provides a bound of the form $F(X_G)>(1-e^{-1})F(X^*)-\sum_i \epsilon_i(x_i^G)c(x_i^G)$, separating greedy knapsack approximation from identification error due to unknown lengthscale.","The experiments use a pre-defined/constructed ensemble of runs (rather than actively running the simulator adaptively from MF-UCB suggestions), so the experimental results are retrospective. The reported experiment summarizes simulator output into a single metric (sea level contribution, SLC), which the authors state can mask different ice-sheet behaviors that could be informative. They also note that visualizations are based on training the GP on only 6 datapoints, which can lead to unstable training compared with realistic deployments (they expect >50 runs).","The design objective integrates variance only over $\theta$ at $\tau=0$, so performance may depend heavily on how the integration measure over $\theta$ is chosen/approximated and may be difficult in higher-dimensional parameter spaces. The method assumes a GP surrogate with an RBF-style kernel and relies on accurate posterior learning of the fidelity-axis lengthscale; misspecification (nonstationarity, discontinuities across fidelities, or heteroskedastic simulation noise) could degrade both utility estimation and the UCB heuristic. Empirical comparisons to established MF acquisition/design baselines (e.g., multi-fidelity IMSE variants, entropy-based criteria, or alternative cost-aware BO/ED methods) appear limited in the provided text, making it hard to attribute gains to MF-UCB specifically. Reproducibility is constrained because implementation details are mentioned (GPy/Emukit, HMC settings) but the full code and datasets are not provided.","The authors plan to run new simulations based on MF-UCB suggestions at higher resolutions (1–5 km) and compare MFED performance across different choices of the confidence parameter ν as well as against other MFED methods. They also aim to move beyond a single scalar summary (SLC) to capture richer ice-sheet behaviors that could provide additional information. More broadly, they hope to demonstrate MFED can be routinely incorporated into glaciology workflows for cost-efficient probabilistic sea-level projections.","Developing a fully online, end-to-end adaptive campaign (with stopping rules and Phase I hyperparameter stabilization) would strengthen practical deployment beyond retrospective analysis on a fixed ensemble. Extending the surrogate/design to multivariate spatial outputs (e.g., thickness fields) or multiple correlated metrics with multi-output GPs could align better with the simulator’s rich outputs. Robustness studies under autocorrelated/biased fidelities, nonstationary cross-fidelity relationships, and alternative cost models (including queue/wall-clock variability) would clarify when MF-UCB is safe to use. Providing an open-source reference implementation and standardized benchmarks for ice-sheet MFED would improve reproducibility and facilitate fair comparison to other MFED criteria.",2307.08449v1,https://arxiv.org/pdf/2307.08449v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:57:15Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,Other,Variable/General (design variables include target distance/eccentricity and target width/size; parameters estimated include 1–4 user-model parameters across studies),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,https://doi.org/10.1145/3544548.3581483|https://arxiv.org/abs/2307.09878|http://jmlr.org/papers/v22/20-1364.html,"The paper proposes an amortized sequential optimal experimental design method for efficiently estimating parameters of simulation-based (POMDP/RL) user models in HCI pointing tasks. Instead of computing Bayesian-optimal designs online (e.g., via expected information gain), it trains an RL “Analyst” policy offline using synthetic users sampled from a prior over model parameters, so that at deployment it can adaptively choose the next experimental trial and output parameter estimates with low latency. The method is demonstrated in three studies of increasing complexity: (1) estimating a single movement/submovement noise parameter from summary trial outcomes, (2) jointly estimating multiple parameters (oculomotor noise, perceptual noise, and a movement-time intercept) from full gaze-fixation sequences, and (3) extending to preference inference by additionally estimating a speed–accuracy tradeoff parameter. Across studies, Analyst-chosen designs (e.g., target width and eccentricity) yield faster error reduction and better parameter recovery than random design baselines, illustrating non-myopic and adaptive design behavior in likelihood-free settings. The approach advances DOE for user-model fitting by amortizing design computation via RL, enabling rapid personalized parameter estimation with few trials after offline training cost.","The Analyst’s experimental-design/estimation problem is posed as a POMDP where the action is $(d_{t+1},\theta_e)$: the next design and current parameter estimate. Training reward is the negative parameter-estimation error, $r(s_t,a_t)=-\lVert \theta_p-\theta_e\rVert_1$, which couples design choice to estimation quality over a sequence (non-myopic objective). The underlying user model is also a POMDP; for gaze-based pointing, oculomotor noise scales with saccade amplitude $\sigma_{\text{ocular}}(t)=\rho_{\text{ocular}}\,\text{Amplitude}(t)$ and perceptual uncertainty scales with eccentricity/size, e.g., $\sigma_o(t)=\rho_{\text{spatial}}\,\text{eccentricity}(t)-\rho_w\,w+\rho_b$; beliefs are updated via a Kalman-filter-style Bayes update (Eq. 1).","In two abstract demonstrations, the learned non-myopic design policy outperforms a one-step lookahead baseline on function-reconstruction discrepancy (Analyst 0.0812 vs baseline 0.1580) and IMSE (0.3909 vs 0.7087), and adaptive design beats non-adaptive and random baselines on logistic-parameter estimation MSE (2.018 vs 6.265 vs 14.434). In Study 1 (summary outcomes), Analyst-chosen target designs reduce parameter-estimation error faster than random designs and recover the movement-noise parameter with high correlation when perceptual noise is low (reported $R^2$ up to 0.80). In Study 2 (full gaze sequences), Analyst jointly recovers perceptual noise ($R^2=0.76$), oculomotor noise ($R^2=0.84$), and movement-time intercept ($R^2=0.99), selecting small targets and large eccentricities; optimized designs again beat random designs over successive experiments. In Study 3 (adding preferences), Analyst can simultaneously infer four parameters including a speed–accuracy preference, though with reduced accuracy and skew compared to capacity parameters; optimized designs still outperform random-design inference.","The authors note the work is a preliminary in-silico investigation and must be validated with human participants; despite prior evidence of human-like behavior in the simulators, further work is needed to confirm real-world effectiveness. They also state generality beyond the tested abstract problems and pointing tasks is unproven and requires broader empirical testing on other HCI tasks. They acknowledge substantial offline training cost (reported as roughly 3–9 hours wall time on a laptop for their simulations) and that more extensive hyperparameter tuning could improve both optimized and random baselines.","The design objective is not expressed in standard DOE optimality terms (e.g., explicit D-/A-/I-optimality or EIG maximization), making it harder to compare theoretically to classical BOED/OED and to reason about statistical efficiency/identifiability guarantees. Parameter recovery is evaluated primarily on simulated users drawn from the same prior/model class used for training, so robustness to model misspecification, distribution shift (different populations/devices), and realistic human-data artifacts (nonstationarity, fatigue, autocorrelation) is unclear. The approach depends on substantial simulator fidelity and on RL training stability; without released code and without ablations on architecture/reward choices, reproducibility and sensitivity to implementation details remain uncertain.","They propose testing the method with human participants (including deployment with an eye tracker) to assess practical viability and potential for real-time, low-latency experimental design and fitting. They also suggest evaluating generality by applying the approach to a broader range of HCI tasks (e.g., menu search, decision making, biomechanical control). Additionally, they highlight the need for improved hyperparameter selection/tuning as task complexity increases, and they discuss exploring implications for personalization and related HCI problems (e.g., adapting interface parameters and potentially enhancing A/B testing and recommenders via fitted user models).","Develop self-starting/online recalibration variants that can safely adapt when the user’s parameters drift over time (learning, fatigue) and that quantify uncertainty (e.g., posterior/credible intervals) rather than only point estimates. Add robustness studies under misspecified simulators and non-ideal data (measurement noise, dropped fixations, time-correlated behavior) and benchmark against strong likelihood-free BOED baselines (e.g., MI estimators) under matched compute budgets. Provide open-source implementations and standardized evaluation suites so different amortized DOE methods for HCI user models can be compared fairly on identical simulators and human datasets.",2307.09878v1,https://arxiv.org/pdf/2307.09878v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:57:59Z
TRUE,Factorial (full)|Factorial (fractional)|Screening|Robust parameter design|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,6 factors (beam model) and 7 factors (crack growth simulation); Variable/General across tested designs,Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,http://archive.org/details/jresv70Cn4p263,"The paper compares three methods for estimating output variability induced by uncertainty in multiple independent inputs: propagation of error (PE), Monte Carlo (MC), and a designed-experiments-based tolerance design (TD) approach. TD uses two-level designs (orthogonal/factorial arrays) to approximate the effect of input variances on response variance with far fewer runs than MC. The methods are demonstrated on (i) an analytical linear elastic 3-point bending beam model with six input parameters and (ii) a nonlinear XFEM crack-growth simulation in an aluminum plate with seven uncertain parameters. Across coefficients of variation from 1% to 20% in the beam model, PE performs well for small variability but degrades as variability increases, while TD with moderately sized arrays provides accurate variance estimates with far fewer trials than MC. In the nonlinear crack model (5% CV), TD accuracy improves with larger arrays, achieving near-MC variability estimates while greatly reducing computational time, supporting the use of DOE-based variance estimation for complex simulations and to guide physical testing.","PE variance estimate is computed via linearization at the center point: $S_v = \sqrt{\sum_i \sigma_i^2\left(\partial v/\partial x_i\right)^2}$ (Eq. 1; expanded for 6 parameters in Eq. 4). MC estimates response standard deviation from $N$ trials using the sample SD: $S_v = \sqrt{\sum_{i=1}^N (v_i-\bar v)^2/(N-1)}$ (Eq. 2). TD represents each factor by two levels (typically $\mu\pm\sigma$) and uses structured 2-level orthogonal/factorial arrays (e.g., 8–64 runs for 6 factors; 8–128 runs for 7 factors) to estimate response variance via model fitting/ANOVA.","For the beam model, PE relative error versus MC(10,000) is very small at low variability (0.06% at 1% CV; 0.58% at 2% CV) but grows substantially at higher variability (14.83% at 10% CV; 60.62% at 20% CV). TD accuracy depends on array size: at 5% CV, TD errors range from 12.80% (8-run) down to 0.17% (32-run); at 10% CV, TD(16) achieves 0.57% error while PE is 14.83%. For the crack-growth simulation (7 factors, 5% CV), MC(10,000) gives mean crack opening displacement $4.0934\times10^{-5}$ m and SD $4.0811\times10^{-6}$ m; TD SD estimates improve with array size, with relative error ranging from 16.09% (12-run) to about -1.53% (32-run), and within ~3% for 64- and 128-run arrays.","The authors note TD accuracy depends on whether the assumed significant parameters are adequate and on any underlying correlations among parameters. They also acknowledge it remains unexplored how well the methods perform when different parameters have different coefficients of variation (rather than a common CV across parameters in their experiments). For the crack model, they note some parameters held constant (e.g., plate thickness) may still significantly affect crack opening displacement, implying potential omitted-variable effects.","TD/Taguchi-style two-level designs can be biased for strongly nonlinear responses or substantial interactions/curvature; using only $\mu\pm\sigma$ levels may not capture tail behavior or bimodality in responses (notably the crack-opening distribution appears bimodal). The study largely assumes input independence and Gaussian variability and does not analyze sensitivity to non-normal inputs or correlated factors, which are common in tolerancing contexts. The work reports variance/SD estimation accuracy but provides limited guidance on confidence intervals/uncertainty of the TD variance estimator itself (especially for small arrays) and limited comparison to alternative modern UQ methods (e.g., polynomial chaos, Latin hypercube, Sobol sampling).",The authors propose an efficiency study of the TD method under lower variability in the parameter space. They also suggest repeating the study with different variabilities for different parameters in each trial to further investigate TD’s variance-estimation capability in simulation experiments.,"Extend TD-based variance estimation to handle correlated inputs and non-Gaussian distributions (e.g., via copulas or moment-matching at multiple quantiles rather than $\pm\sigma$). Evaluate and incorporate interaction/curvature diagnostics and sequential augmentation (e.g., foldover or response-surface follow-up) when nonlinearity is suspected, especially for crack-growth models. Provide estimator uncertainty quantification for TD-derived variance (e.g., bootstrap over fitted models/ANOVA, or Bayesian variance components) and validate on additional real experimental datasets to assess transfer from deterministic simulations to physical testing.",2307.10161v1,https://arxiv.org/pdf/2307.10161v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:58:41Z
FALSE,NA,NA,Not applicable,Not specified,Other,Case study (real dataset)|Other,TRUE,Other,Not provided,https://doi.org/10.1145/3611049|https://arxiv.org/abs/2307.11297,"This paper introduces “Fused Spectatorship,” an HCI interaction design approach in which spectators loan control of their hands to a computational electrical muscle stimulation (EMS) system and then spectate their own (EMS-driven) hands playing games. The authors design and implement a wearable EMS kit and Android app, and instantiate the concept through three hand-gesture games (two competitive, one collaborative) that use EMS to actuate fine and gross motor movements. They evaluate the concept via an in-the-wild qualitative study (n=12) with a pre-study onboarding session, at-home use with video recording, and post-study semi-structured interviews analyzed using inductive thematic analysis. Findings report four user-experience themes and four “fused spectator types,” and participants often could not distinguish whether they were spectating or playing, motivating design and ethical considerations for systems that can take bodily control. The work is methodological/design-oriented rather than DOE-focused, and it does not propose or evaluate experimental design (factorial/optimal/RSM) methods.",Not applicable,"The evaluation involved 12 participants (mean age 30.9, SD 6.52) who used the EMS game kit in-the-wild for one day and were interviewed post-study (mean interview duration 74.38 minutes, SD 14.35). Two coders performed inductive thematic analysis in NVivo, producing 102 final codes with reported inter-rater reliability 96.13% and Cohen’s kappa 0.753; four reported themes were built from 76 codes covering 658 of 930 data units. The authors identify four fused spectator types (Inquisitor, Challenger, Onlooker, Entertainer) and report that 11/12 participants could not distinguish whether they were watching their hands play or playing themselves. Participants played each game at least 7 times (max reported 15) during the at-home phase.","The authors note EMS calibration depends on participants’ body composition, which meant some participants could not fully experience spectating all games. They acknowledge the small sample size (n=12), though argue each participant played each game multiple times (minimum 7, maximum 15). They also note only 3 of 12 participants had no prior EMS experience, limiting conclusions about first-time EMS users.","The study varies multiple factors simultaneously (game type/order, EMS intensity chosen by participants, solo vs shared/social use, and uncontrolled home environments), making it difficult to attribute experiences to specific design features; this is not framed as an experimental design limitation. The qualitative findings may be sensitive to novelty effects and short exposure (one day), and longer-term habituation, safety, and acceptance are not tested. The work provides limited detail on software implementation reproducibility (e.g., precise control-loop timing, logging schema) and does not provide code, which limits independent validation or extension.","The authors suggest future studies with more participants—especially more without prior EMS experience—to examine whether experiences differ. They propose future work on designing more tailored/adjustable EMS controls matched to different fused spectator types and users’ bodily awareness and ease of loaning control. They also suggest deliberately designing social fused spectatorship experiences/performances (e.g., adding affective loops).","A next step would be to run more controlled comparative studies (e.g., EMS vs non-EMS actuation/haptics, or screen-based spectating vs fused spectatorship) to isolate which system features drive key outcomes like agency ambiguity and engagement. Extending evaluation to longer deployments would clarify learning curves, durability of effects, and risks of overuse beyond a single day. Developing and releasing an open-source reference implementation (Android app + microcontroller firmware) and standardized calibration protocols could accelerate replication and benchmarking across labs and application contexts (e.g., accessibility, rehabilitation, esports skill transfer).",2307.11297v4,https://arxiv.org/pdf/2307.11297v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T05:59:41Z
TRUE,Split-plot|Factorial (full)|Other,Other,Not applicable,Variable/General,Theoretical/simulation only|Food/agriculture|Other,Other,TRUE,R,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/emitanaka/edibble|https://CRAN.R-project.org/package=edibble,"This preprint proposes a computational framework—“the grammar of experimental designs”—that provides a unified, declarative language for specifying experimental designs as mutable objects in an object-oriented system. The framework explicitly encodes experimental structure via factors with roles (treatment, unit, record) and their relationships, represented internally as directed acyclic graphs and rendered ultimately as a tabular design table. It is positioned as complementary to existing design-generation algorithms by standardizing structural specification and exposing intermediate design-building steps to improve communication, transparency, and cognitive engagement during planning. The framework is implemented in the R package edibble and illustrated through examples including a classic split-plot design, a complex nested split–split-plot style design, and an unbalanced factorial/confounded-by-experiment setup. The contribution is primarily an interface/representation and process-oriented “grammarware” for DOE specification rather than a new optimality-driven design construction method.",Not applicable,"Not specified; the paper primarily demonstrates the framework via worked examples (e.g., Fisher’s split-plot design with 36 patches × 3 plots, a nested weekly strip/swath/pen/chick design, and an unbalanced 2×2 factorial confounded with 4 experiments) rather than reporting quantitative efficiency/optimality metrics.","The authors note that several functionalities are not discussed or demonstrated (e.g., intended observational record specification, embedded data validation for entry, simulation of observational records, diagnostics/visualization, and deeper discussion of abstract syntax/internal representation). They also state the framework does not address all possible experimental structures and that the default treatment-assignment mechanism is currently simplistic and may be unsuitable or inefficient in many cases; the goal is standardizing specification, not producing the most efficient/optimal design for every structure.","Because the focus is on specification/representation, the paper provides limited empirical evaluation of usability (e.g., user studies) or interoperability with downstream analysis/modeling workflows, which are central to practical adoption. The examples emphasize random/systematic allocation but do not deeply address constraints common in practice (e.g., restricted randomization, spatial/temporal correlation, randomization tests, or multi-site/multiphase logistics) beyond nesting. Reproducibility is shown via seeds, but there is limited discussion of versioning/provenance for iterative design changes in collaborative settings (audit trails, diffing, validation of invariants).","The authors envision extensions to handle broader experimental structures (e.g., undetermined numbers of levels or complex conditional structures) and indicate that features such as data validation, simulation of records, diagnostics, and automated visualization are subjects of future papers/avenues. They also suggest the treatment-assignment step can be substituted with alternative (potentially more sophisticated) methods, implying future integration with more advanced design-generation algorithms.","A natural next step is formal usability/communication evaluation (controlled user studies comparing error rates, comprehension, and time-to-specification versus conventional DOE software and APIs). Providing standardized export/import bridges (e.g., to common DOE/analysis objects in R and other languages) and a stable interchange format (e.g., JSON schema for the factor/level graphs) would strengthen the “unified language” claim across ecosystems. Adding explicit support for constrained/randomization-restricted designs (e.g., spatial blocking, serpentine orders, hard-to-change factors) and validating randomization properties would broaden applicability. Open-source reference implementations in additional languages (e.g., Python/Julia) could test the language-agnostic core and promote cross-domain adoption.",2307.11593v2,https://arxiv.org/pdf/2307.11593v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:00:11Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Computer experiment|Other,Prediction|Model discrimination|Parameter estimation|Screening|Cost reduction|Other,Other,"Variable/General (examples include 1D, 2D, and 3–4 experimental parameters)",Other|Theoretical/simulation only,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/matthewcarbone/ScientificValueAgent,"The paper proposes a “Scientific Value Function” (SVF) that converts high-dimensional experimental observables (e.g., diffraction patterns or spectra) paired with input conditions into a continuous scalar utility, enabling optimal experimental design even when there is no explicit scalar optimization target (epistemic goals such as mapping/characterizing a space). The SVF is used as an objective within sequential Bayesian optimization (the “Scientific Value Agent”, SVA), typically via a Gaussian process surrogate with Matérn kernel and acquisition functions such as Expected Improvement (EI) and Upper Confidence Bound (UCB). The approach is demonstrated on simulated 1D and 2D phase-boundary mapping problems (XRD and spectroscopy) and on an emulated variable-temperature total-scattering study of BaTiO3, showing that SVF-driven adaptive sampling concentrates measurements near regions of rapid change/phase transitions and can markedly outperform a fixed grid strategy in simulations. A passive (non-adaptive) real-data demonstration analyzes a UV–vis nanoparticle synthesis dataset (375 conditions) to highlight high-value regions for human-in-the-loop decision support. Overall, the work advances DOE for autonomous/accelerated experimentation by providing a flexible, model-free (but model-augmentable) utility that makes Bayesian optimization applicable to characterization-focused campaigns.","The Scientific Value Function is defined for a candidate datum (xi, yi) relative to the current dataset DN as:  \n\n$$U(x_i,y_i\mid D_N)=\sum_{(x_j,y_j)\in D_N} g(y_i,y_j)\exp\left\{-\tfrac12\,\frac{h(x_i,x_j)^2}{h_{\min}(x_i\mid D_N)^2}\right\},$$\n\nwhere h(·,·) measures distance/correlation in input space (default Euclidean), g(·,·) measures distance/correlation in observation space (default Euclidean, but can be replaced by physics- or embedding-based metrics), and hmin is the nearest-neighbor distance in input space used for regularization. The SVF values are modeled with a GP (Matérn kernel, homoskedastic noise), scaled to [0,1], and optimized sequentially using Bayesian optimization acquisition functions (EI or UCB with variance weight β≈10 reported).","Across the simulated 1D XRD and 2D periodic-interface studies (each averaged over 300 independent experimental campaigns), SVF-driven Bayesian optimization (EI or UCB) concentrates samples around sharp transition regions and reduces reconstruction error (MSE of reconstructed phase fractions) by roughly an order of magnitude compared with a fixed “optimal grid” sampling baseline for comparable numbers of measurements. In the 2D case, a representative run with N=250 samples shows dense sampling along the sinusoidal phase boundary while still covering surrounding regions. For BaTiO3 variable-temperature total scattering, SVF with only Euclidean spectral distance performs roughly on par with grid sampling, but incorporating a deep-learned embedding into g(yi,yj) enables autonomous up-sampling around subtle phase transitions. A real UV–vis nanoparticle synthesis dataset (N=375 runs; 4 process parameters with a fixed-volume constraint reducing degrees of freedom to 3) is analyzed to produce a per-condition “scientific value” map that highlights regions of dramatic spectral change for expert follow-up.","The paper notes that Bayesian-inference baselines (cluster-then-classify uncertainty sampling) can be strongly model/setting dependent (e.g., sensitive to the chosen number of clusters) and may be less performant than grid search at low N, motivating the choice of grid search as the main benchmark. It also indicates that for BaTiO3 bulk total-scattering data, first-order transitions appear gradual/continuous and a grid search can reconstruct bulk compositions as well as or better than the SVF approach, implying limits when the chosen observable/correlation does not expose the targeted physics. The authors emphasize that performance depends on the choice of correlation functions h and g and that domain knowledge (e.g., embeddings) can be needed for subtle transitions.","The method’s behavior depends critically on the hand-chosen observation-space metric g(yi,yj); Euclidean distance may be poorly calibrated to perceptual/physical differences and can be sensitive to scaling, noise level, preprocessing, and irrelevant spectral features, potentially requiring careful normalization or learned representations. The SVF is summed over all past points, which can become computationally expensive (O(N) per candidate evaluation and heavy GP training costs) for very large campaigns unless approximations (sparse GPs, batching, subsampling) are used. Reported comparisons focus primarily on grid search and one clustering-based active baseline; broader benchmarking against established active learning/information-gain designs, batched BO, and space-filling adaptive strategies would strengthen generality. The approach assumes a meaningful distance structure over input space and does not directly address constraints such as discrete/categorical factors, expensive switching (split-plot), replicates, or heteroskedastic/no-autocorrelated measurement noise common in real experiments.","The authors state that the SVF formulation is compatible with existing optimization tools and can be extended to multi-modal and multi-fidelity experiments, and can integrate existing models of an experimental system through the flexible correlation functions. They also suggest deployment in collaborative analysis tools and human-interfaced acceleration platforms (human-in-the-loop visualization/decision support) beyond fully autonomous operation.","Develop batched/parallel SVA variants for facilities where multiple measurements are taken simultaneously, including acquisition strategies that balance diversity and exploitation under SVF. Provide principled guidance (or automated learning) for choosing/learning g(yi,yj) and h(xi,xj), including invariances, normalization, and uncertainty-aware distances, to improve robustness across instruments and modalities. Extend to constrained and mixed-variable DOE settings (categorical + continuous factors, fixed-sum mixture constraints, and hard-to-change factors/split-plot costs) with explicit constraint handling and switching penalties. Add statistical guarantees or diagnostics (e.g., coverage/uncertainty maps, stopping rules) and more extensive real-world validations across different experimental domains and noise/autocorrelation regimes.",2307.13871v1,https://arxiv.org/pdf/2307.13871v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:00:52Z
FALSE,NA,NA,Not applicable,"Variable/General (FL experimental variables such as heterogeneity type/level, local epochs E, sample rate C, communication rounds T, number of clients N)",Other,Simulation study,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/MMorafah/FedZoo-Bench,"This paper is a comprehensive empirical study of federated learning (FL) experimental variables under statistical heterogeneity, aiming to standardize and improve the comparability of FL experiments rather than proposing a classical design of experiments (DOE) methodology. It analyzes how FL-specific factors—data partitioning mechanism and heterogeneity level (Label Dirichlet and Label Skew), local epochs, client sampling rate, and communication rounds—interact and can cause evaluation metric failures or misleading conclusions. The authors recommend more robust evaluation metrics for global and personalized FL (e.g., averaging performance over the last [C·N] rounds for global FL and evaluating on all available local test data for personalized FL) and provide suggested ranges for key experimental settings (e.g., sample rate 0.1–0.4). They introduce FedZoo-Bench, an open-source PyTorch-based benchmark implementing 22 state-of-the-art FL methods under unified, configurable settings to support reproducible comparisons. Extensive experiments on image classification benchmarks (e.g., CIFAR-10/100 with LeNet-5/ResNet-9) show that algorithm rankings are highly setting-dependent and that simple baselines (FedAvg; FedAvg+Fine-Tuning) remain competitive, underscoring the need for standardized experimental setups and reporting practices.","Key FL setup variables include client sampling rate $C\in(0,1]$, local epochs $E$, batch size $B$, learning rate $\eta$, and communication rounds $T$. Global FL objective: $\theta_g^*=\arg\min_{\theta_g}\sum_{i=1}^N\mathbb{E}_{(x,y)\sim D_i^{train}}[\ell(f_i(x;\theta_g),y)]$. Personalized FL objective: $\{\theta_i^*\}_{i=1}^N=\arg\min_{\{\theta_i\}}\sum_{i=1}^N\mathbb{E}_{(x,y)\sim D_i^{test}}[\ell(f_i(x;\theta_i),y)]$; FedAvg fusion shown as $\theta_g^{t+1}=\sum_{k\in S_t}\frac{|D_k|}{\sum_{j\in S_t}|D_j|}\,\theta_k^{t+1}$.","The study shows that reported global FL accuracy can vary substantially depending on which final communication round is reported (illustrated with up to about 7 percentage points variation over the last 10 rounds in an example), motivating reporting averages over multiple final rounds. It finds that larger client sample rates mitigate heterogeneity effects, with guidance that $C\ge 0.4$ can mask heterogeneity while $C<0.1$ can yield poor convergence; they recommend $0.1\le C\le 0.4$. It demonstrates that FedAvg tends to prefer fewer local epochs under heterogeneity while FedAvg+Fine-Tuning benefits from more local epochs up to a point (around 5–10 depending on heterogeneity). In benchmark comparisons (e.g., gFL setting #1 on CIFAR-10/LeNet-5 with Label Skew 80%), Scaffold achieves higher reported accuracy than FedAvg/FedProx, while in another setting FedProx is best; across personalized settings, no single method is consistently best and simple FedAvg+FT remains competitive.","The authors note that inconsistency across prior FL papers’ experimental settings and ambiguous evaluation metrics can prevent comprehensive comparisons, motivating their recommendations and benchmark. They also mention that high sample rates are generally unfavorable because they increase communication cost, even though they can reduce heterogeneity effects in experiments. For personalized FL newcomers, they acknowledge that adaptation procedures are not always explicitly clear for many methods, so they follow prior work’s procedure (fine-tuning) except where methods specify otherwise.","Although framed as “experimental design,” the work does not implement formal DOE methodology (e.g., factorial/optimal designs, power analyses, or variance decomposition) to quantify main/interaction effects; conclusions are based on selected grids of settings. The empirical study centers heavily on vision datasets (CIFAR-10/100) and a limited set of architectures/optimizers, so recommended ranges (e.g., for $C$ and $E$) may not generalize to other modalities, large-scale models, or strongly nonstationary client behavior. Reporting averages over only 3 runs may be insufficient for high-variance FL settings, and the paper does not deeply analyze sensitivity to other confounders it cites (augmentation, preprocessing, LR schedules) within a controlled ablation framework.","They plan to expand the experimental-variable study to other domains such as natural language processing and graph neural networks to assess how recommended FL experimental settings behave beyond vision. They intend to extend FedZoo-Bench by implementing more algorithms and adding new features, and to establish an open leaderboard for systematic evaluation across datasets and settings. They also suggest further work on evaluation metrics, including developing new metrics that better assess different aspects of FL algorithms, and call for new FL algorithms that improve more consistently across heterogeneous settings.","A natural extension would be to apply formal DOE techniques (fractional factorials or response-surface/optimal designs) to efficiently estimate main and interaction effects among $E$, $C$, heterogeneity type/level, and optimizer/LR schedules, especially as the number of controllable factors grows. Additional validation on real federated logs (non-simulated client availability, varying compute/network constraints, drift over time) would strengthen practical relevance of the recommended settings. Providing a standardized statistical analysis protocol (e.g., confidence intervals, mixed-effects models across clients/runs) and releasing configuration files for all reported experiments would further improve reproducibility and comparability.",2307.15245v1,https://arxiv.org/pdf/2307.15245v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:01:35Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Optimization|Robustness|Other,Other,"Variable/General (examples include 1 input in a toy problem; 2 trajectory parameters for vehicle trajectories; uncertain parameter vector θ includes 3 parameters: m, lf, lr)",Transportation/logistics|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper proposes a safe Bayesian Optimal Experimental Design (BOED) method for nonlinear system identification in robotics, where actions/trajectories are chosen to maximally reduce uncertainty in unknown model parameters. It introduces a computationally tractable and differentiable approximation to expected information gain (EIG) using cubature (Clenshaw–Curtis) to approximate the measurement marginal as a Gaussian mixture, enabling gradient-based trajectory optimization. Safety is incorporated by enforcing collision avoidance via reachability analysis with zonotope reachable sets, yielding a tractable constrained optimization that selects safe, informative trajectories. The approach is demonstrated on (i) a nonlinear measurement toy problem and (ii) a high-fidelity autonomous vehicle model with uncertain parameters, showing orders-of-magnitude fewer samples than Monte Carlo EIG estimation and higher information gain than a sampling-based BOED baseline (without safety), while maintaining safety when constraints are applied. Practical implication: robots can actively and safely excite their dynamics to learn parameters faster, suitable for nonlinear/non-Gaussian parameter priors under Gaussian measurement noise.","The system dynamics are modeled as $\dot{x}(t;u,\theta)=f(x(t;u,\theta),u(t),\theta)$ with measurements $y(t;k,\theta)=g(x(t;k,\theta))+\omega$, $\omega\sim\mathcal{N}(0,\Sigma)$. BOED uses information gain as KL divergence $L(k,y_j)=D_{\mathrm{KL}}\big[p(\theta\mid y_j;k)\,\|\,p(\theta)\big]$ and maximizes expected information gain $J(k)=\sum_{t_j\in T}\mathbb{E}_{y_j,\theta}[L(k,y_j)]$. The paper derives a differentiable cubature-based approximation $\tilde J(k,t_j)$ by approximating the evidence $p(y_j;k)=\int_{\Theta}p(y_j\mid\theta;k)p(\theta)d\theta\approx\sum_{i=1}^N\alpha_i p(y_j\mid\theta_i;k)$ (a GMM), leading to a closed-form expression (Eq. 24–27) involving pairwise Gaussian terms $Z_{i,l}$ computed from means $\mu_i=g(x(t_j;k,\theta_i))$ and covariance $\Sigma$. Safety is enforced via reachability-based constraints over time slices: require $E(t;x_0,k,\theta)\cap O_{\mathrm{obs}}(t)=\emptyset$ (or its zonotope over-approximation) for all $t$ over the horizon, yielding the constrained problem (Opt-E).","In the nonlinear measurement example, the proposed cubature-based EIG approximation matches a Monte Carlo EIG estimate while using 100 cubature points versus $10^5$ Monte Carlo samples, enabling gradient ascent to find optimal inputs (reported optima near $u=0.2$ or $u=1.0$ depending on initialization). In the autonomous-vehicle simulation, mean Shannon information gain over 50 trials is reported as 8.58 ± 2.41 for the baseline [8], 11.36 ± 2.32 for the proposed method without safety, and 7.82 ± 2.12 when safety constraints are enforced. Qualitatively, without safety the method selects aggressive turning to excite yaw dynamics; with safety it selects collision-free trajectories (e.g., lane-change) that trade off some information gain for guaranteed collision avoidance under the reachability model. The method is claimed to require orders-of-magnitude fewer samples than sampling-based BOED while improving informativeness when unconstrained.","The paper explicitly notes that accounting for both uncertainty in the robot’s environment and uncertainty in the dynamics/model parameters is out-of-scope (it assumes the environment representation is given). It also indicates a key modeling requirement: the approach “only requires the measurement likelihood to be Gaussian” (additive Gaussian measurement noise), even though the parameter prior may be arbitrary. In the conclusion, hardware validation is not provided; simulations are used and future work includes hardware demonstrations.","Safety guarantees rely on the quality and conservatism of offline reachable-set over-approximations (computed with CORA and specific reachable-set constructions); mismatch between the reachability model and the real system could make constraints either overly conservative (reducing EIG) or insufficiently safe. The EIG approximation assumes Gaussian measurement likelihood with known covariance $\Sigma$; performance under mis-specified noise, heavy tails, outliers, or correlated/biased sensors is unclear. The computational burden can still grow with the number of cubature points $N$ and time steps (pairwise $Z_{i,l}$ terms scale roughly $O(N^2\,|T|)$), which may limit real-time use for higher-dimensional parameter spaces. Comparisons are primarily to a specific baseline BOED method and do not benchmark against other active-learning/optimal-control excitation strategies (e.g., Fisher information maximization, iLQG-based active identification) under matched compute budgets.","The authors state future work will include hardware demonstrations and applying the approach to additional robotic platforms with parameterized trajectories (e.g., drones and manipulators). They also propose relaxing Assumption 7 to incorporate system-parameter uncertainty into forward-reachable set computation and converting the safety constraint into a probabilistic chance constraint.","Extend the method to handle non-Gaussian and/or unknown measurement noise models (robust likelihoods, heavy-tailed noise, learned noise covariance) and to account for temporally correlated measurements. Develop scalable cubature/approximation schemes for higher-dimensional parameter vectors (sparse grids, adaptive cubature, variational surrogates) to reduce the $O(N^2)$ mixture interaction cost. Provide end-to-end benchmarks on real robot platforms including timing/latency, sensitivity to model mismatch, and comparisons against alternative active identification objectives (Fisher information, prediction error, mutual information approximations) under equal compute. Generalize safety constraints beyond deterministic reachability (chance constraints with calibrated uncertainty, distributionally robust safety) and incorporate uncertainty in the environment/obstacle perception pipeline.",2308.01829v1,https://arxiv.org/pdf/2308.01829v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:02:11Z
TRUE,Bayesian design|Optimal design|Sequential/adaptive,Parameter estimation|Other,Bayesian D-optimal|Other,Variable/General (examples include 2 design variables; design vector of length n; 10 sampling times; 16 measurement times),Healthcare/medical|Pharmaceutical|Other,Simulation study|Markov chain,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/ziq-ao/GradEIG,"The paper addresses Bayesian Experimental Design (BED) by maximizing expected information gain (EIG) and focuses on efficiently estimating the gradient of EIG with respect to design variables for use with stochastic gradient descent. It derives a posterior-expectation representation of the EIG gradient that replaces the intractable marginal score term with a posterior expectation of the tractable likelihood score. Based on this representation, it proposes two gradient estimators: UEEG-MCMC, which uses posterior sampling (via MCMC) to approximate the posterior expectation, and BEEG-AP, a simulation-efficient but biased estimator that reuses a fixed batch of prior samples (atomic prior approximation). Theoretical results connect BEEG-AP to sample-reuse nested Monte Carlo (srNMC), showing it optimizes a lower bound on EIG and characterizing convergence behavior, including poor approximation for large true EIG unless sample sizes grow exponentially. Empirical studies on a Bayesian linear regression toy problem, a nonlinear toy model, a pharmacokinetic sampling-time design, and a STAT5 ODE measurement-time design show UEEG-MCMC is more robust for large-EIG settings, while BEEG-AP is more simulation-efficient for small-EIG settings and both outperform several benchmark approaches (PCE, ACE, GradBED) in the tested regimes.","The EIG objective is $U(\lambda)=\mathbb{E}_{\pi_\theta(\theta)\,l(y\mid\theta,\lambda)}[\log l(y\mid\theta,\lambda)]-\mathbb{E}_{p(y\mid\lambda)}[\log p(y\mid\lambda)]$, with design $\lambda^*=\arg\max_{\lambda\in\mathcal D}U(\lambda)$. A key result is the posterior-expected gradient representation $\nabla_\lambda U(\lambda)=\mathbb{E}_{\pi_\theta(\theta)\pi_\epsilon(\epsilon)\,q(\theta'\mid g(\theta,\epsilon,\lambda),\lambda)}\big[\nabla_\lambda\log l(g(\theta,\epsilon,\lambda)\mid\theta,\lambda)-\nabla_\lambda\log l(g(\theta,\epsilon,\lambda)\mid\theta',\lambda)\big]$. UEEG-MCMC estimates the inner posterior expectation using MCMC posterior samples (Monte Carlo averaging), while BEEG-AP uses an atomic-prior/sample-reuse approximation leading to a weighted sum of likelihood-score differences (Eq. 16) and is equivalent to differentiating the srNMC EIG lower-bound estimator (Eq. 18).","Theory shows the sample-reuse NMC estimator underlying BEEG-AP is (in expectation) a lower bound on the true EIG and increases monotonically with sample size $M$ (Theorem 2), with MSE converging as $O(1/M)$ under bounded-likelihood assumptions (Theorem 3). It also proves that for large true EIG, achieving small error with srNMC/BEEG-AP can require exponentially large $M$ (Theorem 4: if $M\le \exp(U(\lambda)/2)$ then $U(\lambda)-\hat U^{M}_{\mathrm{srNMC}}(\lambda)$ remains bounded away from 0). Empirically, gradient-bias comparisons in a Bayesian linear regression example show UEEG-MCMC’s bias decreases relative to competitors as the true EIG increases, while BEEG-AP is competitive when EIG is small. In application studies (PK sampling-time design and STAT5 ODE measurement-time design), BEEG-AP converges fastest/best in small-EIG noise settings, whereas UEEG-MCMC yields the best posterior-entropy performance in large-EIG (low-noise) settings; ACE is reported to fail in large-EIG regimes due to difficulty learning the posterior inference network.","UEEG-MCMC relies on MCMC posterior sampling, and finite-length chains yield biased posterior samples, which induces bias in the gradient estimate in practice (the estimator is unbiased only with exact posterior samples). BEEG-AP is explicitly biased due to the atomic-prior/sample-reuse approximation and its performance degrades when the ground-truth EIG is large; the paper’s analysis shows sample size (and thus simulation cost) may need to grow exponentially with the EIG to control this error. The authors also note (in experiments) that benchmark methods like ACE can fail to train when there are strong dependencies between observations and parameters (large-EIG scenarios).","The work assumes a tractable likelihood and availability of differentiable simulation paths (or differentiable surrogates) so that $\nabla_\lambda\log l(\cdot)$ can be computed, which may limit applicability to likelihood-free or non-differentiable simulators without additional machinery. Performance is demonstrated on selected examples; broader robustness to model misspecification, heavy-tailed/noisy data, and strong posterior multimodality is not systematically studied, though these can substantially affect MCMC mixing and gradient quality. Practical guidance on tuning (chain length/thinning, step sizes, variance reduction, and stopping criteria) and the sensitivity of results to these choices is limited. The method is presented in a continuous-design, gradient-based setting; discrete or constrained combinatorial design spaces may require different optimization strategies.",None stated.,"Develop self-contained variance-reduction and adaptive-sampling strategies that automatically allocate effort between outer sampling and posterior sampling (e.g., adaptive $M$ and MCMC budget) based on estimated gradient uncertainty. Extend the gradient estimators to settings with intractable likelihoods (likelihood-free/implicit models) by combining the posterior-expected gradient identity with likelihood-ratio/score estimators or differentiable density-ratio surrogates. Provide scalable implementations for high-dimensional or constrained designs (e.g., time-ordering constraints for sampling times) with projection/constraint-handling and rigorous convergence diagnostics. Benchmark robustness under multimodal posteriors, autocorrelated observations, and misspecified noise models, and release a reproducible software package with standardized interfaces for common BED problems.",2308.09888v2,https://arxiv.org/pdf/2308.09888v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:02:59Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Computer experiment|Other,Parameter estimation|Prediction|Cost reduction|Other,Bayesian D-optimal|Other,Variable/General (examples: 2 parameters in Exemplar 1; 5 parameters in Exemplar 2; sequential binary load-step choices along ε11 vs ε22),Manufacturing (general)|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper introduces an Interlaced Characterization and Calibration (ICC) workflow that couples Bayesian calibration of constitutive models with Bayesian optimal experimental design (BOED) to adaptively choose the next load step during material characterization. The design variable is the load-path decision for a cruciform/bi-axial setting, discretized as a binary choice at each step: apply a prescribed strain increment along ε11 or ε22 while holding the other direction fixed. Optimality is defined via expected information gain (EIG), i.e., the expected Kullback–Leibler divergence between posterior and prior, estimated with a nested Monte Carlo estimator; Bayesian inference is approximated in quasi real-time using a Laplace approximation around the MAP. The workflow is demonstrated on two simulated exemplar calibration problems for a plane-stress elastoplastic material point model with Hill48 anisotropic yield and isotropic hardening (unknowns: {F,G} and {F,G,σy,A,n}), using Gaussian-process surrogates trained with Halton space-filling samples to make EIG evaluation feasible. Across repeated noisy trials, adaptive (myopic) BOED load paths—often alternating between ε11 and ε22—produce posteriors with lower joint uncertainty (generalized/total variance) and typically better concentration around the true parameters than naive static load paths.","Bayesian inference updates parameters via Bayes’ rule $\pi(\theta\mid y)\propto f(y\mid\theta)\,\pi(\theta)$, and the posterior is approximated with a Laplace Gaussian $\tilde\pi_L(\theta\mid y)=\mathcal N(\hat\theta,\Sigma_L)$ where $\hat\theta=\arg\min_{\theta}[-\log \pi(\theta\mid y)]$ and $\Sigma_L=H(\hat\theta)^{-1}$. The BOED utility is expected information gain $\mathrm{EIG}(\xi)=\mathbb E_{y\mid\xi}[D_{KL}(\pi(\theta\mid y,\xi)\|\pi(\theta))]$, equivalently $\int\!\int f(y\mid\theta,\xi)\pi(\theta)\log\frac{f(y\mid\theta,\xi)}{f(y\mid\xi)}\,d\theta\,dy$. EIG is approximated by a nested Monte Carlo estimator $\widehat{\mathrm{EIG}}(\xi)=\frac1N\sum_{n=1}^N\log\frac{f(y_n\mid\theta_{n,0},\xi)}{\frac1M\sum_{m=1}^M f(y_n\mid\theta_{n,m},\xi)}$, with $\theta_{n,m}\sim\pi(\theta)$ and $y_n\sim f(y\mid\theta_{n,0},\xi)$.","In Exemplar 1 (unknowns $F,G$), the adaptive BOED almost always selected an alternating load path over 5 steps (e.g., $[\varepsilon_{11},\varepsilon_{22},\varepsilon_{11},\varepsilon_{22},\varepsilon_{11}]$ chosen in 99–100% of trials depending on the case) and achieved lower joint posterior uncertainty than either static $\varepsilon_{11}$-only or $\varepsilon_{22}$-only paths, as measured by both generalized variance $\det(\Sigma)$ and total variance $\mathrm{tr}(\Sigma)$. In Exemplar 2 (unknowns $F,G,\sigma_y,A,n$), multiple near-optimal paths appeared (top paths chosen ~14–35% each), but the adaptive design still produced dramatically smaller generalized variance than static designs; for Case 5 the generalized variance was about $\sim 1.2\times10^{-9}$ (adaptive) versus $\sim 1.16\times10^{-7}$ to $1.6\times10^{-7}$ (static), and for Case 6 about $\sim 3.6\times10^{-10}$ (adaptive) versus $\sim 6.6\times10^{-8}$ (static $\varepsilon_{22}$) and much worse for static $\varepsilon_{11}$ (including failures/very large variances). Adaptive designs also yielded lower average Mahalanobis distance of $\theta_{true}$ from the final posterior than static designs in both exemplar problems, indicating better concentration around the true parameters.","The authors note that the BOED is myopic/greedy (optimizing only the next step’s EIG), so local choices are not guaranteed to yield a globally optimal full load path. They also state that the load-path space is simplified to a binary tree (only positive strain increments along one axis at a time), and that optimizing the strain increment and enriching branch options (e.g., compression, combined increments) is left for future work. They further state that, for simplicity, GP surrogate uncertainty is not propagated through the inference (only the GP mean is used). They also acknowledge numerical issues in EIG estimation (e.g., arithmetic underflow) and address them by increasing inner-sample size $M$.","Because results rely on simulated data from the same constitutive model used for inference (with additive Gaussian noise), performance may be optimistic relative to real experiments with model-form error, boundary-condition uncertainty, and correlated/heteroskedastic measurement noise. The DOE action space is restricted to two discrete load-step directions (ε11 vs ε22) with fixed step sizes and a fixed horizon, so the approach’s benefits under continuous design variables (magnitudes, angles, combined biaxial ratios) and realistic actuator constraints are not demonstrated. Using Laplace’s approximation can be inaccurate for multimodal/non-Gaussian posteriors, which may bias EIG-driven decisions in more complex constitutive models; the paper also notes covariance can extend beyond bounds, but does not provide a bounded-posterior remedy. No open-source implementation details (e.g., surrogate training, optimization settings, convergence criteria) are provided, which limits reproducibility and practical adoption.","The authors propose extending the load-path tree to include additional branch options (e.g., compression, simultaneous strain increments in both directions) and optimizing the strain increment and branch structure. They state the next major step is a live, quasi real-time demonstration on a cruciform specimen in a biaxial load frame with active control, calibrating an FEA model using experimentally measurable quantities such as resultant forces and full-field DIC data. They also indicate future work will need to address practical lab-time constraints and issues such as unwanted sample movement while computation is performed.","A valuable extension would be to treat the design variables as continuous (biaxial strain ratio, increment magnitude, dwell time) and solve a constrained BOED problem that accounts for actuator limits, failure constraints, and experiment duration. Incorporating surrogate uncertainty and model discrepancy directly into the likelihood and EIG (e.g., Bayesian model error terms, robust EIG) would improve reliability on real data. Non-myopic planning (limited-horizon lookahead or approximate dynamic programming) could reduce the risk of greedy decisions in path-dependent plasticity. Providing an open-source reference implementation and benchmarking against alternative utilities/estimators (e.g., Laplace-IS EIG, variational BOED) would strengthen reproducibility and adoption.",2308.10702v2,https://arxiv.org/pdf/2308.10702v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:04:00Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,A-optimal|Bayesian A-optimal,Variable/General (design variables are K boundary pressure activation positions; examples use K=3 and K=10; unknown Lamé perturbation dimension is 2N with N=50),Manufacturing (general)|Healthcare/medical|Environmental monitoring|Other,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper develops a Bayesian optimal experimental design method for a 2D inverse boundary value problem in linear elasticity, where the design variables are the boundary locations of compactly supported pressure activations and the unknown parameters are the Lamé fields. Assuming a linearized forward model around a background material and independent Gaussian prior and additive Gaussian measurement noise, the expected utility reduces to the Bayesian A-optimality criterion: minimizing the trace of the posterior covariance. The authors derive (Fréchet) derivatives of boundary measurements with respect to both the Lamé parameters and activation positions, enabling gradient-based optimization of activation locations; they also discuss sequential (greedy) design and a hybrid sequential + gradient-descent heuristic. Numerical experiments on a rounded-square domain with finite-dimensional parameterization (piecewise-constant-like subdomains) demonstrate that optimized activation locations substantially reduce the A-optimal objective versus naive designs, though local minima are common and global optimality is not guaranteed. The study also shows that optimal designs depend on the chosen background Lamé parameters (material stiffness).","Measurement model: $y = F(p)\alpha + \omega$ with Gaussian prior $\alpha\sim\mathcal N(0,\Gamma_{\mathrm{pr}})$ and noise $\omega\sim\mathcal N(0,\Gamma_{\mathrm{noise}})$. Posterior covariance is $\Gamma_{\mathrm{post}}(p)=\Gamma_{\mathrm{pr}}-\Gamma_{\mathrm{pr}}F(p)^\top\big(F(p)\Gamma_{\mathrm{pr}}F(p)^\top+\Gamma_{\mathrm{noise}}\big)^{-1}F(p)\Gamma_{\mathrm{pr}}$. Bayesian A-optimal design minimizes $\Phi_A(p)=\mathrm{tr}\big(A\Gamma_{\mathrm{post}}(p)A^\top\big)$ over activation positions $p$; $F(p)$ and its derivatives are assembled via bilinear-form identities (e.g., $F_m(p)\eta=-B_\eta(u_p,u_{s_m})$ and $\partial_p F_m(p)\eta=-B_\eta(u_p',u_{s_m})$).","For three activations on a $200^3$ grid (acrylplastic background), exhaustive search finds $p^*\approx(0.72,1.38,3.25)$ with $\Phi_A(p^*)\approx 3.93$; greedy sequential search yields a worse design with $\Phi_A\approx 4.33$, while sequential + gradient descent improves to $\Phi_A\approx 4.01$ (about 2% above the exhaustive optimum). For ten activations (same background), greedy sequential achieves $\Phi_A\approx 0.87$, and both sequential+gradient and pure gradient descent from an equidistant initial guess reach $\Phi_A\approx 0.82$. Material dependence: with three activations, exhaustive-search optima give $\Phi_A\approx 22.62$ (gray cast iron background) and $\Phi_A\approx 0.12$ (rubber background), and using the “intermediate” optimal design on these cases yields noticeably worse values (25.75 and 0.28, respectively).","The authors note that the A-optimality objective is expected to have multiple local minima, so gradient-based and heuristic algorithms are not guaranteed to find the global optimum; only exhaustive search reliably locates a global minimizer in their tests. They also emphasize that their approach relies on linearization and discretization (linear(ized)-Gaussian setting) to make the Bayesian design objective tractable. They restrict attention to optimizing activation locations (not sensor locations), even though the latter could be handled similarly.","The design is optimized for a linearized forward map around a chosen background $(\lambda_0,\mu_0)$, so performance may degrade if true parameters are far from the linearization point or if nonlinearity is strong (potential model-mismatch in practice). Noise is assumed i.i.d. Gaussian with known covariance and measurements are treated as independent; correlated noise, systematic bias, and model-form errors are not studied. Implementation details (COMSOL setup, mesh size, sensor/activation shapes, hyperparameters like correlation length) may strongly affect results, but sensitivity/robustness analyses and reproducible code are not provided.",They identify coping with multiple local minima in the optimization as an important direction for further study. They also propose investigating how the relative scaling/priors for $\lambda$ and $\mu$ and the choice of region-of-interest (ROI) weighting affect optimal designs. Additional extensions suggested include moving to 3D linear elasticity and addressing the original nonlinear inverse boundary value problem rather than a linearized model.,"Developing robust or Bayesian model-mismatch-aware designs (e.g., accounting for uncertainty in the background parameters or nonlinearity) could improve reliability beyond the linearized setting. Extending the framework to jointly optimize activation locations and sensor placements (or sensor weights) with practical constraints (e.g., minimum spacing, forbidden boundary segments) would broaden applicability. Providing an open-source implementation (e.g., reproducing COMSOL workflows or porting to FEniCS/deal.II) and benchmarking against alternative OED criteria (D-, E-, I-optimal; information gain) would strengthen adoption and comparative understanding.",2309.02042v1,https://arxiv.org/pdf/2309.02042v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:04:38Z
TRUE,Response surface|Optimal design,Parameter estimation|Cost reduction,D-optimal,"Variable/General (parameterized by F factors; examples discussed include F=3, 12, 20; levels L typically 2 for linear and ideally 3 for quadratic)",Theoretical/simulation only,Other,TRUE,Other,Not provided,NA,"The paper develops practical algorithms to compute exact or high-quality D-optimal designs when the candidate set is huge but highly structured, specifically for full linear and full quadratic response-surface models with multiple factors and discrete factor levels. The D-optimality problem is posed as selecting (with possible replication) s experimental runs from n=L^F candidates to maximize the log-determinant of the Fisher information matrix, thereby minimizing the generalized variance of the least-squares estimator. The authors exploit row-generation ideas to avoid explicitly enumerating all candidate design points, combining local-search moves with row generation, continuous convex relaxation, and a branch-and-bound framework for exact solution. For linear response-surface models, the row-generation subproblem reduces (after relaxation) to checking only extreme-level vertices (a Boolean quadric problem when L=2), while the quadratic model yields a quartic integer optimization subproblem over an F-dimensional box. The work advances optimal-design computation for response-surface settings where full factorial enumeration is infeasible, by leveraging mathematical-optimization machinery (row generation, convex duality, and B&B) tailored to the structured design matrix.","The discrete D-optimal design problem is formulated as $\max\ \log\det\left(\sum_{\ell\in\{1,\dots,n\}} x_\ell v_\ell v_\ell^T\right)$ subject to $\mathbf{e}^T x=s$, $x\ge 0$, and $x\in\mathbb{Z}^n$, where $v_\ell$ are candidate model rows and $\sum x_\ell v_\ell v_\ell^T=A^T\mathrm{Diag}(x)A$. For linear and quadratic response-surface models, a candidate row is $v^T=(1;\alpha_1,\dots,\alpha_F)$ or $v^T=(1;\alpha_1,\dots,\alpha_F;\alpha_1^2,\dots,\alpha_F^2;\alpha_i\alpha_j)$. In local search, using $M=B-v_jv_j^T$, the update uses $\log\det(M+vv^T)=\log(1+v^T M^{-1} v)+\log\det(M)$, leading to maximizing $v^T M^{-1} v$ over feasible $\alpha$; the continuous relaxation replaces $x\in\mathbb{Z}^n$ by $x\in\mathbb{R}_+^n$ and maximizes $\log\det(A^T\mathrm{Diag}(x)A)$.",NA,None stated.,"The paper focuses on algorithmic solution of D-optimality for discretized linear/quadratic response-surface candidate sets and does not address robustness to model misspecification (e.g., if the true response departs from the assumed linear/quadratic form). The approach assumes an explicitly discretized level set (finite candidate points) and may not directly transfer to continuous-factor optimal design without discretization error control. Practical implementation details (runtime, scalability benchmarks, and solver settings) and reproducible computational results are not provided in the excerpt, limiting assessment of real-world performance. Extensions to common applied complications (constraints on replication bounds, blocked/split-plot structures, or correlated errors) are not treated.",None stated.,"Provide a full computational study (including runtimes, memory, and solution quality) across large F and L, and release reference implementations to support adoption and reproducibility. Extend the row-generation and B&B framework to handle additional design constraints common in practice (e.g., lower/upper bounds on replications, blocking, split-plot, or budget/cost heterogeneity across runs). Investigate robustness and Bayesian variants of D-optimality for response-surface models under prior uncertainty or model misspecification. Develop approximation guarantees or convergence diagnostics for the local-search/row-generation components in the huge-scale structured-candidate setting.",2309.04009v1,https://arxiv.org/pdf/2309.04009v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:04:59Z
TRUE,Optimal design|Other,Parameter estimation|Other,Other,Variable/General (n users/units in a social network; two treatments A/B),Network/cybersecurity|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops theory and algorithms for experimental design in network A/B testing where units are connected in a social network and outcomes exhibit either network-correlated errors (Scenario I) or network interference (Scenario II). It links common variance-minimization design criteria for estimating the treatment effect to graph-cut objectives, especially the quadratic form $x^\top W x$, and formulates balanced assignment constraints via $x^\top \mathbf{1}_n$ (overall balance) and $x^\top W\mathbf{1}_n$ (degree balance). It proposes a randomized design generator (Algorithm 1) that enforces near degree-balance and exact/near treatment balance, and a rerandomization procedure (Algorithm 2) that accepts assignments based on thresholds for $x^\top W x$ (Scenario I) or $|x^\top W x|$ (Scenario II). The paper derives asymptotic normal distributions for the balancing statistic $x^\top W\mathbf{1}_n$ and for the graph-cut objective $x^\top W x$ under the proposed randomization scheme, enabling quantile-based stopping thresholds with controlled rerandomization efficiency. Performance is assessed via synthetic Erdős–Rényi networks and sampled Facebook ego networks, showing the proposed designs yield substantially lower variance percentiles compared with complete random balanced designs and small gaps to theoretical lower bounds in many settings.","Units are assigned via $x\in\{-1,1\}^n$ on a network adjacency matrix $W$, with key design criteria $x^\top W x=\sum_{i,j} w_{ij}x_ix_j$ (graph-cut objective), $x^\top W\mathbf{1}_n=\sum_i d_i x_i$ (degree-balance), and $x^\top \mathbf{1}_n=\sum_i x_i$ (treatment balance). Rerandomization enforces balance constraints $|x^\top W\mathbf{1}_n|\le \delta_1$, $|x^\top \mathbf{1}_n|\le \delta_2$ and accepts designs if $x^\top W x\le c$ (Scenario I) or $|x^\top W x|\le c$ (Scenario II). Asymptotic results include $\frac{\sum_i d_i x_i}{\sqrt{\sum_i (d_i-\bar d)^2}}\Rightarrow N(0,1)$ and $\frac{x^\top W x-\mathrm{tr}(W_0)}{2\sqrt{\sum_{i<j} w_{0,ij}^2}}\Rightarrow N(0,1)$ under stated regularity conditions.","In synthetic networks (Erdős–Rényi) and Facebook ego networks, the proposed rerandomized designs achieve very small variance percentiles among 1000 random balanced designs: for Scenario I, average percentiles are typically below 0.004 across settings (e.g., 0.0019–0.0033 in Table 1); for Scenario II, percentiles are around 0.01 for smaller networks and drop to about 0.005 for larger networks (Table 1). Optimality gaps to theoretical lower bounds are much smaller for the proposed method than for the median random design; in Scenario II with larger networks (e.g., $n=1000$ or $2000$), the reported gaps are essentially 0.0000 in Table 1, indicating near-attainment of the lower bound in those simulations. The paper also provides quantile-based thresholding for rerandomization using the derived asymptotic normal (and folded normal for $|x^\top W x|$) approximations, with numerical studies using $T=5000$, $\alpha=0.005$ (Scenario I) and $\alpha=0.1$ (Scenario II).",None stated.,"The asymptotic approximations and conditions (e.g., eigenvalue and degree-heterogeneity conditions in Proposition 3) may not hold for highly structured or heavy-tailed real networks, potentially degrading threshold calibration and acceptance probabilities. The proposed design targets variance reduction for linear models under two specific network effect scenarios; robustness to model misspecification, non-Gaussian outcomes, heterogeneous treatment effects, or temporal dependence is not established. Implementation details (including computational cost on very large graphs, choice sensitivity for $(\delta_1,\delta_2,c)$, and practical guidance for unknown network parameters such as $\rho$) are limited, and no released code is provided to reproduce the rerandomization algorithms and studies.",None stated.,"Develop finite-sample calibration methods (e.g., permutation/Monte Carlo calibration) for the stopping thresholds when asymptotic conditions are questionable, especially in sparse or highly clustered networks. Extend the design and theory to multi-arm experiments, clustered/blocked network settings, and settings with partial interference or exposure mappings, and evaluate robustness under outcome non-normality and autocorrelation. Provide scalable implementations (e.g., parallel rerandomization, incremental updates of $x^\top W x$) with open-source software and benchmarking on larger industrial-scale graphs.",2309.08797v1,https://arxiv.org/pdf/2309.08797v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:05:34Z
TRUE,Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,"Variable/General (d players/components; COA construction requires d prime power; examples include d=8, 11, 69, 101, 279)",Healthcare/medical|Network/cybersecurity|Theoretical/simulation only|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://github.com/3BIM20162017/CElegansTP,"The paper proposes a fast, unbiased approximation method for Shapley values by replacing simple random sampling of permutations with structured sampling derived from order-of-addition (OofA) experimental designs. Two OofA design constructions are used: component orthogonal arrays (COAs) and Latin squares (LSs), each providing balanced subsets of permutations; the COA-based estimator is emphasized as typically most accurate. The authors derive unbiasedness, efficiency (sum of estimated Shapley values equals the grand coalition value), and variance expressions showing how the design-induced covariance term can reduce variance relative to SRS; in some special cases the COA/LS estimators exactly recover the true Shapley values (zero variance). When d is not a prime power (COA does not exist), they propose augmenting the coalition with null players to reach a prime power and then applying a COA, yielding an unbiased modified estimator. Performance is demonstrated via theory plus simulations (e.g., airport game) and real-data case studies on the 9/11 terrorist network and the C. elegans connectome, where COA/LS sampling generally improves estimation accuracy and sometimes runtime versus SRS/structured random sampling baselines.","Shapley value in permutation form: $\mathrm{Sh}_j(\nu)=\frac{1}{d!}\sum_{\pi\in\Pi}\Delta(\pi,\nu)_j$ with marginal contribution $\Delta(\pi,\nu)_j=\nu(P_j^\pi\cup\{j\})-\nu(P_j^\pi)$. DOE-based estimators average $\Delta(\pi)_j$ over permutations in an LS or COA: $\widehat{\mathrm{Sh}}^{LS}_j=\frac{1}{d\,n^{LS}}\sum_{i=1}^{n^{LS}}\sum_{\pi\in LS_i}\Delta(\pi)_j$ and $\widehat{\mathrm{Sh}}^{COA}_j=\frac{1}{d(d-1)\,n^{COA}}\sum_{i=1}^{n^{COA}}\sum_{\pi\in COA_i}\Delta(\pi)_j$. For non-prime-power d, they define an augmented COA estimator with null players (Eq. (7)) averaging $\Delta(\pi^*\setminus d_c)_j$ over a $COA(d^*(d^*-1),d^*)$.","Theorem 1 establishes that the COA- and LS-based Shapley estimators are unbiased and provides variance formulas: $\mathrm{Var}(\widehat{\mathrm{Sh}}^{COA}_j)=\mathrm{Var}(\widehat{\mathrm{Sh}}^{SRS}_j)+\frac{d(d-1)-1}{d(d-1)n^{COA}}\,\mathrm{Cov}_{COA}$ and similarly for LS with factor $\frac{d-1}{dn^{LS}}\,\mathrm{Cov}_{LS}$. In Example 1 (symmetric voting game) both COA and LS yield zero variance (exact Shapley recovery), while SRS has nonzero variance $(d-1)/(m d^2)$. In Example 2 (linear-normal setting) the COA estimator has zero variance while LS matches SRS variance. In simulations (airport game with d=101) and real networks (9/11 network with d=69 using augmented d*=71), COA/LS/StrRS reduce squared loss and variance versus SRS, with COA typically best; reported timing ratios show COA slightly faster than SRS/LS and far faster than StrRS in that experiment.","The COA construction requires the number of players/components $d$ to be a prime power; otherwise a COA does not exist and must be replaced by LS or by augmenting with null players to reach a prime power. When $d$ is large, COA can be computationally infeasible (e.g., they note for the C. elegans application with $d=279$ that even the smallest COA sample size would take about two days), motivating use of LS instead. They also note it cannot be proven in full generality that COA/LS always have smaller variance than SRS (some variables in one manufacturing-system simulation can have slightly larger variance), although overall variance/squared loss tends to improve.","The DOE-based sampling designs are evaluated mainly against SRS and one structured-random-sampling variant; comparisons to other modern Shapley approximation methods (e.g., kernelSHAP-style weighted linear regression, stratified sampling by subset size, antithetic/permutation pairing, or Quasi-Monte Carlo approaches) are limited, so relative gains across broader baselines are unclear. The method’s performance depends on the cost and noise of evaluating the value function $\nu(\cdot)$; when $\nu$ is itself estimated via Monte Carlo (as noted for sensitivity analysis), design-induced variance reductions may be offset by inner-loop estimation error, but this interaction is not deeply quantified here. Practical guidance on selecting between LS vs COA vs augmented COA (e.g., given a fixed compute budget and large d) is not fully formalized beyond qualitative recommendations. Software and reproducibility details (implementation, randomization choices, and datasets beyond one GitHub link) are not provided, which makes exact replication and adoption harder.",None stated.,"Developing a formal compute-budget allocation strategy (e.g., number of COAs/LSs vs permutations vs inner Monte Carlo samples for $\nu$) could improve practical performance, especially in sensitivity-analysis settings where $\nu$ is noisy. Extending the design-based sampling idea to other structured designs over permutations (e.g., higher-strength component orthogonal arrays, balanced incomplete block–style constructions, or quasi-Monte Carlo sequences on permutation space) may yield further variance reductions. Providing open-source implementations (R/Python) and benchmarking against widely used Shapley approximation toolkits would strengthen adoption and clarify when OofA-based sampling is best. Additional theory on when $\mathrm{Cov}_{COA}$ or $\mathrm{Cov}_{LS}$ is negative (guaranteeing variance reduction) and robustness under dependent/auto-correlated data-generating processes would also be valuable.",2309.08923v1,https://arxiv.org/pdf/2309.08923v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:06:16Z
TRUE,Factorial (full)|Factorial (fractional)|Response surface|Split-plot|Optimal design|Screening|Computer experiment|Other,Parameter estimation|Screening|Optimization|Prediction|Other,D-optimal|I-optimal (IV-optimal)|Other,"Variable/General (examples mentioned include 4, 5, 7, 8, up to 90 factors in virtual experiments)",Other,Other,TRUE,R|Other,Supplementary material (Journal/Publisher)|In text/Appendix,https://www.coursehero.com/|https://www.coursicle.com/|https://sigmazone.com/catapult/|https://www.student.math.uwaterloo.ca/~watfacto/login.htm|https://twilights.be/sprinkler/|https://nathaniel-t-stevens.shinyapps.io/Netflix_Simulator_v2/|https://alanrvazquez.shinyapps.io/Parameter_Tuning_RF/|https://drive.google.com/file/d/18QnrFFCXpXBbnwZu-kq0axmBnZtc0c2F/view|http://users.stat.umn.edu/~gary/book/fcdae.pdf|https://my.uopeople.edu/pluginfile.php/57436/mod_book/chapter/37629/MATH1280.IntroStat.pdf,"This paper is a pedagogical/review study of undergraduate Design of Experiments (DoE) courses in U.S. universities, based on a newly built database of 206 course offerings (2019–2022) with 18 course features. It characterizes what DoE topics are taught (via descriptive statistics and topic modeling of course descriptions), what prerequisites are common, and what textbooks/software/evaluation methods are used. The authors recommend expanding undergraduate DoE instruction to include algorithmically generated (optimal) designs (e.g., D-optimal and I-optimal designs, including blocked and split-plot variants) and propose R-based teaching demos provided in the supplement. They also recommend using virtual experiments for student assessment and provide an updated list of low-overhead web-based simulators plus a new virtual experiment on random-forest hyperparameter tuning. The contribution advances DoE education practice by offering a systematic empirical overview of current course content and concrete, shareable teaching materials.","Not applicable (pedagogy/review/database study rather than proposing a new DOE criterion derivation). The paper references optimal design criteria conceptually, e.g., D-optimality based on the determinant of the variance–covariance (information) matrix for least-squares coefficient estimates, and discusses I-optimality for response-surface optimization, but does not present a new formula central to a new design method.","From 70 unique DoE courses, the main topics extracted from course descriptions were (1) multifactor designs with and without randomization restrictions (including fractional factorial, blocked, split-plot, nested designs) and (2) data analysis and applications. Among 87 courses with textbook data, Montgomery’s “Design and Analysis of Experiments” was most used (40/87), and among 27 courses with software data, R was most used (20/27) followed by JMP (6/27). For 32 courses with evaluation-method data, homework (31/32), exams (28/32), and projects (20/32) were the most common assessment tools. The authors also report that 35 of the 102 sampled top-ranked universities had no undergraduate DoE course in statistics/mathematics/engineering departments per their search process.",None stated.,"Because many database fields are missing for a large fraction of courses (notably software and evaluation), conclusions about tools/assessment may be biased toward courses with publicly accessible syllabi/details. The sampling frame is limited to top U.S. national universities (US News 2022), which may not represent DoE pedagogy at regional universities, liberal arts colleges, or community colleges. The topic analysis depends on catalog descriptions, which can be brief/marketing-oriented and may not reflect actual taught content or depth.","The authors suggest that continued/periodic reassessments of DoE pedagogy are needed to track its evolution, particularly whether newer DoE methods connected to data science (e.g., optimal subsampling for big data and methods for online controlled experiments/A-B testing) begin to appear in undergraduate DoE curricula.","A natural extension is to validate catalog/syllabus-inferred topics against instructor interviews or direct syllabus/content audits and to quantify coverage depth (time spent, assignments) rather than presence/absence. Expanding the database beyond the top-100 national-university ranking and outside the U.S. would test generalizability and identify institutional differences. Providing an open, versioned repository for the dataset and R/JMP topic-analysis pipelines would improve reproducibility and enable community updates.",2309.16961v2,https://arxiv.org/pdf/2309.16961v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:06:35Z
TRUE,Sequential/adaptive|Bayesian design,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,"Not specified (general; reliability setting with possible experimental factors mentioned such as speed, temperature, pull angle)",Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,R,Not provided,NA,"The paper studies Bayesian adaptive design of experiments for expensive physical reliability tests, focusing on early stopping rules based on Bayesian predictive probability (PP) and comparing to frequentist conditional power (CP). The quantity of interest is reliability defined as the probability a response falls within specification limits, treated for both continuous responses (Normal model with unknown mean/variance using conjugate priors) and binary responses (Bernoulli/Binomial model with Beta prior). A simulation study evaluates robustness of PP/CP to model misspecification for continuous data by generating data from Normal, Laplace, Uniform, and shifted-Gamma distributions, and assesses operating characteristics (Type I error, power, stopping time, and decision stability) across 0–9 interim analyses. Results show PP is generally more robust and has higher power than CP but can exhibit inflated Type I error for efficacy/either stopping without adjustment, while futility-only stopping is more stable; CP has lower Type I error with alpha-spending but can lose power and shows more “decision flipping.” A post-hoc aircraft/weapon-system pull-test application (planned 180 runs) shows PP would have stopped at 120 runs (33% savings), demonstrating potential resource reduction in national-security testing contexts.","Predictive probability is defined as $PP = P_{Y|X}\{Y: P(\phi>\phi_0\mid X,Y)>\theta_T\}$ with early stopping thresholds $PP>\theta_U$ (efficacy) or $PP<\theta_L$ (futility). Reliability QoI is $\phi=P(s_l<Z<s_u)$; under the Normal model $Z_i\mid\mu,\sigma^2\sim\mathcal N(\mu,\sigma^2)$ with conjugate Normal–Inverse-Gamma prior and posterior predictive $Z^\*\mid Z\sim t_{2(a+n/2)}(m',\, b'(\nu'+1)/(\nu' a'))$. For Normal-data reliability, $\phi=\Phi((s_u-\mu)/\sigma)-\Phi((s_l-\mu)/\sigma)$; CP is computed as the probability a test statistic based on $\hat\phi$ exceeds a critical value under $H_0:\phi\le\phi_0$ with multiple-testing controlled via alpha spending.","In the continuous-response simulations (planned $n=100$), PP maintains high power (generally >0.7 across DGMs and stopping rules) and is relatively robust to mild distributional deviations, though Uniform DGMs yield notably higher Type I error. CP typically achieves lower Type I error than PP (with alpha spending) but can have substantially lower power when allowing futility stopping and/or many interim looks, and shows higher rates of inconsistent (“flipping”) early-stop decisions. For the Binomial simulations ($n=100$, $p=0.60$ null vs $p=0.75$ alternative), PP has higher and more stable power across numbers of interim analyses, while CP power degrades markedly when interim analyses begin early; PP Type I error is higher than CP and may need adjustment. In the pull-test application (planned 180 pulls; first look at 30), PP crossed the efficacy threshold at 120 pulls, saving ~33% of runs (CP would have stopped at 90).","The authors note that PP depends on the assumed data model and prior specification, and that type I error can be inflated (especially for efficacy-only/either stopping) as the number of interim analyses increases, suggesting that some multiple-testing adjustment may be needed in practice. They also state the application data are proprietary so raw timing data cannot be shared, limiting reproducibility and external validation. They emphasize that substantial statistical expertise is currently needed to implement Bayesian adaptive DOE and that early-stopping decisions should be made jointly with subject-matter experts given high-consequence risks.","The robustness study considers only simple conjugate models (Normal with conjugate priors; Bernoulli with Beta), so conclusions may not generalize to more realistic reliability settings with autocorrelation, nonstationarity, censoring, outliers, or hierarchical structure across test conditions. The work focuses on early stopping (interim monitoring) rather than adaptive selection of factor settings (e.g., utility-based Bayesian optimal design), so “adaptive DOE” is mainly about sample-size adaptation. Comparisons for PP do not implement a principled Bayesian error-control approach (e.g., calibrated thresholds, Bayesian decision-theoretic loss, or posterior error control), making PP vs CP comparisons partly dependent on chosen thresholds and calibration. Computational burden and implementation details (Monte Carlo sample sizes, convergence diagnostics) are not fully operationalized into practitioner-ready guidance or released code.",The authors propose extending robustness investigations to additional data generating mechanisms beyond those studied and to models beyond the simple Normal model. They specifically mention incorporating experimental factors in linear and non-linear ways and relaxing assumptions such as constant variance (heteroscedasticity). They also highlight the need for a comprehensive understanding of PP behavior under different DGMs to help practitioners anticipate outcomes and build confidence in using PP for early stopping in practice.,"Develop calibrated PP stopping thresholds that control frequentist Type I error (or expected loss) across varying numbers/timings of interim looks, analogous to alpha-spending but within a Bayesian decision framework. Extend the approach to regression/GLMM settings with multiple factors and interactions (including hard-to-change factors/split-plot constraints) so that both factor-setting adaptation and early stopping are handled jointly. Address correlated and time-ordered data (e.g., drift, learning effects, facility changes) and robust likelihoods (e.g., t-errors, mixture models) to mitigate sensitivity to outliers and distributional misspecification such as the problematic Uniform-like case. Provide open-source implementation (R/Python) with templates for practitioners, including prior elicitation tools, simulation-based design calibration, and diagnostics for decision stability.",2309.17241v1,https://arxiv.org/pdf/2309.17241v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:07:15Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Screening|Other,G-optimal|Other,Variable/General (d-dimensional measurement vectors; variance model has M=d(d+1)/2 parameters),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops experimental design methods for linear models with structured heteroskedastic noise where the variance depends on covariates as $\sigma_x^2=x^\top\Sigma^*x$ with unknown PSD $\Sigma^*$. It proposes HEAD, a two-phase (sample-splitting) adaptive design that uses G-optimal design in stage 1 to estimate the mean parameter $\theta^*$ and a second G-optimal design over lifted features $\phi_x=\mathrm{vec}(xx^\top)$ to estimate the variance parameters, yielding finite-sample uniform (max absolute) error bounds for estimated variances $\hat\sigma_x^2$. Building on these variance estimates, it introduces H-RAGE for heteroskedastic transductive linear best-arm identification and level-set identification, combining plug-in weighted least squares with optimal experimental design allocations. The authors derive the first instance-dependent lower bounds for these identification problems under heteroskedastic noise and provide near-optimal algorithms whose additional cost for unknown variances is additive rather than multiplicative in the heteroskedasticity ratio $\kappa=\sigma_{\max}^2/\sigma_{\min}^2$. Simulations show substantial sample-complexity reductions versus methods that ignore heteroskedasticity and rely on worst-case variance bounds.",Observation model: $y=x^\top\theta^*+\eta$ with heteroskedastic variance $\sigma_x^2:=x^\top\Sigma^*x$ (Eq. 1). HEAD uses two G-optimal designs: stage 1 chooses $\lambda^*=\arg\min_{\lambda\in\mathcal P_X}\max_{x\in X} x^\top(\sum_{x'}\lambda_{x'}x'x'^\top)^{-1}x$; stage 2 similarly optimizes over $\phi_x$ to estimate $\Sigma^*$ via least squares on squared residuals. H-RAGE uses plug-in variance estimates in an $XY$-allocation objective $q(\lambda;V)=\max_{z\in V}\|z\|^2\big(\sum_{x\in X}\hat\sigma_x^{-2}\lambda_x xx^\top\big)^{-1}$ to set round-wise sampling budgets and elimination thresholds.,"For variance estimation, Theorem 3.1 gives a finite-sample uniform bound on variance estimation error under HEAD: with sufficient budget $\Gamma$, $\Pr(|\hat\sigma_x^2-\sigma_x^2|\le C_{\Gamma,\delta})\ge 1-\delta/2$ with $C_{\Gamma,\delta}=O\big(\sigma_{\max}^2 d^2\sqrt{\log(|X|/\delta)}/\Gamma\big)$. For identification, Theorem 4.1 provides instance-dependent lower bounds in terms of $\psi^*_{\mathrm{OBJ}}$ for BAI/LS under known variances. Theorem 4.2 shows H-RAGE is $\delta$-PAC with sample complexity near the lower bound, incurring only additive penalties for unknown variances (including terms scaling with $d^2$ and $\kappa^2 d^2$) rather than a multiplicative $\kappa$ factor suffered by homoskedastic designs. Empirically (multiple simulated benchmarks), H-RAGE and the heteroskedastic oracle substantially reduce sample complexity compared with RAGE/homoskedastic-oracle baselines.","The paper notes that much of the analysis is presented assuming Gaussian noise “for ease of exposition,” with extensions to strictly sub-Gaussian noise deferred to the Appendix. It also notes practical implementation issues with fractional allocations ($\lambda_x\Gamma/2\notin\mathbb N$), handled via rounding procedures (Appendix A). The lower bound results assume the noise variances are known; the authors explicitly state that obtaining a tighter lower bound when variance parameters are unknown is an open question.","The design and guarantees are developed for a finite arm/measurement set $X$ with $\mathrm{span}(X)=\mathbb R^d$; performance and computation may degrade for very large/continuous design spaces where solving repeated G-optimal/XY allocations is expensive. The structured variance model is quadratic in features ($x^\top\Sigma^*x$); if real heteroskedasticity deviates from this form, plug-in variance estimates could be biased and the claimed sample-complexity gains may not hold. The empirical validation is simulation-based; there is no real-data case study demonstrating robustness to model misspecification, dependence, or nonstationarity that often arise in online experimentation.","The authors explicitly suggest future work on developing a tighter (instance-dependent) lower bound for the setting where variance parameters are unknown, since their main lower bound assumes known variances.","Extending the approach to continuous design regions (approximate designs) and to settings with dependent observations (e.g., time series or contextual drift) would improve practical applicability. Developing robust or misspecification-resistant variants of HEAD/H-RAGE (e.g., allowing variance models beyond $x^\top\Sigma x$ or using robust loss/estimating equations) would clarify sensitivity to real-world heteroskedasticity. Providing open-source implementations and benchmarking computational costs of repeated design optimizations (e.g., Frank–Wolfe and rounding) would help practitioners adopt the methods.",2310.04390v1,https://arxiv.org/pdf/2310.04390v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:07:51Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Other,Prediction|Parameter estimation|Model discrimination|Cost reduction|Other,Not applicable,Variable/General (design points in a compact input space X; vector-valued outputs of dimension d),Theoretical/simulation only|Environmental monitoring|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper develops consistency results for Stepwise Uncertainty Reduction (SUR) sequential experimental design strategies when the underlying model is a vector-valued (multi-output) Gaussian process with continuous sample paths on a compact design space. It extends the scalar SUR consistency framework of Bect et al. (2019) to the multivariate setting by formalizing the link between continuous vector-valued Gaussian processes and Gaussian measures on the Banach space C(X;R^d), and by analyzing conditioning/updates under noisy pointwise observations. A key technical complication is the discontinuity of the Moore–Penrose pseudo-inverse across rank changes, which affects continuity/existence arguments for the SUR sampling criterion in the multi-output case. The paper applies the general results to two uncertainty functionals for excursion set estimation with orthant thresholds: Integrated Bernoulli Variance (IBV) and Excursion Measure Variance (EMV), proving that under (ε-quasi) SUR designs these residual uncertainties converge to 0 almost surely (and additional L1 convergence statements). The work is primarily theoretical but is motivated by applications such as multi-field environmental/ocean sensing (e.g., plume mapping) where correlated outputs are observed jointly.","SUR selects the next design point by minimizing the expected next-step uncertainty: X_{n+1} ∈ argmin_{x∈X} J_n(x), with J_n(x)=E[H(P^ξ_{n+1})|F_n,X_{n+1}=x]=E[H(Cond_{x,Z(x)}(P^ξ_n))|F_n]. The GP posterior update under vector-valued observations uses mn(x)=m(x)+K(x,x_{1:n})Σ(x_{1:n})^{:}(vec(z_{1:n})-vec(m(x_{1:n}))) and kn(x,y)=k(x,y)-K(x,x_{1:n})Σ(x_{1:n})^{:}K(y,x_{1:n})^T (with Σ=k+T and ':' the Moore–Penrose pseudo-inverse). For excursion sets Γ(ξ)={u: ξ(u)≥T} (orthant threshold), IBV is H^IBV_n=∫_X p_n(u)(1-p_n(u)) dμ with p_n(u)=P(ξ(u)≥T|F_n), and EMV is H^EMV_n=Var(μ(Γ(ξ))|F_n).","For ε-quasi SUR designs based on IBV, the paper proves H^IBV_n=H^IBV(P^ξ_n)→0 almost surely, and additionally ∫_X (1_{ξ(u)≥T}-p_n(u))^2 dμ →0 almost surely and in L1. For ε-quasi SUR designs based on EMV, it proves H^EMV_n=H^EMV(P^ξ_n)→0 almost surely and that E[μ(Γ(ξ))|F_n]→μ(Γ(ξ)) almost surely and in L1. It establishes measurability/continuity properties needed for SUR in the multivariate setting, including continuity of the sampling criterion on regions where rank(Σ_n(x)) is constant, and gives existence of exact SUR designs under positive definiteness assumptions on both the GP covariance k and noise covariance T(x). It highlights that rank changes introduce discontinuities via the pseudo-inverse map, making the multivariate extension nontrivial compared with the scalar case.","The analysis focuses on the multivariate excursion set problem for orthant thresholds T={t_1,∞)×…×{t_d,∞) and notes that it remains to be checked whether the results extend to arbitrary closed sets T⊂R^d. The conclusion states that multi-objective optimization functionals (e.g., knowledge gradient/expected improvement) with multivariate Gaussian processes are beyond the scope and are not considered. The authors also indicate that convergence rates are not addressed, leaving only consistency (convergence to zero uncertainty) established.","Results rely on the correctness of the GP model (the function is assumed to be a sample path from the same vector-valued GP prior used for design), which is a strong assumption for practical DOE with model misspecification. The setting assumes continuous sample paths on a compact domain and Gaussian, independent observation noise with known heteroscedastic structure τ(x); robustness to non-Gaussian noise, unknown noise parameters, or model/parameter estimation (Phase I/II issues) is not developed. Practical performance is not benchmarked via simulations or real case studies within this paper, so empirical efficiency, sensitivity to kernel choice, and computational cost of evaluating SUR criteria in high dimensions are not assessed.","The paper suggests checking whether the multivariate consistency results extend from orthant excursion thresholds to the general case of arbitrary closed sets T⊂R^d. It also proposes studying convergence rates of SUR sequential designs to provide fuller theoretical support for effectiveness. Another explicit question raised is whether and how inter-output correlation among (ξ_1,…,ξ_d) enhances convergence rates.","Extending the theory to settings with unknown GP hyperparameters (learned online) and quantifying the impact of hyperparameter uncertainty on SUR consistency would increase practical relevance. Relaxing independence assumptions (e.g., temporal/spatially correlated noise, non-Gaussian errors) and handling non-continuous or non-compact design spaces would broaden applicability. Developing scalable approximations and software implementations for multi-output SUR criteria (e.g., sparse/inducing-point multioutput GPs) and validating them on real multi-sensor field campaigns would connect the theory to operational sequential DOE.",2310.07315v1,https://arxiv.org/pdf/2310.07315v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:08:20Z
TRUE,Bayesian design|Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (examples include 1D design ξ∈(0,1); 15 design variables for sampling times; 2D electrode-position design)",Healthcare/medical|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Upon request,NA,"The paper develops Laplace-based computational strategies for Bayesian optimal experimental design (BOED) when the forward model includes both parameters of interest and nuisance parameters that must be marginalized. It targets maximizing the expected information gain (EIG; expected Kullback–Leibler divergence between posterior and prior for parameters of interest), which becomes a nested-log integral with two inner integrals under nuisance uncertainty. Two new EIG estimators are proposed: (i) a Monte Carlo double-Laplace approach (MC2LA) that applies Laplace’s method to marginalize nuisance parameters and then a Laplace (Gaussian) approximation for the posterior in the parameters of interest, producing a fast but potentially biased estimator; and (ii) a double-loop Monte Carlo estimator with double Laplace-based importance sampling (DLMC2IS) that uses two Laplace approximations as importance densities for the inner integrals to reduce variance and computational cost. The authors provide error/work analysis (bias and variance expansions, optimal allocation of outer/inner samples, and extensions including discretization bias for PDE solvers) and demonstrate performance on four numerical examples: a linear-Gaussian toy problem, a 15-dimensional pharmacokinetics sampling-time design, and two electrical impedance tomography (EIT) designs with FEM-based forward models and nuisance uncertainty.","The experiment uses an additive-noise model $y_i(\xi)=g(\xi,\theta,\phi)+\epsilon_i$ with Gaussian noise, leading to likelihood $p(Y\mid\theta,\phi)\propto\exp\{-\tfrac12\sum_{i=1}^{N_e} r(y_i,\theta,\phi)^\top\Sigma_\epsilon^{-1} r(y_i,\theta,\phi)\}$ and EIG $I=\mathbb{E}_{\theta,\phi,Y}\big[\log p(Y\mid\theta)-\log p(Y)\big]$ with $p(Y\mid\theta)=\int p(Y\mid\theta,\phi)\pi(\phi\mid\theta)\,d\phi$ and $p(Y)=\iint p(Y\mid\vartheta,\varphi)\pi(\vartheta,\varphi)\,d\varphi d\vartheta$. The baseline DLMC estimator is $\hat I_{DL}=\tfrac1N\sum_{n=1}^N\big[\log(\tfrac1{M_1}\sum_{m=1}^{M_1} p(Y^{(n)}\mid\theta^{(n)},\phi^{(n,m)})) - \log(\tfrac1{M_2}\sum_{k=1}^{M_2} p(Y^{(n)}\mid\vartheta^{(n,k)},\varphi^{(n,k)}))\big]$. The proposed methods approximate inner integrals via Laplace: $\int e^{-f(\theta,\phi)}d\phi\approx (2\pi)^{d_\phi/2}\det(\nabla_\phi^2 f(\theta,\hat\phi(\theta)))^{-1/2}e^{-f(\theta,\hat\phi(\theta))}$ and then approximate the posterior over $\theta$ by a Gaussian with covariance $\Sigma^{-1}=\nabla_\theta^2 F(\hat\theta)$, yielding an MC2LA EIG approximation based on $-\tfrac12\log\det(2\pi\Sigma)-\tfrac{d_\theta}{2}-\log\pi(\hat\theta)$ (plus correction terms).","The paper derives leading-order bias and variance approximations for the double-loop EIG estimator under nuisance uncertainty, including bias $\approx C_1/M_1 + C_2/M_2$ (conservative bound) and variance $\approx D_3/N + D_1/(NM_1)+D_2/(NM_2)$, and then gives closed-form optimal allocations $(N^*,M_1^*,M_2^*,\kappa^*)$ for a target tolerance (and an extension including FEM discretization bias). Numerically, Laplace-based importance sampling (DLMC2IS) can reduce the required inner samples dramatically (often to $M_1=M_2\approx 1$ in the linear-Gaussian example) while maintaining tolerance-based error control behavior. In the 15D pharmacokinetics design, optimization of sampling times increased EIG from about 6.12 (geometric schedule) to about 6.25 and suggested beneficial clustering of early/mid measurements. In EIT examples with FEM forward models, the methods are applicable but computational work grows sharply with nuisance-parameter variance; MC2LA may fail when Laplace bias is too large (Example II), in which case DLMC2IS is used.","The authors note that the first estimator (MC2LA) introduces a bias due to nested Laplace approximations and that this bias can render MC2LA ineffective in some settings (e.g., EIT Example II at tolerance 0.2). They also state that all MAP points and Hessians used in Laplace approximations were estimated numerically and that exploiting closed-form expressions or automatic differentiation was beyond the scope. They mention a practical issue for compact-support priors: Gaussian importance densities can draw samples outside the support, though this is often rare when the covariance is highly concentrated.","The performance of Laplace-based methods hinges on near-Gaussian posteriors and (effectively) unimodal, well-identified MAP regions; if the posterior is multimodal or strongly non-Gaussian, both MC2LA and Laplace-based IS can misrepresent tails and produce biased EIG estimates or underestimated uncertainty. The paper’s numerical validation is strong but still limited to four example classes; broader benchmarking against alternative EIG estimators (e.g., nested quadrature in low dimensions, advanced MI estimators, variational bounds) and robustness checks (model misspecification, heavier-tailed noise, stronger parameter correlations) would strengthen generality. Practical reproducibility may be limited because implementation details (optimization routines for MAP/Hessian, stopping criteria, numerical stability choices) materially affect cost and accuracy. For high-dimensional nuisance spaces, repeated per-outer-sample MAP/Hessian computation may become a bottleneck despite asymptotic arguments, especially if Newton iterations are numerous or Hessians are ill-conditioned.",None stated.,"Develop diagnostics and adaptive safeguards to detect when Laplace approximations are unreliable (e.g., multimodality, near-singular Hessians) and switch to more robust importance densities or mixture/Laplace-iterated approximations. Extend the framework to correlated/structured observation models (time series, spatially correlated noise) and to larger-scale PDE-constrained designs where MAP/Hessian computations require adjoints or low-rank Hessian approximations. Provide open-source reference implementations to standardize MAP/Hessian estimation and enable reproducible comparisons, and integrate gradient-based design optimization (with autodiff/adjoints) for scalable high-dimensional design spaces.",2310.10783v3,https://arxiv.org/pdf/2310.10783v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:09:12Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Optimization|Model discrimination|Cost reduction|Other,Not applicable,Not specified (intervention is presence/absence of an SSH vulnerability; allocation also stratified by 4 U.S. regions and trial stage),Network/cybersecurity,Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes an adaptive design (AD) methodology for intrusion data collection that treats honeypot configuration as an intervention, adapting resource allocation across study stages based on interim results. The design is inspired by adaptive randomized controlled trials (RCTs) and uses pre-specified staging, interim analyses, and early stopping endpoints to both (i) establish causal impact of a “corruption” (treatment) and (ii) maximize collection of intrusion events. In their controlled honeypot deployment study, the intervention is an SSH vulnerability (accepting any password for four fake accounts) versus a control configuration, and the outcome/event of interest is a malicious login to those accounts. Interim updates estimate exploitation risk via Kaplan–Meier survival analysis and feed updated incidence rates into power analysis to re-compute sample sizes and adaptively reallocate honeypots across control/treatment and regions. Empirically, the AD approach reduces time and resource usage while maintaining confidence in detecting the causal effect, outperforming a fixed-allocation RCT in attack yield per deployed honeypot and requiring fewer total honeypots than both RCT and a vanilla observational deployment.","The required per-stage total sample size is computed via power analysis for two proportions: $N_{\text{total}} = 2\,\frac{(p_1 q_1 + p_2 q_2)(Z_{1-\alpha/2}+Z_{1-\beta})^2}{(p_1-p_2)^2}$ where $p_1,p_2$ are control/corrupted incidence rates and $q_i=1-p_i$. Incidence rates are updated between stages using Kaplan–Meier survival analysis, with survival recursively updated as $S_{t+1}=S_t\times\frac{N_t-D_{t+1}}{N_t}$, and then inverted/converted to risk rates to weight adaptive allocation for the next stage. AD iterates: deploy $(N_1,N_2)$, observe for stage duration, estimate updated $p_1,p_2$, recompute $N_1,N_2$ by power analysis, and stop early if pre-specified endpoints are met.","With a 12-hour budget cap, the vanilla observational deployment used 140 corrupted honeypots and recorded 137 attacks, but lacked a control group for causal attribution. The fixed RCT used 72 control + 72 corrupted (144 total) and recorded 42 attacks. The adaptive design used 32 control + 87 corrupted (119 total) and recorded 50 attacks—i.e., 17% fewer honeypots than RCT while collecting 19% more attack recordings. The authors report AD can determine the intervention’s impact in at least 33% less total trial duration than the non-adaptive/vanilla baseline while still confidently stating the effect.","The authors note the method may not behave the same in other settings and that if the corruption effect were less apparent (e.g., multiple entry points or exploit sequences), the study would require more time and resources to collect evidence. They also note unanticipated instability/heterogeneity in exploitation rates across regions within the 4-hour stages; because this was not a pre-listed stopping criterion, it could not be used to pause/alter the trial and should be incorporated in future work. They further acknowledge that terminating honeypots upon the first event of interest limits the amount of data acquired, and this choice should be reconsidered for alternative objectives.","The approach assumes independence and a one-event-per-honeypot model with immediate termination, which may bias estimates of exploitation dynamics and prevent learning multi-step attack sequences or dwell-time behaviors. The evaluation is largely a single case study (one corruption type, one cloud provider, one OS image, and four U.S. regions) and may not generalize to other vulnerabilities, services, attacker populations, or non-U.S. deployments. The “optimization” goal (maximize events) is partly at odds with strict confirmatory inference; response-adaptive allocation can complicate unbiased effect estimation if not carefully controlled and reported (e.g., for time trends and non-stationarity in attacker behavior). Practical replication may be sensitive to operational details not fully standardized (e.g., IP reputation effects, cloud-provider network policies, background scanning variability, and sensor fidelity/false positives).","They suggest extending the methodology to multiple vulnerabilities to study interactions among corruptions, potentially requiring multi-arm adaptive platform trial methodology (e.g., REMAP-CAP-like designs). They also state future studies should include the observed regional/time-window instability as an early stopping criterion to trigger discussion on whether to continue the trial.","Developing a multivariate/hierarchical adaptive model that explicitly accounts for region-by-time non-stationarity and covariates (e.g., IP block, ASN, time-of-day) could improve allocation decisions and inference robustness. A self-starting or robust version that relaxes assumptions about known/estimated incidence rates and handles repeated events per honeypot (recurrent event models) would broaden applicability. Providing open-source tooling for deployment, interim analysis, and reporting (including reproducible templates for endpoints and randomization) would substantially aid adoption and peer verification. Additional comparative benchmarks against alternative adaptive allocation strategies (e.g., Thompson sampling/Bayesian bandits with inferential safeguards) and more real-world replications across providers/vulnerabilities would strengthen evidence.",2310.13224v1,https://arxiv.org/pdf/2310.13224v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:09:50Z
FALSE,NA,NA,Not applicable,"3 factors (color, orientation, source annotation) in factorial-style user experiments",Other,Case study (real dataset),TRUE,None / Not applicable,Not provided,https://www.metoffice.gov.uk/|https://de.statista.com/|https://atlasti.com/,"The paper studies how small visual design changes in simple bar charts (color, bar orientation, and source annotation) affect subjective judgments of readability and trustworthiness. It triangulates three empirical study designs: an online single-image rating study (Likert ratings), an online forced-choice pairwise comparison study, and in-person semi-structured think-aloud interviews using similar forced-choice comparisons plus real-world chart prompts. The results show that design choices expected to be objectively inconsequential (especially orientation and color) can strongly influence subjective ratings and choices, and that the experimental method itself changes effect sizes and sometimes conclusions (procedural variance). Methodologically, the authors argue for carefully isolating chart elements, considering interactions/background factors, and incorporating qualitative validation when studying subjective visualization experience. The work contributes primarily to visualization evaluation methodology rather than proposing DOE methods.",Not applicable,"In Study 1 (Rating; n=86, 8 charts each), source annotation substantially increased trustworthiness ratings by more than half a Likert point, while color and vertical orientation increased both readability and trustworthiness by about 0.1–0.2 Likert points. In Study 2 (Choice; n=127, 8 comparisons), vertical orientation increased the probability of being chosen by roughly 40–50 percentage points and color by roughly 20–30 percentage points for both readability and trustworthiness, while source had a smaller effect (detectable mainly when other background factors were held fixed). Study 3 (Qual; n=20) found 19/20 participants preferred vertical bars and provided reasons involving familiarity, reading direction, and perceived professionalism; some did not notice source citations, helping explain the weaker effects in forced-choice settings.","Image size differed between Study 1 and Study 2 because the choice task displayed two charts simultaneously. Participants across studies were drawn from a WEIRD demographic (young computer science students) and Study 1 vs Study 2 cohorts came from different semesters of the same course. The charts were artificially generated and intentionally simple, limiting complexity and realism.","The experiments manipulate only three binary design factors on simple bar charts, so findings may not generalize to other chart types, richer color palettes, typography/layout differences, or realistic multi-element visuals common in practice. Online studies may be sensitive to uncontrolled viewing conditions (screen size, device, ambient light), which could interact with subtle manipulations like source annotations. The paper emphasizes methodological differences but does not fully quantify how stable results are across alternative question wordings, alternative pairwise choice interfaces, or different participant populations beyond HCI students.","The authors state they aim to explore the conceptual link between subjective experience and sensemaking, including experienced understandability of charts, perceived cognitive effort, and how these relate to objective levels of complexity.","Test whether the observed procedural variance persists across broader visualization types (lines, scatterplots, maps) and more realistic multi-factor chart variations, including typography, gridlines, legends, and multi-hue color schemes. Evaluate robustness under non-laboratory viewing contexts (mobile vs desktop, varying resolutions) and with more diverse populations and expertise levels. Provide open-source analysis code and stimuli generation scripts to enable replication and to study how interaction effects change under alternative factorial structures or adaptive experimentation.",2310.13713v1,https://arxiv.org/pdf/2310.13713v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:10:17Z
FALSE,NA,NA,Not applicable,Not applicable (not a DOE paper),Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher)|Personal website|In text/Appendix,https://zenodo.org/records/10018285|https://multifario.sourceforge.io/|https://sourceforge.net/projects/cocotools|https://github.com/mingwu-li/forward|http://doi.org/10.5281/zenodo.4011281|https://doi.org/10.5281/zenodo.6338831,"This paper proposes an efficient workflow to compute and characterize forced response surfaces (FRSs) of periodically forced, damped nonlinear mechanical systems, including high-dimensional finite-element models and systems with internal resonances. The authors build reduced-order models (ROMs) using spectral submanifolds (SSMs) so that periodic orbits of the full system correspond to fixed points of low-dimensional reduced dynamics; they then compute the FRS via two-parameter manifold continuation (Henderson atlas algorithm implemented in COCO) of these fixed points. For non-internally-resonant cases with 2D SSMs, they derive an analytic implicit equation for the FRS in reduced coordinates, avoiding full surface continuation. To avoid computing the full FRS when only key features are needed, they formulate optimization problems whose solutions trace ridges and trenches (curves of extrema across forced response curves) and solve these via successive continuation on augmented KKT/adjoint conditions on the ROM. The approach is demonstrated on a cantilever beam with nonlinear support, a von Kármán plate with 1:1 internal resonance, and a shallow shell with 1:2 internal resonance, showing large computational speedups relative to full-system collocation/shooting while matching sampled full-system forced response curves.","The full system is posed as a forced nonlinear mechanical model (first-order form) $B\dot z = Az + F(z) + \epsilon F_{\mathrm{ext}}(\Omega t)$ with periodic boundary condition $z(0)=z(T)$, $T=2k\pi/\Omega$. SSM reduction yields low-dimensional reduced dynamics whose fixed points correspond to periodic orbits; for 2D SSMs without internal resonance, fixed points satisfy an implicit scalar equation $F(\rho,\Omega,\epsilon)=a(\rho)^2 + (b(\rho)-\Omega)^2\rho^2 - \epsilon^2|f|^2=0$ defining the FRS in reduced coordinates. Response amplitudes are expressed algebraically from SSM expansion coefficients (e.g., an $L_2$-type amplitude simplifies to $A_{L_2}=\sqrt{\sum_{\hat r_i} \hat w_{\hat r_i,I}^* Q\, \hat w_{\hat r_i,I}}$), enabling ridge/trench extraction via KKT conditions and continuation.","On a 50-DOF cantilever beam example, an analytic SSM-based FRS is computed in about 7 seconds and an atlas-based FRS in about 0.5 hours, while six sampled full-system FRCs via collocation take about 6 hours; ridge/trench extraction on the ROM takes about 31 seconds versus about 1.5 days for full-system adjoint/collocation optimization. For a 606-DOF von Kármán plate with 1:1 internal resonance, the full FRS via SSM+continuation is obtained in about 2 hours, whereas four sampled full-system FRCs via shooting+continuation require about 31 days; ridge/trench curves are computed in roughly 1–3 minutes depending on the objective. For a 1320-DOF shallow shell with 1:2 internal resonance, the SSM-based FRS is obtained in about 1.5 hours, while sampled full-system FRCs require roughly 90–180 hours each; ridge/trench extraction takes about 71–81 seconds across different amplitude objectives.",None stated.,"The work is not about designed experiments; parameter sweeps over forcing frequency and amplitude are treated as continuation parameters rather than an experimental design with optimal run selection, randomization, blocking, or inference goals. Empirical validation relies on a limited number of sampled full-system forced response curves; broader benchmarking across noise, model uncertainty, and measurement error is not addressed. The method’s practical usability depends on availability and robustness of SSM computations (choice of truncation order, resonance handling, and convergence), but guidance is mainly heuristic (e.g., checking TI vs TV SSM accuracy).",The authors suggest extending the procedure from forced response surfaces of periodic orbits to forced response surfaces of quasi-periodic orbits. They also note applicability to systems with configuration constraints.,"Developing a systematic, automated error estimator for SSM truncation order and for deciding TI vs TV SSM models would strengthen reliability in practice. Extending the approach to stochastic/uncertain forcing, measurement noise, and parameter identification contexts could connect the continuation results to experimental data workflows. Providing a packaged, end-to-end software pipeline (including COCO continuation setups for ridges/trenches) and broader real-world case studies would improve adoption beyond specialists.",2310.14850v1,https://arxiv.org/pdf/2310.14850v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:10:51Z
TRUE,Optimal design|Bayesian design|Robust parameter design|Other,Parameter estimation|Robustness|Other,Bayesian D-optimal|Bayesian A-optimal|Other,Variable/General (examples include k=2 and k=3 controllable variables; general k in framework),Theoretical/simulation only,Simulation study|Other,TRUE,R,Not provided,NA,"The paper introduces a new decision-theoretic framework, Gibbs optimal design of experiments, extending Bayesian optimal design to settings where the response model may be misspecified or not fully specified. It replaces the model-based data-generating distribution used in Bayesian expected utility with a user-chosen “designer distribution” and uses Gibbs inference (general Bayesian updating based on a loss function) to define posterior uncertainty and utilities. The framework is developed theoretically and connected to classical A- and D-optimality for linear models under sum-of-squares loss, yielding modified objective functions that incentivize replication via pure-error degrees of freedom. A practical computational strategy is proposed that combines normal approximations to the Gibbs posterior, Monte Carlo estimation of expected utility, and approximate coordinate exchange (ACE) optimization. Illustrations cover linear models, count responses with overdispersion (quasi-likelihood loss), and time-to-event responses via Cox partial likelihood, showing Gibbs-optimal designs can be more robust when strong distributional assumptions fail.","Gibbs posterior: $\pi_\ell(t\mid y,X)\propto \exp\{-w\,\ell(t;y,X)\}\,\pi_\ell(t)$. Gibbs expected utility uses a designer distribution $D(r;X)$ and hyperprior $C$: $U_G(X)=\mathbb{E}_C\,\mathbb{E}_{D(r;X)}[u(\theta_{\ell,D}(r;X),y,X)]$, where $\theta_{\ell,D}(r;X)=\arg\min_t\mathbb{E}_{D(r;X)}[w\ell(t;y,X)]$. For linear models with SS loss $\ell_{SS}(t;y,X)=\sum_i(y_i-f(x_i)^Tt)^2$, the target under the designer distribution is $\theta_{SS,D}(r;X)=(F^TF)^{-1}F^T\mu$.","For linear models under SS loss with fixed calibration weight, the resulting Gibbs expected utilities reduce to modified A- and D-type criteria that depend on $F^TF$ and the pure-error degrees of freedom $d=n-q$, and yield $-\infty$ utility when $d=0$ (no replication) because the variance estimate (and thus Gibbs posterior) does not exist. In the worked example (n=16, k=3, p=10), the Gibbs-optimal designs under fixed-w and random-w coincide in performance and achieve about 85% efficiency relative to the D-optimal design, while also ensuring replication (d=6) versus the D-optimal design (d=0). For L2-loss-based Gibbs designs, a maximum projection Latin hypercube achieves about 70–75% efficiency relative to the Gibbs-optimal designs, and the SH-utility Gibbs design attains about 88% efficiency under the NSE objective whereas the NSE-utility Gibbs design performs poorly (about 45%) under the SH objective. Simulation results for calibration-weight choices show the random-w approach produces wider intervals with over-coverage that does not converge to nominal as n increases, whereas the fixed-w approach behaves better.",None stated.,"The work is demonstrated primarily via synthetic/numerical examples; there is no real experimental case study validating the approach in a practical domain. Performance depends critically on user specification of the designer distribution and hypervariable distribution $C$, but guidance for elicitation/robustness to misspecification of these choices is limited. The computational approach (Monte Carlo + repeated optimizations + ACE) can be expensive for large n/k or complex losses, and the paper does not provide wall-clock benchmarks or scalability analysis.","The authors note that, unlike Bayesian inference, Gibbs target parameter values under a fixed design can depend on the design, and they propose future work to address this (e.g., using limiting target parameters as $n\to\infty$ that have design-independent physical interpretation). They also propose studying sensitivity of Gibbs optimal designs to the chosen calibration-weight specification. Finally, they encourage adapting/expanding modern Bayesian design computation methods to Gibbs optimal design, accounting for the extra step of computing $\theta_{\ell,D}(r;X)$.","Develop principled robustness diagnostics and sensitivity analyses for the designer distribution and hyperprior (e.g., worst-case/ambiguity-set designer distributions or PAC-Bayesian bounds) so practitioners can quantify design stability. Provide software implementations (e.g., extending acebayes) with reproducible code and scalable approximations (surrogate modeling/emulation of utilities, variance reduction, parallelization) for higher-dimensional design spaces. Extend the framework to common DOE settings such as constrained designs (blocking/split-plot), discrete factor levels, multivariate responses, and sequential/adaptive experimentation where the designer distribution can be updated between stages.",2310.17440v2,https://arxiv.org/pdf/2310.17440v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:11:25Z
TRUE,Other,Prediction|Other,Not applicable,"Variable/General (discusses p moderators/covariates; examples include p=1,3,5)",Healthcare/medical|Food/agriculture|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper argues that randomized experiments should be planned not primarily for hypothesis tests of average treatment effects, but to minimize prediction error for unit-specific (individual/cluster) treatment effects in a specified target population. Using a two-arm randomized experiment and parametric OLS models, it derives closed-form expressions for unit-level squared prediction error (SPE) and population-average mean squared prediction error (MSPE), clarifying how sample size, allocation, number of moderators, residual outcome variance, and irreducible/idiosyncratic treatment-effect variation affect predictive accuracy. It shows that in small experiments (e.g., cluster trials with few sites), predicting everyone’s effect with the (possibly covariate-adjusted) ATE can have lower MSPE than fitting heterogeneous-effect models unless moderators explain a substantial share of treatment-effect variation. The paper extends the analysis to generalizability/covariate-shift settings where the estimation sample population differs from the prediction target, demonstrating added bias terms and that standard MSPE estimators can be biased. It discusses weighting (inverse-odds/covariate-shift) adjustments to transport predictive models, emphasizing a variance inflation (effective sample size) penalty and illustrating implications with education case studies and planning calculations.","Unit-specific treatment effect is modeled as $\delta_i = Y_i(1)-Y_i(0)=\Delta_A + x_{i|A}'\,\delta + \eta_i$, where $\eta_i$ is idiosyncratic (unpredictable) effect variation. Predictions are formed by fitting separate OLS models in treatment and control and subtracting: $\hat\delta_i = \hat\Delta_A + (\hat\beta_1-\hat\beta_0)'x_{i|A}$. The unit-level prediction error measure is $\mathrm{SPE}(\hat\delta_i)=\mathbb{E}[(\hat\delta_i-\delta_i)^2\mid x_{i|A}]$, yielding (under random sampling/assignment) $\mathrm{SPE}(\hat\delta_i)=\left(\sigma^2_{0|x}/n_0+\sigma^2_{1|x}/n_1\right)\left(1+x_{i|A}'\Sigma_{x|A}^{-1}x_{i|A}\right)+\tau^2_{A|x}$ and population-average $\mathrm{MSPE}=\left(\sigma^2_{0|x}/n_0+\sigma^2_{1|x}/n_1\right)(1+p)+\tau^2_{A|x}$. Under covariate shift (training population $P_A$, target $P_B$), MSPE adds shift and bias terms (e.g., involving Mahalanobis/Burg divergence) and can be mitigated via inverse-odds weighting $w_i\propto (1-\Pr(Z_i=0\mid x_i))/\Pr(Z_i=0\mid x_i)$ with variance inflation $\mathrm{MVIF}=1+\mathrm{Var}(w_i)/\mathbb{E}(w_i)^2$.","Analytically, MSPE for heterogeneous-effect (moderator) models increases with the number of moderators $p$ via estimation variance but decreases as moderators explain outcome variance and treatment-effect variance; the paper provides closed-form MSPE/SPE formulas and an inequality (Eq. 3.26) giving the minimum $R^2_{\tau p}$ needed for moderator models to beat ANCOVA/ATE prediction. Simulations illustrate that with small per-arm sample sizes (e.g., $n\approx 20$), moderator models must explain large portions of treatment-effect variance (e.g., on the order of tens of percent or more depending on $\tau_*^2$ and $p$) to outperform the covariate-adjusted ATE in MSPE, whereas with larger $n$ the required $R^2_{\tau p}$ drops sharply. A planning example based on an education RCT (ASSISTments) shows that designs powered for detecting an average effect can still yield wide prediction intervals for unit-specific effects when treatment-effect heterogeneity is moderate/large. In population-shift examples, covariate distribution differences between training and target populations can inflate MSPE; weighting can reduce bias but incurs a variance inflation penalty (effective sample size loss), illustrated with state-vs-national school populations and a North Carolina vs U.S. example (reported VIF about 1.42 in that case).","The paper restricts attention to a simple two-group randomized experiment and unit-specific predictions based on parametric linear models estimated by OLS to obtain closed-form expressions. It notes that key quantities governing irreducible treatment-effect variation (e.g., correlations between potential outcome residuals $\rho_{01|x}$ and between $Y(0)$ and effects $\rho_{0\eta}$) are not identifiable due to the fundamental problem of causal inference, implying the need for sensitivity/bounding approaches. It also highlights that when the estimation sample is not a random sample of the prediction population, standard predictive error estimators can be biased and key target-population parameters may be unknowable without extra assumptions or data.","The design guidance is derived under linearity, correct model specification (especially for moderator structure), and largely i.i.d. settings; performance under nonlinear/moderately misspecified models or complex interference/cluster dependence is not developed. The weighting/transport discussion assumes adequate measurement of all effect moderators needed for transport (strong ignorability-like conditions) and focuses on variance inflation but does not deeply analyze robustness to weight-model misspecification or practical stabilizing/regularizing strategies beyond noting trimming can induce bias. While the motivation is unit-specific treatment effects, the paper does not provide an explicit algorithmic “optimal design” procedure (e.g., selecting sampling frames/allocations to minimize MSPE under realistic constraints) beyond analytical insights and examples.",None stated.,"Extend the MSPE-based planning framework beyond OLS to modern causal prediction learners (e.g., causal forests/BART) and derive analogous design heuristics or approximations for their error under realistic sample sizes. Develop practical MSPE-minimizing sampling/experimental design algorithms that jointly choose recruitment populations, stratification, and treatment allocation while accounting for covariate shift, positivity, and cost constraints. Provide methods to estimate or bound the unidentified correlations driving idiosyncratic treatment-effect variance (e.g., sensitivity analysis defaults, empirical priors) and validate the resulting planning recommendations on multiple real RCT datasets with known external targets.",2310.18500v1,https://arxiv.org/pdf/2310.18500v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:12:12Z
TRUE,Factorial (fractional)|Screening|Robust parameter design|Other,Screening|Optimization|Cost reduction,Not applicable,5 factors (each at 3 levels; L27 OA; 27 runs; 3 repetitions mentioned),Network/cybersecurity|Theoretical/simulation only,Simulation study|Other,TRUE,C/C++|Other,Not provided,http://servforu.blogspot.com.tr/2012/12/architecture-of-wireless sensor-networks.html,"The paper applies Taguchi Design of Experiments to identify the most significant RPL (Routing Protocol for Low-Power and Lossy Networks) configuration parameter affecting power consumption in IoT/WSN scenarios. Five input factors (Network Size, Mobility Speed, DIO_MIN_INTERVAL, DIO_DOUBLING, and Redundancy Constant) are studied at three levels each using an L27 orthogonal array, reducing the experiment count from 3^5=243 full-factorial cases to 27 simulation scenarios. Experiments are executed in the Contiki/Cooja simulation environment, and power consumption is evaluated (overall average power derived from CPU, LPM, radio listen, and radio transmit power), using the Taguchi smaller-the-better SNR formulation and ANOVA to quantify factor significance. Results indicate DIO_MIN_INTERVAL is the dominant factor (largest delta in SNR response table and highest F-statistic in ANOVA), while Redundancy Constant is not statistically significant at α=0.05. The work demonstrates a cost-efficient screening/robust-design style approach to guide subsequent optimization of RPL for lower energy consumption.","The Taguchi experiment sizing is described via degrees of freedom: $N_{\text{Taguchi}} = 1 + \sum_{i=1}^{NV} (L_i-1)$. For the response (power consumption) the paper uses the Taguchi smaller-the-better SNR: $N_p = 10\,\log\left(\frac{1}{r}\sum_{i=1}^{r} y_i^2\right)$, where $r$ is the number of repetitions and $y$ is the observed response (power). ANOVA quantities are given, including total sum of squares $SS_T=\sum (N_i-\bar N)^2$, factor sums $SS_j=\sum (N_{ji}-\bar N)^2$, contribution $p_j=(SS_j/SS_T)\cdot 100$, and $F_j=(SS_j/Df_j)/(SS_c/Df_c)$.","Using an L27 OA (27 runs) with 5 factors at 3 levels, ANOVA shows factor C (DIO_MIN_INTERVAL) has the largest effect on power consumption with $F=198.96$ and $p\approx 0$ (reported as 0), making it the most significant factor. Factor E (Redundancy Constant) is not significant with $F=2.64$ and $p=0.102>0.05$. The SNR response table ranks factors by delta as: MIN (delta 8.362, rank 1), DOUBLING (2.818, rank 2), Mobility Speed (2.062, rank 3), Network Size (1.501, rank 4), and RC (0.679, rank 5). The authors report repeating simulations three times and observing identical results across repeats, suggesting steady-state for the chosen simulation time/area.","The paper notes simulator-related drawbacks specific to Cooja: reliance on JNI can introduce side effects, and there is dependency on external tools such as compilers/linkers and their runtime arguments. No other methodological limitations are explicitly stated.","The study is entirely simulation-based (Contiki/Cooja) with no real-world testbed validation, so generalizability to deployed IoT/LLN networks is uncertain. The design appears to focus on main effects (L27 OA) and largely concludes interactions are negligible from interaction plots, but the OA choice and analysis may not robustly estimate higher-order interactions or nonlinearities. Reporting of p-values as 0 and lack of confidence intervals/variance details for power estimates limits statistical interpretability, especially given only three repeats. Factors/levels are specific (e.g., fixed area 100m×100m, time 600s), so conclusions about significance may change under different traffic loads, radio models, objective functions, or network topologies.",The authors propose applying optimization techniques (soft computing/hard computing) to the significant factors identified (especially DIO_MIN) to further reduce power consumption and support “green technologies.” They also suggest developing an automated tool that adjusts DIO_MIN based on inputs such as network size and mobility speed to optimize RPL performance.,"Extend the DOE to include additional RPL/LLN factors (e.g., objective function choice, ETX/LQI settings, duty-cycling MAC parameters) and broaden level ranges to test robustness. Validate findings on physical testbeds and under alternative simulators/radio models, including sensitivity to interference and realistic mobility patterns. Consider sequential/adaptive DOE (e.g., follow-up response-surface modeling around the best region) to move from screening to true optimization of energy vs. latency/reliability tradeoffs. Provide reproducible artifacts (simulation configs, code, seeds) and report uncertainty (confidence intervals) for power metrics and factor effects.",2310.19172v1,https://arxiv.org/pdf/2310.19172v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:12:45Z
TRUE,Sequential/adaptive|Computer experiment|Other,Optimization|Other,Not applicable,Variable/General (examples include 56-D and 60-D continuous robot morphology; discrete DNA sequence length 8 and 10),Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/tung-nd/ExPT.git,"The paper introduces few-shot experimental design for black-box optimization, where only a handful of labeled (design, score) pairs are available for a new task, but a larger pool of unlabeled candidate designs is available for pretraining. It proposes Experiment Pretrained Transformers (ExPT), an encoder–decoder model that is pretrained on synthetic functions defined over the unlabeled design domain, enabling gradient-free in-context adaptation to new objectives. Synthetic pretraining is performed by sampling Gaussian process (RBF-kernel) functions on unlabeled inputs and training an inverse model to generate inputs conditioned on a small context set and desired target value (including an assumed optimum value y⋆ at test time). The method is evaluated on Design-Bench tasks (robot morphology optimization and DNA binding affinity sequence design) under two few-shot regimes (random 1% and poorest 1%), using an optimization budget of Q=256 candidate proposals per task. Across tasks and metrics (median/mean/max of evaluated candidates), ExPT achieves the best average performance and ranks, with especially large gains in the hardest setting (reported as ~70% mean-performance improvement over the second-best method on average).","The ED objective is posed as $x^\star \in \arg\max_{x\in\mathcal{X}} f(x)$. ExPT is pretrained via inverse modeling to maximize the conditional log-likelihood of target inputs given context pairs and target outputs: $\max_\theta\,\mathbb{E}[\log p_\theta(x_{m+1:N}\mid x_{1:m},y_{1:m},y_{m+1:N})]=\max_\theta\,\mathbb{E}[\sum_{i=m+1}^N \log p_\theta(x_i\mid x_{1:m},y_{1:m},y_i)]$. Synthetic functions are sampled from a GP prior with RBF kernel $K(x,x')=\sigma^2\exp\big(-\|x-x'\|^2/(2\ell^2)\big)$, and the decoder is trained with a conditional VAE ELBO: $\log p_\theta(x_i\mid h_i)\ge \mathbb{E}_{q_\phi(z\mid x_i,h_i)}[\log p_\theta(x_i\mid z,h_i)]-\mathrm{KL}(q_\phi\|p(z))$.","On Design-Bench few-shot tasks (D’Kitty, Ant, TF Bind 8, TF Bind 10) with Q=256 evaluations, ExPT attains the best overall average scores and ranks in both the random-1% and poorest-1% settings. In the more challenging poorest setting, ExPT is reported to outperform the second-best method by ~70% on average in mean performance, with task-wise mean-score improvements cited as 40% (D’Kitty), 176% (Ant), and 18% (TF Bind 8). In the random setting, ExPT notably improves over the best baseline on Ant by 18% (median), 9% (max), and 10% (mean). A sequential sampling variant (ExPT-Seq) further improves performance (e.g., on Ant-poorest, +20% median and +19% mean vs. simultaneous sampling).","The authors note two key assumptions: access to a larger unlabeled dataset for pretraining and knowledge of the optimal target value $y^\star$ during optimization. They also note that they pretrain separate models per domain and that, during pretraining, they cannot validate on real downstream functions; they suggest synthetic-function validation/early stopping as a potential remedy left for future work.","The approach assumes the domain-specific unlabeled design pool is representative of where optima lie; if the unlabeled set has poor coverage or misses high-performing regions, inverse generation is fundamentally constrained. The requirement to condition on an optimal value $y^\star$ can be unrealistic in many practical settings (optimum unknown/ill-defined), and performance sensitivity to misspecified $y^\star$ is not characterized here. Comparisons are centered on Design-Bench and simulation-oracle tasks; broader real-world experimental validations (e.g., wet lab or physical experiments) and robustness to measurement noise, constraints, and distribution shift in the unlabeled pool remain open. Implementation details (languages/libraries) are not specified in the paper text, which may affect reproducibility despite code availability.","They propose relaxing the assumptions of having a large unlabeled pretraining set and knowing $y^\star$. They also suggest fine-tuning the pretrained ExPT on downstream labeled data to further improve performance. Finally, they express interest in training a single large model that works across all domains (rather than domain-specific pretraining) and studying whether this improves per-domain results.","Study robustness to misspecification or uncertainty in $y^\star$ (e.g., treating $y^\star$ as a tunable/latent variable or using quantile-based targets) and provide guidance for practitioners when $y^\star$ is unknown. Extend evaluation to settings with constraints, noisy/heteroscedastic outcomes, or correlated samples (time series/closed-loop experiments) and quantify performance under such violations. Compare against additional few-shot BO/meta-BO baselines and analyze compute/sample trade-offs, including amortized cost of pretraining versus gains at adaptation. Provide standardized software artifacts (e.g., a pip/conda package) and benchmark scripts to facilitate adoption and fair replication across domains.",2310.19961v1,https://arxiv.org/pdf/2310.19961v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:13:29Z
TRUE,Sequential/adaptive|Bayesian design|Computer experiment|Other,Optimization|Prediction|Robustness|Cost reduction|Other,Not applicable,Variable/General (examples include 1D synthetic; 2D grids; 12D and 14D controller tuning),Food/agriculture|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://gpflow.readthedocs.io/en/develop/notebooks/advanced/heteroskedastic.html|https://github.com/openai/gym/blob/8a96440084a6b9be66b2216b984a1c170e4a061c/gym/envs/box2d/lunar_lander.py#L447,"This paper proposes Batch Thompson Sampling for Replicable Experimental Design (BTS-RED), a Bayesian optimization framework for experimental settings where multiple conditions can be evaluated in parallel and each condition may need multiple replications due to large, heteroscedastic noise. The key methodological idea is to adaptively allocate replications per selected condition so that the empirical-mean observation has an effectively bounded noise variance (controlled by a parameter R^2), enabling regret guarantees despite heteroscedasticity. Two algorithms address known vs. unknown noise variance (the latter models negative variance with a second GP to build an upper confidence bound used for replication allocation), and a third algorithm (Mean-Var-BTS-RED) optimizes a risk-averse mean–variance objective that trades off high mean performance with low variability. The authors provide sublinear regret bounds (asymptotically no-regret) for all variants and show empirical gains over batch TS with fixed replication counts and over sequential baselines. Demonstrations include precision agriculture (optimizing plant growth conditions) and AutoML/hyperparameter tuning with emphasis on reproducibility.","Observations follow $y=f(x)+\epsilon$ with heteroscedastic noise $\epsilon\sim\mathcal N(0,\sigma^2(x))$; when $n$ replications are taken, the empirical mean has variance $\sigma^2(x)/n$. BTS-RED-Known selects $x_t^{(b)}=\arg\max_x f_t^{(b)}(x)$ with $f_t^{(b)}$ sampled from a GP posterior and sets replications $n_t^{(b)}=\lceil\sigma^2(x_t^{(b)})/R^2\rceil$ so that $\sigma^2(x_t^{(b)})/n_t^{(b)}\le R^2$. With unknown noise, a second GP models $g(x)=-\sigma^2(x)$ using the negated unbiased sample variance $\hat y^e=-\frac{1}{n-1}\sum_i (y_i-\bar y)^2=g(x)+\epsilon'$ and uses an upper bound $U_{\sigma^2,t}(x)=-\mu'_{t-1}(x)+\beta'_t\sigma'_{t-1}(x)$ to set $n_t^{(b)}=\lceil U_{\sigma^2,t}(x_t^{(b)})/R^2\rceil$; Mean-Var-BTS-RED selects $x$ by maximizing $\omega f_t^{(b)}(x)+(1-\omega)g_t^{(b)}(x)$.","The paper proves high-probability sublinear regret bounds for BTS-RED-Known and BTS-RED-Unknown (hence asymptotically no-regret) under GP/RKHS assumptions even with heteroscedastic noise, by enforcing a bounded effective noise level $R^2$ through replication. It also provides a theory-guided choice of $R^2$ that minimizes the bound: $R^2=\sigma^2_{\max}(\sqrt{B}+1)/(B-1)$ (with an added tunable scale $\kappa$ in experiments). Mean-Var-BTS-RED similarly achieves sublinear mean–variance regret with a bound decomposing into weighted contributions from the mean GP and the noise-variance GP. Empirically, across synthetic tests, precision-agriculture data-driven experiments, and AutoML (SVM tuning on EMNIST-derived tasks), adaptive replication (BTS-RED variants) outperforms batch Thompson sampling with fixed replication counts and generally beats sequential methods that cannot leverage batching.","The authors note a limitation that unused budget within an iteration is handled using a heuristic procedure to carry over unfinished replications to the next iteration, rather than a fully principled mechanism.","The approach relies on Gaussian noise assumptions for deriving the sample-mean variance and for modeling/upper-bounding variance via a Chi-squared concentration argument; performance may degrade under heavy tails, outliers, or dependent errors. It uses GP surrogates (SE kernels in experiments) and discretization/optimization heuristics (random search, L-BFGS-B) that can struggle in very high-dimensional or nonstationary settings and may be sensitive to hyperparameter fitting. Although code is implied by detailed implementation descriptions (e.g., random Fourier features, GPflow use), reproducibility may be limited without released code and exact experimental configs.","The authors suggest incorporating their adaptive replication strategy into other batch Bayesian optimization algorithms to further improve performance, and exploring modeling choices for the noise variance (e.g., modeling $\log\sigma^2(\cdot)$) for potential empirical gains. They also propose combining the method with neural bandits to broaden applicability to more AI-for-science problems.","A natural extension is to develop principled budget-carryover or synchronous/asynchronous scheduling policies that avoid heuristic handling of leftover replications while preserving regret guarantees. Another direction is robustness to non-Gaussian noise and temporal correlation (e.g., robust estimators for variance, heavy-tailed concentration, or time-series noise models). Finally, providing an open-source reference implementation and benchmarking on standardized BO suites (including constrained and multi-objective settings) would strengthen practical adoption and validation.",2311.01195v1,https://arxiv.org/pdf/2311.01195v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:14:15Z
TRUE,Response surface|Computer experiment|Sequential/adaptive|Other,Optimization|Prediction,Not applicable,Variable/General (vehicle geometry parameterized by Bezier control-point coordinates; at least 6 control points with fixed x/c and varying y/c bounds; RSM described for multiple control-point variables),Transportation/logistics,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper presents an aerodynamic shape-optimization workflow for a prototype vehicle by parameterizing the top and side profiles with Bezier-curve control points and minimizing drag coefficient using CFD-driven optimization. To reduce the number of expensive CFD evaluations, it applies Response Surface Methodology with a Kriging surrogate model to approximate the relationship between control-point parameters (independent variables) and drag coefficient (objective). A Genetic Algorithm is then used to search the design space and identify a globally improved geometry subject to packaging/space constraints on the control-point bounds. CFD modeling is validated against Ahmed body wind-tunnel data and supplemented with grid-independence and grid-convergence studies to support numerical reliability. The optimized vehicle achieves lower drag and drag area, and flowfield plots (velocity/pressure) are used to qualitatively explain the improvements versus the baseline design.","The Kriging surrogate in the RSM is described as a trend-plus-stochastic-process model: $\hat{y}(x)=\sum_{i=1}^k \beta_i f_i(x)+Z(x)$, where $Z(x)$ is a mean-zero stochastic process with covariance $\mathrm{cov}(Z(x^{(i)}),Z(x^{(j)}))=\sigma^2 R(x^{(i)},x^{(j)})$. The GA selection probability is given as $P(x)=\frac{f(x)}{\sum f(x_i)}$ (with fitness tied to drag minimization), and candidate solutions are represented as parameter vectors $x=(x_1,x_2,\ldots)$.","For the baseline (“Initialised Design”), the reported values are $C_d=0.094$, frontal area $A=0.72\,\mathrm{m}^2$, and drag area $C_dA=0.0677\,\mathrm{m}^2$. For the optimized design, $C_d=0.069$, $A=0.48\,\mathrm{m}^2$, and $C_dA=0.0331\,\mathrm{m}^2$. This corresponds to a 26.6% reduction in drag coefficient and a 51.1% reduction in drag area. Validation on the Ahmed body shows drag predictions improving with grid refinement (e.g., fine-grid $C_d\approx0.248$ vs experiment 0.230; error 7.82%).","The authors state that the current approach focuses only on optimizing the side and top profiles (i.e., a simplified representation of the full vehicle geometry). They note that further work is needed to handle more complex geometries via parameterization of three-dimensional geometries.","The DOE/RSM portion does not specify the sampling design used to generate training points for the Kriging model (e.g., Latin hypercube, factorial/CCD), the number of CFD samples, or how surrogate uncertainty is managed, which makes reproducibility and assessment of surrogate fidelity difficult. The optimization results appear tied to steady-state RANS with a specific turbulence model and boundary conditions; sensitivity to modeling choices (turbulence model, inlet turbulence, ground/moving wall, wheel rotation) is not quantified. Constraints and multi-objective tradeoffs (e.g., stability, lift, crosswind sensitivity, manufacturability) are not addressed, which may limit practical applicability. No code or detailed implementation settings for the GA (population size, mutation/crossover rates, stopping criteria) are provided, limiting repeatability.",They propose extending the method beyond the current 2D side/top-profile focus by developing parameterization approaches for full three-dimensional geometries to enable optimization of more complex vehicle shapes.,"Report and standardize the surrogate-training DOE (e.g., space-filling LHS with replication, adaptive infill) and provide sample sizes and surrogate error metrics across the design space, not only a single validation plot. Extend the framework to multi-objective optimization (drag, lift/downforce, yaw robustness) and include uncertainty quantification for CFD/model-form uncertainty. Develop a self-contained, reproducible implementation (shared scripts, GA settings, geometry generator) and benchmark against alternative optimizers (e.g., Bayesian optimization, CMA-ES) under equal CFD budgets. Incorporate transient effects and more realistic on-road boundary conditions (moving ground, rotating wheels, yaw angles) to improve generalizability of the optimized design.",2311.04308v1,https://arxiv.org/pdf/2311.04308v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:14:49Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Cost reduction|Other,Not applicable,Variable/General (multi-armed bandits with K≥2 arms; binary case emphasized),Service industry|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/biyonka/mixture_adaptive_design,"The paper proposes the Mixture Adaptive Design (MAD), an adaptive experimental design for multi-armed bandit (MAB) experiments that enables anytime-valid (continuous-monitoring) causal inference on the average treatment effect (ATE) while still allowing regret-minimizing assignment. MAD mixes any user-chosen bandit assignment algorithm with a Bernoulli (uniform randomization) design using a deterministic mixing schedule \(\delta_t\in(0,1]\), so treatment probabilities remain nonzero even for deterministic bandits. Under bounded outcomes and a mild growth condition on cumulative conditional variances, and when \(\delta_t = \omega(t^{-1/4})\), the authors derive an asymptotic confidence sequence for the ATE that is anytime-valid and whose width shrinks to zero; they also prove finite-time stopping when the long-run ATE is nonzero. The method extends to \(K\ge2\) arms and to batched assignment settings, and simulations (Bernoulli/Normal/heavy-tailed outcomes; TS and UCB) show near-nominal finite-sample coverage, faster stopping vs standard bandits, and little reward loss relative to the underlying bandit.","MAD assignment probabilities are defined by mixing Bernoulli with any bandit \(A\): \(p_t^{\text{MAD}}(w)=\delta_t\cdot(1/K) + (1-\delta_t)p_t^A(w)\) (binary case uses \(1/2\)). The ATE is estimated via an inverse-propensity-weighted (Horvitz–Thompson) estimator: \(\hat\tau_i=\mathbb{1}\{W_i=1\}Y_i/p_i^{\text{MAD}}(1) - \mathbb{1}\{W_i=0\}Y_i/p_i^{\text{MAD}}(0)\), with \(\hat{\bar\tau}_t=t^{-1}\sum_{i=1}^t\hat\tau_i\). The anytime-valid asymptotic confidence sequence takes the form \(\hat{\bar\tau}_t \pm \hat V_t\), where \(\hat V_t=\sqrt{\tfrac{2(\hat S_t\eta^2+1)}{t^2\eta^2}\log\big(\sqrt{\hat S_t\eta^2+1}/\alpha\big)}\) and \(\hat S_t=\sum_{i=1}^t \hat\sigma_i^2\) is based on an unbiased upper-bound variance estimator.","Theory: if outcomes are bounded and \(\sum_{i=1}^t \mathrm{Var}(\hat\tau_i\mid\mathcal F_i)=\Omega(t)\), then with \(\delta_t=\omega(t^{-1/4})\) the MAD confidence sequence is asymptotically anytime-valid and shrinks (\(\hat V_t\to 0\) a.s.). If \(\bar\tau_t\to c\neq 0\), then the stopping time when the CS excludes 0 is finite almost surely. Empirically (simulations up to \(T=10{,}000\), \(N=100\) replicates), MAD attains near-nominal anytime coverage and produces much earlier exclusion of 0 than a standard bandit design, while its average reward approaches that of the underlying bandit (e.g., TS/UCB) and its CS width is close to Bernoulli randomization.","The authors note their main confidence sequence is asymptotic (not exact finite-sample), so exact Type I error at an optional stopping time is not guaranteed without additional conditions; they recommend continuing for additional samples after the first exclusion of 0. They also explain that exact non-asymptotic CSs (e.g., Howard et al. 2021) require assignment probabilities bounded away from 0 and 1 (a \(p_{\min}\) condition), restricting \(\delta_t\) and/or excluding common bandits; such exact CSs can be much wider and less practical. They caution that certain stopping rules may be inappropriate under strong non-stationarity in the ATE even though the CS can track time variation.","The core theoretical guarantees rely on bounded realized outcomes and a linear-growth cumulative conditional variance condition; in practice, heavy-tailed metrics, outliers, or extreme propensity weights (when \(\delta_t\) is small and the bandit heavily exploits) can still yield unstable estimators and slow convergence. The work focuses on design-based (finite-population) ATE estimation; translating results to super-population/generalization targets or incorporating covariates/contextual bandits may require additional assumptions. Practical guidance for choosing \(\delta_t\) is heuristic; there is no data-driven or optimal calibration method provided to balance regret vs inferential precision across heterogeneous applications. Real-world validation beyond simulation (e.g., industry case studies) is not demonstrated in the provided text.",None stated.,"Develop principled, possibly adaptive/data-driven methods for choosing \(\delta_t\) (or mixing schedules) to optimize a formal regret–inference objective under realistic constraints. Extend the framework to contextual and high-dimensional bandits with covariate adjustment, and to settings with interference, clustered assignment, or network effects common in online experiments. Provide robust variants for heavy-tailed or contaminated outcomes (e.g., truncation/winsorization with valid sequential inference) and more extensive empirical validation on real experimentation-platform data, ideally with open-source reference implementations.",2311.05794v4,https://arxiv.org/pdf/2311.05794v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:15:33Z
TRUE,Factorial (full)|Split-plot|Other,Parameter estimation|Screening|Optimization|Prediction|Cost reduction|Other,Not applicable,Variable/General,Food/agriculture|Other,Other,TRUE,R,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/emitanaka/edibble|https://cran.r-project.org/,"This paper presents edibble, an R package implementing Tanaka’s “grammar of experimental designs” framework to plan, manage, and document experiments as composable operations on an internal graph representation. Experiments are specified by declaring experimental factors with roles (units, treatments, records) and encoding their relationships (nesting, crossing, conditioning), then separating high-level treatment/unit allotment from low-level assignment (often randomized or systematic) to produce an experimental design table. The package supports downstream workflow needs including explicit declaration of record/response variables, data validation rules for data entry, exporting designs to Excel with embedded validation, and simulation of record variables via user-defined or automated processes. The paper demonstrates the approach with multiple worked examples (e.g., calf feeding, wheat field trials, a common garden ecology experiment, and a linguistics rating study using Williams Latin squares via custom assignment code), positioning edibble as complementary to algorithm-focused DOE packages by emphasizing modular specification and experiment lifecycle integration. Practical value is framed around reducing planning and data-entry errors and enabling fit-for-purpose analysis through preserved structure metadata and provenance.","Not applicable (software/framework paper rather than a single charting statistic or closed-form design criterion). The core formalism is a pair of directed acyclic graphs (DAGs): a factor graph (relationships among factors) and a level graph (relationships among factor levels), manipulated via functions such as set_units/set_trts/set_rcrds, helper constructors (nested_in, crossed_by, conditioned_on), and link steps (allot_* for factor-level intentions; assign_* for level-level allocation).","The paper’s results are primarily functional demonstrations rather than numeric optimality/efficiency benchmarks. It shows that diverse experimental structures (nested/crossed/conditional treatments, split-plot-like allocations, blocking, and crossover/Williams Latin square sequences via custom ordering) can be constructed and converted into a design table, exported with Excel data validation, and used to simulate/validate record variables. No comparative metrics (e.g., D-efficiency, power, MSE) or run-length-type numerical performance summaries are reported.",The author notes there is no guarantee that a generated design will be appropriate or optimal; users must perform diagnostic checks for their objectives. The default assignment algorithm is described as unlikely to be optimal for a given structure and users are encouraged to modify or supply custom assignment logic. The package is positioned as supporting fixed experimental structures with fixed levels for each factor.,"Because the paper does not provide quantitative comparisons (e.g., efficiencies, power, balance/orthogonality metrics) against established DOE generators, it is hard to assess design-quality implications of the default or example assignment algorithms beyond face validity. The framework focuses on experiments with fixed factor levels; extensions to continuous-factor DOE (e.g., classical RSM/CCD, space-filling designs) and explicit optimal-design criteria are not demonstrated. Adoption may depend on user ability to correctly encode complex constraints; mis-specified graphs could yield plausible-looking but invalid randomization schemes without additional safeguards or diagnostics.","The paper suggests the system could be implemented in other programming languages, though this work focuses on R. It emphasizes that users can incorporate/wrap existing experimental design algorithms into the framework (e.g., custom assignment methods), implying continued extension of available assignment algorithms and integrations as a direction for broader use.","Adding built-in support and diagnostics for design optimality/quality (e.g., balance checks, aliasing/orthogonality summaries, efficiency criteria, randomization validity checks) would strengthen practical guidance. Extending the framework to continuous-factor designs (RSM, space-filling for computer experiments) and explicit optimal-design workflows (D/A/I-optimal) would broaden applicability beyond fixed-level comparative experiments. Providing an official R package vignette library and/or automated report generation (design rationale, randomization seed/protocol, constraints) plus more real-world case studies could improve reproducibility and user onboarding.",2311.09705v1,https://arxiv.org/pdf/2311.09705v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:16:07Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Optimization|Prediction|Other,Bayesian D-optimal|Other,"Variable/General (benchmarks shown for 1D, 2D, 5D; also examples up to 10D)",Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/trsav/HITL-BO,"The paper proposes an expert-guided, human-in-the-loop batch Bayesian optimisation method where a domain expert chooses among a small set of discrete candidate experiments each iteration. Candidate sets are generated by solving a multi-objective problem that jointly maximizes (i) the sum of acquisition/utility values over the batch and (ii) a diversity term defined as the determinant of the covariance matrix (log-det style information volume) of the augmented set including the standard BO optimum. A knee-point solution on the Pareto front (found via NSGA-II) yields alternatives that are both high-utility and sufficiently distinct, aiming to leverage expert strengths in discrete choice without forcing continuous tuning. Performance is evaluated on synthetic objective functions sampled from Gaussian process priors (Matérn 5/2 kernels) under simulated “human” behaviors (expert, adversarial, trusting, and probabilistic correctness). Results indicate faster early convergence when the chooser is informative, and recovery to standard BO regret when choices are effectively random, with degradation mainly under adversarial selection.","The next standard BO point is $x^* = \arg\max_{x\in\mathcal X\subseteq\mathbb R^n} U(x)$. For $p-1$ alternatives stacked in $X\in\mathbb R^{(p-1)\times n}$, batch utility is $\hat U(X)=\sum_{i=0}^{p-1} U(X_i)$. Diversity/variability is $\hat S(X,x^*) = |K_{X_{\text{aug}}}|$ where $X_{\text{aug}}=X\cup x^*$ and $K_{X_{\text{aug}}}$ is the GP covariance matrix over those points; the method solves $\max_X(\hat U(X;\mathcal D_t), \hat S(X,x^*))$ and selects the knee-point Pareto solution for presentation to the expert.","Benchmarks optimize 50 synthetic functions sampled from a GP prior using UCB as the utility, presenting 4 choices per iteration (the utility maximizer plus 3 alternatives). In 1D and 2D experiments, an “Expert” chooser improves convergence versus the “Trusting” (standard BO) baseline especially in early iterations, with diminishing advantage later as automated BO catches up. When the chooser selects the best option only part of the time (e.g., $p(\text{Best})=0.75$ or $0.5$), regret improves relative to standard BO; when choices are effectively random (e.g., $p(\text{Best})=0.25$ with 4 choices), the method recovers standard BO convergence. Performance degrades mainly for an “Adversarial” chooser, particularly in higher dimensions where selecting worst options acts like inefficient space-filling.",The authors note they cannot benchmark real human behaviour because the objectives are randomly sampled functions and due to practical issues; they instead use hypothesised behaviour models (expert/adversarial/trusting/probabilistic) and leave real human benchmarking for future work. They also position broader comparisons to other batch Bayesian optimisation methods as future work rather than completed evaluation in this paper.,"The approach’s practical benefit depends on having reliable domain experts and suitable interfaces; the paper does not quantify human time/cognitive load or how sensitive outcomes are to poor calibration of the information shown (utility values, predictive distributions). The diversity objective uses the GP covariance determinant, which may strongly depend on kernel/hyperparameter learning quality; robustness to GP misspecification, observation noise, constraints, and nonstationarity is not thoroughly analyzed. Computational cost may be substantial (multi-start acquisition optimisation plus NSGA-II each iteration), and the paper does not report wall-clock times or scalability beyond small batch sizes and modest dimensions. The study is simulation-only, so external validity to real experimental systems (measurement noise, delays, batching constraints, safety/feasibility constraints) remains untested.",The authors state intentions to benchmark against other batch Bayesian optimisation methodologies (including local penalization and other multi-objective batch infill strategies). They also plan to investigate whether large-language models can perform the discrete selection step. They further indicate that real human-behaviour benchmarking is left for future work.,"Evaluate the method on real laboratory/industrial human-in-the-loop campaigns with measured human decision times and outcomes, and compare against expert-prior weighting approaches and preference-learning BO baselines. Develop robustness extensions for noisy, constrained, and correlated (time-series) experiments, including feasibility-aware candidate generation and safety constraints. Provide ablations on the diversity term (e.g., log-det vs. distance-based space-filling) and on knee-point selection, plus computational scaling studies and cheaper surrogates for Pareto-set construction. Release a packaged implementation (e.g., Python) and standardized UI/protocols to ensure reproducibility of expert interaction.",2312.02852v1,https://arxiv.org/pdf/2312.02852v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:16:43Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Robustness|Cost reduction|Other,Other,Variable/General,Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an “independent-set design” for randomized experiments on networks with interference, partitioning units into an independent set (used for inference) and an auxiliary set (used to generate/shape exposure). The key idea is to regain near-separate control of (i) unit treatment assignments and (ii) interference/exposure levels by choosing assignments on the auxiliary set, while restricting estimation to non-adjacent (non-interfering) units. For estimating average direct effects at a target exposure level, it assigns treatments on the independent set via complete randomization and chooses auxiliary assignments by solving an integer program that minimizes deviation between realized exposures and the target. For spillover and total effects, it chooses auxiliary assignments by maximizing the variance of induced exposures and then estimates effects (notably under a linear additive outcome model) via regression, with theoretical bias/variance characterizations. The method is evaluated on synthetic graphs via simulation, reporting lower bias/variance than several competing network-experiment designs (complete randomization, full-graph designs, graph clustering, ego-clusters) across multiple random graph families.","Interference exposure for unit $i$ is defined as the proportion of treated neighbors $\rho_i = |N_i|^{-1}\sum_{j\in N_i} Z_j$. With a partition into independent set $V_I$ and auxiliary set $V_A$, exposures on the independent set satisfy $\rho_I = \Gamma Z_A$, where $\Gamma$ is a normalized bipartite adjacency (interference) matrix between $V_I$ and $V_A$. To target a fixed exposure $\rho$, auxiliary assignments solve $\min_{Z_A\in\{0,1\}^{n_A}} \|\Gamma Z_A - \rho \mathbf{1}\|_1$; to diversify exposures (for spillover/total effects), they solve $\max_{Z_A\in\{0,1\}^{n_A}} Z_A^T\Gamma^T\left(I-\frac{1}{n_I}\mathbf{1}\mathbf{1}^T\right)\Gamma Z_A$. Direct effects use a difference-in-means estimator on $V_I$, and spillover/total effects are estimated via regression under $Y_i=\alpha+\beta Z_i+\gamma \rho_i+\epsilon_i$.","Theorem 1 provides a high-probability lower bound on the greedy independent set size under an Erd\H{o}s–R\'enyi graph with edge probability $p=s/n$ (sparse/connected regime): $|V_I| \gtrsim (\log s/s)\,n$. For direct effects, Theorem 2 bounds conditional bias by a term proportional to $\|\Gamma Z_A-\rho\mathbf{1}\|_1/n_I$ (linking the L1 optimization objective to estimator bias) and shows variance is close to the complete-randomization variance when this deviation is small. For spillover effects under the linear additive model, Theorem 3 shows the regression estimator of $\gamma$ is unbiased with conditional variance $\sigma^2/(n_I\,\mathrm{Var}(\rho_I))$, motivating the exposure-variance maximization objective. Simulation studies on synthetic graphs (Erd\H{o}s–R\'enyi, Barab\'asi–Albert, small-world) report that the proposed independent-set design achieves the lowest bias and variance among compared methods for both spillover and direct effect estimation (tables/figures report consistent improvements across multiple $n$ and graph parameters).","The authors note the design estimates causal effects only on the independent set, reducing effective sample size versus using the full network. They also state that computational cost can increase because optimizing assignments on the auxiliary set may be time-consuming for large auxiliary sets. Finally, they acknowledge reliance on an observed network and that performance under network misspecification is currently unknown.","The approach relies heavily on the assumed exposure mapping (dependence only on proportion treated neighbors), which may be inadequate when interference depends on who is treated, tie strengths, higher-order neighborhoods, or complex contagion. Many results for spillover/total effects hinge on the correct specification of a linear additive outcome model; misspecification could yield biased regression-based estimates even if exposure variance is maximized. Practical deployment may also be constrained by the need to solve binary optimization problems (L1 integer program / quadratic program) repeatedly or at scale, and by the feasibility of implementing distinct assignment mechanisms across $V_I$ and $V_A$ in real experiments.","They propose improving computational efficiency of the auxiliary-set assignment optimization. They also suggest extending the framework to settings where the observed network is misspecified, to understand and improve robustness when the network used for design differs from the true interference structure.","Develop self-normalizing/robust exposure models and estimators that remain valid under broader forms of interference (e.g., weighted ties, distance-k interference, or multiple exposure conditions). Provide scalable approximation algorithms with performance guarantees for the auxiliary-set optimization (including distributed/online implementations) and study sensitivity of power/ARL-like metrics to optimization suboptimality. Extend to multivalued treatments, clustered/temporal networks with interference over time, and incorporate uncertainty in the observed network (Bayesian or errors-in-variables approaches) when constructing $V_I$ and choosing $Z_A$.",2312.04026v1,https://arxiv.org/pdf/2312.04026v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:17:23Z
FALSE,Other,Other,Not applicable,Not specified,Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Upon request,NA,"The paper proposes a flip-chip superconducting resonator layout modification to reduce sensitivity of resonator frequency to inter-chip spacing variations, by partially etching the metal on the bottom chip facing the resonator (creating a mixed metal-facing/dielectric-facing region). Electromagnetic simulations show that metal-facing and dielectric-facing regions shift frequency in opposite directions with spacing changes, enabling an etched-length “balance point” where frequency becomes relatively insensitive to spacing. The authors fabricate a large-scale flip-chip with columns of resonators at multiple etched lengths and intentionally introduce chip tilt to generate substantial spacing variation, then measure resonator frequencies via VNA reflection fitting. They quantify improvement by fitting each column’s resonator frequencies to a line and using RMSE as a linearity/stability metric, reporting >3.5× improvement for an optimized etched length (notably 290 µm) versus the unoptimized 0 µm design. They further analyze an additional parasitic mode introduced by the etched region and argue (via simulation-based estimates) it does not materially degrade qubit lifetime, supported by measured qubit T1 values in the fabricated device.","The paper evaluates frequency-linearity using RMSE of a linear fit: $\mathrm{RMSE}=\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i-\hat y_i)^2}$, where $y_i$ are measured resonator frequencies and $\hat y_i$ are values predicted by the least-squares linear fit. For Purcell-limit analysis in the appendix, it models coupled qubit–resonator and qubit–resonator–filter systems with non-Hermitian Hamiltonians, e.g., $H_{qr}=\begin{pmatrix}\omega_q & g_{qr}\\ g_{qr}^* & \omega_r-i\kappa_r/2\end{pmatrix}$ and $H_{qrf}=\begin{pmatrix}\omega_q & g_{qr} & 0\\ g_{qr}^* & \omega_r & g_{rf}\\ 0 & g_{rf}^* & \omega_f-i\kappa_f/2\end{pmatrix}$, and computes $T_1\approx 1/(-2\,\mathrm{Im}(\omega_{e,q}))$.","A non-optimized (0 µm etched length) design can exhibit large frequency sensitivity to spacing; simulations indicate that around a 0.8 µm spacing fluctuation near a 9 µm reference can produce about a 70 MHz resonator frequency fluctuation. In experiments across columns with different etched lengths, an optimized partial dielectric-facing design improves the linearity/stability of the frequency distribution by more than 3.5× relative to the full metal-facing design, as measured by RMSE of linear fits. The optimal etched length predicted by theory/simulation is about 330 µm (not experimentally shown due to fabrication damage on the filter), while 290 µm yields strong experimental improvement and reduces the off-target error of the measured frequency step to less than one-fifth of the original 0 µm design. The device’s measured chip spacing distribution is reported as about $9.6\pm 2.2\,\mu$m with an estimated tilt angle around 219 µrad, providing a deliberately challenging testbed for robustness.","The theoretically optimal etched length (330 µm) is not experimentally presented because of fabrication damage on the filter for that design. The authors also exclude the last two resonators in each column from the linearity/RMSE analysis because those resonators are designed with different shapes. They note that achieving optimal results requires etching a large bottom-chip metal area, which introduces an additional electromagnetic mode that must be checked for potential qubit leakage (and they analyze this risk).","Although framed as an “experimental design approach,” the work does not present a formal DOE methodology (e.g., factorial/optimal design with randomized run order, replicates planned for inference, or statistical modeling of factor effects); instead it is primarily an engineering parameter sweep over etched length. The robustness metric (RMSE of a linear fit) depends on the assumed intended linear frequency allocation and omits other relevant performance dimensions (e.g., impact on Q factors, readout fidelity/crosstalk metrics, yield across multiple chips/batches). The study uses one intentionally tilted assembly; generalizability to typical assemblies, different spacers/bump processes, and wafer-to-wafer variation is not established. Software/tools used for EM simulation and fitting are not specified, which can hinder reproducibility even if code is “available upon request.”","The authors state that the same design principle can be extended to other flip-chip quantum components based on CPW resonator structures, including the Purcell filter and qubit coupling bus. They also suggest applicability beyond superconducting qubits, mentioning potential use in quantum dot systems employing flip-chip architectures.","A natural next step is a structured multi-factor robustness study varying etched length jointly with other design/process factors (e.g., bump height, chip tilt/planarity, resonator geometry, filter proximity) to quantify main effects and interactions and produce a predictive model of frequency sensitivity. Additional validation on multiple chips and assembly conditions would help separate design benefits from single-device idiosyncrasies and support yield/reliability claims. Reporting/benchmarking against alternative robustness strategies (e.g., spacer/process controls alone, different ground-plane patterning) and including metrics tied directly to system performance (readout crosstalk, allocation failures within filter bandwidth, and readout fidelity) would strengthen practical guidance. Providing an open implementation (analysis + simulation setup parameters) or at least specifying the EM solver and fitting pipeline would improve reproducibility.",2312.06405v1,https://arxiv.org/pdf/2312.06405v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:18:19Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Other,Other,"Variable/General (controls: drive frequency ω_d plus 3 parameters per pulse segment: p(t), q(t), Δt; parameters inferred θ={ω, χ, T1, T2})",Semiconductor/electronics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,C/C++|Other,Not provided,https://arxiv.org/abs/2312.10233v2,"The paper presents an online Bayesian experimental design (BExD) framework to characterize superconducting transmon qutrits by adaptively selecting control-pulse experiments that maximize information about uncertain system parameters. Experiments are parameterized by piecewise-constant control pulses (p(t), q(t), segment durations Δt) and a drive frequency ω_d, while the inferred physical parameters include the transition frequency ω, anharmonicity χ, and decoherence times T1 and T2 in Lindblad dynamics. The design criterion is the expected Kullback–Leibler divergence (mutual information) between prior and posterior, approximated with Monte Carlo particles and optimized using differential evolution; posterior updates use a multinomial (shot-noise) likelihood or a Gaussian approximation. The paper additionally proves structural identifiability conditions for both closed (Schrödinger) and open (Lindblad) qutrit models via a Taylor-series approach, giving conditions on the prepared state under which parameters are uniquely identifiable. Simulation case studies using LLNL QuDIT-like parameters show rapid convergence for frequency parameters and slower convergence for decoherence parameters, motivating an iterative pulse-extension strategy that improves estimation of T1 and T2 while maintaining accurate ω and χ estimates under shot noise and parameter uncertainty.","Optimal experiment selection maximizes expected utility $U(\xi)=\mathbb{E}_{p(y|\xi)}[U(y|\xi)]$, with utility taken as expected KL divergence (mutual information): $U(\xi)=\int p(y|\xi)\int p(\theta|y,\xi)\log\frac{p(\theta|y,\xi)}{p(\theta)}\,d\theta\,dy$, solved via $\xi^*=\arg\max_{\xi\in\Xi}U(\xi)$. Posterior updates follow Bayes’ rule sequentially: $p(\theta|y_{1:n},\xi_{1:n})\propto p(\theta)\prod_{i=1}^n p(y_i|\theta,\xi_i)$, implemented with Monte Carlo particles and resampling (Liu–West). For finite shots, likelihood is multinomial $p(y|\theta,\xi,N)=\binom{N}{n_1,\dots,n_M}\prod_i p(y_i|\theta,\xi)^{n_i}$ using measurement probabilities $p(y|\rho)=\mathrm{Tr}(M_y\rho)$; a Gaussian approximation is also discussed for large N.","In simulations with QuDIT-like true parameters (e.g., $\omega\approx 4.0108$ GHz, $\chi\approx 127.8$ MHz, $T_1\approx 45\,\mu s$, $T_2\approx 24\,\mu s$), the inferred distributions for $\omega$ and $\chi$ converge much faster and to much smaller posterior variances than those for $T_1$ and $T_2$ under both exact and stochastic (shot-noise/parameter-uncertainty) experiments. Fixed pulse parameterizations show that shorter pulses better estimate frequency parameters, while longer pulses improve decoherence estimates; iterative pulse extension reduces mean error in decoherence parameters by roughly an order of magnitude versus fixed parameterization (in the reported comparisons). With iterative pulse extension, the model’s mean population prediction errors are reported on the order of $10^{-4}$ with variances around $10^{-3}$, and the approach achieves accurate calibration within 500 epochs (500 experiments) even with severe undersampling (e.g., 500 shots). Identifiability proofs provide explicit sufficient conditions on the prepared quantum state (nonzero real/imaginary parts of specific coherences) required to uniquely identify $\omega,\chi,T_1,T_2$ from single-basis population measurements.","The paper notes that utility-function optimization can be high-dimensional, non-convex, and multi-modal, motivating the use of non-gradient differential evolution, but implying potential optimization difficulty and computational cost. It also notes that the Gaussian likelihood requires estimating an error covariance $\Sigma$ that is not known a priori and must be estimated via Monte Carlo over parameters and random controls. The results are demonstrated primarily on synthetic/test problems, with experimental demonstration described as ongoing/future work.","The approach’s practical runtime/scalability is not fully characterized, especially as pulse segment count (decision variables) and parameter dimension grow (e.g., multi-qubit extensions), where particle filtering and repeated quantum simulations can become expensive. Performance may depend strongly on the chosen pulse parameterization bounds and the differential evolution hyperparameters; without careful tuning, the optimizer may still miss globally informative experiments. The identifiability conditions are derived locally around $t_0$ via low-order Taylor expansions and assume idealized measurement settings (projective z-basis populations and model correctness), so robustness to model mismatch, calibration drifts, and correlated noise processes beyond the Lindblad form is not fully assessed. Comparisons against alternative Bayesian optimal design optimizers (e.g., gradient-based, Bayesian optimization) and against non-adaptive baselines beyond qualitative discussion are limited.",The authors state ongoing and future work includes experimentally demonstrating the technique on multi-qubit systems.,"Extending the framework to explicitly handle model discrepancy (e.g., unknown additional Hamiltonian terms or non-Markovian noise) via hierarchical Bayesian models or adaptive model selection would strengthen real-hardware applicability. Developing more sample-efficient design surrogates (e.g., emulators for utility/likelihood, Bayesian optimization over controls) could reduce the computational cost of repeated utility maximization. Providing open-source implementations and reproducible benchmarks (including real-device datasets) would facilitate adoption and enable fair comparison against other adaptive characterization protocols. Incorporating constraints from hardware pulse-shaping, bandwidth limits, and calibration pipelines (Phase I/Phase II style self-starting variants) would improve deployability on quantum control stacks.",2312.10233v2,https://arxiv.org/pdf/2312.10233v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:19:03Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,A-optimal|Other,Variable/General (design variables are electrode positions parameterized by 2M angles; examples use M=12 electrodes → 24 position parameters),Healthcare/medical,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops Bayesian optimal experimental design methods for choosing electrode positions in head electrical impedance tomography (EIT) to maximize sensitivity to conductivity changes relevant for stroke monitoring. Using a linearized smoothened complete electrode model, Gaussian priors, and additive Gaussian noise, the Bayesian design objective reduces to minimizing a weighted trace of the posterior covariance (Bayesian A-optimality) with respect to electrode positions. The authors derive and implement efficient formulas for gradients of the A-optimality objective by computing derivatives of the forward model with respect to electrode positions, including a computationally favorable mixed derivative expression. Two algorithms are studied: (i) an offline A-optimal electrode-position optimizer given a Gaussian prior and a region of interest (ROI), and (ii) an adaptive approach that first reconstructs a conductivity anomaly using a TV prior with lagged diffusivity and sequential linearizations, then uses the resulting Gaussian covariance approximation as a prior for re-optimizing electrode positions. Numerical experiments on a 3D three-layer head FEM model (with M=12 electrodes) show electrodes move toward the ROI/stroke region and report notable reductions in the A-optimality target (expected L2 reconstruction error proxy) compared with initial configurations.","Bayesian design is posed as maximizing expected utility; under quadratic loss this becomes A-optimality, minimizing the trace of posterior covariance: $\Psi_A(d)=\mathrm{tr}(A\,\Gamma_{\text{post}}(d)A^\top)$. For the linear-Gaussian model $Y=J(d)W+E$ with $W\sim\mathcal N(0,\Gamma_{\text{prior}})$ and $E\sim\mathcal N(0,\Gamma_{\text{noise}})$, the posterior covariance is $\Gamma_{\text{post}}=\Gamma_{\text{prior}}-\Gamma_{\text{prior}}J^\top(J\Gamma_{\text{prior}}J^\top+\Gamma_{\text{noise}})^{-1}J\Gamma_{\text{prior}}$. Electrode positions are optimized via gradient descent with Armijo line search, using derivatives of the Jacobian $J(d)$ w.r.t. electrode-angle parameters derived from mixed derivatives of the (linearized) complete electrode model.","With a Gaussian prior and symmetric ROI, optimizing from a symmetric initial configuration reduced $\tilde\Psi_A=\Psi_A^{1/2}$ by 3.67% (and 3.14–3.80% depending on which electrode feeds current, without re-optimizing). For an ROI corresponding to the right posterior brain quadrant, starting from a symmetric initial configuration reduced $\tilde\Psi_A$ by 22.82%; starting from an ROI-concentrated initial configuration reduced $\tilde\Psi_A$ by 12.34%, and achieved a 1.30% lower minimized value than the symmetric-start solution. In TV-prior inclusion experiments, the adaptive scheme produced priors with high uncertainty near reconstructed anomaly boundaries and the subsequent A-optimal designs clustered electrodes near the inferred stroke region; iterating reconstruction→design further tightened electrode clustering around the stroke location (qualitative improvement shown in figures).",None stated.,"The design optimality is local: the A-optimality objective is minimized with gradient descent and can get trapped in local minima (acknowledged indirectly via clustering/path blocking), and results depend on initialization and electrode-feeding choice. The approach relies heavily on linearization (and on Gaussian approximations for the TV-posterior via lagged diffusivity), so performance for larger conductivity changes, strong model mismatch, or uncertain contact impedances may differ from reported results. Evaluation is primarily simulation on a single head anatomy with fixed noise/covariance assumptions and no external validation on real EIT data.","They propose future research on simultaneous optimization of electrode positions and current patterns, studying the effect of patient-specific anatomy on optimal positions, analyzing the effect of the feeding electrode more carefully, and comparing the gradient-based method with sparsification approaches for electrode selection.","Develop constraints/regularization in the design optimization to prevent impractical electrode clustering and enforce minimum separation or placement feasibility on real heads. Extend the design to account for uncertainty in contact conductances/impedances and for model mismatch (e.g., unknown skull conductivity, anisotropy, or electrode misplacement) using robust or Bayesian hierarchical design criteria. Provide open-source implementation and benchmark against alternative criteria (e.g., Bayesian D-optimal/KL utility) and against non-gradient global or mixed-integer optimization methods under realistic clinical constraints.",2312.10383v1,https://arxiv.org/pdf/2312.10383v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:19:47Z
TRUE,Response surface|Optimal design|Other,Parameter estimation|Prediction|Other,D-optimal|I-optimal (IV-optimal),"K = 1, 2, 3 factors (second-order response surface scenarios; 21 cases with varying N runs)",Theoretical/simulation only,Simulation study|Other,TRUE,Julia|Other,Not provided,NA,"The paper benchmarks how the “greediness” level of exchange algorithms affects their ability to generate exact optimal response-surface designs. It compares element-wise coordinate exchange (most greedy) versus row-wise exchange (medium greedy), each run many times from random starts, and contrasts these with previously published particle swarm optimization (PSO; least greedy) results across 21 small-exact second-order RSM scenarios with factors K=1–3 and varying run sizes N. Performance is evaluated under D-optimality (information/parameter-estimation focus) and I-optimality (average prediction-variance focus) using estimated success probabilities (e.g., achieving ≥95% efficiency and matching the best-known optimal design). The main finding is that element-wise and row-wise exchange have essentially indistinguishable efficacy, while PSO shows modestly better performance for D-optimal designs and for most I-optimal cases under the chosen parametrization. The authors conclude that the greediness dimension warrants broader study over more models and criteria.","The design is an exact N-run matrix $X\in[-1,1]^{N\times K}$ with second-order RSM model matrix $F(X)$ and information matrix $M(X)=F'F$. The D-criterion is defined as $D(X)=\lvert(F'F)^{-1}\rvert$ with $X_D^*=\arg\min_{X\in\mathcal X^N} D(X)$. The scaled prediction variance is $\mathrm{SPV}(x'\mid X)=N\,f'(x')(F'F)^{-1}f(x')$, and the I-criterion is $I(X)=\frac{1}{V}\int_{\mathcal X}\mathrm{SPV}(x'\mid X)\,dx' = \frac{N}{V}\,\mathrm{tr}\{(F'F)^{-1}\int_{\mathcal X} f(x')f'(x')\,dx'\}$, with $X_I^*=\arg\min I(X)$.","Across 21 benchmark scenarios, the element-wise (fully greedy) and row-wise (less greedy) exchange implementations showed essentially no difference in estimated probabilities of generating (i) a ≥95% efficient design and (ii) the best-known optimal design, under both D- and I-optimality. PSO exhibited higher success probabilities for D-optimal designs (most noticeably for K=1 and K=2) and a small edge for many I-optimal cases, but the improvement was not large under the study’s PSO parametrization and was less apparent for K=3. The study also reports that all algorithms performed similarly well for I-optimal designs, suggesting fewer problematic local optima for I than for D in these small-exact settings.","The authors note that conclusions are based on a specific benchmark suite (21 small-exact second-order response-surface scenarios) and only two criteria (D and I), so broader benchmarking is needed. They also indicate that PSO’s advantage may depend on parametrization (e.g., stronger PSO settings might show a clearer benefit, especially for K=3).","PSO results are taken from prior published benchmarking (140 runs) while the exchange algorithms are run 10,000 times in this paper, which can complicate direct, apples-to-apples efficacy comparisons due to differing run budgets and potentially differing compute/time constraints. The study focuses on continuous factors in a standardized hypercube and second-order polynomial models; conclusions may not generalize to constrained regions, discrete factors, mixture constraints, generalized linear/nonlinear models, or correlated errors. Implementation details (e.g., stopping rules, restart strategies, optimizer tolerances, and random initialization distributions) can materially affect exchange-algorithm performance and may limit reproducibility without shared code.","They plan to expand the study to (1) a more comprehensive model set (including first-order, interaction, linear and non-linear models), (2) larger experiments with increased K, (3) a broader set of optimality criteria including A- and G-optimality, and (4) additional versions of CEXCH and other metaheuristics (specifically mentioning GA and SA).","A rigorous compute-budget–matched comparison (equal wall-clock time or equal objective evaluations) across CEXCH variants and PSO would clarify practical trade-offs. Developing and testing self-starting or adaptive restart strategies (including hybrid PSO+local exchange) could isolate when “greediness” helps versus hurts. Extending evaluation to robustness under model misspecification, non-normal/autocorrelated errors, and constrained/irregular design regions would strengthen practical guidance; releasing a reproducible codebase and standardized benchmark harness would also accelerate follow-on research.",2312.12645v1,https://arxiv.org/pdf/2312.12645v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:20:17Z
FALSE,NA,NA,Not applicable,Not specified,Other,Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper reports a research-through-design, research-in-the-wild study that develops and deploys a subtle, “seamless” digital sound installation in the Bakkehuset historic house museum (Copenhagen) to enrich visitor experience and convey immaterial cultural heritage (social atmosphere/presence). The design process includes co-creative workshops with museum staff, iterative prototyping, and an internal test leading to a redesign (short sound vignettes with silent pauses to fit visitor flows). Evaluation is qualitative, based on two rounds of visitor interviews (before installation: 25 visitors across 14 interviews; after installation: 36 visitors across 20 interviews) conducted immediately after visits and analyzed via transcription and coding. Findings indicate most visitors noticed the installation unprompted and described it as adding atmosphere and a sense of human presence, with very few negative responses. The work contributes design insights for hybrid museum experiences in preservation-sensitive historic environments, emphasizing open-ended sonic cues and integration with social/attention dynamics.",Not applicable,"Post-installation, 34 of 36 interviewed visitors expressed a positive view of the sound installation (only 2 expressed dislike). Many visitors reported that the sound created a perceived human presence and enhanced the atmosphere, sometimes initially being mistaken for real-time activity in the room, which drew them toward the living room. Iteration based on an internal test changed the audio from a continuous 15-minute loop to five 1–3 minute scenarios separated by 3–4 minute silent breaks (total ~30 minutes including silence), which the authors argue improved seamless integration with visitor movement and conversation.",None stated,"The evaluation is based on qualitative, post-visit interviews without quantitative outcome measures (e.g., dwell time, engagement metrics) or controlled comparisons, limiting causal attribution. Sampling may be subject to self-selection (some participants recruited via Facebook with free access) and is specific to a single museum and cultural context, which may limit generalizability. The paper provides limited detail on the coding scheme, inter-coder reliability, or analytic validation steps, making it harder to assess robustness of qualitative inferences.","The authors suggest it would be relevant to further study other “seamless” design approaches for historic house museums that challenge or alter perceptions of home, time, historic space, or lived life, rather than catering to conventional expectations of “visiting the past.”","Test the approach across multiple historic houses with different visitor profiles and constraints to assess transferability and boundary conditions. Add mixed-method evaluation (behavioral observation, interaction logs, dwell-time measures, or lightweight surveys) to triangulate interview findings and compare against alternative interventions (e.g., mobile audio guide, labels). Explore accessibility and inclusivity implications (e.g., hearing impairments, language differences) and develop implementation guidelines or open-source toolkits for museum deployment.",2312.13953v1,https://arxiv.org/pdf/2312.13953v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:20:34Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Model discrimination|Optimization|Cost reduction|Other,A-optimal|D-optimal|Other,"Variable/General (high-dimensional/infinite-dimensional parameters; examples with tens of thousands of parameters; sensors: e.g., choose 5 of 50, 10 of 100, 8 of 512)",Environmental monitoring|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2312.14810|https://doi.org/10.1007/978-3-642-04898-2_434|http://www.sciencedirect.com/science/article/pii/S0021999112004597|https://doi.org/10.1016/j.cma.2018.01.053|http://www.sciencedirect.com/science/article/pii/S0045782518300616|https://proceedings.neurips.cc/paper/2019/file/d55cbf210f175f4a37916eafe6c04f0d-Paper.pdf|http://iopscience.iop.org/10.1088/1361-6420/aa6d8e|https://doi.org/10.1016/j.cma.2017.08.016|https://arxiv.org/abs/1804.04301|https://doi.org/10.1016/j.jcp.2015.04.047,"The paper develops a computational framework for Bayesian optimal experimental design (OED) for PDE-constrained inverse problems, focusing on combinatorial sensor selection under high-dimensional/infinite-dimensional parameters. It uses derivative-informed dimension reduction and trains a derivative-informed neural operator (DINO) surrogate that approximates both the parameter-to-observable (PtO) map and its Jacobian, enabling fast evaluation of Laplace-approximation-based OED criteria. The framework provides efficient reduced formulations to compute the MAP point, approximate posterior covariance eigenvalues (via reduced eigenproblems), and three OED criteria: A-optimality (trace), D-optimality (log-det), and Laplace-based expected information gain (EIG). A modified swapping greedy algorithm is used for discrete sensor placement, and the authors provide approximation error analyses for the MAP, eigenvalues, and criteria. Numerical studies on 2D diffusion and 2D/3D nonlinear convection–diffusion–reaction PDEs show large speedups (reported up to 80× in 2D and 1148× in 3D, including offline+online costs) while maintaining high accuracy of optimality evaluations and selected sensor sets.","Sensors are selected via a binary design matrix \(\xi\in\{0,1\}^{d_s\times r_s}\) that chooses \(r_s\) sensors from \(d_s\) candidates, with observations \(y=F_\xi(m)+\epsilon\), \(F_\xi=\xi^T F\), and Gaussian noise \(\epsilon\sim\mathcal N(0,\Gamma_{\text{noise}})\). The Laplace-based MAP is \(m^{y,\xi}_{\text{MAP}}=\arg\min_m \tfrac12\|y-F_\xi(m)\|^2_{\Gamma^{-1}_{\text{noise}}}+\tfrac12\|m-m_{\text{prior}}\|^2_{C^{-1}_{\text{prior}}}\), with posterior covariance \(C^{y,\xi}_{\text{post}}=(H^{y,\xi}_{\text{misfit}}(m_{\text{MAP}})+C^{-1}_{\text{prior}})^{-1}\) and Gauss–Newton Hessian \(H_{\text{misfit}}\approx \nabla F_\xi(m_{\text{MAP}})^T\Gamma^{-1}_{\text{noise}}\nabla F_\xi(m_{\text{MAP}})\). OED criteria include A-optimality (trace of posterior covariance), D-optimality \(\mathbb E[\log\det(I+\tilde H_{\text{misfit}})]\), and Laplace EIG using \(\tfrac12\log\det(I+\tilde H)-\tfrac12\mathrm{tr}(H C_{\text{post}})+\tfrac12\|m_{\text{MAP}}-m_{\text{prior}}\|^2_{C^{-1}_{\text{prior}}}\). DINO replaces expensive PtO/Jacobian evaluations with a surrogate \(F_{NN}(m)=D_{\Psi_F}\circ\Phi_\theta\circ E_{\Psi_m}(m)\) trained with a combined value+Jacobian loss.","On PDE examples with hundreds of candidate sensors and parameters ranging from thousands to ~35,937, the DINO-enabled OED achieves large reported wall-clock speedups versus high-fidelity Bayesian OED: ~80× for a 2D nonlinear convection–diffusion–reaction case and ~1148× for a 3D case (including offline surrogate construction and online evaluation). Per-evaluation timings reported for 2D CDR were PtO/MAP/eigenpairs: FEM 0.8/5.1/0.7 s vs DINO 0.002/0.04/0.007 s; for 3D CDR: FEM 25.5/128.7/10.0 s vs DINO 0.003/0.04/0.02 s. The paper reports high agreement between DINO and FEM optimality computations with R^2 often > 0.99, and shows that including Jacobian information in training substantially reduces errors in Jacobians, MAP estimates, and optimality criteria compared to training without derivatives. The method successfully selects small sensor subsets (e.g., 5/50, 10/100, 8/512) that outperform random placements under A-, D-, and EIG criteria.",The method relies on Laplace and low-rank approximations: it is limited to settings where the Laplace approximation is accurate for the posterior and where the (misfit) Hessian exhibits low-rankness/fast spectral decay. The paper also notes reliance on intrinsic low-dimensional structure/fast spectral decay of the PtO map and misfit Hessian for scalability and accuracy of the reduced formulations.,"The approach is demonstrated largely on synthetic PDE testbeds with simulated noisy observations; broader real-world validation (e.g., field sensor deployments) is not shown. Performance may degrade for strongly non-Gaussian, multimodal, or highly nonlinear posteriors where MAP-based Laplace approximations are poor, and for problems without fast eigenvalue decay where reduced eigenproblems would need large ranks. Sensor selection is optimized with greedy/swapping heuristics, which can miss global optima in difficult combinatorial landscapes; sensitivity to initialization and stopping criteria may matter. Practical deployment would also require robust handling of model mismatch and correlated/structured noise estimation (\(\Gamma_{\text{noise}}\)) beyond assumed specifications.","The authors plan to address limitations of Laplace/low-rank posterior approximations by developing DINO surrogates coupled with variational approximations for potentially highly non-Gaussian posteriors, and by using nonlinear dimension reduction and hierarchical Hessian approximations. They also plan to extend the framework to sequential Bayesian OED with efficient construction of time-dependent DINO surrogates.","Provide open-source implementations and reproducible pipelines (data generation, DINO training, and OED optimization) to support adoption and benchmarking across PDE inverse problems. Extend the method to handle model discrepancy and misspecification (robust OED), including adaptive noise/covariance learning and non-independent observation models. Explore global or hybrid combinatorial optimizers (e.g., mixed-integer relaxations, submodular approximations, or parallel metaheuristics) to reduce dependence on greedy swapping. Investigate active-learning or adaptive sampling strategies for selecting training points (m) to minimize surrogate error specifically in regions most influential for OED (near MAPs and along influential directions).",2312.14810v4,https://arxiv.org/pdf/2312.14810v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:21:43Z
TRUE,Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Model discrimination|Optimization|Prediction|Other,Not applicable,Variable/General (examples include 1 continuous parameter for NV-center DC magnetometry; 2 parameters—1 discrete sign + 1 continuous amplitude—for agnostic Dolinar receiver; more tasks in cited companion work),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://gitlab.com/federico.belliardo/qsensoropt|https://pypi.org/project/qsensoropt/,"The paper proposes a general framework for high-performance Bayesian experimental design in quantum metrology by combining model-aware reinforcement learning with Bayesian inference via particle filtering. The approach optimizes both non-adaptive and adaptive measurement/control policies (implemented as neural networks or fixed control sequences) despite non-differentiable elements in the estimation loop (measurement sampling and particle-filter resampling), by using policy-gradient-style likelihood terms, importance sampling/soft resampling, and differentiable-resampling corrections. It is demonstrated on simulated applications including NV-center DC magnetometry (choosing evolution times in Ramsey experiments to estimate a continuous precession frequency) and an agnostic Dolinar receiver (adaptive beam-splitter settings to discriminate between coherent states with an unknown nuisance amplitude). Across these tasks, the learned strategies outperform common heuristics and reported model-free RL baselines, and can also be trained to optimize frequentist objectives such as minimizing Cramér–Rao bounds via Fisher information. The authors provide an open-source Python/TensorFlow library (qsensoropt) with examples and data to support reuse across multiple sensing platforms.","The adaptive design is defined as a policy mapping controls to posterior summaries, e.g., $x_{t+1}=F_{\lambda}\{P(\theta\mid x_t,y_t),y_t,R_t\}$, with outcomes sampled from the known model $y_{t+1}\sim p(y_{t+1}\mid x_{t+1},\theta)$ (or $p(y_{t+1}\mid x_{t+1},y_t,\theta)$ under weak measurement) and Bayes updates $P(\theta\mid y_{t+1},x_{t+1})\propto p(y_{t+1}\mid x_{t+1},\theta)P(\theta\mid x_t,y_t)$. The posterior is approximated by a particle filter $P(\theta)\approx\sum_{j=1}^N w_j\,\delta(\theta-\theta_j)$, and training minimizes expected estimation/discrimination loss, e.g. MSE $\ell(\hat\theta,\theta)=\mathrm{tr}[G(\hat\theta-\theta)(\hat\theta-\theta)^\top]$ or classification loss $\ell=1-\delta(\hat\theta,\theta)$, using a likelihood-augmented surrogate objective to obtain unbiased gradients through stochastic sampling steps.","In simulated NV-center DC magnetometry with a uniform prior $\omega\in(0,1)$ MHz and varying coherence times $T_2^*$, the model-aware RL optimized adaptive and/or non-adaptive schedules yield lower MSE versus standard heuristics (PGH, $\sigma^{-1}$ variants) and reported model-free RL baselines, often with non-adaptive strategies matching or surpassing adaptive ones for long coherence times. For the agnostic Dolinar receiver with prior $\alpha\sim\mathrm{Unif}[0.05,1.50]$ and uniform sign, adaptive NN-controlled reflectivities reduce classification error probability compared with prior non-ML designs and approach finite-$n$ Helstrom bounds for small $\alpha\lesssim 1$ (notably for $n=4$ reference states). The authors report using particle counts such as $N=480$–$1536$ for NV-center plots and $N=512$ for Dolinar simulations, with training times on the order of hours on an NVIDIA Tesla GPU. They further state the approach can also directly optimize Fisher-information/Cramér–Rao objectives beyond Bayesian estimation.","The authors state the method assumes a pre-characterized, accurate model of the system (“model-aware” rather than learning the model online), and that jointly learning the model and policy is beyond the paper’s scope. They also note that providing the full posterior (all particles/weights) to the neural network is often impractical for continuous parameters, so they typically feed only summary statistics (e.g., mean/covariance). Additionally, they remark that extending to other quantum-information tasks is challenged by rapidly increasing quantum-state-space dimensionality that must be simulated.","Performance claims are largely based on simulations; robustness to model misspecification (common in real quantum sensors) is not systematically quantified, which is critical for “digital twin” approaches. Comparisons may depend on selected baselines and hyperparameter choices; broader benchmarking across alternative Bayesian design methods (e.g., dynamic programming approximations, other amortized BED methods) and across wider priors/noise regimes could change conclusions. Practical deployment constraints (latency, calibration drift, hardware limits on control resolution) and their effect on adaptive policies are not deeply evaluated, especially for real-time Bayesian updates on embedded hardware.","They suggest extending inputs to the agent beyond low-order posterior moments (e.g., higher moments, or even the full particle set when feasible) to learn more detailed optimal strategies. They also note that applying the model-aware RL optimization paradigm to other quantum-information tasks (e.g., quantum error correction, entanglement distillation) would require engineering suitable loss functions. The paper also frames online/combined model-learning plus policy learning as an important direction but outside current scope.","Developing principled robustness methods (e.g., Bayesian model uncertainty, domain randomization, or distributionally robust objectives) would help ensure learned designs remain effective under calibration errors and drift. Providing standardized benchmarks and ablation studies (resampling method choices, likelihood-term estimators, posterior-summary features) plus real-hardware demonstrations would strengthen evidence of practical impact. Extending the framework to constrained/quantized control settings and to partially observable or non-i.i.d. (time-correlated) measurement noise models would broaden applicability in experimental quantum metrology.",2312.16985v3,https://arxiv.org/pdf/2312.16985v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:22:30Z
TRUE,Sequential/adaptive|Other,Screening|Parameter estimation|Prediction|Other,Other,"Variable/General (contexts/covariates are d-dimensional; number of arms K; simulations use K=4 and d∈{2,5})",Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies adaptive experimental design for contextual fixed-budget best-arm identification (BAI) aimed at learning a context-dependent treatment recommendation policy. It derives minimax lower bounds on worst-case expected simple regret (policy regret) in a location-shift bandit model, with the leading constant characterized by conditional outcome variances and policy-class complexity (Natarajan/VC dimension). It proposes the PLAS strategy, which adaptively samples treatment arms using an estimated variance-based target assignment ratio and then learns a recommendation policy by maximizing an empirical policy-value objective using an augmented inverse probability weighted (AIPW) estimator. The authors provide regret upper bounds for PLAS and show asymptotic minimax rate-optimality in that the leading factor matches the derived lower bound as the budget T grows. Simulation studies (K=4, d∈{2,5}, T up to 25,000; 50 trials) show PLAS improves the learned policy value compared with uniform sampling plus policy learning, with an oracle-variance version as a reference.","Target assignment ratios: for K=2, $w^*(a\mid x)=\sigma_a(x)/(\sigma_1(x)+\sigma_2(x))$; for K\ge3, $w^*(a\mid x)=\sigma_a(x)^2/\sum_{b=1}^K\sigma_b(x)^2$. Policy learning uses the AIPW-based empirical value $\hat Q_T(\pi)=\frac1T\sum_{t=1}^T\sum_{a=1}^K\pi(a\mid X_t)\hat\Gamma_t^a$, where $\hat\Gamma_t^a=\frac{\mathbb{1}[A_t=a](c_T(Y_t)-\hat\mu_t^a(X_t))}{\hat w_t(a\mid X_t)}+\hat\mu_t^a(X_t)$ with outcome clipping $c_T(\cdot)$. The learned policy is $\hat\pi_T^{\mathrm{PLAS}}\in\arg\max_{\pi\in\Pi}\hat Q_T(\pi)$ and recommendations draw $\hat a_T(x)\sim\hat\pi_T^{\mathrm{PLAS}}(\cdot\mid x)$.","Lower bounds: there exists a context distribution $\zeta$ such that for any null-consistent strategy with policy class Natarajan dimension $M$, $\sup_{P\in\mathcal P_\zeta}\sqrt{T}\,\mathbb E[R_T(P)]\ge \frac18\,\mathbb E_{X\sim\zeta}\big[\sqrt{M\sum_a\sigma_a(X)^2}\big]+o(1)$ (K\ge3), and for K=2 a tighter bound $\ge \frac18\,\mathbb E_{X\sim\zeta}[\sqrt{M(\sigma_1(X)+\sigma_2(X))^2}]+o(1)$. Upper bound for PLAS: $\sqrt{T}\,\mathbb E[R_T(P)]\le (108.8\,\kappa(\Pi)+870.4)\,\mathbb E_{X\sim\zeta}[\sqrt{\sum_a\sigma_a(X)^2}]+o(1)$ (K\ge3) with an analogous K=2 expression, implying matching leading variance dependence under the proposed variance-based allocation. Simulations (K=4; 50 trials) report higher learned policy value for PLAS than uniform sampling across (d,T) settings (2,10000), (5,10000), and (5,25000).","The authors note that they do not employ non-i.i.d. complexity measures (e.g., sequential Rademacher complexity for martingales) and instead rely on an asymptotic equivalence technique with sample splitting/double machine learning style arguments to leverage i.i.d. policy-learning bounds. They state as future work the need to tighten both the lower and upper bounds and highlight an open issue: for K\ge3 their optimal variance-proportional allocation differs from sampling rules suggested as optimal in some non-contextual fixed-budget BAI literature, and reconciling this gap remains open.","The theoretical guarantees rely on a location-shift/sub-Gaussian outcome model with context-dependent but exogenously fixed variances and assume strong uniform almost-sure convergence of nuisance estimators (Assumption 5.1), which may be hard to satisfy in finite samples and high-dimensional contexts. The method presumes accurate estimation of conditional second moments/variances under adaptive sampling; practical performance may degrade if variance estimation is unstable or if positivity (bounded away from 0) is violated despite truncation. Empirical evaluation is limited to synthetic simulations with a simple piecewise-constant mean structure; no real-world or semi-synthetic benchmark is provided and comparisons are mainly against uniform sampling (missing stronger contextual bandit baselines for pure exploration/policy learning). Implementation details and computational cost of the nonparametric estimators and policy optimization are not specified, which affects reproducibility and applicability.","They propose tightening both lower and upper regret bounds and explicitly flag an open problem: while K=2 yields an optimal allocation proportional to standard deviations, for K\ge3 their variance-proportional allocation differs from optimal sampling rules indicated in prior non-contextual fixed-budget BAI works; bridging this gap is left for future research.","Develop finite-sample tuning guidance and empirically robust variance/second-moment estimators tailored to adaptive data collection (including cross-fitting choices and stabilization) and analyze sensitivity to misspecification. Extend the framework beyond location-shift/sub-Gaussian rewards to handle heavy tails, heteroskedasticity not fixed exogenously, and temporally dependent contexts/outcomes. Add broader experimental benchmarks and real-world case studies (e.g., A/B testing with covariates) and compare against modern contextual pure-exploration bandit algorithms and offline/online policy learning baselines. Provide open-source reference implementations to support adoption and reproducibility.",2401.03756v4,https://arxiv.org/pdf/2401.03756v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:23:12Z
FALSE,NA,NA,Not applicable,Not specified,Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper introduces Electrical Capacitance Volume Sensing (ECVS), a non-imaging modality derived from Electrical Capacitance Volume Tomography (ECVT), for propellant mass gauging in microgravity-relevant spherical tanks. It compares two spherical electrode layouts based on platonic-solid tessellations—an octahedron (8 plates, 28 channels) and a dodecahedron (12 plates, 66 channels)—to reduce sensitivity to fluid position and orientation. The designs are evaluated via COMSOL Multiphysics simulations (sensitivity matrices, dynamic range) and benchtop experiments using mineral oil fills in multiple orientations (upright stratified fill, 45° tilt) plus a moving internal oil-filled ball test for positional stability. The authors propose new performance metrics (Spatial Sensitivity Quotient and Spatial Sensitivity-to-Noise Ratio, SSNR) and show that the dodecahedron design yields substantially more linear and rotation-stable mass/volume-fraction estimates, with markedly lower rotation-induced error than the octahedron across averaging (ECVS) and imaging (ECVT) approaches. Practically, the work provides sensor-geometry guidance for improving microgravity mass gauging robustness, highlighting rotational symmetry and non-adjacent electrode channel richness as key design drivers.","Dynamic range per channel is defined as $DR_n = C_{n,\text{full}} - C_{n,\text{empty}}$. A sensitivity matrix is computed per channel/voxel, $S_n(i)=\iiint (\vec{\zeta}_{n,\text{transmit}}\cdot \vec{\zeta}_{n,\text{receive}})\,dv$, and summarized by the proposed Spatial Sensitivity Quotient $\sigma_n = \frac{\sum_{i=1}^{V} |S_n(i)|/V}{\max_i S_n(i)-\min_i S_n(i)}$. The proposed Spatial Sensitivity-to-Noise Ratio is $SSNR_n = \frac{DR_n\,\sigma_n}{\mathrm{STD}(C_{n,\text{full}})}$ (contrasted with traditional $SNR_n = C_{n,\text{full}}/\mathrm{STD}(C_{n,\text{full}})$).","In experiments, rotation-induced (45°) volume-fraction error using the averaging/ECVS approach is reported as max 17.02% (octahedron) vs 5.94% (dodecahedron), with mean 8.37% vs 1.96%, respectively. Using ECVT imaging with Linear Back Projection (LBP), max/mean errors are 19.44%/7.90% (octahedron) vs 4.22%/1.59% (dodecahedron); with NNMOIRT, 48.66%/10.11% vs 6.14%/2.40%. A ball-position stability test shows the dodecahedron’s maximum error about 0.75% (reported as maximum effect on the overall signal). Reported overall-average SSNR and DR (Table IV) are much higher for the dodecahedron (SSNR 36.970; DR 3162.33) than the octahedron (SSNR 0.958; DR 137.56), though the authors note these depend on electronics/gain and construction.","The authors note the mineral-oil simulant has a higher dielectric constant than the cryogenic propellants of interest, producing a higher signal than true propellants and not fully representing the hardest sensing conditions. They state that temperature effects were not accounted for due to stable lab conditions, but temperature variations in space will change dielectric constant and must be compensated. They also note that their rotation-error estimate is based on only two fill profiles (stratified upright and 45°) and that additional extreme fill configurations (annular, central ball, multiple angles) would better bound error but are difficult to realize in Earth gravity.","The experiments are conducted in 1g with inferred microgravity relevance; performance under true microgravity flow regimes (annular wetting, dispersed two-phase) is not validated and may differ materially. The comparative DR/SSNR results are confounded by differing construction details and electronic gain settings between prototypes (acknowledged partially), which weakens attributing all improvements solely to geometry. No uncertainty analysis is provided for repeatability across rebuilds, long-term drift, or calibration transfer; the disassembly/reassembly sensitivity suggests robustness issues that may matter operationally. The imaging comparisons (LBP, NNMOIRT) are limited to chosen hyperparameters (e.g., NNMOIRT alpha=5) and voxel grid (20×20×20), so conclusions about imaging vs non-imaging may not hold under optimized reconstruction settings or alternative regularization/priors.","They suggest optimizing machine learning or ECVT reconstruction techniques for microgravity fill cases to further improve accuracy, and note that annular microgravity configurations may be better handled with imaging or a well-trained ML model that compensates for fluid position. They also state that temperature-induced dielectric changes in space applications will need to be compensated in the instrument. They imply that increasing plate count would improve image resolution, though it may not improve mass gauging due to SSNR trade-offs, indicating further study of this trade-off.","Validate the sensor designs in relevant reduced-gravity environments (drop tower, parabolic flight, suborbital) and with representative cryogens or low-permittivity analogs to quantify performance in annular/dispersed regimes. Develop a self-starting or in-situ calibration method robust to assembly tolerances, cable motion, and long-term drift, including temperature compensation models. Explore formal multi-objective optimization of electrode geometry (plate shapes/gaps/grounding) for mass gauging metrics (e.g., minimizing rotation sensitivity subject to DR/SSNR constraints) rather than comparing only two platonic layouts. Release an open implementation (simulation workflow + reconstruction/ECVS processing) and benchmark against additional reconstruction/ML baselines with tuned hyperparameters to strengthen reproducibility and comparisons.",2401.04864v2,https://arxiv.org/pdf/2401.04864v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:23:51Z
TRUE,Optimal design|Other,Optimization|Prediction|Cost reduction|Other,D-optimal,Variable/General (rates allocated per source/type/path; learner model dimension d=100 in experiments),Network/cybersecurity|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/neu-spiral/DistributedNetworkLearning,"The paper formulates and solves an experimental design problem over a communication network (“experimental design networks”) where routing and rate allocations from multiple data sources to heterogeneous learners are chosen to maximize aggregate learning utility under network capacity constraints. Learners perform Bayesian linear regression, and utility is based on an expected D-optimality criterion (log-determinant of the posterior/MAP error covariance), yielding a continuous DR-submodular (but non-concave) objective. The authors extend prior work by enabling multicast transmissions, introducing non-differentiable max-type capacity constraints, and proposing centralized and distributed algorithms (distributed Frank–Wolfe and distributed projected gradient ascent) with a (1−1/e) approximation guarantee via DR-submodularity and constraint relaxations. They develop a polynomial-time unbiased gradient estimator under Poisson arrivals and Gaussian feature sources (with heteroskedastic noise), using truncation and Monte Carlo sampling. Simulations on synthetic and real backbone network topologies show the proposed distributed methods achieve near-centralized performance and outperform throughput- and fairness-based baselines in aggregate utility and estimation error.","The learning/DOE objective is D-optimality: for learner ℓ, $G_\ell(X_\ell,n_\ell)=\log\det\big(\mathrm{cov}(\hat\beta^{\mathrm{MAP}}_\ell-\beta_{t_\ell})\big)$ where $\mathrm{cov}(\hat\beta^{\mathrm{MAP}}_\ell-\beta_{t_\ell})=\big(X_\ell^\top \tilde\Sigma_\ell^{-1}X_\ell+(\Sigma^\ell_0)^{-1}\big)^{-1}$. The network design variable is the path rate allocation $\lambda=[\lambda^p_{s,t}]$, constrained by multicast link capacities $\sum_{s,t}\max_{p\in P_{s,t}:e\in p}\lambda^p_{s,t}\le \mu_e$ and source limits $\max_{p\in P_{s,t}}\lambda^p_{s,t}\le \lambda_{s,t}$, and the aggregate utility maximized is $U(\lambda)=\sum_{\ell\in L}(U_\ell(\lambda_\ell)-U_\ell(0))$ with $U_\ell$ the expectation of $G_\ell$ under Poisson arrivals and Gaussian features.","The paper proves the aggregate expected D-optimality utility $U(\lambda)$ is monotone and continuous DR-submodular in the rate allocation variables, enabling approximation guarantees for non-convex maximization. Using a Frank–Wolfe variant with a new Monte Carlo gradient estimator (truncating the Poisson tail and sampling arrivals/features), the centralized and distributed methods achieve a $(1-1/e)$-approximation (up to estimation error terms) under the relaxed multicast constraints and for sufficiently large $\theta$ in the $\ell_\theta$ approximation of the max constraint. Empirically, across multiple synthetic and backbone topologies, the proposed distributed Frank–Wolfe (DFW) and distributed PGA (DPGA) achieve higher aggregate utility and lower estimation error than distributed/centralized throughput (MaxTP) and α-fairness (MaxFair) baselines, while maintaining low constraint infeasibility (reported as typically < 0.1). The distributed variants track their centralized counterparts closely in performance in the experiments.",The authors note a key limitation of their distributed algorithm is that it requires synchronization (a synchronous message-exchange model). They suggest that this may not reflect the reality of large networks and motivates extending the approach to asynchronous settings.,"The distributed method enforces feasibility only approximately via soft constraints and an $\ell_\theta$ (finite-$\theta$) relaxation of multicast max constraints, so practical solutions may violate true hard capacity constraints unless carefully tuned (and performance may depend on $\theta$). The learning model and guarantees rely on assumptions such as steady-state Poisson arrivals, independence, and Bayesian linear regression with Gaussian features/noise; robustness to non-Poisson traffic, autocorrelation, heavy tails, or model mismatch is unclear. The gradient estimation relies on Monte Carlo sampling (with parameters like $N_1,N_2,n'$), which can be computationally expensive and may limit real-time deployment or scale to very large networks without variance-reduction or more efficient estimators.","They propose extending the distributed algorithm from a synchronous to an asynchronous setting (e.g., allowing sources/links to use outdated gradients). They also suggest estimating gradients via shadow prices instead of sampling. Additionally, they highlight studying how a model trained by one learner can benefit other training tasks in experimental design networks as an interesting direction.","Develop self-starting or online versions that learn unknown traffic/noise parameters and maintain feasibility under changing network conditions. Extend the framework beyond Bayesian linear regression to generalized linear models or deep models while retaining tractable design objectives (e.g., approximate information gain) and scalable distributed optimization. Add stronger empirical validation on real traffic traces and implement variance-reduction/closed-form approximations to reduce Monte Carlo cost and improve deployment practicality.",2401.04996v1,https://arxiv.org/pdf/2401.04996v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:24:29Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Screening|Cost reduction|Other,Minimax/Maximin|Space-filling|Other,Variable/General (selects k prompts from a pool of N=99K; no fixed factor count),Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://link.springer.com/book/10.1007/978-3-7908-2151-2|https://github.com/tatsu-lab/alpaca_eval,"The paper proposes an experimental-design (one-shot, non-adaptive) framework for selecting a subset of prompts to label for label-efficient supervised finetuning (SFT) of large language models, aiming to reduce annotation cost while maintaining downstream performance. It evaluates several design heuristics—uncertainty-based scores (mean entropy, least confidence, mean margin) plus a novel min-margin token uncertainty score, and diversity-based methods (k-center coresets and submodular facility location with cosine or RBF kernels)—computed from an initial pretrained model before any labels are collected. The selected prompt subset is then annotated (human or strong LLM) and used to finetune LLaMA-2 7B with LoRA, with evaluation on MMLU, BBH-CoT, and GPT-4-turbo judged AlpacaEval comparisons. Empirically, facility location (especially RBF with tuned kernel width, e.g., γ=0.002) and min-margin provide consistent gains over random sampling across budgets, with the paper reporting ~50% annotation cost savings to match performance of larger random subsets on generative and classification benchmarks. The work positions experimental design as a computationally cheaper alternative to active learning for LLM SFT because it avoids repeated retraining/inference over the pool.","Prompt selection is formulated as choosing a subset S of size k from a pool X to maximize an informativeness objective computed from the initial model. Uncertainty-only selection uses a top-k criterion based on an uncertainty score U(x) (Eq. 1), with definitions including mean token entropy, negative sequence confidence, mean margin, and the proposed min-margin: $U_{\min\,margin}(x)= -\min_{p\in g_p(x)}(\beta_1(p)-\beta_2(p))$. Diversity selection includes k-center minimax covering in embedding space (Eq. 2): $\min_{|S|=k}\max_{i\in X}\min_{j\in S}\|f(x_i)-f(x_j)\|$, and submodular facility location maximization (Eq. 3): $\max_{|S|=k}\sum_{i\in X}\max_{j\in S} w_{ij}$ with $w_{ij}=\exp(-\|f(x_i)-f(x_j)\|^2/\gamma)$ (RBF) or cosine-based similarity. A mixed objective (Eq. 4) adds a concave transform of summed uncertainty to facility location: $\sum_i \max_{j\in S} w_{ij} + \log(1+\sum_{x\in S} U_{\min\,margin}(x))$.","From a 99K FLAN-V2 pool, facility location with an RBF kernel (γ=0.002) improves over random sampling across budgets; at k=45K it reports MMLU 47.63 (±0.24) vs random 45.99 (±0.37) and BBH-CoT 41.30 (±0.60) vs random 39.44 (±0.52). The paper states that compared to training with 90K random prompts (MMLU 47.76 ±0.57; BBH-CoT 40.49 ±0.30), the proposed design methods can save ~50% annotation cost while reaching similar generalization performance. In GPT-4-turbo judging on AlpacaEval, several 45K-designed subsets achieve comparable or better win rates than a 90K-random baseline, with facility location (γ=0.002) showing the strongest gains in the plotted comparison. A kernel-width sensitivity ablation at 45K shows consistently better-than-random performance for γ in {0.001,0.002,0.003,0.004}, with best BBH-CoT reported at γ=0.003 (41.99 ±0.73).",The authors note they only experiment with the 7B LLaMA-2 model due to compute constraints. They assume curated datasets with accurate responses and do not study the impact of inaccurate/noisy responses that can occur with crowd-worker annotation. They also evaluate only post-SFT and do not consider downstream effects of applying RLHF after SFT.,"The approach depends on representations and uncertainty derived from a fixed initial model; if the initial model is miscalibrated or biased, one-shot selection may systematically miss important regions of the prompt/task space. Evaluation is tied to a specific pool (a processed 99K FLAN V2 subset) and a limited evaluation protocol (e.g., 5-shot MMLU, 20% BBH-CoT subset), which may not generalize to other instruction pools, domains, or fully held-out task distributions. Practical deployment would require efficient embedding/score computation and scalable submodular/k-center optimization on very large pools; the paper reports compute but does not provide implementation details or shared code to validate scalability and reproducibility.",They suggest devising new methods within the proposed experimental-design framework to further improve label efficiency. They also propose improving utilization of unlabeled samples beyond only finetuning on the annotated subset.,"Extending the framework to handle noisy/variable-quality annotations (e.g., crowd-worker disagreement, LLM label noise) with robust design objectives would improve practical applicability. Developing self-starting or sequential variants that retain low overhead (e.g., a small number of redesign rounds) could bridge the gap between one-shot design and active learning. Broadening evaluation to larger LLMs, more diverse instruction pools, multilingual settings, and additional safety/alignment metrics (beyond MMLU/BBH/AlpacaEval) would clarify when these design heuristics generalize and how they affect bias and coverage.",2401.06692v3,https://arxiv.org/pdf/2401.06692v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:25:07Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Optimization|Prediction,A-optimal|D-optimal|E-optimal|Other,"Variable/General (design dimension $n_e$; examples include 1D design, 2D boundary-condition parameters, and selecting 4 observation times; inference dimension examples include 6 and 16 parameters)",Healthcare/medical|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Public repository (GitHub/GitLab),https://github.com/DeepTransport/deep-tensor|https://github.com/fastfins/fastfins.m,"The paper proposes a tractable framework for Bayesian optimal experimental design (BOED) in nonlinear inverse problems with intractable (generally non-Gaussian) posteriors by learning a transport-map surrogate for the joint law of design variables, observations, and unknown parameters. The key method constructs an order-preserving Knothe–Rosenblatt transport map using functional tensor trains (FTTs) and a deep composition strategy (DIRT) to approximate the joint density, enabling efficient sampling/evaluation of the conditional distributions needed to compute expected utilities. The approach supports multiple Bayesian design criteria (explicitly A- and D-optimality, and generally A/D/E-type criteria) and extends naturally to greedy sequential (adaptive) experimental design via posterior-preconditioned transport maps that recycle information across stages. Numerical studies demonstrate competitiveness versus nested Monte Carlo for expected information gain on a toy nonlinear model, and showcase sequential/batch BOED in two application-motivated problems: selecting observation times in an SEIR disease model (ODE) and choosing boundary conditions for permeability inversion in an elliptic PDE (groundwater/aquifer setting). The work emphasizes black-box applicability (only needing an unnormalized density), efficient conditional sampling, and feasibility in small-to-moderate data and parameter regimes.","BOED objective: $e^*\in\arg\max_{e\in E}\,\mathbb{E}_{\pi(d\mid e)}[\psi(e,d)]$. A-opt utility: $\psi_A(e,d)=-\mathrm{trace}\,C_{m\mid e,d}$ and D-opt utility (EIG): $\psi_D(e,d)=\mathrm{KL}(\pi(m\mid e,d)\|\pi(m))=\mathbb{E}_{\pi(m\mid e,d)}[\log(\pi(m\mid e,d)/\pi(m))]$. Core construction: a triangular Knothe–Rosenblatt map $T( v_e,v_d,v_m)=(T_e(v_e),\,T_{d\mid e}(v_e,v_d),\,T_{m\mid e,d}(v_e,v_d,v_m))$ so that conditionals (evidence/posterior) can be sampled via fixed design/data slices of $T$; the joint is approximated by pushing forward a product reference density through $T$ (built with FTTs and DIRT).","On the toy nonlinear design problem, the transport-map (DIRT) estimator produced optimal designs concentrated near the true maximizer(s) under a fixed forward-model evaluation budget, whereas nested Monte Carlo (NMC) with the same budget yielded much wider variability in the selected optima. In the SEIR model, KL divergences (posterior-to-prior) reported using 100,000 samples were approximately 5.1 for uniform early sampling, 6.3 for one-at-a-time greedy SOED, and 6.8 for two-at-a-time greedy SOED, indicating improved information gain from the proposed sequential design. In the PDE permeability inversion example, sequential A- and D-optimal boundary-condition designs yielded visibly better posterior mean/variance recovery of a channel structure than random designs; the paper also reports stage-wise computational costs (in FE-solve equivalents) for constructing joint/posterior transports and evaluating incremental utilities.","The authors note the method is typically infeasible for very high- or infinite-dimensional parameter spaces without prior dimensionality reduction (e.g., via reparameterization techniques). They also state the approach is best suited to a small-to-moderate number of experiments (small-data regime). Finally, they highlight the lack of a full theoretical study of approximation error and tensor-train rank bounds for the proposed FTT-based surrogates as an important gap (with only partial first steps available for Gaussian cases).","Performance and feasibility may be sensitive to tensor-train rank growth and to the choice of bridging/tempering schedules in DIRT; practical guidance for selecting these to control cost vs. accuracy is limited. The approach assumes availability of an evaluable unnormalized joint density and (in examples) largely independent Gaussian measurement noise; robustness to model misspecification, heavy tails, outliers, or strong dependence structures is not thoroughly explored. Comparisons are mainly against nested Monte Carlo on a toy example; broader benchmarking against Laplace-based BOED, variational BOED, or modern amortized/sequential design methods (e.g., neural/gradient estimators) would strengthen empirical positioning.","The authors call for a theoretical study of approximation errors and rank bounds for the functional tensor-train surrogates used to build the transport maps, to better understand suitability and limitations (noting existing first steps for Gaussian densities). They also suggest mitigating high-dimensional infeasibility through a-priori dimensionality reduction extensions of existing reparameterization approaches.","Develop self-tuning/adaptive strategies for selecting tempering schedules, defensive-mixture weights, and TT-rank tolerances to guarantee target accuracy with minimal forward solves. Extend the framework more explicitly to non-Gaussian and/or correlated noise models, model discrepancy, and time-series/autocorrelated observations common in sequential monitoring. Provide open, end-to-end reproducible implementations for the full BOED/SOED pipelines (beyond relying on external toolboxes), including standardized interfaces to PDE/ODE solvers and design optimizers, and benchmark across established BOED test suites.",2401.07971v3,https://arxiv.org/pdf/2401.07971v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:25:52Z
TRUE,Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Not applicable,Variable/General (design is sequential control inputs ξ_t; examples include 1D and 2D designs; parameters θ include 2–4 unknowns in case studies),Other,Simulation study|Other,TRUE,Julia,Public repository (GitHub/GitLab),https://github.com/Sahel13/InsideOutSMC2.jl,"The paper proposes a method for sequential Bayesian experimental design (BED) in dynamical systems with non-exchangeable data by reframing design selection as risk-sensitive policy optimization and an equivalent probabilistic inference problem (control-as-inference). It introduces Inside-Out SMC2, a nested sequential Monte Carlo algorithm that jointly estimates expected information gain (EIG) and samples informative design trajectories by nesting an IBIS parameter-posterior filter inside an outer particle filter over outcome–design histories. The method is embedded in a particle MCMC/conditional SMC kernel and optimized via Markovian score climbing to amortize a recurrent design policy without relying on contrastive (sPCE) estimators. Empirical simulations on stochastic pendulum, cart-pole, and double-link systems (50-step horizons) show the learned policies outperform random/PRBS, a myopic look-ahead variant, and iDAD trained on the sPCE bound, especially when EIG is large where sPCE saturates at log(L+1). The approach targets parameter learning in input-constrained stochastic dynamics and provides a practical route to real-time sequential design once the policy is trained.","Design objective is expected information gain over a horizon: $I(\phi)=\mathbb{E}_{p_\phi(z_{0:T})}[H[p(\theta)]-H[p(\theta\mid z_{0:T})]]$, which factorizes into stage rewards $I(\phi)=\mathbb{E}[\sum_{t=1}^T r_t(z_{0:t})]$ with $r_t=\alpha_t+\beta_t$, $\alpha_t=\int p(\theta\mid z_{0:t})\log f(x_t\mid x_{t-1},\xi_{t-1},\theta)d\theta$ and $\beta_t=-\log\int p(\theta\mid z_{0:t-1})f(x_t\mid x_{t-1},\xi_{t-1},\theta)d\theta$. They introduce a risk-sensitive version via potentials $g_t(z_{0:t})=\exp\{\eta r_t(z_{0:t})\}$, yielding $I_\eta(\phi)=\frac{1}{\eta}\log \mathbb{E}_{p_\phi(z_{0:T})}[\exp(\eta\sum_{t=1}^T r_t)]$, and optimize $\log Z_T(\phi)$ using Fisher’s identity within a particle MCMC/CSMC-based Markovian score climbing routine.","Across simulated 50-step dynamical-system design tasks, IO-SMC2-trained amortized policies achieve higher EIG than random, PRBS, and a myopic one-step look-ahead baseline, and typically outperform iDAD trained on the sPCE bound. On the conditionally linear stochastic pendulum, IO-SMC2 attains EIG $3.53\pm0.15$ versus iDAD $2.58\pm0.17$ and PRBS $2.24\pm0.19$; an exact-conjugate variant reaches $3.63\pm0.18$. On nonlinear pendulum, IO-SMC2 reaches $3.72\pm0.17$ vs iDAD $3.01\pm0.29$. On cart-pole, IO-SMC2 reaches EIG $21.23\pm0.62$ while sPCE saturates at $\log(10^6)\approx13.82$ for all policies; the paper notes sPCE would need about $\exp(21)\approx1.3$ billion contrastive samples to match that scale. On double-link, IO-SMC2 and iDAD are comparable (IO-SMC2 $11.53\pm0.49$, iDAD $11.73\pm0.45$).","The authors note the method requires closed-form evaluation of conditional transition densities, making it unsuitable for models with intractable likelihoods (e.g., Markov jump process epidemic models). They state computational cost can be high: IO-SMC2 has time complexity $O(NMT^2)$, and stability considerations may imply $N\propto T$, yielding roughly $O(MT^3)$ (though the outer loop can be parallelized). They also caution scalability in state/observation dimension due to bootstrap particle-filter degeneracy, and identify choosing the tempering parameter $\eta$ as an open bias–variance trade-off problem.","The empirical validation is entirely simulation-based on a small set of SDE benchmarks; the robustness of the learned policies to model misspecification (wrong priors, unmodeled dynamics, discretization error) and to real sensor noise/constraints is not established. The approach depends on tuning several SMC/MCMC hyperparameters (N, M, ESS thresholds, number of IBIS move steps, proposal scaling, and $\eta$), but sensitivity analyses and practitioner-facing guidelines are limited. The policy architecture (GRU/LSTM with tanh-squashed Gaussian outputs) may constrain optimality for complex design constraints or discontinuous optimal inputs, and there is no comparison to alternative modern sequential design objectives (e.g., posterior variance reduction targets, Bayesian RL/POMDP solvers beyond sPCE-based iDAD) under matched compute budgets.","They suggest investigating principled selection/scaling of the tempering parameter $\eta$, aiming for inference that remains effective for any $\eta$ while balancing optimism in policy learning and particle-weight variance. They also note potential use of Inside-Out SMC2 beyond BED wherever pathwise smoothing trajectories under parameter-marginalized models are needed, implying extensions to other control-as-inference or risk-sensitive decision-making settings.","Develop likelihood-free or approximate-likelihood variants (e.g., ABC/Poisson approximations, learned surrogates) to relax the stated requirement of tractable transition densities. Improve high-dimensional state performance using guided/auxiliary proposals or Rao–Blackwellization for the outer filter, and investigate adaptive resampling/tempering schedules for $\eta$. Provide a self-contained software package with reproducible pipelines and diagnostic tools (ESS monitoring, auto-tuning of MCMC proposals, and uncertainty calibration) and validate on real robotic/system-identification datasets with safety/actuation constraints.",2402.07868v4,https://arxiv.org/pdf/2402.07868v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:26:31Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation,Other,"Variable/General (n observation times; examples computed for n ∈ {2,3,4,5})",Theoretical/simulation only|Other,Other,TRUE,C/C++|MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/matt-sk/POPBP-Fisher-Information-Calculator.git|https://github.com/matt-sk/POPBP-Fisher-Information-Optimisation.git,"The paper develops an efficient method to choose optimal (generally non-equidistant) observation times for estimating the birth rate λ of a partially observable pure birth process (POPBP), where each individual is detected independently with known probability p at each observation. The design criterion is to maximize the (scalar) Fisher information for λ from n discrete observations over a fixed time horizon, yielding a locally optimal timing design for parameter estimation. The core contribution is a generating-function-based derivation showing the likelihood’s multivariate generating function is rational, which implies a stable constant-coefficient recurrence for computing the likelihood and its λ-derivative. This recurrence enables much faster numerical computation of Fisher information for n>2 than prior approaches based on high-dimensional infinite summations and truncation. The authors implement the approach (Maple for symbolic precomputation; C++ for fast Fisher-information evaluation; Maple/NAG NLPSolve for optimizing times) and report extensive numerical designs exhibiting “drop values” where optimal times abruptly move off boundary solutions (e.g., t_i*=1) as p varies; code is publicly available.","For the fully observed pure birth process (PBP), the Fisher information for λ at times t1<…<tn (tn=τ) is $FI_{X_n}(\lambda)=x_0\sum_{i=1}^n (t_i-t_{i-1})^2\,\frac{e^{-\lambda t_{i-1}}-e^{-\lambda t_i}}{}$ (Eq. 5), enabling direct timing optimization. For the POPBP, the Fisher information is computed from the likelihood via $FI_{Y_n}(\lambda)=\sum_{y_n}\frac{(\partial L_{Y_n}(y_n\mid\lambda,p)/\partial\lambda)^2}{L_{Y_n}(y_n\mid\lambda,p)}$ (Eq. 8). The key computational advance is that the likelihood generating function is rational, $\phi(u_0,\dots,u_n)=\frac{u_0\,\upsilon_{0,1}\cdots\upsilon_{n-1,n}(pu_1+q)\cdots(pu_n+q)}{1-Q_n}$ with $Q_i$ defined recursively (Eqs. 14–15), implying a constant-coefficient recurrence for $L_{Y_n}$ (Eq. 16) and similarly for its λ-derivative.","The generating-function recursion yields large speedups versus prior truncation-based summation methods, making Fisher-information optimization feasible for more observations (reported capability up to n=5, with results shown mainly for n=2–4). Numerically, optimal observation times exhibit sharp threshold (“drop”) behavior as p increases: for n=2, $D_1(\lambda)$ decreases markedly with λ (e.g., approximately 0.91136 at λ=1, 0.57116 at λ=2, 0.048029 at λ=5). For n=3 and n=4, multiple drop values $D_i(\lambda)$ appear, and at about $p\approx D_j(\lambda)$ several earlier optimal times can drop simultaneously. The authors provide extensive timing benchmarks (on Xeon Gold 6150) showing rapid optimization for small n/λ but rapidly increasing runtimes for larger n and λ (e.g., for graph data at n=3, λ=4, reported ~78 days).","The authors note that computation/optimization time grows quickly with n, λ, and p, and that for n=5 the optimization was prohibitively slow so no results are reported. Their current Fisher-information implementation supports only fixed n values because generating functions and recurrence coefficients are precomputed and hard-coded for each n (implemented for n ∈ {2,3,4,5}). They also state the C++ implementation does not work at p=1 and must be handled separately by using the fully observed PBP Fisher information formula.","The design is locally optimal (depends on fixed/assumed λ and known p), so robustness to misspecification of λ (and especially uncertainty in p) is not established. The work focuses on choosing observation times under an assumed pure birth model and independent binomial detection; performance under model deviations (overdispersion, time-varying λ, dependence/heterogeneous detection, or observation error beyond binomial thinning) is unclear. Optimization is numerical and appears potentially multi-modal with boundary cases (“drops”); more systematic global optimality verification for higher n is not demonstrated. Practical guidance on experimental constraints (minimum spacing, costs per observation, or joint optimization over n) is not developed.","They plan to develop further theoretical results about optimal designs using the numerical patterns (e.g., drop values). They propose speeding optimization by using drop-value bounds to prune boundary regions during optimization (reducing the number of optimizations needed). They also suggest implementing the highly parallel computations on GPUs as hardware memory sizes increase.","Develop robust/Bayesian optimal designs integrating uncertainty in λ (and possibly p) rather than pointwise Fisher-information maximization. Extend the method to related CTMC models (birth–death, epidemic/metapopulation processes) and to settings with unknown/estimated detection probability p. Provide an automated, n-generic recurrence/indexing implementation so users can scale to larger n without hard-coding, and add open-source interfaces (e.g., R/Python bindings) and reproducible benchmarks. Study constrained designs (minimum/maximum gaps, observation costs, or adaptive/sequential re-optimization of future times based on early observations) and compare against alternative optimality criteria (e.g., D-/A-optimality for multi-parameter extensions).",2402.09772v2,https://arxiv.org/pdf/2402.09772v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:27:12Z
TRUE,Optimal design|Other,Parameter estimation|Prediction,Other,Variable/General (matrix-valued arms in R^{d1×d2}; effectively d1·d2 dimensions with low-rank structure),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/jajajang/LowPopArt,"The paper studies low-rank matrix trace regression and low-rank linear bandits, proposing a computationally efficient estimator called LowPopArt (and a bootstrapped Warm-LowPopArt) with operator-norm recovery guarantees that depend on a new hardness measure B(Q) derived from the population covariance of the measurement distribution. Building on this bound, it introduces a novel convex experimental design objective—choosing a sampling distribution π over a given measurement/arm set A to minimize B(Q(π)), defining Bmin(A)=min_{π} B(Q(π)). The authors show this B(Q)-based design can yield tighter guarantees than classical nuclear-norm-penalized least squares, and they relate Bmin(A) to classical E-optimal design via Cmin(A)=max_{π} λmin(Q(π)), including instances where Bmin gives substantially better scaling. They leverage the estimator and the Bmin-based design to create two geometry-adaptive low-rank bandit algorithms (LPA-ETC and LPA-ESTR) with improved regret bounds for general operator-norm-bounded arm sets. Empirical simulations support that Bmin-based exploration and LowPopArt improve recovery and regret versus E-optimal exploration and nuclear-norm regression baselines.","The DOE problem selects a sampling distribution π over the finite arm/measurement set A to minimize the LowPopArt error-bound driver: Bmin(A):=min_{π∈P(A)} B(Q(π)), where Q(π)=E_{a∼π}[vec(a)vec(a)^T]. The criterion B(Q) is defined via block structure of Q^{-1}: B(Q):=max{λ_max(∑_{i=1}^{d2} D_i^{(col)}), λ_max(∑_{i=1}^{d1} D_i^{(row)})}, and Warm-LowPopArt achieves ∥\hatΘ−Θ*∥_{op}≲σ·sqrt(B(Q)/n0) up to logs.","In simulations (e.g., d1=d2=3 recovery and d1=d2=5 bandits), Bmin(A)-based exploration generally outperforms E-optimal (λmin-maximizing) exploration, and LowPopArt improves nuclear-norm recovery error relative to nuclear-norm-regularized least squares. Theoretical examples show Bmin(A)=Θ(d^2) for the unit operator-norm ball and Θ(d^3) for the unit Frobenius-norm ball (d=max{d1,d2}); they also construct an arm set Ahard where Bmin(Ahard)≈1/Cmin(Ahard), yielding up to ~d^{3/2} tighter guarantees than bounds expressed via λmin(Q). Bandit regret bounds include LPA-ETC: \tilde O((Rmax r^2 Bmin(A) T^2)^{1/3}) and LPA-ESTR: \tilde O(σ·(S*^2/Sr^2)·sqrt(Bmin(A) d^{1/2} T)) under a minimum singular value condition, improving dependence on arm-set geometry compared to prior general-arm-set methods.","The paper notes a drawback that, on certain special arm sets (e.g., the unit Frobenius-norm ball), their general algorithms can have inferior guarantees compared with methods tailored to those settings. It also remarks that LowPopArt assumes access to the population covariance/distribution (or direct control of sampling); obtaining precise guarantees when Q is estimated from samples is left open.","The experimental design objective B(Q(π)) is convex and solvable (e.g., via CVXPY), but scaling to very large arm sets A or continuous arm spaces may be computationally challenging without specialized optimization (e.g., Frank–Wolfe/mirror descent) or oracle access. The design is tied to the LowPopArt bound and focuses on operator-norm recovery; it is not shown to be minimax-optimal across broader noise models, dependence structures, or misspecification (e.g., heavy-tailed rewards beyond the estimator’s robustness assumptions). Empirical validation is mostly synthetic; real-world evidence is limited and does not fully test sensitivity to model mismatch or non-i.i.d. sampling typical in deployed bandits.","They explicitly leave as future work obtaining precise performance guarantees when the covariance matrix Q(π) must be estimated from observed samples rather than being known/accessible. In the conclusion, they also highlight designing algorithms that match specialized-setting guarantees (e.g., for unit Frobenius-norm ball) while maintaining generality, and establishing geometry-dependent regret lower bounds for low-rank bandits.","Develop scalable design solvers for massive/discrete/continuous arm sets (e.g., conditional gradient with linear-optimization oracles over A) and study approximate-design effects on B(Q) and downstream regret. Extend the design/estimator to handle autocorrelated rewards, adaptive (non-i.i.d.) sampling beyond the exploration phase, and unknown noise levels, possibly via self-normalized or fully self-starting variants. Provide broader empirical benchmarks and ablations on real datasets (recommendation, drug–target, ads) to validate practical gains and robustness, and release a packaged implementation (e.g., pip/conda) for easier adoption.",2402.11156v2,https://arxiv.org/pdf/2402.11156v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:27:52Z
FALSE,NA,NA,Not applicable,Not specified,Network/cybersecurity|Other,Other,TRUE,R|Other,Supplementary material (Journal/Publisher),https://aspredicted.org/qi8ka.pdf,"This paper reports a pre-registered mixed-design (within- and between-subjects) field experiment evaluating two interactive anti-phishing trainings—group discussion and role-playing—against a no-intervention control group (N=105; 35 per condition). Participants completed repeated-measures surveys at three time points (Q1 pre, Q2 immediate post for training groups only, Q3 day-7 follow-up) assessing anti-phishing self-efficacy (SE) and support-seeking intention (SS), and all participants received three in-situ simulated phishing emails 20–50 days after baseline to measure behavioral outcomes (non-clicking and reporting). Within-group analyses showed both trainings increased SE and SS over time, with role-playing showing a significant between-group improvement vs control for SS deltas at day 7, while SE deltas did not differ significantly between groups. Behavioral results showed no detectable differences in non-clicking across conditions, but both trainings increased phishing-email reporting compared to control across the three tests. Quantitative analyses were conducted in SPSS and R (including lavaan and ggplot2), and qualitative feedback was coded in MAXQDA, supporting increased vigilance, reporting intention, and colleague interaction after training.",Not applicable,"Within-group Friedman tests indicated significant improvements for group discussion in SE (Q1–Q2 adjusted p=.018; Q1–Q3 p<.001) and SS (Q1–Q2 p=.003; Q1–Q3 p=.005); for role-playing, SS improved immediately and at day 7 (both p<.001) and SE improved at day 7 (Q1–Q3 p=.046) but not immediately (Q1–Q2 p=.064). Between groups, SS delta (Q3–Q1) differed (Kruskal–Wallis H(2)=7.169, p=.028) with role-playing vs control significant (adjusted p=.026), while SE deltas did not (H(2)=3.859, p=.145). For in-situ tests, non-clicking totals did not differ across conditions (H(2)=.002, p=.999), but reporting totals did (H(2)=14.662, p<.001) with both discussion vs control (adjusted p=.004) and role-playing vs control (adjusted p=.002) significant; e.g., reporting counts by test (control/discussion/role-play): client-upgrade 3/10/4, data-breach 7/18/23, security-alerts 8/19/20.","The authors note that longer-term retention is unknown and call for longitudinal field studies beyond their short follow-up, citing that training retention may last up to five months. They report that neither self-efficacy nor support-seeking significantly predicted reporting/non-clicking in regression models, suggesting other constructs (e.g., perceived severity, response efficacy) may matter. They caution that emphasizing self-efficacy could induce overconfidence and plan to address overconfidence bias. They also acknowledge limited generalizability (single-university context, possible self-selection of tech-savvy participants) and that prior informed consent may have influenced behavioral outcomes; additionally, richer data (discussion content, created phishing emails) were collected but not analyzed due to space constraints, and the cost/effort of running in-situ tests was outside scope.","The “control” condition differed not only by training content but also by mode (remote vs in-person lab), which can confound training effects with contact time, social presence, and Hawthorne/attention effects. The schedule yields an unbalanced measurement design (Q2 missing for control), complicating interpretation of “immediate” effects and making time-by-condition contrasts less direct. Behavioral outcomes had very low click rates (near ceiling on non-clicking), limiting power to detect differences and potentially masking meaningful effects; reporting may also be influenced by holiday timing and organizational processes. The study is not framed as DOE and does not systematically vary training components (dose, facilitator, group size), so causal attribution to specific elements (discussion vs role-play mechanics) remains coarse.","The authors propose longitudinal field evaluations to examine residual training efficacy beyond their short follow-up window. They suggest investigating additional psychological factors (e.g., perceived severity, response efficacy) that might explain reporting behavior when self-efficacy/support-seeking do not. They plan to address potential overconfidence side effects and to draw on prior work on folk models and security story sharing to improve group discussion quality. They also plan a follow-up paper analyzing participants’ recorded discussions and participant-designed phishing emails, and recommend future work to test generalizability in other organizational contexts and to empirically compare approaches for mitigating informed-consent effects; they additionally recommend studying the hidden costs of deploying phishing tests for evaluation.","Develop and validate a dedicated anti-phishing self-efficacy scale (rather than combining dimensions from other constructs) and test measurement invariance across time and conditions to strengthen longitudinal comparisons. Use a more tightly controlled design that equalizes contact time (e.g., attention-placebo control) and mode (all in-person or all remote) to isolate training mechanisms, and factorially manipulate key components (peer discussion vs attacker-mindset vs examples) to identify active ingredients. Expand behavioral endpoints beyond click/non-click and report counts (e.g., time-to-report, correct classification, help-seeking behavior logs, downstream incident handling quality) and evaluate in organizations with different cultures and baseline security maturity. Provide open-source analysis scripts and implementation materials in a public repository and conduct multi-site replications to improve external validity.",2402.11862v1,https://arxiv.org/pdf/2402.11862v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:28:27Z
TRUE,Factorial (fractional)|Response surface|Sequential/adaptive|Bayesian design,Screening|Optimization|Cost reduction,Bayesian D-optimal|Other,8 factors screened; 4 factors optimized in CCD/ADoE,Manufacturing (general),Case study (real dataset)|Other,TRUE,R|Other,Not provided,https://doi.org/10.1038/s41598-024-80405-2,"The paper develops and experimentally validates a Bayesian adaptive design of experiments (ADoE) approach, implemented via Bayesian optimisation with a Gaussian process surrogate, for real-time optimisation of an industrial injection moulding process. The workflow first screens 8 candidate process parameters using a two-level 1/16 fractional factorial design (16 runs, with 10 repeated shots per run), then builds quadratic response models via a full circumscribed central composite design (CCC) with 4 factors and 31 runs (including 6 centre-point replications). Single-objective and multi-objective optimisation target minimising inline temperature differential (ΔT, a proxy for post-production shrinkage) and cycle time; ADoE is compared against classic RSM-based optimisation using composite desirability and NSGA-II. ADoE uses GP + Expected Improvement acquisition to iteratively select new settings (generated in batches of two) until convergence/threshold criteria are met. Empirically, ADoE reduces the number of experiments needed (16 vs 31 for single-objective ΔT; 22 vs 31 for multi-objective ΔT+cycle time) while achieving comparable or better operating settings, and the new inline ΔT proxy is validated against metrology-based shrinkage measurements.","The inline quality index is defined as the temperature differential between two in-mould thermocouples: $\Delta T = T_{TCA} - T_{TCB}$. Bayesian optimisation uses a Gaussian process prior $f(x)\sim\mathcal{GP}(\mu(x),k(x,x'))$ with observation model $y=f(x)+\varepsilon$, and selects new experimental settings by maximising the Expected Improvement acquisition function $EI(x)$ (given in the paper) based on GP posterior mean and variance. Classic multi-response optimisation uses Derringer–Suich composite desirability $D=(\prod_{i=1}^n d_i(y_i))^{1/n}$ (weights set to 1 here) fitted on quadratic RSM models from the CCD.","Screening used a 1/16 fractional factorial with 8 two-level factors (16 runs) and identified mould temperature, cooling time, holding time, barrel (melt) temperature (plus some interactions) as influential; optimisation proceeded with 4 factors. The CCD was a full circumscribed CCD (α=2) with 4 factors and 31 runs (6 centre-point replications) to fit quadratic RSM models. For single-objective optimisation of ΔT, ADoE reached an experimental ΔT of 5.54°C using 16 total experiments, versus 31 runs for CCD/desirability (predicted ΔT 5.10°C; experimental verification 5.54°C). For multi-objective optimisation (ΔT and cycle time), ADoE achieved ΔT=6.51°C and cycle time=32.9 s in 22 experiments; desirability predicted ΔT=7.42°C and 30.28 s but experimentally gave ΔT=8.2°C and 28.8 s; NSGA-II (on RSM) selected a Pareto point with predicted ΔT=7.03°C and 34.14 s (experimental 6.83°C and 34.93 s). Overall, ADoE reduced experiments by ~50% (single) and ~30% (multi) while producing similar or better optimal settings than comparator methods.","The authors note that the ADoE trajectory (number of loops to convergence) can depend on the size and composition of the initial dataset (here, 12 randomly selected CCD runs), potentially making the optimisation longer or shorter depending on the baseline data. They also note that the ANN surrogate used for comparison was not tuned (architecture/parameters not optimised), which could improve its performance relative to GP/RSM if optimised. They further suggest that more data could refine the recommended ΔT threshold range for ensuring dimensional tolerance without unnecessary cooling time.","Because the adaptive optimisation is initialized from randomly selected CCD points, results may be sensitive to initialization and random seeds, and reproducibility would benefit from reporting these details and repeating the ADoE runs under multiple initializations. The approach is demonstrated on a single component/material (POM) and a specific sensor setup (two thermocouples at chosen locations); generalizability to other parts, polymers, and sensing layouts is not established. The paper relies on ΔT as a proxy for shrinkage/warpage; while validated on selected runs, the strength and stability of this proxy under broader conditions (e.g., drift, ambient changes, autocorrelation, batch-to-batch variation) is not fully characterized. Code and full implementation details (including BO hyperparameters, constraints, and optimization settings) are not provided, limiting direct replication.","The authors propose extending the approach to additional responses (e.g., energy or material consumption) beyond ΔT and cycle time. They suggest comparing the proposed adaptive DoE (ADoE) against other adaptive DoE methods using different initial experimental sets, surrogate models, and acquisition functions, and incorporating a broader range of response variables. They also propose benchmarking against other advanced multi-objective optimisation methods (e.g., improved dragonfly optimisation) to assess comparative performance and robustness.","Developing a self-starting or initialization-robust ADoE strategy (e.g., space-filling initial design, multiple random starts, or Bayesian priors informed by physics/simulation) would strengthen reproducibility and reduce sensitivity to the initial dataset. Extending the method to handle process constraints and defect modes explicitly (e.g., burn marks at high barrel temperatures) via constrained BO would better reflect industrial feasibility. Adding robustness to noise, drift, and autocorrelation in time-ordered production data (e.g., non-iid noise models, change-point detection, or online calibration of sensors) could improve reliability for continuous real-time deployment. Releasing an implementation (scripts/notebooks) and providing detailed BO configuration would enable replication and faster uptake by practitioners.",2402.12077v2,https://arxiv.org/pdf/2402.12077v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:29:09Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,Other,Variable/General (select k experiments/observations from n candidates; examples include n=50 with k up to 50; also k=10 in nonlinear examples),Healthcare/medical|Environmental monitoring|Network/cybersecurity|Pharmaceutical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://arxiv.org/abs/1802.06517,"This paper develops a nonlinear Bayesian optimal experimental design method for selecting a subset of k experiments from an n-sized candidate pool by maximizing mutual information (MI) between selected observations and model parameters. It introduces computationally cheap lower/upper bounds on (conditional) MI derived using logarithmic Sobolev inequalities, enabling a greedy selection algorithm (LSIG) that avoids repeated expensive MI estimation in nonlinear/non-Gaussian settings. The LSIG iteration reduces to computing two matrices from joint samples—an empirical score/gradient outer-product matrix and a (Gaussian-approximated) conditional covariance updated via Schur complements—then choosing the next design via a diagonal criterion. Empirically, LSIG matches standard greedy MI-based selection on a linear-Gaussian benchmark and performs comparably to (or better than) nested Monte Carlo greedy methods while substantially outperforming random selection and Gaussian approximations in nonlinear/non-Gaussian examples (epidemic transmission and spatial Poisson). The work advances Bayesian OED for discrete/combinatorial design by providing an MI-surrogate that is inexpensive and easy to integrate into greedy subset selection without neural MI estimators.","The design problem is subset selection: $A^*=\arg\max_{|A|=k} I(X;Y_A)$, with greedy increments $i^*=\arg\max_{i\in\bar A_\ell} I(X;Y_i\mid Y_{A_\ell})$. Using log-Sobolev inequalities, they bound MI by an expected squared gradient of the log-posterior: $I(X;Y)\le \frac{\kappa}{2}\int \|\nabla_y\log \pi_{X\mid Y}(x\mid y)\|^2_\Gamma\, d\pi_{X,Y}$ (and similarly for conditional MI). The resulting LSIG selection criterion chooses $i^*=\arg\max_i \operatorname{diag}(F\,S,i)$ where $F=\frac1M\sum_{j=1}^M g_j g_j^\top$ with $g_j\approx \nabla_y \log \pi_{X\mid Y}(x_j\mid y_j)$ and $S$ is an (updated) conditional covariance computed via Schur complements.","Complexity analysis (Table 1) shows LSIG requires $O(Mm)$ sample/model-evaluation cost and linear algebra cost $O(nk^3)$, versus NMC-greedy requiring $O(nk M_{in}M_{out})$ samples/model evaluations (typically much larger) and Gaussian approximation costing $O(nk^4)$ linear algebra. In the linear-Gaussian example (with $d=n=50$), LSIG achieves MI curves nearly indistinguishable from standard greedy (closed-form MI), while random selection is noticeably worse. In the epidemic transmission model (negative binomial observations), LSIG achieves MI comparable to NMC-greedy and significantly better than Gaussian approximation (near random). In the spatial Poisson example, LSIG outperforms Gaussian approximation for all k and exceeds NMC-greedy except at very small k (k=1,2), with more consistent selected locations across trials.","The authors note that directly estimating MI in non-Gaussian settings is costly (nested Monte Carlo converges slowly; variational MI bounds require training neural networks), motivating their bound-based approach rather than MI evaluation. They also state that neural-network score estimation methods can make the relative magnitudes of the diagnostic matrix entries sensitive/unstable, which is problematic because LSIG depends on ranking diagonal entries. In the bound construction, they replace an intractable matrix in the log-Sobolev bound with one derived from a Gaussian approximation, acknowledging practical difficulty in obtaining the exact reference measure and associated constants.","The method’s quality depends on assumptions needed for log-Sobolev inequalities (sub-Gaussian reference measures) and on using Gaussian-approximated conditional covariances; performance may degrade when these approximations are poor (e.g., heavy-tailed or strongly multimodal observation distributions). LSIG requires differentiability of the likelihood with respect to observations (or workable approximations) and Monte Carlo estimation of score-like terms, which may be brittle for discrete observations or complex simulators. The paper focuses on discrete subset selection from a candidate pool and does not address continuous design spaces, constraints, or adaptive/sequential designs where observations update the posterior online beyond the greedy surrogate. No public implementation details (software, reproducibility artifacts) are provided, limiting practical uptake.","They plan to extend the approach to an “implicit model” setting where only samples are available and the likelihood and its gradient are intractable, using tools for more consistent and stable score estimation.","A natural extension is to provide theoretical guarantees (e.g., approximation bounds or conditions for near-submodularity) for the LSIG surrogate relative to true MI in nonlinear/non-Gaussian settings. Developing self-normalized or variance-reduced estimators for the required gradient/score matrices, and robustifying the method for discrete or non-differentiable likelihoods, would improve stability. Extending LSIG to continuous design variables (hybrid discrete-continuous) and to adaptive/sequential experimentation with posterior updates at each step would broaden applicability. Packaging the method in an open-source implementation with benchmark suites would enable reproducibility and fair comparisons to modern neural MI estimators and Bayesian OED baselines.",2402.15053v1,https://arxiv.org/pdf/2402.15053v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:29:46Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,Bayesian D-optimal,Variable/General (select k sensors/columns out of m candidate locations; k fixed by budget),Environmental monitoring|Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB|Python,Public repository (GitHub/GitLab),https://github.com/RandomizedOED/css4oed,"The paper develops computationally efficient Bayesian D-optimal experimental design methods for sensor placement in Bayesian linear inverse problems by recasting sensor selection as a Column Subset Selection Problem (CSSP). The D-optimal objective is the expected KL divergence (information gain) from prior to posterior, which for linear-Gaussian models reduces to a log-determinant criterion involving the preconditioned operator $A=\Gamma_{pr}^{1/2}F^T\Gamma_{noise}^{-1/2}$. The authors propose deterministic and randomized CSSP-based algorithms (GKS: truncated SVD + pivoted QR; RAF-OED: randomized sketching and adjoint-free selection; and a leverage-score sampling hybrid) and provide theoretical bounds and complexity estimates, including PDE-solve counts. Numerical experiments on 2D heat equation and seismic travel-time tomography show the methods achieve strong D-optimality with relatively few sensors and that RAF-OED is substantially faster while remaining competitive. The paper also introduces a Bayesian-DEIM data-completion approach to reconstruct unmeasured sensor data from selected measurements without solving the inverse problem.","The Bayesian D-optimal sensor placement objective is $\phi(S)=\log\det\left(I + A(:,S)A(:,S)^T\right)$, where $A=\Gamma_{pr}^{1/2}F^T\Gamma_{noise}^{-1/2}\in\mathbb{R}^{n\times m}$ and $S$ indexes the selected $k$ sensors (columns). The algorithms use a truncated SVD $A\approx U_k\Sigma_kV_k^T$ and then select columns via pivoted QR on $V_k^T$ (or on a sketch $Y=\Omega A$ for RAF-OED). Data completion uses the interpolatory projector $P=V_k(S^TV_k)^{-1}S^T$ and completed data $Pd=V_k(S^TV_k)^{-1}(S^Td)$.","The paper proves D-optimality bounds linking performance to the top-$k$ singular values of $A$ and the conditioning of the selected submatrix of $V_k$ (e.g., $\Psi(\Sigma_k/\|V_{11}^{-1}\|_2)\le\phi(S)\le\Psi(\Sigma_k)$). RAF-OED provides a high-probability bound with additional sketching constants and can be adjoint-free, requiring about $\sim k$ PDE solves versus $\sim 4k$ PDE solves for randomized-SVD-based GKS variants (with one subspace iteration). In experiments (Heat with $k=30$, Seismic with $k=50$), RAF-OED achieved similar D-optimality and reconstruction error to other methods but with substantially lower wall-clock time (e.g., Heat: RAF 41.6s vs RandGKS 124.6s; Seismic: RAF 0.674s vs RandGKS 1.98s). RandGKS consistently outperformed random sensor selections in D-optimality and achieved good reconstructions with relatively small $k$ (e.g., around 20 sensors in the Heat example).","The bulk of the paper assumes uncorrelated observation noise $\Gamma_{noise}=\eta^2 I$; while extension to diagonal noise is described as straightforward, extension to general correlated noise is deferred. The work focuses on linear inverse problems with Gaussian priors, and extensions beyond these assumptions are not addressed in the main development.","Comparisons emphasize a small set of baselines (random selection and a greedy method) and do not extensively benchmark against other modern OED approaches (e.g., continuous relaxations or stochastic binary optimization) on matched computational budgets. Practical performance may depend on how well low-rank structure holds for $A$; if singular values decay slowly, the achievable gap to using all sensors can be large, and the bounds may be loose. The methods are developed for linear settings; applicability to nonlinear forward models would require additional approximation/linearization machinery not evaluated here.","The authors propose extending the approach to other OED criteria (e.g., A-optimality and goal-oriented criteria), to nonlinear inverse problems, and to correlated-noise settings (suggesting techniques related to prior work). They also suggest investigating whether CSSP-related alternatives such as volume sampling and determinantal point processes can provide practical benefits.","Develop self-starting/online variants that update sensor selections sequentially as data arrive (adaptive/streaming OED) and quantify regret or information gain over time. Provide robust versions for model mismatch and non-Gaussian priors/noise (e.g., using Laplace approximations, variational Bayes, or robust log-det surrogates) and validate on real experimental datasets. Release and validate a standardized software API (including reproducible pipelines for PDE-constrained problems) and explore GPU/parallel implementations to demonstrate scalability on truly large-scale PDE models.",2402.16000v3,https://arxiv.org/pdf/2402.16000v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:30:19Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Prediction|Parameter estimation|Cost reduction|Other,Other,"Variable/General (high-dimensional binary mask design; e.g., 640×386 k-space locations; parameters x are 320×320 images)",Healthcare/medical,Simulation study,TRUE,Julia,Not provided,NA,"The paper proposes a scalable Bayesian optimal experimental design (OED) method that jointly learns (i) a conditional normalizing flow (CNF) posterior sampler and (ii) an experimental design for selecting measurements under a sampling budget. The design objective is the expected information gain (EIG), and the authors show that maximizing EIG can be rewritten as maximizing the expected posterior log-likelihood, enabling end-to-end gradient-based optimization of both CNF parameters and design variables. To handle binary design variables (sensor/measurement selection masks), the method uses a probabilistic Bernoulli/indicator-mask formulation with a straight-through (pass-through) gradient approximation and a normalization step to satisfy a fixed sampling budget. The approach is demonstrated on accelerated MRI k-space acquisition, learning an optimized under-sampling mask that improves posterior sample realism and reduces uncertainty relative to a hand-crafted baseline mask. Empirically, the learned design reduces reconstruction error (NMSE of posterior mean) and reduces posterior uncertainty (normalized mean squared standard deviation) over a 100-sample test set.","The expected information gain is defined as $\mathrm{EIG}(M)=\mathbb{E}_{p(y\mid M)}\left[\mathrm{D}_{\mathrm{KL}}(p(x\mid y,M)\,\|\,p(x))\right]$, and is rewritten as $\max_M \mathrm{EIG}(M)=\mathbb{E}_{p(x,y\mid M)}[\log p(x\mid y)]$, which matches the maximum-likelihood objective for conditional generative models. This yields a joint optimization $\max_{\theta,M}\mathbb{E}_{p(x,y\mid M)}[\log p_\theta(x\mid y)]$ for CNF weights $\theta$ and design $M$. For binary designs, a probabilistic mask is sampled via $M(w):=\mathbf{1}_{\frac{s\,w}{\|w\|}<u}$ with $u\sim \mathcal{U}(0,1)$, enforcing average sampling rate (budget) $s$. The MRI training objective (Eq. 9) uses the CNF log-likelihood term $-\tfrac12\|f_\theta(x;A^\top(M(w)\odot y))\|_2^2+\log|\det J_{f_\theta}|$, and posterior sampling is $x=f^{-1}_{\hat\theta}(z;A^\top y_{\mathrm{obs}})$ with $z\sim\mathcal{N}(0,I)$.","Using the FASTMRI knees dataset with a sampling budget of $s=0.025$ (2.5% of k-space frequencies), the learned mask emphasizes low frequencies smoothly, exhibits anisotropic vertical emphasis, and exploits Hermitian symmetry without explicit domain knowledge. On an example test case, the baseline posterior-mean SSIM is reported as 0.57 versus 0.68 for the learned design, and the corresponding posterior-mean NMSE is 0.1053 (baseline) versus 0.0223 (learned design). Over a test set of 100 samples, boxplots indicate consistently reduced posterior uncertainty (normalized mean squared standard deviation) and reduced reconstruction error (NMSE of posterior mean) for the EIG-optimized design compared to the hand-crafted mask. Training time is reported as ~16 hours for the baseline and ~20 hours for the proposed method on a single GPU (due to use of the adjoint Fourier operator).","The authors note an added computational cost from processing sub-sampled data with the adjoint Fourier operator $A^\top$ during training, increasing training time relative to the baseline (16 hours to 20 hours on their hardware). They also state there is an absence of prior literature for scalable Bayesian experimental design specifically in MRI, motivating them to build their own baseline using the same density estimator for controlled comparison.","The evaluation is limited to comparison against a single hand-crafted baseline mask; broader comparisons to other learned sampling/OED approaches (e.g., LOUPE-style methods, reinforcement-learning sequential acquisition, or alternative EIG estimators) would strengthen conclusions. The method depends on simulator fidelity (forward model and noise) and on the CNF’s ability to represent the true posterior; robustness to model mismatch, non-Gaussian noise, and dataset shift is not established. The straight-through gradient estimator for the indicator mask can introduce biased gradients and training instability; sensitivity analyses to this approximation and to hyperparameters (budget normalization, temperature/regularization) are not reported. Practical deployment constraints (scanner hardware limits, trajectory constraints, calibration data, patient motion) and how they restrict feasible masks are not addressed.",None stated.,"Extend the framework to handle additional real-world MRI constraints (trajectory feasibility, coil calibration, motion, and hardware-specific sampling restrictions) and test robustness under forward-model mismatch. Compare against a wider suite of baselines, including other learned under-sampling pattern optimization methods and alternative Bayesian OED/EIG estimators, using standardized benchmarks. Develop self-tuning or sequential/adaptive acquisition variants that update the design online as measurements arrive, potentially leveraging the probabilistic mask parameterization. Provide an open-source reference implementation and ablations on the straight-through estimator, budget normalization, and CNF architecture to clarify stability and generalization behavior.",2402.18337v1,https://arxiv.org/pdf/2402.18337v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:31:11Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Optimization|Other,Other,"Variable/General (binary treatment; covariates are d-dimensional, with examples d=1,3,10,20,25)",Healthcare/medical|Finance/economics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://openreview.net/forum?id=xfj5jjpOaL|https://mattblackwell.org/research/batch_adaptive/|https://biostats.bepress.com/ucbbiostat/paper232.|https://artowen.su.domains/mc/,"The paper proposes an active adaptive experimental design for efficient estimation of an average treatment effect (ATE) with binary treatments, where the experimenter adaptively chooses both (i) the treatment propensity score and (ii) the covariate sampling density of experimental units. It derives the semiparametric efficiency bound for ATE estimation under covariate shift as a functional of the covariate density p(x) and propensity score w(a|x), and then analytically characterizes the efficiency-minimizing choices: Neyman allocation for w*(a|x) proportional to conditional outcome standard deviations, and an “efficient” covariate density p*(x) proportional to the sum of conditional standard deviations. Based on sequential plug-in estimates of these efficient probabilities, it designs the AAS-AIPWIW experiment and an AIPW-with-importance-weighting estimator whose asymptotic variance attains the minimized efficiency bound. Monte Carlo simulations (including synthetic settings and semi-synthetic IHDP data) show reduced MSE relative to RCT/DRCT and to adaptive designs that optimize only the propensity score, with the largest gains when outcome variances vary strongly with covariates.","The semiparametric efficiency bound for regular ATE estimators under covariate density p and propensity score w is $\tau(w,p)=\mathbb{E}_{X\sim q}\!\left[\left(\frac{\sigma_0^2(1)(X)}{w(1\mid X)}+\frac{\sigma_0^2(0)(X)}{w(0\mid X)}\right)\frac{q(X)}{p(X)}\right]$. The efficient propensity score is the Neyman allocation $w^*(a\mid x)=\sigma_0(a)(x)/(\sigma_0(1)(x)+\sigma_0(0)(x))$, and the efficient covariate density is $p^*(x)\propto (\sigma_0(1)(x)+\sigma_0(0)(x))\,q(x)$. The proposed estimator is an AIPW with importance weighting, averaging a score $\Psi_t$ that includes inverse-propensity terms times $q(X_t)/p_t(X_t)$ plus an evaluation expectation under $q$.","The minimized semiparametric efficiency bound under joint optimization of p and w is $\tau^* = \left(\mathbb{E}_{X\sim q}[\sigma_0(1)(X)+\sigma_0(0)(X)]\right)^2$, and the paper proves that under the proposed AAS-AIPWIW experiment, $\sqrt{T}(\hat\theta_T-\theta_0)\Rightarrow\mathcal{N}(0,\tau^*)$. Simulation studies (e.g., T=2000 with 200 replications) show lower empirical MSE for AAS-AIPWIW than RCT/DRCT and than AS-AIPW (which optimizes only w but samples covariates from q). Coverage ratios reported for nominal 95% CIs are 95.5% (RCT) versus 38.0%/26.5% (AS-AIPW/AAS-AIPWIW) in Gaussian-covariate simulations and 48.5%/44.0% in Uniform-covariate simulations, indicating finite-sample calibration issues for adaptive methods in their experiments.","The paper notes that while the asymptotic variance result does not depend on covariate dimension d, high dimensionality can prevent nuisance estimators (e.g., for conditional means/variances) from satisfying the required consistency assumption, suggesting sparsity or benign-overfitting arguments may be needed. It also acknowledges (in simulation discussion) that adaptive methods’ empirical distributions and confidence intervals can exhibit bias at finite T and that such bias should vanish as T grows. The impact statement warns that optimizing sampling/assignment could induce biased sampling or unethical manipulation, and raises privacy/consent concerns in sensitive domains like healthcare.","The main theoretical guarantees rely on strong conditions (e.g., almost-sure convergence of nuisance estimators and sub-Gaussian tails) and implicitly require overlap/lower bounds on assignment/sampling probabilities; practical implementations may need explicit truncation/stabilization of $w_t$ and $p_t$ to avoid extreme weights and variance blow-ups. The framework assumes immediate outcome observation each round and i.i.d. potential outcomes across rounds (no interference/SUTVA, no time trends), which may fail in many field experiments with delays, nonstationarity, or interference. The method also presumes the ability to “choose” or sample units from an adaptively targeted covariate density p(x), which may be infeasible without a large accessible pool and may introduce recruitment/selection constraints not modeled. Finite-sample inference appears problematic (poor coverage in simulations), but the paper does not develop corrected inference (e.g., anytime-valid CIs, robust variance estimation) for the proposed covariate-choice design.","The paper explicitly proposes extensions in Appendix E: (i) a rejection-sampling setting that removes the assumption that q(x) is known by observing covariates from q and accepting/rejecting units to approximate the target p_t; (ii) batch design where p_t and w_t are updated only at certain rounds; and (iii) extension to multiple (K≥3) treatments/policy-value targets with corresponding efficiency bounds and efficient probabilities. It also suggests that in high-dimensional settings, additional arguments (e.g., sparsity or benign overfitting) may be used to justify the nuisance-function convergence assumptions needed for asymptotic efficiency.","Developing finite-sample valid inference (calibrated confidence sequences/intervals) for AAS-AIPWIW under adaptive covariate choice—especially with stabilized/truncated weights—would address the poor coverage observed in simulations. Extending the design to handle delayed outcomes, nonstationarity, or covariate-dependent time trends (e.g., via adaptive discounting or change-point robust methods) would improve applicability. Practical constrained-design variants (e.g., limited recruitment budgets per subgroup, fairness/ethics constraints on oversampling high-variance covariate regions) and sensitivity analyses for violations of SUTVA or overlap would make the approach more deployable. Providing open-source reference implementations and benchmarks against modern adaptive experimentation baselines (contextual bandits with doubly robust estimators, Bayesian adaptive randomization) would facilitate adoption and reproducibility.",2403.03589v2,https://arxiv.org/pdf/2403.03589v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:31:56Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction,D-optimal|A-optimal|Not applicable,"2 factors (inputs: liquid mole fraction l and pressure P); 5 model parameters estimated (a12, a21, b12, b21, c12)",Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://www.hsl.rl.ac.uk,"The paper proposes a general sequential locally optimal design of experiments methodology for nonlinear models, including implicit algebraic models common in chemical engineering (e.g., vapor–liquid equilibrium). The method alternates between performing experiments, updating least-squares parameter estimates, and computing a batch of new experiments via a two-stage locally optimal design that maximizes combined information from prior and proposed experiments. The design criterion is applied to the Fisher information matrix of a linearized model around the current parameter estimate; differentiable criteria such as D- and A-optimality are supported (E-optimality excluded), and two-stage weighting uses an importance factor for previous experiments. Two-stage design subproblems are solved using an adaptive discretization algorithm; weighted optimal designs are then converted to implementable (unweighted) experiment batches by selecting points with non-negligible weights and, if needed, choosing a best subdesign. In a real lab case study estimating NRTL parameters for a narrow azeotropic propanol/propyl acetate system, the sequential D-optimal design achieves comparable prediction error and uncertainty with 15 experiments versus 27 for a traditional full factorial grid, reducing experimental effort substantially.","The core design quantity is the (one-point) Fisher information matrix $m_f(x,\theta)=D_\theta f(x,\theta)^\top\,\Sigma^{-1}\,D_\theta f(x,\theta)$ and the design information matrix $M_f(\xi,\theta)=\sum_{x\in\mathcal X} w_x\, m_f(x,\theta)$ (or $M_f(\tilde x,\theta)=\sum_i m_f(x_i,\theta)$). Locally optimal design for nonlinear models uses linearization-based covariance $\mathrm{Cov}(\hat\theta)\approx M_f(\tilde x,\theta)^{-1}$ and minimizes a convex design criterion such as D-optimality $\Psi_0(M)=\ln\det(M^{-1})$ (A-optimality also discussed). The proposed two-stage objective is $\min_{\xi\in\Xi(\mathcal X)}\Psi\big(\alpha M_f(\xi_{\tilde x^-},\theta)+(1-\alpha)M_f(\xi,\theta)\big)$, combining information from already-run experiments and the new batch.","Using D-optimal sequential two-stage design (with importance factor $\alpha=0.5$, batch size up to 3, and a 10×10 candidate grid for OED), the final locally optimal design had 15 experiments (with replications) and achieved nearly the same predictive performance as a 27-run full factorial design on a 9×3 grid. Reported RMSEs on the full collected dataset were essentially equal: for vapor composition $\rho_v$ about $59.6\times10^{-4}$ (oed3, n=15) vs $59.9\times10^{-4}$ (fed3, n=27); for temperature $\rho_T$ about $16.18\times10^{-2}$ vs $15.75\times10^{-2}$. Worst-case (over the design region) linearization-based uncertainties were also similar: $\sigma^{lin}_v\approx25.47\times10^{-4}$ (oed3) vs $24.92\times10^{-4}$ (fed3) and $\sigma^{lin}_T\approx8.26\times10^{-2}$ (oed3) vs $8.37\times10^{-2}$ (fed3). Sampling-based worst-case uncertainties followed the same pattern (e.g., $\sigma^{sam}_v\approx5.60\times10^{-4}$ for oed3 vs $4.49\times10^{-4}$ for fed3, using $n_{sam}=1000$), supporting the claim of comparable prediction quality with roughly half the experiments.",None stated.,"The approach is locally optimal and relies on linearization around the current least-squares estimate, so performance can degrade if early parameter estimates are poor or the model is strongly nonlinear/multimodal (which the authors note can occur in NRTL fitting). Design construction assumes independent normal measurement errors with known covariance and uses a finite candidate set (gridded) design space, so optimality is restricted to that discretization and may be sensitive to grid resolution. The conversion from weighted to implementable (unweighted) designs uses a heuristic sieving/subdesign selection procedure, and the resulting efficiency relative to the weighted optimum is not theoretically bounded in the paper for the two-stage (non-homogeneous) criterion.","The authors plan to demonstrate the method on additional applications (including liquid–liquid equilibria and reactive systems), extend it to constrained experimental design problems (e.g., cost/location constraints), and investigate improved design criteria beyond linearization-based criteria—particularly sampling-based approximations of parameter or prediction uncertainty.","Develop robust or Bayesian/semi-Bayesian sequential designs that account for parameter uncertainty explicitly (e.g., prior/posterior over $\theta$) to reduce sensitivity to local minima and early-stage mis-specification. Provide theoretical/empirical guarantees for the weighted-to-unweighted conversion step (or integrate integer/rounded design optimization directly) under two-stage criteria. Extend the approach to handle correlated/heteroscedastic measurement errors and practical execution uncertainty (errors-in-variables) directly in the design criterion, and release reference implementations to improve reproducibility and adoption.",2403.09443v1,https://arxiv.org/pdf/2403.09443v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:32:30Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Optimization|Parameter estimation|Prediction|Other,Not applicable,Variable/General (user-defined controllable parameters; unknown parameters θ depend on application),Other,Simulation study|Other,TRUE,Python|Other,Package registry (CRAN/PyPI)|Public repository (GitHub/GitLab)|Personal website,https://qsensoropt-federico-belliardo-aafff0229087adae5a915fec60fdc5d37.gitlab.io/|https://gitlab.com/federico.belliardo/qsensoropt,"The paper presents a model-aware reinforcement learning (RL) framework for optimal experimental design in quantum metrology and quantum parameter estimation, where an agent adaptively chooses experiment controls/measurements to minimize a user-defined precision loss (e.g., mean squared error). The approach uses a differentiable, physics-informed simulator of the quantum sensor (implemented by the user in TensorFlow), combined with Bayesian updating via a particle filter over unknown parameters θ, so the policy can condition on the current posterior and act sequentially. Training is performed via automatic differentiation through the full measurement loop (including stochastic outcome sampling and Bayes updates), yielding a model-aware policy-gradient style optimization; it supports both Bayesian and frequentist estimation objectives. The framework is implemented in the Python package qsensoropt and demonstrated via Monte Carlo simulated experiments in applications including NV-center parameter estimation (e.g., hyperfine coupling) and photonic-circuit tasks, reporting performance better than existing heuristic/adaptive controls in several scenarios. Overall, it advances DOE for quantum experiments by providing a general, reusable software tool to learn adaptive measurement/control policies that improve estimation precision under resource constraints (time, number of measurements, signal intensity).","Key elements are defined algorithmically as a sequential measurement loop: after each measurement outcome y_t, the particle filter updates the posterior over unknown parameters via Bayes’ rule, e.g., $p(\theta\mid y_{1:t}) \propto p(y_t\mid \theta, u_t)\,p(\theta\mid y_{1:t-1})$, where $u_t$ are chosen controls. The agent implements a policy $u_t = \pi_\phi(s_t)$ where the state $s_t$ is summary information from the particle filter (e.g., posterior mean/variance), and training minimizes an end-of-episode loss such as MSE, $\mathcal{L}=\mathbb{E}[\|\hat\theta-\theta\|^2]$, using gradients obtained by automatic differentiation through the differentiable sensor model and filtering loop.","An example result (Fig. 1) shows lower mean squared error for estimating the parallel hyperfine coupling in an NV center when using model-aware RL control compared with the particle-guess heuristic (PGH), across the plotted range of total measurement time $T$, with the RL curve consistently below PGH. The paper also reports that the approach achieves better-than-state-of-the-art controls “for many cases” across NV-center tasks (DC/AC magnetometry, decoherence estimation, hyperfine characterization) and photonic-circuit tasks (multiphase estimation, Dolinar-like receiver extensions, state discrimination/classification). Quantitative improvements beyond the illustrated MSE-vs-time plot are not specified in the provided excerpt.","The authors state that applicability is best when the quantum system is “well-characterized and relatively simple and small,” implying scaling/complexity limits for large or poorly modeled systems. They also require users to implement a differentiable sensor model in TensorFlow, which constrains use to settings where such a model is available and differentiable. No additional explicit limitations are stated in the provided text.","Performance claims are largely supported by Monte Carlo simulation of the measurement loop; real experimental validations, robustness to model mismatch, and sensitivity to non-idealities (drift, calibration errors, decoherence modeling errors) are not established here. The method depends on the choice of posterior summaries fed to the agent and on particle-filter quality; particle degeneracy or insufficient particles could degrade learned policies, especially in higher-dimensional θ. Computational cost may be substantial because training differentiates through many sequential steps and stochastic sampling, potentially limiting practicality for long-horizon experiments or large action spaces.",None stated.,"Test and benchmark the learned policies on real quantum hardware (NV centers, photonic platforms) to quantify gains under experimental non-idealities and constraints. Extend the framework to handle explicit model uncertainty/model mismatch (e.g., Bayesian model averaging, robust/adversarial training) and to relax differentiability requirements (e.g., gradient-free RL or likelihood-free methods) for more complex simulators. Improve scalability to higher-dimensional parameter estimation (larger θ, more controls) via better posterior representations (e.g., amortized inference, normalizing flows) and more efficient sequential design criteria.",2403.10317v1,https://arxiv.org/pdf/2403.10317v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:33:00Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Cost reduction|Other,V-optimal|Other,"Variable/General (feature dimension $d$ / $d_0$; examples include $d=10$, $d=30$, and high-dimensional CLIP/GPT-2 embeddings)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://www.rsna.org/rsnai/ai-image-challenge/rsna-pediatric-bone-age-challenge-2017|https://github.com/mattgroh/fitzpatrick17k|https://physionet.org/content/mimiciii/1.4/|https://archive.ics.uci.edu/dataset/461/drug+review+dataset+druglib+com|https://doi.org/10.24432/C55G6J,"The paper proposes DAVED, a validation-free data acquisition method for decentralized data markets that selects which seller datapoints to buy under a budget constraint. The method is motivated by linear experimental design and uses a kernelized linear regression (feature/embedding) approximation to derive a V-optimal design proxy for expected test prediction error on a buyer’s unlabeled test queries. The resulting discrete selection problem is NP-hard, so the authors use a convex relaxation and an efficient Frank–Wolfe/herding procedure with rank-one (Sherman–Morrison) updates to maintain the inverse information matrix, enabling a federated implementation with low communication. Empirically, DAVED (single-step and multi-step) achieves lower test MSE than multiple data-valuation baselines on synthetic Gaussian data and on medical/text datasets (MIMIC-III, Fitzpatrick17K, RSNA Bone Age, DrugLib), especially for small budgets and heterogeneous per-datapoint costs. The work reframes data acquisition as optimal experimental design targeted to a specific test query set, advancing beyond validation-set-based data valuation approaches that can overfit due to “inference after selection.”","Data are modeled via a linearized feature map $\phi(x)$: $y=\theta^{*\top}\phi(x)+\varepsilon$ (Eq. 3). With selected weights $w\in\{0,1\}^n$, the (generalized) information matrix is $I(w)=\sum_{j=1}^n w_j x_j x_j^\top$ and the V-optimal proxy objective is $\hat L_{ED}(w)=\frac{1}{m}\sum_{i=1}^m x_{i,\text{test}}^\top I(w)^\dagger x_{i,\text{test}}$ subject to a budget $\sum_j w_j c_j\le B$ (Eq. 4). Optimization uses Frank–Wolfe updates $w_{t+1}=(1-\alpha_t)w_t+\alpha_t e_{j_t}$ with coordinate choice $j_t=\arg\max_j \big(-\nabla_{w_j}\hat L(w_t)/c_j\big)$ (Eq. 6), gradient $g_j=\frac{1}{m}\sum_i (x_{i,\text{test}}^\top I(w_t)^\dagger x_{j,\text{train}})^2$ (Eq. 7), and Sherman–Morrison rank-one updates for $I(w_t)^\dagger$ (Eq. 8).","Across multiple datasets, DAVED achieves the best (or near-best) reported test MSE compared to baselines such as Data Shapley, DVRL, LAVA, KNN Shapley, Influence, LOO, and Data-OOB. For example, in Table 1 the DAVED multi-step MSE is 0.16 (Gaussian 100K) versus 0.98–1.26 for several baselines, and 171.4 (RSNA) versus 215.6 (Data OOB) and much larger for others; it also improves Fitzpatrick (0.67) versus 1.35 (Data OOB). Under heterogeneous costs (Table 2), DAVED multi-step remains competitive and often best, e.g., Gaussian shows extremely low reported error (0.04 with $\sqrt c$ costs; 0.2 with $c^2$ costs). Runtime experiments (Figure 5) indicate DAVED is orders of magnitude faster than heavy retraining-based valuation methods (e.g., Data Shapley/LOO), and the single-step variant is the fastest among compared methods.","The authors note that the current federated algorithm “communicates every step,” and suggest reducing communication by integrating local steps (e.g., FedAvg or SCAFFOLD-style updates). They also state that integrating differential privacy techniques is a limitation/opportunity, to provide formal privacy guarantees to buyers and sellers.","The approach relies on a linearized (kernel/embedding) model assumption for label generation and for the proxy objective; if the embedding does not faithfully reflect the downstream model’s learning dynamics or if conditional $p(y\mid x)$ differs between sellers and buyers, selection may be miscalibrated. The proxy uses $I(w)^\dagger$ and can be numerically unstable or sensitive when the information matrix is ill-conditioned, especially in very high dimensions or with small effective sample sizes, requiring regularization choices that may be problem-dependent. The empirical evaluation focuses on MSE/regression-style objectives and does not fully explore classification losses, distribution shift between seller/train and buyer/test beyond the stated assumptions, or robustness to correlated/duplicated seller data typical in markets.","They propose (i) reducing communication by integrating local-update methods such as FedAvg or SCAFFOLD, and (ii) incorporating differential privacy to provide formal privacy guarantees for buyers and sellers.","Extending DAVED beyond the linear/OLS (or GLM) setting to better match deep fine-tuning objectives—e.g., using influence-function or second-order approximations that remain validation-free—could improve fidelity. Developing self-normalized or robust variants that handle seller-test covariate shift and heteroskedastic noise explicitly (including theoretical guarantees) would better match real data markets. Providing a production-ready open-source implementation and standardized benchmarks for data-market acquisition (including adversarial/strategic sellers) would strengthen practical adoption and comparative evaluation.",2403.13893v2,https://arxiv.org/pdf/2403.13893v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:33:39Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Prediction|Parameter estimation|Other,Other,Variable/General (design vector $d\in\mathbb{R}^{n_d}$; examples include 1D/2D designs and multi-sensor placements; parameters $\theta\in\mathbb{R}^{n_\theta}$ range from 1D to N-dimensional).,Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Not provided,https://uq.engin.umich.edu,"The paper proposes a Bayesian goal-oriented optimal experimental design (GO-OED) framework for nonlinear observation and prediction models, where the design maximizes expected information gain (EIG) on predictive quantities of interest (QoIs) rather than on parameters. It introduces a nested Monte Carlo estimator for QoI-EIG that uses MCMC (posterior sampling) and kernel density estimation (KDE) to approximate posterior-predictive densities and compute KL divergence from prior-predictive to posterior-predictive distributions. The design is optimized over the design space using Bayesian optimization to handle noisy objective evaluations and mixed/discrete-continuous design settings. The approach is demonstrated on 1D/2D/multidimensional nonlinear test problems and on a convection–diffusion sensor placement problem, showing that GO-OED can yield substantially different designs than conventional parameter-focused OED. The work positions GO-OED as a practical computational pipeline for predictive-focused design when QoIs depend nonlinearly on parameters.","Observation model: $y = G(\theta,d)+\epsilon$ with Bayesian update $p(\theta\mid y,d)=\frac{p(y\mid\theta,d)p(\theta)}{p(y\mid d)}$. Prediction/QoI model: $z=H(\theta,\eta)$ with prior- and posterior-predictive densities $p(z)=\int p(z\mid\theta)p(\theta)\,d\theta$ and $p(z\mid y,d)=\int p(z\mid\theta)p(\theta\mid y,d)\,d\theta$. Utility is QoI KL divergence $u_z(d,y)=D_{KL}(p(z\mid y,d)\|p(z))$ and expected utility/EIG is $U(d)=\mathbb{E}_{y\mid d}[D_{KL}(p(z\mid y,d)\|p(z))]=I(z;y\mid d)$, maximized via $d^*=\arg\max_{d\in\mathcal D}U(d)$.","Across benchmark nonlinear examples, the GO-OED estimator reproduces the expected trend of reference solutions when the QoI mapping is bijective (GO-OED EIG matches parameter EIG), and yields different optimal designs when the QoI is non-bijective. The paper shows sensitivity of EIG estimates to KDE bandwidth (small bandwidth tends to overestimate EIG; large bandwidth tends to underestimate), motivating adaptive bandwidth selection via cross-validation. In 2D design problems, Bayesian optimization efficiently concentrates evaluations near the optimal region while retaining exploration. In a convection–diffusion sensor placement application, GO-OED designs shift sensor locations toward regions informative for specific future QoIs (future corner concentrations or boundary flux), differing markedly from parameter-EIG designs.","The authors state that the approach depends heavily on MCMC and KDE: MCMC may still be slow for complex posterior manifolds and high-dimensional parameter spaces and can introduce high estimator variance. KDE approximation error can shift optimal design locations, and adaptive bandwidth tuning can still be biased and becomes expensive if frequent re-tuning is required (e.g., for every Monte Carlo sample).","The method’s practicality hinges on having an explicit likelihood (or a workable surrogate), yet many real experimental systems face model discrepancy, unknown noise models, or implicit likelihoods; performance under misspecification is not established. The nested MC + KDE approach can become computationally prohibitive for high-dimensional QoIs (KDE suffers from the curse of dimensionality), and the paper’s remedies are mostly suggested rather than demonstrated at scale. The Bayesian optimization layer may struggle as design dimension grows (e.g., many sensors), and constraints/identifiability issues in large combinatorial sensor-placement settings are not systematically treated (e.g., via specialized acquisition functions or discrete BO variants).","The authors suggest pursuing more efficient EIG estimation methods, including more sophisticated density estimators (Gaussian mixture models, transport maps, normalizing flows), deriving and optimizing bounds for EIG, and building density-ratio estimators that can accommodate implicit-likelihood settings.","Developing scalable alternatives to KDE for multivariate/high-dimensional QoIs (e.g., likelihood-free mutual information estimators, classifier-based density ratios, or variational MI bounds) and benchmarking them on genuinely high-dimensional predictive targets would strengthen applicability. Extending GO-OED to handle model discrepancy, correlated/heteroscedastic noise, and sequential multi-stage experiments (adaptive designs with updating priors between stages) would align the framework with common experimental workflows. Providing an open-source reference implementation with reproducible experiments and guidance on convergence diagnostics (for nested MC and MCMC) would improve adoption and allow systematic sensitivity analyses to BO hyperparameters and Monte Carlo budgets.",2403.18072v2,https://arxiv.org/pdf/2403.18072v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:34:16Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Model discrimination|Cost reduction|Other,Other,Variable/General (spatial well-location design; 1 well drilled per iteration with total budget 4 wells in case study),Environmental monitoring|Energy/utilities|Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes BEACON (Bayesian Experimental design Acceleration with COnditional Normalizing flows), a Bayesian optimal experimental design method for selecting monitoring well locations for CO2 sequestration plume tracking under uncertain reservoir properties. The design criterion is Expected Information Gain (EIG), defined as the expected KL divergence between the plume prior and posterior given well observations, and the method jointly optimizes (i) a conditional normalizing flow for amortized posterior inference and (ii) a probabilistic density over well locations (rather than fixed points) under a well-budget constraint. Training data are generated via fluid-flow simulation to propagate prior plume samples forward in time, then corrupted with noise to emulate observations from which sparse well measurements are sampled via a binary mask. In a synthetic, realistic case study using permeability samples derived from the Compass dataset, BEACON’s optimized well placements are compared against random placement over 4 sequential drilling iterations. Results show reduced posterior uncertainty and consistently lower RMSE of the inferred posterior mean relative to the synthetic ground truth compared with the random baseline, supporting the method’s scalability and performance for large-scale monitoring design.","The design objective is the Expected Information Gain: $\mathrm{EIG}(M)=\mathbb{E}_{p(y_k\mid M)}\big[ D_{\mathrm{KL}}(p(x_k\mid y_k)\,\|\,p(x_k))\big]$. The method performs a joint optimization over normalizing-flow parameters $\theta$ and well-location density parameters $w$ by maximizing the conditional flow log-likelihood (equivalently minimizing KL via maximum likelihood): $\n\hat\theta,\hat w=\arg\max_{\theta,w}\frac{1}{N}\sum_{i=1}^N\left(-\tfrac12\|f_\theta(x_k^{(i)};\,M(w)\odot y_k^{(i)})\|_2^2+\log|\det J_{f_\theta}|\right)$. A stochastic binary mask enforces the well budget $s$ via $M(w):=\mathbf{1}_s\,\frac{w}{w<u}$ with $u\sim\mathcal{U}(0,1)$, and observations are formed as $y_k^{(i)}=x_k^{(i)}+\varepsilon$ then subsampled by $M(w)\odot y_k^{(i)}$.","In the synthetic CO2 sequestration case study, the workflow drills one optimally placed well per iteration for 4 iterations total and compares against randomly selected well locations. Performance is evaluated by RMSE between the inferred posterior mean plume and the known synthetic ground truth, reported as normalized RMSE, and by qualitative/visual reductions in posterior variance. Across all four monitors/iterations in Figure 2, the optimized design yields consistently lower normalized RMSE than random placement and also produces lower posterior variance than having no wells and than random wells. The paper does not report exact numeric RMSE values in text, relying on the plotted results for quantitative comparison.",None stated.,"The validation is primarily a synthetic simulation study with comparison only to a random-placement baseline; stronger baselines (e.g., greedy EIG, classical sensor placement/optimal design heuristics, or gradient-based Bayesian OED methods) are not reported, limiting evidence of superiority beyond trivial baselines. The optimality criterion is EIG, but the paper does not quantify sensitivity to modeling assumptions (noise model, prior over permeability/plume, simulator mismatch) or to practical drilling/operational constraints (access, safety, minimum spacing), which can materially affect real deployments. Details on computational cost, convergence, and reproducibility (hyperparameters, training/optimization settings, and implementation) are limited, and code is not provided, making it difficult to assess scalability claims in practice. The approach treats well placement via a learned density and stochastic masking; the mapping from density to actionable discrete well coordinates and constraints is not fully characterized.",The authors state they will explore optimal seismic imaging acquisition for probabilistic seismic imaging workflows (they include seismic images as inputs but do not optimize seismic acquisition/placement in this work).,"Extend the Bayesian design to handle additional real-world constraints explicitly (e.g., forbidden zones, minimum well spacing, drilling/terrain costs) and report constrained optimal designs. Evaluate robustness under model misspecification (e.g., biased permeability priors, correlated noise, simulator errors) and consider likelihood-free or simulation-based calibration checks. Provide broader benchmarking against established Bayesian OED methods (e.g., stochastic gradient EIG estimators, mutual-information neural estimators) and against classical optimal sensor placement criteria. Release implementation and add ablation studies (flow architecture, mask parameterization, sequential update strategy) to clarify what drives gains and to support adoption.",2404.00075v1,https://arxiv.org/pdf/2404.00075v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:34:48Z
TRUE,Other,Other,Not applicable,3 factors (stimulus–question order/design condition with 5 levels; question modality with 2 levels but only used in one design; repetition of question/image varies by design),Other,Case study (real dataset)|Other,TRUE,R|None / Not applicable|Other,Supplementary material (Journal/Publisher),https://doi.org/10.18419/darus-3380,"This paper reports a controlled, counter-balanced within-subject eye-tracking experiment to compare five stimulus–question presentation designs for visual question answering (VQA) tasks: IQ, QI, QIQ, IQI, and AIA (audio modality). Using image–question pairs from the GQA dataset, the study measures cognitive load (NASA-TLX), task accuracy, and gaze allocation metrics (e.g., hit-any-AOI rate and fixation duration) to identify designs that reduce extraneous cognitive load. Results show the control condition IQ (image→question) yields significantly higher subjective cognitive load and significantly lower accuracy than the other designs, indicating that presenting the question before the image is generally preferable. The auditory AIA condition performs worse than comparable textual conditions (notably lower accuracy than QIQ), and gaze analyses suggest participants may attend less reliably to the question target under audio presentation. The work provides practical guidance for designing VQA-style user studies in visualization/HCI by demonstrating that ordering and modality can materially affect measured performance and gaze behavior.","NASA-TLX per design is computed as the sum of subscale ratings (mental demand, temporal demand, effort, frustration, and performance). Task accuracy is defined as correct answers divided by total questions per design. Hit-any-AOI rate (HAAR) is computed as the number of fixations landing in the target AOI divided by the total number of fixations for an image (then aggregated per participant/design, typically using the median).","NASA-TLX differed significantly across designs (Friedman test: χ²(4)=27.095, p<0.001; large effect w=0.616), with IQ having the highest cognitive load and being significantly higher than the other conditions in post-hoc comparisons. Accuracy also differed significantly (Friedman test: χ²(4)=32, p<0.001; large effect w=0.727); IQ performed significantly worse than other designs, and AIA (second-lowest accuracy) was significantly worse than QIQ (highest accuracy). HAAR differed significantly (Friedman test: χ²(4)=36.69, p<0.001; large effect w=0.77); IQ had the lowest HAAR and QIQ was significantly higher than all designs except IQI, while AIA’s HAAR was significantly lower than QIQ. Mean fixation duration showed a significant difference after log transform (ANOVA with sphericity corrections: F(2.52,27.74)=4.619, p=0.013; η²[g]=0.088), but post-hoc comparisons did not show significant differences between IQ and the other designs.","The images/questions are natural and varied, and saliency aspects (target size/location) varied; while task types were distributed relatively evenly, images were not evenly distributed across conditions. The study could not reliably measure pupil dilation (a more direct cognitive-load indicator) due to the nature of the stimuli, so it relied on indirect gaze measures (HAAR, mean fixation duration) plus NASA-TLX. The within-subject design may introduce learning effects favoring textual modality because AIA was the only auditory condition, potentially contributing to poorer AIA performance.","Because modality is confounded with design (only AIA is auditory and it also differs in repetition/sequence timing), the study cannot cleanly attribute differences to modality alone without an audio counterpart for other sequences (e.g., AI, A-Q formats) and matched exposure time. The sample size is small (12 analyzed participants) and drawn from a narrow age range, limiting generalizability and power for subtle effects. The analysis focuses on a specific VQA task set (GQA-derived) and a particular lab setup/eye tracker; results may differ with other task types, longer questions, different audio pacing, or mobile/remote eye tracking contexts.","The authors suggest extending the study beyond GQA-style natural image-question pairs to other visual tasks and to less natural, more controlled stimuli more typical in visualization research. They also propose exploring additional experimental designs, including further investigation of question modality; for example, creating a direct auditory competitor to QI (an AI design).","Introduce fully crossed factorial manipulations (order × modality × repetition/timing) so modality effects are not confounded with sequence structure and duration, and test adaptive/self-paced audio (replay control) to separate comprehension constraints from modality per se. Validate findings across larger and more diverse participant samples and across multiple VQA datasets/task taxonomies (e.g., counting, spatial relations, attribute queries) with controlled target saliency. Provide an open-source implementation pipeline (stimulus presentation + gaze processing) to support replication and to benchmark gaze metrics (HAAR, fixation duration) under different AOI-definition strategies (manual vs. model-based).",2404.04036v1,https://arxiv.org/pdf/2404.04036v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:35:25Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Prediction|Parameter estimation|Cost reduction|Other,G-optimal|Other,Variable/General (feature dimension d; also uses 768-d Instructor embeddings for some NLP tasks),Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Active In-context Prompt Design (AIPD), framing adaptive selection of few-shot in-context examples for LLM inference as an active learning/experimental design problem with a limited labeling/prompt budget. It introduces GO (a greedy G-optimal design procedure) that selects unlabeled training examples to minimize the worst-case posterior predictive variance over a set of test queries, using a linear-model/Bayesian optimal design proxy that balances uncertainty reduction and diversity. It also introduces SAL, a simulation-based active learning method that estimates uncertainty reduction by sampling from the LLM under hypothetical labeled additions, and proves SAL is equivalent to GO in linear Gaussian models for sufficiently many simulations. Empirically, across UCI/OpenML classification and regression, synthetic NLP tasks, and ARC/PCFG pattern tasks, GO and SAL generally outperform baselines such as uniform sampling, nearest-neighbor similarity, and entropy/disagreement-based selection. The work advances optimal-design-based prompting by providing both implementable black-box LLM procedures and theoretical guarantees in simplified linear settings.","GO selects the next example index via a greedy G-optimal criterion: $I_t = \arg\min_{i\in U_t}\max_{k\in[K]} x_{*,k}^\top(\hat\Sigma_t^{-1}+x_ix_i^\top)^{-1}x_{*,k}$, where $\hat\Sigma_t$ is the (Bayesian) posterior covariance proxy from a linear model. SAL replaces the closed-form variance proxy with simulation: for each candidate $x_i$, simulate labels and then two independent predictions per test point to estimate uncertainty as $\max_k \frac{1}{m}\sum_{j=1}^m(\tilde Y_{t,i,k}^{(j,1)}-\tilde Y_{t,i,k}^{(j,2)})^2$, selecting the $i$ minimizing this quantity. In linear Gaussian models, the simulated squared-difference estimator concentrates around $2(x_{*,k}^\top \Sigma_{t,i} x_{*,k}+\sigma^2)$, linking SAL to the G-optimal objective.","Theoretical results include (i) an $O(1/T)$-type decrease bound on posterior predictive variance for GO under structural assumptions on the pool relative to test points (Theorem 1), and (ii) concentration bounds showing SAL’s Monte Carlo uncertainty estimate approximates the linear-model variance proxy and yields equivalence to GO for sufficiently large $m$ (Theorem 2). In experiments typically using budget $T=5$ and $K=20$ test items per trial, GO and SAL are often the best or among the best methods across classification, regression, ARC, PCFG, and synthetic NLP tasks, outperforming Uniform, Greedy-NN, Least-disagreement, and Max-Entropy baselines in the majority of reported settings. SAL is implemented with approximations (preselecting 5 candidates via GO and using $m=1$) due to computational cost, yet still performs strongly in many tasks (e.g., ARC/PCFG table results show large reductions in 0-1 error vs baselines in several tasks).","SAL is computationally expensive because evaluating each candidate can require many LLM queries; the paper therefore uses approximations in experiments (GO as a preprocessing step and $m=1$) to make SAL practical. GO’s selection rule does not depend on observed labels in the linear-model proxy, which the authors note is undesirable in non-linear settings like LLMs and motivates SAL. Theoretical analysis is carried out under linear models with Gaussian noise and additional structural assumptions on the example pool (e.g., closeness partitions), which constrains the scope of formal guarantees.","The mapping from LLM behavior to the linear-Gaussian posterior covariance proxy is not formally validated; performance may depend heavily on prompt formatting, sampling temperature, and the stability of LLM output distributions. Experimental comparisons may be sensitive to implementation choices (e.g., SAL approximation with $m=1$ and GO-preselection) that could disadvantage or advantage certain baselines, and the computational budget in LLM calls is not normalized across methods. The approach assumes access to unlabeled pools and an oracle/human to provide labels, but does not model label noise or annotator disagreement explicitly, which can affect optimal-design criteria in practice.","The authors suggest extending the AIPD framework beyond text to tasks involving images, videos, or other modalities using multimodal LLMs. They also mention integrating active learning with diffusion models as a promising future research direction.","A natural extension is to develop self-starting/online variants that handle unknown or drifting test distributions and explicitly trade off LLM query cost vs labeling cost, including budgeted comparisons in terms of total LLM calls. Another direction is to relax independence/linear-Gaussian assumptions by incorporating robust or nonparametric uncertainty surrogates and studying performance under output stochasticity and prompt-induced correlations. Providing open-source implementations and standardized benchmarks (with controlled prompting and sampling settings) would improve reproducibility and enable fairer comparisons to alternative acquisition strategies (e.g., BALD-style mutual information, core-set methods using accessible embeddings).",2404.08846v2,https://arxiv.org/pdf/2404.08846v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:36:04Z
TRUE,Split-plot|Other,Parameter estimation|Screening|Cost reduction|Other,Not applicable,Variable/General (treatment indicator; design varies by number of PDX lines n and animals per line per arm m),Healthcare/medical|Other,Simulation study,TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=PDXpower,"The paper introduces PDXpower, an R package for simulation-based power analysis and sample size determination in pre-clinical patient-derived xenograft (PDX) studies under a mixed crossed/nested (mixed) design. It targets time-to-event outcomes that may be uncensored or right-censored (Type I administrative censoring), and it simultaneously determines the needed number of PDX lines (clusters) and animals per line per treatment arm. For uncensored outcomes it uses a mixed-effects ANOVA model with a PDX-line random effect to capture inter-line heterogeneity and intra-line correlation; for censored outcomes it uses a Cox frailty model with a normal random effect and Weibull baseline hazard. Performance is evaluated via Monte Carlo simulations across grids of (n,m), reporting type I error control and power for detecting a treatment effect, and the package provides workflows both with pilot data (estimate variance components/model parameters first) and without pilot data (parameter elicitation via medians/ICC or Weibull assumptions).","Mixed-effects ANOVA for uncensored outcomes: $\log Y_{ij}=\beta_0 + D_{ij}\beta + \alpha_i + \epsilon_{ij}$ with $\alpha_i\sim N(0,\tau^2)$ and $\epsilon_{ij}\sim N(0,\sigma^2)$, testing $H_0:\beta=0$ via a Wald test. Cox frailty model for right-censored outcomes: $\lambda_{ij}(t)=\lambda_0(t\mid\lambda,\nu)\exp\{D_{ij}\beta+\alpha_i\}$ with Weibull baseline hazard parameters $(\lambda,\nu)$ and $\alpha_i\sim N(0,\tau^2)$; Type I censoring at common administrative time $C$ uses $(\tilde Y_{ij},\delta_{ij})=(\min\{Y_{ij},C\}, I(Y_{ij}\le C))$. Power is estimated as the Monte Carlo proportion of simulated datasets rejecting $H_0$ over grids of PDX lines $n$ and animals per arm per line $m$.","In simulation studies over $n\in\{3,\dots,10\}$ and $m\in\{3,\dots,8\}$ with 2,000 Monte Carlo replicates, Monte Carlo type I error for the Wald test is close to the nominal 0.05 for both the mixed-effects ANOVA model and the Cox frailty model. Power increases as either the number of PDX lines $n$ or animals per line per arm $m$ increases; example tables illustrate design combinations achieving ~80% power at 5% significance (e.g., for the ANOVA-with-pilot example: (n,m) ≈ (3,4), (4,3), or (6,2); for the frailty-with-pilot example: (3,8), (4,6), (5,5), (6,4), or (7,3)). The authors also report computational timing: ~0.5 minutes for 500 replicates under mixed-effects ANOVA versus ~20 minutes for Cox frailty power curves on an 8-core M1 Pro MacBook Pro, motivating coarser grids or alternative models for speed.","The authors note that power calculations for censored outcomes using Cox’s frailty model can be computationally intensive (e.g., ~20 minutes for 500 Monte Carlo samples for their Figure 5), and results may be obtained faster by using a coarser (n,m) grid with fewer combinations. They also state the censored-outcome module is tailored specifically to Type I (administrative) censoring, which is typical in PDX experiments but limits direct applicability to other censoring mechanisms. They remark that users need basic R knowledge, motivating future development of an R Shiny interface.","The approach relies on correctly specified working models (log-normal mixed-effects ANOVA for uncensored times; Weibull-baseline Cox frailty with normal random effects for censored times); misspecification (non-Weibull hazards, non-normal random effects, time-varying treatment effects, informative censoring) could materially affect power estimates. The paper emphasizes a two-arm setting (control vs treatment) for exposition; extensions to multiple treatments, dose-response, or additional covariates/interactions may require careful adaptation and may change the effective design/DF and power. Reported operating characteristics are primarily simulation-based and do not include extensive real-data validation across varied tumor types/labs or sensitivity analyses over a wider range of heterogeneity/correlation structures and allocation ratios.","They propose exploring alternative working models to improve computational efficiency for censored outcomes, including other frailty distributions (e.g., gamma frailty) and standard fixed-effects Cox models treating PDX as fixed effects, noting further investigation is warranted. They also plan to build interactive web applications (R Shiny) on top of PDXpower so users without R knowledge can perform the power analysis workflows.","Add sensitivity-analysis utilities that vary key nuisance parameters (ICC/\tau^2, \sigma^2, baseline hazard shape, censoring time) to show robustness of recommended (n,m) pairs under uncertainty in pilot estimates. Extend the framework to multi-arm and factorial treatment structures (e.g., combination therapies) and to longitudinal tumor volume endpoints, which are common in PDX studies and would better align with broader preclinical design needs. Provide faster approximations or surrogate models (e.g., emulators for the power surface over (n,m)) and publish reproducible benchmarks plus example scripts/vignettes for common PDX design scenarios.",2404.08927v1,https://arxiv.org/pdf/2404.08927v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:36:43Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Cost reduction|Other,Other,Variable/General (examples include 1 design variable; 400-dimensional design vector; selecting k measurement times),Manufacturing (general)|Transportation/logistics|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a variational Bayesian optimal experimental design method (vOED-NFs) that maximizes expected information gain (EIG) in model parameters using a Barber–Agakov (BA) lower bound, avoiding explicit likelihood evaluations and thus supporting implicit-likelihood models. It represents variational posteriors with conditional normalizing flows (coupling-layer conditional invertible neural networks) and optionally uses a summary network (e.g., LSTM) for high-dimensional observations. The authors derive Monte Carlo estimators and stochastic gradient expressions enabling simultaneous gradient-based optimization of both variational parameters and design variables. The approach is validated on benchmark OED problems, a PDE-governed cathodic electrophoretic deposition (e-coating) experiment design problem, and an aphid population model with intractable likelihood, showing reduced EIG-estimation bias and improved posterior approximation versus simpler variational families. Empirically, using about 4–5 coupling-layer compositions yields better EIG lower-bound tightness under fixed forward-model budgets and captures non-Gaussian and multi-modal posteriors well.","Bayesian OED objective uses expected information gain (mutual information) $U(d)=\mathbb{E}_{Y|d}[\mathrm{KL}(p(\theta|y,d)\|p(\theta))]=\iint p(\theta,y|d)\log\frac{p(\theta|y,d)}{p(\theta)}\,d\theta dy$. vOED replaces the posterior with a variational approximation and maximizes the BA lower bound $U_L(d;\lambda)=\mathbb{E}_{\Theta,Y|d}[\log q(\theta|y;\lambda)-\log p(\theta)]$, estimated by MC $\hat U_L=\frac1N\sum_i(\log q(\theta^{(i)}|y^{(i)};\lambda)-\log p(\theta^{(i)}))$, and optimized jointly over $(d,\lambda)$ via stochastic gradients. Normalizing flows parameterize $q$ using invertible maps with change-of-variables $\log q(x;\lambda)=\log p_Z(f(x;\lambda)) + \sum_i \log\left|\det \partial f_i/\partial x\right|$, implemented with coupling layers (RealNVP-style) and conditioning on $y$ (cINN).","Across examples, vOED-NFs produces EIG lower-bound estimates that track high-quality nested Monte Carlo (NMC) much better than Gaussian variational families, and it avoids explicit likelihood evaluations for implicit-likelihood settings. In the low-dimensional nonlinear case, vOED-NFs closely matches the high-quality NMC EIG curve while vOED with Gaussian variational posteriors misidentifies the optimal design. In the high-dimensional linear design case (20 parameters, 400-dim design), vOED-NFs achieves a much tighter bound (reported $\hat U_L\approx 20.94\,(0.19)$) than a Gaussian/Gamma variational baseline ($\approx 12.07\,(0.11)$), and yields slightly higher NMC-estimated EIG at the found design ($\approx 24.91\,(0.24)$ vs $24.28\,(0.24)$). The paper reports that using ~4–5 coupling-layer compositions typically stabilizes improvements in EIG estimation bias/tightness under fixed forward-model budgets, and posterior samples from vOED-NFs agree well with SMC/ABC references, including multi-modal marginals.","The authors note a lack of analytical results linking bound estimator quality (tightness) to the quality of the resulting design optimization; non-tight but consistently biased bounds across the design space could still yield good designs, but this is not characterized. They also state that high-dimensional parameter settings may require complex normalizing-flow architectures with many variational parameters, making optimization challenging and susceptible to local minima. Finally, despite improved sampling efficiency relative to NMC, the method can still require a non-trivial number of forward-model runs to tighten bounds and obtain high-quality posteriors.","The method’s performance depends heavily on neural-network architecture and hyperparameter tuning (e.g., number of coupling layers, learning rates, batch sizes), but there is no systematic robustness/sensitivity analysis or automated model-selection strategy for practice. Comparisons are selective: while NMC is used as a reference where feasible, broader benchmarking against other modern implicit-likelihood OED approaches (e.g., sequential neural MI estimators, classifier-based ratio estimation, diffusion/flow-based SBI design loops) is limited. Practical deployment details (software release, reproducibility assets, computational scaling with expensive simulators, and guidance on diagnosing poor variational fits/bound gaps) are not provided.","They propose exploring more efficient and scalable transport-map architectures for vOED, specifically mentioning rectification operators and probability flow ODEs. They also suggest extending vOED-NFs to other OED structures including goal-oriented OED and sequential OED. More broadly, they highlight the need to improve sampling efficiency of bound-based EIG strategies so estimator variance remains smaller than bound gap (bias).","Develop principled diagnostics and stopping criteria that quantify bound tightness and posterior approximation quality during design optimization, including cross-validation schemes to mitigate overfitting of $q(\theta|y;\lambda)$ to finite simulation pools. Extend the framework to handle common experimental constraints more explicitly (e.g., discrete/categorical design choices, batch experiments, cost/feasibility constraints) and to incorporate model misspecification/robust design objectives beyond pure EIG. Provide open-source implementations and standardized benchmarks (including expensive PDE simulators and implicit models) to assess computational scaling and reproducibility, and investigate variance-reduction/control-variate strategies for the BA-bound gradient estimators.",2404.13056v2,https://arxiv.org/pdf/2404.13056v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:37:27Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction|Other,D-optimal,9 parameters (battery model parameters estimated); input dimension examples: 25 decision variables (24 current jumps + initial voltage) and a concatenated setting with 6 current “jumps” per interval.,Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python,Not provided,NA,"The paper develops optimal input (experimental) design methods to improve parameter estimation for a lithium-ion battery Single Particle Model (SPM) governed by PDEs, framing design as a multi-PDE-constrained optimization problem. It uses an adaptive/sequential scheme that iteratively (i) optimizes a new current profile (and, in one variant, starting voltage) to maximize parameter observability, (ii) generates/stores voltage response data, and (iii) re-estimates parameters by constrained least squares using all collected experiments. The design objective is quantified through an approximate Fisher information matrix built from finite-difference sensitivities, and the design criterion is D-optimality (minimizing −log det of the summed information matrices), with regularization and an additional penalty to force diversity among multiple designed inputs. Two experimental-design strategies are studied: a collection of multiple short independent experiments (which can cause very long lab time due to SOC preparation/rest) and a concatenated design built from successive optimized intervals with imposed rest phases to reduce total experimental time. Performance is demonstrated on virtual (noise-free) data and then assessed on real battery-cell measurements; the concatenated strategy reduces experiment time substantially but can suffer accuracy degradation on real data compared with the long full protocol, highlighting model–reality mismatch and practical constraints.","The design uses a D-optimality criterion based on an approximate Fisher information matrix. Sensitivities are approximated by finite differences: $s^j_{\mu,u} \approx (v_{\mu+\nu e_j,u}-v_{\mu,u})/\nu$ with $\nu=10^{-3}$, and the information matrix is formed by trapezoidal integration of sensitivity inner products, $(M)_{j\ell}=\sum_{k=0}^{K-1} \frac{s^j_k s^\ell_k+s^j_{k+1}s^\ell_{k+1}}{2}(t_{k+1}-t_k)$. The optimized input array $u$ minimizes $\Phi(u)=-\log_{10}\det\big(\sum_{m=1}^{n-1}M^{[m]}_{\mu_{n-1}}+M^u_{\mu_{n-1}}\big)+\gamma\|u\|_2^2$ (with optional diversity penalty) subject to box constraints on current levels and initial voltage.","In virtual (noise-free) experiments, both DOE strategies enable highly accurate recovery of the 9 target parameters; reported relative parameter errors reach approximately $3.73\times 10^{-10}$ (collection-of-inputs setting) and $9.74\times 10^{-12}$ (concatenated setting). The concatenated design yields a designed experiment of 6,480 s (9 intervals of 120 s plus rests; 108 minutes) versus much longer effective lab time when executing many separate short experiments with SOC preparation/rest (reported 63,362 s total for the “full data” collection test). On real battery data, the short “cut” collection-of-inputs protocol (600 s) fails to identify parameters reliably, whereas using full data with preparation/rest (63,362 s) produces a well-fitting parameter set; the concatenated approach can be robustened by including preparation/rest data (14,199 s) to produce parameters with comparable fit quality while being >4× shorter than the longest test. The paper also tracks a Hessian conditioning metric $\beta=\lambda_{\max}(H)/\lambda_{\min}(H)$, showing improved conditioning across iterations, consistent with easier/less local-minima-prone estimation.","The authors note that the stopping rule for the adaptive design may not guarantee convergence and could generate an infinite number of inputs, so they impose a maximum iteration safeguard in practice. They also acknowledge that in real experiments the simplified model (SPM with only 9 tuned parameters and other parameters fixed) cannot perfectly reproduce measured battery data, and that accuracy may degrade substantially for the concatenated design when moving from virtual to real data. Additionally, they state that their chosen input shapes/lengths (numbers of jumps, pause lengths, total duration) are somewhat arbitrary/intuitive, and that different choices (e.g., longer jumps/pauses/experiments) might yield better fits.","The DOE criterion is based on finite-difference sensitivities and a local (around current estimate) Fisher-information approximation; this can be brittle for strongly nonlinear, nonconvex inverse problems and may yield designs that are suboptimal or misleading under model mismatch or noise. The method largely assumes independence/adequacy of the voltage-error structure (e.g., no explicit colored-noise or autocorrelation model), yet real battery data are temporally correlated and affected by hysteresis/temperature drift, potentially invalidating the information-matrix interpretation. Comparisons are not positioned against a broad set of alternative battery OED methods (e.g., Bayesian/robust OED, global-sensitivity-driven designs, multiobjective designs incorporating safety/constraints), so the incremental benefit relative to state-of-the-art could be hard to gauge. Practical implementability may be limited by constraints not modeled in the optimization (current slew-rate limits, hardware constraints, safety limits at high/low SOC, thermal constraints), which can materially affect realizable designs.","They encourage future work to obtain better results by generalizing the experimental design choices (e.g., using longer current jumps, longer pauses, or longer experiments) and by potentially tuning additional model parameters that were fixed (e.g., more Redlich–Kister parameters) to improve fit to real data. They also suggest the approach may be even more effective for more complex battery models where the electrochemical dynamics are better captured and the numerical model can more completely reproduce real measurements. More broadly, they point to incorporating explicit resource management (time/energy) a priori in DOE to prevent waste.","Develop robust/Bayesian OED variants that explicitly account for model discrepancy (SPM vs real cell), measurement noise, and uncertainty in nuisance parameters, potentially using expected information gain or minimax criteria. Extend the input parameterization to include realistic actuator constraints (slew-rate, smoothness), thermal/SOC safety constraints, and multiobjective trade-offs (identifiability vs degradation/aging vs time). Validate across multiple cells/chemistries and operating temperatures to quantify generalization and repeatability, and add benchmark comparisons to established battery excitation protocols and contemporary OED approaches. Provide open-source implementations and reproducible experiment scripts (including data preprocessing for preparation/rest phases) to facilitate adoption and fair comparative evaluation.",2404.15797v1,https://arxiv.org/pdf/2404.15797v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:38:00Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Screening|Optimization|Prediction|Other,Other,"Variable/General (examples include 60D rover path finding, 12D lunar lander; also discrete pools such as 1000 MOFs)",Pharmaceutical|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/vertaix/Quality-Weighted-Vendi-Score,"This paper proposes quality-weighted Vendi scores (qVS), extending the Vendi Score diversity metric by multiplying the Vendi diversity score of a selected set by the set’s average quality score, with an additional order parameter q to tune the quality–diversity trade-off. The authors use qVS as an objective for diverse experimental design in sequential decision-making settings, focusing on two canonical DOE/ED problems: diverse active search (rare-target discovery in a finite pool) and diverse Bayesian optimization (finding multiple high-performing, diverse solutions). For optimization of qVS over candidate sets, they use gradient-based multi-start optimization in continuous domains when gradients are available, and a sequential greedy heuristic for discrete/black-box settings and batch selection. Empirically across applications (molecular/photoswitch discovery, bulk metallic glass discovery, FashionMNIST recommendation, rover path planning, lunar lander policy optimization, and MOF property optimization), qVS-based policies produce more diverse high-quality batches and report large gains in “effective discoveries” versus baselines. The framework is positioned as a flexible alternative to diversity via hard distance constraints (e.g., local penalization/ROBOT), replacing them with a single tunable diversity order q.","The Vendi Score is defined as $\mathrm{VS}(X;k)=\exp\left(-\sum_{i=1}^n \lambda_i\log\lambda_i\right)$ where $\{\lambda_i\}$ are normalized eigenvalues of the kernel similarity matrix $K$ for set $X$. The proposed quality-weighted score is $\mathrm{qVS}(X;k,s)=\left(\frac{1}{n}\sum_{i=1}^n s(x_i)\right)\mathrm{VS}(X;k)$, and the order-$q$ generalization uses a Rényi-entropy form $\mathrm{VS}_q(X;k)=\exp\left(\frac{1}{1-q}\log\sum_i \lambda_i^q\right)$ with appropriate limits for $q\in\{0,1,\infty\}$. Batch selection is posed as maximizing qVS over subsets (combinatorial in discrete spaces) and approximated via a sequential greedy marginal-gain rule (adding the next $x$ that most increases qVS).","Across multiple active-search datasets (photoswitch molecules, bulk metallic glasses, FashionMNIST), qVS-AS achieves the highest or near-highest Vendi Scores under many evaluation orders q, often outperforming tuned diversity-aware baselines (ECI/SELECT) and random search on diversity metrics. For Bayesian optimization, qVS-BayesOpt (often with $q=1$) is competitive with or better than TuRBO and ROBOT on continuous tasks (rover path finding, lunar lander), and it improves performance in the discrete MOF search where TuRBO/ROBOT are not applicable. The paper reports that qVS-based algorithms yield roughly a 70%–170% increase in the number of “effective discoveries” compared to baselines. In the MOF task, varying q changes best-found storage values (e.g., Table 4 shows best storage around 22.07 for $q=0.1$ vs 20.47 for $q=0$), illustrating sensitivity to the diversity-order hyperparameter.","The authors do not provide theoretical approximation guarantees for the greedy optimization strategy when applied to qVS, noting they only empirically observe it works well and is efficient at scale. They also note that if the domain geometry is well understood and hard distance constraints are desired, constraint-based methods like ROBOT may be preferable because qVS does not explicitly enforce those constraints. Additionally, they acknowledge performance depends on setting the order parameter q appropriately, and some q values can perform worse than standard (diversity-blind) BayesOpt.","The method’s practical effectiveness depends on the choice of similarity/kernel function $k$ (or distance-to-similarity mapping), but guidance for kernel selection and robustness to kernel misspecification is limited; poor kernels could yield misleading diversity scores. qVS uses a simple average-quality multiplier, which can be sensitive to the scale/calibration of the quality scores (e.g., UCB/probabilities/objective values) and may require normalization to compare across iterations or tasks. The empirical evaluation is broad but primarily simulation/benchmark driven; there is limited evidence of end-to-end wet-lab or real operational deployment where measurement noise, batch constraints, and experimental failures matter. Computational cost of repeated eigenvalue computations for VS/VS_q in large batch sizes or large candidate sets may be significant; scalability details (e.g., low-rank updates/approximations) are not emphasized.","They suggest dynamically setting or adapting the order parameter q based on search progress, motivated by observations that some q values help while others hurt (e.g., in the MOF task).","Developing provable approximation guarantees (or identifying conditions such as submodularity/approximate submodularity) for greedy maximization of qVS would strengthen the method’s theoretical footing. Exploring robustness to model misspecification (miscalibrated probabilities in active search, noisy objectives, correlated observations) and extending to constrained/feasible-set optimization would improve real-world applicability. Providing scalable implementations (e.g., incremental eigenvalue updates, randomized low-rank approximations, or differentiable surrogate bounds) and releasing a polished library/API for batch DOE would facilitate adoption. Extending qVS objectives to multi-objective settings (quality as a vector), contextual factors, and explicit cost-aware designs would align with many practical DOE scenarios.",2405.02449v1,https://arxiv.org/pdf/2405.02449v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:38:37Z
TRUE,Optimal design|Computer experiment|Other,Optimization|Parameter estimation|Prediction|Cost reduction,Other,Variable/General (theory for dimension-truncated/infinite-dimensional parameters); numerical example uses 100 parameters (s=100) and 3 sensors (k=3),Environmental monitoring|Other|Theoretical/simulation only,Approximation methods|Simulation study|Other,TRUE,None / Not applicable,Not provided,https://web.maths.unsw.edu.au/~fkuo/lattice/,"The paper develops quasi-Monte Carlo (QMC) methods and convergence theory for Bayesian optimal experimental design (BOED) problems governed by parametric PDEs, where the BOED objective is the expected information gain (EIG) defined via the Kullback–Leibler divergence between prior and posterior. The key computational challenge is a nested (double) high-dimensional integral over parameters and data; the authors establish parametric and mixed regularity bounds for the inner likelihood integral and the outer data integral/logarithm term. They analyze two QMC strategies: a full tensor product of QMC rules for the parameter and data domains, and a sparse tensor (Smolyak-type) combination of QMC rules that substantially improves the overall rate for the nested expectation. The sparse tensor approach is shown to recover convergence behavior comparable to a single high-dimensional QMC integral (up to logarithmic factors), while remaining robust with respect to the parameter dimension under summability assumptions. Numerical experiments on a 2D elliptic PDE with an unknown diffusion coefficient and sensor-placement design confirm the predicted rates and demonstrate near O(N^{-1}) behavior for the sparse tensor method versus near O(N^{-1/2}) for the naive full tensor approach (for first-order outer cubature), with further improvements under a periodic reparameterization enabling higher-order lattice convergence.","The DOE objective is the expected information gain (EIG) for design \(\xi\): \(\mathrm{EIG}(\xi)=\int\!\int \log\big(\pi(\theta\mid y,\xi)/\pi(\theta)\big)\,\pi(\theta\mid y,\xi)\,\pi(y\mid\xi)\,d\theta\,dy\), equivalently involving the nested integral \(\int \log\big(\int f(\theta,y;\xi)d\theta\big)\,\big(\int f(\theta,y;\xi)d\theta\big)\,dy\) with \(f(\theta,y;\xi)=C\exp\{-\tfrac12\|y-G_s(\theta,\xi)\|^2_{\Gamma^{-1}}\}\). QMC approximation uses randomly shifted rank-1 lattice rules \(Q_{n,R}(F)=\frac1R\sum_{r=1}^R\frac1n\sum_{i=1}^n F(\{t_i+\Delta^{(r)}\})\), and the sparse tensor estimator combines difference operators \(\Delta^{(1)}_{\ell_1}\Delta^{(2)}_{\ell_2}\) over levels satisfying \(\ell_1+\ell_2\le L\) (or generalized \(\sigma\ell_1+\ell_2/\sigma\le L\)).","Theory: for the inner parameter integral, dimension-independent QMC rates are obtained under a summability condition \(b\in \ell^p\) (with \(p\in(0,1)\)) on derivative growth, enabling lattice-rule RMS errors scaling like \(n^{\max\{-1/p+1/2,-1+\delta\}}\) with suitable weights. For the nested EIG, the full tensor product QMC leads to a reduced overall rate (roughly halved due to nesting), while the proposed sparse tensor product yields RMS error \(\lesssim (2^L)^{-1+\delta}(L+1)\) (up to logs) and constant independent of parameter dimension \(s\) when CBC-constructed weights are used. Numerically on a 2D elliptic PDE with \(s=100\) parameters and \(k=3\) sensors, the full tensor product shows convergence close to \(O(N^{-1/2})\) in total work \(N\), whereas the sparse tensor product is nearly \(O(N^{-1})\) for the non-periodic setting. Under a periodic parameterization with higher-order outer cubature, observed decay is close to \(O(N^{-1})\) for full tensor and roughly \(O(N^{-2})\) for sparse tensor (in the reported asymptotic fits).","The authors note that while they obtain parameter-dimension-independent behavior under assumptions, dependence on the data dimension \(k\) and the noise covariance \(\Gamma\) can be important, especially for informative or sequential data collection. They also remark that balancing the truncation/cut-off parameter \(K\) for the data domain with the number of QMC points is not addressed in the present analysis and would require a more refined study.","The work focuses on accurate evaluation of EIG for candidate designs; it does not address the computational complexity of optimizing over large/continuous design spaces beyond the small discrete sensor-placement example, so end-to-end BOED scalability is unclear. The approach relies on strong parametric regularity (analytic-type derivative bounds) and (effectively) independence structure compatible with QMC; performance for non-smooth models, discontinuous observations, or strongly nonlinear/non-Gaussian likelihoods may degrade. Numerical experiments are limited to a single elliptic PDE and do not explore robustness to discretization error, solver tolerances, or model misspecification (e.g., correlated noise, model error), which can materially affect EIG-based design in practice.","They propose extending the analysis beyond the presented elliptic model problem to a broader class of forward problems. They also plan to investigate methods (e.g., preconditioner-based techniques) to mitigate the dependence of convergence/performance on data dimension and noise covariance, and they mention that balancing the data-domain truncation parameter \(K\) against the number of QMC points is left for future work.","Developing gradient-based or surrogate-assisted optimization algorithms that exploit the proposed sparse-tensor/QMC estimators would make the BOED loop practical for large or continuous design spaces. Extending the framework to sequential/adaptive BOED (multi-stage designs) and to richer observation models (correlated/heteroscedastic noise, non-Gaussian likelihoods) would broaden applicability. Providing open-source implementations (including CBC weight construction and sparse-tensor estimators) and benchmarking against alternative EIG estimators (multilevel MC/QMC, Laplace/transport-map surrogates, neural surrogates) on multiple PDE problems would strengthen empirical validation.",2405.03529v2,https://arxiv.org/pdf/2405.03529v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:39:21Z
TRUE,Optimal design|Sequential/adaptive,Model discrimination|Optimization|Parameter estimation|Prediction|Cost reduction,Not applicable,"Variable/General (case study uses ~5 key design parameters: initial temperature T0, step durations τn (n=2–4), in-line mixer speeds Nn (n=2,4))",Manufacturing (general)|Food/agriculture|Pharmaceutical,Simulation study|Other,TRUE,Python|Julia|Other,Not provided,http://arxiv.org/abs/2304.01117|http://arxiv.org/abs/2301.11356,"The paper proposes an iterative digital framework that integrates knowledge-guided symbolic regression (SR) with model-based design of experiments (MBDoE) to automate discovery of mechanistic rate expressions and optimize a process flow diagram (PFD) for formulated product manufacturing. At each iteration, SR (implemented via PySR) produces a Pareto front of interpretable candidate kinetic expressions for intrinsic dynamics dX/dt = f(X,C), and MBDoE selects the next experiment to discriminate between candidates and, optionally, to optimize the PFD simultaneously. MBDoE is formulated as a multiobjective optimization over recipe/PFD parameters ϑ that balances expected information gain (via variance in predicted final KPI across candidate models) against process optimality (batch time and KPI target satisfaction), with an adaptive exploration–exploitation weight α updated from observed gains. The approach is evaluated using in-silico experiments from a newly constructed mechanistic simulator of recycle emulsification with multi-step operation and lumped phase kinetics, demonstrating recovery of ground-truth mechanisms in few iterations and substantial batch-time reduction while meeting a target KPI. The work advances MBDoE practice by coupling it tightly to iterative symbolic model discovery under structural physical constraints, emphasizing that new experiments—not parsimony criteria alone—are needed to reliably discriminate mechanistic hypotheses.","The intrinsic kinetics are modeled as dX/dt = f(X,C) with KPI mapping ψ = h(X); SR searches candidate analytic expressions for the reaction rates r1,r2,r3 as functions of phase concentrations X and operating conditions C=[T,γ̇]. MBDoE chooses the next experiment by minimizing a multiobjective: J(ϑ)=α·JM(ϑ)/JM,max + (1−α)·JP(ϑ)/JP,max subject to bounds on ϑ, where JM is an information gain term defined as negative variance of predicted final KPI across the top candidate models (e.g., JM = −var([ψ̂1,…,ψ̂NS])) and JP penalizes batch time τ and deviation from the KPI target within tolerance. The exploration–exploitation weight is updated as α = ΔJM/(ΔJM+ΔJP), using realized information and optimality gains from the previous iteration.","In Case Study 1 (knowledge discovery), the prediction MAPE for the final product KPI dropped rapidly with iterative MBDoE: from ~3.9% initially to ~0.14% by iteration 3 when rebuilding expressions from scratch (1A), and to ~0.30% by iteration 4 when carrying expressions forward between iterations (1B). Lifting the knowledge-guided structural constraint (Case Study 1C) substantially worsened early performance, with MAPE decreasing from 25.6% to ~1.98% and plateauing after ~7 iterations, showing the benefit of physics-guided constraints under small data. In Case Study 2 (simultaneous discrimination and optimization), the total batch time was reduced from 100 min (iteration 0) to 59 min while achieving a final KPI of 6.79 within a ±3% tolerance of the target 7, and α automatically transitioned toward near-pure optimization once information gains became small (e.g., α≈0.00085 by iteration 7). Overall, the in-silico results indicate the SR-MBDoE loop can recover the ground-truth mechanistic structure in a small number of designed experiments while improving process performance.","The authors note that selecting expressions based on parsimony/complexity criteria can bias selection toward overly simple approximations even when a more complex expression is the ground truth; only additional carefully designed experiments can reliably discriminate among similarly fitting candidates. They also emphasize that without constraining SR with prior physical knowledge, discovering accurate mechanisms for complex systems is very challenging and may yield only local approximations with reduced interpretability. The framework’s demonstration is performed on a constructed in-silico process model to test the methodology.","The study’s performance evidence is largely simulation-based using a synthetic mechanistic model that matches the assumed SR search operators and structural constraints; real experimental noise, unmodeled disturbances, and measurement constraints (e.g., limited access to phase concentrations or local shear/temperature) may reduce identifiability and discrimination power. The MBDoE information objective uses variance of predicted final KPI across a small set of top models, which may not align with classical optimal design criteria (e.g., Fisher information-based D/A/I-optimality) or guarantee parameter identifiability for all kinetics. Computational cost/scalability could be significant (multiple SR populations and Bayesian optimization over a multi-step recipe), and the effect of autocorrelation/time-series dependence and unknown parameters in Phase I initialization is not deeply explored. Comparisons against established MBDoE/active-learning baselines for kinetic discovery (e.g., Bayesian model discrimination with posterior probabilities) are not provided.","The authors suggest improving termination of MBDoE by introducing a stopping criterion based on the adaptive exploration–exploitation weight α falling below a threshold (e.g., αt≈1×10−3), indicating that further experiments provide little information gain and the final experiment should focus on optimization. They also state that the constrained expression-tree approach can be restricted to other structural forms when different prior physical knowledge is believed, implying extension to alternative mechanistic hypotheses or partially known structures.","Validate the SR-MBDoE loop on real formulated-product manufacturing datasets with realistic measurement limitations and experimental noise, including robustness to biased/uncertain sensors and batch-to-batch variability. Extend the information objective to Bayesian model discrimination (posterior model probabilities) and/or Fisher information-based criteria to better connect to standard optimality measures and ensure parameter identifiability. Develop self-starting/online variants that handle unknown initial conditions and allow adaptive control during a batch (closed-loop experimental design). Provide open-source reference implementations and benchmarks, and test scalability to higher-dimensional PFD decision spaces, multivariate KPIs, and constrained/expensive experiments with safety and operability constraints.",2405.04592v1,https://arxiv.org/pdf/2405.04592v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:39:58Z
TRUE,Sequential/adaptive|Computer experiment|Other,Prediction|Parameter estimation|Robustness|Cost reduction,Not applicable,Variable/General (design space $\Omega\subset\mathbb{R}^d$; examples mostly 2D),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a gradient-based adaptive design of experiments (DoE) / adaptive sampling methodology aimed at building accurate metamodels from limited, potentially noisy, and often unreplicated experiments. The next sample point is chosen by maximizing a heuristic objective $S(x)=Q_L Q_S Q_E$ combining (i) estimated nonlinearity via the metamodel Laplacian, (ii) a space-filling/anti-clumping spacing term based on an RBF-style distance penalty, and (iii) a leave-one-out (LOO) cross-validation error term interpolated over the design space. The metamodel is built using HOLMES (higher-order local maximum entropy) approximants, augmented with a boundary-corrected kernel density estimation (KDE) scheme to adapt local kernel width/spacing and improve robustness on clumped datasets. Through 2D benchmark functions, the method generally reduces interpolation error faster than full factorial and Latin hypercube sampling for the same number of samples, and it is shown to be more noise-resistant than comparable RBF-based adaptive sampling in the unreplicated/noisy setting. The authors also demonstrate a variant for time-dependent functions by projecting existing samples forward in time and then applying the same adaptive sampling logic, suggesting potential extensions to adaptive resolution/PDE-related contexts.","The adaptive sampling rule selects the next point by $x_{a+1}=\arg\max_{x\in\Omega} S(x)$ with $S(x)=Q_L Q_S Q_E$. Nonlinearity is quantified by the metamodel curvature $s(x)=|\nabla^2 u_I(x)|$ and normalized to $Q_L=s_n(x)$. Spacing uses an RBF-based function $Q_S=(1-H(x))^2$ where $H(x)=\sum_{a=1}^{N_a}\nu_a\exp(-\xi\|x-x_a\|^2)$ and $\xi$ is set via a support radius relationship (e.g., $\xi=-\log(\mathrm{tol})/R_{\mathrm{supp}}^2$, $R_{\mathrm{supp}}\propto d_{\mathrm{fill}}$). Error uses leave-one-out CV $\epsilon_{\mathrm{LOO}}(x^*)=|u(x^*)-u_I^*(x^*)|$ normalized and interpolated to define $Q_E(x)=\epsilon_{\mathrm{LOO},n}(x)=\sum_{a=1}^{N_a} w_a(x)\,\epsilon_{\mathrm{LOO},n}(x_a)$.","On 2D canonical/benchmark functions, the adaptive method reduces metamodel $\ell_2$ error more rapidly than Latin hypercube and full factorial for the same added samples (e.g., large early error drops on the Gaussian hill example after only a handful of adaptively placed points). The method’s spacing hyperparameter $R_0$ materially affects clumping vs. space-filling; the authors report most consistent performance for $R_0\in[1,1.5]$ and use $R_0=1.25$ thereafter. In noise studies, HOLMES-based interpolation is substantially more resistant than exact RBF interpolation (Gaussian and Wendland kernels) under both additive and proportional Gaussian noise, and the full DoE scheme outperforms a similar adaptive sampling method (Mackman et al.) as noise increases (illustrated by lower final interpolation error for the same total sample budget). For time-dependent functions (2D wave equation drum example), periodically updating adaptively placed nodes yields modest integrated error improvements over a static full factorial grid, with forward-projection variants helping when update intervals are longer.","The authors note boundary-related difficulties: HOLMES derivatives are problematic/non-existent on (and numerically unstable near) the domain boundary, so they restrict candidate sampling away from boundaries, which could be problematic if regions of interest lie on the boundary. They also state that HOLMES evaluation is typically slower than an equivalent RBF evaluation (though with different scaling properties), implying computational cost may limit applicability depending on experiment speed. For time-dependent adaptation, they acknowledge the approach is basic and only sensible for small forward projections, with efficacy depending on projection accuracy and on the experiment’s timescales and ability to pause/update.","Most demonstrations are in 2D with exhaustive grid search for maximizing $S(x)$; performance and practicality in higher-dimensional spaces (where grid search is infeasible) and with more sophisticated optimizers is not established. The method contains multiple design/tuning choices (e.g., $R_0$, neighbor counts for KDE, HOLMES parameters such as order/norm/locality) and the interactions among these are only partially explored, so robustness across diverse real experimental constraints is uncertain. Comparisons focus mainly on FF/LHS and one prior adaptive heuristic; broader benchmarking against modern Bayesian optimization/GP-based adaptive DoE or space-filling optimal designs could change conclusions in some regimes.","They suggest adapting the approach to the solution of partial differential equations, highlighting that its unstructured multidimensional nature could suit particle-based continuum mechanics models (e.g., smoothed particle hydrodynamics) that need principled dynamic resolution refinement during simulation. They also note that integrating the adaptive sampling more directly into computer experiments (periodically pausing to insert points) is a plausible direction, especially for time-dependent simulations where small accuracy gains can accumulate over time.","A natural extension is a scalable high-dimensional optimization strategy for maximizing $S(x)$ (e.g., multistart continuous optimization, surrogate-assisted search, or acquisition maximization methods) along with updated normalization schemes that do not rely on exhaustive evaluation over a grid. Additional work could analyze statistical properties under unknown/noisy parameters (self-starting variants, uncertainty quantification for $Q_E$, and robustness to autocorrelation/heteroscedastic noise) and develop principled guidelines for choosing $R_0$, KDE neighbor count $k$, and HOLMES parameters. Packaging an open-source implementation and benchmarking against GP/Kriging-based adaptive DoE (e.g., EI/UCB, IMSPE/I-optimal sequential design) on standard test suites and real experimental datasets would strengthen practical adoption.",2405.04624v1,https://arxiv.org/pdf/2405.04624v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:40:45Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Cost reduction|Other,A-optimal|Other,"Variable/General (focus on sensor placement/selection; examples use 1600 candidate sensor locations in subsurface flow and 214 candidate locations in tsunami case; parameters include ~40 boundary-flux dof + 1600 field dof in subsurface, and 44 slip magnitudes + 44 rakes in tsunami)",Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.5281/zenodo.4025432|https://github.com/rjleveque/tohoku2011-paper1,"The paper develops a non-intrusive A-optimal experimental design method for sensor selection in large-scale nonlinear Bayesian inverse problems, using the Bayesian approximation error (BAE) framework to account for error introduced by replacing the nonlinear forward model with a linear surrogate. A key theoretical contribution is proving the resulting uncertainty-aware A-optimal objective (and thus the optimal design) is asymptotically independent of the particular linearization when full BAE statistics (including cross-covariance terms) are used, enabling use of a trivial surrogate such as the zero map. The method is designed for sensor placement (subset selection from candidate locations) and supports marginalized OED where nuisance/auxiliary parameters are integrated out. The approach is derivative-free and black-box in the sense it only requires parameter–data sample pairs to estimate BAE error statistics via Monte Carlo sampling. Numerical studies on a subsurface flow inverse problem and a tsunami source detection problem show the uncertainty-aware designs outperform random designs and can outperform “uncertainty-unaware” designs that ignore model error or auxiliary-parameter uncertainty.","Sensor subset selection is formulated as minimizing an A-optimal utility, typically $\min_{w\in\{0,1\}^s,\,\sum w_i=k}\,\mathbb{E}_{d|w}[\operatorname{tr}(C_{v|d,w})]$; in the linear-Gaussian case the posterior covariance is $C_{v|w}=(C_{vv}^{-1}+F^\ast\Sigma(w)F)^{-1}$. Model discrepancy is handled by the BAE decomposition $d=G(v)+e=Fv+\varepsilon(v)+e=Fv+\eta(v)$ with a conditional-Gaussian approximation for $\eta|v$ using estimated means/covariances (including $C_{\varepsilon v}$). The uncertainty-aware posterior covariance used for A-optimality under a sensor-selection matrix $W(w)$ is $C^{F}_{v|w}(w)=\big(C_{vv}^{-1}+F_e^\ast W^T (W\Gamma_{\eta|v}W^T)^{-1}WF_e\big)^{-1}$ with $F_e=F+C_{\varepsilon v}C_{vv}^{-1}$, and the trace criterion is invariant to the choice of linear surrogate $F$ (asymptotically in the Monte Carlo estimation of error statistics).","Using the zero-map surrogate ($F\equiv 0$) with BAE-based error statistics, the greedy A-optimal sensor placements consistently reduce posterior uncertainty (trace of posterior covariance) more than randomly selected sensor configurations in both numerical examples. In the subsurface-flow problem, the 20-sensor design found by the proposed method also yields lower “true” posterior covariance trace (estimated via pCN MCMC under the full nonlinear model) than random 20-sensor designs. In marginalized OED for boundary flux, accounting for auxiliary-parameter uncertainty (permeability field) produces markedly different and better-performing designs than an uncertainty-unaware approach that ignores it. In the tsunami example, uncertainty-aware designs improve reconstruction accuracy: reported relative errors (truth vs posterior mean) are about 0.26 vs 0.44 for slip magnitudes and 0.13 vs 0.17 for rakes (uncertainty-aware vs uncertainty-unaware design) under the uncertainty-aware inversion model.","The authors note the approach may be poorly suited for highly nonlinear parameter-to-observable maps or strongly non-Gaussian priors, with no guarantee that designs remain effective for the original (nonlinear) problem. Independence from the choice of linear surrogate holds only asymptotically; accurate estimation of second-order error statistics may require many expensive forward (PDE) solves, especially if the surrogate is weak (e.g., the zero map), though control variates might help. They also acknowledge that explicitly forming covariance matrices/operators can become infeasible at very large scales, suggesting low-rank or matrix-free variants to avoid this bottleneck.","The design optimization uses a greedy subset-selection heuristic for an NP-hard problem; performance can be suboptimal and depends on candidate set geometry and diminishing-returns behavior, but no approximation guarantees are established for the specific uncertainty-aware objective. Evaluation focuses on two application examples with particular priors/noise models; robustness to misspecified priors, correlated/heteroscedastic noise beyond the described cases, and model misspecification other than linearization error is not systematically studied. The method’s reliance on Monte Carlo error-statistic estimation may be challenging when prior sampling is expensive or when forward evaluations are extremely costly; guidance on sample-size selection (q) and stopping criteria is limited.","They propose studying surrogate-dependent statistical convergence and using control variates (e.g., derivatives) to accelerate convergence of BAE error-statistic estimates when only limited high-fidelity samples are affordable. They suggest exploring model-free OED using experimental (rather than synthetic) parameter–data pairs when an accurate forward model is unavailable. They also call for a more rigorous study of how well designs obtained from the linearization-based, uncertainty-aware objective minimize uncertainty in the true nonlinear posterior.","Developing scalable matrix-free/low-rank implementations of the covariance and trace computations (including randomized estimators) would broaden applicability to truly massive candidate-sensor sets and state dimensions. Extending beyond A-optimality to information-theoretic utilities (e.g., expected KL / D-optimal) under BAE, and assessing whether similar linearization invariance holds, would be valuable. Additional work could incorporate time/adaptive sensor placement (sequential OED) and handle autocorrelated spatio-temporal observations common in environmental sensing, with principled treatment of model/measurement error correlations.",2405.07412v1,https://arxiv.org/pdf/2405.07412v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:41:35Z
TRUE,Optimal design|Computer experiment|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,D-optimal|Space-filling|Minimax/Maximin|Other,"Variable/General (case study: structure type {hcp,fcc,bcc} and elastic deformation/strain state; candidate pool size 100,000; selected design size 1,000)",Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://public.lanl.gov/afv/VoterEAMchapter.pdf|https://www.annualreviews.org/doi/10.1146/annurev-physchem-082720-034254|https://linkinghub.elsevier.com/retrieve/pii/S000926140401125X|https://link.aps.org/doi/10.1103/PhysRevLett.104.136403|https://www.nature.com/articles/s41467-018-06169-2|http://arxiv.org/abs/1512.06054|https://link.aps.org/doi/10.1103/PhysRevB.99.014104|https://linkinghub.elsevier.com/retrieve/pii/S1359645421003608|https://www.nature.com/articles/s41563-020-0777-6|https://link.aps.org/doi/10.1103/PhysRevB.87.184115|https://link.aps.org/doi/10.1103/PhysRevLett.93.175503|https://pubs.acs.org/doi/10.1021/acs.jpclett.0c01061|https://linkinghub.elsevier.com/retrieve/pii/S0927025617304536|https://link.aps.org/doi/10.1103/PhysRevMaterials.7.043801|https://pubs.aip.org/jcp/article/153/9/094110/199441/An-entropy-maximization-approach-to-automated|https://link.aps.org/doi/10.1103/PhysRev.140.A1133|https://onlinelibrary.wiley.com/doi/10.1002/qua.24927|https://pubs.acs.org/doi/10.1021/acs.chemrev.1c00022|https://www.tandfonline.com/doi/full/10.1080/02664768700000020|https://www.jstor.org/stable/2986264?origin=crossref|http://arxiv.org/abs/1512.07328|https://doi.org/10.5281/zenodo.11191242|http://arxiv.org/abs/1103.1625|https://linkinghub.elsevier.com/retrieve/pii/037837589090122B|https://linkinghub.elsevier.com/retrieve/pii/S1878029611001319|https://link.aps.org/doi/10.1103/PhysRevB.90.104108|https://linkinghub.elsevier.com/retrieve/pii/0927025696000080|https://link.aps.org/doi/10.1103/PhysRevB.54.11169|https://link.aps.org/doi/10.1103/PhysRevB.47.558|https://link.aps.org/doi/10.1103/PhysRevB.59.1758|https://link.aps.org/doi/10.1103/PhysRevB.40.3616,"The paper proposes using classical statistical design of experiments (DOE) and optimal design concepts to construct efficient, transferable training sets for kernel-based machine-learning interatomic potentials, focusing on Gaussian Approximation Potentials (GAP) / Gaussian process regression (GPR). It discusses label-free (off-line) criteria to assess informativeness of candidate atomic configurations before running expensive DFT calculations, aiming to reduce confirmation bias and improve coverage in high-dimensional descriptor spaces (e.g., SOAP). Two main design ideas are treated: maximum-entropy sampling for GPR (maximize log det of the training covariance/kernel matrix) and a conditional max–min (space-filling) design that greedily maximizes the minimum kernel distance between selected configurations. In a case study building a 1,000-structure database of elastically deformed Zr lattices (hcp/fcc/bcc) from pools of 100,000 candidates, the optimized designs reduce overlap in descriptor space and yield GAP models with more consistent off-sample performance than random “representative” subsets. The work positions DOE-style, a priori selection as a low-infrastructure alternative/complement to on-the-fly active learning for generating expensive ab initio labels.","For linear regression DOE motivation, the coefficient covariance is $\Sigma=\sigma^2(X^\top X)^{-1}$ and D-optimality maximizes $\det(\sigma^{-2}X^\top X)$. For GPR/GAP, a maximum-entropy criterion is $\max\;\log\det K$ where $K$ is the training covariance/kernel matrix. The paper’s space-filling selection uses the squared kernel distance between candidates $p,q$: $D_K^2=k(p,p)+k(q,q)-2k(p,q)$, and a greedy max–min algorithm that iteratively selects the point maximizing its minimum distance to the current design.","In the Zr elastic-deformation case study, two optimized 1,000-point training sets (OPT1/OPT2) selected from independent pools of 100,000 candidates each show a clear shift of within-set pairwise kernel distances toward larger values compared with random representative sets, indicating reduced redundancy/overlap. Cross-validation across four datasets (OPT1, OPT2, RND1, RND2) shows models trained on optimized sets deliver more consistent and generally better worst-case off-sample errors (assessed via the highest 0.99-quantile absolute error across test sets) than models trained on representative random subsets. The results support the claim that DOE-style kernel-distance space filling improves transferability even though the optimization used an energy-based kernel while the final GAP training also used forces and virials.","The authors note that optimization is not always guaranteed to be successful: in high-dimensional descriptor/kernel spaces the algorithm may select nearly orthogonal examples, producing a training set that is poor for interpolation. They emphasize a key challenge is choosing an appropriate distance/dissimilarity measure and restricting the search space (e.g., limiting deformation eigenvalues) to avoid extreme, unhelpful configurations. They also note that the optimization used a kernel associated with energies, whereas the final model training incorporated forces and virials, making the design criterion somewhat simplified relative to the full training objective.","The study focuses on a single, relatively structured case (elastic deformations of perfect Zr lattices), so it is unclear how robust the approach is for more complex materials phenomena (defects, surfaces, phase transitions) where candidate generation and domain constraints are harder. The greedy max–min procedure depends strongly on kernel/descriptor hyperparameters; without systematic sensitivity analysis, the selected design may vary materially with these choices. Comparisons are mainly against random subsampling from the same source-sampling; stronger baselines (e.g., farthest-point sampling with alternative metrics, leverage-score/pivoted Cholesky selection, or other DOE criteria like I-/G-optimal designs for GPR) are not comprehensively benchmarked. Computational cost/memory scalability for very large candidate pools is discussed qualitatively but not quantified with timings or memory footprints on realistic descriptor sizes.","The authors suggest developing better underlying/source sampling methods—especially for many-body representations—to ensure adequate coverage of descriptor space, potentially leveraging advances in configuration-space exploration. They also highlight the need for more efficient optimization algorithms (e.g., “lazy”/memory-efficient variants), since conditional max–min and exchange algorithms are practical mainly when descriptors or kernel matrices are precomputed and stored, which can be limiting for very large candidate sets.","Extending the DOE criteria to explicitly account for mixed label types (energies, forces, virials) via multi-output/multi-fidelity GPR design or derivative-observation kernels could better align design with the final GAP training objective. Robust/self-tuning designs that jointly optimize kernel hyperparameters and the selected set (or iterate between them) could reduce dependence on ad hoc hyperparameter choices. Testing under distribution shifts (e.g., including defects, finite-temperature MD snapshots, surfaces) and against established active-learning acquisition functions would better establish generality. Providing an open-source reference implementation integrated with common atomistic ML tooling (QUIP/GAP, ASE) would improve reproducibility and adoption.",2405.08636v1,https://arxiv.org/pdf/2405.08636v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:42:44Z
TRUE,Optimal design|Computer experiment|Screening|Other,Prediction|Cost reduction|Parameter estimation,Minimax/Maximin|Space-filling|Not applicable|Other,"Variable/General (design in feature space: 16-D contrastive, 18-D microstructural descriptors, 512-D VAE latent; candidate set 6,825 MVEs)",Manufacturing (general)|Other,Simulation study|Other,TRUE,Other,Not provided,http://energy.gov/downloads/doe-public-access-plan,"The paper proposes a design-of-experiments (DOE) strategy to efficiently select microstructural volume elements (MVEs) for expensive micromechanics simulations used to train deep-learning full-field surrogate models. It compares three microstructure feature representations—(i) a β-VAE latent vector (512-D), (ii) a novel self-supervised contrastive feature distillation model producing statistical descriptors (16-D), and (iii) domain-inspired microstructural descriptors using grain size plus volume-averaged generalized spherical harmonics (18-D). Using these features as the design space, it evaluates three space-filling/subsampling criteria: greedy conditional maximin (maxi-min), maximum projection (maxPro), and data twinning. Surrogate accuracy (validation loss) is assessed by training a 3D U-net to predict voxelwise stress tensors from MVEs simulated with PRISMS-Plasticity; DOE-selected training sets achieve up to ~8–9% improved validation loss versus random selection, with performance depending on feature dimensionality and the optimization difficulty of the design criterion. The work demonstrates that initial DOE selection of MVEs can measurably improve surrogate prediction performance for micromechanical localization problems while highlighting robustness tradeoffs among space-filling criteria.","The space-filling criteria include (i) a maximin distance design maximizing the minimum pairwise distance: $\max_D\min_{i,j} d(x_i,x_j)$ (implemented greedily as conditional maximin), and (ii) the maxPro design minimizing an overlap-penalizing product across coordinate-wise distances: $\min_D \sum_{i<j}\big(\prod_{l=1}^p |d_k(x_{il},x_{jl})|\big)^{-2}$. The self-supervised contrastive feature model is trained with a margin-based loss $L=\max(0, d(x_a,x_p)-d(x_a,x_n)+1/2)$ where $d$ is Euclidean distance in latent space.","Across combinations of three feature types and three design criteria, DOE-based subsampling improves surrogate validation loss relative to random selection by up to 8.8% (reported for the VAE+maxPro combination at 25% training-set usage). Data twinning yields a relatively consistent ~5% boost across feature types, while maxPro can degrade at larger selected set sizes (e.g., 50% of data) due to optimization complexity, especially in high dimension (VAE latent $p=512$). The contrastive features with greedy conditional maximin are reported as the most robust/monotonic with increasing training set size. The authors also find error tails worsen for large-grain textured MVEs, suggesting more samples are needed in those regions of microstructure space.","The authors note that the material response is simplified to elastic behavior to enable rapid dataset generation, implying results should extend but are not yet shown for more expensive inelastic constitutive models. They also state that DOE performance depends on design-optimization complexity, and that certain criteria (notably maxPro) can deteriorate for larger selected set sizes in their setting due to optimization difficulty. They acknowledge that the surrogate improvements are problem- and architecture-dependent and difficult to assess a priori.","The DOE criteria are applied after reducing complex microstructures to Euclidean feature spaces; performance may be sensitive to feature scaling/normalization and whether Euclidean distance is physically meaningful for microstructure similarity. Designs are selected from a finite candidate set of synthetically generated MVEs (Neper) with specific texture/grain-size regimes, so generalization to other microstructure generators, materials systems, or real micrographs is uncertain. The study optimizes designs greedily (suboptimal) and does not report runtime/complexity comparisons for design construction, which affects practical applicability when candidate sets grow. Improvements are reported primarily via validation loss; additional metrics (calibration, robustness to distribution shift, uncertainty quantification) are not explored.","The authors suggest the approach may be even more beneficial when scaled to larger problems and larger candidate sets (more MVEs and larger simulation budgets). They also indicate that the central contribution—efficient DOE for micromechanical surrogate training—should extend beyond the elastic model to more advanced inelastic constitutive responses. Data availability is stated as “upon reasonable request,” implying future sharing and broader evaluation may follow.","A natural extension is to develop sequential/adaptive (active learning) versions that iteratively update the feature space and design criterion based on surrogate uncertainty or error diagnostics, rather than a one-shot initial DOE. Robust/physics-aware distance metrics (e.g., invariances to symmetries, crystallographic equivalences) and non-Euclidean designs could reduce sensitivity to representation choices. Benchmarking against additional space-filling methods (Latin hypercube, Sobol/low-discrepancy, support points) and including computational cost of design construction would strengthen practitioner guidance. Providing open-source implementations and standardized datasets would enable reproducibility and accelerate adoption in ICME workflows.",2405.10135v1,https://arxiv.org/pdf/2405.10135v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:43:57Z
TRUE,Sequential/adaptive|Other,Model discrimination|Cost reduction|Other,Other,"Variable/General (nodes/variables in DAG; examples include N=5–10 in simulations, 13 nodes in SACHS network)",Theoretical/simulation only|Healthcare/medical|Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper formulates intervention design for causal discovery as a fixed-confidence, online learning (pure-exploration bandit) problem where interventions are chosen sequentially to identify the true causal DAG with probability at least 1−δ using as few interventional samples as possible. It proposes a track-and-stop algorithm that adaptively selects interventions from a graph separating system (a set of intervention targets that cuts every edge at least once) using an allocation-matching rule and stops when an information-distance-based confidence condition is met. The method leverages candidate interventional distributions enumerated from CPDAG/MPDAG representations and compares them to empirical interventional distributions to orient previously unoriented edges. The authors provide a problem-dependent expected-sample upper bound and show asymptotic optimality relative to a lower bound expressed via KL divergences between interventional distributions under competing DAGs. Simulation studies on randomly generated chordal (moral) DAGs and a semi-synthetic SACHS Bayesian network show the proposed adaptive strategy achieves lower structural Hamming distance (SHD) with substantially fewer interventional samples than several baselines (random interventions, DCT-based active learning, sensitivity search, and GIES).","Interventional distributions are defined by truncated factorization under do-interventions: $P_S(v)=P(v\mid do(S=s))=\prod_{V_i\notin S} P(v_i\mid pa(V_i))$ (Eq. 1). The fixed-confidence lower bound and the algorithm’s allocation aim are driven by a max–min KL objective: $c(D^*)=\sup_{\alpha\in\Delta(I)}\min_{D\neq D^*}\sum_{s\in I}\alpha_s\,\mathrm{KL}(P^{D^*}_s\|P^D_s)$ (Eq. 3), while stopping uses an accumulated evidence statistic $d_t=\min_{D\neq D_t^*}\sum_{s\in I} N_t(s)\,\mathrm{KL}(\bar P_{s,t}\|P^D_s)$ (Eq. 5) with a concentration-based threshold $f_t(d_t)\le \delta$ (Eq. 6). Intervention selection tracks the estimated optimal allocation via an AdaHedge-style update (Eq. 8) and allocation matching, with forced exploration ensuring each intervention is sampled sufficiently often.","The authors prove soundness (probability of returning an incorrect graph is at most $\delta$ and $P(\tau_\delta=\infty)=0$) and derive asymptotic sample complexity: $\lim_{\delta\to 0} \log(1/\delta)/\mathbb{E}[\tau_\delta]=c(D^*)$ for the exact algorithm, and equals a related constant for the practical variant. Empirically, across random chordal graphs (e.g., complete graphs with N=5–7 and Erdös–Rényi chordal graphs with N=10 at densities $\rho\in\{0.1,0.15,0.2\}$), the track-and-stop method achieves lower SHD for the same number of interventional samples than random interventions, DCT-based active learning, sensitivity search, and GIES. On the SACHS Bayesian network (13 nodes, 17 edges), the proposed method also yields markedly lower SHD at equal sample budgets. Qualitatively, the proposed approach’s sample needs grow more slowly with graph size/density than competitors in the reported plots.","The paper assumes causal sufficiency (no latent variables), discrete variables, and that the observational distribution is available and faithful to the true DAG; these conditions may not hold in practice. The authors note the exact algorithm is computationally intensive because it requires enumerating DAGs in the Markov equivalence class, with worst-case complexity exponential in the number of unoriented edges. For the practical implementation, they acknowledge it may output contradictory edge orientations or a result that violates DAG constraints, and recommend reducing $\delta$ and continuing if the returned structure is not a DAG.","The empirical evaluation is limited to synthetic chordal/moral DAG generation and the SACHS benchmark; performance on non-chordal structures, larger graphs, or different data-generating mechanisms (e.g., continuous, mixed, or nonlinear SEMs) is not demonstrated. The approach relies on accurately computing/approximating KL divergences between candidate interventional distributions; in realistic settings with unknown observational distribution (finite Phase I data), model misspecification and CI test errors could materially affect both allocation and stopping behavior. Practical deployment could be constrained by intervention feasibility (ethical/physical limits, costs varying by target set), yet the design does not explicitly incorporate heterogeneous intervention costs or constraints beyond sample count. No implementation details (runtime, scalability, hyperparameters) or released code are provided, limiting reproducibility and making it hard to assess computational feasibility of the “practical” variant at scale.","The paper suggests (in the practical algorithm remark) continuing experimentation with a smaller $\delta$ if the practical implementation returns a non-DAG (i.e., to further reduce error probability and resolve contradictions). In the supplementary material, they also explore and motivate a setting where the starting CPDAG is wrong due to limited observational data, implying extension beyond the ‘true observational distribution available’ assumption as an additional direction.","Extend the framework to handle finite observational (Phase I) data explicitly, including uncertainty in the initial CPDAG/MEC and propagation of that uncertainty into allocation and stopping guarantees. Generalize beyond discrete variables to continuous or mixed data and to nonlinear/heteroskedastic SEMs, potentially requiring different divergence estimators and robustness analysis. Incorporate intervention costs/constraints and partial compliance (hard-to-intervene nodes, soft interventions) into the allocation objective to better match real experimental settings. Provide scalable implementations and open-source code, plus benchmarks on larger and more diverse causal discovery datasets, including settings with latent confounding or selection bias.",2405.11548v3,https://arxiv.org/pdf/2405.11548v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:44:40Z
TRUE,Optimal design|Sequential/adaptive,Parameter estimation|Prediction|Cost reduction,D-optimal|A-optimal,Variable/General (design point dimension examples: 2 inputs in stationary example; 3 inputs in dynamic example; parameter dimension examples: 2 parameters and 6 parameters),Manufacturing (general)|Other,Simulation study|Other,TRUE,Python|Fortran,Not provided,NA,"The paper develops adaptive discretization algorithms for locally optimal experimental design (OED) for nonlinear prediction models, where designs are probability measures over a design space and are optimized via convex criteria applied to the Fisher information matrix. It refines a recent adaptive discretization method by allowing ε-approximate optimality conditions and approximate solutions of the discretized design subproblem and the “worst violator” (sensitivity minimization) subproblem, reducing per-iteration computational effort. The authors prove new termination, convergence, and convergence-rate results: finite termination at approximately optimal designs, a general sublinear rate under local smoothness assumptions, and a linear rate when the criterion is strongly convex and the design space is finite (covering common A- and D-criteria). Practical performance is demonstrated on two chemical engineering OED problems (one stationary algebraic model and one dynamic ODE model) on large finite candidate sets, comparing favorably to vertex-direction methods in iterations and runtime. The work advances the theory of convergence rates for classical OED algorithms and provides implementable adaptive schemes for large candidate sets.","Approximate designs are discrete probability measures $\xi=\sum_{i=1}^n w_i\,\delta_{x_i}$. The Fisher information matrix is $M_f(\xi,\theta)=\int_{\mathcal X} D_\theta f(x,\theta)^\top\,\Sigma^{-1}\,D_\theta f(x,\theta)\,d\xi(x)$. The locally optimal design problem is $\min_{\xi\in\Xi(\mathcal X)} \Psi(M_f(\xi,\theta))$ with convex antitonic criteria, notably D: $\Psi_0(M)=\log\det(M^{-1})$ and A: $\Psi_1(M)=\mathrm{tr}(M^{-1})$. The adaptive algorithm iterates by solving a discretized design problem on $\mathcal X_k$ and adding a (approx.) worst violator $x_k\in\arg\min_{x\in\mathcal X}\psi(M_f(\xi_k,\theta),x)$ where $\psi$ is a sensitivity/directional-derivative function; termination uses the $\varepsilon$-approximate condition $\psi(M_f(\xi_k,\theta),x)\ge -\varepsilon\ \forall x$.","The paper proves (i) finite termination at $\varepsilon$-approximately optimal designs under mild continuity/convexity assumptions, including iteration upper bounds via packing-number arguments when the sensitivity is (locally) Lipschitz. (ii) A sublinear rate for strict variants under local L-smoothness: $\Psi(M(\xi_k,\theta)) - \Psi^* \le C/(k+1)$ (more precisely, $\le \tfrac{2}{k+2}(\Psi(M(\xi_1))-\Psi^* + L\,\mathrm{diam}(M(\Xi(\mathcal X)))^2 + c + \bar c)$). (iii) A linear rate for finite design spaces when $\Psi$ is µ-strongly convex and L-smooth: $\Psi(M(\xi_k,\theta)) - \Psi^* \le C r^k$ with $r\in[1/2,1)$. In the stationary CSTR example (10,201 candidates), both exchange and non-exchange variants terminate in 1 iteration and outperform a vertex-direction method in runtime (≈0.5s vs 36s) and objective value. In the dynamic Williams–Otto example (808,020 candidates), both variants terminate in 3 iterations and yield better A/D criterion values than factorial and random designs; enumeration is used for violator search and SQP is used for the discretized convex subproblem.","The authors’ convergence-rate results require additional assumptions beyond basic convexity, notably local L-smoothness for sublinear rates and (for linear rates) µ-strong convexity together with finiteness of the design space. In the applications, the design space is chosen as a large but finite candidate set, and the violator problem is solved by enumeration, which may not be feasible for truly continuous or extremely large candidate spaces. The approach is locally optimal (depends on a reference parameter value $\theta$) and relies on standard regularity and (in the modeling setup) normally distributed measurement errors with known covariance.","The method is presented for locally optimal (single-point) designs; it does not address Bayesian/robust design formulations that integrate over parameter uncertainty, which are often crucial when the reference $\theta$ is inaccurate. Practical scalability hinges on repeatedly solving discretized convex design problems (e.g., via SQP) and computing sensitivities; for expensive simulators or high-dimensional inputs, these steps can dominate and may need surrogate models or specialized solvers. The numerical examples use finite candidate grids; performance and guarantees for continuous design spaces would require global optimization or reliable approximate worst-violator search, which is nontrivial for nonconvex sensitivity landscapes.",None stated.,"Extend the adaptive discretization framework to Bayesian or minimax-robust optimal design criteria to mitigate dependence on a single reference parameter value. Develop and analyze non-enumerative worst-violator search (e.g., gradient-based, branch-and-bound, or surrogate-assisted global optimization) for very large or continuous candidate spaces, with corresponding approximate-violator guarantees. Provide open-source implementations and benchmarks to facilitate adoption, and study computational tradeoffs with alternative solvers (e.g., specialized Frank–Wolfe/vertex-exchange methods) for the discretized subproblem under different criteria and model structures.",2406.01541v1,https://arxiv.org/pdf/2406.01541v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:45:20Z
FALSE,Other,Other,Not applicable,Variable/General,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/artificial-scientist-lab/metadesign,"The paper introduces “meta-design,” where a transformer model is trained to generate human-readable Python meta-programs that produce entire families of quantum optics experiments (and related circuit/graph constructions) for varying system size N. Training data are synthetically generated by sampling random Python programs and executing them (via PyTheus for photonic experiments, and via Qiskit for circuit tasks) to obtain the corresponding quantum states for N=0,1,2; the model learns the inverse mapping from the concatenated target states to code. At inference, many candidate programs are sampled and evaluated by executing them for larger N (e.g., N=3,4 and beyond) and computing state fidelity against target classes; the method rediscovered known constructions (e.g., GHZ, W, Bell variants) and discovered two previously unknown generalizing constructions (Spin-1/2 and Majumdar–Ghosh) under the given experimental resource constraints. The approach is positioned as a way to extract general, interpretable design principles (in code form) and reduce the exploding computational costs of direct experiment-by-experiment optimization. The authors also demonstrate extensions to quantum circuit and quantum graph-state meta-design by training similar models on synthetically generated code/state pairs.","Key quantities include the fidelity between produced and target quantum states (overlap/inner product squared) used to score generated codes, and an optimization-loss expression used for cost comparison: $L(\omega)=|\langle\psi_{\text{target}}|\psi(\omega)\rangle|^2$. For computational-cost discussion, they estimate FLOPs to compute amplitudes via perfect matchings (Hafnian-like sums) and give an approximate scaling $\mathrm{FLOP}(|\psi(\omega)\rangle)\approx k^N\cdot (N/2)\cdot (N-1)!!$ for N particles and local dimension k. The core “method equation” is procedural: a generated meta-program defines a function (conceptually) like `construct_setup(N)` that outputs an experiment blueprint whose simulated output state is compared to the desired class.","Training data generation comprised ~56 million synthetic samples (about 50,000 CPU hours) for the main photonic task; the main transformer used ~133M parameters and was trained for 750k steps on 4×A100-40GB GPUs. On 20 hand-picked target state classes, the method produced perfectly generalizing meta-solutions for 6 classes; it rediscovered 4 previously known constructions (GHZ, W, 2d-Bell, 3d-Bell) and found 2 previously unknown generalizations (Spin-1/2 and Majumdar–Ghosh) that extrapolated beyond the training range (N=0,1,2). For an additional sampling phase, they generated ~800–2500 candidate programs per target in ~4 hours on an RTX 6000 GPU and selected best codes by requiring fidelity 1 on successive N before ranking by average fidelity up to N=4. They also report auxiliary tasks (quantum circuits and graph states) where separately trained models (89M parameters, 12 layers) successfully produced extrapolating codes for several classes (e.g., GHZ/AKLT/MG circuits; linear/ring/star graph states).","The authors note that their approach currently constrains continuous parameters to a limited subset of special numbers (e.g., integers and $\sqrt{2}$), and generating arbitrary real-number parameters via token sequences would be inefficient. They also state that the method relies on being able to generate large amounts of training examples efficiently; for domains where example generation is expensive, data-efficiency becomes a limiting factor. They further highlight ambiguity from prompting with only the first three sequence elements: codes can match N=0,1,2 yet fail to follow the intended pattern for N\u22653, and outputs are influenced by the synthetic training distribution. Finally, some target classes may be physically unrealizable under the allowed experimental resources, so failure may reflect infeasibility rather than model weakness.","Although outputs are “human-readable,” the paper does not establish a systematic protocol to verify physical implementability beyond the simulator assumptions (e.g., robustness to experimental noise, loss, imperfect components), so real-lab transfer is unclear. Performance evaluation is largely based on fidelity under simulation for a limited set of target classes and sampling heuristics; broader baselines against alternative program-synthesis or symbolic-regression methods for the same tasks are limited. The approach may be sensitive to the handcrafted synthetic program grammar/vocabulary and to distribution shift between synthetic data and real design constraints, but this sensitivity is not fully quantified. Finally, the search at inference still depends on stochastic sampling and execution-based filtering, which can become computationally heavy as N grows or if simulation fidelity evaluation is expensive.","They propose extending the method to handle continuously parameterized designs by developing ways to create arbitrary numbers more efficiently. They suggest studying how to improve training data efficiency when generating examples is computationally expensive. They also point to the open question of how neural scaling laws (data/model/compute scaling) translate to symbolic/mathematical solution quality in settings like theirs, to guide improving model quality and reducing sampling needed. More broadly, they suggest applying meta-design to other scientific/engineering domains (e.g., microscopy, gravitational-wave detectors, high-energy physics hardware, and functional molecule design).","Developing uncertainty-aware or Bayesian selection criteria for sampled programs (e.g., trading off fidelity, complexity, and robustness) could make the inferred meta-solutions more reliable and interpretable. Extending evaluation to noisy/realistic experimental models (loss, detector inefficiency, component tolerances) and demonstrating at least one physical/lab validation would strengthen practical relevance. Incorporating formal verification or constraint-solving to guarantee syntactic and semantic validity (and reduce reliance on brute-force sampling) could improve scalability. Releasing standardized benchmarks and baselines for “meta-design” tasks across physics domains would enable reproducible comparisons and clearer progress tracking.",2406.02470v2,https://arxiv.org/pdf/2406.02470v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:46:00Z
FALSE,NA,NA,Not applicable,Not specified,Other,Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper proposes a design method for interactive narrative installations that maps users’ natural physical behaviors (distance, identity, location, direction, movement) to elements of film “camera language” (shot scale, angle, camera movement, montage) to create more immersive storytelling in physical space. The method is demonstrated through an installation, “Still Walking,” in which a user rotates/pushes a 65-inch screen; an angle sensor (via Arduino) drives Unity3D-rendered animation sequences and branching story events across a woman’s life stages. The authors articulate hypotheses linking user distance to shot scale (more/less story content) and user position/direction to viewing angle, and they map user movement (rotation speed/pauses) to camera follow/stop and montage-like continuity. Implementation details include mock-up testing (screen size, animation composition, sensors, structure), a rotating display with slip ring for power, and software that uses angle data to control pre-authored animation frames. Observational feedback from an exhibition with thousands of participants is used to identify usability issues (e.g., users not realizing they should push; difficulty discovering branching plots) and to motivate improvements in guidance/signage and trigger angles.",Not applicable,"Quantitative performance results typical of DOE are not reported. The paper reports qualitative/usage outcomes from an exhibition setting: thousands experienced the installation, and videos were shared on social media with millions of views; users generally praised the interaction and story but often required instructions to discover the “push” interaction and branching plots. After adding an on-frame sign (“Push slowly”), the authors report improved understanding for young and middle-aged users. The interaction duration is described as roughly 40–100 seconds, with a full story rotation around ~40 seconds and scenes lasting ~10–15 seconds.","The authors note usability limitations observed in exhibition: without accompanying instructions, users often do not realize they should interact by “push,” and some users fail to discover branching plots or the correct stopping angle even when informed. They also imply aesthetic limitations of adding instructional notices on the frame, suggesting instructions should be integrated into the installation design or placed on the ground.","The evaluation is largely observational and anecdotal, with no controlled study design, predefined metrics (e.g., task success rate, time-to-discovery, retention), or statistical analysis, limiting causal claims about the proposed mapping method. The method’s generalizability is unclear because it is demonstrated on a single installation with one interaction modality (rotation/push) and pre-scripted content; it is not tested across different narratives, sensors, or physical layouts. Technical reproducibility is limited: while Arduino and Unity3D are mentioned, the paper does not provide implementation code, calibration procedures, or detailed hardware specifications beyond high-level descriptions.","The authors suggest extending from single-user interaction to multi-user interactive narratives where interactions among users and their behaviors affect storytelling. They also propose connecting different physical spaces so that users’ behaviors in separate locations can jointly trigger story interactions. More broadly, they frame the method as an experimental exploration and invite further study of the relationship between narrative and user behavior.","A rigorous user study could be added to validate the mapping hypotheses (distance→shot scale; position/direction→angle; movement→camera movement) with measurable outcomes such as comprehension, emotional impact, and interaction discoverability. The approach could be extended to additional sensing modalities (depth cameras, IMUs, vision-based pose) and to adaptive or personalized narrative control (e.g., learning-based branching based on behavioral patterns). Releasing open-source code and build documentation (Arduino/Unity project, sensor calibration, mechanical drawings) would enable replication and comparative evaluation against alternative interaction designs.",2406.02866v1,https://arxiv.org/pdf/2406.02866v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:46:24Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Optimization|Prediction|Cost reduction|Other,Other,Variable/General (design over treatment assignments/saturation levels; factors include cluster saturation levels and seeding allocations),Healthcare/medical|Environmental monitoring|Food/agriculture|Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/SteveJWR/ardexp|https://github.com/SteveJWR/SBMNetReg,"The paper develops a framework for causal inference under network interference when the underlying network is only partially observed (e.g., ARD, egocentric sampling, respondent-driven sampling, induced subgraphs). It combines structural causal models with network formation models (notably node-exchangeable/graphon approximations via stochastic blockmodels) to debias estimation of exposure mappings and derive single-network asymptotic inference for Z-estimators and linear models. For experimental design, it proposes choosing treatment assignments using only partial network data to either minimize estimator variance for target contrasts or to optimally seed diffusion processes; the design search is made tractable by optimizing cluster saturation levels and is implemented via Bayesian optimization over an expensive variance objective. Performance is validated primarily through semi-synthetic simulation studies on real village networks from India and Malawi diffusion/adoption studies, showing substantial RMSE reductions from optimized designs relative to uniform designs and demonstrating gains for seeding strategies using partial network information. The contribution advances interference/SPC-style design literature by offering design and inference tools that remain usable when full network measurement is infeasible and by integrating model-based variance objectives with partial-network measurement error corrections.","The design objective is to choose a treatment allocation (or saturation vector) to minimize the variance of an estimator/contrast under a network model learned from partial data. They define the contrast variance conditional on assignment as \(\upsilon_{\varphi}(a;\theta)=\mathrm{Var}(\varphi^\top \hat\beta\mid a,\theta)\) and optimize the saturation-randomized objective \(V(\tau;\theta)=\mathbb{E}_{a\sim P_{\tau}}[\upsilon_{\varphi}(a;\theta)]\). Treatment assignments are restricted to cluster-level saturations \(\tau\in[0,1]^J\) (to avoid the \(2^n\) search), and Bayesian optimization with a GP surrogate and UCB acquisition is used to minimize \(V(\tau;\theta)\) evaluated via Monte Carlo draws of graphs/treatments.","In the information-diffusion design example (T=3 rounds), Bayesian-optimized designs (using partial network data) reduce RMSE substantially versus uniform/random seeding: RMSE for estimating \(\alpha_1\) drops by about 38% (±12%) under optimized designs, compared to about 11% (±2%) from using complete network data without design optimization; for all parameters, RMSE drops about 45% (±10%) versus 18% (±2%). In the complex-contagion seeding application, model-based targeting of optimal blocks/within-block degree improves adoption ratios relative to degree seeding, with reported average increases of 1.13× (±0.12) for optimal blocks and 1.28× (±0.13) for optimal degree-within-blocks; an “optimal seeding strategy” achieves about 1.50× (±0.16) adoption relative to degree seeding. Across examples, results are demonstrated via semi-synthetic simulations on observed village networks (India/Malawi) using ARD-generated partial data and SBM-based network models.","The paper notes that inference and plug-in causal estimation depend on correct specification (or useful approximation) of both the outcome/interference model (exposure mapping/structural causal model) and the network formation model; misspecification can induce bias, and their graphon-approximation bound is described as potentially conservative and dependent on good alignment of clusters with observed traits. They also emphasize that their main focus is on outcomes measured at a single time point (often summarized at a fixed horizon \(T\)), even when diffusion has an underlying temporal mechanism. They further imply that some estimators/design evaluations rely on simulation/semi-synthetic setups built from observed graphs rather than extensive real-world end-to-end experimental deployments.","The experimental design component optimizes asymptotic variance (and seeding objectives) under an estimated generative network model; if the estimated SBM/graphon approximation is poor, the chosen design could be suboptimal or even harmful, and robustness guarantees to such design misspecification are limited. The Bayesian optimization approach requires repeated expensive Monte Carlo evaluations (graph draws × treatment draws), which may be computationally heavy for very large networks or richer outcome models, and performance can be sensitive to GP/kernel/acquisition settings. Practical constraints common in field experiments (noncompliance, attrition, interference across design clusters, ethical constraints) are not deeply integrated into the design objective, which may affect deployability.","They suggest exploring semiparametric estimation approaches with partial network data (e.g., in the spirit of recent semiparametric network methods) and considering additional structured assumptions on potential outcomes to improve estimation. They also highlight extending from single-time-point outcomes to panel settings and staggered rollouts, and developing outcome model classes that more explicitly incorporate temporal structure for design and inference. The paper also mentions that future work could include sensitivity analysis for response functions and latent graph model misspecification.","Developing robust (minimax or Bayesian) design criteria that explicitly account for uncertainty in the learned network model \(\hat\theta\) and outcome model \(\hat\beta\), beyond plug-in objectives, would make the design recommendations more reliable. Extending the framework to handle autocorrelated outcomes, unknown baseline parameters (self-starting/Phase I style issues), and realistic field complications (noncompliance, missing outcomes, spillovers across randomized clusters) would broaden applicability. Providing scalable software implementations and benchmarks (e.g., standardized design+inference comparisons across multiple partial network sampling schemes and network regimes) and validating on fully real experimental datasets (not semi-synthetic outcomes) would strengthen empirical credibility.",2406.11940v1,https://arxiv.org/pdf/2406.11940v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:47:06Z
TRUE,Bayesian design|Optimal design|Other,Parameter estimation|Model discrimination|Cost reduction|Other,Bayesian D-optimal|Other,Variable/General (examples include d=m=100; sensor subset size s=10; goal subspace dimension r=5 or r=10),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/qchen95/Coupled-input-output-DR,"The paper proposes a coupled input–output dimension reduction framework for nonlinear maps $G:\mathbb{R}^d\to\mathbb{R}^m$ that explicitly accounts for the interdependence between reduced parameter subspaces $U_r$ and reduced output/data subspaces $V_s$. Using Poincaré and Cramér–Rao-type inequalities, it derives computable derivative-based upper/lower bounds for the $L^2$ model-reduction error and connects these bounds to Bayesian posterior approximation error and to lower bounds on (goal-oriented) expected information gain (EIG). The resulting optimization reduces to eigen-decompositions of two diagnostic matrices $H_X(V_s)$ and $H_Y(U_r)$, and a practical alternating eigendecomposition algorithm is introduced for jointly finding $(U_r,V_s)$. For goal-oriented Bayesian experimental design (sensor placement), the method yields a closed-form coordinate-selection rule: pick the sensors corresponding to the largest diagonal entries of $H_Y(U_r)$, bypassing explicit EIG estimation and combinatorial search; a relaxed orthogonal-design variant followed by empirical interpolation (EIM) is also proposed. Numerical experiments on a conditioned diffusion example and nonlinear Burgers’ equation demonstrate that the approach captures causal structure, provides tight Sobol’ index bounds, converges quickly in alternation, and produces sensor sets with strong goal-oriented EIG performance compared with baselines.","Reduced coordinates are $X_r=U_r^\top X$ and $Y_s=V_s^\top G(X)$. The core derivative-based objective is to maximize $\mathbb{E}[\|V_s^\top\nabla G(X)U_r\|_F^2]=\mathrm{Tr}(V_s^\top H_Y(U_r)V_s)=\mathrm{Tr}(U_r^\top H_X(V_s)U_r)$, where $H_X(V_s)=\mathbb{E}[\nabla G(X)^\top V_sV_s^\top\nabla G(X)]$ and $H_Y(U_r)=\mathbb{E}[\nabla G(X)U_rU_r^\top\nabla G(X)^\top]$. In goal-oriented BOED with coordinate sensor selection $V_\tau=[e_\tau]$, maximizing the EIG lower bound reduces to $\max_{|\tau|=s}\sum_{i\in\tau}(H_Y(U_r))_{ii}$, i.e., choose indices of the largest diagonal entries. For relaxed orthogonal designs, $V_s^*(U_r)$ is given by the top-$s$ eigenvectors of $H_Y(U_r)$.","In the conditioned diffusion experiment (with $d=m=100$, typically $r=s=10$, and $M=10{,}000$ prior samples for diagnostics), the alternating eigendecomposition converges in about 3 iterations across 10 random initializations, and optimizing the upper bound decreases the true relative $L^2$ error while the lower/upper bounds track the error closely. For goal-oriented GSA, the derivative-based upper and lower bounds in Proposition 4.1 tightly track the computed goal-oriented total Sobol’ indices and correctly reflect temporal causality. In the Burgers’ equation goal-oriented BOED example (with $d=m=100$, goal dimension $r=5$, sensors $s=10$, diagnostics from $M=1{,}000$ samples), the proposed diagonal-entry sensor rule $\tau^*$ and the EIM-mapped sensors $\tilde\tau$ generally outperform random placements in goal-oriented EIG and are competitive with Fedorov–Wynn designs, especially when nonlinearity degrades linearized-design performance.",None stated.,"The BOED development assumes additive Gaussian observation noise and leverages bounds tied to an $L^2$ surrogate error; sensor selections optimize a lower bound rather than the true EIG, so optimality for EIG is not guaranteed and may vary by problem. The approach requires access to Jacobian–vector and adjoint-Jacobian–vector products (or accurate gradient approximations), which may be costly or unstable for black-box simulators and can be sensitive to Monte Carlo error in estimating $H_X$ and $H_Y$. The alternating eigendecomposition is a heuristic without general global optimality guarantees and may converge to stationary points depending on initialization and spectral gaps.","The authors highlight open questions on convergence of the alternating eigendecomposition algorithm and on (adaptive) rank selection for $r$ and $s$. They propose extending the framework to nonlinear quantities of interest $\psi_r(X)$ and $\phi_s(G(X))$, which would modify the diagnostic matrices via the chain rule, and note that associated error analysis is a natural next step. They also suggest extending the coupled framework to operator learning settings where $G$ acts between function spaces, leveraging infinite-dimensional versions of Poincaré/Cramér–Rao inequalities.","It would be valuable to benchmark the BOED sensor rule against state-of-the-art EIG estimators (e.g., transport/VI-based) on larger-scale PDE inverse problems to quantify the bound gap and robustness. Developing self-normalized, variance-reduced, or quasi-Monte Carlo estimators for $H_X$ and $H_Y$, along with finite-sample confidence intervals for the selected sensors, would strengthen practical reliability. Extending the method to correlated/noisy time-series observations (non-i.i.d. likelihoods), non-Gaussian noise models, and explicit computational-cost/optimality tradeoffs (e.g., budgeted designs) would broaden applicability.",2406.13425v2,https://arxiv.org/pdf/2406.13425v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:47:47Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction|Other,Not applicable,"Variable/General (examples: 6 parameters for 3-TC ODE; 4 parameters for Lotka–Volterra; design vector is time-sampling weights over N time points: N=400 for 3-TC, N=200 for PPM)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study,TRUE,Python|Other,Not provided,https://www.sciencedirect.com/science/article/pii/0169260794902011|https://jnm.snmjournals.org/content/64/supplement_1/P1449|https://doi.org/10.1080/00031305.2023.2249522|https://proceedings.mlr.press/v89/papamakarios19a.html|https://doi.org/10.1371/journal.pone.0162303|https://jmahaffy.sdsu.edu/courses/f09/math636/lectures/lotka/qualde2.html|https://doi.org/10.1177/106002806900300904|https://doi.org/10.1208/s12248-022-00743-9,"The paper proposes a “deep optimal experimental design” approach for parameter estimation in nonlinear ODE-governed systems using a likelihood-free neural estimator (LFE). Instead of classical bilevel optimal design (outer design optimization with repeated inner inverse-problem solves such as MAP/L-BFGS), the method jointly trains network parameters and design variables (measurement-selection/weighting vector) in a single optimization using self-supervised simulation from the prior and the ODE forward model with Gaussian noise. Two design parameterizations are studied: (i) continuous nonnegative weights trained via SGD with a nonnegative soft-shrink sparsification operator, and (ii) binary selection weights optimized via a block-coordinate scheme combining SGD for network weights with Tabu search for the binary design. The design variables correspond to choosing/weighting measurement time points (and implicitly measurement effort/accuracy) to reduce both parameter error and a data-fit term. Experiments on a 3-tissue compartment PET kinetic model and a Lotka–Volterra predator–prey model show that learned designs at low sparsity substantially improve normalized MSE risks compared with random designs and can reduce required measurements while maintaining estimation quality.","The experimental observation model is $d = F(q,\omega) + \epsilon$ with $\epsilon\sim\mathcal N(0,\sigma^2 I)$, where $\omega$ (or discretized weights $w$) encodes experimental settings such as sampling times. Classical MAP estimation is $\hat q(\omega)=\arg\min_q \frac{1}{2\sigma^2}\|F(q,\omega)-d\|^2 + R(q)$ and design minimizes expected risk $\ell_T(\omega)=\ell_q(\omega)+\gamma \ell_d(\omega)$ over $\omega$. The deep/LFE formulation replaces the inner solve with a network $\hat q = F_\theta^\dagger(w\odot d, w, \sigma)$ and jointly minimizes $\ell_T(\theta,w)=\mathbb E\|F_\theta^\dagger(\cdot)-q\|^2 + \gamma\ell_d(\theta,w)$; continuous $w$ is sparsified via $w\leftarrow s_\rho(w-\mu\nabla_w,\rho)$ with $s_\rho(t)=\max(t-\rho,0)$.","Across both ODE examples, optimal learned sparse sampling schedules reduce total normalized risk $\ell_T$ versus random designs, especially at low sparsity. For the 3-tissue compartment model at sparsity 2, continuous-$w$ design achieves $\ell_T\approx 6.18\times10^{-2}$ vs random-design mean $\approx 1.46$; binary/Tabu achieves $\ell_T\approx 5.78\times10^{-2}$ vs random mean $\approx 8.07\times10^{-2}$. For the predator–prey model at sparsity 4, continuous-$w$ achieves $\ell_T\approx 6.45\times10^{-3}$ vs random mean $\approx 1.43\times10^{-2}$; binary/Tabu achieves $\ell_T\approx 5.87\times10^{-3}$ vs random mean $\approx 1.19\times10^{-2}$. The paper also reports large compute-time advantages over a greedy/L-BFGS baseline in supplementary experiments (e.g., far fewer forward/backward passes to obtain $w_{opt}$).",None stated.,"The method assumes access to a reasonably specified prior over parameters and the ability to simulate the ODE forward model at scale; performance may degrade under prior misspecification or model mismatch (common in real experiments). The approach is demonstrated only on simulated data with i.i.d. Gaussian (often multiplicative) noise and does not address autocorrelation, systematic biases, outliers, or unknown noise levels typical of real measurement systems. Practical interpretability/constraints on designs (e.g., minimum spacing between sampling times, discrete allowable visit times, subject burden) are not explicitly incorporated beyond sparsity, and the Tabu/SGD training may be sensitive to hyperparameters and initialization. No public code is provided, which limits reproducibility and assessment of implementation details (ODE solver choices, training stability, etc.).","The authors suggest extending the approach to sensor placement problems for systems governed by partial differential equations (PDEs), noting that the design space becomes much larger and that scaling behavior would require further study.","Develop robust/self-starting variants that handle unknown noise levels, model mismatch, and correlated/heteroskedastic measurement errors, and incorporate hard operational constraints on sampling schedules. Extend to multivariate observations, partial observability settings, and jointly designing what to measure (which state/species) as well as when to measure. Provide theoretical connections/guarantees relating the learned objective to classical design criteria (e.g., D/A/I-optimality) and study identifiability and uncertainty quantification of the learned estimator/design. Release reference implementations and benchmark on real experimental datasets (e.g., clinical TACs) to validate practical utility and reproducibility.",2406.14003v3,https://arxiv.org/pdf/2406.14003v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:48:27Z
TRUE,Optimal design|Bayesian design|Computer experiment|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction,D-optimal|Bayesian D-optimal,2 design variables (injection time and initial/feed concentration),Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper develops and applies Bayesian optimal experimental design (OED) for parameter estimation in a mechanistic chromatography model (Equilibrium Dispersive Model with a two-component Langmuir isotherm). The design variables are injection time and feed (initial) concentration, and the design criterion is D-optimality implemented as expected information gain (KL divergence between posterior and prior). Because repeated PDE solves make nested Monte Carlo OED expensive, the authors introduce a surrogate based on Piecewise Sparse Linear Interpolation (PSLI) over sparse grids to approximate the forward map/likelihood efficiently while maintaining accuracy. The Bayesian inverse problem is solved with MCMC (DRAM), and OED utility surfaces are computed for different numbers of observation times; only synthetic data are used. Results indicate that strategically chosen designs reduce posterior uncertainty in the adsorption/dispersion parameters and that beyond threshold values (and beyond ~15 observation time instances) additional increases in injection time/concentration or sampling frequency provide little improvement.","Chromatography forward model is a nonlinear parabolic PDE system (EDM) with Langmuir isotherm: $\partial_\tau c_i + F\,\partial_\tau q_i + \partial_y c_i = \frac{1}{2N_{tp}}\partial^2_{y} c_i$, with $q_i = \frac{Q_s b_i c_i}{1+\sum_{\ell=1}^{N_c} b_\ell c_\ell}$ and injection boundary condition $c_{i,0}(\tau)=c_i^{\mathrm{Feed}}$ for $\tau\le \tau_{inj}$ and 0 otherwise. Observations are $c = G(\theta;d)+\eta$ with i.i.d. Gaussian noise $\eta\sim\mathcal N(0,\sigma^2 I)$. The Bayesian D-OED utility is expected information gain: $U(d)=\mathbb E_c\left[D_{KL}(p(\theta\mid c;d)\Vert p_0(\theta))\right]=\iint p(\theta\mid c;d)\log\frac{p(\theta\mid c;d)}{p_0(\theta)}\,d\theta\,p(c;d)\,dc$, approximated via (accelerated) nested Monte Carlo, with $G$ replaced by a PSLI sparse-grid surrogate $G_N$.","All results are simulation-based with synthetic data for $N_s\in\{8,15,20\}$ time samples (equidistant on $[0.5,9.5]$) and noise level $\sigma=0.05$; true parameters include $b_1=0.05$, $b_2=0.10$, $Q_s=10$, and $N_{tp}=70$. The design domain is $\tau_{inj}\in[0.05,3]$ and $c^{Feed}\in[1,15]$, and the surrogate is trained with $N=1105$ nodes (design-space grid used for training: 14 nodes per direction). Utility surfaces show strong dependence on $c^{Feed}$ and diminishing periodic/oscillatory structure in $\tau_{inj}$ as $N_s$ increases; beyond about 15 observation times, improvements in parameter uncertainty are reported to plateau. Computationally, the surrogate greatly reduces evaluation time versus the high-resolution PDE model (reported reductions of roughly an order of magnitude for utility evaluation and about three orders of magnitude for an 80,000-step MCMC chain, per Table 3).",The study uses exclusively synthetic data rather than real chromatographic experiments. The authors note that due to parameter uncertainty they do not optimize individual sampling instants; only injection time and injected concentration are treated as design variables. They also remark that theoretical smoothness/regularity verification of the forward map needed for interpolation guarantees is outside the scope of the work.,"The OED results may be sensitive to modeling assumptions (e.g., Gaussian i.i.d. measurement noise, outlet-only measurements, and the specific EDM/Langmuir structure) and may not transfer directly to real experiments with detector dynamics, baseline drift, or autocorrelated errors. Only two design variables are optimized and the surrogate is trained on a fixed grid; performance for broader operating ranges, additional controls (e.g., flow rate, gradient elution profiles), or higher-dimensional parameter/design spaces is not demonstrated. No public code or reproducible workflow is provided, making it difficult to validate surrogate construction, tuning choices (e.g., sparse-grid level, training design), or optimization settings.",None stated.,"Validate the Bayesian OED recommendations on real chromatographic experiments and assess robustness under realistic noise/measurement models (heteroskedasticity, correlation, detector effects). Extend the design space to include additional controllable variables (e.g., flow rate, column properties, gradient/elution profiles) and potentially optimize sampling times jointly with injection settings using robust/adaptive OED. Provide an open-source implementation (MATLAB/Python) and benchmark PSLI against alternative surrogates (GP/emulators, multifidelity methods) for discontinuities/high-gradient regimes typical in PDE-based chromatography.",2406.19835v2,https://arxiv.org/pdf/2406.19835v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:49:07Z
TRUE,Supersaturated|Screening|Other,Screening|Parameter estimation|Cost reduction,Other,"Variable/General (examples include k=31, 96, 144, 150, 192, 288; scalable to hundreds or thousands of factors)",Healthcare/medical|Pharmaceutical|Other,Simulation study|Case study (real dataset)|Other,TRUE,MATLAB,Supplementary material (Journal/Publisher),https://arxiv.org/abs/2407.06173,"The paper proposes CRowS (Constrained Row Screening), a new class of large row-constrained supersaturated two-level designs for high-throughput screening, where each well contains a pool of compounds but with a user-specified cap c on the number of compounds per well. Designs are constructed by approximately minimizing the unconditional UE(s^2) criterion (average squared off-diagonal elements of the augmented information matrix), extending classic E(s^2)-optimal supersaturated design construction to settings with non-balanced columns induced by row constraints. The authors develop a coordinate-exchange heuristic tailored to the row constraint, including fast update formulas for the objective when flipping or swapping entries, and provide initial lower bounds on the criterion under tight constraints. For analysis they adopt a main-effects model with k≥n and use the Lasso to identify active compounds, integrating design and analysis into the overall CRowS workflow. Via simulations and physical experiments on a VIM-2 metallo-β-lactamase inhibitor assay, they show CRowS improves screening performance over one-compound-one-well and the poolHiTS/STD pooling approach, while highlighting the impact of very tight pool-size constraints on performance.","Design optimality uses the unconditional E(s^2) criterion: with L=[1,X], S=L' L, and OS=\sum_{i<j} s_{ij}^2, the objective is UE(s^2)=OS/(k(k+1)). CRowS designs solve \min UE(s^2) subject to x_{ij}\in\{-1,1\} and \sum_{j=1}^k x_{ij}\le 2c-k for each row i, which enforces at most c entries at +1 per run (well). Data are analyzed under the main-effects model y=\beta_0\mathbf{1}+X\beta+\varepsilon with k\ge n, using the Lasso \min_{\beta_0,\beta} \frac{1}{2n}\|y-\beta_0\mathbf{1}-X\beta\|_2^2+\lambda\|\beta\|_1, with a post-thresholding and BIC-based model selection step in simulations.","In an empirical study for n=96, k=144, varying c, the row constraint becomes effectively non-binding around c\approx 60–70 (k/2=72), after which UE(s^2), slack, and true-positive rates (TPR) stabilize near the unconstrained behavior. Simulation comparisons across multiple (n,k,c) settings show CRowS attains markedly higher TPR than one-compound-one-well and generally better error control than poolHiTS/STD, while very small c (e.g., c=10) reduces TPR relative to larger c. In four proof-of-concept VIM-2 assays (k=31 with one known inhibitor plus 30 inert compounds), the Lasso profiles identified the known inhibitor in three experiments (including with c=10), while the most restrictive design (c=5) failed to detect it but produced no false positives. The paper also reports that sparse synergistic interactions can degrade hit detection in simulations, while antagonistic interactions can improve it, relative to a no-interaction baseline.","The authors note that while their construction algorithm is fast for the sizes studied, generating approximately optimal designs with both n and k in the thousands (e.g., 1,536-well plates and very large libraries) may take on the order of days. They also state that pooling can increase liquid-handling workload because each well includes up to c compounds, which must be traded off against reduced plate counts and improved hit rates. They further acknowledge potential concerns about statistical or chemical interactions (including ‘promiscuous inhibitors’) that can affect screening results and require additional study.","Performance and analysis are tied to a main-effects modeling framework with Lasso-based selection; if effects are not sparse, if effect directions are unknown, or if strong interactions/collinearity dominate, hit identification may degrade and the current workflow may require modification. The simulation protocol in the main comparison section assumes (for convenience) known mean/variance and approximate normality/independence of responses, whereas real HTS data often exhibit spatial/plate effects, heteroscedasticity, and other artifacts that could affect both design evaluation and selection thresholds. The proposed design criterion (UE(s^2)) targets global orthogonality properties rather than directly optimizing Lasso recovery probability or decision-theoretic screening loss under constraints, so optimality for the downstream selection task is not guaranteed. Practical guidance on choosing c, n, and k for a target power/FDR (beyond empirical plots) remains limited.","They propose further theoretical work to address feasibility/attainability questions behind the lower bounds (e.g., when designs can simultaneously achieve the best possible column-sum structure and pairwise column-distance structure). They also suggest more rigorous characterization of when row constraints are guaranteed to be tight (noting their empirical observation that tightness persists until c is about 40%–45% of k). They call for exploring larger-scale applications to evaluate real-world hit discovery performance and for developing adjustments to reduce false positives in large-effect settings or to use CRowS as a secondary screen to eliminate false positives from an initial screen.","Developing design criteria or construction methods that directly target sparse recovery (e.g., restricted isometry/coherence measures, Bayesian decision criteria, or explicit optimization of expected TPR/FDR under a generative model) could better align design optimality with Lasso-based hit calling. Extending CRowS to accommodate plate/layout effects, batch effects, and correlated errors (e.g., via blocking/row-column structures or mixed models) would increase robustness for real HTS operations. Incorporating interaction-aware modeling (e.g., hierarchical sparse interaction models) together with interaction-informed design constraints could mitigate losses under synergistic interactions. Providing open-source, scalable implementations (e.g., parallel GPU/cluster versions) and practical selection guidance (calibrated thresholds, self-starting variance estimation, and reproducible benchmarking on public HTS datasets) would improve adoption.",2407.06173v2,https://arxiv.org/pdf/2407.06173v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:49:50Z
TRUE,Computer experiment|Other,Parameter estimation|Other,Not applicable,"Variable/General (input $X\in\mathbb{R}^d$, output $Y\in\mathbb{R}^k$)",Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper studies asymptotic efficiency bounds and efficient influence functions for estimating global sensitivity indices—Sobol’ (variance-based) and Cramér–von Mises (distribution-based), including generalized and universal variants—under two data-generation/DOE contexts: the Pick-Freeze scheme and the Given-Data setting. In Pick-Freeze, the DOE provides paired outputs $(Y,Y')$ for each input realization (exchangeable bivariate sampling), enabling natural empirical estimators; in Given-Data, one observes i.i.d. pairs $(X,Y)$ and must handle conditional objects like $m(x)=\mathbb{E}[Y\mid X=x]$ or conditional CDFs. The authors derive the efficient influence functions and corresponding semiparametric efficiency bounds for the key components $(\psi,\phi_1,\phi_2)$ entering the standard index decomposition, for Sobol’, (generalized) Cramér–von Mises, and universal indices in both DOE settings. They discuss how known estimator constructions (empirical estimators in Pick-Freeze; kernel/one-step style estimators in Given-Data under smoothness/consistency conditions) can achieve these bounds, thereby clarifying what “optimal” inference means for Cramér–von Mises-type indices and extending the efficiency theory beyond Sobol’ indices. The work is largely theoretical and positions the results within semiparametric efficiency/influence-function methodology for sensitivity analysis of computer codes/black-box models.","The indices are written as $SI=(\psi-\phi_2)/(\phi_1-\phi_2)$ with components depending on the target measure: for Sobol’, $S=\mathrm{Var}(\mathbb{E}[Y\mid X]) / \mathrm{Var}(Y)$ and $\psi=\mathbb{E}[\mathbb{E}[Y\mid X]^2]=\mathbb{E}[\mathbb{E}[Y\mid X]Y]$ (Pick-Freeze alternative $\psi=\mathbb{E}[YY']$). For Cramér–von Mises, $SI=\frac{\int \mathrm{Var}(F(z\mid X))\,dF_Y(z)}{\int F_Y(z)(1-F_Y(z))\,dF_Y(z)}$ with $\psi=\int F(z\mid X)^2\,dQ(z)$ (and $Q=F_Y$ for the standard CvM index), $\phi_1=\int F_Y(z)\,dQ(z)$, $\phi_2=\int F_Y(z)^2\,dQ(z)$. Efficient influence functions are derived for these components under Pick-Freeze (exchangeable $(Y,Y')$) and Given-Data ($(X,Y)$) models using semiparametric tangent spaces and the Riesz representation of pathwise derivatives.","In Pick-Freeze, the paper gives closed-form efficient influence functions for Sobol’ components, e.g., for $\psi=\mathbb{E}[YY']$, $\psi^e(y,y')=yy'-\psi$, and analogous symmetric forms for $\phi_1=\mathbb{E}[Y^2]$ and $\phi_2=\mathbb{E}[Y]^2$. For generalized CvM indices, it provides efficient influence functions for $\psi_Q=\int \mathbb{P}(Y\le z, Y'\le z)\,dQ(z)$ and for $\phi_{1,Q},\phi_{2,Q}$, and extends to the case where $Q$ is unknown (requiring augmentation terms from the $Q$-tangent space). In Given-Data, it derives the efficient influence function for $\psi=\mathbb{E}[\mathbb{E}[Y\mid X]^2]$ as $\psi^e(x,y)=(2y-m(x))m(x)-\psi$, and for generalized CvM it gives $\psi_Q^e(x,y)=\int (2\mathbf{1}\{y\le z\}-p(z\mid x))p(z\mid x)\,dQ(z)-\psi_Q$. The paper argues that empirical estimators achieve efficiency in Pick-Freeze, while in Given-Data efficiency can be achieved via one-step procedures given sufficiently strong consistency of preliminary estimators of $m(x)$ or conditional CDF/probability functions.",None stated.,"The paper is primarily theoretical and does not appear to provide a comprehensive simulation study or real-data case study demonstrating finite-sample performance of the efficient procedures, especially in high-dimensional $X$ where nuisance estimation is challenging. Achieving the stated efficiency in the Given-Data setting relies on strong conditions on preliminary estimators (e.g., smoothness/consistency rates for nonparametric regression or conditional CDF estimation), which may be hard to meet in practice. The treatment is focused on i.i.d. sampling and does not address common applied complications such as dependent inputs, correlated observations, or model/code stochasticity beyond the discussed frameworks.",None stated.,"Develop and benchmark practical, stable implementations (with tuning/regularization guidance) of the one-step efficient estimators for generalized CvM and universal indices in moderate-to-high dimensions, including cross-fitting/sample-splitting to relax empirical process conditions. Extend the efficiency theory to settings with dependent inputs, autocorrelated outputs, or stochastic computer models with more complex replication structures than Pick-Freeze. Provide finite-sample studies and real-world case studies comparing efficient estimators to standard Monte Carlo sensitivity workflows, and investigate robust versions under heavy tails or outliers.",2407.15468v2,https://arxiv.org/pdf/2407.15468v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:50:26Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Prediction|Model discrimination|Optimization|Cost reduction|Other,D-optimal|A-optimal|I-optimal (IV-optimal)|G-optimal|E-optimal|V-optimal|Bayesian D-optimal|Bayesian A-optimal|Compound criterion|Space-filling|Minimax/Maximin|Other,"Variable/General (design variables depend on application; examples include sensor locations/times, observation selection, covariate settings; regression features dimension p; n observations; candidate set size m).",Healthcare/medical|Pharmaceutical|Environmental monitoring|Food/agriculture|Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/2407.16212,"This article is a comprehensive survey of optimal experimental design (OED), covering formulations of design criteria and computational methods to evaluate and optimize them for modern, complex (often nonlinear, high-dimensional, and PDE-based) models. It reviews classical alphabetic optimality for linear models (A-, D-, E-, G-, I-/V-optimality and variants) and emphasizes Bayesian/decision-theoretic utilities, particularly expected information gain (mutual information) for parameter learning, goal-oriented prediction, and model discrimination. The paper surveys computational strategies for estimating information-based criteria, including nested Monte Carlo estimators (and multilevel/importance-sampling refinements), variational bounds via density/transport approximations and contrastive estimators, and dimension-reduction methods (e.g., likelihood-informed subspaces) for high-dimensional settings. It also surveys optimization algorithms for design construction in both discrete/combinatorial settings (subset selection, submodular methods, convex relaxations and rounding) and continuous settings (gradient-based and derivative-free stochastic optimization). Finally, it presents emerging approaches for sequential/non-myopic OED framed as Markov decision processes, including approximate dynamic programming, reinforcement learning, and bound-based methods, and highlights open challenges such as robustness to model misspecification and scalable inference/design for complex systems.","The survey frames parametric models as $p(y\mid\theta,\xi)$ and defines the Fisher information $F(\theta,\xi)=\mathbb{E}[\nabla_\theta\log p(Y\mid\theta,\xi)\,\nabla_\theta\log p(Y\mid\theta,\xi)^\top]$. Classical criteria include D-optimality via $\min_{\xi}\log\det(F(\xi)^{-1})$ and A-optimality via $\min_{\xi}\mathrm{tr}(F(\xi)^{-1})$, with Bayesian analogues using posterior covariance $\Gamma_{\Theta\mid Y}(\xi)=(G(\xi)^\top\Gamma_{Y\mid\Theta}(\xi)^{-1}G(\xi)+\Gamma_\Theta^{-1})^{-1}$. The central Bayesian decision-theoretic criterion is expected information gain (mutual information): $U_{\mathrm{KL}}(\xi)=\mathbb{E}_{Y,\Theta\mid\xi}\big[\log\tfrac{p(Y\mid\Theta,\xi)}{p(Y\mid\xi)}\big]=I(Y;\Theta\mid\xi)$, which reduces to Bayesian D-optimality in the linear-Gaussian case.","The paper reports that in linear-Gaussian models, maximizing expected information gain is equivalent to Bayesian D-optimal design, with a closed form $I(Y;\Theta\mid\xi)=\tfrac12(\log\det\Gamma_\Theta-\log\det\Gamma_{\Theta\mid Y}(\xi))=\tfrac12\sum_j\log(1+\lambda_j)$ for generalized eigenvalues $\lambda_j$. It summarizes known performance properties of nested Monte Carlo EIG estimators: the vanilla estimator has $\mathrm{bias}=C_1(\xi)/M+O(M^{-2})$ and $\mathrm{Var}=C_2(\xi)/N+C_3(\xi)/(NM)+\cdots$, implying optimal MSE rate $O(W^{-2/3})$ under budget $W=(M+1)N$. It highlights that multilevel Monte Carlo variants can recover $O(\epsilon^{-2})$ complexity to achieve MSE $\epsilon^2$ for EIG estimation under certain constructions, improving over standard nested MC. It also notes set-function structure results used in design optimization, e.g., log-determinant (D-optimality) is submodular in discrete selection settings and mutual information objectives are submodular under conditional independence, enabling greedy algorithms with constant-factor guarantees.","As a survey, the article notes multiple open challenges rather than a single method’s limitations; it emphasizes that information-based criteria can be difficult to evaluate in practice due to strong nonlinearities, high parameter dimension, high per-sample cost, and implicit/likelihood-free models. It also highlights that sequential/non-myopic OED is computationally demanding, with scalability and efficient belief-state representations (beyond storing full histories) remaining major obstacles. The paper further notes that many advanced estimators/bounds and sequential RL-based methods still lack strong numerical analysis and approximation guarantees in complex settings.","Because this is a survey, many algorithmic claims depend on cited papers; the article does not provide a unified empirical benchmark suite across criteria estimators and optimizers, so practitioners may still lack clear guidance on method selection under realistic industrial constraints. It discusses software and computational methods extensively but does not provide reference implementations, which can slow adoption and reproducibility. Application domains are broad and illustrative; domain-specific practical constraints (e.g., experimental logistics, regulatory constraints in trials, measurement failure mechanisms) are not systematically encoded into worked end-to-end case studies. Some key modern DOE topics (e.g., definitive screening designs, robust parameter design, mixture designs) are discussed mainly as “other”/context rather than being developed to the same depth as OED for complex models.","The article explicitly calls for advances in sequential OED: more efficient belief-state/posterior representations, scalable computation of posteriors and information measures in higher dimensions, improved sample efficiency and architectures for reinforcement-learning-based policies, and stronger approximation/convergence theory for value functions and policies. It also highlights open needs in robustness, including OED under model misspecification and distributionally robust/robust-Bayesian formulations, and risk-aware (beyond expectation) utilities. Additional opportunities include better tailoring of optimization algorithms to design-criterion structure, and improved ways to quantify when feedback and lookahead in sequential design are worth their computational cost.","Developing open-source, modular software (with standard test problems and benchmarking protocols) for OED criteria estimation (nested MC, MLMC, variational bounds, transport) and design optimization (discrete and continuous) would materially improve reproducibility and adoption. More systematic comparisons of variational MI bounds (transport/flows vs. critic-based vs. contrastive) under controlled regimes (high MI, heavy tails, implicit models, strong nonlinearity) would clarify practical tradeoffs. Extending OED computations to handle common data issues—autocorrelation, nonstationarity, missingness, censored data, and unknown/estimated noise models—in a unified way would broaden applicability. Finally, integrating modern constraint-handling (safety, ethics, resource scheduling) into sequential OED/RL formulations with provable guarantees would be a high-impact direction for real deployments.",2407.16212v1,https://arxiv.org/pdf/2407.16212v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:51:14Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Screening|Cost reduction|Other,D-optimal|Other,"Variable/General (examples include monomer library sizes M=10 and M=3; polymer length l in [10,70]; k-mers with k=2; feature dimension F depends on encoding; subset sizes K=100 and R=81 in simulations)",Pharmaceutical|Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes efficient approximate algorithms for constrained Design of Experiments in copolymer (including nucleic-acid therapeutics) engineering, where the goal is to select an optimal subset of candidate polymers to test. It reformulates several constrained “balanced coverage” polymer-design objectives as selecting a K-densest subgraph in a typically very dense compatibility graph, and introduces two ADMM-based approximate integer-programming approaches (an lp-box ADMM relaxation and a sparse-ADMM variant for weighted relationships). For general numerical feature encodings, it presents an ADMM-style approximation method for subset selection under D-optimality, i.e., choosing rows of a given design matrix to maximize the determinant of the Fisher information matrix. Performance is demonstrated via simulations with planted optimal structures (cliques and planted D-optimal subsets), showing high recovery or near-optimal designs in dense regimes where degree-based heuristics are ineffective. The work emphasizes retrospective/subset-selection DoE under real-world synthesis/feasibility/budget constraints rather than generative classical designs (e.g., BIBD) and connects to active learning/Bayesian experimental design motivations.","Dense-subgraph DoE is posed as an integer program on the complement graph: minimize $x^T\bar A x$ subject to $x_i\in\{0,1\}$ and $\sum_i x_i=K$ (Eq. 2), then relaxed via lp-box constraints $x\in[0,1]^S\cap\{\|x-\tfrac12\|_p^p=S/2^p\}$ (Eq. 3) and solved with ADMM updates. Weighted/soft-membership variant adds an $\ell_1$ sparsity penalty: $\min_x\; x^T\bar W x+\lambda\|x\|_1$ with $x\in[0,1]^S$ and $\sum_i x_i=K$ (Eq. 23). D-optimal subset selection is written as minimizing the negative log-determinant of the information matrix: $\min_x -\log\det(\sum_i x_i a_i a_i^T)$ with $x_i\in[0,1]$ and $\sum_i x_i=R$ (Eq. 26), using an ADMM scheme with Sherman–Morrison-based matrix-inverse updates (Eqs. 29–33).","In clique-recovery simulations for dense-subgraph DoE (900 random polymers plus a planted clique of size 100, monomer library size $M=10$, $k=2$, polymer lengths $l\in\{10,15,\dots,70\}$, 500 runs), the Sparse-ADMM approach recovered the planted clique in 100% of runs across lengths, while the lp-box AIP-ADMM recovered it in 100% of runs for $l\ge 20$ and produced very dense near-optimal subgraphs for shorter lengths. For larger graphs (total size about 1000–4000 nodes with a planted clique of 100 at $l=50$), both algorithms achieved 100% recovery even when the clique was only about 1/40 of the graph. For a harder dense regime ($l=10$, median graph density about 0.90–0.92), exact clique recovery often failed but the methods still found subgraphs with density around ~0.98 (substantially above random). For D-optimal subset selection, a planted $R=81$ design embedded among 100 random polymers (total 181) was not exactly recovered over 100 runs, but the best found designs were full-rank and achieved near-maximal sums of log-eigenvalues of $A_R^T A_R$ compared to typical random subsets that were rank-deficient.",None stated.,"Results are demonstrated almost entirely via simulated/planted-structure experiments; there is no end-to-end real experimental case study showing improved learning or property prediction from the selected polymer subsets under laboratory constraints. The proposed ADMM formulations are non-convex in key settings (lp-sphere constraint; indefinite quadratic forms), so solution quality depends on initialization and hyperparameters and lacks global optimality guarantees; comparisons to strong baselines for densest-subgraph and D-optimal subset selection (e.g., Charikar-style heuristics, modern k-clique/k-densest solvers, leverage-score/greedy D-optimal row selection) are limited. Practical deployment would also require clear guidance for translating domain constraints into graph edges/weights or feature encodings, and sensitivity analyses to noise/misspecification in those encodings are not explored.","The author suggests (i) benchmarking these algorithms against methods developed for typical sparse real-world networks (social/web/communication graphs) to test performance in other graph regimes, and (ii) analyzing time and memory costs more deeply, noting that the sparse conjugate-gradient linear solve inside ADMM is the main computational bottleneck. The paper also notes broader applicability beyond polymer design (general molecular and experimental design subset selection) but leaves further validation to future work.","Provide open-source reference implementations and reproducible benchmarks (including wall-clock time, scaling, and ablations on hyperparameters/initializations) to facilitate adoption and fair comparison. Extend the methods to explicitly handle measurement noise, model uncertainty, and sequential updating (true adaptive DoE/active learning loops) rather than one-shot subset selection from a fixed candidate set. Validate on real polymer/nucleic-acid experimental campaigns with downstream metrics (improved regression/ML performance, better uncertainty reduction, or faster discovery) and investigate robustness to correlated/structured constraints and to autocorrelated or batch effects in laboratory measurements.",2408.02166v1,https://arxiv.org/pdf/2408.02166v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:51:52Z
FALSE,NA,NA,Not applicable,Variable/General,Healthcare/medical|Theoretical/simulation only|Other,Other,NA,R|Python,Not applicable (No code used),NA,"This paper is a narrative primer for glial biologists on experimental design and analysis considerations for single-cell and single-nucleus ‘omics (sc/snRNA-seq, snATAC-seq, and multiome). It emphasizes experimental planning topics such as whether to use cells vs nuclei, enriching for rare glial populations, avoiding untracked pooling by using multiplexing, and counterbalancing/randomizing samples across library and sequencing batches to prevent confounding. On the analysis side, it outlines a typical workflow including QC (thresholding, doublet detection, ambient RNA correction), normalization choices, optional batch correction for visualization/integration, clustering and cell-type/state annotation, and differential expression (including cautions against using batch-corrected/denoised matrices for DE due to type-I error inflation). It also discusses downstream analyses such as pathway enrichment, regulatory network/ligand–receptor inference, trajectory analysis, and compositional modeling, with guidance on when pseudobulk approaches are appropriate for cohort-level comparisons. Overall, the contribution is practical guidance rather than proposing a new DOE method, control-chart method, or novel experimental design construction criterion.",Not applicable,NA,None stated,"As a broad primer, recommendations may not transfer cleanly across platforms (10x vs plate-based), modalities (RNA vs ATAC vs multiome), and study goals without more prescriptive decision rules or validated checklists. Many suggestions (e.g., choosing normalization/batch correction by visualization/diagnostic metrics, running multiple tools and intersecting results) can be subjective and may reduce reproducibility without explicit quantitative thresholds and preregistered analysis plans. The article does not provide worked power/sample-size examples or validated calculators tailored to common glial study designs, which limits actionable guidance for planning N (donors vs cells per donor).",None stated,"Providing concrete, modality-specific sample-size/power calculation examples (e.g., donors × cells per donor × sequencing depth) and templates for counterbalancing/multiplexing would make the experimental design guidance more actionable. A benchmarking-style companion with standardized datasets for glial contexts comparing QC, normalization, batch correction, DE, and compositional methods (with recommended defaults) would improve prescriptiveness and reproducibility. Releasing reference implementations (e.g., R/Python notebooks) that operationalize the decision points (including diagnostics and reporting checklists) would facilitate adoption and consistent reporting across labs.",2408.06521v1,https://arxiv.org/pdf/2408.06521v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:52:11Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Cost reduction|Other,D-optimal|A-optimal|E-optimal|Other,"Variable/General (controls over time u(t), sampling decisions w(t), and parameters p=(p̂, θ); examples include 2 mechanistic parameters + up to 151 ANN weights; SVD truncations ns=1..10; layerwise lumping uses J layers (e.g., J=4).",Food/agriculture|Manufacturing (general)|Pharmaceutical|Other,Simulation study|Other,TRUE,Julia|Other,Not provided,https://arxiv.org/abs/2408.07143,"The paper proposes optimal experimental design (OED) methods for universal differential equations (UDEs), i.e., mechanistic ODE models in which unknown terms are replaced by embedded neural networks. It formulates OED as a mixed-integer optimal control problem with time-varying controls and binary/relaxed sampling decisions, using Fisher information matrix (FIM)-based criteria (A-, D-, E-optimality) to plan informative experiments for joint estimation of mechanistic parameters and ANN weights. To address overparameterization and ill-posedness from large neural networks, it introduces and compares dimension-reduction strategies: truncated SVD of the FIM and a new “lumping” approach that scales groups of ANN weights (all weights or layerwise) via artificial parameters, yielding lower-dimensional variational differential equations and regularized FIMs. Numerical studies (Lotka–Volterra with an ANN interaction term; urethane reaction kinetics with an ANN rate term) show OED improves data-efficiency and extrapolation compared with equidistant sampling, with optimized controls and sampling yielding large reductions in OED objectives. The work also uses a global information gain function (from Pontryagin conditions) to analyze sampling structure and proposes it as a heuristic to choose SVD truncation levels in ill-posed OED problems.","The UDE is defined by an ODE with hybrid RHS $\dot x(t)=f_S(\hat f(x(t),u(t),\hat p),\,U(x(t),\theta))$ where $U$ is a feed-forward ANN. OED augments the dynamics with variational differential equations for sensitivities $G=\partial x/\partial p$: $\dot G(t)=f_x(x,u,p)G(t)+f_p(x,u,p)$, and evolves the Fisher information matrix via $\dot F(t)=\sum_{i=1}^{n_y} w_i(t)\,(h_x^i(x(t))G(t))^\top(h_x^i(x(t))G(t))$. The design minimizes a scalar criterion $\phi(F(t_f))$ such as A-optimality ($\mathrm{tr}(F^{-1})$), D-optimality ($(\det(F^{-1}))^{1/n_p}$), or E-optimality (max eigenvalue of $F^{-1}$), over controls $u(t)$ and sampling decisions $w(t)\in\{0,1\}^{n_y}$ (often relaxed to $[0,1]^{n_y}$).","In the Lotka–Volterra ODE parameter-estimation example, optimizing sampling and/or controls reduced posterior uncertainty substantially compared with equidistant sampling and zero control (e.g., standard deviations for $(\hat p_1,\hat p_3)$ drop from about (0.057, 0.082) in $w_0$–$u_0$ to about (0.017, 0.006) in $w^*$–$u^*$). For ANN-weight training in the Lotka–Volterra UDE, optimized controls typically reduced the D-criterion by roughly an order of magnitude versus no control across reduction strategies (Table 2), and SVD-based reductions often achieved the lowest reported objective values (e.g., $\phi_D\approx1.53\times10^{-4}$ for $w^*$–$u^*$–SVD2). In concurrent estimation (mechanistic parameter plus ANN weights), increasing the SVD truncation level $n_s$ generally improves the A-criterion (Table 3) and the paper empirically shows convergence of optimal samplings with $n_s\to n_t$ using scaled global information gain functions. In the urethane reaction case study, SVD-based reductions outperform lumping in the reported D-criterion values (Table 4), while lumping remains computationally simpler but less effective for that benchmark.","The authors note that concurrent estimation of mechanistic parameters and ANN weights lacks efficient, principled algorithms; for their benchmarks they use an iterative heuristic alternating between Gauss–Newton for parameter estimation and Adam for ANN training. They also state that deciding how often to recompute SVDs during OED optimization (since the best truncation depends on the current iterate) is an open question. Additionally, because (OED) is nonconvex and solved with a local NLP solver (Ipopt), results can reflect local optima; this is explicitly mentioned for the urethane example where outliers are attributed to local optima and noise.","The approach is largely local/linearization-based through the FIM around an initial guess $\bar p$, so performance may degrade under strong nonlinearity, multi-modality, or large parameter uncertainty typical in UDE training. The method assumes a well-posed sensitivity system and effectively relies on differentiability and solver stability; stiff dynamics or discontinuous controls/sampling could make gradients/sensitivities unreliable. Comparisons are primarily to equidistant sampling and among proposed reductions; broader baselines from Bayesian OED/active learning (e.g., entropy/information-theoretic, GP-based, Bayesian neural nets) are not empirically benchmarked. Practical experimental constraints (actuator limits beyond simple bounds, measurement delays, correlated noise, model mismatch) are not deeply explored, which may affect real-world applicability.","They suggest using global information gain as (i) a criterion for choosing the SVD truncation hyperparameter, (ii) a heuristic to decide when to update SVD decompositions during optimization, and (iii) a basis to design penalizations/regularizers in sequential (online) OED for UDE to obtain ANN weights that still fit data while yielding better sampling designs. They also position lumping as potentially exploitable in settings such as training sparse ANNs where interpretability of grouped sensitivities could be beneficial.","A natural next step is extending the framework to Bayesian OED for UDEs (posterior-averaged criteria, priors over ANN weights) and assessing robustness to large initial-guess errors. Developing self-starting or adaptive/sequential designs that update the reduced subspace (SVD/lumping groups) online with uncertainty quantification could improve reliability for real experiments. Broader validation on real experimental datasets (not only synthetic benchmarks) and inclusion of autocorrelated/non-Gaussian measurement noise would strengthen practical impact. Releasing reference implementations and benchmarks (e.g., DynamicOED.jl scripts for the paper) would improve reproducibility and accelerate adoption.",2408.07143v2,https://arxiv.org/pdf/2408.07143v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:52:56Z
TRUE,Optimal design|Bayesian design|Other,Prediction|Parameter estimation|Model discrimination|Other,Other,"Variable/General (examples include 1D, 2D, and N-dimensional extensions; design variable ξ is 1D/2D in benchmarks and scalar in applications)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/atlantac96/LF-GO-OED.git,"The paper proposes LF-GO-OED, a likelihood-free goal-oriented Bayesian optimal experimental design method that maximizes expected information gain (mutual information) on downstream quantities of interest (QoIs) rather than on model parameters. It targets settings with nonlinear observation and prediction models and, crucially, implicit/intractable likelihoods. The approach uses approximate Bayesian computation (ABC) to generate posterior(-predictive) samples without likelihood evaluations, then trains supervised density-ratio estimators (uLSIF) to approximate the key ratios in the QoI EIG objective, avoiding explicit posterior-predictive density estimation (e.g., KDE). The method is validated on tractable 1D/2D nonlinear benchmarks (matching nested Monte Carlo or agreeing with an MCMC+KDE GO-OED baseline) and demonstrated on implicit scientific models (stochastic SIR epidemiology and stochastic FitzHugh–Nagumo neuroscience), showing goal-oriented optimal designs can differ substantially from parameter-learning designs. The main practical implication is an implementable GO-OED pipeline for implicit models where likelihood-based Bayesian OED is infeasible, with the tradeoff of needing per-design training of ratio estimators and ABC tuning.","The goal-oriented expected utility is the mutual information between data Y and QoI Z under design ξ: $U_Z(\xi)=\iint p(z,y\mid\xi)\log\frac{p(z\mid y,\xi)}{p(z)}\,dy\,dz$, equivalently $U_Z(\xi)=\iint p(z,y\mid\xi)\log\frac{p(y\mid z,\xi)}{p(y\mid\xi)}\,dy\,dz$. LF-GO-OED replaces the intractable density ratios with learned estimators $\hat r_1(z,y)\approx p(z\mid y,\xi)/p(z)$ or $\hat r_2(z,y)\approx p(y\mid z,\xi)/p(y\mid\xi)$ and estimates utility via Monte Carlo: $\hat U_{Z,1}(\xi)=\frac1N\sum_{i=1}^N\log\hat r_1(z_i,y_i)$ or $\hat U_{Z,2}(\xi)=\frac1N\sum_{i=1}^N\log\hat r_2(z_i,y_i)$ with $(z_i,y_i)\sim p(z,y\mid\xi)$. Posterior sampling needed to train these ratios is done via ABC using summary statistics and a tolerance $\epsilon$ with acceptance based on $D(S(y'),S(y_{obs}))<\epsilon$, and the ratio learner uses uLSIF least-squares density-ratio fitting.","On a nonlinear 1D benchmark where $Z=\Theta$, LF-GO-OED produced expected utility curves nearly identical to a nested Monte Carlo estimator, with least-informative design at $\xi=0$ and local maxima at $\xi=0.2$ and $\xi=1$. On a nonlinear 2D benchmark with Rosenbrock QoI, LF-GO-OED found an optimum around $(\xi_1^*,\xi_2^*)=(0,0.2)$ with $\hat U_{Z,1}\approx 3.17$, matching the MCMC+KDE GO-OED optimum location (reported $U_Z\approx 2.97$). For the implicit stochastic SIR model, the non-goal-oriented case ($Z=\Theta$) yielded an optimal observation time near $\xi^*=0.4$ with $\hat U_{Z,1}(\xi^*)\approx 2.07$, while a goal-oriented QoI $Z=R(0.3)+R(0.4)+R(0.5)$ shifted the optimum to about $\xi^*=0.2$ with $\hat U_{Z,1}\approx 0.64$. For the stochastic FitzHugh–Nagumo model, the parameter-learning design was around $\xi^*=0.69$ with $\hat U_{Z,1}\approx 1.94$, whereas a goal of spike-rate at current 0.2 shifted the optimum to about $\xi^*=0.78$ with $\hat U_{Z,1}\approx 0.77$.","The authors note LF-GO-OED can be computationally taxing because it trains a density-ratio estimator for each candidate design and data realization. They also state that ABC-based OED is mainly effective in low-dimensional design spaces and requires careful choice/design of summary statistics, distance metric, and tolerance/threshold, which can be time-consuming to tune.","The method’s utility estimates depend on the accuracy/stability of density-ratio estimation (uLSIF) and ABC posterior approximation; errors from these two approximations can compound and may bias the design objective, especially in higher-dimensional data/QoI settings. Because ratio estimators are (re)trained per design, comparisons across designs may have differing estimator variance, which can make optimization noisy and potentially mis-rank designs without careful replication/variance control. The approach relies on user-chosen summaries for ABC (not learned end-to-end), which may discard information relevant to the QoI and thus change the inferred optimal design; this risk is not fully quantified. Real-world validation is limited to simulated/benchmarked models; there is no discussion of robustness to model misspecification or to practical experimental constraints (costs, batching, sequential/adaptive experimentation) beyond brief mentions of optimization approaches.","They propose developing accurate amortized versions of the density-ratio estimator to reduce the per-design training burden. They also suggest exploring more efficient and scalable alternatives to ABC for OED, given ABC’s limitations in higher-dimensional settings and its need for tuning summary statistics, distance metrics, and thresholds.","Investigate end-to-end learned summary statistics (e.g., neural summaries) tailored to QoI-mutual-information objectives to reduce manual ABC tuning and information loss. Provide theoretical or empirical uncertainty quantification for the estimated utility surface (e.g., confidence intervals/credible bands for $U_Z(\xi)$) and incorporate this uncertainty into the design optimization (robust or risk-aware OED). Extend LF-GO-OED to sequential/multi-stage experimental design (adaptive policies) with explicit budget/cost constraints and to multi-fidelity simulators. Benchmark against more recent simulation-based inference methods (e.g., neural posterior/likelihood/ratio estimation, classifier two-sample tests) under identical compute budgets, and study scalability to higher-dimensional QoIs where uLSIF degrades.",2408.09582v1,https://arxiv.org/pdf/2408.09582v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:53:34Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Optimization|Prediction|Robustness|Cost reduction|Other,D-optimal|Bayesian D-optimal|Compound criterion|Other,Variable/General (n units; treatment probabilities p or p_j; covariates dimension d; time periods T in panel settings),Food/agriculture|Healthcare/medical|Manufacturing (general)|Service industry|Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This manuscript is a tutorial-style paper on experimental design for causal inference framed explicitly as optimization problems. It formalizes randomized experiment designs (e.g., Bernoulli and completely randomized designs) and covariate-dependent designs (stratified randomization, matched-pair designs), and connects them to three decision-theoretic optimization frameworks: robust (minimax), stochastic/Bayesian (risk minimization under a prior/data-generating process), and deterministic (model-based) optimization. It presents and synthesizes optimality results such as minimax optimality of balanced treatment allocation under permutation invariance and bounded-outcome uncertainty sets, and optimal matched-pair stratification under a baseline function derived from covariates. Under linear models, it links causal-design problems to classical optimal design criteria (DA-optimality for treatment assignment; D-optimality for subset selection) and discusses computational approaches (e.g., reductions to Max-Cut and connections to semidefinite programming). It also covers modern experimental settings including switchback/temporal experiments and synthetic control “designs,” and surveys active research directions when standard causal assumptions (no interference, homogeneity, non-adaptivity) fail.","Key design definitions include Bernoulli randomization with independent assignments $\eta(w)=\prod_{j=1}^n p^{\mathbb{1}\{w_j=1\}}(1-p)^{\mathbb{1}\{w_j=0\}}$ and completely randomized designs that condition on exactly $pn$ treated units. Causal estimators central to the design criteria are the difference-in-means $\hat\tau_{DM}=\frac{\sum_j Y_j\mathbb{1}\{W_j=1\}}{\sum_j \mathbb{1}\{W_j=1\}}-\frac{\sum_j Y_j\mathbb{1}\{W_j=0\}}{\sum_j \mathbb{1}\{W_j=0\}}$ and IPW $\hat\tau_{IPW}=\frac{1}{n}\sum_j \frac{Y_j\mathbb{1}\{W_j=1\}}{\Pr(W_j=1)}-\frac{1}{n}\sum_j \frac{Y_j\mathbb{1}\{W_j=0\}}{\Pr(W_j=0)}$. The robust-design objective is posed as minimax risk $\min_\eta\max_{y(1),y(0)\in\mathcal Y} \mathbb{E}_{W\sim\eta}(\hat\tau-\tau)^2$, yielding balanced allocations in the paper’s main examples; in model-based (deterministic) design under $Y=Z\theta+\varepsilon$, the variance of OLS $\mathrm{Var}(\hat\theta)=\sigma^2(Z^\top Z)^{-1}$ leads to DA-optimal assignment equivalently maximizing $w^\top(I - X(X^\top X)^{-1}X^\top)w$.","Under permutation-invariant uncertainty over unit fixed effects in an additive model (Wu 1981 setting), a balanced completely randomized design assigning $n/2$ units to treatment and $n/2$ to control is shown minimax optimal (Theorem 1). Under bounded potential outcomes $Y_j(w)\in[-b,b]$ and Bernoulli designs with unit-level probabilities $p_j$, the minimax solution for IPW estimation is the balanced Bernoulli design $p_j=1/2$ for all units, with worst-case outcomes at the bounds $y_j(1)=y_j(0)=\pm b$ (Theorem 2). For covariate-based stratification with within-stratum complete randomization and equal treatment probability 1/2, the optimal stochastic design reduces to matched pairs: sorting by the baseline function $g(x)$ and pairing adjacent units minimizes the MSE proxy (Theorem 3). In the deterministic/model-based framework, the treatment-assignment problem is linked to DA-optimality and shown equivalent to maximizing a quadratic form $w^\top(I - X(X^\top X)^{-1}X^\top)w$, and classical D-optimal subset selection is presented as minimizing $\det(\sum_{j\in S} x_j x_j^\top)^{-1}$.","The manuscript states it is a narrower tutorial scope focused on causal-inference experimental design through an optimization lens, and explicitly omits large parts of the classical optimal experimental design literature and optimal Bayesian design literature except where they overlap with causal design (noted in the introduction). It also adopts only the potential outcomes framework and omits the structural causal model approach. It notes that other group invariances beyond permutation invariance (e.g., rotation groups) are beyond scope, and that algorithmic/computational details (e.g., approximation algorithms for DA-optimal design) are skipped.","Many optimality results highlighted (e.g., minimax balance) rely on strong symmetry/boundedness or homoscedastic linear-model assumptions; robustness to heteroskedasticity, clustering, and heavy tails is not systematically developed. The paper is largely theoretical/tutorial and does not provide comprehensive simulation benchmarks or empirical validations comparing designs across realistic data-generating processes and constraint sets (e.g., budget, logistics, interference). Several “design” problems (e.g., synthetic control design) are formulated as challenging optimization programs, but practical guidance on tuning, feasibility under constraints, and sensitivity to misspecification (baseline function, linear factor model rank) is limited. Extensions to high-dimensional covariates (d comparable to n), multiple treatments, or constrained randomization common in practice are discussed only selectively.","The conclusion emphasizes that the optimization framing opens a range of research opportunities to study causal experimental design under different uncertainty models (robust, stochastic, deterministic) and recommends choosing frameworks based on uncertainty faced by the experimenter. It also highlights, in the discussion of assumption violations (Section 6), active directions including interference (network/marketplace/temporal), treatment heterogeneity with covariate balancing (including covariate-adaptive designs), and adaptive/response-adaptive experiments with valid inference (including sequential testing/anytime-valid inference).","Develop unified computational toolkits and benchmarks that implement the paper’s robust/stochastic/deterministic design formulations for common experimental constraints (costs, minimum detectable effect, multiple metrics, ramp-up limits) and compare against standard A/B practice. Extend key optimality results to settings with unknown nuisance parameters, heteroskedasticity, cluster dependence, and interference models with misspecified exposure mappings, including robust or distributionally-robust variants. Study multi-arm and continuous-treatment versions of the optimization-based design criteria, including adaptive allocation rules with guaranteed inference validity and controlled risk (e.g., always-valid confidence sequences). Provide practical guidance for learning baseline functions and priors from historical data while accounting for distribution shift and feedback loops, and analyze the sensitivity of “optimal” partitions/assignments to these learned components.",2408.09607v2,https://arxiv.org/pdf/2408.09607v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:54:18Z
TRUE,Computer experiment|Sequential/adaptive,Optimization|Prediction,Space-filling,5 factors (AC simulation control parameters); initial ensemble 5000 runs (4182 valid),Transportation/logistics|Energy/utilities,Case study (real dataset)|Other,TRUE,Other,Not provided,https://www.danfoss.com/en/service-and-support/downloads/dcs/coolselector-2/|https://www.maplesoft.com|https://www.mathworks.com|https://www.wolfram.com/mathematica,"The paper proposes an “Interactive Design of Experiments” (IDoE) workflow for optimizing a complex air-conditioning (cooling) system simulated with a computationally expensive, non-invertible numerical model. It starts from an offline initial ensemble (space-filling Latin Hypercube sample) and iteratively refines sampling in regions of interest selected interactively in an augmented pressure–enthalpy (p–h) diagram, supported by coordinated multiple views. A deep neural network is trained as an approximate inverse model mapping desired output characteristics (four characteristic points in the p–h diagram) to suggested control parameters, which are then validated and locally perturbed via additional simulation runs. The method is demonstrated in an automotive use case with five control parameters, where interactive iterations improve coefficient of performance (COP) and accelerate convergence relative to manual trial-and-error. The contribution is primarily an interactive/sequential DOE methodology for simulation-based optimization rather than classical factorial/response-surface DOE.",Not applicable,"The simulation model has five control parameters; an initial Latin Hypercube ensemble of 5000 runs (4182 valid) is generated with ~10 seconds per run (~14 hours total). The inverse surrogate is a fully connected DNN with three hidden layers (Tanh), trained for 300 epochs with AdamW and MSE loss; it achieves an average test MSE of 0.25. In the demonstrated scenario, the workflow reaches a higher COP in three interactive iterations, using ~20 simulation refinements per iteration with ±5% perturbations around DL-predicted settings. Domain expert feedback reports analysis speedups of about an order of magnitude (days reduced to hours).","The authors state limitations in scalability to higher-dimensional control spaces: with parallel coordinates, the practical limit is about a dozen control parameters. They also note the approach depends on relatively fast simulations to generate iterative refinements (their users tolerated a few minutes for ~20 runs), though parallelization can help. They additionally acknowledge the UI is complex and usability could be improved.","The DOE component relies on Latin Hypercube plus local perturbations around DL-suggested points, but does not quantify design efficiency (e.g., coverage, discrepancy, or information-based metrics) or compare against alternative sequential design strategies (e.g., Bayesian optimization, EI/UCB, adaptive space-filling). The inverse model quality is reported mainly via MSE, without uncertainty quantification or calibration, which can be important when steering expensive simulations and enforcing constraints. Results are shown for a single detailed scenario and limited iterations; generalization across scenarios and robustness to simulator failures/invalid runs are not systematically evaluated.",They suggest leveraging advances such as deep learning to further speed up the simulation solver and augment the tool’s capabilities. They propose incorporating additional constraints into the DL-based prediction when specifying design goals and expanding interface support accordingly. They also mention enabling exploration of scenarios outside the default control-parameter space and implementing an adaptive interface that adjusts to user needs.,"Add uncertainty-aware sequential design (e.g., Bayesian optimization or active learning with constraint handling) to choose refinement runs more efficiently than fixed ±5% perturbations. Provide formal DOE/space-filling diagnostics and comparative benchmarks versus established simulation-DOE baselines (adaptive LHS, maximin designs, trust-region methods). Release a reproducible implementation or at least pseudo-code/configuration details (sampling, perturbation, retraining cadence) and validate on additional domains/simulators with different dimensionalities and run-time characteristics.",2408.12607v1,https://arxiv.org/pdf/2408.12607v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:54:51Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,Not applicable,"Variable/General (general N participants, k arms, p covariates; examples include 2-arm and multi-arm; group formation and interference settings)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/wangmagg/inspection-guided-randomization,"The paper proposes Inspection-Guided Randomization (IGR), a flexible restricted randomization framework that filters treatment assignments by analyst-specified, domain-informed design desiderata rather than balance alone. IGR separates (i) enumerating a candidate pool of allocations (including via directed search such as genetic algorithms), (ii) restricting to high-scoring allocations using a fitness function and rule, and (iii) randomizing uniformly or with specified probabilities over the accepted set that is locked in ex ante and pre-registered. The framework is illustrated through simulation vignettes motivated by an experiment with interference (spillovers along a network/geographic proximity) and a group formation experiment where effects depend on group composition. The authors discuss design-based inference using Fisher randomization tests over the pre-registered accepted allocations and analyze unbiasedness conditions via a “mirror property,” including when interference-targeting constraints can be asymmetric. Simulations show IGR can reduce bias from interference and improve precision relative to complete randomization and other benchmark designs, with trade-offs governed by weights in the fitness function and possible covariate adjustment in analysis.","IGR scores each allocation z with a fitness function f(z,X) that aggregates inspection metrics such as (i) interference proxies like inverse minimum treated–control Euclidean distance, $m_{\text{InvMinEuclid}}(z)=\left(\min_{i\neq j, z_i\neq z_j}\|\ell_i-\ell_j\|_2^2\right)^{-1}$, or a network exposure fraction $m_{\text{FracExpo}}(z,A)=\frac{\sum_i \mathbf{1}\{\sum_{j\in N(i)} z_j > q|N(i)|\}(1-z_i)}{\sum_i (1-z_i)}$; and (ii) balance metrics like Mahalanobis distance between treated/control covariate means. A typical aggregated fitness is a weighted sum, e.g. $f(z,X)=w\,m_{\text{FracExpo}}(z,A)+(1-w)\,m_{\text{Mahalanobis}}(z,X)$, then a restriction rule keeps top-scoring allocations (e.g., those below a score threshold/percentile) and randomizes over the accepted set $P_{\text{IGR}}(Z=z)\propto \mathbf{1}\{z\in Z_{\text{accepted}}\}$.","In the interference simulation, restricting to the top 0.5th percentile of 100,000 enumerated allocations yields 500 accepted allocations; with mirror allocations included and $w_{\text{Interference}}=0.75$, IGR reduces absolute bias of the difference-in-means estimator to 88.31±0.56% of the bias under unrestricted complete randomization, and to 86.18±0.37% when $w_{\text{Interference}}=1$. At small interference weights, variance can drop sharply (reported example: variance 0.003±0.0007 at $w=0.125$ vs 0.61±0.10 under complete randomization), while large weights can inflate variance (example: 1.01±0.28 at $w=0.875$). Excluding mirror allocations can produce very large bias reductions for some samples (e.g., 26.67% of complete randomization bias) but can also worsen bias for others (e.g., 103.89%), indicating sensitivity to mirror-property violations. Regression adjustment reduces variance especially at large interference weights and can stabilize bias when mirrors are excluded, supporting a design–analysis trade-off between interference targeting and covariate balance.","The authors note that enumeration typically uses only a tiny subset of the $k^N$ possible allocations (i.e., $M\ll k^N$), so desirable allocations may be missed unless the candidate pool is enlarged or searched via optimization (e.g., genetic algorithms). They also discuss risks of over-restriction: too few accepted allocations can limit p-value granularity for randomization tests, and highly correlated accepted allocations can harm power and potentially affect asymptotic inference. They emphasize that coarse or misspecified inspection metrics (e.g., an overly coarse interference metric) may not target the true design desiderata and require iterative refinement during Evaluation & Adaptation.","The empirical demonstrations rely heavily on simulated data-generation mechanisms; performance may change substantially under different interference processes, covariate structures, or outcome models, and more real-world validations would strengthen external validity. The framework’s effectiveness depends on analyst-chosen metrics/weights and the quality of prior knowledge; in practice, stakeholders may disagree on desiderata and weights, and robustness to misspecification is not fully characterized. Computational burden and scalability to very large N (especially with network-based metrics requiring adjacency information) may be substantial, and practical guidance on runtime/complexity and default settings is limited. Comparisons are primarily to standard randomization benchmarks; broader benchmarking against specialized network/cluster randomization designs (e.g., graph cluster randomization) or modern covariate-adaptive methods could better situate gains.","The authors propose developing theory characterizing how close the hand-crafted IGR fitness function must be to the true conditional mean-squared error for IGR to reliably outperform naive randomization. They suggest leveraging pilot studies, multi-wave experiments, or historical experiments to learn/refine fitness functions (e.g., estimating and extrapolating interference networks) before the main trial. They also point to the need for accessible software (potentially a dashboard) that supports all IGR steps, including interactive visualizations for Evaluation & Adaptation.","Developing principled, uncertainty-aware (e.g., Bayesian or robust) versions of IGR that account for ambiguity in interference/network structure and covariate prognostic strength could reduce sensitivity to metric misspecification. Extending IGR to handle time-series/longitudinal experiments, adaptive/sequential experimentation (e.g., bandits with restricted randomization), and high-dimensional covariates with regularized balance metrics would broaden applicability. Formalizing optimality/efficiency results (e.g., bounds on variance/MSE under restricted assignment mechanisms) and providing diagnostics for when restriction harms inference would help practitioners choose restriction strength. Providing reference implementations in common statistical languages and standardized benchmark suites across domains (education, public health, online platforms) would improve adoption and reproducibility.",2408.14669v2,https://arxiv.org/pdf/2408.14669v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:55:34Z
TRUE,Sequential/adaptive|Bayesian design,Optimization|Parameter estimation,Not applicable,"Variable/General (example application: 1 design variable torque per time step over T=50 sequential experiments; parameter vector includes 2 physical parameters (mass, length) / transformed to 3 parameters in conjugate form).",Other,Simulation study,TRUE,Julia,Public repository (GitHub/GitLab),https://github.com/Sahel13/InsideOutNPF.jl,"The paper proposes the Inside–Out Nested Particle Filter (IO–NPF), a fully recursive algorithm for amortized sequential Bayesian experimental design (BED) in non-exchangeable (history-dependent) settings. It frames policy optimization for design selection as maximum-likelihood estimation in a non-Markovian state-space model and targets a Feynman–Kac distribution whose normalizing constant corresponds to a proxy expected information gain objective. Methodologically, IO–NPF replaces the non-recursive inner MCMC-based update of prior work (IO–SMC2) with an O(1)-cost jittering kernel, reducing overall complexity from O(NMT^2) to O(NMT), and adds a Rao–Blackwellized backward-sampling scheme to mitigate trajectory/path degeneracy. The authors provide consistency/convergence guarantees as the number of inner particles M grows. In simulations on a stochastic pendulum design problem (torque as design, angle/velocity as outcomes), IO–NPF with backward sampling improves information-gain performance over IO–SMC2 while offering favorable runtime/efficiency tradeoffs, and provides a public Julia implementation.","The design policy is a stochastic policy $\pi_\phi(\xi_t\mid z_{0:t})$ and the objective is expected information gain $I(\phi)=\mathbb{E}_{p_\phi(z_{0:T})}[H(p(\theta)) - H(p(\theta\mid z_{0:T}))]$. Under static noise, $I(\phi)\equiv \mathbb{E}_{p_\phi(z_{0:T})}[\sum_{t=1}^T r_t(z_{0:t})]$ with $r_t(z_{0:t})=-\log\int p(\theta\mid z_{0:t-1}) f(x_t\mid x_{t-1},\xi_{t-1},\theta)\,d\theta$, and potentials $g_t=\exp\{\eta r_t\}$. IO–NPF approximates the marginal predictive $p^M(x_{t+1}\mid z_{0:t},\xi_t)=\frac1M\sum_{m=1}^M f(x_{t+1}\mid x_t,\xi_t,\theta_t^m)$ and uses $g_t^M(z_{0:t+1})=\exp\{-\eta\log p^M(x_{t+1}\mid z_{0:t},\xi_t)\}$ for weighting in a nested particle filter.","In the stochastic pendulum simulation with $T=50$ experiments, the reported mean EIG estimates (± sd over 25 seeds) are: Random 1.37±0.08, iDAD 2.58±0.17, IO–NPF 2.98±0.19, IO–SMC2 3.35±0.20, IO–NPF+BS 3.47±0.18, and an ideal conjugate baseline IO–SMC2 Exact 3.54±0.20. Runtime per amortization iteration (seconds) is reported as IO–NPF 0.34±0.01, IO–SMC2 5.71±0.08, IO–NPF+BS 2.73±0.07, and IO–SMC2 Exact 1.36±0.04. Figure results show IO–NPF without backward sampling underperforms IO–SMC2, while IO–NPF with backward sampling outperforms IO–SMC2 and approaches the exact baseline in this conjugate setting.","The authors state that a main limitation (shared with IO–SMC2) is the need to know the Markovian outcome likelihood/transition model for the experiment outcomes, which may not be available in some applications. They also note that they currently provide asymptotic consistency and aim for stronger error guarantees.","The empirical validation is primarily on a single simulated benchmark (stochastic pendulum) and may not demonstrate robustness across a wide variety of nonlinear/nonconjugate models, observation noise regimes, or high-dimensional design spaces. Performance and stability may depend on tuning choices (e.g., number of particles N,M, tempering parameter $\eta$, jitter kernel scaling), but sensitivity analyses and practical auto-tuning guidance are limited. The approach assumes correct model specification for $f(\cdot)$ and the Markovian dynamics structure; misspecification, partial observability complications, or autocorrelated/nonstationary noise could degrade policy learning and design quality.","The authors propose addressing settings where the Markovian outcome likelihood is unavailable in future research. They also aim to derive stronger non-asymptotic error bounds for IO–NPF, leveraging related results on backward-sampling stability to provide stronger guarantees on learned policies.","Extend the method to likelihood-free or simulator-only settings (e.g., using surrogate density models or implicit likelihood estimators) to relax the known-likelihood requirement while retaining recursion. Evaluate IO–NPF on a broader suite of applications (e.g., nonlinear/non-Gaussian systems, higher-dimensional designs, partial observability) and include systematic ablations on N, M, jitter parameters, and backward-sampling settings. Provide packaged implementations, reproducible scripts, and practical diagnostics (ESS/path-degeneracy metrics, automatic kernel scaling) to support real-world adoption and reliable tuning.",2409.05354v2,https://arxiv.org/pdf/2409.05354v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:56:05Z
TRUE,Factorial (fractional)|Response surface|Screening|Sequential/adaptive,Screening|Optimization|Prediction,Not applicable,"8 factors in screening (roof overhang depth and WWR by 4 orientations); reduced to 3 factors for RSM optimization (south+west overhang depth, west WWR, south WWR)",Environmental monitoring|Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/juan-gamero-salinas/rsm-thermal-daylight-optimization,"The paper presents a sequential DOE-based workflow combining a resolution V 2^(8-2) fractional factorial screening design with Response Surface Methodology (RSM) and desirability functions to jointly minimize Indoor Overheating Hours (IOH) and maximize Useful Daylight Illuminance (UDI) in a tropical social-housing simulation model. Eight two-level passive-design factors (WWR and roof overhang depth by orientation) are first screened using stepwise regression and Lasso to identify the most influential drivers of overall desirability, then condensed to three factors for optimization. The authors run sequential first-order RSM experiments with the method of steepest ascent, followed by a second-order Central Composite Design (CCD) to estimate curvature and locate a stationary optimum, with model selection supported by AIC/BIC comparisons. The resulting optimum (overhang depth ~3.78 m on south/west, west WWR ~3.76%, south WWR ~29.34%) achieves D≈0.625 (IOH≈8.33%, UDI≈79.67%) and robustness is quantified via 1,000 bootstrap replications to form 95% confidence intervals. The work advances building-performance optimization practice by showing a computationally efficient, transparent DOE/RSM alternative to large-sample evolutionary optimization for multi-objective simulation problems.","Screening uses a resolution V 2^(8-2) fractional factorial design (64 runs) with factors coded at {-1,+1}. Multi-objective optimization uses Derringer–Suich desirability: individual desirability for minimizing IOH, d_IOH = ((H-IOH)/(H-G))^w on [G,H] (else 0/1 by region), and for maximizing UDI, d_UDI = ((UDI-U)/(G-U))^w on [U,G]; overall desirability is the geometric mean D = (d_IOH·d_UDI)^(1/2). RSM fits first-order and second-order polynomials (linear and quadratic response surface models) to D, and the CCD is configured as rotatable with center points to estimate curvature and locate the stationary point via canonical analysis (eigenvalues).","The resolution V 2^(8-2) fractional factorial screening reduced runs from 256 to 64 and identified roof overhang depth (south, west) and WWR (west, south) as the dominant drivers of desirability. Sequential first-order RSM steps along steepest ascent improved D to about 0.6165 before curvature indicated the need for a quadratic model. The rotatable CCD (17 runs) produced a second-order model with improved fit (AIC -90.82, BIC -99.28 vs. first-order AIC -57.11, BIC -59.09) and a stationary maximum (all eigenvalues negative). The validated optimum achieved D≈0.6247 with IOH≈8.33% and UDI≈79.67%, and bootstrap 95% CIs were approximately: overhang [3.47, 3.94] m, west WWR [1.54, 5.59]%, south WWR [26.19, 31.78]%.","The authors note the optimization is context-specific and limited to the selected passive factors (WWR and roof overhangs), while other potentially important overheating/daylight-related measures (e.g., roof U-value/double roof; east façade glare control) were held fixed or not explored. They also state that variations in glazing visible light transmittance/reflectance were not included due to lack of manufacturer data and typical architectural practice constraints. They further acknowledge the study used a single parametric dwelling model and suggest broader typology coverage for generalizability, and mention that calibrated digital models would strengthen robustness assessment.","The workflow relies on deterministic simulation outputs and standard RSM polynomial assumptions; if the true response surface is highly non-quadratic or has strong interactions beyond those modeled, the CCD-based optimum could be locally misleading. The factor reduction step (collapsing south+west overhang into one factor and randomizing “non-significant” factors) may hide interactions or constrain feasible design choices, potentially biasing the found optimum. The desirability-function setup (linear weights, bounds set from observed min/max in the screening runs) can materially affect the optimum; sensitivity of results to these choices is not fully explored. Practical constructability/cost constraints (e.g., a 3.78 m overhang) are not explicitly integrated as design constraints in the DOE/RSM optimization.","The paper suggests future studies should further explore the relationship between overheating and daylight (beyond this first study of its kind in that pairing). It also recommends incorporating calibrated digital models to better evaluate robustness, and broadening scope by applying the methodology to a diverse set of randomly selected social-housing typologies rather than a single model. The authors also mention potential refinement by adding other measures not included in the DOE (e.g., roof thermal improvements; additional solar protection on the east) to further improve IOH/UDI performance.","A useful extension would be to add explicit feasibility/cost/constructability constraints (and possibly a third objective such as cost or embodied carbon) so the DOE/RSM optimum is implementable. Robust/self-starting or Bayesian RSM designs could incorporate uncertainty in weather files, occupancy schedules, infiltration, and material properties rather than treating simulations as noise-free. Expanding from three factors to a constrained mixture/combined-factor formulation (separate south vs west overhangs again) could reveal directional trade-offs and interactions suppressed by factor aggregation. Releasing an R package/script that reproduces the full pipeline (design generation → simulation orchestration → RSM/desirability/bootstrapping) would improve reproducibility and adoption.",2409.09093v1,https://arxiv.org/pdf/2409.09093v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:56:54Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Prediction|Parameter estimation|Screening|Optimization|Other,D-optimal|A-optimal|Other,"Variable/General (infinite-dimensional random field parameter; discretized to dm=14,003 DOF; reduced to r=64 latent dims)",Healthcare/medical,Simulation study|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2409.09141,"The paper develops a computational framework for sequential Bayesian optimal experimental design (SBOED) for dynamical systems governed by large-scale time-dependent PDEs with infinite-dimensional random field parameters. It introduces an adaptive terminal formulation that yields an equivalent objective maximizing a conditional expected information gain (EIG), simplifying sequential design compared with cumulative incremental formulations. For scalable evaluation of the design objective, the authors combine Laplace approximation of the posterior with a low-rank approximation of the prior-preconditioned Gauss–Newton Hessian to compute KL-based information gain efficiently. To accelerate repeated PDE solves and Jacobian evaluations inside SBOED, they propose a derivative-informed latent attention neural operator (LANO) surrogate that learns both parameter-to-observable maps and their Jacobians using derivative-informed dimension reduction plus latent attention dynamics. In a tumor-growth monitoring example, the method adaptively selects MRI imaging times and achieves high accuracy vs FEM while delivering an amortized ~180× speedup for solving the SBOED problem.","The experimental design selects observation times via a binary vector $\xi\in\{0,1\}^K$ with $\sum_k\xi_k=d$ and maximizes expected information gain measured by KL divergence, e.g., static BOED $\xi^*=\arg\max_{\xi} \mathbb{E}_{\pi(y\mid \xi)}[D_{KL}(\mu(m\mid y,\xi)\|\mu(m))]$. For adaptive SBOED from time $t_i$, the paper shows equivalence between maximizing expected cumulative incremental KL gains and maximizing a terminal KL objective $\mathbb{E}[D_{KL}(\mu(m\mid y_{1:i:K},\xi_{1:i:K})\|\mu(m))]$. The KL objective is approximated under Laplace + low-rank Hessian eigenpairs as $\tfrac12\sum_{j=1}^r\big(\log(1+\lambda_j)-\lambda_j/(1+\lambda_j)\big)+\tfrac12\|m_{MAP}-m_{prior}\|^2_{\Gamma^{-1}_{prior}}$ (or its reduced-space analogue).","In the tumor-growth MRI timing study (selecting $d=4$ observation days out of 10), LANO achieves substantially lower surrogate errors than neural ODE and DIPNet for both the PtO map and its reduced Jacobian across time (e.g., PtO errors decreasing to ~2.27% by day 10; reduced Jacobian errors ~1.5–4%). MAP point estimation using LANO attains mean relative error ~1.52% (std 0.44%) vs FEM with daily observations, and ~1.31% (std 0.56%) with sparse observations {day 2,5,8}. Eigenvalue-related information gain terms are approximated with mean relative error ~0.8% (std 0.44%), and the MAP-norm term with mean relative error ~0.2% (std 0.03%). In timing benchmarks, LANO speeds up PtO evaluation by ~388×, MAP optimization by ~57×, eigenpair computation by ~1364×, and information gain computation by ~197×, yielding an amortized ~180× speedup when accounting for offline data generation and training.","The authors note that their framework relies on Laplace approximation and low-rank posterior covariance structure; it may be inadequate when the posterior is far from Gaussian or the posterior covariance is not low rank. They also imply that the current demonstration uses synthetic observations from PDE simulations, so the advantage of adaptive SBOED over static design may be larger with real-world observational discrepancies.","The design space is restricted primarily to selecting observation times from a fixed candidate set (binary subset selection); extensions to continuous-time designs or joint time–space sensor placement may change computational and optimization challenges substantially. Performance is demonstrated on one main application (tumor growth PDE) with specific discretization and noise assumptions (Gaussian i.i.d. observation noise); robustness to model misspecification, non-Gaussian noise, or temporal correlation in measurement error is not fully explored. The surrogate training cost is significant and requires high-fidelity PDE/Jacobian data; the break-even point (when amortization pays off) may depend strongly on how many design queries/tasks are solved and on transferability across patients/domains.","They suggest extending beyond scenarios where Laplace approximation/low-rank structure are valid, potentially using variational inference to accelerate optimality-criteria evaluation. They propose extending SBOED to select both observation times and spatial locations (sensor placement), potentially using greedy/swapping-greedy optimization strategies. They also mention adaptively refining the neural network to predict beyond the training horizon to enable a predictive digital twin.","Develop self-starting or online-updated versions of LANO that update the surrogate as new real observations arrive, reducing reliance on large offline FEM training sets and improving robustness to model–data mismatch. Add principled uncertainty quantification for the surrogate (e.g., Bayesian neural operator or ensemble methods) and propagate surrogate uncertainty into EIG optimization to avoid overconfident designs. Explore alternative design criteria (e.g., I-/G-optimal prediction-focused criteria) and multi-fidelity SBOED that mixes cheap surrogate evaluations with occasional FEM corrections to control bias.",2409.09141v2,https://arxiv.org/pdf/2409.09141v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:57:48Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction,A-optimal|Not applicable,"Variable/General (sensor-placement over m candidate locations; example uses m=334, selects m0 up to 36)",Environmental monitoring|Other,Simulation study|Other,TRUE,Other|Python,Not provided,https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html#scipy.optimize.minimize,"The chapter develops a “global optimum-informed” greedy algorithm for A-optimal experimental design in a sensor placement setting for Bayesian inverse problems. The design variable is a binary mask over candidate sensor locations, with an A-optimal objective $J(w)=\mathrm{tr}(C_{\text{post}}(w))$ based on the posterior covariance of the unknown field/source. The method leverages first-order optimality conditions for a convex relaxation (continuous weights $0\le w\le 1$ with budget constraint) to identify redundant indices where the relaxed global optimum has $w_k^*=0$, then removes those sensors from the greedy design and re-greeds to replace them. Numerical experiments on a Helmholtz PDE inverse source problem (finite elements, multi-frequency complex measurements) show the informed-greedy sequence yields uniformly better A-optimal objective values than standard greedy for all tested sensor budgets. The improved sequence is closer to the relaxed global optimum and yields modest but consistent reconstruction-error reductions in an illustrated example.","Observation model: $g_w = F_w f + \varepsilon$, with $\varepsilon\sim\mathcal N(0,\sigma^2 I)$. Bayesian linear-Gaussian posterior: $C_{\text{post}}(w)=\big(F_w^*\Sigma^{-1}F_w + C_{\text{prior}}^{-1}\big)^{-1}$ and $m_{\text{post}}(w)=m_{\text{prior}}+C_{\text{post}}F_w^*\Sigma^{-1}(g_w-F_w m_{\text{prior}})$. A-optimal criterion: $J(w)=\mathrm{tr}(C_{\text{post}}(w))$; optimization posed as binary budgeted sensor selection $w\in\{0,1\}^m,\ \|w\|_0\le m_0$ and its convex relaxation $0\le w\le 1,\ \sum_k w_k\le m_0$.","In the Helmholtz inverse-source experiment with $m=334$ candidate locations and designs up to $m_0=36$, standard greedy (Algorithm 1) computed $(w_{m_0})_{m_0=1}^{36}$ in 100.0 seconds on the stated CPU, while the global-optimum-informed greedy (Algorithm 2) computed $(w^*_{m_0})_{m_0=6}^{36}$ in 701.4 seconds (629.1 seconds spent computing relaxed global optima). The informed-greedy approach improved the A-optimal objective $J$ for every tested $m_0$, with an average improvement of 4.2% and a best-case improvement of 22.3% relative to standard greedy. For a showcased case at $m_0=12$, the $L^2$ reconstruction error decreased from approximately $4.69\times 10^{-2}$ (greedy) to $4.57\times 10^{-2}$ (informed-greedy), about a 2.7% improvement.",The authors note Algorithm 2 is slower than standard greedy primarily due to repeatedly computing the relaxed global optimum $w^*$. They also state the method does not exploit “dominant” indices where $w_k^*=1$ and that the greedy update step can re-introduce “redundant” indices with $w_k^*=0. They suggest that performing the global-informed update less often could yield significant speedups.,"The approach depends on solving (potentially many times) a convex-relaxed OED problem; scalability may be limited for very large candidate sets or more expensive forward models, and performance may hinge on the efficiency/accuracy of gradient computations. The numerical validation is shown for one PDE inverse-source configuration (geometry, frequencies, noise model), so generalizability to other inverse problems, correlated/colored noise, model mismatch, or nonlinearity is not established. Comparisons are mainly against standard greedy; broader benchmarking against other scalable OED methods (e.g., exchange/coordinate methods, randomized/streaming, submodular approximations) is limited in this chapter. The method’s behavior when nuisance constraints exist (e.g., spatial separation, accessibility, grouped sensors) is not addressed.",The authors propose improving Algorithm 2 by incorporating dominant indices satisfying $w_k^*=1$ and preventing the greedy update from re-introducing redundant indices with $w_k^*=0. They also suggest reducing computational cost by performing the global optimum-informed update step less frequently to obtain significant speedups.,"Develop a principled schedule for when to recompute the relaxed optimum (e.g., based on a bound gap $J(w_{m_0})-J(w^*)$ or on detected redundancy) to balance quality vs. runtime. Extend the framework to handle autocorrelated/colored noise, unknown hyperparameters (empirical Bayes), or nonlinear forward maps with Laplace/Gauss–Newton approximations. Add constraints common in sensor placement (minimum spacing, clusters, installation costs) and study whether redundancy tests remain effective under such constraints. Provide an open-source implementation (e.g., Python/NGSolve scripts) and larger-scale benchmarks to establish robustness and computational scaling.",2409.09963v2,https://arxiv.org/pdf/2409.09963v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:58:39Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Parameter estimation|Cost reduction|Other,Other,Variable/General (end-effector pose on S^3×R^3; experiments shown for a 7-DoF arm; DH error vector has 4·n_j parameters),Transportation/logistics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian optimal experimental design method for online robot kinematic calibration, where the “design points” are end-effector poses on the non-Euclidean space S^3×R^3 (quaternion rotation + translation). It models a geodesic-distance-based calibration error objective with a Gaussian process that uses a geometry-aware (Riemannian Matérn/heat-type) kernel valid on S^3 and a product kernel on S^3×R^3, then selects the next measurement pose sequentially using the GP-UCB acquisition rule. Measurements are obtained via vision-based localization using fiducial markers (AprilTags) and compared to nominal forward kinematics; collected data are then used in a bounded quadratic program to estimate corrections to Denavit–Hartenberg parameters. The approach is demonstrated in simulation and on NASA JPL’s OWLAT testbed with a 7-DoF Barrett WAM arm, showing improved sample efficiency versus random sampling and successful calibration with about 20 pose measurements. Overall, it advances calibration DOE by making the pose-selection procedure adaptive, geometry-aware, and directly optimized in task space rather than joint space.","The DOE selects poses by Bayesian optimization: at iteration k choose x_k = argmax_x [\mu_{k-1}(x) + \sqrt{\beta_k}\,\sigma_{k-1}(x)] (GP-UCB), with x=(q,p)\in S^3\times\mathbb{R}^3. The sampled objective is a normalized weighted sum of translation and rotation errors, using fp(p)=\|p-\tilde p\| and fq(q)=d_{S^3}(q,\tilde q)=2\cos^{-1}(|\langle q,\tilde q\rangle|), combined as f(p,q)=-(\alpha_1 fp/\sup|fp| + \alpha_2 fq/\sup|fq|). DH parameter corrections are then estimated from the collected measurements via a bounded least-squares quadratic program: \delta^* = argmin_\delta \|\Delta_n - J_n\delta\|^2 s.t. \delta_{lb}\le\delta\le\delta_{ub}.","In simulation on the OWLAT scenario with a 7-DoF Barrett WAM, the method is run for 20 sequentially chosen pose measurements and is reported to converge faster and achieve better calibration/data efficiency than random sampling (objective f improves more quickly over iterations, per Fig. 1). In physical experiments on NASA JPL’s OWLAT testbed with injected encoder biases, the method is reported to successfully recalibrate the arm using 20 pose measurements with “impressive accuracy,” overcoming the injected biases (per Fig. 3 narrative). Quantitative ARL-style metrics are not applicable; performance is communicated via objective-vs-iteration plots and qualitative/experimental success.","The conclusion notes that the objective function f can have discontinuities (in the current formulation/learning with rotations), and future work is needed to improve the smoothness of f to address these discontinuities. No other explicit limitations are stated in the provided text.","The approach depends on accurate pose measurements from the vision system (AprilTags/camera); calibration performance may degrade under occlusion, poor lighting, or imperfect camera-to-base calibration, but this dependence is not analyzed in depth. The GP-UCB Bayesian optimization requires repeated global maximization of the acquisition function over a constrained pose space, which can be computationally and constraint-handling intensive in practice; runtime/scalability with larger search spaces is not fully characterized. The method assumes time-invariant DH parameter errors and does not directly address joint backlash, compliance, thermal drift, or dynamic effects that can make errors configuration- and time-dependent.",Future work will focus on improving the smoothness of the objective function f while considering learning with rotations to address discontinuities in f.,"Extend the DOE to explicitly handle safety and feasibility constraints (collision, joint limits, visibility) within constrained Bayesian optimization and report guarantees or empirical reliability under these constraints. Develop a self-starting/robust variant that accounts for unknown measurement noise levels, outliers, and correlated errors (e.g., camera bias), and evaluate sensitivity to vision-system miscalibration. Provide an open-source implementation (e.g., ROS package) and broader benchmarking against classical calibration DOE criteria (e.g., D-optimality/observability indices) under matched constraints and equal measurement budgets.",2409.10802v3,https://arxiv.org/pdf/2409.10802v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T06:59:12Z
FALSE,NA,NA,Not applicable,"Variable/General (parametric sweeps over Mn, Fn, L0/L, L/W; 5 swimmer designs, 2 experimentally tested)",Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,Other,Not provided,NA,"The paper develops an experimentally calibrated, fully coupled solid–fluid computational model to design and compare magnetic soft robotic swimmers (MSRSs) operating at low Reynolds number. Five swimmer concepts (two undulatory and three helical) are evaluated primarily by normalized steady-state swimming speed (body lengths per cycle, blpc), with parametric sweeps over non-dimensional magnetic and fluid numbers (Mn, Fn), normalized magnetic length (L0/L), and aspect ratio (L/W). Two swimmers (a helical finger-shaped and an undulatory carangiform-like) are fabricated and tested in a viscous fluid; measured swimming performance is compared against model predictions to validate the framework. The study additionally evaluates qualitative performance attributes important for real use—on-the-fly bi-directionality under field reversal and robustness to initial orientation errors (roll/pitch/yaw tilts)—to recommend designs for speed versus versatility. Overall, the finger-shaped helical swimmer is identified as the fastest, while the carangiform-like undulatory swimmer is identified as the most versatile and stable.","Key dimensionless groups and performance metric are defined: $\mathrm{Mn}=\frac{12 B M \bar{L} L_0}{E h^2}$ and $\mathrm{Fn}=\frac{12 \mu \bar{L}^3 f_m}{E h^3}$, where $E$ is Young’s modulus, $\mu$ fluid viscosity, $B$ magnetic field magnitude, $M$ magnetization magnitude, $\bar{L}$ characteristic length (defined as $\bar{L}=\sqrt{LW}$), $L_0$ magnetic segment length, $h$ thickness, and $f_m$ actuation frequency. The normalized steady-state swimming speed is $\mathrm{blpc}=\frac{c}{\bar{L} f_m}$ with $c$ the steady swimming speed. Magnetic body torque is given by $\mathbf{N}=\mathbf{M}\times \mathbf{B}$.","Across the five concepts, the maximum reported blpc values (with corresponding $(\mathrm{Mn},\mathrm{Fn})$) are summarized in Table 1: finger-shaped helical 0.31 (191, 5), field-induced helical 0.11 (398, 5), drag-induced helical 0.13 (53, 15), carangiform-like undulatory 0.12 (526, 5), and anguilliform-like undulatory 0.12 (255, 5). Experiments for the finger-shaped swimmer (shown at Fn=20 and 30) and carangiform-like swimmer (Fn=60 and 90) show measured blpc trends agreeing with model predictions over Mn sweeps (figures show close agreement with regression fits). The paper reports design sensitivities: increased viscosity (higher Fn) generally lowers blpc; partial magnetization (L0/L) can improve undulatory performance up to a point (rigid-like behavior near L0/L≈0.9–1 reduces symmetry breaking); and helical swimmers can fail via coiling or undulatory via self-contact at high loading. Stability analysis indicates the carangiform-like swimmer is the most robust to initial roll/pitch/yaw tilts, and bi-directionality is achieved on-the-fly for the field-induced and undulatory swimmers but not for the finger-shaped or drag-induced helical swimmers.","The authors note practical fabrication/control limitations for ciliary swimmers: non-reciprocal contraction/relaxation requires highly inhomogeneous remnant magnetization profiles and precise magnetic-field settings that depend on fluid viscosity, which are challenging even with state-of-the-art laboratory facilities. They therefore restrict the study to a set of five swimmers (two undulatory and three helical) that are more feasible to fabricate and validate experimentally. They also state that their reported results focus on steady-state values unless explicitly mentioned otherwise.","Despite using the term “experimental design,” the study does not present a formal DOE/optimal design methodology (e.g., factorial/response-surface/optimal designs); parameter exploration is primarily via physics-guided parametric sweeps, which may miss interaction structure or uncertainty-aware sampling efficiencies. Code and full reproducibility details are not provided in this paper (the text points readers to prior work for source code and benchmarks), which limits independent replication and extension. Experimental validation appears limited to two of five designs and a subset of Fn/Mn conditions; broader experimental coverage (more geometries, magnetization profiles, and uncertainty quantification across repeats) would strengthen generalizability. The modeling assumptions (e.g., Stokes-flow regime, uniform fields after calibration, material property constancy) may not hold in more complex biomedical environments with boundaries, non-Newtonian fluids, or field gradients.",None stated.,"A natural extension would be to introduce a formal DOE or Bayesian/optimal experimental design to select Mn/Fn/L0/L/L/W sampling points efficiently under experimental budget constraints and to quantify parameter interactions. Additional validation in more application-realistic fluids (e.g., non-Newtonian mucus analogs), with confinement/wall effects and field non-uniformities, would improve translational relevance. Releasing simulation/analysis code and providing standardized benchmark datasets would enable reproducible comparisons across swimmer designs and future methods. Extending the framework to multi-objective optimization (speed, stability, steering radius, power/field limits) with uncertainty propagation would better support practical design decisions.",2409.11215v1,https://arxiv.org/pdf/2409.11215v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:00:05Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable|Other,Variable/General (design variable u in Ω; parameter dimension K; examples use K=9 and c≈2K sensors),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Upon request,NA,"The paper proposes a sensitivity-preserving experimental design framework for inverse problems by selecting a small subset of experimental settings (e.g., sensor locations) that preserves the conditioning/eigen-structure of the Fisher Information Matrix (FIM) of the full experiment. Instead of optimizing a classical A-/D-optimal criterion, it reframes data selection as a randomized linear algebra (matrix sketching) problem, using importance sampling proportional to row “energy” of a whitened sensitivity matrix $J=\Gamma^{-1/2}G(p^*)$ to approximate the full FIM. It derives high-probability bounds showing that, with sufficiently many sampled designs $c$, the down-sampled FIM maintains a positive minimum eigenvalue and comparable inverse condition number, thereby retaining local identifiability/sensitivity around a presumed true parameter $p^*$. To draw samples from the induced sensitivity-informed distribution, it integrates gradient-free ensemble sampling methods (Ensemble Kalman Sampler and Consensus-Based Sampler) and a greedy acceptance rule that keeps updates improving a convexity/conditioning metric. Numerical experiments on Schrödinger potential reconstruction demonstrate that a small number of sensors (e.g., $c=2K$) selected by the proposed sampling pipeline can substantially improve the minimum eigenvalue and conditioning of the FIM relative to naive (normal or uniform) placements, sometimes even exceeding the full-data design due to downweighting uninformative measurements.","The full Fisher information is $I(\Omega)=G(p^*)^\top\Gamma^{-1}G(p^*)=\int_\Omega J_{u,:}J_{u,:}^\top\,d\mu(u)$ with $J=\Gamma^{-1/2}G(p^*)$. The down-sampled (reweighted) design draws $u\sim \pi\mu$ and uses weights $1/(c\pi(u))$, giving $I(\Omega_c)=\sum_{u\in\Omega_c}\frac{1}{c\pi(u)}J_{u,:}J_{u,:}^\top$. The optimal importance distribution is $\tilde\pi(u)\propto\|J_{u,:}\|_2^2$ (equivalently $\tilde\mu(u)\propto e^{-\Phi(u)}$ with $\Phi(u)=-\log\|J_{u,:}\|_2^2$), and a sufficient sample size bound is provided to guarantee eigenvalue/conditioning preservation with high probability.","A high-probability guarantee (Theorem 2) states that if $\pi(u)\ge \beta\tilde\pi(u)$ and $c$ satisfies an explicit bound (Eq. 18), then with probability $\ge 1-\delta$ the down-sampled FIM retains $\lambda_{\min}^c\ge \lambda_{\min}^\Omega-\varepsilon>0$ and an inverse-condition-number lower bound relative to the full design. In the Schrödinger example with $K=9$, full data (841 sensors) had $(\lambda_{\min},c_{\mathrm{inv}})\approx(0.8,8.18\times10^{-4})$, while a naive normal initial design with $c=18$ gave $(1.48\times10^{-4},1.54\times10^{-7})$. After 25 greedy iterations, EKS improved to about $(2.06,2.25\times10^{-3})$ and CBS to $(1.41,1.56\times10^{-3})$, far outperforming repeated greedy random sampling from the same initial distribution. With source-term design included, inverse condition number improved from $\sim3.0\times10^{-9}$ (normal initial) to $\sim1.7\times10^{-3}$ (EKS) and $\sim8.8\times10^{-4}$ (CBS).","The authors note that ensemble-based sampling methods (EKS/CBS) lack non-asymptotic convergence rates, so samples may not accurately represent the target importance distribution in finite time. They also state that the explicit sample-size bound depends on quantities not known a priori (e.g., $\|J\|_F$ and $\lambda_{\min}^\Omega$), making parameter tuning uncertain. Finally, they acknowledge a key practical drawback common in experimental design: constructing the optimal sampling density requires knowledge of the true parameter $p^*$, which is generally unavailable.","The theoretical preservation guarantee is local around a presumed $p^*$ and hinges on the FIM at $p^*$ being well-conditioned; performance may degrade under model mismatch, nonlocal nonlinearity, or multiple plausible parameter values. The framework emphasizes preserving the full-design eigen-structure rather than directly optimizing prediction/estimation risk (e.g., integrated posterior variance), so it may be suboptimal for specific downstream objectives. The method also presumes access to sensitivities (or their approximations) over the design space; for large-scale PDEs this can still be expensive, and the paper does not fully quantify computational cost vs. classical OED baselines or compare against established optimal design solvers on equal compute budgets.","The authors propose clarifying the relationship between their sensitivity-preserving approach and Bayesian optimal experimental design criteria such as K- and E-optimality that also target (inverse) condition number or minimum eigenvalue. They suggest mitigating the need for knowing $p^*$ by integrating ideas from sequential experimental design, potentially alternating between design and parameter inference. They also indicate potential extensions to modern inversion frameworks that incorporate Gaussian processes or neural networks within least-squares optimization.","Developing a self-starting/robust variant that averages or hedges the sampling density over a prior/posterior on $p$ (rather than a single $p^*$) would improve practicality and connect more directly to Bayesian OED. Establishing finite-time convergence/approximation guarantees for the combined (EKS/CBS + greedy acceptance) pipeline, including bias induced by greedy selection, would strengthen theoretical justification. Broader empirical benchmarks against classical optimal design algorithms (A/D/E-optimal, greedy determinant/trace methods) on matched PDE problems and costs, plus releasing reference implementations, would improve reproducibility and adoption.",2409.15906v2,https://arxiv.org/pdf/2409.15906v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:00:57Z
TRUE,Optimal design|Other,Parameter estimation|Cost reduction,D-optimal|A-optimal,"Variable/General (linear regression with n parameters; examples use n=m/10 with m∈{50,60,80,100,120})",Theoretical/simulation only,Simulation study|Other,TRUE,Julia,Public repository (GitHub/GitLab),https://github.com/liangling98/PNOD.jl,"The paper proposes PNOD, a projected Newton framework to accelerate branch-and-bound (BnB) for computing exact (integer) optimal experimental designs, formulated as an NP-hard MINLP over integer replicate counts subject to a fixed budget N. The key contribution is improving the efficiency of solving continuous relaxations at each BnB node by using a projected Newton method for self-concordant objectives, where each Newton QP subproblem is solved efficiently via a globally convergent vertex exchange method. The framework is demonstrated for two standard optimality criteria: A-optimal (trace of inverse information matrix) and D-optimal (log-determinant of information matrix), with explicit gradient/Hessian expressions for implementation. Extensive numerical experiments on synthetic independent and correlated design matrices compare PNOD against prior BnB (Co-BnB) and a Frank–Wolfe-based mixed-integer convex approach (Boscia), showing PNOD evaluates far more BnB nodes per second and typically achieves better solution quality/accuracy within time limits. The method is released as an open-source Julia package (PNOD.jl) intended to extend to other generalized self-concordant design criteria.","The exact design problem is posed over integer replicate counts x∈Z_+^m with budget constraint ∑_{i=1}^m x_i=N and information matrix X(x)=∑_{i=1}^m x_i a_i a_i^T = A^T Diag(x) A. The objective is f(x) = -\log \phi(X(x)), where \phi is Kiefer’s matrix mean (notably p=0 gives D-optimal via \phi(X)=det(X)^{1/n}, and p=-1 corresponds to A-optimal). D-optimal uses f(x)= -\log\det(X(x)) with gradient ∇f(x)= -diag(A(X(x))^{-1}A^T) and Hessian ∇^2 f(x)=(A(X(x))^{-1}A^T)\circ(A(X(x))^{-1}A^T); A-optimal uses f(x)=\log\operatorname{tr}(X(x)^{-1}) with gradient involving diag(A X(x)^{-2} A^T) scaled by \operatorname{tr}(X(x)^{-1}). Continuous relaxations at BnB nodes solve \min f(x)\ s.t.\ e^T x=N,\ \ell\le x\le u; projected Newton iterates by approximately solving a constrained QP surrogate q(z;x)=f(x)+\langle\nabla f(x),z-x\rangle+\tfrac12\langle z-x,\nabla^2 f(x)(z-x)\rangle.","Across synthetic instances with m∈{50,60,80,100,120}, n=m/10, and budget N=\lfloor 3n/2\rfloor, PNOD processes substantially more BnB nodes per second than both Boscia and Co-BnB for both A- and D-criteria and for independent/correlated data (per the node-throughput plots). In total wall-clock time comparisons under a 2-hour time limit (abstol=1e-2, reltol=1e-6), PNOD is consistently faster than Co-BnB and is often faster than Boscia except notably for D-optimal with correlated data where Boscia is faster. PNOD generally returns lower (better) objective values than Boscia, indicating higher solution quality, while matching or improving upon Co-BnB. The paper reports implementation in PNOD.jl and shows that improved node evaluation enables exploring many more nodes within the same time budget, increasing the likelihood of reaching the exact optimum.","The authors note that the poor performance of prior BnB implementations is partly due to the huge number of nodes and that improving branching/search/pruning strategies is “highly nontrivial and problem dependent”; their work focuses instead on accelerating node evaluation rather than solving the node-explosion issue. They also report a practical issue/bug: Co-BnB “fails to terminate correctly when the time limit is reached,” highlighting solver robustness concerns in existing tooling. They indicate their illustrative focus is on D- and A-optimal criteria, while extensions require the objective to be (generalized) self-concordant.","Empirical evaluation is limited to synthetic linear-regression design matrices (independent/correlated random A) and does not include real experimental design case studies (e.g., response surface, mixture, blocked/split-plot constraints), so practical performance on domain-specific design spaces is unclear. The approach relies on solving many continuous relaxations with gradients/Hessians involving inverses of information matrices; numerical stability and computational scaling for very large n or ill-conditioned A (common in practice) are not fully explored. Comparisons are restricted to specific solvers/implementations (Co-BnB and Boscia) and a particular BnB strategy from Bonobo.jl; results may vary under different branching/pruning rules or commercial MINLP solvers. The work targets exact designs with replicate-count variables; extensions to design spaces with nonlinear models, constraints beyond simple bounds/sum, or adaptive/sequential experimentation are not demonstrated.","The authors state that although they use D-optimal and A-optimal designs as illustrative examples, the package/framework “can be easily extended to other criteria as long as the objective function f in (OD) is generalized self-concordant.” They also imply broader real-world applicability enabled by releasing the open-source PNOD.jl package.","A natural extension is to incorporate more advanced, problem-aware branching/pruning (cutting planes, strong branching, warm starts across sibling nodes) to reduce node counts in addition to speeding node evaluation. It would be valuable to test robustness under unknown/noisy parameter estimates (Phase I/II style) and under autocorrelated or heteroskedastic errors, which affect the information matrix modeling. Extending the framework to constrained design spaces common in practice (blocked/split-plot, cost-weighted runs, mixture/simplex regions, and multivariate responses) would broaden impact. Providing benchmarking scripts and standardized datasets, plus profiling and numerical-stability guidance for ill-conditioned information matrices, would improve reproducibility and adoption.",2409.18392v1,https://arxiv.org/pdf/2409.18392v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:01:49Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Cost reduction|Other,Other,Variable/General (2-arm treatment assignment; with covariates X in general dimension),Healthcare/medical|Service industry|Other|Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,NA,"This paper studies optimal adaptive experimental design for estimating the average treatment effect (ATE) in a two-arm randomized experiment with heterogeneous covariates. It establishes a non-asymptotic lower bound on the achievable mean-squared error/variance of unbiased ATE estimators under any non-anticipating (possibly adaptive) allocation, showing the generalized Neyman allocation is the fundamental statistical limit. The authors propose a low-switching batched adaptive allocation framework that learns outcome variance functions and updates assignment policies only a few times, coupling doubly robust (AIPW) estimation with a contextual bandit-style policy optimization objective. For the no-covariate case, they give a low-switching “incomplete randomization” design where cross terms vanish and the AIPW reduces to a weighted difference-in-means across batches, yielding near-Neyman-optimal variance with finite-sample guarantees. For the covariate case, they use sample-splitting/cross-fitting to remove adaptive bias and prove their estimator’s variance approaches the semiparametric efficiency bound at a rate depending on nonparametric regression rates for mean and variance estimation; simulations show near-optimal accuracy with only 2–3 policy updates.","The estimand is the ATE $\tau=\mathbb{E}[\mu^{(1)}(X)-\mu^{(0)}(X)]$. The core estimator is the (cross-fit) AIPW: $\hat\tau=\frac{1}{n}\sum_i\{\hat\mu^{(1)}(X_i)-\hat\mu^{(0)}(X_i)+\frac{W_i(Y_i-\hat\mu^{(1)}(X_i))}{e_i(X_i)}-\frac{(1-W_i)(Y_i-\hat\mu^{(0)}(X_i))}{1-e_i(X_i)}\}$, with optimal (generalized Neyman) propensity $e^*(x)=\sigma^{(1)}(x)/(\sigma^{(1)}(x)+\sigma^{(0)}(x))$ and efficiency bound $V^*=\frac{1}{n}(\mathbb{E}[\sigma^{(1)}(X)+\sigma^{(0)}(X)]^2+\mathbb{E}[\tau(X)-\tau]^2)$. Their low-switching design updates batchwise $e_k(x)$ by plugging in (truncated) variance estimates, e.g., $e_k(x)=\tilde\sigma^{(1)}_{k-1}(x)/(\tilde\sigma^{(1)}_{k-1}(x)+\tilde\sigma^{(0)}_{k-1}(x))$ with enforced exploration bounds, and they show the MSE gap to $V^*$ corresponds to a bandit-style regret term involving $\sum_i \sigma^{(1)}(X_i)^2/e_i(X_i)+\sigma^{(0)}(X_i)^2/(1-e_i(X_i))$.","In the homogeneous (no-covariate) case, they prove a finite-sample guarantee that the low-switching design yields an unbiased estimator with variance $\le (1+\tilde O(n^{-1+1/K}))V^*$ (where $K$ is the number of batches) and matches a Bayes/information-theoretic lower bound up to lower-order terms. In the covariate case, they prove the cross-fit AIPW estimator is unbiased and achieves $\mathrm{Var}(\hat\tau)\le (1+\tilde O(n^{-\alpha})+\tilde O(K n^{-1+1/S_K}))V^*$, and with $K=O(\log n)$ they state convergence to the efficiency bound at rate $\tilde O(n^{-\min(\alpha,\beta)})$ (with $\alpha,\beta$ the oracle regression rates for mean and variance). They also prove a universal lower bound: for any non-anticipating adaptive design and any unbiased estimator, $\mathrm{Var}(\hat\tau)\ge V^*$, with equality iff the design uses generalized Neyman allocation and the idealized AIPW estimator. Simulations with $n=2000$ show the low-switching method reduces L2 loss substantially versus non-adaptive allocation and approaches the optimal (generalized) Neyman benchmark with only 2–3 policy updates.","The authors assume access to an offline nonparametric regression oracle for the variance function with specified high-probability rates, and explicitly note that minimax variance-function estimation rates (especially in high-dimensional random design) are still open in general. They also acknowledge they do not provide a matching lower bound for the covariate-case convergence rate factor (the $(1+\tilde O(n^{-\min(\alpha,\beta)}))$ approach-to-efficiency rate), leaving optimality of this rate unresolved. For the CLT result, they additionally require (for that theorem) that variances are uniformly upper and lower bounded independent of $n$.","The practical performance depends heavily on how the “variance oracle” and mean regression are implemented; the paper’s finite-sample claims rely on oracle-rate conditions that may be hard to certify for common ML regressors, especially for heteroskedastic variance estimation. The low-switching framework reduces adaptivity costs but introduces design choices (batch schedule, truncation/exploration thresholds, split proportions) that may be sensitive and are not fully tuned/validated across diverse data-generating processes. The theory largely targets unbiased estimation (and often assumes conditional independence/no interference and correct specification of ignorability), and it does not develop robustness to misspecification of regression/variance models beyond the doubly robust structure. Real-world validation is limited to simulations; no applied case study demonstrates operational constraints (delayed outcomes, noncompliance, attrition) and how the batching interacts with them.","They propose exploring Bayesian adaptive experimental design within their low-switching/bandit equivalence, suggesting that batched Thompson sampling variants might yield both strong empirical performance and frequentist guarantees. They suggest leveraging offline/historical data to warm-start or improve online experimental design, drawing on bandits-with-offline-data ideas. They also highlight extending the framework to resource/budget-constrained experimentation (bandits with knapsacks), and discuss broader extensions such as non-stationary outcomes/distribution shift, learning interference structures, and dynamic treatment effects/policies beyond static ATE.","Provide implementable, theory-backed variance-function estimators (or self-normalized procedures) that remove the strong “variance oracle” assumption, especially for high-dimensional covariates. Develop adaptive batch-sizing and stopping rules that optimize MSE under explicit operational costs (switching costs, delayed feedback, platform constraints) rather than fixing $K$ and a schedule a priori. Extend the lower-bound and optimality results to biased-but-valid estimators (e.g., regularized, stabilized AIPW) and to settings with non-overlap/positivity violations where $e^*(x)$ may approach 0/1 in practice. Release reference software and benchmarking suites to compare against modern adaptive experimentation methods (e.g., doubly robust bandit inference, switchback designs, non-stationary/adaptive CI methods) on standardized synthetic and real datasets.",2410.05552v3,https://arxiv.org/pdf/2410.05552v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:02:43Z
FALSE,NA,NA,Not applicable,Not specified,Transportation/logistics|Other,Case study (real dataset)|Other,NA,C/C++|Other,Not provided,NA,"The paper proposes a control system structure for an Autonomous Underwater Helicopter (AUH) to perform fixed-orientation docking onto a subsea docking system (SDS) using acoustic–inertial–optical guidance. The docking procedure is split into Homing (acoustic-inertial multisensor fusion using IMU+DVL+USBL) and Landing (monocular-camera optical guidance), with Landing further segmented into three altitude phases and using a linear speed decision rule to mitigate inertia and current disturbances. A success criterion for the final landing phase is defined using threshold checks on yaw/pitch/roll/depth relative to desired docking values. The method is validated experimentally via pool tests (maneuvering pond) and a sea trial in the South China Sea, demonstrating feasibility and robustness of the overall docking procedure. The software stack is built on MOOS-IvP (C++ autonomy framework), with PID-based motion controllers and a stated plan to replace some PID loops with LADRC-TD in future work.","Optical guidance computes deviation angles from the image spot center: $\alpha=\arctan\left(\frac{2\bar u}{M}\tan\alpha_0\right)$ and $\beta=\arctan\left(\frac{2\bar v}{N}\tan\beta_0\right)$, then converts to light coordinates using depth difference $h$ (Eqs. 1–4). The visible illumination radius is $R=\tan(\rho/2)\,h$ (Eq. 5). A piecewise linear speed decision sets desired speed $v$ as a function of radial error $r$ with inner/outer radii $R_i,R_o$ (Eq. 6). Docking success is evaluated by $\Phi= f(\theta)+f(\varphi)+f(\phi)+f(z)$ where each $f(\cdot)$ is a binary threshold indicator (Eqs. 7–11).","Pool experiments in a 45 m diameter, 6 m deep pond showed three successful representative docking trajectories, including maintaining hover near (0,0) while stepping through three Landing altitudes (reported approximately 4.8 m, 3.2 m, and 0.2 m). The AUH could recover when it temporarily left the illumination range by re-acquiring guidance and re-aligning yaw. In sea trials in the South China Sea, the AUH completed CloseToDocking through Landing3 and successfully landed on the SDS center despite currents and turbidity; the full docking procedure took about 13 minutes. Parameter settings for phases include Homing speed reduction from 1 m/s to 0.3 m/s within 15 m, and Landing phase target altitudes of 5 m, 3.5 m, and ~0.2 m with empirical distance/yaw thresholds (Tables 3–4).","The authors state that although the AUH can successfully dock onto the SDS, the procedure still takes considerable time and the success rate remained low. They suggest improving the motion controller for higher accuracy and faster response as a way to address these issues.","The experimental evaluation is limited to a small number of demonstrated pool trajectories and a representative sea-trial curve, with no statistical success-rate reporting, uncertainty quantification, or controlled comparisons against alternative docking controllers/guidance schemes. Many components are treated as black boxes (e.g., the commercial acoustic-inertial fusion system), making it difficult to reproduce results or attribute performance gains to specific algorithmic choices. The approach appears sensitive to environmental and hardware specifics (illumination cone, turbidity, camera parameters, SDS geometry), but robustness across broader operating conditions (stronger currents, different lighting, acoustic dropouts, sensor biases) is not systematically analyzed.",Future work is stated to involve implementing a motion controller with higher accuracy and faster response to reduce docking time and improve the currently low success rate. The paper also mentions that LADRC-TD is planned to replace PID in the yaw and depth controllers in future work.,"Provide reproducible benchmarks by reporting success rates over many trials, distributions of docking time and terminal pose error, and ablation studies (e.g., segmented aligning vs. single-phase descent, speed-decision on/off). Extend the method to handle degraded sensing (temporary camera loss/turbidity, USBL dropouts) with explicit fault detection and adaptive switching. Release an implementation (e.g., MOOS-IvP modules/configs) and add simulation-based stress tests to explore parameter sensitivity (thresholds, $R_i/R_o$, altitude schedule) and to tune controllers systematically rather than empirically.",2410.06953v1,https://arxiv.org/pdf/2410.06953v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:03:18Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Optimization,D-optimal|A-optimal|E-optimal|Other,Variable/General (vectors v_i in R^d; d-dimensional parameter space; budget k ≥ d),Theoretical/simulation only,Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops a unified deterministic rounding framework for classical discrete optimal experimental design with repetitions, using the method of interlacing polynomials together with standard convex relaxations. Given vectors v_1,...,v_m \in \mathbb{R}^d and an experiment budget k \ge d, it constructs a multiset S of size k to optimize D-design (maximize det^{1/d}), A-design (minimize tr((\sum_{i\in S} v_i v_i^\top)^{-1})), and E-design (maximize \lambda_{\min}(\sum_{i\in S} v_i v_i^\top)). The approach recovers best-known approximation guarantees for D- and A-design and improves E-design guarantees in the small-budget regime (k close to d), via a greedy descent on an interlacing-family tree of expected characteristic polynomials. It also yields an optimal approximation for a generalized ratio objective (based on elementary symmetric polynomials) that subsumes both D- and A-design. The algorithms are polynomial-time with stated runtime O(k m d^{\omega+1}) (\omega is the matrix-multiplication exponent) and are deterministic derandomizations of sampling/conditional-expectation style methods enabled by efficient computation of the relevant expected polynomials.","The design chooses a multiset S of size k to optimize one of: D: maximize (det(\sum_{i\in S} v_i v_i^\top))^{1/d}; A: minimize tr((\sum_{i\in S} v_i v_i^\top)^{-1}); E: maximize \lambda_{\min}(\sum_{i\in S} v_i v_i^\top). The convex relaxation uses weights x\in\mathbb{R}^m_+ with \sum_i x(i)\le k and objective applied to X(x)=\sum_i x(i) v_i v_i^\top. The interlacing-family node polynomial is a conditional expected characteristic polynomial f_{s_1,...,s_i}(x)=\mathbb{E}[\det(x I_d-\sum_{j=1}^k u_j u_j^\top)\mid u_1=v_{s_1},...,u_i=v_{s_i}], and the root has closed form f_{\emptyset}(x)=x^{d-k}\prod_{i=1}^d (1-(\lambda_i/k)\partial_x) x^k, where \lambda_i are eigenvalues of X(x).","For E-design, the paper gives a deterministic rounding algorithm with approximation factor (1-\sqrt{(d-1)/k})^{-2} for any k\ge d, implying a d^2-approximation when k=d and (1+\varepsilon)-approximation when k \gtrsim d/\varepsilon^2. For D-design, it provides a deterministic approximation factor k\cdot((k-d)!/k!)^{1/d}, which is \le k/(k-d+1), yielding (1+\varepsilon)-approximation when k \gtrsim d/\varepsilon and recovering the \le e guarantee when k=d. For A-design, it gives a deterministic k/(k-d+1)-approximation for any k\ge d. For the generalized ratio objective (NST22), it yields approximation k\cdot((k-l)!/k!)^{1/(l-l')} (and specialized bounds when k=l), with the same rounding framework and runtime O(k m d^{\omega+1}).","The authors state they do not yet know how to apply the interlacing polynomial method to the more general “without repetitions” setting (where each vector can be selected at most once), because it is unclear how to define the appropriate probability distributions and expected polynomials; they leave this as an open question. They also note an open gap for E-design at k=d: their bound implies a d^2-approximation while the best known integrality gap of the relaxation is only d, and closing this gap is left open.","The framework depends on solving (or having access to) the convex relaxation solution x and on linear-algebraic computations (including eigenvalue-related operations and polynomial root approximations), which may be computationally heavy for large m and d despite polynomial-time guarantees (runtime O(k m d^{\omega+1})). The paper focuses on the “with repetitions” model and does not provide empirical validation on real datasets or practical guidance on numerical stability/implementation details for computing the conditional expected characteristic polynomials and their minimum roots. Additionally, the guarantees are approximation-ratio results relative to the relaxation and may be sensitive to the conditioning of X and to finite-precision issues when normalizing for E-design (w_i=X^{-1/2}v_i).","They propose extending the method to the “without repetitions” setting as a main open direction. They also highlight the open problem of improving the E-design approximation at k=d to match the known integrality gap (closing the d vs. d^2 gap), noting potential implications for restricted invertibility.","A natural next step is to provide a practical implementation (e.g., robust numerical routines for computing the interlacing-family polynomials and their min-roots) and benchmark it against existing rounding methods (proportional volume sampling, regret-minimization, local search) on synthetic and real design instances. Another direction is to study robustness to model misspecification/regularization (e.g., ridge/Bayesian variants) and to extend the deterministic interlacing approach to constrained designs (e.g., partition/matroid constraints) beyond simple cardinality-with-repetition. Finally, developing tighter analyses (or alternative potential functions beyond \lambda_{\min} and derivatives at 0) could help close remaining approximation gaps in the small-budget regime and yield improved constants.",2410.11390v1,https://arxiv.org/pdf/2410.11390v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:03:45Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Parameter estimation|Optimization|Cost reduction|Other,Other,Variable/General (continuous design vector ξ ∈ R^d; sequential designs over K steps),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/jcopo/ContrastiveDiffusions,"The paper proposes CoDiff, a gradient-based Bayesian Optimal Experimental Design (BOED) method that maximizes expected information gain (EIG) for continuous designs, including sequential design. It introduces a pooled posterior (a logarithmic pooling / geometric mixture of posteriors across simulated outcomes) to serve as an efficient importance-sampling proposal, enabling a new tractable EIG gradient estimator. The approach is cast as a bi-level “optimization through sampling” framework, yielding a single-loop algorithm that jointly updates design variables and sampler states, reducing the cost of repeated posterior sampling. CoDiff supports both density-based samplers (e.g., Langevin/SMC-style) and data-based samplers by leveraging diffusion-model conditional sampling for inverse problems. Experiments on sequential source localization and MNIST inpainting show improved information-gain bounds and reconstruction quality versus baselines, demonstrating scalability to high-dimensional generative-prior settings.","The design objective is to maximize expected information gain: I(ξ)=E_{p(y|ξ)}[KL(p(θ|y,ξ) || p(θ))] and, in the sequential case, I_k(ξ,D_{k-1})=E_{p(y|ξ,D_{k-1})}[KL(p(θ|y,ξ,D_{k-1})||p(θ|D_{k-1}))]. The key methodological contribution is a pooled-posterior proposal q_{ξ,N}(θ) ∝ ∏_{i=1}^N p(θ|y_i,ξ)^{ν_i} ∝ p(θ)∏_{i=1}^N p(y_i|θ,ξ)^{ν_i}, which yields an efficient importance-sampling-based EIG gradient estimator ∇_ξ I(ξ) ≈ (1/N)∑_{i=1}^N [ g(ξ,y_i,θ_i,θ_i) − (1/M)∑_{j=1}^M w_{i,j} g(ξ,y_i,θ_i,θ'_j) ], with weights w_{i,j} = p(θ'_j|y_i,ξ)/q_{ξ,N}(θ'_j) (or self-normalized weights).","In sequential source localization (K=30, C=2 sources), CoDiff is reported to clearly outperform RL-BOED, VPCE, PASOA/SMC, and random baselines in terms of SPCE/SNMC information-gain bounds and posterior quality; it is stated to improve SPCE by about 30% over non-myopic RL-BOED and to achieve an L2 Wasserstein distance about two orders of magnitude lower. In MNIST inpainting with sequential 7×7 masks on 28×28 images, optimized designs improve SSIM versus random masking across measurements (Table 1): after 6 measurements, CoDiff SSIM is 0.826 vs 0.463 for random (with intermediate gains at each step). Reported per-step runtime is ~2.9s for source localization (N+M=400) and ~7.3s per design step for the MNIST diffusion-based setting (hardware-dependent).","The authors state that CoDiff remains a greedy (myopic) approach rather than non-myopic planning. They also note it requires an explicit likelihood expression, and that when using diffusion models for inverse problems they currently handle only linear forward models. They suggest that extending to non-linear forward models depends on ongoing advances in conditional diffusion for inverse problems.","The method’s performance likely depends on the quality/mixing of the inner sampling operators (Langevin/DiGS or diffusion posterior sampling), so biased/approximate samples may affect EIG-gradient accuracy and stability, especially in hard multi-modal posteriors. The pooled posterior q_{ξ,N} uses importance weights that can suffer from weight degeneracy in higher dimensions or when posteriors are far apart, potentially requiring large N/M or additional variance-reduction/resampling beyond what is explored. Comparisons are largely simulation-based on two tasks; broader empirical validation on real experimental systems (with model misspecification, constraints, and non-i.i.d. data) is not demonstrated.","They propose extending beyond greedy design to non-myopic approaches (e.g., RL-style lookahead) in the data-based setting. They suggest expanding diffusion-based inverse-problem support from linear to non-linear forward models as the field progresses. They also mention extending applicability to settings without explicit likelihoods by incorporating simulation-based inference / likelihood-free methods.","Developing theory or diagnostics for robustness of the pooled-posterior importance weights (e.g., effective sample size bounds, adaptive ν_i choices, or tempering schedules) could improve stability in high-dimensional settings. Providing self-starting/unknown-parameter variants (Phase I/II style) and handling autocorrelated observations or model misspecification would broaden practical use. Packaging the method into a reusable library with standardized benchmarks and ablations (sampler choice, N/M scaling, diffusion discretization effects) would help practitioners reproduce and apply CoDiff reliably.",2410.11826v2,https://arxiv.org/pdf/2410.11826v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:04:33Z
TRUE,Factorial (full)|Response surface|Screening|Sequential/adaptive,Optimization|Screening|Prediction|Cost reduction,D-optimal|Other,7 hyperparameters (reduced case uses 5 after dropping 2 insignificant ones),Finance/economics|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://sites.google.com/view/arc2024/home|https://www.mdpi.com/2227-9091/6/2/29|https://doi.org/10.57745/P0KHAG|https://www.sciencedirect.com/science/article/pii/S0957417418303178|https://ssrn.com/abstract=3320525|https://arxiv.org/abs/1807.02811|https://onlinelibrary.wiley.com/doi/abs/10.1155/2022/8513719|https://arxiv.org/abs/2308.03317,"The paper proposes a DOE-based hyperparameter optimization workflow for Combined Actuarial Neural Networks (CANN), using an initial two-level full factorial design with center points to screen effects and fit a first-order response surface, followed by a sequential “path of steepest descent” search, and then a second-order response surface fitted via a Central Composite Design (CCD). The response is the out-of-sample Poisson deviance loss on a French motor third-party liability dataset; the method is intended to efficiently locate near-optimal settings under limited computational resources. The study considers 7 hyperparameters (optimizer, neurons in three hidden layers, epochs, batch size, number of hidden layers) and a reduced variant dropping two statistically insignificant factors after the first phase. The second-order stage uses CCD star points with rotatability-based alpha and evaluates two practical fixes for out-of-bounds nominal optimizer levels (clipping vs modulo rotation), comparing them using the D-criterion determinant of $(X^\top X)^{-1}$. Empirically, tuning all hyperparameters achieved minimum observed loss 0.245823 in 288 runs, while dropping two factors achieved near-minimum loss 0.245976 in 188 runs, demonstrating substantial run savings vs grid search (e.g., 4^7=16384 vs 288).","The response surface is modeled as $y_i=h(x_{1,i},\dots,x_{k,i})+\varepsilon_i$ and approximated first by a first-order model $E[Y]=\beta_0+\sum_{j=1}^k\beta_j X_j^\*$ and later by a second-order model $E[Y]=\beta_0+\sum_{j=1}^p\beta_j X_j+\sum_{j=1}^p\beta_{jj}X_j^2+\sum_{j<\ell}\beta_{j\ell}X_jX_\ell$. Factors are coded via $x_{j,i}^*=(x_{j,i}-m_j)/s_j$ mapping min/max/mid to -1/1/0, and new search points are generated along steepest descent with $X_h=(t/s)\,b^*$ where $b^*$ is the vector of estimated linear effects. CCD rotatability is enforced with star distance $\alpha=\pm\left(2^{p-f}n_c/n_s\right)^{1/4}$, and a D-criterion comparison uses $D=|(X^\top X)^{-1}|$.","Initial screening used a $2^7$ full factorial plus a center point with 4 replications, totaling 132 runs, and identified optimizer, N2, epochs, batch size, and number of layers as significant while N1 and N3 were insignificant. A steepest-descent phase added 20 points and found the same new optimum for complete and reduced tuning: Op=5, N1=20, N2=18, N3=10, Ep=703, Bh=8542, Lr=3 with loss 0.24598. Second-order CCDs used 146 runs (complete, p=7) and 46 runs (reduced, p=5); modulo rotation of out-of-range optimizer star points yielded smaller D-criterion determinants than clipping. Overall minima reported were 0.245823 (288 runs, tuning all hyperparameters) and 0.245976 (188 runs, dropping two hyperparameters), versus grid search costs such as $4^7=16384$ runs.",None stated.,"The response surface models treat the optimizer (a nominal factor with 7 categories) as an ordered numeric variable and handle out-of-range star points via ad-hoc clipping/modulo, which may distort the geometry and interpretability of the fitted quadratic surface. Results are based on a single dataset and a single random train/test split; performance and selected “optimal” hyperparameters may vary under different splits or repeated runs given neural network stochasticity. The approach assumes a quadratic approximation is adequate locally; strong non-smoothness or interactions beyond second order could reduce reliability of the predicted optimum, especially with mixed discrete/continuous factors.","The authors suggest integrating surrogate models (e.g., nonlinear convex quadrature surrogate hyperparameter optimization, NCQS) to reduce evaluations further, and exploring homotopy-based approaches to guide search and avoid local minima. They also propose hybrid approaches combining surrogate models or homotopy methods with RSM (surrogates to locate promising regions, RSM to refine). Finally, they recommend extending the methodology beyond neural networks to other ML algorithms such as SVMs, gradient boosting, and ensembles.","Develop DOE/RSM variants explicitly designed for mixed discrete-continuous hyperparameters (e.g., categorical factors handled via appropriate coding and split-plot or mixed-integer design strategies) rather than numericizing categories like optimizers. Add robustness to stochastic training by incorporating replication/repeated training at design points and modeling noise (e.g., via random effects or Bayesian RSM), and validate across multiple datasets/splits. Provide an open-source implementation to enable reproducibility and broader adoption, and compare against modern HPO baselines (Bayesian optimization, Hyperband/BOHB) under equal computational budgets.",2410.12824v1,https://arxiv.org/pdf/2410.12824v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:05:19Z
TRUE,Computer experiment|Screening|Sequential/adaptive|Other,Screening|Prediction|Parameter estimation|Cost reduction|Other,Space-filling|Not applicable,Variable/General,Energy/utilities|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/ERIGrid2/toolbox_doe_sa|https://salib.readthedocs.io/en/latest/index.html,"The paper presents an open-source toolbox to support Design of Experiments (DoE) and sensitivity analysis for complex energy-system studies conducted in co-simulation and hardware/laboratory tests. Users define an object-oriented experiment configuration (JSON) including default parameters, factor variation ranges/distributions, sampling strategy, and target metrics; the toolbox then generates experiment “recipes” (complete parameter sets) for each run. The toolbox supports multiple sampling/design strategies including Sobol sequences, Latin Hypercube Sampling, extreme-point designs, one-at-a-time (OAT), Sobol-indices designs, and eFAST, and pairs these with analysis methods such as ANOVA/MANOVA, Kriging (Gaussian-process) metamodeling, and variance-based sensitivity indices. Two application cases demonstrate usage: a multi-energy co-simulation in mosaik (e.g., Sobol indices with 1024 samples plus metamodel visualizations) and a hybrid energy storage study using ANOVA/MANOVA screening and Sobol indices (large run counts due to nuisance-factor sampling and resets). Overall, the contribution is a practical, extensible workflow and implementation that standardizes experiment specification, sampling, and post-analysis for CPES testing and simulation-based studies.","In the HESS case, storage peak powers are parameterized by fixing total capacity $P^{\max}_{\mathrm{HESS}}$ and using relative fractions $a_{SC}$ and $a_{Li}$ so that $P^{\max}_i = P^{\max}_{\mathrm{HESS}}(a_{SC}+a_{Li}+(1-a_{SC}-a_{Li}))$ for $i\in\{SC,Li,VRB\}$. The toolbox computes variance-based sensitivity measures including first-order Sobol index $S1$ and total-effect index $ST$ (visualized for target metrics). For fast screening it applies ANOVA/MANOVA, using the ANOVA p-value (e.g., significance threshold typically 0.05) to identify influential factors.","For the multi-energy co-simulation benchmark, Sobol-indices GSA with 1024 samples indicates PV scaling has the strongest first-order effect on electricity self-consumption, while heat-profile scaling has the strongest first-order effect on minimum supply temperature (with heat-pump power also influential). For the hybrid energy storage system, ANOVA shows VRB capacity fraction $cf$ significantly affects Losses_hess (F=59.907739, p=0.000000) and restoration factor $rf$ significantly affects Degradation_li (F=20.544288, p=0.000041), while $a_{SC}$ and $a_{Li}$ are not significant in the reported ANOVA table. In the HESS Sobol study, using n=512 leading to 3072 parameter combinations and (after nuisance-factor handling) 23040 simulation runs, the total-effect indices are large for many factors, suggesting strong interaction/nonlinear effects even when first-order effects are smaller.",The authors note the toolbox can be further extended to support more sampling/analysis methods. They explicitly state that handling of known/controllable nuisance factors is not yet integrated in a dedicated way and was done manually in the second application case; they suggest adding explicit blocking support for nuisance factors. They also note some parts may need to be generalized further and that adapters to more execution frameworks would increase usability.,"Although positioned for both simulation and hardware tests, the paper provides limited detail on validation in real hardware experiments (the second case uses a simulation setup as a black-box), so practical hardware integration/latency, data-quality, and operational constraints are not fully assessed. The toolbox appears focused on sampling/SA workflows rather than classical optimal design (e.g., D-/A-/I-optimal designs) and does not provide guidance on choosing sample sizes beyond rules-of-thumb (e.g., Sobol indices ≥1000). Comparisons to alternative DOE/SA tooling beyond SALib (e.g., other UQ frameworks, Bayesian optimization/sequential design) and runtime/compute cost benchmarks of the toolbox itself are not systematically reported.","They propose extending the toolbox to support additional sampling and analysis methods. They explicitly suggest integrating specific handling of known and controllable nuisance factors (e.g., blocking) directly into the toolbox, rather than doing it manually. They also suggest improving generality and adding adapters to additional execution frameworks, and mention future integration/import of uncertainty descriptions from Holistic Test Description (HTD) templates to streamline experiment development.","Add classical optimal-design capabilities (e.g., D-/I-optimal designs) and sequential design strategies (e.g., adaptive sampling/metamodel-based refinement) to reduce required runs for expensive co-simulations and hardware tests. Provide robust versions that address autocorrelation/time-series outputs, nonstationarity, and uncertain/unknown noise models common in energy-system experiments. Release ready-to-run examples and a packaged API (e.g., PyPI/conda) plus benchmark suites to quantify computational overhead, scalability with many factors, and reproducibility across platforms.",2410.16923v1,https://arxiv.org/pdf/2410.16923v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:06:04Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Optimization|Cost reduction|Other,Bayesian D-optimal,"Variable/General (graph edges as parameters; $p=\binom{m}{2}$ edge-cost coefficients; examples include m=6,8,10 and m=20)",Transportation/logistics,Simulation study,TRUE,R,Not provided,NA,"The paper proposes a Bayesian learning-and-optimization framework for last-mile delivery routing by treating unknown travel costs between delivery zones (edges of a complete undirected graph) as random parameters and designing a small set of delivery-route experiments (Hamiltonian circuits) to estimate them efficiently. A Bayesian linear regression model links the observed total cost of a tested circuit to the included edges, with Normal priors on edge costs, and posterior updating is used to obtain cost estimates. The experimental design problem is to select subsets of Hamiltonian circuits that maximize the Bayesian D-optimality criterion, and the authors prove that the full set of circuits is optimal for any concave permutation-invariant criterion, then provide a recursive construction of optimal half-fractions. For tighter budgets, they propose heuristic construction methods (a bubble-sort style greedy exchange and a simulated annealing search) to find highly D-efficient circuit subsets without enumerating all routes. Simulations show that the bubble-sort method attains higher relative D-efficiency than simulated annealing for various m and run sizes, and that using Bayesian (and often frequentist) estimated edge costs yields lower true route costs than using priors alone when combined with common TSP heuristics.","Design/regression model: for a circuit $a$, $y(a)=\sum_{j<k}\beta_{jk}x_{jk}(a)+\epsilon(a)$ with $\beta_{jk}\sim N(\mu_{jk},\sigma^2_{jk})$ and $\epsilon(a)\sim N(0,\sigma^2)$, where $x_{jk}(a)\in\{0,1\}$ indicates whether edge $(j,k)$ is in the circuit. Posterior mean (ridge-like) estimator: $\hat\beta=(X^TX+R)^{-1}(X^TY+R\mu)$ where $R$ is diagonal with prior precisions. Bayesian D-optimal design criterion for a subset $D$ of circuits: $\phi(D)=|X_D^T X_D+R|$, and relative D-efficiency is computed via $\left(\frac{|X_D^T X_D/n+R|}{|X_f^T X_f/N+R|}\right)^{1/\binom{m}{2}}$.","In simulation comparing design-construction heuristics (with $R=\tau^2 I$, $\tau^2=0.01$), bubble sort achieved higher median relative D-efficiency than simulated annealing across tested cases: for example, at $m=10$, bubble sort medians were 0.875 (n=46), 0.967 (n=91), and 0.985 (n=136) versus 0.807, 0.898, and 0.929 for simulated annealing. Even when the design used fewer than 0.1% of all Hamiltonian circuits (e.g., $m=10$ where $n/N<0.001$), bubble sort found designs with ~98% relative D-efficiency. In routing simulations with $m=20$ and multiple noise scenarios, using data-driven edge-cost estimates (Bayesian posterior estimates or frequentist ridge estimates) generally reduced true route-cost distributions versus using prior costs only, with Bayesian estimates often better at smaller n and converging toward frequentist performance at very large n.",None stated.,"The DOE formulation assumes the linear additive edge-cost model is adequate and that circuit-level errors are i.i.d. Normal; real travel times can be non-Normal, heteroskedastic, and temporally correlated (traffic regimes), which can affect both design optimality and estimator performance. The experimental unit is a full Hamiltonian circuit, which may be operationally expensive and may not reflect constraints like time windows, capacity, or nonstationary conditions during a run. The design search heuristics (bubble sort/annealing) are evaluated mainly via simulation and relative D-efficiency; additional robustness checks (e.g., misspecified priors, different R structures, constraints on feasible routes) and more real-data validation would strengthen practical guidance.","The authors suggest extending the model beyond main effects (edge costs) to include interaction effects between edges, though the interaction space is large. They also propose considering dual-response objectives that minimize not only mean travel cost but also variability (variance) of travel costs. Another stated direction is developing online/batch settings where route-cost information arrives over time and the routing/design decisions are updated sequentially.","Developing constrained-design versions that respect operational feasibility (time windows, driver hours, capacity, drone battery constraints) would make the DOE directly deployable. A self-starting/robust Bayesian design that accounts for autocorrelation and regime switching (traffic/weather) or heavy-tailed errors could improve reliability. Providing open-source implementations (e.g., an R package) and benchmarking against additional optimal-design algorithms (e.g., coordinate exchange, Fedorov variants with sampling, modern metaheuristics) would aid adoption and reproducibility.",2410.17392v1,https://arxiv.org/pdf/2410.17392v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:06:42Z
TRUE,Response surface|Split-plot|Other|Optimal design,Parameter estimation|Prediction|Model discrimination|Other,D-optimal|A-optimal|Compound criterion|Other,Variable/General (examples include 3 factors; 1 HS + 4 ES factors; 2 HS + 3 ES factors; and 7 two-level + 1 five-level + 4 three-level factors across strata),Manufacturing (general)|Food/agriculture|Other,Other,TRUE,Other,Supplementary material (Journal/Publisher)|Not provided,NA,"This paper develops a general methodology for constructing response surface designs under restricted randomization with arbitrary multi-stratum unit structures formed by combinations of nesting and crossing (e.g., row×column, split-row×column, strip-split-plot, split-row×split-column). The approach extends the stratum-by-stratum construction framework to mixed crossed/nested structures and uses compound optimality criteria to balance efficiency for fixed-effect estimation with inferential needs such as ensuring pure error degrees of freedom (and optionally lack-of-fit checking) within strata. Designs are optimized sequentially by stratum using point-exchange (and constrained interchange/swapping steps where replication across crossed factors permits), treating blocking/unit effects as fixed at the design stage to avoid requiring prior variance component values and to perform well under large higher-stratum variance components. The paper illustrates the method on multiple motivating examples and compares against mixed-model fixed-effects D-optimal designs from JMP where feasible, showing that adding pure-error degrees of freedom typically costs a modest-to-moderate loss in D/A-efficiency while enabling inference. For a very large 500-run example, the authors report providing constructed designs in supplementary material and note practical computational strategies (limited starting designs, parallel runs).","The experimental results are modeled using a linear mixed model $Y=T\mu + Zb + \varepsilon$ with $\varepsilon\sim N(0,\sigma^2 I)$ and uncorrelated random effects across strata; the response surface approximation uses $E(Y)=X\beta$ with GLS estimator $\hat\beta=(X'V^{-1}X)^{-1}X'V^{-1}Y$ and $\mathrm{Var}(\hat\beta)=\sigma^2 (X'V^{-1}X)^{-1}$. Designs are constructed via a stratum-specific compound optimality criterion (Eq. 5) combining determinant-based terms (D/DP-type), degrees-of-freedom/inference components, and a weighted-A (trace) component, with weights $\kappa$ summing to 1. The criterion uses stratum-specific projection/adjustment matrices $Q_s$ for completely randomized ($Q_s=I-\frac{1}{m_s}11'$), blocked ($Q_s=I-Z_{bs}(Z'_{bs}Z_{bs})^{-1}Z'_{bs}$), and crossed row×column structures ($Q_s=I-Z_{rc}(Z^{\star\prime}_{rc}Z^{\star}_{rc})^{-1}Z'_{rc}$).","Across worked examples, DS-optimal designs (and JMP mixed-model fixed-effects D-optimal designs when available) often allocate essentially no pure-error df, whereas the proposed (DP)S and composite criteria can create designs with nontrivial pure-error df in multiple strata while retaining high estimation efficiency. In Example 1 (7 days × 4 times; three 3-level factors), the compromise design achieves both pure-error and lack-of-fit df with DS-efficiency around 93% relative to the JMP D* design (depending weakly on variance ratios), while the (DP)S design yields substantially more pure-error df but lower DS/AS efficiencies. In Example 2 (26 days × 2 periods; 1 HS + 4 ES three-level factors), enforcing pure-error df reduces DS-efficiency to about 82% (MSS(DP)S) or 87% (compromise) relative to D*, but AS-efficiency can improve markedly as day-to-day variance increases. In Example 3 (strip-split-plot; 60 runs) and Example 4 (split-row×split-column; 500 runs), the (DP)S and compromise designs provide sizable pure-error df in multiple strata with DS-efficiencies roughly 89%/99% (Example 3, relative to MSSDS) and about 86%/91% (Example 4, relative to MSSDS), with efficiencies reported to be fairly robust to changes in variance ratios.","The authors assume a “simple orthogonal unit structure” (balanced nesting and complete crossing) so that the implied mixed-model covariance structure is correct without strong additional assumptions; relaxing this is described as straightforward but would require potentially debatable modeling assumptions (e.g., constant variance components under unequal replication). They also note computational burden for very large problems (e.g., 500-run, 261-parameter model), limiting the number of starting designs used due to runtime, even with parallel computation. They report that for some cases JMP could not find a baseline fixed-effects D-optimal design (or produced inferior designs), limiting direct comparisons.","The method is largely demonstrated through constructed examples and efficiency comparisons; broader benchmarking against alternative modern algorithms for multi-stratum RSM (e.g., coordinate-exchange variants, Bayesian/robust designs, space-filling constrained designs) is limited, so relative performance outside the illustrated structures is uncertain. Treating blocking/unit effects as fixed at the design stage improves robustness to large higher-stratum variances, but may be conservative when variance components are small and could sacrifice efficiency compared with variance-informed mixed-model optimal designs. Practical implementation details (software, runtime scaling, convergence diagnostics, and reproducibility) are not fully specified in the paper text provided, which may hinder adoption without accompanying code.","The discussion explicitly notes that exploring how designs change under different requirements/compound-criterion weightings is an interesting area for future research, and that the general stratum-by-stratum strategy could be used with criteria other than the compound criteria used here.","Developing and releasing an open-source implementation (e.g., R/Python package) with reproducible scripts for all examples (including the 500-run case) and scalable parallelization would significantly improve practical uptake. Extending the framework to non-orthogonal/unbalanced multi-stratum structures (unequal subplot numbers, incomplete crossings) with principled variance modeling or robust worst-case criteria would broaden applicability. Additional work could study robustness to model misspecification (nonquadratic surfaces), correlated/heteroscedastic errors, and incorporate Bayesian/uncertainty-aware criteria for variance components while still maintaining inferential properties like pure-error df.",2410.18734v1,https://arxiv.org/pdf/2410.18734v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:07:28Z
FALSE,NA,NA,Not applicable,Not specified,Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes First-Glance Off-Policy Selection (FPS), an offline reinforcement-learning policy selection framework for human-centric systems where a new participant arrives with only an initial state observation available (no prior trajectory data for that individual). FPS addresses participant heterogeneity by partitioning participants into sub-groups (via clustering on initial-state features), augmenting subgroup trajectories using a VAE-based generative model to improve state-action coverage, and then selecting the best pre-trained candidate policy per subgroup using an unbiased importance-sampling-based estimator with a variance bound. The method is evaluated in a real-world intelligent tutoring setting (1,288 students across multiple semesters) and a simulated sepsis treatment environment, showing improved outcomes over standard off-policy evaluation/selection baselines (WIS/PDIS/FQE/WDR/MAGIC/DualDICE and augmentation variants). The work advances offline policy selection for high-stakes domains by making selection personalized at intake time (t=0) and by combining subgrouping with data augmentation to stabilize estimation and selection under limited data.","FPS builds subgroup-specific policy selection criteria using an unbiased estimator of the value improvement over the behavior policy: \(\hat D^{\pi,\beta}_{K_m}=\frac{1}{|I_m|}\sum_{i\in I_m}\big(\omega_i\sum_{t=1}^T\gamma^{t-1}r_t^{(i)}-\sum_{t=1}^T\gamma^{t-1}r_t^{(i)}\big)\), where \(\omega_i=\prod_{t=1}^T \pi(a_t^{(i)}|s_t^{(i)})/\beta(a_t^{(i)}|s_t^{(i)})\). The paper provides a variance bound: \(\mathrm{Var}(\hat D)\le \|\sum_{t=1}^T\gamma^{t-1}r_t\|_\infty^2\,(1/\mathrm{ESS}-1/N)\).","In the real-world intelligent education experiment (1,288 students; policies deployed to the 6th-semester cohort), FPS improved learning outcomes by 208% versus instructor hand-crafted selection criteria and by 136% versus existing OPS methods’ selected policies (reported in terms of normalized learning gain improvements). FPS also produced the smallest gap between estimated and true policy rewards among compared methods in that setting. In the simulated sepsis experiment (with offline dataset sizes N=2,500/5,000/10,000 and no VAE augmentation), FPS achieved the lowest absolute error and top-1 regret and the highest return among WIS/PDIS/FQE/WDR/MAGIC; e.g., at N=5,000 FPS AE=0.006±0.00 and Return=0.149±0.01 with Regret@1=0.020±0.00.","The authors note that the work focuses specifically on off-policy selection (OPS) rather than offline RL policy optimization, to isolate the benefits of subgroup partitioning and trajectory augmentation. They also state future work could derive estimators that allow bias-variance trade-offs by integrating alternatives such as WDR or MAGIC in place of pure IS weighting, implying current estimator choices trade bias for variance control.","The approach depends on the quality and stability of subgroup partitioning from initial-state features; mis-clustering (distribution shift, feature drift, or insufficient initial information) could lead to systematically wrong policy assignments. The IS-based estimator and its variance behavior can be fragile under large importance weights (poor overlap between behavior and target policies), and the VAE augmentation may introduce model bias/artifacts that affect selection despite improved coverage. Real-world validation is concentrated in a single tutoring deployment and one simulated healthcare benchmark, so generalizability to other HCS settings (different dynamics, partial observability, non-stationarity, or stronger confounding) is not fully established.","They suggest extending FPS toward an offline RL policy optimization framework (beyond OPS). They also propose deriving estimators (for Proposition 2.5) that enable explicit bias-variance trade-offs, for example by integrating WDR or MAGIC to substitute for (or complement) the importance sampling weights.","A natural extension is a self-starting/adaptive version that updates subgroup assignment and/or policy choice after a small number of interactions (t>0) while maintaining safety constraints. Additional work could study robustness to misspecification and distribution shift (e.g., covariate shift in incoming participants), including calibration/uncertainty-aware selection and safeguards for out-of-distribution initial states. Providing open-source reference implementations and standardized benchmarks for “first-glance” personalized OPS would also strengthen reproducibility and adoption in practice.",2410.20017v1,https://arxiv.org/pdf/2410.20017v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:08:02Z
TRUE,Other|Sequential/adaptive,Optimization|Prediction|Cost reduction|Other,Not applicable,"Variable/General (example dataset: 20,600 input parameters reduced to ~20, then ~15–19 after expert filtering)",Semiconductor/electronics|Manufacturing (general),Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a generic workflow for optimizing complex manufacturing processes “based on DOE” by chaining data cleaning, AI-based modelization, and multi-objective optimization into a single software pipeline. The methodology emphasizes automated consolidation of heterogeneous experimental datasets (parsing machine files, homogenization, outlier removal, redundancy/collinearity handling, and duplicate-experiment aggregation via median) and parameter reduction using statistical/collinearity ranking and exhaustive subset search. For modeling, it uses multi-output AI approaches (notably deep neural networks, with discussion of multi-objective extensions via decomposition methods and copulas) to create surrogate models of experimental outcomes. Optimization is performed on the learned surrogate models to generate candidate “recipes,” including multi-objective optimization via Pareto-front exploration with user-facing dashboards and exploration–exploitation hyperparameter tuning. The approach is validated on MicroLED manufacturing data (Aledia) with multiple output criteria (e.g., maxWPE and Lambda peak), demonstrating that substantial feature reduction can retain predictive performance and enable recipe optimization without additional experiments.",NA,"The industrial dataset contains 20,600 input parameters and 2 output parameters. Automatic data reduction selected 20 parameters, after which process experts removed 5, yielding a final reduced set; another reported multi-output modeling example uses 19 input parameters to predict maxWPE and Lambda peak. Model selection is guided by predictive metrics (RMSE; adjusted R² noted to show similar trends), and optimized candidate recipes are produced by surrogate-based optimization and assessed through Pareto-front dashboards. Example reported fit diagnostics include R² values shown on plots for deep neural networks (e.g., ~0.97 train and ~0.87 test for maxWPE; ~0.93 train and ~0.65 test for Lambda peak) under an independence assumption between objectives.","The authors note practical constraints typical of DOE in complex processes: experiments are costly and the number of DOE campaigns is limited, restricting available data for drawing conclusions. They also indicate that standard DOE can make physical intuition difficult when multiple modifications occur simultaneously, motivating the AI-assisted approach. For multi-objective settings, they caution that collapsing objectives into single “merit functions” can hide dependencies and introduce uncertainty, motivating explicit multi-output handling.","Although framed as DOE-based, the paper does not specify classical DOE structures (e.g., factor levels, randomization, blocking, fractional factorial/RSM) or how experiments were originally designed, limiting reproducibility and clarity on design validity. Performance evidence is largely descriptive (plots/metrics) without systematic benchmarking against standard DOE/RSM surrogate approaches (e.g., Gaussian processes, RSM/kriging, Bayesian optimization baselines) under matched budgets. The workflow appears to depend on extensive proprietary data preparation and expert curation; robustness to measurement error, autocorrelation, and dataset shift across process stages is not established, and implementation details (software stack, optimizer specifics) are not provided.","The authors propose future research directions including Bayesian optimization based on surrogate models, reinforcement controls, optimal control policies, and sequence-based neural networks for time-series data. They also state that for more than three objectives, more sophisticated optimization strategies will be adapted to higher-dimensional objective spaces.","A valuable extension would be to explicitly integrate DOE planning (e.g., sequential DOE/Bayesian experimental design) to recommend the next experiments under cost/throughput constraints rather than optimizing only on historical surrogates. Robustness studies under realistic manufacturing issues—autocorrelation, drift, batch effects, missingness mechanisms, and measurement-system variation—would clarify when the pipeline remains reliable. Publishing reference implementations or at least pseudocode for the cleaning, reduction, and multi-objective optimizer (including hyperparameters and stopping rules) would improve reproducibility and practitioner adoption.",2410.21294v1,https://arxiv.org/pdf/2410.21294v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:08:37Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Optimization|Prediction|Model discrimination|Other,Other,"Variable/General (depends on task; e.g., 1D x in toy regression; decision-aware AL uses covariates x plus discrete decision a with Nd=4; HPO tasks use dx up to 16)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/huangdaolang/amortized-decision-aware-bed,"The paper proposes an amortized, decision-aware Bayesian experimental design (BED) framework that optimizes experiments for downstream decision utility rather than (only) parameter information gain. It introduces Decision Utility Gain (DUG) and its expectation (EDUG) as utility-centric design criteria, and a Transformer Neural Decision Process (TNDP) architecture that jointly amortizes (i) sequential design selection via a policy head and (ii) predictive distribution approximation via a prediction head. The query policy is trained with a non-myopic reinforcement-learning objective (REINFORCE with discounted returns), aiming to improve final decision quality across multi-step experiment budgets. Empirical studies on synthetic regression, decision-aware active learning, and top-k hyperparameter optimization show TNDP produces more decision-informative queries and improves downstream decision performance compared with non-decision-aware baselines and other acquisition methods. The work advances BED by explicitly integrating downstream decision-making into amortized design generation and by coupling design and predictive modeling in a single Transformer-based neural-process architecture.","Traditional BED uses expected information gain: $\mathrm{EIG}(\xi)=\mathbb{E}_{p(y\mid \xi)}[H[p(\theta)]-H[p(\theta\mid \xi,y)]]$. The paper defines Decision Utility Gain: $\mathrm{DUG}(\xi_t,y_t)=\max_{a\in A}\mathbb{E}_{p(y_\Xi\mid h_{1:t})}[u(y_\Xi,a)]-\max_{a\in A}\mathbb{E}_{p(y_\Xi\mid h_{1:t-1})}[u(y_\Xi,a)]$, and its expectation $\mathrm{EDUG}(\xi_t)=\mathbb{E}_{p(y_t\mid \xi_t,h_{1:t-1})}[\mathrm{DUG}(\xi_t,y_t)]$. TNDP trains a predictive head by Gaussian NLL $\mathcal{L}^{(p)}=-\sum_{t=1}^T\sum_{i=1}^{n_p}\log \mathcal{N}(y_i^{(p)}\mid \mu_{i,t},\sigma^2_{i,t})$ and a policy head with REINFORCE $\mathcal{L}^{(q)}=-\sum_{t=1}^T R_t\log \pi(\xi_t\mid h_{1:t-1})$, using discounted returns $R_t=\sum_{k=t}^T \alpha^{k-t} r_k$.","Across synthetic decision-aware active learning (Nd=4), TNDP achieves the highest proportion of correct downstream decisions over acquisition steps compared to GP-based baselines including random sampling, uncertainty sampling, targeted EIG, and decision-EIG (figure shows clear separation with TNDP on top across steps). In top-k hyperparameter optimization on four HPO-B meta-datasets (ranger, rpart, svm, xgboost) with $k=3$, $T=50$, TNDP consistently attains the best average utility trajectory and improves especially in early queries (first ~10 steps). A deployment-time comparison in decision-aware active learning reports acquisition time ~0.015s and total time ~0.31s for TNDP versus hundreds to thousands of seconds per step for T-EIG/D-EIG GP methods, demonstrating large amortization speedups. The authors estimate ~10 GPU-hours on a Tesla V100 (32GB) to reproduce each experiment, with ~8GB memory use.","The authors note that training the query head is an RL problem and using basic REINFORCE can be unstable, especially with sparse rewards; more advanced RL methods (e.g., PPO) could help but add hyperparameters and cost. Like other amortized approaches, the method requires substantial data and upfront training time, and Transformer quadratic complexity can bottleneck when the query set is very large. They assume an independent Gaussian likelihood (potentially missing correlations), assume the model is well-specified (misspecification/utility shifts can hurt), require fixed design dimensionality, and train for a fixed finite horizon (fixed-step length), limiting applicability to variable/infinite-horizon settings.","The evaluation is largely simulation/benchmark driven; real-world validation is limited (the retrosynthesis experiment is described as using a non-public meta-dataset), which hinders independent replication and assessment of practical robustness. The policy is trained with sampled candidate query sets (for continuous spaces) and, at deployment, may require additional optimization over candidates; performance may be sensitive to candidate-set construction and not fully characterized. Comparisons in some tasks mix amortized and non-amortized baselines with different computational budgets; without equalizing wall-clock or training resources, it can be hard to separate methodological gains from resource advantages. The method’s behavior under common experimental constraints (e.g., costs, batch/parallel experiments, safety/ethical constraints in clinical querying) is not directly studied.","They propose exploring more advanced RL algorithms (e.g., PPO) to improve stability for sparse-reward tasks and developing more sample-efficient training to reduce data and training requirements. They suggest modeling correlations by replacing independent Gaussian likelihoods with a joint multivariate normal output or autoregressive prediction. They also highlight robust experimental design under model misspecification, extending the system to accept variable-dimensional designs (dimension-agnostic amortization), and developing approaches that can handle infinite-horizon experimental design rather than a fixed finite horizon.","Developing constraint-aware and cost-sensitive variants (e.g., incorporating per-query costs, budgets, or feasibility constraints) would make the framework closer to real experimental settings. Extending TNDP to batch/parallel query selection (common in labs and HPO) and studying regret/utility guarantees under batching would broaden applicability. Providing open-source, standardized evaluation suites (including public real-world datasets) and reporting sensitivity analyses (discount factor $\alpha$, query-set size $n_q$, horizon $T$) would improve reproducibility and practitioner guidance. Investigating calibration and uncertainty quality of the predictive head (beyond downstream utility) and adding diagnostics for failure under distribution shift would strengthen deployment readiness.",2411.02064v2,https://arxiv.org/pdf/2411.02064v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:09:22Z
TRUE,Optimal design|Sequential/adaptive,Prediction|Parameter estimation|Cost reduction|Other,Other,Variable/General (examples include 15 parameters for SW MoS2 potential; power-grid state variables; ocean environment + source/receiver parameters),Energy/utilities|Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/yonatank93/information-matching,"The paper proposes an information-matching criterion for optimal experimental design (OED) and active learning that selects a minimal subset of training data from a candidate pool while guaranteeing a target precision for downstream quantities of interest (QoIs). The method uses Fisher Information Matrices (FIMs) for the training scenario and the QoI/prediction scenario and chooses nonnegative weights on candidate data by solving a convex semidefinite program that enforces an information dominance constraint (training FIM ⪰ QoI FIM) while promoting sparsity via an ℓ1 objective. A theorem shows that when the constraint holds, the propagated QoI covariance from the selected training data is bounded by the target QoI covariance (up to third-order terms), enabling designs that focus only on QoI-relevant parameter combinations in sloppy/unidentifiable models. The approach is demonstrated in (i) PMU sensor placement for power-network observability, (ii) hydrophone placement for ocean acoustic source localization without full environmental inversion, and (iii) an active-learning loop for fitting interatomic potentials, where only a handful of selected configurations can meet the QoI precision target. Overall, it advances OED by reframing design optimality as matching the information required for prediction rather than globally shrinking parameter uncertainty via A/D/E-optimality criteria.","Training loss is weighted least squares: $\ell(\theta)=\tfrac12\sum_{m=1}^M w_m\|p_m-f(\theta;x_m)\|_2^2$, with $w_m=1/\sigma_m^2$. The training-data FIM decomposes additively: $I(\theta)=\sum_{m=1}^M w_m I_m(\theta)$ with $I_m(\theta)=J_f(\theta;x_m)^T J_f(\theta;x_m)$. The QoI (prediction) FIM is $J(\theta)=J_g(\theta)^T\Sigma^{-1}J_g(\theta)$, and the design is obtained by solving $\min\|w\|_1$ s.t. $w_m\ge0$ and $\sum_m w_m I_m\succeq J$ (a convex SDP); a resulting guarantee is $\mathrm{Cov}(g)\preceq \Sigma+O(\epsilon^3)$.","In the IEEE 39-bus PMU placement benchmark, the method selects PMU locations consistent with prior literature for full observability and produces different placements when only partial-area observability is required (by allowing infinite uncertainty for external states via zeroing corresponding entries in $J$). In ocean acoustics source localization at 200 Hz, the optimal receiver set is about 5% of candidate locations while meeting the specified localization targets; across multiple sediment types and frequencies, at most about 8% of receivers are sufficient. In active learning for a 15-parameter Stillinger–Weber MoS2 potential trained from a pool of 2000 candidate configurations, seven selected configurations suffice to achieve a target QoI uncertainty (10% of the full-dataset-trained model’s predictions). The authors also report that different initial parameter guesses may lead to different selected points/weights, but the final propagated QoI uncertainty still satisfies the target bound when the feasibility constraint holds.","The authors state that solving the information-matching SDP depends on the richness of the initial candidate pool: if any QoI-relevant parameter combination cannot be constrained by any candidate data, the optimization becomes infeasible. They note that while convex solvers can detect infeasibility, identifying how to augment the candidate set with additional informative candidates is an open, problem-dependent question. They also remark that the active-learning outcome (selected configurations/weights/parameters) can depend on initialization, though the final uncertainty bound remains valid if feasible.","The method relies on local, FIM-based (essentially linearized/Gaussian) uncertainty propagation around a parameter estimate, which can misrepresent uncertainty for strongly nonlinear models or multimodal posteriors, especially far from the linear regime. The approach presumes independent observations and a correctly specified noise model (e.g., weighted least squares with known/meaningful variances), which may be unrealistic in many experimental settings. Computational scalability can be limited by the need to compute Jacobians for every candidate point and to solve a semidefinite program that may become expensive for very high-dimensional parameterizations (common in modern ML). Comparisons to alternative QoI-focused design criteria (e.g., Bayesian decision-theoretic utility, expected information gain, or direct I-/G-optimality for prediction) are not fully benchmarked across a wide range of canonical DOE test problems.","The authors suggest extending the method to larger models and machine-learning applications, including machine-learned interatomic potentials, and studying feasibility and potential advantages in that setting. They also propose deeper theoretical analysis of the limiting behavior of the optimization to provide guidance on robustness and broaden applicability across domains.","A valuable extension would be to develop a Bayesian (prior-averaged) or robust counterpart to information-matching that accounts for FIM variability over plausible parameter regions rather than relying on a single local estimate. Another direction is incorporating correlated noise/temporal dependence (especially for sensor networks and time series) and unknown noise variances via self-starting or joint estimation. Practical adoption would be helped by providing principled rules for candidate-set augmentation when infeasible (e.g., gradient-based synthesis of new candidates) and by releasing reusable software components/tutorials for common DOE problems. Finally, broader benchmarking against standard DOE/active-learning baselines (A/D/E/I-optimal designs, mutual information, BALD, etc.) on shared suites would clarify when information-matching yields the largest gains.",2411.02740v5,https://arxiv.org/pdf/2411.02740v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:10:01Z
TRUE,Optimal design|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,Other,Variable/General (n units with d covariates; assignment probabilities can be equal or unequal),Healthcare/medical|Food/agriculture|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/pengzhang91/MWU,"The paper studies experimental design for randomized controlled trials (RCTs) with general (possibly unequal) treatment assignment probabilities and connects it to the Distributional Discrepancy Minimization (DDM) problem. It proves an NP-hardness result: approximating the optimal DDM objective within a constant additive error is NP-hard (even when the optimum is zero), implying strong limits on instance-optimal polynomial-time design. The authors propose a matrix Multiplicative Weights Update (MWU) algorithm for DDM that improves on the Gram–Schmidt walk (GSW) approach when assignment probabilities are unequal, while matching it up to constants when probabilities are equal. Using the Harshaw et al. framework, they define an MWU-based RCT design that reduces worst-case mean-squared error (MSE) for the Horvitz–Thompson estimator of the average treatment effect (ATE) by better balancing observed covariates while preserving robustness to unobserved components/model misspecification. Simulations on synthetic matrices and the LaLonde dataset show MWU achieves lower DDM objective values across assignment probabilities and often lower ATE estimation MSE, especially when outcomes are linear or nearly linear in covariates.",ATE is $\tau=\frac1n\sum_{i=1}^n(a_i-b_i)$ and the Horvitz–Thompson estimator is $\hat\tau=\frac1n\left(\sum_{i:z_i=1}\frac{a_i}{p_i}-\sum_{i:z_i=-1}\frac{b_i}{1-p_i}\right)$. The design goal is to minimize worst-case $\mathrm{MSE}_z(\hat\tau)=\mathbb{E}_z[(\hat\tau-\tau)^2]=\frac{1}{n^2}\mu^\top\mathrm{Cov}(z)\mu$ with $\mu_i=\frac{a_i}{p_i}+\frac{b_i}{1-p_i}$. DDM is formulated as minimizing $f_B(D)=\|\mathrm{Cov}_{z\sim D}(Bz)\|$ over feasible assignment distributions with prescribed marginals $\Pr(z_i=1)=p_i$; with balance/robustness tradeoff via augmented matrix $B=\begin{pmatrix}\sqrt\phi I\\ \sqrt{1-\phi}X^\top\end{pmatrix}$.,"Hardness: there exists a universal constant $c>0$ such that for certain unequal-probability vectors $p$ it is NP-hard to distinguish instances with optimal DDM value $0$ from those with value at least $c\,\alpha^2/(2\beta-1)^2$, implying no polytime algorithm can achieve a constant additive-approximation guarantee unless P=NP. Algorithmic guarantee (Theorem 4.2): MWU finds a feasible distribution $D$ with $f_B(D)\le (1+\epsilon)^2\min\{f_B(D_{\text{Bernoulli}}),\,1+1/\epsilon\}$ in polynomial time, improving over the GSW bound when $f_B(D_{\text{Bernoulli}})\ll 1$ (highly unequal assignment). Empirically, MWU yields the lowest $\|\mathrm{Cov}(Bz)\|$ across assignment probabilities on synthetic matrices and the LaLonde covariate datasets, and reduces ATE-estimation MSE relative to GSW and common designs when outcomes are linear or nearly linear in covariates (e.g., reported average improvements with competitors’ best MSE at least 2.61× MWU(0.5) in the linear setting).","The authors note the design assumes outcomes depend (nearly) linearly on covariates, similar to prior work; they suggest addressing this by adding higher-order covariate terms/interactions or using kernel methods. They also state the MWU algorithm is slower than GSW and other standard designs, though still polynomial-time and practical for small-to-moderate experiment sizes.","The method is primarily evaluated via simulations and limited real-world covariate datasets, so external validity to diverse RCT settings (e.g., strong nonlinearity, heavy-tailed outcomes, interference, noncompliance, clustering) is unclear. Practical adoption may require careful tuning of MWU hyperparameters (e.g., $\epsilon$, iteration counts, covariance estimation via Monte Carlo) and efficient implementations; without standardized software packaging, reproducibility and ease-of-use may be limited. Comparisons focus on a set of common designs; the study may omit other modern covariate-balancing/randomization methods (e.g., more recent constrained randomization or optimization-based approximate balancing variants under unequal allocation), which could affect relative performance conclusions.","They propose refining the theoretical upper bound in their MWU guarantee (Eq. (6)) to better reflect the empirically observed improvement over GSW, potentially proving a bound directly relative to the achieved GSW value rather than its worst-case upper bound. They also suggest mitigating the near-linearity limitation by incorporating higher-order covariate terms/interactions or using kernel methods.","Extend the MWU design framework to clustered/blocked or network-interference RCTs and to multi-arm trials with unequal allocations. Develop self-starting or adaptive/sequential versions that update assignment probabilities or the tradeoff parameter $\phi$ as data accrue while maintaining feasibility constraints. Provide robustness analyses and benchmarks under covariate measurement error, autocorrelation, and heavy-tailed/noisy covariates, and release a well-documented software package (e.g., R/Python) with defaults and diagnostics for practitioners.",2411.02956v2,https://arxiv.org/pdf/2411.02956v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:10:46Z
TRUE,Sequential/adaptive|Computer experiment|Other,Model discrimination|Prediction|Other,Not applicable,"Variable/General (example includes 5 agents; many configurable attributes such as personality traits, goals, network structure, platform features, and simulation duration)",Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.codabench.org/competitions/3888/,"The paper proposes a structured framework for designing reliable experiments using Generative Agent-Based Modeling (GABM) implemented in Google DeepMind’s Concordia platform. It provides a step-by-step methodology covering conceptualization (defining WHO/WHEN/WHERE/HOW), tool selection (LLM choice and API configuration), agent and environment design (agent traits, goals, memory), and execution via a Game Master mechanism. A demonstration case study simulates information spread and misinformation dynamics on a fictional social network with five agents (influencers, regular users, and a misinformation agent) to illustrate how experimental scenarios can be configured and run repeatedly. Reliability guidance emphasizes validation against prior real-world experiments when available, repeated runs to compare aggregate statistics when only aggregate ground truth exists, and reasoned plausibility checks when no ground truth is available. Overall, the contribution is a practical experimental-design guide for computational social science simulations using LLM-driven agents rather than a classical DOE construction paper.",Not applicable (no classical DOE or optimal design equations; the methodology is procedural and simulation-driven).,"The paper primarily presents a methodological guide and an illustrative simulation walkthrough rather than reporting benchmark ARL-style performance metrics or DOE efficiencies. The demonstration describes qualitative/trace outputs of agent interactions (e.g., misinformation engagement, counter-posts, evolving reactions) and suggests running simulations multiple times to compute and compare statistical measures when only aggregate information is available. No specific numerical effect sizes, confidence intervals, or optimized design parameters are reported in the provided text.",None stated.,"The framework does not specify formal DOE structures (factors/levels/randomization/blocking/replication plans) or statistical power considerations for simulation experiments, which can make results sensitive to arbitrary scenario choices. Reliability guidance is largely qualitative; without explicit calibration targets, uncertainty quantification, or sensitivity/robustness analyses, conclusions may depend heavily on LLM prompt/model choice and stochasticity. The demonstration uses a small toy setting (five agents) and may not generalize to larger networks or different social mechanisms without systematic experimental plans and reporting standards.","Future work includes creating an interactive web platform to make GABMs more accessible (character creation/customization, behavior adjustment, visualization of agent details). The authors also suggest enabling simulations that compare interactions among agents with similar or contrasting personalities or political ideologies to broaden applicability across fields.","Develop a formal experimental design layer for GABM studies (explicit factor/level definitions, replication plans, random seeds, and pre-registered analysis) and standard reporting for reproducibility. Add systematic sensitivity analyses over LLM choice, prompting, memory settings, and network structure, with uncertainty quantification for key outcomes. Provide open-source reference implementations (or a Concordia plugin) and benchmark suites so different researchers can compare results across standardized scenarios and datasets.",2411.07038v1,https://arxiv.org/pdf/2411.07038v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:11:11Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Other,D-optimal|A-optimal|Bayesian D-optimal|Bayesian A-optimal,Variable/General (examples include 1–4 factors; both profile/functional and scalar factors),Pharmaceutical|Theoretical/simulation only|Other,Other,TRUE,R|C/C++,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://cran.r-project.org/web/packages/fdesigns/index.html|https://github.com/damianosmichaelides/fdesigns,"The paper introduces the R package fdesigns for constructing Bayesian optimal designs for experiments where some factors are functions over time (profile/dynamic factors), within functional linear models and functional generalized linear models. The approach reduces the infinite-dimensional design problem by representing both profile factors and time-varying coefficients using basis expansions (notably B-splines), yielding a finite-dimensional model matrix and enabling standard optimal-design computations. For functional linear models, the package supports Bayesian A- and D-optimal designs via conjugate normal-inverse-gamma structure and an optional roughness-penalty prior (through a smoothing parameter \(\lambda\)). For functional GLMs (binomial-logit and Poisson-log), it implements pseudo-Bayesian A- and D-optimality using the Fisher information and approximates prior expectations via quadrature (Gauss-Hermite/Gauss-Legendre) or Monte Carlo. Designs are computed using (parallel) coordinate-exchange optimization, with examples including simple functional regression settings and a biopharmaceutical bioreactor dynamic experiment with one profile factor and three scalar factors.","Design optimality is defined through expected utility \(\Psi(\xi)=\mathbb{E}_{\theta,y}[u(\theta,y,\xi)]\). The functional linear model is \(y_i=\int_0^T f^T(x_i(t))\,\beta(t)\,dt+\varepsilon_i\), with basis expansions \(\beta_q(t)=b_q^T(t)\,\theta_q\) and \(x_{ij}(t)=\sum_l \gamma_{ijl}c_{jl}(t)\), yielding a finite linear model \(y=Z\theta+\varepsilon\). Bayesian A- and D-optimality are implemented as \(\Psi_{A}(\Gamma)=-\mathrm{tr}(Z^TZ+V^{-1})^{-1}\) and \(\Psi_{D}(\Gamma)=\log|Z^TZ+V^{-1}|\), with roughness-penalty prior precision \(V^{-1}=\lambda R_0\). For functional GLMs, pseudo-Bayesian criteria use \(I(\theta,\Gamma)=Z^T W Z + \lambda R_0\) with objectives \(-\mathrm{tr}(I^{-1})\) and \(\log|I|\), then averaged over the prior via quadrature or Monte Carlo.","The paper demonstrates that fdesigns can generate Bayesian/pseudo-Bayesian A- and D-optimal designs for experiments with profile factors using coordinate exchange and (for GLMs) quadrature or Monte Carlo integration. Example outputs reported include: a 4-run Bayesian D-optimal design for a single-profile-factor functional linear model (objective value 0.4051947; 4 iterations), a 12-run Bayesian A-optimal design for a two-profile-factor model with interaction (objective 13.33739; 12 iterations), a 12-run pseudo-Bayesian D-optimal design for a functional logistic model using quadrature (objective 847.976; 31 iterations), and a 12-run pseudo-Bayesian A-optimal design for a functional Poisson model using Monte Carlo (objective 2.212573; 4 iterations). A biopharmaceutical (Ambr250) example produces a 12-run A-optimal design with four distinct step-function profiles for the dynamic feed-volume factor and boundary/orthogonal settings for scalar factors.","The paper notes that identifying optimal designs for complex models with multiple profile factors can substantially increase computational demand, so the worked examples are intentionally kept relatively simple for fast demonstration. For quadrature-based pseudo-Bayesian designs, it states that high-dimensional integrals require many quadrature points to achieve accurate approximation, increasing computational cost; it suggests using Monte Carlo for complicated models.","The implemented functional-GLM support is limited to binomial-logit and Poisson-log families, so broader GLM families/links are not covered. The methodology assumes a particular basis-expansion specification (and tuning choices such as spline degree/knots or smoothing \(\lambda\)), and design optimality can be sensitive to these modeling choices and to the assumed priors; guidance for robustness to misspecification is limited. As with many coordinate-exchange approaches, solutions may be locally optimal and can depend on starting designs, especially in higher-dimensional functional settings, and the paper does not provide a broad benchmarking study across alternative algorithms or large-scale scenarios.","The authors state plans to extend fdesigns to accommodate more types of models and additional design criteria, including criteria tailored specifically to profile factors, to broaden the package’s applicability and impact in optimal experimental design.","Useful extensions would include supporting additional response families/links (e.g., Gamma, negative binomial) and relaxing assumptions via correlated errors or within-run autocorrelation for dynamic processes. Adding robustness features—such as design criteria that hedge against basis/\(\lambda\)/prior misspecification or model uncertainty—would improve practical reliability. Providing a self-contained benchmarking suite and scalable approximations (e.g., variational or Laplace-based expected-utility approximations, quasi-MC) plus richer diagnostics and vignettes would help practitioners apply the methods to larger, more complex functional experiments.",2411.09225v2,https://arxiv.org/pdf/2411.09225v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:11:58Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Model discrimination|Cost reduction|Other,Other,Variable/General (design variables are seismometer/array locations; example uses 3 nodal stations + 1 array; also shows up to 10 receivers),Environmental monitoring|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/dominik-strutz/WoWED-volcano,"The paper presents a near-real-time Bayesian experimental design workflow and an open-source Python package to optimize seismic monitoring network layouts for volcanoes by selecting station/array locations that maximize expected information gain (EIG) about earthquake source locations. It is novel in optimizing networks that simultaneously account for travel-time, amplitude, and array-derived (back-azimuth/incident-angle or slowness) measurements, enabling hybrid designs across common volcano-monitoring data types. The design objective is EIG (Shannon-information reduction / KL-divergence form) computed efficiently using a DN (Gaussian-evidence) approximation during optimization and validated/interpreted using nested Monte Carlo (NMC). A genetic algorithm is used to search constrained design spaces derived from digital elevation models and practical deployment constraints, with automated access to global topography/volcano databases for rapid setup. Example results (Etna) show that optimized designs reduce expected posterior uncertainty versus random and quasi-random (Sobol) placements and quantify diminishing returns as receiver count increases, and a heterogeneous-velocity extension is demonstrated using ray-based travel-time fields from an eikonal solver.","Bayesian experimental design objective is to choose sensor locations $\xi$ maximizing expected information gain: $\xi^*=\arg\max_{\xi\in\Xi} \mathrm{EIG}(\xi)$. EIG is defined via Shannon information as $\mathrm{EIG}=\mathbb{E}_{p(d\mid\xi)}[I(p(m\mid d,\xi))-I(p(m))]$ and rearranged to depend on likelihood/evidence: $\mathrm{EIG}(\xi)=\mathbb{E}_{p(m)}[I(p(d\mid m,\xi))]-I(p(d\mid\xi))$. It is estimated either by nested Monte Carlo (NMC) (Eq. 5) or by the DN approximation (Eq. 6) that treats $p(d\mid\xi)$ as Gaussian using the sample covariance $C_d$ (Eqs. 6–7); travel-time forward model in a homogeneous medium is $t=t_{src}+\frac{1}{v}\sqrt{(x_{rec}-x_{src})^2+(y_{rec}-y_{src})^2+(z_{rec}-z_{src})^2}$ (Eq. 11), with travel-time uncertainty $\sigma_t^2=\sigma_p^2+t\,\sigma_v^2$ (Eq. 12), and amplitude model $A=\frac{1}{r}\exp(-C t)$ with $C=\pi f/Q$ (Eq. 13).","In the Etna example optimizing a network of three nodal stations plus one array (travel-time+amplitude for nodes; travel-time+amplitude+back-azimuth for array) yields an optimal design with EIG reported as 5.7 nats and an approximated expected posterior standard deviation of about 545 m. Optimization with a genetic algorithm (population size 64) converges in roughly ~200 generations, with only slight improvements up to ~600 generations for that scenario. Comparisons versus 1000 random designs and 1000 Sobol (space-filling) designs show optimal designs consistently achieve lower expected standard deviation, and the Sobol designs are intermediate (better than uniform random, sometimes approaching optimal). For heterogeneous velocity-model experiments (3 receivers + 1 array), designs optimized under homogeneous/layered/heterogeneous $v_p$ models evaluated under the heterogeneous model give EIGs of 5.08, 5.10, and 5.26 nats respectively, corresponding to approximate standard deviations of 667 m, 662 m, and 628 m.","The authors note several tractability-driven assumptions: (i) often assuming a homogeneous velocity model unless a more expensive ray/eikonal approach is used; (ii) assuming a Gaussian data likelihood, with results sensitive to variance choices and potential non-Gaussianity; and (iii) assuming independent datum uncertainties, despite potential correlations (e.g., shared ray paths across data types). They also state that optimizing networks to constrain the full source solution including the moment tensor is not addressed and may be computationally challenging.","The optimization is demonstrated primarily through synthetic/forward-simulated evaluations and an illustrative case (Etna), so real-time field validation on real eruptions/unrest datasets and operational constraints (telemetry, power, permitting, land access, vandalism risk, seasonal noise, instrument failures) are not systematically benchmarked. The DN approximation is used heavily for speed but can fail under strongly non-Gaussian evidence (e.g., multimodal priors/low noise); while mentioned, the paper does not provide a general diagnostic or safeguard beyond rerunning with NMC. The design variables are continuous/geospatial but constrained to DEM grids; discretization and resolution effects (and sensitivity to DEM/topography errors) are not quantified. Array modeling is simplified (treating an array as a single point measurement with specified azimuth/slowness uncertainties), and practical array geometry choices (aperture, number of elements) are not explicitly optimized.","They suggest that most assumptions can be relaxed when more information is available (e.g., moving from homogeneous to heterogeneous velocity models with more complex forward functions) and note flexibility to incorporate more complex constraints and data types. They explicitly mention a major extension: designing networks optimized to constrain the full source solution including the moment tensor, but state it is not currently clear how to do so in a computationally efficient way.","Develop self-starting/online (streaming) updates where priors and optimal layouts adapt as unrest evolves and new events are located, including decision rules for redeployment under logistics constraints. Extend to correlated-noise and site-dependent uncertainties (spatially varying picking error, attenuation/quality-factor uncertainty, coherent noise fields) and evaluate robustness under model misspecification. Provide standardized benchmarking across multiple volcanoes and real datasets with operational metrics (deployment time, cost, telemetry constraints) and release reproducible experiment configurations. Add multi-objective designs (e.g., joint optimization for detection, location, and magnitude/energy estimation) and incorporate explicit costs/penalties for access, maintenance, and risk.",2411.11015v1,https://arxiv.org/pdf/2411.11015v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:12:46Z
TRUE,Bayesian design|Optimal design|Sequential/adaptive|Other,Parameter estimation|Screening|Model discrimination|Prediction|Other,Other,Variable/General (experiments modeled as menus/partitions over alternatives; no fixed factor count),Finance/economics|Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops a normative/axiomatic theory of experimental design for revealed-preference choice experiments aimed at identifying a subject’s utility function under partial identification. It proposes three normative principles—Structural Invariance (only what can be identified matters), Identification Separability (value of what is identified is independent of counterfactual identifications), and Information Monotonicity (more informative experiments are weakly preferred)—plus an expected-utility-over-randomization axiom. The main representation theorem shows these principles are equivalent to maximizing an Expected Identification Value functional: experiments are evaluated by the prior-weighted expected value of the identifiable set of utility types induced by each observable outcome. The framework unifies several Bayesian design objectives as special cases of the identification-value index (e.g., Shannon-entropy reduction, hypothesis-testing style criteria) and discusses implications for discrete choice experiments, partial observability, and adaptive/dynamic procedures (represented as lotteries over experiments or coarser partitions).","The core representation is an expected identification value functional: $F(\pi)=\sum_{(A,P)\in\mathrm{supp}(\pi)}\Big(\sum_{C\in P}\tau(W_{A,C})\,\mu(W_{A,C})\Big)\,\pi(A,P)$, where $W_{A,B}=\{u\in U: B\cap\arg\max_{x\in A}u(x)\neq\emptyset\}$ is the set of utility types consistent with observing an outcome cell $B$. The index $\tau$ must satisfy an information-is-good constraint (subadditivity/monotonicity in expectation), e.g. for $W\subseteq V$: $\tau(W)\mu(W\mid V)+\tau(V\setminus W)(1-\mu(W\mid V))\ge \tau(V)$, and normalization $\tau(U)=0$. A key special case is Shannon-entropy design with $\tau(W)=-\log\mu(W)$, yielding $F(\pi)=-\sum_{(A,P)}\sum_{C\in P}\mu(W_{A,C})\log\mu(W_{A,C})\,\pi(A,P)$.","Theorem 1: the axioms (expected utility over randomization + Information Monotonicity + Structural Invariance + Identification Separability) hold iff preferences over randomized experiments admit the Expected Identification Value representation. In the expected-utility (linear utility) environment, Structural Invariance is equivalently captured by translation invariance (via Minkowski mixing of menus) plus belief consistency (ignoring $\mu$-null outcomes) (Theorem 3), and richness of feasible experiments follows under a regular prior (Theorem 2). Strengthening the axioms yields a characterization of Shannon-entropy (expected information gain) as the unique functional form under additional symmetry/additivity requirements (Theorem 4). The framework also shows how adaptive/dynamic procedures and partial observability can be represented as (randomized) experiments with appropriate menus and partitions, linking dynamic elicitation to the same axiomatic evaluation criterion.","The authors note they abstract away from the physical/implementation details of experiments and instead model experiments as menus plus partitions capturing observability, emphasizing conceptual/normative guidance rather than operational design recipes. They also note technicalities when extending to settings without prior beliefs and that some equivalence results for general multi-round adaptive procedures are left informal due to heavy notation. In discussing Shannon entropy, they state that the symmetry requirement implicitly imposes a specific risk attitude for the analyst, which they view as beyond what can be justified purely on normative grounds.","The theory evaluates experiments through identifiable sets under a maintained model class $U$ and prior $\mu$; misspecification of the utility domain (e.g., violations of expected utility, context effects, bounded rationality) could make the normative ranking fragile. The framework is largely axiomatic and does not provide computational tools for constructing optimal menus/partitions in realistic high-dimensional choice settings (nor complexity results), which may limit direct practical deployment. It assumes away ties via $\mu$-probability-zero indifference (regularity), an assumption that can be problematic with discrete/rounded data or deterministic subjects, potentially affecting implementability of partitions/outcomes. Empirical validation is not provided (no simulations or applied case study), so it remains unclear how the axioms map to performance under noise, limited sample sizes, or incentive-compatibility constraints typical in experimental economics.","They discuss extending or applying the framework to settings without prior beliefs (belief-free models) and to dynamic/adaptive experiments and environments with observability constraints (e.g., dynamic games, off-path unobservability), indicating the framework can capture such procedures via appropriately constructed (randomized) experiments. They also suggest applying the expected-utility specialization to non-linear models by identifying the appropriate invariance axiom for the alternative preference model.","Develop algorithmic methods to compute approximately optimal menus/partitions under specific parametric or nonparametric utility classes (including complexity/approximation guarantees) and to handle large choice sets. Extend the axioms/representation to accommodate stochastic choice (random utility with observation noise) and to robust/self-starting design under unknown or misspecified priors (e.g., minimax or Bayesian-robust criteria). Provide empirical case studies (lab/field DCEs, online platform experiments) comparing entropy-based, hypothesis-testing, and other $\tau$ specifications under real constraints (budget, incentive compatibility, respondent fatigue). Generalize to multi-agent settings (heterogeneous populations) and multivariate outcomes where the designer learns both preference distributions and individual types, connecting to modern adaptive survey/active learning methods.",2411.11625v1,https://arxiv.org/pdf/2411.11625v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:13:23Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,A-optimal|D-optimal,"Variable/General (design variable θ is continuous; examples include θ=(c,τ) with c∈{x,y,z}, τ∈[0,3], and θ=(θ1,θ2)∈[0,1]^2; unknown parameter dimension d=3 for Lorenz and d=100 for Schrödinger discretization)",Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops algorithms for continuous, nonlinear optimal experimental design (OED) where the design space is continuously indexed and the design is a probability measure ρ over the design domain Ω. It formulates nonlinear OED as a bilevel optimization: an inner inverse problem estimates unknown parameters σ* given ρ, and an outer problem optimizes ρ using classical A- and D-optimality criteria defined via a Fisher-information/Gauss–Newton matrix I[ρ;σ]. To handle optimization over the infinite-dimensional space of probability measures, the authors use Wasserstein-2 gradient flow and implement it with a Monte Carlo particle method, yielding particle dynamics for candidate measurement locations/times. Two methods are proposed: a brute-force bilevel scheme that (approximately) solves the inner inverse problem at each outer iteration, and a streamlined scheme that updates σ* in one step using an implicit-function-theorem-derived evolution equation coupled to the particle flow. Numerical experiments on the Lorenz-63 system (optimal measurement times for x,y,z) and a steady-state Schrödinger inverse problem (optimal source–detector spatial pairs) show that the learned designs concentrate on informative regions and improve reconstruction compared with uniform sampling baselines.","The inverse problem is σ* := argmin_{σ∈R^d} Loss[σ;ρ] with Loss[σ;ρ]=∫_Ω (M(θ;σ)−data(θ))^2 ρ(θ)dθ, and the OED is ρ_opt := argmin_{ρ∈P_{r2}(Ω)} F[ρ;σ*[ρ]] (or argmax for D-optimal). The nonlinear Fisher information is I[ρ;σ]=∫_Ω C(θ;σ)ρ(θ)dθ with C(θ;σ)=∇_σ M(θ;σ)∇_σ M(θ;σ)^T, and the criteria are F_A=Tr(I^{-1}) and F_D=log det(I). The outer optimization uses Wasserstein gradient flow ∂_t ρ = ∇_θ·(ρ ∇_θ(δF/δρ)), implemented via particles ρ≈(1/N)∑_i δ_{θ_i} with dynamics \dot{θ}_i = -∇_θ(δF/δρ)(θ_i), with explicit velocity fields for A- and D-optimality (involving ∇_θ∇_σ M^T, I^{-2} or I^{-1}, and ∇_σ M).","On Lorenz-63, the learned A- and D-optimal design measures over observation time concentrate into a small set of high-density “spike” time windows for each observed coordinate (x,y,z), and these windows align closely with benchmark designs computed using the true parameters; the authors report recovering 9/14 (D-optimal) and 7/10 (A-optimal) benchmark-important time windows under a 5% density threshold criterion. Using the optimized design for “strategic sampling” yields faster convergence and better final values than uniform sampling when solving the inverse problem for σ, as shown by lower loss and parameter error curves over iterations. On the Schrödinger inverse problem with θ=(θ1,θ2)∈[0,1]^2, both A- and D-optimal designs evolve toward concentration near the diagonal θ1≈θ2 (source and detector close), and reconstructions of a 1D potential σ(x) (d=100 discretization) improve markedly from the initial to final iterations. Warm-start initializations reduce iterations needed and further sharpen concentration patterns (e.g., tighter diagonal concentration for A-optimal in the Schrödinger case).","The authors state that convergence theory for the adaptive gradient-flow method (Algorithm 2) is not well understood, including how convergence depends on particle count N and iterations T and how the coupling between ρ and σ*[ρ] affects guarantees. They also note efficiency limitations: hyperparameters (e.g., T and step sizes) are tuned manually rather than chosen adaptively, and the method does not reuse previously collected measurement data(θ) after particles move, which is undesirable when measurements are costly.","The method relies on accurate and often expensive derivatives (∇_σ M and mixed derivatives ∇_θ∇_σ M, and sometimes Hess_σ M), which may be difficult to obtain or unstable for black-box simulators; the Schrödinger experiments explicitly drop second-order terms, suggesting potential sensitivity to such approximations. The approach assumes a known noise model and uses local-information criteria (via Fisher/Gauss–Newton at σ*), which may be less reliable for strongly nonlinear, multimodal inverse problems where local curvature is not representative globally. Practical constraints common in DOE (e.g., discrete measurement schedules, costs differing across θ, batching constraints, or hard-to-change factors) are not incorporated, so direct experimental implementability may be limited. The paper does not indicate publicly available reference implementations, which may hinder reproducibility for complex PDE/ODE setups.","They propose developing convergence theory for the streamlined adaptive gradient-flow method, including dependence on N and T and analyzing the coupled evolution of ρ and σ* (e.g., whether evolving ρ leads σ*[ρ] toward σ_true). They also suggest improving efficiency by choosing hyperparameters automatically/adaptively and by devising ways to recycle or better reuse costly measurements data(θ) rather than discarding them after particle updates.","A natural extension is to incorporate practical experimental constraints and costs directly into F or the feasible set (e.g., penalizing frequent changes, enforcing discrete-time grids, or adding budget constraints) to make designs deployable. Robust/self-starting variants that explicitly handle unknown noise levels, model mismatch, and non-Gaussian errors (or heavy-tailed noise) would improve real-world reliability. Extending the framework to multivariate/multi-output measurements with correlated noise and to designs targeting model discrimination (not only local parameter covariance) would broaden applicability. Providing an open-source implementation (e.g., in Python/JAX or Julia) with automatic differentiation and benchmark scripts for Lorenz/PDE cases would materially improve reproducibility and adoption.",2411.14332v2,https://arxiv.org/pdf/2411.14332v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:14:08Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Other,Parameter estimation|Optimization|Prediction|Other,Other,Variable/General (examples mostly 2 parameters; method for general p-dimensional θ),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Personal website|Public repository (GitHub/GitLab),https://puq.readthedocs.io/en/latest/|https://github.com/parallelUQ/PUQ,"The paper studies sequential experimental design (active learning) for Bayesian calibration of expensive simulation models when evaluations can be executed in parallel (batch) computing environments. It proposes a new performance model (a Monte Carlo benchmarking framework) to compare sequential acquisition strategies under different batch sizes, worker counts, acquisition times, and heterogeneous simulation runtimes, using metrics such as wall-clock time, speedup, idle time, and computing-hours consumption. Methodologically, it introduces calibration-focused acquisition functions adapted from Bayesian optimization—probability of improvement (PI) and expected improvement (EI) defined on absolute loss to observed data—and combines them with a variance-reduction criterion (EIVAR) via an alternating HYBRID strategy to balance exploitation and exploration of high-posterior regions. The framework supports synchronous and asynchronous parallel updates and uses a constant-liar strategy to build batches. Experiments on synthetic test functions show HYBRID generally improves calibration error and posterior approximation compared with EI, EIVAR, and random sampling, while the performance model reveals when larger or smaller batch sizes are preferable depending on the acquisition/simulation time ratio and runtime variability; implementations are provided in the PUQ Python package.","The sequential parallel DOE is formalized in Algorithm 1, where at each stage t a batch of b new parameters is selected by minimizing an acquisition function A_k(θ) over a candidate set, while pending simulations run on w workers; batching is handled via a constant-liar emulator update. Calibration is based on y = η(θ) + ε with ε ~ N(0,σ^2), and the emulator is a Gaussian process with predictive mean/variance m_t(θ), s_t^2(θ) (Eq. 3) using a Matérn-3/2 correlation (Eq. 4). The paper derives PI (Eq. 5) and EI/unimprovement (Eq. 7) for absolute loss δ_t = min_j |y-η(θ_j)|, and uses EIVAR (Eqs. 8–9) to select θ that minimizes integrated posterior variance; the proposed performance model simulates stage/job completion times (Algorithm 2) given acquisition-time and simulation-time inputs.","On six synthetic calibration problems (e.g., Himmelblau, Hölder, Easom, Sphere, Matyas, Ackley) with n0=10 initial points, candidate lists of size 1,000 per stage, and 30 replications, HYBRID consistently attains lower calibration error δ_t and often better posterior estimation (MAD) than EI, EIVAR, and random sampling, especially for multimodal or small-region-of-interest posteriors where EI can get trapped in local optima. Parallel experiments with w=125 and b∈{1,5,25,125} show that larger batches can slow progress in terms of evaluations needed (constant-liar degradation) while smaller batches increase GP refitting/acquisition overhead, motivating tradeoffs. Using the performance model, an example reports reaching error level α=0.1 requiring n_k(1,0.1)=447, n_k(4,0.1)=767, and n_k(128,0.1)=895 evaluations (illustrating batch-size impact on convergence). Sensitivity studies show optimal batch size shifts toward larger b when acquisition dominates and toward smaller b when simulation time and/or runtime variability dominates, and scalability diagnostics quantify speedup, idle time, and computing-hours costs across (b,w) grids.","The paper assumes scalar simulation outputs in its main development and notes that extending the acquisition function to high-dimensional outputs is left for future work. It also assumes (for simplicity) known observation noise variance σ^2 (suggesting plug-in estimation if unknown). In the performance analysis, it assumes uniform variability in simulation time across the parameter space, acknowledging that in practice runtime may depend on θ (e.g., shorter/less variable in high-posterior regions).","The empirical validation is largely based on synthetic benchmark functions; real-world calibration case studies would be needed to confirm practical gains under realistic simulator behavior, constraints, and model discrepancy. The batching approach relies on constant-liar updates, which can be sensitive to the choice of the liar constant and may bias batch selection for strongly nonstationary or heteroskedastic simulators; alternative batch methods are not deeply compared. The GP emulator uses a specific kernel choice (Matérn 3/2) and a standard GP formulation that may not scale well to very large n without sparse/approximate GP methods, which could materially affect acquisition time in practice. The calibration setup uses a simple additive Gaussian error model without explicit model discrepancy terms, which can be important in calibration settings and may change acquisition behavior.","The authors state that extending the acquisition function to handle high-dimensional simulation outputs is future work. They suggest using the performance model to study alternative regimes such as dynamically choosing batch size at each stage (rather than fixed b), and extending the model to support preemptive job allocation (instead of non-preemptive). They also propose improving HYBRID by automatically balancing exploration/exploitation (rather than alternating EI and EIVAR deterministically) and studying performance when simulation time depends on parameter location (θ-dependent runtimes).","Develop self-starting or robust variants of the calibration acquisitions that better handle unknown σ^2, model discrepancy, and non-Gaussian observation noise, and analyze their impact on the performance model’s conclusions. Extend benchmarking to include additional parallel/batch BO methods (e.g., Thompson sampling, q-EI, kriging believer variants) and evaluate fairness under matched computational budgets (including emulator fitting time). Provide comprehensive real-data/HPC case studies with logged runtimes and queueing/communication overhead to validate the Monte Carlo performance model assumptions. Add scalable GP/surrogate options (sparse GP, local GPs, neural surrogates) within PUQ and quantify when surrogate scaling, rather than simulation time, becomes the bottleneck in large sequential campaigns.",2412.00654v1,https://arxiv.org/pdf/2412.00654v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:15:15Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Optimization|Other,Not applicable,"Variable/General (network units N, arms K, clusters C; exposure states |U_E| up to |d_s|^C)",Theoretical/simulation only|Other,Exact distribution theory|Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online experimental design framework for settings with network interference, formulated as a multi-armed bandit over an exposure-mapped action space (MAB-N). It uses exposure mapping to reduce the exponentially large joint assignment space over networked units into a smaller exposure-super-arm space, and studies a bi-objective trade-off between cumulative regret (welfare) and estimation accuracy of causal effects (ATEs) between exposure conditions. The authors derive a minimax lower bound showing that any policy/estimator pair must satisfy a Pareto trade-off of order \(\sqrt{R(T,\pi)\,e(T,\hat\Delta)}=\Omega(\sqrt{|\mathcal U_E|})\), and characterize a sufficient condition for Pareto-optimality. They introduce a two-stage algorithm (UCB-TSN) that first uniformly explores exposure conditions to enable accurate ATE estimation and then applies UCB to control regret, proving matching (up to logs) Pareto-optimal performance; an EXP3-based variant handles adversarial rewards. Simulations on a synthetic clustered network illustrate that UCB-TSN achieves low regret comparable to standard UCB while substantially improving ATE estimation stability versus purely exploitative baselines.","Exposure mapping reduces assignments \(A_t\in \mathcal K^\mathcal U\) to exposure states \(S_t=(S(i,A_t,H))_{i\in\mathcal U}\) with \(|\mathcal U_s|=d_s\), and defines exposure outcomes/rewards by marginalizing over assignments consistent with \(S_t\): \([\tilde Y_i(S_t),\tilde r_{i,t}(S_t)]^\top=\sum_{A\in\mathcal K^\mathcal U}[Y_i(A),r_{i,t}(A)]^\top\,\mathbb P(A_t=A\mid S_t)\). The exposure-based regret is \(R_\nu(T,\pi)=T\frac{1}{N}\sum_i \tilde Y_i(S^*)-\frac{1}{N}\mathbb E_\pi[\sum_{t=1}^T\sum_i \tilde r_{i,t}(S_t)]\) where \(S^*=\arg\max_{S\in\mathcal U_E}\sum_i\tilde Y_i(S)\). The key trade-off is the minimax lower bound \(\inf_{\hat\Delta}\max_{\nu}\sqrt{R_\nu(T,\pi)\,e_\nu(T,\hat\Delta)}=\Omega(\sqrt{|\mathcal U_E|})\), and UCB-TSN achieves (up to logs) \(e(T)=\tilde O(\sqrt{|\mathcal U_E|/T_1})\) and \(R(T)=\tilde O(\sqrt{|\mathcal U_E|T}+T_1)\) with a two-stage explore-then-UCB scheme.","A general minimax lower bound establishes that for any online policy \(\pi\), the best achievable joint performance satisfies \(\inf_{\hat\Delta}\max_{\nu}\sqrt{R_\nu(T,\pi)e_\nu(T,\hat\Delta)}=\Omega(\sqrt{|\mathcal U_E|})\), implying estimation error cannot decay faster than \(\Omega(\sqrt{|\mathcal U_E|/T})\) in worst case when regret is linear. The paper gives a Pareto-frontier characterization: Pareto-optimal pairs satisfy \(\max_\nu \sqrt{R_\nu e_\nu}=\tilde O(\sqrt{|\mathcal U_E|})\). The proposed UCB-TSN algorithm achieves \(\mathbb E|\hat\Delta_T^{(i,j)}-\Delta^{(i,j)}|=\tilde O(\sqrt{|\mathcal U_E|/T_1})\) after a uniform exploration phase of length \(T_1\ge |\mathcal U_E|\), and regret \(R(T)=\tilde O(\sqrt{|\mathcal U_E|T}+T_1)\); choosing \(T_1\ge \sqrt{|\mathcal U_E|T}\) yields Pareto-optimality. Experiments on a 101-node clustered star-like network show UCB-TSN attains regret close to a standard UCB-style baseline while producing markedly smaller and less variable maximum ATE estimation error than the baseline; a uniform-exploration baseline has the worst regret but similarly low estimation error.",None stated.,"The work treats the exposure mapping and clustering as pre-specified; performance and causal interpretability can depend strongly on how well the mapping captures true interference, and misspecification can bias estimated ATEs. The main theory is minimax/worst-case and focuses on bounds in terms of \(|\mathcal U_E|\) and \(T\), with limited guidance on practical selection of \(T_1\), exposure mappings, or clusters under budget and network uncertainty. Implementation details for large networks may be computationally heavy because sampling requires enumerating or generating assignments consistent with a desired exposure super-arm, which can be hard without additional structure.","They propose extending the framework to reinforcement learning in networked environments where interference affects short- and long-term rewards, and to broader bandit variants including fully adversarial bandits, design-based experiments, and continual/anytime-valid inference. They also suggest exploring neural bandits and graph neural network-based models. Another stated direction is to handle generalized network structures—dynamic, unknown, and heterogeneous interference—potentially via adaptive clustering techniques.","Develop practical, data-driven procedures to choose or learn exposure mappings/clusters (e.g., via model selection, graph embeddings, or sensitivity analysis) with guarantees under mapping misspecification. Provide scalable sampling/optimization methods to realize \(\mathbb P(A_t=A\mid S_t)\) and to generate assignments consistent with exposure constraints in large networks. Extend theory to settings with autocorrelated outcomes, non-sub-Gaussian noise, unknown/partially observed networks, and to offline+online hybrid (Phase I/II) designs with valid inference under adaptive stopping.",2412.03727v3,https://arxiv.org/pdf/2412.03727v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:16:06Z
TRUE,Screening|Optimal design|Sequential/adaptive|Other,Optimization|Prediction|Cost reduction|Other,Space-filling|Minimax/Maximin|Other,2 factors (Sb and Te nonstoichiometry in AgSb1+xTe2+y; x and y varied over bounded ranges),Energy/utilities|Other,Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab)|Personal website,https://github.com/JanPohls/MODEM|https://carrollpohlslab.ext.unb.ca/software/,"The paper develops a Design of Experiments (DoE) + machine learning workflow to optimize thermoelectric performance in the AgSb1+xTe2+y system with very few physical syntheses. A Latin square DoE is used to propose compositions within specified bounds for Sb and Te off-stoichiometry, with a Monte Carlo routine that maximizes inter-point distances to improve coverage of the search space. Measured average figure of merit (zT averaged over 300–623 K) from synthesized pellets is fed to support vector machine models (regression to model performance trends and classification to define an “optimal region”), and the predicted optimal region is used as a constraint for the next DoE cycle (sequential refinement). In three cycles totaling eight samples, they identify AgSb1.021Te2.04 with zT_peak = 1.61±0.24 at ~600 K and average zT = 1.18±0.18 (300–623 K), reported as >30% higher than prior best literature values for this material family. The workflow is implemented as an open-source GUI tool (MODEM) intended to generalize to multivariable materials/process optimization problems.","Thermoelectric optimization targets the figure of merit $zT = \frac{S^2 T}{\rho\,\kappa}$ and uses the average $\bar{zT}$ over 300–623 K as the response for the DoE/ML loop. The DoE uses a Latin square construction (each level used once per factor across the design) and a Monte Carlo search that maximizes the spread/coverage of experiments by optimizing distances between design points (space-filling via distance-based criterion). Machine learning uses support vector regression to model $\bar{zT}$ as a function of composition and support vector classification to delineate a high-performance region that becomes a constraint for subsequent DoE cycles.","Across three DoE/ML cycles (8 total experiments), the best composition identified is AgSb1.021Te2.04 with $zT_{peak}=1.61\pm0.24$ at ~600 K and average $\bar{zT}=1.18\pm0.18$ (300–623 K), stated to be >30% above best prior literature for AgSb1+xTe2+y. Earlier-cycle best included AgSb1.0175Te2.025 with $zT_{peak}=1.36\pm0.20$ and $\bar{zT}=0.73\pm0.11$, while second-cycle AgSb1.02Te2.055 reached $zT_{peak}=1.54\pm0.23$ with similar $\bar{zT}=0.73\pm0.11$. The optimized sample also shows very low thermal conductivity (~0.4 W m−1 K−1) and absence of detectable secondary phases compared with other compositions that often showed Ag2Te or other impurities.",None stated.,"The DoE is limited to two composition variables (Sb and Te off-stoichiometry) and the Latin-square-with-distance maximization does not guarantee global optimality relative to formal optimal-design criteria (e.g., D-optimality) or Bayesian optimization; results may depend on chosen bounds/step sizes and the Monte Carlo search heuristic. The dataset is extremely small (8 samples) while SVM regression/classification is used to define an “optimal region,” so model stability/uncertainty and sensitivity to noise/outliers could be substantial (they note zT uncertainty ~15%). Practical transfer to other labs may be affected by uncontrolled synthesis/process factors (cooling rate, sintering conditions, impurity control) that are not explicitly modeled in the DoE factors.","The authors state the DoE/ML strategy can be adapted to multi-variable problems (e.g., adding a third dimension to optimize doped samples) and extended to include four or more elements and multiple synthesis/process parameters. They also note that further slight composition adjustments within the reduced optimal region could increase average zT further by continuing the iterative cycle.","Add explicit process parameters (e.g., cooling rate, sintering temperature/pressure/time) as factors and use multi-response optimization (simultaneously targeting PF, κ, density/phase purity) with constraints. Incorporate uncertainty-aware or Bayesian optimization/surrogate modeling with acquisition functions to better balance exploration vs exploitation under very small experimental budgets. Provide ablation studies comparing Latin square + distance heuristic against alternative DOE strategies (e.g., D-optimal or maximin Latin hypercube) and release reproducible scripts/workflows (including random seeds and design-generation settings) for full reproducibility.",2412.04699v1,https://arxiv.org/pdf/2412.04699v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:16:42Z
TRUE,Optimal design,Parameter estimation,A-optimal|D-optimal,Variable/General (examples with 1 and 2 functional predictors),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|R,Upon request,https://shorturl.at/kgNzP|https://doi.org/10.1093/jrsssc/qlac003|https://academic.oup.com/jrsssc/article-pdf/72/2/231/50296623/qlac003.pdf|http://www.worldcat.org/isbn/9780387400808|https://CRAN.R-project.org/package=fda,"The paper develops optimal designs of experiments for function-on-function linear regression models where both responses and experimental factors are time-varying functions (“dynamic factors”). It derives an integrated least-squares estimator for the functional coefficient surface(s) and provides the estimator’s variance in a Kronecker-product form, enabling design criteria to be expressed via the information matrix built from basis expansions of the functional predictors. Classical A- and D-optimality are extended to this functional setting, and the authors show the resulting criteria depend on maximizing/minimizing functions of $Z^T Z$ and do not depend on the unknown error-process covariance $\Sigma$. The paper computes example exact optimal designs under various B-spline basis choices (including cases where predictor and coefficient use different bases) using a coordinate-exchange optimization algorithm, and illustrates the approach with a simulated-data proof-of-concept comparing A-optimal vs random designs. Practical guidance is emphasized: optimal designs depend strongly on the chosen basis family/degree/knots for representing predictors and coefficient functions, and knot alignment can materially affect efficiency.","Model: $y_n(t)=\beta_0(t)+\sum_{i=1}^p\int_0^T \beta_i(s,t)x_{ni}(s)\,ds+\varepsilon_n(t)$. With basis expansions, the model becomes $y(t)=ZB\theta(t)+\varepsilon(t)$ where $Z=\int X(s)H(s)^T ds$ (or $Z=\Gamma J_{HH}$ / $\Gamma J_{CH}$ under basis choices). The integrated SSE minimizer yields $\hat B=(Z^T Z)^{-1}Z^T\int y(t)\theta(t)^Tdt$, and $\mathrm{Var}(\mathrm{vec}(\hat B))=\Sigma\otimes (Z^T Z)^{-1}$. A-optimal designs minimize $\mathrm{tr}(Z^T Z)^{-1}$; D-optimal designs maximize $\det(Z^T Z)$.","They prove that both functional A- and D-optimality criteria are invariant to the unknown error covariance in the response-basis domain (the $\Sigma$ term factors out of trace/determinant via Kronecker product properties). Numerical examples (typically with $N=12$ runs) show that increasing predictor-basis breakpoints can improve A-optimality when the coefficient surface is represented with a richer basis, but gains can diminish and depend on knot alignment between predictor and coefficient bases. In a two-predictor example (different spline degrees/breakpoints per predictor), an A-optimality criterion value of 6.425 is reported for the fixed settings in Table 4. A simulated proof-of-concept indicates the A-optimal design yields an estimated coefficient surface visually closer to the truth than a random design.",None stated.,"The design construction is tied to finite-dimensional basis truncations (choice of basis family/degree/knots and truncation sizes), so optimality is conditional on these modeling choices and may not transfer if the true coefficient/predictors are poorly represented. Computation relies on a coordinate-exchange heuristic with many random starts and is reported to struggle when the parameter dimension is high, suggesting scalability and global-optimality concerns for richer bases or more predictors. The worked examples are primarily simulation/algorithmic demonstrations; broader empirical validation on real experimental studies with dynamic controllability constraints (e.g., actuator limits, smoothness/monotonicity) is not shown.","They suggest extending the function-on-function model to include non-functional factors and concurrent functional interactions, and note time-delayed interactions are more challenging due to higher-dimensional coefficient structures. They also propose developing optimal designs for penalized estimators when richer basis expansions make the unpenalized model non-identifiable, e.g., adding derivative penalties on the functional coefficient.","Develop constrained optimal designs that explicitly encode implementability constraints on dynamic factors (rate-of-change bounds, smoothness, monotonicity, or energy/cost constraints) and assess robustness when such constraints bind. Extend the framework to handle autocorrelated/heteroskedastic errors in time and uncertainty in $\Sigma$ beyond the basis-domain factorization, including self-starting/Phase-I estimation of covariance components. Provide open-source software and benchmarking against alternative optimal-design algorithms (e.g., multiplicative/vertex-exchange, SDP relaxations, Bayesian optimal design) and study scalability as the number of basis coefficients and predictors grows.",2412.14284v1,https://arxiv.org/pdf/2412.14284v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:17:25Z
TRUE,Response surface|Optimal design|Computer experiment|Other,Parameter estimation|Model discrimination|Prediction|Robustness|Cost reduction|Other,D-optimal|A-optimal|Compound criterion|Other,"Variable/General (examples include k=2, k=3, and k=3–9; package supports general k; defaults use point exchange for k<=4 and coordinate exchange for k>=5)",Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/vkstats/MOODE,"The paper introduces MOODE, an R package for constructing multi-objective optimal designs for response surface experiments under model uncertainty (primary vs. potential terms). It combines inference-focused criteria (e.g., (DP)S- and LP-optimality incorporating pure-error degrees of freedom via F-quantiles), model-sensitivity criteria for detecting lack-of-fit in the direction of potential terms, and robustness criteria based on minimizing the MSE of primary-model estimators under contamination. These components are combined through weighted compound (product) criteria (determinant- and trace-based) to target competing experimental goals directly. MOODE implements modified-Fedorov point exchange and coordinate exchange search algorithms to find nearly optimal exact designs from discrete candidate sets. The package is demonstrated on two case studies, including a 3-factor response-surface setting and comparisons to 12-run Plackett–Burman designs for 3–9 two-level factors, illustrating trade-offs among replication (pure error DF), lack-of-fit detection, and robustness to interactions/higher-order terms.","Primary model: $Y_i=\beta_0+\sum_{j=1}^p\beta_j x_{ij}+\varepsilon_i$; encompassing model adds potential terms: $Y_i=\beta_0+x_{i1}^\top\beta_1+x_{i2}^\top\beta_2+\varepsilon_i$. Robustness is quantified via the MSE matrix $\mathrm{MSE}(\hat\beta_1)=\sigma^2 M^{-1}+A_1\beta_2\beta_2^\top A_1^\top$ with $M=X_1^\top(I_n-\frac{1}{n}J_n)X_1$ and alias matrix $A_1=M^{-1}X_1^\top(I_n-\frac{1}{n}J_n)X_2$. Multi-objective designs minimize compound criteria such as $\phi_{\det}(D)=\phi_{(DP)S}(D)^{\kappa_{DP}}\,\phi_{LoF-DP}(D)^{\kappa_{LoF}}\,\phi_{MSE(DS)}(D)^{\kappa_{MSE}}$ (and an analogous trace-based version) with weights summing to 1.","In the 3-factor, 36-run case study (primary second-order; potential cubic/third-order), compound-weighted designs achieved high simultaneous efficiencies across inference, lack-of-fit detection, and MSE-robustness (e.g., for weights 0.4/0.2/0.4: about 94.20% (DP)S, 89.30% LoF-DP, and 99.50% MSE(DS)), while single-objective designs performed poorly on at least one competing criterion. Degrees of freedom trade-offs were explicit: designs emphasizing (DP)S increased pure-error DF (replication), whereas designs emphasizing MSE increased distinct treatments (less replication). For 12-run, two-level designs with k=3–9 (primary main effects; potential two-factor interactions), compound designs maintained nonzero LP-efficiency and generally preserved some pure-error DF where feasible, unlike L-optimal (Plackett–Burman) designs which often had little/no replication leading to zero LP-efficiency for larger k. The authors note occasional optimizer variability (e.g., an efficiency slightly above 100% due to empirical search) and that more random starts may improve some found designs.","The authors state that the implemented exchange algorithms are heuristic and do not guarantee convergence to the global optimum; they recommend multiple random starts (and increasing Nstarts) to mitigate local optima. They also note in an example that some solutions may be improved by using more random starts, indicating sensitivity to initialization and search settings. They further mention that some aspects (candidate list construction, design region choice) are not discussed in the paper and are left to package documentation.","The approach assumes a discrete candidate set of factor levels; performance and optimality can depend strongly on how Levels and the candidate region are specified, and the paper does not provide systematic guidance for continuous-factor regions beyond this discretization. Robustness and sensitivity criteria rely on a specific prior structure for potential terms (e.g., $\beta_2\sim N(0,\sigma^2\tau^2 I)$ and default $\tau^2$), and results may be sensitive to $\tau^2$ without a full sensitivity analysis. Comparisons focus on a limited set of benchmarks (e.g., Plackett–Burman for screening) and do not comprehensively benchmark against alternative robust/model-uncertain optimal design methods across broader scenarios (constraints, correlated errors, non-normality, random effects). Practical adoption may be hindered if computational cost rises for large k or large candidate sets; while parallelism is supported, runtime/complexity scaling is not systematically quantified.","The authors state that future work will add blocking factors to the optimal designs. They also suggest potential extensions including implementing different optimization algorithms (e.g., nature-inspired heuristics) and incorporating different model classes such as hybrid nonlinear models.","Providing principled workflows for choosing $\tau^2$ and the compound weights (e.g., elicitation tools, Pareto-front exploration, or automated weight selection) would strengthen practical use of multi-objective criteria. Extending MOODE to settings with autocorrelation, heteroskedasticity, or non-normal errors (and to generalized linear models) would broaden applicability in real processes. Adding diagnostics for design robustness (e.g., sensitivity of efficiencies to discretization of factor levels/candidate region) and producing a CRAN package release with vignettes could improve accessibility and reproducibility. More extensive benchmarking against other model-robust criteria (e.g., Bayesian/average-optimal, discrimination criteria) and inclusion of larger real-data case studies would help validate performance in practice.",2412.17158v1,https://arxiv.org/pdf/2412.17158v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:18:14Z
FALSE,NA,NA,Not applicable,Not specified,Energy/utilities,Simulation study|Case study (real dataset),TRUE,Other,Not provided,NA,"The paper proposes and analyzes three high-gain DC–DC boost converter topologies derived from a modified classical Ćuk converter, intended to increase low photovoltaic (PV) voltages. For each topology, it derives steady-state voltage-gain relationships using inductor volt–second balance/KVL and reports expected gains of approximately 10×, 20×, and 29× at a 0.9 duty ratio. Performance is evaluated via LTspice numerical simulations and validated with laboratory hardware prototypes driven by PWM gate signals, with duty ratio swept from 10% to 90% and output voltage recorded. The study compares simulated and measured waveforms/voltage-gain curves and notes practical issues such as voltage spikes and efficiency drop at very high duty ratios due to increased switching losses and parasitics. The work is primarily power-electronics topology analysis and validation rather than design-of-experiments methodology.","Key relationships are derived via inductor volt–second balance, written as $kV_{L,\mathrm{ON}}=(1-k)V_{L,\mathrm{OFF}}$, combined with KVL for ON/OFF switching intervals. The first topology’s ideal voltage gain is given as $\frac{V_o}{V_i}=-\frac{1}{1-k}$. The second topology’s overall gain is $\frac{V_{Co}}{V_1}=-\frac{2}{1-k}$. The third topology’s gain is $\frac{V_{co}}{V_1}=-\frac{2+k}{1-k}$ (claimed to yield \~29× at $k=0.9$).","The paper reports theoretical gains of 10 V/V, 20 V/V, and 29 V/V for the first, second, and third converters respectively at 90% duty ratio. Experimental prototypes and LTspice simulations are compared via duty-cycle sweeps (10%–90%) and waveform plots; the curves are reported to closely match. For the first topology, at 5 V input and 80% duty ratio, an experimental output of 21.5 V is reported, about 3.5 V lower than theoretical. For the second topology, the design specification targets 50 V output from 5 V input (2.5 W) at 120 kHz; for the third topology, the specification targets 70 V output from 5 V input (0.35 W) with reported \~29 V/V gain at $k=0.9$. Efficiency is stated to drop appreciably at very high duty ratio due to increased switching losses and voltage drops.","The authors state that a major drawback of the proposed circuits is reduced efficiency at a 90% duty ratio, because switching losses increase at higher duty ratio leading to a large voltage drop. They also note high voltage spikes on the switch in prototype waveforms due to parasitic inductance from wiring, which can stress/malfunction switching devices unless mitigated (e.g., snubber circuit).","The analysis largely assumes idealized components/steady-state operation; effects of component tolerances, temperature, magnetic saturation, and non-ideal diode/MOSFET dynamics are not systematically quantified. Prototype validation focuses on voltage gain/waveforms and does not provide comprehensive efficiency, thermal, EMI, or reliability characterization across load range and duty cycle, which are critical for PV applications. The operating point uses very high duty cycles (up to 0.9), which in practice can be constrained by controller limits, minimum off-time, and increased current ripple/stress; these constraints are not fully explored. No open hardware schematics/layout guidance is provided, despite parasitics being a key source of discrepancies/spikes.",None stated.,"Future work could include full efficiency mapping (including conduction/switching losses) versus duty cycle, load, and switching frequency, with thermal measurements and device stress analysis. Developing EMI-aware PCB layouts and standardized snubber/clamp networks for each topology would address the observed spikes and improve practical robustness. Extending validation to realistic PV operating conditions (MPPT integration, irradiance variation, input ripple, and dynamic response) and to closed-loop control design would strengthen applicability. Releasing LTspice schematics/netlists and measurement data would improve reproducibility and adoption.",2412.18329v1,https://arxiv.org/pdf/2412.18329v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:18:51Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Screening|Model discrimination|Prediction|Other,Other,"Variable/General (design variables depend on environment; e.g., hyperbolic discounting uses 3 design variables iR, dR, D; location finding chooses measurement location; others choose time, student-question pair, etc.)",Healthcare/medical|Environmental monitoring|Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/kanishkg/boxing-gym,"This paper introduces BoxingGym, a benchmark of 10 interactive environments for evaluating autonomous agents’ capabilities in experimental design and model discovery within a simulated scientific discovery loop. Each environment is implemented as a generative probabilistic model (implemented in PyMC) that allows an agent to choose an experimental design (intervention) and receive outcomes, enabling sequential experimentation. Experimental design quality is scored via Bayesian optimal experimental design using expected information gain (EIG), estimated with a nested Monte Carlo estimator, and reported as an information-regret measure relative to the best among random candidate experiments. Model discovery is evaluated via a communication task: after interacting for a fixed number of experiments, the “scientist” agent produces a token-limited natural-language explanation that is given to a “novice” agent, whose predictive accuracy measures how well the discovered model/generalization transfers. Across multiple LLMs (7B–32B, open and closed), larger and closed-source models generally perform better, but all models still struggle, indicating experimental design and model discovery remain challenging open problems for LLM agents.","The benchmark formalizes experiments with a prior over latent parameters $p(\theta)$ and a simulator $p(y\mid \theta,d)$; running an experiment chooses a design $d$ and samples $y\sim p(y\mid d)=\mathbb{E}_{p(\theta)}[p(y\mid\theta,d)]$. Experimental design is evaluated by expected information gain $\mathrm{EIG}(d)=\mathbb{E}_{p(y\mid d)}[H[p(\theta)]-H[p(\theta\mid y,d)]]$. Because EIG is intractable, it is estimated with a nested Monte Carlo estimator $\hat\mu_{\mathrm{NMC}}(d)=\frac{1}{N}\sum_{n=1}^N \log\frac{p(y_n\mid \theta_{n,0},d)}{\frac{1}{M}\sum_{m=1}^M p(y_n\mid \theta_{n,m},d)}$, where $\theta_{n,m}\overset{iid}{\sim}p(\theta)$ and $y_n\sim p(y\mid \theta=\theta_{n,0},d)$.","The authors report that larger models (32B) consistently outperform smaller (7B) variants across environments for both experiment selection and downstream prediction, and that closed-source models typically achieve better overall results than open-source alternatives. They find that providing domain prior information in prompts often does not help and can hurt performance (e.g., by inducing overly strong, uncorrected assumptions or overfitting to limited observations). EIG-regret analysis shows some alignment between selecting informative experiments and predictive accuracy for GPT-4o (low regret and strong prediction), but also cases where good prediction does not imply low EIG regret (e.g., Qwen-32B). Box’s Apprentice (LM augmented with explicit PyMC model fitting) does not reliably improve over a plain LLM baseline and tends to favor overly simple functional forms with limited data.","The paper notes that BoxingGym uses pre-defined experimental paradigms rather than requiring agents to design experiments “from scratch.” It also explicitly states it ignores resource constraints (e.g., costs/budgets) in experiment selection. Finally, it acknowledges limited coverage of scientific domains and suggests expanding to more diverse fields and richer human-behavior simulations.","Because the benchmark environments are simulated with known generative models, EIG is computed under (near-)model-correctness; this may overestimate real-world utility where the data-generating process is misspecified, nonstationary, or partially observed. The EIG-regret baseline is defined relative to the best of 100 random candidate experiments, which can be a weak proxy for true optimal designs and may vary with candidate-set construction. Results are sensitive to agent prompting/interface constraints (language-only actions, validity checking, token limits) and may conflate “scientific reasoning” with instruction-following and formatting robustness rather than pure design optimality. Limited real-data validation: while domains are “real-world inspired,” the evaluation largely occurs in simulated settings, so practical effectiveness on real experimental workflows is not established.","The authors propose extending BoxingGym to include experiment design from scratch (rather than fixed paradigms), incorporating resource constraints into the design loop, and adding environments from more diverse scientific fields. They also suggest improving the human-behavior environments (Moral Machines, Emotions) with more sophisticated participant simulations. Additionally, they recommend exploring augmentations such as data visualization, model validation tooling, and web-based research strategies to improve experimental guidance and discovery.","A valuable extension would be to evaluate robustness when the agent’s assumed model class differs from the true simulator (explicit model misspecification), including distribution shift and unmodeled confounding, to better mirror real science. Adding constraints/objectives beyond information gain—e.g., cost-aware utility, ethical constraints, and safety/feasibility constraints—would make designs more realistic and permit studying multi-objective design trade-offs. Providing standardized baselines from classical BOED (e.g., myopic EIG maximization with amortized estimators, Bayesian optimization over designs) would better calibrate how far LLM agents are from strong algorithmic OED methods. Releasing a reference implementation that logs full trajectories and supports reproducible ablations (prompt variants, action-space restrictions, uncertainty reporting calibration) would help disentangle reasoning vs. interface effects and enable finer-grained diagnostics.",2501.01540v2,https://arxiv.org/pdf/2501.01540v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:19:38Z
FALSE,Sequential/adaptive|Bayesian design,Optimization,Other,Variable/General (examples include 9 hyperparameters in simulation; 15 in CASH OpenML; 29–47 in NAS; 16 search spaces in HPO-B),Other,Simulation study|Case study (real dataset),TRUE,Python,Not provided,https://github.com/hyperopt/hyperopt|https://github.com/scikit-optimize/scikit-optimize|https://github.com/automl/SMAC3|https://github.com/maxc01/addtree,"The paper proposes AttnBO, a Bayesian optimization framework for conditional (tree-structured) hyperparameter search spaces where configurations vary in dimensionality and structure across subspaces. It introduces structure-aware hyperparameter embeddings (identity, index, value, and parent identity) and a self-attention (Transformer) encoder to map variable-length configurations into a unified latent space. A single deep-kernel Gaussian process (DKGP) with a Matérn 5/2 kernel is then fit on this latent space, enabling shared modeling of all subspaces and capturing relationships between semantically related hyperparameters across subspaces. The method selects new evaluations via Expected Improvement (EI), optimized per subspace (L-BFGS) for batch/parallel querying, with a fallback to random search over the whole space when subspaces are too many. Empirical results on a synthetic tree-structured function, NAS on CIFAR-10, OpenML tabular tasks (SVM/XGBoost/CASH), and warm-start meta-learning on HPO-B show improved efficiency and performance versus SMAC, TPE, Bandits-BO, and AddTree, at modest additional runtime for training the attention encoder.","AttnBO uses deep kernel learning where configurations are embedded and encoded before GP modeling: $k_{\text{deep}}(x,x'|\theta,\omega)=k(\phi(x,\omega),\phi(x',\omega)|\theta)$ and parameters are trained by maximizing GP log marginal likelihood. The structure-aware token embedding for each hyperparameter concatenates identity, index, value, and parent-identity embeddings: $\mathrm{emb}(p)=\mathrm{concat}(\mathrm{id\_emb}(p),\mathrm{idx\_emb}(p),\mathrm{value\_emb}(p),\mathrm{id\_emb}(p^{\uparrow}))$, and a configuration is a sequence $\mathrm{emb}(x)=[\mathrm{emb}(p_1),...,\mathrm{emb}(p_{d_i})]$. GP posterior prediction follows standard GP formulas: $\mu(x_*)=k_*^\top K^{-1}y$ and $\mathrm{var}(x_*)=k(x_*,x_*)-k_*^\top K^{-1}k_*$, with EI used as the acquisition function.","Across tasks, AttnBO is reported as best-performing compared with Random Search, SMAC, TPE, Bandits-BO, and AddTree on (i) a tree-structured synthetic benchmark, (ii) NAS on CIFAR-10, and (iii) OpenML SVM/XGBoost/CASH spaces (average ranking plots show AttnBO leading). For sample efficiency on the synthetic benchmark, the paper states AttnBO achieves performance with 100 observations comparable to separate GPs needing 200 observations. Runtime on the synthetic benchmark for 100 BO iterations is reported as 17.05 minutes for AttnBO vs 11.96 for Bandits-BO and 12.73 for AddTree. On HPO-B, warm-starting (AttnBO_WS) improves performance over training from scratch and is competitive with reported transfer-learning baselines in the benchmark.","The authors note increased computational cost relative to simpler BO baselines due to training the deep attention encoder, though they argue the overhead is small (about 5 minutes more than Bandits-BO/AddTree over 100 iterations on the synthetic task) and becomes negligible when objective evaluations are expensive. They also state that optimizing EI in each subspace can be computationally expensive when there are many subspaces; in such cases, they suggest using random search over the whole space to optimize EI (or alternative acquisition approaches like Thompson sampling).","The method relies on training a deep encoder jointly with GP hyperparameters, which can be sensitive to optimization choices and may require careful tuning or substantial data before the learned representation stabilizes, especially in very small-budget BO settings. The work assumes the conditional structure can be represented as a tree (or a combination of trees); more general dependency graphs or complex constraints may not fit this representation directly. Comparisons may be incomplete for modern mixed/conditional HPO methods (e.g., recent multi-fidelity, constraint-aware, or neural surrogate BO variants), and practical robustness to noisy, heteroscedastic, or correlated observations is not thoroughly characterized. No publicly released implementation for AttnBO is indicated, which limits reproducibility and adoption.",None stated.,"A natural extension is to handle more general conditional structures beyond trees (e.g., DAG constraints, conditional activation with multiple parents) and to study robustness under non-i.i.d. noise, heteroscedasticity, or temporal correlation common in training-based evaluations. Incorporating multi-fidelity or early-stopping signals (common in NAS/HPO) into the surrogate and acquisition could further improve sample efficiency and wall-clock time. Developing a self-starting/online variant that reduces Phase-I training burden for the attention encoder and providing an open-source reference implementation would improve practicality and reproducibility. Additional ablations on representation choices (pooling vs token, embedding dimensions) across diverse benchmarks would clarify when attention-based unification yields the largest gains.",2501.04260v2,https://arxiv.org/pdf/2501.04260v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:20:16Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Other,Optimization|Prediction|Cost reduction|Other,Not applicable,Variable/General (high-dimensional design parameters; examples include 2 parameters and panel placement/size),Other,Simulation study|Other,TRUE,Other,Not provided,https://arxiv.org/abs/2501.04448|https://pos.sissa.it/|https://cds.cern.ch/record/2721370|https://arxiv.org/abs/1712.04741|https://www.facebook.com/yann.lecun/posts/10155003011462143|http://cds.cern.ch/record/2139578|https://doi.org/10.5281/zenodo.13963379|https://indico.cern.ch/event/1198609/contributions/5366510/|https://doi.org/10.5281/zenodo.8394819,"This paper (conference proceeding/talk write-up) discusses AI-assisted experimental design for particle physics instruments using differentiable programming, where detector/design parameters are optimized by gradient descent through a simulation-to-inference pipeline. The core idea is to make the end-to-end experiment pipeline differentiable (or replace intractable components with differentiable surrogate models) and then minimize a loss that combines physics performance with a penalized cost/constraint term. It surveys proofs-of-concept including differentiable optimization of a muon-tomography detector layout (panel placement/size) and optimization of a parallel-plate avalanche counter with optical readout for neutron tomography, reporting improved prediction error after optimization and convergence behavior across initializations. The article emphasizes computational barriers to scaling such approaches to LHC-scale designs and points to neuromorphic computing (spiking neural networks) and quantum machine learning as potential future paradigms to reduce power/data needs and enable scalable differentiable optimization. Overall, it positions gradient-based, simulation-driven design optimization as a new methodology for navigating high-dimensional detector design spaces that are difficult to explore manually or via brute-force scans.","The design problem is formulated as a constrained/penalized expected-loss minimization over design parameters $\theta$: $\hat{\theta}=\arg\min_{\theta}\int L\big[A(\zeta(\phi,\theta)),c(\theta)\big] \, p(z\mid x(\phi),\theta)\, f(x,\phi)\, dx\, dz$. The pipeline includes stochastic sampling $x\sim f(x,\phi)$, detector response $z\sim p(z\mid x(\phi),\theta)$, optional reconstruction/feature building $\zeta(\phi,\theta)=R[z(\phi,\theta),\nu(\phi)]$, and a summary statistic $s=A[\zeta(\phi,\theta)]$. Optimization is performed via gradient descent using automatic differentiation, requiring either closed-form differentiability of $p(z\mid x,\theta)f(x,\phi)$ or a differentiable surrogate (e.g., neural network).","The proceeding reports qualitative/graphical improvements in mean squared error after gradient-based optimization in proof-of-concept studies (e.g., muon tomography layout optimization and a parallel-plate avalanche counter optimization). For the avalanche-counter example, the optimized solution for two design parameters is reported to converge to the same optimum largely independent of initialization, and one parameter’s optimum matches an independent brute-force configuration scan. The work highlights that even these low-dimensional optimizations are computationally demanding (e.g., requiring dedicated CUDA kernels in TomOpt) and argues current CPU/GPU/FPGA approaches do not scale to LHC-complexity designs. No single consolidated ARL/efficiency table is provided in this write-up; results are primarily demonstrated via example figures and referenced works.","The author notes major computational limitations: even low-dimensional proofs-of-concept are computationally challenging, and scaling to LHC-scale experiments is described as unfeasible with current CPU/GPU/FPGA-based technology. The author also states that optimization is not meant to mandate a single ‘optimal’ solution; rather it should provide a landscape of near-optimal solutions, since some considerations may be unmodeled due to discreteness/nondifferentiability or simplifying assumptions. The write-up also indicates differentiability constraints: the required distributions must be writable in closed form or replaced with differentiable surrogate models.","As a high-level proceeding, the paper does not fully specify the experimental-design search space, constraints, or optimization hyperparameters in a way that would permit replication without consulting the cited works (e.g., TomOpt). It also does not provide a systematic comparison against classical DOE/optimal-design criteria (e.g., D-/I-optimality) or established design-of-computer-experiments baselines (e.g., Latin hypercube, Bayesian optimization), making it hard to position performance relative to standard DOE practice. The methodology’s robustness to simulator mismatch, stochastic gradient noise, and nonconvexity (local minima sensitivity) is discussed conceptually but not quantified here. Finally, it largely focuses on simulation-driven optimization; real-world operational constraints (manufacturability, alignment tolerances, maintenance) are acknowledged conceptually but not evaluated quantitatively.","The author explicitly argues that scaling to LHC-scale design requires a ‘change in paradigm’ and discusses two future directions: neuromorphic computing using spiking neural networks (including proposing neuromorphic readout for Q-Pix detectors) and long-term prospects for quantum machine learning, leveraging analytically differentiable quantum circuits and potential data-efficiency gains. The proceeding also mentions ongoing/forthcoming work with different constraints on detector panels (accepted by MARESEC; to be published).","A natural extension is to integrate discrete design choices (component counts, topology, on/off placement) via differentiable relaxations or mixed-integer/gradient hybrid optimization, since many detector-design decisions are inherently discrete. Another direction is to develop uncertainty-aware/Bayesian design objectives (e.g., expected information gain) to explicitly trade off physics sensitivity and systematic/nuisance-parameter robustness, rather than relying mainly on surrogate loss terms. Building standardized benchmarking suites (common simulators, constraints, compute budgets) would enable fair comparison with alternative design optimizers such as Bayesian optimization and evolutionary methods. Finally, releasing reusable software modules and reference implementations (including surrogate models and differentiable simulators) would accelerate adoption beyond the specific proof-of-concept studies cited.",2501.04448v1,https://arxiv.org/pdf/2501.04448v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:20:59Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Optimization|Cost reduction|Other,Not applicable,"Variable/General (signal dimension m, Hankel order L, number of segments p, lengths Ti, weights αi)",Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper proposes three weighted “collectively persistently exciting” (CPE) informativity conditions—mosaic (MCPE), cumulative (CCPE), and hybrid (HCPE)—to relax the classical single-trajectory persistency of excitation requirement for data-driven identification and control of LTI systems. It analyzes transformation relations among these conditions and proves rank properties that extend Willems’ fundamental lemma to multiple trajectories via alternative Hankel-matrix constructions with nonzero weighting factors. It then presents open-loop experimental design procedures (offline or online) to synthesize input sequences across multiple segments so that the chosen CPE condition holds even though each individual segment is not PE. The methods are validated through illustrative examples including least-squares identification, distributed adaptive identification, data-driven state-feedback via LMIs, and data-driven MPC, with comparisons of computational burden across the three matrix structures. Weighting factors are shown to mitigate numerical conditioning issues when different trajectories have disparate magnitudes, improving identification performance in simulation.","The core informativity tests are full-row-rank Hankel constructions: MCPE requires $H^{\mathrm{mos}}_L=[\alpha_1 H_L(z_1)\;\cdots\;\alpha_p H_L(z_p)]$ to have rank $mL$; CCPE requires $H^{\mathrm{cum}}_L=\sum_{i=1}^p \alpha_i H_L(z_i)$ to have rank $mL$; HCPE stacks a cumulative part with a mosaic part. Under these conditions, rank identities analogous to Willems’ lemma hold, e.g. $\mathrm{rank}\,\begin{bmatrix}H_1(x)\\ H_L(u)\end{bmatrix}=n+mL$ (with the corresponding mos/cum/hyb operator), enabling trajectory representation $\begin{bmatrix}\bar x_{[0,L-1]}\\ \bar u_{[0,L-1]}\end{bmatrix}=\begin{bmatrix}H_L(x)\\ H_L(u)\end{bmatrix}g$. The paper also gives constructive input-design recursions (e.g., Eq. (6)) to generate sequences so the composite Hankel matrix becomes full row rank while each individual segment remains non-PE.","Theorem 2 provides minimal length requirements for achieving rank $mL$ with the proposed constructions: MCPE needs $\sum_i T_i \ge mL+p(L-1)$, CCPE needs $T_0\ge (m+1)L-1$, and HCPE needs $T_0+\sum_{i=\bar p+1}^p T_i\ge mL+(p-\bar p+1)(L-1)$, while ensuring each segment has $\mathrm{rank}(H_L(z_i))<mL$. In a batch-reactor LS identification example (n=4, m=2) the authors synthesize order-$L=5$ CPE using $p=10$ trajectories and report identification-error curves that remain satisfactory across noise levels up to $\sigma_w=0.1$. They demonstrate that improper weighting (large magnitude disparities in $\alpha_i$) degrades MCPE performance, while appropriate weights reduce identification error. In data-driven MPC on the same system, reported optimization runtimes differ by matrix structure: CCPE 0.1919 s, MCPE 3.1657 s, HCPE 1.8771 s (Table 1).",None stated.,"The “experimental design” procedures are constructive rank-synthesis recipes tailored to Hankel full-row-rank conditions, but they do not optimize a statistical objective (e.g., A/D/I-optimality) or explicitly handle actuator/state constraints beyond brief discussion of online feasibility; practical constrained input design may be nontrivial. Empirical validation is primarily simulation-based and limited to a few illustrative plants (converter, batch reactor), so robustness to measurement noise, unmodeled dynamics, and closed-loop data collection constraints is not extensively benchmarked against standard input-design baselines (e.g., PRBS/multisine with constraints). The weighting-factor discussion is exploratory; no systematic method is provided to choose optimal weights, and ill-conditioning in large Hankel matrices may remain an issue for high dimensions or large p.","The authors propose extending the approach to distributed (rather than centralized) data-driven control while ensuring the three CPE conditions across multiple control signals. They also identify systematic design of optimal weighting factors $\alpha_i$ as an open problem because weights affect performance and effectiveness of data-driven methods. Finally, they suggest studying how different experiment-design methods affect parameter uncertainty and control performance, aiming to integrate uncertainty-aware objectives into the CPE-based framework.","Develop constrained input-design variants that guarantee CPE while respecting hard bounds (inputs/states) and safety constraints, potentially via optimization-based or MPC-like synthesis rather than rule-based construction. Provide finite-sample statistical analyses (bias/variance, confidence regions) for identification under CPE with noise and unknown initial states, and compare against classical optimal input design and modern targeted exploration methods on standardized benchmarks. Extend the framework to output-only (partially observed) systems, nonlinear systems, and systems with autocorrelated disturbances, including self-starting/adaptive versions that update L and segment allocation online.",2501.10030v4,https://arxiv.org/pdf/2501.10030v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:21:45Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Optimization|Parameter estimation|Prediction,Other,Variable/General (examples: Case 1 has 1 design variable; Case 2 has 2 design variables for probe location; parameters inferred include 8 turbulence-model parameters),Energy/utilities|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/tcoonsUM/mf-eig/,"The paper develops a Bayesian optimal experimental design method that targets maximizing expected information gain (EIG) in model parameters, but accelerates EIG estimation for expensive nonlinear physics models via a novel multi-fidelity EIG (MF-EIG) estimator. MF-EIG is built within the approximate control variate (ACV) framework and includes a reparameterization of EIG so that Monte Carlo expectations are taken over easily sampled noise/parameter variables rather than requiring data-model solves to generate observations, enabling ACV compatibility. The resulting estimator is unbiased with respect to the high-fidelity mean of the chosen high-fidelity utility approximation and is variance-minimizing under a fixed computational budget; practical enhancements include low-fidelity inner-loop sample-size optimization and inner-loop sample reuse. Performance is demonstrated on (i) a 1D nonlinear benchmark and (ii) a turbulent-flow Bayesian calibration problem (RANS/SST turbulence-closure parameters) with multi-grid solvers, showing roughly one- to two-orders-of-magnitude variance reduction versus single-fidelity nested Monte Carlo EIG estimation. The work advances Bayesian OED practice by providing a general, implementable multi-fidelity variance-reduction approach for EIG estimation that avoids surrogate-induced bias while exploiting correlated lower-fidelity models.","The Bayesian OED objective is the expected information gain: $U(\xi)=\mathbb{E}_{Y,\Theta\mid\xi}[\log\tfrac{p(\Theta\mid Y,\xi)}{p(\Theta)}]=\mathbb{E}_{Y,\Theta\mid\xi}[\log\tfrac{p(Y\mid\Theta,\xi)}{p(Y\mid\xi)}]$. A key step reparameterizes the expectation over data-noise $E$ (with $Y=h(E;\Theta,\xi)$) so $U(\xi)=\mathbb{E}_{E,\Theta\mid\xi}[u_E(\xi,E,\Theta)]$, enabling multi-fidelity ACV. The MF-EIG estimator then uses ACV across utility models: $\tilde U(\xi)=\hat U_0(\xi,z_0)+\sum_{m=1}^M \alpha_m(\hat U_m(\xi,z_m^*)-\hat U_m(\xi,z_m))$, with $\hat U_m=(1/N_m)\sum_i u_m(\xi,z_m^{(i)})$ and $u_m$ often implemented as an NMC-style log-ratio utility with an inner Monte Carlo estimate of the evidence term.","Across the two numerical studies, MF-EIG achieves one- to two-orders-of-magnitude variance reduction relative to single-fidelity nested Monte Carlo (NMC) EIG estimation under matched computational budgets. In Case 1 (nonlinear benchmark), variance reduction ratios are about 6–9× without inner-loop sample reuse and about 17–22× with reuse (design-averaged). In Case 2 (turbulent flow probe-placement design), design-averaged empirical variance reduction is about 61× (naïve inner-loop sizes) and 70× (optimized inner-loop sizes), while at the optimal design location the empirical reduction reaches about 94× and 233×, respectively. The experiments also show that optimizing low-fidelity inner-loop sample sizes yields modest additional gains (≈10–20% on average), whereas inner-loop sample reuse can roughly double variance reduction in the benchmark example.","The authors note that MF-EIG (like multi-fidelity methods generally) requires the existence of useful lower-fidelity models that are substantially cheaper yet highly correlated with the high-fidelity model; without such models, gains are limited. They also state that low-fidelity models must share the same parameterization/inputs as the high-fidelity model (i.e., the same $Z=\{E,\Theta\}$), and suggest that handling models that share only subsets of parameters is an open challenge. They further acknowledge that using a single, design-averaged covariance matrix (and thus one estimator structure) across the whole design space can be suboptimal where correlations vary strongly by design point.","The work focuses on variance reduction relative to a chosen high-fidelity utility approximation $u_0$; when $u_0$ is NMC-based, the overall estimator can still have non-negligible bias versus the true EIG for finite inner-loop size $N_{\text{in},0}$, and practical guidance for selecting $N_{\text{in},0}$ under strict budgets is limited. Covariance/cost estimation relies on pilot sampling and empirical covariance matrices; in high-dimensional settings or with limited pilots, covariance estimation error could materially affect the claimed variance optimality and allocations. The approach is demonstrated on two problems; broader benchmarks (e.g., higher-dimensional design spaces, non-invertible/non-injective data models, or strongly non-Gaussian/noisy likelihoods) and sensitivity to ACV family choice are not exhaustively explored.","They propose exploring ways to systematically construct low-fidelity models in MF-EIG by manipulating inner-loop sample sizes in NMC-style utilities (rather than only changing the underlying physics/solver fidelity). They also suggest extending the method to cases where models do not share the full set of parameters (only subsets), citing adaptive-basis sampling as a possible direction. Finally, they highlight integrating MF-EIG more tightly with the OED optimization loop via design-adaptive estimator structures (since correlations vary across the design space) and improving/strategizing pilot sampling and correlation sharing across designs to reduce pilot cost and improve efficiency.","Developing self-tuning or online-learning versions of MF-EIG that update covariance/cost estimates and allocations during the design search could reduce reliance on expensive upfront pilot studies and improve robustness. Extending the reparameterization and MF-EIG construction to non-invertible or partially observed data models (where $h^{-1}$ may not exist globally) would broaden applicability to many realistic experimental settings. Providing theoretical or empirical guidance on balancing bias (through $N_{\text{in},0}$ and likelihood approximations) versus variance reduction (through multi-fidelity ACV) for end-to-end design optimality would help practitioners choose budgets and hyperparameters. Packaging the method into a reproducible software workflow (including uncertainty quantification for covariance estimation and allocation sensitivity) could accelerate adoption in Bayesian OED practice.",2501.10845v2,https://arxiv.org/pdf/2501.10845v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:22:46Z
TRUE,Optimal design|Computer experiment|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Optimization|Screening|Cost reduction|Other,D-optimal|Bayesian D-optimal|Minimax/Maximin|Other,"Variable/General (examples include d=8, d=12; many qualitative factors)",Transportation/logistics|Semiconductor/electronics|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2501.14616,"The paper proposes QuIP, an experimental design framework for expensive computer simulators with many qualitative (categorical) factors, using a Gaussian process surrogate with an exchangeable kernel. For initial design, it derives a maximin Hamming-distance criterion (motivated by an asymptotic D-optimality result under a scaled exchangeable kernel) and reformulates the design search as a multi-dimensional bottleneck assignment problem solvable to global optimality via integer programming. For sequential design, it reformulates Active Learning MacKay (maximize posterior variance) and UCB Bayesian optimization acquisitions as sparse d-adic assignment problems, enabling efficient global optimization with IP solvers (implemented with Gurobi). Extensive simulation/path-planning experiments (maze solving, greedy-snake) and a rover trajectory application show QuIP yields better prediction and optimization performance than random sampling, a Monte Carlo candidate-set optimizer, metaheuristics (differential evolution), and LVGP baselines, especially as the number of factors/levels grows. The work advances DOE for qualitative-factor computer experiments by exploiting assignment-problem structure to make optimal/semi-optimal design computation practical and with solver-provided optimality-gap diagnostics.","The qualitative-factor design space is $\mathcal{X}=[M]^d$ and the GP uses an exchangeable kernel $\gamma_{E,\theta}(x,x')=\exp\{-\sum_{l=1}^d \theta_l\,\mathbf{1}(x_l\neq x'_l)\}=\exp\{-\theta\,d_H(x,x')\}$ (isotropic case). Initial design uses the maximin criterion $D_n^*=\arg\max_{D_n}\min_{i\neq i'} d_H(x_i,x_{i'})$, and via one-hot encoding reformulates it as an integer program maximizing $\min_{i,i'}\{d-\mathrm{tr}(I_i I_{i'}^\top)\}$. Sequential design uses ALM $x_{n+1}^*=\arg\max_x \sigma_n^2(x)=\arg\min_x \gamma(x,D_n)\Gamma^{-1}(D_n)\gamma(D_n,x)$ and UCB $x_{n+1}^*=\arg\max_x\{\mu_n(x)+\lambda\sigma_n(x)\}$, both rewritten as sparse assignment-form IPs over one-hot variables.","In initial-design experiments (e.g., $n=50$, $M\in\{10,20,30\}$, $d\in\{M+5,M+15,M+25\}$), QuIP achieves higher maximin Hamming distances than differential evolution after a short solver warm-up, with the performance gap growing as $d$ and/or $M$ increase; for some settings (notably $M=30$) it reaches the stated upper bound of $d-1$, suggesting global optimality. In the maze-solving problem ($d=12$, $M=5$) with $n=40$ initial points, QuIP sequential design improves predictive RRMSE versus random and candidate-set baselines and achieves lower best-path costs under UCB. In the greedy-snake problem ($d=12$, $M=5$), QuIP attains markedly higher best cumulative rewards than random, candidate-set, and LVGP approaches. In rover trajectory optimization ($d=8$, $M=9$, $n=30$ initial plus 200 sequential runs), QuIP achieves the lowest best-so-far cost among compared methods, and Gurobi’s estimated optimality gaps closely track oracle optima in a diagnostic scatterplot.",None stated.,"The approach relies on a specific GP covariance choice for qualitative inputs (exchangeable kernel), and the key IP reformulations/efficiency may not carry over to more flexible qualitative kernels (e.g., unrestrictive correlation matrices or latent embeddings) or mixed qualitative–quantitative inputs without additional approximations. Most evidence is from simulated/path-planning-style benchmarks; broader real-world validation with truly expensive simulators and larger-scale categorical spaces would strengthen practical claims. Sequential design requires repeated GP hyperparameter estimation (MLE) and repeated IP solves; robustness to hyperparameter misspecification and computational scaling for very large $d$ and/or $M$ beyond the tested ranges is not fully characterized.","The authors propose extending the IP-based design reformulations to broader GP models for qualitative factors, including order-of-addition GP models and hierarchically sparse GPs. They also plan to develop integer-programming-based sequential design for other objectives such as fault localization and multi-armed bandits. Finally, they suggest applications to discrete decision-making in public policy as a promising direction.","Develop self-starting or robust variants that reduce dependence on accurate GP hyperparameter estimation (e.g., Bayesian hyperpriors or sensitivity-aware acquisition optimization). Extend QuIP to mixed qualitative–quantitative factor spaces and constrained design regions common in engineering, potentially combining space-filling for continuous inputs with assignment/IP structure for categorical inputs. Provide open-source implementations (e.g., Python/R package) and benchmark scaling studies comparing Gurobi to open-source MILP solvers, including runtime/optimality-gap tradeoffs for very large categorical spaces.",2501.14616v2,https://arxiv.org/pdf/2501.14616v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:23:24Z
TRUE,Sequential/adaptive|Other,Screening|Optimization|Cost reduction|Other,Not applicable,356 genes (pairwise double-knockdowns; 356×356 interaction matrix),Healthcare/medical,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper proposes a Deep Active Learning (DeepAL) framework for sequential experimental design to efficiently discover synergistic host gene-pair knockdowns that reduce HIV viral load, avoiding brute-force testing over all 356×356 pairs. It incorporates a heterogeneous biomedical knowledge graph (SPOKE) via an R-GCN to learn gene embeddings, and uses a bilinear regression model to predict viral-load outcomes for unseen gene pairs. The experimental design component is an iterative batch active learning loop that selects new gene-pair experiments each round using acquisition strategies (e.g., greedy, BADGE-style, ensemble optimism via lower quantiles, and maximum predictive variance) to balance exploitation and exploration. Uncertainty quantification is achieved using deep ensembles (M=20) to drive acquisitions that require uncertainty (optimism/quantile and maximum variance). On the HIV genetic interaction dataset, the ensemble DeepAL approach reportedly uncovers 92% of the top-400 gene pairs after observing <6.3% of the full matrix, with additional analyses including ablations and pathway enrichment for interpretability.","The R-GCN representation update aggregates relation-specific neighbor messages: $h_i^{(l+1)}=\sigma\big(\sum_{r\in R}\sum_{j\in N_i^r}\frac{1}{c_{i,r}}W_r^{(l)}h_j^{(l)}\big)+W_0^{(l)}h_i^{(l)}$. Viral load for a gene pair is predicted by a bilinear model with nonlinearity: $\hat y_{ij}=\mathrm{Softplus}(x_i^\top A x_j)+b$, where $x_i,x_j$ are gene embeddings. The active learning loop fits parameters by minimizing Huber loss over observed pairs: $\sum_{(i,j)\in S'}\mathrm{Huber}(\hat y_{ij}-y_{ij})$, then selects the next batch via an acquisition function (e.g., ensemble lower quantile or ensemble predictive standard deviation).","In the main experiments (17 rounds with batch size 400 gene-pairs per round), the proposed ensemble DeepAL framework is reported to uncover 92% of the top-400 gene pairs after observing less than 6.3% of the full 356×356 interaction matrix. Across acquisition strategies, ensemble “optimism” using a 10% quantile achieves the best terminal Coverage@400, while the maximum-variance strategy yields better MAE on unseen pairs (more exploration). Ablations indicate benefits from (i) ensembles vs. a single model for improved long-run coverage, (ii) using SPOKE-based graph embeddings vs. unconstrained/free embeddings, (iii) self-supervised initialization vs. random initialization (especially early rounds), and (iv) fine-tuning embeddings during the active-learning loop vs. freezing them. Supplementary results show similar trends for batch sizes 200 (33 rounds) and 800 (9 rounds).",None stated.,"The work evaluates acquisition performance by simulating sequential querying on an already-measured HIV double-knockdown matrix rather than conducting true prospective wet-lab selection and measurement, so real-world experimental noise, costs, and assay constraints may not be fully captured. The approach is specialized to pairwise interventions; scalability and method behavior for higher-order (3+) gene combinations or other perturbation modalities are not demonstrated. The method relies on the SPOKE knowledge graph and a sampled subgraph (random walks), so performance may vary with knowledge-graph coverage/quality and subgraph sampling choices, and generalization beyond HIV-related genes is unclear. Code is not provided, limiting reproducibility and practical adoption.","The authors propose integrating pre-trained large language models into the framework to provide a warm start for initial recommendation steps. They also suggest incorporating additional knowledge-graph information such as node features to better differentiate gene nodes. Finally, they propose developing more efficient graph-learning inference (e.g., neighbor subsampling) to enable using the full knowledge graph rather than a sampled subgraph.","Validate the method prospectively in a closed-loop experimental setting (wet-lab or high-fidelity assay pipeline) to confirm that the sequential selections translate into real reductions in experimental burden and improved discovery of synergistic pairs. Extend the design to handle practical constraints (e.g., forbidden/unsafe gene targets, batch effects, plate layouts, asymmetric costs, replicate allocation) and to incorporate measurement noise models explicitly. Generalize beyond pairwise knockdowns to higher-order combinations and to other endpoints (toxicity/viability, off-target effects), potentially via structured models or combinatorial bandits with constraints. Provide an open-source implementation with standardized benchmarks and comparisons to additional Bayesian optimization / optimal design baselines to improve reproducibility and positioning in the broader DOE literature.",2502.01012v1,https://arxiv.org/pdf/2502.01012v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:24:01Z
TRUE,Optimal design|Computer experiment|Other,Parameter estimation|Prediction|Optimization|Cost reduction,Minimax/Maximin|Space-filling|Other,"Variable/General (m components; designs for n=km runs, k≥1; quantitative part has m factors; combined QS design often treated as 2m inputs: m quantitative + m sequence positions)",Healthcare/medical|Food/agriculture|Transportation/logistics|Other|Theoretical/simulation only,Simulation study|Other,TRUE,R,Supplementary material (Journal/Publisher),https://doi.org/10.1360/SCM-2024-0039,"The paper proposes a new class of optimal designs for experiments that jointly involve quantitative factors (component doses/amounts) and sequence factors (order-of-addition), termed quantitative-sequence (QS) factors, where the design space is semi-discrete and very large. It constructs QS designs using transformations (Williams transformation and variants), good lattice point sets, and Latin-square-based constructions, targeting designs that are marginally coupled, pair-balanced for adjacent-order effects, space-filling, and (asymptotically) column-orthogonal. For the quantitative part X, the designs are Latin hypercube designs optimized under maximin L1/L2 distance; for the sequence part O, designs are optimized under maximin Hamming distance, pair-balance, and low average absolute inter-column correlation. Constructions cover n=m (notably m=p−1 for odd primes p, plus broader even-m cases via Latin squares) and general n=km (k≥2) with marginally coupled structure, with theoretical bounds and asymptotic optimality results. A simulation study on a QS Traveling Salesman Problem shows improved Bayesian optimization/active learning performance when using the proposed initial designs compared with a prior QS-learning initial design and with random designs.","Quantitative space-filling is defined via maximin Lq distance for LHDs: $d_q(x_i,x_j)=(\sum_{k=1}^m |x_{ik}-x_{jk}|^q)^{1/q}$ and $d_q(X)=\min_{i<j} d_q(x_i,x_j)$, with bounds $d_{1,\text{upper}}=\lfloor (n+1)m/3\rfloor$ and $d_{2,\text{upper}}=\sqrt{\lfloor n(n+1)m/6\rfloor}$. Sequence space-filling uses Hamming distance $d_H(o_i,o_j)=\sum_{k=1}^m \mathbb{I}(o_{ik}\neq o_{jk})$ and $d_H(O)=\min_{i<j} d_H(o_i,o_j)$, with $d_{H,\text{upper}}=m$ if $n\le m$ else $m-1$. Pair-balance is defined by equal adjacent subsequence counts $t_{i,j}$ for all ordered pairs $i\neq j$, and orthogonality is assessed by average absolute column correlations $r_{\text{ave}}(O)=\sum_{u\ne v}|r_{uv}(O)|/(m(m-1))$. Key construction tool is the Williams transformation (for odd prime p): $W(x)=2x$ for $0\le x<p/2$ and $W(x)=2(p-x)-1$ for $p/2\le x\le p-1$; designs are built from good lattice point sets, modular shifts, and applying $W(\cdot)$ plus leave-one-out and permutations to obtain Latin squares/LHDs.","For n=m=p−1 (p odd prime), the leave-one-out (Williams-transformed) lattice constructions yield sequence designs that are Latin squares, pair-balanced with $t_{i,j}=1$, and maximin Hamming with $d_H(O)=m$ (achieving the bound). Choosing $O=\tilde E_{b_1^*}$ minimizes $r_{\text{ave}}(O)$ and gives asymptotic orthogonality with $r_{\text{ave}}(O)=O(1/m)\to 0$; the associated quantitative design achieves asymptotically optimal L1 distance $d_1(X)\ge (1-O(1/m))d_{1,\text{upper}}$ and large L2 distance ($\approx0.817\,d_{2,\text{upper}}$ asymptotically). For n=km (k≥2), the constructed QS designs are marginally coupled, pair-balanced with $t_{i,j}=k$, have asymptotically maximin Hamming distance (e.g., $d_H(O)\ge m-3$; for n=2m a refined construction achieves $d_H(O)=m-2$), and retain strong L1/L2 space-filling bounds for X. In the TSP simulation (m=6), the proposed initial design reached a high profit earlier (by the 9th sequential point) and achieved a higher final best profit (222.84) than the comparator initial design (207.37), while a random design with 300 points maxed at 204.93.",None stated.,"The work focuses on constructions optimizing distance/correlation criteria and proves asymptotic results, but does not provide a broad empirical evaluation across many practical data-generating processes or noise/constraint settings (e.g., non-i.i.d. errors, correlated responses, constraints on feasible sequences). Sequence orthogonality is treated as column-orthogonality (average absolute correlation) rather than full combinatorial orthogonality (strength-2 OA), which may limit balance of higher-order sequence-position combinations in small n settings. Implementation details for the provided algorithms (threshold accepting, permutations) and reproducibility depend on supplementary code; without it, some optimized variants (e.g., level-permuted Williams Latin square) may be hard to replicate exactly.","The authors propose studying whether similar theoretical results can be obtained for other values of m beyond the prime-related/even-m constructions, and exploring a generalized leave-one-out approach by constructing a larger design (m′=p−1>m) and deleting runs/columns while maintaining good distance/orthogonality (with approximate pair-balance). They also suggest developing methods to handle restricted quantitative levels by collapsing the LHD to a balanced fractional factorial X′ while preserving a marginally coupled structure and studying additional space-filling properties for (X′,O). Another stated direction is improving the quantitative part X for k≥3 (analogous to improvements shown for k=1,2) via structured level permutation/random optimization while preserving marginal coupling, and developing modeling techniques and studying design efficiency under models for this new QS setting.","Provide systematic benchmarking versus alternative QS/order-of-addition design methods (e.g., metaheuristics for OOA designs, D-/I-optimal model-based QS designs) across multiple m,k and noise models, including robustness to autocorrelation and heteroskedasticity. Develop self-starting/online update rules for sequential/adaptive QS designs where the next runs preserve marginal coupling and pair-balance while optimizing an acquisition criterion. Extend constructions and theory to constrained sequence spaces (forbidden adjacencies, precedence constraints) and to multivariate responses or multi-objective optimization common in biomedical/process applications, with accompanying open-source software packaging for broader adoption.",2502.03241v1,https://arxiv.org/pdf/2502.03241v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:24:52Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Other,Bayesian D-optimal,"Variable/General (examples include 3 design variables for spatiotemporal sampling: $d=\{d_x,d_y,d_t\}$; parameters include 2D source location plus additional parametric/NN discrepancy parameters)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/AIMS-Madison/BED-Model-Discrepancy,"The paper proposes a hybrid sequential Bayesian experimental design (sBED) framework to actively learn model discrepancy in digital-twin models while continuing to design informative experiments for low-dimensional physics parameters. Designs are chosen sequentially by maximizing expected information gain (mutual information) using KL divergence between posterior and prior, then real/simulated measurements at the selected spatiotemporal design points are used to update physics parameters via Bayesian inference and update high-dimensional discrepancy parameters via optimization (MAP/gradient-based training of a neural-network correction term). To avoid calibrating discrepancy on uninformative or misleading data, the authors introduce an ensemble-based information-gain indicator that approximates KL divergence using Ensemble Kalman Inversion (EKI) under a Gaussian assumption. Numerical studies on a convection–diffusion source inversion problem compare (i) full Bayesian treatment of low-dimensional parametric error versus the hybrid method and (ii) high-dimensional structural discrepancy represented by a neural network, showing the hybrid approach can reduce bias and improve posterior localization of the source. The paper also examines an ill-posed setting where a physical parameter (time-varying convection coefficient) is optimized jointly with the neural discrepancy, demonstrating that neural correction can compensate model errors and still enable accurate inference of the primary target parameters, while highlighting identifiability challenges.","Bayesian design selects $d^*=\arg\max_{d\in\mathcal D}\mathrm{EIG}(d)$ where $\mathrm{EIG}(d)=\mathbb E_{y|d}[D_{\mathrm{KL}}(p(\theta|y,d)\|p(\theta))]$ and utility is $U=D_{\mathrm{KL}}(p(\theta|y,d)\|p(\theta))$. The modeling framework includes a discrepancy-corrected dynamical model $\partial_t u = G(u;\theta_G)+\mathrm{NN}(u;\theta_{NN})$, with sequential Bayesian updating for $\theta_G$ and gradient-based optimization for $\theta_{NN}$ at each stage. Data informativeness for discrepancy calibration is approximated via EKI updates and a Gaussian-ensemble KL approximation $\tilde D_{\mathrm{KL}}(\mathcal N(\bar\theta_K,\Sigma_K)\|\mathcal N(\bar\theta_0,\Sigma_0))$ computed from initial and updated ensembles.","In the convection–diffusion source inversion example with structural discrepancy, calibrating the discrepancy using an informative design substantially improved field accuracy versus the baseline model (reported MSE reduced from 0.142 to 0.005 and relative error from 1.553 to 0.297), and also improved posterior concentration around the true source location. The ensemble-based KL indicator distinguished “good” vs “poor” designs for discrepancy calibration, with good designs yielding faster KL growth over EKI iterations and better discrepancy correction. For parametric model error (incorrect source-strength parameter), the hybrid approach achieved similar posterior localization to full joint Bayesian treatment while reducing per-stage forward-model evaluations from a 51×51×31 grid (80,631) to a 51×51 grid (2,601), shifting the extra parameter estimation to gradient-based optimization.","The authors note potential ill-posedness in hybrid physics+NN models, including cases where the network can “explain away” or replace physics-based terms, and mitigate this in examples by constraining the NN to learn only stationary/spatial discrepancy while physics handles time dependence. They also state that averaging multiple MAP peaks for a point estimate can fail if the true posterior is genuinely multimodal, potentially yielding an estimate between modes that degrades performance. They further note that relying solely on model simulations to compute EIG (without real measurements) performed worse and needs further refinement because model discrepancy biases the likelihood used in EIG calculations.","The proposed discrepancy-calibration step uses deterministic optimization (MAP/gradient descent) for potentially high-dimensional NN parameters, which can be sensitive to initialization, nonconvexity, and overfitting to sparse sequential data; uncertainty in discrepancy is not propagated into design decisions. The ensemble-based informativeness metric assumes Gaussianity of prior/posterior over discrepancy parameters and uses EKI approximations, which may be inaccurate for strongly nonlinear/multimodal discrepancy landscapes and can mis-rank candidate designs. Comparisons are mainly simulation-based and problem-specific (convection–diffusion and an appendix acoustic example), so generalization to real experiments with unmodeled noise sources, correlated errors, and constraints/costs on experimentation is not established. The method’s computational cost for gradient-based EIG optimization in high-dimensional design spaces and repeated PDE solves/adjoints is not fully benchmarked against alternative modern sBED approximations (e.g., variational MI estimators, SMC-based design) under equal compute budgets.",They suggest using the ensemble-based information-gain approximation not only as a gate/indicator but to directly identify optimal designs and informative data specifically for discrepancy calibration. They also propose extending the ensemble-based information-gain approximation to handle non-Gaussian prior and posterior distributions.,"A natural extension is to propagate discrepancy uncertainty (e.g., via Bayesian neural networks or ensembles) into the BED objective so that design decisions reflect both physics-parameter and discrepancy uncertainty. Another direction is to develop robust/self-starting variants that handle unknown noise covariance, autocorrelated observations, or misspecified noise models common in field measurements. Broader empirical validation on real experimental datasets and standardized benchmarks, plus ablation studies on NN architecture/regularization and the effect of gating thresholds for “informative” data, would strengthen practical guidance. Finally, multi-objective design formulations (balancing information for $\theta_G$ and $\theta_{NN}$, cost, and identifiability) and constrained sensor-trajectory optimization could improve performance in realistic sequential sensing scenarios.",2502.05372v2,https://arxiv.org/pdf/2502.05372v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:25:44Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (examples include 1D, 10D, 100D design vectors; and 1D time/design variables)",Healthcare/medical|Other,Simulation study,TRUE,Python|Other,Public repository (GitHub/GitLab),NA,"The paper links simulation-based inference (SBI) and Bayesian optimal experimental design (BOED) by optimizing mutual-information (MI) lower bounds using normalized generative surrogate likelihoods (e.g., conditional normalizing flows), enabling design optimization even with non-differentiable simulators. It introduces a regularized InfoNCE-style MI objective (INCE-λ) that stabilizes simultaneous training of the amortized likelihood and optimization of designs, and it proposes using a parameterized design distribution (truncated Normal) plus design checkpoints to avoid poor local optima/sparse-reward issues in gradient-based design. The method targets designs that maximize expected information gain (EIG), i.e., MI between parameters and observations conditional on the design, to improve downstream posterior accuracy and calibration. Empirical evaluations include SBI benchmarks and BOED tasks (noisy linear model with up to 100 design dimensions, an SIR epidemiology model, and a non-differentiable BMP biology simulator), showing improved inference calibration/accuracy versus BOED baselines and highlighting that higher EIG does not always yield better predictive performance. Overall, it advances likelihood-free/implicit-model BOED by unifying amortized SBI training with BOED optimization through MI bounds without requiring differentiable simulators.","The BOED objective is expected information gain $I(\xi)=\mathbb{E}_{p(y\mid \xi)}[H[p(\theta)]-H[p(\theta\mid y,\xi)]]$, equivalent to mutual information $\mathrm{MI}(\theta;y\mid \xi)=\mathbb{E}_{p(\theta)p(y\mid \theta,\xi)}\left[\log \frac{p(y\mid \theta,\xi)}{p(y\mid \xi)}\right]$. They optimize an InfoNCE-style MI lower bound using a surrogate likelihood $p_\phi(y\mid \theta,\xi)$: $L_{\mathrm{NCE}}(\xi,\phi;L)=\mathbb{E}\left[\log \frac{p_\phi(y\mid \theta_0,\xi)}{\frac{1}{1+L}\sum_{\ell=0}^L p_\phi(y\mid \theta_\ell,\xi)}\right]$, and a regularized variant $L_{\mathrm{NCE}-\lambda}=L_{\mathrm{NCE}}+\lambda\,\mathbb{E}[\log p_\phi(y\mid \theta_0,\xi)]$ (INCE-λ). Designs may be sampled from a parameterized truncated Normal $\xi\sim \mathcal{N}_{\mathrm{trunc}}(\mu_\xi,\sigma_n^2)$ and optimized by gradients through the surrogate model.","On a noisy linear model BOED task, the estimated EIG increases with the number of design dimensions (reported for 1D, 10D, and 100D designs), and adding $\lambda$ regularization improves training stability in high-dimensional design spaces at a slight cost in EIG. In the SIR epidemiology task (T=2 rounds), differentiable baselines achieve higher EIG (~2.67–2.69) but SBI-BOED achieves better calibration/accuracy metrics (lower L-C2ST and lower median distance), with SBI-BOED EIG ranging roughly 1.01–1.63 depending on $\lambda$. In the non-differentiable BMP simulator task (T=3), SBI-BOED outperforms MINEBED-BO on EIG (~10.38–10.39 vs 9.05) and dramatically improves L-C2ST (~0.001–0.004 vs 0.23) and median distance (~0.60–0.61 vs 0.87). An ablation shows naive gradient design optimization without a design distribution can fail to find high-EIG designs in the first SIR round, while using a design distribution succeeds.","They note that naive gradient-based design optimization can fail in sparse-reward/low-signal regions and motivate the use of a design distribution and checkpoints to mitigate local minima. They report that mutual-information optimization does not avoid mode collapse in the Two Moons likelihood-based SBI benchmark, attributing it to deficiencies/bias in marginal likelihood approximation and finite contrastive sampling. They also mention practical difficulty sampling from the product likelihood with simple NUTS MCMC for the SIR/BMP settings beyond their chosen rounds (e.g., $T>2$), suggesting more sophisticated samplers may be needed.","The method’s performance depends on the fidelity and calibration of the surrogate likelihood $p_\phi(y\mid \theta,\xi)$; when the surrogate is misspecified or suffers training pathologies, the resulting EIG gradients may be misleading and yield suboptimal designs. The InfoNCE family is known to be biased/upper-bounded by $\log(L+1)$ and sensitive to the choice of contrastive sample count and proposal; comparisons may depend on fixing L and architecture/hyperparameters fairly across baselines. Real-world practicality may be limited when simulator calls are very expensive or when designs are constrained/discrete, since the approach relies on repeated simulation and gradient-based optimization of design distributions (potentially requiring careful reparameterizations for constraints/discreteness).","They suggest future work should incorporate calibration and predictive accuracy directly into the experimental-design optimization objective rather than relying only on EIG/MI. They also note that more sophisticated design distributions/policy-network parameterizations (akin to RL policies) could replace their simple truncated Normal design distribution. They point to extending the surrogate likelihood beyond normalizing flows to other likelihood-based generative models (e.g., diffusion/flow-matching) that may handle higher-dimensional data better and offer additional design-optimization flexibility.","Developing self-starting/robust BOED variants that handle model misspecification, heavy-tailed noise, and autocorrelated observations would improve applicability to real experiments. Extending the approach to discrete/combinatorial design spaces (e.g., categorical interventions) with appropriate gradient estimators or relaxations would broaden use in lab automation and screening. Providing open-source implementations with standardized benchmarks and ablation suites (including wall-clock cost and simulator-call budgets) would strengthen reproducibility and practitioner adoption.",2502.08004v1,https://arxiv.org/pdf/2502.08004v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:26:28Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Not applicable,"Variable/General (two-sided platform setting; items partitioned into Treated/Untreated/Placebo with probability p, plus relevance threshold r; user/query randomized to QA/QB)",Service industry|Other,Simulation study,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/2502.09806v1|https://kaggle.com/competitions/expedia-personalized-sort|https://eng.lyft.com/experimentation-in-a-ridesharing-marketplace-b39db027a66e|https://doordash.engineering/2019/02/20/experiment-rigor-for-switchback-experiment-analysis/|http://www.jstor.org/stable/1806070|http://www.jstor.org/stable/23364965|https://arxiv.org/abs/2109.11647|https://arxiv.org/abs/1305.6979,"The paper proposes a new marketplace experimentation scheme, Two-Sided Prioritized Ranking (TSPR), to estimate the Total Average Treatment Effect (TATE) of item-side interventions under within-listing interference in two-sided platforms. Users/queries are randomized into two groups (QA, QB), while items are partitioned into Treated, Untreated, and Placebo subsets; the recommender system is modified to prioritize Treated items for QB and Untreated items for QA, while keeping access to all items and consistent item treatment status across users. The design exploits position bias and uses partial outcomes (cumulative outcomes of the first l ranked items) to construct an estimator for TATE, with a relevance threshold r to avoid surfacing low-quality matches. Evaluation uses Monte Carlo simulations built from an Expedia hotel search impressions dataset and a semi-synthetic click/booking model; across 500 runs (with p=0.25), TSPR closely recovers the simulated ground-truth TATE. A baseline item-side (Horvitz–Thompson-style) estimator substantially overestimates the magnitude of TATE due to unaddressed interference, while TSPR yields confidence intervals that more often contain the ground truth.","TATE is defined as $\theta = \mathbb{E}[Y\mid \forall i\in I: T_i=1]-\mathbb{E}[Y\mid \forall i\in I: T_i=0]$, where $Y=\sum_i y_i$ is the query-level outcome. The method defines partial outcomes $Y_l=\sum_{i=1}^l y_i$ and partial treatment effects $\theta_l$, and assumes $\theta_l=\frac{\mathbb{E}[Y_l\mid \forall i:T_i=0]}{\mathbb{E}[Y\mid \forall i:T_i=0]}\,\theta$ to relate partial exposure to the full TATE. This yields an estimand linking observable differences in partial outcomes between QA and QB (for block size $l$) to $\theta$, and a frequency-weighted aggregate estimator (Eq. 6) with bootstrap standard errors.","In simulations, the platform-wide treatment (applied to all items) reduces conversion by 0.05, giving a ground-truth TATE of about -0.05. Under TSPR with 25% treatment coverage (p=0.25) and relevance threshold r=1.7, across 500 simulated experiments the average estimated TATE is -0.047 with average bootstrapped SE 0.016. The naive/item-side estimator reports an average effect of -0.091 with average SE 0.014 (roughly double the true magnitude), indicating substantial bias from interference. Sensitivity analysis shows TSPR estimates are relatively stable for moderate changes in r, but higher r increases standard errors by shrinking feasible treated/untreated block sizes.","The authors note that their evaluation is based on semi-synthetic simulations rather than live deployments, so additional real-world validation across platforms would strengthen generalizability. They also explicitly assume negligible cross-user (user-side) interference (e.g., slack supply) and focus on item-side interventions; extending the approach to user-side interventions or settings with significant cross-user interference is described as an open challenge.","The estimator relies on strong structural assumptions about how treatment impact scales with partial outcomes (Assumption 1) and on the approximation that the no-treatment partial outcome can be substituted by an untreated-top-block outcome; these may fail if substitution/complementarity patterns are highly nonlinear or if lower-ranked items still meaningfully affect outcomes. Practical implementation requires modifying ranking logic and selecting a relevance threshold r; miscalibration could change marketplace composition at the top of the list in ways that affect outcomes beyond treatment (algorithmic confounding), especially if relevance scores are themselves influenced by treatment-related features. The paper does not report performance under autocorrelated user behavior, repeated queries/users, or endogenous seller responses, which are common in marketplaces and could alter interference patterns and long-run effects.",The authors suggest validating TSPR with real-world data/experiments on diverse platforms to assess practical implementation and generalizability. They also propose extending the design beyond item-side interventions to user-side interventions and to environments with meaningful cross-user interference.,"Develop self-starting/online calibration procedures for choosing p and r (and possibly the prioritized depth) to balance user-experience constraints with statistical efficiency, and characterize finite-sample bias/variance under misspecification of Assumption 1. Extend the framework to dynamic, equilibrium, or strategic-response settings (e.g., sellers adjusting prices/availability) and to repeated-query users, where interference and treatment effects may evolve over time. Provide open-source reference implementations and integration guidance for common ranking stacks, along with diagnostics to detect when TSPR assumptions (position bias decay, sufficient inventory per query, negligible cross-user interference) are violated.",2502.09806v1,https://arxiv.org/pdf/2502.09806v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:27:11Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,D-optimal,Variable/General (examples shown for 1D and 2D; discussion for general m and designs of size 2^m+1),Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|None / Not applicable,Not provided,https://arxiv.org/abs/2502.12753|https://github.com|https://arxiv.org/abs/2203.06877|https://dl.acm.org/doi/10.5555/3600270.3601418|https://doi.org/10.1016/j.inffus.2019.12.012|https://doi.org/10.1016/j.suscom.2023.100857|https://doi.org/10.1016/S0378-3758(99)00018-X|http://jmlr.org/papers/v24/22-0142.html|https://doi.org/10.1017/S0962492924000023|https://doi.org/10.1088/1742-6596/2171/1/012025|https://doi.org/10.2139/ssrn.3196985|https://doi.org/10.3390/e23010018|https://doi.org/10.1002/hast.973|https://doi.org/10.1007/978-3-031-35918-7|https://christophm.github.io/interpretable-ml-book|https://doi.org/10.1016/S0378-3758(95)00197-2|https://doi.org/10.1016/j.neucom.2024.128282|https://doi.org/10.1093/biomet/79.4.763|https://doi.org/10.1145/2939672.2939778|https://doi.org/10.1016/j.neucom.2024.127969|https://doi.org/10.1145/3381831|https://doi.org/10.1007/978-3-030-33607-3_49|https://doi.org/10.1145/3375627.3375830|https://arxiv.org/abs/2006.05714|https://doi.org/10.1080/01605682.2020.1865846|https://doi.org/10.1080/01621459.2020.1863221|https://doi.org/10.1007/s11222-024-10436-2|https://doi.org/10.3390/make3030027|https://doi.org/10.1145/3447548.3467274|https://eprints.soton.ac.uk/361127/,"The paper proposes “Green LIME,” a modification of LIME that replaces random neighborhood sampling with optimal design of experiments to reduce the number of expensive primary-model evaluations needed for a local linear surrogate explanation. The core DOE contribution is a D-optimal design formulation for locally weighted (kernel-weighted) local linear regression around the reference point, with a small fixed “regularizing” weight at the center point to avoid impractical boundary-supported designs. In 1D, the resulting optimal approximate design is a symmetric three-point design (−u, 0, +u), where u is chosen by minimizing a closed-form D-criterion depending on kernel width; for larger required sample sizes, the design is implemented via allocation/rounding plus “jittering” (adding small Gaussian noise) around support points. The method is evaluated on an illustrative regression example by repeated runs and metrics (NWISE and weighted correlation) showing comparable or improved local fidelity and improved stability versus standard LIME at the same sample size, implying computational/energy savings by needing fewer function evaluations for similar explanation quality. The work positions DOE as a general tool for efficient data collection in XAI, with discussion of extensions to higher dimensions (e.g., 2^m+1 support) and other data types (e.g., superpixel-based image LIME).","The local surrogate is a kernel-weighted linear regression around a reference point: minimize $\sum_{i=1}^N \lambda_{\kappa,i}(y_i-h(z_i)^T\theta)^2$ with weights $\lambda_{\kappa,i}=K_{\kappa}(d(z_i,z_0))$ and default RBF kernel $K_{\kappa}(d)=\exp(-d^2/(2\kappa^2))$. For DOE, an approximate design $\xi=\{(z_q,p_q)\}$ yields information-like matrices $M_{11}(\xi)=\sum p_q\lambda_{\kappa,q}h(u_q)h(u_q)^T$ and $M_f(\xi)=\sum p_q\lambda_{\kappa,q}^2h(u_q)h(u_q)^T$, and the (normalized) MSE matrix $R(\xi)=M_{11}^{-1}M_f M_{11}^{-1}$; the D-criterion is $\Psi_D=\log|R(\xi)|$. In 1D with a fixed center weight $\delta=1/(N+1)$, the D-criterion reduces to an explicit function $D(u,\kappa)$ (Eq. 4) for the symmetric 3-point design $\{-u,0,+u\}$, and for the RBF kernel it satisfies a scaling property so optimizing $D(u,1)$ suffices and the solution rescales as $u^*(\kappa)=\kappa u^*(1)$.","On the 1D illustrative example, explanations were generated for 11 reference points with 100 runs each, comparing standard LIME sampling to the proposed D-optimal (ODE) sampling at the same sample size (notably $N=11$ in the main comparison). Average results (Table 1) show ODE achieves lower NWISE (better weighted squared error) for most units and markedly higher weighted correlation for some cases (e.g., unit 0: CORR 0.8586 vs 0.9983; unit 5: 0.3547 vs 0.9587), with a noted boundary-effect anomaly at the last unit. Boxplots across sample sizes (11, 51, 501, 5001) indicate reduced variability (greater stability) as sample size increases and show the ODE approach is generally more stable than LIME because support points anchor sampling and jittering adds controlled dispersion. The method’s central practical result is that, by choosing informative design points rather than many low-weight random points, similar or better local fidelity can be achieved with fewer primary-model evaluations.","The authors note the approach assumes a well-specified kernel width $\kappa$; kernel-width selection itself is not addressed, and poor neighborhood specification can harm both LIME and the proposed method (e.g., increasing sample size can worsen NWISE under misspecification). They also emphasize that extending the pure optimal-design approach to higher dimensions becomes computationally challenging, and that more elaborate methods to cope with dimensionality are left for future work. The illustrative example’s smooth surface limits the apparent benefit of jittering; the authors argue differences should be more pronounced in realistic applications.","The empirical evaluation is largely confined to an illustrative (synthetic) 1D regression setting; there is no real-world case study demonstrating wall-clock/energy savings or explanation utility on practical tabular ML models. The proposed jittering procedure introduces extra hyperparameters (e.g., jitter variance) and rounding/allocation choices that can materially affect results, but sensitivity analyses and guidance for selecting these settings are not developed. The design theory uses local linear surrogates with kernel weights and largely sidesteps bias/model-misspecification analysis for deterministic primary models, so there is limited theoretical guarantee of explanation fidelity when the primary model is strongly nonlinear or discontinuous in the neighborhood. Finally, although LIME commonly includes feature selection and (often) ridge regularization, the DOE method is not fully integrated with these practical LIME variants, potentially limiting direct drop-in applicability.","They propose extending the method to other data modalities, especially image LIME where superpixels correspond to binary/dummy variables, enabling DOE over 0/1 perturbations. They discuss generalization to higher dimensions via structured symmetric designs (e.g., points in the Cartesian product $\{-u,u\}^m$ plus a center point, with support size $2^m+1$) and suggest leveraging rotational invariance/robustness, conjecturing smaller designs may achieve similar criterion values. They also mention extending to feature-selection settings (e.g., fitting penalized models such as lasso) and exploring joint designs that are optimal for multiple reference points simultaneously.","Develop adaptive/self-tuning procedures that jointly choose the kernel width $\kappa$, sample size, and jitter variance based on a small pilot design, to provide a fully automated “green” LIME pipeline with performance guarantees. Extend the optimal-design construction to constrained or mixed-variable perturbations typical in tabular ML (categorical levels, bounds, feasibility constraints) and to distribution-aware designs that better match the data manifold to mitigate out-of-distribution perturbation issues. Provide broader benchmarking on standard tabular datasets and black-box models (GBDTs, neural nets) with runtime/energy measurements and comparisons against stabilized/deterministic LIME variants and other efficient explainers. Finally, analyze robustness under dependence among features (non-diagonal covariance) and autocorrelation/structured inputs, since LIME’s and the paper’s default sampling assumptions can be unrealistic in practice.",2502.12753v2,https://arxiv.org/pdf/2502.12753v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:28:27Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Not applicable,Variable/General (sequential RCT treatment probability; contextual multigroup extension with |G| groups),Healthcare/medical|Service industry|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/amazon-science/adaptive-abtester,"The paper develops adaptive, sequential randomized controlled trial (RCT) designs for unbiased average treatment effect (ATE) estimation in the design-based (finite-population) potential outcomes framework, using inverse-probability-weighted (IPW) estimators with adaptively chosen Bernoulli treatment probabilities. It strengthens prior ClipOGD guarantees by proposing ClipOGD (a strongly-convexity-exploiting variant) that achieves anytime expected Neyman regret (o(log T)) under boundedness and lower-bound assumptions on potential outcomes, improving over the prior (o(T)) rate. In the contextual setting with pre-treatment covariates, it introduces multigroup Neyman regret and proposes MGATE, a sleeping-experts-style aggregation of per-group adaptive designs that guarantees anytime (o(T)) multigroup regret simultaneously over a family of possibly overlapping groups. Theoretical results provide regret bounds and convergence of adaptive propensities toward the hindsight-optimal nonadaptive propensity, plus conditions for conservative confidence intervals for the adaptive IPW estimator. Empirical studies on synthetic data and several real datasets (e.g., microfinance and online experiments/LLM benchmarking) show reduced Neyman regret and more stable convergence compared to the baseline ClipOGD0.","ATE: $\tau_T=\frac{1}{T}\sum_{t=1}^T(y_t(1)-y_t(0))$. Adaptive IPW estimator: $\hat\tau_T=\frac{1}{T}\sum_t Y_t\left(\frac{Z_t}{p_t}-\frac{1-Z_t}{1-p_t}\right)$. Per-round Neyman objective (variance surrogate): $f_t(p)=\frac{y_t(1)^2}{p}+\frac{y_t(0)^2}{1-p}$ and Neyman regret: $\mathrm{Reg}^\mathrm{Var}_T=\max_{p^*\in(0,1)}\sum_{t=1}^T\big(f_t(p_t)-f_t(p^*)\big)$; ClipOGD updates use projected gradient steps with clipping $p_t\in[\delta_t,1-\delta_t]$ and gradient estimate based on observed $Y_t$ and $Z_t$.","Under Assumption 3.1 (uniform boundedness and a positive lower bound on outcome magnitudes/second moments), ClipOGD achieves an anytime regret bound $\mathbb{E}[\mathrm{Reg}^\mathrm{Var}_T]=O\big(h(T)^5\log T+(h^{-1}(1+C/c))^2\big)$, yielding \~$O(\log T)$ by choosing slowly growing $h$. The design probabilities converge in squared mean to the hindsight-optimal nonadaptive propensity under mild additional conditions, with a stated rate $\tilde O(\log T/T)$ in an i.i.d. superpopulation special case. In the contextual multigroup setting, MGATE attains $\mathrm{Reg}^{\mathrm{Var}}_{\mathrm{MG},T}=O\big(\sqrt{|\mathcal G|}\,h(T)^5\sqrt{T}\big)$, i.e., sublinear regret simultaneously for all predefined (possibly overlapping) groups. Experiments on synthetic and real datasets show lower and more rapidly vanishing Neyman regret for ClipOGD versus ClipOGD0, and lower group-conditional regret for MGATE in contextual trials.","The authors note that choosing the clipping function $h$ can be nontrivial: slower-growing $h$ improves the asymptotic term but can make the inverse term $h^{-1}(\cdot)$ large, and convergence can be slow when the problem is poorly conditioned (large $C/c$) with optimal propensities near the boundary. They also state that they only prove conservative Chebyshev-type confidence intervals (not Wald-type), leaving asymptotic normality/Wald CIs for future work. They further emphasize that their illustrative contextual groups in experiments are synthetic/post-hoc and rely on information that would be unobservable in real ATE experiments.","The strongest $\tilde O(\log T)$ regret guarantee relies on strengthened boundedness/lower-bound assumptions on potential outcomes; performance and guarantees may degrade when outcomes are heavy-tailed, heteroskedastic, or near-zero in ways that weaken strong convexity. The methods are presented for binary treatment/control with Bernoulli assignment and IPW estimation; extensions to multi-arm trials, clustered/split-plot constraints, interference, or noncompliance are not addressed. Practical deployment would also need guidance for selecting $h$ and for handling delayed outcomes or covariate shift; while mentioned conceptually, these are not systematically evaluated across realistic operational constraints. The GitHub link is provided, but the paper does not specify the exact software environment/language, dependencies, or a reproducibility checklist within the text provided.",They express hope that future work will further explore the well-conditioning properties and tradeoffs involved in choosing the clipping function (and the role of $C/c$ and boundary-optimal propensities). They also state they hope future work will study multigroup adaptive designs beyond the sequential finite-population setting considered in the paper. They suggest exploring Wald-type confidence intervals/asymptotic normality of the adaptive IPW estimator as a future direction.,"Develop self-normalized or robust (e.g., truncation/Huberization) variants to improve performance under heavy-tailed outcomes and weaker moment assumptions while retaining regret guarantees. Extend the framework to multi-arm and factorial treatment structures, and to settings with operational constraints (batching, delayed feedback, cluster randomization, or interference) common in field experiments. Provide principled, data-adaptive tuning rules for clipping/step sizes (or parameter-free variants) with finite-sample guidance and diagnostic checks. Add broader real-world validations and packaged software (e.g., an R/Python library with tutorials) plus standardized benchmarks for comparing adaptive ATE-efficient designs.",2502.17427v1,https://arxiv.org/pdf/2502.17427v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:29:29Z
TRUE,Optimal design|Sequential/adaptive,Parameter estimation|Prediction,Other,"Variable/General (design variables are sensor/illumination locations; parameters are high-dimensional PDE fields, e.g., nm=4225 and nm=3969 in examples)",Healthcare/medical|Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable|Other,Public repository (GitHub/GitLab),https://github.com/DeepTransport/deep-tensor|https://github.com/fastfins/fastfins.m,"The paper proposes a fast framework for sequential optimal experimental design (sOED) in Bayesian inverse problems with expensive PDE-based forward models and very high-dimensional parameters. The design objective is to choose experiments greedily by maximizing incremental expected information gain (iEIG), but instead of estimating iEIG via costly nested Monte Carlo, the authors derive a derivative-based upper bound on iEIG expressed via a log-determinant of an average Fisher-information matrix after a transport-map preconditioning. The same bound yields likelihood-informed subspaces (LIS) that reduce parameter dimension, and the framework combines this with conditional triangular measure transport (Knothe–Rosenblatt maps) built using tensor-train density surrogates to enable amortized, stage-wise posterior inference. A restart strategy periodically rebuilds transport maps using a data-dependent LIS to control accumulated approximation error in long sequential campaigns. Numerical PDE examples—diffusivity field estimation with optimal sensor placement and a simplified photoacoustic imaging problem with joint laser/sensor placement—show that designs maximizing the bound closely match nested-MC iEIG optima and outperform random designs, while remaining computationally scalable.","The stage-k design objective is the incremental expected information gain: $\Psi_k(e_k)=\mathbb{E}_{y_k\mid e_k,H_{k-1}}\big[D_{\mathrm{KL}}(\pi(\cdot\mid e_k,y_k,H_{k-1})\Vert \pi(\cdot\mid H_{k-1}))\big]$. Using a transport map $T_{k-1}$ that maps the stage-(k−1) posterior to a standard Gaussian, the iEIG is bounded by a log-determinant of an average Fisher information matrix in transformed coordinates: $\Psi_k(e_k)\le \Psi^{\mathrm{UB}}_k(e_k)=\tfrac12\log\det(I+H_I(e_k,T_{k-1}))$, with Monte Carlo estimator $H_I(e_k,T_{k-1})\approx \tfrac1N\sum_i (\nabla T_{k-1}(w^{(i)}))^\top \nabla G(e_k,m^{(i)})^\top \Gamma_\eta(e_k)^{-1}\nabla G(e_k,m^{(i)})\nabla T_{k-1}(w^{(i)})$. Designs are chosen greedily as $e_k^*\in\arg\max_{e_k}\Psi^{\mathrm{UB}}_k(e_k)$, and the leading eigenspace of $H_I$ defines a (data-free) LIS used for subspace-accelerated conditional transport.","In Problem 1 (elliptic PDE diffusivity inversion) with 121 candidate sensor locations and parameter dimension $n_m=4225$, an 8-stage sequential design using the bound (with $N=100$ Monte Carlo samples) produces iEIG contours that closely align with a nested Monte Carlo iEIG reference (reported with $N=10{,}000$) and largely agrees on selected optimal locations beyond the first stage. In Problem 2 (photoacoustic imaging) with parameter dimension $n_m=3969$ and designs combining illumination boundary choice ($e_1\in\{0,3\}$) and one of 50 sensors, a 5-stage design using $N=500$ samples consistently yields higher nested-MC-estimated iEIG than 50 random designs per stage, with near-zero iEIG when sensor and laser are placed on opposite boundaries. The paper reports LIS dimensions on the order of ~9–19 in the examples (stage-dependent), and uses reduced-order models (ROMs) to accelerate transport-map construction (e.g., ~40× speedup in Problem 1 ROM, ~130× in Problem 2 ROM). The Gaussian-approximation-based comparator matches early-stage designs but diverges in later stages when posteriors become more non-Gaussian, with lower achieved KL divergence than the proposed approach.","The authors note that in long sequential campaigns, recursive use of approximate (especially conditional) transport maps can accumulate error, making it expensive to maintain accuracy across stages and increasing the cost of sampling and Jacobian evaluation as map layers grow. They also state that relying solely on the data-free LIS may overestimate the number of important directions, which can reduce efficiency. They therefore introduce a periodic restart step that rebuilds a transport map from scratch using a data-dependent LIS to improve accuracy and stability.","The methodology relies on access to derivatives (Jacobians/adjoints) of the PDE forward map to form Fisher-information-based objectives; in applications without adjoints or with non-differentiable simulators, applicability may be limited. Performance and robustness under model misspecification (e.g., non-Gaussian or correlated noise, PDE discretization error, or misspecified priors) are not systematically explored, even though the design criterion is information-theoretic and sensitive to likelihood assumptions. Comparisons focus on nested Monte Carlo and a specific Gaussian-approximation method; broader baselines (e.g., other mutual-information surrogates, variational/flow-based OED, or nonmyopic sOED) and wall-clock/runtime comparisons across methods are limited. The design space is largely discrete candidate sets (sensor grids); extension and demonstrated performance for continuous design optimization (with constraints) is not fully developed in the examples.","The paper suggests further exploration of a hybrid strategy: using Gaussian-approximation-based design in early stages when the posterior remains close to Gaussian, then switching to transport-map-based design as the posterior becomes more non-Gaussian. It also notes that computational cost could be further reduced by using surrogate models for the forward map (e.g., neural-network surrogates), beyond the ROM approach demonstrated. These directions are framed as future investigations in the discussion of numerical results and algorithm scalability.","Extending the framework to handle model/likelihood misspecification (e.g., unknown noise levels, correlated noise, or Bayesian approximation error models) would increase practical reliability of information-gain-based designs. Developing continuous-design optimization (gradient-based over sensor locations/times) with constraints and uncertainty quantification of the bound approximation could broaden applicability beyond discrete candidate sets. Providing open-source end-to-end scripts (including PDE setups and reproducible experiments) and benchmarking against additional modern sOED methods (e.g., reinforcement-learning amortized design, variational MI estimators, normalizing-flow OED) would clarify tradeoffs in accuracy vs. compute. Investigating nonmyopic/lookahead variants using the same transport/LIS machinery could improve performance when budgets are limited and greedy designs are suboptimal.",2502.20086v1,https://arxiv.org/pdf/2502.20086v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:30:21Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Optimization|Cost reduction|Other,Not applicable,Variable/General (study varies problem dimension d; test problems include d=1 and d=2; experimental budgets expressed as 15d–50d and initial samples 2d/5d/10d),Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper studies Bayesian optimization (BO) as a sequential/adaptive design strategy for physical experiments with intrinsically noisy response surfaces, focusing on how modeling and decision choices affect efficiency under tight experimental budgets. It runs a large simulation experiment varying controllable BO/GP settings (initial DoE size, replication, acquisition function, covariance/kernel choice) and noise/problem conditions (dimension, noise magnitude, and heteroscedastic noise form) across three benchmark objective functions with additive Gaussian noise. Performance is evaluated by the GAP between the found optimum and the true global optimum at multiple budget levels (25d, 37.5d, 50d), with multiple random restarts/replications per configuration. Key findings include consistently superior performance of the Matérn covariance over Gaussian and power-exponential kernels, and strong dependence of optimal BO settings on available experimental budget (notably for initial sample size, use of replicates, and acquisition function choice). The work provides practical recommendations for configuring BO for noisy optimization and highlights that conventional acquisition functions may over-exploit when observation noise inflates posterior uncertainty, motivating improved Bayesian treatment of GP hyperparameters or new acquisition functions for noisy/sparse regimes.","The BO procedure selects the next experiment by maximizing an acquisition function: $x^{new}_t = \arg\max_{x\in\chi} \alpha(x\mid D)$ (Eq. 7), where $D$ is the accumulated data. The GP observation model is $y(x_i)=f(x_i)+\varepsilon_i$ with $\varepsilon_i\sim\mathcal N(0,S(x_i))$, yielding $Y\sim\mathcal N(M\beta, K_n+\Sigma_n)$; under homoscedastic noise $\Sigma_n=\tau I$ and hyperparameters are fit by MLE (Eq. 2), with posterior mean/variance given in Eqs. (5)–(6). Acquisition functions evaluated include upper confidence (Eq. 8), probability of improvement (Eq. 9), modified expected improvement for noisy settings (Eq. 10), knowledge gradient (Eq. 11), and predictive entropy search (Eq. 12); kernels compared include Gaussian (Eq. 1), power exponential (Eq. 13), and Matérn (Eq. 14).","Across 18,225 total simulation runs (3 objective functions × 36 controllable/noise combinations × 5 acquisition functions? as configured, repeated 5 times), the Matérn covariance function is reported to outperform Gaussian and power exponential kernels on all test problems. Main-effects and interaction plots show the experimental budget strongly influences which controllable settings work best: larger initial designs (e.g., 10d vs 2d/5d) generally reduce GAP, and replication becomes more beneficial as the total budget increases. For acquisition choice, PI tends to perform best under smaller budgets, while UCB with a relatively large exploration parameter (e.g., $\pi=5$) becomes more appropriate under larger budgets. Higher noise magnitude worsens GAP, and heteroscedastic noise that is minimized near the global optimum (“good”) yields better optimization outcomes than “bad” noise (maximized near the optimum) or homoscedastic noise.","The authors assume specific functional forms for experimental uncertainty (homoscedastic or variance proportional to the response) and focus on normally distributed noise, cautioning that conclusions may not transfer when the noise distribution differs. They do not study batch (parallel) acquisition, noting this could matter for performance but is less practical under very small experimental budgets. They fit GP hyperparameters via maximum likelihood, so uncertainty in hyperparameters is not propagated into the posterior predictive distribution, which may contribute to acquisition functions over-emphasizing exploitation in noisy/small-budget settings.","The study uses only low-dimensional benchmarks (explicitly d=1 and d=2); findings may not generalize to moderate/high-dimensional BO where kernel learning and acquisition optimization are harder and where different initialization/space-filling strategies dominate. The initial DoE is described as “uniformly distributed,” but the exact design generator (e.g., LHS vs Sobol vs random) is not fixed/benchmarked, which can materially affect BO performance under small budgets. Comparisons are restricted to GP-based BO with a small set of kernels/acquisitions; strong baselines for noisy optimization (e.g., trust-region BO, Thompson sampling variants, replicated/noise-aware EI variants, robust objective formulations) are not reported, limiting strength of practical superiority claims.","The authors suggest addressing the apparent over-exploitation of conventional acquisition functions in noisy settings by (i) using a Bayesian approach that accounts for uncertainty in GP hyperparameters and/or (ii) developing problem-specific acquisition functions tailored to noisy response surfaces. More broadly, they call for new statistical methods for optimizing noisy response surfaces in sparse-data, small-budget scenarios to make adaptive selection of physical experiments more systematic.","Provide validated guidance on initial design construction by explicitly comparing space-filling generators (LHS, Sobol, maximin designs) and incorporating constraints typical of physical experiments (hard-to-change factors, discrete settings). Extend the simulation study to higher dimensions and to correlated/non-Gaussian noise (e.g., heavy-tailed, skewed, or temporally correlated errors) and evaluate robustness of recommendations. Release reference implementations and reproducible experiment scripts, and test recommendations on real laboratory datasets to confirm that GAP-based findings translate to practical experimental settings.",2503.00327v1,https://arxiv.org/pdf/2503.00327v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:31:00Z
TRUE,Sequential/adaptive|Bayesian design|Other,Model discrimination|Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (benchmarks with 8, 20, 27, and 37 nodes/variables)",Healthcare/medical|Finance/economics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://causalcoat.github.io/legit,"The paper studies experimental design for causal discovery in an online setting where interventions are expensive, and the goal is to choose intervention targets (variables/nodes) to efficiently identify a causal DAG. It proposes LeGIT (Large Language Model Guided Intervention Targeting), a hybrid framework that uses an LLM to “warm up” early intervention choices based on variable meta-descriptions (to avoid noisy/unstable numerical scores with limited interventional data), then switches to a numerical targeting method (primarily gradient-based intervention targeting, GIT) for later rounds. The intervention type used is single-node hard interventions, producing interventional distributions of the form $P_i(X)=\tilde P(X_i\mid PA_i)\prod_{j\neq i}P(X_j\mid PA_j)$. Across four benchmark Bayesian-network datasets (Asia, Child, Insurance, Alarm), LeGIT improves structure-learning quality versus CBED, AIT, GIT, random baselines, and a human baseline, reporting better SHD/SID and higher BSF under both standard and low interventional-sample budgets. Practically, the method positions LLMs as scalable assistants for early-stage experimental design in causal discovery, complementing rather than replacing established active-intervention strategies.","The online causal discovery loop iteratively selects an intervention target $I$ each round, queries interventional data, and refits a causal discovery model. Under a single-node hard intervention on $X_i$, the paper writes the induced interventional distribution as $P_i(X)=\tilde P(X_i\mid PA_i)\prod_{j\neq i}P(X_j\mid PA_j)$ (Eq. 1), where the mechanism for $X_i$ is replaced. LeGIT’s targeting rule is staged: early rounds use an LLM-proposed list of intervention targets (warmup and a bootstrapped pass over “isolated/missing” nodes), then later rounds select targets via a numerical method (e.g., GIT) once the estimated graph is clearer.","With T = 33 rounds and batch size $|D^{I}_{int}|=32$ (N = 1056 interventional samples), LeGIT achieves the best average SHD on all four benchmarks (Alarm 17.4, Insurance 12.6, Child 2.2, Asia 0.8) and generally improves SID and BSF versus GIT and other baselines (e.g., Insurance SID 200.6 vs GIT 243.8; BSF 0.8205 vs 0.7960). In the low-data setting $|D^{I}_{int}|=16$ (N = 528), LeGIT also improves over GIT on SHD across all datasets (e.g., Alarm 21.0 vs 27.2; Insurance 18.2 vs 22.4; Child 4.4 vs 6.0; Asia 1.4 vs 1.8) and shows stronger gains under constrained budgets. The paper also reports that LeGIT can surpass a human baseline on the more complex datasets (Alarm and Insurance) in the main setting.","The authors restrict the study to hard (single-node) interventions “for simplicity and consistency,” noting that soft interventions are beyond the paper’s scope and left for future work. They also note intrinsic LLM limitations such as limited context length and hallucination, motivating a bootstrapped stage and a later switch to numerical methods once the graph becomes more complex. They state that after an initial clearer graph is obtained, even for humans it becomes difficult to determine the best experimental design, so LeGIT relies on numerical targeting thereafter.","The evaluation is confined to four standard Bayesian-network benchmarks from a repository; it is unclear how robust LeGIT is to real experimental constraints such as imperfect interventions, measurement error, hidden confounding, or feedback/temporal dynamics. The method depends on LLM prompting quality and on access to a specific proprietary model (GPT-4o), and the paper does not quantify sensitivity to prompt phrasing, model choice, or stochasticity beyond self-consistency. While comparisons include CBED/AIT/GIT, the work does not establish optimality guarantees for the LLM-guided target list relative to formal utility criteria (e.g., mutual information), nor does it provide cost/latency analysis for LLM calls in real-time experimentation.","The paper explicitly notes that soft interventions (which modify mechanisms without removing dependencies) are outside the current scope and “may be explored in future research.” It also suggests, via discussion of LLM limitations (context length and hallucination), that improving robustness/extraction of LLM knowledge is important and motivates further exploration of how to better leverage LLM guidance beyond early-stage warmup.","A natural extension is to generalize LeGIT to soft and imperfect interventions and to settings with unknown intervention effects, aligning more closely with practical lab/field experimentation. Another direction is to develop principled hybrid criteria that fuse LLM priors with information-theoretic or Bayesian utilities (e.g., MI/BALD) throughout all rounds rather than a hard stage switch. Additional work could assess robustness under non-i.i.d. data (autocorrelation, nonstationarity), latent confounding, and high-dimensional graphs, and provide open-source implementations plus standardized benchmarks to improve reproducibility and practitioner adoption.",2503.01139v2,https://arxiv.org/pdf/2503.01139v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:31:42Z
TRUE,Sequential/adaptive|Bayesian design|Computer experiment|Optimal design|Other,Prediction|Parameter estimation|Cost reduction|Other,Other,Variable/General (input is long-term environmental vector x; dimension not specified; surrogate parameters θ(x) can be multivariate),Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a design-of-experiments framework for estimating long-term extreme structural responses (return-period quantiles) when the short-term response model is a black-box stochastic simulator with heteroscedastic, potentially non-Gaussian output noise. A parametric conditional response distribution \(\hat g_{Y\mid\theta}(y\mid\theta)\) (e.g., Weibull) is assumed, with its parameter vector \(\theta(x)\) modeled over environmental inputs \(x\) via Gaussian-process regression using noisy parameter observations obtained from simulator-driven MLE and Fisher-information covariance. The quantity of interest is a percentile of the \(N_y\)-year maximum response distribution, and the DOE chooses new environmental conditions by minimizing an acquisition function equal to the expected posterior variance of the estimated extreme quantile after adding one new experiment. To make the Bayesian sequential design computationally feasible, the authors combine (i) a finite-dimensional GP “frozen realization” approximation, (ii) the unscented transform for uncertainty propagation, and (iii) importance sampling over environmental states to approximate the long-term marginal response and its extreme-value distribution without brute-force surrogate Monte Carlo. The method is positioned as enabling acquisition functions and sequential designs that would otherwise be prohibitively expensive for long time horizons, particularly in structural/ocean/offshore reliability contexts.","The long-term extreme is \(Y_{N_y}=\max\{Y_1,\ldots,Y_N\}\) with \(G_{N_y}(y)=G(y)^N\), and the QOI is the return-level quantile \(z=G_{N_y}^{-1}(p)\). Simulator-derived noisy observations of surrogate distribution parameters are modeled as \(\theta_{\mathrm{obs}}(x)=\theta(x)+\varepsilon(x)\) with \(\varepsilon(x)\sim\mathcal N(0,\Sigma(x))\), and a GP posterior over \(\theta(x)\) is propagated through the long-term marginalization \(\hat g(y\mid e)=\int \hat g_{Y\mid\theta}(y\mid \hat\theta_k(x\mid e)) f_X(x)\,dx\). The sequential Bayesian DOE uses the uncertainty objective \(H_k=\mathrm{var}[\hat z]\) and acquisition \(s_k(x)=\mathbb E_{P_k\times Q_k}[H_{k+1}]\), approximated via importance sampling and the unscented transform.","No numerical benchmark results are reported in the manuscript; the contribution is methodological and algorithmic. The paper argues that naive Monte Carlo for a 25-year extreme with 1-hour sea states would require about \(\approx 220{,}000\) surrogate evaluations per estimate, whereas the proposed importance-sampling-based approach can reduce this to a much smaller \(M\) (illustratively \(\approx 5{,}000\), problem-dependent) and makes \(M\) independent of the return period length \(N_y\). The authors hypothesize that this efficiency enables more sophisticated sequential DOE acquisition functions that were previously too costly and may yield more confident extreme-quantile estimates. Empirical validation against brute-force Monte Carlo is explicitly deferred to future work.","The authors note that the method approximates the marginalization needed to obtain \(G(y)\), and then raises this approximation to a large power \(N\); consequently, small approximation errors in \(G(y)\) can compound into much larger errors in the extreme distribution. They also state that results can be sensitive to the choice and tail fit of the assumed parametric conditional response distribution used in the surrogate model, especially in tail regions. They emphasize the need to further quantify the impact of these issues.","The approach relies on assuming a correct parametric family for \(Y\mid x\) and on asymptotic normality of MLE-based parameter estimates with Fisher-information covariance; in small-sample regimes or with model misspecification, the Gaussian observation model for \(\theta_{\mathrm{obs}}\) and its \(\Sigma(x)\) may be inaccurate. The “finite-dimensional GP freezing” approximation effectively enforces perfect correlation across inputs for a given latent \(U\), which can distort functional uncertainty representation and may bias the variance-of-quantile objective used in the acquisition. The heuristic for predicting \(\Sigma(x)\) for new candidate points (nearest-neighbor reuse) may be unreliable in regions where simulator noise changes rapidly, potentially mis-ranking acquisition values. The paper does not specify practical guidance for optimizing the acquisition over \(x\) (e.g., gradients, constraints, mixed variables), nor does it discuss behavior under dependent/serially correlated short-term maxima, which can violate the i.i.d. assumption in \(Y_1,\ldots,Y_N\).",The authors state that the next stage is to demonstrate performance empirically on a test problem that is representative of real applications but still computationally efficient enough to generate brute-force Monte Carlo reference results. They also suggest further exploration of how errors in the marginalization approximation and surrogate tail fit influence the final extreme-quantile estimates.,"Evaluate robustness to distributional misspecification (e.g., nonparametric or semi-parametric conditional tails, EVT-based conditional models) and to violations of independence/stationarity across short-term blocks. Develop principled models for \(\Sigma(x)\) (e.g., a secondary GP or heteroscedastic likelihood) and study how uncertainty in \(\Sigma(x)\) propagates into the acquisition. Compare the proposed UT-based propagation with alternatives (e.g., sparse GP sampling, quasi-Monte Carlo, analytic approximations) and quantify the impact of the “perfect-correlation” frozen-GP approximation on design choices. Provide open-source implementation and benchmarking across standard structural reliability test functions and higher-dimensional environmental inputs, including constrained design spaces and multi-objective QOIs (multiple return periods / multiple limit states).",2503.01566v1,https://arxiv.org/pdf/2503.01566v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:32:12Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,Other,Variable/General (examples: location-finding with 2D design coordinate; CES with 6 design dimensions for two 3-item baskets),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/yasirbarlas/RL-BOED|https://github.com/csiro-mlai/RL-BOED,"The paper studies reinforcement-learning (RL) methods for Bayesian optimal experimental design (BOED) in a sequential/adaptive setting, where a policy selects designs to maximize expected information gain (EIG). It formulates BOED as a hidden-parameter MDP and trains agents offline using the sequential prior-contrastive estimation (sPCE) lower bound on EIG as the RL reward, avoiding explicit posterior computation. It compares several SAC-based RL algorithms (REDQ, DroQ, SBR, SUNRISE) and a combined SUNRISE-DroQ variant, focusing on how well learned design policies generalize under distributional shift/model changes at deployment. Experiments on two simulated BOED tasks—2D location finding and a 6D constant-elasticity-of-substitution (CES) preference model—show that algorithm choice significantly impacts information-gain performance and robustness, with dropout/ensemble-based approaches often providing better generalization. The work provides empirical performance and training-time comparisons and recommends considering computational budget when choosing RL training algorithms for sequential design policies.","The design utility is expected information gain (EIG), expressed as $\mathrm{EIG}(\xi)=\mathbb{E}_{p(\theta)p(y\mid\theta,\xi)}[\log p(\theta\mid y,\xi)-\log p(\theta)]$. For a policy over $T$ experiments, $\mathrm{EIG}_T(\pi)=\mathbb{E}_{p(\theta)p(h_T\mid\theta,\pi)}\big[\log\tfrac{p(h_T\mid\theta,\pi)}{p(h_T\mid\pi)}\big]$, and they optimize the sPCE bound using contrastive prior samples $\theta_{0:L}$: $\mathrm{sPCE}(\pi,L,T)=\mathbb{E}[\log \tfrac{p(h_T\mid\theta_0,\pi)}{\frac{1}{L+1}\sum_{\ell=0}^L p(h_T\mid\theta_\ell,\pi)}]$. In the HiP-MDP formulation the per-step reward is an incremental sPCE term: $r_t=\log p(y_t\mid\theta_0,\xi_t)-\log(C_t\cdot\mathbf{1})+\log(C_{t-1}\cdot\mathbf{1})$, where $C_t$ stores history likelihood products over contrastive parameter samples.","Across generalization tests in the location-finding task (trained at $K=2$ objects, evaluated at $K\in\{1,2,3,4,5\}$ with $T=30$), SUNRISE and DroQ were among the strongest, and the SUNRISE-DroQ combination achieved top or near-top sPCE in several shifted settings (e.g., sPCE $12.143\pm0.013$ at $K=3$ vs SUNRISE $12.133\pm0.013$; $11.453\pm0.016$ at $K=5$ vs SUNRISE $11.445\pm0.016$). In the CES task (trained at $\nu=0.005$, tested also at $\nu=0.01$ with $T=10$), DroQ achieved the best sPCE on both $\nu$ values (e.g., $14.090\pm0.020$ at $\nu=0.005$ and $12.683\pm0.022$ at $\nu=0.01$), indicating superior robustness to the likelihood change. Training-time tradeoffs were substantial: for location finding, REDQ averaged ~13.23h vs SUNRISE ~21.44h vs SUNRISE-DroQ ~29.77h; for CES, REDQ ~8.95h vs DroQ ~13.18h vs SUNRISE-DroQ ~18.16h. Overall, the results suggest ensemble/dropout techniques improve generalization but at increased computational cost.","The authors note the results are indicative and limited to the two studied BOED problems, leaving open how close the learned policies are to optimal and what factors drive convergence to particular policies. They also point out that training-time costs can be high (especially for SUNRISE and SUNRISE-DroQ), which may limit practicality depending on available compute. They further observe that strong performance on the training setting does not necessarily translate to best performance under distribution drift (e.g., varying $K$ in location finding).","The evaluation is entirely simulation-based; there is no real experimental deployment, so practical issues (measurement noise quirks, constraints, safety, human-in-the-loop effects for CES) are not validated. Generalization tests vary only a small number of model aspects (e.g., $K$ and $\nu$), so robustness to other misspecifications (prior shift, different noise models, correlated observations, alternative likelihood families, constraints/costs) remains unclear. The approach assumes the ability to sample from and compute likelihoods for many contrastive parameter samples (large $L$), which may be infeasible for expensive simulators or implicit-likelihood settings without additional approximations.","They suggest studying longer experimental horizons at deployment time, since practitioners may be able to run more experiments than the training budget. They propose investigating how likelihood parameters affect final sPCE, and extending experiments to higher-dimensional scenarios (e.g., 3D location finding). They also propose crafting alternative/robust utility functions to better handle model misspecification and improve robustness of experimental design under distribution shift.","Provide theoretical or empirical guarantees/diagnostics for policy optimality gaps (e.g., benchmarks against near-optimal dynamic programming for small instances) and sensitivity analyses over priors and encoder architectures. Extend to constrained and cost-aware design (e.g., action costs, switching costs, safety constraints) and to settings with autocorrelation/non-i.i.d. observations. Release a reusable software package (e.g., pip/conda) with standardized BOED environments and evaluation protocols to facilitate broader benchmarking across RL and non-RL sequential design methods.",2503.05905v2,https://arxiv.org/pdf/2503.05905v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:33:03Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,Variable/General (design chooses M observation inputs such as sensor locations; examples include M=3 time points for damped oscillator and M=30 spatial points for 2D Eikonal; PDE parameters can be finite-dimensional or function-valued),Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/apivich-h/pied,"The paper proposes PIED, a physics-informed experimental design framework for PDE-governed inverse problems that optimizes observation/design inputs (e.g., sensor locations) for one-shot (non-adaptive) data collection under limited budget. PIED uses physics-informed neural networks (PINNs) as both forward simulators and inverse solvers inside a fully differentiable architecture, enabling continuous gradient-based optimization of the design parameters and parallel evaluation across sampled PDE parameters. To reduce compute, it introduces a shared meta-learned PINN initialization via REPTILE and proposes two differentiable surrogate criteria for design optimization: FIST (few-step inverse solver training) and MoTE (NTK-based model training estimate). Across noisy simulation benchmarks (wave equation, Navier–Stokes, Eikonal) and real datasets (groundwater flow and scratch-assay cell growth), FIST/MoTE yield lower expected inverse-parameter error than baselines including mutual-information/EIG-inspired methods, grid, and random designs. The work advances PINN-based inverse problems by integrating PINN training dynamics directly into a practical, one-shot DOE/experimental design loop.","The inverse problem and DOE objective are formalized as: (i) infer PDE parameters $\hat\beta(X,Y) \approx \arg\min_\beta \|u_\beta(X)-Y\|^2$ and (ii) choose observation inputs $X$ to minimize expected parameter error $L(X)=\mathbb{E}_{\beta,Y}\,\|\hat\beta(X,Y)-\beta\|^2$. PIED evaluates candidate $X$ by running (or approximating) PINN-based inverse solves over sampled reference parameters and maximizing the aggregated criterion $\alpha(X)=\frac{1}{N}\sum_{i=1}^N -\|\hat\beta_i(X,\tilde Y_i)-\beta_i\|^2$. MoTE further approximates converged inverse estimates using an empirical NTK / linearized training expression (Eq. 9 in the paper).","On five scenarios (Wave, Navier–Stokes, Eikonal, Groundwater, Cell Growth), the proposed FIST and MoTE criteria achieve the best or near-best median expected inverse-parameter error among tested methods. Reported medians (Table 1) include: Wave: FIST 3.87 vs MI 4.46 and Random 5.23; Navier–Stokes: MoTE 1.18 and FIST 2.10 vs Grid 4.51 and Random 6.19; Eikonal (function-valued): FIST 0.74 and MoTE 0.76 vs Grid 1.56 and Random 1.82. On real datasets, Groundwater: FIST 1.93 vs MI 2.10 and Random 3.44; Cell Growth: FIST 2.62 vs VBOED 2.82 and Random 3.63 (all as reported, with task-specific scaling factors in the table). The paper also shows PINN-based forward models can be both more accurate and faster overall than classical numerical solvers for ED in the Eikonal setting (Figure 6).","The authors note they mainly evaluate PIED with “vanilla PINN architectures” and do not verify performance with more complex differentiable physics-informed architectures (e.g., physics-informed deep operators). They also state the framework relies on PINNs as both forward simulators and inverse solvers, so it is not directly applicable to non-PINN solvers. Finally, they acknowledge practical PINN training difficulties for some problems, which may require improved collocation/adaptive sampling strategies.","The method’s effectiveness depends on differentiating through (or approximating) PINN training; performance may be sensitive to hyperparameters (training steps r, perturbation levels, optimizer choices) and could be unstable for stiff/multiscale PDEs where PINNs are known to struggle. The ED objective is evaluated using sampled reference parameters from an assumed prior/range; robustness to prior misspecification is only lightly explored (one mismatch test) and may be more problematic in higher dimensions or real deployments. Comparisons are mainly against selected MI/EIG-inspired and heuristic baselines; broader DOE/optimal design baselines common in PDE-constrained OED (e.g., classical A/D/I-optimal design using adjoints/Laplace approximations) are discussed but not comprehensively benchmarked under equal budgets.","They suggest applying PIED to other differentiable physics-informed architectures beyond PINNs, such as operator learning methods. They also mention that improving PINN training (e.g., via better collocation point selection) could further boost performance of the framework.","Extending PIED to explicitly handle unknown/noisy model discrepancy and correlated/non-Gaussian observation noise (common in physical experiments) would improve practicality. Developing principled links to classical PDE-constrained optimal experimental design (e.g., D-/A-/I-optimality under Laplace/posterior approximations) could provide stronger theoretical guarantees and better baseline parity. Providing a self-starting or uncertainty-calibrated version (e.g., Bayesian PINNs, ensembles) and releasing standardized benchmarks/software utilities would help reproducibility and adoption in scientific domains.",2503.07070v1,https://arxiv.org/pdf/2503.07070v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:33:48Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,"Variable/General (sequentially selects 1 input variable: intensity measure (IM) value; model parameters are (α, β))",Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian framework to estimate probit-lognormal seismic fragility curves from limited binary outcomes (failure/non-failure) while avoiding likelihood degeneracy that commonly occurs with small datasets. It introduces a constrained reference prior (a proper, slightly informed variant of the Jeffreys/reference prior) and provides an analytical approximation that depends on the (lognormal-approximated) intensity measure distribution. The core DOE contribution is a sequential design strategy that selects the next intensity measure value by maximizing an information-theoretic criterion (expected δ-divergence/mutual-information-like impact of the new (IM, BIS) pair on the posterior). The method is demonstrated on a nuclear-industry piping-system case study using a large database of synthetic/filtered seismic signals and extensive numerical simulations for benchmarking. Results show the sequential DOE rapidly reduces degeneracy probability and achieves lower bias and narrower credible intervals than standard sampling, often with fewer than ~100 observations, and the authors propose stopping criteria based on changes in the DOE index and fragility-median evolution.","Fragility (probit-lognormal) model: $P_f(a)=\Pr(Z=1\mid A=a,\theta)=\Phi\!
\left(\frac{\log a-\log \alpha}{\beta}\right)$ with parameters $\theta=(\alpha,\beta)$. Sequential DOE chooses $a_{k+1}=\arg\max_{a\in\mathcal A} I_{k+1}(a)$ where $I_{k+1}(a)=\mathbb E_{Z_{k+1}\mid a,\mathbf z_k}\big[D_\delta(p(\theta\mid \mathbf z_k,\mathbf a_k)\Vert p(\theta\mid \mathbf z_{k+1},\mathbf a_{k+1}))\big]$; the paper derives a Monte Carlo approximation for evaluating this criterion. A proper constrained reference prior family is proposed, $\pi^*_\gamma(\theta)\propto J(\theta)\,\beta^\gamma$ (with $0<\gamma<2$), and an analytical approximation $\tilde\pi^*_\gamma(\theta)\propto \frac{1}{\alpha(\beta^{1-\gamma}+\beta^{3-\gamma})}\exp\left(-\frac{(\log\alpha-\mu_A)^2}{2\sigma_A^2+2\beta^2}\right)$.","In replicated experiments (100 replications) on a piping-system case study, the DOE strategy substantially lowers the probability that the collected sample yields a degenerate likelihood compared with standard IM sampling, with degeneracy dropping to near zero by roughly k≈20 (PGA) to k≈50 (PSA) depending on the IM. Error metrics (quadratic error, squared bias to a reference nonparametric fragility curve, and credibility width) improve faster for DOE at small sample sizes (notably for k<50), reaching satisfactory accuracy with on the order of tens of observations. With PSA as IM, the DOE estimates approach the probit-lognormal model-bias floor by around k≈50–100 while maintaining thinner credible intervals than standard sampling. The paper also reports that DOE performance is relatively insensitive to the prior-constraint hyperparameter γ over the tested range (γ∈{0.1,…,1.9}), and provides practical stopping thresholds (e.g., using $V I_k\approx10^{-3}$ or $V P_k\approx5\%$) that correspond to roughly k≈40–60 in their application.","The authors state that the influence of the seismic-signal generator (SSG) and seismic scenario on the DOE approach is out of scope. They also note that even with a proper posterior, degenerate-likelihood cases can still yield unrealistic estimates with $\beta\to 0$ unless a strongly informative prior near $\beta\to 0$ is used, which would in turn penalize learning. They caution that proposed stopping thresholds are not based on existing guidance and should be chosen by practitioners for their specific objectives and systems.","The sequential criterion optimizes information gain for the assumed probit-lognormal model; if model misspecification is strong (e.g., multimodal/heteroscedastic response not captured by the parametric curve), the DOE may over-focus on regions that reduce posterior uncertainty but increase real predictive error. The approach assumes conditional independence of observations and relies on an assumed/estimated IM distribution (approximated as lognormal) that enters the prior; misspecifying this distribution could affect robustness and calibration. Practical implementation requires access to a large signal database and the ability to realize target IM levels (or nearest-neighbor selection), which may be difficult for physical testing where IM targeting and repeatability are constrained. No open-source implementation details are provided, which can hinder reproducibility and adoption.","The authors explicitly state that studying the influence of the seismic-signal generator (SSG) is out of scope, implicitly leaving it for future investigation. They also note the DOE methodology can be applied with any proper prior (suggesting extensions beyond their constrained reference-prior choice).","Extend the DOE to handle additional complexities common in seismic/structural data: correlated observations, model discrepancy, and nonparametric or semi-parametric fragility representations (e.g., monotone splines) to mitigate model bias. Develop self-starting/Phase-I-to-Phase-II procedures that account for unknown IM distribution and parameter uncertainty jointly, and assess robustness to IM-distribution misspecification. Provide reproducible software (e.g., an R/Python package) and benchmark against alternative Bayesian optimal design criteria (e.g., expected KL, D-optimality on Fisher information, or utility functions tied to risk metrics). Explore multi-objective or constrained DOE that incorporates experimental feasibility (IM targeting constraints) and costs, and extend to multivariate IMs or vector-valued design variables (e.g., frequency-content features beyond scalar IM).",2503.07343v4,https://arxiv.org/pdf/2503.07343v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:34:32Z
FALSE,NA,NA,Not applicable,Not specified,Transportation/logistics|Other,Simulation study|Case study (real dataset),TRUE,Other,Not provided,https://youtu.be/HO7FJyZiJ3E|https://youtu.be/70ead3rQcok,"The paper proposes an integrated approach for multi-robot pursuit of an unknown moving target using only bearing (direction) information, combining (i) a cooperative 3D bearing-only pseudo-linear information filter in information form (u-PLIF) for target state (position/velocity) estimation and (ii) a multi-agent reinforcement learning (MARL) framework for pursuit control under practical constraints (limited field of view, kinematics, range maintenance, collision avoidance, and observability). The u-PLIF reformulates the nonlinear bearing measurement using an orthogonal projection operator to obtain pseudo-linear measurement equations and uses an information-filter update to efficiently fuse multiple bearings while remaining robust when measurements drop due to FoV loss. For sim-to-real transfer, the authors introduce adjustable low-level control gains (first-order lag behavior) during training to match real AGV dynamics and apply spectral normalization to actor networks to enforce smoother, more robust policies (spectral-normalized MADDPG with a Lipschitz bound). Performance is evaluated through physics-based simulation and real experiments using three custom AGVs with motion-capture-derived bearings (with added noise) and ROS-based deployment, demonstrating successful zero-shot transfer and stable pursuit with accurate target estimation. Overall, the contribution advances practical pursuit-evasion robotics by tightly coupling estimation, MARL control, and sim-to-real robustness techniques with experimental validation.","Target and pursuer dynamics are modeled with discrete double-integrator state transitions (Eq. (1)) and bearing measurements as a normalized relative position vector with additive Gaussian noise (Eq. (2)). Nonlinearity is handled by applying an orthogonal projection operator $P_g = I - \frac{g}{\|g\|}\frac{g^T}{\|g\|}$ (Eq. (3)) to derive pseudo-linear measurement relations (Eqs. (4)-(5)). Multi-measurement fusion is performed in information-filter form with prediction and correction updates that sum per-agent information contributions, using a pseudo-inverse due to rank deficiency (Section III-C). Sim-to-real dynamics matching uses first-order lag control $a=k_v(Rv_d-v)$ and $\alpha=k_\omega(\omega_d-\omega)$ (Eq. (7)), and policy smoothness is encouraged via spectral normalization by bounding the actor network Lipschitz constant through layer spectral norms (Eqs. (8)-(9)).","In real AGV experiments (three pursuers, one target), the number of pursuers detecting the target rises to three by about 4.5 s, with brief FoV-induced dropouts that do not destabilize estimation when at least two bearings remain. Target position and velocity estimation errors converge to near zero within about 1 s; after convergence, position error stays within 0.02 m and velocity error within 0.2 m/s. Range control error decreases from roughly 1.5 m to below 0.3 m while observability is maintained near a reported theoretical maximum of 2.25. The authors report successful zero-shot transfer of MARL policies from simulation to real AGVs, with action jitter attributed primarily to noisy bearing observations and mitigated by Lipschitz-constrained (spectral-normalized) policies.","The authors note they do not include a real onboard camera; instead, bearings are reconstructed from motion-capture data with deliberately added noise because the focus is on validating estimation and control algorithms independent of the visual detection module. They also frame the current work as a demonstration prototype and indicate that more aggressive target tracking remains to be addressed in future work.","The experimental evaluation appears limited in scale (a small number of AGVs, a motion-capture arena, and a manually controlled target), so generalization to larger teams, outdoor environments, and real visual detection pipelines (with missed detections, latency, and false positives) is not established. The estimation and control design assumes Gaussian noise and relies on accurate pursuer states and synchronized measurements; robustness to significant time delays, communication dropouts, and correlated/non-Gaussian perception errors is not clearly characterized. Comparisons against alternative modern MARL baselines and alternative bearing-only filters in the same experimental setup are not detailed in the provided excerpt, making it hard to quantify relative gains beyond the reported absolute performance.",Future research will focus on enabling more aggressive target tracking and incorporating model-based approaches to improve explainability and performance.,"A natural next step is to integrate real vision-based bearing extraction (including detection uncertainty and missed detections) and quantify end-to-end performance under realistic sensing latency and false alarms. Extending the framework to larger heterogeneous teams with intermittent communication, explicit bandwidth constraints, and decentralized execution would improve practical applicability. Additional robustness studies (e.g., non-Gaussian noise, time-varying biases, delayed/asynchronous measurements) and broader baseline comparisons (other MARL algorithms and alternative estimation filters) would better establish where the approach is most advantageous.",2503.08740v2,https://arxiv.org/pdf/2503.08740v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:35:07Z
TRUE,Sequential/adaptive|Bayesian design|Other,Prediction|Optimization|Cost reduction|Other,Other,"1 factor (projection angle), chosen sequentially from a discrete grid (e.g., N=180 angles)",Semiconductor/electronics|Manufacturing (general)|Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://doi.org/10.5281/zenodo.3830790|https://doi.org/10.5281/zenodo.2657340|https://doi.org/10.5281/zenodo.3384092,"The paper proposes Diffusion Active Learning (DAL), a sequential experimental design method for inverse problems (validated on computed tomography) that uses a pre-trained unconditional diffusion model as a learned, data-dependent prior. At each acquisition step, it draws posterior samples via conditional diffusion sampling with “Soft Data Consistency” (early-stopped gradient steps enforcing measurement consistency), then selects the next CT projection angle by maximizing predictive uncertainty (sample variance) in measurement space. The “design” variable is the next measurement angle from a discrete angle grid (e.g., 180 angles at 1° increments), iteratively chosen until a measurement budget is exhausted, aiming to reduce required projections (and thus dose/time) while improving reconstruction quality. Empirically on three real datasets (chip, composite materials, lung CT), DAL matches a target reconstruction quality (PSNR) with up to ~4× fewer measurements than baselines and provides >2× speed-ups versus a Laplace-based Bayesian design baseline, with gains strongest on highly structured (chip/composite) images. The work advances DOE for imaging by coupling a learned generative prior with an uncertainty-sampling acquisition rule to produce data-driven adaptive measurement designs for CT.","Measurements follow $y_\psi = A_\psi(x^*) + \epsilon$ and reconstruction (Gaussian noise) is posed as $\min_x \sum_{\psi\in\Psi}\|A_\psi(x)-y_\psi\|_2^2$. The sequential acquisition chooses the next angle by maximum posterior predictive variance using conditional samples $\{x_t^i\}_{i=1}^k$: $\psi_{t+1}=\arg\max_{\psi\in\Phi}\frac{1}{k}\sum_{i=1}^k\|A_\psi(x_t^i)-A_\psi(\bar x_t)\|^2$, where $\bar x_t=\frac{1}{k}\sum_i x_t^i$. Conditional diffusion sampling is enforced via “Soft Data Consistency” by taking a fixed, early-stopped number of gradient steps toward the data-consistency objective at each reverse-diffusion step.","Across three real CT datasets, diffusion-based reconstruction achieved the highest PSNR regardless of acquisition, and DAL further improved PSNR on the structured Chip and Composite datasets compared with uniform scanning and dataset-agnostic generative AL baselines. For the scientific datasets benchmarked, DAL reached the same average PSNR with up to 4× fewer measurements (reported as up to 4× lower X-ray dose). Table 2 reports measurements to reach PSNR=30 dB under active learning: Chip 27 (Diffusion) vs 53 (Laplace) and >100 (SWAG); Composite 15 (Diffusion) vs 65 (Laplace) and >100 (SWAG/Ensemble); Lung 18 (Diffusion) vs 34 (Laplace). Runtime experiments on 512×512 show DAL taking <~2 minutes per step and yielding >2× speed-ups compared to the Laplace baseline over an active learning loop.","The authors note that training the diffusion prior requires sufficient domain-specific training data, which may not always be available. They acknowledge higher computational cost versus classical reconstruction methods (though acceptable when acquisition takes days) and warn about reconstruction bias when the test sample is out-of-distribution relative to the training set, which can be problematic for defect detection or high-stakes medical use. They also acknowledge sim-to-real gaps and additional real-world experimental issues (e.g., alignment, measurement noise) not fully captured in their evaluation.","The acquisition rule is greedy (myopic) and may be suboptimal relative to multi-step/lookahead experimental design when angle choices have strong long-horizon interactions; the paper does not quantify regret or optimality guarantees for the diffusion-based posterior approximation. The evaluation relies on synthetic forward projections (Radon transform) even when using real reconstructed volumes as ground truth, so conclusions about real hardware constraints (angle-dependent noise, mechanical limits, calibration drift) may not fully transfer. The method’s sensitivity to mis-specified noise models (Gaussian vs Poisson), correlated noise, or model mismatch in $A_\psi$ is not systematically stress-tested. No public implementation details (code, reproducible configs) are provided in the paper text, which limits reproducibility and practical adoption.","They suggest pre-training larger “foundation” diffusion models over diverse tomographic images and/or fine-tuning large pre-trained diffusion models (e.g., Stable Diffusion) on small domain datasets to reduce data requirements. They highlight the need to address sim-to-real gaps in real tomography setups, including distribution shifts/out-of-distribution samples, sample alignment, and measurement noise, and point to OOD-robust diffusion methods as a direction. They also note that applying DAL to other differentiable forward models (e.g., MRI or ptychographic reconstruction) is possible beyond the CT focus.","Developing non-myopic (lookahead) or budget-aware planning variants of DAL (e.g., dynamic programming/RL with diffusion posterior surrogates) could improve designs when greedy uncertainty sampling is inefficient. Extending DAL to handle unknown/estimated system parameters (self-calibration) and model uncertainty in the forward operator $A_\psi$ would better match practice in CT. Robust design variants for non-Gaussian/Poisson noise and for correlated or angle-dependent noise, with explicit robustness metrics, would strengthen claims about dose reduction. Providing an open-source reference implementation and standardized benchmarks for adaptive CT acquisition would improve reproducibility and facilitate comparison with competing experimental design methods.",2504.03491v1,https://arxiv.org/pdf/2504.03491v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:36:00Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Optimization|Prediction|Other,Bayesian D-optimal|Not applicable|Other,"Variable/General (design variable $\theta$; examples include 1D $\theta\in[-1,1]$ and 2D $\theta\in[0,1]^2$ / sensor location in $\Omega\subset\mathbb{R}^2$)",Environmental monitoring|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian optimal experimental design (OED) utility based on the expected Wasserstein-$p$ distance between the prior and posterior, termed a Wasserstein information criterion (W-optimality). For $p=2$ it derives a closed-form expression in linear-Gaussian (Gaussian regression/Bayesian inverse problem) settings, relating the criterion to prior/posterior covariance operators and contrasting it with classical A- and D-optimality (via NSD and EIG/KL-based criteria). The main theoretical contribution is a stability (perturbation) analysis of the $p=1$ criterion, providing explicit error bounds under perturbations of the likelihood/forward model and under prior perturbations, with extensions/partial results for $p=2$; these yield convergence rates when using empirical prior approximations. Computationally, the paper demonstrates evaluability of the $p=2$ criterion using optimal transport machinery (Brenier map/Monge–Ampère PDE) and illustrates behavior in simulated examples, including a nonlinear forward-map sensor design and a heat-diffusion sensor placement inverse problem. Comparisons show the Wasserstein criterion can favor different design locations than EIG (KL), often promoting more balanced information gain across parameters rather than concentrating on a subset.","The expected Wasserstein-$p$ utility is defined as $U_p(\theta)=\mathbb{E}_{\pi(y;\theta)}\, W_p^p\big(\mu,\mu^y(\cdot;\theta)\big)$, where $\mu$ is the prior and $\mu^y$ the posterior. For Gaussian measures, $W_2^2(\mu_1,\mu_2)=\|m_1-m_2\|^2+\mathrm{Tr}\big(C_1+C_2-2(C_1^{1/2}C_2C_1^{1/2})^{1/2}\big)$, and in linear-Gaussian inverse problems the paper shows $U_2=2\mathrm{Tr}(C_0)-2\mathrm{Tr}\big((C_0^{1/2}C_{\text{post}}C_0^{1/2})^{1/2}\big)$. The forward/likelihood model is $y=G(x;\theta)+\epsilon$ with Gaussian noise, leading to stability bounds such as $\mathbb{E}_{\pi(y)}W_1(\mu_y,\mu_y^*)\le K\,(\mathbb{E}_\mu\|G-G^*\|_\Gamma^2)^{1/2}$ and prior-perturbation bounds scaling with $W_2(\mu,\tilde\mu)$.","The paper proves finiteness/well-posedness of $U_p$ for $p\ge1$ and establishes stability of $U_1$ under both likelihood (forward-model) and prior perturbations, yielding explicit bounds in terms of $\Delta G=(\mathbb{E}_\mu\|G-G^*\|_\Gamma^2)^{1/2}$ and Wasserstein distances between priors. For empirical prior approximations $\mu_M$, it links error rates in $U_1$ to convergence of $W_2(\mu,\mu_M)$ (e.g., in 1D Gaussian examples citing $\mathbb{E}W_1(\mu,\mu_M)\lesssim M^{-1/2}$ and $\mathbb{E}W_2(\mu,\mu_M)\lesssim \sqrt{\log\log M / M}$) and demonstrates numerically that the expected-utility error decays approximately like $M^{-1/2}$ in the presented toy problem. In nonlinear and PDE-based simulated design examples, the Wasserstein-$2$ criterion selects different optimal sensor/design locations than EIG (KL), with observed posteriors under Wasserstein-selected designs being more concentrated in the showcased cases.","The authors note that stability is established mainly for Gaussian likelihood models and that extending stability to more general likelihoods is an open direction. They also state that the coupling condition between the forward-model Lipschitz constant and the prior’s exponential moment (the $L_1$–$L_2$ restriction) is restrictive and should be relaxed. Finally, they acknowledge that their computational demonstrations are limited to low-dimensional examples and that developing efficient high-dimensional methods remains future work.","The computational sections describe numerical schemes (Smolyak quadrature, Monte Carlo, Monge–Ampère PDE/RBF collocation) but do not provide reproducible implementation details (software, parameter settings, runtime/complexity), which may hinder practical adoption and benchmarking. Empirical comparisons to EIG are illustrative but limited in scope (few examples) and do not provide systematic performance metrics (e.g., expected posterior risk, robust out-of-sample criteria, sensitivity to noise/model mismatch) across broader scenarios. The proposed W-optimality criteria may be challenging to scale to high-dimensional parameter/design spaces because evaluating/approximating Wasserstein distances or transport maps can be expensive and numerically delicate; this scaling challenge is not quantified in the paper’s experiments.",The paper proposes studying stability of the Wasserstein criterion for more general (non-Gaussian) likelihood models. It explicitly suggests relaxing the restrictive condition linking the Lipschitz constant of $G$ to the prior’s exponential moment (the $L_1$–$L_2$ coupling). It also states the need to develop efficient computational methods for high-dimensional problems beyond the low-dimensional demonstrations provided.,"Developing scalable approximations for $U_p$ in high dimensions (e.g., sliced/entropic Wasserstein, stochastic OT, variational transport maps, or GPU-friendly differentiable OT) would directly address computational bottlenecks and enable gradient-based design optimization in large problems. Extending the criterion and stability theory to settings with unknown nuisance parameters (noise variance, hyperparameters) and to sequential/online design (updating designs as data arrive) would improve practical applicability. A more comprehensive benchmarking framework—evaluating downstream estimation error, credible-set calibration, and robustness under model misspecification/autocorrelation—would clarify when Wasserstein-based utilities outperform EIG/NSD in practice.",2504.10092v1,https://arxiv.org/pdf/2504.10092v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:36:54Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Optimization|Cost reduction|Other,E-optimal|Other,Variable/General (experimental design parameters: wavelength + incidence angles; demonstrated with 2 model parameters of interest),Semiconductor/electronics|Other,Simulation study|Other,TRUE,None / Not applicable,Upon request|Not provided,NA,"The paper demonstrates coherent EUV scatterometry for reconstructing the 3D profile of a 2D periodic semiconductor interconnect structure, estimating an average copper dishing depth of 1.60 ± 0.05 nm from measured first-order diffraction efficiency vs incidence angle at ~29.5 nm. A forward diffraction model (rigorous coupled-wave analysis, RCWA) is fit to multi-angle data using a genetic algorithm, and parameter uncertainty is quantified via Monte Carlo resampling of measurement noise. The main methodological contribution is a mathematically grounded experimental design framework using a noise-aware, normalized Fisher information matrix (FIM) to optimize measurement settings (incidence angles and wavelength) for maximum sensitivity to parameters of interest. The design criterion is to maximize the minimum eigenvalue of the normalized FIM (a maximin/E-optimal-style objective) to improve identifiability in the least-sensitive parameter direction and to reduce parameter correlations. A global optimization over wavelength and two incidence angles predicts an optimal wavelength near 14.1 nm (near the Si absorption edge) and optimized angles (~15.3° and 17.3°), showing sensitivity depends strongly on both wavelength and angle and can be improved via a priori DOE computations.","The measured first-order diffraction efficiency is computed from CCD counts as $DE=(N_{+1}+N_{-1})/(N_{+1}+N_{-1}+N_0)$. The (approximate) measurement variance used for DOE is derived via Gaussian error propagation of photon (Poisson) + camera noise, yielding $\sigma^2_{DE}$ as a function of $DE_m$, order counts, and their variances (Eq. (1)/Eq. (S5)). The Fisher information matrix under independent Gaussian errors is $F_{kl}=\sum_i \sigma_i^{-2}\,(\partial f_{\theta_i}/\partial p_k)(\partial f_{\theta_i}/\partial p_l)$, then normalized as $F' = J^T F J$ using expected parameter ranges, and the DOE objective maximizes the smallest eigenvalue of $F'$ over wavelength and incidence angles.","Using seven incidence angles at ~29.5 nm, the reconstructed copper dishing depth is reported as 1.60 ± 0.05 nm (with uncertainty assessed by 500 Monte Carlo reconstructions). DOE optimization of wavelength and two incidence angles (for a two-parameter case: dishing depth + angle offset) identifies an optimum at wavelength 14.1 nm with incidence angles 15.3° and 17.3°, maximizing the minimum eigenvalue of the normalized FIM. The analysis shows incidence-angle selection can reduce parameter correlation (via choosing points with opposite sign derivative w.r.t. angle) and that shorter wavelength improves sensitivity primarily through increased penetration depth in the SiCN surface layer. A practical constraint is included by capping integration time at 5 minutes in the noise model to maintain throughput and avoid additional stochastic errors.","The authors note experimental error bars were higher than expected due to debris/contamination from sample dicing that could not be fully cleaned, and they include a thin carbon layer in the model to account for it. They also emphasize that the optimized settings depend on the assumed noise model and prior parameter ranges, and that uncertainty in a global incidence-angle offset can make it harder to target the most sensitive angular regions. They caution that as more model parameters are introduced, the optimization becomes higher-dimensional with more local minima and increased computation time.","The optimal-design results are demonstrated primarily for a simplified two-parameter inference problem; performance for the full multi-parameter reconstruction (e.g., material density, wavelength offset, etc.) is not fully quantified under the DOE framework. The DOE assumes independent, approximately Gaussian measurement errors and relies on an RCWA forward model; model-form error (e.g., roughness, line-edge variation, nonuniformity across the scan) could reduce realized information gain. No public code or standardized benchmark comparison (e.g., against alternative DOE criteria like D- or I-optimality, or Bayesian/robust design under model uncertainty) is provided, which limits reproducibility and generalization.","They state that reducing sample contamination from preparation should enable deep sub-nanometer sensitivity for axial profiles and 2D structures. They also propose extending the FIM-based design optimization to additional experimental parameters (e.g., polarization, sample face rotation, and additional EUV wavelengths/harmonics) and to throughput-focused constraints (e.g., reallocating integration time, multiplexing with multiple harmonics). They further discuss applying the methodology to low-dose constraints relevant for soft materials/biological samples and to related modalities such as reflectometry and imaging reflectometry with appropriate noise models.","A natural extension is a Bayesian/robust DOE that explicitly marginalizes over uncertainty in nuisance parameters and possible model discrepancy (e.g., contamination layers, roughness), yielding designs less sensitive to prior misspecification. Providing an open-source implementation (RCWA + noise model + FIM optimization) and validating recommended designs on multiple real wafers/samples would strengthen reproducibility and industrial transfer. Extending the framework to jointly design multi-wavelength (harmonic-comb) measurements and multiple diffraction orders under dose/throughput constraints could yield more practical high-throughput recipes for complex profiles and multivariate parameter sets.",2504.12133v1,https://arxiv.org/pdf/2504.12133v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:37:33Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,Bayesian D-optimal|Other,Variable/General (design parameter p ∈ D ⊆ R^{dimp}; examples include 1D design p and PDE example with scalar p; parameter dimension d up to 8 in experiments),Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper proposes a fully gradient-free framework for sequential Bayesian optimal experimental design (BOED) that selects designs adaptively as data arrive. Design optimization is done with Ensemble Kalman Inversion (EKI), while posterior updates/sampling are performed with the gradient-free Affine-Invariant Langevin Dynamics (ALDI) interacting particle system. To avoid expensive nested Monte Carlo estimators for the Expected Information Gain (EIG), the authors develop variational Gaussian (joint) and parametrized Laplace posterior approximations that provide computable upper and lower bounds on the EIG; these bounds are analyzed in near-linear and small-noise/large-data regimes. Numerical experiments (linear Gaussian, mildly nonlinear, and PDE-constrained inverse problems including a 1D heat equation) demonstrate that the method can produce increasingly informative sequential designs and that Laplace refinement can tighten the lower bound when Gaussian approximations are insufficient. Overall, the work advances BOED practice for black-box and PDE-based models by combining derivative-free sampling, derivative-free optimization, and scalable EIG bounding/estimation.","Sequential observation model: $y_n = G_n(u,p_n) + \eta_n$ with Gaussian noise $\eta_n\sim\mathcal N(0,\Gamma)$. The BOED objective uses Expected Information Gain: $\mathrm{EIG}(p)=\mathbb E_{u,y\mid p}\left[\log \frac{\pi(y\mid u,p)}{\pi(y\mid p)}\right]=\mathbb E_{y\mid p}\big[\mathrm{KL}(\mu_{u\mid y,p}\|\mu)\big]$. EIG is estimated via sample averages with variational approximations: upper bound uses an approximate marginal $\tilde\pi(y\mid p)$ and lower bound uses an approximate posterior $\tilde\pi(u\mid y,p)$; Gaussian joint approximation gives closed-form $\tilde\mu_{u\mid y,p}$ and $\tilde\mu_{y\mid p}$, and a parametrized Laplace posterior $\mathcal N(u_\varepsilon,\varepsilon C_\varepsilon)$ can tighten the lower bound in informative-data regimes.","For the linear Gaussian example, the KL divergences between true and Gaussian-approximated marginal/posterior distributions empirically converge at the predicted $O(J^{-1/2})$ rate as the number of joint samples $J$ increases (plots use $J$ from $10^1$ to $10^5$). In the linear case, EKI successfully converges to the design optimum (benchmark computed with MATLAB fminunc and a high-fidelity double-loop estimator with $J_{\text{ref}}=10^7$), and the upper/lower EIG bounds tighten along the EKI trajectory. In the near-linear example $G(u,p)=A(p)u+\tau u^2$, decreasing $\tau$ improves the Gaussian approximation and makes bounds approach the linear case; Laplace approximation yields a noticeably tighter lower bound across $\tau$ values. In the PDE (1D heat equation) sequential setting with $N=15$ time steps and observations at select times, the EKI-driven designs increase informativeness and the Gaussian bound framework remains effective even in nonlinear, PDE-based cases (with an added Laplace lower bound in a 1D parameter/observation setup).","The paper notes that directly using a parametrized Laplace approximation for each possible observation/design can be computationally prohibitive because it requires solving an optimization problem per observation. It also states that in the general nonlinear setting, gradient-free ALDI retains affine invariance but lacks a theoretical guarantee of convergence to the target distribution (despite good numerical performance). The authors emphasize that Gaussian surrogate approximations can be insufficiently accurate in nonlinear or non-Gaussian settings, motivating refinement (Laplace or more flexible approximations).","The demonstrated workflow depends on multiple tuning choices (ensemble sizes for EKI/ALDI, time horizons, step sizes, regularization, variance inflation, tolerance for bound gaps); sensitivity analyses or robust default recommendations are not clearly established. The approach is validated on synthetic examples; practical issues such as model discrepancy, misspecified priors, expensive forward-model budgets, and real experimental constraints (discrete/categorical designs, safety/feasibility constraints) are not thoroughly addressed. Comparisons against other state-of-the-art sequential BOED optimizers/samplers (e.g., policy-based/DP approximations, transport-map sequential methods, gradient-based BOED where gradients exist) are limited, so relative efficiency under equal compute budgets is unclear.","Future work will improve EIG bound quality using more flexible/adaptive approximations beyond Gaussian surrogates, explicitly mentioning Gaussian mixtures to tighten bounds in nonlinear or multimodal settings. The authors also propose extending the framework to handle model misspecification and non-Gaussian noise to enhance robustness and applicability to real-world experimental scenarios.","Developing principled adaptive switching rules (and cost-aware criteria) between Gaussian, Laplace, and mixture surrogates would make the sequential pipeline more automatic and reliable. Providing theoretical or empirical convergence diagnostics for gradient-free ALDI in nonlinear problems (and/or adding Metropolis correction) could strengthen correctness guarantees. Extending the framework to constrained, discrete, or combinatorial design spaces and to multivariate/functional observations (e.g., sensor placement) would broaden applicability, especially in PDE settings; releasing reference implementations would accelerate adoption and reproducibility.",2504.13320v3,https://arxiv.org/pdf/2504.13320v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:38:10Z
TRUE,Bayesian design|Optimal design|Sequential/adaptive|Other,Model discrimination|Cost reduction|Other,Other,"Variable/General (design variables are which MFA mass-flow data record(s) to collect; case study considers 33 candidate data records, with 1- or 2-record batches; 16 candidate network-structure models from 4 uncertain edges)",Environmental monitoring|Other,Simulation study|Other,TRUE,Python,Supplementary material (Journal/Publisher),https://doi.org/10.7302/k35m-xz34,"The paper develops a Bayesian optimal experimental design (BOED) framework to intelligently choose which material-flow data to collect in a material flow analysis (MFA) when the network structure (presence/absence of edges) is uncertain. The design objective is model/network discrimination, using expected Kullback–Leibler divergence (mutual information) between the network-structure prior and posterior as the expected utility. It derives three nested Monte Carlo estimators of this mutual information (data–model joint MC, model enumeration, and data-marginal MC) and introduces a reduced-bias expected-utility estimation approach via estimator choice and sample-reuse strategies. A case study on the 2012 U.S. steel MFA evaluates 33 candidate mass-flow data records and shows the highest-utility records (and record pairs) are those most connected to uncertain edges; single-record BOED rankings align with realized uncertainty reduction after collecting published USGS/WorldSteel data. The study highlights that optimal data choices depend on the batch size of data collection (single vs. multiple records) and compares estimator accuracy/speed tradeoffs.","Design utility is defined as expected information gain in the discrete network-structure model index: $U(\xi)=\mathbb{E}_{y\mid \xi}[D_{\mathrm{KL}}(p(M\mid y,\xi)\|p(M))]$, which equals mutual information between $M$ and $y$. Model posterior follows Bayes’ rule $p(M_m\mid y,\xi)\propto p(y\mid M_m,\xi)p(M_m)$, where the model evidence is $p(y\mid M_m,\xi)=\int p(y\mid \theta_m,M_m,\xi)p(\theta_m\mid M_m)\,d\theta_m$. Observations use a relative-noise model $y_k=G_k(\theta_m,\xi;M_m)(1+\epsilon_k)$ with $\epsilon_k\sim\mathcal{N}(0,\sigma_k^2)$, yielding a likelihood with a $1/G_k$ Jacobian term. The BOED solution is $\xi^*=\arg\max_{\xi\in\Xi}U(\xi)$, evaluated via three nested Monte Carlo estimators (Eqs. 16, 18, 20) with sample reuse to reduce cost.","In the U.S. steel 2012 case study, the candidate model set comprises 16 network structures generated by permuting 4 uncertain edges, and the design space includes 33 candidate mass-flow data records. For single-record collection, all three estimators identify mass flow #19 (BOF to continuous casting slabs) as the highest-expected-utility record, with #16 (scrap to EAF) second. For two-record batch designs (561 combinations), the maximum expected utility occurs for collecting #11 (pig iron to BOF) and #19 together; other high-utility pairs include (#3,#19), (#16,#19), and (#19,#20). Computationally, with 160,000 unique MFA solves per design (via sample reuse), Ub1 and Ub3 take ~30 seconds while Ub2 takes ~10 minutes on an Intel i7-9700K; across 100 trials, Ub2 shows the lowest standard deviation/RMSE among estimators for expected-utility estimation.","The framework does not incorporate heterogeneous data-collection costs/difficulty across sources; the authors suggest adding a cost-per-information metric (e.g., USD per nat/bit) into the optimization in Eq. (6). It targets reduction of network-structure uncertainty specifically, and does not directly optimize reduction of total mass-flow uncertainty (which also includes parametric uncertainty), motivating goal-oriented BOED extensions. They also note the nested Monte Carlo structure can appear computationally expensive, though in their case study it runs quickly with sample reuse.","The approach assumes the true network is among a finite, pre-enumerated candidate set; if key structural alternatives are missing, BOED may over-optimize within a mis-specified model class. The likelihood uses independent Gaussian relative-noise with fixed (often assumed) noise levels (e.g., 10%); correlated errors, reporting biases, or heavy-tailed noise in compiled databases could change utilities and rankings. Results depend on priors (Dirichlet/truncated normal) and prior elicitation quality; sensitivity analysis to priors and noise assumptions is not emphasized in the provided excerpt. Scaling to many uncertain edges can be challenging because the number of candidate structures grows exponentially ($2^{n_L}$), potentially making model enumeration and even precomputation infeasible without additional structure-learning approximations.","They propose applying the BOED framework to additional theoretical and real-world MFA case studies to derive practical heuristics for targeted data collection. They also suggest developing goal-oriented BOED methods that directly target reduction in MFA mass-flow uncertainty (accounting for both parametric and network-structure uncertainty), rather than network-structure uncertainty alone. The paper also notes potential adaptation of the framework to other industrial ecology contexts such as targeted measurements/experiments in life cycle assessment models and discrimination among alternative model forms in environmental process modeling.","Develop scalable BOED for large candidate-structure spaces using structure-learning surrogates (e.g., variational Bayes, amortized inference, or active learning over edges) to avoid $2^{n_L}$ growth. Incorporate explicit budgeted or constrained design (knapsack-style) with heterogeneous costs, acquisition times, and data availability/latency, enabling practical sequential policies. Extend to settings with dependent/biased data sources (shared measurement systems or common reporting standards) via hierarchical noise models and correlated likelihoods. Provide open-source tooling (packaged library) and benchmarking suites across multiple MFA domains to test robustness of design rankings under prior/noise misspecification and to encourage adoption.",2504.13382v1,https://arxiv.org/pdf/2504.13382v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:39:07Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Other,Other,1 parameter (anode rate constant k_n) in the main case study; framework intended to extend to multiple parameters/groups,Energy/utilities|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper proposes a real-time, reinforcement-learning (TD3) based optimal experimental design strategy to generate an input current profile that maximizes parameter identifiability for a Li-ion cell electrochemical model. The OED objective is based on Fisher Information (FI), computed from the voltage sensitivity to a target parameter (case study: anode rate constant k_n), with additional penalties for violating voltage constraints. The method is validated in a hardware-in-the-loop (HIL) setup (MathWorks/dSPACE with an Elektro Automatik cycler), where the RL agent adapts excitation online using measured terminal voltage and an estimated cathode surface concentration state. Identified parameters are obtained offline via Bayesian optimization by minimizing squared voltage error between experimental measurements and the reduced-order model. Results show the RL-HIL designed profile achieves much lower drive-cycle verification error than standard 1C/5C constant-discharge tests, while requiring far less test time than an RCID protocol and delivering comparable verification accuracy.","The OED objective maximizes Fisher Information for a parameter $\theta$ using voltage sensitivities: $FI = \frac{1}{\sigma_y^2}\sum_{k=1}^N\left(\frac{\partial y_k}{\partial \theta}\right)^2$ (Eq. 3), yielding an input-design problem $\max_{u_{1:N}}\sum_{k=1}^N\left(\frac{\partial y_k}{\partial \theta}\right)^2$ subject to nonlinear dynamics (Eq. 4). For the RL formulation, the per-step reward is $r_k = \left(\frac{\partial V_k}{\partial k_n}k_n\right)^2$ under normal operation and a fixed penalty $M$ under constraint violation (Eq. 12). Offline identification minimizes squared voltage error: $\theta_i^* = \arg\min_{\theta\in\Theta}\sum_{k=0}^{N-1}(V_{exp}(I_k)-V_k(\theta,I_k))^2$ (Eq. 15).","Across compared experimental designs, the RL-HIL profile achieved the lowest reported drive-cycle verification median error of 12.32 mV with FI = 0.08 V^2 in 1800 s (30 minutes). The RCID protocol achieved FI = 0.42 V^2 and 13.33 mV median verification error but required about 103333 s (≈28 hours). Conventional fixed 1C and 5C discharge profiles had much higher verification errors (47.98 mV and 47.31 mV) with low FI (0.01 V^2 and 0.05 V^2) despite experiment lengths of 3781 s and 701 s, respectively. The Bayesian optimization identification was reported to converge consistently across ten initial guesses within the constraint set $k_n\in[2\times10^{-11},1\times10^{-8}]$.",None stated.,"The demonstrated OED is primarily a single-parameter case study (k_n); performance and identifiability may change materially for multi-parameter estimation where sensitivities are correlated and FI must consider the full matrix (e.g., D-optimality). The reward uses a sensitivity-based term that relies on an analytical model and symbolic differentiation, which may be difficult to replicate for higher-fidelity models or in the presence of unmodeled dynamics and temperature/aging effects. The approach depends on a state estimate (cse,p) from a learned SINDYc model; estimator bias or drift could affect the RL policy and identifiability outcomes. Code is not provided, which limits reproducibility and independent verification of the RL training and HIL integration details.",The authors state that the framework will be extended to generate OEDs for other model parameters or for groups of parameters altogether.,"Extend the method to true multi-parameter optimal design using matrix-valued FI objectives (e.g., D-/A-optimal) and study robustness under parameter cross-coupling and practical constraints. Develop self-starting/robust variants that account for unknown noise levels, sensor biases, temperature variation, and cell-to-cell variability, and evaluate performance across multiple cells/aging states. Provide an open-source implementation (MATLAB/Python) and standardized benchmarks against established OED solvers and battery test protocols to improve reproducibility. Investigate safety-aware RL formulations (e.g., constrained RL) to enforce voltage/current/thermal limits without relying on large penalty terms.",2504.15578v1,https://arxiv.org/pdf/2504.15578v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:39:43Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction,D-optimal,"3 parameters (r, K, C0)",Food/agriculture|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/Jane0917728/Optimal-Experimental-Design-and-Parameter-Estimation-with-Autocorrelated-Observation-Noise,"The paper develops an optimal experimental design framework to choose observation time points that minimize parameter uncertainty when fitting dynamical models with observation noise. It considers both independent Gaussian noise and autocorrelated Ornstein–Uhlenbeck (OU) noise, showing that noise correlation structure can substantially change which time points are optimal. Two objective functions are used: a local criterion based on maximizing the determinant of the Fisher information matrix (D-optimality), and a global criterion based on Sobol’ total-effect indices assembled into a “global information matrix,” also optimized via determinant maximization. The optimization is performed for the logistic growth ODE with three parameters (r, K, C0), with constraints on the experiment horizon and a minimum separation between measurements. Through simulation-based profile likelihood inference, the authors show optimized schedules often yield tighter confidence intervals (sometimes enabling reliable estimation with fewer observations) and that assuming the wrong noise model can misstate uncertainty and shift optimal designs.","The dynamical system is the logistic ODE $\frac{dC}{dt}=rC(1-C/K)$ with explicit solution $C(t)=\frac{C_0K}{(K-C_0)e^{-rt}+C_0}$. For IID Gaussian noise, the Fisher information entries are $F_{ij}=\frac{1}{\hat\sigma^2_{\mathrm{IID}}}\,\theta_i\theta_j\,\frac{\partial C}{\partial\theta_i}\left(\frac{\partial C}{\partial\theta_j}\right)^T$, and the D-optimal design maximizes $\det(F)$ over observation times with spacing constraints. For OU noise, the covariance is $\Sigma_{ij}=\frac{\sigma^2_{\mathrm{OU}}}{2\phi}e^{-\phi|t_i-t_j|}$, yielding $F_{ij}=\theta_i\theta_j\,\frac{\partial C}{\partial\theta_i}\Sigma^{-1}\left(\frac{\partial C}{\partial\theta_j}\right)^T$, and the same determinant criterion is used; the global alternative forms $G_{ij}=\sum_s S_i(t_s)S_j(t_s)$ (IID) or $G_{ij}=S_i\Sigma^{-1}S_j^T$ (OU) and maximizes $\det(G)$.","Under uncorrelated noise, both FIM- and Sobol-based optimized schedules cluster measurements into early-time groups (informing $r$ and $C_0$) and late-time points near steady state (informing $K$), and the authors report that with 5 optimized observations all three parameters can be estimated with closed profile-likelihood confidence intervals whereas 5 evenly spaced points fail to tightly identify $r$ and $C_0$. Under correlated OU noise (e.g., $\phi=0.02$), optimal schedules shift toward more evenly spread early measurements up to roughly mid-experiment with typically a single late observation near $t_{\mathrm{final}}$, reflecting diminished information gain from closely spaced correlated observations. Across 1000-simulation studies, increasing the number of observations reduces mean confidence-interval widths, and optimized designs generally improve precision versus evenly spaced sampling (with an exception noted for $K$, which can slightly favor even spacing). The paper also shows that misspecifying correlated noise as IID can materially distort perceived uncertainty, changing which parameters appear more/less confidently estimated.",None stated.,"The work focuses on a single illustrative ODE (logistic growth) with three parameters; it is unclear how the optimization behavior scales to higher-dimensional or structurally more complex models (e.g., stiff systems, partially observed states). The local (FIM) designs depend on assumed nominal parameter values, but robustness to poor initial guesses is not systematically evaluated beyond contrasting with a global Sobol-based objective. The OU setting assumes the autocorrelation parameter $\phi$ is known “to avoid issues with practical identifiability,” which limits applicability when correlation strength must be inferred from the same data. Empirical validation is simulation-heavy, with no real experimental dataset demonstrating end-to-end gains under practical constraints beyond minimum time spacing.","The authors propose extending the methodology to other mathematical biology models to derive more general insights about autocorrelated measurement processes. They also suggest developing a method to diagnose whether observation noise is correlated or independent. Finally, they note it would be promising to optimize experimental conditions beyond measurement times (e.g., external inputs) and to consider multi-objective optimal experimental design that incorporates observation costs versus estimation quality.","A self-starting or Bayesian-design variant that integrates uncertainty over $\phi$ (and possibly $\sigma$) could make the approach usable when correlation parameters are unknown and must be learned from data. Extending the design criteria to explicitly target prediction accuracy (e.g., I- or G-optimality) could better support downstream forecasting goals, especially when late-time behavior is of primary interest. Providing an open-source MATLAB implementation as a reusable toolbox (or porting to Python/R) with standardized benchmarks would improve adoption and facilitate comparisons to alternative OED methods (e.g., Bayesian OED, particle-based designs). Incorporating model misspecification-robust objectives (e.g., minimizing worst-case CI width over plausible noise models) would address the demonstrated sensitivity to incorrect noise assumptions.",2504.19233v1,https://arxiv.org/pdf/2504.19233v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:40:24Z
TRUE,Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Model discrimination,Bayesian D-optimal|Other,"Variable/General (design variable d is spatiotemporal coordinate; example uses d={d_x,d_y,d_t})",Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Upon request,NA,"The paper proposes a hybrid Bayesian experimental design (BED) framework for calibration under model discrepancy, where low-dimensional physical parameters are treated with standard BED and full Bayesian updating while high-dimensional discrepancy parameters (neural-network coefficients) are handled via an auto-differentiable ensemble Kalman inversion (AD-EKI). The key idea is to approximate information gain for high-dimensional parameters using an ensemble-based Gaussian KL divergence between the initial and EKI-updated ensembles, yielding a differentiable utility that enables gradient-based optimization over design variables. The method is framed for sequential (adaptive) experimentation, iteratively selecting designs for physical parameters and discrepancy calibration, updating physical posteriors and retraining discrepancy networks. Demonstrations use a convection–diffusion contaminant source inversion problem where the design is a single spatiotemporal measurement location; the approach corrects both parametric error (wrong source strength) and structural error (wrong source functional form) and reduces bias in inferred source location. The paper also empirically studies scalability of AD-EKI with respect to ensemble size and number of EKI iterations, including the impact of checkpointing on memory/time.","BED selects designs by maximizing expected utility: $d^*=\arg\max_{d\in\mathcal D}\mathbb E[U(\theta,y,d)]$ with utility as KL information gain $U= D_{\mathrm{KL}}(p(\theta\mid y,d)\|p(\theta))$ and expected information gain $\mathrm{EIG}(d)=\mathbb E_{y\mid d}[D_{\mathrm{KL}}(p(\theta\mid y,d)\|p(\theta))]$. For discrepancy calibration, EKI updates an ensemble via $\theta_{n+1}^{(j)}=\theta_n^{(j)}+\Sigma_{\theta g}^n(\Sigma_{gg}^n+\Gamma)^{-1}(y^{(j)}-g_n^{(j)})$, and the information gain is approximated by a closed-form Gaussian KL between the initial and final ensembles: $\tilde D_{\mathrm{KL}}=\tfrac12\{\mathrm{tr}[(\Sigma_0)^{-1}\Sigma_K]-d_\theta+\ln(\det\Sigma_0/\det\Sigma_K)+(\bar\theta_0-\bar\theta_K)^\top(\Sigma_0)^{-1}(\bar\theta_0-\bar\theta_K)\}$.","In the parametric-error source-inversion example, sequential BED with AD-EKI-based error correction drives the inferred MAP source location closer to the truth across stages, whereas a baseline that ignores discrepancy shows larger and less stable estimation error (distance metric $D$ in Eq. (22)). The corrected discrepancy parameter (source magnitude $\theta_s$) is shown to converge toward the true value over correction iterations. In the structural-error example with a 37-parameter neural-network discrepancy model, the proposed approach yields posterior source-location estimates that converge to the true location and improves over the no-correction baseline; using true measured data for design gives faster convergence than using predicted data, but both become comparable after several stages. Scalability experiments report roughly linear growth of peak memory and time with ensemble size, and memory growth with EKI iterations without checkpointing but approximately constant peak memory with checkpointing.","The authors acknowledge that the ensemble-based KL approximation is formally derived under Gaussian assumptions and may degrade for strongly skewed or multimodal posteriors. They also note potential issues with small ensemble sizes: low-rank sample covariances restrict updates to the ensemble subspace, leading to biased estimates, underestimated uncertainty, and possible divergence or slow convergence. They further mention sensitivity of EIG to prior misspecification as a general concern, though they argue their framework reduces this sensitivity in their setting.","The work is validated only on a “classical” numerical convection–diffusion example; no real experimental/field case study is provided to demonstrate practical data-collection constraints, sensor noise complexities, or model-form uncertainty in practice. The hybrid split between “full Bayesian” for low-dimensional parameters and EKI/gradient-based training for discrepancy implicitly assumes the two-stage decoupling (and use of a MAP plug-in for $\theta_G$ when designing for $\theta_{NN}$) does not materially harm joint optimality; the approximation error from this decoupling is not quantified. Comparisons are mainly against a no-correction baseline (and qualitative references to prior work) rather than a broad set of modern high-dimensional BED baselines (e.g., variational/flow-based EIG estimators) on matched compute budgets.","The authors state that techniques to improve robustness of ensemble Kalman methods under non-Gaussianity or small-ensemble effects—such as covariance inflation, localization, dropout, and improved covariance estimation—can be integrated into their framework, and they leave the detailed integration and evaluation of these enhancement techniques as future work.","A useful next step is benchmarking against contemporary scalable BED methods (e.g., variational EIG with normalizing flows, Laplace/INLA variants, and neural-operator surrogates) on shared testbeds and compute budgets to clarify when AD-EKI is preferable. Extending the framework to correlated/non-Gaussian observation noise and temporally/spatially correlated data (common in PDE sensing) would improve realism, as would a self-starting/online variant that handles unknown noise levels and hyperparameters. Providing open-source code and a reproducible benchmark suite (including real-data case studies such as environmental monitoring or fluid experiments) would materially strengthen adoption and external validation.",2504.20319v2,https://arxiv.org/pdf/2504.20319v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:41:11Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Robustness|Cost reduction,D-optimal|Bayesian D-optimal,Variable/General (mixed discrete and continuous factors; examples include 9 factors (8 discrete + 1 continuous) and 6 factors (1 discrete + 5 continuous)),Manufacturing (general)|Semiconductor/electronics|Healthcare/medical|Other,Simulation study|Other,TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=ForLion,"The paper develops Expected-Weighted (EW) D-optimal design theory and computation for experiments under general parametric models with mixed discrete and continuous factors, targeting robustness to unknown/misspecified parameter values. It formalizes both integral-based EW D-optimality using a prior on parameters and sample-based EW D-optimality using bootstrapped or sampled parameter draws, and proves existence/structure results (including support-size bounds up to p(p+1)/2) and an equivalence-theorem-style optimality condition via a sensitivity function. Methodologically, it introduces an EW ForLion algorithm (adapting the ForLion framework) to compute EW D-optimal approximate designs over mixed-factor regions, plus a rounding algorithm to convert approximate designs to exact designs on prespecified grids and sample sizes. The approach is demonstrated on real experiments under multinomial logistic models and generalized linear models, showing high efficiency relative to locally D-optimal designs and improved robustness to parameter misspecification while often reducing the number of distinct experimental settings (time/cost). Software is implemented in R and released via the ForLion CRAN package.","EW D-optimality maximizes the determinant of the expected Fisher information: for a design $\xi=\{(x_i,w_i)\}$, $f_{EW}(\xi)=\left|\sum_i w_i\,\mathbb{E}[F(x_i,\Theta)]\right|$, where $\mathbb{E}[F(x,\Theta)]=\int_{\Theta}F(x,\theta)Q(d\theta)$. With pilot/bootstrapped parameter draws $\hat\theta_1,\ldots,\hat\theta_B$, sample-based EW uses $f_{SEW}(\xi)=\left|\sum_i w_i\,\hat{\mathbb{E}}[F(x_i,\Theta)]\right|$ where $\hat{\mathbb{E}}[F(x,\Theta)]=\frac{1}{B}\sum_{j=1}^B F(x,\hat\theta_j)$. Optimality is characterized by the sensitivity $d(x,\xi)=\mathrm{tr}(F(\xi)^{-1}F_x)$ and EW D-optimality iff $\max_{x\in\mathcal{X}} d(x,\xi)\le p$.","For the paper-feeder application (multinomial/cumulative logit with non-proportional odds, $p=32$), the proposed EW ForLion design achieved 100% relative efficiency (baseline for comparison) while the original allocation on 183 settings achieved 63.03%, and Bu et al. (2020)-style EW designs restricted to the original 183 settings achieved about 88.15–88.16%. Grid-based EW designs at 2.5 spacing achieved 98.40% efficiency but at much higher computation (e.g., 15,201s and 69,907s) and more support points, whereas EW ForLion used 38 points and required less time than grid-based search; rounded exact designs with grids (e.g., 0.1) maintained ~98.46% efficiency with very fast rounding time (~0.28s). Robustness checks against 100 locally D-optimal designs (one per bootstrapped parameter set) showed EW ForLion had the highest five-number-summary efficiencies (min 0.8363, median 0.9364, max 0.9542), outperforming alternative robust designs.","The authors note that the resulting EW D-optimal design depends on the chosen prior distribution or on the sampled/bootstrapped parameter vectors, and that for some multinomial models the feasible parameter space can be non-rectangular, making direct integral-based expectation calculations difficult (motivating sampling/bootstrapping). They also mention computational intensity in some settings, e.g., using only $B=100$ bootstraps for the paper-feeder example due to computation time.","The paper’s empirical validation is largely via a small number of case studies; broader benchmarking across many mixed-factor problems (varying dimension, correlation/overdispersion, model misspecification, or constraints) would strengthen generalizability. The robustness notion is primarily parameter-misspecification within the assumed model class; robustness to model-form misspecification (wrong link, omitted interactions, overdispersion, dependence) is not systematically analyzed. Practical implementation details (e.g., guidance for choosing algorithmic thresholds $\delta,\epsilon$ and rounding hyperparameters; sensitivity to these choices) are discussed but not fully standardized, which may affect reproducibility in difficult designs.","The authors suggest the sample-based strategy could be incorporated with other design criteria, such as minimax and Bayesian optimality, as a practical tool for researchers. They also emphasize applying the approach under reasonable prior/parameter-vector choices for robust design construction in practice, implying further exploration of prior/parameter-aggregation strategies.","Extending EW ForLion to handle correlated/clustered responses or time-series dependence (where Fisher information is not a simple sum across runs) would broaden applicability. Developing diagnostics and automated/default tuning rules for the merging and rounding thresholds, and providing sensitivity analyses within the software, would improve practical robustness. Additional work could address constraints such as forbidden factor combinations, multiple responses/objectives (compound criteria), and sequential/adaptive EW design where parameter draws are updated online during experimentation.",2505.00629v3,https://arxiv.org/pdf/2505.00629v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:41:54Z
TRUE,Bayesian design|Sequential/adaptive,Parameter estimation|Prediction|Other,Other,Variable/General (examples: FA with K=43; MoFA/PPCA mixture with 10 components each with 70 latent dimensions),Other|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Personal website,https://homepages.inf.ed.ac.uk/ckiw/mypages/software.html,"The paper models foveated visual fixations (“glimpses”) as known linear retinal transformations (location-dependent downsampling) of a latent high-resolution image and couples this with factor analysis (FA) and mixtures of factor analyzers (MoFA) to enable exact Gaussian inference of latent variables from one or multiple fixations. It formulates the selection of the next fixation location as a Bayesian experimental design problem, using Expected Information Gain (EIG; mutual information) as the criterion; for FA it derives a closed-form EIG expression, and for MoFA it derives an analytically tractable upper bound. Using Frey faces and MNIST (digit ‘2’) data with discrete candidate offsets, it searches for fixation locations that maximize EIG (or the MoFA bound) and shows these “BED” choices reduce reconstruction RMSE compared to random fixation choices. It also presents gradient-based learning of FA/MoFA parameters directly from retinal-transformed observations (Y-data) with location-dependent noise models. Overall, the work advances active sensing/active vision by combining explicit linear sensing geometry with exact or bounded Bayesian design objectives for efficient fixation planning and reconstruction.","Retinal observation model: $y_a = V_{\ell(a)} x$ with known location-dependent linear operator $V_{\ell(a)}$. FA generative model: $x = \mu + Wz + e$ with $z\sim\mathcal N(0,I_K)$ and diagonal noise; under RT, $y_\xi$ is Gaussian and the EIG for FA has closed form $\mathrm{EIG}(z\mid\xi)=\tfrac12\left[\log\lvert W_\xi W_\xi^T+\Psi^y_\xi\rvert-\log\lvert\Psi^y_\xi\rvert\right]$. For MoFA, an information-gain bound is derived: $\mathrm{EIG}_{\text{mix}}\le H(\pi)+\sum_m \pi_m\,\mathrm{EIG}_m(z_m\mid\xi)$.","On Frey faces, BED-selected fixations outperform random fixations in reconstruction RMSE: with one fixation RMSE is 0.1126 (BED) vs 0.1251 (random), and with two fixations 0.0952 (BED) vs 0.1081 (random), with sign-test p-values $7.65\times10^{-17}$ (1 fixation) and $2.33\times10^{-35}$ (2 fixations). On MNIST ‘2’s with a MoFA model and the EIG upper bound, RMSE is 0.1416 (BED) vs 0.2058 (random) for one fixation and 0.1078 (BED) vs 0.1734 (random) for two fixations, with p-values $4.92\times10^{-129}$ and $2.13\times10^{-193}$. For learning from Y-data on Frey faces, log-likelihood per example improves from an independent Gaussian baseline (69.08) to optimized models, reaching 115.59 when optimizing $W$ and location-specific noise variances from a PCAMV initialization.","For MoFA, the EIG is not available in closed form; the paper relies on an upper bound and notes that, on their data, the gap between upper and lower bounds can be large, so they choose the upper bound based on empirical separation of components. The paper also notes that the simple factorization $p(z,y_{1:J})=p(z)\prod_j p(y_j\mid z)$ can over-count evidence when retinal observations overlap (acknowledged in the multiple-glimpse fusion discussion). Additionally, it states they were not able to derive an EM algorithm for estimating $W$ and location-dependent noise parameters with varying $V_{\ell(i)}$, using gradient ascent instead.","The experimental-design search is over a discrete grid of candidate offsets (35 or 36), so scalability to continuous fixation locations or larger search spaces is not demonstrated. The evaluation focuses on reconstruction RMSE on relatively simple, centered datasets (faces and a single MNIST class), leaving uncertainty about robustness to cluttered natural scenes, occlusions, or significant distribution shift. The FA/MoFA assumptions (Gaussian latents, linear loadings, diagonal noise, and largely i.i.d. observations) may not match real eye-movement data with temporal dependence, sensor noise correlations, or non-Gaussian structure; overlap handling is acknowledged but not fully resolved in the main model used for results.","The paper suggests extending the approach to deep generative models for $p_\theta(x\mid z)$ while addressing harder inference for $p(z\mid y_a)$, potentially using variational encoders that take both $y_a$ and location $\ell(a)$ as input and approximating EIG (e.g., via bounds or nested Monte Carlo). It proposes broadening the retinal transformation beyond 2D shifts to more general 3D viewpoint changes (linking to novel view synthesis/NeRF-style settings) while incorporating variable-resolution sensors and Bayesian experimental design. It also suggests moving from single-object datasets to richer multi-object scenes requiring structured generative models capturing object co-occurrence and relationships.","Develop an explicitly overlap-aware observation model for multiple fixations (e.g., a consistent latent-image likelihood that prevents double-counting) and quantify how overlap affects EIG-based planning. Extend the design criterion to task-conditioned objectives (e.g., classification or detection) and compare EIG to task-specific utilities on shared benchmarks with human fixation data. Provide open-source, reproducible implementations (including the BED search and learning code) as a versioned repository/package and test computational performance for larger images and more candidate fixation locations.",2505.01249v2,https://arxiv.org/pdf/2505.01249v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:42:38Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment|Other,Prediction|Parameter estimation|Cost reduction|Other,I-optimal (IV-optimal)|Other,"Variable/General (calibration parameters p; examples include p=1, p=2, p=7)",Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Case study (real dataset),TRUE,R|Python,Public repository (GitHub/GitLab),https://github.com/parallelUQ/PUQ,"The paper proposes a batch sequential (parallel) experimental design framework for Bayesian calibration of expensive stochastic simulation models, where design decisions allocate each batch either to replication at existing parameter settings or to exploration of new settings. It introduces two new integrated-variance (IVAR) acquisition criteria that target uncertainty in the (unnormalized) posterior density rather than global emulator error: (i) an IVAR-based replicate allocation rule over existing design points, and (ii) an IVAR-based greedy batch construction rule for selecting new parameter locations with a user-specified number of replicates per new point. The approach uses Gaussian-process emulators for stochastic outputs (stochastic kriging assumptions with heteroskedastic noise), implemented via hetGP-style modeling of input-dependent intrinsic variance; high-dimensional simulator outputs are handled by fitting independent emulators for each output component. Performance is demonstrated via Monte Carlo experiments on synthetic test problems and applications to epidemiological compartmental simulators (SIR and SEIRDS), showing improved posterior learning accuracy compared with IMSE-based sequential designs, variance-greedy heuristics, and space-filling baselines, especially by concentrating effort in posterior-relevant regions and avoiding high-noise but low-posterior regions. The method is designed to exploit parallel computing resources through batch synchronous updates and provides open-source implementation in the PUQ Python package.","The stochastic simulator is modeled as $\zeta(\vartheta)=\eta(\vartheta)+\nu$ with $\nu\sim\mathrm{MVN}(0,R(\vartheta))$ and observed data as $y=\eta(\theta)+\varepsilon$, $\varepsilon\sim\mathrm{MVN}(0,\Sigma)$. The (unnormalized) posterior is $\tilde p(\theta\mid y)\propto p(y\mid\theta)p(\theta)$ with Gaussian likelihood $p(y\mid\theta)=\mathcal N(y;\eta(\theta),\Sigma)$. The emulator predictive distribution is $\eta(\vartheta)\mid D_t\sim\mathrm{MVN}(\mu_t(\vartheta),S_t(\vartheta))$, and Lemma 3.1 gives closed-form $\mathbb E[\tilde p(\theta\mid y)]$ and $\mathrm{Var}[\tilde p(\theta\mid y)]$ under emulator uncertainty, which are integrated over $\Theta$ to form the IVAR criteria for replication (optimize allocations $\Delta a_t$) and exploration (choose new $\hat\vartheta$ to minimize $\int \mathbb E[\mathrm{Var}(\tilde p)]\,d\theta$).","Across synthetic calibration problems (unimodal/banana/bimodal) and batch sizes $b\in\{8,16,32,64\}$ with fixed total simulation budgets (e.g., 256 runs), IVAR-based batch sequential design yields lower mean absolute difference (MAD) between estimated and true posteriors than IMSE-based sequential design and a variance-greedy baseline, while better concentrating samples within the calibration-relevant region. In illustrative 1D examples, IMSE tends to over-allocate replicates to high intrinsic-noise regions even when posterior mass is near zero, whereas IVAR shifts replicates toward high-posterior regions and avoids wasteful replication. In epidemiological simulators (SIR with $p=2,d=3$ and SEIRDS with $p=7,d=6$), IVAR improves posterior prediction accuracy over IMSE and typically provides broader, more shape-faithful coverage of the posterior than pointwise-variance heuristics. The paper also reports emulator validation (MAPE) for mean predictions and intrinsic variance estimation on SIR/SEIRDS reference sets, indicating low MAPE for mean predictions (<10% in reported outputs) and moderate error for intrinsic variance (≤25%).","The paper notes that its derivations assume known intrinsic variance functions $r_j(\cdot)$, which must be estimated in practice; it addresses this via hetGP but acknowledges that kernel choices (e.g., Gaussian kernel smoothness) may be inappropriate for some simulators and additional kernels (e.g., Matérn) are left for future implementation. It also states that extensions to include a discrepancy term in the calibration model (Kennedy–O’Hagan) are left as future work due to identifiability/complexity issues. The exploration batch construction uses a kriging-believer-style update during batch selection, and the text notes potential prediction inaccuracies if the emulator is poor in unexplored regions.","The independent-emulator approach across output dimensions ignores cross-output correlation, which could reduce efficiency or misrepresent posterior uncertainty when outputs are strongly dependent. The decision rule that dedicates an entire batch to either replication or exploration (not a mix) may be suboptimal in some settings and could be sensitive to user-chosen $(\hat b,\hat a)$ and candidate-set generation; robustness of performance to these choices is not fully characterized. Computational cost of IVAR integration over $\Theta$ can grow quickly with parameter dimension, and while approximations (reference grids/LHS) are used, scalability limits and error from numerical integration are not fully quantified for high-dimensional calibration beyond the SEIRDS example.","The paper suggests extending the method to calibration models with an explicit discrepancy term (Kennedy–O’Hagan framework). It proposes adding support for alternative covariance kernels (e.g., Matérn) in the software to relax the Gaussian kernel’s smoothness assumptions. It also calls for studying one-at-a-time versus batch updates, integrating exploration and replication within the same batch via decision rules, and developing guidance for selecting the exploration hyperparameters $(\hat b,\hat a)$; it further mentions extensions to physical experiment design and joint design of simulation and physical experiments, and exploring asynchronous batch procedures and performance modeling trade-offs.","A self-starting/online calibration setting with unknown $\Sigma$ and unknown/estimated observation error could be incorporated to better match practical Phase I/Phase II workflows. Extending the acquisition criteria to multivariate-output emulators that model cross-output covariance (e.g., coregionalization, latent factor GPs) could improve posterior uncertainty quantification. More systematic comparisons against modern Bayesian optimization/batch active learning baselines (e.g., qEI/qKG variants adapted to likelihood/posterior learning) and robustness studies under model misspecification (non-Gaussian noise, simulator failures, heavy tails) would strengthen practical guidance.",2505.03990v1,https://arxiv.org/pdf/2505.03990v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:43:27Z
TRUE,Split-plot|Computer experiment|Bayesian design|Other,Other,Not applicable,Variable/General,Other|Theoretical/simulation only,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/ucla-cdl/planetlang,"The paper introduces a formal grammar of composable operators for specifying experimental assignment procedures (e.g., counterbalancing, Latin squares, crossing and nesting designs) aimed at improving clarity and reproducibility of human-subject experiments. It instantiates this grammar as PLanet, a Python-based DSL that compiles user-specified assignment requirements into a constraint-satisfaction problem over design matrices and solves it using an SMT solver (Z3), producing viable experimental plans and unit-to-plan assignment tables. The approach separates (i) identifying experimental units (via relational tables/SQL), (ii) constructing condition sequences (plans) under matrix constraints, and (iii) mapping plans to units, enabling exploration of alternative designs under practical constraints (e.g., limited participants). The authors evaluate expressivity by re-implementing 15 HCI experiments and report PLanet can express 14/15, outperforming Touchstone and edibble, and they present case studies with three researchers showing PLanet helps externalize implicit assumptions and explore design alternatives. The work advances experimental-design tooling by formalizing assignment as composable, verifiable constraints rather than a fixed menu of canonical designs.","Key formalism includes representing assignment plans as a design matrix and expressing operators as constraints over matrix rows/columns; counterbalancing is encoded as the constraint that each condition appears equally often in each column (position) across plans. The nest operator is mapped directly to the Kronecker product: for matrices $A\in\mathbb{R}^{n\times m}$ and $B\in\mathbb{R}^{k\times p}$, $A\otimes B$ forms a $kn\times mp$ block matrix with blocks $A_{ij}B$. Plan generation is formulated as an SMT satisfiability problem solved with Z3, using bitvectors to encode assignments and masking to reason about individual variables and their combinations.","In an expressivity study over 15 sampled HCI experiments, PLanet could express 14/15 designs, compared with edibble at 12/15 and Touchstone at 8/15 (with additional limitations noted due to missing/unfinished implementations). The single inexpressible design required assigning multiple unique (non-repeating) orders per participant (three distinct orders drawn from a 12×12 Latin square), which none of the evaluated tools could represent. Case studies with three researchers showed they could replicate or prototype designs in PLanet and use operations like counterbalance, cross, nest, and limit_plans to explore alternatives and make constraints explicit (e.g., identifying that limiting plans can yield multifactorial Latin-square-like assignments under participant constraints).","The authors note PLanet cannot currently express designs that assign multiple unique orders to each participant while ensuring no order repeats, because PLanet constrains distributions of individual conditions rather than treating whole orders as assignable entities. They also acknowledge their usability evidence is limited to three first-use case studies and did not include an experimental-design expert, and that broader/longer field deployments could reveal additional needs and benefits.","The paper centers on assignment procedures and largely assumes that the specified constraints (e.g., counterbalancing) align with valid statistical analysis plans; it provides limited guidance on how generated assignments interact with model specification, missing-data handling, or violations like noncompliance. Performance/scalability of SMT-based plan generation is discussed qualitatively (combinatorial growth) but not benchmarked systematically across large-factor/large-trial settings, which may affect practicality for complex designs. Real-world validation is mainly through re-encoding published designs and small researcher case studies; there is limited evidence from end-to-end deployments where PLanet-generated assignments are used in actual data collection at scale.","The authors propose extending the formalism beyond assignment to also formalize measurement and sampling, including distinguishing assigned versus measured variables and integrating sampling procedures over populations via the SQL-based unit system. They also suggest building tools to assess and compare experiments for semantic equivalence and exploring generalization to other disciplines (e.g., balancing across unit attributes rather than time). Finally, they call for expanding scope to adaptive experiments (e.g., microrandomized trials), which may require extending the matrix representation to sparse, multidimensional tensors or other structures to support uneven trial counts and data-dependent assignment.","Providing a reference implementation with richer diagnostics (e.g., automatically reporting balance/orthogonality metrics, detecting aliasing/partial confounding, and generating CONSORT-style assignment diagrams) would improve practitioner trust and interpretability. Adding native support for common DOE constructs used outside HCI—such as split-plot restrictions with hard-to-change factors, blocked/fractional factorial designs, and optimal design criteria—could broaden adoption and help connect PLanet’s operator grammar to classical DOE objectives. A systematic computational study (runtime/solution diversity) and integration into experiment platforms (e.g., jsPsych/online study runners) would clarify scalability and reduce friction for real deployments.",2505.09094v2,https://arxiv.org/pdf/2505.09094v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:44:11Z
TRUE,Optimal design|Computer experiment|Sequential/adaptive|Other,Prediction|Parameter estimation|Cost reduction|Other,Space-filling|Minimax/Maximin|Other,"Variable/General (examples include d=3,4,5,6,9, up to 15 in comparisons; general d discussed)",Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Python,Not provided,https://spacefillingdesigns.nl|http://neilsloane.com/oadir/|https://www.sciencedirect.com/science/article/pii/S0098135417302090,"This paper is a selective review of design of experiments for computer emulation, emphasizing space-filling designs used to build accurate surrogate models (especially Gaussian process/kriging emulators) for uncertainty quantification and digital twin applications. It surveys major space-filling design families and criteria, including Latin hypercube designs and their distance-based (maximin/minimax, ϕp), orthogonality-based (orthogonal/nearly-orthogonal LHDs), and projection-based variants (OA-based LHDs, ARD, MaxPro, UP), as well as low-discrepancy sequences and designs for constrained and advanced settings (multi-fidelity nested designs, sliced designs, mixed qualitative–quantitative/marginally coupled designs, weighted/minimum-energy/non-uniform designs). A modeling-oriented justification is provided by connecting fill distance and separation distance to approximation and GP prediction error bounds, motivating minimax (fill distance) and maximin (separation) criteria. The review includes numerical comparisons using GP fitting (Matérn 3/2 kernel via DiceKriging) across several test simulators (e.g., Robot Arm and benchmark functions) and reports trade-offs among criteria and computational costs. The paper concludes with research directions in scalability, adaptive/active sampling, handling mixed/non-Euclidean or constrained spaces, and integration with machine learning.","Latin hypercube generation: $x_{ij}=(l_{ij}-u_{ij})/n$ with $u_{ij}\sim\mathrm{Unif}(0,1)$ (Eq. 1). GP/kriging model: $y_i=\mu+Z(x_i)+\epsilon_i$ with covariance $\tau^2K(x_1,x_2;\theta)+\sigma^2\delta(x_1,x_2)$ and predictor $\hat y(x)=\hat\mu+k(x)^\top(K+\eta I)^{-1}(y-\hat\mu\mathbf 1)$. Space-filling links use fill distance $h_{X,\Omega}=\sup_{x\in\Omega}\min_{x_j\in X}\|x-x_j\|_2$ (Eq. 4) and separation distance $h_{X,\mathrm{sep}}=\min_{x\neq x'\in X}\|x-x'\|_2$, with maximin maximizing the latter and minimax minimizing worst-case nearest-neighbor distance; projection criteria include ARD (Eq. 8) and MaxPro $\psi(X)=\left[\frac{1}{\binom{n}{2}}\sum_{i<j}\frac{1}{\prod_{l=1}^d(x_{il}-x_{jl})^2}\right]^{1/d}$ (Eq. 9).","In Example 5.1 (algorithmic comparison for maximin LHD generation over $n\in\{30,50,70,100,150,200\}$ and $d\in\{3,6,9,12,15\}$), the R package SLHD consistently achieved better maximin performance (smaller $\phi_p$ with $p=50$ and larger minimum distance) than DiceDesign’s ESE and SA methods across all tested cases. In Example 5.3 (10 runs, 4 factors), the ARD criterion (with $J=\{1,2\}$, $\lambda=1$) values were 0.306 (maximin), 0.292 (ARD), 0.297 (MaxPro), and 0.292 (UP), indicating slightly improved 2D projection properties over maximin. In Example 6.1 (Robot Arm simulator; $n=81,162,256$; 50 replications; Matérn 3/2 GP), smaller designs favored the distance-distributed ‘lhsbeta’ method while maximinLHD performed worst; for larger $n$, UP gave the best prediction accuracy and OALHS the worst. In Example 6.2 (four benchmark functions with $d=3,5,6,9$ and $n=50,100,200,300$), no single design dominated across all settings; ‘lhsbeta’ tended to have higher prediction error but remained acceptable on the log-RMSPE scale, and ARD/UP were substantially faster to compute than maximinLHD/MaxPro as $d$ increases (Table 6).",None stated.,"As a selective review, the numerical comparisons cover a limited set of simulators, kernels (primarily Matérn 3/2 in the reported experiments), and tuning/implementation choices, so conclusions about “best” designs may not generalize to other surrogate models, nonstationary GP settings, or different acquisition objectives (e.g., calibration, contour finding). Many design criteria are discussed conceptually, but reproducible end-to-end code for the paper’s full empirical pipeline is not provided, making replication of all figures/tables dependent on reimplementation. The work largely assumes i.i.d. inputs within a bounded hyper-rectangle (often scaled to $[0,1]^d$) and standard GP modeling assumptions; practical issues like simulator failures, heteroskedastic noise, or strong input constraints may require additional robustness analysis beyond the reviewed methods.","The paper highlights future directions in (i) scalability—more computationally efficient algorithms for large-scale and high-dimensional design construction, (ii) adaptivity—sequential/active learning strategies for dynamically refining designs, (iii) generalizability—extensions of space-filling ideas to non-Euclidean and mixed input spaces (categorical/functional variables), and (iv) integration with AI—using machine learning to automate design selection and optimization.","Developing standardized benchmark suites and reporting protocols (common simulators, kernels, budgets, and metrics beyond RMSPE such as calibration or uncertainty accuracy) would improve comparability across new design methods. More theory and practice for joint design-and-model selection (e.g., designs optimized for nonstationary/treed GPs, deep kernel learning, or Bayesian neural emulators) could broaden applicability. Providing open-source reference implementations (including constrained, multi-fidelity, and mixed-input cases) and diagnostic tools (e.g., projection-uniformity and constraint-feasibility summaries) would materially increase adoption by practitioners.",2505.09596v2,https://arxiv.org/pdf/2505.09596v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:45:01Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Model discrimination|Cost reduction|Other,Other,"2 design variables (We, R*∞); parameter dimension varies by constitutive model (2–3 continuous parameters) plus discrete model index (5 models)",Other,Simulation study|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/InertialMicrocavitationRheometry/IMR_RBF_BOED,"The paper proposes an accelerated Bayesian optimal experimental design (BOED) workflow, RBF–BOED, that reduces the number of expensive forward-model simulations needed to estimate expected information gain (EIG). The key idea is to run the deterministic forward model only on a small, uniformly subsampled batch of prior parameter draws, then use local radial basis function (polyharmonic spline + polynomial augmentation) interpolation to approximate model outputs at all remaining Monte Carlo samples used by the EIG estimator. Designs are chosen by maximizing EIG (a KL-divergence mutual-information objective), and design-space exploration is demonstrated via Bayesian optimization with a Gaussian-process surrogate over the 2D design space. The method is demonstrated on laser-induced cavitation experiments for hydrogel viscoelastic characterization using inertial microcavitation rheometry (IMR), including both single-constitutive-model and multi-model (5-model) design scenarios. Reported results show accurate EIG landscapes and near-baseline performance with large reductions in forward simulations (e.g., ~8% of full cost in the 5-model case).","Design selection maximizes expected information gain: $\mathrm{EIG}(d)=\mathbb{E}_{\theta}\,\mathbb{E}_{Y\mid\theta,d}[\log p(Y\mid\theta,d)-\log p(Y\mid d)]$, with likelihood $p(Y\mid\theta,d)=\mathcal{N}(Y_e(\theta,d),\Sigma)$ and evidence $p(Y\mid d)=\int p(Y\mid\theta,d)p(\theta)\,d\theta$. EIG is estimated with a nested Monte Carlo estimator (DLMC/NMC): $\hat{\mu}_{\mathrm{NMC}}(d)=\frac{1}{N_2}\sum_{j=1}^{N_2}\log\Big[\frac{p(Y^{(j)}\mid\theta^{(0,j)},d)}{\frac{1}{N_1}\sum_{i=1}^{N_1}p(Y^{(j)}\mid\theta^{(i,j)},d)}\Big]$. RBF–BOED replaces expensive outputs with local RBF-interpolated surrogates: $\hat{Y}_e(\phi_0) = \sum_{j=1}^n w_j Y_e(\phi_j)$ where weights come from an augmented PHS RBF system (with polynomial moment constraints) built on the $n$ nearest neighbors in a uniformly subsampled batch of size $N_B\ll N_{\mathrm{EIG}}$.","For a benchmark 3D nonlinear design problem, RBF–BOED produces qualitatively accurate EIG estimates even with small batches (e.g., $N_B=50$) and converges near the Re–NMC baseline by $N_B\approx 500$ (reported as ~2.5% of the baseline computational cost). In LIC single-model design examples, two-parameter models (LKV/NHKV) recover high-fidelity EIG with batch sizes below about $N_B\approx 200$, corresponding to ~96% reduction in forward simulations; more complex 3-parameter constitutive models require larger batches (about $N_B\approx 500$) for stable accuracy. In the 5-model (model-averaged) LIC design, baseline Re–NMC uses $N_{\mathrm{EIG}}=2.5\times10^4$ samples, while RBF–BOED achieves comparable EIG estimates with fewer than about $N_B\approx 2000$ IMR simulations (~8% of the full computational cost; >92% reduction). The authors also report an average compute figure of ~300 CPU core-hours to generate $N_B=10^4$ IMR simulations for the multi-model case and cite ~12.5× BOED acceleration (900 vs 11250 CPU core-hours) for the 5-model design-space study.","The authors note that local RBF surrogates are most efficient in low-dimensional parameter spaces and may struggle as parameter and observation dimensions grow, because required sample sizes increase rapidly and surrogate quality can degrade. They also state the current approach assumes a known, tractable likelihood; applicability is limited when the likelihood is intractable or only implicitly defined (likelihood-free settings). Finally, the implementation focuses on estimating EIG values (not EIG gradients), so it relies on search strategies like Bayesian optimization rather than direct gradient-based design optimization.","The empirical comparisons focus largely on specific LIC/IMR settings (2D design space and a small set of constitutive models); performance and tuning sensitivity (neighbor count $n$, polynomial degree $q$, batch sampling strategy) may vary substantially across other BOED problems and is not systematically stress-tested. Because interpolation is performed separately within each model’s parameter space, discontinuities at model boundaries (discrete model index) and unequal model priors/weights could affect stability in broader model-averaging contexts. The approach depends on smoothness of the forward map in parameters; if the simulator exhibits bifurcations, sharp regime changes, or non-smooth behavior, local RBF interpolation could introduce biased likelihoods and thus biased EIG-driven designs. Also, the MATLAB/implementation details suggest a significant engineering burden (nearest-neighbor search, solving many small linear systems) that could become nontrivial in higher dimensions or large $N_{\mathrm{EIG}}$, even if forward simulations are reduced.","They suggest incorporating dimensionality reduction and/or adaptive stencil selection to improve scalability in higher-dimensional parameter spaces. They propose extending local RBF surrogates to include local variance estimates (analogous to GP variance) to broaden applicability to likelihood-free or implicitly defined likelihood settings. They also propose computing Jacobians from batch samples using RBF-based finite differences (RBF–FD) to estimate EIG gradients and enable gradient-based optimization, noting that bubble-collapse/rebound dynamics may require temporal segmentation for stable gradient estimation.","A natural extension is a sequential (multi-stage) BOED loop that updates the prior/posterior after each physical experiment and reuses/adapts the RBF batch to avoid resampling from scratch. It would also be valuable to develop principled batch-sizing rules (adaptive $N_B$ selection) based on EIG estimator uncertainty or surrogate error bounds, to guarantee target accuracy at minimal simulation cost. Additional work could benchmark RBF–BOED against alternative surrogate families common in BOED (GP surrogates for the simulator, polynomial chaos, reduced-order models) on standardized test suites. Finally, providing a fully reproducible workflow (scripts for data generation, EIG estimation, and BO loops) and multi-language implementations (e.g., Python) would improve adoption.",2505.13283v1,https://arxiv.org/pdf/2505.13283v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:45:55Z
TRUE,Optimal design|Other,Prediction|Parameter estimation|Optimization|Other,Other,Variable/General (R spatial regions; binary treatment per region; cluster count m varied from 1 to m_max=R^(2/3)),Transportation/logistics|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/Mamba413/CausalGraphCut,"The paper proposes a causal graph cut (CGC) approach for designing spatial (and more generally network) experiments in the presence of both interference and correlated outcomes. The design space is restricted to cluster-randomized designs that partition regions into clusters and randomize treatment independently across clusters (balanced at p=0.5), aiming to minimize the MSE of a doubly robust (DR) ATE estimator under neighborhood interference. The authors derive an MSE decomposition showing that interference induces boundary-driven error terms that favor grouping neighboring regions, while positive spatial correlation favors separating regions into different clusters, creating an intrinsic tradeoff. They introduce a quadratic surrogate objective (based on the covariance matrix and adjacency graph) that upper bounds the leading interference contribution and incorporates correlation, enabling efficient optimization via spectral/graph-cut methods; the number of clusters is selected by plugging candidate designs into the (estimated) MSE expression. Simulations across multiple synthetic spatial layouts/covariance structures and a city-scale ridesharing dispatch simulator show CGC achieves substantially lower MSE than baselines (spectral clustering, three-net, causal clustering, global/individual designs), including about a 3.5× MSE reduction in the simulator, and an iterative scheme can learn designs by estimating the covariance from repeated experiments.","The ATE target is $\mathrm{ATE}=\sum_{i=1}^R \mathbb{E}[g_i(\mathbf{1},O)-g_i(\mathbf{0},O)]$ under a neighborhood interference model $Y_i=g_i(A,O)+e_i$. The DR estimator is $\widehat{\mathrm{ATE}}_{DR}=\frac{1}{N}\sum_{t=1}^N\sum_{i=1}^R[\nu_{i,t}(1)-\nu_{i,t}(0)]$ with $\nu_{i,t}(a)=g_i(a,O_t)+\frac{T_{i,t}(a)}{\mathbb{E}[T_{i,t}(a)]}(Y_{i,t}-g_i(a,O_t))$ and $T_{i,t}(a)=\prod_{j\in N_i}\mathbb{I}(A_{j,t}=a_j)$. For two clusters, the surrogate design objective is proportional to a weighted cut: $\frac{8R}{N}\sum_{i\in C_1}\sum_{i'\in C_2} W_{ii'}\Sigma^+_{ii'}-\frac{8}{N}\sum_{i\in C_1}\sum_{i'\in C_2}\Sigma_{ii'}$, where $W$ is the adjacency matrix and $\Sigma^+_{ii'}=\max(\Sigma_{ii'},0)$; it is optimized via Laplacian eigenvectors (graph cuts/spectral clustering).","In the real-data-based ridesharing dispatch simulator (R=85 hexagonal regions), the proposed CGC design yields an ATE estimator MSE approximately 3.5 times smaller than benchmark approaches. Across synthetic spatial layouts (square/rectangle/circle/fan) and covariance structures (constant, truncated constant, exponential) with correlation strength $\rho$ varying, CGC/OCGC consistently achieve substantially smaller MSE than causal clustering (CC), three-net (TNET), and spectral clustering (SC), and also outperform naive global (GD) and individual (ID) designs. Reported aggregate improvements versus GD and ID are about 54% and 72%, respectively, in the synthetic studies. CGC closely matches the oracle-CGC (OCGC) when the covariance is known, indicating the iterative covariance-estimation-and-redesign procedure is effective.","The authors omit the second-order interference term $I_2$ from the surrogate objective to keep optimization tractable, acknowledging these second-order effects may be non-negligible; including $I_2$ would significantly increase computational complexity and remains an open practical challenge. They also note the framework primarily targets settings with repeated independent experiments over time (as in ridesharing), and may require adaptation for other settings, though they show promising single-experiment results when a proxy covariance matrix is available.","The method relies on an estimated spatial covariance matrix $\Sigma$; performance could degrade when $\Sigma$ is poorly estimated (small N, nonstationarity, heteroskedasticity) or misspecified, and the paper’s robustness checks are limited to additive noise proxies rather than realistic covariance estimation errors. The design class is restricted to balanced cluster-randomized Bernoulli assignments with a shared treatment probability (fixed at p=0.5), which may be suboptimal when costs, constraints, unequal cluster sizes, or multiple treatment arms exist. The theoretical development and objective are tailored to neighborhood interference and a specific DR ATE definition comparing all-treated vs all-control; extensions to broader exposure mappings, partial population treatment, or alternative estimands may not carry over directly. Practical deployment may face issues with temporal dependence between “repeated” experiments (e.g., carryover, day-to-day autocorrelation) that violate the assumed independence of repeated triplets.","They propose developing computationally tractable methods that properly account for the omitted second-order interference term $I_2$ in the objective. They also suggest extending/adapting the approach beyond repeated-experiment settings to more general experimental settings, motivated by empirically strong performance in single-experiment scenarios when prior covariance information is available.","Develop self-starting/robust variants that jointly estimate $\Sigma$ and interference structure under nonstationarity and temporal dependence (e.g., spatio-temporal covariance, carryover) and quantify design sensitivity to covariance estimation error. Extend the graph-cut surrogate to multi-arm treatments, constrained randomization (budget/coverage constraints), and unequal treatment probabilities, and study optimality of p\neq0.5 under alternative loss functions. Provide theory and software for alternative estimands/exposure mappings (beyond neighborhood all-treated/all-control indicators), and for settings with partial interference or heterogeneous neighborhood sizes. Add broader real-world validations and benchmarking against modern interference-aware design methods (e.g., integer-programming-based optimal designs) under realistic operational constraints.",2505.20130v3,https://arxiv.org/pdf/2505.20130v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:46:38Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Prediction|Cost reduction,D-optimal|Other,"Variable/General (d categorical design variables with m_j choices; select k_j per mode; examples include time-dependent sensors (space×time), seismic (sources×receivers), X-ray (angles×detectors), flow (x×y×z))",Environmental monitoring|Energy/utilities|Healthcare/medical|Other,Simulation study,TRUE,MATLAB|Other,Not provided,https://arxiv.org/abs/2506.00336|https://doi.org/10.1137/19M1261043|https://doi.org/10.1017/jfm.2023.269|https://doi.org/10.1137/15M1044680|https://doi.org/10.1137/090771806|https://doi.org/10.1002/mrm.29212|https://academic.oup.com/gji/article-pdf/236/3/1309/55270901/ggad492.pdf|https://onlinelibrary.wiley.com/doi/pdf/10.1002/mrm.21391|https://doi.org/10.1093/gji/ggad492,"The paper develops algorithms for Bayesian optimal experimental design (OED) when design variables have a multiway (tensor) structure, so selections must be made jointly across modes (e.g., sources×receivers, angles×detectors) rather than as independent matrix columns. Using the Gaussian linear inverse-problem setting, it targets D-optimality / expected information gain (EIG), cast as maximizing a log-determinant objective based on a weighted forward operator matrix A. The main methodological contribution is a structured generalization of column subset selection (CSSP) via tensor mode-unfoldings and three algorithmic templates—IndSelect, SeqSelect, and IterSelect—plus a Sketch-First randomized variant that reduces cost and can avoid needing the adjoint operator. Performance is demonstrated via numerical experiments on time-dependent inverse problems, seismic tomography, X-ray tomography, and flow reconstruction, showing near-best EIG designs and reported speedups up to ~50× over greedy structured baselines. The work advances Bayesian OED by providing scalable, structure-respecting selection methods with analyzed computational complexity and empirical effectiveness on large-scale inverse problems.","In the linear-Gaussian Bayesian inverse problem, the posterior precision is $\Gamma_{\text{post}}^{-1}=\Gamma_{\text{pr}}^{-1}+F^\top R^{-1}F$, and the EIG/D-optimal objective is $\phi_{\text{EIG}}=\log\det(I+A^\top A)=\log\det(I+AA^\top)$ with $A:=\Gamma_{\text{pr}}^{1/2}F^\top R^{-1/2}=\sigma_R^{-1}\Gamma_{\text{pr}}^{1/2}F^\top$. A (structured) subset is represented by a Kronecker-structured selection operator $S=S_d\otimes\cdots\otimes S_1$ selecting $k_j$ indices per mode, and the subsampled criterion is $\phi_{\text{EIG}}(S)=\log\det\bigl(I+(AS)(AS)^\top\bigr)$. The Sketch-First method forms a row sketch $Y=\Omega A$ with Gaussian $\Omega\in\mathbb{R}^{r\times N}$, $r=K+p$, and performs structured selection on $Y$ as a proxy for $A$.","Across four application testbeds (time-dependent inverse problem, seismic tomography, X-ray tomography, and flow reconstruction), IndSelect/SeqSelect/IterSelect achieve EIG values better than large sets of random structured designs (e.g., 5×10^3 random configurations in seismic and X-ray experiments). In the time-dependent sensor-placement example with $n_s=28$, for $k=5$ the selected design is better than 96.13% of all possible designs, and for $k=22$ better than 99.7% of all designs; IterSelect converged in $n_{\text{iter}}=2$ iterations. In seismic tomography with $k_1=k_2=10$, restricting to ~7% of all source–receiver pairs increased relative reconstruction error from 9.4% (full) to 11.5% while maintaining high EIG. Reported runtimes show Sketch-First reduces time by 1–2 orders of magnitude versus non-sketched variants in several settings, and the paper reports speedups up to ~50× over corresponding greedy structured approaches.","The authors note they were not able to derive structured analogs of the CSSP/EIG guarantee (e.g., Lemma 1) for the structured column selection setting, leaving a theoretical gap. They also state they only used a specific randomization strategy (Sketch-First) though many variants are possible. They further mention interest in extending the approach to nonlinear OED problems and to other OED criteria beyond the D-optimal/EIG criterion studied here.","The experimental evaluations appear to rely on synthetic/benchmark problems and comparisons primarily against greedy structured baselines and random designs; broader benchmarking against other modern Bayesian OED solvers (e.g., continuous relaxations, gradient-based optimization, submodular approximations where applicable) is not emphasized. The core methodology assumes a linear-Gaussian model with effectively diagonal noise covariance (or an interpretation that diagonalizes per-design contributions); performance and robustness under correlated noise, model mismatch, or nonlinear/non-Gaussian posteriors is not established. Practical guidance on hyperparameter choices (e.g., oversampling p, sketch dimension r, stopping tolerance) and sensitivity analyses across these choices is limited. No public implementation is indicated, which may hinder reproducibility and adoption despite algorithmic clarity.",They propose exploring more efficient tensor decomposition computations to potentially derive new structured column selection algorithms. They suggest investigating additional randomization variants beyond the Sketch-First approach. They highlight the need for theoretical analysis establishing guarantees analogous to Lemma 1 for structured selection. They also state extensions to nonlinear OED problems and to other optimality criteria are of interest and under investigation.,"Developing self-tuning/adaptive schemes for choosing sketch size r (or oversampling p) based on target error in EIG or singular-spectrum decay would make the randomized approach more robust. Extending the framework to handle correlated noise (non-diagonal R) and unknown/noise-parameter estimation would broaden applicability in real sensing systems. Adding diagnostics for identifiability and mode-wise interaction (e.g., assessing when independent vs iterative selection is needed) could improve practitioner usability. Providing open-source implementations (e.g., MATLAB/Python) with standardized benchmarks and real-data case studies would strengthen reproducibility and accelerate uptake in the Bayesian inverse problems community.",2506.00336v1,https://arxiv.org/pdf/2506.00336v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:47:34Z
TRUE,Split-plot|Factorial (fractional)|Other,Parameter estimation|Screening|Model discrimination|Prediction|Cost reduction|Other,Not applicable,Variable/General,Theoretical/simulation only|Manufacturing (general)|Food/agriculture|Healthcare/medical|Other,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper develops a general projector–rank degrees-of-freedom (df) partition theorem for complex experimental designs where classical ANOVA df accounting breaks down (e.g., split-plots, blocked/nested layouts, fractional factorials, and missing/unequal replication). It shows that for N observations the total information (N−1 df) can be decomposed exactly across factorial effects (including aliased effect classes in fractions) and randomization strata by projecting each effect’s contrast space onto each stratum and taking the intersection’s dimension (rank/trace identities). This yields integer (not Satterthwaite/Kenward–Roger) numerator/denominator df and produces closed-form df tables for unbalanced split-plot, row–column (Latin/lattice), and crossed–nested mixed designs; classical results (Cochran identity, Yates split-plot df, Box–Hunter resolution) appear as corollaries. The paper introduces diagnostics—df-retention ratio ρ, df deficiency δ, and variance-inflation index α—to quantify information loss due to blocking, fractionation, and imbalance, extending resolution ideas beyond regular balanced 2-level fractions. Simulation and empirical studies show the method controls type-I error in split-plot and nested designs, can recover power (reported up to ~60% vs ~40% in a blocked 2^(5−1) example) without extra runs by choosing generators to maximize ρ, and is much faster than bootstrap-based df approximations in timing benchmarks.","The central identity is the df partition: for strata projectors {P_s} and aliased factorial contrast spaces C_E, \(N-1=\sum_{s\in S}\sum_{E\in \mathcal E}\dim(C_E\cap \operatorname{Im} P_s)\). The df contribution of effect E in stratum s is \(\dim(C_E\cap \operatorname{Im} P_s)=\operatorname{rank}(X_E' P_s X_E)=\operatorname{trace}(P_E P_s)\), where X_E spans C_E and P_E is the OLS projector onto C_E. Information-loss metrics are defined as \(\rho(E)=d_E^{obs}/d_E^{ideal}\) with \(d_E^{obs}=\sum_s \dim(C_E\cap \operatorname{Im}P_s)\), df deficiency \(\delta(E)=d_E^{ideal}-d_E^{obs}\), and a variance-inflation/aliasing index \(\alpha(E)\) based on traces of an alias (dispersion) matrix.","In a split-plot simulation with a=3, b=4, c=4 (N=48) and 20% missing subplots, the exact projector–rank df test kept empirical size near nominal (0.040 balanced; 0.051 with missing), while a naive ANOVA denominator produced severely inflated false positives (0.529 balanced; 0.466 with missing). In a nested Sire→Dam→Animal simulation (72 animals, 20% missing variant), the exact-df test was closer to nominal than naive one-way ANOVA (exact: 2.7% balanced, 7.7% missing; naive: 0.3% balanced, 1.0% missing), illustrating naive procedures can be overly conservative in clustered hierarchies. In a blocked 2^(5−1) design-selection study, a ρ-optimized generator choice reduced alias loss (δ) from 5 df lost to 2 df lost and improved power for main effects from ~0.42 to ~0.61 (exact-df analysis) with narrower mean CI width (1.87 to 1.47). Timing benchmarks in Python report projector–rank df computation under 10 ms for N=10^5, with bootstrap-based df estimation 50–90× slower (e.g., 0.0073 s vs 0.648 s at N=100,000).",None stated.,"The framework’s validity and practical performance rely on correctly specifying randomization strata/projectors and the mixed-model covariance decomposition; in messy real studies, strata may be ambiguous, partially crossed, or misspecified, which can affect df allocation and test validity. The empirical validation is largely simulation-based and focused on canonical designs; broader real-data case studies across domains and comparisons to modern mixed-model software workflows (including parameter estimation uncertainty and small-sample behavior under model misspecification) are limited. Although computation is described as efficient, implementing the full projector/rank machinery for arbitrary user-specified designs (especially with multiple crossed random factors and complex alias structures) may require careful software engineering and diagnostic tooling not provided here.","The paper suggests extending the rank-based df identity beyond Gaussian linear mixed models by replacing stratum projectors with working-weight projectors for non-Gaussian outcomes, enabling analytic df for logistic split-plots and Poisson strip-plots that are currently handled via Monte Carlo methods. It also notes that integrating exact df and the ρ/δ/α diagnostics into common R/Python mixed-model and DOE software would provide practical alternatives to numeric/approximate df methods and improve pipeline automation for large-scale screening.","Develop end-to-end open-source implementations (e.g., R and Python) that accept design specifications (including missingness, blocking, nesting/crossing, and fractional generators), automatically construct projectors and aliased contrast spaces, and output ANOVA/df tables plus ρ/δ/α diagnostics with visualization. Study robustness to violations of core assumptions (autocorrelation, heteroscedasticity, nonrandom missingness, and variance-component misspecification) and provide guidance/sensitivity analyses for practitioners. Extend the approach to multivariate responses and high-dimensional factor screening settings, and benchmark against contemporary methods (e.g., parametric bootstrap, KR, Bayesian mixed models) on standardized real-world datasets.",2506.01619v3,https://arxiv.org/pdf/2506.01619v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:48:18Z
TRUE,Sequential/adaptive|Bayesian design|Other,Screening|Optimization|Cost reduction|Other,Other,Variable/General (A treatments/arms; M metrics; fixed budgets T (exploration) and Tv (validation)),Service industry|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes a fixed-budget, two-phase adaptive experimentation framework for online A/B/N settings with many treatments and multiple metrics under heterogeneous variances. Phase I is an adaptive exploration phase (multi-armed bandit best-arm identification) that aims to select the treatment most likely to pass Phase II validation; Phase II is a conventional validation A/B test (non-Bayesian z-test with known variances, or Bayesian A/B testing with normal priors) run on the recommended treatment versus control to infer effects such as ATE. The authors show that the best treatment for maximizing worst-metric validation success is characterized by a per-metric z-value (signal-to-noise plus a validation-dependent constant), and propose SHRVar, a sequential-halving-style algorithm that samples treatments and control using a novel relative-variance allocation and eliminates arms based on estimated worst-metric z-values. They prove an exponentially decreasing identification error bound under fixed exploration budget with an exponent governed by a new complexity measure H3 that generalizes successive halving (homogeneous variance) and SHVar (heterogeneous variance). Extensive numerical experiments (10^5 repetitions) demonstrate improved best-treatment identification accuracy and higher downstream validation success probability compared with SH, SHVar, uniform sampling, and Neyman allocation, especially as variance heterogeneity increases.","The target treatment is defined by a robust max-min validation objective: $a^*=\arg\max_{a\in\mathcal A\setminus\{0\}}\min_i \mathbb P(E_{v,i})$. The paper shows this is equivalent to maximizing worst-metric z-values: $a^*=\arg\max_{a\ne 0}\min_i z_{a,i}$ where $z_{a,i}=\frac{\mu_{a,i}-\mu_{0,i}}{\sigma_{a,i}^2+\sigma_{0,i}^2}+\xi_{a,i}$ and $\xi_{a,i}$ depends on the validation test (non-Bayesian z-test or Bayesian posterior-threshold test). SHRVar uses relative variances $\rho_{a,i}^2=\frac{\sigma_{a,i}^2}{\sigma_{a,i}^2+\sigma_{0,i}^2}$ and $\lambda_{a,i}^2=\frac{\sigma_{0,i}^2}{\sigma_{a,i}^2+\sigma_{0,i}^2}$ to allocate samples across treatments and control in each halving stage, and eliminates arms by estimated $\min_i \hat z_{s}(a,i)$ (or a confidence-based variant).","The main theoretical result is an exponential fixed-budget error bound for SHRVar: $\mathbb P(\hat a\neq a^*)\le 6M\log_2 A\cdot \exp\{-T/(2H_3\log_2 A)\}$, where $H_3$ generalizes classic successive halving complexity measures to multi-metric, control-vs-treatment, heterogeneous-variance settings. In simulations with $10^5$ repetitions, SHRVar achieves higher exploration accuracy than uniform-sampling SH-z and variance-based SHVar-z, and higher downstream validation success probability than SH and SHVar, with the advantage increasing as variance heterogeneity grows. In a single-metric experiment with $A=27$ treatments, SHRVar maintains validation success rates above ~0.8 even when variance heterogeneity is high, while SH/SHVar success degrades substantially. In larger multi-metric settings (e.g., $A=128$, $M=3$), SHRVar outperforms SH-z, SHVar-z, and a Neyman-allocation baseline on both identification accuracy and probability of passing validation.","The authors note that their main theoretical and algorithmic development assumes known reward variances $\sigma_{a,i}^2$ (though they empirically evaluate a plug-in unknown-variance variant, SHRVar-Ada). They also mention that using $\min_i \hat z_{s}(a,i)$ for elimination can suffer from underestimation bias when sampling deviates from the variance-equalizing allocation, motivating the confidence-based elimination variant (SHRVar-c).","The model assumes metric independence and sub-Gaussian rewards; real online experiments often have correlated metrics, heavy tails, interference, or temporal dependence, which could affect both z-based characterization and allocation optimality. Validation is treated as fixed-budget with equal split control/treatment and known-variance z-tests (or a specific normal-prior Bayesian test); practical A/B testing often uses sequential monitoring, multiple-testing corrections across metrics, CUPED/covariate adjustment, and unknown variances, which may change the optimal exploration target. The work focuses on identifying a single best treatment under a worst-metric criterion; extensions to constraints/guardrails with heterogeneous thresholds, or to selecting multiple candidates for validation, are not developed.",The conclusion states future directions including incorporating unknown variance estimations more fully and extending/generalizing the algorithm and results to linear bandits.,"A natural extension is to handle correlated metrics via multivariate modeling (e.g., estimating joint covariance and using multivariate test statistics or robust multiple-testing control) and to study how this changes the sampling allocation. Another direction is to integrate practical variance reduction and inference techniques (covariate adjustment/CUPED, cluster-robust variance, sequential validation) into the z-value characterization and to provide guarantees under unknown/estimated variances and non-stationarity. It would also be valuable to generalize the framework to selecting a slate of top candidates for validation (top-K) and to incorporate explicit guardrail constraints and multiplicity control across many metrics.",2506.03062v1,https://arxiv.org/pdf/2506.03062v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:49:00Z
TRUE,Factorial (full)|Factorial (fractional)|Optimal design|Sequential/adaptive|Other,Parameter estimation|Screening|Prediction|Cost reduction|Other,A-optimal|Other,"Variable/General (p treatments; theory for general p, bounded interaction order k=o(p))",Healthcare/medical|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Supplementary material (Journal/Publisher),NA,"The paper introduces a probabilistic factorial experimental design for combinatorial interventions with p binary (on/off) treatments, where each unit’s treatment vector is sampled independently from a product Bernoulli distribution parameterized by a dosage vector d\in[0,1]^p. Under a bounded-order (k-way) interaction model expressed via a Fourier/Boolean-function basis, the authors study optimal choice of d for estimating the interaction coefficients from noisy outcomes. In the passive (one-shot) setting, they provide a closed-form near-optimal design: d_i=1/2 for all treatments, which corresponds to inducing an (approximately) uniform distribution over combinations and is optimal up to a 1+O(ln(n)/n) factor for their MSE objective. In the multi-round active setting, they propose a near-optimal acquisition rule that selects d by minimizing a trace-of-inverse-eigenvalues objective involving the expected information matrix \Sigma(d) plus accumulated past information, and solve it numerically. Theoretical guarantees are validated via simulations and extensions are discussed for supply constraints, heteroskedastic rounds, sparsity/cardinality constraints, and matching a target combinatorial distribution.","Treatment assignment is randomized via a product Bernoulli design: x_{m,i}=1 with prob d_i and -1 with prob 1-d_i (Eq. (1)). Outcomes follow y_m=f(x_m)+\epsilon_m with f expanded in the Fourier/Boolean basis f(x)=\sum_{S\subseteq[p]}\beta_S\phi_S(x), \phi_S(x)=\prod_{i\in S}x_i, and bounded interactions \beta_S=0 for |S|>k. The expected (per-sample) information matrix under dosage d is \Sigma(d) with entries \Sigma(d)_{S,S'}=\prod_{i\in S\triangle S'}(2d_i-1) (Eq. (5)); passive optimality yields d_i=1/2. Active design chooses d_T=\arg\min_{d\in[0,1]^p}\sum_{i=1}^K 1/\lambda_i\big(\Sigma(d)+\tfrac1n\sum_{t<T}X_t^\top X_t\big) (Eq. (7)).","They prove that in the passive setting the uniform half-dosage design d=(1/2,\ldots,1/2) is near-optimal for minimizing expected squared error of the (truncated) OLS estimator, within a multiplicative factor 1+O(\ln(n)/n), regardless of interaction order k. Under half-dosage, they bound estimation error as E\|\hat\beta-\beta\|_2^2 \le (2K\sigma^2+1)/n for n larger than n_0=O(K^3\ln K), implying sample complexity O(K^3\ln K)=O(k p^{3k}\ln p) to estimate a k-way interaction model (with K=\sum_{i=0}^k \binom{p}{i}). For constrained supply \sum_i d_i\le L and additive k=1, they show uniform dosage d_i=L/p is near-optimal (1+O(\ln(n)/n)). Simulations show half-dosage performs close to or slightly worse than a carefully constructed fractional factorial design in a single round, while the proposed active acquisition can outperform fixed half-dosage when per-round n is small.","The authors note the probabilistic design assumes a product infection/assignment mechanism and no interference, which may fail under interference, censoring, spatial interactions, or when some combinations cause unit death and thus become unobservable. They also state the outcome model omits unit-specific covariates, limiting personalized or covariate-adjusted effect prediction. They further mention that additional constraints (e.g., sparse interventions) and alternative objectives (e.g., targeting specific outcomes) are not fully explored.","The near-optimality results are tied to a specific squared-error objective for estimating Fourier coefficients using (truncated) OLS/related estimators; practical DOE goals like power for specific contrasts, effect sparsity, or model selection are not directly optimized. The design assumes binary treatment presence/absence and independent assignment across treatments and units; correlated treatment delivery mechanisms or hard constraints on co-occurrence are not modeled except via limited extensions. Empirical validation is simulation-only; no real experimental dataset demonstrates robustness to violations such as non-Gaussian noise, unknown \sigma^2, or misspecified k in a way that affects downstream scientific decisions. Computational cost of the active criterion scales poorly with p and k (eigenvalue computations over K\approx O(p^k)), which may limit usability for moderate k without further approximations.","They suggest relaxing the product-mechanism assumption to handle interference/censoring (e.g., spatial interactions in tissues or missing combinations due to cell death). They propose extending the combinatorial intervention model to incorporate unit-specific covariates for more personalized predictions. They also mention exploring additional constraints (e.g., sparse interventions) and alternative objectives (e.g., optimizing specific outcome variables).","Develop self-starting/robust variants that handle unknown noise variance, heavy-tailed errors, and dependence across units (batch effects beyond simple heteroskedastic scaling, autocorrelation, or interference). Provide design-and-analysis workflows for model selection (estimating k and selecting relevant interactions) jointly with adaptive dosage selection, including multiple-testing/FDR control for discovered interactions. Extend the framework to multi-level dosages (continuous treatment intensities per unit) and to correlated/structured treatment assignment (e.g., pathway-structured co-perturbations) while retaining tractable optimality criteria. Release a reusable software package implementing the active acquisition (with scalable approximations such as stochastic trace estimation, low-rank updates, or convex relaxations) and benchmark on real combinatorial biology/engineering datasets.",2506.03363v1,https://arxiv.org/pdf/2506.03363v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:49:53Z
TRUE,Optimal design,Parameter estimation|Screening|Cost reduction|Other,D-optimal,Variable/General (feature dimension $d$ for linear reward / preference model),Other|Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/andrschl/isaac_rlhf,"The paper studies reinforcement learning from human feedback (RLHF) where the learner receives binary trajectory-level preference comparisons under a BradleyTerry (logistic) preference model with a linear reward parameterization. It proposes randomized preference optimization (RPO), a meta-algorithmic framework that uses maximum-likelihood logistic regression to estimate reward parameters, samples reward parameters from Gaussian posteriors (Thompson-style randomized exploration), and calls an RL oracle to compute approximately optimal policies. To reduce preference query complexity, it introduces a practical lazy-update variant (Algorithm 3) that collects batches of trajectory pairs and applies greedy approximate D-optimal experimental design to select the most informative comparisons, enabling parallel preference labeling. The paper provides high-probability regret bounds for online variants and a PAC-style last-iterate guarantee for a preference-free exploration variant that defers querying to a final batch. Experiments on gridworld and Isaac Lab cartpole with synthetic preferences show competitive performance with reward-based RL while using substantially fewer preference queries, especially with the D-optimal selection routine.","Preferences follow a BradleyTerry model: $\mathbb{P}(\tau\succ\tau')=\sigma(\langle\theta_*,\phi(\tau)-\phi(\tau')\rangle)$ with sigmoid $\sigma(x)=1/(1+e^{-x})$. Reward estimation uses the logistic negative log-likelihood $L_{\mathcal D_t}(\theta)= -\sum_{(x,y)\in\mathcal D_t}\big[y\log\sigma(\langle\theta,x\rangle)+(1-y)\log\sigma(-\langle\theta,x\rangle)\big]$, where design points are $x=\phi(\tau)-\phi(\tau')$. The optimal design subproblem maximizes information gain via (approximate) D-optimality: maximize $\log\det\big(V_{t_{\text{stop}}}+\sum_{x\in\mathcal D} n_x x x^\top\big)$ subject to $\sum n_x=|\mathcal D|$, implemented with a greedy log-det update.","Algorithm 1 (RPO-Regret) achieves a high-probability regret bound $R(T)=\tilde O\big(\sqrt{\kappa d^3 T}\,\big)$ (with polylog factors as $\log^3(dT/\delta)$ in the statement). Algorithm 2 (RPO-Explore) outputs a final policy with suboptimality $\mathrm{SubOpt}(\hat\pi)=\tilde O\big(\sqrt{\kappa d^3/T}\,\big)$, while collecting all preferences in a single batch at the end. Algorithm 3 (LRPO-OD-Regret) preserves regret up to constants, $R(T)\le \tilde O\big(\sqrt{(1+C)\kappa d^3 T}\,\big)$, and limits the number of batch-update events to a logarithmic quantity in $T$ (explicit bound provided in Theorem 4.1). Empirically on Isaac-Cartpole-v0, the D-optimal lazy variant reduces the number of preference queries substantially while quickly matching the regret/performance of the per-round querying method.","The authors note that the BradleyTerry preference model may not capture the complexity of real human feedback and suggest extending to richer preference models. They also acknowledge that requiring an RL oracle call at every RLHF step can be computationally demanding, especially with practical policy-optimization oracles that are not reward-free and can have high sample complexity. Finally, they state that experiments are limited to tabular and simple continuous-control tasks with synthetic feedback, and that evaluation on more complex tasks and with real human or LLM-generated feedback is needed.","The optimal design routine operates on a batch of collected trajectory-pair features and uses a greedy approximation; its computational cost and scalability with large batch sizes or high-dimensional features $d$ could be significant, especially if many candidate pairs are stored. The theoretical guarantees rely on linear reward features and a correctly specified logistic preference model with relatively strong independence/stationarity assumptions; robustness to misspecification, nonstationary preferences, annotator inconsistency, or correlated trajectories is not developed. The experiments use synthetic preferences derived from known reward terms; real preference noise, ambiguity, and labeling delays/costs might change query-efficiency conclusions. Practical implementation details (e.g., how to choose $\lambda, C, \beta_t$, batch sizes, and trajectory truncation horizon) may strongly affect performance but are not fully standardized for deployment.","They propose extending beyond BradleyTerry to richer preference models, reducing the need for RL-oracle calls (or making reward-free oracles practical), and evaluating on more complex tasks with real human feedback or LLM-generated feedback rather than synthetic labels.","Developing robust/self-calibrating design criteria for preference data (e.g., Bayesian D-optimality or objectives tailored to policy improvement rather than parameter estimation) could better match RLHF goals. Extending the approach to handle unknown/learned features, nonlinear reward models, or deep representations would broaden applicability. Incorporating explicit models of annotator noise/cost and adaptive stopping rules could yield principled query budgets and better real-world efficiency. Providing and benchmarking standardized software implementations across common RLHF environments would improve reproducibility and facilitate practitioner adoption.",2506.09508v2,https://arxiv.org/pdf/2506.09508v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:50:46Z
TRUE,Sequential/adaptive|Computer experiment|Bayesian design|Optimal design|Other,Prediction|Cost reduction|Other,I-optimal (IV-optimal)|Other,"Variable/General (d-dimensional input; examples with d=1,2; application with d=2)",Transportation/logistics|Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Other,Not provided,https://www.earthquakecanada.ca|https://robertmarks.org/Classes/ENGR5358/Papers/functions.pdf,"The paper proposes a fully Bayesian sequential design strategy (LA-SIS) for expensive heteroscedastic stochastic computer simulations, aimed at accurate prediction of the mean response surface under limited evaluation budgets. A dual–Gaussian process surrogate models both the mean function and the log noise variance, and the next design point is chosen by minimizing a lookahead criterion based on an empirical expected integrated mean-square prediction error (EEIMSPE), i.e., a grid-summed posterior predictive variance (a Bayesian ALC/IMSPE-style criterion). Posterior uncertainty in GP hyperparameters and latent noise is propagated into the acquisition function using Monte Carlo, and sequential importance sampling (with ESS-triggered NUTS re-sampling) is used to update posterior samples efficiently between iterations. The approach is demonstrated via synthetic 1D and 2D heteroscedastic test problems and a real motivating application in seismic design of wood-frame podium buildings (ABAQUS simulations), showing improved or more robust RMSE performance compared with empirical Bayes lookahead variants, a homoscedastic Bayesian variant, and the frequentist hetGP sequential design baseline.","Observation model: $Y(x)=f(x)+\epsilon(x)$ with heteroscedastic variance $\mathrm{Var}[\epsilon(x)]=g(x)$ and $v(x)=\log g(x)$. Surrogate: independent GPs $f\sim\mathrm{GP}(0,k_f)$ and $v\sim\mathrm{GP}(\mu_0,k_v)$; conditional GP prediction uses $\mathbb{E}[Y(x_*)|\cdot]=\gamma^\top (K_f+K_\epsilon)^{-1}y$ and $\mathrm{Var}[Y(x_*)|\cdot]=\sigma_f^2+\exp(v_*)-\gamma^\top (K_f+K_\epsilon)^{-1}\gamma$. Sequential design criterion: $\mathrm{EEIMSPE}(D_n)=\sum_{x_*\in\mathcal{D}} \mathrm{Var}[Y(x_*)|D_n]$, and the next point minimizes a lookahead version (with optional weights) using a kriging-believer plug-in $\hat y_{n+1}=\mathbb{E}[Y(\tilde x_{n+1})|D_n]$; posterior expectations are approximated via (sequential) importance sampling and occasional NUTS refresh when ESS drops below $\tau$.","Across 100 macro-replications in 1D and 2D synthetic heteroscedastic examples, LA-SIS yields consistently lower RMSE than the empirical-Bayes plug-in lookahead (LA-EB), and it improves over or matches a homoscedastic Bayesian lookahead (LA-Homo) as iterations accrue; compared to hetGP, Bayesian methods are more stable early, and in the 2D example hetGP required roughly ~50 additional iterations (about 200 vs 150) to match LA-SIS performance. In the seismic podium-building case study, the method fits a mean drift surface over a 2D grid (mass and stiffness ratios) and reveals clear heteroscedasticity; most design sites are evaluated once, reflecting efficiency under an expensive simulator. The inferred safe/meeting-performance region from the stochastic, heteroscedastic mean-surface modeling is larger than (and contains) a previously reported deterministic-kriging “reliable region,” while being more conservative than a simple linear code criterion, due to explicit accounting for stochastic noise and broader exploration.","The authors note the strategy is designed for heteroscedastic noise and is not tailored for truly homoscedastic simulations; while a large lengthscale in the variance GP can approximate homoscedasticity, it is not straightforward or optimal. They also state they do not address model selection between homoscedastic vs heteroscedastic surrogate forms, nor do they evaluate performance on homogeneous-noise simulation functions. They further mention batch sequential design (selecting multiple points per iteration) is not developed here.","The design space is discretized to a finite candidate grid for EEIMSPE evaluation/minimization; performance and scalability may degrade or depend on grid resolution in higher dimensions, and continuous-domain optimization of the acquisition is not addressed. The method assumes independent observation noise across runs and relies on a GP noise model for $\log g(x)$; autocorrelation across simulation replications or non-Gaussian noise/heavy tails could reduce validity. Computational overhead from repeated EEIMSPE evaluations, Monte Carlo nesting (N1×N2), and occasional NUTS may be substantial for larger $n$ or larger candidate sets, and the paper does not provide open-source implementation to assess practical runtime/engineering tradeoffs broadly.","They propose future work on deciding whether variance should be modeled as homogeneous or heteroscedastic (e.g., fitting both and selecting via log-likelihood as suggested in prior work) and evaluating performance on homogeneous simulation functions. They also suggest extending to batch sequential design to add multiple evaluations per iteration, potentially leveraging distributed/parallel computing to reduce wall-clock runtime.","Extending EEIMSPE optimization beyond a fixed candidate grid (e.g., continuous optimization with gradients/approximations) and studying scalability with higher-dimensional inputs would broaden applicability. Developing robust/self-starting variants that handle unknown simulator stability issues, non-Gaussian noise, or correlated outputs (e.g., common random numbers) could improve real-world reliability. Providing a reproducible software implementation (e.g., an R/Python package interoperable with hetGP/GP libraries) and benchmarking against additional modern Bayesian acquisition functions for stochastic simulators (e.g., fully Bayesian IMSPE/ALC with replication-aware policies) would strengthen adoption and comparative understanding.",2506.09722v1,https://arxiv.org/pdf/2506.09722v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:51:31Z
TRUE,Optimal design|Sequential/adaptive|Computer experiment,Parameter estimation|Prediction|Cost reduction|Other,Not applicable|Other,Variable/General (examples include 2 parameters for 1D rod; 9 parameters for 2D plate; measurement dimension m set by number of sensors/thermometers),Other|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/sandialabs/MrHyDE/tree/main/scripts/DCI/OED-SVD,"The paper develops two new optimal experimental design (OED) criteria tailored to the data-consistent inversion (DCI) framework that avoid repeatedly solving inverse problems. The criteria—expected scaling effect (ESE) and expected skewness effect (ESK)—use geometric properties of the quantity-of-interest (QoI) map and are computed from singular values of sampled Jacobians, enabling efficient design scoring. The authors formulate both simultaneous (choose all measurements at once from a discrete candidate set) and sequential/greedy designs that add measurements providing complementary information. Numerical studies for heat diffusion with uncertain conductivities show the criteria select intuitive sensor placements and that designs with higher utility produce notably different (often tighter/less redundant) updated parameter measures under DCI compared with suboptimal designs. The work advances OED for measure-theoretic inversion by replacing KL/EIG-based utilities with Jacobian/SVD-based geometric utilities that are computationally cheaper and aligned with DCI solution properties.","Local scaling effect is defined from Jacobian singular values as $\mathrm{SE}_Q(\lambda)=(\prod_{k=1}^m \sigma_k)^{-1}$ (or $+\infty$ if rank-deficient). The expected scaling effect is the harmonic mean over $\Lambda$: $\mathrm{ESE}(Q)=\Big(\frac{1}{\mu(\Lambda)}\int_\Lambda \frac{1}{\mathrm{SE}_Q(\lambda)}\,d\mu\Big)^{-1}$. Skewness is defined via row-wise redundancy and computed using SVDs: $\mathrm{SK}_Q(\lambda)=\max_k \frac{\|j_k\|\,\mu_{m-1}(\mathrm{Pa}(J_{Q,k}))}{\mu_m(\mathrm{Pa}(J_Q))}$, and expected skewness $\mathrm{ESK}(Q)$ is again a harmonic-mean integral; utility is taken as $U(Q)=\mathrm{ESE}^{-1}(Q)$ or $\mathrm{ESK}^{-1}(Q)$ with $Q_{\mathrm{opt}}=\arg\max_{Q\in\mathcal Q} U(Q)$.","In the 1D rod example with two thermometers (design indexed over $\Omega\times\Omega$), the ESE-based local maxima reported include design points (1.0, 0.45) with $\mathrm{ESE}^{-1}=906.9$, (0.55, 0.0) with 900.9, (1.0, 0.0) with 890.5, and (0.55, 0.45) with 801.5; the ESK-based prominent maxima include (1.0, 0.0) with $\mathrm{ESK}^{-1}=0.995$, and (0.575, 0.25) and (0.75, 0.425) with 0.950. Monte Carlo estimation uses 10,000 parameter samples for the 2-parameter rod case and 1,000 samples for the 9-parameter plate case, with Jacobians approximated by forward finite differences using perturbation $10^{-5}$. The greedy algorithm for the 2D plate with nine thermometers selects a center location first and then places additional sensors in a spatially dispersed, symmetric pattern, yielding an interpretable one-sensor-per-plate style configuration.","The authors restrict attention to discrete candidate design sets and state they ""do not explore different approaches for solving the optimization problem"" beyond searching over discretized designs, leaving continuous optimization over the design space to future work. They also note that, because the number of outputs can greatly exceed inputs in their numerical setups, adjoint-based derivative computation is not practical for their Jacobian evaluations (they use finite differences instead).","The proposed criteria rely on locally differentiable QoI maps and Jacobian/SVD calculations; for expensive simulators, finite-difference Jacobians (especially for high-dimensional parameters) can still be costly and sensitive to step-size/solver noise, and the paper does not deeply study numerical stability/variance of the Monte Carlo Jacobian-SVD estimates. The utility is based on geometric properties of the deterministic QoI map; handling observation noise models, model discrepancy, or non-smooth/chaotic outputs may require modifications, but robustness to these issues is not systematically evaluated. Comparisons are mainly against intuition/qualitative DCI outcomes rather than against a broad suite of established OED utilities (e.g., EIG/KL) under matched computational budgets, so relative efficiency/accuracy tradeoffs are not fully benchmarked.","The authors indicate that continuous optimization over the design space $\mathcal Q$ (rather than searching over a discretized finite candidate set) is left to a future study. They also mention a future study will consider using the OED approaches to optimize data acquisition when quantities of interest are learned/derived from high-dimensional noisy data (e.g., via kernel PCA) rather than prespecified QoI maps.","Developing adjoint/automatic-differentiation-enabled implementations for Jacobian/SVD-based utilities could scale the approach to larger parameter dimensions and more complex PDE models while reducing finite-difference error. Extending the criteria to explicitly incorporate measurement noise, model discrepancy, and correlated observational error (e.g., through generalized Jacobians or Fisher-like preconditioning) would improve realism for physical experimentation. Broader empirical benchmarking versus EIG/KL and other modern OED criteria under equal compute budgets, along with sensitivity analyses for sampling size and finite-difference step choice, would clarify when the geometric utilities are preferable in practice.",2506.12157v1,https://arxiv.org/pdf/2506.12157v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:52:19Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,Not applicable,"Variable/General (n units with unit-level assignment probabilities; optional covariates Vi, synthetic examples use 3 covariates)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Supplementary material (Journal/Publisher),https://arxiv.org/abs/2506.12677,"The paper studies budget-constrained experimental design where each unit i has a target treatment probability pi with \(\sum_i p_i=B\) (a fixed treatment budget), and the goal is to realize a binary assignment with exactly B treated units while preserving these marginals. It proposes using dependent randomized rounding via the swap rounding algorithm to convert fractional assignment probabilities into an integral treatment vector that exactly satisfies the budget and preserves \(\Pr(A_i=1)=p_i\). The induced negative dependence among treatment indicators yields variance reduction for inverse propensity weighted (Horvitz–Thompson) estimators and, more generally, any estimator linear in assignments with nonnegative coefficients, while retaining unbiasedness. The paper provides martingale-based proofs of unbiasedness, derives a variance decomposition showing covariance terms are nonpositive under swap rounding, proposes a covariate-ordered variant that preferentially swaps similar units, and demonstrates performance gains through synthetic and semi-synthetic experiments in infant health and public housing settings under fixed budgets.","The design problem is to map fractional probabilities \(p_i\in[0,1]\) with \(\sum_{i=1}^n p_i=B\) into binary assignments \(A\in\{0,1\}^n\) such that \(\sum_i A_i=B\) and \(\Pr(A_i=1)=p_i\). The main estimators compared are the standard IPW/Horvitz–Thompson ATE estimator under independent assignment, \(\tau_{\text{IPW}}=\frac1n\sum_i \left(\frac{A_iY_i(1)}{p_i}-\frac{(1-A_i)Y_i(0)}{1-p_i}\right)\), and the swap-rounding version \(\tau_{\text{swap}}\) with \(A_i'\) generated by swap rounding. A key variance-reduction mechanism is negative covariance \(\rho_{ij}=\mathrm{Cov}(A_i',A_j')\), with \(\rho_{ij}=-p_ip_j\) if \(p_i+p_j\le 1\) and \(\rho_{ij}=-(1-p_i)(1-p_j)\) if \(p_i+p_j>1\), which makes cross-terms in \(\mathrm{Var}(\tau_{\text{swap}})\) nonpositive.","The paper proves (Proposition 1) that the swap-rounding IPW estimator is unbiased for the sample ATE (and for the population ATE under an i.i.d. superpopulation model). It proves (Proposition 2) variance reduction: conditional on potential outcomes, \(\mathrm{Var}(\tau_{\text{swap}})\le \mathrm{Var}(\tau_{\text{IPW}})\), with strict inequality when at least one swapped pair has nonzero outcomes. It provides an unbiased/consistent variance estimator for \(\tau_{\text{swap}}\) under the superpopulation assumption (Proposition 3) and asymptotic normality with martingale CLT leading to Wald-style confidence intervals (Theorem 1). Empirically (synthetic and semi-synthetic studies), swap rounding and especially the covariate-ordered variant typically achieve lower empirical variance than standard IPW and several baselines under fixed treatment budgets, sometimes rivaling or outperforming biased low-variance methods like self-normalized IPW.","The authors note that the assumptions used to justify additional variance reduction from covariate-ordered pairing are restrictive and should be viewed as a stylized model for when the technique is beneficial, not something expected to always hold. They also state that in semi-synthetic evaluations, having true propensity scores and both potential outcomes is not possible in real data, so they “sacrifice data quality” and rely on simulation (simulated counterfactuals / simulated treatment effects) to evaluate methods.","The work assumes known/target assignment probabilities \(p_i\) (or \(p(V_i)\)) and focuses on rounding them; in practice, \(p_i\) may be estimated and uncertainty/feedback from estimating \(p_i\) is not fully integrated into design and inference. The strongest variance and CLT results rely on i.i.d. sampling and bounded propensity away from 0/1; robustness to interference, clustering, or time-series dependence is not developed, despite many resource-constrained policy settings having such structure. The paper does not clearly specify implementation details and computational complexity tradeoffs for the covariate-ordering step (a TSP-like problem) in large n deployments beyond heuristic references.",None stated.,"Develop self-starting or fully design-based inference procedures (and variance estimators) that do not require a superpopulation i.i.d. assumption, and quantify sensitivity to propensity misspecification when \(p_i\) are estimated. Extend swap-rounding-based designs to clustered/interference settings (e.g., partial interference, network experiments) and to multi-arm or continuous treatments under multiple simultaneous resource constraints. Provide a scalable, reproducible software implementation (with complexity benchmarks) for covariate-ordered swap rounding and study practical diagnostics (balance, effective sample size, weight extremity) for practitioners.",2506.12677v1,https://arxiv.org/pdf/2506.12677v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:53:03Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Screening|Optimization|Prediction|Cost reduction|Other,G-optimal|Other,Variable/General (feature dimension d; finite arms K),Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies finite-armed semiparametric bandits where rewards follow $r_t=x_{a_t}^\top\theta^*+\nu_t+\eta_t$ with an unknown (potentially adversarial) shift $\nu_t$, and introduces an experimental-design (DOE) approach for this setting. It formulates a nonconvex G-optimal design objective tailored to orthogonalized regression (centered features) and proposes a simple, efficient design algorithm (DEO) that reduces to computing a standard linear-bandit G-optimal design on shifted features and then mixing it with a fixed mass on a reference arm. The resulting design achieves $O(\sqrt d)$ control of the worst-case prediction variance proxy $\max_i\|x_i-\mu\|_{\Sigma_p^{-1}}$ and yields a dimension-optimal nonasymptotic estimation error analysis for orthogonalized regression (removing an extra $\sqrt d$ loss from prior Cauchy–Schwarz analyses). Building on this design, the paper proposes a phase-elimination algorithm (SBE) that attains minimax-optimal regret $\tilde O(\sqrt{dT}\,\log K)$ and also provides the first PAC and best-arm identification (BAI) guarantees for semiparametric bandits with sample complexities matching linear-bandit rates up to logs. Experiments (simulations with synthetic shifts) compare cumulative regret against prior semiparametric bandit algorithms and empirically illustrate the predicted $\sqrt t$-rate behavior under the proposed design.","Semiparametric reward model: $r_t=x_{a_t}^\top\theta^*+\nu_t+\eta_t$, where $\nu_t$ is bounded and chosen before action selection. Orthogonalized (centered) ridge regression estimator: $\hat\theta_t=(\hat V_t+\beta_t I)^{-1}\sum_{s=1}^t \tilde x_{a_s} r_s$, with $\tilde x_{a_s}=x_{a_s}-\mathbb E[x_{a_s}\mid \mathcal H_{s-1}]$ and $\beta_t=\log(t/\delta)$. Design objective (orthogonalized G-optimal design): $\min_{\mu\in\mathbb R^d,\,p\in\Delta(K)}\max_{i\in[K]}\|x_i-\mu\|_{\Sigma_p^{-1}}$ where $\Sigma_p=\sum_i p_i(x_i-\bar x_p)(x_i-\bar x_p)^\top$ and $\bar x_p=\sum_i p_i x_i$.","DEO design guarantees (Theorem 3): for the returned policy $p_{\mathrm{deo}}$, $\|x_i-x_1\|_{\Sigma_{\mathrm{deo}}^{-1}}\le 2\sqrt d$ for all arms and $\|x_i-\bar x_{\mathrm{deo}}\|_{\Sigma_{\mathrm{deo}}^{-1}}\le 4\sqrt d$, with support size at most $d(d+1)/2$. A new nonasymptotic bound for orthogonalized regression (Theorem 4) yields leading error term of order $\sqrt{L\log(t/\delta)/t}$ (where $L=\|z\|^2_{\Sigma_p^{-1}}$), achieving the optimal $\sqrt d$ scaling under DEO (removing an extra $\sqrt d$ factor from prior analyses). The main adaptive algorithm SBE achieves high-probability regret $\tilde O(\sqrt{dT}\,\log K)$ (Theorem 6) and a gap-dependent logarithmic regret $\tilde O((d/\Delta_*)\log K)$ (Theorem 7). It also provides BAI sample complexity $\tilde O(d\log K/\Delta_*^2)$ and PAC sample complexity $\tilde O(d\log K/\varepsilon^2)$ (Theorem 8 / Corollary 5).","The authors note that while SBE achieves BAI with sample complexity $\tilde O(d\log K/\Delta_*^2)$, this is not instance-optimal for fixed instances: in linear bandits the instance-dependent minimax sample complexity can be smaller than $\Omega(d/\Delta_*^2)$, so their BAI guarantee can be practically and theoretically suboptimal. They suggest improving instance-dependent pure-exploration performance as a future direction. They also point out the fixed-budget setting is not addressed and is a natural counterpart to their fixed-confidence results.","The proposed DEO uses a distinguished reference arm (smallest index in the active set) and assigns it fixed probability mass (1/2), which may be suboptimal in practice and could be sensitive to feature scaling/geometry, especially when some arms are clearly dominated. Theoretical guarantees rely on bounded features/parameters and sub-Gaussian noise, and the shift $\nu_t$ must be chosen before action selection; settings with heavy tails, unbounded shifts, or shifts reacting to the chosen arm are not covered. Empirical evaluation appears limited to synthetic scenarios and a small set of baselines/hyperparameter tuning choices; broader benchmarks (e.g., contextual datasets, delayed feedback, nonstationarity beyond additive shifts) and ablations (impact of the 1/2 mixing, choice of reference) would strengthen practical conclusions. No implementation details or open-source code are provided, which limits reproducibility and adoption.","The conclusion highlights two directions: (1) refining pure-exploration algorithms to achieve optimal (instance-dependent) sample complexities, potentially without simultaneously optimizing regret, and (2) developing methods for the fixed-budget setting as a counterpart to fixed-confidence pure exploration in linear bandits. Earlier, the paper also emphasizes improving the instance-dependent minimax BAI sample complexity beyond the presented $\tilde O(d\log K/\Delta_*^2)$ bound as a promising direction.","Developing a fully self-starting or parameter-free version of the design/algorithm (removing reliance on fixed constants, explicit phase schedules, or the hard-coded 1/2 mixture) could improve robustness and usability. Extending the design framework to settings with dependent data (contextual/adaptive feature generation, autocorrelated noise) or to shifts that can depend on the chosen arm (action-dependent confounding) would broaden applicability. Providing software and practical guidance (numerical solvers for the linear G-optimal subproblem, scalable implementations for large K, diagnostics for design adequacy) would help adoption. It would also be valuable to explore alternative optimality criteria (e.g., I-/V-optimal for prediction, or instance-dependent designs) and to characterize when the nonconvex orthogonalized design admits better-than-$O(\sqrt d)$ constants or reduced exploration mass on the reference arm.",2506.13390v2,https://arxiv.org/pdf/2506.13390v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:53:54Z
TRUE,Response surface|Screening|Supersaturated|Optimal design|Bayesian design|Computer experiment|Other,Prediction|Screening|Parameter estimation|Other,I-optimal (IV-optimal)|Bayesian D-optimal|E-optimal|Other,Variable/General (examples studied: RSM m=3 and m=6; screening m=7; supersaturated m=22–26 with n<m),Theoretical/simulation only,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/weeseml/CVandDOE|https://github.com/xliusufe/RidgeVar,"The paper empirically studies whether cross-validation (CV)—especially leave-one-out CV (LOOCV)—is effective for model selection and tuning when analyzing small, structured designed experiments, contrasting it with cautions in the DOE literature and with Breiman’s “little bootstrap” (LB) alternative for X-fixed prediction error. Using simulation, it evaluates prediction performance in response surface methodology (RSM) settings across several design classes (CCD variants, Box–Behnken, I-optimal, and MaxPro space-filling) and compares full second-order regression, best-subsets regression selected by CV/LOOCV/LB, and random forests tuned by CV/LOOCV. It also evaluates screening performance (power and type I error) for nonregular screening designs and for supersaturated designs under multiple optimality criteria, comparing lasso variants, Gauss-lasso, and best-subsets regression selected by CV/LOOCV/LB. Overall, LOOCV-based regression is frequently competitive or best across both prediction and screening tasks, while k-fold CV can be uneven and random forests tuned with 5-fold CV can overfit in simpler true-model scenarios. LB performs adequately for RSM prediction but is generally outperformed in screening scenarios and can show higher variability, motivating further tuning/estimation work (e.g., variance estimation and LB tuning parameters).","Key formulas include the k-fold CV error estimate $\mathrm{RMSPE}(k)=\frac{1}{k}\sum_{i=1}^k \mathrm{RMSPE}_i$ with per-fold $\mathrm{RMSPE}_i=\sqrt{\frac{1}{n^*}\sum_{f\in F_i}(y_f-\hat y_f)^2}$; LOOCV is the special case $k=n$. The full second-order RSM model used as a baseline is $E(Y_h)=\beta_0+\sum_{i=1}^m\beta_i X_{ih}+\sum_{i=1}^m\beta_{ii}X_{ih}^2+\sum_{i<j}\beta_{ij}X_{ih}X_{jh}$. The little bootstrap procedure evaluates model size via $\mathrm{RSS}_s=y'(I-H_s)y$, a perturbation $\tilde y=y+\tilde\epsilon$ with $\tilde\epsilon_i\sim N(0,t\hat\sigma^2)$, a bias term $B=\frac{1}{t^2}\tilde\epsilon'\hat{\tilde y}_s$, and selects size $s$ minimizing $\mathrm{PE}_s=\mathrm{RSS}_s+2\bar B_s$.","Across RSM simulations (m=6,n=52 and m=3,n=16), regression methods using 5-fold CV, LOOCV, or LB often produced similar RMSPE, with LOOCV frequently showing less variability; the full second-order regression model was robust when prediction was the goal. Random forests tuned with 5-fold CV tended to overfit when the true surface was second-order, whereas random forests tuned with LOOCV were competitive. In screening with n=20,m=7 (main effects plus some 2FIs), regression with LOOCV provided the best balance of high power and controlled type I error among the compared methods, while lasso LOOCV and Gauss-lasso had higher type I error. In supersaturated screening (e.g., (12,26), (14,24), (18,22)), lasso CV showed the worst power; regression LOOCV and Gauss-lasso were highlighted as strong overall choices, with LOOCV generally improving results relative to 5-fold CV and LB showing similar or worse performance than LOOCV.","The authors note that some screening results are missing because regression with LOOCV and LB failed for the MEPI design, possibly due to implementation issues in the best-subsets algorithm (leaps/regsubsets) and/or complex aliasing among two-factor interaction columns induced by that design. They also state that LB’s higher variability in prediction may be due to using too few bootstrap iterations (they used $n_{bootstrap}=25$), suggesting that further investigation is needed. They additionally acknowledge that the lasso CV/LOOCV procedures might improve with added thresholding or with subjective visual evaluation of coefficient/profile plots.","The study is entirely simulation-based; conclusions may depend on the chosen data-generating mechanisms (e.g., McDaniel–Ankenman testbed surfaces, specific sparsity/magnitude settings, and normal i.i.d. errors), so robustness to nonnormality, heteroskedasticity, and correlated errors typical in some experiments is not established. Comparisons focus on specific implementations (best subsets via leaps, random forests via ranger, lasso via glmnet) and specific tuning schemes (5-fold vs LOOCV), so results may change with alternative resampling schemes (e.g., blocked CV, design-respecting folds), alternative model selection methods (e.g., hierarchical/heredity-constrained selection), or different hyperparameter grids for RF. The screening evaluation uses power/type I error on effects declared active, but does not deeply address post-selection inference or uncertainty quantification, which is important for practitioner decision-making. Some design classes are treated as given (from prior literature/supplements) rather than optimized under explicit cost/constraint structures, limiting guidance for real experimental planning under practical constraints.","The authors suggest investigating whether the little bootstrap can be tuned to improve performance, including exploring alternatives for estimating $\sigma^2$ (they used a ridge-based estimator in screening) and examining the impact of LB tuning parameters such as the variance scaling factor $t$ and the number of bootstrap samples $n_{bootstrap}$. They also highlight that the ridge regression-based estimator of $\sigma^2$ in designed experiments—especially in settings like supersaturated designs where variance estimation is difficult—has not been explored and could be important future work. They additionally note the possibility that adding thresholding to lasso CV/LOOCV could improve type I error, and that augmenting lasso selection with visual/profile diagnostics might improve performance.","Extend the evaluation to design-respecting resampling strategies (e.g., fold assignments that preserve orthogonality/alias structure, blocked CV for split-plot or hard-to-change factors) and to autocorrelated or batch-correlated errors common in industrial and laboratory experimentation. Develop or test heredity-constrained or hierarchy-respecting model selection paired with CV/LOOCV to better align with effect principles in DOE (weak/strong heredity) and potentially reduce false discoveries in screening. Provide broader real-data case studies and benchmarking across multiple domains to validate simulation findings and assess sensitivity to missing runs, outliers, and measurement drift. Package and document the DOE+CV workflows (best subsets + LOOCV/LB, screening metrics, SSD settings) into an R package with reproducible vignettes to improve accessibility and reduce implementation failure modes observed for some designs.",2506.14593v1,https://arxiv.org/pdf/2506.14593v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:54:48Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Prediction|Optimization|Other,Other,Variable/General (design parameter ξ ∈ R^d; example: radial patterns use ξk ∈ R^15 per step with 15 lines; K=20 steps),Healthcare/medical,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper proposes a sequential Bayesian experimental design (BED) method for active MRI k-space acquisition that adaptively selects physically-feasible subsampling trajectories (Cartesian/radial/spiral) to accelerate scans while preserving reconstruction and downstream task performance. At each acquisition step, the next batch design parameter ξ is optimized by maximizing expected information gain (EIG), updating the posterior over targets and enabling task-aware acquisition (e.g., joint reconstruction and anomaly/lesion segmentation). To cope with high-dimensional intractable posteriors, the approach uses diffusion-based generative models for posterior sampling, combined with a joint sampling–optimization scheme (“Contrastive Diffusions/CoDiff”) and stochastic gradient optimization over ξ under acquisition constraints and budget. Empirically, on fastMRI knee at 25% k-space, the method achieves mean SSIM 91.34, outperforming prior adaptive subsampling baselines including ADS (91.26) and SeqMRI (91.08), and it shows strong performance even at very high acceleration. The framework also demonstrates that downstream segmentation can be inferred accurately with fewer measurements than required for high-fidelity image reconstruction, highlighting the value of task-aware BED for adaptive stopping and targeted acquisition.","MRI measurement model: $Y = M_\xi \odot F_r X F_c^T + E$ and in vector form $y = S_\xi(F_c\otimes F_r)x + e = A_\xi x + e$ (extended to joint target $\theta=(x,z)$). The BED objective maximizes expected information gain $I(\xi)=\mathbb E_{p(y|\xi)}[H(p(\theta)) - H(p(\theta|y,\xi))]=\mathbb E_{p(y|\xi)}[\mathrm{KL}(p(\theta|y,\xi)\|p(\theta))]$, solved sequentially via $\xi_k^*\in\arg\max_\xi I_k(\xi)$ with the prior replaced by the current posterior. Posterior sampling is performed with (conditional) diffusion reverse SDEs; the conditional score is approximated using Diffusion Posterior Sampling and Tweedie’s formula for $\hat\theta_0^{(t)}=T(\theta^{(t)})$ to enable gradient-based optimization of EIG with diffusion-generated samples.","On the fastMRI knee test set with 25% k-space (≈4× acceleration), the proposed method achieves mean SSIM 91.34 versus ADS 91.26, SeqMRI 91.08, Fixed-mask DPS 90.13, LOUPE 89.52, and PG-MRI 87.97 (Random: 90.2). On fastMRI brain (radial) during sequential acquisition, PSNR/SSIM improve with k-space percent; e.g., iteration 1: PSNR 16.0957, SSIM 0.7890 at 3.3659% k-space; iteration 9: PSNR 20.6784, SSIM 0.9104 at 15.6789% k-space. The paper reports that even at very high acceleration (e.g., ~4% k-space, ~25×), SSIM remains in the 0.80–0.90 range, and shows qualitatively that segmentation maps can be recovered reliably with less sampling than needed for high-fidelity reconstruction.","The current procedure is described for single-coil acquisition and 2D images; extending to multi-coil (parallel imaging) and 3D volumes is noted as important. Performance depends on the quality of learned diffusion priors and thus requires sufficiently large training datasets; for downstream tasks it requires paired ground-truth data $(x,z)$ (reconstruction/segmentation) in adequate quantity.","The evaluation appears largely restricted to specific pattern families (Cartesian/radial/spiral) and a particular budget/schedule (e.g., K=20, 25% k-space), so sensitivity to different budgets, stopping rules, coil configurations, and scanner constraints is not fully characterized. Computational cost and wall-clock feasibility for real-time on-scanner adaptive optimization (including repeated diffusion sampling and gradient steps) are not quantified in terms of latency constraints typical for MRI. The conditional diffusion approximation (DPS with a Dirac approximation and Hessian-related term approximations) may affect robustness, especially under model mismatch (non-stationary or correlated noise, motion, or artifacts), but robustness analyses are not presented.","Extend the framework to multi-coil acquisition to leverage parallel imaging and to 3D imaging rather than stacks of 2D slices. Investigate additional subsampling schemes/trajectories beyond the Cartesian/radial/spiral settings considered (e.g., as suggested by Lazarus et al. and Boyer et al.).","Develop a real-time/low-latency implementation with explicit timing benchmarks and on-scanner constraints, possibly via amortized design networks or warm-started optimizers to reduce per-step computation. Add principled adaptive stopping criteria tied to posterior uncertainty or task-specific risk rather than a fixed number of steps/budget. Extend to robustness under non-i.i.d. noise, motion, and mild model mismatch, and to multi-task settings where acquisition must balance several downstream objectives simultaneously; provide open-source code to enable reproducible benchmarking.",2506.16237v1,https://arxiv.org/pdf/2506.16237v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:55:34Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Cost reduction|Other,Other,"2 factors (electrochemical parameters: anode and cathode rate constants $k_n$, $k_p$)",Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper proposes an optimal experimental design (OED) method for Li-ion battery electrochemical parameter identification that uses deep reinforcement learning (TD3) to optimize the input current excitation. The OED objective is to maximize information content for estimating two key parameters—the anode and cathode rate constants ($k_n$, $k_p$)—using sensitivity-based Fisher Information (FI) computed from terminal-voltage sensitivity. The DRL policy is compared against a nonlinear model predictive control (NMPC) OED formulation and conventional test procedures (1C constant-current discharge, RCID, and a standard drive cycle) under the same current/voltage constraints and horizon. In simulation on an electrochemical equivalent circuit model (E-ECM) of a 30Ah NMC-graphite cell, the DRL-designed excitations yield higher FI and lower least-squares parameter estimation errors than NMPC and conventional tests, while using shorter experiment time than conventional procedures. The work advances battery OED literature by demonstrating a constraint-aware, model-free DRL approach that can outperform a model-based closed-loop optimizer (NMPC) in information yield per experiment duration.","FI is defined from local voltage sensitivity under i.i.d. Gaussian noise as $\mathrm{FI}=\frac{1}{\sigma_y^2}\sum_{k=1}^N\left(\frac{\partial y_k}{\partial \theta}\right)^2$, and the Cram\'er–Rao lower bound is $\sigma^2(\hat\theta)\ge \mathrm{FI}^{-1}$. The OED reward/objective used in both TD3 and NMPC is sensitivity-squared of voltage to the target parameter, i.e., per-step reward $r_k=(\partial V_k/\partial\theta_i)^2$ with a penalty $M$ for constraint violation, and NMPC minimizes $\sum_{k=0}^{H-1} - (\partial V_k/\partial\theta_i)^2$ subject to nonlinear state dynamics and bounds on current and voltage. Parameter estimation is evaluated via least-squares: $\theta_i^* = \arg\min\sum_{k=0}^{N-1}(V_k(\theta_i,I_k)-\tilde V_k(\tilde\theta_i,I_k))^2$.","Across 1800 s experiments, the DRL OED achieves higher average FI than NMPC for both parameters: for $k_p$, $2.90\times10^{12}$ (DRL) vs $1.33\times10^{12}$ (NMPC); for $k_n$, $4.86\times10^{12}$ (DRL) vs $1.65\times10^{12}$ (NMPC). Median absolute parameter-estimation errors are lowest for DRL: 0.244% for $k_p$ and 0.233% for $k_n$, compared with NMPC’s 0.340% ($k_p$) and 0.304% ($k_n$). Conventional tests are less informative and/or much longer (e.g., RCID length 102,212 s with median errors 1.072% for $k_p$ and 0.721% for $k_n$; 1C discharge length 3,800 s with 1.903% and 1.492%). The NMPC online computation time is reported as much higher than DRL policy execution (about 5 s vs 5.2 ms per time step), while DRL requires long offline training (~24 hrs).","The authors note that while NMPC can achieve comparable parameter-estimation accuracy, it has high online computational burden due to continuous optimization, which may restrict practical applicability. They also acknowledge that the DRL approach requires a long training phase (about 24 hours) to learn the optimal policy. The study’s results are based on simulation using the E-ECM model and synthetic voltage data for the least-squares identification tests.","The OED criterion is essentially single-parameter, local-sensitivity FI (sensitivity-squared) rather than a multi-parameter Fisher Information Matrix with a standard scalarization (e.g., D-optimality), so it may not ensure joint identifiability or control parameter correlations when estimating multiple parameters simultaneously. The analysis appears to rely on model-based sensitivities and assumes i.i.d. Gaussian measurement noise and adequate model fidelity; robustness to model mismatch, colored noise, bias, hysteresis, and unmodeled dynamics is not established. Constraints and operating conditions are fixed (SoC starts at 100%, 1800 s horizon, specific current/voltage bounds), so generalization to other initial SoC, temperatures, aging states, and practical actuator limits (slew rate, power limits, thermal constraints) is unclear. No shared implementation/code is provided, which limits reproducibility and benchmarking against other OED methods (e.g., multi-objective MPC/OED, Bayesian OED, or persistent excitation metrics).",Future work will integrate the learned DRL policy to characterize an actual Li-ion cell via hardware-in-the-loop and perform parameter estimation using voltage measurements from a real battery.,"Extend the OED formulation from single-parameter sensitivity rewards to a full Fisher Information Matrix objective (e.g., D-/A-/E-optimal) for simultaneous estimation of multiple electrochemical parameters with correlation handling. Evaluate robustness under realistic measurement conditions (colored noise, bias, sensor drift), process disturbances, temperature dynamics, and model mismatch, including aged-cell parameter drift. Incorporate additional practical constraints and costs (current slew-rate limits, energy/heat generation limits, safety constraints) and study transfer/generalization of the DRL policy across SoC/temperature ranges and cell-to-cell variability. Provide an open-source MATLAB/Python reference implementation and standardized benchmarks to enable fair comparisons with alternative OED strategies (e.g., Bayesian optimization, stochastic MPC, space-filling input designs).",2506.19146v1,https://arxiv.org/pdf/2506.19146v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:56:31Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,"Variable/General (dynamic input sequence design; examples include 1–2 inputs; ARX/NARX regressors with na=3, nb=3 in benchmarks)",Manufacturing (general)|Other,Simulation study|Other,TRUE,MATLAB|Python,Not provided,http://cse.lab.imtlucca.it/˜bemporad/ideal,"The paper proposes an online (runtime) design-of-experiments framework for nonlinear system identification that chooses the next excitation input via active-learning acquisition functions rather than random/persistent excitation. Existing active-learning methods for static regression (IDW-based “ideal”, GSx, iGS, and QBC) are adapted to dynamical settings by optimizing over the admissible current input (or an L-step horizon) while updating model parameters recursively with a (extended) Kalman filter; for state-space/RNN models the approach also reconstructs hidden-state trajectories using EKF + RTS smoothing (and L-BFGS refinement). The main acquisition function combines an inverse-distance-weighting (IDW) proxy for predictive uncertainty with an IDW exploration/diversity term, and it incorporates input constraints and soft output constraints through penalty terms and an uncertainty-inflated constraint margin. The methods are evaluated on nonlinear system-ID benchmarks (two-tank, ethylene oxidation, industrial robot arm) using noisy synthetic data, showing improved sample efficiency over passive/random excitation; “ideal” is generally best or among the best, while QBC is reported as performing poorly in experiments. The work advances classical OED for system ID by providing a computationally practical, sequential/adaptive input-design approach that blends exploration–exploitation and accommodates constraints during online learning.","For NARX models, the next input is selected by maximizing an acquisition over admissible inputs: $u_k = \arg\max_{u\in\mathcal U}\, a(x_k(u)) - p(x_k(u))$, where $a(x)=s^2(x)+\delta z(x)$, $s^2(x)=\sum_{j=0}^k v_j(x)\|y_j-f(x_{j-1},\theta_k)\|_2^2$ is an IDW variance proxy, and $z(x)=\tfrac{2}{\pi}\tan^{-1}\big((\sum_{j=0}^k w_j(x))^{-1}\big)$ promotes exploration. A multi-step (receding-horizon) variant optimizes a length-$L$ input sequence and sums exploration/penalties over the horizon (with $s^2$ only for the first step). For state-space/RNN models, the acquisition is applied to $q=[x_{k|k}^\top,u^\top]^\top$ using an IDW variance that can include both output and state prediction errors (Eq. (15)).","Across three benchmarks with pool-based input selection and noisy synthetic data, the proposed online active-learning DoE methods generally achieve lower test RMSE / higher $R^2$ than passive (random) excitation for a given number of samples, indicating improved sample efficiency. With soft output constraints, adding a penalty term reduces mean constraint violation (MCV) substantially for active methods, whereas passive sampling can appear competitive in RMSE by violating constraints. The “ideal” (IDW uncertainty + exploration) acquisition is consistently best or near-best among the compared AL rules (GSx, iGS), while QBC was implemented but omitted from reported experiments due to poor performance relative to passive sampling. The paper also reports runtime characteristics: acquisition evaluation dominates computation for NARX (EKF update is negligible), while for RNN state-space models EKF updates and periodic EKF+RTS state reconstruction add substantial cost.","The authors note that closed-loop identification (inputs depending on past outputs) can introduce bias; they argue the exploration term is designed to mitigate this but suggest instrumental-variable techniques could be incorporated to further address closed-loop effects. They also mention that if the sampling frequency is too high for real-time optimization, the method may need to be run on a digital twin and replayed on the real process, implying real-time applicability can be limited by computation. They report that QBC performed poorly in their tests and is therefore not included in experimental results.","The acquisition functions are heuristic and are not tied to classical Fisher-information optimality (e.g., D-optimality), so there are no guarantees of global optimality or statistical efficiency beyond empirical performance. The evaluations rely on simulated/noisy synthetic data from known benchmarks with specific hyperparameter settings (e.g., $\delta$, penalty weights, EKF covariances), leaving sensitivity/robustness to tuning and model mismatch only partially explored. Pool-based discretization of inputs (finite candidate set) simplifies optimization but may limit applicability or performance when inputs are continuous/high-dimensional or when actuation constraints create nontrivial feasible sets. The approach assumes known scaling statistics (means/standard deviations) and uses EKF-based recursive learning, which can be brittle under strong nonlinearities, non-Gaussian noise, or unmodeled dynamics; comparisons to more modern Bayesian/OED or information-theoretic input-design baselines are limited.","Future work will address integrating the active-learning input-design strategies with closed-loop control, such as model predictive control. They also suggest incorporating instrumental-variable techniques as a way to further handle potential closed-loop identification bias.","Extending the framework to continuous-input optimization with efficient gradient-based solvers (and guarantees/approximations) would broaden applicability beyond pool-based sampling. A principled link to information-based OED criteria (e.g., Fisher information, Bayesian experimental design) and theoretical performance guarantees (or regret-style bounds) would strengthen the methodology. More extensive robustness studies—unknown scaling, autocorrelated disturbances, non-Gaussian noise, actuator dynamics, and constraints with safety guarantees—would help deployment on real systems. Providing an open-source reference implementation for both MATLAB and Python pipelines (including RNN state reconstruction and acquisition optimization) would improve reproducibility and adoption.",2506.21754v1,https://arxiv.org/pdf/2506.21754v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:57:20Z
TRUE,Optimal design|Bayesian design,Parameter estimation|Robustness|Cost reduction,D-optimal|Bayesian D-optimal,"Variable/General (supports discrete-only, continuous-only, and mixed-factor designs; examples include 1 factor and 5 factors)",Semiconductor/electronics|Food/agriculture|Theoretical/simulation only|Other,Other,TRUE,R,Package registry (CRAN/PyPI),https://CRAN.R-project.org/package=ForLion,"This paper introduces the R package ForLion for constructing optimal experimental designs in design spaces containing continuous factors, discrete factors, or a mixture of both. The package implements the ForLion algorithm to compute locally D-optimal approximate designs (maximizing |F(\xi,\theta)| for a specified parameter vector) for linear models, generalized linear models, and multinomial logistic models, and implements the EW ForLion algorithm to compute robust “EW D-optimal” designs by maximizing the determinant of an expected Fisher information matrix under a prior (integral-based) or via Monte Carlo/bootstrapped parameter draws (sample-based). A key practical contribution is an included rounding procedure that converts approximate designs to exact designs with integer allocations and user-chosen rounding grids for continuous factors, while aiming to preserve high efficiency and reduce distinct settings. The paper demonstrates usage on an ordered multinomial response experiment (house flies emergence vs. gamma radiation) and a mixed-factor logistic regression example from semiconductor reliability (electrostatic discharge), illustrating locally D-optimal and robust EW D-optimal designs. Overall, it contributes software and algorithms targeted at mixed-factor optimal design without discretizing continuous factors upfront, emphasizing guaranteed optimality for the implemented criteria and practical implementability via exact-design conversion.","For local D-optimality, the design criterion is the determinant of the Fisher information matrix: maximize $f_{\theta}(\xi)=|F(\xi,\theta)|$ where $F(\xi,\theta)=\sum_{i=1}^m w_i F(x_i,\theta)$. For robust EW D-optimality under a prior $Q$, the criterion is $f_{EW}(\xi)=\left|\mathbb{E}\{F(\xi,\Theta)\}\right|=\left|\int_{\Theta}F(\xi,\theta)\,Q(d\theta)\right|$; a sample-based approximation maximizes $f_{SEW}(\xi)=\left|\frac{1}{B}\sum_{j=1}^B F(\xi,\hat\theta_j)\right|$. The ForLion stopping/optimality check uses the sensitivity function $d(x,\xi)=\mathrm{tr}\left(F(\xi)^{-1}F(x)\right)$ and declares D-optimality when $\max_{x\in\mathcal{X}} d(x,\xi)\le p$.","In the house-flies MLM example on $[0,200]$ Gy, the locally D-optimal approximate design had 3 support points (0, 103.5300, 149.2116) with weights (0.2027, 0.3981, 0.3992); rounding to a 0.1-Gy grid and $N=3500$ produced an exact design with relative efficiency 0.9999989. The sample-based EW D-optimal approximate design for the same example used 4 support points (0, 103.4258, 148.7592, 149.5954), and after rounding/merging produced a 3-point exact design with reported relative efficiency 1.000056. In the semiconductor ESD GLM mixed-factor example (4 binary factors plus continuous Voltage in [25,45]), a locally D-optimal approximate design with 15 support points was reported; converting to an exact design with rounding level 0.1 and $N=500$ yielded 14 points with relative efficiency 1.000069. For the same ESD setting, an integral-based EW D-optimal design with 18 support points was computed (reported runtime ~2331 seconds on the authors’ machine), and an exact rounded design had relative efficiency 1.000431; sample-based EW designs based on simulated priors achieved reported relative efficiencies as low as ~0.9924 vs the integral-based EW benchmark in their comparison.",None stated.,"The methodology and software focus almost exclusively on D- and EW D-optimality; other criteria (A-, I-, c-, etc.) are not implemented and would require additional development (the paper notes possible extension but does not provide it). Reported empirical demonstrations are limited to a small number of examples; there is no broad benchmark suite across many models/factor structures, nor extensive sensitivity analyses for tuning parameters (e.g., merging thresholds $\delta,\delta_0,\delta_2$) that may affect support size and practical performance. Robustness is handled via EW (expectation of information) which is not equivalent to full Bayesian D-optimality and may be sensitive to prior misspecification; practical guidance on prior elicitation and diagnostics is limited.",The authors note that the current package concentrates on D-optimality and suggest it can be extended to other optimality criteria if corresponding lift-one algorithms are developed.,"Develop and validate implementations for additional criteria (A-, I-, G-, c-, and compound criteria) and provide guidance for choosing among them for prediction vs estimation goals. Extend the framework to common practical complications such as correlated/longitudinal responses, model misspecification, constraints (e.g., forbidden combinations, cost-weighted runs), and multi-response settings. Provide a comprehensive benchmarking study and/or an accompanying vignette suite comparing ForLion/EW ForLion with competing mixed-factor design software under standardized scenarios, plus publish reproducible scripts and performance profiles for scalability.",2507.00923v1,https://arxiv.org/pdf/2507.00923v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:58:04Z
TRUE,Optimal design|Other,Parameter estimation|Cost reduction,D-optimal,"Variable/General (regression dimension n typically ≤ 50; m ≫ n; N ∈ {n,…,n+10})",Theoretical/simulation only|Healthcare/medical|Other,Simulation study|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/StefanoCipolla/D Optimal Desing Matlab,"The paper studies the exact D-optimal experimental design (EED) problem, i.e., selecting integer replicate counts over a large candidate set of regression points to maximize the log-determinant of the Fisher information matrix. It proposes a hybrid algorithm that first solves the continuous (approximate/limit) D-optimal design via a column-generation scheme, where each restricted master problem is solved to high accuracy with a primal–dual interior-point semidefinite-programming solver (SDPT3), enabling rapid identification of the optimal support when m ≫ n. Using the identified support, it then constructs a near-optimal feasible exact (integer) design using local-search exchange heuristics restricted to that support, and proves the same worst-case approximation bound as if the local search were run on the full candidate set. Extensive computational experiments on large synthetic datasets and large UCI datasets show substantial speedups and robust support identification versus Frank–Wolfe methods for the MVEE/limit problem, and orders-of-magnitude faster performance with comparable objective values versus branch-and-bound mixed-integer approaches (e.g., Boscia) in challenging regimes N ≈ n. The work advances scalable computation of high-quality exact D-optimal designs for very large candidate sets by combining column generation, elimination rules (Harman–Ponzato), and support-restricted integer local search with provable guarantees.","Exact D-optimal design is formulated as maximizing $g_0(u)=\ln\det(XUX^T)$ over simplex weights $u\ge 0$, $\sum_i u_i=1$, with integrality constraints $u_i N\in\mathbb{Z}$ (equivalently $u_i=n_i/N$). The continuous relaxation (limit/approximate design) drops integrality; its dual is the MVEE problem $\min_H \ln\det(H^{-1})$ s.t. $x_i^T H x_i\le n$. Column generation adds points based on maximum violation $z(\bar H)=\max_i\{0, x_i^T\bar H x_i-n\}$, and uses the Harman–Ponzato rule to eliminate non-support points via the threshold $h_n(\varepsilon)$, discarding points with $x^T H x<h_n(\varepsilon)$.","In the targeted regime (moderate dimension $n\le 50$, very large candidate sets $m$ up to $10^7$, and small experiment budgets $N\in\{n,\dots,n+10\}$), the proposed column-generation + SDP approach identifies supports and solves the limit problem substantially faster than Frank–Wolfe baselines, often by around two orders of magnitude on harder instances (as reported for $n=50$). For exact D-optimal design on medium-sized synthetic instances (up to $m=500,n=50$), the full pipeline produces objective values matching or improving mixed-integer branch-and-bound solutions while running far faster (with Boscia often hitting a 1800s time cap, while the proposed method solves instances in seconds to tens of seconds). Theoretical guarantees show support-restricted local search achieves the same worst-case approximation bound as local search over the full dataset, with bound factor depending only on $(n,N)$ (e.g., via $h(N,n)=n\log\frac{N}{N-n+1}$ and $\det(G)\ge((N-n+1)/N)^n\det(X N U_I^* X^T)$). Real-world tests on UCI-scale datasets (HIGGS, SUSY, SGEMM GPU) confirm consistent speed and small optimality gaps for produced exact designs.",None stated.,"The approach is specialized to D-optimality for (generalized) linear regression settings where Fisher information has the stated form and candidate regressors span $\mathbb{R}^n$; extensions to other models/criteria may require nontrivial reformulation. Performance and correctness depend on solving restricted SDPs to high accuracy (e.g., tight SDPT3 tolerances) and on repeated evaluation of violations over very large $m$, which may be memory/IO bound for truly massive datasets. The paper emphasizes regimes with $N\approx n$; behavior for much larger $N$ or under practical complications (unknown error structure, heteroskedasticity, correlated observations) is not developed.",None stated.,"Develop self-starting/online variants that update the support and exact design as new candidate points stream in (adaptive/streaming column generation). Extend the framework to other optimality criteria (A-, I-, G-optimality) and to constrained designs (e.g., costs, blocks, or linear constraints on counts) while preserving scalability. Add robustness to model misspecification and data issues common in practice (heteroskedasticity, leverage/outliers, collinearity), and provide an open-source implementation as a reusable package with standardized benchmarks and interfaces to common DOE workflows.",2507.03210v1,https://arxiv.org/pdf/2507.03210v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:58:47Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Model discrimination|Robustness|Cost reduction|Other,Bayesian D-optimal|Not applicable|Other,"Variable/General (causal graph nodes/variables; examples include d=10, d=20, d=30 with multi-target interventions such as 3 interventions per stage)",Healthcare/medical|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable|Other,Not provided,https://arxiv.org/abs/2507.07359,"The paper proposes GO-CBED, a goal-oriented Bayesian framework for sequential (non-myopic) experimental design for causal learning in structural causal models (SCMs). Instead of maximizing expected information gain (EIG) about the full causal model (graph and mechanisms), GO-CBED maximizes EIG about a user-specified causal quantity of interest (QoI), such as a particular interventional effect or the causal graph itself. To make EIG optimization tractable, it introduces a variational lower bound on EIG and jointly trains (i) a transformer-based policy network for selecting intervention targets and values over multiple stages and (ii) variational posteriors (normalizing flows for causal-effect QoIs; factorized Bernoulli edges for graph QoIs). The policy is trained offline via simulated interventional trajectories and then deployed as an amortized, real-time intervention selector. Empirically, GO-CBED outperforms myopic/greedy and discovery-oriented baselines on synthetic ER/scale-free SCMs and semi-synthetic gene regulatory networks (DREAM E. coli/Yeast), especially under limited budgets and nonlinear mechanisms, showing that aligning design objectives with the causal query improves efficiency.","Design variable (intervention) is encoded as \(\xi:=\{I, s_I\}\) where \(I\) is the intervention target set and \(s_I\) the intervened values (perfect interventions \(do(X_I=s_I)\)). The non-myopic goal-oriented objective maximizes EIG on a QoI \(z\): \(I_T(\pi)=\mathbb{E}[\log \tfrac{p(z\mid h_T)}{p(z)}]\) with \(\xi_t=\pi(h_{t-1})\), where history \(h_t=\{\xi_{1:t},x_{1:t}\}\). Because \(p(z\mid h_T)\) is intractable, they optimize the variational lower bound \(I_{T;L}(\pi;\lambda,\phi)=\mathbb{E}[\log \tfrac{q_\lambda(z\mid f_\phi(h_T))}{p(z)}]\), tight when \(q_\lambda(z\mid f_\phi(h_T))=p(z\mid h_T)\).","Across multiple simulated benchmarks, GO-CBED policies trained for the target QoI (GO-CBED-z) achieve higher prior-omitted EIG (or its lower bound) on the query than policies trained for full-model learning (e.g., GO-CBED-G for graph discovery) and random intervention selection, with gains most pronounced for nonlinear causal mechanisms and limited experimental budgets. In the motivating linear-Gaussian fixed-graph example (T=10), GO-CBED trained on parameter-QoI can have higher EIG on parameters but performs substantially worse on the actual causal-query EIG than GO-CBED-z, demonstrating objective misalignment. On semi-synthetic DREAM E. coli gene regulatory networks, GO-CBED-z shows a marked improvement over discovery-oriented and random baselines after early stages (around stage 3) and ends with substantially higher EIG on the causal-effect QoI. For causal discovery evaluations, GO-CBED improves structural recovery metrics (lower expected structural Hamming distance and higher F1) relative to baselines such as CAASL, SoftCBED, DiffCBED, and random across ER/SF and DREAM settings.",The authors state that scalability is constrained by the complexity of the underlying causal models and the neural network architectures. They also note that effectiveness depends on having prior knowledge over causal structures and mechanisms. They further mention a need for improved robustness to changing experimental horizons and dynamic model updates during experimentation.,"The approach relies on a “world simulator” for offline training (sampling \(G,\theta\) and simulating interventional outcomes); performance in real labs may degrade under model misspecification or when the simulator cannot faithfully represent the system. The variational posterior for graphs uses independent Bernoulli edges, which may poorly capture strong structural dependencies/acyclicity constraints and can misrepresent posterior uncertainty. The method’s compute and data requirements for offline RL training (many simulated environments/steps) may be substantial, potentially limiting accessibility and reproducibility without released code and trained models. Comparisons may be sensitive to how baselines’ inference networks are trained/tuned and to the choice of QoIs and intervention constraints (e.g., perfect interventions and selected value ranges).",They propose incorporating foundation models as high-fidelity world simulators for offline policy training (notably biological foundation models). They also suggest extending GO-CBED to multi-target intervention settings and to non-differentiable likelihoods. Additional directions include improving policy robustness to changing experimental horizons and enabling dynamic model updates during experimentation.,"Develop self-starting or online-adaptive variants that can update the simulator/posterior with real experimental data and handle model misspecification (e.g., robust or Bayesian model averaging over simulator families). Extend to settings with soft/imperfect interventions, hidden confounding, cycles, or measurement error, which are common in real causal systems. Provide principled constraints for experimental feasibility (ethics/cost/safety) and incorporate explicit budget/cost terms into the sequential objective. Release reference implementations, trained policies, and standardized benchmarks to enable reproducible comparisons and ablation studies on architecture and variational choices.",2507.07359v1,https://arxiv.org/pdf/2507.07359v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T07:59:31Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,Other,Variable/General (design variables include number of quadrants/samples m and subsampling proportion q; k polymer categories typically 10),Environmental monitoring,Simulation study|Case study (real dataset)|Other,TRUE,Other,Not provided,https://remarco.org/,"The paper proposes a Bayesian optimal experimental design framework for microplastics (MP) monitoring campaigns under budget constraints. It models MP counts per sampled quadrant as Poisson with a Gamma prior on the abundance rate (Poisson–Gamma) and polymer-category counts as Multinomial with a Dirichlet prior (Dirichlet–Multinomial), enabling conjugate closed-form posteriors. Design is sequential: first choose the number of spatial samples/quadrants m (with fixed sampled area A per quadrant), then choose the proportion q of observed particles to send for costly polymer identification (e.g., FTIR/Raman) subject to a cost constraint. The design criterion is based on expected posterior variance reduction for abundance and composition, combined with a realistic cost model; the remaining expectation over total counts is computed via simulation from the predictive distribution. Simulations and an empirical-data-inspired example show the recommended m and q adapt to prior abundance beliefs and relative costs, often implying categorizing around ~100 particles and reallocating effort between field sampling and lab analysis as conditions change.","Counts model: for quadrant j=1,...,m, $N_j\mid\Lambda=\lambda\sim\mathrm{Poisson}(A\lambda)$ with prior $\Lambda\sim\mathrm{Gamma}(\alpha,\beta)$, giving posterior $\Lambda\mid N\sim\mathrm{Gamma}(\alpha+\sum_j N_j,\beta+mA)$. Composition model: conditional on $N_j=n_j$, category counts $S_j\mid P,N_j\sim\mathrm{Multinomial}(n_j,P)$ with prior $P\sim\mathrm{Dirichlet}(\gamma)$, and under subsampling $\bar n(q)=\lfloor nq\rfloor$ the posterior remains Dirichlet with updated counts. Design losses use variance-ratio reduction: $L_1=\mathrm{Var}(\Lambda\mid\cdot)/\mathrm{Var}(\Lambda)$ and $L_2=\mathrm{Tr}(\Sigma_1)/\mathrm{Tr}(\Sigma_0)$, with closed forms for prior-expected variance reduction (Theorems 1–2) and a budget constraint based on $c_0+m(c_1+c_2A)+c_3n+c_4\bar n(q)$.","Closed-form prior-expected variance reduction for abundance is $L_1^*(m)=\big(1+(\nu_0/\lambda_0)mA\big)^{-1}$. A closed-form prior-expected variance reduction for composition under subsampling is given in Theorem 2 as a function of $\bar n(q)$ and $\gamma_0$. In an illustrated budget equivalent to sampling 12 quadrants (A=0.252 m^2, r1=5×10^-5, r2=3×10^-3), optimal designs in two priors yield m*≈7 (lower expected abundance) versus m*≈4 (higher expected abundance). The framework reproduces the practical behavior that q starts near 1 for small n and then decreases while $\bar n(q)$ stays roughly around ~100, implying limited marginal benefit of categorizing far more than ~100 particles under typical costs.","The authors assume a fixed, standardized within-quadrat sampled area A and do not optimize A. They also assume conditional independence and site homogeneity across sampled quadrants (all $N_j$ from a common Poisson rate), noting that homogeneity/within-site variability is not addressed and that replication was not explicitly optimized. They note the expectation in the overall objective (Eq. 16) cannot be computed analytically and is approximated via simulation.","The design criterion is variance-reduction-based and may not align with specific regulatory decisions (e.g., exceedance/threshold detection) or utility functions; different goals could change the optimal allocation. The Poisson assumption may be fragile under spatial clustering/overdispersion (common in environmental counts), suggesting a Negative Binomial or hierarchical spatial model could materially alter recommendations. The paper implies computation is quick but does not provide implementation details, making reproducibility and adoption harder, and comparisons to alternative Bayesian design criteria (e.g., expected KL information gain) are not fully benchmarked.","They propose future work to address site homogeneity/within-site variability explicitly, emphasizing analysis of spatial consistency across replicates and its implications for standardized monitoring guidelines. They also suggest adding an intermediate step in the sequential analysis to decide the maximum number of MPs for identification prior to polymer categorization.","Extend the framework to handle overdispersed and spatially correlated counts (e.g., Gamma–Poisson mixed effects, log-Gaussian Cox processes) and to optimize both m and the sampled area A (or adaptive quadrat placement) when A is not fixed. Develop decision-focused utilities (threshold exceedance, source attribution, trend detection over time) and compare variance-based losses to KL-based or Bayesian decision-theoretic objectives. Provide open-source software (R/Python) and practical templates for practitioners, including sensitivity tools for priors and cost parameters and guidance for Phase I prior elicitation from pilot studies.",2507.08170v1,https://arxiv.org/pdf/2507.08170v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:00:18Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design,Parameter estimation|Prediction|Model discrimination|Other,Other,"Variable/General (design variables ξ; examples include T=10 location-finding with 2D designs; T=20 temporal discounting with design ξ=(R,D); CES with 6D design for two baskets)",Healthcare/medical|Finance/economics|Other|Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Not provided,https://books.google.co.uk/books?id=eBSgoAEACAAJ|https://proceedings.mlr.press/v235/iollo24a.html|https://arxiv.org/abs/2410.11826|https://arxiv.org/abs/2409.05354|https://proceedings.mlr.press/v235/iqbal24a.html|https://proceedings.neurips.cc/paper/2021/file/d811406316b669ad3d370d78b51b1d2e-Paper.pdf|https://arxiv.org/abs/2110.15335|https://www.sciencedirect.com/science/article/pii/S0967066102001867,"The paper proposes Stepwise Deep Adaptive Design (Step-DAD), a semi-amortized, policy-based Bayesian experimental design method that periodically fine-tunes a pretrained design policy during a live sequential experiment. The objective is to maximize total expected information gain (EIG; mutual information between parameters θ and outcomes y) over a T-step experiment, improving robustness and decision quality relative to fully amortized policies like Deep Adaptive Design (DAD) and to traditional greedy adaptive BED. Step-DAD decomposes total EIG into pre- and post-update segments and introduces an infer–refine procedure: perform posterior inference using data gathered so far, then refine the policy by stochastic gradient ascent on a variational EIG lower bound (e.g., sequential PCE / contrastive bounds) for the remaining steps. The method is evaluated via simulation on multiple canonical BED tasks (source location finding, hyperbolic temporal discounting questionnaires, and a constant elasticity of substitution preference model), showing consistent EIG improvements and better robustness to prior perturbations and to extrapolated horizons. Practically, the approach trades small amounts of test-time computation for substantially improved adaptive designs while retaining much of the real-time benefit of amortized policies.","The design objective is Expected Information Gain (EIG): $I(\xi,y)=\mathbb{E}_{p(y\mid \xi)}[H[p(\theta)]-H[p(\theta\mid \xi,y)]]$, and for policies $\pi$ the total EIG over $T$ steps is $I_{1\to T}(\pi)=\mathbb{E}_{p(h_T\mid \pi)}[H[p(\theta)]-H[p(\theta\mid h_T)]]$. Step-DAD uses a decomposition $I_{1\to T}(\pi)=I_{1\to \tau}(\pi)+\mathbb{E}_{p(h_\tau\mid \pi)}[I^{h_\tau}_{\tau+1\to T}(\pi)]$ to justify mid-experiment refinement. Policy refinement optimizes a sequential contrastive variational lower bound (sPCE) on remaining EIG: $\mathcal{L}^{h_{\tau_k}}_{\tau_k+1\to T}(\pi)=\mathbb{E}\big[\log \frac{p(h_{\tau_k+1:T}\mid \theta_0,h_{\tau_k},\pi)}{\frac{1}{L+1}\sum_{\ell=0}^L p(h_{\tau_k+1:T}\mid \theta_\ell,h_{\tau_k},\pi)}\big]$ (with contrastive samples $\theta_\ell$).","Across simulated benchmarks, Step-DAD consistently improves total EIG versus fully amortized DAD given comparable (or smaller) training budgets. In source location finding (T=10), DAD achieves an upper-bound EIG of about 7.089, while Step-DAD at $\tau=6$ reports ~7.765 (upper) / 7.759 (lower), and Step-DAD with reduced pretraining (10K) can match or exceed DAD with 50K steps for $\tau>3$. In hyperbolic temporal discounting (T=20), DAD has ~4.808 (upper) while Step-DAD ($\tau=10$) achieves ~6.721 (upper). In the CES model (T=10), Step-DAD ($\tau=5$) reports ~14.623 (upper) vs DAD ~11.478 (upper), and Step-Static is also competitive with greedy baselines in that setting.",None stated.,"The method’s benefits depend on the availability of test-time compute for posterior approximation and policy fine-tuning; in latency-critical settings this may be infeasible. Results are primarily simulation-based on benchmark models; real-world experimental deployments and sensitivity to model misspecification beyond the tested prior-perturbation scenario are not fully established. Performance comparisons use variational lower/upper bounds (sPCE/sNMC) and a conservative difference estimator for Step-DAD, so true EIG improvements could differ if bounds are loose in certain regimes (as the authors note for at least one experiment).",None stated.,"Evaluate Step-DAD in real experimental pipelines (e.g., online personalization studies) with operational constraints and nonstationarities to validate robustness claims beyond simulated prior shifts. Develop self-starting or uncertainty-aware refinement schedules (learn $\tau_k$ adaptively) and parameter-efficient fine-tuning methods to reduce test-time compute while retaining gains. Extend analysis to broader forms of misspecification (likelihood errors, dependence/autocorrelation, constraints) and provide more formal guarantees or diagnostics for when refinement is expected to help.",2507.14057v1,https://arxiv.org/pdf/2507.14057v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:01:03Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Computer experiment|Other,Parameter estimation|Optimization|Prediction|Screening|Cost reduction|Other,Other,Variable/General (examples include 1D parameter θ; 2 parameters in KdV; 2D design spaces; 22D alanine dipeptide MD with 2 collective variables),Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Acc-BOED, an accelerated Bayesian optimal experimental design framework that targets expensive simulation settings by reformulating the expected utility (information gain) computation from a nested integral into an independent double-integral form using Bayes’ rule and a likelihood-ratio representation. It uses Gaussian process regression for surrogate modeling and learns the ratio p(y|d,z)/p(y|d) via machine-learning conditional density estimation (Kernel Mixture Network), avoiding repeated constrained GP refits. A covariance-based screening criterion selects informative (d,z) pairs for training and filters prior samples during Monte Carlo utility evaluation, reducing computation while preserving design quality. The method is demonstrated on surrogate construction (2D trigonometric, alanine dipeptide free-energy surface), parameter estimation (Gaussian error function, KdV PDE parameters), and failure probability estimation (circle and four-branch limit-state problems), with reported per-iteration speedups of about 6–13× versus a basic BOED baseline while achieving similar accuracy to BOED and outperforming LHS/random baselines in learning curves.","The BOED objective selects d* by maximizing expected KL information gain: d* = argmax_{d\in D} \int p(z)\int p(y|d,z)\log\frac{p(y|d,z)}{p(y|d)}\,dy\,dz (Eq. 6), and is further rewritten into an independent double-integral form by multiplying/dividing by p(y|d) (Eq. 7). The conditional density is approximated as \hat p(y|d,z) \propto q_{CDE}(y|d,z)\,p(y|d) (Eq. 8), where q_{CDE} learns the density ratio p(y|d,z)/p(y|d); informative regions are defined by a covariance threshold \Omega_z = \{d: \mathrm{COV}(y_d,y_z)\ge \epsilon_{cov}\} (Eq. 12) and z-sample filtering uses \mathrm{COV}(y_{d_i},y_z)>\epsilon_{cov} (Eq. 13).","Across six test cases, Acc-BOED reduces per-iteration execution time from 1234–3251 s (basic BOED) to 138–346 s, corresponding to speedups of about 6.39× to 13.58× (Table 1). In the circle failure-probability example, Acc-BOED achieves an estimated failure probability 0.002516 versus ground truth 0.002460 at 55 points (relative error 2.2764%), outperforming random (0.004006; 62.8455% error) and LHS (0.002799; 13.7805% error) (Table 2). For the four-branch system at 50 points, Acc-BOED estimates 0.002342 vs 0.00225 (4.0889% error), while random and LHS are both near 0.00063–0.00066 (≈71–72% error) (Table 3). At 110 points for the four-branch system, Acc-BOED reaches 0.002296 (2.0444% error), compared to random 0.001174 (47.8222%) and LHS 0.002351 (4.4889%) (Table 4).","The paper notes that several related BOED approaches (e.g., variational/lower-bound utility approximations) are not discussed due to space limitations. It also highlights limitations of prior work it compares against (e.g., gradient dependence, high computational cost), motivating the proposed acceleration approach.","The method relies heavily on GP modeling assumptions (e.g., kernel choice, smoothness, and typically independent Gaussian noise) and uses GP covariance as a proxy for informativeness; if the surrogate is misspecified or the process is nonstationary/heavy-tailed, covariance screening may discard useful regions. The conditional density estimation step (KMN) introduces an additional model with its own hyperparameters/training stability concerns, yet sensitivity analyses and ablations (e.g., effect of \epsilon_{cov}, KMN architecture, or alternative CDE models) are not clearly quantified here. Practical reproducibility is limited because implementation details and code are not provided, and the optimization routine for argmax_d of the Monte Carlo utility is not fully specified in this excerpt.","Future work will explore integrating other advanced density-estimation/approximation techniques—such as normalizing flows and variational methods—to handle higher-dimensional functions and more complex uncertainty structures, extending the applicability and performance of Acc-BOED.","A useful extension would be a principled/adaptive strategy to set \epsilon_{cov} (and quantify its bias–variance trade-off) and to provide theoretical guarantees on approximation error of the learned ratio q_{CDE}. Additional work could address settings with correlated/heteroscedastic noise, time-series outputs, and fully self-starting (unknown-parameter) Phase I/II style updates, as well as broader benchmarking against modern BOED baselines (e.g., variational EIG, CNF/flow-based EIG, acquisition functions from Bayesian optimization) on standardized test suites with open-source implementations.",2507.15235v1,https://arxiv.org/pdf/2507.15235v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:01:46Z
FALSE,Other,Other,Not applicable,Not specified,Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher),https://huggingface.co/datasets/JH976/Perovskite-R1|https://huggingface.co/qQwen/QwQ-32B|https://github.com/hiyouga/LLaMA-Factory,"The paper introduces Perovskite-R1, a domain-specialized large language model for discovering perovskite solar cell (PSC) precursor additives and generating experimental protocols via structured prompting. The authors curate 1,232 perovskite-related papers and a 33,269-compound library, convert PDFs to text, segment documents, and use OpenAI o1 to generate instruction-tuning QA pairs with chain-of-thought reasoning; they then fine-tune QwQ-32B using LoRA. The model is benchmarked against other LLMs on a perovskite QA benchmark and reports higher accuracy, including a stated ~10% improvement over the base QwQ-32B. For experimental validation, the study compares two Perovskite-R1-recommended additives (3,5-difluoropyridine-2-carboxylic acid and 5-hydroxy-2-methylbenzoic acid) versus two manually selected additives (gallic acid and caffeic acid) added at equal concentration to Cs0.05MA0.1FA0.85PbI3 devices, showing the AI-selected additives improve PCE while manual picks reduce it. Overall, the contribution is an AI-assisted, closed-loop literature-mining-to-lab-validation framework rather than a new formal DOE methodology.",Not applicable,"In the additive comparison on Cs0.05MA0.1FA0.85PbI3 PSCs (all additives at 0.1 mg/mL, AM 1.5G 100 mW/cm^2), control PCE is 18.30%. Perovskite-R1-recommended additives achieve PCEs of 18.58% (AI-DFCA) and 18.63% (AI-HMBA), while manually selected additives reduce PCE to 11.56% (Manual-GA) and 13.47% (Manual-CA). On the QA benchmark, Perovskite-R1 scores 86.92/85.06/84.63 (easy/medium/hard) versus QwQ-32B at 76.27/76.84/74.75, described as a ~10% accuracy gain over the base model.","The authors note that in single-turn QA the model’s outputs can be superficial and depend strongly on question phrasing, motivating multi-turn dialogue mechanisms. They also state that control over structured experimental design tasks is limited: process parameters such as molar concentration, additive ratio, and spin-coating speed often require manual post-processing because the model does not precisely model feasibility windows. They mention that current protocol generation is primarily via text prompts, and propose adding feasibility constraint checking and simulations in future.","The experimental validation is limited to a small number of additives (two AI-selected vs two manual) in a single perovskite composition/device process, so generality across chemistries, architectures, and labs is uncertain. The “manual” comparator may not represent best-in-class non-AI screening strategies (e.g., Bayesian optimization, high-throughput DOE, or cheminformatics-guided selection), making it hard to attribute gains specifically to the LLM. The work does not present a statistical DOE plan (randomization/blocking/replication, factor screening, interaction modeling) for the wet-lab studies, which limits causal attribution and reproducibility claims. Reliance on chain-of-thought-style data generation and prompting may introduce uncontrolled biases or leakage from literature patterns into recommendations without rigorous uncertainty quantification.","They propose improving depth via multi-turn dialogue and potentially reinforcement learning (e.g., RLHF) plus dialogue knowledge tracking for better consecutive-question understanding. For experimental protocol control, they plan a knowledge-graph-based feasibility constraint checking module and integration of physical simulation models for parameter-space mapping and boundary-condition verification. They also intend to expand beyond additive defect passivation to broader tasks such as interface engineering, solvent optimization, stability regulation, and device architecture co-design, and to integrate with automated synthesis/high-throughput experimentation for stronger closed-loop autonomy.","A natural extension is to incorporate explicit DOE/active-learning loops (e.g., Bayesian optimization or sequential experimental design) that propose batches of experiments with quantified uncertainty and constraints, rather than primarily text-based protocols. More rigorous, multi-lab validation with replication, randomization, and reporting of variability (confidence intervals) would strengthen claims about additive effectiveness and model robustness. Building standardized baselines against established materials-informatics screening pipelines (descriptor/QSAR + optimization) and ablation studies (literature-only vs library-only vs CoT vs no-CoT) would clarify which components drive performance. Releasing an end-to-end, versioned pipeline with deterministic preprocessing and evaluation scripts would improve reproducibility and facilitate broader adoption.",2507.16307v1,https://arxiv.org/pdf/2507.16307v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:02:50Z
TRUE,Bayesian design|Sequential/adaptive,Parameter estimation|Screening|Optimization|Prediction|Cost reduction|Other,Not applicable,Variable/General (examples include 4-parameter logistic regression; 6-parameter survival model with 5 nuisance parameters; multi-stage group sequential sample sizes),Healthcare/medical|Theoretical/simulation only,Approximation methods|Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/sgolchi/BartBvM,"The paper proposes an efficient Bayesian design-of-experiments methodology for sample size determination and Bayesian operating characteristics when models include nuisance parameters. It uses large-sample posterior normality (Bernstein–von Mises) to approximate the sampling distribution of posterior decision summaries such as $\tau(Y_n)=P(\psi(\theta)>\psi_0\mid Y_n)$, reducing the need for nested Monte Carlo. The only unknown quantity driving the approximation is a variance/scale term (linked to Fisher information) that depends on nuisance parameters; this term is learned over the parameter space using a probabilistic Bayesian additive regression trees (BART) emulator trained on a limited set of simulation-derived estimates. The approach supports both fixed designs and Bayesian group sequential (adaptive) designs via a multivariate normal approximation across interim analyses, enabling fast computation of assurance, stopping probabilities, expected sample size, and decision-theoretic expected cost. Performance is demonstrated in a stylized logistic-regression example and a time-to-event clinical trial–motivated example, showing close agreement with Monte Carlo while substantially reducing computation.","Decision statistic: $\tau(Y_n)=P(\psi(\theta)>\psi_0\mid Y_n)$ and its probit transform $\gamma_n=\Phi^{-1}(\tau)$. Under BvM/MLE asymptotics, $\gamma_n\approx \sqrt{n}\,\frac{\psi^*-\psi_0}{\lambda(\theta)}+\varepsilon$ with $\varepsilon\sim N(0,1)$, where $\lambda(\theta)$ is a nuisance-parameter–dependent scale (estimated/emulated). This yields an analytic power approximation $\Pi(\theta)\approx 1-\Phi\big(\Phi^{-1}(u)-\sqrt{n}(\psi(\theta)-\psi_0)/\hat\lambda(\theta)\big)$ and Bayesian assurance via integrating over a design prior. For group sequential designs, $(\gamma_{n_1},\ldots,\gamma_{n_T})$ is approximated as multivariate normal with covariance entries $\text{Cov}(\varepsilon_j,\varepsilon_k)=\sqrt{n_j/n_k}$, enabling stopping probabilities by multivariate normal integration.","In the 4-parameter logistic-regression example, a 40-point Latin hypercube training set is used to emulate $\lambda(\theta)$; LOOCV plots show close agreement between BART-predicted and simulated $\lambda$ and between BART-BvM power estimates and Monte Carlo power. In the clinical-trial–motivated Bayesian GSD example, a 60-point Latin hypercube training set and design-prior Monte Carlo integration with 100,000 draws are used to compute integrated stopping/power curves and decision-theoretic costs. Reported overall Bayesian assurance is about 0.81 (final cumulative) for design D1 and 0.82 for D2, with integrated expected sample size 365 (D1) vs 341 (D2) and integrated expected cost 394 (D1) vs 384 (D2) under the chosen cost weights. LOOCV assessment notes a maximum deviation from Monte Carlo cumulative power under 0.04 at some interim analyses.","The authors note that the Bernstein–von Mises normal approximation may be poor for small sample sizes and in more complex models, which can degrade the quality of operating-characteristic approximations. They also state that BART prediction quality may be sensitive to the size and distribution of the training set depending on the analysis model. They recommend using the method as an efficient exploratory tool, followed by full Monte Carlo evaluation for a small set of finalist designs.","The method depends on regularity conditions (e.g., identifiable parametric model, well-behaved MLE/posterior) and may be unreliable in weakly identified models, boundary parameters (e.g., near-zero hazards/censoring extremes), heavy-tailed posteriors, or models with nonstandard asymptotics. Learning a single scalar scale parameter $\lambda(\theta)$ may be insufficient when posterior dependence/shape changes materially across the parameter space or when decision summaries are not well captured by a probit-normal approximation. For group sequential settings, the assumed correlation structure derived from canonical joint distribution may not hold under model misspecification, complex missingness/censoring, or computational approximations (e.g., MCMC error). Practical performance will also hinge on how well the Latin hypercube (or other) training design covers the high-density region of the design prior, especially in higher dimensions.","They suggest extensions to improve performance when the normal approximation is inadequate, including small-sample corrections to the sampling distribution and using linear approximations (as in related recent work) rather than relying purely on posterior normality. They also note generalization beyond one-sided hypotheses (e.g., to interval hypotheses) is straightforward and implied as an extension direction.","Developing principled training-set construction and sequential enrichment strategies (active learning) for $\lambda(\theta)$ could improve emulator efficiency and robustness in higher-dimensional nuisance-parameter spaces. Extending the approach to handle model misspecification, autocorrelation/cluster dependence, and non-regular problems (e.g., mixture models, near-boundary parameters) would broaden applicability. Providing calibrated uncertainty quantification that propagates emulator error, MCMC error (when used for training), and BvM approximation error into assurance/ESS estimates would improve trust for regulatory settings. Packaging the method into a maintained software library with templates for common clinical-trial endpoints (binary, continuous, survival) and built-in diagnostics for approximation adequacy would facilitate adoption.",2508.03948v1,https://arxiv.org/pdf/2508.03948v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:03:36Z
TRUE,Optimal design|Other,Parameter estimation|Other,D-optimal|A-optimal|Other,Variable/General (parameter dimension d; finite design space of size n; examples with n=d=2 and n=d=3),Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper studies optimal experimental design (OED) on a finite design space, formulated as maximizing an optimality criterion ϕ applied to the Fisher-information moment matrix M(w)=∑_i w_i A_i over the simplex of design weights w. Its main methodological contribution is an optimization-based proof that the classical multiplicative algorithm (MA) produces a monotone nondecreasing sequence of objective values for a broad class of criteria previously analyzed by Yu (2010), avoiding auxiliary-variable/EM-style statistical arguments. The proof leverages a matrix Cauchy–Schwarz inequality and a criterion transformation ψ(M)=−ϕ(M^{-1}), with assumptions that ψ is differentiable, isotonic, and concave; additional sufficient conditions for strict monotonicity are derived (notably, λ∈(0,1) yields strict improvement under mild conditions). The paper also provides contrasting toy examples showing that MA can cycle or fail depending on the power parameter λ and the chosen optimality criterion, highlighting limitations for criteria like the c-criterion and motivating discussion of generalized designs for parameter subsystems. Overall, it advances the theoretical understanding of when and why MA is monotone/strictly monotone across common OED criteria (D-, A-, and pth-mean) and clarifies scenarios where standard MA can break down.","The OED moment (information) matrix is M(w)=\sum_{i=1}^n w_i A_i, where A_i is the Fisher information at design point x_i, and the design maximizes f(w)=\Phi(M(w)) with \Phi(M)=\varphi(M) if M\succ0 and −\infty otherwise. The multiplicative algorithm update is w^{k+1}_i \propto w^k_i\, (\nabla_i f(w^k))^{\lambda} with \lambda\in(0,1], followed by normalization to sum to 1. A key transformation used is \psi(M)=−\varphi(M^{-1}), and the gradient relationship \nabla\varphi(M)=M^{-1}(\nabla\psi(M^{-1}))M^{-1} is used in the monotonicity proof.","Under assumptions that \psi is isotonic and concave on S_{++}^d (with differentiability) and that \nabla_i f(w^k)>0 on the support of w^k, the paper proves f(w^{k+1})\ge f(w^k) for any \lambda\in(0,1], and feasibility M(w^{k+1})\succ0 is preserved. It gives sufficient conditions for strict monotonicity: for \lambda\in(0,1), if w^{k+1}\ne w^k (equivalently, not all supported gradient components are equal), then f(w^{k+1})>f(w^k); for \lambda=1, strict monotonicity additionally follows if \psi is strictly isotonic and strictly concave. Example 1 (A-optimal with n=d=2 and A_i=e_ie_i^T) shows cycling at \lambda=1 (swapping coordinates), one-step convergence at \lambda=1/2, and global linear convergence with contraction factor |1−2\lambda| for \lambda\in(0,1). Example 2 (c-criterion with n=d=3, c=(1,1,0)) shows MA can produce an iterate on the boundary where the objective becomes −\infty / nondifferentiable, preventing further progress, illustrating limitations when gradient positivity fails.","The paper notes that monotonicity does not imply MA can solve the OED problem when the objective criterion \varphi is nonconcave; concavity is still needed for convergence-to-optimum results (as in Yu’s theorem). It explicitly highlights that for criteria like the c-criterion, the condition \nabla_i f(w^k)>0 on the support may fail, causing MA to generate infeasible/nondifferentiable iterates and stall. It also acknowledges limitations of MA on certain optimality criteria and poses open problems to better understand MA behavior in such cases.","The analysis is primarily for approximate (continuous) designs on a finite design space; it does not address exact/integer run allocations or continuous design regions beyond discretization. Results rely on Fisher-information matrices and (implicitly) model correctness; robustness to model misspecification, heteroskedasticity, or dependence in responses is not studied. Practical stopping rules, numerical stability (e.g., near-singular M(w)), and computational complexity for large n,d are not developed, and no software implementation is provided. Empirical benchmarking against other modern OED solvers (e.g., FW variants, interior-point, coordinate-exchange) is not included, so practical performance implications beyond monotonicity are unclear.","The paper discusses extending monotone convergence theory to a generalized OED formulation for estimating parameter subsystems (feasibility cone F(K)) where optimal designs may lie on the boundary (some weights zero). It poses open questions about what happens when MA sets a positive weight to exactly zero: whether feasibility in F(K) is preserved, whether monotonicity still holds despite support changes, and whether an optimal solution exists with that coordinate zero (justifying “coordinate dropping”). It suggests that resolving these questions could yield a more general monotone convergence theory for MA beyond the ri(\Delta_n) setting and deepen understanding of MA under problematic criteria like the c-criterion.","Developing a principled variant of MA that handles support changes (zero weights) while guaranteeing feasibility and monotonicity for (OED1) could be pursued, e.g., via subgradient-based updates or safeguarded projections. Extending the monotonicity/strict monotonicity theory to settings with unknown parameters (Bayesian/robust designs) or to correlated/non-i.i.d. data (generalized information structures) would improve applicability. Providing practical implementations and numerical studies on large-scale designs (large n and d) would clarify when the theoretical advantages translate into computational benefits versus alternative first-order or second-order solvers. Finally, characterizing precisely which optimality criteria beyond Yu’s class admit MA monotonicity (or provable failure modes) would strengthen guidance on when MA should or should not be used.",2508.07074v1,https://arxiv.org/pdf/2508.07074v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:04:02Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design|Other,Parameter estimation|Optimization|Cost reduction|Other,Not applicable,1 factor (single unknown frequency parameter ω; control variable is evolution time t),Other|Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Personal website,https://doi.org/10.54499/UID/50014/2023,"The paper proposes WES (Window Expansion Strategy) and an annealed variant (aWES) for low-cost adaptive Bayesian experimental design in single-qubit frequency estimation, where the controllable design variable is the evolution time t and the parameter of interest is the Hamiltonian frequency ω. WES performs greedy (locally optimal) Bayesian design by sampling a fixed number of candidate times within an adaptively expanding time window and selecting the time that minimizes expected posterior variance; aWES instead targets a desired effective sample size (ESS) to improve robustness under multimodality and numerical instability. The methods are evaluated via numerical simulations (sequential Monte Carlo / MCMC resampling inference, randomized true frequencies, binomial measurement outcomes) under both noiseless dynamics and decoherence with finite coherence time (T=500). Results show WES achieves the fastest and most reliable learning, saturating the Heisenberg-limit scaling in the ideal case and outperforming common heuristics (σ heuristic and particle-guess heuristic), while aWES is competitive and can be better under decoherence. The key practical contribution is an adjustable trade-off between classical optimization cost and metrological performance through fixed-per-iteration candidate sampling and adaptive window expansion.","The experiment is modeled by a two-level Hamiltonian $H=\frac{\omega}{2}\sigma_i$, with binary outcome likelihood $P(x\mid \omega;t)=\big(\sin^2(\omega t/2)\big)^x\big(\cos^2(\omega t/2)\big)^{1-x}$ for $x\in\{0,1\}$ and controllable evolution time $t$. Bayesian updating uses $P(\omega\mid x;t)\propto L(\omega\mid x;t)P(\omega)$ with $L=P(x\mid\omega;t)$. WES selects $t$ by brute-force evaluating a utility over candidate times in a window $[t_{\min},t_{\max}]$ (variance-minimization for WES; ESS-to-50% targeting for aWES) and expands the window (doubling $t_{\max}$) after repeated selections near the top of the window.","Curve fits of RMSE vs cumulative evolution time (log-log) give Heisenberg-limit slope −1.00 for both WES (factor 1.78) and aWES (factor 2.66) under ideal conditions, while the σ heuristic achieves slope −0.86 and PGH −0.66; random times is −0.44. In the ideal case WES is reported as the most reliable and fastest-learning strategy across runs; aWES is close but somewhat less stable early on. Average experiments to achieve the plotted results: WES ≈ $3\times 10^2$, aWES ≈ $3\times 10^2$, SH ≈ $6\times 10^2$, PGH ≈ $7\times 10^1$, RTS ≈ $4\times 10^4$. Under decoherence (T=500), WES/aWES maintain the lowest errors, with aWES noted as slightly better than WES in the noisy case.","The authors note that common heuristics (σ heuristic/PGH) rely on normality assumptions that do not always hold and are ill-suited when coherence is finite because they push to ever-increasing evolution times. They also state their WES constants were chosen heuristically (though robust across tested instances), and that they use brute-force optimization for candidate-time selection for clarity and to avoid confounding effects of optimizer specifics. They acknowledge that aWES can be more irregular/unstable in early stages and that SH can exhibit problematic runs where learning stagnates.","The work is demonstrated primarily via simulations on a single-parameter single-qubit model with a known likelihood form and (in the noisy case) known coherence time, so robustness to model mismatch (unknown noise parameters, calibration errors, SPAM errors, drift) is unclear. Utility optimization is limited to random sampling of 50 candidate times per iteration within a 1D window, which may be less efficient than principled 1D global optimization or adaptive sampling, and performance may depend on these hyperparameters (warm-up shots, hit threshold, candidate count). No implementation details (language, runtime, reproducibility steps) are extractable here beyond a general statement that code/datasets exist, so practical deployment cost and numerical stability across platforms is uncertain.","The authors propose extending tests to more complicated likelihoods, including multi-parameter and/or multi-modal settings, where numerical representation becomes more challenging. They suggest investigating more sophisticated optimization methods beyond brute-force search, potentially applying WES-style dynamic search-range definition to general optimizers and comparing cost-to-performance versus static-range optimization. They also propose extending comparisons to machine-learning-based Bayesian experimental design methods used in quantum problems.","A natural extension is a self-calibrating design that jointly estimates ω and nuisance/noise parameters (e.g., coherence time, readout bias) with adaptive control selection robust to model mismatch and drift. Another direction is to provide a theoretical analysis of regret/optimality (or finite-sample bounds) for the window-expansion rule and ESS-targeting objective, including sensitivity to hyperparameters (candidate count, hit threshold, shot batching). Packaging a reproducible open-source reference implementation (with benchmarks and runtime/memory profiling) would improve adoption and allow fair comparison to alternative Bayesian design optimizers (e.g., Bayesian optimization over t).",2508.07120v1,https://arxiv.org/pdf/2508.07120v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:04:49Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,D-optimal|Other,Variable/General,Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper formulates optimal sensor placement for continuous-time stochastic filtering as an optimal experimental design (OED) problem where the design variable is a probability measure (sensor schedule) \(\xi\) over time, generalizing discrete sensor times (Dirac mixtures) to continuous sensor densities. It derives “scheduled filtering” equations (measure-weighted observation models) and corresponding Zakai/log-Zakai filtering dynamics in which \(\xi\) enters both drift and diffusion of the observation process. The main methodological contribution is an adjoint-based derivation of the Fr\u00e9chet derivative (gradient) of general OED utilities with respect to \(\xi\), yielding a backwards-in-time adjoint differential equation analogous in structure/cost to solving the Zakai equation. In the linear-Gaussian special case, the approach reduces to weighted Kalman\u2013Bucy equations and an explicit gradient formula for utilities depending on the covariance (Riccati) trajectory. Numerical experiments on a logistic-growth parameter example and a 2D linear-Gaussian example demonstrate that gradient-based optimization concentrates sensor mass at informative times and improves information/uncertainty criteria compared with uniform schedules.","Design variable is a probability measure \(\xi\) on \([0,T]\), with discrete placement as \(\xi=\sum_i \alpha_i\,\delta_{t_i}\). The scheduled observation model is \(dZ(t)=g(X(t))\,d\xi(t)+d\Delta_\xi(t)\); if \(\xi\) has density \(\xi(t)\), then \(dZ(t)=g(X(t))\,\xi(t)\,dt+\sqrt{\xi(t)}\,\Gamma^{1/2}(t)\,dW(t)\). The log-Zakai equation becomes \(d\log p_t(x)=\frac{L^\*p_t(x)}{p_t(x)}dt-\tfrac12\|g(x)\|^2_{\Gamma^{-1}(t)}\,d\xi(t)+g(x)^\top\Gamma^{-1}(t)\,dZ(t)\), and the OED objective is \(\min_{\xi\in\mathcal P}\,\mathbb E\,U(p)\) (e.g., based on KL gain or posterior variance/trace).","For the logistic-growth guiding example, projected gradient descent starting from \(\xi_0=\mathrm{Unif}[0,6]\) drives \(\xi\) to peak near \(t\approx 3\), consistent with the single-point D-optimal timing \(\tau=\log(z_0^{-1}-1)/x_0\). In the linear-Gaussian 2D example with time-varying observation operator, optimizing \(\int_0^6\mathrm{trace}(C(t))dt\) yields sensor schedules concentrated just after \(t=0\) and just after \(t=3\), reducing integrated filtering uncertainty relative to a uniform schedule. The paper also provides an explicit gradient expression in the linear-Gaussian case: \(\delta_\xi V(t)=-\langle\Lambda(t),\,C(t)A(t)C(t)\rangle\) with an adjoint matrix ODE for \(\Lambda\), enabling efficient gradient-based optimization. Quantitative ARL-style metrics are not applicable; performance is shown via posterior contraction/trace plots and utility decrease over iterations.","The authors state that most calculations are ""mostly formal"" and omit rigorous justification (e.g., well-posedness of the adjoint equation). They also note that examples are kept very low-dimensional because solving the (log-)Zakai equation becomes unfeasible in high dimensions, limiting immediate practical applicability without alternative filtering approximations.","The proposed optimization operates over sensor schedules \(\xi\) (often assumed to have a Lebesgue density) and then uses projected gradient steps; the practical impact of discretization/projection choices and constraint handling (nonnegativity/normalization) on convergence and optimality is not systematically analyzed. Utility gradients in the nonlinear case require Monte Carlo over signal/observation paths plus solving forward filtering and backward adjoint PDEs, which may be computationally prohibitive and sensitive to numerical error; the paper provides limited discussion of computational scaling, variance reduction, or robustness of the stochastic gradient estimator. Comparisons against established discrete sensor placement heuristics (greedy, convex relaxations, particle-based adaptive design) are discussed conceptually but not benchmarked empirically on shared tasks.","They suggest exploring optimization methods better suited to stochastic objectives than simple projected gradient descent, explicitly mentioning SPSA. They propose developing gradient flow methods on probability distributions (e.g., Wasserstein or Rao-Fisher flows) and extending the adjoint-gradient theory to practical filters such as particle filters rather than solving Zakai/Kushner\u2013Stratonovich PDEs. They also highlight open questions about translating optimal smooth schedules to optimal discrete observation times (e.g., via sampling), extending to adaptive/on-the-fly sensor placement, and generalizing the theory from temporal scheduling to spatial sensor placement problems.","A valuable extension would be to provide provable convergence guarantees (or at least empirical diagnostics) for the projected gradient procedure under measure constraints, including sensitivity to the chosen projection/operator and discretization of \([0,T]\). Another direction is to integrate variance-reduction and unbiased/low-bias gradient estimators (e.g., control variates, reparameterization where possible) to make the nonlinear Monte Carlo gradient practical. Broader validation on benchmark problems (including discrete-time sensor selection baselines and particle-filter implementations) and guidance on how to select/regularize \(\xi\) to avoid overly spiky designs under model misspecification would improve adoption.",2508.12288v1,https://arxiv.org/pdf/2508.12288v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:05:37Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Model discrimination|Prediction|Other,Other,"Variable/General (sequential queries/questions; e.g., up to 20 turns in 20 Questions; 5 turns in preference elicitation)",Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes BED-LLM, a general-purpose framework that uses sequential Bayesian experimental design to make LLMs ask adaptive, information-seeking questions in multi-turn interactions. At each turn it generates candidate multiple-choice questions, estimates each candidate’s expected information gain (EIG) about a latent target $\theta$ using an LLM-derived probabilistic model, selects the question with highest estimated EIG, and updates beliefs via a filtered hypothesis distribution to enforce consistency with prior answers. The method emphasizes a prior–likelihood joint factorization $p(\theta)p_{\mathrm{LLM}}(y\mid \theta,x)$ and avoids deterministic-likelihood simplifications that reduce EIG to marginal predictive entropy. Empirically, it improves success rates in 20 Questions across multiple LLM backbones and target categories and improves movie-preference elicitation quality (via an LLM-as-judge rating protocol), including under questioner–answerer model mismatch. Ablations show that explicit EIG maximization and rejection/filtering-based belief updates are key contributors, and that prior–likelihood factorization can outperform a data–estimation alternative when $\theta$ is more complex than the response space.","The design criterion is the expected information gain (EIG), expressed as $\mathrm{EIG}_\theta(x)=H[p(\theta)]-\mathbb{E}_{p(y;x)}[H(p(\theta\mid y;x))]$ and equivalently $\mathrm{EIG}_\theta(x)=H[p(y;x)]-\mathbb{E}_{p(\theta)}[H(p(y\mid\theta;x))]$. For candidate question $x_t$, they use a Rao–Blackwellized estimator: $\widehat{\mathrm{EIG}}(x_t)\approx \frac{1}{N}\sum_{n=1}^N\sum_{y\in\mathcal{Y}} p_{\mathrm{LLM}}(y\mid \theta^{(n)},x_t)\log p_{\mathrm{LLM}}(y\mid \theta^{(n)},x_t) - \sum_{y\in\mathcal{Y}}\hat p(y\mid h_{t-1},x_t)\log \hat p(y\mid h_{t-1},x_t)$, where $\hat p(y\mid h_{t-1},x_t)=\frac{1}{N}\sum_{n=1}^N p_{\mathrm{LLM}}(y\mid \theta^{(n)},x_t)$ and $\theta^{(n)}\sim p_f(\theta;h_{t-1})$. The sequential joint model used in BED-LLM is $p(\theta,y_{t+1};h_t,x_{t+1})=p_f(\theta;h_t)\,p_{\mathrm{LLM}}(y_{t+1}\mid \theta,x_{t+1})$, with $p_f$ constructed by sampling from in-context beliefs then filtering/rejecting hypotheses inconsistent with prior Q/A pairs.","In 20 Questions (100 targets per dataset), BED-LLM achieves higher end-of-game success than Naive and Split across Animals/Celebrities/Things and multiple LLMs; e.g., on Celebrities with Llama-3.1-8B it reaches 58% vs 10% (Naive) and 35% (Split), and the abstract reports a 5.8× gain in success rate for celebrity guessing with Llama-3.1-8B. On Animals, Qwen2.5-72B reaches 95% with BED-LLM vs 45% (Naive) and 87% (Split); GPT-4o reaches 93% vs 45% and 83%. Ablations indicate that replacing EIG with predictive-entropy objectives reduces performance, and using simple in-context belief updates (ICL Beliefs) causes large drops (e.g., Animals with GPT-4o-mini: 18% vs 88%). In preference elicitation (200 users; 5 turns), BED-LLM yields higher mean recommendation ratings than Naive and an entropy-based baseline, with the largest gains under questioner–answerer model mismatch.",None stated.,"The approach relies on an LLM-provided probabilistic response model (logits/logprobs) over a constrained answer space (multiple-choice), which may limit applicability to free-form responses and providers without stable probability access. The belief update $p_f(\theta;h)$ uses heuristic sampling, rejection thresholds, deduplication, and a uniform reweighting over retained hypotheses; performance may be sensitive to these hyperparameters and to hypothesis-generation mode collapse. Empirical evaluation is largely LLM-simulated (LLM answerers and LLM-as-judge), so real-user behavior, calibration, and conversational pragmatics (e.g., ambiguity, non-cooperation) are not fully validated. Computational cost (multiple candidate questions, multiple hypotheses, repeated likelihood calls per turn) may be substantial in real deployments and is not summarized as end-to-end latency/cost.",None stated.,"Extend BED-LLM beyond multiple-choice to richer response spaces (free-text) using structured latent response variables, constrained decoding, or learned compressions to keep entropy estimates meaningful. Develop self-starting/robust variants that handle unknown or shifting answerer models (real users), including miscalibration, adversarial/strategic responses, and conversational noise. Provide principled alternatives to heuristic filtering (e.g., approximate Bayesian inference, particle filters, learned proposal distributions) and study sensitivity to rejection thresholds and diversity prompts. Release reference implementations and benchmarks with real user studies and standardized cost/latency reporting to assess practical deployment tradeoffs.",2508.21184v2,https://arxiv.org/pdf/2508.21184v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:06:28Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Cost reduction|Other,Other,Variable/General (controls are measurement times/inter-pulse delays τ; target parameters are hyperfine couplings for n spins; n sampled 1–50 in training),Semiconductor/electronics|Theoretical/simulation only|Other,Simulation study|Case study (real dataset),TRUE,Other,Not provided,NA,"The paper proposes an offline quantum experimental design method to reduce measurement time for nuclear-spin detection/characterization with NV centers in diamond. It introduces surrogate information gain (SIG), a computationally tractable variance-based proxy for Bayesian expected information gain, used to pre-select the most informative measurement time points (τ) before data acquisition. The selected subset of data points is used to train and deploy a deep-learning signal-to-image model (SALI) for identifying nuclear spins and estimating hyperfine coupling components (A_z, A_⊥). In the high-field regime (validated on experimental data from an NV coupled to 13C nuclei), SIG-based point selection plus reduced repetitions yields up to an 85% reduction in measurement time (from 11 hours to 1.6 hours) with modest performance degradation. In the low-field regime (simulation-based), SIG combined with higher temporal resolution is predicted to reduce total time by ~60% (8 hours to 3.2 hours) while maintaining performance comparable to a full-measurement reference.","Bayesian expected information gain is defined as $\mathrm{EIG}(\tau)=\mathbb{E}_{\pm x}\big[S(p(\vec A)) - S(p(\vec A\mid \pm x))\big]$. The proposed surrogate information gain is $\mathrm{SIG}(\tau)=\mathbb{E}\{\mathrm{Var}_{\vec A\sim p(\vec A)}[P_{\pm x}(\tau\mid \vec A)]\}$, which simplifies for binary outcomes to $\mathrm{SIG}(\tau)=\mathrm{Var}_{\vec A\sim p(\vec A)}[P_x(\tau\mid \vec A)]$. The design uses SIG to select the top $N_p$ time points (τ values) with highest variance across prior-sampled synthetic signals.","High-field experimental validation (B_z = 404 G): selecting $N_p=4000$ τ-points per signal (for N=32 and N=256 pulse sequences) computed from 50,000 synthetic samples reduces measurement time by ~50% (8 hours to 4 hours) while detecting 27 spins with 25 matching prior reference predictions (vs 29–30 spins with 27 matches in full-data settings). Further reducing repetitions per point from $N_m=250$ to $N_m=100$ decreases total time to ~1.6 hours and is reported as an 85% reduction versus an 11-hour 4-input baseline, with only slight performance degradation. Low-field (B_z = 40.4 G, simulations): SIG-based selection with $N_p=8000$ points over $\tau\in[1,50]\,\mu s$ at 1 ns resolution yields comparable/better performance than an 8-hour reference while enabling ~3.2-hour acquisition at $N_m=100$ (60% time reduction).","The authors note that the approach is non-adaptive (offline) and therefore does not leverage potential additional time savings from real-time adaptive experimental design. They also state that low-field regime results are demonstrated only on simulated data (not experimental), and characterization remains less effective at low magnetic field than at high field.","SIG is a heuristic proxy for information gain; maximizing signal variance under the prior may not always align with minimizing posterior uncertainty or improving spin-identification accuracy, especially under model mismatch or non-Gaussian/heteroscedastic noise. Performance depends on having an accurate physics-based simulator and a representative prior over (n, A_z, A_⊥); prior misspecification could lead to suboptimal point selection and biased detection. The method is validated experimentally only on a specific NV/13C dataset (high-field) and may not generalize without retuning to other sensors, pulse sequences, drift/instability, or correlated/colored noise in real experiments.","They explicitly pose as an open question whether acquisition time can be further reduced using real-time adaptive techniques, and suggest investigating whether SIG or other figures of merit/approaches can enable further reduction of acquisition and processing time. They also point to variational Bayesian inference as promising for adaptive, real-time selection on minute timescales.","Develop a self-starting/adaptive version that updates the prior/posterior online and recomputes SIG (or an approximate EIG) under experimental constraints, including overhead costs for switching τ. Study robustness to model mismatch (imperfect decoherence model, pulse errors, drift) and to correlated noise, and include calibration/diagnostic tools to detect when SIG-selected regions are no longer informative. Provide open-source implementations and benchmarking across multiple NV datasets and alternative sensing protocols (e.g., different dynamical decoupling families) to establish generality and best-practice parameter choices (N_p, τ grid, N_m).",2508.21450v1,https://arxiv.org/pdf/2508.21450v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:07:09Z
TRUE,Optimal design|Sequential/adaptive|Other,Model discrimination|Parameter estimation|Cost reduction|Other,Not applicable,Variable/General (n nodes/variables in a causal graph; intervention targets are subsets of nodes; also considers bounded intervention size M),Theoretical/simulation only|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper studies experimental design for causal structure learning when the true causal graph is a directed mixed graph (DMG) that may contain both directed cycles and bidirected edges from latent confounders. It derives worst-case lower bounds on (i) the maximum number of variables that must be intervened on in a single experiment and (ii) the total number of experiments needed to identify all directed edges and all non-adjacent bidirected edges, under two regimes: d-separation (Scenario 1) and σ-separation (Scenario 2). It proposes a multi-stage, largely non-adaptive intervention-design framework that combines conditional-independence (CI) tests with do-see tests to recover all directed edges and all non-adjacent bidirected edges, and to recover single-adjacent bidirected edges (but not double-adjacent bidirected edges). It also provides bounded-intervention versions of the separating-system constructions (with per-experiment size limited by M) and shows the proposed procedures are tight to the lower bounds up to logarithmic factors. The work is primarily theoretical, providing constructive separating-system-based algorithms and complexity/lower-bound results for intervention planning in cyclic, confounded causal graphs.","Interventions are designed as separating systems over node subsets. A key construction is the SCC-Anc separating system: for SCC-Anc partition $T^G=\{T^G_1,\dots,T^G_{l+1}\}$, for each level $k$ and index $i$, define $I_{k,i}=T^G_k\cup(\cup_{j: i\le m_{k,j}} S_{k,j}\setminus\{X^i_{k,j}\})$ (Eq. 6), ensuring for each node $X\in S_{k,j}$ there exists an intervention with $T^G_k\cup(S_{k,j}\setminus\{X\})\subseteq I$ and $X\notin I$. Parent/edge identification is based on interventional dependence tests such as $Pa_G(X)=\{Y:(X\not\perp\!\!\!\perp Y)_{P^{do(I)}}\}$ under appropriate $I$, and non-adjacent bidirected edges via $(X\not\perp\!\!\!\perp Y\mid Pa_G\{X,Y\})_{P^{do(I)}}$ when $Pa_G(\{X,Y\})\subseteq I$ and $X,Y\notin I$.","Lower bounds are given for identifying directed edges: if $\max_{I\in\mathcal I}|I|<|T^G_{l+1}|_n+\zeta^{l+1,G}_{\max}-1$ then the directed part is not identifiable (Theorem 17), and if $|\mathcal I|<\sum_{k=1}^{l+1}\zeta^{k,G}_{\max}$ then the directed part is not identifiable (Theorem 19). For non-adjacent bidirected edges, if $\max_{I\in\mathcal I}|I|<\max_{[X,Y]\in B_N}|Pa_G(X\cup Y)|$ then they are not identifiable (Theorem 23), and if $|\mathcal I|<cc(G^{uc})$ then they are not identifiable (Theorem 27). The proposed unbounded algorithm identifies the directed part using at most $2\lceil\log_2(\chi(G^{obs}_r))\rceil+\sum_{k=1}^{l+1}\zeta^{k,G}_{\max}$ experiments (Corollary 47), and (ignoring NP-hard subroutines) recovers $RD(G)$ with an added $cc(G^{uc})+2\chi_s(G^u)$ term (Corollary 60); practical randomized/greedy substitutes yield polylog/poly-degree bounds with high probability. The method cannot identify double-adjacent bidirected edges (those with both $(X,Y)$ and $(Y,X)$ directed edges) even with do-see tests, which the authors treat as a remaining gap.","The authors state that adjacent bidirected edges cannot be identified through CI/r-separation relations alone, so lower bounds for that case are not well defined and are left unaddressed. They also state their approach cannot recover double-adjacent bidirected edges (bidirected edges between nodes connected by directed edges in both directions), because even do-see tests become ineffective in that case. They note NP-hard subproblems (minimum vertex coloring, minimum edge clique cover, minimum strong edge coloring) and recommend approximate randomized/greedy constructions for practical use.","The paper is largely theoretical and does not provide an empirical/simulation benchmark demonstrating finite-sample performance of CI and do-see testing under noise, limited samples, or model misspecification; results assume access to correct CI/do-see decisions consistent with d/σ-faithfulness. The framework focuses on hard, full-support interventions and does not analyze practical constraints such as imperfect interventions, measurement error, selection bias, or costs that vary by variable beyond the bounded-size M constraint. It targets exact edge identifiability (except double-adjacent bidirected edges) but does not quantify robustness when assumptions (e.g., faithfulness, independence of exogenous variables, or stability under intervention) are violated, nor does it provide implementation guidance/software for large graphs where test multiplicity and complexity could be substantial.","The authors explicitly propose extending the framework to soft interventions (modifying conditional distributions rather than severing parent links). They also suggest developing adaptive experimental design strategies where interventions depend on previous outcomes, rather than being predetermined. They call for instance-specific (graph/MEC-dependent) lower bounds beyond worst-case results, and for principled methods to identify double-adjacent bidirected edges, which remain unresolved.","A natural extension is to develop finite-sample theory and practical testing procedures (including multiple-testing control) for CI and do-see tests in cyclic/confounded settings, and to study how sampling variability affects the separating-system guarantees. Another direction is integrating cost-sensitive or constrained intervention planning (heterogeneous costs, feasibility constraints, partial observability) and providing approximation guarantees when NP-hard subroutines are replaced by heuristics. Extending the approach to autocorrelated/time-series data (common in feedback systems) and to nonlinear/nonparametric SCMs with weaker faithfulness conditions would improve practical applicability, as would releasing reference implementations and benchmark suites for DMG intervention design.",2509.01887v1,https://arxiv.org/pdf/2509.01887v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:07:59Z
TRUE,Sequential/adaptive|Other,Screening|Optimization|Cost reduction|Other,Not applicable,"Variable/General (participant covariates X ∈ R^d; examples include d=2 synthetic, d=9 PRO-ACT, d=25 IHDP)",Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes MPED-RobustCAL, a sequential matched-pair experimental design that actively enrolls participants from regions with high treatment effects when the average population effect is small. It reformulates identification of the high-effect region Ω_γ as a binary classification (active learning) task, querying labels only in the disagreement region DIS(C) and positive region POS(C), while simultaneously running a statistically valid sequential two-sample test to decide H0 vs H1. The authors provide label-complexity analysis showing the learned enrollment region Ω̂_γ encloses Ω_γ and converges to it more sample-efficiently than passive learning, with guarantees tied to VC dimension and disagreement coefficient. Empirically, simulations on synthetic data and experiments on PRO-ACT (ALS/Riluzole) and IHDP demonstrate higher testing power than conventional MPED and higher true-positive-rate (coverage of responders) than prior active enrollment baselines. The design targets cost-efficient detection of treatment efficacy under a hard label/experimental budget while reducing risk of missing true responder subpopulations.","Outcome model: Y_A(X)=A\Delta(X)+f(X)+E with Gaussian noise (Eq. 1). Two-sample hypotheses compare treatment vs control response distributions: H0: p_{Y|A}(y|0)=p_{Y|A}(y|1) vs H1: not equal (Eq. 2). Active-learning label for a covariate point uses within-pair outcome difference exceeding a threshold: \tilde Z=1 if Y_1(\tilde X)-Y_0(\tilde X')\ge \gamma (or swapped by assignment), else 0; Bayes classifier q*(x)=1{P(\tilde Z=1|X=x)\ge 0.5} and Ω_γ={x: q*(x)=1}. MPED-RobustCAL enrolls from Ω̂_γ=DIS(C)\cup POS(C) and updates C using empirical risk filtering; theoretical bounds give label complexity and enclosure ratio \mathcal R=|Ω̂_γ|/|Ω_γ|\le 1+\theta_{q*}(0)\epsilon/P(Ω_γ) (Theorem 4.5).","On synthetic data, testing power improves substantially over conventional MPED: e.g., at label budget B=400 power increases from 0.15 (conventional) to 0.61 (MPED-RobustCAL), and at B=700 from 0.22 to 0.85 (100 runs). Type I error under H0 is controlled below \alpha=0.05 across budgets (e.g., 0.038–0.046 for B=200–700). On PRO-ACT, power improves at all tested budgets (e.g., B=400: 0.67 conventional vs 0.79 MPED-RobustCAL). Across synthetic/IHDP/PRO-ACT, MPED-RobustCAL achieves higher true positive rate (TPR) for capturing high-effect regions than regression-based adaptive enrichment and \tau-BALD baselines, supporting its “enclosing” property.",None stated.,"The approach assumes high-quality matching and unconfoundedness within matched pairs (Assumption 3.2) and relies on a user-chosen clinically meaningful threshold \gamma; misspecification of matching quality or \gamma could degrade both region identification and power. Statistical validity is guaranteed for Type I error, but power and region-coverage depend on model/classifier choices, committee construction, and noise conditions (e.g., bounded/Massart noise, Bayes classifier in hypothesis class), which may not hold in practice. Implementation details (classifiers, sequential test, matching tolerance) can materially affect results, yet no released code is provided, limiting reproducibility.",None stated.,"Extend MPED-RobustCAL to settings with autocorrelated outcomes, non-Gaussian/heavy-tailed noise, or interference between units, and study robustness when matching is imperfect or costly. Develop guidance for selecting \gamma and adapting it online, and provide self-starting/unknown-variance versions of the sequential test. Broaden evaluation to additional real-world trials and heterogeneous treatment-effect benchmarks, and release an open-source implementation with standardized baselines to support reproducibility and adoption.",2509.10742v2,https://arxiv.org/pdf/2509.10742v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:08:39Z
TRUE,Optimal design|Computer experiment,Parameter estimation|Prediction|Other,A-optimal|D-optimal,Variable/General (examples include M=2 and M=25 parameters),Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/leyffer/m2dtIceSheet,"The paper formulates an optimal experimental design (OED) problem for a moving sensor whose continuous-time path is optimized to minimize posterior uncertainty in a linear Bayesian inverse problem constrained by a linear PDE. For linear models with additive Gaussian noise, it derives a closed-form posterior covariance as a function of the sensor path via the Fisher information, and considers standard A- and D-optimal criteria (trace and determinant of the posterior covariance). The sensor path is parameterized by an ODE-driven control model (e.g., unicycle kinematics) with obstacle-avoidance and feasibility constraints, yielding a constrained nonlinear program solved with an interior-point method (IPOPT) using first-order information and quasi-Newton Hessian approximations. A discretization is proposed that remains consistent under time refinement, enabling warm-starting from coarse to fine temporal grids and showing convergence behavior as the path time step decreases. Computational demonstrations use a convection–diffusion PDE for pollutant transport (including a higher-dimensional parameterized initial-condition case), showing that optimized trajectories increase informativeness by steering the sensor through regions that best distinguish parameter effects.","The posterior covariance depends on the path through $\Sigma_{\text{post}}(p)=\big(G(p)^*C_{\text{noise}}^{-1}G(p)+\Sigma_{\text{pr}}^{-1}\big)^{-1}$, where $G(m;p)$ maps parameters to continuous-time measurements along the trajectory. The Fisher information is $M_{\text{Fisher}}(p)=G(p)^*C_{\text{noise}}^{-1}G(p)$, with entries $\langle C_{\text{noise}}^{-1/2}g_i(p),C_{\text{noise}}^{-1/2}g_j(p)\rangle_{L^2([0,T])}$. The OED objective minimizes either A-optimality $\Psi_A=\mathrm{tr}(\Sigma_{\text{post}})$ or D-optimality $\Psi_D=\det(\Sigma_{\text{post}})$ subject to ODE path dynamics and obstacle constraints; discrete gradients are given (e.g., Eqns. (32)–(33)) in terms of $dG/dp$ and traces involving $\Sigma_{\text{post}}$.","In the 2-parameter pollutant-source example, IPOPT optimization from two of three initial guesses substantially reduced posterior uncertainty: A-OED decreased from 0.791 to 0.493 (×0.624) and from 1.149 to 0.729 (×0.635), while D-OED decreased from 0.073 to 0.038 (×0.527) and from 0.166 to 0.089 (×0.534). A third initialization primarily reduced the regularization term rather than uncertainty (A-OED increased slightly from 1.090 to 1.119). Under temporal refinement, the paper reports convergence of the discretized objective/gradient with increasing compute time, and demonstrates warm-starting: repeated halving of $\Delta t$ reduced the required IPOPT iterations dramatically (down to 1 iteration at $\Delta t\approx1.5\times10^{-4}$) while the objective changed by only ~3.4e-6. For a reduced 25-dimensional parameterization of an initial-condition field, warm-starting from a previously optimized path reduced A-OED from 6.359e-3 to 5.461e-3 (×0.859).","The authors restrict the main methodological exposition to linear PDEs/linear parameter-to-observable maps (noting nonlinear PDEs would require a Laplace approximation and more general utility functions such as expected information gain). They also note that extending efficiently and scalably to infinite-dimensional parameter fields is deferred, and their high-dimensional example uses a parameter-reduced posterior approximation rather than a full infinite-dimensional treatment. They acknowledge computational expense in assembling measurements/integrals and motivate approximations (e.g., pointwise measurements) and warm-starting to manage cost.","Results are demonstrated on a single main PDE example (convection–diffusion) with specific noise/prior choices; broader benchmarking across diverse PDE types, noise correlations, and sensor models is limited. The optimization is nonconvex and solved to local optimality with IPOPT; there is no systematic study of global optimality or sensitivity to initialization beyond a few local minima. Practical issues like model mismatch, discrete/irregular sampling, actuation uncertainty beyond the measurement kernel, and robust design under uncertainty in dynamics/obstacles are not deeply evaluated. Software/implementation details (e.g., reproducibility of all experiments, exact solver settings, and computational environment) are not fully characterized in the text excerpt beyond using IPOPT via Python and providing a repository link.","The authors state plans to extend from a single moving sensor to multiple sensors and to stationary-sensor placement in a continuous domain. They plan to consider more general OED utilities (e.g., expected information gain) to handle nonlinear parameter-to-observable maps. They also plan to expand the parameter-reduced posterior approach toward infinite-dimensional parameter spaces and to reduce computational cost using strategic model reduction and multifidelity methods. They further intend to improve robustness to aleatoric uncertainties and develop cheaper approaches for generating initial guesses.","Developing self-starting or online/sequential (adaptive) trajectory optimization that updates the planned path as data arrives would improve practicality for time-critical deployments. Adding explicit robustness objectives/constraints (e.g., worst-case or risk-sensitive criteria) for model mismatch, navigation errors, and uncertain obstacle maps could make the method safer and more deployable. Providing open-source reference implementations for the full pipeline (including PDE solves, gradient verification, and IPOPT interfaces) and standardized benchmarks against alternative informative path planning methods would strengthen reproducibility and comparative validation. Extending to multi-objective designs (e.g., combine A-/D-optimality with energy/time costs or detection thresholds) and to constrained sensing modalities (limited field-of-view, intermittent sampling) would broaden applicability.",2509.15961v1,https://arxiv.org/pdf/2509.15961v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:09:26Z
TRUE,Sequential/adaptive|Bayesian design,Prediction|Cost reduction|Other,Other,Variable/General (examples include 1 design variable in linear-Gaussian; 2D sensor displacement in source detection),Environmental monitoring|Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper develops a Bayesian optimal stopping framework for sequential Bayesian experimental design where the number of experiments need not be fixed in advance. The authors formulate joint design-and-stop decisions as a finite-horizon Markov decision process with rewards based on information gain (KL divergence between posteriors) plus experimental costs, and prove an optimal stopping rule: stop when terminal reward exceeds expected continuation value. To learn policies in settings without tractable dynamic programming, they propose an actor-critic deterministic policy-gradient method that learns a design (experiment-selection) policy and a Q-function for continuation value. They identify a circular dependency between the stopping rule and learned continuation value that can cause premature stopping and unstable learning, and resolve it with a curriculum learning schedule that gradually increases the probability of applying the stopping rule. Numerical studies on a linear-Gaussian benchmark (with analytic solution) and a contaminant source-detection/sensor-movement problem demonstrate improved convergence stability and higher achieved utility versus vanilla joint training and simple threshold stopping, especially under negative per-experiment costs and strong sequential dependence.","Observations follow $y_k = G_k(\theta,\xi_k; I_k)+\epsilon_k$ with Bayesian update $p(\theta\mid y_k,\xi_k,I_k) \propto p(y_k\mid \theta,\xi_k,I_k)\,p(\theta\mid I_k)$. Rewards use KL information gain and cost, e.g. incremental reward $r_k = D_{KL}(p(\theta\mid I_{k+1})\|p(\theta\mid I_k)) + c_k(\xi_k)$ when continuing. The optimal stopping set is $T_k=\{s_k: r_T^S(s_k)\ge \mathbb{E}[r_k^C + V_{k+1}(F_k(s_k,\mu_k(s_k),y_k))]\}=\{s_k: r_T^S(s_k)\ge Q_k(s_k,\mu_k(s_k))\}$, i.e., stop when terminal reward dominates continuation value.","Theoretical results include: (i) an optimal stopping characterization (stop iff terminal reward exceeds expected continuation value), and (ii) equivalence of terminal vs incremental reward formulations. Empirically, on the linear-Gaussian benchmark the learned policy recovers analytic behavior (e.g., with zero cost it learns to stop at the horizon; with negative costs it learns optimal early stopping), while naive threshold rules can stop prematurely. In the convection–diffusion contaminant source-detection problem, vanilla policy-gradient training can converge to overly-early stopping under negative costs, whereas curriculum learning sustains exploration longer, yields more stable convergence, and achieves higher rewards with more appropriate stopping stages across tested cost settings.","The authors note that evaluating terminal rewards at every decision stage can be expensive when posterior updates are costly or when reward structures are high-dimensional. They also state that relying on the optimal-stopping theorem to define the stopping policy induces circular dependencies in training, motivating exploration of alternative formulations that directly parameterize the stopping policy (potentially trading off optimality or convergence speed).","The empirical evaluation appears limited to simulated/numerical examples; broader validation on real experimental campaigns or standardized benchmarks would strengthen practical credibility. The approach relies on accurate approximation of posteriors/ KL rewards (including grid discretization for KL in nonconjugate cases), which may not scale well with higher-dimensional parameters or complex models and can introduce bias/variance in learned policies. The method’s performance may be sensitive to neural network architecture choices, exploration noise, and curriculum schedule design, but systematic sensitivity/robustness analyses are not clearly established from the provided excerpt.","The authors suggest investigating alternative formulations that directly parameterize the stopping policy (instead of deriving it via Theorem 1), which may avoid circular dependencies and simplify training even if they converge more slowly or sacrifice theoretical optimality. They also implicitly motivate work on reducing the computational burden of evaluating terminal rewards repeatedly when posterior updates or reward calculations are expensive.","Extending the framework to settings with unknown/learned noise models, model misspecification, or temporally correlated observations (common in sequential sensing) would improve robustness. Developing scalable KL/information-gain estimators (e.g., variational or particle-based methods) and providing software implementations would help adoption in higher-dimensional Bayesian inverse problems. Additional work could study regret/sample-complexity guarantees for the learned stopping/design policies and compare against stronger baselines (e.g., dynamic programming approximations, Bayes-optimal bandit-style methods, or alternative RL algorithms) on a wider suite of sequential design tasks.",2509.21734v1,https://arxiv.org/pdf/2509.21734v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:10:17Z
TRUE,Other|Sequential/adaptive,Parameter estimation|Prediction|Cost reduction|Other,Not applicable,Variable/General (high-dimensional covariates; stratification via matched pairs/blocks; treatment assignment probability p typically 0.5 but can vary),Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes “Generative Stratification,” a pre-experiment stratification/blocking procedure for randomized controlled trials that uses an LLM to synthesize high-dimensional covariates (including text) into a one-dimensional prognostic score for forming matched pairs (or other strata) before randomization. The method approximates the Bai (2022) optimal stratification score $g^*(X)=\mathbb{E}[Y(1)+Y(0)\mid X]$ by prompting an LLM with unit covariates and experimental context to predict potential outcomes under treatment and control, then constructing $\hat g_{LLM}(X)$ and sorting units to create strata. Randomization occurs within strata, preserving unbiasedness while aiming to reduce the variance of the difference-in-means estimator. Performance is evaluated via a simulation study using four published experimental datasets (subsampling ~1,000 observations; 3,000 replications) and shows variance/MSE reductions relative to simple randomization and commonly used low-dimensional stratification rules. The paper also proposes a hybrid pairing distance that blends Mahalanobis matching on observed covariates with proximity in the LLM score, yielding additional efficiency gains over Mahalanobis-only pairing and over post-experiment regression adjustment in their simulations.","The theoretical target for optimal stratification is $g^*(X)=\mathbb{E}[Y(1)+Y(0)\mid X]$. The LLM-based proxy score is constructed from LLM-predicted potential outcomes as $\hat g_{LLM}(X_i)=\hat Y_i(1)+\hat Y_i(0)$ for $p=0.5$, or $\hat g_{LLM}(X_i)=\hat Y_i(1)/p+\hat Y_i(0)/(1-p)$ for unequal assignment. For combining with Mahalanobis matching, the paper defines a hybrid pairing cost $c_\lambda(i,j)=\lambda\,(\hat g_i-\hat g_j)^2/\hat s_g^2+(1-\lambda)(X_i-X_j)^\top \hat\Sigma_X^{-1}(X_i-X_j)$, with $\lambda\in[0,1]$.","Across four empirical applications, matched-pair stratification using the LLM score reduced the MSE of the difference-in-means estimator by about 10%–50% relative to simple randomization (e.g., ~34% for Abel 2020; ~52% for Gerber 2020). Standard errors under generative stratification were on average 15.3% lower than simple randomization and 12.6% lower than the original papers’ stratification rules. A hybrid design that combines the LLM score with Mahalanobis matching further improved MSE versus Mahalanobis-only pairing by about 2%–13% (and about ~50% versus regression adjustment in their reported comparisons). One case (Barrera-Osorio 2019) showed essentially no improvement over the original stratification when the original stratifier (grade) was already highly predictive.","The authors note that their empirical applications are constrained by the limited pre-treatment information available in existing experimental datasets. They suggest the approach could yield larger gains when richer unstructured pre-treatment data (e.g., text responses, interview transcripts, detailed behavioral histories) are available, and as LLM predictive capabilities improve.","The evaluation relies on simulated “ground truth” potential outcomes created via an imputation/modeling step using observed experimental data; gains may be sensitive to the imputation model and may not translate directly to new experiments with different outcome-generating processes. The method’s practical performance may depend on prompt engineering, stability across model versions, and access/cost constraints of proprietary LLMs; robustness to distribution shift and to adversarial or noisy covariate text is not established. No released implementation details (code, exact prompts per dataset, data pipelines) are provided in the excerpt, which limits reproducibility and makes it difficult to assess computational burden at larger scales or under stricter privacy constraints.","The authors suggest that the value of Generative Stratification may increase with access to richer, unstructured pre-treatment information and with continued improvements in LLM capability at predicting individual-level heterogeneity. They express hope that ongoing progress in LLM models and training data will amplify the efficiency gains achievable with their approach.","Develop and test self-contained, open-source implementations (including standardized prompt templates, caching, and audit logs) and evaluate sensitivity to model choice, temperature, and prompt variations. Extend the framework to clustered/stratified multi-arm trials, unequal allocation with finite-sample guarantees, and settings with interference or covariate-adaptive randomization constraints. Provide theoretical and empirical robustness analyses under covariate shift, measurement error in covariates/text, and privacy-preserving prompting (e.g., redaction or synthetic covariates), alongside real-world prospective validations where stratification is implemented before observing outcomes.",2509.25709v1,https://arxiv.org/pdf/2509.25709v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:10:55Z
TRUE,Bayesian design|Sequential/adaptive|Other,Prediction|Parameter estimation|Optimization|Other,Bayesian D-optimal|Other,Variable/General (examples include K=20 and K=100 parameter dimensions; design variable ξ ∈ D),Healthcare/medical|Environmental monitoring|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Approximation methods|Other,TRUE,None / Not applicable,Not provided,https://web.maths.unsw.edu.au/~fkuo/lattice/index.html|https://people.cs.kuleuven.be/~dirk.nuyens/qmc-generators/,"The paper studies Bayesian optimal experimental design (OED) under the expected information gain (EIG) utility and focuses on a common additive-noise setting where the likelihood differential entropy is design-independent or available in closed form, reducing EIG to maximizing the differential entropy of the evidence (“maximum entropy sampling”). It proposes a scalable approximation of the evidence density via a surrogate forward model and a Gaussian-mixture (kernel-density) surrogate built from prior samples or (randomized) quasi-Monte Carlo (QMC) cubature points, then estimates the evidence entropy using Monte Carlo (or optionally cubature) without further expensive likelihood/forward solves. Theoretical results provide RMSE rates for the entropy estimator of order O(δ_K + N^{-1/2} + M^{-1/2}) for i.i.d.-sampled mixtures and O(δ_K + N^{-1/2} + M^{-1}) (more precisely M^{-1+γ}) for randomized QMC-based mixtures, where δ_K is forward-model/surrogate error and M is the number of forward evaluations used to build the evidence surrogate. Numerical experiments on a linear Gaussian deconvolution model (with closed-form evidence entropy) and a nonlinear PDE (Darcy flow) confirm the predicted convergence with respect to M and illustrate acceleration from QMC and higher-order lattice variants. Overall, the work advances computational methodology for Bayesian OED by decoupling entropy evaluation from expensive forward evaluations and by proving improved convergence under mild smoothness assumptions compared with nested EIG estimators.","The OED utility is the expected information gain (EIG) $U(\xi)=\mathbb{E}[D_{\mathrm{KL}}(\mu_Y,\mu)]$, rewritten as $U(\xi)=\mathrm{Ent}(\pi(\cdot;\xi)) - \mathbb{E}_\mu\mathrm{Ent}(\pi(\cdot|X;\xi))$ with differential entropy $\mathrm{Ent}(\rho)=-\int \rho(y)\log\rho(y)\,dy$. Under additive noise $y=G(x;\xi)+\varepsilon(\xi)$ with $\varepsilon$ independent of $x$, the second term becomes the noise entropy (e.g., Gaussian: $\tfrac12 d(1+\log 2\pi)+\tfrac12\log\det\Gamma(\xi)$), so the design objective reduces to maximizing evidence entropy $J(\xi)=\mathrm{Ent}(\pi(\cdot;\xi))=-\int \pi(y;\xi)\log\pi(y;\xi)\,dy$. They approximate the evidence by a Gaussian-mixture surrogate $\pi^{K}_{M}(y)=\frac1M\sum_{m=1}^M \pi^{K}(y|X_m)$ (mixture of $\mathcal{N}(G_K(X_m),\Gamma)$ components), and estimate $J$ by $\hat J^{K}_{M,N}(\xi)=-(1/N)\sum_{n=1}^N \log\pi^{K}_{M}(Y_n)$ with $Y_n\sim \pi^{K}_{M}$.","A bias–variance decomposition yields $\mathbb{E}(J-\hat J^{K}_{M,N})^2=(\mathrm{Ent}(\pi)-\mathrm{Ent}(\pi^{K}_{M}))^2 + \frac{1}{N}\mathrm{Var}_{\pi^{K}_{M}}[\log\pi^{K}_{M}(Y)]$. For i.i.d. prior samples used to form the Gaussian-mixture evidence surrogate, the paper proves an RMSE bound $O(\delta_K + N^{-1/2} + M^{-1/2})$ (equivalently MSE $O(\delta_K^2+1/N+1/M)$) under Lipschitz forward maps and sub-Gaussian priors. With randomized rank-1 lattice QMC points for constructing the mixture (uniform prior case, and discussed for Gaussian priors), the evidence-approximation term accelerates to $O(M^{-1+\gamma})$ (stated as $M^{-(2-\gamma)}$ in MSE) leading to RMSE $O(\delta_K + N^{-1/2} + M^{-1})$ up to $\gamma>0$. Numerical experiments show the MC mixture has approximately $M^{-1/2}$ RMSE decay, while randomized QMC mixtures show near $M^{-1}$ decay; a tent-transformed lattice variant empirically exhibits near second-order decay in the PDE example.","The paper notes that the MC estimator for differential entropy is not central and could be replaced by other cubature/entropy estimators; additional computational overhead of such alternatives is deferred to future work. It also states that the full theoretical bound for the Gaussian-prior QMC case is not proved (they provide a proposition and then ‘do not prove (46) but only numerically validate’ the hypothesized rate). In the deconvolution experiment, they acknowledge that their theorem’s sufficient condition ($L_1^2<\tfrac{1}{12}L_2$) is not satisfied for the chosen parameters, so the proven rate is not theoretically guaranteed in that setup.","The approach relies on the special additive-noise structure where the likelihood entropy is design-independent or explicitly computable; for many practical models (heteroscedastic noise, non-additive noise, design-dependent noise beyond covariance) the reduction to maximum-entropy evidence may not hold. The mixture surrogate is built from pushforwards of prior samples/QMC points; in high-dimensional or highly informative-likelihood regimes, the evidence may be challenging to approximate accurately with a finite mixture without adaptive/importance sampling, potentially worsening constants despite dimension-independent rates. The paper focuses on pointwise-in-design error bounds and does not fully treat optimization over the design space (e.g., gradients, nonconvexity, or uniform-in-ξ guarantees in practice).","They indicate that the additional computational overhead of higher-order/deterministic entropy-evaluation cubatures (beyond the main MC estimator) will be analyzed in future work. They also suggest (via discussion and remarks) further exploration of alternative entropy approximation techniques and higher-order QMC constructions beyond what is formally analyzed, with some numerical testing already included.","A natural extension is to develop and analyze gradient-based (or stochastic-gradient) optimization of the maximum-entropy objective with respect to the design variable ξ, including differentiating through the surrogate evidence and controlling optimization error. Broadening beyond additive Gaussian noise to handle non-Gaussian and design-dependent noise models (where the likelihood entropy term is not constant) would expand applicability, requiring partial reintroduction of EIG terms or new decompositions. It would also be useful to add adaptive/importance sampling or sequential experimental design loops (update prior/posterior between experiments) and study how surrogate reuse or multilevel surrogates affect total cost in realistic OED workflows.",2510.00734v1,https://arxiv.org/pdf/2510.00734v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:11:59Z
TRUE,Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,Other,Variable/General (examples include 6D design variable for CES; 2D sensor location for LF),Finance/economics|Other,Simulation study,TRUE,Python|Other,Public repository (GitHub/GitLab),github.com/GavinKerrigan/mtd,"The paper proposes a new geometric framework for Bayesian optimal experimental design, replacing mutual information (MI) objectives with a transport-based dependence measure called mutual transport dependence (MTD). MTD is defined as an optimal-transport discrepancy between the joint distribution p(θ,y|d) and the product of marginals p(θ)p(y|d), allowing the design objective to encode task-specific geometry via a chosen sample-level cost function. Unlike MI-based expected information gain, MTD can be estimated purely from samples without likelihood evaluations and avoids nested expectations, enabling gradient-based optimization when the simulator is differentiable via reparameterization. The authors provide theoretical links to MI (upper bounds under strong log-concavity) and closed-form results in a linear-Gaussian model, showing MTD remains bounded even as noise vanishes. Empirically, in sequential design benchmarks (CES in behavioral economics and source location finding), MTD yields competitive or lower RMSE than MI (estimated via PCE), and weighted/transformed costs demonstrate how MTD can be tuned to downstream error metrics and region-detection objectives.","Mutual information baseline is I(d)=KL[p(θ,y|d) || p(θ)p(y|d)] (also written as an expectation of log density ratios). The proposed criterion is MTD: T_c(d)=OT_c[p(θ,y|d), p(θ)p(y|d)]=min_{γ∈Π} E_γ[c(θ,y,θ',y')] where c is a user-chosen sample-level transport cost on (Θ×Y)^2. Special/related objectives include the target transport dependence T_c^(θ)(d)=E_{p(y|d)}[OT_c(p(θ|y,d),p(θ))] and data transport dependence T_c^(y)(d)=E_{p(θ)}[OT_c(p(y|θ,d),p(y|d))], obtainable as limiting cases via weighted joint costs c_η=η||θ-θ'||^p+||y-y'||^p. Example costs used include quadratic Euclidean costs ||θ-θ'||^2+||y-y'||^2 and weighted variants w(θ)(||θ-θ'||^2+||y-y'||^2).","On the CES benchmark after T=10 sequential designs (50 seeds), MTD designs achieve lower parameter RMSE than MI/PCE designs for all reported parameters (e.g., for ρ: 0.018±0.005 vs PCE 0.047±0.012; for u: 3.671±2.810 vs PCE 8.902±5.749). For location finding (T=25), MTD yields lower RMSE across most iterations, with MI/PCE catching up near the end; additionally, a weighted-cost MTD rapidly reduces expected 0–1 loss for detecting whether a source lies in a critical region R. The theory shows (under strong log-concavity and quadratic costs) transport-dependence objectives are upper-bounded by MI, and in a linear-Gaussian model MI diverges as observation noise → 0 while MTD remains bounded (T_c(d)≤2). Runtime reported: optimizing one design under MTD is ~30s vs ~120s for PCE (implementation-dependent).",None stated.,"Empirical evaluations are limited to a small set of benchmarks (CES and location finding) and comparisons are primarily against MI estimated via PCE; broader baselines (e.g., classical Fisher-information optimal designs or other BED objectives) are not systematically benchmarked under matched compute budgets. The gradient-based optimization relies on differentiable simulation/reparameterization; for non-differentiable simulators or discrete outcomes, practical optimization may require alternative methods not developed here. OT computation via linear programming on empirical samples can be computationally heavy and may scale poorly with large sample sizes or high-dimensional θ,y unless approximate/entropic OT variants are used (not the main focus).",The paper notes that transport-dependence constructions generalize to sequential settings by posterior updating and suggests extension to a policy setting by considering optimal transport over entire experimental rollouts.,"Develop scalable approximations (e.g., Sinkhorn/entropic OT, stochastic OT solvers) and study their bias–variance tradeoffs for design optimization in higher dimensions. Extend the approach to non-differentiable or discrete simulators (e.g., using gradient estimators, surrogate models, or derivative-free optimization) and to settings with model misspecification, autocorrelated data, or unknown nuisance parameters. Provide broader empirical validation across domains (e.g., physical sciences, engineering) and compare against classical optimality criteria (D-/A-/I-optimal) and modern implicit BED methods under controlled compute budgets, ideally with a robust software package.",2510.14848v1,https://arxiv.org/pdf/2510.14848v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:12:40Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/yujnkm/UCSB_3D|https://github.com/Unity-Technologies/ml-agents,"This PhD dissertation is primarily about designing interactive, personalized augmented-reality (AR) theater experiences and studying user behavior, navigation, and engagement in wide-area AR stages. It presents multiple AR systems and HCI studies, including wide-area digital-twin pipelines, outdoor AR user studies (lighting and navigation aids), interactive AR installations (Spatial Orchestra; Reality Distortion Room), and an AR theater platform (Dynamic Theater) plus an extension using imitation-learning-trained virtual audiences (Audience Amplify). Empirical evaluations include controlled user studies and pilot studies measuring usability, navigation performance, cognitive load proxies, engagement, perceived time, and subjective experience in AR environments. The work uses simulation/training environments (digital twins) and ML agents to model audience behaviors, and it reports statistically significant behavioral effects in several studies (e.g., navigation aids improving search; distortion effects shifting locomotion; virtual audiences increasing engagement/time spent). The dissertation also notes the platform is shared as open source and was used in a university course to create AR theater projects.",NA,"Notable quantitative findings reported include: (1) Dynamic Theater users overestimated stage duration (actual ~100.5s per stage vs perceived ~165.8s; p=2×10^-9), consistent with high immersion; overall enjoyment mean ~4.35/5, perceived 3D audio mean ~4.2/5. (2) Reality Distortion Room: particles increased total walking distance (mean ~16.7m without vs ~28.8m with particles; p<0.001); axis/center distortions produced statistically significant directional and distance-to-center locomotion changes (e.g., ANOVA p<0.00005 for axis effects; p<0.001 for center effects). (3) Audience Amplify main study: participants spent more time and reported higher interest/involvement with ML-trained virtual audiences than with no audience (e.g., time F(1,19)=28.672, p<.001; interest p=.047; involvement p=.024).","Authors note several constraints across studies: small sample sizes in some experiments (e.g., pilot studies) and brief trial durations limit generalizability; some effects may be influenced by novelty. Several systems depend on specialized hardware or setups (e.g., full-surround projection SAR for Reality Distortion Room), making replication/deployment at home unlikely. Some methods (e.g., billboarding dancers in Dynamic Theater) can break 3D perception with abrupt viewpoint changes, and results may be specific to corridor-like layouts used in studies. Virtual audiences are not fully interactive and training data did not include fine gestures (hands/faces), limiting realism.","This is not a DOE-focused work, but from an experimental-methods perspective several studies may be sensitive to uncontrolled environmental confounds (outdoor lighting, background clutter, participant familiarity with AR) and multiple-comparison risks across many questionnaire items and subscales. Several evaluations rely heavily on self-report Likert measures and bespoke experience essays, which can be affected by demand characteristics and novelty; preregistration and standardized primary endpoints are not evident from the provided excerpts. Some comparisons are asymmetric (e.g., “no-audience” conditions lacking audio while audience conditions include audio), potentially conflating “presence of audience” with “presence of additional audio content.” Reproducibility may be limited if the key AR theater platform repository is not explicitly linked in the dissertation text beyond general statements (only UCSB_3D is explicitly URL-cited).","Future work explicitly suggested includes: expanding open-source AR theater platforms and adapting them to additional devices (e.g., Meta Quest); integrating higher-fidelity volumetric performer capture for more lifelike virtual performers; improving AI agents/virtual audiences with richer behaviors and responsiveness (e.g., gestures, facial expressions, dancing/singing participation); training agents to better adapt to varied floor layouts and contexts; and adding richer multimodal immersion such as spatialized audio and more realistic/localized AR simulations (e.g., climate-change scenarios with real-time scanning/geographic context).","A valuable next step would be to formalize experimental protocols with clearer primary hypotheses/endpoints per study and ensure balanced stimulus components (e.g., audio parity across audience/no-audience conditions) to isolate causal factors. Broader external validation in multiple physical venues/layouts (not only corridor or specific campus locations) would strengthen generalizability and provide design rules that are less space-specific. Packaging and releasing the AR theater platform as a documented, versioned toolkit (with datasets for trained audiences and scripts for analysis) would materially improve reproducibility and adoption by other labs. Finally, robustness studies under realistic constraints (tracking drift, occlusion errors, multi-user interference, accessibility across body sizes/heights) would help transition from research prototypes to deployable AR theater systems.",2510.22098v1,https://arxiv.org/pdf/2510.22098v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:13:46Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Cost reduction|Other,Minimax/Maximin|Not applicable|Other,Variable/General (choose subset of parameters/moments/experiments; examples include 3 parameters in GE example and 4 areas in site-selection example),Finance/economics|Other,Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.7910/DVN/DPESAK,"The paper develops a unified framework for experimental design when experimental estimates are combined with potentially biased external (observational or other-context) evidence to estimate policy-relevant functionals (e.g., GE effects, externally valid averages). The design problem jointly selects which experiment(s) or moments to collect and how to allocate sample size under arbitrary budget/feasibility constraints, while also choosing how to combine experimental and observational estimates via linear (asymptotically linear/GMM) estimators. Designs are evaluated using a minimax proportional-regret (“adaptation regret”) criterion relative to an oracle that knows the observational bias magnitude and optimally chooses both design and estimator, yielding an explicit bias–variance trade-off. The regret is characterized as the maximum of a normalized variance component and a normalized worst-case bias component, which leads to a tractable mixed-integer quadratic programming formulation for selecting experiments and sample allocation. Applications illustrate designing cash-transfer experiments to learn GE effects and optimizing site selection for a microfinance intervention; the proposed design can sharply reduce worst-case bias/MSE relative to variance-only (Neyman) designs.","The paper’s core estimator combines experimental and observational point estimates using shrinkage weights: for parameters j in the experimentally learned set E, $\hat\theta_j(\gamma)=\gamma_j\tilde\theta^{exp}_j+(1-\gamma_j)\tilde\theta^{obs}_j$ (and $\hat\theta_j=\tilde\theta^{obs}_j$ for $j\notin E$), with target $\tau(\theta)\approx \omega^\top\theta$. The robust objective uses adaptation regret $R=\sup_{B\ge 0}\sup_{\|b\|\le B}\text{MSE}_b(E,\Sigma,\gamma)/\text{MSE}^*(B)$, and the main characterization shows $R(E,\Sigma,\gamma)=\max\{\alpha/\alpha^*,\beta/\beta^*\}$ where $\alpha$ is variance and $\beta$ is worst-case squared bias. Under independence and a cost constraint, the variance term can be written as $\alpha(s)=\frac{1}{n}(\sum_j s_j|\omega_j|v_j/\sqrt{c_j})^2+(\omega\odot(1-s))^\top\Sigma_{obs}(\omega\odot(1-s))$, with $s_j=x_j\gamma_j$, and the bias term as $\beta(s)=(\sum_j(1-s_j)|\omega_j|)^2$; optimal sample sizes satisfy a Neyman-style allocation $n_j^*\propto |\omega_j|v_js_j/\sqrt{c_j}$.","In the cash-transfer GE illustration, the regret-optimal design choice depends on total experimental sample size: with small samples a combination of CCT and UCT is optimal (allocating most observations to CCT), while for larger samples the wage (job program) experiment becomes most informative. With 1,000 participants, relative to the variance-minimizing (Neyman) design, the optimal two-arm design achieves “more than a 400% reduction in bias (and adaptation regret)” while keeping variance no larger than about 30% of the Neyman variance; for a single-arm design it yields about a 140% reduction in bias at roughly a 12% variance increase. In the microfinance site-selection application, using observational estimates to choose one or two areas for randomization and then validating with an experimentally calibrated bias, the proposed design reduces MSE by more than 250% compared to a variance-optimal benchmark and tracks the oracle MSE more closely (reported ratios near 1–1.2 versus about 2.5+ for Neyman in their plots).","The paper notes that its key regret characterization relies on restricting attention to estimators that are (asymptotically) linear in experimental/observational moments; it contrasts this with optimal nonlinear shrinkage rules where quasi-convexity fails and computation becomes harder. It also discusses that for nonlinear targets/moments the framework relies on local asymptotics (bias local to zero with $\|b_n\|^2\sqrt{n}\to 0$) so higher-order terms are negligible, and notes potential inference distortions when the same observational data are used both to choose the design and estimate Jacobians, recommending sample splitting to retain valid CLT-based inference.","The framework’s practical performance depends on having credible ex-ante variance/cost models for each candidate experiment (e.g., per-unit variances $v_j^2$ and cost parameters $c_j$); misspecification of these inputs could change design rankings but is not systematically stress-tested. Worst-case bias is modeled via norm-balls (often $\ell_\infty$) that treat biases as bounded but otherwise adversarial; in settings where biases have structure (e.g., sign restrictions, correlations, covariate-dependent external validity) the design might be overly conservative or miss opportunities to exploit structure. The optimization can require mixed-integer quadratic programming over potentially large menus of experiments/moments, which may become computationally demanding at scale and may need heuristics; runtime/scalability is not benchmarked. The empirical illustrations are stylized/calibrated and do not report full reproducibility artifacts or robustness checks to alternative calibrations (e.g., alternative demand elasticities or bias norms).","The conclusion highlights extensions to settings where researchers have well-specified priors about bias (e.g., Bayesian models) and suggests these could be incorporated through additional moment restrictions in the general framework. It also points to sequential or adaptive experimental choices as a future direction (citing adaptive design/bandit-style literatures).","A useful extension would be to develop sensitivity and robustness analysis to misspecification of the design inputs (variance/cost models), e.g., distributionally robust or Bayesian-robust design over uncertainty in $\Sigma(E)$ and costs. Another direction is to provide scalable algorithms/approximations for large experiment menus (many candidate sites/arms/moments), including decomposition methods or greedy/convex relaxations with approximation guarantees. It would also be valuable to integrate richer bias structures (sign/correlation constraints, hierarchical external-validity models, covariate shift) and to study resulting regret characterizations. Finally, providing open-source software with templates for common economics designs (multi-arm RCTs, site selection, interference/GE models) and conducting broader empirical benchmarks across multiple real RCT–observational combinations would improve adoption and validation.",2510.23434v3,https://arxiv.org/pdf/2510.23434v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:14:49Z
TRUE,Response surface|Optimal design|Bayesian design|Sequential/adaptive|Other,Parameter estimation|Prediction|Robustness|Other,A-optimal|D-optimal|Bayesian A-optimal|Bayesian D-optimal|Other,Variable/General (focus on 1–2 factors; examples with q=1 and q=2; due to computational issues),Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper develops pseudo-Bayesian optimal exact designs for fitting fractional polynomial (FP) response surface models, where predictors are transformed via the Box–Tidwell power/log transformation and the model is nonlinear in the power parameter(s). It applies the exact-design construction methodology of Gilmour & Trinca (2012) to one- and two-factor FP models, using reparameterizations (e.g., in terms of interpretable slope/curvature quantities like $\gamma_1$ and $\gamma_{11}$) to make prior specification more meaningful and approximately independent. Designs are optimized under a weighted As-optimality criterion (a weighted average of variances of selected parameters) with the Bayesian/pseudo-Bayesian criterion formed by integrating (approximating via Monte Carlo sampling) over prior distributions for nonlinear parameters, including discrete priors for transformation powers $\alpha$. The resulting Bayesian optimal exact designs are compared against standard response-surface designs (equally spaced designs, CCD projections, and point-prior locally optimal designs), showing that common designs can be substantially inefficient for estimating FP power parameters and that Bayesian exact designs typically require more support points to gain robustness. A brief two-factor illustration shows pseudo-Bayesian D-optimal designs provide markedly higher robustness to uncertainty in transformation parameters than locally optimal designs.","Fractional polynomial transformation: $x^{(\alpha)}=x^\alpha$ for $\alpha\neq 0$ and $x^{(\alpha)}=\log(x)$ for $\alpha=0$. Bayesian design criterion: $\phi(X)=\mathbb{E}_\theta\{\varphi(M(X,\theta))\}=\int \varphi(M(X,\theta))p(\theta)d\theta$, approximated by Monte Carlo as $\tilde\phi(X)=\frac{1}{r}\sum_{j=1}^r \varphi(M(X,\tilde\theta_j))$; for weighted As-optimality, $\tilde\phi(X)=\sum_{j=1}^r \mathrm{tr}\{W\,M(X,\tilde\theta_j)\}$ (intercept weight set to 0). Asymptotic covariance uses the information matrix $M(X,\theta)=F(X,\theta)'F(X,\theta)/\sigma^2$ based on derivatives $\partial\eta(x,\theta)/\partial\theta$.","For one-factor first-order FP models, the paper reports Bayesian optimal exact designs (e.g., for $n=12$, range $[0.1,1]$, and a discrete 7-point prior on $\alpha$) that concentrate replications at extremes with two interior points; a representative optimum is $x=(0.1,0.1895,0.5225,1)$ with replications $(3,4,2,3)$. In extensive efficiency tables, popular designs such as CCD projections and 3-level designs can be very inefficient relative to the Bayesian optimum (e.g., efficiencies reported as low as ~11% in the shown comparisons), while 4-level designs equally spaced in a guessed transformed metric can be relatively competitive (often ~80–95% efficient depending on priors). For second-order one-factor FP models with $n=20$, the Bayesian optimal designs often use 5–7 support points and standard designs are generally much farther from optimal (e.g., a 5-level equally replicated design in log-scale is cited as robust but only ~86% efficient in the best case considered). In the two-factor illustration, pseudo-Bayesian D-optimal designs achieve uniformly high robustness with Bayesian efficiencies above ~87% across prior scenarios, whereas locally D-optimal designs lose efficiency substantially under misspecified $\alpha$ values.","The authors restrict most computations to one- and two-factor cases due to computational issues/long computing time, noting that multidimensional grid searches become difficult as factors increase. They also state that, especially for second-order FP models, they cannot guarantee the obtained exact designs are global optima, though they outperformed alternative designs considered. They highlight that complete search can be prohibitively time-consuming compared with exchange algorithms for larger problems.","The work largely evaluates designs via criterion values and relative efficiencies under the assumed nonlinear model with independent errors (and typically normality in the response error term), without a systematic robustness study to model misspecification beyond uncertainty in FP power parameters. Practical guidance is given for priors/weights, but real empirical case studies demonstrating end-to-end performance (fit quality, diagnostics, operational constraints) are not shown in the provided text, which may limit practitioner confidence. The methodology depends on prior choices (discrete masses for $\alpha$, normal priors for $\gamma$’s) and on Monte Carlo approximation size $r$, yet sensitivity to these implementation choices (e.g., $r$ adequacy, candidate grid resolution effects) is only partially explored.","They suggest extension to multi-factor designs is straightforward conceptually, but the main challenge is computational burden from searching multidimensional grids. They propose that one practical alternative for more factors is to build candidate sets from factorial combinations in transformed metrics based on best guesses for the $\alpha$’s, then improve designs by local search around points (analogous to the Donev & Atkinson adjustment algorithm).","Develop scalable optimization implementations (e.g., coordinate-exchange with modern heuristics, gradient-based or surrogate optimization, parallel computing) and release open-source software to make pseudo-Bayesian exact FP design construction routine. Extend to correlated errors and common response-surface complications (heteroscedasticity, autocorrelation, random effects, split-plot restrictions), where exact designs may differ materially. Provide broader empirical validation on real response-surface experiments and benchmark against additional modern alternatives (e.g., I-/G-optimal or prediction-oriented Bayesian criteria) to guide practitioners when prediction/optimization, not just parameter estimation, is the primary goal.",2510.24349v1,https://arxiv.org/pdf/2510.24349v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:15:35Z
TRUE,Response surface|Screening|Definitive screening|Other,Screening|Parameter estimation|Prediction|Cost reduction|Other,D-optimal|Other,Variable/General (examples and catalog: 7–20 factors; prior OMARS catalogs up to 5 factors; discussion includes 6–20),Food/agriculture|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|R|Python|Other,Supplementary material (Journal/Publisher),https://arxiv.org/abs/2511.02984,"The paper proposes a construction method for large orthogonal minimally aliased response surface (OMARS) designs by concatenating two definitive screening designs (DSDs), yielding cOMARS designs that study quantitative factors at three levels with economical run sizes. The method uses column permutations and foldover operations (an adapted CC/VNS heuristic) to minimize aliasing among second-order effects (quadratic terms and two-factor interactions), and provides analytical characterizations of several aliasing/correlation patterns, including results based on J4-characteristics for interaction–interaction aliasing. The authors build a catalog of cOMARS designs for 7–20 factors (typically with a center run) and compare aliasing metrics against benchmark DSDs and available OMARS designs; for some factor sizes cOMARS minimizes maximum aliasing, while DSDs may minimize overall SSQ aliasing. A simulation study (7-factor setting) compares cOMARS plus an OMARS-tailored analysis method to D-optimal and Bayesian D-optimal non-orthogonal designs analyzed via the Dantzig selector, reporting power, type-I error, FDR, and RMSE; no method dominates, but cOMARS is competitive when many linear effects and few second-order effects are active. Supplementary material includes derivations, a MATLAB implementation of the adapted CC/VNS algorithm, a ZIP catalog of designs, and R/Python code to reproduce simulations.","A cOMARS design is constructed by stacking two parent DSDs plus optional center runs: $C = \begin{pmatrix} D_1 \\ D_2 \\ 0_{n_0\times m}\end{pmatrix}$, giving run size $4m+n_0$ (even $m$) or $4(m+1)+n_0$ (odd $m$). The correlation between quadratic-effect columns for factors $i$ and $j$ is $r_{ii,jj}=\frac{n_0(n-2)-4}{(n-1)(n_0+4)}$ (with $n=m$ if $m$ even, $n=m+1$ if $m$ odd). For a quadratic and interaction not sharing a factor, $|r_{ii,jk}|\in\left\{\sqrt{\frac{4n+n_0}{(n_0+4)(n-1)(n-2)}},\,0\right\}$; interaction–interaction correlations are characterized via $J_4$-characteristics, yielding discrete possible values depending on $n\bmod 4$.","The paper derives closed-form expressions for several second-order aliasing correlations in cOMARS designs and proves a result enumerating possible $J_4$-characteristic values (hence possible interaction–interaction correlation values) depending on $n \equiv 0$ or $2 \pmod 4$. A catalog of optimized cOMARS designs for 7–20 factors is produced (Table 3), with two optimization targets: minimizing SSQ of interaction-column correlations (‘s’) or minimizing a weighted frequency objective (‘f’). For the motivating 7-factor 34-run design, the maximum absolute correlation among second-order columns is 0.367 and the median is 0; its SSQ (16.083) is competitive with or better than many benchmark 34-run OMARS/DSD alternatives (Table 4). In simulations (7 factors; 1000 reps; 5 active linear, 4 active interactions, and 1–3 active quadratics), cOMARS achieved higher average power for linear effects (≈0.955–0.964) than the compared D-optimal/Bayesian D-optimal designs, while non-orthogonal designs had higher interaction power; type-I error rates were generally <0.05 except one interaction case (0.073) for cOMARS when 3 quadratics were active (Table 7).",None stated.,"The proposed cOMARS construction is restricted to designs obtained by concatenating two DSDs derived from (smallest-run) conference designs, so it may miss superior OMARS designs outside this construction class. Performance comparisons emphasize aliasing/correlation summaries and selected simulation scenarios; broader robustness (e.g., non-normal errors, correlated observations, or different sparsity/heredity patterns) is not established. Practical adoption depends on the specialized analysis workflow (e.g., Hameed et al. method) and on access to the supplementary code/catalog; the paper does not describe a fully packaged, user-friendly software distribution (e.g., CRAN/PyPI).","The authors note that larger cOMARS designs can be generated by concatenating DSDs derived from larger-run conference designs in the Schoen et al. (2022) catalog, trading off different aliasing patterns. They also suggest augmenting cOMARS designs with axial runs (following Liu et al., 2022) to improve estimation of quadratic effects and prediction accuracy when a follow-up response-surface modeling phase is desired.","Develop a self-contained software package (R/Python) that generates optimized cOMARS designs for user-specified factor counts, center runs, and constraints, and automates the CC/VNS search with reproducible seeds and diagnostics. Extend theory and evaluation to settings with randomization restrictions (split-plot/hard-to-change factors), missing runs, and autocorrelated/process data, where orthogonality/aliasing properties and analysis methods may change. Provide guidance on sequential strategies (e.g., start with cOMARS for screening, then augment adaptively with axial/extra runs) with operating characteristics and cost/benefit comparisons to Bayesian optimal sequential designs.",2511.02984v2,https://arxiv.org/pdf/2511.02984v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:16:20Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design,Parameter estimation|Prediction|Other,Bayesian D-optimal|Other,Variable/General (design dimension $d_{\xi}$; examples include 2D simplex allocation in SIR and $J=2$ sensor orientations in moving-source task),Healthcare/medical|Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://anonymous.4open.science/r/badpods-2C8D,"The paper proposes an online Bayesian adaptive design (BAD) framework for partially observable dynamical systems modeled as nonlinear state-space models with latent states, where the likelihood and expected information gain (EIG) are intractable. It derives new Monte Carlo estimators of the EIG and its design gradient that explicitly marginalize latent-state trajectories, enabling stochastic gradient ascent over continuous design spaces at each time step. The approach is implemented with nested particle filters (NPFs) to maintain a joint posterior over parameters and states online without reprocessing the full history, giving per-step cost that scales linearly in time and comes with consistency (almost sure convergence) guarantees for the EIG estimator under regularity conditions. Empirical evaluation on a partially observed two-group stochastic SIR model (design allocates sampling effort across groups) and a moving-source localization task (design sets sensor orientations) shows that the proposed sequential method (BAD-PODS) yields higher accumulated information gain than random designs and a static (offline-optimized) BED baseline, with gains compounding over time. The method advances Bayesian experimental design for dynamical systems by addressing partial observability and providing a non-amortized, fully online alternative to surrogate-based likelihood-free design methods.","The design at time $t$ is chosen to maximize expected information gain $I(\xi_t)=\mathbb{E}_{p(\theta\mid h_{t-1})p(y_t\mid\theta,\xi_t)}[\log p(y_t\mid\theta,\xi_t)-\log p(y_t\mid\xi_t)]$ (Eq. 1), with $h_{t-1}$ the past design/observation history. In a state-space model $x_t\sim f(x_t\mid x_{t-1},\theta,\xi_t)$, $y_t\sim g(y_t\mid x_t,\theta,\xi_t)$, the likelihood and evidence are $L_{\theta,\xi_t}(y_t)=\mathbb{E}[g(y_t\mid x_t,\theta,\xi_t)]$ and $Z_{\xi_t}(y_t)=\mathbb{E}[g(y_t\mid x_t,\theta,\xi_t)]$ with expectations over latent states (and over $\theta$ for $Z$), yielding $I(\xi_t)=\mathbb{E}[\log(L/Z)]$ (Eq. 4). The EIG gradient is expressed via Fisher’s identity as $\nabla_{\xi_t}I(\xi_t)=\mathbb{E}_{\Gamma_{\xi_t}}\big[\nabla L/L-\nabla Z/Z + \log(L/Z)\,s_{\theta,\xi_t}(x_{t-1:t},y_t)\big]$ with design score $s_{\theta,\xi_t}=\nabla_{\xi_t}\log g+\nabla_{\xi_t}\log f$ (Eq. 5), and is estimated using NPF-based nested Monte Carlo (Eqs. 6–8).","Across 50 Monte Carlo seeds, the proposed sequential BAD-PODS achieves higher total EIG (TEIG) than random designs and the static BED baseline in both experiments. In the two-group SIR task, TEIG at $t=200$ is 5.918 [5.765, 6.055] for BAD-PODS versus 5.621 [5.466, 5.793] for random; static results are reported up to $t=150$ where BAD-PODS is 4.748 [4.609, 4.871] versus static 3.547 [3.454, 3.658]. In the moving-source task, TEIG at $t=40$ is 1.483 [1.318, 1.822] for BAD-PODS versus 1.217 [1.136, 1.344] for random, with static reported up to $t=20$ where BAD-PODS is 0.709 [0.650, 0.855] versus static 0.570 [0.541, 0.604]. The TEIG improvements increase over time, and in the moving-source study BAD-PODS also yields lower sensor pointing error than baselines, indicating designs better aligned with the true target bearing.","The paper notes that existing baselines for this exact partially observable online setting are lacking, so comparisons are limited to random designs and a static (offline) BED variant of their approach. It also reports that the static BED baseline becomes computationally infeasible at longer horizons/high-dimensional design spaces, so static results are truncated at larger $t$ values.","The empirical validation is limited to two simulated case studies; there is no real-data demonstration, so practical issues (model misspecification, nonstationarity, sensor failures) are not assessed. The method’s performance and stability may depend sensitively on particle counts (M, N, L), jittering-kernel tuning, and stochastic-gradient hyperparameters, but robustness/ablation over these choices is not fully characterized. Assumptions used for consistency (e.g., bounded positive likelihood, differentiability w.r.t. design, regularity of the log(L/Z) integrand) can be restrictive in realistic SSMs with heavy tails, near-zero likelihoods, or discrete/non-differentiable design constraints.",None stated.,"Extend BAD-PODS to handle common SSM complications such as unknown observation/transition noise scales, autocorrelated or misspecified noise, and non-differentiable or combinatorial design constraints (e.g., discrete actions, batching). Provide systematic guidance and adaptive schemes for choosing particle numbers and jittering/SGA hyperparameters (including variance-reduction and control variates) to improve efficiency and reliability. Validate on real-world partially observed dynamical systems (e.g., epidemiological surveillance streams or real sensor-network deployments) and release a maintained software package for broader adoption.",2511.04403v1,https://arxiv.org/pdf/2511.04403v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:17:08Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design,Parameter estimation|Prediction|Robustness|Cost reduction,Bayesian D-optimal|Other,Variable/General (examples include: linear regression covariates; PK sampling time; location-finding coordinates in d dimensions),Healthcare/medical|Pharmaceutical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/yasirbarlas/GBOED,"The paper proposes Generalised Bayesian Optimal Experimental Design (GBOED), extending Bayesian optimal experimental design to the generalized Bayesian/Gibbs inference framework to improve robustness under model misspecification. It replaces likelihood-based Bayesian updating with loss-based Gibbs posteriors and introduces a new information-theoretic acquisition function, the Gibbs expected information gain (Gibbs EIG), derived via a generalized (pseudo-)information framework. The method targets robustness to outliers and incorrect assumptions about the observation noise distribution while retaining competitive performance under well-specification. Empirical studies on three sequential design problems—Bayesian linear regression, pharmacokinetics blood-sampling time selection, and high-dimensional location finding—show improved predictive performance vs standard BOED in misspecified settings, with weighted score matching (IMQ weighting) often performing best. The paper also proposes an exponential-decay schedule to tune the IMQ kernel shrinkage parameter to avoid overly aggressive downweighting early in small-data regimes.","Gibbs posterior update: $\pi(\theta\mid y,\xi) \propto \exp(-\omega\,\ell_\theta(\xi,y))\,\pi(\theta)$. The proposed utility is the Gibbs EIG: $\mathrm{EIG}_{\mathrm{Gibbs}}(\xi)=\tilde I(\Theta;\tilde Y\mid \xi)$, and it is expressed for computation via importance sampling as $\mathbb{E}_{\pi(\theta)p(y\mid\theta,\xi)}\Big[( -\omega\ell_\theta(\xi,y)-\log \tilde\pi(y\mid\xi))\,\frac{\exp(-\omega\ell_\theta(\xi,y))}{p(y\mid\theta,\xi)}\Big]$. Weighted score matching uses an IMQ weight $r_{\mathrm{IMQ}}(\xi,y)=\omega\big(1+\frac{(y-\gamma(\xi))^2}{c(\xi)^2}\big)^{-1/2}$, and an exponential-decay schedule is proposed for $c$: $c(i)=q_1\exp(-b(i-1))+q_2$.","Across simulated sequential design problems, GBOED is reported as comparable to BOED under well-specification and substantially better under misspecification (asymmetric outliers; misspecified noise). In the location-finding benchmark (Table 1), for $d=2$ under asymmetric outliers, BOED has MMD 0.571 (SE 0.017) vs GBOED 0.179 (SE 0.004), and NLL 4.393 (0.122) vs 1.300 (0.017). Under misspecified error variance at $d=2$, BOED NLL is 5.635 (0.150) vs GBOED 2.851 (0.075), with similar improvements shown for higher $d$ in several settings. The ablation studies indicate that gains come from both Gibbs inference and the Gibbs EIG acquisition, with weighted score matching (and either Laplante-style tuning or the proposed exponential-decay tuning for IMQ parameters) frequently achieving the strongest robustness.","The authors note that the importance-sampling approach used to compute the Gibbs EIG (Theorem 1) can suffer high variance and numerical instability if the misspecified statistical model is a poor proposal distribution. They also acknowledge that the nested Monte Carlo (NMC) estimator has slow convergence and could be replaced by variational estimators. They state that the framework depends on choosing an appropriate learning rate $\omega$ and that a learning-rate selection method tailored to experimental design is still lacking. Finally, they note scalability challenges for complicated/high-dimensional design problems (e.g., location finding) and suggest amortization/policy-learning approaches as potential remedies.","The empirical validation is entirely simulation-based; robustness and practicality on real experimental datasets (with real operational constraints and measurement idiosyncrasies) remain untested. The method’s performance may be sensitive to user-chosen loss/scoring rule and its hyperparameters (e.g., IMQ kernel settings and decay schedule), which could introduce substantial tuning burden or instability across applications. The approach still relies on an assumed likelihood model $p(y\mid\theta,\xi)$ as a proposal for Gibbs EIG estimation; in severe misspecification, both the proposal and score-based losses derived from the model may fail, potentially undermining robustness. Comparisons focus on BOED variants; broader baselines from robust active learning, likelihood-free/amortized BED, or adversarially robust design criteria are not comprehensively benchmarked.","They propose exploring alternative proposal distributions for importance sampling when the statistical model is unsuitable, improving Gibbs EIG approximation beyond NMC (e.g., variational estimators), and developing learning-rate selection methods appropriate for sequential experimental design. They also suggest improving scalability and non-myopic design via amortization and policy-learning approaches, citing recent work on deep adaptive design and reinforcement-learning-based design. They mention investigating misspecification and generalizability in amortized experimental design settings as a further direction.","Developing principled, automated hyperparameter selection for robust scoring rules (e.g., jointly adapting $\omega$, $\gamma$, and $c$ with uncertainty quantification) would likely be critical for adoption. Extending the framework to dependent data (temporal/spatial correlation), constrained experiments (batching, multi-fidelity, or safety constraints), and multivariate outcomes would broaden applicability and test robustness claims. Providing theoretical guarantees (e.g., bounds on regret/information loss under misspecification, or conditions for variance control in the importance-sampling estimator) would strengthen confidence in the approach. Packaging the method into a reusable library with reference implementations of losses, estimators, and design optimizers (beyond Bayesian optimization) would improve reproducibility and facilitate practitioner uptake.",2511.07671v1,https://arxiv.org/pdf/2511.07671v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:18:04Z
TRUE,Computer experiment|Sequential/adaptive|Other,Prediction|Cost reduction|Other,Space-filling|Minimax/Maximin|Not applicable,"Variable/General (examples include 1D, 4D, and 3 inputs for spatiotemporal case: longitude, latitude, time)",Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://www.institut-seism.fr/en/|https://arxiv.org/abs/2503.16027|https://theses.hal.science/tel-01240812|http://bobby.gramacy.com/surrogates/,"The paper develops an efficient bi-fidelity Gaussian-process (GP) surrogate modeling approach for noisy outputs when the high-fidelity (HF) and low-fidelity (LF) input designs are not necessarily nested. It generalizes the recursive AR(1) / co-kriging construction by treating the LF posterior process as an input to the HF model and by explicitly incorporating additive Gaussian noise at both fidelities, which removes the usual interpolation simplifications. For training, LF hyperparameters are fit by maximum likelihood, while HF parameters are estimated via an EM algorithm that treats the unobserved LF posterior at HF locations as latent; a closed-form M-step update is derived when the scaling factor $\rho(x)$ is a parametric linear predictor, enabling a decoupled and scalable optimization strategy. The method is benchmarked on noisy analytical test functions (including a 4D Park function) and compared to HF-only GP and a recent multi-fidelity PCE approach, showing strongest benefits when HF data are scarce. A real case study on sea surface temperature (satellite MODIS as LF, MWRA stations as HF) demonstrates results consistent with prior non-recursive noisy MF-GP approaches while improving computational complexity (claimed reduction from $\mathcal{O}((N_L+N_H)^3)$ to $\mathcal{O}(N_L^3+N_H^3)$).","The design/surrogate model is a noisy recursive AR(1) co-kriging: $Y_H(x)=\rho(x)\,\tilde Y_L(x)+\Delta_H(x)$ with $\rho(x)=g_L(x)^\top\beta_\rho$ and additive noises $Z_L=Y_L+\varepsilon_L$, $Z_H=Y_H+\varepsilon_H$ (independent, homoscedastic Gaussian). For non-nested designs, the HF conditioning uses $m_{Y_H}(x)=m_{AR}(x)+k_{AR}(x)^\top(K_{AR}+\sigma^2_{\varepsilon,H}I)^{-1}(z^H_{tr}-m_{AR})$ with $K_{AR}=(\rho\rho^\top)\odot V_{\tilde Y_L}(X_H,X_H)+\sigma_H^2R_H$. HF hyperparameters are optimized by EM, with closed-form M-step updates for $(\beta_{\rho},\beta_H)$ and $\sigma_H^2$, and numerical optimization for $(\theta_H,\eta_H)$ via an L-BFGS-B routine on a reduced objective involving $\log\det(R_H+\eta_H I)$ and $\log\hat\sigma_H^2$.","In a 1D noisy benchmark, the proposed MF-GP achieves lower normalized error (1−Q²) than a noisy multi-fidelity PCE comparator, while showing competitive coverage metrics; the HF-only GP is often comparable except when HF data are very scarce. On the 4D Park function with (example) $N_L=100, N_H=20$ and noise variances $\sigma^2_{\varepsilon,L}=\sigma^2_{\varepsilon,H}=1$, reported predictivities include $Q^2\approx0.942$ (MF) versus $Q^2\approx0.871$ (HF-only) on one run, with both models tending to underestimate uncertainty in interval-reliability plots. In a larger repeated benchmark (up to 500 replications per setting), MF-GP improves 1−Q² for small $N_H$ but becomes similar to HF-only as $N_H$ increases; HF-only generally yields better prediction-interval reliability (IAEPI) than MF in this noisy setting. On the sea-surface-temperature dataset (LF MODIS imagery, HF MWRA stations; about $N_H=195$ HF train points and $N_L=2526$ LF points), the method produces spatiotemporal SST predictions and uncertainty maps consistent with prior published MF-GP results while using a more scalable decoupled estimation strategy.",None stated.,"Although the paper is motivated by non-nested designs and sequential design, it does not present or evaluate a concrete sequential DOE policy (acquisition criteria, fidelity selection rules, or cost-aware stopping), so DOE contribution is mainly contextual. The experimental designs used in benchmarks are largely space-filling (LHS/maximin LHS) without systematic comparison to alternative designs (e.g., adaptive infill, D/I-optimal, or cost-aware multi-fidelity designs), limiting conclusions about “best” DOE choices. Noise is assumed additive, Gaussian, independent, and homoscedastic at each fidelity; many real simulators/experiments exhibit heteroscedasticity and correlation (e.g., temporal dependence in SST), which could affect estimation and EM performance. No public implementation is provided, making it harder to assess practical computational gains and reproduce the EM optimization behavior on large-scale problems.","The authors propose improving estimation of kernel and noise parameters, studying more complex noise models (e.g., input-dependent noise variance), and exploring ways for HF noise variance estimation to be informed by the LF-GP. They also mention developing sequential design of experiments strategies that are not limited to nested input sets, to improve MF-GP performance when budget allows.","Develop and benchmark an explicit multi-fidelity sequential/adaptive DOE strategy (e.g., cost-weighted expected improvement / integrated variance reduction) that chooses both the next input location and fidelity under noisy, non-nested settings. Extend the method to heteroscedastic and correlated noise (temporal/spatial covariance for SST; replicate observations) and assess robustness of EM updates under misspecification. Provide open-source code and standardized benchmark protocols (including wall-clock time, numerical stability, and scalability vs. $N_L,N_H$) to validate the claimed computational advantages. Explore design strategies for non-nested multi-fidelity settings that explicitly control nesting structure (partial nesting) and compare to pure space-filling designs.",2511.20183v1,https://arxiv.org/pdf/2511.20183v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:19:01Z
FALSE,NA,NA,Not applicable,Not specified,Network/cybersecurity,Case study (real dataset)|Simulation study|Other,TRUE,Python|Other,Upon request,https://dx.doi.org/10.21227/dwkg-n940|https://dx.doi.org/10.21227/39z8-w554|https://dx.doi.org/10.21227/xdw9-3677,"The paper introduces GAMBiT, a cognitive-informed cyber-defense framework that embeds “cognitive triggers” (deceptive but plausible network artifacts) to elicit specific attacker cognitive biases and “cognitive sensors” to infer latent attacker cognitive states from behavior and telemetry. It reports three rounds of human-subject experiments (total n=61) in a simulated small-business enterprise network comparing trigger vs. control conditions to quantify behavioral impact. The triggers target biases including loss aversion, base-rate neglect, confirmation bias, sunk-cost fallacy, and availability bias; sensors include an LLM-based Attack Summarization Module mapping Suricata/NetFlow to MITRE ATT&CK technique signals and a PsychSim-based theory-of-mind defender that updates beliefs over bias models. Empirically, trigger exposure reduces mission progress, diverts actions off the true attack path, and increases attacker detectability (e.g., higher Suricata alert counts). The work advances cyber-deception/SPC-adjacent evaluation by operationalizing cognition as a manipulable and measurable defensive surface, validated via controlled, instrumented human-subject experiments in a realistic cyber range.","The paper defines diversion as the proportion of commands directed to the true attack path: $\frac{\#\,\text{commands on 13 critical VMs}}{\#\,\text{commands on all 38 VMs}}$ (Eq. 1). For sunk-cost modeling in the defender’s bias model, continued pursuit is framed as $\text{ExpectedReward} \ge -\lambda\cdot \text{SunkCost}$ with $\lambda\ge 0$ representing degree of sunk-cost bias. The cognitive-sensor outputs are probability distributions over five cognitive vulnerabilities (normalized to sum to 1) used for theory-of-mind belief updating in PsychSim.","Across three rounds (EXP1 trigger n=19; EXP2 control n=20; EXP2 trigger n=22), mission progress differed by condition with a statistically significant group effect (Kruskal–Wallis p=0.0495), with control participants achieving higher median progress than trigger groups. Trigger exposure reduced the share of commands directed to the true attack path: in EXP2, control had a higher proportion on-path (Mean=67.823, SD=12.692) than trigger (Mean=49.501, SD=24.256), with a significant main effect (two-way ANOVA F(1,35)=10.37, p=0.003). Detectability increased under triggers: Suricata alerts on it-ubuntu-1 were higher in trigger vs control (t=2.25, p=0.0381). Specific trigger L.12.1 (loss aversion) showed strong diversion effects toward site-proxy, including a significant increase in proportion of MITRE ATT&CK techniques targeting that VM (F(1,37)=11.59, p<0.01).","The authors note that the experiments used an open-world, naturalistic decision-making design where not all participants encountered each trigger, making trigger-specific effects difficult to isolate and compare (especially when triggers overlap in location and behavioral impacts). They also state that some triggers in Experiment 1 were rarely encountered due to placement in peripheral/low-traffic areas, motivating relocation and instrumentation improvements in Experiment 2. They further indicate that some model components (e.g., TTP detection-risk factors initialized by SMEs) require additional refinement with further research, and that some planned sensing (cognitive flexibility surveillance sensor) was not yet implemented or delivered as software.","As a human-subject cyber-range study, results may depend on participant mix, prior skills, and the specific range tooling/logging configuration; external validity to real attackers and real enterprise environments is uncertain. The trigger/control comparison is partially confounded by iterative engineering changes across rounds (e.g., trigger placement changes and added logging/Suricata improvements), which can affect outcomes independently of cognitive manipulation. Statistical analyses focus on aggregate effects and selected triggers; broader multiplicity control and robustness checks across many behavioral metrics/triggers are not clearly described here. Sensor pipelines that use LLMs (ASM) may be sensitive to prompt/mapping choices and data drift, and the paper does not provide fully reproducible code artifacts publicly to enable independent verification.","The authors propose moving from static trigger deployments to adaptive triggers that can be dynamically selected/deployed by autonomous defender agents as part of real-time cognitive defense strategies. They also indicate plans to combine LLM- and GNN-based ASM approaches into an ensemble in future work. More broadly, they state future work will extend toward personalized cognitive modeling and autonomous orchestration of defense policies.","Release a reproducible software package (or containerized pipeline) for trigger instrumentation, log parsing, and sensor inference to enable independent replication and benchmarking. Evaluate robustness under more realistic operational conditions (unknown parameters, imperfect logging, adaptive human adversaries, and defenders co-evolving), including longitudinal studies to test learning/adaptation by attackers. Use factorial/ablation-style experimental designs to disentangle effects of individual triggers, trigger density, and placement from general deception/noise effects. Extend cognitive sensing to multivariate/sequence models with calibrated uncertainty, and validate sensor outputs against independent psychological measures (e.g., standardized bias/risk-taking instruments) to strengthen construct validity.",2512.00098v1,https://arxiv.org/pdf/2512.00098v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:19:47Z
TRUE,Optimal design|Other,Parameter estimation|Cost reduction|Other,A-optimal|D-optimal,Variable/General (examples: 4 parameters in analytic test; 3–4 hemodynamics parameters in aorta test),Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper proposes an optimal experimental design (OED) framework to construct undersampling masks for MRI k-space acquisition that are tailored to estimating specific inverse-problem parameters directly from k-space data. Designs are built by optimizing Fisher-information–based criteria (A- and D-optimality) under a hard budget on the number of sampled frequencies, using combinatorial optimization (greedy selection and a restricted exchange algorithm). The approach is demonstrated on (i) an analytical 2D signal with known informative frequencies and (ii) a PDE-constrained aortic hemodynamics inverse problem (Navier–Stokes with Windkessel outlets), where the unknowns include inflow amplitude and/or Windkessel distal resistances. Across simulated noisy experiments (SNR≈15), OED-optimized masks yield substantially lower parameter-estimation error and variance than conventional pseudo-random Gaussian or central “circle” masks, especially at high acceleration (R≈200), enabling ~10× acquisition-time speedup while maintaining accuracy. The study also reports that masks are fairly robust to moderate inaccuracies in the parameter values used to compute sensitivities, though very low budgets can be more sensitive to the optimization algorithm choice.","The design variable is a binary sampling mask $S\in\{0,1\}^N$ with budget constraint $\sum_i S_i=N_S$ and acceleration factor $R=N/(\sum_i S_i)$. The OED problem is $\hat S=\arg\min_S\Phi(S)$, where $\Phi$ is A-optimality $\mathrm{tr}(F^{-1})$ or D-optimality $\det(F^{-1})$ and $F$ is the Fisher information matrix built from k-space sensitivities. Sensitivities are formed from $\tilde Y_k(\theta_0)=H_F(A_k(\theta_0))\odot S$ and $\partial \tilde Y_k/\partial\theta_j$ (finite differences in the PDE case), yielding $F=\sum_{k=1}^{N_T}\sum_{i=1}^{N}\frac{2}{\sigma_y^2}\Re\big((G_k)^*(G_k)^T\big)$ with $G_k$ stacking vectorized sensitivities.","In both the analytical and aortic-hemodynamics simulations (complex Gaussian noise with SNR≈15), OED masks (A- or D-optimal) consistently reduce parameter-estimation error and variance relative to conventional pseudo-random Gaussian and central-circle masks at the same sampling budget. The authors report that optimal masks with very small per-slice budgets (e.g., $N_S=5$; acceleration $R\approx 200$) can outperform conventional masks even at much larger budgets (e.g., $N_S=50$), corresponding to >10× acquisition-time speedup while maintaining accuracy. For low budgets, the restricted exchange algorithm can outperform the greedy construction (lower error/variance), but it becomes far more computationally expensive in the PDE case (reported runtimes up to hours for the largest budget). Differences between A- and D-optimality are generally minor in the presented tests, and criterion values correlate well with achieved estimation error/variance.","The authors note the lack of validation on real MRI data as a limitation. They also point out that real acquisitions use multiple coils with nonuniform coil sensitivity maps, and that magnitudes/background phases can vary in space and time; their method assumes these quantities are known and would need adaptation/robustness checks when only estimates are available. They further highlight that the exchange optimization can be computationally unfeasible at higher resolution or with more parameters, and that designs may only hold within a range of parameter values used for sensitivity calculations.","The OED formulation is Fisher-information based and therefore locally optimal around an assumed parameter value $\theta_0$; performance may degrade under strong nonlinearity/multimodality or if the inverse solver converges to local minima, especially with very sparse masks. The comparison set of “conventional” masks is limited (Gaussian pseudo-random and central circle), omitting other widely used MRI trajectories/patterns (e.g., variable-density Poisson-disc, radial/spiral, CAIPIRINHA-like schemes) that could be stronger baselines. The PDE test case uses simulated ground-truth forward solves and additive proper complex Gaussian noise; model discrepancy, physiological variability, and temporal/spatial correlation in noise are not addressed and could affect both FIM accuracy and mask optimality. Practical constraints of MRI hardware/trajectory feasibility beyond selecting k-space lines/points (e.g., gradient constraints, eddy currents, SAR, navigator requirements) are not explicitly incorporated into the discrete mask optimization.","The authors propose integrating more advanced optimization methods (e.g., improved exchange initialization, probabilistic/stochastic binary optimization, or continuous relaxations with rounding) to improve scalability and solution quality. They suggest exploring additional OED criteria (e.g., c-criterion and criteria accounting for differing parameter scales) and incorporating parameter uncertainty into the design (robust/Bayesian OED). They also plan to extend the approach to real MRI data, including modeling/using coil sensitivity maps and handling time/space variability in magnitude and background phase.","A valuable extension would be to enforce MRI-feasible sampling constraints directly (e.g., line/trajectory constraints, variable-density structure, hardware gradient limits) so optimized masks can be deployed without post hoc modification. Developing self-starting or adaptive/sequential mask design—updating designs online using interim parameter/posterior estimates—could reduce reliance on a fixed $\theta_0$ and improve robustness. Broader benchmarking against state-of-the-art undersampling patterns and reconstruction-free parameter-estimation pipelines would clarify practical gains. Finally, releasing a reproducible implementation (code and parameter files) and testing across multiple anatomies/pathologies would strengthen translational evidence.",2512.06712v1,https://arxiv.org/pdf/2512.06712v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:20:35Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Other,Optimization|Other,Minimax/Maximin|Bayesian A-optimal|Other,1 factor (binary treatment assignment),Healthcare/medical|Theoretical/simulation only|Other,Exact distribution theory|Approximation methods|Other,FALSE,None / Not applicable,Not applicable (No code used),https://arxiv.org/abs/2302.03117,"The paper studies adaptive experimental design for treatment choice with two (binary) treatments under a fixed budget T, where the goal is to recommend the treatment with the larger mean outcome at the end of the experiment. It proposes a two-stage Neyman allocation (TSNA) procedure: an initial uniform allocation stage to estimate treatment-specific standard deviations, followed by an allocation stage that assigns treatments in proportion to the estimated standard deviations (Neyman allocation), and finally selects the treatment with the larger sample mean. The main theoretical contribution is to prove that this single adaptive design is asymptotically minimax-optimal and Bayes-optimal for simple regret, with upper bounds matching derived minimax and Bayes lower bounds including exact constants. The analysis covers mean-parameterized canonical exponential-family outcome models (sub-Gaussian) and uses change-of-measure arguments to derive lower bounds and CLT/large-deviation tools to establish matching upper bounds. Practically, the results justify variance-adaptive Neyman allocation (implemented in two stages) as an optimal fixed-budget design for welfare-maximizing treatment choice under both worst-case and prior-averaged criteria.","The TSNA design targets the Neyman allocation ratio $w^* = \sigma_1/(\sigma_1+\sigma_0)$, estimated after a first-stage uniform allocation via $\hat\sigma^2_{d,rT}$ and $\hat w_{rT}=\hat\sigma_{1,rT}/(\hat\sigma_{1,rT}+\hat\sigma_{0,rT})$. In stage 2, treatment 1 is assigned with probability $\hat\pi_{rT}=\tilde\pi_{1,rT}/(\tilde\pi_{1,rT}+\tilde\pi_{0,rT})$ where $\tilde\pi_{1,rT}=\max\{\hat w_{rT}-r/(1-r)^2,0\}$ and $\tilde\pi_{0,rT}=\max\{1-\hat w_{rT}-r/(1-r)^2,0\}$. The final decision rule is $\hat d_T=\arg\max_{d\in\{0,1\}}\hat\mu_{d,T}$, and regret is $\mathrm{Regret}_\mu=\mathbb{E}_\mu[\mu_{d^*}-\mu_{\hat d_T}]=\Delta_\mu\,\mathbb{P}_\mu(\hat d_T\neq d^*)$ with $\Delta_\mu=|\mu_1-\mu_0|$.","The paper derives a minimax lower bound $\inf_\delta\liminf_{T\to\infty}\sqrt{T}\sup_{\mu\in M^2}\mathrm{Regret}_\mu^\delta\ge (\sigma_1+\sigma_0)\Phi(-1)$ (with $\sigma_d^2=\sup_{\mu\in M}\sigma_d^2(\mu)$) and shows TSNA attains the matching upper bound, implying asymptotic minimax optimality with the same constant. It also derives a Bayes lower bound $\inf_\delta\liminf_{T\to\infty}T\int \mathrm{Regret}_\mu^\delta\,dH(\mu)\ge \tfrac14\sum_{d\in\{0,1\}}\int h_d(\mu\mid \mu_{\setminus d})(\sigma_1(\mu)+\sigma_0(\mu))^2\,dH_{\setminus d}(\mu)$ and shows TSNA achieves the corresponding matching upper bound (asymptotically, with $r\to 0$ for Bayes). The proofs use change-of-measure lower bounds (à la Kaufmann et al.) and CLT/large-deviation bounds to control misidentification probability under local alternatives $\Delta_\mu=O(1/\sqrt{T})$.","The paper notes that direct Neyman allocation is infeasible because standard deviations are unknown, motivating the two-stage procedure with a tuning split ratio $r\in(0,1)$; Bayes optimality is stated “as $r\to 0$” (Corollary 6.4), indicating dependence on the staging/tuning choice. It also emphasizes that the outcome space/parameter space must be chosen to satisfy regularity (e.g., Bernoulli means bounded away from 0/1 so variance is nonzero and Fisher information exists), which restricts admissible parameter regions.","The core results are for two arms; extension to multiple treatments (K>2) is nontrivial because optimal allocation/decision boundaries and lower bounds change, and variance-proportional allocation may no longer be minimax/Bayes optimal for regret with exact constants. The design assumes i.i.d. outcomes (no interference, stationarity) and sub-Gaussian/exponential-family regularity; performance under heavy tails, skewness, heteroskedasticity beyond the specified variance function, or time-varying outcomes is unclear. Practical deployment issues (delayed outcomes, covariates, noncompliance, ethics constraints) are not addressed, and the TSNA stage-1 “block” allocation (first treat=1 then treat=0) may be undesirable when there are temporal trends or learning effects unless randomized within stage.","The paper explicitly notes that sequential (rather than two-stage) estimation of the ideal allocation ratio can improve finite-sample performance and suggests applying/connecting results from adaptive ATE-estimation work (e.g., Kato et al. 2020; Cook et al. 2024; Dai et al. 2023; Neopane et al. 2024; Noarov et al. 2025) to the treatment-choice setting as an important future direction.","Developing a fully sequential/self-normalized version of variance-adaptive Neyman allocation (without a fixed first-stage fraction r) with finite-sample regret guarantees and constant-sharp asymptotics would strengthen practical applicability. Extending the minimax/Bayes constant-matching theory to K-armed treatment choice, contextual/covariate-adaptive experiments, and settings with delayed or batched feedback would broaden impact. Robust variants handling heavy-tailed outcomes (e.g., via median-of-means or Catoni estimators) and dependent data (autocorrelation, nonstationarity) could make the design reliable in more real-world experiments, and open-source reference implementations would facilitate adoption.",2512.08513v1,https://arxiv.org/pdf/2512.08513v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:21:05Z
TRUE,Other,Parameter estimation|Screening|Model discrimination|Prediction|Other,Not applicable,Variable/General (example reported: 29 influence factors; 12 design points in Cuban Missile Crisis script),Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://anonymous.4open.science/r/FSTS-DE1E,"The paper proposes an LLM-driven framework that automates experimental design for multi-agent social simulations using a film-production metaphor: a Screenwriter Agent generates candidate experimental “scripts,” Director Agents verify scientific validity/feasibility and select a final script, and an Actor Factory instantiates LLM-powered agents to execute the experiment. The experimental script is formalized as a structured plan containing goal, influence factors, response variables, and parameterized design points, with input/output controls and JSON formatting constraints to improve reliability. The framework is evaluated on a 13-day Cuban Missile Crisis simulation where the selected script includes 29 influence factors, 4 response variables, and 12 design points; agent decisions and outcomes are compared to historical records using semantic similarity measures. A counterfactual experiment (forcing “Kennedy” to be consistently tough) shifts the simulated outcome from peaceful resolution to limited conflict, illustrating sensitivity to design changes. Additional scenarios (digital services market, Meituan delivery) are used to demonstrate generalization, including mention of using orthogonal experimental design to assign factor levels in at least one script.","The experimental script is defined as $S=\langle G,I,R,D\rangle$, where $G$ is the experimental objective, $I$ the set of influence factors, $R$ the response variables, and $D$ the parameterized design points. Script selection is performed by a Chief Director using a weighted score: $\text{Score}(S_i)=\sum_{j=1}^6 w_j a_{i,j}$ over six criteria (scientific soundness, implementation difficulty, controllability, risk/robustness, requirement alignment, ethics/compliance), and the final script is $S=\arg\max_{S_i}\text{Score}(S_i)$. Actor agents are specified as $\langle P,I,K,G\rangle$ (intrinsic attributes, assigned influence factors, knowledge set, role goals) to execute the script in simulation.","In the Cuban Missile Crisis benchmark, the system generated four candidate scripts; the Chief Director selected Script 2 with a score of 83.5. Script 2 contained 29 influence factors, 4 response variables, and 12 experimental design points. Semantic similarity between simulated agent decisions and historical leadership actions was 53.50 using Sentence-BERT and 73.40 using GPT-5-mini; the final simulated outcome was “peaceful resolution, but tense relations,” matching historical records. In the counterfactual setting where “Kennedy” always stayed tough, similarity was 52.88 (Sentence-BERT) and 66.30 (GPT-5-mini), and the outcome shifted to “limited conflict—local military confrontations occurred, but no escalation to full-scale war.”",None stated.,"The work frames “experimental design” as automated script generation for simulations, but it does not specify classical DOE properties (randomization, blocking, replication plans, aliasing/confounding structure) or provide formal guarantees about identifiability or estimator variance under the proposed design points. Reported use of “orthogonal experimental design” is not detailed (e.g., which orthogonal arrays, factor levels, or balance criteria), making reproducibility and comparison to standard DOE methods difficult. Evaluation is largely scenario-driven and uses semantic similarity metrics, which may not directly validate causal identification or statistical efficiency of the experimental designs.",None stated.,"A clear next step is to formalize the design-point construction step with explicit DOE strategies (e.g., orthogonal arrays, fractional factorials, or optimal designs) and quantify properties like confounding control and estimation precision. Extending evaluation to broader settings (multiple historical cases, varying noise/autocorrelation in agent behavior, unknown/estimated baseline parameters) and reporting sensitivity to LLM model choices would strengthen robustness claims. Providing a standardized benchmark suite and an open-source implementation that logs full scripts/design matrices would enable rigorous comparative studies and replication.",2512.08935v1,https://arxiv.org/pdf/2512.08935v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:21:43Z
FALSE,Computer experiment|Other,Prediction|Other,Space-filling,Variable/General (examples include 2 factors; 4 factors; 6 factors; and simulations from d=10 up to d=500; mentions scaling to d=1000 for memory comparison),Manufacturing (general)|Semiconductor/electronics|Environmental monitoring|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Python|MATLAB|C/C++|Other,Not provided,https://drive.google.com/file/d/1XLQTd0XdqQPQ3f5jLM1aJsb3eL2lO5Eh/view|https://depmap.org/portal/data_page/?tab=allData&releasename=CCLE%202019&filename=CCLE_RPPA_20181003.csv,"The paper proposes the Robust Local Gaussian Process (RLGP), a local (nearest-neighbor) GP surrogate modeling framework aimed at accurate prediction on multidimensional response surfaces with abrupt jumps/discontinuities and heterogeneous neighborhoods. RLGP robustifies local GP fitting by introducing a sparse mean-shift (outlyingness) vector for local responses and optimizing it under an 10 constraint, combined with a multivariate perspective transformation that stabilizes concomitant covariance/scale estimation. It develops a scalable block coordinate descent algorithm with iterative quantile-thresholding to estimate the sparse outlier adjustments and GP hyperparameters, with a single main tuning parameter (trimming level q) and an adaptive q selection via MAD/Tukey rule. Empirical evaluation includes four real datasets (CNT yield, compressive-sensing STEM image intensities, corrosion sensor currents, and cancer proteomics) and synthetic high-dimensional simulations; RLGP is consistently competitive or best in predictive accuracy and probabilistic scoring, while remaining computationally feasible up to hundreds of dimensions. The work advances the local/partitioned GP literature by directly downweighting conflicting boundary-adjacent points within neighborhoods rather than relying on explicit global partitioning schemes.","RLGP replaces the standard local GP likelihood with a robust objective using S=Sigma^{1/2}: (1/2)(y-1\mu-\gamma)^T S^{-1}(y-1\mu-\gamma) + (c_0/2)\mathrm{Tr}(S), subject to \|\gamma\|_0\le q and \Sigma=\nu I + C(\theta_0,\vartheta). The covariance uses a squared-exponential kernel c(x_i,x_j)=\theta_0\exp(-\vartheta\|x_i-x_j\|^2). Posterior prediction at x^* is Gaussian with mean \mu(x^*)=\mu + c_*^T\Sigma^{-1}(y-1\mu-\gamma) and variance \sigma^2(x^*)=c(x^*,x^*)-c_*^T\Sigma^{-1}c_* (plug-in with estimated parameters).","On real datasets, RLGP achieves (MSE, CRPS, time per test point) of: Nanotube (1.05, 0.08, 0.32s), CSImage (0.14, 0.29, 0.41s), Corrosion (0.68, 0.41, 0.30s), and Cancer (1.33, 0.63, 0.05s). In high-dimensional simulations using LHS with n_train=10d for d=1000, RLGP attains the lowest MSE among compared scalable baselines (DeepGP, BNN) across tested dimensions (e.g., at d=25 MSE 5.22 vs DeepGP 6.54 and BNN 7.08), with runtimes roughly 0.20.4s per test point on CPU. The jump GP baselines (JGP-L/Q) are slow and fail to scale beyond d=10 in reported experiments. Adaptive-q trimming yields strong performance across interior and boundary scenarios and is reported to be not highly sensitive to exact q values.",None stated.,"Despite discussing experimental pipelines and sequential design motivation, the paper does not propose or evaluate a DOE strategy; nearest-neighbor subdesigns are formed from available data rather than designed runs. Several reported comparisons rely on specific software stacks (GPU-accelerated deep models vs CPU-only RLGP) and chosen hyperparameter settings, which can affect fairness and reproducibility without released code. The robustness mechanism depends on selecting neighborhood size n and trimming level q (even if adaptive), and performance may vary under strong input-dependent noise, autocorrelation, or very sparse boundary sampling beyond the evaluated settings.","The authors state they will extend RLGP to downstream tasks including active learning, model calibration, and sensitivity analysis to further enhance surrogate modeling for complex high-dimensional systems.","A natural extension is to integrate RLGP directly into sequential experimental design (e.g., discontinuity-aware acquisition functions) and compare against standard Bayesian optimization/active learning baselines. Additional work could study theoretical properties and robustness under non-i.i.d. simulator noise, correlated errors, and misspecified kernels, as well as develop open-source implementations (and possibly GPU/parallel versions) to improve reproducibility and adoption.",2512.12574v1,https://arxiv.org/pdf/2512.12574v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:22:24Z
TRUE,Sequential/adaptive|Other,Optimization|Parameter estimation|Prediction|Other,Not applicable,"Variable/General (concentrations, circuit geometries, and flow velocities/flow-rate ratios; CNC vs CNF; single vs double sheath flow)",Other,Other,TRUE,None / Not applicable,Not provided,https://formlabs.com/fr/blog/sla-dlp-impression-3D-comparee/?srsltid=AfmBOoqTREGOzr_krZsn6v1DRNbi8L2BxtYuic-jn66_o3F1UYu_L-sR,"The paper develops and experimentally tunes a millifluidic flow-focusing process to align nanocellulose (CNC or CNF) in a central stream using external sheath flows, and extends it to a double flow-focusing setup where a secondary xyloglucan (XG) sheath promotes self-assembly/gelation to form biomimetic hydrogel filaments. The authors vary key process factors—nanocellulose type and concentration, sheath/central flow rates (extensional shear), and channel/circuit geometry—using glass tubing for simple FF and optimized transparent MSLA 3D-printed circuits for double FF. In-situ polarized optical microscopy (POM) is used to qualitatively assess orientation via birefringence color maps, showing higher birefringence with higher concentration and larger sheath-to-core flow-rate differences, consistent with literature. The optimized 3D-printed square-channel circuit (1.65 mm width) enables clear POM observation and produces ~0.35 mm wide liquid filaments that gel in a water tank, though the gel filaments are currently too fragile to handle out of water. The work is positioned as a process/circuit experimental design platform intended for future optimization (including AI-based tools) of fibre architecture and mechanical performance.",Not applicable (no formal DOE model/response equation or optimality criterion; tuning is assessed qualitatively via POM birefringence as a proxy for nanocellulose orientation).,"Qualitatively, birefringence intensity increases with nanocellulose concentration and with increased extensional shear (larger difference between sheath and suspension flow rates), with especially organized birefringence patterns reported for 6 wt% CNC. In double flow-focusing, example flow rates are reported as 0.9 mL/min (water), 0.24 mL/min (CNC), and 0.9 mL/min per side (XG), yielding a focused liquid filament of about 0.35 mm width in 1.65 mm square channels. The optimized Formlabs Form 4 + Clear Resin V5 plus polishing/varnish finishing reduces background birefringence and improves sharpness for in-situ POM. Gelled filaments are obtained after exit into a rotating water tank but are fragile and break when handled outside water.","The initial glass-tube setup does not allow a second flow-focusing step simultaneously with microscopic observation, motivating the move to 3D-printed circuits. Early MSLA prints suffered from high background birefringence of the circuit material and insufficient optical sharpness, requiring machine/resin selection and surface finishing (polish + varnish). The produced gel filaments are currently too fragile to be handled outside water without breaking.","The study does not describe a formal DOE plan (factor ranges/levels, randomization, replication, blocking) nor quantitative response metrics for orientation or mechanical properties; conclusions are largely qualitative from POM images. Reported results may be sensitive to uncontrolled factors (temperature, residence time, rheology/aging, aggregation state despite sonication/filtration, and pump stability), which are not systematically analyzed. The work does not yet link process factors to downstream mechanical performance of dried fibres, limiting practical optimization guidance.","Next, the authors plan to promote stronger gelation and initiate drying to obtain dry biomimetic fibres. They intend to optimize velocities, circuit geometry, and suspension concentrations for faster/stronger gelation, and to vary XG molecular weight (tunable via ultrasonic treatment). They also propose changing the reception tank solvent to promote drying, and running sensitivity trials over these parameters to reach mechanically performant dry fibres.","Introduce a quantitative orientation metric (e.g., birefringence retardation calibration, image-derived order parameter, SAXS/WAXS) and couple it with mechanical testing after controlled drying to enable response-surface/optimal design studies. Add replication and statistical modeling to separate effects of concentration, flow-rate ratio, and geometry (including interaction effects) and to improve robustness/reproducibility. Develop/open-source CAD + control software workflows (and potentially Bayesian/sequential optimization) for automated exploration of geometry and flow conditions under constraints like clogging and filament integrity.",2512.13220v1,https://arxiv.org/pdf/2512.13220v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:23:00Z
TRUE,Supersaturated|Screening|Other,Screening|Parameter estimation|Other,E-optimal|Other,"Variable/General (two-level designs with n runs; starting OA has q=n−1, n−2, or n−3 columns; augmented designs have m=q(q+1)/2, m=q(q+1)/2−1, m=q(q−1)/2, or m=2q−1 columns depending on construction)",Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops theoretical results for constructing two-level supersaturated screening designs via Wu’s method of adding partially aliased two-factor interaction columns to a two-level orthogonal array (OA). Extending Bulutoglu and Cheng (2003), it proves E(s^2)-optimality (minimizing average squared correlations among columns) for augmented designs when the starting design is any OA(n,q,2,2) with q=n−1, n−2, or n−3, under partial-aliasing/no full-aliasing conditions. The authors derive closed-form expressions for sums of squared J-characteristics (orders 3 and 4) for these OAs (and related column-deletion cases), then use them to compute E(s^2) exactly and show it attains known lower bounds in several constructions. They also analyze variants (dropping one column; using only interaction columns; using interactions involving one parent factor) and identify which are optimal, asymptotically optimal, or not optimal. The contribution is primarily methodological/theoretical, enlarging the catalog of provably E(s^2)-optimal supersaturated designs and providing a unifying J-characteristics/GWP-based proof framework.","The screening model is the first-order main-effects model $Y=b_0+\sum_{i=1}^f b_i x_i+\epsilon$, with a two-level design supersaturated when $n<f+1$. The optimality target is the $E(s^2)$ criterion (Booth & Cox), defined as the average of squared off-diagonal elements of $X^\top X$ for the (intercept-free) model matrix $X$. The paper uses $J$-characteristics $J_s(S)=\sum_{i=1}^n \prod_{j\in S} c_{ij}$ and the identity $A_s^g=\frac{1}{n^2}\sum_{|S|=s} J_s(S)^2$ to express $E(s^2)$ for Wu-augmented designs as a function of sums of squares of $J_3$ and $J_4$ terms (with multiplicity 6 each in $X^\top X$ when all two-factor interactions are added).","For starting arrays $H(n,q)$ that are OAs with $q=n-1, n-2,$ or $n-3$, the design formed by augmenting $H$ with all its two-factor interaction columns (yielding $m=q(q+1)/2$ columns) is proven $E(s^2)$-optimal by achieving the Das et al. (2008) lower bound. A second family, formed by augmenting with all interactions but deleting one column ($m=q(q+1)/2-1$) is also shown $E(s^2)$-optimal for $q=n-1$ and $q=n-2$. Using only interaction columns ($m=q(q-1)/2$) is $E(s^2)$-optimal for $q=n-1$ and $q=n-2$, and asymptotically $E(s^2)$-optimal for $q=n-3$ (gap to the lower bound tends to 0 as $n\to\infty$). The ‘single parent factor’ augmentation ($m=2q-1$) is $E(s^2)$-optimal only for $q=n-1$ and is proven non-optimal for $q=n-2$ and $q=n-3$.",The authors note that extending the methodology to starting designs with fewer than $n-3$ columns would require establishing additional results on the relevant $J$-characteristics (analogous to Lemmas 1 and 2) before $E(s^2)$-optimality can be examined in those cases.,"The results are purely theoretical and restricted to two-level supersaturated designs constructed via Wu-style augmentation with partially aliased interaction columns; they do not address how practitioners should analyze data from these SSSDs (e.g., variable selection, effect heredity, or Bayesian/shrinkage modeling), which is central in supersaturated practice. The optimality notion is narrowly $E(s^2)$; no robustness checks are provided for other criteria (e.g., prediction-based criteria, model-uncertainty criteria) or for non-idealities like missing runs, non-normal errors, or correlated responses. Conditions such as “no fully aliased columns/partial aliasing of interactions” may be nontrivial to verify or enforce in applied settings, but practical construction/verification guidance and tooling are not provided. No computational search comparisons are included to quantify how these constructions perform versus other modern SSSD families beyond the theoretical bound comparisons.","They suggest the methodology could be extended to examine $E(s^2)$-optimality for starting designs with fewer than $n-3$ columns, contingent on deriving new lemmas for the corresponding sums of squared $J$-characteristics.","Developing practical construction algorithms (and software) that, given $n$ and desired $m$, output the specific OA-derived Wu-augmented design while checking partial-aliasing constraints would improve usability. Extending proofs and constructions to settings with unknown/estimated effects under effect-sparsity priors (e.g., Bayesian $E(s^2)$ analogs or criteria tied to variable selection performance) would better connect design optimality to analysis goals in supersaturated experiments. Studying robustness of these designs under correlation/heteroscedasticity or under model misspecification (e.g., presence of interactions violating the assumed screening model) would strengthen applied relevance. Multilevel or mixed-level supersaturated analogs, and/or multivariate response extensions, would broaden the scope beyond two-level factors.",2512.14378v2,https://arxiv.org/pdf/2512.14378v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:23:34Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Cost reduction|Other,A-optimal|V-optimal|Other,Variable/General (latent preference parameter θ ∈ R^d; experiments use CLIP embeddings with d=768),Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes ED-PBRL, an optimal-experimental-design (OED) approach for selecting preference queries to efficiently personalize generative models via preference-based reinforcement learning. It formulates query selection as maximizing information about a latent linear reward parameter θ using the (regularized) Fisher information matrix (FIM), and derives a tractable θ-agnostic surrogate objective by approximating softmax choice probabilities as uniform and rewriting the expected FIM in terms of state visitation (occupancy) measures. The resulting scalarized design criterion uses classical optimal design scalarizations (A/V-design variants based on traces involving the inverse information matrix) and is optimized with a Frank–Wolfe/Convex-RL procedure that yields K exploration policies. Empirically, ED-PBRL is evaluated on (i) synthetic ground-truth preference models and (ii) an LLM-simulated preference oracle (GPT-4.1-mini) for text-to-image style personalization, showing fewer preference queries are needed than random query selection. The work advances active/query-efficient preference learning for personalization by connecting PBRL policy selection to tractable OED over visitation measures with theoretical concavity/convergence guarantees in tabular settings.","Preferences are modeled with a multinomial logit: $P(x_q\text{ best})=\exp(\theta^{*\top}\phi(x_q))/\sum_{k=1}^K \exp(\theta^{*\top}\phi(x_k))$. Parameters are fit via regularized MLE: $\hat\theta=\arg\max_\theta \sum_{t,h,q} y_{t,h,q}\log p(q\mid t,h,\theta)-\frac\lambda2\|\theta\|_2^2$. The OED objective maximizes a scalarization of the (approximate) regularized information matrix: $\arg\max_{d_{1:K}\in\mathcal D} s\big(T\sum_{h=1}^H \tilde I_h(d^h_{1:K})+\lambda I_d\big)$ with $\tilde I_h(d^h_{1:K})=\Phi^\top\big(\frac1K\sum_{q=1}^K\mathrm{diag}(d_q^h)-\bar d^h\bar d^{h\top}\big)\Phi$ (uniform $p\approx 1/K$ surrogate).","On synthetic ground-truth preference vectors (e.g., Sunny/Medieval/Technological styles built from CLIP text embeddings), ED-PBRL yields lower cosine error and lower preference-prediction error than random exploration as interaction episodes increase. In the LLM-simulated preference study with $K=4$, $H=6$, and regularization $\lambda\in\{0.1,1,10,100\}$, ED-PBRL maintains roughly \~60% held-out accuracy at $\lambda=100$ across training sizes while random exploration degrades more with less data; the random-guess baseline is $1/K=25\%$. At 50 training episodes and $\lambda=100$, average held-out accuracy reported is 60.4% (ED-PBRL) vs 51.1% (random), a +9.4 percentage-point gain (with per-style gains up to +28.3pp for Cyberpunk). Design optimization for a ~5000-token vocabulary is reported to take ~10 minutes on a single NVIDIA A100 GPU.","The paper notes that using ML algorithms that influence data collection can generate long-term biases in collected datasets and should be used cautiously, and that a comprehensive ethics framework would be needed before deployment. It also states the study is a proof-of-concept and that bias aspects require further investigation before any deployment, which is out of scope. For the experiments, it explicitly uses LLM-simulated preferences rather than human participants (to avoid human-subject concerns), implying results may not fully reflect real human feedback.","The θ-agnostic design surrogate relies on approximating choice probabilities by $1/K$, which may be inaccurate when options are not symmetric/diverse or when θ is far from the prior assumption; performance could depend strongly on this approximation. The method is developed primarily for finite-horizon MDPs with known transitions and linear-in-features rewards; robustness to model misspecification, nonlinearity, and realistic preference noise/miscalibration is not fully established. Reported experiments are limited to a specific prompt-construction MDP with CLIP embeddings and include an LLM proxy; more extensive human studies and broader baselines (other active preference learning/query selection methods) would be needed for practical validation. Implementation details (software stack) and code are not provided, which can hinder reproducibility and fair benchmarking.","The paper suggests an adaptive variant of ED-PBRL: collect preferences in batches, update $\hat\theta$, then re-optimize visitation measures/policies, replacing the uniform $p\approx 1/K$ approximation with an estimated $\hat p(q\mid h)$ (e.g., computed from $\hat\theta$) to refine intermediate designs. It also indicates the evaluation protocol is directly transferable to future human studies, implying a natural next step is running human-participant experiments rather than LLM-simulated preferences.","Developing principled Bayesian OED versions that integrate over a prior on θ (rather than using the uniform $1/K$ surrogate) could improve design robustness and interpretability. Extending ED-PBRL to settings with unknown/estimated dynamics, continuous/high-dimensional state spaces, and non-i.i.d. (correlated) feedback would broaden applicability to real generative-model interaction logs. Providing self-starting/online variants with computational budgets and safety constraints (e.g., avoiding low-quality or harmful queries) would be important for deployment. Releasing reference implementations and benchmarking against modern active preference learning methods (e.g., information gain, BALD-style criteria, IDRL-like approaches) on shared human-feedback datasets would strengthen empirical conclusions.",2512.19057v1,https://arxiv.org/pdf/2512.19057v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:29:34Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Optimization|Cost reduction|Other,A-optimal|E-optimal|Not applicable|Other,"Variable/General (design variables are sensor locations/sampling times/view angles; budgeted M points; examples include m=2–200 time points, M=1–100 pixels, and |w|=10 or 45 CT angles)",Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,Other,Not provided,NA,"The paper proposes Neural Optimal Design of Experiments (NODE), a single-level learning framework for optimal experimental design in inverse problems that jointly trains (i) a neural reconstruction model and (ii) a fixed-budget set of continuous design variables representing measurement locations (e.g., sensor positions, sampling times, CT view angles). By parameterizing the design directly as M locations rather than weights over a dense candidate grid, NODE enforces sparsity “by construction” and avoids ℓ1-penalty tuning and bi-level optimization. The method supports continuous design spaces and introduces an adaptive/sequential extension that conditions future measurement choices on previously collected data. NODE is validated on (1) an analytically tractable exponential-growth regression benchmark where endpoint-optimal A-optimal designs are derived in closed form, (2) MNIST sparse pixel sampling for reconstruction and digit classification, and (3) a sparse-view X-ray CT example using LoDoPaB data; across these settings it reports improved reconstruction/classification relative to random and variance-based baselines and yields non-uniform, structure-aware CT angle allocations.","Core NODE objective (single-level, fixed-budget design): minimize over network parameters θ and design locations w the expected reconstruction loss plus optional data-consistency term: $\min_{\theta, w\in W}\,\mathbb{E}_{x,\varepsilon}\|\hat x-x\|_2^2+\gamma\|P_w(y(\hat x))-P_w(d(x,\varepsilon))\|_2^2$, with $\hat x=\Phi_\theta(P_w(y(x)+\varepsilon))$ (Eq. 8). Adaptive NODE conditions on past measurements by augmenting the network input and optimizing only new design points: $\hat x=\Phi^{\text{adapt}}_\theta(P_w(y(x)+\varepsilon), P_{w_{i-1}}(y_{i-1}))$ and minimizing the corresponding conditional risk (Eqs. 10–11). For the exponential-growth benchmark, the A-optimal criterion reduces to minimizing $\mathrm{tr}((A(t)^TA(t))^{-1})$, yielding endpoint-only optimal designs and an optimal split with $k\approx m(\sqrt2-1)$ points at $t=1$.","For the exponential-growth benchmark, the authors prove any A-optimal design places all sampling times at the endpoints $t\in\{0,1\}$ and the optimal fraction at $t=1$ converges to $\sqrt2-1\approx0.4142$ (and $2-\sqrt2\approx0.5858$ at $t=0$); NODE empirically recovers this structure for design sizes $m=2,\ldots,200$ with learned endpoint fractions clustering around these limits. In MNIST experiments with sensor budgets $M\in\{1,10,\ldots,100\}$ and noise $\sigma=0.05$, NODE designs yield lower test reconstruction error distributions than random or “highest-variance” fixed designs for both MSE and max-norm losses, and higher digit-classification accuracy across budgets. In sparse-view CT on LoDoPaB, NODE learns non-uniform projection-angle sets (tested at 10 and 45 angles) that improve filtered backprojection reconstructions versus uniformly spaced angles and, in an adaptive setting (adding 5 angles per stage for 3 stages), successive NODE-selected angles improve reconstructions in a case-specific way.","The authors note that the reconstruction network architecture $\Phi_\theta$ depends on the sensor budget $M$, so exploring different budgets typically requires retraining, which can be computationally expensive for budget–performance studies. They also state that their interpolation strategy for discrete design spaces borrows from image processing and is not intrinsic to OED, motivating the development of interpolation schemes tailored to discrete design optimization. Finally, they remark that extending NODE (and related single-level methods) to D-optimality is not directly addressed because it would require access to the full posterior rather than point estimates.","Performance and learned designs depend on the choice/capacity of the reconstruction network and training setup (optimizer settings, epochs, batch sizes), so comparisons can be sensitive to architecture/hyperparameter selection and may not isolate the design method alone. The method relies on differentiability (or smooth interpolation) of the measurement operator with respect to design variables; for many real sensor-placement problems, physical constraints and discrete admissible sets may make this relaxation inaccurate or may introduce projection/rounding artifacts. Results are demonstrated on selected tasks (MNIST sampling and a specific CT pipeline); broader baselines from classical OED (e.g., greedy mutual information, Fisher-information-based design, or modern Bayesian OED approximations) are not comprehensively benchmarked head-to-head in all examples.","They propose using autoencoders/latent representations to reduce dependence of $\Phi_\theta$ on the sensor budget and amortize training across multiple budgets. They suggest developing interpolation approaches specifically tailored to discrete design optimization and link this to semi-infinite programming and mollifier-based smoothing as potential principled alternatives. They also identify extending NODE to D-optimality as future work, suggesting generative models (diffusion models or conditional normalizing flows) for posterior sampling or a two-stage workflow combining a trained generative model with simulation-based inference for design optimization.","Developing self-starting or uncertainty-calibrated variants (e.g., with conformal or Bayesian neural reconstructions) could quantify design-induced uncertainty and improve robustness to prior/model mismatch. Extending NODE to constrained and multi-objective settings (e.g., hardware motion limits, minimum angular separation, or joint reconstruction-and-detection objectives) would increase applicability in medical imaging and sensor placement. Providing an open-source reference implementation and standardized benchmarks (including computational cost and wall-clock comparisons) would facilitate reproducibility and fair comparison with classical and Bayesian OED methods.",2512.23763v2,https://arxiv.org/pdf/2512.23763v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:30:37Z
TRUE,Optimal design|Bayesian design|Other,Parameter estimation|Other,E-optimal|Other,Variable/General (examples include single measurement position β and single scalar parameter ϵ; general formulation allows C measurement points and multi-parameter ϵ),Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper investigates optimal experimental design (OED) for inverse problems using the explicit constraint force method (ECFM), where model parameters are chosen to minimize the magnitude of auxiliary “constraint forces” that enforce agreement between the model and measurement data. It reviews standard Fisher-information-based OED and the E-criterion (maximizing the minimum eigenvalue) and then derives an analogous deterministic E-criterion viewpoint via a Gauss–Newton Hessian approximation and a prior over parameters. The main methodological contribution is proposing a constraint-force-based OED objective: maximize the minimum eigenvalue of the expected Hessian (over a prior) of the squared constraint-force norm with respect to model parameters. Through analytic toy PDE examples (a 1D boundary value problem), the study finds that constraint-force OED tends to place sensors in the stiffest regions (e.g., at a Dirichlet boundary), yielding impractical designs under noise/finite precision. The paper concludes provisionally that ECFM is not a viable basis for OED, despite ECFM’s usefulness for certain inverse problems when models may be misspecified.","Standard OED uses the E-criterion on a Fisher-information or Gauss–Newton matrix, e.g. $\arg\max_{\beta}\,\min(\mathrm{eig}(I(\epsilon;\beta)))$ with $I(\epsilon)=\frac{1}{\sigma^2}\sum_i \partial_\epsilon f(x_i)\,\partial_\epsilon f(x_i)^\top$, and a Bayesian/prior-averaged form $\int I(\epsilon;\beta)\,\rho(\epsilon)\,d\epsilon$. ECFM inverse problems are posed as $\min_{\theta,\epsilon,\lambda}\tfrac12\|\lambda\|^2$ s.t. $R(\theta;\epsilon)+\Gamma(\beta)\lambda=0$ and $M(\beta)\theta-V(\beta)=0$. The proposed constraint-force OED objective is $\arg\max_{\beta}\min\big(\mathrm{eig}(\int \nabla^2_{\epsilon}\tfrac12\|\lambda(\epsilon,\beta)\|^2\,\rho(\epsilon)\,d\epsilon)\big)$, where $\lambda(\epsilon,\beta)$ is obtained from a coupled PDE/measurement linear system.","On analytic 1D examples with a single sensor location $\beta\in[0,1]$, constraint-force-based OED can become indifferent to $\beta$ (parameterized boundary traction case) or push the optimum to the stiffest location $\beta=0$ (parameterized source term and misspecified-model cases). In contrast, standard Fisher-information sensitivity typically favors locations where the state response is largest/most sensitive (often near $\beta=1$ in the examples). For the parameterized material case, the constraint-force optimum jumps to a boundary ($\beta=0$ or $\beta=1$) depending on problem parameters (e.g., traction/load ratios), indicating non-smooth dependence of the optimal design on settings. These findings support the paper’s conclusion that constraint-force E-criterion OED yields impractical sensor placements under realistic noise/precision constraints.","The examples assume full access to the data-generating process in advance (the true response $v(\beta)$ is treated as known), which the author notes is unrealistic and removes the practical motivation for OED. The paper also notes that constraint-force-optimal designs rely on measurements where responses are very small (stiff regions), making them impractical with measurement noise and finite precision. The conclusion is described as provisional, indicating the approach and objective choice may be the issue rather than ECFM itself.","The evaluation is limited to highly idealized, mostly 1D analytic examples (often single-parameter, single-measurement), so behavior in realistic multi-parameter, multi-sensor PDE inverse problems is not demonstrated. The proposed OED objective relies on Hessians (and in implementation would require high-order sensitivities/third derivatives), which may be computationally prohibitive or numerically unstable for large-scale PDE solvers; no scalable approximation strategy is validated. Comparisons are primarily conceptual (constraint-force criterion vs Fisher-information criterion) rather than comprehensive benchmarking against established OED criteria (D-, A-, I-optimality) or robust/noise-aware formulations that might mitigate boundary-attraction behavior. Practical constraints (sensor exclusion zones, minimum signal-to-noise requirements, cost/feasibility constraints) are not incorporated into the optimization, which could materially change the optimal designs.","The author suggests that the E-criterion may be the wrong objective for ECFM-based OED and that alternative, more “exotic” approaches to improving or reformulating the constraint-force minimization problem might be explored. The paper implies further investigation is needed to determine where measurements should be placed for ECFM inverse problems if one were to use ECFM in practice.","Explore noise-aware or signal-to-noise-constrained ECFM design criteria (e.g., enforce minimum response magnitude or incorporate measurement precision explicitly) to prevent designs collapsing to stiff/zero-response regions. Study alternative OED criteria for ECFM (e.g., prediction-error-based or discrepancy-weighted objectives) and compare against D-/A-/I-optimality under consistent and misspecified models. Develop computationally tractable approximations to avoid third-order sensitivities (e.g., adjoint-based Hessian-vector products, randomized trace/eigenvalue methods, surrogate modeling). Validate on higher-dimensional PDE inverse problems with multiple sensors/parameters and realistic constraints/costs, including real or high-fidelity simulated case studies.",2601.04557v1,https://arxiv.org/pdf/2601.04557v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:31:20Z
TRUE,Optimal design|Sequential/adaptive|Bayesian design|Other,Prediction|Parameter estimation|Optimization|Other,D-optimal,"Variable/General (design variables are sensor locations/displacements; example uses 3 sensors with 2D displacements per sensor → 6 design variables per stage, over N=4 stages)",Environmental monitoring|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a scalable sequential Bayesian optimal experimental design (SBOED) framework for PDE-governed inverse problems with infinite-dimensional random-field parameters by casting design selection as a finite-horizon Markov decision process and learning an amortized design policy via policy-gradient reinforcement learning. Utility is based on a Laplace-approximated Bayesian D-optimality criterion, computed from dominant generalized eigenvalues of a Gauss–Newton Hessian, enabling information-gain rewards without full posterior sampling. To make policy training and reward evaluation feasible, the authors introduce dual dimension reduction (active subspace for parameters; PCA for states) and a derivative-informed latent attention neural operator (LANO) surrogate that predicts both the PDE solution map and its Jacobian. They additionally propose an eigenvalue-based reward evaluation strategy that uses prior samples as proxies for MAP points to avoid repeated MAP optimizations in the RL loop. In a contaminant source-tracking sensor-placement case study, the surrogate-accelerated approach achieves about 100× speedup versus high-fidelity FEM, learns interpretable “upstream” sensor placement policies, and outperforms random placements (win rate >97%).","Sequential design maximizes expected cumulative information gain (MDP objective), with D-optimality rewards under a Laplace posterior: terminal utility is approximated as $\mathrm{Dopt}(\mu_{post}\|\mu_{prior}) \approx \tfrac12 \sum_{j=1}^r \log(1+\lambda_j)$, where $\lambda_j$ are dominant generalized eigenvalues from $H_{GN}(m_{MAP}) w_j = \lambda_j C_{prior}^{-1} w_j$. The MAP point is defined by minimizing a negative log-posterior objective combining data misfit and prior regularization, and the posterior covariance is approximated as $(H_{GN}+C_{prior}^{-1})^{-1}$ with low-rank structure. Designs $d_n$ (sensor displacements/placements) are selected sequentially by a learned policy $\pi_n(x_n)$ operating on the design/observation history state $x_n=[d_{1:n-1},y_{1:n-1}]$.","In the contaminant source-tracking case study, the LANO surrogate attains average relative errors of about 2.13% for the reduced state and 2.42% for the reduced Jacobian on a test set. Surrogate-accelerated D-optimality matches FEM closely (reported relative RMSE about 1.29% over 500 test cases), while eigenvalue computation is accelerated from 13.46 s (FEM) to 0.14 s (LANO), yielding ~100× speedup for that step. Using the proposed proxy strategy (evaluating curvature/eigenvalues at the prior sample instead of solving for MAP) achieves ~0.44% relative RMSE and correlation ~0.99 versus the MAP-based D-optimality across tested configurations. Learned policies consistently outperform random sensor placements with >97% win rates across multiple prior draws and exhibit an interpretable upstream movement strategy, which flips appropriately under a reversed flow field.",None stated.,"The demonstrated design setting is a specific PDE case study (advection–diffusion with Gaussian prior/noise and Laplace/Gauss–Newton approximations); performance and accuracy may degrade for strongly non-Gaussian posteriors, model discrepancy, or heavier-tailed/heteroscedastic noise where D-optimality under Laplace is less reliable. The method’s effectiveness depends on surrogate fidelity for both the forward map and Jacobian; surrogate error can bias reward estimates and learned policies, especially near early times where Jacobian errors are highest. The paper does not document publicly available implementation details (e.g., exact RL training environment, mesh/PDE solver settings beyond summary), which may hinder reproducibility and practical adoption.","The authors suggest extending the approach to more complex nonlinear and multiphysics models, to non-Gaussian priors and likelihoods, and to robust objectives that account for model discrepancy. They also mention incorporating hard design constraints and enabling online adaptation of the policy and surrogate under distribution shift.","Developing uncertainty-aware or risk-sensitive reward evaluation that propagates surrogate error into the design utility (e.g., Bayesian surrogate/ensembles) could improve reliability of learned policies. Extending the framework to multivariate/multi-output observation designs with correlated noise and to settings with temporal/process autocorrelation would broaden applicability. Providing open-source software and standardized benchmarks (including ablations for proxy-MAP, rank truncation, and dimension reduction choices) would strengthen reproducibility and clarify when the approach is most advantageous.",2601.05868v1,https://arxiv.org/pdf/2601.05868v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:32:02Z
TRUE,Optimal design|Sequential/adaptive|Other,Parameter estimation|Prediction|Other,D-optimal|A-optimal|E-optimal|Other,Variable/General (design is a discrete sensor trajectory/path of length n on a navigation mesh; examples use n=7 and n=19 nodes; mesh sizes 75 and 332 nodes; also considers k-th order Markov policy order k=3 or 5),Environmental monitoring|Other,Simulation study,TRUE,Python,Public repository (GitHub/GitLab),https://gitlab.com/ahmedattia/pyoed|https://web.cels.anl.gov/∼aattia/pyoed/|http://energy.gov/downloads/doe-public-access-plan,"The paper proposes a probabilistic, trajectory-based optimal experimental design (OED) framework for mobile sensors on a navigation mesh, formulating path design as a discrete graph optimization and then converting it to a stochastic optimization over parameters of a Markov policy. The approach treats the OED utility as a black box and uses policy-gradient estimators (score-function gradients) with variance reduction via an optimal baseline, enabling optimization without differentiating the utility or enforcing smooth relaxations. Three policy families are developed: a first-order (memoryless) Markov chain policy and two higher-order Markov policies based on the Raftery mixture-of-lags model and a generalized lag-dependent transition model, with analysis of computational trade-offs. Numerical experiments on a PDE-based advection–diffusion Bayesian inverse problem demonstrate that the learned policy samples near-optimal trajectories and explores the tail of the utility distribution, with results shown for D-optimality (log-det posterior covariance) and additional A- and E-optimal criteria. Implementations are stated to be available through the PyOED software framework.","The discrete path OED problem is posed as $\zeta_{\mathrm{opt}}\in\arg\max_{\zeta\in V^n}U(\zeta)$ on a directed graph, then replaced by policy optimization $p_{\mathrm{opt}}\in\arg\max_p\,\mathbb{E}_{\zeta\sim P(\zeta\mid p)}[U(\zeta)]$ with score-function gradient $\nabla_p\mathbb{E}[U]=\mathbb{E}[U(\zeta)\nabla_p\log P(\zeta\mid p)]$ estimated by Monte Carlo. The first-order trajectory policy factorizes as $P(\zeta)=P(\zeta_1)\prod_{t=1}^{n-1}P(\zeta_{t+1}\mid\zeta_t)$ with initial and transition probabilities modeled via odds $w=p/(1-p)$ and normalized to probabilities; higher-order variants use mixtures over lags $\sum_{i=1}^k\lambda_i P(\zeta_{t+1}\mid\zeta_{t+1-i})$ (and a generalized lag-dependent $P^{(i)}$). The OED utility in experiments includes $U(\zeta)=\log\det(\Gamma_{\text{post}}(\zeta))$ (D-opt), with $\Gamma_{\text{post}}=(F^*\Gamma_{\text{noise}}^{-1}F+\Gamma_{\text{pr}}^{-1})^{-1}$; parameter updates use projected stochastic gradient ascent/descent and an optimal control-variate baseline $b$ to reduce variance.","On a coarse mesh with 75 nodes and trajectory length $n=7$, the authors enumerate 307,200 feasible paths, reporting a unique global optimum with D-optimal objective value $-21714.37$; the first-order policy finds a near-optimal path with value $-21714.28$ (gap 0.09) and samples other near-optimal trajectories from the learned policy. Higher-order policies (orders $k=3$ and $k=5$) can be less stable when lag weights are optimized, and fixed/decaying lag weights (e.g., Eq. 3.19) are recommended; some higher-order settings achieve near-optimal gaps (e.g., about 0.10–0.25 in the coarse-mesh examples shown). Variance-reduction via the baseline substantially lowers stochastic gradient variance compared to no baseline (demonstrated by replicated error/variance plots), improving optimization stability. On a fine mesh with 332 nodes and $n=19$, higher-order policies generally achieve lower (better) D-optimal objective values than the first-order policy in the reported runs, with similar qualitative optimal regions for sensor paths; additional experiments cover groups of $s=7$ sensors and A-/E-optimal criteria.","Global optimality is not guaranteed; the method is expected to explore the tail of the utility distribution but may miss the true optimum. Higher-order policies can be unstable when lag weights are optimized and may require more iterations; they can also generate effective “jumps” that violate direct mesh connectivity unless lag weights are decayed/fixed, the order is kept small, or the mesh is refined. The utility is treated as a black box, so expensive utility evaluations can limit sample sizes, motivating (but not developing) acceleration via surrogates/randomization.","The empirical evaluation is focused on a specific PDE (advection–diffusion) with synthetic data and largely independent observation noise; robustness to model misspecification, strong noise correlations, or non-Gaussian/nonlinear inverse problems is not systematically benchmarked. Comparisons against established trajectory-OED or informative path planning baselines (e.g., gradient-based trajectory optimization, sampling-based planners with information objectives) are limited, making it hard to quantify advantages beyond the proposed policy family. The approach assumes a pre-defined navigation mesh and fixed path length, and scalability is discussed mainly for sampling/gradient computation rather than end-to-end cost dominated by repeated PDE solves for utility evaluation. Practical constraints such as dynamic obstacles, energy budgets, turn/smoothness constraints, and multi-agent collision avoidance are only indirectly handled (e.g., by penalizing the utility) and not explicitly built into the policy model here.","The paper notes that selecting the best policy model and Markov order could be automated using a model-choice/information criterion, which is left for future work. It also states that enforcing history-dependent constraints (e.g., no-visit constraints) is left for future work, and suggests path smoothness could be encouraged via utility penalties or by fitting smooth curves between coarse-mesh nodes. The authors also mention exploring acceleration techniques (randomization or surrogates) is beyond the current work.","Develop self-starting/online variants where the policy adapts during deployment as data arrive (closed-loop Bayesian experimental design) and evaluate regret vs. open-loop designs. Extend to richer constraints and dynamics: continuous-time vehicle dynamics, energy/time budgets, collision avoidance for multi-agent sensing, and explicit feasibility constraints that prevent higher-order ‘jump’ artifacts. Provide broader benchmarking against informative path planning and optimal control approaches across multiple inverse problems (nonlinear PDEs, non-Gaussian posteriors) and with realistic correlated/noisy measurements. Release a reproducible benchmark suite and packaged implementations of the proposed policies/optimizers (with examples and default hyperparameters) to facilitate adoption beyond PyOED power users.",2601.11473v1,https://arxiv.org/pdf/2601.11473v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:33:07Z
TRUE,Bayesian design|Sequential/adaptive|Optimal design|Other,Parameter estimation|Model discrimination|Prediction|Other,Bayesian D-optimal|Other,Variable/General (examples include 2D source location parameters; also discusses high-dimensional discrepancy parameters via neural networks/ensemble methods),Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper studies Bayesian experimental design (BED) for inverse problems with and without model discrepancy, comparing utility functions based on Kullback–Leibler (KL) divergence versus Wasserstein distances (W1/W2). It identifies a key limitation of Wasserstein utilities under bounded, non-informative (e.g., uniform) priors: the Wasserstein distance depends on the absolute location of posterior mass and can create “false rewards” (apparent information gain) when posterior mass shifts toward the boundary. Using a classical sequential source-inversion (convection–diffusion plume) problem, it finds KL-based BED converges faster when the forward model is accurate, while Wasserstein-based BED is more robust when model discrepancy is present because it avoids overconfident, mislocated posteriors and facilitates discrepancy correction. For discrepancy handling, it builds on an auto-differentiable ensemble Kalman inversion (AD-EKI) framework, alternating designs for low-dimensional physical parameters with designs/updates for discrepancy (error) parameters, including a re-update strategy that recomputes posteriors from the original prior using accumulated data after model correction. Computation of Wasserstein utilities is done via entropic-regularized discrete optimal transport (Sinkhorn) for differentiability and efficiency, and results are demonstrated primarily through numerical experiments and reward/posterior evolution plots across sequential stages.","BED is posed as maximizing expected utility over design variables: $d^* = \arg\max_{d\in\mathcal D}\,\mathbb E[U(\theta,y,d)]$ with joint $p(\theta,y\mid d)$ (Eq. 1). KL utility uses $U= D_{\mathrm{KL}}(p(\theta\mid y,d)\|p(\theta))$ and expected information gain $\mathrm{EIG}(d)=\int D_{\mathrm{KL}}(p(\theta\mid y,d)\|p(\theta))\,p(y\mid d)\,dy$ (Eqs. 2–3). Wasserstein utility uses $W_p$ (Eq. 4), with Gaussian closed form for $W_2^2$ (Eq. 5) and, for general posteriors, an entropic-regularized discrete OT approximation solved by Sinkhorn: $\min_{\gamma\ge0}\langle C,\gamma\rangle+\varepsilon D_{\mathrm{KL}}(\gamma\|ab^\top)$ subject to marginal constraints (Eq. 6), yielding $\widehat W_2=\sqrt{\mathrm{OT}^{(2)}_\varepsilon(\mu,\nu)}$ (Eq. 7).","In a 1D analytic example with a uniform prior on $[-A,A]$, the Wasserstein-2 distance for shifted symmetric point-mass posteriors increases with shift (Eq. 8), demonstrating location-driven inflation unrelated to information gain; KL against a uniform prior does not change with such shifts. In a 2D toy inverse example, Wasserstein reward versus design location exhibits a non-monotone pattern with a boundary-induced local maximum, implying Wasserstein-based BED may prefer boundary-near designs that produce misleading/truncated posteriors. In sequential convection–diffusion source inversion without discrepancy, KL-based designs yield faster posterior contraction (lower early-stage uncertainty), while Wasserstein-based designs can be biased toward boundary locations and require roughly one additional measurement stage to reach similar uncertainty in the demonstrated setup. With model discrepancy (parametric error in source strength), KL tends to produce overly concentrated but mislocated posteriors that slow correction, whereas Wasserstein utilities retain more mass near the truth and lead to faster convergence in both source location inference and discrepancy-parameter correction in the reported cases.","The authors note their discrepancy study considers only parametric error (not structural discrepancy), to focus on metric comparison and simplify interpretation. They also state that entropic regularization yields a biased approximation to the unregularized $W_2$, trading accuracy for speed, scalability, and automatic differentiation. For the alternating re-update/model-correction procedure, they explicitly state the attained solution is not guaranteed globally optimal and accuracy is constrained by measurement noise and the framework’s limitations; formal convergence criteria and denoising/regularization are left open.","The work appears to rely heavily on a specific inverse-problem benchmark (source inversion with convection–diffusion) and illustrative toy examples, so generality across other BED settings (different priors, unbounded domains, different observation operators, or complex likelihoods) is not fully established. The “false reward” issue is tied to bounded-support priors and boundary effects; practical workflows often use broader/support-extended priors or transformations, so the prevalence of the issue may depend on modeling choices that are not exhaustively explored. Wasserstein computation is via entropic-regularized OT on discretized/histogrammed samples, which can be sensitive to binning/jitter choices and regularization strength; robustness of conclusions to these numerical settings is not thoroughly characterized here. No publicly released implementation limits reproducibility and independent verification of the sequential design and discrepancy-correction pipeline.","They state that the difference between KL and Wasserstein in long-term or accumulated reward (beyond greedy, stage-wise design) is left for future work. They also state that formal convergence criteria and dedicated denoising/regularization strategies for the alternating re-update and model-correction scheme remain topics for future work.","A useful extension would be to evaluate translation-invariant/relative-translation-invariant Wasserstein variants as utilities to mitigate boundary/location-driven artifacts while retaining OT robustness under misspecification. Additional studies under unknown/noisy hyperparameters, autocorrelated observations, and more realistic structural model discrepancy (not just parametric error) would clarify robustness in practice. Benchmarking against other robust BED utilities (e.g., Jensen–Shannon, Hellinger, energy distance, sliced Wasserstein, or misspecification-robust/generalized Bayesian posteriors) would situate the findings more broadly. Providing open-source code and sensitivity analyses for OT regularization, discretization, and AD-EKI settings would materially improve reproducibility and practical adoption.",2601.16425v1,https://arxiv.org/pdf/2601.16425v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:33:56Z
TRUE,Sequential/adaptive|Computer experiment|Other,Prediction|Cost reduction|Other,Not applicable,"Variable/General (examples include m=2,3,5,8; initial design size 5m; batch size b=2,3,5; added points=20 in one-point comparisons)",Theoretical/simulation only,Simulation study|Other,TRUE,R,Not provided,https://CRAN.R-project.org/package=UniDOE,"The paper develops sequential experimental design methods for computer experiments modeled with Gaussian process (Kriging) surrogates, targeting improved global fit over the full input domain. It proposes two new one-point (batch size b=1) observation-based criteria: a gradient-based criterion that prioritizes regions with large (expected) gradient magnitude while maintaining distance from existing points, and a variance-based criterion that controls error via both predictive variance and gradient-estimation variance bounds. To address inefficiency of repeated single-point rounds, it also introduces a general batch sequential design framework (“cluster-based top-b”) that extends any one-point criterion to batch selection while avoiding point clustering near the global maximizer. Performance is evaluated via simulations on standard benchmark functions (Branin, Hartmann-3, Ackley-5, Zakharov and Rosenbrock at multiple dimensions), comparing RMSE and maximum absolute error against existing sequential criteria (EI variants, max variance, and mixture discrepancy space-filling). Results show the proposed methods generally outperform competitors: the gradient-based criterion tends to yield lower RMSE (average error), while the variance-based criterion tends to yield lower MAE (worst-case error), and the batch framework often matches or exceeds one-point sequential performance with fewer rounds.","The Kriging predictor and variance are $\hat y_X(x)=r_X(x)^\top K(X)^{-1}Y_X$ and $s_X^2(x)=\tau^2\big(k(x,x)-r_X(x)^\top K(X)^{-1}r_X(x)\big)$. The proposed gradient-based acquisition is $\phi_{\text{gra}}(x)=\sqrt{\mathbb{E}\|\nabla f(x)\|^2}\, d(x,x^*)+|\hat y_X(x)-f(x^*)|$, using $\mathbb{E}\|\nabla f(x)\|^2=\|\nabla \hat y_X(x)\|^2+\tau^2\,\mathrm{tr}\big(\nabla^2 k(x,x)-\nabla r_X(x)^\top K(X)^{-1}\nabla r_X(x)\big)$. The variance-based criterion is $\phi_{\text{var}}(x)=\min\Big(\sqrt{k(x,x)-r_X(x)^\top K(X)^{-1}r_X(x)},\;\sqrt{\sum_{i=1}^m g_i(x)}\, d(x,x^*)\Big)$ where $g_i(x)$ are diagonal elements of $\nabla^2 k(x,x)-\nabla r_X(x)^\top K(X)^{-1}\nabla r_X(x)$.","Across multiple benchmark functions, the proposed criteria are most often best or near-best among seven one-point sequential methods (EI0/EI1/EI2, max variance, MD space-filling, and the two proposed). In one-point design comparisons, $\phi_{\text{var}}$ frequently gives the smallest MAE (e.g., for Branin $f_1$, MAE 2.05 vs 3.07 for EI2 and 3.79 for max-variance), while $\phi_{\text{gra}}$ often gives the smallest RMSE (e.g., for Branin, RMSE 0.52 vs 0.72 for EI1/EI2). In batch settings (cluster-based top-b with b=2,3,5), designs using $\phi_{\text{gra}}$ or $\phi_{\text{var}}$ typically outperform other batch criteria in RMSE or MAE respectively, and the batch framework improves over a sequential MD baseline in many cases. The authors recommend $\phi_{\text{gra}}$ when minimizing RMSE (average error) and $\phi_{\text{var}}$ when minimizing MAE (maximum error), and generally recommend the cluster-based top-b batch approach when batch sampling is feasible.",None stated.,"The empirical evaluation is entirely simulation-based on smooth benchmark functions; results may not transfer to noisy, heteroskedastic, constrained, or expensive real simulators with model inadequacy. The methods require fitting/refitting Kriging models and optimizing acquisition functions (and, for batch selection, discretizing/candidate-set clustering), which may be computationally challenging in high dimensions; performance under higher-dimensional settings beyond the tested m up to 8 is unclear. The paper references R functions/packages used to generate designs and test sets, but does not provide reproducible code or implementation details for the proposed criteria and batch framework, limiting immediate adoption and verification.",None stated.,"Extend the criteria and batch framework to settings with observation noise, correlated errors, or nonstationary/heteroskedastic Gaussian processes, and study robustness to covariance misspecification and hyperparameter uncertainty. Develop scalable optimization and candidate-generation strategies (e.g., continuous optimization, multi-start, parallel/batched acquisition optimization) for higher-dimensional inputs and expensive acquisition evaluation. Provide an open-source implementation (e.g., an R/Python package) and validate on real computer experiments/engineering simulators, including wall-clock cost comparisons and guidance on choosing batch size b and clustering parameters (α, β).",2601.16431v1,https://arxiv.org/pdf/2601.16431v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:34:40Z
TRUE,Sequential/adaptive|Bayesian design|Optimal design|Computer experiment|Other,Screening|Prediction|Cost reduction|Other,Other,5 factors (+ laser power treated as separate fixed setting in runs),Manufacturing (general)|Energy/utilities|Transportation/logistics,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper formulates discovery of feasible metal additive manufacturing (DED) process settings as an AI-driven adaptive experimental design (active search) problem, where each configuration is labeled success/failure after printing and quality analysis. It proposes BEAM, a closed-loop, batch sequential strategy that trains a probabilistic surrogate classifier on accumulated experiments (initialized from 37 prior failed lab trials) and selects the next batch of configurations using a nonmyopic acquisition function that balances exploitation (high predicted feasibility) and exploration (expected future discovery value), with additional hard constraints from domain knowledge to prune the search space. The deployed case study targets printing the NASA copper alloy GRCop-42 on Inconel 718 at low laser powers; across four laser-power regimes (950W, 700W, 600W, 500W) BEAM searches a discretized space of >100 million configurations per regime. In real laboratory deployment, BEAM found at least one defect-free configuration for each laser power within a budget of 10 experiments per power (and three feasible configurations at 600W), achieving successful prints within ~3 months versus several months of manual trial-and-error with no successes. The work advances DOE practice in AM by emphasizing feasibility discovery in sparse, discontinuous spaces via adaptive, batch sequential design rather than classical response optimization.","The feasibility of a configuration is modeled as an unknown binary function $f(x)\in\{0,1\}$ over a $d$-dimensional parameter space $X$, and the goal is to maximize discoveries $\max_{|S|\le T}\sum_{x\in S} f(x)$. BEAM trains a probabilistic surrogate classifier giving $p_t(x)=P(f(x)=1\mid D_t)$, and selects the next configuration(s) by maximizing an acquisition function $\alpha_t(x)=p_t(x)+\beta_t(x)$, where $p_t(x)$ is exploitation and $\beta_t(x)$ approximates the nonmyopic exploration value via an expectation over future top feasibility probabilities after updating with outcome $y\sim\text{Bernoulli}(p_t(x))$.","The controllable process space comprises 5 parameters (feed rate, gas flow rate, Inconel thickness, scan speed, layer height) discretized to yield >100 million candidate configurations per fixed laser power setting. With experimental budget $T=10$ per laser power (batch size $B=2$), BEAM discovered feasible configurations for all tested powers: 950W (1), 700W (1), 600W (3), and 500W (1), i.e., at least one feasible setting within 10 trials for each regime. The baseline manual effort from collaborators comprised 37 attempted configurations with zero successes, indicating a large practical efficiency gain for the adaptive design approach. Feasible settings tended to use very high scan speeds and lower-than-normal layer heights per the authors’ deployment insights.",None stated.,"The acquisition/optimality objective is not tied to a standard, well-defined DOE criterion (e.g., D-/I-optimality) and the nonmyopic term is approximated with assumptions that may affect performance or reproducibility across problems. Evaluation is largely a single-lab case study without extensive ablations against strong adaptive baselines (e.g., Bayesian optimization variants for classification/level-set estimation, Thompson sampling, entropy search for feasibility), so generalization is uncertain. Implementation details are incomplete (e.g., exact constraint rules, candidate set generation, scaling/normalization choices, and how batches are selected jointly), and no code is provided, making independent replication harder.","The authors suggest extending the approach beyond a single composition to test many alloy compositions and leverage AI to accelerate broader alloy design, aiming to replace legacy alloys with improved designs. They also indicate that BEAM can be employed for other metal alloys and AM processes and that deployment insights can guide improved initialization and effectiveness in future use cases.","Provide open-source reference implementations and standardized benchmarks for AM feasibility discovery to enable reproducibility and fair comparison across adaptive design methods. Extend BEAM to handle autocorrelated/temporal drift, unknown/nuisance factors, and multi-fidelity signals (cheap proxies before expensive full quality analysis). Develop principled batch selection (joint, diversity-aware) and incorporate richer surrogate models (e.g., GP classification or calibrated ensemble methods) with explicit uncertainty calibration under severe class imbalance.",2601.17587v1,https://arxiv.org/pdf/2601.17587v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:35:28Z
TRUE,Other,Model discrimination|Parameter estimation|Other,Not applicable,Variable/General,Other,Simulation study|Other,TRUE,Other,Not provided,http://arxiv.org/ps/astro-ph/0305146v1|www.pas.rochester.edu/~wma,"The paper presents a designed high-energy-density laboratory experiment to study how a strong/hypersonic shock propagates through a highly inhomogeneous (“clumpy”) medium, motivated by astrophysical environments (molecular clouds, YSO outflows, planetary nebulae, AGN) and also relevant to foam targets in HED/ICF contexts. The experimental target concept uses an 8 ns, ~140 eV Z-pinch x-ray drive to accelerate a polystyrene layer across a vacuum gap into a carbon-foam layer, forming a blast wave whose rarefaction then drives a sustained shock through a downstream region containing randomly placed dense plastic rods embedded in low-density DVB foam. The design explicitly includes comparator configurations (no clumps at interclump density; and a uniform medium at the same average density as the clumpy region) to isolate the effect of inhomogeneity on shock dynamics. Two numerical codes are used in support of the design: HYADES radiation-hydrodynamics for drive/target coupling and AMRCLAW adaptive-mesh hydrodynamics for 2D simulations of the shock–multi-clump interaction and synthetic radiography. Simulations predict that, under the proposed target parameters, the shock traverses the clumpy medium faster than a smooth medium of equal average density (about 15.6% faster), implying that using averaged properties of inhomogeneous media can misestimate key dynamical timescales like shock crossing time.","Key shock–clump interaction timescales are defined as the shock crossing time $t_{SC}=2a_0/v_S$, the clump crushing time $t_{CC}=(\chi/(F_{c1}F_{st}))^{1/2} t_{SC}$ (with $\chi=\rho_c/\rho_a$), and the clump destruction time $t_{CD}=\alpha t_{SC}$ (typically $\alpha\approx 2$). A critical transverse separation for clump–clump interaction before destruction is given by $d_{crit}=2\left(a_0+v_{exp}(t_{CD}-t_{CC})\right)$, with $v_{exp}$ linked to the clump internal sound speed (and $\gamma=5/3$ targeted by ionizing/heating the clumps). The paper also expresses enhanced kinetic energy available to downstream clumps as a sum of post-shock flow and accelerated clump-debris contributions (their Eq. 7).","For the three simulated configurations (no clumps at interclump density; clumpy medium; and smooth medium at the same average density as the clumpy case), average shock speeds are reported as ~$57.1$ km/s, ~$51.95$ km/s, and ~$44.94$ km/s, respectively. Thus, the shock in the clumpy case propagates about $7.01$ km/s (15.6%) faster than in the smooth-average-density case, despite identical mean density, demonstrating a substantial dynamical impact of inhomogeneity. The predicted difference in shock arrival time at the backplate between the clumpy and smooth-average cases is ~6 ns, which is well above the stated diagnostic time resolution (~100 ps), indicating experimental discriminability. Synthetic radiographs at 5 keV and 10 keV show that the global shock and individual bow shocks/clump collapse should be observable with backlighting diagnostics.","The authors note the AMRCLAW simulations do not reach the fully converged-resolution regime for individual shock–clump interactions (each clump radius is resolved by ~16 zones, less than resolutions used in convergence studies), due to computational resource limits, though they argue it is sufficient for global shock-propagation properties. They also state that the experimental/target geometry provides limited downstream distance beyond the clumpy section, making it infeasible (in their matched simulations) to follow the longer-term evolution after the clump system breaks up and homogenizes. They further indicate some fine-scale density spikes are equation-of-state table artifacts and not physically important for the overall evolution.","The design is not a classical DOE framework: it does not specify a structured factor plan (e.g., factorial/optimal) for varying clump fraction, density ratio, size, and spacing, so inference about sensitivities and interactions would be limited unless additional shots are planned. The main AMRCLAW study is 2D slab-symmetric with rod-like clumps, which can differ materially from fully 3D clump/sphere behavior (turbulence, vorticity cascade, mixing rates, and shock morphology). The random placement is described, but the dependence of results on random realizations (replication/seed sensitivity) is not quantified; without multiple realizations, uncertainty in the predicted 15.6% difference could be under-characterized. Code and input decks are not shared, which limits reproducibility and independent validation of the target tuning and synthetic diagnostics.","The authors suggest the target design could be adapted to other HED facilities (e.g., high-intensity lasers) beyond the Z machine. They also propose that similar studies with morphologies better optimized to particular foam microstructures could improve understanding of foam equation-of-state and shock behavior (including the role of turbulent energy and history dependence) in HED/ICF-relevant settings. They imply additional experimental and simulation work is needed to further study how clumpy/inhomogeneous structure affects global dynamics and timescale estimates in astrophysical contexts.","A natural extension is a systematic parametric experimental campaign varying (at minimum) clump volume fraction, density contrast $\chi$, clump size distribution, and spatial correlation/spacing using a designed plan (e.g., fractional factorial or space-filling) to quantify main effects and interactions on shock speed and mixing. Running multiple random realizations per setting (or controlled spatial patterns) would allow uncertainty quantification of shock-arrival metrics and improve statistical confidence. A full 3D simulation/experiment with spherical clumps (or 3D-printed structures) would better match many astrophysical clump geometries and could change the interaction-regime boundaries and turbulence generation. Finally, providing open-source scripts/input decks for HYADES/AMRCLAW surrogate setups (or a simplified reproducible model) would substantially improve reproducibility and enable broader comparison with alternative hydro codes and radiation-transport treatments.",astro-ph/0305146v1,https://arxiv.org/pdf/astro-ph/0305146v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:36:01Z
TRUE,Optimal design|Sequential/adaptive,Parameter estimation|Cost reduction,D-optimal,1 factor (current pulse height / covariate x=I),Semiconductor/electronics,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper develops D-optimal experimental designs for switching measurements on superconducting Josephson junctions, where each current pulse produces a binary outcome (switch/no switch). The response is modeled with a complementary log-log (Gumbel-type) regression, derived from physical escape-rate arguments, and the authors derive the locally D-optimal design for estimating the two model parameters. They show (via known general results) that the D-optimal design is a two-point design, with optimal linear predictors at approximately z*1=0.9796 and z*2=-1.3377 (corresponding to response probabilities about 0.93 and 0.23), implying asymmetric placement on the response curve. For practical use, they propose a sequential design: a heuristic/binary-search procedure to obtain initial estimates ensuring MLE existence, followed by repeated updating using MLE and choosing the next two pulse heights according to the D-optimal rule. Performance is demonstrated with real switching data from an aluminum Josephson junction at 20 mK and with Monte Carlo simulations showing the sequential design approaches the (oracle) D-optimal design as sample size grows, enabling faster data acquisition in superconducting electronics experiments.","Binary response model (complementary log-log / Gumbel CDF): $P(Y=1)=1-\exp\{-\exp(ax+b)\}$ with linear predictor $z=ax+b$. The Fisher information has weights $g(z)=\frac{e^{2z}}{e^{\exp(z)}-1}$ and for a two-point design the determinant is proportional to $g(z_1)g(z_2)(z_1-z_2)^2/a^2$, maximized at $z_1^*\approx0.979633$ and $z_2^*\approx-1.33774$. The sequential procedure repeatedly estimates $(a,b)$ by MLE and sets the next pulse heights to $x_i=(z_i^*-\hat b)/\hat a$ (after an initial binary-search phase to secure feasible MLEs).","The derived D-optimal design is a two-point design with optimal linear predictors $z_1^*\approx0.979633$ and $z_2^*\approx-1.33774$, corresponding to response probabilities $F(z_1^*)\approx0.9303$ and $F(z_2^*)\approx0.2308$. In a real experiment (20 mK) the sequential design reached practically sufficient accuracy after about 20–30 stages (roughly 6,000–17,000 pulses total); a 50-stage run used 117,288 pulses and took 8 min 16 s, of which 1 min 30 s was computation. In simulations (500 runs, true $a=0.24,b=-61$), mean-squared errors under the sequential design were larger than an oracle D-optimal design for small $n$ but became close for larger $n$ (e.g., at $n=200$: MSE($\hat a$) 0.001771 vs 0.000667; MSE($\hat b$) 112.42 vs 43.46; gaps shrink further by $n\ge 1000$).","They note that choosing the number of measurements per stage was handled empirically and that a systematic time-optimal rule (accounting for per-measurement time and reconfiguration overhead) is non-trivial and likely only approximately solvable. They also acknowledge that more complicated Josephson-junction structures may require more flexible models than the presented complementary log-log regression, and that deriving optimal designs for exact physical models may be difficult due to complicated functional forms.","The D-optimal design is local (depends on the true/estimated parameters), so early-stage performance can be sensitive to poor initial estimates despite the proposed heuristic, and no formal robustness guarantees (e.g., minimax) are provided. The modeling assumes independent Bernoulli outcomes at fixed covariates and a constant switching rate during each pulse; in practice, temporal dependence, drift (nonstationarity), or pulse-to-pulse correlation could degrade optimality and MLE properties. The paper does not provide implementation details (software, numerical optimization settings) for the MLE and sequential updates, which can affect reproducibility and real-time feasibility across labs.","They suggest studying other optimality criteria beyond D-optimality, which may yield different designs but could be analytically challenging. They propose developing a more systematic method to choose the number of measurements per stage to minimize expected total experiment time given measurement and reconfiguration costs. They also suggest reducing total time by overlapping estimation and measurement (simultaneous processing), and extending modeling/design to more flexible parametric models or adaptive sequential designs for tracking parameter changes with environmental variables.","Developing robust (e.g., Bayesian or minimax) sequential designs that explicitly account for initial-parameter uncertainty could improve early-stage efficiency and reliability. Extending the approach to handle autocorrelated or drifting environments (self-starting charts/designs, state-space or change-point tracking) would better match real cryogenic experiments. Providing open-source reference implementations and benchmarking against alternative sequential design strategies (e.g., Thompson sampling/Bayesian optimal design, approximate I-optimality for prediction of the full curve) would improve reproducibility and practical adoption.",cond-mat/0610507v1,https://arxiv.org/pdf/cond-mat/0610507v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:36:44Z
FALSE,NA,NA,Not applicable,Not specified,Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper presents the mission and measurement architecture (“experimental design” in the mission-design sense) for LATOR, a proposed space experiment to measure relativistic light deflection near the Sun with extremely high precision. It describes a geometrically redundant measurement setup using two micro-spacecraft and a ~100 m baseline optical interferometer on the International Space Station, combined with laser ranging, to estimate post-Newtonian parameters (especially the Eddington parameter γ) and related effects (second-order deflection, solar quadrupole J2, and frame-dragging). Key elements include heterodyne interferometry, fiber-linked local oscillator distribution with metrology, and strategies to resolve fringe ambiguity using ISS orbital baseline-projection variability. The paper provides an error-budget-style performance discussion (pm-level path-length accuracy translating to ~10^-8 accuracy on γ) and outlines spacecraft/ISS instrument subsystems and operations. It is not a statistical design-of-experiments (DOE) methodology paper (no factorial/optimal design construction); rather, it is an experimental/mission design description in physics/space engineering.","Angle estimation in the heterodyne interferometer is given in forms such as $\theta=\arcsin\!\left(\frac{(2\pi n+\phi_1-\phi_2)\,\lambda}{2\pi b}\right)$ (Eq. 1) and with fiber metrology correction $\theta=\arcsin\!\left(\frac{(2\pi n+\phi_1-\phi_2+m_1)\,\lambda}{2\pi b}\right)$ (Eq. 2), where $b$ is baseline length, $\lambda$ laser wavelength, $n$ an integer fringe ambiguity term, $\phi_i$ measured phases, and $m_1$ metrology-measured phase variation. For two-spacecraft differential astrometry, a differential form is given (Eq. 3) combining fringe terms and phase differences across stations, exploiting common-mode cancellation of fiber-length changes.","The mission targets measurement of the PPN parameter $\gamma$ with accuracy ~1 part in $10^8$ (claimed ~3000× improvement over Cassini). The paper reports an expected differential interferometric delay accuracy of about ±5.4 pm, corresponding to an estimated $\sigma_\gamma \sim 0.9\times 10^{-8}$. It cites first-order solar deflection of 1.75 arcsec corresponding to ~0.85 mm delay on a 100 m baseline, with a conservative per-measurement delay accuracy target of ~10 pm (≈0.1 prad ≈0.02 μas). It also projects measurement of second-order deflection at ~2×10^-3 accuracy, frame dragging at ~10^-2, and solar quadrupole moment parameter $J_2$ to ~1 part in 20 (assuming $J_2\simeq 10^{-7}$).","The authors note that error-source evaluation and mitigation analysis is ongoing and that the performance/error-budget discussion is “for illustration purposes only,” with corresponding analysis and simulations to be published later. They also indicate that detailed mission covariance analysis combining measurement errors with orbit/velocity uncertainties is still under investigation and will be reported elsewhere.","Because the paper is primarily a concept/architecture description, many performance claims rely on assumed instrument stabilities and simplified link/SNR calculations rather than end-to-end validated simulations or experimental demonstrations in the exact operational environment (ISS dynamics, thermal gradients, stray light, operational constraints). The work does not provide reproducible computational details (inputs, models, or code) for the stated accuracy projections, limiting independent verification. From a DOE perspective, it does not formalize design selection/optimization (e.g., optimal scheduling of observations, robustness to model/parameter uncertainties) using statistical optimality criteria; mission geometry choices are argued qualitatively/engineering-wise rather than via explicit optimal design methodology.","They state intentions to further study the 3:2 Earth-resonant trajectory as the baseline and to investigate an option where the two spacecraft move in opposite directions during conjunctions to reduce dependence on solar impact-parameter knowledge, with results to be reported elsewhere. They also state they intend to publish detailed analysis and simulations of error sources and mitigation methods in subsequent publications, and to report detailed results of mission covariance analysis elsewhere.","A valuable extension would be a formal end-to-end covariance/optimal design study of observation scheduling and geometry (e.g., optimizing times, baseline projections, and ranging/angle measurement cadence) under realistic ISS dynamics and noise models, potentially using optimal experimental design criteria. Additional work could assess robustness to non-idealities (autocorrelated measurement noise, unmodeled systematics, outliers, intermittent link dropouts) and provide open-source simulation tools for independent validation. Prototype demonstrations (hardware-in-the-loop or on-orbit technology demos) and explicit trade studies comparing alternative interferometer configurations/baselines would strengthen credibility and guide practical implementation.",gr-qc/0410044v1,https://arxiv.org/pdf/gr-qc/0410044v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:37:21Z
TRUE,Supersaturated|Optimal design|Screening|Other,Screening|Cost reduction|Parameter estimation|Other,E-optimal|Other,"Variable/General (m two-level factors; focus on supersaturated designs with m > N-1, often m is a multiple of N-1; examples include N=10 with m=14,15; N=14 with m=17,18,19; N=20 with m up to 380)",Theoretical/simulation only,Other,TRUE,Other,Not provided,NA,"The paper develops construction methods for two-level supersaturated designs that are optimal under the Booth–Cox $E(s^2)$ criterion, which minimizes average squared column correlations (nonorthogonality) in the design matrix. It shows that attaining the Nguyen–Tang–Wu lower bound for $E(s^2)$ is equivalent to the existence of a balanced incomplete block design (BIBD) with specified parameters, and it provides BIBD constructions using difference families over finite fields to generate $E(s^2)$-optimal supersaturated designs with distinct blocks (no completely aliased columns). When the Nguyen–Tang–Wu bound cannot be achieved, the paper derives improved lower bounds for $E(s^2)$ that apply more broadly than earlier results, including cases where $N\equiv 2\pmod 4$ and the relevant multiplier is odd. The authors also report new optimal designs found via computer search that achieve the improved bounds, including new 10-run and 14-run $E(s^2)$-optimal designs. Overall, the contribution advances theory and construction of near-orthogonal screening designs when runs are fewer than factors, linking optimality to combinatorial design structures and providing practical constructions.","A two-level supersaturated design is represented by an $N\times m$ matrix $X\in\{-1,1\}^{N\times m}$ with balanced columns; nonorthogonality is measured via $X^TX$ with entries $s_{ij}=(X^TX)_{ij}$. The objective criterion is $E(s^2)=\sum_{i<j}s_{ij}^2/\binom{m}{2}$, and the Nguyen–Tang–Wu bound is $E(s^2)\ge \dfrac{m-N+1}{(m-1)(N-1)}N^2$. The paper also uses the identity $\mathrm{SS}(X^TX)=\mathrm{SS}(XX^T)$ (sum of squares) and the relation $E(s^2)=[\mathrm{SS}(X^TX)-mN^2]/[m(m-1)]$ to derive improved lower bounds.","The paper provides explicit difference-family-based constructions (Theorems 2.1–2.3) that yield BIBDs and hence $E(s^2)$-optimal supersaturated designs achieving the Nguyen–Tang–Wu bound for many parameter sets, with attention to ensuring distinct blocks (no duplicated columns up to sign). It derives improved lower bounds for $E(s^2)$ when the Nguyen–Tang–Wu bound is not attainable (Theorem 3.1), covering cases not handled by Butler et al. (2001), particularly when $N\equiv 2\pmod 4$ and the quotient parameter is odd. Using computer search (citing Nguyen’s Gendex), the authors report new optimal designs, including examples such as $N=10,m=14$ with $E(s^2)=5.0549$, $N=10,m=15$ with $E(s^2)=5.5238$, and $N=14,m=17$ with $E(s^2)=4.9412$ (plus additional 14-run cases).",None stated.,"The work is primarily combinatorial/theoretical and targets two-level, balanced-column supersaturated designs under effect-sparsity assumptions; it does not address robustness of optimality under model violations (e.g., active interactions, non-sparsity). Practical guidance for analysis after running such designs (e.g., estimation/selection methods, false discovery control) is not a focus, despite screening being the motivating use case. The computer-search component is referenced but not fully reproducible because implementation details and code are not provided, and comparisons across competing construction algorithms are limited.","The paper notes that whether the Nguyen–Tang–Wu bound can be achieved for every admissible multiple of $N-1$ in the relevant congruence classes appears difficult and remains unknown, indicating an open problem about existence in full generality.","Extending the constructions and bounds to settings with nonregular runs (e.g., mixed-level factors), constraints, or split-plot/hard-to-change factors would broaden applicability. Developing self-contained, reproducible software implementations (and benchmarks versus modern search/optimization methods) would increase practical uptake. Further work could study performance under nonideal conditions (unknown/estimated noise variance, correlated errors, or non-sparse effects) and connect $E(s^2)$-optimality to downstream screening accuracy under contemporary variable-selection models.",math/0410090v1,https://arxiv.org/pdf/math/0410090v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:37:47Z
TRUE,Supersaturated|Optimal design|Screening|Other,Screening|Parameter estimation|Cost reduction,Other,"Variable/General (explicit constructions for SSDs with m potentially much larger than N; e.g., SSD$(s^n, s^m)$ with $m=k(s^n-1)/(s-1)$, $m=2(s^n-1)/(s-1)-1$, etc.)",Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),www.stat.ucla.edu/~hqxu/|preprints.stat.ucla.edu/,"The paper develops theory and explicit algebraic constructions for optimal multi-level supersaturated designs (SSDs), i.e., designs with fewer runs than needed to estimate all main effects, intended for factor screening under effect sparsity. Using the generalized minimum aberration (GMA) framework (generalized wordlength pattern $A_1,A_2,\ldots$), the authors derive a new lower bound on $A_2$ for balanced multi-level SSD$(N,s^m)$ designs and give conditions for attaining it (row coincidence counts differ by at most one). They then provide general construction methods—motivated by Addelman–Kempthorne orthogonal array constructions—using linear/quadratic polynomials over finite fields and additive characters to analytically establish optimality and aliasing properties, including classes without fully aliased columns. The paper also extends results to mixed-level SSDs, deriving a lower bound for mixed-level $A_2$ and proposing a replacement method to build optimal mixed-level SSDs from multi-level ones. Tables list many small optimal 3-, 4-, and 5-level SSDs and compare them to existing designs under alternative nonorthogonality criteria (e.g., ave/max $\chi^2$, ave/max $f$).","The GMA criterion minimizes generalized wordlength pattern components $A_j(D)$, where $A_j(D)=N^{-2}\sum_k\left|\sum_{i=1}^N x^{(j)}_{ik}\right|^2$ for the orthonormal-contrast model matrix $X_j$. For SSD$(N,s^m)$ designs, the paper relates $A_2$ to row-coincidence moments $K_t(D)$ and derives a new lower bound: $A_2(D)\ge \frac{m(s-1)(ms-m-N+1)}{2(N-1)}+\frac{(N-1)s^2}{2N}\eta(1-\eta)$, where $\eta$ is the fractional part of $m(N-s)/((N-1)s)$; equality holds iff all row coincidence counts differ by at most one. For mixed-level SSD$(N,s_1\cdots s_m)$ designs, a bound is given: $A_2\ge (\sum_{k=1}^m s_k-m)(\sum_{k=1}^m s_k-m-N+1)/(2(N-1))$.","A new tight (in many cases) lower bound on $A_2$ is established for balanced multi-level SSD$(N,s^m)$ designs, with a sharp attainability condition based on near-constant pairwise row coincidences; any design attaining it is GMA-optimal. For prime-power $s$, the authors prove existence of broad classes of optimal SSDs achieving the bound, including for $N=s^2$ and any number of factors $m$, and show a periodicity property for the minimum achievable $A_2$ as $m$ grows when saturated OA$(N,t,s,2)$ exist. Explicit constructions are provided via finite-field polynomial labeling (linear/quadratic) that yield optimal SSDs often without fully aliased columns; projected $A_2$ values and their frequencies are derived analytically for several construction families. The paper also derives a mixed-level $A_2$ lower bound and shows how replacement with saturated OAs preserves overall $A_2$ and can yield mixed-level SSDs meeting that bound.",None stated.,"The work is primarily theoretical and construction-focused; it does not provide comprehensive empirical guidance on downstream analysis (e.g., variable selection/modeling) under effect sparsity, nor robust performance under model violations (non-sparsity, strong interactions, nonlinearity) beyond GMA-based proxy arguments. Constructions rely on finite-field/prime-power level structures and balanced columns; while broad, this may limit direct applicability for arbitrary run sizes/level counts without additional transformations. Comparisons to existing designs are limited to a few small-run cases and a subset of criteria (e.g., ave/max $f$, max $\chi^2$), and no software implementation is provided for practitioners.","For mixed-level SSDs, the authors note that it remains unclear whether mixed-level SSDs achieving the mixed-level $A_2$ lower bound (Theorem 10) are optimal under GMA, indicating a need for further development on GMA optimality in the mixed-level setting.","Developing practical algorithms/software to generate these algebraic SSDs for user-specified constraints (forbidden combinations, blocking, split-plot structure) would broaden adoption. Extending the constructions and optimality theory to accommodate correlated errors/autocorrelation, non-balanced columns, or quantitative-factor modeling (response surface screening) would improve real-world robustness. More systematic benchmarking tied to modern sparse-model selection methods (e.g., LASSO/SCAD with interactions) could clarify practical gains beyond nonorthogonality summaries and projected $A_2$ distributions.",math/0603079v1,https://arxiv.org/pdf/math/0603079v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:38:26Z
FALSE,NA,NA,Not applicable,Not specified,Other,Simulation study|Other,TRUE,MATLAB,Not provided,www.cs.nyu.edu/overton/software/hifoo|www.compleib.de,"This document is a collection of control design computational experiments demonstrating the MATLAB package HIFOO for fixed-order H∞ controller design and related optimization goals. It walks through several benchmark control problems (static output feedback stabilization, spectral abscissa minimization for a two-mass–spring system, a NASA HiMAT aircraft robust MIMO H∞ design, the four-disk system, and examples illustrating controller order drop and non-proper H∞ optima). Performance is assessed by achieved H∞ norms and closed-loop pole locations, with repeated randomized optimization runs and warm-starting from previous controllers to improve results. Comparisons are made against standard Robust Control Toolbox routines (e.g., mixsyn, hinfsyn) and against reported best results in the literature, highlighting cases where low-order controllers found by HIFOO are competitive. The work is not about statistical design of experiments; “experiments” refer to computational control-design trials and benchmarks rather than DOE methodology.","Key problem instances include the two-mass–spring open-loop transfer function $P(s)=\frac{1}{s^4+2s^2}$ and the minimum-sensitivity example with (known) non-proper optimal controller $K(s)=5-\frac{5}{6}s$. Controller synthesis is performed by calling HIFOO to optimize objectives such as the closed-loop $H_\infty$ norm (via augmented plants built with functions like augw/mktito) or the spectral abscissa (maximum real part of closed-loop eigenvalues). Closed-loop poles are evaluated from the feedback interconnection (e.g., via eig(feedback(P,-K)) or equivalently eig(A+B*K*C) in state-space).","For the two-mass–spring benchmark, HIFOO finds second-order controllers with closed-loop spectral abscissa improved through restarts (reported values progressing to about −0.7572), close to an analytic local optimum of $\alpha=\sqrt{15}/5\approx0.7746$ (i.e., target abscissa −0.7746). In the NASA HiMAT example, Robust Control Toolbox mixsyn yields a 10th-order controller with achieved performance 0.7885, while HIFOO obtains a 3rd-order controller with performance 0.9897 and similar time/frequency response characteristics. In the four-disk example, literature reports best 8th-order performance 1.1272, while HIFOO produces low-order controllers with improved norms versus reduction methods (e.g., order 2: 1.2438; order 1: 1.4256). In the order-drop example, HIFOO approaches the optimal $H_\infty$ value (~21.5284) and illustrates near pole/zero cancellation leading effectively to reduced controller order.","The author notes that results (e.g., stabilizing static output feedback gains) can differ across runs because HIFOO’s optimization algorithms are randomized. It is also stated that controllers obtained by spectral abscissa optimization can be non-robust because clustered eigenvalues are very sensitive to perturbations. For the minimum-sensitivity example, it is explicitly noted that allowing non-proper controllers could address coefficient blow-up, but non-proper controllers are “currently not implemented in hifoo.”","The document provides benchmark demonstrations but does not offer a systematic experimental protocol (e.g., fixed random seeds, number of runs, or statistical summaries) to quantify variability or reliability of the randomized solver across problems. Comparisons to competing methods are selective and problem-specific, and computational cost/runtime is not reported, making it hard to assess efficiency trade-offs for practitioners. The examples assume correct modeling and do not examine robustness to modeling error, noise, constraints, or implementation issues beyond the H∞ framework, which may limit direct transfer to real-world experimental validation.","The text suggests that it could be possible to allow for non-proper controllers in HIFOO (to avoid coefficient blow-up in problems where the optimum is non-proper), but notes that this capability is not currently implemented. No other explicit future research directions are stated.","A natural extension would be to add explicit support for non-proper controller parameterizations (and appropriate regularization) to better handle problems where the H∞ optimum is non-proper or involves poles/zeros tending to infinity. More systematic benchmarking could be done by reporting distributions over many randomized starts (success rates, variability of achieved norms, runtimes) and by providing reproducible scripts with fixed seeds. Packaging the examples as a public benchmark suite and adding interfaces to modern environments (or providing open repositories for scripts/data) would improve reproducibility and adoption.",math/0609028v1,https://arxiv.org/pdf/math/0609028v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:38:59Z
TRUE,Factorial (full)|Other,Parameter estimation|Prediction|Screening|Other,Not applicable,"Variable/General (examples include 4 factors in a replicated 2^4, 5 factors in an unreplicated 2^5; also a 5-factor 3×2×4×10×3 and a 3-factor 2×3×3)",Semiconductor/electronics|Other,Simulation study|Other,TRUE,R|None / Not applicable|Other,Not provided,NA,"The paper advocates regression tree models (GUIDE and LOTUS) as interpretable modeling tools for data from designed experiments, including replicated and unreplicated complete factorial designs. It shows how tree-based piecewise constant and piecewise (simple/multiple/stepwise) linear models can represent interactions as differences in conditional main effects, often yielding more intuitive interpretations than hierarchical ANOVA/GLM terms. For replicated 2-level factorials, the approach builds a nested sequence of piecewise linear models and selects a parsimonious tree via cross-validation to minimize prediction mean squared error (PMSE), comparing against IER/EER significance-based model selection and stepwise AIC. For unreplicated factorials (where error variance is not estimable), GUIDE provides an alternative to half-normal plot methods and Lenth-type procedures, while respecting effect heredity through tree structure. Extensions are demonstrated for non-normal responses via piecewise Poisson regression trees (count data) and piecewise logistic regression trees (proportions), with examples showing that key interacting factors are revealed by early splits and that fitted models can be simpler to interpret while maintaining or improving predictive fit.","PMSE is defined as $\mathrm{PMSE}=\sum_i \mathbb{E}(\hat\mu_i-\mu_i)^2$ over design points. Traditional model selection criteria discussed include $\mathrm{AIC}=n\log(\tilde\sigma^2)+2\nu$ for stepwise selection among hierarchical ANOVA models. GUIDE constructs recursive partitions of the predictor space (e.g., conditions like $X\le c$ or $X\in A$) and fits within-node models constrained to main effects at most, selecting tree size by cross-validation to minimize estimated PMSE; LOTUS extends this idea to logistic regression, and GUIDE to Poisson regression (piecewise GLMs).","In simulations for a replicated $2^4$ with 6 replicates per cell (96 observations) across four generating models (Null, Unif, Exp, Hier), the GUIDE variants (piecewise constant, best simple linear, and stepwise/multiple linear) achieved consistently low relative PMSE across scenarios, while IER and AIC tended to overfit under the Null model and EER tended to underfit in non-null models. In simulations for an unreplicated $2^4$ (16 observations), GUIDE methods again provided a better PMSE compromise than Lenth’s IER (more overfitting) and Lenth’s EER (more underfitting). In case studies, GUIDE/LOTUS produced compact trees that highlighted major interacting factors (e.g., for the Poisson solder-skip data, early splits were on Opening, Mask, and Solder; a simplified piecewise main-effects Poisson tree used only two splits). The piecewise stepwise linear GUIDE model for the unreplicated $2^5$ reactor data algebraically matched the classic Box–Hunter–Hunter selected interaction model while providing a conditional-effects interpretation through the tree structure.",None stated.,"The paper’s methodology is centered on complete factorial designs (replicated equally or unreplicated) and does not develop design-construction/optimality theory; it focuses on analysis/modeling rather than DOE planning. Performance claims rely heavily on simulation settings and selected case studies; broader benchmarks (e.g., against modern regularization methods for factorials, Bayesian model averaging, or Gaussian process/treed GP approaches) are not included. Practical guidance for tuning choices (e.g., split selection thresholds, cross-validation setup, handling of aliasing/estimability in unreplicated settings) is discussed conceptually but could be more prescriptive for practitioners.",The conclusion lists several future directions: developing formal ways to distinguish whether tree complexity is due to interactions versus main effects by examining coefficients of cross-product terms in the algebraic representation; using tree models to suggest which high-order effects might matter for subsequent traditional stepwise regression; conducting more simulation studies to assess how well tree models estimate true response surfaces; extending tree methods beyond normal/count/binary responses to unordered nominal responses (potentially via classification-tree methods such as CRUISE/QUEST); and leveraging tree methods for unbalanced designs and sequential response-surface experimentation where some factor combinations are missing or sampling is adaptive.,"Develop self-starting/Phase-I-style procedures for unreplicated factorials that quantify uncertainty in splits and within-node effect estimates (e.g., via permutation/bootstrapped stability selection) and provide adjusted inference. Extend to fractional factorials and aliasing structures explicitly (including resolution considerations), with methods that incorporate design generators/defining relations into split/fit constraints. Provide open-source, reproducible implementations and standardized simulation suites for DOE settings, including autocorrelation, random effects (blocking/split-plot), and heteroscedasticity, plus real industrial benchmarks to validate predictive and interpretability claims.",math/0611192v1,https://arxiv.org/pdf/math/0611192v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:39:41Z
TRUE,Factorial (fractional),Parameter estimation|Screening|Model discrimination|Other,Not applicable,"Variable/General (examples include 4–10 factors; e.g., 2^(7-3)=16 runs for 7 factors; 2^(4-1)=8 runs for 4 factors)",Manufacturing (general)|Semiconductor/electronics|Theoretical/simulation only,Simulation study|Other,TRUE,Other,Not provided,http://www.4ti2.de,"The paper develops conditional exact tests for factor effects in fractional factorial designed experiments when the response is discrete (Poisson counts or Binomial ratios), where standard large-sample chi-square approximations can be unreliable and enumerating the conditional sample space is infeasible. It formulates log-linear (Poisson) and logistic (Binomial) GLM null models for single-observation-per-run designs, and bases inference on the conditional distribution given sufficient statistics under the null. To sample from the resulting constrained sample space, it uses Markov chain Monte Carlo driven by Markov bases (Diaconis–Sturmfels), ensuring the chain is connected over the fiber of feasible tables/design responses. The paper analyzes correspondences between 2^(p−q) fractional factorial designs and models for 2^(p−q) contingency tables, identifying when standard hierarchical contingency-table results apply and when new (non-hierarchical) models arise. An illustrative wave-soldering (electronics manufacturing) example shows the exact conditional p-value (via MCMC) can be much larger than the asymptotic p-value, indicating asymptotic tests may overstate significance for these data.","For Poisson log-linear models, the null is specified by a covariate matrix partition $X=(X_0\;X_1)$ with $H_0:\beta_\nu=\cdots=\beta_{k-1}=0$, and inference uses the conditional distribution on the fiber $F(X_0' y_o)=\{y\ge0: X_0' y=X_0' y_o\}$: $f(y\mid X_0'y=X_0'y_o)=C(X_0'y_o)\prod_{i=1}^k 1/(y_i!)$. For Binomial-logistic models with totals $n_i$, the analogous conditional distribution is $f(y\mid X_0'y=X_0'y_o,n)=C(X_0'y_o,n)\prod_{i=1}^k 1/(y_i!(n_i-y_i)!)$, expressed via a Lawrence lifting to match the Poisson-form fiber. The goodness-of-fit test statistic example is the likelihood-ratio deviance $G^2(y)=2\sum_{i=1}^k y_i\log(y_i/\hat\mu_i)$, and MCMC sampling over the fiber is performed using Markov-basis moves $z$ in $\ker(X_0')\cap\mathbb Z^k$ with a Metropolis/Hastings-type update.","In the wave-soldering 2^(7−3)=16-run example (Poisson counts; model AC/BD/E/F/G), the observed deviance is $G^2(y_o)=19.096$ with an asymptotic $\chi^2_6$ p-value of 0.0040, while the MCMC-estimated exact conditional p-value is 0.032 (estimated SD 0.0045), showing the asymptotic approximation overstates significance. A minimal Markov basis for the example’s $X_0'$ (10×16) computed via 4ti2 contains 35 moves. The paper also tabulates mappings from common 8-run and 16-run fractional factorial designs (via generators/aliasing) to corresponding contingency-table models, and identifies cases where the induced sufficient statistics do not correspond to hierarchical models.","The authors note that computing a Markov basis can be problematic for larger experiments: current algorithms may take a very long time for 64-run or larger designs. They indicate that for 16- or 32-run designs (Poisson case), software such as 4ti2 is practical, but scalability remains a limitation.","The methodology assumes independent runs and correctly specified GLM structure (Poisson/logistic with canonical links) and conditions on sufficient statistics; practical performance under overdispersion, zero inflation, or run-to-run dependence is not addressed. The paper focuses on designs with a single observation per run (often created by aggregating replicates), which may discard information or complicate modeling when replicate-level variation matters. It demonstrates MCMC estimation on one main example but provides limited guidance on diagnosing chain mixing, convergence, and sensitivity to the chosen Markov basis or proposal mechanism in real DOE settings.","The authors suggest extending the approach to more complicated designs and deriving appropriate Markov bases for them, explicitly mentioning three-level designs and balanced incomplete block designs. They also emphasize further leveraging correspondences with $2^p$ contingency-table models to reuse general Markov-basis results where possible.","Developing scalable Markov-basis computation or alternative samplers (e.g., dynamic Markov bases, sequential importance sampling, or hit-and-run–type methods on fibers) for 64+ run designs would broaden applicability. Extending the framework to handle overdispersion (negative binomial/quasi-likelihood), random effects for replicate structure, and autocorrelated/blocked runs would better match industrial DOE data. Providing open-source implementations and practical workflows (model specification → basis computation → MCMC diagnostics → p-values) would improve uptake by practitioners.",math/0611463v1,https://arxiv.org/pdf/math/0611463v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:40:32Z
FALSE,NA,NA,Not applicable,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops the Piecewise Moments Method (PMM), a generalized Lanczos moments technique to efficiently compute inclusive nuclear electroweak response surfaces over the full $(\omega,q)$ plane when operators depend on momentum transfer $q$. Exploiting the fact that harmonic-oscillator single-particle matrix elements yield a finite polynomial in $y=(qb/2)^2$ times $e^{-y}$, the method decomposes the $q$-dependent starting vector into polynomial components and performs separate Lanczos runs for each component, combining them with a symmetric “square-root” resolution function to preserve positivity. The authors benchmark PMM against “exact moments” Lanczos reconstructions along fixed-$y$ cuts for electromagnetic multipoles in a full sd-shell calculation of $^{28}$Si, studying convergence as the number of Lanczos iterations $n$ increases and the energy-resolution width $\sigma$ decreases. PMM is found to be numerically stable (unlike exact moment-inversion approaches) and to reproduce the exact-moments smoothed responses to typically <0.01% after convergence (around $n\sim 200$ for $\sigma\sim 0.5$ MeV in their tests). The work is positioned as enabling practical, high-fidelity response-surface generation for applications such as neutrino/electron scattering where state-by-state summations are infeasible.","The central response is $S(\omega,q)=\sum_i |\langle \psi_{E_i}|O(q)|\mathrm{g.s.}\rangle|^2\,\delta(\omega-E_i)$, with harmonic-oscillator matrix elements giving $\langle \alpha|O_J(q)|\beta\rangle = y^{(J-K)/2} e^{-y}\, p_{\alpha\beta}(y)$ where $y=(qb/2)^2$ and $p(y)$ is a finite polynomial. PMM constructs a positive-definite smoothed response by writing (Lorentzian example) $S_L(\omega,q)=\frac{\sigma}{\pi} y^{J-K} e^{-2y}\langle \phi_L(y)|\phi_L(y)\rangle$ with $|\phi_L(y)\rangle=(\omega-H+i\sigma)^{-1}\sum_{j=0}^m c_j y^j |v^{(j)}_1\rangle$, then approximating each Green’s-function action via separate Lanczos continued fractions and combining cross-terms through overlaps between Lanczos vectors from different starts (Eq. 25). For a Gaussian (or other) resolution, the method analogously applies the square-root of the resolution operator symmetrically and uses Lanczos eigen-decompositions to assemble the surface (Eq. 29).","In the $^{28}$Si sd-shell electromagnetic test case, the most complex multipoles (C0 and M1) require only three Lanczos calculations (three polynomial terms in $y$) to define the full $(\omega,q)$ response surface. For fixed-$y$ cuts (where an exact-moments Lanczos approach is available), PMM discrepancies visible at $n\approx 50$ iterations disappear with more iterations; by $n\approx 200$ and Lorentzian $\sigma=0.5$ MeV, PMM and exact-moments curves are visually indistinguishable on plotted scales. After convergence, the maximum reported PMM vs exact-moments difference is about 0.01% (notably for the C0 multipole), with convergence requiring more iterations as $\sigma$ decreases (e.g., $\sigma=0.25$ MeV converges more slowly than $\sigma=1.0$ MeV). Alternative moment-inversion methods (naive determinant-based inversion and Legendre-moment variants) are shown to suffer numerical instability (e.g., failure when $\beta_n^2<0$) at moderate iteration counts, motivating PMM’s formulation.","The authors note that for any fixed number of Lanczos iterations $n$, there is a minimum energy-resolution width $\sigma$ below which $n$ is insufficient for convergence, so very fine resolution demands more iterations. They also identify that momentum-dependent operators in finite shell-model spaces can violate unitarity because strength can leak to states outside the model space at larger $q$, potentially distorting inclusive cross-section trends if uncorrected. They further point out that in larger (multi-$\hbar\omega$) spaces, spurious center-of-mass excitations can contaminate results for one-body operators unless projected out or otherwise controlled.","The demonstrated efficiency relies critically on using harmonic-oscillator single-particle bases so that operator matrix elements reduce to a low-order polynomial in $y$; applicability and efficiency may degrade for other bases (e.g., Woods–Saxon) or when effective operators break this structure. Most numerical validation is against fixed-$y$ cuts in a single nucleus/model space and interaction (full sd-shell $^{28}$Si with Brown–Wildenthal), so general performance across nuclei, larger spaces, two-body currents, and different interactions is not established. The approach assumes that a chosen smoothing kernel adequately represents experimental/physical broadening; sensitivity to kernel choice or to realistic continuum coupling is not fully explored. No public implementation details are provided, which may hinder reproducibility and adoption.","They propose extending the method to weak interactions (adding axial-current operators) to treat neutrino response functions up to about 1 GeV. They suggest studying and correcting the loss of unitarity for momentum-dependent operators in truncated shell-model spaces, quantifying probability leakage outside the model space to estimate uncertainties or apply corrections. They also propose addressing spurious center-of-mass motion in larger shell-model spaces, either by adding a large $\alpha H_{\mathrm{CM}}$ term to push spurious states out or by removing center-of-mass excitations from $O(y)|\mathrm{g.s.}\rangle$ at the outset, and note the need for numerical tests of such procedures for full-spectrum response surfaces.","A natural extension is to benchmark PMM against continuum-aware approaches (e.g., Lorentz integral transform / continuum RPA / coupled-cluster response) to assess how well the smoothed discrete-shell-model surface approximates true continuum responses across $q$. Developing self-contained, reusable software (and possibly a data format for storing Lanczos tridiagonal elements and cross-overlaps) would make the “numerical effective theory” aspect practical for downstream simulation codes (e.g., supernova transport). It would also be valuable to extend PMM to include two-body electroweak currents and consistently renormalized effective operators, where the polynomial-in-$y$ structure may be more complicated. Finally, more systematic studies of robustness to basis parameter choices (oscillator length $b$), truncation schemes, and autocorrelation/finite-precision effects in long Lanczos runs would strengthen guidance for practitioners.",nucl-th/0508034v1,https://arxiv.org/pdf/nucl-th/0508034v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:41:30Z
TRUE,Bayesian design|Optimal design|Other,Model discrimination|Prediction|Parameter estimation|Other,Other,Variable/General,Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops a conceptual framework for experimental design using inductive logic, emphasizing the duality between the logic of assertions (inference/probability/Bayes) and the logic of questions (inquiry/relevance). It introduces a relevance (bearing) calculus for questions, analogous to probability calculus for assertions, and discusses the conjecture that relevance can be expressed as an entropy functional of the probabilities of the assertions answering a question. In the experimental-design framing, one chooses an experimental question E to resolve a scientific issue S by maximizing the relevance b(E|S∨H), which under the relevance–entropy conjecture corresponds to maximizing mutual information between experiment outcomes and the scientific issue/model class. The resulting design principle balances (i) maximizing the entropy of possible experimental outcomes (unbiasedness) and (ii) minimizing expected conditional entropy of outcomes given the scientific scenarios (sharper likelihoods on average). The work is theoretical and positions experimental design as analogous to designing a communication channel optimized to transmit information relevant to the scientific issue.","Experimental design is framed as maximizing relevance of an experimental question E to a scientific issue S given background knowledge H: $b(E\mid S\vee H)=\frac{b(E\mid S\vee H)\,b(S\mid H)}{b(S\mid H)}$ (Eq. 42; denominator constant across E), leading to maximizing $b(E\vee S\mid H)$. Using question-algebra identities, $b(E\vee S\mid H)=b(E\mid H)-b(E\wedge\neg S\mid H)$ (Eq. 45). Under the relevance–entropy conjecture, this becomes mutual information: $I(E;S)=H(E)-H(E\mid S)$ (Eq. 46), with an explicit entropy/log-probability expansion for $b(E\vee S\mid H)$ given in Eq. (47).","The main result is a general optimality principle: select experiments by maximizing relevance of the common question $E\vee S$ (equivalently, mutual information $I(E;S)$ if the relevance–entropy conjecture holds). The derived criterion implies two practical desiderata for good designs: maximize outcome entropy $H(E)$ (maximize potential informativeness/unbiasedness of results) while minimizing expected conditional entropy $H(E\mid S)$ (data should sharply constrain model parameters on average). The paper further shows formal analogies between Bayes’ theorem in assertion space and corresponding identities in question/relevance space, motivating information-theoretic interpretations of inquiry and design. No numerical performance comparisons or applied case-study metrics are reported.","The paper explicitly notes that the key link used to connect relevance to entropy is not proven: the relevance–entropy relationship is described as a conjecture with “no proof yet” and therefore remains unestablished. It also states that the logic of questions (inquiry) is less understood than the logic of assertions, and that it is “yet unclear how to completely relate these two spaces” (assertions vs. questions), indicating incompleteness of the overarching theory. The author remarks that intuition for manipulating questions is lacking and that more investigation is needed to explore whether the spaces are isomorphic and to develop computations in question space without reverting to assertions.","The experimental-design criterion is presented at a high level without specifying concrete experimental constraints (costs, factor ranges, discrete run budgets) or providing algorithms to construct designs under those constraints, limiting direct usability in classical DOE settings. The mapping from “experimental questions” to standard DOE choices (runs, factor levels, randomization, blocking/split-plot restrictions) is not operationalized, so it does not yield implementable factorial/RSM/optimal-design recipes. The approach effectively assumes a probabilistic model class for outcomes and scientific scenarios; robustness to model misspecification and practical issues (noise, dependence, non-identifiability) are not analyzed. No empirical or simulation validation is provided to demonstrate that the proposed criterion produces better designs than established Bayesian/optimal design methods in typical applications.","The author calls for proving (or otherwise resolving) the relevance–entropy conjecture and clarifying how to relate the spaces of assertions and questions, including investigating whether they are isomorphic. The paper suggests exploring the hypothesized dual mapping from relevance to probability (the function G) and understanding its properties. It also notes the need to develop intuition and methods to compute directly in question space (e.g., prior relevance) without translating back to assertion space via the conjectured entropy link.","Develop explicit computational methods (e.g., variational or Monte Carlo estimators of $I(E;S)$) and optimization algorithms to construct practical designs under realistic constraints (finite runs, discrete factor levels, nuisance parameters, costs). Connect the “question” formalism to standard DOE objects (design matrices, blocking/split-plot constraints, sequential/adaptive experimentation) and show how classical criteria (D-, A-, I-optimality) arise as special cases or approximations. Evaluate the framework through benchmark simulation studies and real case studies comparing against Bayesian optimal design and information-gain design in typical experimental settings. Extend the theory to handle model misspecification/robust design, correlated observations, and nonparametric model classes where mutual-information objectives can be unstable or difficult to estimate.",physics/0204068v1,https://arxiv.org/pdf/physics/0204068v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:42:15Z
TRUE,Optimal design|Sequential/adaptive|Other,Prediction|Parameter estimation|Cost reduction,V-optimal|D-optimal|E-optimal|G-optimal|Other,"Variable/General (design variables are choice of measurable species and measurement time points; model has 56 parameters, 41 species)",Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Personal website|Upon request,http://sloppycell.sourceforge.net,"The paper applies optimal experimental design to a nonlinear ODE model of EGFR signaling, trafficking, and down-regulation that includes regulation by a Cool-1 (β-Pix)/Cdc42/Cbl complex. Using a weighted least-squares fit to 11 Western-blot time-series datasets, the authors estimate 56 unknown parameters and quantify parameter and prediction uncertainties via the Fisher Information Matrix (FIM) and linearized propagation, with a weak Gaussian prior to regularize near-singular information. They then formulate design as choosing additional measurements (which species and at what times) to minimize the average prediction variance for a difficult-to-measure state (the Cool-1–Cdc42–Cbl triple complex), emphasizing V-optimality (prediction-focused) while discussing D/E/G-optimality alternatives. A sequential (rank-one update) procedure identifies that measuring total active Cdc42 at early times most reduces uncertainty in the triple-complex trajectory; an approximate continuous-design refinement concentrates weight on a small set of early time points despite allowing many candidate points. New lab measurements of total active Cdc42 are consistent with model predictions (up to a scale factor) and substantially tighten uncertainty bounds for the triple complex, supporting its formation in wild-type cells under EGF stimulation.","The model is fit by minimizing a weighted least-squares cost $C(\theta)=\sum_{\alpha=1}^D\sum_{i=1}^{m_\alpha}\big((y_\alpha(t_{\alpha i},\theta)-d_{\alpha i})/\sigma_{\alpha i}\big)^2$, where $\theta$ are log-parameters. The Fisher Information Matrix is approximated by $M=J^\top J$ with sensitivities $J=(1/\sigma_{\alpha i})\,\partial y_\alpha(t_{\alpha i},\theta)/\partial\theta\,|_{\hat\theta}$. Prediction variance for an (un)measured trajectory component is propagated as $\mathrm{Var}(\hat y_\beta(t))\approx (\partial y_\beta/\partial\theta)^\top M^{-1}(\partial y_\beta/\partial\theta)$ evaluated at $\hat\theta$, and sequential design uses the rank-one update $M_{n+1}=M_n+s s^\top$ for candidate measurement sensitivity vector $s$ (via Sherman–Morrison–Woodbury for fast updates).","The V-optimal design targeting the triple-complex trajectory selects measuring total active Cdc42 as the single most informative additional observable, with optimal timing in the early post-stimulation window. When allowing 160 hypothetical candidate time points for Cdc42 between 0 and 80 minutes (approximate continuous design), the optimized design measure concentrates on roughly a dozen early time points, indicating few measurements can approximate the continuous optimum. Adding these optimally designed Cdc42 measurements markedly reduces the uncertainty bands on the predicted triple-complex time course (as shown by comparing their Fig. 4 vs Fig. 5b/7b), while leaving overall parameter uncertainties (eigenvalues/diagonal of $M^{-1}$) largely unchanged (their Fig. 6). New Western-blot data for total active Cdc42 agrees with the predicted trajectory up to a multiplicative scaling factor, and yields a comparable uncertainty reduction for the triple complex, supporting appreciable complex formation in wild-type cells.","The authors note that FIM/linearized uncertainty estimates are approximate and therefore they supplemented them with computationally intensive Bayesian MCMC credible intervals (used for validation, not inside the design loop). They also acknowledge that the model is a “lumped” and incomplete representation (omits known intermediates and alternative pathways), and that additional biology (e.g., alternative endocytosis mechanisms, endosomal signaling, autocrine signaling, erbB interactions) could require model extension. They caution that agreement with new Cdc42 data is not a strict validation because prior uncertainty on that observable was large, so many outcomes would have been consistent with the model.","The proposed designs are local (based on sensitivities at a single best-fit parameter set and assumed error structure), so design optimality may degrade under model misspecification or if parameters differ materially from the fitted values. The design criterion treats measurement choices mainly as species/time-point selection and does not explicitly optimize practical experimental constraints (batch effects, replicate allocation, assay limits of detection, correlated errors in densitometry, or costs per assay). The approach is tailored to averaged population-level Western-blot readouts and may not transfer directly to single-cell or stochastic regimes, nor to settings with strong temporal autocorrelation or systematic normalization artifacts.","They suggest extending the mathematical model as new experiments probe additional mechanisms (e.g., receptor signaling from early endosomes, alternative endocytic pathways, autocrine signaling, erbB-family interactions). They also argue more generally that prediction-focused experimental design should be integrated into iterative model refinement and hypothesis testing in biological systems.","Develop robust/Bayesian optimal designs that integrate over posterior parameter uncertainty (rather than relying on a local FIM) to reduce sensitivity to best-fit dependence and model mismatch. Extend the design framework to include explicit constraints and costs (replicates, limited sampling times, multiplexing, assay noise models) and to multi-objective criteria combining prediction precision for several unmeasurable species. Provide a reusable software implementation (e.g., a SloppyCell example notebook and scripts) and publish the SBML model/parameter sets in a persistent public repository to improve reproducibility.",q-bio/0610024v1,https://arxiv.org/pdf/q-bio/0610024v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:42:59Z
FALSE,NA,NA,Not applicable,Not specified,Network/cybersecurity|Semiconductor/electronics|Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies the Yuen 2000 “Y-00” quantum stream cipher and analyzes its security in realistic optical communication settings, focusing on attacks using heterodyne measurement and collective quantum unambiguous measurement. It proposes an implementation design using intensity modulation (amplitude/intensity levels divided into 2M states with M bases) and overlapping selection keying (OSK) to randomize the mapping between data bits and basis states. Security arguments emphasize that without the secret key, an eavesdropper faces an effectively M-ary quantum detection problem with unavoidable quantum-noise-induced errors, making known/chosen-plaintext key recovery require exponential resources or impractically many copies. The work reports an experimental demonstration of a 1 Gbps encrypted transmission over ~20 km of single-mode fiber using a 1550 nm DFB laser diode and synchronized LFSR-based running key generation. Eye diagrams show clear eye opening for the legitimate receiver (binary detection with key) and no usable eye opening for the eavesdropper (M-ary without key), supporting the “advantage creation” premise of Y-00 under the tested setup.","The modulation design relates neighboring-state distinguishability to the basis count M and signal amplitude, e.g., for phase modulation $\Delta_{PM}\approx \tfrac{2\pi |\alpha|}{2M}$ and for intensity modulation $\Delta_{IM}=\tfrac{|\alpha_{\max}|^2-|\alpha_{\min}|^2}{2M}$. A design target sets the neighboring-state error probability via a Gaussian tail integral: $P_e(i,i+1)=\tfrac12-\tfrac{1}{\sqrt{2\pi}}\int_0^{t_0} e^{-t^2/2}dt$ with $t_0=\Delta/2$. The paper also gives an unambiguous discrimination success probability for M symmetric coherent states: $P_D(\mathrm{QUM})=M\min_k |c_k|^2$ with coefficients $|c_k|^2=\tfrac1M\sum_{j=1}^M e^{2\pi i jk/M}\, e^{|\alpha|^2(e^{2\pi i j/M}-1)}$.","An intensity-modulated Y-00 prototype is demonstrated at 1 Gbps over approximately 20 km of single-mode fiber, using a 1550 µm DFB laser and photodiode hardware rated for 10 Gbps. The number of bases is controlled in the range $M\approx 100$ to $200$, with launch power kept around −25 to −20 dBm (after attenuation from ~0 dBm continuous output). The running key is generated by a hardware LFSR with a 20-bit secret key and full transmitter/receiver synchronization. Reported eye patterns show Bob (with key, binary detection) achieves a clear eye opening, while Eve (without key, effectively M-ary) shows no eye opening, implying substantially worse discrimination performance under the tested conditions.","The authors state that a general proof of security for the Y-00 quantum stream cipher “still remains,” and that the demonstrated system’s security corresponds to enhancing conventional stream-cipher security by quantum-noise randomization rather than establishing full unconditional security in all models. They also note that more sophisticated experiments (including demonstrating heterodyne attack explicitly and additional randomizations) will be reported later, implying the current experimental validation is limited in scope.","The experimental evidence is largely qualitative (eye diagrams) and does not report quantitative BER/ARL-style metrics for Bob vs. Eve or security metrics (e.g., mutual information bounds, key equivocation) under calibrated eavesdropper measurement models. The eavesdropper model in the experiment assumes comparable technology and does not fully address stronger adversaries (e.g., optimized collective measurements, improved receiver noise figures, or access to side channels and synchronization leakage). Use of a short 20-bit LFSR in the demo may not reflect cryptographically strong PRNG practice, and the security arguments rely heavily on physical-layer assumptions (coherent-state statistics, independence, idealized measurement constraints) that may be sensitive to implementation imperfections.","The paper states that more sophisticated experiments will be reported in subsequent work, including demonstration of the heterodyne attack and application to a high dense TV system, and that applying optical amplifiers to extend distance will be reported in a subsequent paper. It also indicates that implementing several additional randomizations (beyond the demonstrated setup) is part of planned future experiments.","Future work would benefit from reporting quantitative security/performance metrics (BER for Bob and Eve across distances, mutual information/equivocation estimates, and sensitivity analyses over M, power, and loss), and benchmarking against optimized eavesdropper receivers. Extending the design to handle practical impairments (fiber nonlinearities, drift, imperfect synchronization, detector saturation, side-channel leakage) and providing a robust parameter-selection methodology for M and power under constraints would improve deployability. Publishing reproducible implementation details (hardware/FPGA logic, PRNG/keying control, calibration procedures) and open-source simulation code would allow independent verification and broader comparison with alternative physical-layer security schemes.",quant-ph/0507043v1,https://arxiv.org/pdf/quant-ph/0507043v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:43:38Z
TRUE,Sequential/adaptive|Bayesian design|Other,Parameter estimation|Prediction|Other,Other,"Variable/General (design dimension d_ξ; examples include 1D, 2D, and J-sensor orientation designs)",Healthcare/medical|Environmental monitoring|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://anonymous.4open.science/r/badpods-2C8D,"The paper proposes BAD-PODS, a fully online sequential Bayesian experimental design method for partially observed dynamical systems modeled as nonlinear state-space models with latent states. It derives new Monte Carlo estimators for the expected information gain (EIG) and its gradient that explicitly marginalize latent trajectories, enabling stochastic-gradient optimization of continuous designs despite intractable likelihoods/evidence. The approach couples these estimators with nested particle filters to update the joint state–parameter posterior online without reprocessing the full history, achieving linear-in-horizon computational scaling and providing asymptotic consistency guarantees for the EIG estimator. Empirically, BAD-PODS is evaluated on simulated partially observed models (two-group SIR sampling allocation, moving-source sensor orientation, and ecological harvest control) and is shown to perform close to an oracle grid-search baseline while outperforming random and static (offline) design baselines in accumulated information gain. The work advances sequential BED by addressing the combined challenges of latent-state marginalization and long-horizon online feasibility with theoretical and algorithmic support.","Designs are chosen by maximizing expected information gain (EIG): $I(\xi_t)=\mathbb{E}_{p(\theta|h_{t-1})p(y_t|\theta,\xi_t)}[\log p(y_t|\theta,\xi_t)-\log p(y_t|\xi_t)]$, with $p(y_t|\xi_t)=\mathbb{E}_{p(\theta|h_{t-1})}[p(y_t|\theta,\xi_t)]$. For state-space models, the latent state $x_t$ evolves via $x_t\sim f(x_t|x_{t-1},\theta,\xi_t)$ and observations via $y_t\sim g(y_t|x_t,\theta,\xi_t)$, making the likelihood/evidence require marginalization over latent trajectories. The paper derives an EIG gradient representation (their Eq. (6)) involving $\nabla_{\xi_t}L/L$, $\nabla_{\xi_t}Z/Z$, and a design score term $\nabla_{\xi_t}\log g + \nabla_{\xi_t}\log f$, then approximates these with nested Monte Carlo using nested particle filters (their Eqs. (7)–(10)).","Across three simulated benchmarks, BAD-PODS achieves total EIG (TEIG) close to an oracle grid-search baseline and higher than random and static (offline) design baselines. For the two-group SIR at $t=200$, TEIG is 5.918 (BAD-PODS) vs 6.043 (oracle) and 5.621 (random), with the static baseline becoming computationally infeasible at longer horizons. For the moving-source model at $t=50$, TEIG is 1.733 (BAD-PODS) vs 1.805 (oracle) and 1.435 (random), with static truncated. The paper also reports runtime/memory advantages over static optimization (e.g., at SIR $T=150$, BAD-PODS 12.39 min vs static 31.05 min, and substantially lower memory).","The paper notes that no existing methods directly target the same setting (fully online BED under partial observability), which constrains comparisons largely to random designs, an oracle grid-search variant, and a static non-adaptive baseline. In the moving-source experiment, it explicitly notes that perfect sensor pointing is not achieved and outliers are expected due to intrinsically low signal-to-noise regimes where pointing becomes weakly identifiable. It also indicates that static optimization becomes infeasible at longer horizons, limiting reported static-baseline results.","Because evaluation is primarily simulation-based on three exemplar models, it is unclear how robust the method is to model misspecification, heavy-tailed noise, or nonstationarities that violate the assumed state-space model structure. The approach depends on nested particle filters and repeated Monte Carlo EIG/gradient estimation, which can be computationally heavy and sensitive to particle degeneracy and tuning choices (M, N, jitter scale, optimizer hyperparameters), potentially limiting real-time deployment in resource-constrained settings. Comparisons omit other modern likelihood-free/sequential design approaches (e.g., amortized policy learning or variational MI bounds adapted to SSMs), so relative performance against broader alternatives is not established. Practical guidance on choosing particle counts and optimization steps to meet fixed latency budgets is limited.",None stated.,"Extending BAD-PODS to handle unknown/estimated observation and transition noise scales (and other nuisance parameters) in a self-calibrating manner would improve practical applicability. Developing robust variants for misspecified or heavy-tailed observation models and for autocorrelated/heteroscedastic noise could strengthen reliability in real sensing and epidemiological data. Incorporating variance-reduction and adaptive-particle schemes (e.g., adaptive resampling/jitter, control variates, or Rao–Blackwellization where possible) could reduce computation while preserving consistency. Providing a maintained software package and benchmarks on real-world partially observed datasets (beyond simulation) would help validate usability and generalization.",2511.04403v2,https://arxiv.org/pdf/2511.04403v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-31T10:17:55Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Model discrimination|Cost reduction|Other,Minimax/Maximin|Other,Not specified,Healthcare/medical|Service industry|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://wrds-www.wharton.upenn.edu/pages/about/data-vendors/revelio-labs/,"The paper develops a design-based randomized experimental design to compare two predetermined matching plans (treatment vs. control matching) on a finite population under “matching interference,” where choosing an edge from one plan can preclude feasible edges elsewhere. It introduces the disagreement set (edges appearing in exactly one plan) and shows that for one-to-one matchings it uniquely decomposes into disjoint alternating paths and even-length cycles, which localize interference. Based on this structure, it proposes the Alternating Path Randomized (AP) Design that sequentially randomizes edge realization along each path/cycle with a single conditional probability parameter p while maintaining feasibility, and uses a Horvitz–Thompson estimator for the average treatment effect. Under a minimax (worst-case variance) criterion with bounded outcomes, it analyzes variance growth along components, shows overall variance decays on the order of 1/N, and derives that the asymptotically optimal p for long components converges to √2−1≈0.4142. It proves a finite-population CLT for the estimator under complex mixtures of short/long components and extends the design to many-to-one matchings via graph constructions (augmenting paths + Euler-tour decompositions) that yield admissible alternating decompositions under capacity constraints; simulations using workforce matching data illustrate variance reductions vs. a naive whole-matching switch design and validate approximate normality.","Disagreement set: $\Delta M^{(t,c)}=(M^t\cup M^c)\setminus(M^t\cap M^c)$. AP design randomization along each alternating path/cycle uses $\Pr(W_{i,1}=1)=\frac{p}{1+p}$ and for $j\ge2$, $\Pr(W_{i,j}=1\mid W_{i,j-1}=0)=p$ and $\Pr(W_{i,j}=1\mid W_{i,j-1}=1)=0$ (with a deterministic closure rule for the final edge in cycles). Estimation uses a Horvitz–Thompson form: $\hat\tau_{AP}=\frac{1}{N}\sum_{(u,v)\in\Delta M_t}\frac{\mathbb{1}\{W_{uv}=1\}Y_{uv}}{\Pr(W_{uv}=1)}-\frac{1}{N}\sum_{(u,v)\in\Delta M_c}\frac{\mathbb{1}\{W_{uv}=1\}Y_{uv}}{\Pr(W_{uv}=1)}$, and the minimax-optimal $p$ for long paths converges to $\sqrt2-1$.","The naive design that randomizes between implementing $M^t$ vs. $M^c$ has non-vanishing variance, with worst-case variance $4B^2$ under bounded outcomes. Under AP design, variance decomposes by alternating components and, in the worst case, grows linearly with component length; aggregating yields overall $\mathrm{Var}(\hat\tau_{AP})=O(1/N)$. The minimax-optimal conditional probability satisfies $\lim_{k\to\infty}p_k^*=\sqrt2-1\approx0.4142$ for long alternating paths/cycles. In a workforce matching simulation (500 randomizations; n=10–50), AP variance decreases with n (e.g., true var about 0.8983 at n=10 to 0.1848 at n=50) while naive variance stays around ~3.09; the estimated variance upper bound tracks true variance closely, and normality is supported (Shapiro–Wilk statistic 0.9999, p=0.2817) for normalized estimates.","The paper notes that the closed-form variance expressions contain cross-terms (e.g., products of adjacent-edge potential outcomes) that are not identifiable from observed data because adjacent edges along an alternating component are never simultaneously realized under feasibility constraints. As a result, the true variance cannot be directly estimated, motivating the use of an estimable conservative upper bound for inference.","The design and inference are developed for comparing two fixed, predetermined matchings; it does not address adaptive/online scenarios where matchings themselves are learned or updated during experimentation, which is common in platform A/B tests. Practical implementation may require substantial engineering access to enforce sequential edge randomization and to ensure compliance with the realized experimental matching, especially in dynamic markets with arrivals/departures. Empirical evaluation is primarily simulation-based using constructed match plans; broader real-world validations across different market structures (e.g., thick vs. thin, strategic behavior, time-varying capacities) are not shown, and robustness to outcome distributions beyond boundedness/independence across components is not fully stress-tested.",None stated,"Extend AP design to dynamic matching markets with time-indexed arrivals and departures, where feasibility and interference evolve and may require online (adaptive) randomization rules. Develop tighter, implementable variance estimators (or studentized procedures) that reduce conservativeness while remaining identifiable under matching interference. Generalize to comparisons among more than two matchings/algorithms (multi-arm designs) and to settings with additional interference channels (e.g., prices, congestion) beyond capacity constraints inherent in match feasibility. Provide open-source reference implementations and deployment guidelines for platform experimentation teams, including diagnostics for admissible decompositions in many-to-one settings.",2601.21036v1,https://arxiv.org/pdf/2601.21036v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-31T10:18:40Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Model discrimination|Prediction|Other,Not applicable,Variable/General (two-sided factors: items N and time periods T; treatment probability p for each item-time pair),Food/agriculture|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,https://www.ruoxuanxiong.com/data-driven-switchback-design.pdf,"The paper studies how to design A/B tests to evaluate inventory control policies in multi-item, multi-period lost-sales systems with a shared capacity constraint, where standard causal estimators suffer bias due to interference. It analyzes two canonical designs—switchback experiments (randomize over time blocks) and item-level randomization (randomize over items)—and derives the direction of bias of a naive inverse-probability-weighted (IPW) estimator under two regimes: (i) treatment improves forecast mean bias and (ii) treatment reduces forecast variance. Motivated by two-sided randomization, it proposes a pairwise randomization design that independently assigns treatment at each item–time cell, and characterizes how its bias combines cross-item cannibalization (capacity competition) and temporal carryover (inventory state). Theoretical results show SW tends to underestimate GTE when treatment corrects downward mean bias but overestimates when treatment reduces forecast variance; IR tends to overestimate in the mean-bias regime and is asymptotically unbiased in the variance-reduction regime; PR is upper-bounded by IR bias in the mean-bias regime and shares SW’s asymptotic positive bias in the variance-reduction regime. Numerical experiments using the FreshRetailNet-50K retail dataset (Dingdong Fresh) via trace-driven simulation validate these patterns and provide practical guidance on when to use IR vs PR vs SW.","The causal estimand is the Global Treatment Effect (GTE): $\mathrm{GTE}=\frac{1}{NT}\sum_{n,t}\mathbb{E}[R_{n,t}\mid W=\mathbf{1}]-\frac{1}{NT}\sum_{n,t}\mathbb{E}[R_{n,t}\mid W=\mathbf{0}]$. The estimator is a Horvitz–Thompson/IPW form: $\widehat{\mathrm{GTE}}=\frac{1}{NT}\sum_{n,t}\left(\frac{W_{n,t}R_{n,t}}{p}-\frac{(1-W_{n,t})R_{n,t}}{1-p}\right)$ under the marginal constraint $\mathbb{P}(W_{n,t}=1)=p$. The three designs are: Switchback (SW) with $W_{1,t}=\cdots=W_{N,t}\sim\mathrm{Bernoulli}(p)$ i.i.d. over $t$; Item-level randomization (IR) with $W_{n,t}$ constant over $t$ for each $n$ and $W_{n,\cdot}\sim\mathrm{Bernoulli}(p)$; and Pairwise randomization (PR) with i.i.d. cell assignments $W_{n,t}\sim\mathrm{Bernoulli}(p)$.","Scenario 1 (treatment reduces common downward mean bias): theory predicts GTE $\ge 0$, SW bias $\le 0$ (systematic underestimation due to inventory carryover), IR bias $\ge 0$ (systematic overestimation due to capacity-driven cannibalization), and PR bias $\le$ IR bias (tradeoff of the two forces). Scenario 2 (treatment reduces forecast variance with unchanged mean) in a mean-field $N\to\infty$ regime: $\lim_{N\to\infty}\mathrm{GTE}^{(N)}\ge 0$, $\lim_{N\to\infty}\mathrm{Bias}_{\mathrm{SW}}^{(N)}\ge 0$, $\lim_{N\to\infty}\mathrm{Bias}_{\mathrm{IR}}^{(N)}=0$, and $\lim_{N\to\infty}\mathrm{Bias}_{\mathrm{PR}}^{(N)}=\lim_{N\to\infty}\mathrm{Bias}_{\mathrm{SW}}^{(N)}\ge 0$. Empirically (FreshRetailNet-50K, 30 randomizations per setting, varying capacity tight/medium/loose), the violin plots match the sign patterns: SW shows negative bias in Scenario 1 and positive bias in Scenario 2, IR is upward biased in Scenario 1 and near-unbiased in Scenario 2, and PR typically lies between SW and IR in Scenario 1 while tracking SW in Scenario 2.","The authors note their analytical results rely on structural assumptions such as a “no-overshoot” condition (Assumption 1) and other regularity/mild conditions; they also remark that in the numerical study the no-overshoot assumption may not always hold. They further emphasize that the zero replenishment lead-time assumption is made for tractability, since lost-sales models with positive lead times are notoriously complex.","The work focuses on a specific inventory policy class (myopic/rolling-horizon with a particular allocation/quantile structure), so conclusions about bias may change under different operational policies (e.g., multi-period optimization, positive lead times, or non-myopic heuristics). The empirical evaluation is trace-driven simulation with imputation and a modeled substitution mechanism rather than true online experimentation, so real-world operational frictions (implementation delays, noncompliance, demand feedback) may alter interference patterns. Design comparisons center on the naive IPW estimator; alternative estimators (e.g., model-based adjustment, state-aware methods, cluster/switchback refinements) could change the bias–variance tradeoff and are not benchmarked here.","The paper indicates it provides “practical implications and future work” in Section 6, but no concrete, specific future research directions are explicitly detailed in the provided text beyond recommending when to use IR/PR and cautioning about SW.","Extend the bias analysis and recommended designs to settings with positive replenishment lead times and richer operational constraints (multiple capacities, ordering fixed costs), where temporal interference is stronger. Develop state-aware or self-normalized estimators that explicitly incorporate inventory dynamics (initial inventory as a mediator) to reduce bias under SW/PR without sacrificing variance. Validate the design guidance with real online field experiments (not only simulation) and provide open-source implementation to facilitate adoption in practice.",2501.11996v2,https://arxiv.org/pdf/2501.11996v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-03T10:27:46Z
TRUE,Sequential/adaptive|Other,Parameter estimation|Optimization|Model discrimination|Cost reduction|Other,Other,Variable/General (design points are selected from a finite pool X; feature dimension κ and cost-vector dimension d are general).,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies sequential (adaptive) experimental design in a predict-then-optimize setting where predictions feed into a downstream linear optimization problem and performance is measured by decision loss (SPO loss) rather than prediction error. It proposes a computationally tractable, decision-focused acquisition criterion based on directional predictive uncertainty: normalize predicted cost vectors and sample design points with probability proportional to the maximum pairwise disagreement in these normalized directions over the current hypothesis confidence set. The resulting importance-weighted sequential design algorithm (IWSD-DU) alternates between sampling a design from a finite pool, observing a noisy label, updating an importance-weighted empirical SPO loss, and eliminating hypotheses via a shrinking confidence set. The authors prove non-asymptotic excess SPO-risk bounds and sample-complexity guarantees under a directional margin (non-degeneracy) condition, and show that under an optimality-gap assumption the method attains an earlier stopping time than decision-blind baselines. Empirically, it is evaluated on a cardiovascular disease dataset (cluster-based pool construction) and a real-world LLM task-allocation/job-assignment study with human evaluation, showing comparable decision risk with substantially fewer labels than decision-blind sequential alternatives.","Downstream decision is a contextual linear program: for context x, choose w in a bounded polyhedral set S to minimize E[c^\top w\mid x]. Decision quality is measured by the Smart Predict-then-Optimize loss $\ell_{\mathrm{SPO}}(c,\hat c)=c^\top w^*(\hat c)-c^\top w^*(c)$, where $w^*(\hat c)=\arg\min_{w\in S}\hat c^\top w$. The proposed design score for a candidate pooled design point $X^{(i)}$ at iteration t is the directional uncertainty $p_{t,i}=\max_{h_1,h_2\in H_t}\left\|\frac{h_1(X^{(i)})}{\|h_1(X^{(i)})\|}-\frac{h_2(X^{(i)})}{\|h_2(X^{(i)})\|}\right\|$, and sampling probabilities are $\pi_{t,i}=p_{t,i}/\sum_j p_{t,j}$.","The paper proves a high-probability non-asymptotic excess decision-risk bound for IWSD-DU: with probability at least $1-\delta$, for all $T\ge 1$, $R_{\mathrm{SPO}}(h_T)-R_{\mathrm{SPO}}(h^*)\le 4\gamma L\sqrt{\log(2T|H|/\delta)/T}$, where $L=\Delta(S)\,\rho(C)/(2\eta)$ depends on the feasible-region diameter, label radius, and a directional margin parameter, and $\gamma$ aggregates maximal directional differences over the pool. This yields sample complexity on the order of $\tilde O(\varepsilon^{-2}\log(1/\delta))$ to achieve excess SPO risk at most $\varepsilon$. Under an additional optimality-gap assumption, they show the proposed method excludes all suboptimal hypotheses (finite stopping time) while a decision-blind ℓ2-uncertainty design can retain suboptimal hypotheses with nontrivial probability for some distributions. In experiments, they report achieving the same SPO/decision risk with substantially fewer labels than decision-blind sequential baselines on cardiovascular diagnosis and an LLM allocation task (with human scoring).",None stated.,"The theory relies on a finite hypothesis class (or a complexity replacement) and a directional margin/non-degeneracy condition tied to distance-to-degeneracy; in practice, verifying or enforcing these conditions (and computing related geometry for general S) may be nontrivial outside the finite-pool setting. The downstream optimization is linear with a bounded polyhedral feasible region and uses SPO loss; extensions to nonlinear/convex programs, unbounded regions, or other decision losses are not addressed. Empirical studies do not provide shared code and the real-world LLM study uses relatively small, bespoke human-evaluated data, which may limit reproducibility and external validity of performance claims.",None stated.,"Develop versions for infinite/continuous design spaces (beyond a finite pool) with provable guarantees and practical acquisition optimization. Extend the directional-uncertainty principle to other downstream problems (e.g., mixed-integer programs, convex programs) and other decision-focused losses, including settings where scale invariance does not hold. Provide self-starting/robust variants handling unknown or drifting marginals, dependence/autocorrelation in sequential outcomes, and practical diagnostics plus open-source implementations for reproducible benchmarking.",2602.05340v1,https://arxiv.org/pdf/2602.05340v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-06T10:20:59Z
