id,submitted,updated,title,abstract,authors,affiliations,link_abstract,link_pdf,link_doi,comment,journal_ref,doi,primary_category,categories,pdf_url
2602.10144v1,2026-02-09T10:45:13Z,2026-02-09 10:45:13,When LLMs get significantly worse: A statistical approach to detect model degradations,"Minimizing the inference cost and latency of foundation models has become a crucial area of research. Optimization approaches include theoretically lossless methods and others without accuracy guarantees like quantization. In all of these cases it is crucial to ensure that the model quality has not degraded. However, even at temperature zero, model generations are not necessarily robust even to theoretically lossless model optimizations due to numerical errors. We thus require statistical tools to decide whether a finite-sample accuracy deviation is an evidence of a model's degradation or whether it can be attributed to (harmless) noise in the evaluation. We propose a statistically sound hypothesis testing framework based on McNemar's test allowing to efficiently detect model degradations, while guaranteeing a controlled rate of false positives. The crucial insight is that we have to confront the model scores on each sample, rather than aggregated on the task level. Furthermore, we propose three approaches to aggregate accuracy estimates across multiple benchmarks into a single decision. We provide an implementation on top of the largely adopted open source LM Evaluation Harness and provide a case study illustrating that the method correctly flags degraded models, while not flagging model optimizations that are provably lossless. We find that with our tests even empirical accuracy degradations of 0.3% can be confidently attributed to actual degradations rather than noise.",Jonas Kübler|Kailash Budhathoki|Matthäus Kleindessner|Xiong Zhou|Junming Yin|Ashish Khetan|George Karypis,,https://arxiv.org/abs/2602.10144v1,https://arxiv.org/pdf/2602.10144v1,,https://openreview.net/forum?id=cM3gsqEI4K,ICLR 2026,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/2602.10144v1.pdf
2602.07170v1,2026-02-06T20:12:45Z,2026-02-06 20:12:45,Bayesian Dynamic Gamma Models for Route-Level Travel Time Reliability,"Route-level travel time reliability requires characterizing the distribution of total travel time across correlated segments -- a problem where existing methods either assume independence (fast but miscalibrated) or model dependence via copulas and simulation (accurate but expensive). We propose a conjugate Bayesian dynamic Gamma model with a common random environment that resolves this trade-off. Each segment's travel time follows a Gamma distribution conditional on a shared latent environment process that evolves as a Markov chain, inducing cross-segment dependence while preserving conditional independence. A moment-matching approximation yields a closed-form $F$-distribution for route travel time, from which the Planning Time Index, Buffer Index, and on-time probability are computed instantly -- at the same $O(1)$ cost as independence-based methods. The conjugate structure ensures that Bayesian posterior updates and the full predictive distribution are available in closed form as new sensor data arrives. Applied to 16 sensors spanning 8.26 miles on I-55 in Chicago, the model achieves 95.4% coverage of nominal 90\% predictive intervals versus 34--37% for independence-based convolution, at identical computational cost.",Vadim Sokolov|Refik Soyer,,https://arxiv.org/abs/2602.07170v1,https://arxiv.org/pdf/2602.07170v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2602.07170v1.pdf
2602.05082v1,2026-02-04T22:04:07Z,2026-02-04 22:04:07,Reliable Explanations or Random Noise? A Reliability Metric for XAI,"In recent years, explaining decisions made by complex machine learning models has become essential in high-stakes domains such as energy systems, healthcare, finance, and autonomous systems. However, the reliability of these explanations, namely, whether they remain stable and consistent under realistic, non-adversarial changes, remains largely unmeasured. Widely used methods such as SHAP and Integrated Gradients (IG) are well-motivated by axiomatic notions of attribution, yet their explanations can vary substantially even under system-level conditions, including small input perturbations, correlated representations, and minor model updates. Such variability undermines explanation reliability, as reliable explanations should remain consistent across equivalent input representations and small, performance-preserving model changes. We introduce the Explanation Reliability Index (ERI), a family of metrics that quantifies explanation stability under four reliability axioms: robustness to small input perturbations, consistency under feature redundancy, smoothness across model evolution, and resilience to mild distributional shifts. For each axiom, we derive formal guarantees, including Lipschitz-type bounds and temporal stability results. We further propose ERI-T, a dedicated measure of temporal reliability for sequential models, and introduce ERI-Bench, a benchmark designed to systematically stress-test explanation reliability across synthetic and real-world datasets. Experimental results reveal widespread reliability failures in popular explanation methods, showing that explanations can be unstable under realistic deployment conditions. By exposing and quantifying these instabilities, ERI enables principled assessment of explanation reliability and supports more trustworthy explainable AI (XAI) systems.",Poushali Sengupta|Sabita Maharjan|Frank Eliassen|Shashi Raj Pandey|Yan Zhang,,https://arxiv.org/abs/2602.05082v1,https://arxiv.org/pdf/2602.05082v1,,,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2602.05082v1.pdf
2602.02875v1,2026-02-02T22:33:07Z,2026-02-02 22:33:07,Shiha Distribution: Statistical Properties and Applications to Reliability Engineering and Environmental Data,"This paper introduces a new two-parameter distribution, referred to as the Shiha distribution, which provides a flexible model for skewed lifetime data with either heavy or light tails. The proposed distribution is applicable to various fields, including reliability engineering, environmental studies, and related areas. We derive its main statistical properties, including the moment generating function, moments, hazard rate function, quantile function, and entropy. The stress--strength reliability parameter is also derived in closed form. A simulation study is conducted to evaluate its performance. Applications to several real data sets demonstrate that the Shiha distribution consistently provides a superior fit compared with established competing models, confirming its practical effectiveness for lifetime data analysis.",F. A. Shiha,,https://arxiv.org/abs/2602.02875v1,https://arxiv.org/pdf/2602.02875v1,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2602.02875v1.pdf
2602.02432v1,2026-02-02T18:31:58Z,2026-02-02 18:31:58,Maximizing Reliability with Bayesian Optimization,"Bayesian optimization (BO) is a popular, sample-efficient technique for expensive, black-box optimization. One such problem arising in manufacturing is that of maximizing the reliability, or equivalently minimizing the probability of a failure, of a design which is subject to random perturbations - a problem that can involve extremely rare failures ($P_\mathrm{fail} = 10^{-6}-10^{-8}$). In this work, we propose two BO methods based on Thompson sampling and knowledge gradient, the latter approximating the one-step Bayes-optimal policy for minimizing the logarithm of the failure probability. Both methods incorporate importance sampling to target extremely small failure probabilities. Empirical results show the proposed methods outperform existing methods in both extreme and non-extreme regimes.",Jack M. Buckingham|Ivo Couckuyt|Juergen Branke,,https://arxiv.org/abs/2602.02432v1,https://arxiv.org/pdf/2602.02432v1,,"25 pages, 9 figures",,,cs.LG,cs.LG|math.OC|stat.ML,https://arxiv.org/pdf/2602.02432v1.pdf
2602.01929v1,2026-02-02T10:29:30Z,2026-02-02 10:29:30,Probabilistic function-on-function nonlinear autoregressive model for emulation and reliability analysis of dynamical systems,"Constructing accurate and computationally efficient surrogate models (or emulators) for predicting dynamical system responses is critical in many engineering domains, yet remains challenging due to the strongly nonlinear and high-dimensional mapping from external excitations and system parameters to system responses. This work introduces a novel Function-on-Function Nonlinear AutoRegressive model with eXogenous inputs (F2NARX), which reformulates the conventional NARX model from a function-on-function regression perspective, inspired by the recently proposed $\mathcal{F}$-NARX method. The proposed framework substantially improves predictive efficiency while maintaining high accuracy. By combining principal component analysis with Gaussian process regression, F2NARX further enables probabilistic predictions of dynamical responses via the unscented transform in an autoregressive manner. The effectiveness of the method is demonstrated through case studies of varying complexity. Results show that F2NARX outperforms state-of-the-art NARX model by orders of magnitude in efficiency while achieving higher accuracy in general. Moreover, its probabilistic prediction capabilities facilitate active learning, enabling accurate estimation of first-passage failure probabilities of dynamical systems using only a small number of training time histories.",Zhouzhou Song|Marcos A. Valdebenito|Styfen Schär|Stefano Marelli|Bruno Sudret|Matthias G. R. Faes,,https://arxiv.org/abs/2602.01929v1,https://arxiv.org/pdf/2602.01929v1,,,,,math.DS,math.DS|stat.CO|stat.ML,https://arxiv.org/pdf/2602.01929v1.pdf
2602.01912v1,2026-02-02T10:18:31Z,2026-02-02 10:18:31,Reliable Real-Time Value at Risk Estimation via Quantile Regression Forest with Conformal Calibration,"Rapidly evolving market conditions call for real-time risk monitoring, but its online estimation remains challenging. In this paper, we study the online estimation of one of the most widely used risk measures, Value at Risk (VaR). Its accurate and reliable estimation is essential for timely risk control and informed decision-making. We propose to use the quantile regression forest in the offline-simulation-online-estimation (OSOA) framework. Specifically, the quantile regression forest is trained offline to learn the relationship between the online VaR and risk factors, and real-time VaR estimates are then produced online by incorporating observed risk factors. To further ensure reliability, we develop a conformalized estimator that calibrates the online VaR estimates. To the best of our knowledge, we are the first to leverage conformal calibration to estimate real-time VaR reliably based on the OSOA formulation. Theoretical analysis establishes the consistency and coverage validity of the proposed estimators. Numerical experiments confirm the proposed method and demonstrate its effectiveness in practice.",Du-Yi Wang|Guo Liang|Kun Zhang|Qianwen Zhu,,https://arxiv.org/abs/2602.01912v1,https://arxiv.org/pdf/2602.01912v1,,,,,stat.ML,stat.ML|cs.AI|cs.LG|q-fin.RM,https://arxiv.org/pdf/2602.01912v1.pdf
2602.01378v1,2026-02-01T18:25:44Z,2026-02-01 18:25:44,Context Dependence and Reliability in Autoregressive Language Models,"Large language models (LLMs) generate outputs by utilizing extensive context, which often includes redundant information from prompts, retrieved passages, and interaction history. In critical applications, it is vital to identify which context elements actually influence the output, as standard explanation methods struggle with redundancy and overlapping context. Minor changes in input can lead to unpredictable shifts in attribution scores, undermining interpretability and raising concerns about risks like prompt injection. This work addresses the challenge of distinguishing essential context elements from correlated ones. We introduce RISE (Redundancy-Insensitive Scoring of Explanation), a method that quantifies the unique influence of each input relative to others, minimizing the impact of redundancies and providing clearer, stable attributions. Experiments demonstrate that RISE offers more robust explanations than traditional methods, emphasizing the importance of conditional information for trustworthy LLM explanations and monitoring.",Poushali Sengupta|Shashi Raj Pandey|Sabita Maharjan|Frank Eliassen,,https://arxiv.org/abs/2602.01378v1,https://arxiv.org/pdf/2602.01378v1,,,,,cs.CL,cs.CL|cs.AI|stat.ML,https://arxiv.org/pdf/2602.01378v1.pdf
2602.01285v1,2026-02-01T15:34:45Z,2026-02-01 15:34:45,Multi-LLM Adaptive Conformal Inference for Reliable LLM Responses,"Ensuring factuality is essential for the safe use of Large Language Models (LLMs) in high-stakes domains such as medicine and law. Conformal inference provides distribution-free guarantees, but existing approaches are either overly conservative, discarding many true-claims, or rely on adaptive error rates and simple linear models that fail to capture complex group structures. To address these challenges, we reformulate conformal inference in a multiplicative filtering setting, modeling factuality as a product of claim-level scores. Our method, Multi-LLM Adaptive Conformal Inference (MACI), leverages ensembles to produce more accurate factuality-scores, which in our experiments led to higher retention, while validity is preserved through group-conditional calibration. Experiments show that MACI consistently achieves user-specified coverage with substantially higher retention and lower time cost than baselines. Our repository is available at https://github.com/MLAI-Yonsei/MACI",Kangjun Noh|Seongchan Lee|Ilmun Kim|Kyungwoo Song,,https://arxiv.org/abs/2602.01285v1,https://arxiv.org/pdf/2602.01285v1,,Accepted to ICLR 2026,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2602.01285v1.pdf
2602.02583v1,2026-01-31T17:51:01Z,2026-01-31 17:51:01,Copula-Based Aggregation and Context-Aware Conformal Prediction for Reliable Renewable Energy Forecasting,"The rapid growth of renewable energy penetration has intensified the need for reliable probabilistic forecasts to support grid operations at aggregated (fleet or system) levels. In practice, however, system operators often lack access to fleet-level probabilistic models and instead rely on site-level forecasts produced by heterogeneous third-party providers. Constructing coherent and calibrated fleet-level probabilistic forecasts from such inputs remains challenging due to complex cross-site dependencies and aggregation-induced miscalibration. This paper proposes a calibrated probabilistic aggregation framework that directly converts site-level probabilistic forecasts into reliable fleet-level forecasts in settings where system-level models cannot be trained or maintained. The framework integrates copula-based dependence modeling to capture cross-site correlations with Context-Aware Conformal Prediction (CACP) to correct miscalibration at the aggregated level. This combination enables dependence-aware aggregation while providing valid coverage and maintaining sharp prediction intervals. Experiments on large-scale solar generation datasets from MISO, ERCOT, and SPP demonstrate that the proposed Copula+CACP approach consistently achieves near-nominal coverage with significantly sharper intervals than uncalibrated aggregation baselines.",Alireza Moradi|Mathieu Tanneau|Reza Zandehshahvar|Pascal Van Hentenryck,,https://arxiv.org/abs/2602.02583v1,https://arxiv.org/pdf/2602.02583v1,,,,,cs.LG,cs.LG|stat.AP,https://arxiv.org/pdf/2602.02583v1.pdf
2601.21856v1,2026-01-29T15:28:25Z,2026-01-29 15:28:25,Blind Ultrasound Image Enhancement via Self-Supervised Physics-Guided Degradation Modeling,"Ultrasound (US) interpretation is hampered by multiplicative speckle, acquisition blur from the point-spread function (PSF), and scanner- and operator-dependent artifacts. Supervised enhancement methods assume access to clean targets or known degradations; conditions rarely met in practice. We present a blind, self-supervised enhancement framework that jointly deconvolves and denoises B-mode images using a Swin Convolutional U-Net trained with a \emph{physics-guided} degradation model. From each training frame, we extract rotated/cropped patches and synthesize inputs by (i) convolving with a Gaussian PSF surrogate and (ii) injecting noise via either spatial additive Gaussian noise or complex Fourier-domain perturbations that emulate phase/magnitude distortions. For US scans, clean-like targets are obtained via non-local low-rank (NLLR) denoising, removing the need for ground truth; for natural images, the originals serve as targets. Trained and validated on UDIAT~B, JNU-IFM, and XPIE Set-P, and evaluated additionally on a 700-image PSFHS test set, the method achieves the highest PSNR/SSIM across Gaussian and speckle noise levels, with margins that widen under stronger corruption. Relative to MSANN, Restormer, and DnCNN, it typically preserves an extra $\sim$1--4\,dB PSNR and 0.05--0.15 SSIM in heavy Gaussian noise, and $\sim$2--5\,dB PSNR and 0.05--0.20 SSIM under severe speckle. Controlled PSF studies show reduced FWHM and higher peak gradients, evidence of resolution recovery without edge erosion. Used as a plug-and-play preprocessor, it consistently boosts Dice for fetal head and pubic symphysis segmentation. Overall, the approach offers a practical, assumption-light path to robust US enhancement that generalizes across datasets, scanners, and degradation types.",Shujaat Khan|Syed Muhammad Atif|Jaeyoung Huh|Syed Saad Azhar,,https://arxiv.org/abs/2601.21856v1,https://arxiv.org/pdf/2601.21856v1,,"11 pages, 13 figures",,,eess.IV,eess.IV|cs.CV|stat.ML,https://arxiv.org/pdf/2601.21856v1.pdf
2601.21200v2,2026-01-29T02:59:04Z,2026-02-05 06:45:59,Provably Reliable Classifier Guidance via Cross-Entropy Control,"Classifier-guided diffusion models generate conditional samples by augmenting the reverse-time score with the gradient of the log-probability predicted by a probabilistic classifier. In practice, this classifier is usually obtained by minimizing an empirical loss function. While existing statistical theory guarantees good generalization performance when the sample size is sufficiently large, it remains unclear whether such training yields an effective guidance mechanism.
  We study this question in the context of cross-entropy loss, which is widely used for classifier training. Under mild smoothness assumptions on the classifier, we show that controlling the cross-entropy at each diffusion model step is sufficient to control the corresponding guidance error. In particular, probabilistic classifiers achieving conditional KL divergence $\varepsilon^2$ induce guidance vectors with mean squared error $\widetilde O(d \varepsilon )$, up to constant and logarithmic factors. Our result yields an upper bound on the sampling error of classifier-guided diffusion models and bears resemblance to a reverse log-Sobolev--type inequality. To the best of our knowledge, this is the first result that quantitatively links classifier training to guidance alignment in diffusion models, providing both a theoretical explanation for the empirical success of classifier guidance, and principled guidelines for selecting classifiers that induce effective guidance.",Sharan Sahu|Arisina Banerjee|Yuchen Wu,,https://arxiv.org/abs/2601.21200v2,https://arxiv.org/pdf/2601.21200v2,,"31 pages, 3 figures",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2601.21200v2.pdf
2601.17518v1,2026-01-24T16:39:20Z,2026-01-24 16:39:20,Comparisons of policies based on relevation and replacement by a new one unit in reliability,"The purpose of this paper is to study the role of the relevation transform, where a failed unit is replaced by a used unit with the same age as the failed one, as an alternative to the policy based on the replacement by a new one. In particular, we compare the stochastic processes arising from a policy based on the replacement of a failed unit by a new one and from the one in which the unit is being continuously subjected to a relevation policy. The comparisons depend on the aging properties of the units under repair.", Belzunce| F.| Martínez-Riquelme| C.| Mercader|J. A.| Ruiz|J. M,,https://arxiv.org/abs/2601.17518v1,https://arxiv.org/pdf/2601.17518v1,https://doi.org/10.1007/s11749-020-00710-6,17 pages. Author accepted manuscript,"TEST (2021), 30, 211-227",10.1007/s11749-020-00710-6,math.ST,math.ST|stat.AP|stat.ME,https://arxiv.org/pdf/2601.17518v1.pdf
2601.16174v2,2026-01-22T18:19:52Z,2026-02-03 12:52:33,Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints,"Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.",Yiyao Yang,,https://arxiv.org/abs/2601.16174v2,https://arxiv.org/pdf/2601.16174v2,,"22 pages, 5 figures, 5 propositions",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2601.16174v2.pdf
2601.08655v1,2026-01-13T15:31:55Z,2026-01-13 15:31:55,Reliability Modeling of Single-Sided Aluminized Polyimide Films during Storage Considering Stress-Induced Degradation Mechanism Transition,"Single-sided aluminized polyimide films (SAPF) are widely used in thermal management of aerospace systems. Although the reliability of SAPF in space environments has been thoroughly studied, its reliability in ground environments during storage is always ignored, potentially leading to system failure. This paper aims to investigate the reliability of SAPF in storage environments, focusing on the effects of temperature and relative humidity. Firstly, the relationship between the performance degradation of SAPF and aluminum corrosion is identified. Next, considering the presence of two distinct stages in the influence of temperature on aluminum corrosion, a novel degradation model accounting for the degradation mechanism transition is developed. Additionally, a parameter analysis method is proposed for determining SAPF degradation mechanism based on experimental data. Then, a statistical analysis method incorporating an improved rime optimization algorithm is employed for parameter estimation, and the reliability model is established. Experimental results demonstrate that the proposed method effectively identifies two distinct stages in the impact of temperature on SAPF performance degradation. Furthermore, the proposed degradation model outperforms traditional degradation models with unchanged degradation mechanism in terms of degradation prediction accuracy, extrapolation capability and robustness, indicating its suitability for describing the degradation pattern of SAPFs.",Shi-Shun Chen|Dong-Hua Niu|Wen-Bin Chen|Jia-Yun Song|Ya-Fei Zhang|Xiao-Yang Li|Enrico Zio,,https://arxiv.org/abs/2601.08655v1,https://arxiv.org/pdf/2601.08655v1,https://doi.org/10.1109/TR.2025.3648418,,"IEEE Transactions on Reliability, 2026. https://ieeexplore.ieee.org/document/11342365",10.1109/TR.2025.3648418,stat.AP,stat.AP,https://arxiv.org/pdf/2601.08655v1.pdf
2512.23019v1,2025-12-28T17:40:14Z,2025-12-28 17:40:14,Reliability Analysis of a 1-out-of-n Cold Standby Redundant System under the Generalized Lindley Distribution,"Cold standby 1-out-of-n redundant systems are well-established models in system reliability engineering. To date, reliability analyses of such systems have predominantly assumed exponential, Erlang, or Weibull failure distributions for their components. The Lindley distribution and its generalizations represent a significant class of statistical distributions in reliability engineering. Certain generalized Lindley distributions, due to the appealing characteristics of their hazard functions, can serve as suitable alternatives to other well-known lifetime distributions like the Weibull. This study investigates the reliability of a 1-out-of-n cold standby redundant system with perfect and imperfect switching, assuming that the active component failure times follow the Generalized Lindley distribution. We derive a closed-form expression for the system reliability. To achieve this, the distribution of the sum of n independent and identically distributed random variables following the Generalized Lindley distribution is first determined using the moment-generating function approach.",Afshin Yaghoubi|Esmaile Khorram|Omid Naghshineh Arjmand,,https://arxiv.org/abs/2512.23019v1,https://arxiv.org/pdf/2512.23019v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2512.23019v1.pdf
2512.16012v2,2025-12-17T22:40:40Z,2026-01-12 23:46:26,Reliability-Targeted Simulation of Item Response Data: Solving the Inverse Design Problem,"Monte Carlo simulations are the primary methodology for evaluating Item Response Theory (IRT) methods, yet marginal reliability - the fundamental metric of data informativeness - is rarely treated as an explicit design factor. Unlike in multilevel modeling where the intraclass correlation (ICC) is routinely manipulated, IRT studies typically treat reliability as an incidental outcome, creating a ""reliability omission"" that obscures the signal-to-noise ratio of generated data. To address this gap, we introduce a principled framework for reliability-targeted simulation, transforming reliability from an implicit by-product into a precise input parameter. We formalize the inverse design problem, solving for a global discrimination scaling factor that uniquely achieves a pre-specified target reliability. Two complementary algorithms are proposed: Empirical Quadrature Calibration (EQC) for rapid, deterministic precision, and Stochastic Approximation Calibration (SAC) for rigorous stochastic estimation. A comprehensive validation study across 960 conditions demonstrates that EQC achieves essentially exact calibration, while SAC remains unbiased across non-normal latent distributions and empirical item pools. Furthermore, we clarify the theoretical distinction between average-information and error-variance-based reliability metrics, showing they require different calibration scales due to Jensen's inequality. An accompanying open-source R package, IRTsimrel, enables researchers to standardize reliability as a controlled experimental input.",JoonHo Lee,,https://arxiv.org/abs/2512.16012v2,https://arxiv.org/pdf/2512.16012v2,,,,,stat.ME,stat.ME|stat.CO,https://arxiv.org/pdf/2512.16012v2.pdf
2512.13892v2,2025-12-15T20:50:54Z,2025-12-23 12:54:15,"One Permutation Is All You Need: Fast, Reliable Variable Importance and Model Stress-Testing","Reliable estimation of feature contributions in machine learning models is essential for trust, transparency and regulatory compliance, especially when models are proprietary or otherwise operate as black boxes. While permutation-based methods are a standard tool for this task, classical implementations rely on repeated random permutations, introducing computational overhead and stochastic instability. In this paper, we show that by replacing multiple random permutations with a single, deterministic, and optimal permutation, we achieve a method that retains the core principles of permutation-based importance while being non-random, faster, and more stable. We validate this approach across nearly 200 scenarios, including real-world household finance and credit risk applications, demonstrating improved bias-variance tradeoffs and accuracy in challenging regimes such as small sample sizes, high dimensionality, and low signal-to-noise ratios. Finally, we introduce Systemic Variable Importance, a natural extension designed for model stress-testing that explicitly accounts for feature correlations. This framework provides a transparent way to quantify how shocks or perturbations propagate through correlated inputs, revealing dependencies that standard variable importance measures miss. Two real-world case studies demonstrate how this metric can be used to audit models for hidden reliance on protected attributes (e.g., gender or race), enabling regulators and practitioners to assess fairness and systemic risk in a principled and computationally efficient manner.",Albert Dorador,,https://arxiv.org/abs/2512.13892v2,https://arxiv.org/pdf/2512.13892v2,,,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/2512.13892v2.pdf
2512.06682v1,2025-12-07T06:38:57Z,2025-12-07 06:38:57,Partially Observable Markov Decision Process Framework for Operating Condition Optimization Using Real-Time Degradation Signals,"In many engineering systems, proper predictive maintenance and operational control are essential to increase efficiency and reliability while reducing maintenance costs. However, one of the major challenges is that many sensors are used for system monitoring. Analyzing these sensors simultaneously for better predictive maintenance optimization is often very challenging. In this paper, we propose a systematic decision-making framework to improve the system performance in manufacturing practice, considering the real-time degradation signals generated by multiple sensors. Specifically, we propose a partially observed Markov decision process (POMDP) model to generate the optimal capacity and predictive maintenance policies, given the fact that the observation of the system state is imperfect. Such work provides a systematic approach that focuses on jointly controlling the operating conditions and preventive maintenance utilizing the real-time machine deterioration signals by incorporating the degradation constraint and non-observable states. We apply this technique to the bearing degradation data and NASA aircraft turbofan engine dataset, demonstrating the effectiveness of the proposed method.",Boyang Xu|Yunyi Kang|Xinyu Zhao|Hao Yan|Feng Ju,,https://arxiv.org/abs/2512.06682v1,https://arxiv.org/pdf/2512.06682v1,,Accepted for publication in Journal of Quality Technology,,,stat.AP,stat.AP,https://arxiv.org/pdf/2512.06682v1.pdf
2512.06294v1,2025-12-06T04:45:31Z,2025-12-06 04:45:31,Interpretable Neural Approximation of Stochastic Reaction Dynamics with Guaranteed Reliability,"Stochastic Reaction Networks (SRNs) are a fundamental modeling framework for systems ranging from chemical kinetics and epidemiology to ecological and synthetic biological processes. A central computational challenge is the estimation of expected outputs across initial conditions and times, a task that is rarely solvable analytically and becomes computationally prohibitive with current methods such as Finite State Projection or the Stochastic Simulation Algorithm. Existing deep learning approaches offer empirical scalability, but provide neither interpretability nor reliability guarantees, limiting their use in scientific analysis and in applications where model outputs inform real-world decisions. Here we introduce DeepSKA, a neural framework that jointly achieves interpretability, guaranteed reliability, and substantial computational gains. DeepSKA yields mathematically transparent representations that generalise across states, times, and output functions, and it integrates this structure with a small number of stochastic simulations to produce unbiased, provably convergent, and dramatically lower-variance estimates than classical Monte Carlo. We demonstrate these capabilities across nine SRNs, including nonlinear and non-mass-action models with up to ten species, where DeepSKA delivers accurate predictions and orders-of-magnitude efficiency improvements. This interpretable and reliable neural framework offers a principled foundation for developing analogous methods for other Markovian systems, including stochastic differential equations.",Quentin Badolle|Arthur Theuer|Zhou Fang|Ankit Gupta|Mustafa Khammash,,https://arxiv.org/abs/2512.06294v1,https://arxiv.org/pdf/2512.06294v1,,,,,q-bio.MN,q-bio.MN|cs.LG|math.PR|q-bio.QM|stat.ML,https://arxiv.org/pdf/2512.06294v1.pdf
2512.04566v2,2025-12-04T08:29:17Z,2025-12-16 15:50:52,Reliable Statistical Guarantees for Conformal Predictors with Small Datasets,"Surrogate models (including deep neural networks and other machine learning algorithms in supervised learning) are capable of approximating arbitrarily complex, high-dimensional input-output problems in science and engineering, but require a thorough data-agnostic uncertainty quantification analysis before these can be deployed for any safety-critical application. The standard approach for data-agnostic uncertainty quantification is to use conformal prediction (CP), a well-established framework to build uncertainty models with proven statistical guarantees that do not assume any shape for the error distribution of the surrogate model. However, since the classic statistical guarantee offered by CP is given in terms of bounds for the marginal coverage, for small calibration set sizes (which are frequent in realistic surrogate modelling that aims to quantify error at different regions), the potentially strong dispersion of the coverage distribution around its average negatively impacts the relevance of the uncertainty model's statistical guarantee, often obtaining coverages below the expected value, resulting in a less applicable framework. After providing a gentle presentation of uncertainty quantification for surrogate models for machine learning practitioners, in this paper we bridge the gap by proposing a new statistical guarantee that offers probabilistic information for the coverage of a single conformal predictor. We show that the proposed framework converges to the standard solution offered by CP for large calibration set sizes and, unlike the classic guarantee, still offers relevant information about the coverage of a conformal predictor for small data sizes. We validate the methodology in a suite of examples, and implement an open access software solution that can be used alongside common conformal prediction libraries to obtain uncertainty models that fulfil the new guarantee.",Miguel Sánchez-Domínguez|Lucas Lacasa|Javier de Vicente|Gonzalo Rubio|Eusebio Valero,,https://arxiv.org/abs/2512.04566v2,https://arxiv.org/pdf/2512.04566v2,,,,,cs.LG,cs.LG|physics.data-an|stat.ML,https://arxiv.org/pdf/2512.04566v2.pdf
2512.03354v1,2025-12-03T01:37:42Z,2025-12-03 01:37:42,Breaking Determinism: Stochastic Modeling for Reliable Off-Policy Evaluation in Ad Auctions,"Online A/B testing, the gold standard for evaluating new advertising policies, consumes substantial engineering resources and risks significant revenue loss from deploying underperforming variations. This motivates the use of Off-Policy Evaluation (OPE) for rapid, offline assessment. However, applying OPE to ad auctions is fundamentally more challenging than in domains like recommender systems, where stochastic policies are common. In online ad auctions, it is common for the highest-bidding ad to win the impression, resulting in a deterministic, winner-takes-all setting. This results in zero probability of exposure for non-winning ads, rendering standard OPE estimators inapplicable. We introduce the first principled framework for OPE in deterministic auctions by repurposing the bid landscape model to approximate the propensity score. This model allows us to derive robust approximate propensity scores, enabling the use of stable estimators like Self-Normalized Inverse Propensity Scoring (SNIPS) for counterfactual evaluation. We validate our approach on the AuctionNet simulation benchmark and against 2-weeks online A/B test from a large-scale industrial platform. Our method shows remarkable alignment with online results, achieving a 92\% Mean Directional Accuracy (MDA) in CTR prediction, significantly outperforming the parametric baseline. MDA is the most critical metric for guiding deployment decisions, as it reflects the ability to correctly predict whether a new model will improve or harm performance. This work contributes the first practical and validated framework for reliable OPE in deterministic auction environments, offering an efficient alternative to costly and risky online experiments.",Hongseon Yeom|Jaeyoul Shin|Soojin Min|Jeongmin Yoon|Seunghak Yu|Dongyeop Kang,,https://arxiv.org/abs/2512.03354v1,https://arxiv.org/pdf/2512.03354v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2512.03354v1.pdf
2512.03109v1,2025-12-02T05:59:18Z,2025-12-02 05:59:18,E-valuator: Reliable Agent Verifiers with Sequential Hypothesis Testing,"Agentic AI systems execute a sequence of actions, such as reasoning steps or tool calls, in response to a user prompt. To evaluate the success of their trajectories, researchers have developed verifiers, such as LLM judges and process-reward models, to score the quality of each action in an agent's trajectory. Although these heuristic scores can be informative, there are no guarantees of correctness when used to decide whether an agent will yield a successful output. Here, we introduce e-valuator, a method to convert any black-box verifier score into a decision rule with provable control of false alarm rates. We frame the problem of distinguishing successful trajectories (that is, a sequence of actions that will lead to a correct response to the user's prompt) and unsuccessful trajectories as a sequential hypothesis testing problem. E-valuator builds on tools from e-processes to develop a sequential hypothesis test that remains statistically valid at every step of an agent's trajectory, enabling online monitoring of agents over arbitrarily long sequences of actions. Empirically, we demonstrate that e-valuator provides greater statistical power and better false alarm rate control than other strategies across six datasets and three agents. We additionally show that e-valuator can be used for to quickly terminate problematic trajectories and save tokens. Together, e-valuator provides a lightweight, model-agnostic framework that converts verifier heuristics into decisions rules with statistical guarantees, enabling the deployment of more reliable agentic systems.",Shuvom Sadhuka|Drew Prinster|Clara Fannjiang|Gabriele Scalia|Aviv Regev|Hanchen Wang,,https://arxiv.org/abs/2512.03109v1,https://arxiv.org/pdf/2512.03109v1,,,,,cs.LG,cs.LG|cs.AI|stat.AP|stat.ML,https://arxiv.org/pdf/2512.03109v1.pdf
2512.00640v1,2025-11-29T21:28:54Z,2025-11-29 21:28:54,A State-Space Approach to Modeling Tire Degradation in Formula 1 Racing,"Tire degradation plays a critical role in Formula 1 race strategy, influencing both lap times and optimal pit-stop decisions. This paper introduces a Bayesian state-space modeling framework for estimating the latent degradation dynamics of Formula 1 tires using publicly available timing data from the FastF1 Python API. Lap times are modeled as a function of fuel mass and latent tire pace, with pit stops represented as state resets. Several model extensions are explored, including compound-specific degradation rates, time-varying degradation dynamics, and a skewed t observation model to account for asymmetric driver errors. Using Lewis Hamilton's performance in the 2025 Austrian Grand Prix as a case study, the proposed framework demonstrates superior predictive performance over an ARIMA(2,1,2) baseline, particularly under the skewed t specification. Although compound-specific degradation differences were not statistically distinct, the results show that the state-space approach provides interpretable, probabilistic, and computationally efficient estimates of tire degradation. This framework can be generalized to multi-race or multi-driver analyses, offering a foundation for real-time strategy modeling and performance prediction in Formula 1 racing.",Cole Cappello|Andrew Hoegh,,https://arxiv.org/abs/2512.00640v1,https://arxiv.org/pdf/2512.00640v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2512.00640v1.pdf
2512.08956v1,2025-11-28T09:26:45Z,2025-11-28 09:26:45,DW-KNN: A Transparent Local Classifier Integrating Distance Consistency and Neighbor Reliability,"K-Nearest Neighbors (KNN) is one of the most used ML classifiers. However, if we observe closely, standard distance-weighted KNN and relative variants assume all 'k' neighbors are equally reliable. In heterogeneous feature space, this becomes a limitation that hinders reliability in predicting true levels of the observation.
  We propose DW-KNN (Double Weighted KNN), a transparent and robust variant that integrates exponential distance with neighbor validity. This enables instance-level interpretability, suppresses noisy or mislabeled samples, and reduces hyperparameter sensitivity.
  Comprehensive evaluation on 9 data-sets helps to demonstrate that DW-KNN achieves 0.8988 accuracy on average. It ranks 2nd among six methods and within 0.2% of the best-performing Ensemble KNN. It also exhibits the lowest cross-validation variance (0.0156), indicating reliable prediction stability. Statistical significance test confirmed ($p < 0.001$) improvement over compactness weighted KNN (+4.09\%) and Kernel weighted KNN (+1.13\%). The method provides a simple yet effective alternative to complex adaptive schemes, particularly valuable for high-stakes applications requiring explainable predictions.",Kumarjit Pathak|Karthik K|Sachin Madan|Jitin Kapila,,https://arxiv.org/abs/2512.08956v1,https://arxiv.org/pdf/2512.08956v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2512.08956v1.pdf
2511.18464v1,2025-11-23T14:15:50Z,2025-11-23 14:15:50,Reliable Selection of Heterogeneous Treatment Effect Estimators,"We study the problem of selecting the best heterogeneous treatment effect (HTE) estimator from a collection of candidates in settings where the treatment effect is fundamentally unobserved. We cast estimator selection as a multiple testing problem and introduce a ground-truth-free procedure based on a cross-fitted, exponentially weighted test statistic. A key component of our method is a two-way sample splitting scheme that decouples nuisance estimation from weight learning and ensures the stability required for valid inference. Leveraging a stability-based central limit theorem, we establish asymptotic familywise error rate control under mild regularity conditions. Empirically, our procedure provides reliable error control while substantially reducing false selections compared with commonly used methods across ACIC 2016, IHDP, and Twins benchmarks, demonstrating that our method is feasible and powerful even without ground-truth treatment effects.",Jiayi Guo|Zijun Gao,,https://arxiv.org/abs/2511.18464v1,https://arxiv.org/pdf/2511.18464v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2511.18464v1.pdf
2511.16164v1,2025-11-20T09:05:39Z,2025-11-20 09:05:39,Achieving Skilled and Reliable Daily Probabilistic Forecasts of Wind Power at Subseasonal-to-Seasonal Timescales over France,"Accurate and reliable wind power forecasts are crucial for grid stability, balancing supply and demand, and market risk management. Even though short-term weather forecasts have been thoroughly used to provide short-term renewable power predictions, forecasts involving longer prediction horizons still need investigations. Despite the recent progress in subseasonal-to-seasonal weather probabilistic forecasting, their use for wind power prediction usually involves both temporal and spatial aggregation achieve reasonable skill. In this study, we present a forecasting pipeline enabling to transform ECMWF subseasonal-to-seasonal weather forecasts into wind power forecasts for lead times ranging from 1 day to 46 days at daily resolution. This framework also include post-processing of the resulting power ensembles to account for the biases and lack of dispersion of the weather forecasts. We show that our method is able to outperform a climatological baseline by 50 % in terms of both Continuous Ranked Probability Skill Score and Ensemble Mean Squared Error while also providing near perfect calibration of the forecasts for lead times ranging from 15 to 46 days.",Eloi Lindas|Yannig Goude|Philippe Ciais,,https://arxiv.org/abs/2511.16164v1,https://arxiv.org/pdf/2511.16164v1,,,,,cs.LG,cs.LG|stat.AP,https://arxiv.org/pdf/2511.16164v1.pdf
2511.08952v1,2025-11-12T03:55:59Z,2025-11-12 03:55:59,A new approach to reliability assessment based on Exploratory factor analysis,"We need to collect data in any science and reliability is a fundamental problem for measurement in all of science. Reliability means calculation the variance ratio. Reliability was defined as the fraction of an observed score variance that was not error. here are a lot of methods to estimated reliability. All of these indicators of dependability and stability are in contradiction to the long held belief that a problem with test-retest reliability is that it introduces memory effects of learning and practice. As a result, Kuder and Richardson developed a method named KR20 before advances in computational speed made it trivial to find the factor structure of tests, and were based upon test and item variances. These procedures were essentially short cuts for estimating reliability. Exploratory factor analysis is also a Traditional method to calculate the reliability. It focus on only one variable in the liner model, a statistical method that can be used to collect an important type of validity evidence. but in reality, we need to focus on many more variables. So we will introduce a novel method following.",Shibo Diao,,https://arxiv.org/abs/2511.08952v1,https://arxiv.org/pdf/2511.08952v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2511.08952v1.pdf
2511.07038v1,2025-11-10T12:36:35Z,2025-11-10 12:36:35,Conservative Software Reliability Assessments Using Collections of Bayesian Inference Problems,"When using Bayesian inference to support conservative software reliability assessments, it is useful to consider a collection of Bayesian inference problems, with the aim of determining the worst-case value (from this collection) for a posterior predictive probability that characterizes how reliable the software is. Using a Bernoulli process to model the occurrence of software failures, we explicitly determine (from collections of Bayesian inference problems) worst-case posterior predictive probabilities of the software operating without failure in the future. We deduce asymptotic properties of these conservative posterior probabilities and their priors, and illustrate how to use these results in assessments of safety-critical software. This work extends robust Bayesian inference results and so-called conservative Bayesian inference methods.",Kizito Salako|Rabiu Tsoho Muhammad,,https://arxiv.org/abs/2511.07038v1,https://arxiv.org/pdf/2511.07038v1,,,,,stat.AP,stat.AP|cs.SE,https://arxiv.org/pdf/2511.07038v1.pdf
2511.11613v1,2025-11-04T03:26:11Z,2025-11-04 03:26:11,Physics-Informed Neural Network-based Reliability Analysis of Buried Pipelines,"Buried pipelines transporting oil and gas across geohazard-prone regions are exposed to potential ground movement, leading to the risk of significant strain demand and structural failure. Reliability analysis, which determines the probability of failure after accounting for pertinent uncertainties, is essential for ensuring the safety of pipeline systems. However, traditional reliability analysis methods involving computationally intensive numerical models, such as finite element simulations of pipeline subjected to ground movement, have limited applications; this is partly because stochastic sampling approaches require repeated simulations over a large number of samples for the uncertain variables when estimating low probabilities. This study introduces Physics-Informed Neural Network for Reliability Analysis (PINN-RA) for buried pipelines subjected to ground movement, which integrates PINN-based surrogate model with Monte Carlo Simulation (MCS) to achieve efficient reliability assessment. To enable its application under uncertain variables associated with soil properties and ground movement, the PINN-based surrogate model is extended to solve a parametric differential equation system, namely the governing equation of pipelines embedded in soil with different properties. The findings demonstrate that PINN-RA significantly reduces the computational effort required and thus accelerates reliability analysis. By eliminating the need for repetitive numerical evaluations of pipeline subjected to permanent ground movement, the proposed approach provides an efficient and scalable tool for pipeline reliability assessment, enabling rapid decision-making in geohazard-prone regions.",Pouya Taraghi|Yong Li|Samer Adeeb,,https://arxiv.org/abs/2511.11613v1,https://arxiv.org/pdf/2511.11613v1,,This manuscript has been submitted to Reliability Engineering & System Safety and is currently under peer review,,,cs.CE,cs.CE|cs.LG|physics.data-an|stat.AP,https://arxiv.org/pdf/2511.11613v1.pdf
2511.00772v1,2025-11-02T02:45:54Z,2025-11-02 02:45:54,Reliable Curation of EHR Dataset via Large Language Models under Environmental Constraints,"Electronic health records (EHRs) are central to modern healthcare delivery and research; yet, many researchers lack the database expertise necessary to write complex SQL queries or generate effective visualizations, limiting efficient data use and scientific discovery. To address this barrier, we introduce CELEC, a large language model (LLM)-powered framework for automated EHR data extraction and analytics. CELEC translates natural language queries into SQL using a prompting strategy that integrates schema information, few-shot demonstrations, and chain-of-thought reasoning, which together improve accuracy and robustness. On a subset of the EHRSQL benchmark, CELEC achieves execution accuracy comparable to prior systems while maintaining low latency, cost efficiency, and strict privacy by exposing only database metadata to the LLM. CELEC also adheres to strict privacy protocols: the LLM accesses only database metadata (e.g., table and column names), while all query execution occurs securely within the institutional environment, ensuring that no patient-level data is ever transmitted to or shared with the LLM. Ablation studies confirm that each component of the SQL generation pipeline, particularly the few-shot demonstrations, plays a critical role in performance. By lowering technical barriers and enabling medical researchers to query EHR databases directly, CELEC streamlines research workflows and accelerates biomedical discovery.",Raymond M. Xiong|Panyu Chen|Tianze Dong|Jian Lu|Benjamin Goldstein|Danyang Zhuo|Anru R. Zhang,,https://arxiv.org/abs/2511.00772v1,https://arxiv.org/pdf/2511.00772v1,,,,,cs.DB,cs.DB|cs.LG|stat.AP,https://arxiv.org/pdf/2511.00772v1.pdf
2511.00553v1,2025-11-01T13:35:15Z,2025-11-01 13:35:15,Breaking Down the Scoring: Interrater Reliability and National Bias in Olympic Breaking,"Introduction: The inclusion of Breaking in the 2024 Paris Olympic Games introduced a distinctive competition format in which two athletes compete head-to-head in battle rounds. Unlike other aesthetic sports, where athletes receive independent scores for their performances, Breaking judges assess the relative performance quality between two competitors across five predefined evaluation aspects. This may considerably increase their cognitive load and warrants a thorough examination of judging reliability and potential national bias, the tendency of judges to favor athletes from their own country. Methods: Official scoring data from the 2024 Olympic Breaking events were analyzed. Interrater reliability was assessed using the Intraclass Correlation Coefficient, while national bias was estimated using mixed-effects modelling based on judges' individual scores. Results: The analyses revealed low to moderate interrater reliability in Breaking, notably lower than levels reported in other aesthetic sports. Moreover, substantial national bias was detected, with judges displaying a systematic tendency to favor athletes from their own country. Discussion: Although the observed national bias, despite its notable magnitude, likely had only a limited effect on final competition outcomes, the relatively low interrater reliability highlights potential weaknesses in the current scoring framework that may warrant refinement. Nonetheless, Breaking's unique system of aggregating judges' scores into discrete votes appears to reduce the influence of individual judges on competition outcomes, thereby enhancing the system's robustness against unilateral score manipulation.",Patrick Alexander Braeunig,"Sportwissenschaftliches Institut, Universität des Saarlandes",https://arxiv.org/abs/2511.00553v1,https://arxiv.org/pdf/2511.00553v1,,"26 pages, 2 figures",,,physics.soc-ph,physics.soc-ph|stat.AP,https://arxiv.org/pdf/2511.00553v1.pdf
2511.00395v2,2025-11-01T04:33:51Z,2025-11-16 05:35:52,Is Representational Similarity Analysis Reliable? A Comparison with Regression,"Representational Similarity Analysis (RSA) is a popular method for analyzing neuroimaging and behavioral data. Here we evaluate the accuracy and reliability of RSA in the context of model selection, and compare it to that of regression. Although RSA offers flexibility in handling high-dimensional, cross-modal, and cross-species data, its reliance on a transformation of raw data into similarity structures may result in the loss of critical stimulus-response information. Across extensive simulation studies and empirical analyses, we show that RSA leads to lower model selection accuracy, regardless of sample size, noise level, feature dimensionality, or multicollinearity, relative to regression. While principal component analysis and feature reweighting mitigate RSA's deficits driven by multicollinearity, regression remains superior in accurately distinguishing between models. Empirical data and a follow-up fMRI simulation further support these conclusions. Our findings suggest that researchers should carefully consider which approach to use: RSA is less effective than linear regression for model selection and fitting when direct stimulus-response mappings are available.",Chuanji Gao|Gang Chen|Svetlana V. Shinkareva|Rutvik H. Desai,,https://arxiv.org/abs/2511.00395v2,https://arxiv.org/pdf/2511.00395v2,,,,,stat.ME,stat.ME|stat.CO,https://arxiv.org/pdf/2511.00395v2.pdf
2510.23874v3,2025-10-27T21:26:20Z,2025-11-01 04:07:39,From Stochasticity to Signal: A Bayesian Latent State Model for Reliable Measurement with LLMs,"Large Language Models (LLMs) are increasingly used to automate classification tasks in business, such as analyzing customer satisfaction from text. However, the inherent stochasticity of LLMs, in terms of their tendency to produce different outputs for the same input, creates a significant measurement error problem that is often neglected with a single round of output, or addressed with ad-hoc methods like majority voting. Such naive approaches fail to quantify uncertainty and can produce biased estimates of population-level metrics. In this paper, we propose a principled solution by reframing LLM variability as a statistical measurement error problem and introducing a Bayesian latent state model to address it. Our model treats the true classification (e.g., customer dissatisfaction) as an unobserved latent variable and the multiple LLM ratings as noisy measurements of this state. This framework allows for the simultaneous estimation of the LLM's false positive and false negative error rates, the underlying base rate of the phenomenon in the population, the posterior probability of the true state for each individual observation, and the causal impact of a business intervention, if any, on the latent state. Through simulation studies, we demonstrate that our model accurately recovers true parameters where naive methods fail. We conclude that this methodology provides a general and reliable framework for converting noisy, probabilistic outputs from LLMs into accurate and actionable insights for scientific and business applications.",Yichi Zhang|Ignacio Martinez,,https://arxiv.org/abs/2510.23874v3,https://arxiv.org/pdf/2510.23874v3,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2510.23874v3.pdf
2510.23666v1,2025-10-26T14:44:19Z,2025-10-26 14:44:19,Beyond Normality: Reliable A/B Testing with Non-Gaussian Data,"A/B testing has become the cornerstone of decision-making in online markets, guiding how platforms launch new features, optimize pricing strategies, and improve user experience. In practice, we typically employ the pairwise $t$-test to compare outcomes between the treatment and control groups, thereby assessing the effectiveness of a given strategy. To be trustworthy, these experiments must keep Type I error (i.e., false positive rate) under control; otherwise, we may launch harmful strategies. However, in real-world applications, we find that A/B testing often fails to deliver reliable results. When the data distribution departs from normality or when the treatment and control groups differ in sample size, the commonly used pairwise $t$-test is no longer trustworthy. In this paper, we quantify how skewed, long tailed data and unequal allocation distort error rates and derive explicit formulas for the minimum sample size required for the $t$-test to remain valid. We find that many online feedback metrics require hundreds of millions samples to ensure reliable A/B testing. Thus we introduce an Edgeworth-based correction that provides more accurate $p$-values when the available sample size is limited. Offline experiments on a leading A/B testing platform corroborate the practical value of our theoretical minimum sample size thresholds and demonstrate that the corrected method substantially improves the reliability of A/B testing in real-world conditions.",Junpeng Gong|Chunkai Wang|Hao Li|Jinyong Ma|Haoxuan Li|Xu He,,https://arxiv.org/abs/2510.23666v1,https://arxiv.org/pdf/2510.23666v1,,"11 pages, 3 figures",,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2510.23666v1.pdf
2510.22558v1,2025-10-26T07:29:01Z,2025-10-26 07:29:01,Surface decomposition method for sensitivity analysis of first-passage dynamic reliability of linear systems,"This work presents a novel surface decomposition method for the sensitivity analysis of first-passage dynamic reliability of linear systems subjected to Gaussian random excitations. The method decomposes the sensitivity of first-passage failure probability into a sum of surface integrals over the constrained component limit-state hypersurfaces. The evaluation of these surface integrals can be accomplished, owing to the availability of closed-form linear expressions of both the component limit-state functions and their sensitivities for linear systems. An importance sampling strategy is introduced to further enhance the efficiency for estimating the sum of these surface integrals. The number of function evaluations required for the reliability sensitivity analysis is typically on the order of 10^2 to 10^3. The approach is particularly advantageous when a large number of design parameters are considered, as the results of function evaluations can be reused across different parameters. Two numerical examples are investigated to demonstrate the effectiveness of the proposed method.",Jianhua Xian|Sai Hung Cheung|Cheng Su,,https://arxiv.org/abs/2510.22558v1,https://arxiv.org/pdf/2510.22558v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2510.22558v1.pdf
2510.21335v1,2025-10-24T10:59:21Z,2025-10-24 10:59:21,Conditional Forecasts and Proper Scoring Rules for Reliable and Accurate Performative Predictions,"Performative predictions are forecasts which influence the outcomes they aim to predict, undermining the existence of correct forecasts and standard methods of elicitation and estimation. We show that conditioning forecasts on covariates that separate them from the outcome renders the target distribution forecast-invariant, guaranteeing well-posedness of the forecasting problem. However, even under this condition, classical proper scoring rules fail to elicit correct forecasts. We prove a general impossibility result and identify two solutions: (i) in decision-theoretic settings, elicitation of correct and incentive-compatible forecasts is possible if forecasts are separating; (ii) scoring with unbiased estimates of the divergence between the forecast and the induced distribution of the target variable yields correct forecasts. Applying these insights to parameter estimation, conditional forecasts and proper scoring rules enable performatively stable estimation of performatively correct parameters, resolving the issues raised by Perdomo et al. (2020). Our results expose fundamental limits of classical forecast evaluation and offer new tools for reliable and accurate forecasting in performative settings.",Philip Boeken|Onno Zoeter|Joris M. Mooij,,https://arxiv.org/abs/2510.21335v1,https://arxiv.org/pdf/2510.21335v1,,,,,math.ST,math.ST|stat.ML,https://arxiv.org/pdf/2510.21335v1.pdf
2510.17543v2,2025-10-20T13:52:58Z,2025-10-24 10:49:01,Reliable Inference in Edge-Cloud Model Cascades via Conformal Alignment,"Edge intelligence enables low-latency inference via compact on-device models, but assuring reliability remains challenging. We study edge-cloud cascades that must preserve conditional coverage: whenever the edge returns a prediction set, it should contain the true label with a user-specified probability, as if produced by the cloud model. We formalize conditional coverage with respect to the cloud predictive distribution, and introduce a conformal alignment-based (CAb) cascading mechanism that certifies this property with user control over the risk level. Our method casts escalation from edge to cloud models as a multiple-hypothesis testing (MHT) problem, tailoring conformal alignment (CA) to select which inputs can be safely handled at the edge. The proposed CAb model cascading method yields statistical guarantees on the average fraction of edge decisions that satisfy cloud-level conditional coverage. The procedure applies to arbitrary edge prediction sets, including variants of conformal prediction (CP), and exposes a tunable trade-off among coverage, deferral rate, and set size. Experiments on CIFAR-100 image classification and the TeleQnA question-answering (QA) benchmark show that the proposed CAb cascade maintains the target conditional coverage for edge predictions while substantially reducing offloading to the cloud and incurring modest increases in prediction-set size.",Jiayi Huang|Sangwoo Park|Nicola Paoletti|Osvaldo Simeone,,https://arxiv.org/abs/2510.17543v2,https://arxiv.org/pdf/2510.17543v2,,Under Review,,,cs.LG,cs.LG|eess.SP|stat.ML,https://arxiv.org/pdf/2510.17543v2.pdf
2510.17472v2,2025-10-20T12:14:12Z,2025-10-23 14:03:17,Certified Self-Consistency: Statistical Guarantees and Test-Time Training for Reliable Reasoning in LLMs,"Recent advances such as self-consistency and test-time reinforcement learning (TTRL) improve the reliability of large language models (LLMs) without additional supervision, yet their underlying mechanisms and statistical guarantees remain poorly understood. We present a unified framework for certifiable inference in LLMs, showing that majority voting provides a statistical certificate of self-consistency: under mild assumptions, the aggregated answer coincides with the mode of the model's terminal distribution with high probability. We derive finite-sample and anytime-valid concentration bounds that quantify this confidence, and introduce the Martingale Majority Certificate (MMC), a sequential stopping rule that adaptively determines when sufficient samples have been drawn. We further prove that label-free post-training methods such as TTRL implicitly sharpen the answer distribution by exponentially tilting it toward its mode, thereby reducing the number of samples required for certification. Building on this insight, we propose new post-training objectives that explicitly optimise this trade-off between sharpness and bias. Together, these results explain and connect two central test-time scaling strategies, self-consistency and TTRL, within a single statistical framework for label-free, certifiable reliability in reasoning LLMs.",Paula Cordero-Encinar|Andrew B. Duncan,,https://arxiv.org/abs/2510.17472v2,https://arxiv.org/pdf/2510.17472v2,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2510.17472v2.pdf
2510.17085v1,2025-10-20T01:27:59Z,2025-10-20 01:27:59,Data Reliability Scoring,"How can we assess the reliability of a dataset without access to ground truth? We introduce the problem of reliability scoring for datasets collected from potentially strategic sources. The true data are unobserved, but we see outcomes of an unknown statistical experiment that depends on them. To benchmark reliability, we define ground-truth-based orderings that capture how much reported data deviate from the truth. We then propose the Gram determinant score, which measures the volume spanned by vectors describing the empirical distribution of the observed data and experiment outcomes. We show that this score preserves several ground-truth based reliability orderings and, uniquely up to scaling, yields the same reliability ranking of datasets regardless of the experiment -- a property we term experiment agnosticism. Experiments on synthetic noise models, CIFAR-10 embeddings, and real employment data demonstrate that the Gram determinant score effectively captures data quality across diverse observation processes.",Yiling Chen|Shi Feng|Paul Kattuman|Fang-Yi Yu,,https://arxiv.org/abs/2510.17085v1,https://arxiv.org/pdf/2510.17085v1,,"39 pages, 5 figures",,,cs.LG,cs.LG|cs.GT|stat.ML,https://arxiv.org/pdf/2510.17085v1.pdf
2510.16937v1,2025-10-19T17:21:36Z,2025-10-19 17:21:36,Prediction-Augmented Trees for Reliable Statistical Inference,"The remarkable success of machine learning (ML) in predictive tasks has led scientists to incorporate ML predictions as a core component of the scientific discovery pipeline. This was exemplified by the landmark achievement of AlphaFold (Jumper et al. (2021)). In this paper, we study how ML predictions can be safely used in statistical analysis of data towards scientific discovery. In particular, we follow the framework introduced by Angelopoulos et al. (2023). In this framework, we assume access to a small set of $n$ gold-standard labeled samples, a much larger set of $N$ unlabeled samples, and a ML model that can be used to impute the labels of the unlabeled data points. We introduce two new learning-augmented estimators: (1) Prediction-Augmented Residual Tree (PART), and (2) Prediction-Augmented Quadrature (PAQ). Both estimators have significant advantages over existing estimators like PPI and PPI++ introduced by Angelopoulos et al. (2023) and Angelopoulos et al. (2024), respectively. PART is a decision-tree based estimator built using a greedy criterion. We first characterize PART's asymptotic distribution and demonstrate how to construct valid confidence intervals. Then we show that PART outperforms existing methods in real-world datasets from ecology, astronomy, and census reports, among other domains. This leads to estimators with higher confidence, which is the result of using both the gold-standard samples and the machine learning predictions. Finally, we provide a formal proof of the advantage of PART by exploring PAQ, an estimation that arises when considering the limit of PART when the depth its tree grows to infinity. Under appropriate assumptions in the input data we show that the variance of PAQ shrinks at rate of $O(N^{-1} + n^{-4})$, improving significantly on the $O(N^{-1}+n^{-1})$ rate of existing methods.",Vikram Kher|Argyris Oikonomou|Manolis Zampetakis,,https://arxiv.org/abs/2510.16937v1,https://arxiv.org/pdf/2510.16937v1,,"45 pages, 9 Figures",,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2510.16937v1.pdf
2510.16740v1,2025-10-19T07:33:03Z,2025-10-19 07:33:03,Bayesian reliability acceptance sampling plans for competing risks data under interval censoring,"We obtain a reliability acceptance sampling plan for independent competing risk data under interval censoring schemes using the Bayesian approach. At first, the Bayesian reliability acceptance sampling plan is obtained where the decision criteria of accepting a lot is pre-fixed. For large samples, computing Bayes risk is computationally intensive. Therefore, an approximate Bayes risk is obtained using the asymptotic properties of the maximum likelihood estimators. Lastly, the Bayesian reliability acceptance sampling plan is obtained, where the decision function is arbitrary. The manufacturer can derive an optimal decision function by minimizing the Bayes risk among all decision functions. This optimal decision function is known as Bayes decision function. The optimal sampling plan is obtained by minimizing the Bayes risk. The algorithms are provided for the computation of optimum Bayesian reliability acceptance sampling plan. Numerical results are provided and comparisons between the Bayesian reliability acceptance sampling plans are carried out.",Biswabrata Pradhan|Rathin Das,,https://arxiv.org/abs/2510.16740v1,https://arxiv.org/pdf/2510.16740v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2510.16740v1.pdf
2510.15013v1,2025-10-16T14:10:24Z,2025-10-16 14:10:24,Reliable data clustering with Bayesian community detection,"From neuroscience and genomics to systems biology and ecology, researchers rely on clustering similarity data to uncover modular structure. Yet widely used clustering methods, such as hierarchical clustering, k-means, and WGCNA, lack principled model selection, leaving them susceptible to noise. A common workaround sparsifies a correlation matrix representation to remove noise before clustering, but this extra step introduces arbitrary thresholds that can distort the structure and lead to unreliable results. To detect reliable clusters, we capitalize on recent advances in network science to unite sparsification and clustering with principled model selection. We test two Bayesian community detection methods, the Degree-Corrected Stochastic Block Model and the Regularized Map Equation, both grounded in the Minimum Description Length principle for model selection. In synthetic data, they outperform traditional approaches, detecting planted clusters under high-noise conditions and with fewer samples. Compared to WGCNA on gene co-expression data, the Regularized Map Equation identifies more robust and functionally coherent gene modules. Our results establish Bayesian community detection as a principled and noise-resistant framework for uncovering modular structure in high-dimensional data across fields.",Magnus Neuman|Jelena Smiljanić|Martin Rosvall,,https://arxiv.org/abs/2510.15013v1,https://arxiv.org/pdf/2510.15013v1,,,,,stat.ML,stat.ML|cs.LG|physics.data-an|stat.ME,https://arxiv.org/pdf/2510.15013v1.pdf
2510.11506v1,2025-10-13T15:16:45Z,2025-10-13 15:16:45,Algorithmic analysis of a complex reliability system subject to multiple events with a preventive maintenance strategy and a Bernoulli vacation policy through MMAPs,"In this work, a single-unit multi-state system is considered. The system is subject to internal failures, as well as external shocks with multiple consequences. It also incorporates a preventive maintenance strategy and a Bernoulli vacation policy for the repairperson. It is algorithmically modeled in both continuous and discrete time using Marked Markovian Arrival Processes (MMAP). The system's operation/degradation level is divided into an indeterminate number of levels. Upon returning from a vacation period, the repair technician may initiate corrective repair, perform preventive maintenance, replace the unit, remain idle at the workplace, or begin a new vacation period. The decision in the latter two cases is made probabilistically based on the system's operational level. This methodology allows the model and its associated measures to be algorithmically derived in both transient and stationary regimes, presented in a matrix-algorithmic form. Analytical-matrix methods are used to obtain the system's steady-state behaviour as well as various performance measures. Costs and rewards are introduced to analyze when the system becomes profitable. Measures associated with costs over time and in the stationary regime are defined and considered for optimization studies. A numerical example demonstrates the versatility of the model by solving a probabilistic optimization problem using a multi-objective Pareto analysis approach and performing a comparative evaluation of multiple models. Genetic algorithm is applied to find the optimization results in the reduced solution space. All modeling and associated measures have been computationally implemented in Matlab.",Juan Eloy Ruiz-Castro|Hugo Alaín Zapata-Ceballos,,https://arxiv.org/abs/2510.11506v1,https://arxiv.org/pdf/2510.11506v1,https://doi.org/10.1016/j.ress.2025.111744,,"Reliability Engineering and System Safety, 266, 111744, 2026",10.1016/j.ress.2025.111744,stat.ME,stat.ME,https://arxiv.org/pdf/2510.11506v1.pdf
2510.09315v1,2025-10-10T12:14:17Z,2025-10-10 12:14:17,Reliability Sensitivity with Response Gradient,"Engineering risk is concerned with the likelihood of failure and the scenarios when it occurs. The sensitivity of failure probability to change in system parameters is relevant to risk-informed decision making. Computing sensitivity is at least one level more difficult than the probability itself, which is already challenged by a large number of input random variables, rare events and implicit nonlinear `black-box' response. Finite difference with Monte Carlo probability estimates is spurious, requiring the number of samples to grow with the reciprocal of step size to suppress estimation variance. Many existing works gain efficiency by exploiting a specific class of input variables, sensitivity parameters, or response in its exact or surrogate form. For general systems, this work presents a theory and associated Monte Carlo strategy for computing sensitivity using response values and gradients with respect to sensitivity parameters. It is shown that the sensitivity at a given response threshold can be expressed via the expectation of response gradient conditional on the threshold. Determining the expectation requires conditioning on the threshold that is a zero-probability event, but it can be resolved by the concept of kernel smoothing. The proposed method offers sensitivity estimates for all response thresholds generated in a single Monte Carlo run. It is investigated in a number of examples featuring sensitivity parameters of different nature. As response gradient becomes increasingly available, it is hoped that this work can provide the basis for embedding sensitivity calculations with reliability in the same Monte Carlo run.",Siu-Kui Au|Zi-Jun Cao,,https://arxiv.org/abs/2510.09315v1,https://arxiv.org/pdf/2510.09315v1,https://doi.org/10.1016/j.strusafe.2025.102683,"45 pages, 8 figures. Submitted to Structural Safety (Elsevier) on 5 Oct 2025",,10.1016/j.strusafe.2025.102683,stat.ME,stat.ME|cs.LG|stat.ML,https://arxiv.org/pdf/2510.09315v1.pdf
2510.04087v2,2025-10-05T08:23:08Z,2025-10-10 21:47:22,Best of mini-N in-loop Sampling: A Contextual Quality Reward Model for Reliable and Efficient Best-of-N Sampling,"Modern preference alignment techniques, such as Best-of-N (BoN) sampling, rely on reward models trained with pairwise comparison data. While effective at learning relative preferences, this paradigm fails to capture a signal of response acceptability, leaving systems vulnerable to selecting the least bad of many unacceptable options. This is particularly problematic for hard prompts, where the risk of such false acceptances increases with the number of samples. In this paper, we address this critical reliability gap by introducing a new data collection and modeling framework. By augmenting preference data with an outside option, inspired by discrete choice models, we train a reward model that can distinguish not just what is better, but what is good enough. We leverage this capability to create an adaptive inference strategy, best of mini-N in-loop, which partitions the generation budget into sequential loops with a calibrated, early-exit condition. Our experiments show that when tuned as an alignment guardrail, it reduces reliability failures by 70%, and when tuned as an inference accelerator, it improves average inference speed by over 22% in IMDB-sentiment setting. We thus provide a principled and flexible framework for practitioners to explicitly manage the trade-off between reliability and computational efficiency.",Hyung Gyu Rho|Sian Lee,,https://arxiv.org/abs/2510.04087v2,https://arxiv.org/pdf/2510.04087v2,https://doi.org/10.21203/rs.3.rs-7594024/v1,,,10.21203/rs.3.rs-7594024/v1,stat.ME,stat.ME|cs.AI|cs.LG,https://arxiv.org/pdf/2510.04087v2.pdf
2509.18459v1,2025-09-22T22:29:17Z,2025-09-22 22:29:17,Evaluating Bias Reduction Methods in Binary Emax Model for Reliable Dose-Response Estimation,"The Binary Emax model is widely employed in dose-response analysis during Phase II clinical studies to identify the optimal dose for subsequence confirmatory trials. The parameter estimation and inference heavily rely on the asymptotic properties of Maximum Likelihood (ML) estimators; however, this approach may be questionable under small or moderate sample sizes and is not robust to violation of model assumptions. To provide a reliable solution, this paper examines three bias-reduction methods: the Cox-Snell bias correction, Firth-score modification, and a maximum penalized likelihood estimator (MPLE) using Jeffreys prior. Through comprehensive simulation studies, we evaluate the performance of these methods in reducing bias and controlling variance, especially when model assumptions are violated. The results demonstrate that both Firth and MPLE methods provide robust estimates, with MPLE outperforming in terms of stability and lower variance. We further illustrate the practical application of these methods using data from the TURANDOT study, a Phase II clinical trial. Our findings suggest that MPLE with Jeffreys prior offers an effective and reliable alternative to the Firth method, particularly for dose-response relationships that deviate from monotonicity, making it valuable for robust parameter estimation in dose-ranging studies.",Jiangshan Zhang|Vivek Pradhan|Yuxi Zhao,,https://arxiv.org/abs/2509.18459v1,https://arxiv.org/pdf/2509.18459v1,https://doi.org/10.1080/10543406.2026.2627387,,,10.1080/10543406.2026.2627387,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/2509.18459v1.pdf
2509.12420v2,2025-09-15T20:16:09Z,2025-09-17 18:32:51,System Reliability Estimation via Shrinkage,"In a coherent reliability system composed of multiple components configured according to a specific structure function, the distribution of system time to failure, or system lifetime, is often of primary interest. Accurate estimation of system reliability is critical in a wide range of engineering and industrial applications, forming decisions in system design, maintenance planning, and risk assessment. The system lifetime distribution can be estimated directly using the observed system failure times. However, when component-level lifetime data is available, it can yield improved estimates of system reliability. In this work, we demonstrate that under nonparametric assumptions about the component time-to-failure distributions, traditional estimators such as the Product-Limit Estimator (PLE) can be further improved under specific loss functions. We propose a novel methodology that enhances the nonparametric system reliability estimation through a shrinkage transformation applied to component-level estimators. This shrinkage approach leads to improved efficiency in estimating system reliability.",Beidi Qiang|Edsel Pena,,https://arxiv.org/abs/2509.12420v2,https://arxiv.org/pdf/2509.12420v2,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2509.12420v2.pdf
2508.12896v1,2025-08-18T12:53:38Z,2025-08-18 12:53:38,"Reliability, Embeddedness, and Agency: A Utility-Driven Mathematical Framework for Agent-Centric AI Adoption","We formalize three design axioms for sustained adoption of agent-centric AI systems executing multi-step tasks: (A1) Reliability > Novelty; (A2) Embed > Destination; (A3) Agency > Chat. We model adoption as a sum of a decaying novelty term and a growing utility term and derive the phase conditions for troughs/overshoots with full proofs. We introduce: (i) an identifiability/confounding analysis for $(α,β,N_0,U_{\max})$ with delta-method gradients; (ii) a non-monotone comparator (logistic-with-transient-bump) evaluated on the same series to provide additional model comparison; (iii) ablations over hazard families $h(\cdot)$ mapping $ΔV \to β$; (iv) a multi-series benchmark (varying trough depth, noise, AR structure) reporting coverage (type-I error, power); (v) calibration of friction proxies against time-motion/survey ground truth with standard errors; (vi) residual analyses (autocorrelation and heteroskedasticity) for each fitted curve; (vii) preregistered windowing choices for pre/post estimation; (viii) Fisher information & CRLB for $(α,β)$ under common error models; (ix) microfoundations linking $\mathcal{T}$ to $(N_0,U_{\max})$; (x) explicit comparison to bi-logistic, double-exponential, and mixture models; and (xi) threshold sensitivity to $C_f$ heterogeneity. Figures and tables are reflowed for readability, and the bibliography restores and extends non-logistic/Bass adoption references (Gompertz, Richards, Fisher-Pry, Mansfield, Griliches, Geroski, Peres). All code and logs necessary to reproduce the synthetic analyses are embedded as LaTeX listings.",Faruk Alpay|Taylan Alpay,,https://arxiv.org/abs/2508.12896v1,https://arxiv.org/pdf/2508.12896v1,,"17 pages, 7 figures, 4 tables",,,cs.AI,cs.AI|cs.HC|stat.ME,https://arxiv.org/pdf/2508.12896v1.pdf
2508.12068v1,2025-08-16T14:48:21Z,2025-08-16 14:48:21,A Severity-Aware Reliability Index for Risk-Informed Structural Design,"Classical measures of structural reliability, such as the probability of failure and the related reliability index, are still widely applied in practice. However, these measures are frequency-based only, and they do not give information about the severity of failure once it happens. This missing aspect can cause underestimation of risks, in particular when rare events produce very undesirable consequences. In this paper, a new reliability framework is proposed to address this issue. The framework is based on a new concept, called the Expected Failure Deficit (EFD), which is defined as the average deficiency of the system response when failure occurs. From this quantity, a new reliability index is introduced, called the Severity-Aware Reliability Index, which evaluates the consequence of failure in comparison with the Gaussian benchmark. The mathematical formulation is derived and it is shown that the inverse mapping exists in a restricted domain, which can be interpreted as an indicator of excessive tail risks. A Severity Classification System with five levels is also proposed and calibrated from analytical expressions. Numerical examples, including Gaussian, mildly nonlinear, and heavy-tailed cases, demonstrate that the proposed framework agrees with classical measures in standard situations, while being able to detect hidden severity in more complex cases. The method can therefore be used not only to quantify severity of failure, but also to classify risks in support of engineering design.",Moussa Leblouba|Samer Barakat|Raghad Awad,,https://arxiv.org/abs/2508.12068v1,https://arxiv.org/pdf/2508.12068v1,,"29 pages, 8 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2508.12068v1.pdf
2508.09569v1,2025-08-13T07:39:17Z,2025-08-13 07:39:17,Optimal Designs for Gamma Degradation Tests,"This paper analytically investigates the optimal design of gamma degradation tests, including the number of test units, the number of inspections, and inspection times. We first derive optimal designs with periodic inspection times under various scenarios. Unlike previous studies that typically rely on numerical methods or fix certain design parameters, our approach provides an analytical framework to determine optimal designs. In addition, the results are directly applicable to destructive degradation tests when number of inspection is one. The investigation is then extended to designs with aperiodic inspection times, a topic that has not been thoroughly explored in the existing literature. Interestingly, we show that designs with periodic inspection times are the least efficient. We then derive the optimal aperiodic inspection times and the corresponding optimal designs under two cost constraints. Finally, two examples are presented to validate the proposed methods and demonstrate their efficiency in improving reliability estimation.",Hung-Ping Tung|Yu-Wen Chen,,https://arxiv.org/abs/2508.09569v1,https://arxiv.org/pdf/2508.09569v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2508.09569v1.pdf
2508.09054v1,2025-08-12T16:13:51Z,2025-08-12 16:13:51,CVCM Track Circuits Pre-emptive Failure Diagnostics for Predictive Maintenance Using Deep Neural Networks,"Track circuits are critical for railway operations, acting as the main signalling sub-system to locate trains. Continuous Variable Current Modulation (CVCM) is one such technology. Like any field-deployed, safety-critical asset, it can fail, triggering cascading disruptions. Many failures originate as subtle anomalies that evolve over time, often not visually apparent in monitored signals. Conventional approaches, which rely on clear signal changes, struggle to detect them early. Early identification of failure types is essential to improve maintenance planning, minimising downtime and revenue loss. Leveraging deep neural networks, we propose a predictive maintenance framework that classifies anomalies well before they escalate into failures. Validated on 10 CVCM failure cases across different installations, the method is ISO-17359 compliant and outperforms conventional techniques, achieving 99.31% overall accuracy with detection within 1% of anomaly onset. Through conformal prediction, we provide uncertainty estimates, reaching 99% confidence with consistent coverage across classes. Given CVCMs global deployment, the approach is scalable and adaptable to other track circuits and railway systems, enhancing operational reliability.",Debdeep Mukherjee|Eduardo Di Santi|Clément Lefebvre|Nenad Mijatovic|Victor Martin|Thierry Josse|Jonathan Brown|Kenza Saiah,"Innovation and Smart Mobility, Alstom|Digital and Integrated Systems, Alstom|Digital and Integrated Systems, Alstom|Digital and Integrated Systems, Alstom|Digital and Integrated Systems, Alstom|Project System Engineering, Alstom|Digital and Integrated Systems, Alstom|Digital and Integrated Systems, Alstom",https://arxiv.org/abs/2508.09054v1,https://arxiv.org/pdf/2508.09054v1,,"Peer-reviewed conference paper. Presented at ICROMA 2025 (International Conference on Railway Operations Modelling and Analysis), Dresden, Germany. https://tu-dresden.de/raildresden2025 8 pages, 6 figures, 1 table",,,cs.AI,cs.AI|cs.LG|stat.ML,https://arxiv.org/pdf/2508.09054v1.pdf
2508.07556v2,2025-08-11T02:33:53Z,2025-09-06 12:35:53,Uncertainty-Driven Reliability: Selective Prediction and Trustworthy Deployment in Modern Machine Learning,"Machine learning (ML) systems are increasingly deployed in high-stakes domains where reliability is paramount. This thesis investigates how uncertainty estimation can enhance the safety and trustworthiness of ML, focusing on selective prediction -- where models abstain when confidence is low.
  We first show that a model's training trajectory contains rich uncertainty signals that can be exploited without altering its architecture or loss. By ensembling predictions from intermediate checkpoints, we propose a lightweight, post-hoc abstention method that works across tasks, avoids the cost of deep ensembles, and achieves state-of-the-art selective prediction performance. Crucially, this approach is fully compatible with differential privacy (DP), allowing us to study how privacy noise affects uncertainty quality. We find that while many methods degrade under DP, our trajectory-based approach remains robust, and we introduce a framework for isolating the privacy-uncertainty trade-off. Next, we then develop a finite-sample decomposition of the selective classification gap -- the deviation from the oracle accuracy-coverage curve -- identifying five interpretable error sources and clarifying which interventions can close the gap. This explains why calibration alone cannot fix ranking errors, motivating methods that improve uncertainty ordering. Finally, we show that uncertainty signals can be adversarially manipulated to hide errors or deny service while maintaining high accuracy, and we design defenses combining calibration audits with verifiable inference.
  Together, these contributions advance reliable ML by improving, evaluating, and safeguarding uncertainty estimation, enabling models that not only make accurate predictions -- but also know when to say ""I do not know"".",Stephan Rabanser,,https://arxiv.org/abs/2508.07556v2,https://arxiv.org/pdf/2508.07556v2,,PhD Thesis,,,cs.LG,cs.LG|cs.AI|cs.CY|stat.ML,https://arxiv.org/pdf/2508.07556v2.pdf
2508.03896v1,2025-08-05T20:34:04Z,2025-08-05 20:34:04,Reliable Programmatic Weak Supervision with Confidence Intervals for Label Probabilities,"The accurate labeling of datasets is often both costly and time-consuming. Given an unlabeled dataset, programmatic weak supervision obtains probabilistic predictions for the labels by leveraging multiple weak labeling functions (LFs) that provide rough guesses for labels. Weak LFs commonly provide guesses with assorted types and unknown interdependences that can result in unreliable predictions. Furthermore, existing techniques for programmatic weak supervision cannot provide assessments for the reliability of the probabilistic predictions for labels. This paper presents a methodology for programmatic weak supervision that can provide confidence intervals for label probabilities and obtain more reliable predictions. In particular, the methods proposed use uncertainty sets of distributions that encapsulate the information provided by LFs with unrestricted behavior and typology. Experiments on multiple benchmark datasets show the improvement of the presented methods over the state-of-the-art and the practicality of the confidence intervals presented.",Verónica Álvarez|Santiago Mazuelas|Steven An|Sanjoy Dasgupta,,https://arxiv.org/abs/2508.03896v1,https://arxiv.org/pdf/2508.03896v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2508.03896v1.pdf
2508.00210v1,2025-07-31T23:22:46Z,2025-07-31 23:22:46,Efficient rare event estimation for multimodal and high-dimensional system reliability via subset adaptive importance sampling,"Estimating rare events in complex systems is a key challenge in reliability analysis. The challenge grows in multimodal problems, where traditional methods often rely on a small set of design points and risk overlooking critical failure modes. Further, higher dimensions make the probability mass harder to capture and demand substantially larger sample sizes to estimate failures. In this work, we propose a new sampling strategy, subset adaptive importance sampling (SAIS), that combines the strengths of subset simulation and adaptive multiple importance sampling. SAIS iteratively refines a set of proposal distributions using weighted samples from previous stages, efficiently exploring complex and high-dimensional failure regions. Leveraging recent advances in adaptive importance sampling, SAIS yields low-variance estimates using fewer samples than state-of-the-art methods and achieves pronounced improvements in both accuracy and computational cost. Through a series of benchmark problems involving high-dimensional, nonlinear performance functions, and multimodal scenarios, we demonstrate that SAIS consistently outperforms competing methods in capturing diverse failure modes and estimating failure probabilities with high precision.",Sara Helal|Victor Elvira,,https://arxiv.org/abs/2508.00210v1,https://arxiv.org/pdf/2508.00210v1,,,,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2508.00210v1.pdf
2507.23293v1,2025-07-31T07:18:28Z,2025-07-31 07:18:28,Bayesian reliability acceptance sampling plan sampling plans under adaptive accelerated type-II censored competing risk data,"In recent times, products have become increasingly complex and highly reliable, so failures typically occur after long periods of operation under normal conditions and may arise from multiple causes. This paper employs simple step-stress partial accelerated life testing (SSSPALT) within the competing risks framework to determine the Bayesian reliability acceptance sampling plan (BRASP) under type-II censoring. Elevating the stress during the life test incurs an additional cost that increases the cost of the life test. In this context, an adaptive scenario is also considered in that sampling plan. The adaptive scenario is as follows: the stress is increased after a certain time if the number of failures up to that point is less than a pre-specified number of failures. The Bayes decision function and Bayes risk are derived for the general loss function. An optimal BRASP under that adaptive SSSPALT is obtained for the quadratic loss function by minimizing Bayes risk. An algorithm is provided to determine the optimal proposed BRASP. Further, comparative studies are conducted between the proposed BRASP, the conventional non-accelerated BRASP, and the conventional accelerated BRASP under type-II censoring to evaluate the effectiveness of the proposed approach. Finally, the methodology is illustrated using real data.",Rathin Das|Soumya Roy|Biswabrata Pradhan,,https://arxiv.org/abs/2507.23293v1,https://arxiv.org/pdf/2507.23293v1,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2507.23293v1.pdf
2507.20268v2,2025-07-27T13:31:02Z,2025-10-20 07:55:53,Reliable Wireless Indoor Localization via Cross-Validated Prediction-Powered Calibration,"Wireless indoor localization using predictive models with received signal strength information (RSSI) requires proper calibration for reliable position estimates. One remedy is to employ synthetic labels produced by a (generally different) predictive model. But fine-tuning an additional predictor, as well as estimating residual bias of the synthetic labels, demands additional data, aggravating calibration data scarcity in wireless environments. This letter proposes an approach that efficiently uses limited calibration data to simultaneously fine-tune a predictor and estimate the bias of synthetic labels, yielding prediction sets with rigorous coverage guarantees. Experiments on a fingerprinting dataset validate the effectiveness of the proposed method.",Seonghoon Yoo|Houssem Sifaou|Sangwoo Park|Joonhyuk Kang|Osvaldo Simeone,,https://arxiv.org/abs/2507.20268v2,https://arxiv.org/pdf/2507.20268v2,,,,,cs.LG,cs.LG|eess.SP|stat.ML,https://arxiv.org/pdf/2507.20268v2.pdf
2507.19663v1,2025-07-25T20:34:03Z,2025-07-25 20:34:03,Adaptive Bayesian Data-Driven Design of Reliable Solder Joints for Micro-electronic Devices,"Solder joint reliability related to failures due to thermomechanical loading is a critically important yet physically complex engineering problem. As a result, simulated behavior is oftentimes computationally expensive. In an increasingly data-driven world, the usage of efficient data-driven design schemes is a popular choice. Among them, Bayesian optimization (BO) with Gaussian process regression is one of the most important representatives. The authors argue that computational savings can be obtained from exploiting thorough surrogate modeling and selecting a design candidate based on multiple acquisition functions. This is feasible due to the relatively low computational cost, compared to the expensive simulation objective. This paper addresses the shortcomings in the adjacent literature by providing and implementing a novel heuristic framework to perform BO with adaptive hyperparameters across the various optimization iterations. Adaptive BO is subsequently compared to regular BO when faced with synthetic objective minimization problems. The results show the efficiency of adaptive BO when compared any worst-performing regular Bayesian schemes. As an engineering use case, the solder joint reliability problem is tackled by minimizing the accumulated non-linear creep strain under a cyclic thermal load. Results show that adaptive BO outperforms regular BO by 3% on average at any given computational budget threshold, critically saving half of the computational expense budget. This practical result underlines the methodological potential of the adaptive Bayesian data-driven methodology to achieve better results and cut optimization-related expenses. Lastly, in order to promote the reproducibility of the results, the data-driven implementations are made available on an open-source basis.",Leo Guo|Adwait Inamdar|Willem D. van Driel|GuoQi Zhang,,https://arxiv.org/abs/2507.19663v1,https://arxiv.org/pdf/2507.19663v1,,"data-driven design, adaptive hyperparameters, Bayesian optimization, solder joint reliability, micro-electronics",,,stat.ML,stat.ML|cs.LG|physics.comp-ph,https://arxiv.org/pdf/2507.19663v1.pdf
2507.14666v3,2025-07-19T15:34:58Z,2026-02-05 20:29:54,What Quality Engineers Need to Know about Degradation Models,"Degradation models play a critical role in quality engineering by enabling the assessment and prediction of system reliability based on data. The objective of this paper is to provide an accessible introduction to degradation models. We explore commonly used degradation data types, including repeated measures degradation data and accelerated destructive degradation test data, and review modeling approaches such as general path models and stochastic process models. Key inference problems, including reliability estimation and prediction, are addressed. Applications across diverse fields, including material science, renewable energy, civil engineering, aerospace, and pharmaceuticals, illustrate the broad impact of degradation models in industry. We also discuss best practices for quality engineers, software implementations, and challenges in applying these models. This paper aims to provide quality engineers with a foundational understanding of degradation models, equipping them with the knowledge necessary to apply these techniques effectively in real-world scenarios.",Jared M. Clark|Jie Min|Mingyang Li|Richard L. Warr|Stephanie P. DeHart|Caleb B. King|Lu Lu|Yili Hong,,https://arxiv.org/abs/2507.14666v3,https://arxiv.org/pdf/2507.14666v3,,"38 pages, 16 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/2507.14666v3.pdf
2507.09178v1,2025-07-12T07:53:49Z,2025-07-12 07:53:49,The BdryMatérn GP: Reliable incorporation of boundary information on irregular domains for Gaussian process modeling,"Gaussian processes (GPs) are broadly used as surrogate models for expensive computer simulators of complex phenomena. However, a key bottleneck is that its training data are generated from this expensive simulator and thus can be highly limited. A promising solution is to supplement the learning model with boundary information from scientific knowledge. However, despite recent work on boundary-integrated GPs, such models largely cannot accommodate boundary information on irregular (i.e., non-hypercube) domains, and do not provide sample path smoothness control or approximation error analysis, both of which are important for reliable surrogate modeling. We thus propose a novel BdryMatérn GP modeling framework, which can reliably integrate Dirichlet, Neumann and Robin boundaries on an irregular connected domain with a boundary set that is twice-differentiable almost everywhere. Our model leverages a new BdryMatérn covariance kernel derived in path integral form via a stochastic partial differential equation formulation. Similar to the GP with Matérn kernel, we prove that sample paths from the BdryMatérn GP satisfy the desired boundaries with smoothness control on its derivatives. We further present an efficient approximation procedure for the BdryMatérn kernel using finite element modeling with rigorous error analysis. Finally, we demonstrate the effectiveness of the BdryMatérn GP in a suite of numerical experiments on incorporating broad boundaries on irregular domains.",Liang Ding|Simon Mak|C. F. Jeff Wu,,https://arxiv.org/abs/2507.09178v1,https://arxiv.org/pdf/2507.09178v1,,,,,stat.ME,stat.ME|math.ST|stat.ML,https://arxiv.org/pdf/2507.09178v1.pdf
2507.04553v1,2025-07-06T22:07:57Z,2025-07-06 22:07:57,AL-SPCE -- Reliability analysis for nondeterministic models using stochastic polynomial chaos expansions and active learning,"Reliability analysis typically relies on deterministic simulators, which yield repeatable outputs for identical inputs. However, many real-world systems display intrinsic randomness, requiring stochastic simulators whose outputs are random variables. This inherent variability must be accounted for in reliability analysis. While Monte Carlo methods can handle this, their high computational cost is often prohibitive. To address this, stochastic emulators have emerged as efficient surrogate models capable of capturing the random response of simulators at reduced cost. Although promising, current methods still require large training sets to produce accurate reliability estimates, which limits their practicality for expensive simulations. This work introduces an active learning framework to further reduce the computational burden of reliability analysis using stochastic emulators. We focus on stochastic polynomial chaos expansions (SPCE) and propose a novel learning function that targets regions of high predictive uncertainty relevant to failure probability estimation. To quantify this uncertainty, we exploit the asymptotic normality of the maximum likelihood estimator. The resulting method, named active learning stochastic polynomial chaos expansions (AL-SPCE), is applied to three test cases. Results demonstrate that AL-SPCE maintains high accuracy in reliability estimates while significantly improving efficiency compared to conventional surrogate-based methods and direct Monte Carlo simulation. This confirms the potential of active learning in enhancing the practicality of stochastic reliability analysis for complex, computationally expensive models.",A. Pires|M. Moustapha|S. Marelli|B. Sudret,,https://arxiv.org/abs/2507.04553v1,https://arxiv.org/pdf/2507.04553v1,,,,,stat.ME,stat.ME|stat.CO|stat.ML,https://arxiv.org/pdf/2507.04553v1.pdf
2506.19536v1,2025-06-24T11:45:33Z,2025-06-24 11:45:33,Programming Geotechnical Reliability Algorithms using Generative AI,"Programming reliability algorithms is crucial for risk assessment in geotechnical engineering. This study explores the possibility of automating and accelerating this task using Generative AI based on Large Language Models (LLMs). Specifically, the most popular LLM, i.e., ChatGPT, is used to test the ability to generate MATLAB codes for four classical reliability algorithms. The four specific examples considered in this study are: (1) First Order Reliability Method (FORM); (2) Subset simulation; (3) Random field simulation; and (4) Bayesian update using Gibbs sampling. The results obtained using the generated codes are compared with benchmark methods. It is found that the use of LLMs can be promising for generating reliability codes. Failure, limitations, and challenges of adopting LLMs are also discussed. Overall, this study demonstrates that existing LLMs can be leveraged powerfully and can contribute toward accelerating the adoption of reliability techniques in routine geotechnical engineering.",Atma Sharma|Jie Zhang|Meng Lu|Shuangyi Wu|Baoxiang Li,,https://arxiv.org/abs/2506.19536v1,https://arxiv.org/pdf/2506.19536v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2506.19536v1.pdf
2506.18663v1,2025-06-23T14:05:04Z,2025-06-23 14:05:04,A Structural Causal Model for Electronic Device Reliability: From Effects to Counterfactuals,"Electronic devices exhibit changes in electrical resistance over time at varying rates, depending on the configuration of certain components. Since measuring overall electrical resistance requires partial disassembly, only a limited number of measurements are performed over thousands of operating hours. This leads to censored failure times, whether under natural stress or under accelerated stress conditions. To address these challenges, including device-specific failure thresholds, a parametric structural causal model is developed to extract information from both observational and experimental data, with the aim of estimating causal effects and counterfactuals, regardless of the applied stress regime. Synthetic data are used to illustrate the main findings.",Federico Mattia Stefanini|Nedka Dechkova Nikiforova|Rossella Berni,"Department of Environmental Science and Policy, University of Milan, Via Celoria 2, 20133 Milan, Italy|Department of Statistics Computer Science Applications 'G. Parenti', University of Florence, viale Morgagni 59, Florence, 50134 Florence, Italy|Department of Statistics Computer Science Applications 'G. Parenti', University of Florence, viale Morgagni 59, Florence, 50134 Florence, Italy",https://arxiv.org/abs/2506.18663v1,https://arxiv.org/pdf/2506.18663v1,,"23 pages, 7 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/2506.18663v1.pdf
2506.18482v3,2025-06-23T10:35:36Z,2025-12-15 10:13:42,Reliability-Adjusted Prioritized Experience Replay,"Experience replay enables data-efficient learning from past experiences in online reinforcement learning agents. Traditionally, experiences were sampled uniformly from a replay buffer, regardless of differences in experience-specific learning potential. In an effort to sample more efficiently, researchers introduced Prioritized Experience Replay (PER). In this paper, we propose an extension to PER by introducing a novel measure of temporal difference error reliability. We theoretically show that the resulting transition selection algorithm, Reliability-adjusted Prioritized Experience Replay (ReaPER), enables more efficient learning than PER. We further present empirical results showing that ReaPER outperforms PER across various environment types, including the Atari-10 benchmark.",Leonard S. Pleiss|Tobias Sutter|Maximilian Schiffer,,https://arxiv.org/abs/2506.18482v3,https://arxiv.org/pdf/2506.18482v3,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2506.18482v3.pdf
2506.16095v1,2025-06-19T07:31:03Z,2025-06-19 07:31:03,Intelligent Operation and Maintenance and Prediction Model Optimization for Improving Wind Power Generation Efficiency,"This study explores the effectiveness of predictive maintenance models and the optimization of intelligent Operation and Maintenance (O&M) systems in improving wind power generation efficiency. Through qualitative research, structured interviews were conducted with five wind farm engineers and maintenance managers, each with extensive experience in turbine operations. Using thematic analysis, the study revealed that while predictive maintenance models effectively reduce downtime by identifying major faults, they often struggle with detecting smaller, gradual failures. Key challenges identified include false positives, sensor malfunctions, and difficulties in integrating new models with older turbine systems. Advanced technologies such as digital twins, SCADA systems, and condition monitoring have significantly enhanced turbine maintenance practices. However, these technologies still require improvements, particularly in AI refinement and real-time data integration. The findings emphasize the need for continuous development to fully optimize wind turbine performance and support the broader adoption of renewable energy.",Xun Liu|Xiaobin Wu|Jiaqi He|Rajan Das Gupta,,https://arxiv.org/abs/2506.16095v1,https://arxiv.org/pdf/2506.16095v1,https://doi.org/10.1109/ICHORA65333.2025.11017307,"7 pages, 3 figures","Proc. 7th Int. Congr. on Human-Computer Interaction, Optimization and Robotic Applications (ICHORA), IEEE, pp. 1-7, 2025",10.1109/ICHORA65333.2025.11017307,eess.SY,eess.SY|stat.AP,https://arxiv.org/pdf/2506.16095v1.pdf
2506.14843v1,2025-06-16T12:41:13Z,2025-06-16 12:41:13,CACTUS as a Reliable Tool for Early Classification of Age-related Macular Degeneration,"Machine Learning (ML) is used to tackle various tasks, such as disease classification and prediction. The effectiveness of ML models relies heavily on having large amounts of complete data. However, healthcare data is often limited or incomplete, which can hinder model performance. Additionally, issues like the trustworthiness of solutions vary with the datasets used. The lack of transparency in some ML models further complicates their understanding and use. In healthcare, particularly in the case of Age-related Macular Degeneration (AMD), which affects millions of older adults, early diagnosis is crucial due to the absence of effective treatments for reversing progression. Diagnosing AMD involves assessing retinal images along with patients' symptom reports. There is a need for classification approaches that consider genetic, dietary, clinical, and demographic factors. Recently, we introduced the -Comprehensive Abstraction and Classification Tool for Uncovering Structures-(CACTUS), aimed at improving AMD stage classification. CACTUS offers explainability and flexibility, outperforming standard ML models. It enhances decision-making by identifying key factors and providing confidence in its results. The important features identified by CACTUS allow us to compare with existing medical knowledge. By eliminating less relevant or biased data, we created a clinical scenario for clinicians to offer feedback and address biases.",Luca Gherardini|Imre Lengyel|Tunde Peto|Caroline C. W. Klaverd|Magda A. Meester-Smoord|Johanna Maria Colijnd|EYE-RISK Consortium|E3 Consortium|Jose Sousa,,https://arxiv.org/abs/2506.14843v1,https://arxiv.org/pdf/2506.14843v1,,,,,cs.LG,cs.LG|cs.CV|stat.AP,https://arxiv.org/pdf/2506.14843v1.pdf
2506.07804v1,2025-06-09T14:33:28Z,2025-06-09 14:33:28,Enhancing Adversarial Robustness with Conformal Prediction: A Framework for Guaranteed Model Reliability,"As deep learning models are increasingly deployed in high-risk applications, robust defenses against adversarial attacks and reliable performance guarantees become paramount. Moreover, accuracy alone does not provide sufficient assurance or reliable uncertainty estimates for these models. This study advances adversarial training by leveraging principles from Conformal Prediction. Specifically, we develop an adversarial attack method, termed OPSA (OPtimal Size Attack), designed to reduce the efficiency of conformal prediction at any significance level by maximizing model uncertainty without requiring coverage guarantees. Correspondingly, we introduce OPSA-AT (Adversarial Training), a defense strategy that integrates OPSA within a novel conformal training paradigm. Experimental evaluations demonstrate that our OPSA attack method induces greater uncertainty compared to baseline approaches for various defenses. Conversely, our OPSA-AT defensive model significantly enhances robustness not only against OPSA but also other adversarial attacks, and maintains reliable prediction. Our findings highlight the effectiveness of this integrated approach for developing trustworthy and resilient deep learning models for safety-critical domains. Our code is available at https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction.",Jie Bao|Chuangyin Dang|Rui Luo|Hanwei Zhang|Zhixin Zhou,,https://arxiv.org/abs/2506.07804v1,https://arxiv.org/pdf/2506.07804v1,,,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2506.07804v1.pdf
2506.05882v2,2025-06-06T08:49:55Z,2025-10-03 15:35:22,Fusion of heterogeneous data for robust degradation prognostics,"Assessing the degradation state of an industrial asset first requires evaluating its current condition and then to project the forecast model trajectory to a predefined prognostic threshold, thereby estimating its remaining useful life (RUL). Depending on the available information, two primary categories of forecasting models may be used: physics-based simulation codes and datadriven (machine learning) approaches. Combining both modelling approaches may enhance prediction robustness, especially with respect to their individual uncertainties. This paper introduces a methodology for fusion of heterogeneous data in degradation prognostics. The proposed approach acts iteratively on a computer model's uncertain input variables by combining kernel-based sensitivity analysis for variable ranking with a Bayesian framework to inform the priors with the heterogeneous data. Additionally, we propose an integration of an aggregate surrogate modeling strategy for computationally expensive degradation simulation codes. The methodology updates the knowledge of the computer code input probabilistic model and reduces the output uncertainty. As an application, we illustrate this methodology on a toy model from crack propagation based on Paris law as well as a complex industrial clogging simulation model for nuclear power plant steam generators, where data is intermittently available over time.",Edgar Jaber|Emmanuel Remy|Vincent Chabridon|Mathilde Mougeot|Didier Lucor,"EDF R\&D PRISME, CB, LISN|EDF R\&D PRISME|EDF R\&D PRISME|ENSIIE, CB|LISN",https://arxiv.org/abs/2506.05882v2,https://arxiv.org/pdf/2506.05882v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2506.05882v2.pdf
2506.04573v1,2025-06-05T02:45:04Z,2025-06-05 02:45:04,Transform-Resampled Double Bootstrap Percentile with Applications in System Reliability Assessment,"System reliability assessment(SRA) is a challenging task due to the limited experimental data and the complex nature of the system structures. Despite a long history dating back to \cite{buehler1957confidence}, exact methods have only been applied to SRA for simple systems. High-order asymptotic methods, such as the Cornish-Fisher expansion, have become popular for balancing computational efficiency with improved accuracy when data are limited, but frequently encounter the ""bend-back"" problem in high-reliability scenarios and require complex analytical computations. To overcome these limitations, we propose a novel method for SRA by modifying the double bootstrap framework, termed the double bootstrap percentile with transformed resamples. In particular, we design a nested resampling process for log-location-scale lifetime models, eliminating the computational burden caused by the iterative resampling process involved in the conventional double bootstrap. We prove that the proposed method maintains the high-order convergence property, thus providing a highly accurate yet computationally efficient confidence limit for system reliability. Moreover, the proposed procedure is straightforward to implement, involving only a simple resampling operation and efficient moment estimation steps. Numerical studies further demonstrate that our approach outperforms the state-of-the-art SRA methods and, at the same time, is much less susceptible to the bend-back issue.",Junpeng Gong|Xu He|Zhaohui Li,,https://arxiv.org/abs/2506.04573v1,https://arxiv.org/pdf/2506.04573v1,,"36 pages, 2 algorithms",,,stat.ME,stat.ME,https://arxiv.org/pdf/2506.04573v1.pdf
2506.11072v1,2025-06-02T18:55:53Z,2025-06-02 18:55:53,Can We Trust Machine Learning? The Reliability of Features from Open-Source Speech Analysis Tools for Speech Modeling,"Machine learning-based behavioral models rely on features extracted from audio-visual recordings. The recordings are processed using open-source tools to extract speech features for classification models. These tools often lack validation to ensure reliability in capturing behaviorally relevant information. This gap raises concerns about reproducibility and fairness across diverse populations and contexts. Speech processing tools, when used outside of their design context, can fail to capture behavioral variations equitably and can then contribute to bias. We evaluate speech features extracted from two widely used speech analysis tools, OpenSMILE and Praat, to assess their reliability when considering adolescents with autism. We observed considerable variation in features across tools, which influenced model performance across context and demographic groups. We encourage domain-relevant verification to enhance the reliability of machine learning models in clinical applications.",Tahiya Chowdhury|Veronica Romero,,https://arxiv.org/abs/2506.11072v1,https://arxiv.org/pdf/2506.11072v1,,"5 pages, 1 figure, 3 tables",,,eess.AS,eess.AS|cs.CL|cs.CY|cs.SD|stat.AP,https://arxiv.org/pdf/2506.11072v1.pdf
2506.08028v1,2025-06-02T00:31:53Z,2025-06-02 00:31:53,Sensor Fusion for Track Geometry Monitoring: Integrating On-Board Data and Degradation Models via Kalman Filtering,"Track geometry monitoring is essential for maintaining the safety and efficiency of railway operations. While Track Recording Cars (TRCs) provide accurate measurements of track geometry indicators, their limited availability and high operational costs restrict frequent monitoring across large rail networks. Recent advancements in on-board sensor systems installed on in-service trains offer a cost-effective alternative by enabling high-frequency, albeit less accurate, data collection. This study proposes a method to enhance the reliability of track geometry predictions by integrating low-accuracy sensor signals with degradation models through a Kalman filter framework. An experimental campaign using a low-cost sensor system mounted on a TRC evaluates the proposed approach. The results demonstrate that incorporating frequent sensor data significantly reduces prediction uncertainty, even when the data is noisy. The study also investigates how the frequency of data recording influences the size of the credible prediction interval, providing guidance on the optimal deployment of on-board sensors for effective track monitoring and maintenance planning.",Huy Truong-Ba|Jacky Chin|Michael E. Cholette|Pietro Borghesani,,https://arxiv.org/abs/2506.08028v1,https://arxiv.org/pdf/2506.08028v1,,,,,eess.SY,eess.SY|stat.AP,https://arxiv.org/pdf/2506.08028v1.pdf
2506.01044v1,2025-06-01T15:08:46Z,2025-06-01 15:08:46,A novel stratified sampler with unbalanced refinement for network reliability assessment,"We investigate stratified sampling in the context of network reliability assessment. We propose an unbalanced stratum refinement procedure, which operates on a partition of network components into clusters and the number of failed components within each cluster. The size of each refined stratum and the associated conditional failure probability, collectively termed failure signatures, can be calculated and estimated using the conditional Bernoulli model. The estimator is further improved by determining the minimum number of component failure $i^*$ to reach system failure and then by considering only strata with at least $i^*$ failed components. We propose a heuristic but practicable approximation of the optimal sample size for all strata, assuming a coherent network performance function. The efficiency of the proposed stratified sampler with unbalanced refinement (SSuR) is demonstrated through two network reliability problems.",Jianpeng Chan|Iason Papaioannou|Daniel Straub,,https://arxiv.org/abs/2506.01044v1,https://arxiv.org/pdf/2506.01044v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2506.01044v1.pdf
2506.00499v1,2025-05-31T10:32:51Z,2025-05-31 10:32:51,Federated learning framework for collaborative remaining useful life prognostics: an aircraft engine case study,"Complex systems such as aircraft engines are continuously monitored by sensors. In predictive aircraft maintenance, the collected sensor measurements are used to estimate the health condition and the Remaining Useful Life (RUL) of such systems. However, a major challenge when developing prognostics is the limited number of run-to-failure data samples. This challenge could be overcome if multiple airlines would share their run-to-failure data samples such that sufficient learning can be achieved. Due to privacy concerns, however, airlines are reluctant to share their data in a centralized setting. In this paper, a collaborative federated learning framework is therefore developed instead. Here, several airlines cooperate to train a collective RUL prognostic machine learning model, without the need to centrally share their data. For this, a decentralized validation procedure is proposed to validate the prognostics model without sharing any data. Moreover, sensor data is often noisy and of low quality. This paper therefore proposes four novel methods to aggregate the parameters of the global prognostic model. These methods enhance the robustness of the FL framework against noisy data. The proposed framework is illustrated for training a collaborative RUL prognostic model for aircraft engines, using the N-CMAPSS dataset. Here, six airlines are considered, that collaborate in the FL framework to train a collective RUL prognostic model for their aircraft's engines. When comparing the proposed FL framework with the case where each airline independently develops their own prognostic model, the results show that FL leads to more accurate RUL prognostics for five out of the six airlines. Moreover, the novel robust aggregation methods render the FL framework robust to noisy data samples.",Diogo Landau|Ingeborg de Pater|Mihaela Mitici|Nishant Saurabh,,https://arxiv.org/abs/2506.00499v1,https://arxiv.org/pdf/2506.00499v1,,,,,cs.LG,cs.LG|cs.DC|cs.ET|eess.SY|stat.ML,https://arxiv.org/pdf/2506.00499v1.pdf
2505.24097v1,2025-05-30T00:59:25Z,2025-05-30 00:59:25,Performative Risk Control: Calibrating Models for Reliable Deployment under Performativity,"Calibrating blackbox machine learning models to achieve risk control is crucial to ensure reliable decision-making. A rich line of literature has been studying how to calibrate a model so that its predictions satisfy explicit finite-sample statistical guarantees under a fixed, static, and unknown data-generating distribution. However, prediction-supported decisions may influence the outcome they aim to predict, a phenomenon named performativity of predictions, which is commonly seen in social science and economics. In this paper, we introduce Performative Risk Control, a framework to calibrate models to achieve risk control under performativity with provable theoretical guarantees. Specifically, we provide an iteratively refined calibration process, where we ensure the predictions are improved and risk-controlled throughout the process. We also study different types of risk measures and choices of tail bounds. Lastly, we demonstrate the effectiveness of our framework by numerical experiments on the task of predicting credit default risk. To the best of our knowledge, this work is the first one to study statistically rigorous risk control under performativity, which will serve as an important safeguard against a wide range of strategic manipulation in decision-making processes.",Victor Li|Baiting Chen|Yuzhen Mao|Qi Lei|Zhun Deng,,https://arxiv.org/abs/2505.24097v1,https://arxiv.org/pdf/2505.24097v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2505.24097v1.pdf
2505.18891v1,2025-05-24T22:32:22Z,2025-05-24 22:32:22,Degradation-Aware and Machine Learning-Driven Uncertainty Quantification in Crystal Plasticity Finite Element: Texture-Driven Plasticity in 316L Stainless Steel,"The mechanical properties and long-term structural reliability of crystalline materials are strongly influenced by microstructural features such as grain size, morphology, and crystallographic texture. These characteristics not only determine the initial mechanical behavior but also govern the progression of degradation mechanisms, such as strain localization, fatigue damage, and microcrack initiation under service conditions. Variability in these microstructural attributes, introduced during manufacturing or evolving through in-service degradation, leads to uncertainty in material performance. Therefore, understanding and quantifying microstructure-sensitive plastic deformation is critical for assessing degradation risk in high-value mechanical systems. This study presents a first-of-its-kind machine learning-driven framework that couples high-fidelity crystal plasticity finite element (CPFE) simulations with data-driven surrogate modeling to accelerate degradation-aware uncertainty quantification in welded structural alloys. Specifically, the impact of crystallographic texture variability in 316L stainless steel weldments, characterized via high-throughput electron backscatter diffraction (EBSD), is examined through CPFE simulations on calibrated representative volume elements (RVEs). A polynomial chaos expansion-based surrogate model is then trained to efficiently emulate the CPFE response using only 200 simulations, reducing computational cost by several orders of magnitude compared to conventional Monte Carlo analysis. The surrogate enables rapid quantification of uncertainty in stress-strain behavior and identifies texture components such as Cube and Goss as key drivers of degradation-relevant plastic response.",Dinesh Kumar|Eralp Demir|Julio Spadotto|Kazuma Kobayashi|Syed Bahauddin Alam|Brian Connolly|Ed Pickering|Paul Wilcox|David Knowles|Mahmoud Mostafavi,,https://arxiv.org/abs/2505.18891v1,https://arxiv.org/pdf/2505.18891v1,,,,,stat.AP,stat.AP|cond-mat.mtrl-sci,https://arxiv.org/pdf/2505.18891v1.pdf
2505.18693v1,2025-05-24T13:37:48Z,2025-05-24 13:37:48,Simultaneous Optimization of Efficiency and Degradation in Tunable HTL-Free Perovskite Solar Cells with MWCNT-Integrated Back Contact Using a Machine Learning-Derived Polynomial Regressor,"Perovskite solar cells (PSCs) without a hole transport layer (HTL) offer a cost-effective and stable alternative to conventional architectures, utilizing only an absorber layer and an electron transport layer (ETL). This study presents a machine learning (ML)-driven framework to optimize the efficiency and stability of HTL-free PSCs by integrating experimental validation with numerical simulations. Excellent agreement is achieved between a fabricated device and its simulated counterpart at a molar fraction \( x = 68.7\% \) in \(\mathrm{MAPb}_{1-x}\mathrm{Sb}_{2x/3}\mathrm{I}_3\), where MA is methylammonium. A dataset of 1650 samples is generated by varying molar fraction, absorber defect density, thickness, and ETL doping, with corresponding efficiency and 50-hour degradation as targets. A fourth-degree polynomial regressor (PR-4) shows the best performance, achieving RMSEs of 0.0179 and 0.0117, and \( R^2 \) scores of 1 and 0.999 for efficiency and degradation, respectively. The derived model generalizes beyond the training range and is used in an L-BFGS-B optimization algorithm with a weighted objective function to maximize efficiency and minimize degradation. This improves device efficiency from 13.7\% to 16.84\% and reduces degradation from 6.61\% to 2.39\% over 1000 hours. Finally, the dataset is labeled into superior and inferior classes, and a multilayer perceptron (MLP) classifier achieves 100\% accuracy, successfully identifying optimal configurations.",Ihtesham Ibn Malek|Hafiz Imtiaz|Samia Subrina,,https://arxiv.org/abs/2505.18693v1,https://arxiv.org/pdf/2505.18693v1,,Under review in Elsevier Renewable Energy,,,cs.LG,cs.LG|eess.SP|stat.ML,https://arxiv.org/pdf/2505.18693v1.pdf
2505.18659v2,2025-05-24T11:53:29Z,2025-12-02 13:47:23,Adaptive Prediction-Powered AutoEval with Reliability and Efficiency Guarantees,"Selecting artificial intelligence (AI) models, such as large language models (LLMs), from multiple candidates requires accurate performance estimation. This is ideally achieved through empirical evaluations involving abundant real-world data. However, such evaluations are costly and impractical at scale. To address this challenge, autoevaluation methods leverage synthetic data produced by automated evaluators, such as LLMs-as-judges, reducing variance but potentially introducing bias. Recent approaches have employed semi-supervised prediction-powered inference (PPI) to correct for the bias of autoevaluators. However, the use of autoevaluators may lead in practice to a degradation in sample efficiency compared to conventional methods using only real-world data. In this paper, we propose R-AutoEval+, a novel framework that provides finite-sample reliability guarantees on the model evaluation, while also ensuring an enhanced (or at least no worse) sample efficiency compared to conventional methods. The key innovation of R-AutoEval+ is an adaptive construction of the model evaluation variable, which dynamically tunes its reliance on synthetic data, reverting to conventional methods when the autoevaluator is insufficiently accurate. Experiments on the use of LLMs-as-judges for the optimization of quantization settings for the weights of an LLM, for prompt design in LLMs, and for test-time reasoning budget allocation in LLMs confirm the reliability and efficiency of R-AutoEval+.",Sangwoo Park|Matteo Zecchin|Osvaldo Simeone,,https://arxiv.org/abs/2505.18659v2,https://arxiv.org/pdf/2505.18659v2,,NeurIPS 2025 (spotlight),,,stat.ML,stat.ML|cs.AI|cs.LG|stat.ME,https://arxiv.org/pdf/2505.18659v2.pdf
2505.18510v1,2025-05-24T05:14:49Z,2025-05-24 05:14:49,The tail wags the distribution: Only sample the tails for efficient reliability analysis,"To ensure that real-world infrastructure is safe and durable, systems are designed to not fail for any but the most rarely occurring parameter values. By only happening deep in the tails of the parameter distribution, failure probabilities are kept small. At the same time, it is essential to understand the risk associated with the failure of a system, no matter how unlikely. However, estimating such small failure probabilities is challenging; numerous system performance evaluations are necessary to produce even a single system state corresponding to failure, and each such evaluation is usually significantly computationally expensive. To alleviate this difficulty, we propose the Tail Stratified Sampling (TSS) estimator - an intuitive stratified sampling estimator for the failure probability that successively refines the tails of the system parameter distribution, enabling direct sampling of the tails, where failure is expected to occur. The most general construction of TSS is presented, highlighting its versatility and robustness for a variety of applications. The intuitions behind the formulation are explained, followed by a discussion of the theoretical and practical benefits of the method. Various details of the implementation are presented. The performance of the algorithm is then showcased through a host of analytical examples with varying failure domain geometries and failure probabilities as well as multiple numerical case studies of moderate and high dimensionality. To conclude, a qualitative comparison of TSS against the existing foundational variance-reduction methods for reliability analysis is presented, along with suggestions for future developments.",Promit Chakroborty|Michael D. Shields,,https://arxiv.org/abs/2505.18510v1,https://arxiv.org/pdf/2505.18510v1,,"33 pages, 12 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2505.18510v1.pdf
2505.15728v1,2025-05-21T16:34:11Z,2025-05-21 16:34:11,Are machine learning interpretations reliable? A stability study on global interpretations,"As machine learning systems are increasingly used in high-stakes domains, there is a growing emphasis placed on making them interpretable to improve trust in these systems. In response, a range of interpretable machine learning (IML) methods have been developed to generate human-understandable insights into otherwise black box models. With these methods, a fundamental question arises: Are these interpretations reliable? Unlike with prediction accuracy or other evaluation metrics for supervised models, the proximity to the true interpretation is difficult to define. Instead, we ask a closely related question that we argue is a prerequisite for reliability: Are these interpretations stable? We define stability as findings that are consistent or reliable under small random perturbations to the data or algorithms. In this study, we conduct the first systematic, large-scale empirical stability study on popular machine learning global interpretations for both supervised and unsupervised tasks on tabular data. Our findings reveal that popular interpretation methods are frequently unstable, notably less stable than the predictions themselves, and that there is no association between the accuracy of machine learning predictions and the stability of their associated interpretations. Moreover, we show that no single method consistently provides the most stable interpretations across a range of benchmark datasets. Overall, these results suggest that interpretability alone does not warrant trust, and underscores the need for rigorous evaluation of interpretation stability in future work. To support these principles, we have developed and released an open source IML dashboard and Python package to enable researchers to assess the stability and reliability of their own data-driven interpretations and discoveries.",Luqin Gan|Tarek M. Zikry|Genevera I. Allen,,https://arxiv.org/abs/2505.15728v1,https://arxiv.org/pdf/2505.15728v1,,"17 pages main text, 5 main text figures. 57 pages in total with Appendix and Bibliography",,,stat.ML,stat.ML|cs.LG|stat.AP,https://arxiv.org/pdf/2505.15728v1.pdf
2505.14918v2,2025-05-20T21:12:58Z,2025-12-19 21:50:40,Reliable Decision Support with LLMs: A Framework for Evaluating Consistency in Binary Text Classification Applications,"This study introduces a framework for evaluating consistency in large language model (LLM) binary text classification, addressing the lack of established reliability assessment methods. Adapting psychometric principles, we determine sample size requirements, develop metrics for invalid responses, and evaluate intra- and inter-rater reliability. Our case study examines financial news sentiment classification across 14 LLMs (including claude-3-7-sonnet, gpt-4o, deepseek-r1, gemma3, llama3.2, phi4, and command-r-plus), with five replicates per model on 1,350 articles. Models demonstrated high intra-rater consistency, achieving perfect agreement on 90-98% of examples, with minimal differences between expensive and economical models from the same families. When validated against StockNewsAPI labels, models achieved strong performance (accuracy 0.76-0.88), with smaller models like gemma3:1B, llama3.2:3B, and claude-3-5-haiku outperforming larger counterparts. All models performed at chance when predicting actual market movements, indicating task constraints rather than model limitations. Our framework provides systematic guidance for LLM selection, sample size planning, and reliability assessment, enabling organizations to optimize resources for classification tasks.",Fadel M. Megahed|Ying-Ju Chen|L. Allision Jones-Farmer|Younghwa Lee|Jiawei Brooke Wang|Inez M. Zwetsloot,,https://arxiv.org/abs/2505.14918v2,https://arxiv.org/pdf/2505.14918v2,,26 pages,,,cs.CL,cs.CL|cs.LG|stat.ML,https://arxiv.org/pdf/2505.14918v2.pdf
2505.12181v1,2025-05-18T00:42:21Z,2025-05-18 00:42:21,Reliable fairness auditing with semi-supervised inference,"Machine learning (ML) models often exhibit bias that can exacerbate inequities in biomedical applications. Fairness auditing, the process of evaluating a model's performance across subpopulations, is critical for identifying and mitigating these biases. However, such audits typically rely on large volumes of labeled data, which are costly and labor-intensive to obtain. To address this challenge, we introduce $\textit{Infairness}$, a unified framework for auditing a wide range of fairness criteria using semi-supervised inference. Our approach combines a small labeled dataset with a large unlabeled dataset by imputing missing outcomes via regression with carefully selected nonlinear basis functions. We show that our proposed estimator is (i) consistent regardless of whether the ML or imputation models are correctly specified and (ii) more efficient than standard supervised estimation with the labeled data when the imputation model is correctly specified. Through extensive simulations, we also demonstrate that Infairness consistently achieves higher precision than supervised estimation. In a real-world application of phenotyping depression from electronic health records data, Infairness reduces variance by up to 64% compared to supervised estimation, underscoring its value for reliable fairness auditing with limited labeled data.",Jianhui Gao|Jessica Gronsbell,,https://arxiv.org/abs/2505.12181v1,https://arxiv.org/pdf/2505.12181v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2505.12181v1.pdf
2505.08578v2,2025-05-13T13:54:36Z,2025-11-15 17:32:34,Extreme Conformal Prediction: Reliable Intervals for High-Impact Events,"Conformal prediction is a popular method to construct prediction intervals for black-box machine learning models with marginal coverage guarantees. In applications with potentially high-impact events, such as flooding or financial crises, regulators often require very high confidence for such intervals. However, if the desired level of confidence is too large relative to the amount of data used for calibration, then classical conformal methods provide infinitely wide, thus, uninformative prediction intervals. In this paper, we propose a new method to overcome this limitation. We bridge extreme value statistics and conformal prediction to provide reliable and informative prediction intervals with high-confidence coverage, which can be constructed using any black-box extreme quantile regression method. A weighted version of our approach can account for nonstationary data. The advantages of our extreme conformal prediction method are illustrated in a simulation study and in an application to flood risk forecasting.",Olivier C. Pasche|Henry Lam|Sebastian Engelke,,https://arxiv.org/abs/2505.08578v2,https://arxiv.org/pdf/2505.08578v2,,,,,stat.ME,stat.ME|stat.AP|stat.ML,https://arxiv.org/pdf/2505.08578v2.pdf
2504.15794v2,2025-04-22T11:14:56Z,2026-01-14 06:58:15,Residual lifetime prediction for heterogeneous degradation data by Bayesian semi-parametric method,"Degradation data are considered for assessing reliability in highly reliable systems. The usual assumption is that degradation units come from a homogeneous population. But in presence of high variability in the manufacturing process, this assumption is not true in general; that is different sub-populations are involved in the study. Predicting residual lifetime of a functioning unit is a major challenge in the degradation modeling especially in heterogeneous environment. To account for heterogeneous degradation data, we have proposed a Bayesian semi-parametric approach to relax the conventional modeling assumptions. We model the degradation path using Dirichlet process mixture of normal distributions. Based on the samples obtained from posterior distribution of model parameters we obtain residual lifetime distribution for individual unit. Transformation based MCMC technique is used for simulating values from the derived residual lifetime distribution for prediction of residual lifetime. A simulation study is undertaken to check performance of the proposed semi-parametric model compared with parametric model. Fatigue Crack Size data is analyzed to illustrate the proposed methodology.",Barin Karmakar|Biswabrata Pradhan,,https://arxiv.org/abs/2504.15794v2,https://arxiv.org/pdf/2504.15794v2,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2504.15794v2.pdf
2504.14744v1,2025-04-20T21:27:23Z,2025-04-20 21:27:23,On the Tunability of Random Survival Forests Model for Predictive Maintenance,"This paper investigates the tunability of the Random Survival Forest (RSF) model in predictive maintenance, where accurate time-to-failure estimation is crucial. Although RSF is widely used due to its flexibility and ability to handle censored data, its performance is sensitive to hyperparameter configurations. However, systematic evaluations of RSF tunability remain limited, especially in predictive maintenance contexts. We introduce a three-level framework to quantify tunability: (1) a model-level metric measuring overall performance gain from tuning, (2) a hyperparameter-level metric assessing individual contributions, and (3) identification of optimal tuning ranges. These metrics are evaluated across multiple datasets using survival-specific criteria: the C-index for discrimination and the Brier score for calibration. Experiments on four CMAPSS dataset subsets, simulating aircraft engine degradation, reveal that hyperparameter tuning consistently improves model performance. On average, the C-index increased by 0.0547, while the Brier score decreased by 0.0199. These gains were consistent across all subsets. Moreover, ntree and mtry showed the highest average tunability, while nodesize offered stable improvements within the range of 10 to 30. In contrast, splitrule demonstrated negative tunability on average, indicating that improper tuning may reduce model performance. Our findings emphasize the practical importance of hyperparameter tuning in survival models and provide actionable insights for optimizing RSF in real-world predictive maintenance applications.",Yigitcan Yardımcı|Mustafa Cavus,,https://arxiv.org/abs/2504.14744v1,https://arxiv.org/pdf/2504.14744v1,https://doi.org/10.1109/ASYU67174.2025.11208507,,"2025 Innovations in Intelligent Systems and Applications Conference (ASYU), Bursa, Turkiye, 2025, pp. 1-6",10.1109/ASYU67174.2025.11208507,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2504.14744v1.pdf
2504.12156v1,2025-04-16T15:04:00Z,2025-04-16 15:04:00,Predictive Multiplicity in Survival Models: A Method for Quantifying Model Uncertainty in Predictive Maintenance Applications,"In many applications, especially those involving prediction, models may yield near-optimal performance yet significantly disagree on individual-level outcomes. This phenomenon, known as predictive multiplicity, has been formally defined in binary, probabilistic, and multi-target classification, and undermines the reliability of predictive systems. However, its implications remain unexplored in the context of survival analysis, which involves estimating the time until a failure or similar event while properly handling censored data. We frame predictive multiplicity as a critical concern in survival-based models and introduce formal measures -- ambiguity, discrepancy, and obscurity -- to quantify it. This is particularly relevant for downstream tasks such as maintenance scheduling, where precise individual risk estimates are essential. Understanding and reporting predictive multiplicity helps build trust in models deployed in high-stakes environments. We apply our methodology to benchmark datasets from predictive maintenance, extending the notion of multiplicity to survival models. Our findings show that ambiguity steadily increases, reaching up to 40-45% of observations; discrepancy is lower but exhibits a similar trend; and obscurity remains mild and concentrated in a few models. These results demonstrate that multiple accurate survival models may yield conflicting estimations of failure risk and degradation progression for the same equipment. This highlights the need to explicitly measure and communicate predictive multiplicity to ensure reliable decision-making in process health management.",Mustafa Cavus,,https://arxiv.org/abs/2504.12156v1,https://arxiv.org/pdf/2504.12156v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2504.12156v1.pdf
2504.09706v1,2025-04-13T19:57:10Z,2025-04-13 19:57:10,Modeling Discrete Coating Degradation Events via Hawkes Processes,"Forecasting the degradation of coated materials has long been a topic of critical interest in engineering, as it has enormous implications for both system maintenance and sustainable material use. Material degradation is affected by many factors, including the history of corrosion and characteristics of the environment, which can be measured by high-frequency sensors. However, the high volume of data produced by such sensors can inhibit efficient modeling and prediction. To alleviate this issue, we propose novel metrics for representing material degradation, taking the form of discrete degradation events. These events maintain the statistical properties of continuous sensor readings, such as correlation with time to coating failure and coefficient of variation at failure, but are composed of orders of magnitude fewer measurements. To forecast future degradation of the coating system, a marked Hawkes process models the events. We use the forecast of degradation to predict a future time of failure, exhibiting superior performance to the approach based on direct modeling of galvanic corrosion using continuous sensor measurements. While such maintenance is typically done on a regular basis, degradation models can enable informed condition-based maintenance, reducing unnecessary excess maintenance and preventing unexpected failures.",Matthew Repasky|Henry Yuchi|Fritz Friedersdorf|Yao Xie,,https://arxiv.org/abs/2504.09706v1,https://arxiv.org/pdf/2504.09706v1,,,,,stat.AP,stat.AP|stat.ML,https://arxiv.org/pdf/2504.09706v1.pdf
2504.09508v1,2025-04-13T10:14:34Z,2025-04-13 10:14:34,Quality Control and Structural Reliability -- A Unified Framework for Integrating Conformity Assessment and Partial Safety Factors,"Ensuring structural reliability remains a core concern in civil engineering, yet the quantitative effects of quality control measures on material variability and safety margins are not fully understood, especially for materials other than reinforced concrete. This study addresses this gap by presenting a probabilistic framework that integrates Bayesian updating, acceptance sampling, and operating characteristic (OC) curves to model conformity assessment as a probabilistic filter. In doing so, it refines prior distributions of key material and execution parameters based on quality control outcomes, linking reductions in the coefficient of variation directly to adjustments in partial safety factors. Applying the framework to a masonry wall example demonstrates how systematic quality control efforts, particularly those targeting parameters with higher importance such as masonry unit strength and execution quality-produce substantial gains in structural reliability. The analysis shows that combined quality control measures can lower the partial safety factor from a baseline of 1.5 to about 1.38, corresponding to an improvement factor of roughly 1.09 and material savings of approximately 8%. Conversely, controlling parameters with negligible influence, such as mortar properties, provides limited benefit. These findings encourage focusing quality control resources on the most influential parameters and integrating results into semi-probabilistic design methods. By offering a transparent, standards-compatible approach, the framework supports the refinement of design guidelines, promotes more efficient resource allocation, and enhances overall structural safety in the built environment.",Tammam Bakeer|Wolfram Jaeger,,https://arxiv.org/abs/2504.09508v1,https://arxiv.org/pdf/2504.09508v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2504.09508v1.pdf
2504.09310v3,2025-04-12T19:05:00Z,2025-04-27 11:36:45,Conformal Calibration: Ensuring the Reliability of Black-Box AI in Wireless Systems,"AI is poised to revolutionize telecommunication networks by boosting efficiency, automation, and decision-making. However, the black-box nature of most AI models introduces substantial risk, possibly deterring adoption by network operators. These risks are not addressed by the current prevailing deployment strategy, which typically follows a best-effort train-and-deploy paradigm. This paper reviews conformal calibration, a general framework that moves beyond the state of the art by adopting computationally lightweight, advanced statistical tools that offer formal reliability guarantees without requiring further training or fine-tuning. Conformal calibration encompasses pre-deployment calibration via uncertainty quantification or hyperparameter selection; online monitoring to detect and mitigate failures in real time; and counterfactual post-deployment performance analysis to address ""what if"" diagnostic questions after deployment. By weaving conformal calibration into the AI model lifecycle, network operators can establish confidence in black-box AI models as a dependable enabling technology for wireless systems.",Osvaldo Simeone|Sangwoo Park|Matteo Zecchin,,https://arxiv.org/abs/2504.09310v3,https://arxiv.org/pdf/2504.09310v3,,submitted for a journal publication,,,cs.IT,cs.IT|cs.LG|eess.SP|stat.AP,https://arxiv.org/pdf/2504.09310v3.pdf
2504.09206v1,2025-04-12T13:14:35Z,2025-04-12 13:14:35,Rethinking Remaining Useful Life Prediction with Scarce Time Series Data: Regression under Indirect Supervision,"Supervised time series prediction relies on directly measured target variables, but real-world use cases such as predicting remaining useful life (RUL) involve indirect supervision, where the target variable is labeled as a function of another dependent variable. Trending temporal regression techniques rely on sequential time series inputs to capture temporal patterns, requiring interpolation when dealing with sparsely and irregularly sampled covariates along the timeline. However, interpolation can introduce significant biases, particularly with highly scarce data. In this paper, we address the RUL prediction problem with data scarcity as time series regression under indirect supervision. We introduce a unified framework called parameterized static regression, which takes single data points as inputs for regression of target values, inherently handling data scarcity without requiring interpolation. The time dependency under indirect supervision is captured via a parametrical rectification (PR) process, approximating a parametric function during inference with historical posteriori estimates, following the same underlying distribution used for labeling during training. Additionally, we propose a novel batch training technique for tasks in indirect supervision to prevent overfitting and enhance efficiency. We evaluate our model on public benchmarks for RUL prediction with simulated data scarcity. Our method demonstrates competitive performance in prediction accuracy when dealing with highly scarce time series data.",Jiaxiang Cheng|Yipeng Pang|Guoqiang Hu,,https://arxiv.org/abs/2504.09206v1,https://arxiv.org/pdf/2504.09206v1,https://doi.org/10.24963/ijcai.2025/1018,,,10.24963/ijcai.2025/1018,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2504.09206v1.pdf
2504.08377v4,2025-04-11T09:26:37Z,2025-11-18 00:58:07,Proofs as Explanations: Short Certificates for Reliable Predictions,"We consider a model for explainable AI in which an explanation for a prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it exists) such that all classifiers $h' \in H$ that make at most $b$ mistakes on $S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has label $y$ under the assumption that (1) the target function $h^\star$ belongs to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example, if $b=0$ and $H$ is the family of linear classifiers in $\mathbb{R}^d$, and if $x$ lies inside the convex hull of the positive data points in $S$ (and hence every consistent linear classifier labels $x$ as positive), then Carathéodory's theorem states that $x$ lies inside the convex hull of $d+1$ of those points. So, a set $S'$ of size $d+1$ could be released as an explanation for a positive prediction, and would serve as a short proof of correctness of the prediction under the assumption of realizability.
  In this work, we consider this problem more generally, for general hypothesis classes $H$ and general values $b\geq 0$. We define the notion of the robust hollow star number of $H$ (which generalizes the standard hollow star number), and show that it precisely characterizes the worst-case size of the smallest certificate achievable, and analyze its size for natural classes. We also consider worst-case distributional bounds on certificate size, as well as distribution-dependent bounds that we show tightly control the sample size needed to get a certificate for any given test example. In particular, we define a notion of the certificate coefficient $\varepsilon_x$ of an example $x$ with respect to a data distribution $D$ and target function $h^\star$, and prove matching upper and lower bounds on sample size as a function of $\varepsilon_x$, $b$, and the VC dimension $d$ of $H$.",Avrim Blum|Steve Hanneke|Chirag Pabbaraju|Donya Saless,,https://arxiv.org/abs/2504.08377v4,https://arxiv.org/pdf/2504.08377v4,,"Fixed Crefs, added reference to open question on tolerance Carathéodory, other minor changes",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2504.08377v4.pdf
2504.05484v1,2025-04-07T20:28:49Z,2025-04-07 20:28:49,Modeling Multivariate Degradation Data with Dynamic Covariates Under a Bayesian Framework,"Degradation data are essential for determining the reliability of high-end products and systems, especially when covering multiple degradation characteristics (DCs). Modern degradation studies not only measure these characteristics but also record dynamic system usage and environmental factors, such as temperature, humidity, and ultraviolet exposures, referred to as the dynamic covariates. Most current research either focuses on a single DC with dynamic covariates or multiple DCs with fixed covariates. This paper presents a Bayesian framework to analyze data with multiple DCs, which incorporates dynamic covariates. We develop a Bayesian framework for mixed effect nonlinear general path models to describe the degradation path and use Bayesian shape-constrained P-splines to model the effects of dynamic covariates. We also detail algorithms for estimating the failure time distribution induced by our degradation model, validate the developed methods through simulation, and illustrate their use in predicting the lifespan of organic coatings in dynamic environments.",Zhengzhi Lin|Xiao Liu|Yisha Xiang|Yili Hong,,https://arxiv.org/abs/2504.05484v1,https://arxiv.org/pdf/2504.05484v1,,"46 pages, 24 figures",,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2504.05484v1.pdf
2504.07131v1,2025-04-06T04:58:45Z,2025-04-06 04:58:45,Embedding Reliability Verification Constraints into Generation Expansion Planning,"Generation planning approaches face challenges in managing the incompatible mathematical structures between stochastic production simulations for reliability assessment and optimization models for generation planning, which hinders the integration of reliability constraints. This study proposes an approach to embedding reliability verification constraints into generation expansion planning by leveraging a weighted oblique decision tree (WODT) technique. For each planning year, a generation mix dataset, labeled with reliability assessment simulations, is generated. An WODT model is trained using this dataset. Reliability-feasible regions are extracted via depth-first search technique and formulated as disjunctive constraints. These constraints are then transformed into mixed-integer linear form using a convex hull modeling technique and embedded into a unit commitment-integrated generation expansion planning model. The proposed approach is validated through a long-term generation planning case study for the Electric Reliability Council of Texas (ERCOT) region, demonstrating its effectiveness in achieving reliable and optimal planning solutions.",Peng Liu|Lian Cheng|Benjamin P. Omell|Anthony P. Burgard,,https://arxiv.org/abs/2504.07131v1,https://arxiv.org/pdf/2504.07131v1,,"5 pages,3 figures. IEEE PES general meeting 2025",,,cs.AI,cs.AI|stat.ML,https://arxiv.org/pdf/2504.07131v1.pdf
2503.24262v1,2025-03-31T16:08:11Z,2025-03-31 16:08:11,New Statistical Framework for Extreme Error Probability in High-Stakes Domains for Reliable Machine Learning,"Machine learning is vital in high-stakes domains, yet conventional validation methods rely on averaging metrics like mean squared error (MSE) or mean absolute error (MAE), which fail to quantify extreme errors. Worst-case prediction failures can have substantial consequences, but current frameworks lack statistical foundations for assessing their probability. In this work a new statistical framework, based on Extreme Value Theory (EVT), is presented that provides a rigorous approach to estimating worst-case failures. Applying EVT to synthetic and real-world datasets, this method is shown to enable robust estimation of catastrophic failure probabilities, overcoming the fundamental limitations of standard cross-validation. This work establishes EVT as a fundamental tool for assessing model reliability, ensuring safer AI deployment in new technologies where uncertainty quantification is central to decision-making or scientific analysis.",Umberto Michelucci|Francesca Venturini,,https://arxiv.org/abs/2503.24262v1,https://arxiv.org/pdf/2503.24262v1,,,,,cs.LG,cs.LG|cs.AI|stat.ME|stat.ML,https://arxiv.org/pdf/2503.24262v1.pdf
2503.24248v1,2025-03-31T15:58:50Z,2025-03-31 15:58:50,Optimizing PCA for Health and Care Research: A Reliable Approach to Component Selection,"PCA is widely used in health and care research to analyze complex HD datasets, such as patient health records, genetic data, and medical imaging. By reducing dimensionality, PCA helps identify key patterns and trends, which can aid in disease diagnosis, treatment optimization, and the discovery of new biomarkers. However, the primary goal of any dimensional reduction technique is to reduce the dimensionality in a data set while keeping the essential information and variability. There are a few ways to do this in practice, such as the Kaiser-Guttman criterion, Cattell's Scree Test, and the percent cumulative variance approach. Unfortunately, the results of these methods are entirely different. That means using inappropriate methods to find the optimal number of PCs retained in PCA may lead to misinterpreted and inaccurate results in PCA and PCA-related health and care research applications. This contradiction becomes even more pronounced in HD settings where n < p, making it even more critical to determine the best approach. Therefore, it is necessary to identify the issues of different techniques to select the optimal number of PCs retained in PCA. Kaiser-Guttman criterion retains fewer PCs, causing overdispersion, while Cattell's scree test retains more PCs, compromising reliability. The percentage of cumulative variation criterion offers greater stability, consistently selecting the optimal number of components. Therefore, the Pareto chart, which shows both the cumulative percentage and the cut-off point for retained PCs, provides the most reliable method of selecting components, ensuring stability and enhancing PCA effectiveness, particularly in health-related research applications.",Nuwan Weeraratne|Lyn Hunt|Jason Kurz,,https://arxiv.org/abs/2503.24248v1,https://arxiv.org/pdf/2503.24248v1,,Accepted as a poster presentation at NIHR Statistics Group Conference 2025,,,stat.ME,stat.ME,https://arxiv.org/pdf/2503.24248v1.pdf
2503.22924v2,2025-03-29T01:01:06Z,2025-08-13 19:27:38,Asymptotic Standard Errors for Reliability Coefficients in Item Response Theory,"Reliability is a crucial index of measurement precision and is commonly reported in substantive research using latent variable measurement models. However, reliability coefficients, often treated as fixed values, are estimated from sample data and thus inherently subject to sampling variability. There are two categories of item response theory (IRT) reliability coefficients according to the regression framework of measurement precision (Liu, Pek, & Maydeu-Olivares, 2025b): classical test theory (CTT) reliability and proportional reduction in mean squared error (PRMSE). We focus on quantifying their sampling variability in this article. Unlike existing approaches that can only handle sampling variability due to item parameter estimation, we consider a scenario in which an additional source of variability arises from substituting population moments with sample moments. We propose a general strategy for computing SEs that account for both sources of sampling variability, enabling the estimation of model-based reliability coefficients and their SEs in long tests. We apply the proposed framework to two specific reliability coefficients: the PRMSE for the latent variable and the CTT reliability for the expected a posteriori score of the latent variable. Simulation results confirm that the derived SEs accurately capture the sampling variability across various test lengths in moderate to large samples.",Youjin Sung|Yang Liu,,https://arxiv.org/abs/2503.22924v2,https://arxiv.org/pdf/2503.22924v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2503.22924v2.pdf
2503.22401v1,2025-03-28T13:10:04Z,2025-03-28 13:10:04,Generative Reliability-Based Design Optimization Using In-Context Learning Capabilities of Large Language Models,"Large Language Models (LLMs) have demonstrated remarkable in-context learning capabilities, enabling flexible utilization of limited historical information to play pivotal roles in reasoning, problem-solving, and complex pattern recognition tasks. Inspired by the successful applications of LLMs in multiple domains, this paper proposes a generative design method by leveraging the in-context learning capabilities of LLMs with the iterative search mechanisms of metaheuristic algorithms for solving reliability-based design optimization problems. In detail, reliability analysis is performed by engaging the LLMs and Kriging surrogate modeling to overcome the computational burden. By dynamically providing critical information of design points to the LLMs with prompt engineering, the method enables rapid generation of high-quality design alternatives that satisfy reliability constraints while achieving performance optimization. With the Deepseek-V3 model, three case studies are used to demonstrated the performance of the proposed approach. Experimental results indicate that the proposed LLM-RBDO method successfully identifies feasible solutions that meet reliability constraints while achieving a comparable convergence rate compared to traditional genetic algorithms.",Zhonglin Jiang|Qian Tang|Zequn Wang,,https://arxiv.org/abs/2503.22401v1,https://arxiv.org/pdf/2503.22401v1,,"17 pages, 11 figures, 4tables",,,cs.LG,cs.LG|stat.ME,https://arxiv.org/pdf/2503.22401v1.pdf
2503.20767v2,2025-03-26T17:52:19Z,2025-07-02 21:17:20,Reliable algorithm selection for machine learning-guided design,"Algorithms for machine learning-guided design, or design algorithms, use machine learning-based predictions to propose novel objects with desired property values. Given a new design task -- for example, to design novel proteins with high binding affinity to a therapeutic target -- one must choose a design algorithm and specify any hyperparameters and predictive and/or generative models involved. How can these decisions be made such that the resulting designs are successful? This paper proposes a method for design algorithm selection, which aims to select design algorithms that will produce a distribution of design labels satisfying a user-specified success criterion -- for example, that at least ten percent of designs' labels exceed a threshold. It does so by combining designs' predicted property values with held-out labeled data to reliably forecast characteristics of the label distributions produced by different design algorithms, building upon techniques from prediction-powered inference. The method is guaranteed with high probability to return design algorithms that yield successful label distributions (or the null set if none exist), if the density ratios between the design and labeled data distributions are known. We demonstrate the method's effectiveness in simulated protein and RNA design tasks, in settings with either known or estimated density ratios.",Clara Fannjiang|Ji Won Park,,https://arxiv.org/abs/2503.20767v2,https://arxiv.org/pdf/2503.20767v2,,ICML 2025,,,cs.LG,cs.LG|q-bio.QM|stat.ML,https://arxiv.org/pdf/2503.20767v2.pdf
2503.14106v1,2025-03-18T10:21:32Z,2025-03-18 10:21:32,Reliable uncertainty quantification for 2D/3D anatomical landmark localization using multi-output conformal prediction,"Automatic anatomical landmark localization in medical imaging requires not just accurate predictions but reliable uncertainty quantification for effective clinical decision support. Current uncertainty quantification approaches often fall short, particularly when combined with normality assumptions, systematically underestimating total predictive uncertainty. This paper introduces conformal prediction as a framework for reliable uncertainty quantification in anatomical landmark localization, addressing a critical gap in automatic landmark localization. We present two novel approaches guaranteeing finite-sample validity for multi-output prediction: Multi-output Regression-as-Classification Conformal Prediction (M-R2CCP) and its variant Multi-output Regression to Classification Conformal Prediction set to Region (M-R2C2R). Unlike conventional methods that produce axis-aligned hyperrectangular or ellipsoidal regions, our approaches generate flexible, non-convex prediction regions that better capture the underlying uncertainty structure of landmark predictions. Through extensive empirical evaluation across multiple 2D and 3D datasets, we demonstrate that our methods consistently outperform existing multi-output conformal prediction approaches in both validity and efficiency. This work represents a significant advancement in reliable uncertainty estimation for anatomical landmark localization, providing clinicians with trustworthy confidence measures for their diagnoses. While developed for medical imaging, these methods show promise for broader applications in multi-output regression problems.",Jef Jonkers|Frank Coopman|Luc Duchateau|Glenn Van Wallendael|Sofie Van Hoecke,,https://arxiv.org/abs/2503.14106v1,https://arxiv.org/pdf/2503.14106v1,,"33 pages, 10 figures",,,cs.CV,cs.CV|cs.AI|stat.ML,https://arxiv.org/pdf/2503.14106v1.pdf
2503.13404v1,2025-03-17T17:34:34Z,2025-03-17 17:34:34,Fed-Joint: Joint Modeling of Nonlinear Degradation Signals and Failure Events for Remaining Useful Life Prediction using Federated Learning,"Many failure mechanisms of machinery are closely related to the behavior of condition monitoring (CM) signals. To achieve a cost-effective preventive maintenance strategy, accurate remaining useful life (RUL) prediction based on the signals is of paramount importance. However, the CM signals are often recorded at different factories and production lines, with limited amounts of data. Unfortunately, these datasets have rarely been shared between the sites due to data confidentiality and ownership issues, a lack of computing and storage power, and high communication costs associated with data transfer between sites and a data center. Another challenge in real applications is that the CM signals are often not explicitly specified \textit{a priori}, meaning that existing methods, which often usually a parametric form, may not be applicable. To address these challenges, we propose a new prognostic framework for RUL prediction using the joint modeling of nonlinear degradation signals and time-to-failure data within a federated learning scheme. The proposed method constructs a nonparametric degradation model using a federated multi-output Gaussian process and then employs a federated survival model to predict failure times and probabilities for in-service machinery. The superiority of the proposed method over other alternatives is demonstrated through comprehensive simulation studies and a case study using turbofan engine degradation signal data that include run-to-failure events.",Cheoljoon Jeong|Xubo Yue|Seokhyun Chung,,https://arxiv.org/abs/2503.13404v1,https://arxiv.org/pdf/2503.13404v1,,,,,cs.AI,cs.AI|cs.LG|stat.ML,https://arxiv.org/pdf/2503.13404v1.pdf
2503.13335v1,2025-03-17T16:15:02Z,2025-03-17 16:15:02,Reliable and Efficient Amortized Model-based Evaluation,"Comprehensive evaluations of language models (LM) during both development and deployment phases are necessary because these models possess numerous capabilities (e.g., mathematical reasoning, legal support, or medical diagnostic) as well as safety risks (e.g., racial bias, toxicity, or misinformation). The average score across a wide range of benchmarks provides a signal that helps guide the use of these LMs in practice. Currently, holistic evaluations are costly due to the large volume of benchmark questions, making frequent evaluations impractical. A popular attempt to lower the cost is to compute the average score on a subset of the benchmark. This approach, unfortunately, often renders an unreliable measure of LM performance because the average score is often confounded with the difficulty of the questions in the benchmark subset. Item response theory (IRT) was designed to address this challenge, providing a reliable measurement by careful controlling for question difficulty. Unfortunately, question difficulty is expensive to estimate. Facing this challenge, we train a model that predicts question difficulty from its content, enabling a reliable measurement at a fraction of the cost. In addition, we leverage this difficulty predictor to further improve the evaluation efficiency through training a question generator given a difficulty level. This question generator is essential in adaptive testing, where, instead of using a random subset of the benchmark questions, informative questions are adaptively chosen based on the current estimation of LLM performance. Experiments on 22 common natural language benchmarks and 172 LMs show that this approach is more reliable and efficient compared to current common practice.",Sang Truong|Yuheng Tu|Percy Liang|Bo Li|Sanmi Koyejo,,https://arxiv.org/abs/2503.13335v1,https://arxiv.org/pdf/2503.13335v1,,,,,cs.CL,cs.CL|cs.AI|cs.LG|stat.AP,https://arxiv.org/pdf/2503.13335v1.pdf
2503.15545v1,2025-03-16T13:51:59Z,2025-03-16 13:51:59,Data-Driven Approximation of Binary-State Network Reliability Function: Algorithm Selection and Reliability Thresholds for Large-Scale Systems,"Network reliability assessment is pivotal for ensuring the robustness of modern infrastructure systems, from power grids to communication networks. While exact reliability computation for binary-state networks is NP-hard, existing approximation methods face critical tradeoffs between accuracy, scalability, and data efficiency. This study evaluates 20 machine learning methods across three reliability regimes full range (0.0-1.0), high reliability (0.9-1.0), and ultra high reliability (0.99-1.0) to address these gaps. We demonstrate that large-scale networks with arc reliability larger than or equal to 0.9 exhibit near-unity system reliability, enabling computational simplifications. Further, we establish a dataset-scale-driven paradigm for algorithm selection: Artificial Neural Networks (ANN) excel with limited data, while Polynomial Regression (PR) achieves superior accuracy in data-rich environments. Our findings reveal ANN's Test-MSE of 7.24E-05 at 30,000 samples and PR's optimal performance (5.61E-05) at 40,000 samples, outperforming traditional Monte Carlo simulations. These insights provide actionable guidelines for balancing accuracy, interpretability, and computational efficiency in reliability engineering, with implications for infrastructure resilience and system optimization.",Wei-Chang Yeh,,https://arxiv.org/abs/2503.15545v1,https://arxiv.org/pdf/2503.15545v1,,,,,cs.LG,cs.LG|math.NA|stat.ML,https://arxiv.org/pdf/2503.15545v1.pdf
2503.10175v1,2025-03-13T08:57:17Z,2025-03-13 08:57:17,Towards more reliable public transportation Wi-Fi Origin-Destination matrices: Modeling errors using synthetic noise and optical counts,"To continuously monitor mobility flows aboard public transportation, low-cost data collection methods based on the passive detection of Wi-Fi signals are promising technological solutions, but they yield uncertain results. We assess the accuracy of these results in light of a three-month experimentation conducted aboard buses equipped with Wi-Fi sensors in a sizable French conurbation. We put forward a method to quantify the error between the stop-to-stop origin-destination (O-D) matrix produced by Wi-Fi data and the ground truth, when the (estimated and real) volumes per boarding and alighting are known. To do so, the error in the estimated matrix is modeled by random noise. Neither additive, nor multiplicative noise replicate the experimental results. Noise models that concentrate on the short O-D trips and/or the central stops better reflect the structure of the error. But only by introducing distinct uncertainties between the boarding stop and the alighting stop can we recover the asymmetry between the alighting and boarding errors, as well as the correct ratios between these aggregate errors and the O-D error. Thus, our findings give insight into the main sources of error in the Wi-Fi based reconstruction of O-D matrices. They also provide analysts with an automatic and reproducible way to control the quality of O-D matrices produced by Wi-Fi data, using (readily available) count data.",Léa Fabre|Caroline Bayart|Alexandre Nicolas|Patrick Bonnel,"LAET, LSAF|CHROME, LAET, LSAF|ILM, CNRS|ENTPE, LAET",https://arxiv.org/abs/2503.10175v1,https://arxiv.org/pdf/2503.10175v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2503.10175v1.pdf
2503.05907v1,2025-03-07T20:05:02Z,2025-03-07 20:05:02,Real-time Bus Travel Time Prediction and Reliability Quantification: A Hybrid Markov Model,"Accurate and reliable bus travel time prediction in real-time is essential for improving the operational efficiency of public transportation systems. However, this remains a challenging task due to the limitations of existing models and data sources. This study proposed a hybrid Markovian framework for real-time bus travel time prediction, incorporating uncertainty quantification. Firstly, the bus link travel time distributions were modeled by integrating various influential factors while explicitly accounting for heteroscedasticity. Particularly, the parameters of the distributions were estimated using Maximum Likelihood Estimation, and the Fisher Information Matrix was then employed to calculate the 95\% uncertainty bounds for the estimated parameters, ensuring a robust and reliable quantification of prediction uncertainty of bus link travel times. Secondly, a Markovian framework with transition probabilities based on previously predicted bus link travel times was developed to predict travel times and their uncertainties from a current location to any future stop along the route. The framework was evaluated using the General Transit Feed Specification (GTFS) Static and Realtime data collected in 2023 from Gainesville, Florida. The results showed that the proposed model consistently achieved better prediction performance compared to the selected baseline approaches (including historical mean, statistical and AI-based models) while providing narrower uncertainty bounds. The model also demonstrated high interpretability, as the estimated coefficients provided insights into how different factors influencing bus travel times across links with varying characteristics. These findings suggest that the model could serve as a valuable tool for transit system performance evaluation and real-time trip planning.",Yuran Sun|James Spall|Wai Wong|Xilei Zhao,,https://arxiv.org/abs/2503.05907v1,https://arxiv.org/pdf/2503.05907v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2503.05907v1.pdf
2503.04588v3,2025-03-06T16:26:21Z,2026-01-25 19:34:29,Fiducial Inference for Random-Effects Calibration Models: Advancing Reliable Quantification in Environmental Analytical Chemistry,"This article addresses calibration challenges in analytical chemistry by employing a random-effects calibration curve model and its generalizations to capture variability in analyte concentrations. The model is motivated by specific issues in analytical chemistry, where measurement errors remain constant at low concentrations but increase proportionally as concentrations rise. To account for this, the model permits the parameters of the calibration curve, which relate instrument responses to true concentrations, to vary across different laboratories, thereby reflecting real-world variability in measurement processes. Traditional large-sample interval estimation methods are inadequate for small samples, leading to the use of an alternative approach, namely the fiducial approach. The calibration curve that accurately captures the heteroscedastic nature of the data, results in more reliable estimates across diverse laboratory conditions. It turns out that the fiducial approach, when used to construct a confidence interval for an unknown concentration, produces a slightly wider width while achieving the desired coverage probability. Applications considered include the determination of the presence of an analyte and the interval estimation of an unknown true analyte concentration. The proposed method is demonstrated for both simulated and real interlaboratory data, including examples involving copper and cadmium in distilled water.",Soumya Sahu|Thomas Mathew|Robert Gibbons|Dulal K. Bhaumik,,https://arxiv.org/abs/2503.04588v3,https://arxiv.org/pdf/2503.04588v3,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2503.04588v3.pdf
2503.01566v1,2025-03-03T14:09:57Z,2025-03-03 14:09:57,Efficient Long-Term Structural Reliability Estimation with Non-Gaussian Stochastic Models: A Design of Experiments Approach,"Extreme response assessment is important in the design and operation of engineering structures, and is a crucial part of structural risk and reliability analyses. Structures should be designed in a way that enables them to withstand the environmental loads they are expected to experience over their lifetime, without designs being unnecessarily conservative and costly. An accurate risk estimate is essential but difficult to obtain because the long-term behaviour of a structure is typically too complex to calculate analytically or with brute force Monte Carlo simulation. Therefore, approximation methods are required to estimate the extreme response using only a limited number of short-term conditional response calculations. Combining surrogate models with Design of Experiments is an approximation approach that has gained popularity due to its ability to account for both long-term environment variability and short-term response variability. In this paper, we propose a method for estimating the extreme response of black-box, stochastic models with heteroscedastic non-Gaussian noise. We present a mathematically founded extreme response estimation process that enables Design of Experiment approaches that are prohibitively expensive with surrogate Monte Carlo. The theory leads us to speculate this method can robustly produce more confident extreme response estimates, and is suitable for a variety of domains. While this needs to be further validated empirically, the method offers a promising tool for reducing the uncertainty decision-makers face, allowing them to make better informed choices and create more optimal structures.",Sebastian Winter|Christian Agrell|Juan Camilo Guevara Gómez|Erik Vanem,,https://arxiv.org/abs/2503.01566v1,https://arxiv.org/pdf/2503.01566v1,,,,,stat.CO,stat.CO|stat.AP,https://arxiv.org/pdf/2503.01566v1.pdf
2502.17007v1,2025-02-24T09:38:31Z,2025-02-24 09:38:31,All You Need for Counterfactual Explainability Is Principled and Reliable Estimate of Aleatoric and Epistemic Uncertainty,"This position paper argues that, to its detriment, transparency research overlooks many foundational concepts of artificial intelligence. Here, we focus on uncertainty quantification -- in the context of ante-hoc interpretability and counterfactual explainability -- showing how its adoption could address key challenges in the field. First, we posit that uncertainty and ante-hoc interpretability offer complementary views of the same underlying idea; second, we assert that uncertainty provides a principled unifying framework for counterfactual explainability. Consequently, inherently transparent models can benefit from human-centred explanatory insights -- like counterfactuals -- which are otherwise missing. At a higher level, integrating artificial intelligence fundamentals into transparency research promises to yield more reliable, robust and understandable predictive models.",Kacper Sokol|Eyke Hüllermeier,,https://arxiv.org/abs/2502.17007v1,https://arxiv.org/pdf/2502.17007v1,,,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2502.17007v1.pdf
2502.12386v1,2025-02-17T23:50:36Z,2025-02-17 23:50:36,"Bridging the Data Gap in AI Reliability Research and Establishing DR-AIR, a Comprehensive Data Repository for AI Reliability","Artificial intelligence (AI) technology and systems have been advancing rapidly. However, ensuring the reliability of these systems is crucial for fostering public confidence in their use. This necessitates the modeling and analysis of reliability data specific to AI systems. A major challenge in AI reliability research, particularly for those in academia, is the lack of readily available AI reliability data. To address this gap, this paper focuses on conducting a comprehensive review of available AI reliability data and establishing DR-AIR: a data repository for AI reliability. Specifically, we introduce key measurements and data types for assessing AI reliability, along with the methodologies used to collect these data. We also provide a detailed description of the currently available datasets with illustrative examples. Furthermore, we outline the setup of the DR-AIR repository and demonstrate its practical applications. This repository provides easy access to datasets specifically curated for AI reliability research. We believe these efforts will significantly benefit the AI research community by facilitating access to valuable reliability data and promoting collaboration across various academic domains within AI. We conclude our paper with a call to action, encouraging the research community to contribute and share AI reliability data to further advance this critical field of study.",Simin Zheng|Jared M. Clark|Fatemeh Salboukh|Priscila Silva|Karen da Mata|Fenglian Pan|Jie Min|Jiayi Lian|Caleb B. King|Lance Fiondella|Jian Liu|Xinwei Deng|Yili Hong,,https://arxiv.org/abs/2502.12386v1,https://arxiv.org/pdf/2502.12386v1,,"34 pages, 12 figures",,,stat.AP,stat.AP|cs.AI,https://arxiv.org/pdf/2502.12386v1.pdf
2502.15772v2,2025-02-16T13:36:56Z,2025-08-16 16:47:38,Rashomon perspective for measuring uncertainty in the survival predictive maintenance models,"The prediction of the Remaining Useful Life of aircraft engines is a critical area in high-reliability sectors such as aerospace and defense. Early failure predictions help ensure operational continuity, reduce maintenance costs, and prevent unexpected failures. Traditional regression models struggle with censored data, which can lead to biased predictions. Survival models, on the other hand, effectively handle censored data, improving predictive accuracy in maintenance processes. This paper introduces a novel approach based on the Rashomon perspective, which considers multiple models that achieve similar performance rather than relying on a single best model. This enables uncertainty quantification in survival probability predictions and enhances decision-making in predictive maintenance. The Rashomon survival curve was introduced to represent the range of survival probability estimates, providing insights into model agreement and uncertainty over time. The results on the CMAPSS dataset demonstrate that relying solely on a single model for RUL estimation may increase risk in some scenarios. The censoring levels significantly impact prediction uncertainty, with longer censoring times leading to greater variability in survival probabilities. These findings underscore the importance of incorporating model multiplicity in predictive maintenance frameworks to achieve more reliable and robust failure predictions. This paper contributes to uncertainty quantification in RUL prediction and highlights the Rashomon perspective as a powerful tool for predictive modeling.",Yigitcan Yardimci|Mustafa Cavus,,https://arxiv.org/abs/2502.15772v2,https://arxiv.org/pdf/2502.15772v2,https://doi.org/10.1109/SIU66497.2025.11112410,"4 pages, 1 figures","2025 33rd Signal Processing and Communications Applications Conference (SIU), Sile, Istanbul, Turkiye, 2025, pp. 1-4",10.1109/SIU66497.2025.11112410,stat.AP,stat.AP|cs.LG,https://arxiv.org/pdf/2502.15772v2.pdf
2502.10985v1,2025-02-16T04:07:33Z,2025-02-16 04:07:33,Is Elo Rating Reliable? A Study Under Model Misspecification,"Elo rating, widely used for skill assessment across diverse domains ranging from competitive games to large language models, is often understood as an incremental update algorithm for estimating a stationary Bradley-Terry (BT) model. However, our empirical analysis of practical matching datasets reveals two surprising findings: (1) Most games deviate significantly from the assumptions of the BT model and stationarity, raising questions on the reliability of Elo. (2) Despite these deviations, Elo frequently outperforms more complex rating systems, such as mElo and pairwise models, which are specifically designed to account for non-BT components in the data, particularly in terms of win rate prediction. This paper explains this unexpected phenomenon through three key perspectives: (a) We reinterpret Elo as an instance of online gradient descent, which provides no-regret guarantees even in misspecified and non-stationary settings. (b) Through extensive synthetic experiments on data generated from transitive but non-BT models, such as strongly or weakly stochastic transitive models, we show that the ''sparsity'' of practical matching data is a critical factor behind Elo's superior performance in prediction compared to more complex rating systems. (c) We observe a strong correlation between Elo's predictive accuracy and its ranking performance, further supporting its effectiveness in ranking.",Shange Tang|Yuanhao Wang|Chi Jin,,https://arxiv.org/abs/2502.10985v1,https://arxiv.org/pdf/2502.10985v1,,23pages,,,cs.LG,cs.LG|cs.AI|stat.ME|stat.ML,https://arxiv.org/pdf/2502.10985v1.pdf
2502.04793v2,2025-02-07T09:55:24Z,2025-02-24 13:31:14,$t$-Testing the Waters: Empirically Validating Assumptions for Reliable A/B-Testing,"A/B-tests are a cornerstone of experimental design on the web, with wide-ranging applications and use-cases. The statistical $t$-test comparing differences in means is the most commonly used method for assessing treatment effects, often justified through the Central Limit Theorem (CLT). The CLT ascertains that, as the sample size grows, the sampling distribution of the Average Treatment Effect converges to normality, making the $t$-test valid for sufficiently large sample sizes. When outcome measures are skewed or non-normal, quantifying what ""sufficiently large"" entails is not straightforward.
  To ensure that confidence intervals maintain proper coverage and that $p$-values accurately reflect the false positive rate, it is critical to validate this normality assumption. We propose a practical method to test this, by analysing repeatedly resampled A/A-tests. When the normality assumption holds, the resulting $p$-value distribution should be uniform, and this property can be tested using the Kolmogorov-Smirnov test. This provides an efficient and effective way to empirically assess whether the $t$-test's assumptions are met, and the A/B-test is valid. We demonstrate our methodology and highlight how it helps to identify scenarios prone to inflated Type-I errors. Our approach provides a practical framework to ensure and improve the reliability and robustness of A/B-testing practices.",Olivier Jeunen,,https://arxiv.org/abs/2502.04793v2,https://arxiv.org/pdf/2502.04793v2,,,,,stat.ME,stat.ME|cs.LG,https://arxiv.org/pdf/2502.04793v2.pdf
2502.03062v2,2025-02-05T10:48:12Z,2025-05-24 02:55:42,Change Point Detection in the Frequency Domain with Statistical Reliability,"Effective condition monitoring in complex systems requires identifying change points (CPs) in the frequency domain, as the structural changes often arise across multiple frequencies. This paper extends recent advancements in statistically significant CP detection, based on Selective Inference (SI), to the frequency domain. The proposed SI method quantifies the statistical significance of detected CPs in the frequency domain using $p$-values, ensuring that the detected changes reflect genuine structural shifts in the target system. We address two major technical challenges to achieve this. First, we extend the existing SI framework to the frequency domain by appropriately utilizing the properties of discrete Fourier transform (DFT). Second, we develop an SI method that provides valid $p$-values for CPs where changes occur across multiple frequencies. Experimental results demonstrate that the proposed method reliably identifies genuine CPs with strong statistical guarantees, enabling more accurate root-cause analysis in the frequency domain of complex systems.",Akifumi Yamada|Tomohiro Shiraishi|Shuichi Nishino|Teruyuki Katsuoka|Kouichi Taji|Ichiro Takeuchi,,https://arxiv.org/abs/2502.03062v2,https://arxiv.org/pdf/2502.03062v2,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2502.03062v2.pdf
2501.17401v1,2025-01-29T03:43:12Z,2025-01-29 03:43:12,Gradient-free Importance Sampling Scheme for Efficient Reliability Estimation,"This work presents a novel gradient-free importance sampling-based framework for precisely and efficiently estimating rare event probabilities, often encountered in reliability analyses of engineering systems. The approach is formulated around our foundational Approximate Sampling Target with Post-processing Adjustment (ASTPA) methodology. ASTPA uniquely constructs and directly samples an unnormalized target distribution, relaxing the optimal importance sampling distribution (ISD). The target's normalizing constant is then estimated using our inverse importance sampling (IIS) scheme, employing an ISD fitted based on the obtained samples. In this work, a gradient-free sampling method within ASTPA is developed through a guided dimension-robust preconditioned Crank-Nicolson (pCN) algorithm, particularly suitable for black-box computational models where analytical gradient information is not available. To boost the sampling efficiency of pCN in our context, a computationally effective, general discovery stage for the rare event domain is devised, providing (multi-modal) rare event samples used in initializing the pCN chains. A series of diverse test functions and engineering problems involving high dimensionality and strong nonlinearity is presented, demonstrating the advantages of the proposed framework compared to several state-of-the-art sampling methods.",Elsayed Eshra|Konstantinos G. Papakonstantinou,,https://arxiv.org/abs/2501.17401v1,https://arxiv.org/pdf/2501.17401v1,,arXiv admin note: text overlap with arXiv:2405.14149,,,stat.ME,stat.ME|stat.AP|stat.CO,https://arxiv.org/pdf/2501.17401v1.pdf
2501.15194v3,2025-01-25T12:13:38Z,2025-02-04 06:44:31,Reliable Pseudo-labeling via Optimal Transport with Attention for Short Text Clustering,"Short text clustering has gained significant attention in the data mining community. However, the limited valuable information contained in short texts often leads to low-discriminative representations, increasing the difficulty of clustering. This paper proposes a novel short text clustering framework, called Reliable \textbf{P}seudo-labeling via \textbf{O}ptimal \textbf{T}ransport with \textbf{A}ttention for Short Text Clustering (\textbf{POTA}), that generate reliable pseudo-labels to aid discriminative representation learning for clustering. Specially, \textbf{POTA} first implements an instance-level attention mechanism to capture the semantic relationships among samples, which are then incorporated as a semantic consistency regularization term into an optimal transport problem. By solving this OT problem, we can yield reliable pseudo-labels that simultaneously account for sample-to-sample semantic consistency and sample-to-cluster global structure information. Additionally, the proposed OT can adaptively estimate cluster distributions, making \textbf{POTA} well-suited for varying degrees of imbalanced datasets. Then, we utilize the pseudo-labels to guide contrastive learning to generate discriminative representations and achieve efficient clustering. Extensive experiments demonstrate \textbf{POTA} outperforms state-of-the-art methods. The code is available at: \href{https://github.com/YZH0905/POTA-STC/tree/main}{https://github.com/YZH0905/POTA-STC/tree/main}.",Zhihao Yao|Jixuan Yin|Bo Li,,https://arxiv.org/abs/2501.15194v3,https://arxiv.org/pdf/2501.15194v3,,,,,cs.LG,cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/2501.15194v3.pdf
2501.14090v1,2025-01-23T20:50:12Z,2025-01-23 20:50:12,Making Reliable and Flexible Decisions in Long-tailed Classification,"Long-tailed classification is challenging due to its heavy imbalance in class probabilities. While existing methods often focus on overall accuracy or accuracy for tail classes, they overlook a critical aspect: certain types of errors can carry greater risks than others in real-world long-tailed problems. For example, misclassifying patients (a tail class) as healthy individuals (a head class) entails far more serious consequences than the reverse scenario. To address this critical issue, we introduce Making Reliable and Flexible Decisions in Long-tailed Classification (RF-DLC), a novel framework aimed at reliable predictions in long-tailed problems. Leveraging Bayesian Decision Theory, we introduce an integrated gain to seamlessly combine long-tailed data distributions and the decision-making procedure. We further propose an efficient variational optimization strategy for the decision risk objective. Our method adapts readily to diverse utility matrices, which can be designed for specific tasks, ensuring its flexibility for different problem settings. In empirical evaluation, we design a new metric, False Head Rate, to quantify tail-sensitivity risk, along with comprehensive experiments on multiple real-world tasks, including large-scale image classification and uncertainty quantification, to demonstrate the reliability and flexibility of our method.",Bolian Li|Ruqi Zhang,,https://arxiv.org/abs/2501.14090v1,https://arxiv.org/pdf/2501.14090v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2501.14090v1.pdf
2501.11730v1,2025-01-20T20:29:40Z,2025-01-20 20:29:40,Transformer Vibration Forecasting for Advancing Rail Safety and Maintenance 4.0,"Maintaining railway axles is critical to preventing severe accidents and financial losses. The railway industry is increasingly interested in advanced condition monitoring techniques to enhance safety and efficiency, moving beyond traditional periodic inspections toward Maintenance 4.0.
  This study introduces a robust Deep Autoregressive solution that integrates seamlessly with existing systems to avert mechanical failures. Our approach simulates and predicts vibration signals under various conditions and fault scenarios, improving dataset robustness for more effective detection systems. These systems can alert maintenance needs, preventing accidents preemptively. We use experimental vibration signals from accelerometers on train axles.
  Our primary contributions include a transformer model, ShaftFormer, designed for processing time series data, and an alternative model incorporating spectral methods and enhanced observation models. Simulating vibration signals under diverse conditions mitigates the high cost of obtaining experimental signals for all scenarios. Given the non-stationary nature of railway vibration signals, influenced by speed and load changes, our models address these complexities, offering a powerful tool for predictive maintenance in the rail industry.",Darío C. Larese|Almudena Bravo Cerrada|Gabriel Dambrosio Tomei|Alejandro Guerrero-López|Pablo M. Olmos|María Jesús Gómez García,,https://arxiv.org/abs/2501.11730v1,https://arxiv.org/pdf/2501.11730v1,,,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2501.11730v1.pdf
2501.07949v1,2025-01-14T09:05:59Z,2025-01-14 09:05:59,One cut-point phase-type distributions in Reliability. An application to Resistive Random Access Memories,"A new probability distribution to study lifetime data in reliability is introduced in this paper. This one is a first approach to a non-homogeneous phase-type distribution. It is built by considering one cut-point in the non-negative semi-line of a phase-type distribution. The density function is defined and the main measures associated, such as the reliability function, hazard rate, cumulative hazard rate and the characteristic function are also worked out. This new class of distributions enables to decrease the number of parameter in the estimate when inference is considered. Besides, the likelihood distribution is built to estimate the model parameters by maximum likelihood. Several applications by considering Resistive Random Access Memories compare the adjustment when phase type distributions and one cut-point phase-type distributions are considered. The developed methodology has been computationally implemented in R-cran.",Christian Acal|Juan Eloy Ruiz-Castro|David Maldonado|Juan B. Roldán,,https://arxiv.org/abs/2501.07949v1,https://arxiv.org/pdf/2501.07949v1,https://doi.org/10.3390/math9212734,,"Mathematics 2021, 9(21), 2734",10.3390/math9212734,stat.ME,stat.ME,https://arxiv.org/pdf/2501.07949v1.pdf
2501.06429v1,2025-01-11T04:03:29Z,2025-01-11 04:03:29,Reliable Imputed-Sample Assisted Vertical Federated Learning,"Vertical Federated Learning (VFL) is a well-known FL variant that enables multiple parties to collaboratively train a model without sharing their raw data. Existing VFL approaches focus on overlapping samples among different parties, while their performance is constrained by the limited number of these samples, leaving numerous non-overlapping samples unexplored. Some previous work has explored techniques for imputing missing values in samples, but often without adequate attention to the quality of the imputed samples. To address this issue, we propose a Reliable Imputed-Sample Assisted (RISA) VFL framework to effectively exploit non-overlapping samples by selecting reliable imputed samples for training VFL models. Specifically, after imputing non-overlapping samples, we introduce evidence theory to estimate the uncertainty of imputed samples, and only samples with low uncertainty are selected. In this way, high-quality non-overlapping samples are utilized to improve VFL model. Experiments on two widely used datasets demonstrate the significant performance gains achieved by the RISA, especially with the limited overlapping samples, e.g., a 48% accuracy gain on CIFAR-10 with only 1% overlapping samples.",Yaopei Zeng|Lei Liu|Shaoguo Liu|Hongjian Dou|Baoyuan Wu|Li Liu,,https://arxiv.org/abs/2501.06429v1,https://arxiv.org/pdf/2501.06429v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2501.06429v1.pdf
2412.20416v1,2024-12-29T09:49:08Z,2024-12-29 09:49:08,Hierarchical Bayesian Modeling for Uncertainty Quantification and Reliability Updating using Data,"Quantifying uncertainty and updating reliability are essential for ensuring the safety and performance of engineering systems. This study develops a hierarchical Bayesian modeling (HBM) framework to quantify uncertainty and update reliability using data. By leveraging the probabilistic structure of HBM, the approach provides a robust solution for integrating model uncertainties and parameter variability into reliability assessments. The framework is applied to a linear mathematical model and a dynamical structural model. For the linear model, analytical solutions are derived for the hyper parameters and reliability, offering an efficient and precise means of uncertainty quantification and reliability evaluation. In the dynamical structural model, the posterior distributions of hyper parameters obtained from the HBM are used directly to update the reliability. This approach relies on the updated posteriors to reflect the influence of system uncertainties and dynamic behavior in the reliability predictions. The proposed approach demonstrates significant advantages over traditional Bayesian inference by addressing multi-source uncertainty in both static and dynamic contexts. This work highlights the versatility and computational efficiency of the HBM framework, establishing it as a powerful tool for uncertainty quantification and reliability updating in structural health monitoring and other engineering applications.",Xinyu Jia|Weinan Hou|Costas Papadimitriou,,https://arxiv.org/abs/2412.20416v1,https://arxiv.org/pdf/2412.20416v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2412.20416v1.pdf
2412.13731v1,2024-12-18T11:08:56Z,2024-12-18 11:08:56,Reliability analysis for non-deterministic limit-states using stochastic emulators,"Reliability analysis is a sub-field of uncertainty quantification that assesses the probability of a system performing as intended under various uncertainties. Traditionally, this analysis relies on deterministic models, where experiments are repeatable, i.e., they produce consistent outputs for a given set of inputs. However, real-world systems often exhibit stochastic behavior, leading to non-repeatable outcomes. These so-called stochastic simulators produce different outputs each time the model is run, even with fixed inputs. This paper formally introduces reliability analysis for stochastic models and addresses it by using suitable surrogate models to lower its typically high computational cost. Specifically, we focus on the recently introduced generalized lambda models and stochastic polynomial chaos expansions. These emulators are designed to learn the inherent randomness of the simulator's response and enable efficient uncertainty quantification at a much lower cost than traditional Monte Carlo simulation. We validate our methodology through three case studies. First, using an analytical function with a closed-form solution, we demonstrate that the emulators converge to the correct solution. Second, we present results obtained from the surrogates using a toy example of a simply supported beam. Finally, we apply the emulators to perform reliability analysis on a realistic wind turbine case study, where only a dataset of simulation results is available.",Anderson V. Pires|Maliki Moustapha|Stefano Marelli|Bruno Sudret,,https://arxiv.org/abs/2412.13731v1,https://arxiv.org/pdf/2412.13731v1,,,,,stat.CO,stat.CO|stat.ME|stat.ML,https://arxiv.org/pdf/2412.13731v1.pdf
2412.08380v1,2024-12-11T13:42:50Z,2024-12-11 13:42:50,A general approach to optimal imperfect maintenance activities of a repairable equipment with imperfect maintenance and multiple failure modes,"In this paper we describe a general approach to optimal imperfect maintenance activities of a repairable equipment with independent components. Most of the existing works on optimal imperfect maintenance activities of a repairable equipment with independent components. In addition, it is assumed that all the components of the equipment share the same model and the same maintenance intervals and that effectiveness of maintenance is known. In this paper we take a different approach. In order to formalize the uncertainty on the occurrence of failures and on the effect of maintenance activities we consider, for each component, a class of candidate models obtained combining models for failure rate with models for imperfect maintenance and let the data select the best model (that might be different for the different components). All the parameters are assumed to be unknown and are jointly estimated via maximum likelihood. Model selection is performed, separately for each component, using standard selection criteria that take into account the problem of over-parametrization. The selected models are used to derive the cost per unit time and the average reliability of the equipment, the objective functions of a Multi-Objective Optimization Problem with maintenance intervals of each single component as decision variables. The proposed procedure is illustrated using a real data example.",Rubén Mullor|Julio Mulero|Mario Trottini,,https://arxiv.org/abs/2412.08380v1,https://arxiv.org/pdf/2412.08380v1,https://doi.org/10.1016/j.cie.2018.12.032,,Computers & Industrial Engineering 128 (2019) 24-31,10.1016/j.cie.2018.12.032,math.OC,math.OC|stat.AP,https://arxiv.org/pdf/2412.08380v1.pdf
2412.01120v2,2024-12-02T04:45:10Z,2025-03-07 18:34:16,Reliable and scalable variable importance estimation via warm-start and early stopping,"As opaque black-box predictive models become more prevalent, the need to develop interpretations for these models is of great interest. The concept of variable importance and Shapley values are interpretability measures that applies to any predictive model and assesses how much a variable or set of variables improves prediction performance. When the number of variables is large, estimating variable importance presents a significant computational challenge because re-training neural networks or other black-box algorithms requires significant additional computation. In this paper, we address this challenge for algorithms using gradient descent and gradient boosting (e.g. neural networks, gradient-boosted decision trees). By using the ideas of early stopping of gradient-based methods in combination with warm-start using the dropout method, we develop a scalable method to estimate variable importance for any algorithm that can be expressed as an iterative kernel update equation. Importantly, we provide theoretical guarantees by using the theory for early stopping of kernel-based methods for neural networks with sufficiently large (but not necessarily infinite) width and gradient-boosting decision trees that use symmetric trees as a weaker learner. We also demonstrate the efficacy of our methods through simulations and a real data example which illustrates the computational benefit of early stopping rather than fully re-training the model as well as the increased accuracy of our approach.",Zexuan Sun|Garvesh Raskutti,,https://arxiv.org/abs/2412.01120v2,https://arxiv.org/pdf/2412.01120v2,,"Preliminary version accepted in AISTATS, 2025",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2412.01120v2.pdf
2411.19104v2,2024-11-28T12:38:46Z,2025-01-10 11:20:44,"Algorithmic modelling of a complex redundant multi-state system subject to multiple events, preventive maintenance, loss of units and a multiple vacation policy through a MMAP","A complex multi-state redundant system undergoing preventive maintenance and experiencing multiple events is being considered in a continuous time frame. The online unit is susceptible to various types of failures, both internal and external in nature, with multiple degradation levels present, both internally and externally. Random inspections are continuously monitoring these degradation levels, and if they reach a critical state, the unit is directed to a repair facility for preventive maintenance. The repair facility is managed by a single repairperson, who follows a multiple vacation policy dependent on the operational status of the units. The repairperson is responsible for two primary tasks: corrective repairs and preventive maintenance. The time durations within the system follow phase-type distributions, and the model is constructed using Markovian Arrival Processes with marked arrivals. A variety of performance measures, including transient and stationary distributions, are calculated using matrix-analytic methods. This approach enables the expression of key results and overall system behaviour in a matrix-algorithmic format. In order to optimize the model, costs and rewards are integrated into the analysis. A numerical example is presented to showcase the model's flexibility and effectiveness in real-world applications.",Juan Eloy Ruiz-Castro|Hugo Alaín Zapata-Ceballos,,https://arxiv.org/abs/2411.19104v2,https://arxiv.org/pdf/2411.19104v2,https://doi.org/10.1016/j.matcom.2024.11.005,,"Mathematics and Computers in Simulation, Volume 230, April 2025, Pages 165-192",10.1016/j.matcom.2024.11.005,stat.ME,stat.ME,https://arxiv.org/pdf/2411.19104v2.pdf
2411.15185v1,2024-11-19T03:00:02Z,2024-11-19 03:00:02,Hybrid Gaussian Process Regression with Temporal Feature Extraction for Partially Interpretable Remaining Useful Life Interval Prediction in Aeroengine Prognostics,"The estimation of Remaining Useful Life (RUL) plays a pivotal role in intelligent manufacturing systems and Industry 4.0 technologies. While recent advancements have improved RUL prediction, many models still face interpretability and compelling uncertainty modeling challenges. This paper introduces a modified Gaussian Process Regression (GPR) model for RUL interval prediction, tailored for the complexities of manufacturing process development. The modified GPR predicts confidence intervals by learning from historical data and addresses uncertainty modeling in a more structured way. The approach effectively captures intricate time-series patterns and dynamic behaviors inherent in modern manufacturing systems by coupling GPR with deep adaptive learning-enhanced AI process models. Moreover, the model evaluates feature significance to ensure more transparent decision-making, which is crucial for optimizing manufacturing processes. This comprehensive approach supports more accurate RUL predictions and provides transparent, interpretable insights into uncertainty, contributing to robust process development and management.",Tian Niu|Zijun Xu|Heng Luo|Ziqing Zhou,,https://arxiv.org/abs/2411.15185v1,https://arxiv.org/pdf/2411.15185v1,,,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2411.15185v1.pdf
2411.11238v1,2024-11-18T02:13:11Z,2024-11-18 02:13:11,Reliable Learning of Halfspaces under Gaussian Marginals,"We study the problem of PAC learning halfspaces in the reliable agnostic model of Kalai et al. (2012). The reliable PAC model captures learning scenarios where one type of error is costlier than the others. Our main positive result is a new algorithm for reliable learning of Gaussian halfspaces on $\mathbb{R}^d$ with sample and computational complexity $$d^{O(\log (\min\{1/α, 1/ε\}))}\min (2^{\log(1/ε)^{O(\log (1/α))}},2^{\mathrm{poly}(1/ε)})\;,$$ where $ε$ is the excess error and $α$ is the bias of the optimal halfspace. We complement our upper bound with a Statistical Query lower bound suggesting that the $d^{Ω(\log (1/α))}$ dependence is best possible. Conceptually, our results imply a strong computational separation between reliable agnostic learning and standard agnostic learning of halfspaces in the Gaussian setting.",Ilias Diakonikolas|Lisheng Ren|Nikos Zarifis,,https://arxiv.org/abs/2411.11238v1,https://arxiv.org/pdf/2411.11238v1,,,,,cs.LG,cs.LG|cs.DS|stat.ML,https://arxiv.org/pdf/2411.11238v1.pdf
2411.01324v2,2024-11-02T18:13:29Z,2025-01-20 10:31:51,Reliability Acceptance Sampling Plans under Progressive Type-I Interval Censoring Schemes in Presence of Dependent Competing Risks,"We discuss the development of reliability acceptance sampling plans under progressive Type-I interval censoring schemes in the presence of competing causes of failure. We consider a general framework to accommodate the presence of independent or dependent competing risks and derive the expression for the Fisher information matrix under this framework. We also discuss the asymptotic properties of the maximum likelihood estimators, which are essential in obtaining the sampling plans. Subsequently, we specialize in a frailty model, which allows us to accommodate the dependence among the potential causes of failure. The frailty model provides an independent competing risks model as a limiting case. We then present the traditional sampling plans for both independent and dependent competing risks models using producer and consumer risks. We also consider the design of optimal PIC-I schemes in this context and use a c optimal design criterion, which helps us to obtain more useful reliability acceptance sampling plans in the presence of budgetary constraints. We conduct a comprehensive numerical experiment to examine the impact of the level of dependence among the potential failure times on the resulting sampling plans. We demonstrate an application of the developed methodology using a real-life example and perform a simulation study to study the finite sample properties of the developed sampling plans. The methodology developed in this article has the potential to improve the design of optimal censoring schemes in the presence of competing risks while taking into account budgetary constraints.",Rathin Das|Soumya Roy|Biswabrata Pradhan,,https://arxiv.org/abs/2411.01324v2,https://arxiv.org/pdf/2411.01324v2,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2411.01324v2.pdf
2410.22751v1,2024-10-30T07:14:40Z,2024-10-30 07:14:40,Novel Subsampling Strategies for Heavily Censored Reliability Data,"Computational capability often falls short when confronted with massive data, posing a common challenge in establishing a statistical model or statistical inference method dealing with big data. While subsampling techniques have been extensively developed to downsize the data volume, there is a notable gap in addressing the unique challenge of handling extensive reliability data, in which a common situation is that a large proportion of data is censored. In this article, we propose an efficient subsampling method for reliability analysis in the presence of censoring data, intending to estimate the parameters of lifetime distribution. Moreover, a novel subsampling method for subsampling from severely censored data is proposed, i.e., only a tiny proportion of data is complete. The subsampling-based estimators are given, and their asymptotic properties are derived. The optimal subsampling probabilities are derived through the L-optimality criterion, which minimizes the trace of the product of the asymptotic covariance matrix and a constant matrix. Efficient algorithms are proposed to implement the proposed subsampling methods to address the challenge that optimal subsampling strategy depends on unknown parameter estimation from full data. Real-world hard drive dataset case and simulative empirical studies are employed to demonstrate the superior performance of the proposed methods.",Yixiao Ruan|Zan Li|Zhaohui Li|Dennis K. J. Lin|Qingpei Hu|Dan Yu,,https://arxiv.org/abs/2410.22751v1,https://arxiv.org/pdf/2410.22751v1,,"28 pages, 3 figures, to be published in Statistics and Its Interface",,,stat.ME,stat.ME|stat.CO,https://arxiv.org/pdf/2410.22751v1.pdf
2410.21350v1,2024-10-28T13:01:08Z,2024-10-28 13:01:08,Enhanced sequential directional importance sampling for structural reliability analysis,"Sequential directional importance sampling (SDIS) is an efficient adaptive simulation method for estimating failure probabilities. It expresses the failure probability as the product of a group of integrals that are easy to estimate, wherein the first one is estimated with Monte Carlo simulation (MCS), and all the subsequent ones are estimated with directional importance sampling. In this work, we propose an enhanced SDIS method for structural reliability analysis. We discuss the efficiency of MCS for estimating the first integral in standard SDIS and propose using Subset Simulation as an alternative method. Additionally, we propose a Kriging-based active learning algorithm tailored to identify multiple roots in certain important directions within a specificed search interval. The performance of the enhanced SDIS is demonstrated through various complex benchmark problems. The results show that the enhanced SDIS is a versatile reliability analysis method that can efficiently and robustly solve challenging reliability problems",Kai Chenga|Iason Papaioannou|Daniel Straub,,https://arxiv.org/abs/2410.21350v1,https://arxiv.org/pdf/2410.21350v1,,,,,stat.ME,stat.ME|math.PR,https://arxiv.org/pdf/2410.21350v1.pdf
2410.20443v1,2024-10-27T13:40:39Z,2024-10-27 13:40:39,A Robust Topological Framework for Detecting Regime Changes in Multi-Trial Experiments with Application to Predictive Maintenance,"We present a general and flexible framework for detecting regime changes in complex, non-stationary data across multi-trial experiments. Traditional change point detection methods focus on identifying abrupt changes within a single time series (single trial), targeting shifts in statistical properties such as the mean, variance, and spectrum over time within that sole trial. In contrast, our approach considers changes occurring across trials, accommodating changes that may arise within individual trials due to experimental inconsistencies, such as varying delays or event duration. By leveraging diverse metrics to analyze time-frequency characteristics specifically topological changes in the spectrum and spectrograms, our approach offers a comprehensive framework for detecting such variations. Our approach can handle different scenarios with various statistical assumptions, including varying levels of stationarity within and across trials, making our framework highly adaptable. We validate our approach through simulations using time-varying autoregressive processes that exhibit different regime changes. Our results demonstrate the effectiveness of detecting changes across trials under diverse conditions. Furthermore, we illustrate the effectiveness of our method by applying it to predictive maintenance using the NASA bearing dataset. By analyzing the time-frequency characteristics of vibration signals recorded by accelerometers, our approach accurately identifies bearing failures, showcasing its strong potential for early fault detection in mechanical systems.",Anass B. El-Yaagoubi|Jean-Marc Freyermuth|Hernando Ombao,,https://arxiv.org/abs/2410.20443v1,https://arxiv.org/pdf/2410.20443v1,https://doi.org/10.1111/jtsa.70032,,"Journal of Time Series Analysis, 2025; 0:1-19",10.1111/jtsa.70032,stat.ME,stat.ME,https://arxiv.org/pdf/2410.20443v1.pdf
2410.16608v2,2024-10-22T01:40:43Z,2025-04-01 02:20:44,Assessing and improving reliability of neighbor embedding methods: a map-continuity perspective,"Visualizing high-dimensional data is essential for understanding biomedical data and deep learning models. Neighbor embedding methods, such as t-SNE and UMAP, are widely used but can introduce misleading visual artifacts. We find that the manifold learning interpretations from many prior works are inaccurate and that the misuse stems from a lack of data-independent notions of embedding maps, which project high-dimensional data into a lower-dimensional space. Leveraging the leave-one-out principle, we introduce LOO-map, a framework that extends embedding maps beyond discrete points to the entire input space. We identify two forms of map discontinuity that distort visualizations: one exaggerates cluster separation and the other creates spurious local structures. As a remedy, we develop two types of point-wise diagnostic scores to detect unreliable embedding points and improve hyperparameter selection, which are validated on datasets from computer vision and single-cell omics.",Zhexuan Liu|Rong Ma|Yiqiao Zhong,,https://arxiv.org/abs/2410.16608v2,https://arxiv.org/pdf/2410.16608v2,,"49 pages, 20 figures",,,stat.ME,stat.ME|cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/2410.16608v2.pdf
2410.10572v4,2024-10-14T14:49:32Z,2025-05-08 01:45:26,Regularized Robustly Reliable Learners and Instance Targeted Attacks,"Instance-targeted data poisoning attacks, where an adversary corrupts a training set to induce errors on specific test points, have raised significant concerns. Balcan et al (2022) proposed an approach to addressing this challenge by defining a notion of robustly-reliable learners that provide per-instance guarantees of correctness under well-defined assumptions, even in the presence of data poisoning attacks. They then give a generic optimal (but computationally inefficient) robustly reliable learner as well as a computationally efficient algorithm for the case of linear separators over log-concave distributions.
  In this work, we address two challenges left open by Balcan et al (2022). The first is that the definition of robustly-reliable learners in Balcan et al (2022) becomes vacuous for highly-flexible hypothesis classes: if there are two classifiers h_0, h_1 \in H both with zero error on the training set such that h_0(x) \neq h_1(x), then a robustly-reliable learner must abstain on x. We address this problem by defining a modified notion of regularized robustly-reliable learners that allows for nontrivial statements in this case. The second is that the generic algorithm of Balcan et al (2022) requires re-running an ERM oracle (essentially, retraining the classifier) on each test point x, which is generally impractical even if ERM can be implemented efficiently. To tackle this problem, we show that at least in certain interesting cases we can design algorithms that can produce their outputs in time sublinear in training time, by using techniques from dynamic algorithm design.",Avrim Blum|Donya Saless,,https://arxiv.org/abs/2410.10572v4,https://arxiv.org/pdf/2410.10572v4,,,,,cs.LG,cs.LG|cs.CR|cs.DS|stat.ML,https://arxiv.org/pdf/2410.10572v4.pdf
2410.07627v2,2024-10-10T05:43:07Z,2025-03-20 05:08:24,Automatic Curriculum Expert Iteration for Reliable LLM Reasoning,"Hallucinations (i.e., generating plausible but inaccurate content) and laziness (i.e. excessive refusals or defaulting to ""I don't know"") persist as major challenges in LLM reasoning. Current efforts to reduce hallucinations primarily focus on factual errors in knowledge-grounded tasks, often neglecting hallucinations related to faulty reasoning. Meanwhile, some approaches render LLMs overly conservative, limiting their problem-solving capabilities. To mitigate hallucination and laziness in reasoning tasks, we propose Automatic Curriculum Expert Iteration (Auto-CEI) to enhance LLM reasoning and align responses to the model's capabilities--assertively answering within its limits and declining when tasks exceed them. In our method, Expert Iteration explores the reasoning trajectories near the LLM policy, guiding incorrect paths back on track to reduce compounding errors and improve robustness; it also promotes appropriate ""I don't know"" responses after sufficient reasoning attempts. The curriculum automatically adjusts rewards, incentivizing extended reasoning before acknowledging incapability, thereby pushing the limits of LLM reasoning and aligning its behaviour with these limits. We compare Auto-CEI with various SOTA baselines across logical reasoning, mathematics, and planning tasks, where Auto-CEI achieves superior alignment by effectively balancing assertiveness and conservativeness. The code is available at https://github.com/SalesforceAIResearch/Auto-CEI .",Zirui Zhao|Hanze Dong|Amrita Saha|Caiming Xiong|Doyen Sahoo,,https://arxiv.org/abs/2410.07627v2,https://arxiv.org/pdf/2410.07627v2,,20 pages,,,cs.LG,cs.LG|cs.AI|cs.CL|stat.ML,https://arxiv.org/pdf/2410.07627v2.pdf
2409.19910v2,2024-09-30T03:26:40Z,2025-07-17 10:18:02,From Likelihood to Limit State: A Reliability-Inspired Framework for Bayesian Evidence Estimation and High-dimensional Sampling,"Bayesian analysis plays a crucial role in estimating distribution of unknown parameters for given data and model. Due to the curse of dimensionality, it becomes difficult for high-dimensional problems, especially when multiple modes exist. This paper introduces an efficient Bayesian posterior sampling algorithm, based on a new interpretation of evidence from the perspective of structural reliability estimation. That is, the evidence can be equivalently formulated as an integration of failure probabilities, by regarding the likelihood function as a limit state function. The evidence is then evaluated with subset simulation (SuS) algorithm. The posterior samples can be obtained following the principle of importance resampling as a postprocessing procedure. The estimation variance is derived to quantify the inherent uncertainty associated with the SuS estimator of evidence. The effective sample size is introduced to measure the quality of posterior sampling. Three benchmark examples are first considered to illustrate the performance of the proposed algorithm by comparing it with two state-of-art algorithms. It is then used for the finite element model updating, showing its applicability in practical engineering problems. The proposed SuS algorithm exhibits comparable or even better performance in evidence estimation and posterior sampling, compared to the aBUS and MULTINEST algorithms, especially when the dimension of unknown parameters is high.",Zihan Liao|Binbin Li|Hua-Ping Wan,,https://arxiv.org/abs/2409.19910v2,https://arxiv.org/pdf/2409.19910v2,,"33 pages, 8 figures, 36 references",,,stat.ME,stat.ME,https://arxiv.org/pdf/2409.19910v2.pdf
2408.13089v2,2024-08-23T14:16:10Z,2024-08-26 06:40:07,On the good reliability of an interval-based metric to validate prediction uncertainty for machine learning regression tasks,"This short study presents an opportunistic approach to a (more) reliable validation method for prediction uncertainty average calibration. Considering that variance-based calibration metrics (ZMS, NLL, RCE...) are quite sensitive to the presence of heavy tails in the uncertainty and error distributions, a shift is proposed to an interval-based metric, the Prediction Interval Coverage Probability (PICP). It is shown on a large ensemble of molecular properties datasets that (1) sets of z-scores are well represented by Student's-$t(ν)$ distributions, $ν$ being the number of degrees of freedom; (2) accurate estimation of 95 $\%$ prediction intervals can be obtained by the simple $2σ$ rule for $ν>3$; and (3) the resulting PICPs are more quickly and reliably tested than variance-based calibration metrics. Overall, this method enables to test 20 $\%$ more datasets than ZMS testing. Conditional calibration is also assessed using the PICP approach.",Pascal Pernot,,https://arxiv.org/abs/2408.13089v2,https://arxiv.org/pdf/2408.13089v2,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2408.13089v2.pdf
2408.10083v2,2024-08-19T15:25:49Z,2025-07-10 15:51:41,A Case Study on Quantifying Reliability under Extreme Risk Constraints in Space Missions,"In this paper, we employ a Bayesian approach to uncertainty quantification of computer simulations used to assess the probability of rare events. As a case study, we assess the reliability of an Earth reentry capsule for sample return missions that must be able to withstand the reentry loads in order to land intact. Our study uses Gaussian Process modeling under a Bayesian regime to analyze the reentry vehicle's resilience against operational stress. This Bayesian framework allows for a detailed probabilistic evaluation of the system's reliability, indicating our ability to verify stringent safety goals of rare events with a 0.999999 of probability of success. The findings underscore the effectiveness of Bayesian methods for complex uncertainty quantification analyses of computer simulations, providing valuable insights for computational reliability analysis in a risk-averse setting.",Dawn L. Sanderson|Amy Braverman|Giuseppe Cataldo|Ralph C. Smith|Richard L. Smith,,https://arxiv.org/abs/2408.10083v2,https://arxiv.org/pdf/2408.10083v2,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2408.10083v2.pdf
2408.10288v1,2024-08-19T09:47:01Z,2024-08-19 09:47:01,Augmenting train maintenance technicians with automated incident diagnostic suggestions,"Train operational incidents are so far diagnosed individually and manually by train maintenance technicians. In order to assist maintenance crews in their responsiveness and task prioritization, a learning machine is developed and deployed in production to suggest diagnostics to train technicians on their phones, tablets or laptops as soon as a train incident is declared. A feedback loop allows to take into account the actual diagnose by designated train maintenance experts to refine the learning machine. By formulating the problem as a discrete set classification task, feature engineering methods are proposed to extract physically plausible sets of events from traces generated on-board railway vehicles. The latter feed an original ensemble classifier to class incidents by their potential technical cause. Finally, the resulting model is trained and validated using real operational data and deployed on a cloud platform. Future work will explore how the extracted sets of events can be used to avoid incidents by assisting human experts in the creation predictive maintenance alerts.",Georges Tod|Jean Bruggeman|Evert Bevernage|Pieter Moelans|Walter Eeckhout|Jean-Luc Glineur,,https://arxiv.org/abs/2408.10288v1,https://arxiv.org/pdf/2408.10288v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2408.10288v1.pdf
2408.00734v1,2024-08-01T17:26:21Z,2024-08-01 17:26:21,Bayesian reliability acceptance sampling plans under adaptive simple step stress partial accelerated life test,"In the traditional simple step-stress partial accelerated life test (SSSPALT), the items are put on normal operating conditions up to a certain time and after that the stress is increased to get the failure time information early. However, when the stress increases, an additional cost is incorporated that increases the cost of the life test. In this context, an adaptive SSSPALT is considered where the stress is increased after a certain time if the number of failures up to that point is less than a pre-specified number of failures. We consider determination of Bayesian reliability acceptance sampling plans (BSP) through adaptive SSSALT conducted under Type I censoring. The BSP under adaptive SSSPALT is called BSPAA. The Bayes decision function and Bayes risk are obtained for the general loss function. Optimal BSPAAs are obtained for the quadratic loss function by minimizing Bayes risk. An algorithm is provided for computation of optimum BSPAA. Comparisons between the proposed BSPAA and the conventional BSP through non-accelerated life test (CBSP) and conventional BSP through SSSPALT (CBSPA) are carried out.",Rathin Das|Biswabrata Pradhan,,https://arxiv.org/abs/2408.00734v1,https://arxiv.org/pdf/2408.00734v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2408.00734v1.pdf
2408.05231v3,2024-08-01T09:01:27Z,2025-04-29 13:37:38,Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law,"A new unsupervised predictive maintenance analysis method based on the renormalization group approach used to discover critical behavior in complex systems has been proposed. The algorithm analyzes univariate time series and detects critical points based on a newly proposed theorem that identifies critical points using a Log Periodic Power Law function fits. Application of a new algorithm for predictive maintenance analysis of industrial data collected from reciprocating compressor systems is presented. Based on the knowledge of the dynamics of the analyzed compressor system, the proposed algorithm predicts valve and piston rod seal failures well in advance.",Bogdan Łobodziński,,https://arxiv.org/abs/2408.05231v3,https://arxiv.org/pdf/2408.05231v3,https://doi.org/10.1177/24518492251331375,"14 pages, 4 figures, 1 table",,10.1177/24518492251331375,stat.AP,stat.AP|cs.AI|physics.data-an,https://arxiv.org/pdf/2408.05231v3.pdf
2407.18648v1,2024-07-26T10:29:16Z,2024-07-26 10:29:16,Fast and Reliable Probabilistic Reflectometry Inversion with Prior-Amortized Neural Posterior Estimation,"Reconstructing the structure of thin films and multilayers from measurements of scattered X-rays or neutrons is key to progress in physics, chemistry, and biology. However, finding all structures compatible with reflectometry data is computationally prohibitive for standard algorithms, which typically results in unreliable analysis with only a single potential solution identified. We address this lack of reliability with a probabilistic deep learning method that identifies all realistic structures in seconds, setting new standards in reflectometry. Our method, Prior-Amortized Neural Posterior Estimation (PANPE), combines simulation-based inference with novel adaptive priors that inform the inference network about known structural properties and controllable experimental conditions. PANPE networks support key scenarios such as high-throughput sample characterization, real-time monitoring of evolving structures, or the co-refinement of several experimental data sets, and can be adapted to provide fast, reliable, and flexible inference across many other inverse problems.",Vladimir Starostin|Maximilian Dax|Alexander Gerlach|Alexander Hinderhofer|Álvaro Tejero-Cantero|Frank Schreiber,,https://arxiv.org/abs/2407.18648v1,https://arxiv.org/pdf/2407.18648v1,,,,,physics.app-ph,physics.app-ph|cond-mat.soft|cs.LG|stat.ML,https://arxiv.org/pdf/2407.18648v1.pdf
2408.03953v1,2024-07-23T08:20:32Z,2024-07-23 08:20:32,How reliable are remote sensing maps calibrated over large areas? A matter of scale?,"Remote sensing data are increasingly available and frequently used to produce forest attributes maps. The sampling strategy of the calibration plots may directly affect predictions and map qualities. The aim of this manuscript is to evaluate models transferability at different spatial scales according to the sampling efforts and the calibration domain of these models. Forest inventory plots from locals and regionals networks were used to calibrate randomForest (RF) models for stand basal area predictions. Auxiliary data from ALS flights and a Sentinel-2 image were used. Model transferability was assessed by comparing models developed over a given area and applied elsewhere. Performances were measured in terms of precision (RMSE and bias), coefficient of determination (R2) and the proportion of extrapolated predictions. Regional networks were also thinned to evaluate the effect of sampling efforts on models' performances. Local models showed large bias and extrapolation issues when applied elsewhere. Local issues of regional models were also observed, raising transferability and extrapolation concerns. An increase in sampling efforts was shown to reduce extrapolation issues. The outcoming results of this study underline the importance of considering models' validity domain while producing forest attribute maps, since their transferability is of crucial importance from a forest management perspective.",Andrey Ramirez Luigui|Jean-Pierre Renaud|Cédric Vega,"UL, ONF|ONF|IGN",https://arxiv.org/abs/2408.03953v1,https://arxiv.org/pdf/2408.03953v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2408.03953v1.pdf
2407.12700v1,2024-07-17T16:22:32Z,2024-07-17 16:22:32,Bayesian Joint Modeling of Interrater and Intrarater Reliability with Multilevel Data,We formulate three generalized Bayesian models for analyzing interrater and intrarater reliability in the presence of multilevel data. Stan implementations of these models provide new estimates of interrater and intrarater reliability. We also derive formulas for calculating marginal correlations under each of the three models. Comparisons of the kappa estimates and marginal correlations across the different models are presented from two real-world datasets. Simulations demonstrate properties of the different measures of agreement under different model assumptions.,Nour Hawila|Arthur Berg,,https://arxiv.org/abs/2407.12700v1,https://arxiv.org/pdf/2407.12700v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2407.12700v1.pdf
2407.12557v1,2024-07-17T13:40:08Z,2024-07-17 13:40:08,Comparing Homogeneous And Inhomogeneous Time Markov Chains For Modelling Degradation In Sewer Pipe Networks,"Sewer pipe systems are essential for social and economic welfare. Managing these systems requires robust predictive models for degradation behaviour. This study focuses on probability-based approaches, particularly Markov chains, for their ability to associate random variables with degradation. Literature predominantly uses homogeneous and inhomogeneous Markov chains for this purpose. However, their effectiveness in sewer pipe degradation modelling is still debatable. Some studies support homogeneous Markov chains, while others challenge their utility. We examine this issue using a large-scale sewer network in the Netherlands, incorporating historical inspection data. We model degradation with homogeneous discrete and continuous time Markov chains, and inhomogeneous-time Markov chains using Gompertz, Weibull, Log-Logistic and Log-Normal density functions. Our analysis suggests that, despite their higher computational requirements, inhomogeneous-time Markov chains are more appropriate for modelling the nonlinear stochastic characteristics related to sewer pipe degradation, particularly the Gompertz distribution. However, they pose a risk of over-fitting, necessitating significant improvements in parameter inference processes to effectively address this issue.",Lisandro A. Jimenez-Roa|Tiedo Tinga|Tom Heskes|Marielle Stoelinga,,https://arxiv.org/abs/2407.12557v1,https://arxiv.org/pdf/2407.12557v1,,,Proceedings of the 34th European Safety and Reliability Conference (ESREL) 2024,,stat.ME,stat.ME,https://arxiv.org/pdf/2407.12557v1.pdf
2407.02464v1,2024-07-02T17:44:00Z,2024-07-02 17:44:00,Reliable Confidence Intervals for Information Retrieval Evaluation Using Generative A.I,"The traditional evaluation of information retrieval (IR) systems is generally very costly as it requires manual relevance annotation from human experts. Recent advancements in generative artificial intelligence -- specifically large language models (LLMs) -- can generate relevance annotations at an enormous scale with relatively small computational costs. Potentially, this could alleviate the costs traditionally associated with IR evaluation and make it applicable to numerous low-resource applications. However, generated relevance annotations are not immune to (systematic) errors, and as a result, directly using them for evaluation produces unreliable results.
  In this work, we propose two methods based on prediction-powered inference and conformal risk control that utilize computer-generated relevance annotations to place reliable confidence intervals (CIs) around IR evaluation metrics. Our proposed methods require a small number of reliable annotations from which the methods can statistically analyze the errors in the generated annotations. Using this information, we can place CIs around evaluation metrics with strong theoretical guarantees. Unlike existing approaches, our conformal risk control method is specifically designed for ranking metrics and can vary its CIs per query and document. Our experimental results show that our CIs accurately capture both the variance and bias in evaluation based on LLM annotations, better than the typical empirical bootstrapping estimates. We hope our contributions bring reliable evaluation to the many IR applications where this was traditionally infeasible.",Harrie Oosterhuis|Rolf Jagerman|Zhen Qin|Xuanhui Wang|Michael Bendersky,,https://arxiv.org/abs/2407.02464v1,https://arxiv.org/pdf/2407.02464v1,https://doi.org/10.1145/3637528.3671883,KDD '24,,10.1145/3637528.3671883,cs.IR,cs.IR|stat.ML,https://arxiv.org/pdf/2407.02464v1.pdf
2407.00716v2,2024-06-30T14:52:27Z,2024-09-03 11:17:06,On a General Theoretical Framework of Reliability,"Reliability is an essential measure of how closely observed scores represent latent scores (reflecting constructs), assuming some latent variable measurement model. We present a general theoretical framework of reliability, placing emphasis on measuring the association between latent and observed scores. This framework was inspired by McDonald's (2011) regression framework, which highlighted the coefficient of determination as a measure of reliability. We extend McDonald's (2011) framework beyond coefficients of determination and introduce four desiderata for reliability measures (estimability, normalization, symmetry, and invariance). We also present theoretical examples to illustrate distinct measures of reliability and report on a numerical study that demonstrates the behavior of different reliability measures. We conclude with a discussion on the use of reliability coefficients and outline future avenues of research.",Yang Liu|Jolynn Pek|Alberto Maydeu-Olivares,,https://arxiv.org/abs/2407.00716v2,https://arxiv.org/pdf/2407.00716v2,https://doi.org/10.1111/bmsp.12360,,,10.1111/bmsp.12360,stat.ME,stat.ME,https://arxiv.org/pdf/2407.00716v2.pdf
2406.14904v2,2024-06-21T06:51:13Z,2025-01-17 15:59:34,Enhancing reliability in prediction intervals using point forecasters: Heteroscedastic Quantile Regression and Width-Adaptive Conformal Inference,"Constructing prediction intervals for time series forecasting is challenging, particularly when practitioners rely solely on point forecasts. While previous research has focused on creating increasingly efficient intervals, we argue that standard measures alone are inadequate. Beyond efficiency, prediction intervals must adapt their width based on the difficulty of the prediction while preserving coverage regardless of complexity. To address these issues, we propose combining Heteroscedastic Quantile Regression (HQR) with Width-Adaptive Conformal Inference (WACI). This integrated procedure guarantees theoretical coverage and enables interval widths to vary with predictive uncertainty. We assess its performance using both a synthetic example and a real world Electricity Price Forecasting scenario. Our results show that this combined approach meets or surpasses typical benchmarks for validity and efficiency, while also fulfilling important yet often overlooked practical requirements.",Carlos Sebastián|Carlos E. González-Guillén|Jesús Juan,,https://arxiv.org/abs/2406.14904v2,https://arxiv.org/pdf/2406.14904v2,,,,,stat.ME,stat.ME|cs.LG|stat.ML,https://arxiv.org/pdf/2406.14904v2.pdf
2406.09548v2,2024-06-13T19:29:37Z,2024-08-12 08:02:06,Between Randomness and Arbitrariness: Some Lessons for Reliable Machine Learning at Scale,"To develop rigorous knowledge about ML models -- and the systems in which they are embedded -- we need reliable measurements. But reliable measurement is fundamentally challenging, and touches on issues of reproducibility, scalability, uncertainty quantification, epistemology, and more. This dissertation addresses criteria needed to take reliability seriously: both criteria for designing meaningful metrics, and for methodologies that ensure that we can dependably and efficiently measure these metrics at scale and in practice. In doing so, this dissertation articulates a research vision for a new field of scholarship at the intersection of machine learning, law, and policy. Within this frame, we cover topics that fit under three different themes: (1) quantifying and mitigating sources of arbitrariness in ML, (2) taming randomness in uncertainty estimation and optimization algorithms, in order to achieve scalability without sacrificing reliability, and (3) providing methods for evaluating generative-AI systems, with specific focuses on quantifying memorization in language models and training latent diffusion models on open-licensed data. By making contributions in these three themes, this dissertation serves as an empirical proof by example that research on reliable measurement for machine learning is intimately and inescapably bound up with research in law and policy. These different disciplines pose similar research questions about reliable measurement in machine learning. They are, in fact, two complementary sides of the same research vision, which, broadly construed, aims to construct machine-learning systems that cohere with broader societal values.",A. Feder Cooper,,https://arxiv.org/abs/2406.09548v2,https://arxiv.org/pdf/2406.09548v2,,Ph.D. Dissertation,,,cs.LG,cs.LG|cs.AI|cs.CY|stat.ML,https://arxiv.org/pdf/2406.09548v2.pdf
2406.08867v3,2024-06-13T07:11:51Z,2025-03-11 08:57:53,Robust Bayesian approach for reliability prognosis of nondestructive one-shot devices under cumulative risk model,"The present study aims to determine the lifetime prognosis of highly durable nondestructive one-shot devices (NOSD) units under a step-stress accelerated life testing (SSALT) experiment applying a cumulative risk model (CRM). In an SSALT experiment, CRM retains the continuity of hazard function by allowing the lag period before the effects of stress change emerge. When prior information about the model parameters is available, Bayesian inference is crucial. In a Bayesian analysis of such lifetime data, conventional likelihood-based Bayesian estimation frequently fails in the presence of outliers in the dataset. This work incorporates a robust Bayesian approach utilizing a robustified posterior based on the density power divergence measure. The order restriction on shape parameters has been incorporated as a prior assumption to reflect the decreasing expected lifetime with increasing stress levels. In testing of hypothesis, a Bayes factor is implemented based on the robustified posterior. In Bayesian estimation, we exploit Hamiltonian Monte Carlo, which has certain advantages over the conventional Metropolis-Hastings algorithms. Further, the influence functions are examined to evaluate the robust behaviour of the estimators and the Bayes factor. Finally, the analytical development is validated through a simulation study and a real data analysis.",Shanya Baghel|Shuvashree Mondal,,https://arxiv.org/abs/2406.08867v3,https://arxiv.org/pdf/2406.08867v3,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2406.08867v3.pdf
2406.02751v2,2024-06-04T20:11:03Z,2024-06-06 21:46:44,Bayesian Statistics: A Review and a Reminder for the Practicing Reliability Engineer,"This paper introduces and reviews some of the principles and methods used in Bayesian reliability. It specifically discusses methods used in the analysis of success/no-success data and then reminds the reader of a simple Monte Carlo algorithm that can be used to calculate the posterior distribution of a system's reliability. This algorithm is especially useful when a system's reliability is modeled through the reliability of its subcomponents, yet only system-level data is available.",Carsten H. Botts,,https://arxiv.org/abs/2406.02751v2,https://arxiv.org/pdf/2406.02751v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2406.02751v2.pdf
2406.00424v1,2024-06-01T12:41:50Z,2024-06-01 12:41:50,A Batch Sequential Halving Algorithm without Performance Degradation,"In this paper, we investigate the problem of pure exploration in the context of multi-armed bandits, with a specific focus on scenarios where arms are pulled in fixed-size batches. Batching has been shown to enhance computational efficiency, but it can potentially lead to a degradation compared to the original sequential algorithm's performance due to delayed feedback and reduced adaptability. We introduce a simple batch version of the Sequential Halving (SH) algorithm (Karnin et al., 2013) and provide theoretical evidence that batching does not degrade the performance of the original algorithm under practical conditions. Furthermore, we empirically validate our claim through experiments, demonstrating the robust nature of the SH algorithm in fixed-size batch settings.",Sotetsu Koyamada|Soichiro Nishimori|Shin Ishii,,https://arxiv.org/abs/2406.00424v1,https://arxiv.org/pdf/2406.00424v1,,Accepted to RLC 2024,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2406.00424v1.pdf
2405.15514v1,2024-05-24T12:57:40Z,2024-05-24 12:57:40,On the Convexity and Reliability of the Bethe Free Energy Approximation,"The Bethe free energy approximation provides an effective way for relaxing NP-hard problems of probabilistic inference. However, its accuracy depends on the model parameters and particularly degrades if a phase transition in the model occurs. In this work, we analyze when the Bethe approximation is reliable and how this can be verified. We argue and show by experiment that it is mostly accurate if it is convex on a submanifold of its domain, the 'Bethe box'. For verifying its convexity, we derive two sufficient conditions that are based on the definiteness properties of the Bethe Hessian matrix: the first uses the concept of diagonal dominance, and the second decomposes the Bethe Hessian matrix into a sum of sparse matrices and characterizes the definiteness properties of the individual matrices in that sum. These theoretical results provide a simple way to estimate the critical phase transition temperature of a model. As a practical contribution we propose $\texttt{BETHE-MIN}$, a projected quasi-Newton method to efficiently find a minimum of the Bethe free energy.",Harald Leisenberger|Christian Knoll|Franz Pernkopf,,https://arxiv.org/abs/2405.15514v1,https://arxiv.org/pdf/2405.15514v1,,This work has been submitted to the Journal of Machine Learning Research (JMLR) for possible publication,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/2405.15514v1.pdf
2405.03834v2,2024-05-06T20:35:38Z,2024-11-24 21:40:28,Covariance-free Bi-fidelity Control Variates Importance Sampling for Rare Event Reliability Analysis,"Multifidelity modeling has been steadily gaining attention as a tool to address the problem of exorbitant model evaluation costs that makes the estimation of failure probabilities a significant computational challenge for complex real-world problems, particularly when failure is a rare event. To implement multifidelity modeling, estimators that efficiently combine information from multiple models/sources are necessary. In past works, the variance reduction techniques of Control Variates (CV) and Importance Sampling (IS) have been leveraged for this task. In this paper, we present the CVIS framework; a creative take on a coupled CV and IS estimator for bifidelity reliability analysis. The framework addresses some of the practical challenges of the CV method by using an estimator for the control variate mean and side-stepping the need to estimate the covariance between the original estimator and the control variate through a clever choice for the tuning constant. The task of selecting an efficient IS distribution is also considered, with a view towards maximally leveraging the bifidelity structure and maintaining expressivity. Additionally, a diagnostic is provided that indicates both the efficiency of the algorithm as well as the relative predictive quality of the models utilized. Finally, the behavior and performance of the framework is explored through analytical and numerical examples.",Promit Chakroborty|Somayajulu L. N. Dhulipala|Michael D. Shields,"Dept. of Civil and Systems Engg, Johns Hopkins University|Idaho National Laboratory|Dept. of Civil and Systems Engg, Johns Hopkins University",https://arxiv.org/abs/2405.03834v2,https://arxiv.org/pdf/2405.03834v2,,"36 pages, 7 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2405.03834v2.pdf
2405.10329v3,2024-05-06T03:22:38Z,2024-07-03 00:53:21,Causal inference approach to appraise long-term effects of maintenance policy on functional performance of asphalt pavements,"Asphalt pavements as the most prevalent transportation infrastructure, are prone to serious traffic safety problems due to functional or structural damage caused by stresses or strains imposed through repeated traffic loads and continuous climatic cycles. The good quality or high serviceability of infrastructure networks is vital to the urbanization and industrial development of nations. In order to maintain good functional pavement performance and extend the service life of asphalt pavements, the long-term performance of pavements under maintenance policies needs to be evaluated and favorable options selected based on the condition of the pavement. A major challenge in evaluating maintenance policies is to produce valid treatments for the outcome assessment under the control of uncertainty of vehicle loads and the disturbance of freeze-thaw cycles in the climatic environment. In this study, a novel causal inference approach combining a classical causal structural model and a potential outcome model framework is proposed to appraise the long-term effects of four preventive maintenance treatments for longitudinal cracking over a 5-year period of upkeep. Three fundamental issues were brought to our attention: 1) detection of causal relationships prior to variables under environmental loading (identification of causal structure); 2) obtaining direct causal effects of treatment on outcomes excluding covariates (identification of causal effects); and 3) sensitivity analysis of causal relationships. The results show that the method can accurately evaluate the effect of preventive maintenance treatments and assess the maintenance time to cater well for the functional performance of different preventive maintenance approaches. This framework could help policymakers to develop appropriate maintenance strategies for pavements.",Lingyun You|Nanning Guo|Zhengwu Long|Fusong Wang|Chundi Si|Aboelkasim Diab,,https://arxiv.org/abs/2405.10329v3,https://arxiv.org/pdf/2405.10329v3,,"The arXiv version needs to be withdrawn since the model needs to be validated and updated with advanced machine learning technologies to enhance the accuracy of the model, and there are some crucial definition errors of symbols in the arXiv version",,,stat.AP,stat.AP|cs.AI,https://arxiv.org/pdf/2405.10329v3.pdf
2404.15060v2,2024-04-23T14:07:56Z,2024-11-02 10:32:15,Fast and reliable confidence intervals for a variance component,"We show that confidence intervals in a variance component model, with asymptotically correct uniform coverage probability, can be obtained by inverting certain test-statistics based on the score for the restricted likelihood. The results apply in settings where the variance is near or at the boundary of the parameter set. Simulations indicate the proposed test-statistics are approximately pivotal and lead to confidence intervals with near-nominal coverage even in small samples. We illustrate our methods' application in spatially-resolved transcriptomics where we compute approximately 15,000 confidence intervals, used for gene ranking, in less than 4 minutes. In the settings we consider, the proposed method is between two and 28,000 times faster than popular alternatives, depending on how many confidence intervals are computed.",Yiqiao Zhang|Karl Oskar Ekvall|Aaron J. Molstad,,https://arxiv.org/abs/2404.15060v2,https://arxiv.org/pdf/2404.15060v2,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2404.15060v2.pdf
2404.13321v1,2024-04-20T08:31:17Z,2024-04-20 08:31:17,Accelerated System-Reliability-based Disaster Resilience Analysis for Structural Systems,"Resilience has emerged as a crucial concept for evaluating structural performance under disasters because of its ability to extend beyond traditional risk assessments, accounting for a system's ability to minimize disruptions and maintain functionality during recovery. To facilitate the holistic understanding of resilience performance in structural systems, a system-reliability-based disaster resilience analysis framework was developed. The framework describes resilience using three criteria: reliability, redundancy, and recoverability, and the system's internal resilience is evaluated by inspecting the characteristics of reliability and redundancy for different possible progressive failure modes. However, the practical application of this framework has been limited to complex structures with numerous sub-components, as it becomes intractable to evaluate the performances for all possible initial disruption scenarios. To bridge the gap between the theory and practical use, especially for evaluating reliability and redundancy, this study centers on the idea that the computational burden can be substantially alleviated by focusing on initial disruption scenarios that are practically significant. To achieve this research goal, we propose three methods to efficiently eliminate insignificant scenarios: the sequential search method, the n-ball sampling method, and the surrogate model-based adaptive sampling algorithm. Three numerical examples, including buildings and a bridge, are introduced to prove the applicability and efficiency of the proposed approaches. The findings of this study are expected to offer practical solutions to the challenges of assessing resilience performance in complex structural systems.",Taeyong Kim|Sang-ri Yi,,https://arxiv.org/abs/2404.13321v1,https://arxiv.org/pdf/2404.13321v1,,"25 pages, 18 figures",,,stat.AP,stat.AP|eess.SY,https://arxiv.org/pdf/2404.13321v1.pdf
2404.12478v1,2024-04-18T19:21:28Z,2024-04-18 19:21:28,"A New Reliable & Parsimonious Learning Strategy Comprising Two Layers of Gaussian Processes, to Address Inhomogeneous Empirical Correlation Structures","We present a new strategy for learning the functional relation between a pair of variables, while addressing inhomogeneities in the correlation structure of the available data, by modelling the sought function as a sample function of a non-stationary Gaussian Process (GP), that nests within itself multiple other GPs, each of which we prove can be stationary, thereby establishing sufficiency of two GP layers. In fact, a non-stationary kernel is envisaged, with each hyperparameter set as dependent on the sample function drawn from the outer non-stationary GP, such that a new sample function is drawn at every pair of input values at which the kernel is computed. However, such a model cannot be implemented, and we substitute this by recalling that the average effect of drawing different sample functions from a given GP is equivalent to that of drawing a sample function from each of a set of GPs that are rendered different, as updated during the equilibrium stage of the undertaken inference (via MCMC). The kernel is fully non-parametric, and it suffices to learn one hyperparameter per layer of GP, for each dimension of the input variable. We illustrate this new learning strategy on a real dataset.",Gargi Roy|Dalia Chakrabarty,,https://arxiv.org/abs/2404.12478v1,https://arxiv.org/pdf/2404.12478v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2404.12478v1.pdf
2404.08613v1,2024-04-12T17:22:29Z,2024-04-12 17:22:29,Using Explainable AI and Transfer Learning to understand and predict the maintenance of Atlantic blocking with limited observational data,"Blocking events are an important cause of extreme weather, especially long-lasting blocking events that trap weather systems in place. The duration of blocking events is, however, underestimated in climate models. Explainable Artificial Intelligence are a class of data analysis methods that can help identify physical causes of prolonged blocking events and diagnose model deficiencies. We demonstrate this approach on an idealized quasigeostrophic model developed by Marshall and Molteni (1993). We train a convolutional neural network (CNN), and subsequently, build a sparse predictive model for the persistence of Atlantic blocking, conditioned on an initial high-pressure anomaly. Shapley Additive ExPlanation (SHAP) analysis reveals that high-pressure anomalies in the American Southeast and North Atlantic, separated by a trough over Atlantic Canada, contribute significantly to prediction of sustained blocking events in the Atlantic region. This agrees with previous work that identified precursors in the same regions via wave train analysis. When we apply the same CNN to blockings in the ERA5 atmospheric reanalysis, there is insufficient data to accurately predict persistent blocks. We partially overcome this limitation by pre-training the CNN on the plentiful data of the Marshall-Molteni model, and then using Transfer Learning to achieve better predictions than direct training. SHAP analysis before and after transfer learning allows a comparison between the predictive features in the reanalysis and the quasigeostrophic model, quantifying dynamical biases in the idealized model. This work demonstrates the potential for machine learning methods to extract meaningful precursors of extreme weather events and achieve better prediction using limited observational data.",Huan Zhang|Justin Finkel|Dorian S. Abbot|Edwin P. Gerber|Jonathan Weare,,https://arxiv.org/abs/2404.08613v1,https://arxiv.org/pdf/2404.08613v1,,"29 pages, 10 figures",,,physics.ao-ph,physics.ao-ph|stat.ML,https://arxiv.org/pdf/2404.08613v1.pdf
2404.04824v1,2024-04-07T06:23:18Z,2024-04-07 06:23:18,Mixup Domain Adaptations for Dynamic Remaining Useful Life Predictions,"Remaining Useful Life (RUL) predictions play vital role for asset planning and maintenance leading to many benefits to industries such as reduced downtime, low maintenance costs, etc. Although various efforts have been devoted to study this topic, most existing works are restricted for i.i.d conditions assuming the same condition of the training phase and the deployment phase. This paper proposes a solution to this problem where a mix-up domain adaptation (MDAN) is put forward. MDAN encompasses a three-staged mechanism where the mix-up strategy is not only performed to regularize the source and target domains but also applied to establish an intermediate mix-up domain where the source and target domains are aligned. The self-supervised learning strategy is implemented to prevent the supervision collapse problem. Rigorous evaluations have been performed where MDAN is compared to recently published works for dynamic RUL predictions. MDAN outperforms its counterparts with substantial margins in 12 out of 12 cases. In addition, MDAN is evaluated with the bearing machine dataset where it beats prior art with significant gaps in 8 of 12 cases. Source codes of MDAN are made publicly available in \url{https://github.com/furqon3009/MDAN}.",Muhammad Tanzil Furqon|Mahardhika Pratama|Lin Liu| Habibullah|Kutluyil Dogancay,,https://arxiv.org/abs/2404.04824v1,https://arxiv.org/pdf/2404.04824v1,,accepted for publication in Knowledge-based Systems,"Knowledge-based Systems, 2024",,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2404.04824v1.pdf
2403.12822v1,2024-03-19T15:24:17Z,2024-03-19 15:24:17,FORM-based global reliability sensitivity analysis of systems with multiple failure modes,"Global variance-based reliability sensitivity indices arise from a variance decomposition of the indicator function describing the failure event. The first-order indices reflect the main effect of each variable on the variance of the failure event and can be used for variable prioritization; the total-effect indices represent the total effect of each variable, including its interaction with other variables, and can be used for variable fixing. This contribution derives expressions for the variance-based reliability indices of systems with multiple failure modes that are based on the first-order reliability method (FORM). The derived expressions are a function of the FORM results and, hence, do not require additional expensive model evaluations. They do involve the evaluation of multinormal integrals, for which effective solutions are available. We demonstrate that the derived expressions enable an accurate estimation of variance-based reliability sensitivities for general system problems to which FORM is applicable.",Iason Papaioannou|Daniel Straub,,https://arxiv.org/abs/2403.12822v1,https://arxiv.org/pdf/2403.12822v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2403.12822v1.pdf
2403.11125v2,2024-03-17T07:17:07Z,2024-04-20 15:07:41,Machine learning-based system reliability analysis with Gaussian Process Regression,"Machine learning-based reliability analysis methods have shown great advancements for their computational efficiency and accuracy. Recently, many efficient learning strategies have been proposed to enhance the computational performance. However, few of them explores the theoretical optimal learning strategy. In this article, we propose several theorems that facilitates such exploration. Specifically, cases that considering and neglecting the correlations among the candidate design samples are well elaborated. Moreover, we prove that the well-known U learning function can be reformulated to the optimal learning function for the case neglecting the Kriging correlation. In addition, the theoretical optimal learning strategy for sequential multiple training samples enrichment is also mathematically explored through the Bayesian estimate with the corresponding lost functions. Simulation results show that the optimal learning strategy considering the Kriging correlation works better than that neglecting the Kriging correlation and other state-of-the art learning functions from the literatures in terms of the reduction of number of evaluations of performance function. However, the implementation needs to investigate very large computational resource.",Lisang Zhou|Ziqian Luo|Xueting Pan,,https://arxiv.org/abs/2403.11125v2,https://arxiv.org/pdf/2403.11125v2,,,,,stat.ML,stat.ML|cs.LG|math.PR,https://arxiv.org/pdf/2403.11125v2.pdf
2403.10300v2,2024-03-15T13:42:10Z,2024-05-15 23:14:24,The reliability of the gender Implicit Association Test (gIAT) for high-ability careers,"Males outnumber females in many high-ability careers in the fields of science, technology, engineering, and mathematics, STEM, and academic medicine, to name a few. These differences are often attributed to subconscious bias as measured by the gender Implicit Association Test, gIAT. We compute p-value plots for results from two meta-analyses, one examines the predictive power of gIAT, and the other examines the predictive power of vocational interests, i.e. personal interests, and behaviors, for explaining gender differences in high-ability careers. The results are clear, the gender Implicit Association Test provides little or no information on male versus female differences, whereas vocational interests are strongly predictive. Researchers of implicit bias should expand their modeling to include additional relevant covariates. In short, these meta-analyses provide no support for the gender Implicit Association Test influencing choice and gender differences of high-ability careers.",S. Stanley Young|Warren B. Kindzierski,,https://arxiv.org/abs/2403.10300v2,https://arxiv.org/pdf/2403.10300v2,,"24 pages, 8 figures, 2 tables, 71 references",,,stat.AP,stat.AP,https://arxiv.org/pdf/2403.10300v2.pdf
2403.10182v5,2024-03-15T10:38:48Z,2025-01-13 06:51:13,Fast and reliable uncertainty quantification with neural network ensembles for industrial image classification,"Image classification with neural networks (NNs) is widely used in industrial processes, situations where the model likely encounters unknown objects during deployment, i.e., out-of-distribution (OOD) data. Worryingly, NNs tend to make confident yet incorrect predictions when confronted with OOD data. To increase the models' reliability, they should quantify the uncertainty in their own predictions, communicating when the output should (not) be trusted. Deep ensembles, composed of multiple independent NNs, have been shown to perform strongly but are computationally expensive. Recent research has proposed more efficient NN ensembles, namely the snapshot, batch, and multi-input multi-output ensemble. This study investigates the predictive and uncertainty performance of efficient NN ensembles in the context of image classification for industrial processes. It is the first to provide a comprehensive comparison and it proposes a novel Diversity Quality metric to quantify the ensembles' performance on the in-distribution and OOD sets in one single metric. The results highlight the batch ensemble as a cost-effective and competitive alternative to the deep ensemble. It matches the deep ensemble in both uncertainty and accuracy while exhibiting considerable savings in training time, test time, and memory storage.",Arthur Thuy|Dries F. Benoit,,https://arxiv.org/abs/2403.10182v5,https://arxiv.org/pdf/2403.10182v5,https://doi.org/10.1007/s10479-024-06440-4,Accepted Manuscript version of an article published in Annals of Operations Research,Ann Oper Res (2024),10.1007/s10479-024-06440-4,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2403.10182v5.pdf
2402.18187v1,2024-02-28T09:22:52Z,2024-02-28 09:22:52,Reliability of Redundant M-Out-Of-N Architectures With Dependent Components: A Comprehensible Approach With Monte Carlo Simulation,"Redundant architectures can improve the reliability of complex systems. However, component dependencies can affect the architecture and negate the benefit of redundancy. In this paper, we develop three component dependency models and analyze the reliability of different M-out-of-N configurations using Monte Carlo simulation. The first model assumes a linear component dependency. The second and third models consider common cause failures, in the latter for all components and in the second for random groups of components. As expected, the results show that interdependency degrades the reliability of parallel 1ooN systems while improving it for serial NooN systems. Interestingly, 2oo3 systems produce intermediate results that show an improvement in reliability for certain indicators and a deterioration for some others, depending on the type of dependency models. The results show nonlinear properties of MooN systems with dependent components, which suggest careful handling in applications. An online simulation platform based on Monte Carlo Simulation enables product designers to use the models efficiently and achieve tailored results",Tim Maurice Julitz|Antoine Tordeux|Nadine Schlüter|Manuel Löwer,,https://arxiv.org/abs/2402.18187v1,https://arxiv.org/pdf/2402.18187v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2402.18187v1.pdf
2402.15703v1,2024-02-24T03:41:09Z,2024-02-24 03:41:09,Is Offline Decision Making Possible with Only Few Samples? Reliable Decisions in Data-Starved Bandits via Trust Region Enhancement,"What can an agent learn in a stochastic Multi-Armed Bandit (MAB) problem from a dataset that contains just a single sample for each arm? Surprisingly, in this work, we demonstrate that even in such a data-starved setting it may still be possible to find a policy competitive with the optimal one. This paves the way to reliable decision-making in settings where critical decisions must be made by relying only on a handful of samples.
  Our analysis reveals that \emph{stochastic policies can be substantially better} than deterministic ones for offline decision-making. Focusing on offline multi-armed bandits, we design an algorithm called Trust Region of Uncertainty for Stochastic policy enhancemenT (TRUST) which is quite different from the predominant value-based lower confidence bound approach. Its design is enabled by localization laws, critical radii, and relative pessimism. We prove that its sample complexity is comparable to that of LCB on minimax problems while being substantially lower on problems with very few samples.
  Finally, we consider an application to offline reinforcement learning in the special case where the logging policies are known.",Ruiqi Zhang|Yuexiang Zhai|Andrea Zanette,,https://arxiv.org/abs/2402.15703v1,https://arxiv.org/pdf/2402.15703v1,,22 pages,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2402.15703v1.pdf
2402.13852v3,2024-02-21T14:56:36Z,2024-06-07 11:16:12,Neural Control System for Continuous Glucose Monitoring and Maintenance,"Precise glucose level monitoring is critical for people with diabetes to avoid serious complications. While there are several methods for continuous glucose level monitoring, research on maintenance devices is limited. To mitigate the gap, we provide a novel neural control system for continuous glucose monitoring and management that uses differential predictive control. Our approach, led by a sophisticated neural policy and differentiable modeling, constantly adjusts insulin supply in real-time, thereby improving glucose level optimization in the body. This end-to-end method maximizes efficiency, providing personalized care and improved health outcomes, as confirmed by empirical evidence. Code and data are available at: \url{https://github.com/azminewasi/NeuralCGMM}.",Azmine Toushik Wasi,,https://arxiv.org/abs/2402.13852v3,https://arxiv.org/pdf/2402.13852v3,,"9 Pages, 4 figures, ICLR 2024 Tiny Papers Track https://openreview.net/forum?id=Te4P3Cn54g",The Second Tiny Papers Track at ICLR 2024,,cs.LG,cs.LG|cs.AI|cs.NE|eess.SY|stat.ML,https://arxiv.org/pdf/2402.13852v3.pdf
2402.10043v5,2024-02-15T16:05:35Z,2024-08-19 08:55:28,Negative impact of heavy-tailed uncertainty and error distributions on the reliability of calibration statistics for machine learning regression tasks,"Average calibration of the (variance-based) prediction uncertainties of machine learning regression tasks can be tested in two ways: one is to estimate the calibration error (CE) as the difference between the mean absolute error (MSE) and the mean variance (MV); the alternative is to compare the mean squared z-scores (ZMS) to 1. The problem is that both approaches might lead to different conclusions, as illustrated in this study for an ensemble of datasets from the recent machine learning uncertainty quantification (ML-UQ) literature. It is shown that the estimation of MV, MSE and their confidence intervals becomes unreliable for heavy-tailed uncertainty and error distributions, which seems to be a frequent feature of ML-UQ datasets. By contrast, the ZMS statistic is less sensitive and offers the most reliable approach in this context, still acknowledging that datasets with heavy-tailed z-scores distributions should be considered with great care. Unfortunately, the same problem is expected to affect also conditional calibrations statistics, such as the popular ENCE, and very likely post-hoc calibration methods based on similar statistics. Several solutions to circumvent the outlined problems are proposed.",Pascal Pernot,,https://arxiv.org/abs/2402.10043v5,https://arxiv.org/pdf/2402.10043v5,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2402.10043v5.pdf
2402.09754v1,2024-02-15T07:08:11Z,2024-02-15 07:08:11,Robust SVD Made Easy: A fast and reliable algorithm for large-scale data analysis,"The singular value decomposition (SVD) is a crucial tool in machine learning and statistical data analysis. However, it is highly susceptible to outliers in the data matrix. Existing robust SVD algorithms often sacrifice speed for robustness or fail in the presence of only a few outliers. This study introduces an efficient algorithm, called Spherically Normalized SVD, for robust SVD approximation that is highly insensitive to outliers, computationally scalable, and provides accurate approximations of singular vectors. The proposed algorithm achieves remarkable speed by utilizing only two applications of a standard reduced-rank SVD algorithm to appropriately scaled data, significantly outperforming competing algorithms in computation times. To assess the robustness of the approximated singular vectors and their subspaces against data contamination, we introduce new notions of breakdown points for matrix-valued input, including row-wise, column-wise, and block-wise breakdown points. Theoretical and empirical analyses demonstrate that our algorithm exhibits higher breakdown points compared to standard SVD and its modifications. We empirically validate the effectiveness of our approach in applications such as robust low-rank approximation and robust principal component analysis of high-dimensional microarray datasets. Overall, our study presents a highly efficient and robust solution for SVD approximation that overcomes the limitations of existing algorithms in the presence of outliers.",Sangil Han|Kyoowon Kim|Sungkyu Jung,,https://arxiv.org/abs/2402.09754v1,https://arxiv.org/pdf/2402.09754v1,,,,,stat.ML,stat.ML|cs.LG|math.ST,https://arxiv.org/pdf/2402.09754v1.pdf
2402.09020v1,2024-02-14T08:51:53Z,2024-02-14 08:51:53,Bayesian reliability acceptance sampling plan with optional warranty under hybrid censoring,"This work considers design of Bayesian reliability acceptance sampling plan (RASP) under hybrid censored life test for the products sold under optional warranty. The consumer and manufacturer agree on a common lifetime distribution of the product. However, they differ in the assessment of the prior distributions because of the adversarial nature of the consumer and manufacturer. The consumer takes decision based on his/her utility and prior belief without warranty offer by the manufacturer. If the decision is rejection, manufacturer provides warranty offer to the consumer. If the consumer rejects the lot with a warranty, the manufacturer conducts life test under hybrid censoring scheme (HCS) and provide lifetime information to the consumer. The consumer updates his/her belief based on lifetime information provided by the manufacturer. The consumer then takes decision of acceptance or rejection of lot based on updated belief. Task of the manufacturer is to determine the optimal life testing plan.",Rathin Das|Biswabrata Pradhan,,https://arxiv.org/abs/2402.09020v1,https://arxiv.org/pdf/2402.09020v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2402.09020v1.pdf
2402.03108v1,2024-02-05T15:36:03Z,2024-02-05 15:36:03,"Perceived Vulnerability to Disease Scale: Factorial structure, reliability, and validity in times of Portugal's COVID-19 pandemic lockdown","The present study examines the factor structure of a Portuguese version of the Perceived Vulnerability to Disease Scale (PVD), designed to assess individual differences in chronic concerns about transmission of infectious diseases. Method: Data from a Portuguese convenience sample (n=1203), collected during the first Covid-19 pandemic lockdown. Results: the scale revealed, through an exploratory factor analysis (EFA) and a confirmatory factor analysis (CFA), a slight superiority of a three-factor model over the existing two-factor models of the 15-item original PVD and of the 10-item PVD established with another Portuguese sample (Ferreira et al., 2022). Conclusions: This higher level of differentiation in terms of a perceived resistance to infectious diseases could be explained by the pandemic context which may have differentiated the responses regarding the perception of Resistance. On the other hand, this new factor increases the comprehensive and evaluative dimension and implications of the construct assessed by PVD.",Ana Paula Martins|María C. Vega-Hernández|Francisca Ribeiro Soares|Rosa Marina Afonso,,https://arxiv.org/abs/2402.03108v1,https://arxiv.org/pdf/2402.03108v1,,"25 pages, 3 figures, 4 tables",,,stat.AP,stat.AP,https://arxiv.org/pdf/2402.03108v1.pdf
2402.01098v1,2024-02-02T02:21:06Z,2024-02-02 02:21:06,Bayesian Deep Learning for Remaining Useful Life Estimation via Stein Variational Gradient Descent,"A crucial task in predictive maintenance is estimating the remaining useful life of physical systems. In the last decade, deep learning has improved considerably upon traditional model-based and statistical approaches in terms of predictive performance. However, in order to optimally plan maintenance operations, it is also important to quantify the uncertainty inherent to the predictions. This issue can be addressed by turning standard frequentist neural networks into Bayesian neural networks, which are naturally capable of providing confidence intervals around the estimates. Several methods exist for training those models. Researchers have focused mostly on parametric variational inference and sampling-based techniques, which notoriously suffer from limited approximation power and large computational burden, respectively. In this work, we use Stein variational gradient descent, a recently proposed algorithm for approximating intractable distributions that overcomes the drawbacks of the aforementioned techniques. In particular, we show through experimental studies on simulated run-to-failure turbofan engine degradation data that Bayesian deep learning models trained via Stein variational gradient descent consistently outperform with respect to convergence speed and predictive performance both the same models trained via parametric variational inference and their frequentist counterparts trained via backpropagation. Furthermore, we propose a method to enhance performance based on the uncertainty information provided by the Bayesian models. We release the source code at https://github.com/lucadellalib/bdl-rul-svgd.",Luca Della Libera|Jacopo Andreoli|Davide Dalle Pezze|Mirco Ravanelli|Gian Antonio Susto,,https://arxiv.org/abs/2402.01098v1,https://arxiv.org/pdf/2402.01098v1,,"26 pages, 3 figures",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2402.01098v1.pdf
2401.11328v1,2024-01-20T21:19:26Z,2024-01-20 21:19:26,A Hierarchical Decision-Based Maintenance for a Complex Modular System Driven by the { MoMA} Algorithm,"This paper presents a maintenance policy for a modular system formed by K independent modules (n-subsystems) subjected to environmental conditions (shocks). For the modeling of this complex system, the use of the Matrix-Analytical Method (MAM) is proposed under a layered approach according to its hierarchical structure. Thus, the operational state of the system (top layer) depends on the states of the modules (middle layer), which in turn depend on the states of their components (bottom layer). This allows a detailed description of the system operation to plan maintenance actions appropriately and optimally. We propose a hierarchical decision-based maintenance strategy with periodic inspections as follows: at the time of the inspection, the condition of the system is first evaluated. If intervention is necessary, the modules are then checked to make individual decisions based on their states, and so on. Replacement or repair will be carried out as appropriate. An optimization problem is formulated as a function of the length of the inspection period and the intervention cost incurred over the useful life of the system. Our method shows the advantages, providing compact and implementable expressions. The model is illustrated on a submarine Electrical Control Unit (ECU).",M. L. Gamiz|D. Montoro-Cazorla|M. C. Segovia-Garcia,,https://arxiv.org/abs/2401.11328v1,https://arxiv.org/pdf/2401.11328v1,,"43 pages, 6 figures",,,eess.SY,eess.SY|stat.ME,https://arxiv.org/pdf/2401.11328v1.pdf
2401.10796v1,2024-01-19T16:31:13Z,2024-01-19 16:31:13,Reliability analysis for data-driven noisy models using active learning,"Reliability analysis aims at estimating the failure probability of an engineering system. It often requires multiple runs of a limit-state function, which usually relies on computationally intensive simulations. Traditionally, these simulations have been considered deterministic, i.e., running them multiple times for a given set of input parameters always produces the same output. However, this assumption does not always hold, as many studies in the literature report non-deterministic computational simulations (also known as noisy models). In such cases, running the simulations multiple times with the same input will result in different outputs. Similarly, data-driven models that rely on real-world data may also be affected by noise. This characteristic poses a challenge when performing reliability analysis, as many classical methods, such as FORM and SORM, are tailored to deterministic models. To bridge this gap, this paper provides a novel methodology to perform reliability analysis on models contaminated by noise. In such cases, noise introduces latent uncertainty into the reliability estimator, leading to an incorrect estimation of the real underlying reliability index, even when using Monte Carlo simulation. To overcome this challenge, we propose the use of denoising regression-based surrogate models within an active learning reliability analysis framework. Specifically, we combine Gaussian process regression with a noise-aware learning function to efficiently estimate the probability of failure of the underlying noise-free model. We showcase the effectiveness of this methodology on standard benchmark functions and a finite element model of a realistic structural frame.",Anderson V. Pires|Maliki Moustapha|Stefano Marelli|Bruno Sudret,,https://arxiv.org/abs/2401.10796v1,https://arxiv.org/pdf/2401.10796v1,,,,,stat.CO,stat.CO|stat.AP|stat.ME,https://arxiv.org/pdf/2401.10796v1.pdf
2401.05244v1,2024-01-10T16:15:42Z,2024-01-10 16:15:42,Reliability Analysis of Complex Systems using Subset Simulations with Hamiltonian Neural Networks,"We present a new Subset Simulation approach using Hamiltonian neural network-based Monte Carlo sampling for reliability analysis. The proposed strategy combines the superior sampling of the Hamiltonian Monte Carlo method with computationally efficient gradient evaluations using Hamiltonian neural networks. This combination is especially advantageous because the neural network architecture conserves the Hamiltonian, which defines the acceptance criteria of the Hamiltonian Monte Carlo sampler. Hence, this strategy achieves high acceptance rates at low computational cost. Our approach estimates small failure probabilities using Subset Simulations. However, in low-probability sample regions, the gradient evaluation is particularly challenging. The remarkable accuracy of the proposed strategy is demonstrated on different reliability problems, and its efficiency is compared to the traditional Hamiltonian Monte Carlo method. We note that this approach can reach its limitations for gradient estimations in low-probability regions of complex and high-dimensional distributions. Thus, we propose techniques to improve gradient prediction in these particular situations and enable accurate estimations of the probability of failure. The highlight of this study is the reliability analysis of a system whose parameter distributions must be inferred with Bayesian inference problems. In such a case, the Hamiltonian Monte Carlo method requires a full model evaluation for each gradient evaluation and, therefore, comes at a very high cost. However, using Hamiltonian neural networks in this framework replaces the expensive model evaluation, resulting in tremendous improvements in computational efficiency.",Denny Thaler|Somayajulu L. N. Dhulipala|Franz Bamer|Bernd Markert|Michael D. Shields,,https://arxiv.org/abs/2401.05244v1,https://arxiv.org/pdf/2401.05244v1,,,,,stat.ML,stat.ML|cs.LG|stat.AP|stat.CO,https://arxiv.org/pdf/2401.05244v1.pdf
2401.02736v2,2024-01-05T10:14:39Z,2024-06-25 08:55:16,On the numerical reliability of nonsmooth autodiff: a MaxPool case study,"This paper considers the reliability of automatic differentiation (AD) for neural networks involving the nonsmooth MaxPool operation. We investigate the behavior of AD across different precision levels (16, 32, 64 bits) and convolutional architectures (LeNet, VGG, and ResNet) on various datasets (MNIST, CIFAR10, SVHN, and ImageNet). Although AD can be incorrect, recent research has shown that it coincides with the derivative almost everywhere, even in the presence of nonsmooth operations (such as MaxPool and ReLU). On the other hand, in practice, AD operates with floating-point numbers  (not real numbers), and there is, therefore, a need to explore subsets on which AD can be numerically incorrect. These subsets include a bifurcation zone (where AD is incorrect over reals) and a compensation zone (where AD is incorrect over floating-point numbers but correct over reals). Using SGD for the training process, we study the impact of different choices of the nonsmooth Jacobian for the MaxPool function on the precision of 16 and 32 bits. These findings suggest that nonsmooth MaxPool Jacobians with lower norms help maintain stable and efficient test accuracy, whereas those with higher norms can result in instability and decreased performance. We also observe that the influence of MaxPool's nonsmooth Jacobians on learning can be reduced by using batch normalization, Adam-like optimizers, or increasing the precision level.",Ryan Boustany,TSE-R,https://arxiv.org/abs/2401.02736v2,https://arxiv.org/pdf/2401.02736v2,,,"Transactions on Machine Learning Research Journal, 2024, 23 p",,cs.LG,cs.LG|math.NA|math.OC|stat.ML,https://arxiv.org/pdf/2401.02736v2.pdf
2401.01789v1,2024-01-03T15:42:45Z,2024-01-03 15:42:45,Deep learning the Hurst parameter of linear fractional processes and assessing its reliability,"This research explores the reliability of deep learning, specifically Long Short-Term Memory (LSTM) networks, for estimating the Hurst parameter in fractional stochastic processes. The study focuses on three types of processes: fractional Brownian motion (fBm), fractional Ornstein-Uhlenbeck (fOU) process, and linear fractional stable motions (lfsm). The work involves a fast generation of extensive datasets for fBm and fOU to train the LSTM network on a large volume of data in a feasible time. The study analyses the accuracy of the LSTM network's Hurst parameter estimation regarding various performance measures like RMSE, MAE, MRE, and quantiles of the absolute and relative errors. It finds that LSTM outperforms the traditional statistical methods in the case of fBm and fOU processes; however, it has limited accuracy on lfsm processes. The research also delves into the implications of training length and valuation sequence length on the LSTM's performance. The methodology is applied by estimating the Hurst parameter in Li-ion battery degradation data and obtaining confidence bounds for the estimation. The study concludes that while deep learning methods show promise in parameter estimation of fractional processes, their effectiveness is contingent on the process type and the quality of training data.",Dániel Boros|Bálint Csanády|Iván Ivkovic|Lóránt Nagy|András Lukács|László Márkus,,https://arxiv.org/abs/2401.01789v1,https://arxiv.org/pdf/2401.01789v1,,,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/2401.01789v1.pdf
2312.11355v1,2023-12-18T17:10:10Z,2023-12-18 17:10:10,Vesicoureteral Reflux Detection with Reliable Probabilistic Outputs,"Vesicoureteral Reflux (VUR) is a pediatric disorder in which urine flows backwards from the bladder to the upper urinary tract. Its detection is of great importance as it increases the risk of a Urinary Tract Infection, which can then lead to a kidney infection since bacteria may have direct access to the kidneys. Unfortunately the detection of VUR requires a rather painful medical examination, called voiding cysteourethrogram (VCUG), that exposes the child to radiation. In an effort to avoid the exposure to radiation required by VCUG some recent studies examined the use of machine learning techniques for the detection of VUR based on data that can be obtained without exposing the child to radiation. This work takes one step further by proposing an approach that provides lower and upper bounds for the conditional probability of a given child having VUR. The important property of these bounds is that they are guaranteed (up to statistical fluctuations) to contain well-calibrated probabilities with the only requirement that observations are independent and identically distributed (i.i.d.). Therefore they are much more informative and reliable than the plain yes/no answers provided by other techniques.",Harris Papadopoulos|George Anastassopoulos,,https://arxiv.org/abs/2312.11355v1,https://arxiv.org/pdf/2312.11355v1,https://doi.org/10.1016/j.ins.2014.11.046,,"Information Sciences, Volume 308, Pages 113-124, 2015",10.1016/j.ins.2014.11.046,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2312.11355v1.pdf
2312.10494v2,2023-12-16T16:16:28Z,2024-01-06 13:38:01,Do Bayesian Neural Networks Improve Weapon System Predictive Maintenance?,"We implement a Bayesian inference process for Neural Networks to model the time to failure of highly reliable weapon systems with interval-censored data and time-varying covariates. We analyze and benchmark our approach, LaplaceNN, on synthetic and real datasets with standard classification metrics such as Receiver Operating Characteristic (ROC) Area Under Curve (AUC) Precision-Recall (PR) AUC, and reliability curve visualizations.",Michael Potter|Miru Jun,,https://arxiv.org/abs/2312.10494v2,https://arxiv.org/pdf/2312.10494v2,,,,,cs.LG,cs.LG|stat.AP,https://arxiv.org/pdf/2312.10494v2.pdf
2312.09606v1,2023-12-15T08:39:02Z,2023-12-15 08:39:02,Reliable Prediction Intervals with Regression Neural Networks,"This paper proposes an extension to conventional regression Neural Networks (NNs) for replacing the point predictions they produce with prediction intervals that satisfy a required level of confidence. Our approach follows a novel machine learning framework, called Conformal Prediction (CP), for assigning reliable confidence measures to predictions without assuming anything more than that the data are independent and identically distributed (i.i.d.). We evaluate the proposed method on four benchmark datasets and on the problem of predicting Total Electron Content (TEC), which is an important parameter in trans-ionospheric links; for the latter we use a dataset of more than 60000 TEC measurements collected over a period of 11 years. Our experimental results show that the prediction intervals produced by our method are both well-calibrated and tight enough to be useful in practice.",Harris Papadopoulos|Haris Haralambous,,https://arxiv.org/abs/2312.09606v1,https://arxiv.org/pdf/2312.09606v1,https://doi.org/10.1016/j.neunet.2011.05.008,,"Neural Networks, Volume 24, Issue 8, Pages 842-851, 2011",10.1016/j.neunet.2011.05.008,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2312.09606v1.pdf
2312.04972v2,2023-12-08T11:04:59Z,2024-08-05 15:47:28,Comparison of Probabilistic Structural Reliability Methods for Ultimate Limit State Assessment of Wind Turbines,"The probabilistic design of offshore wind turbines aims to ensure structural safety in a cost-effective way. This involves conducting structural reliability assessments for different design options and considering different structural responses. There are several structural reliability methods, and this paper will apply and compare different approaches in some simplified case studies. In particular, the well known environmental contour method will be compared to a more novel approach based on sequential sampling and Gaussian processes regression for an ultimate limit state case study. For one of the case studies, results will also be compared to results from a brute force simulation approach. Interestingly, the comparison is very different from the two case studies. In one of the cases the environmental contours method agrees well with the sequential sampling method but in the other, results vary considerably. Probably, this can be explained by the violation of some of the assumptions associated with the environmental contour approach, i.e. that the short-term variability of the response is large compared to the long-term variability of the environmental conditions. Results from this simple comparison study suggests that the sequential sampling method can be a robust and computationally effective approach for structural reliability assessment.",Hong Wang|Odin Gramstad|Styfen Schär|Stefano Marelli|Erik Vanem,,https://arxiv.org/abs/2312.04972v2,https://arxiv.org/pdf/2312.04972v2,https://doi.org/10.1016/j.strusafe.2024.102502,,,10.1016/j.strusafe.2024.102502,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/2312.04972v2.pdf
2312.00186v1,2023-11-30T20:48:20Z,2023-11-30 20:48:20,Planning Reliability Assurance Tests for Autonomous Vehicles,"Artificial intelligence (AI) technology has become increasingly prevalent and transforms our everyday life. One important application of AI technology is the development of autonomous vehicles (AV). However, the reliability of an AV needs to be carefully demonstrated via an assurance test so that the product can be used with confidence in the field. To plan for an assurance test, one needs to determine how many AVs need to be tested for how many miles and the standard for passing the test. Existing research has made great efforts in developing reliability demonstration tests in the other fields of applications for product development and assessment. However, statistical methods have not been utilized in AV test planning. This paper aims to fill in this gap by developing statistical methods for planning AV reliability assurance tests based on recurrent events data. We explore the relationship between multiple criteria of interest in the context of planning AV reliability assurance tests. Specifically, we develop two test planning strategies based on homogeneous and non-homogeneous Poisson processes while balancing multiple objectives with the Pareto front approach. We also offer recommendations for practical use. The disengagement events data from the California Department of Motor Vehicles AV testing program is used to illustrate the proposed assurance test planning methods.",Simin Zheng|Lu Lu|Yili Hong|Jian Liu,,https://arxiv.org/abs/2312.00186v1,https://arxiv.org/pdf/2312.00186v1,,"29 pages, 5 figures",,,stat.AP,stat.AP|cs.AI,https://arxiv.org/pdf/2312.00186v1.pdf
2311.13821v1,2023-11-23T06:17:31Z,2023-11-23 06:17:31,HypUC: Hyperfine Uncertainty Calibration with Gradient-boosted Corrections for Reliable Regression on Imbalanced Electrocardiograms,"The automated analysis of medical time series, such as the electrocardiogram (ECG), electroencephalogram (EEG), pulse oximetry, etc, has the potential to serve as a valuable tool for diagnostic decisions, allowing for remote monitoring of patients and more efficient use of expensive and time-consuming medical procedures. Deep neural networks (DNNs) have been demonstrated to process such signals effectively. However, previous research has primarily focused on classifying medical time series rather than attempting to regress the continuous-valued physiological parameters central to diagnosis. One significant challenge in this regard is the imbalanced nature of the dataset, as a low prevalence of abnormal conditions can lead to heavily skewed data that results in inaccurate predictions and a lack of certainty in such predictions when deployed. To address these challenges, we propose HypUC, a framework for imbalanced probabilistic regression in medical time series, making several contributions. (i) We introduce a simple kernel density-based technique to tackle the imbalanced regression problem with medical time series. (ii) Moreover, we employ a probabilistic regression framework that allows uncertainty estimation for the predicted continuous values. (iii) We also present a new approach to calibrate the predicted uncertainty further. (iv) Finally, we demonstrate a technique to use calibrated uncertainty estimates to improve the predicted continuous value and show the efficacy of the calibrated uncertainty estimates to flag unreliable predictions. HypUC is evaluated on a large, diverse, real-world dataset of ECGs collected from millions of patients, outperforming several conventional baselines on various diagnostic tasks, suggesting a potential use-case for the reliable clinical deployment of deep learning models.",Uddeshya Upadhyay|Sairam Bade|Arjun Puranik|Shahir Asfahan|Melwin Babu|Francisco Lopez-Jimenez|Samuel J. Asirvatham|Ashim Prasad|Ajit Rajasekharan|Samir Awasthi|Rakesh Barve,,https://arxiv.org/abs/2311.13821v1,https://arxiv.org/pdf/2311.13821v1,,Published at TMLR,"Transactions on Machine Learning Research (TMLR), 2023",,cs.LG,cs.LG|cs.AI|cs.CE|stat.AP,https://arxiv.org/pdf/2311.13821v1.pdf
2312.00794v1,2023-11-17T03:44:15Z,2023-11-17 03:44:15,Informative Priors Improve the Reliability of Multimodal Clinical Data Classification,"Machine learning-aided clinical decision support has the potential to significantly improve patient care. However, existing efforts in this domain for principled quantification of uncertainty have largely been limited to applications of ad-hoc solutions that do not consistently improve reliability. In this work, we consider stochastic neural networks and design a tailor-made multimodal data-driven (M2D2) prior distribution over network parameters. We use simple and scalable Gaussian mean-field variational inference to train a Bayesian neural network using the M2D2 prior. We train and evaluate the proposed approach using clinical time-series data in MIMIC-IV and corresponding chest X-ray images in MIMIC-CXR for the classification of acute care conditions. Our empirical results show that the proposed method produces a more reliable predictive model compared to deterministic and Bayesian neural network baselines.",L. Julian Lechuga Lopez|Tim G. J. Rudner|Farah E. Shamout,,https://arxiv.org/abs/2312.00794v1,https://arxiv.org/pdf/2312.00794v1,,Published in ML4H 2023 Findings Track Collection,,,cs.CV,cs.CV|cs.AI|cs.CY|cs.LG|stat.AP,https://arxiv.org/pdf/2312.00794v1.pdf
2311.04812v1,2023-11-08T16:36:03Z,2023-11-08 16:36:03,Is it possible to obtain reliable estimates for the prevalence of anemia and childhood stunting among children under 5 in the poorest districts in Peru?,"In this article we describe and apply the Fay-Herriot model with spatially correlated random area effects (Pratesi, M., & Salvati, N. (2008)), in order to predict the prevalence of anemia and childhood stunting in Peruvian districts, based on the data from the Demographic and Family Health Survey of the year 2019, which collects data about anemia and childhood stunting for children under the age of 12 years, and the National Census carried out in 2017. Our main objective is to produce reliable predictions for the districts, where sample sizes are too small to provide good direct estimates, and for the districts, which were not included in the sample. The basic Fay-Herriot model (Fay & Herriot, 1979) tackles this problem by incorporating auxiliary information, which is generally available from administrative or census records. The Fay-Herriot model with spatially correlated random area effects, in addition to auxiliary information, incorporates geographic information about the areas, such as latitude and longitude. This permits modeling spatial autocorrelations, which are not unusual in socioeconomic and health surveys. To evaluate the mean square error of the above-mentioned predictors, we use the parametric bootstrap procedure, developed in Molina et al. (2009).",Anna Sikov|José Cerda-Hernández|Eduardo Haro,,https://arxiv.org/abs/2311.04812v1,https://arxiv.org/pdf/2311.04812v1,https://doi.org/10.21678/apuntes.95.1811,"32 pages, in Spanish language, 8 figures",Vol. 50 Núm. 95 (2023): Apuntes 95,10.21678/apuntes.95.1811,stat.ME,stat.ME|stat.AP|stat.CO,https://arxiv.org/pdf/2311.04812v1.pdf
2310.18567v5,2023-10-28T02:28:51Z,2025-06-10 18:19:18,Reliability modeling and statistical analysis of accelerated degradation process with memory effects and unit-to-unit variability,"A reasonable description of the degradation process is essential for credible reliability assessment in accelerated degradation testing. Existing methods usually use Markovian stochastic processes to describe the degradation process. However, degradation processes of some products are non-Markovian due to the interaction with environments. Misinterpretation of the degradation pattern may lead to biased reliability evaluations. Besides, owing to the differences in materials and manufacturing processes, products from the same population exhibit diverse degradation paths, further increasing the difficulty of accurate reliability estimation. To address the above issues, this paper proposes an accelerated degradation model incorporating memory effects and unit-to-unit variability. The memory effect in the degradation process is captured by the fractional Brownian motion, which reflects the non-Markovian characteristic of degradation. The unit-to-unit variability is considered in the acceleration model to describe diverse degradation paths. Then, lifetime and reliability under normal operating conditions are presented. Furthermore, to give an accurate estimation of the memory effect, a new statistical analysis method based on the expectation maximization algorithm is devised. The effectiveness of the proposed method is verified by a simulation case and a real-world tuner reliability analysis case. The code for the simulation case is publicly available at https://github.com/dirge1/FBM_ADT.",Shi-Shun Chen|Xiao-Yang Li|Wenrui Xie,,https://arxiv.org/abs/2310.18567v5,https://arxiv.org/pdf/2310.18567v5,https://doi.org/10.1016/j.apm.2024.115788,,"Applied Mathematical Modelling, 2024: 115788",10.1016/j.apm.2024.115788,stat.AP,stat.AP,https://arxiv.org/pdf/2310.18567v5.pdf
2310.10232v1,2023-10-16T09:42:27Z,2023-10-16 09:42:27,Efficient seismic reliability and fragility analysis of lifeline networks using subset simulation,"Various simulation-based and analytical methods have been developed to evaluate the seismic fragilities of individual structures. However, a community's seismic safety and resilience are substantially affected by network reliability, determined not only by component fragilities but also by network topology and commodity/information flows. However, seismic reliability analyses of networks often encounter significant challenges due to complex network topologies, interdependencies among ground motions, and low failure probabilities. This paper proposes to overcome these challenges by a variance-reduction method for network fragility analysis using subset simulation. The binary network limit-state function in the subset simulation is reformulated into more informative piecewise continuous functions. The proposed limit-state functions quantify the proximity of each sample to a potential network failure domain, thereby enabling the construction of specialized intermediate failure events, which can be utilized in subset simulation and other sequential Monte Carlo approaches. Moreover, by discovering an implicit connection between intermediate failure events and seismic intensity, we propose a technique to obtain the entire network fragility curve with a single execution of specialized subset simulation. Numerical examples demonstrate that the proposed method can effectively evaluate system-level fragility for large-scale networks.",Dongkyu Lee|Ziqi Wang|Junho Song,,https://arxiv.org/abs/2310.10232v1,https://arxiv.org/pdf/2310.10232v1,https://doi.org/10.1016/j.ress.2025.110947,,,10.1016/j.ress.2025.110947,stat.AP,stat.AP|stat.CO,https://arxiv.org/pdf/2310.10232v1.pdf
2309.10977v2,2023-09-20T00:37:35Z,2024-06-01 18:55:12,PAGER: A Framework for Failure Analysis of Deep Regression Models,"Safe deployment of AI models requires proactive detection of failures to prevent costly errors. To this end, we study the important problem of detecting failures in deep regression models. Existing approaches rely on epistemic uncertainty estimates or inconsistency w.r.t the training data to identify failure. Interestingly, we find that while uncertainties are necessary they are insufficient to accurately characterize failure in practice. Hence, we introduce PAGER (Principled Analysis of Generalization Errors in Regressors), a framework to systematically detect and characterize failures in deep regressors. Built upon the principle of anchored training in deep models, PAGER unifies both epistemic uncertainty and complementary manifold non-conformity scores to accurately organize samples into different risk regimes.",Jayaraman J. Thiagarajan|Vivek Narayanaswamy|Puja Trivedi|Rushil Anirudh,,https://arxiv.org/abs/2309.10977v2,https://arxiv.org/pdf/2309.10977v2,,Published at ICML 2024,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2309.10977v2.pdf
2309.07332v1,2023-09-13T22:04:50Z,2023-09-13 22:04:50,Reliability-based cleaning of noisy training labels with inductive conformal prediction in multi-modal biomedical data mining,"Accurately labeling biomedical data presents a challenge. Traditional semi-supervised learning methods often under-utilize available unlabeled data. To address this, we propose a novel reliability-based training data cleaning method employing inductive conformal prediction (ICP). This method capitalizes on a small set of accurately labeled training data and leverages ICP-calculated reliability metrics to rectify mislabeled data and outliers within vast quantities of noisy training data. The efficacy of the method is validated across three classification tasks within distinct modalities: filtering drug-induced-liver-injury (DILI) literature with title and abstract, predicting ICU admission of COVID-19 patients through CT radiomics and electronic health records, and subtyping breast cancer using RNA-sequencing data. Varying levels of noise to the training labels were introduced through label permutation. Results show significant enhancements in classification performance: accuracy enhancement in 86 out of 96 DILI experiments (up to 11.4%), AUROC and AUPRC enhancements in all 48 COVID-19 experiments (up to 23.8% and 69.8%), and accuracy and macro-average F1 score improvements in 47 out of 48 RNA-sequencing experiments (up to 74.6% and 89.0%). Our method offers the potential to substantially boost classification performance in multi-modal biomedical machine learning tasks. Importantly, it accomplishes this without necessitating an excessive volume of meticulously curated training data.",Xianghao Zhan|Qinmei Xu|Yuanning Zheng|Guangming Lu|Olivier Gevaert,,https://arxiv.org/abs/2309.07332v1,https://arxiv.org/pdf/2309.07332v1,,,,,cs.LG,cs.LG|cs.AI|cs.CV|q-bio.GN|q-bio.QM|stat.AP|stat.ML,https://arxiv.org/pdf/2309.07332v1.pdf
2309.08559v1,2023-09-10T13:11:37Z,2023-09-10 13:11:37,Towards reliable predictive analytics: a generalized calibration framework,"Calibration is a pivotal aspect in predictive modeling, as it ensures that the predictions closely correspond with what we observe empirically. The contemporary calibration framework, however, is predominantly focused on prediction models where the outcome is a binary variable. We extend the logistic calibration framework to the generalized calibration framework which includes all members of the exponential family of distributions. We propose two different methods to estimate the calibration curve in this setting, a generalized linear model and a non-parametric smoother. In addition, we define two measures that summarize the calibration performance. The generalized calibration slope which quantifies the amount of over- or underfitting and the generalized calibration slope or calibration-in-the-large that measures the agreement between the global empirical average and the average predicted value. We provide an illustrative example using a simulated data set and hereby show how we can utilize the generalized calibration framework to assess the calibration of different types of prediction models.",Bavo De Cock Campo,,https://arxiv.org/abs/2309.08559v1,https://arxiv.org/pdf/2309.08559v1,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2309.08559v1.pdf
2309.01778v3,2023-09-04T19:39:21Z,2024-06-05 09:19:37,CONFIDERAI: a novel CONFormal Interpretable-by-Design score function for Explainable and Reliable Artificial Intelligence,"Everyday life is increasingly influenced by artificial intelligence, and there is no question that machine learning algorithms must be designed to be reliable and trustworthy for everyone. Specifically, computer scientists consider an artificial intelligence system safe and trustworthy if it fulfills five pillars: explainability, robustness, transparency, fairness, and privacy. In addition to these five, we propose a sixth fundamental aspect: conformity, that is, the probabilistic assurance that the system will behave as the machine learner expects. In this paper, we present a methodology to link conformal prediction with explainable machine learning by defining a new score function for rule-based classifiers that leverages rules predictive ability, the geometrical position of points within rules boundaries and the overlaps among rules as well, thanks to the definition of a geometrical rule similarity term. Furthermore, we address the problem of defining regions in the feature space where conformal guarantees are satisfied, by exploiting the definition of conformal critical set and showing how this set can be used to achieve new rules with improved performance on the target class. The overall methodology is tested with promising results on several datasets of real-world interest, such as domain name server tunneling detection or cardiovascular disease prediction.",Sara Narteni|Alberto Carlevaro|Fabrizio Dabbene|Marco Muselli|Maurizio Mongelli,,https://arxiv.org/abs/2309.01778v3,https://arxiv.org/pdf/2309.01778v3,https://doi.org/10.1016/j.patcog.2025.112219,"26 pages, 3 figures, international journal","Pattern Recognition, Volume 171, Part B, 2026, 112219, ISSN 0031-3203",10.1016/j.patcog.2025.112219,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2309.01778v3.pdf
2308.14240v2,2023-08-28T00:46:16Z,2025-05-16 06:01:36,Bayesian Multivariate Track Geometry Degradation Modelling and its use in Condition-Based Inspection,"Effective maintenance of railway infrastructure is crucial for safe and comfortable transportation. Among the various degradation modes, track geometry deformation due to repeated loading significantly impacts operational safety. Detecting and maintaining acceptable track geometry involves the use of track recording vehicles (TRVs) that inspect and record geometric parameters. This study aims to develop a novel track geometry degradation model that considers multiple indicators and their correlations, accounting for both imperfect manual and mechanized tamping. A multivariate Wiener model is formulated to capture the characteristics of track geometry degradation. To address data limitations, a hierarchical Bayesian approach with Markov Chain Monte Carlo (MCMC) simulation is employed. This research contributes to the analysis of a multivariate predictive model, which considers the correlation between the degradation rates of multiple indicators, providing insights for rail operators and new track-monitoring systems. The model's performance is validated through a real-world case study on a commuter track in Queensland, Australia, using actual data and independent test datasets. Additionally, the study demonstrates the application of the proposed multivariate degradation model in developing a condition-based inspection policy for track geometry, potentially reducing the number of TRV runs while maintaining abnormal detection levels and failure rates.",Huy Truong-Ba|Sinda Rebello|Michael E. Cholette|Venkat Reddy|Pietro Borghesani,,https://arxiv.org/abs/2308.14240v2,https://arxiv.org/pdf/2308.14240v2,https://doi.org/10.1007/s40534-025-00394-4,,"Railway Engineering Science, 2025",10.1007/s40534-025-00394-4,stat.AP,stat.AP,https://arxiv.org/pdf/2308.14240v2.pdf
2308.13811v1,2023-08-26T08:28:44Z,2023-08-26 08:28:44,The Spearman-Brown Formula and Reliabilities of Random Test Forms,"It is shown that the psychometric test reliability, based on any true-score model with randomly sampled items and conditionally independent errors, converges to 1 as the test length goes to infinity, assuming some fairly general regularity conditions. The asymptotic rate of convergence is given by the Spearman-Brown formula, and for this it is not needed that the items are parallel, or latent unidimensional, or even finite dimensional. Simulations with the 2-parameter logistic item response theory model reveal that there can be a positive bias in the reliability of short multidimensional tests, meaning that applying the Spearman-Brown formula in these cases would lead to overprediction of the reliability that will result from lengthening the tests. For short unidimensional tests under the 2-parameter logistic model the reliabilities are almost unbiased, meaning that application of the Spearman-Brown formula in these cases leads to predictions that are approximately unbiased.",Jules L. Ellis,,https://arxiv.org/abs/2308.13811v1,https://arxiv.org/pdf/2308.13811v1,https://doi.org/10.1007/s11336-024-09956-7,"35 pages, 6 figures","Ellis, J. L., Sijtsma, K. (2024). Proof of Reliability Convergence to 1 at Rate of Spearman-Brown Formula for Random Test Forms and Irrespective of Item Pool Dimensionality. Psychometrika, 89(3), 774-795",10.1007/s11336-024-09956-7,stat.ME,stat.ME,https://arxiv.org/pdf/2308.13811v1.pdf
2308.07247v1,2023-08-14T16:32:24Z,2023-08-14 16:32:24,Can we Agree? On the Rashōmon Effect and the Reliability of Post-Hoc Explainable AI,"The Rashōmon effect poses challenges for deriving reliable knowledge from machine learning models. This study examined the influence of sample size on explanations from models in a Rashōmon set using SHAP. Experiments on 5 public datasets showed that explanations gradually converged as the sample size increased. Explanations from <128 samples exhibited high variability, limiting reliable knowledge extraction. However, agreement between models improved with more data, allowing for consensus. Bagging ensembles often had higher agreement. The results provide guidance on sufficient data to trust explanations. Variability at low samples suggests that conclusions may be unreliable without validation. Further work is needed with more model types, data domains, and explanation methods. Testing convergence in neural networks and with model-specific explanation methods would be impactful. The approaches explored here point towards principled techniques for eliciting knowledge from ambiguous models.",Clement Poiret|Antoine Grigis|Justin Thomas|Marion Noulhiane,,https://arxiv.org/abs/2308.07247v1,https://arxiv.org/pdf/2308.07247v1,,"13 pages, 6 figures and 6 tables",,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2308.07247v1.pdf
2308.04420v3,2023-08-08T17:36:43Z,2024-08-09 17:14:07,Contour Location for Reliability in Airfoil Simulation Experiments using Deep Gaussian Processes,"Bayesian deep Gaussian processes (DGPs) outperform ordinary GPs as surrogate models of complex computer experiments when response surface dynamics are non-stationary, which is especially prevalent in aerospace simulations. Yet DGP surrogates have not been deployed for the canonical downstream task in that setting: reliability analysis through contour location (CL). In that context, we are motivated by a simulation of an RAE-2822 transonic airfoil which demarcates efficient and inefficient flight conditions. Level sets separating passable versus failable operating conditions are best learned through strategic sequential designs. There are two limitations to modern CL methodology which hinder DGP integration in this setting. First, derivative-based optimization underlying acquisition functions is thwarted by sampling-based Bayesian (i.e., MCMC) inference, which is essential for DGP posterior integration. Second, canonical acquisition criteria, such as entropy, are famously myopic to the extent that optimization may even be undesirable. Here we tackle both of these limitations at once, proposing a hybrid criterion that explores along the Pareto front of entropy and (predictive) uncertainty, requiring evaluation only at strategically located ""triangulation"" candidates. We showcase DGP CL performance in several synthetic benchmark exercises and on the RAE-2822 airfoil.",Annie S. Booth|S. Ashwin Renganathan|Robert B. Gramacy,,https://arxiv.org/abs/2308.04420v3,https://arxiv.org/pdf/2308.04420v3,,"19 pages, 11 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2308.04420v3.pdf
2307.15247v1,2023-07-28T00:59:14Z,2023-07-28 00:59:14,Is this model reliable for everyone? Testing for strong calibration,"In a well-calibrated risk prediction model, the average predicted probability is close to the true event rate for any given subgroup. Such models are reliable across heterogeneous populations and satisfy strong notions of algorithmic fairness. However, the task of auditing a model for strong calibration is well-known to be difficult -- particularly for machine learning (ML) algorithms -- due to the sheer number of potential subgroups. As such, common practice is to only assess calibration with respect to a few predefined subgroups. Recent developments in goodness-of-fit testing offer potential solutions but are not designed for settings with weak signal or where the poorly calibrated subgroup is small, as they either overly subdivide the data or fail to divide the data at all. We introduce a new testing procedure based on the following insight: if we can reorder observations by their expected residuals, there should be a change in the association between the predicted and observed residuals along this sequence if a poorly calibrated subgroup exists. This lets us reframe the problem of calibration testing into one of changepoint detection, for which powerful methods already exist. We begin with introducing a sample-splitting procedure where a portion of the data is used to train a suite of candidate models for predicting the residual, and the remaining data are used to perform a score-based cumulative sum (CUSUM) test. To further improve power, we then extend this adaptive CUSUM test to incorporate cross-validation, while maintaining Type I error control under minimal assumptions. Compared to existing methods, the proposed procedure consistently achieved higher power in simulation studies and more than doubled the power when auditing a mortality risk prediction model.",Jean Feng|Alexej Gossmann|Romain Pirracchio|Nicholas Petrick|Gene Pennello|Berkman Sahiner,,https://arxiv.org/abs/2307.15247v1,https://arxiv.org/pdf/2307.15247v1,,,,,cs.LG,cs.LG|stat.ME|stat.ML,https://arxiv.org/pdf/2307.15247v1.pdf
2307.12237v1,2023-07-23T06:06:38Z,2023-07-23 06:06:38,Demonstration of a Response Time Based Remaining Useful Life (RUL) Prediction for Software Systems,"Prognostic and Health Management (PHM) has been widely applied to hardware systems in the electronics and non-electronics domains but has not been explored for software. While software does not decay over time, it can degrade over release cycles. Software health management is confined to diagnostic assessments that identify problems, whereas prognostic assessment potentially indicates when in the future a problem will become detrimental. Relevant research areas such as software defect prediction, software reliability prediction, predictive maintenance of software, software degradation, and software performance prediction, exist, but all of these represent diagnostic models built upon historical data, none of which can predict an RUL for software. This paper addresses the application of PHM concepts to software systems for fault predictions and RUL estimation. Specifically, this paper addresses how PHM can be used to make decisions for software systems such as version update and upgrade, module changes, system reengineering, rejuvenation, maintenance scheduling, budgeting, and total abandonment. This paper presents a method to prognostically and continuously predict the RUL of a software system based on usage parameters (e.g., the numbers and categories of releases) and performance parameters (e.g., response time). The model developed has been validated by comparing actual data, with the results that were generated by predictive models. Statistical validation (regression validation, and k-fold cross validation) has also been carried out. A case study, based on publicly available data for the Bugzilla application is presented. This case study demonstrates that PHM concepts can be applied to software systems and RUL can be calculated to make system management decisions.",Ray Islam|Peter Sandborn,Mohammad Rubyet Islam|,https://arxiv.org/abs/2307.12237v1,https://arxiv.org/pdf/2307.12237v1,,"This research methodology has opened up new and practical applications in the software domain. In the coming decades, we can expect a significant amount of attention and practical implementation in this area worldwide","Published in the Journal of Prognostics and Health Management in March, 2023",,cs.SE,cs.SE|cs.LG|stat.AP,https://arxiv.org/pdf/2307.12237v1.pdf
2307.12003v1,2023-07-22T07:16:11Z,2023-07-22 07:16:11,Reliability of the g factor over time in Italian INVALSI data (2010-2022): What can achievement-g tell us about the Flynn effect?,"Generational intelligence test score gains over large parts of the 20th century have been observed to be negatively associated with psychometric g. Recent reports about changes in the cross-temporal IQ trajectory suggest that ability differentiation may be responsible for both changes in g as well as increasingly (sub)domain specific and inconsistent trajectories. Schooling is considered to be a main candidate cause for the Flynn effect, which suggests that school achievement might be expected to show similar cross-temporal developments. In the present study, we investigated evidence for cross-temporal changes in achievement-based g in a formal large-scale student assessment in Italy (i.e., the INVALSI assessment; N = 1,900,000). Based on data of four school grades (i.e., grades 2, 5, 8, and 10) over 13 years (2010-2022), we observed little evidence for changes in achievement g in general. However, cross-temporal trajectories were differentiated according to school grade, indicating cross-temporal g decreases for lower grade students whilst changes for higher grade students were positive. These findings may be interpreted as tentative evidence for age-dependent achievement-g differentiation. The presently observed achievement g trajectory appears to be consistent with recently observed evidence for a potential stagnation or reversal of cognitive test score gains.",Jakob Pietschnig|Sandra Oberleiter|Enrico Toffalini|David Giofre,,https://arxiv.org/abs/2307.12003v1,https://arxiv.org/pdf/2307.12003v1,https://doi.org/10.1016/j.paid.2023.112345,,"Personality and Individual Differences, 2023, 214, 112345",10.1016/j.paid.2023.112345,stat.AP,stat.AP,https://arxiv.org/pdf/2307.12003v1.pdf
2306.12497v2,2023-06-21T18:12:58Z,2024-03-04 19:29:04,Density Uncertainty Layers for Reliable Uncertainty Estimation,"Assessing the predictive uncertainty of deep neural networks is crucial for safety-related applications of deep learning. Although Bayesian deep learning offers a principled framework for estimating model uncertainty, the common approaches that approximate the parameter posterior often fail to deliver reliable estimates of predictive uncertainty. In this paper, we propose a novel criterion for reliable predictive uncertainty: a model's predictive variance should be grounded in the empirical density of the input. That is, the model should produce higher uncertainty for inputs that are improbable in the training data and lower uncertainty for inputs that are more probable. To operationalize this criterion, we develop the density uncertainty layer, a stochastic neural network architecture that satisfies the density uncertain criterion by design. We study density uncertainty layers on the UCI and CIFAR-10/100 uncertainty benchmarks. Compared to existing approaches, density uncertainty layers provide more reliable uncertainty estimates and robust out-of-distribution detection performance.",Yookoon Park|David M. Blei,,https://arxiv.org/abs/2306.12497v2,https://arxiv.org/pdf/2306.12497v2,,Published in AISTATS 2024,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2306.12497v2.pdf
2306.11040v1,2023-06-19T16:05:53Z,2023-06-19 16:05:53,Application of Deep Learning for Predictive Maintenance of Oilfield Equipment,"This thesis explored applications of the new emerging techniques of artificial intelligence and deep learning (neural networks in particular) for predictive maintenance, diagnostics and prognostics. Many neural architectures such as fully-connected, convolutional and recurrent neural networks were developed and tested on public datasets such as NASA C-MAPSS, Case Western Reserve University Bearings and FEMTO Bearings datasets to diagnose equipment health state and/or predict the remaining useful life (RUL) before breakdown. Many data processing and feature extraction procedures were used in combination with deep learning techniques such as dimensionality reduction (Principal Component Analysis) and signal processing (Fourier and Wavelet analyses) in order to create more meaningful and robust features to use as an input for neural networks architectures. This thesis also explored the potential use of these techniques in predictive maintenance within oil rigs for monitoring oilfield critical equipment in order to reduce unpredicted downtime and maintenance costs.",Abdeldjalil Latrach,,https://arxiv.org/abs/2306.11040v1,https://arxiv.org/pdf/2306.11040v1,https://doi.org/10.13140/RG.2.2.12595.09762,,,10.13140/RG.2.2.12595.09762,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2306.11040v1.pdf
2306.10279v1,2023-06-17T07:10:55Z,2023-06-17 07:10:55,Variance-based reliability sensitivity with dependent inputs using failure samples,"Reliability sensitivity analysis is concerned with measuring the influence of a system's uncertain input parameters on its probability of failure. Statistically dependent inputs present a challenge in both computing and interpreting these sensitivity indices; such dependencies require discerning between variable interactions produced by the probabilistic model describing the system inputs and the computational model describing the system itself. To accomplish such a separation of effects in the context of reliability sensitivity analysis we extend on an idea originally proposed by Mara and Tarantola (2012) for model outputs unrelated to rare events. We compute the independent (influence via computational model) and full (influence via both computational and probabilistic model) contributions of all inputs to the variance of the indicator function of the rare event. We compute this full set of variance-based sensitivity indices of the rare event indicator using a single set of failure samples. This is possible by considering $d$ different hierarchically structured isoprobabilistic transformations of this set of failure samples from the original $d$-dimensional space of dependent inputs to standard-normal space. The approach facilitates computing the full set of variance-based reliability sensitivity indices with a single set of failure samples obtained as the byproduct of a single run of a sample-based rare event estimation method. That is, no additional evaluations of the computational model are required. We demonstrate the approach on a test function and two engineering problems.",Max Ehre|Iason Papaioannou|Daniel Straub,,https://arxiv.org/abs/2306.10279v1,https://arxiv.org/pdf/2306.10279v1,,,,,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/2306.10279v1.pdf
2306.05436v1,2023-06-08T01:45:06Z,2023-06-08 01:45:06,Remaining Useful Life Modelling with an Escalator Health Condition Analytic System,"The refurbishment of an escalator is usually linked with its design life as recommended by the manufacturer. However, the actual useful life of an escalator should be determined by its operating condition which is affected by the runtime, workload, maintenance quality, vibration, etc., rather than age only. The objective of this project is to develop a comprehensive health condition analytic system for escalators to support refurbishment decisions. The analytic system consists of four parts: 1) online data gathering and processing; 2) a dashboard for condition monitoring; 3) a health index model; and 4) remaining useful life prediction. The results can be used for a) predicting the remaining useful life of the escalators, in order to support asset replacement planning and b) monitoring the real-time condition of escalators; including alerts when vibration exceeds the threshold and signal diagnosis, giving an indication of possible root cause (components) of the alert signal.",Inez M. Zwetsloot|Yu Lin|Jiaqi Qiu|Lishuai Li|William Ka Fai Lee|Edmond Yin San Yeung|Colman Yiu Wah Yeung|Chris Chun Long Wong,,https://arxiv.org/abs/2306.05436v1,https://arxiv.org/pdf/2306.05436v1,,"14 pages, 12 figures, 7 tables",,,stat.AP,stat.AP|cs.CY,https://arxiv.org/pdf/2306.05436v1.pdf
2306.03759v2,2023-06-06T15:18:23Z,2023-10-11 14:15:45,A metric for assessing and optimizing data-driven prognostic algorithms for predictive maintenance,"Prognostic Health Management aims to predict the Remaining Useful Life (RUL) of degrading components/systems utilizing monitoring data. These RUL predictions form the basis for optimizing maintenance planning in a Predictive Maintenance (PdM) paradigm. We here propose a metric for assessing data-driven prognostic algorithms based on their impact on downstream PdM decisions. The metric is defined in association with a decision setting and a corresponding PdM policy. We consider two typical PdM decision settings, namely component ordering and/or replacement planning, for which we investigate and improve PdM policies that are commonly utilized in the literature. All policies are evaluated via the data-based estimation of the long-run expected maintenance cost per unit time, using monitored run-to-failure experiments. The policy evaluation enables the estimation of the proposed metric. We employ the metric as an objective function for optimizing heuristic PdM policies and algorithms' hyperparameters. The effect of different PdM policies on the metric is initially investigated through a theoretical numerical example. Subsequently, we employ four data-driven prognostic algorithms on a simulated turbofan engine degradation problem, and investigate the joint effect of prognostic algorithm and PdM policy on the metric, resulting in a decision-oriented performance assessment of these algorithms.",Antonios Kamariotis|Konstantinos Tatsis|Eleni Chatzi|Kai Goebel|Daniel Straub,,https://arxiv.org/abs/2306.03759v2,https://arxiv.org/pdf/2306.03759v2,https://doi.org/10.1016/j.ress.2023.109723,,"Reliability Engineering & System Safety, Volume 242, February 2024, 109723",10.1016/j.ress.2023.109723,stat.AP,stat.AP,https://arxiv.org/pdf/2306.03759v2.pdf
2305.19885v2,2023-05-31T14:23:28Z,2024-05-08 19:43:22,Reliability analysis of arbitrary systems based on active learning and global sensitivity analysis,"System reliability analysis aims at computing the probability of failure of an engineering system given a set of uncertain inputs and limit state functions. Active-learning solution schemes have been shown to be a viable tool but as of yet they are not as efficient as in the context of component reliability analysis. This is due to some peculiarities of system problems, such as the presence of multiple failure modes and their uneven contribution to failure, or the dependence on the system configuration (e.g., series or parallel). In this work, we propose a novel active learning strategy designed for solving general system reliability problems. This algorithm combines subset simulation and Kriging/PC-Kriging, and relies on an enrichment scheme tailored to specifically address the weaknesses of this class of methods. More specifically, it relies on three components: (i) a new learning function that does not require the specification of the system configuration, (ii) a density-based clustering technique that allows one to automatically detect the different failure modes, and (iii) sensitivity analysis to estimate the contribution of each limit state to system failure so as to select only the most relevant ones for enrichment. The proposed method is validated on two analytical examples and compared against results gathered in the literature. Finally, a complex engineering problem related to power transmission is solved, thereby showcasing the efficiency of the proposed method in a real-case scenario.",Maliki Moustapha|Pietro Parisi|Stefano Marelli|Bruno Sudret,,https://arxiv.org/abs/2305.19885v2,https://arxiv.org/pdf/2305.19885v2,https://doi.org/10.1016/j.ress.2024.110150,,"Reliability Engineering & System Safety, vol. 248 (110150), August 2024, p. 1-14",10.1016/j.ress.2024.110150,stat.ME,stat.ME|stat.AP|stat.CO,https://arxiv.org/pdf/2305.19885v2.pdf
2306.01769v1,2023-05-28T21:33:15Z,2023-05-28 21:33:15,Towards a Technology-Driven Adaptive Decision Support System for Integrated Pavement and Maintenance strategies (TDADSS-IPM): focus on risk assessment framework for climate change adaptation,"Decision Support Systems for pavement and maintenance strategies have traditionally been designed as silos led to local optimum systems. Moreover, since big data usage didn't exist as result of Industry 4.0 as of today, DSSs were not initially designed adaptive to the sources of uncertainties led to rigid decisions. Motivated by the vulnerability of the road assets to the climate phenomena, this paper takes a visionary step towards introducing a Technology-Driven Adaptive Decision Support System for Integrated Pavement and Maintenance activities called TDADSS-IPM. As part of such DSS, a bottom-up risk assessment model is met via Bayesian Belief Networks (BBN) to realize the actual condition of the Danish roads due to weather condition. Such model fills the gaps in the knowledge domain and develops a platform that can be trained over time, and applied in real-time to the actual event.",Shahrzad Pour|Amir Masoumi|Niels Skov Dujardin,,https://arxiv.org/abs/2306.01769v1,https://arxiv.org/pdf/2306.01769v1,,,,,cs.AI,cs.AI|stat.AP,https://arxiv.org/pdf/2306.01769v1.pdf
2305.16578v1,2023-05-26T02:00:27Z,2023-05-26 02:00:27,Computation of Reliability Statistics for Finite Samples of Success-Failure Experiments,"Computational method for statistical measures of reliability, confidence, and assurance are available for infinite population size. If the population size is finite and small compared to the number of samples tested, these computational methods need to be improved for a better representation of reality. This article discusses how to compute reliability, confidence, and assurance statistics for finite number of samples. Graphs and tables are provided as examples and can be used for low number of test sample sizes. Two open-source python libraries are provided for computing reliability, confidence, and assurance with both infinite and finite number of samples.",Sanjay M. Joshi,,https://arxiv.org/abs/2305.16578v1,https://arxiv.org/pdf/2305.16578v1,,"6 pages, 4 figures, 1 table",,,cs.IT,cs.IT|stat.ME,https://arxiv.org/pdf/2305.16578v1.pdf
2305.14979v5,2023-05-24T10:13:32Z,2023-11-09 13:07:22,Assessment of the Reliablity of a Model's Decision by Generalizing Attribution to the Wavelet Domain,"Neural networks have shown remarkable performance in computer vision, but their deployment in numerous scientific and technical fields is challenging due to their black-box nature. Scientists and practitioners need to evaluate the reliability of a decision, i.e., to know simultaneously if a model relies on the relevant features and whether these features are robust to image corruptions. Existing attribution methods aim to provide human-understandable explanations by highlighting important regions in the image domain, but fail to fully characterize a decision process's reliability. To bridge this gap, we introduce the Wavelet sCale Attribution Method (WCAM), a generalization of attribution from the pixel domain to the space-scale domain using wavelet transforms. Attribution in the wavelet domain reveals where and on what scales the model focuses, thus enabling us to assess whether a decision is reliable. Our code is accessible here: \url{https://github.com/gabrielkasmi/spectral-attribution}.",Gabriel Kasmi|Laurent Dubus|Yves-Marie Saint Drenan|Philippe Blanc,,https://arxiv.org/abs/2305.14979v5,https://arxiv.org/pdf/2305.14979v5,,"18 pages, 10 figures, 3 tables. Camera-ready version accepted at the XAI in action workshop at NeurIPS 2023",,,cs.CV,cs.CV|cs.AI|stat.ML,https://arxiv.org/pdf/2305.14979v5.pdf
2305.01709v5,2023-05-02T18:23:38Z,2023-09-27 13:33:52,ssROC: Semi-Supervised ROC Analysis for Reliable and Streamlined Evaluation of Phenotyping Algorithms,"$\textbf{Objective:}$ High-throughput phenotyping will accelerate the use of electronic health records (EHRs) for translational research. A critical roadblock is the extensive medical supervision required for phenotyping algorithm (PA) estimation and evaluation. To address this challenge, numerous weakly-supervised learning methods have been proposed. However, there is a paucity of methods for reliably evaluating the predictive performance of PAs when a very small proportion of the data is labeled. To fill this gap, we introduce a semi-supervised approach (ssROC) for estimation of the receiver operating characteristic (ROC) parameters of PAs (e.g., sensitivity, specificity).
  $\textbf{Materials and Methods:}$ ssROC uses a small labeled dataset to nonparametrically impute missing labels. The imputations are then used for ROC parameter estimation to yield more precise estimates of PA performance relative to classical supervised ROC analysis (supROC) using only labeled data. We evaluated ssROC through in-depth simulation studies and an extensive evaluation of six PAs from Mass General Brigham (MGB).
  $\textbf{Results:}$ ssROC produced ROC parameter estimates with minimal bias and significantly lower variance than supROC in the simulated and semi-synthetic data. For the five PAs from MGB, the estimates from ssROC are 30% to 60% less variable than supROC on average.
  $\textbf{Discussion:}$ ssROC enables precise evaluation of PA performance without demanding large volumes of labeled data. ssROC is also easily implementable in open-source $\texttt{R}$ software.
  $\textbf{Conclusion:}$ When used in conjunction with weakly-supervised PAs, ssROC facilitates the reliable and streamlined phenotyping necessary for EHR-based research.",Jianhui Gao|Clara-Lea Bonzel|Chuan Hong|Paul Varghese|Karim Zakir|Jessica Gronsbell,,https://arxiv.org/abs/2305.01709v5,https://arxiv.org/pdf/2305.01709v5,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2305.01709v5.pdf
2304.12609v1,2023-04-25T06:49:56Z,2023-04-25 06:49:56,A Bi-fidelity DeepONet Approach for Modeling Uncertain and Degrading Hysteretic Systems,"Nonlinear systems, such as with degrading hysteretic behavior, are often encountered in engineering applications. In addition, due to the ubiquitous presence of uncertainty and the modeling of such systems becomes increasingly difficult. On the other hand, datasets from pristine models developed without knowing the nature of the degrading effects can be easily obtained. In this paper, we use datasets from pristine models without considering the degrading effects of hysteretic systems as low-fidelity representations that capture many of the important characteristics of the true system's behavior to train a deep operator network (DeepONet). Three numerical examples are used to show that the proposed use of the DeepONets to model the discrepancies between the low-fidelity model and the true system's response leads to significant improvements in the prediction error in the presence of uncertainty in the model parameters for degrading hysteretic systems.",Subhayan De|Patrick T. Brewick,,https://arxiv.org/abs/2304.12609v1,https://arxiv.org/pdf/2304.12609v1,,"23 pages, 15 figures",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2304.12609v1.pdf
2304.09836v2,2023-04-19T17:38:42Z,2023-06-06 15:39:51,Regions of Reliability in the Evaluation of Multivariate Probabilistic Forecasts,"Multivariate probabilistic time series forecasts are commonly evaluated via proper scoring rules, i.e., functions that are minimal in expectation for the ground-truth distribution. However, this property is not sufficient to guarantee good discrimination in the non-asymptotic regime. In this paper, we provide the first systematic finite-sample study of proper scoring rules for time-series forecasting evaluation. Through a power analysis, we identify the ""region of reliability"" of a scoring rule, i.e., the set of practical conditions where it can be relied on to identify forecasting errors. We carry out our analysis on a comprehensive synthetic benchmark, specifically designed to test several key discrepancies between ground-truth and forecast distributions, and we gauge the generalizability of our findings to real-world tasks with an application to an electricity production problem. Our results reveal critical shortcomings in the evaluation of multivariate probabilistic forecasts as commonly performed in the literature.",Étienne Marcotte|Valentina Zantedeschi|Alexandre Drouin|Nicolas Chapados,,https://arxiv.org/abs/2304.09836v2,https://arxiv.org/pdf/2304.09836v2,,"47 pages, 37 figures, camera-ready version, Fortieth International Conference on Machine Learning (ICML 2023)",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2304.09836v2.pdf
2304.06252v1,2023-04-13T04:14:47Z,2023-04-13 04:14:47,Adaptive active subspace-based metamodeling for high-dimensional reliability analysis,"To address the challenges of reliability analysis in high-dimensional probability spaces, this paper proposes a new metamodeling method that couples active subspace, heteroscedastic Gaussian process, and active learning. The active subspace is leveraged to identify low-dimensional salient features of a high-dimensional computational model. A surrogate computational model is built in the low-dimensional feature space by a heteroscedastic Gaussian process. Active learning adaptively guides the surrogate model training toward the critical region that significantly contributes to the failure probability. A critical trait of the proposed method is that the three main ingredients-active subspace, heteroscedastic Gaussian process, and active learning-are coupled to adaptively optimize the feature space mapping in conjunction with the surrogate modeling. This coupling empowers the proposed method to accurately solve nontrivial high-dimensional reliability problems via low-dimensional surrogate modeling. Finally, numerical examples of a high-dimensional nonlinear function and structural engineering applications are investigated to verify the performance of the proposed method.",Jungho Kim|Ziqi Wang|Junho Song,,https://arxiv.org/abs/2304.06252v1,https://arxiv.org/pdf/2304.06252v1,https://doi.org/10.1016/j.strusafe.2023.102404,,,10.1016/j.strusafe.2023.102404,stat.AP,stat.AP,https://arxiv.org/pdf/2304.06252v1.pdf
2304.01006v2,2023-03-26T19:03:16Z,2023-06-04 14:42:48,Statistical reliability of meta_analysis research claims for gas stove cooking_childhood respiratory health associations,"Odds ratios or p_values from individual observational studies can be combined to examine a common cause_effect research question in meta_analysis. However, reliability of individual studies used in meta_analysis should not be taken for granted as claimed cause_effect associations may not reproduce. An evaluation was undertaken on meta_analysis of base papers examining gas stove cooking, including nitrogen dioxide, NO2, and childhood asthma and wheeze associations. Numbers of hypotheses tested in 14 of 27 base papers, 52 percent, used in meta_analysis of asthma and wheeze were counted. Test statistics used in the meta_analysis, 40 odds ratios with 95 percent confidence limits, were converted to p_values and presented in p_value plots. The median and interquartile range of possible numbers of hypotheses tested in the 14 base papers was 15,360, 6,336_49,152. None of the 14 base papers made mention of correcting for multiple testing, nor was any explanation offered if no multiple testing procedure was used. Given large numbers of hypotheses available, statistics drawn from base papers and used for meta-analysis are likely biased. Even so, p-value plots for gas stove_current asthma and gas stove_current wheeze associations show randomness consistent with unproven gas stove harms. The meta-analysis fails to provide reliable evidence for public health policy making on gas stove harms to children in North America. NO2 is not established as a biologically plausible explanation of a causal link with childhood asthma. Biases_multiple testing and p-hacking_cannot be ruled out as explanations for a gas stove_current asthma association claim. Selective reporting is another bias in published literature of gas stove_childhood respiratory health studies. Keywords gas stove, asthma, meta-analysis, p-value plot, multiple testing, p_hacking",Warren B. Kindzierski|S. Stanley Young|John D. Dunn,,https://arxiv.org/abs/2304.01006v2,https://arxiv.org/pdf/2304.01006v2,https://doi.org/10.5539/ijsp.v12n3p40,International Journal of Statistics and Probability (2023),,10.5539/ijsp.v12n3p40,stat.AP,stat.AP,https://arxiv.org/pdf/2304.01006v2.pdf
2303.13023v2,2023-03-23T04:13:15Z,2023-09-13 20:09:30,Relaxation-based importance sampling for structural reliability analysis,"This study presents an importance sampling formulation based on adaptively relaxing parameters from the indicator function and/or the probability density function. The formulation embodies the prevalent mathematical concept of relaxing a complex problem into a sequence of progressively easier sub-problems. Due to the flexibility in constructing relaxation parameters, relaxation-based importance sampling provides a unified framework for various existing variance reduction techniques, such as subset simulation, sequential importance sampling, and annealed importance sampling. More crucially, the framework lays the foundation for creating new importance sampling strategies, tailoring to specific applications. To demonstrate this potential, two importance sampling strategies are proposed. The first strategy couples annealed importance sampling with subset simulation, focusing on low-dimensional problems. The second strategy aims to solve high-dimensional problems by leveraging spherical sampling and scaling techniques. Both methods are desirable for fragility analysis in performance-based engineering, as they can produce the entire fragility surface in a single run of the sampling algorithm. Three numerical examples, including a 1000-dimensional stochastic dynamic problem, are studied to demonstrate the proposed methods.",Jianhua Xian|Ziqi Wang,,https://arxiv.org/abs/2303.13023v2,https://arxiv.org/pdf/2303.13023v2,https://doi.org/10.1016/j.strusafe.2023.102393,,,10.1016/j.strusafe.2023.102393,stat.AP,stat.AP|stat.CO,https://arxiv.org/pdf/2303.13023v2.pdf
2303.06174v2,2023-03-10T19:19:18Z,2023-08-30 12:08:29,Joint Optimization of Production and Maintenance in Offshore Wind Farms: Balancing the Short- and Long-Term Needs of Wind Energy Operation,"The rapid increase in scale and sophistication of offshore wind (OSW) farms poses a critical challenge related to the cost-effective operation and management of wind energy assets. A defining characteristic of this challenge is the economic trade-off between two concomitant processes: power production (the primary driver of short-term revenues), and asset degradation (the main determinant of long-term expenses). Traditionally, approaches to optimize production and maintenance in wind farms have been conducted in isolation. In this paper, we conjecture that a joint optimization of those two processes, achieved by rigorously modeling their short- and long-term dependencies, can unlock significant economic benefits for wind farm operators. In specific, we propose a decision-theoretic framework, rooted in stochastic optimization, which seeks a sensible balance of how wind loads are leveraged to harness short-term electricity generation revenues, versus alleviated to hedge against longer-term maintenance expenses. Extensive numerical experiments using real-world data confirm the superior performance of our approach, in terms of several operational performance metrics, relative to methods that tackle the two problems in isolation.",Petros Papadopoulos|Farnaz Fallahi|Murat Yildirim|Ahmed Aziz Ezzat,,https://arxiv.org/abs/2303.06174v2,https://arxiv.org/pdf/2303.06174v2,,,,,eess.SY,eess.SY|math.OC|stat.AP,https://arxiv.org/pdf/2303.06174v2.pdf
2303.05060v1,2023-03-09T06:20:58Z,2023-03-09 06:20:58,Stress-strength reliability estimation for type-1 pathway generated exponential distribution with applications to AIDS incubation time,"The Type-1 Pathway Generated Exponential distribution (PGE-1) is introduced. We have considered the estimate of the stress-strength parameter when the stress and strength components are statistically independent and follow PGE-1 distributions with distinct parameters. The point estimate of the stress-strength reliability is obtained using maximum likelihood method. The interval estimates are obtained using asymptotic confidence intervals and the parametric bootstrap method. To verify the performance of the methods developed, an extensive Monte Carlo simulation study has been conducted. The application of the developed results is illustrated on AIDS blood transfusion data.",Gladwin James V.|Thomas Xavier|Nicy Sebastian,,https://arxiv.org/abs/2303.05060v1,https://arxiv.org/pdf/2303.05060v1,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2303.05060v1.pdf
2303.04526v2,2023-03-08T11:51:26Z,2023-07-09 16:13:25,Student's t-Distribution: On Measuring the Inter-Rater Reliability When the Observations are Scarce,"In natural language processing (NLP) we always rely on human judgement as the golden quality evaluation method. However, there has been an ongoing debate on how to better evaluate inter-rater reliability (IRR) levels for certain evaluation tasks, such as translation quality evaluation (TQE), especially when the data samples (observations) are very scarce. In this work, we first introduce the study on how to estimate the confidence interval for the measurement value when only one data (evaluation) point is available. Then, this leads to our example with two human-generated observational scores, for which, we introduce ``Student's \textit{t}-Distribution'' method and explain how to use it to measure the IRR score using only these two data points, as well as the confidence intervals (CIs) of the quality evaluation. We give quantitative analysis on how the evaluation confidence can be greatly improved by introducing more observations, even if only one extra observation. We encourage researchers to report their IRR scores in all possible means, e.g. using Student's \textit{t}-Distribution method whenever possible; thus making the NLP evaluation more meaningful, transparent, and trustworthy. This \textit{t}-Distribution method can be also used outside of NLP fields to measure IRR level for trustworthy evaluation of experimental investigations, whenever the observational data is scarce.
  Keywords: Inter-Rater Reliability (IRR); Scarce Observations; Confidence Intervals (CIs); Natural Language Processing (NLP); Translation Quality Evaluation (TQE); Student's \textit{t}-Distribution",Serge Gladkoff|Lifeng Han|Goran Nenadic,,https://arxiv.org/abs/2303.04526v2,https://arxiv.org/pdf/2303.04526v2,,"Accepted to RANLP2023: Recent Advances in Natural Language Processing, Varna, Bulgaria. 30 Aug - 8 Sep \url{https://ranlp.org/ranlp2023/}",,,cs.CL,cs.CL|cs.IT|math.NA|stat.AP,https://arxiv.org/pdf/2303.04526v2.pdf
2303.03384v1,2023-03-06T18:59:19Z,2023-03-06 18:59:19,Restoration-Degradation Beyond Linear Diffusions: A Non-Asymptotic Analysis For DDIM-Type Samplers,"We develop a framework for non-asymptotic analysis of deterministic samplers used for diffusion generative modeling. Several recent works have analyzed stochastic samplers using tools like Girsanov's theorem and a chain rule variant of the interpolation argument. Unfortunately, these techniques give vacuous bounds when applied to deterministic samplers. We give a new operational interpretation for deterministic sampling by showing that one step along the probability flow ODE can be expressed as two steps: 1) a restoration step that runs gradient ascent on the conditional log-likelihood at some infinitesimally previous time, and 2) a degradation step that runs the forward process using noise pointing back towards the current iterate. This perspective allows us to extend denoising diffusion implicit models to general, non-linear forward processes. We then develop the first polynomial convergence bounds for these samplers under mild conditions on the data distribution.",Sitan Chen|Giannis Daras|Alexandros G. Dimakis,,https://arxiv.org/abs/2303.03384v1,https://arxiv.org/pdf/2303.03384v1,,29 pages,,,cs.LG,cs.LG|math.ST|stat.ML,https://arxiv.org/pdf/2303.03384v1.pdf
2303.03343v3,2023-03-06T18:26:10Z,2023-05-16 15:11:37,Mortality Rates of US Counties: Are they Reliable and Predictable?,"We examine US County-level observational data on Lung Cancer mortality rates in 2012 and overall Circulatory Respiratory mortality rates in 2016 as well as their ""Top Ten"" potential causes from Federal or State sources. We find that these two mortality rates for 2,812 US Counties have remarkably little in common. Thus, for predictive modeling, we use a single ""compromise"" measure of mortality that has several advantages. The vast majority of our new findings have simple implications that we illustrate graphically.",Robert L. Obenchain|S. Stanley Young,,https://arxiv.org/abs/2303.03343v3,https://arxiv.org/pdf/2303.03343v3,,"3 Tables, 18 Figures, 20 Pages, 29 References",,,stat.AP,stat.AP|cs.CY|stat.CO,https://arxiv.org/pdf/2303.03343v3.pdf
2303.03167v1,2023-03-03T01:20:58Z,2023-03-03 01:20:58,Computation of Reliability Statistics for Success-Failure Experiments,"Reliability is probability of success in a success-failure experiment. Confidence in reliability estimate improves with increasing number of samples. Assurance sets confidence level same as reliability to create one number for easier communication. Assuming binomial distribution for the samples, closed-form expression exists only for calculating confidence. The Wilson Score interval with continuity correction provides approximate closed-form expression for reliability. Brent's method was found to provide fast and accurate estimate for both reliability and assurance computations. Graphs and tables are provided for several number of samples. Two open-source python libraries are introduced for computing reliability, confidence, and assurance.",Sanjay M. Joshi,,https://arxiv.org/abs/2303.03167v1,https://arxiv.org/pdf/2303.03167v1,,"6 pages, 4 figure, 3 tables",,,stat.ME,stat.ME,https://arxiv.org/pdf/2303.03167v1.pdf
2303.01117v1,2023-03-02T10:00:37Z,2023-03-02 10:00:37,In all LikelihoodS: How to Reliably Select Pseudo-Labeled Data for Self-Training in Semi-Supervised Learning,"Self-training is a simple yet effective method within semi-supervised learning. The idea is to iteratively enhance training data by adding pseudo-labeled data. Its generalization performance heavily depends on the selection of these pseudo-labeled data (PLS). In this paper, we aim at rendering PLS more robust towards the involved modeling assumptions. To this end, we propose to select pseudo-labeled data that maximize a multi-objective utility function. The latter is constructed to account for different sources of uncertainty, three of which we discuss in more detail: model selection, accumulation of errors and covariate shift. In the absence of second-order information on such uncertainties, we furthermore consider the generic approach of the generalized Bayesian alpha-cut updating rule for credal sets. As a practical proof of concept, we spotlight the application of three of our robust extensions on simulated and real-world data. Results suggest that in particular robustness w.r.t. model choice can lead to substantial accuracy gains.",Julian Rodemann|Christoph Jansen|Georg Schollmeyer|Thomas Augustin,,https://arxiv.org/abs/2303.01117v1,https://arxiv.org/pdf/2303.01117v1,,"9 pages, 1 figure, under review",,,stat.ML,stat.ML|cs.LG|math.ST|stat.ME,https://arxiv.org/pdf/2303.01117v1.pdf
2302.12074v1,2023-02-23T15:01:06Z,2023-02-23 15:01:06,Active learning for structural reliability analysis with multiple limit state functions through variance-enhanced PC-Kriging surrogate models,"Existing active strategies for training surrogate models yield accurate structural reliability estimates by aiming at design space regions in the vicinity of a specified limit state function. In many practical engineering applications, various damage conditions, e.g. repair, failure, should be probabilistically characterized, thus demanding the estimation of multiple performance functions. In this work, we investigate the capability of active learning approaches for efficiently selecting training samples under a limited computational budget while still preserving the accuracy associated with multiple surrogated limit states. Specifically, PC-Kriging-based surrogate models are actively trained considering a variance correction derived from leave-one-out cross-validation error information, whereas the sequential learning scheme relies on U-function-derived metrics. The proposed active learning approaches are tested in a highly nonlinear structural reliability setting, whereas in a more practical application, failure and repair events are stochastically predicted in the aftermath of a ship collision against an offshore wind substructure. The results show that a balanced computational budget administration can be effectively achieved by successively targeting the specified multiple limit state functions within a unified active learning scheme.",J. Moran A.|P. G. Morato|P. Rigo,,https://arxiv.org/abs/2302.12074v1,https://arxiv.org/pdf/2302.12074v1,,,,,cs.LG,cs.LG|cs.AI|stat.AP|stat.CO,https://arxiv.org/pdf/2302.12074v1.pdf
2302.06031v1,2023-02-13T00:17:47Z,2023-02-13 00:17:47,Reliable Bayesian Inference in Misspecified Models,"We provide a general solution to a fundamental open problem in Bayesian inference, namely poor uncertainty quantification, from a frequency standpoint, of Bayesian methods in misspecified models. While existing solutions are based on explicit Gaussian approximations of the posterior, or computationally onerous post-processing procedures, we demonstrate that correct uncertainty quantification can be achieved by replacing the usual posterior with an intuitive approximate posterior. Critically, our solution is applicable to likelihood-based, and generalized, posteriors as well as cases where the likelihood is intractable and must be estimated. We formally demonstrate the reliable uncertainty quantification of our proposed approach, and show that valid uncertainty quantification is not an asymptotic result but occurs even in small samples. We illustrate this approach through a range of examples, including linear, and generalized, mixed effects models.",David T. Frazier|Robert Kohn|Christopher Drovandi|David Gunawan,,https://arxiv.org/abs/2302.06031v1,https://arxiv.org/pdf/2302.06031v1,,,,,stat.ME,stat.ME|math.ST|stat.CO,https://arxiv.org/pdf/2302.06031v1.pdf
2302.04582v1,2023-02-09T11:47:10Z,2023-02-09 11:47:10,Reliable event rates for disease mapping,"When analyzing spatially referenced event data, the criteria for declaring rates as ""reliable"" is still a matter of dispute. What these varying criteria have in common, however, is that they are rarely satisfied for crude estimates in small area analysis settings, prompting the use of spatial models to improve reliability. While reasonable, recent work has quantified the extent to which popular models from the spatial statistics literature can overwhelm the information contained in the data, leading to oversmoothing. Here, we begin by providing a definition for a ""reliable"" estimate for event rates that can be used for crude and model-based estimates and allows for discrete and continuous statements of reliability. We then construct a spatial Bayesian framework that allows users to infuse prior information into their models to improve reliability while also guarding against oversmoothing. We apply our approach to county-level birth data from Pennsylvania, highlighting the effect of oversmoothing in spatial models and how our approach can allow users to better focus their attention to areas where sufficient data exists to drive inferential decisions. We then conclude with a brief discussion of how this definition of reliability can be used in the design of small area studies.",Harrison Quick|Guangzi Song,,https://arxiv.org/abs/2302.04582v1,https://arxiv.org/pdf/2302.04582v1,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2302.04582v1.pdf
2301.06676v2,2023-01-17T03:17:07Z,2024-04-28 05:53:46,"Explainable, Interpretable & Trustworthy AI for Intelligent Digital Twin: Case Study on Remaining Useful Life","Artificial intelligence (AI) and Machine learning (ML) are increasingly used in energy and engineering systems, but these models must be fair, unbiased, and explainable. It is critical to have confidence in AI's trustworthiness. ML techniques have been useful in predicting important parameters and in improving model performance. However, for these AI techniques to be useful for making decisions, they need to be audited, accounted for, and easy to understand. Therefore, the use of explainable AI (XAI) and interpretable machine learning (IML) is crucial for the accurate prediction of prognostics, such as remaining useful life (RUL), in a digital twin system, to make it intelligent while ensuring that the AI model is transparent in its decision-making processes and that the predictions it generates can be understood and trusted by users. By using AI that is explainable, interpretable, and trustworthy, intelligent digital twin systems can make more accurate predictions of RUL, leading to better maintenance and repair planning, and ultimately, improved system performance. The objective of this paper is to explain the ideas of XAI and IML and to justify the important role of AI/ML in the digital twin framework and components, which requires XAI to understand the prediction better. This paper explains the importance of XAI and IML in both local and global aspects to ensure the use of trustworthy AI/ML applications for RUL prediction. We used the RUL prediction for the XAI and IML studies and leveraged the integrated Python toolbox for interpretable machine learning~(PiML).",Kazuma Kobayashi|Syed Bahauddin Alam,,https://arxiv.org/abs/2301.06676v2,https://arxiv.org/pdf/2301.06676v2,https://doi.org/10.1016/j.engappai.2023.107620,,Engineering Applications of Artificial Intelligence 129 (2024): 107620,10.1016/j.engappai.2023.107620,cs.LG,cs.LG|stat.AP|stat.CO,https://arxiv.org/pdf/2301.06676v2.pdf
2301.01850v5,2023-01-04T23:28:58Z,2023-04-14 17:11:53,Bayesian Weapon System Reliability Modeling with Cox-Weibull Neural Network,"We propose to integrate weapon system features (such as weapon system manufacturer, deployment time and location, storage time and location, etc.) into a parameterized Cox-Weibull [1] reliability model via a neural network, like DeepSurv [2], to improve predictive maintenance. In parallel, we develop an alternative Bayesian model by parameterizing the Weibull parameters with a neural network and employing dropout methods such as Monte-Carlo (MC)-dropout for comparative purposes. Due to data collection procedures in weapon system testing we employ a novel interval-censored log-likelihood which incorporates Monte-Carlo Markov Chain (MCMC) [3] sampling of the Weibull parameters during gradient descent optimization. We compare classification metrics such as receiver operator curve (ROC) area under the curve (AUC), precision-recall (PR) AUC, and F scores to show our model generally outperforms traditional powerful models such as XGBoost and the current standard conditional Weibull probability density estimation model.",Michael Potter|Benny Cheng,,https://arxiv.org/abs/2301.01850v5,https://arxiv.org/pdf/2301.01850v5,https://doi.org/10.1109/RAMS51473.2023.10088222,"Pre-print with minor revisions, published at The 69th Annual Reliability and Maintainability Symposium, January 23-26, 2023, FL, USA",,10.1109/RAMS51473.2023.10088222,stat.AP,stat.AP|cs.AI|cs.LG|math.PR,https://arxiv.org/pdf/2301.01850v5.pdf
2301.01477v1,2023-01-04T07:48:08Z,2023-01-04 07:48:08,Reliability Analysis of Load-sharing Systems using a Flexible Model with Piecewise Linear Functions,"Aiming for accurate estimation of system reliability of load-sharing systems, a flexible model for such systems is constructed by approximating the cumulative hazard functions of component lifetimes using piecewise linear functions. The advantages of the resulting model are that it is data-driven and it does not use prohibitive assumptions on the underlying component lifetimes. Due to its flexible nature, the model is capable of providing a good fit to data obtained from load-sharing systems in general, thus resulting in an accurate estimation of important reliability characteristics. Estimates of reliability at a mission time, quantile function, mean time to failure, and mean residual time for load-sharing systems are developed under the proposed model involving piecewise linear functions. Maximum likelihood estimation and construction of confidence intervals for the proposed model are discussed in detail. The performance of the proposed model is observed to be quite satisfactory through a detailed Monte Carlo simulation study. Analysis of a load-sharing data pertaining to the lives of a two-motor load-sharing system is provided as an illustrative example. In summary, this article presents a comprehensive discussion on a flexible model that can be used for load-sharing systems under minimal assumptions.",Shilpi Biswas|Ayon Ganguly|Debanjan Mitra,,https://arxiv.org/abs/2301.01477v1,https://arxiv.org/pdf/2301.01477v1,,"28 pages, 3 figures",,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2301.01477v1.pdf
2301.01239v1,2023-01-03T17:33:40Z,2023-01-03 17:33:40,Use of survival analysis and simulation to improve maintenance planning of high voltage instrument transformers in the Dutch transmission system,"This paper describes the use of survival analysis and simulation to model the lifetime of high voltage instrument transformers in the Dutch transmission sys-tem. To represent asset aging, the non-parametric Kaplan-Meier method is used to enable the fitting of Weibull distribution. Such an approach is implemented on three different voltage levels, namely 110kV, 150kV, and 220/380kV. Real failure and inspection data is used to achieve a realistic failure model of the instrument trans-formers. Failure and maintenance data occurring between 1989 and 2021 have been used for this study. In spite of missing and low-quality data, a rich failure database could still be prepared. This study also offers insights into factors (i.e., voltage level, in-service age) influencing the remaining life from both graphical survival function and parametric Weibull distribution analysis. Based on the derived statistics, future possible maintenance planning scenarios are simulated under a complex system modelling framework in a digital twin enabled platform. Eventually, the scenarios are evaluated in terms of replacement costs (CAPEX), inspection hours, and unavailability hours.",Swasti R. Khuntia|Fatma Zghal|Ranjan Bhuyan|Erik Schenkel|Paul Duvivier|Olivier Blancke|Witold Krasny,,https://arxiv.org/abs/2301.01239v1,https://arxiv.org/pdf/2301.01239v1,,"12 pages, 5 figures, 16th WCEAM Proceedings 2022",16th WCEAM Proceedings 2022,,stat.AP,stat.AP,https://arxiv.org/pdf/2301.01239v1.pdf
2212.07940v1,2022-12-14T11:41:03Z,2022-12-14 11:41:03,"A Stress-Strength Reliability Model using Exponential-Gamma(3,$λ$) Distribution","One of the important problem in reliability analysis is computation of stress-strength reliability. But it is impractical to compute it in certain situations. So the estimation stay as an alternative solution to get an approximate value of the reliability. There are research papers which deals with stress-strength reliability analysis using statistical distributions. In this paper, a stress-strength reliability model for exponential-gamma$(3,λ)$ distribution is introduced. The maximum likelihood estimator (MLE) for the model parameters is derived. Asymptotic distribution and confidence interval for the maximum likelihood estimates of stress-strength reliability, $R=P(X>Y)$, are given. The numerical illustration is performed using Monte Carlo simulations. The results are analyzed with real data analysis.",Beenu Thomas|V. M. Chacko,,https://arxiv.org/abs/2212.07940v1,https://arxiv.org/pdf/2212.07940v1,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2212.07940v1.pdf
2212.06303v1,2022-12-13T00:57:09Z,2022-12-13 00:57:09,MAntRA: A framework for model agnostic reliability analysis,"We propose a novel model agnostic data-driven reliability analysis framework for time-dependent reliability analysis. The proposed approach -- referred to as MAntRA -- combines interpretable machine learning, Bayesian statistics, and identifying stochastic dynamic equation to evaluate reliability of stochastically-excited dynamical systems for which the governing physics is \textit{apriori} unknown. A two-stage approach is adopted: in the first stage, an efficient variational Bayesian equation discovery algorithm is developed to determine the governing physics of an underlying stochastic differential equation (SDE) from measured output data. The developed algorithm is efficient and accounts for epistemic uncertainty due to limited and noisy data, and aleatoric uncertainty because of environmental effect and external excitation. In the second stage, the discovered SDE is solved using a stochastic integration scheme and the probability failure is computed. The efficacy of the proposed approach is illustrated on three numerical examples. The results obtained indicate the possible application of the proposed approach for reliability analysis of in-situ and heritage structures from on-site measurements.",Yogesh Chandrakant Mathpati|Kalpesh Sanjay More|Tapas Tripura|Rajdip Nayek|Souvik Chakraborty,,https://arxiv.org/abs/2212.06303v1,https://arxiv.org/pdf/2212.06303v1,,,,,stat.ME,stat.ME|cs.LG|stat.ML,https://arxiv.org/pdf/2212.06303v1.pdf
2212.05772v1,2022-12-12T08:50:27Z,2022-12-12 08:50:27,Multi-Dimensional Self Attention based Approach for Remaining Useful Life Estimation,"Remaining Useful Life (RUL) estimation plays a critical role in Prognostics and Health Management (PHM). Traditional machine health maintenance systems are often costly, requiring sufficient prior expertise, and are difficult to fit into highly complex and changing industrial scenarios. With the widespread deployment of sensors on industrial equipment, building the Industrial Internet of Things (IIoT) to interconnect these devices has become an inexorable trend in the development of the digital factory. Using the device's real-time operational data collected by IIoT to get the estimated RUL through the RUL prediction algorithm, the PHM system can develop proactive maintenance measures for the device, thus, reducing maintenance costs and decreasing failure times during operation. This paper carries out research into the remaining useful life prediction model for multi-sensor devices in the IIoT scenario. We investigated the mainstream RUL prediction models and summarized the basic steps of RUL prediction modeling in this scenario. On this basis, a data-driven approach for RUL estimation is proposed in this paper. It employs a Multi-Head Attention Mechanism to fuse the multi-dimensional time-series data output from multiple sensors, in which the attention on features is used to capture the interactions between features and attention on sequences is used to learn the weights of time steps. Then, the Long Short-Term Memory Network is applied to learn the features of time series. We evaluate the proposed model on two benchmark datasets (C-MAPSS and PHM08), and the results demonstrate that it outperforms the state-of-art models. Moreover, through the interpretability of the multi-head attention mechanism, the proposed model can provide a preliminary explanation of engine degradation. Therefore, this approach is promising for predictive maintenance in IIoT scenarios.",Zhi Lai|Mengjuan Liu|Yunzhu Pan|Dajiang Chen,,https://arxiv.org/abs/2212.05772v1,https://arxiv.org/pdf/2212.05772v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2212.05772v1.pdf
2212.05515v2,2022-12-11T14:38:32Z,2024-11-01 21:04:27,Reliability Study of Battery Lives: A Functional Degradation Analysis Approach,"Renewable energy is critical for combating climate change, whose first step is the storage of electricity generated from renewable energy sources. Li-ion batteries are a popular kind of storage units. Their continuous usage through charge-discharge cycles eventually leads to degradation. This can be visualized in plotting voltage discharge curves (VDCs) over discharge cycles. Studies of battery degradation have mostly concentrated on modeling degradation through one scalar measurement summarizing each VDC. Such simplification of curves can lead to inaccurate predictive models. Here we analyze the degradation of rechargeable Li-ion batteries from a NASA data set through modeling and predicting their full VDCs. With techniques from longitudinal and functional data analysis, we propose a new two-step predictive modeling procedure for functional responses residing on heterogeneous domains. We first predict the shapes and domain end points of VDCs using functional regression models. Then we integrate these predictions to perform a degradation analysis. Our approach is fully functional, allows the incorporation of usage information, produces predictions in a curve form, and thus provides flexibility in the assessment of battery degradation. Through extensive simulation studies and cross-validated data analysis, our approach demonstrates better prediction than the existing approach of modeling degradation directly with aggregated data.",Youngjin Cho|Quyen Do|Pang Du|Yili Hong,,https://arxiv.org/abs/2212.05515v2,https://arxiv.org/pdf/2212.05515v2,https://doi.org/10.1214/24-AOAS1931,"27 pages,19 figures",,10.1214/24-AOAS1931,stat.ME,stat.ME,https://arxiv.org/pdf/2212.05515v2.pdf
2212.02895v4,2022-12-06T11:38:22Z,2025-02-14 17:35:40,Training Neural Networks on Data Sources with Unknown Reliability,"When data is generated by multiple sources, conventional training methods update models assuming equal reliability for each source and do not consider their individual data quality. However, in many applications, sources have varied levels of reliability that can have negative effects on the performance of a neural network. A key issue is that often the quality of the data for individual sources is not known during training. Previous methods for training models in the presence of noisy data do not make use of the additional information that the source label can provide. Focusing on supervised learning, we aim to train neural networks on each data source for a number of steps proportional to the source's estimated reliability by using a dynamic re-weighting strategy motivated by likelihood tempering. This way, we allow training on all sources during the warm-up and reduce learning on less reliable sources during the final training stages, when it has been shown that models overfit to noise. We show through diverse experiments that this can significantly improve model performance when trained on mixtures of reliable and unreliable data sources, and maintain performance when models are trained on reliable sources only.",Alexander Capstick|Francesca Palermo|Tianyu Cui|Payam Barnaghi,,https://arxiv.org/abs/2212.02895v4,https://arxiv.org/pdf/2212.02895v4,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2212.02895v4.pdf
2212.02688v1,2022-12-06T01:19:21Z,2022-12-06 01:19:21,Online Bayesian prediction of remaining useful life for gamma degradation process under conjugate priors,"Gamma process has been extensively used to model monotone degradation data. Statistical inference for the gamma process is difficult due to the complex parameter structure involved in the likelihood function. In this paper, we derive a conjugate prior for the homogeneous gamma process, and some properties of the prior distribution are explored. Three algorithms (Gibbs sampling, discrete grid sampling, and sampling importance resampling) are well designed to generate posterior samples of the model parameters, which can greatly lessen the challenge of posterior inference. Simulation studies show that the proposed algorithms have high computational efficiency and estimation precision. The conjugate prior is then extended to the case of the gamma process with heterogeneous effects. With this conjugate structure, the posterior distribution of the parameters can be updated recursively, and an efficient online algorithm is developed to predict remaining useful life of multiple systems. The effectiveness of the proposed online algorithm is illustrated by two real cases.",Ancha Xu,,https://arxiv.org/abs/2212.02688v1,https://arxiv.org/pdf/2212.02688v1,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2212.02688v1.pdf
2211.10212v1,2022-11-18T13:02:35Z,2022-11-18 13:02:35,A reliable data-based smoothing parameter selection method for circular kernel estimation,"A new data-based smoothing parameter for circular kernel density (and its derivatives) estimation is proposed. Following the plug-in ideas, unknown quantities on an optimal smoothing parameter are replaced by suitable estimates. This paper provides a circular version of the well-known Sheather and Jones bandwidths (DOI: 10.1111/j.2517-6161.1991.tb01857.x), with direct and solve-the-equation plug-in rules. Theoretical support for our developments, related to the asymptotic mean squared error of the estimator of the density, its derivatives, and its functionals, for circular data, are provided. The proposed selectors are compared with previous data-based smoothing parameters for circular kernel density estimation. This paper also contributes to the study of the optimal kernel for circular data. An illustration of the proposed plug-in rules is also shown using real data on the time of car accidents.",Jose Ameijeiras-Alonso,,https://arxiv.org/abs/2211.10212v1,https://arxiv.org/pdf/2211.10212v1,,,,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2211.10212v1.pdf
2211.09542v1,2022-11-17T14:02:16Z,2022-11-17 14:02:16,Bayesian improved cross entropy method for network reliability assessment,"We propose a modification of the improved cross entropy (iCE) method to enhance its performance for network reliability assessment. The iCE method performs a transition from the nominal density to the optimal importance sampling (IS) density via a parametric distribution model whose cross entropy with the optimal IS is minimized. The efficiency and accuracy of the iCE method are largely influenced by the choice of the parametric model. In the context of reliability of systems with independent multi-state components, the obvious choice of the parametric family is the categorical distribution. When updating this distribution model with standard iCE, the probability assigned to a certain category often converges to 0 due to lack of occurrence of samples from this category during the adaptive sampling process, resulting in a poor IS estima tor with a strong negative bias. To circumvent this issue, we propose an algorithm termed Bayesian improved cross entropy method (BiCE). Thereby, the posterior predictive distribution is employed to update the parametric model instead of the weighted maximum likelihood estimation approach employed in the original iCE method. A set of numerical examples illustrate the efficiency and accuracy of the proposed method.",Jianpeng Chan|Iason Papaioannou|Daniel Straub,,https://arxiv.org/abs/2211.09542v1,https://arxiv.org/pdf/2211.09542v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2211.09542v1.pdf
2211.08036v3,2022-11-15T10:36:21Z,2023-01-01 15:23:55,Provably Reliable Large-Scale Sampling from Gaussian Processes,"When comparing approximate Gaussian process (GP) models, it can be helpful to be able to generate data from any GP. If we are interested in how approximate methods perform at scale, we may wish to generate very large synthetic datasets to evaluate them. Naïvely doing so would cost \(\mathcal{O}(n^3)\) flops and \(\mathcal{O}(n^2)\) memory to generate a size \(n\) sample. We demonstrate how to scale such data generation to large \(n\) whilst still providing guarantees that, with high probability, the sample is indistinguishable from a sample from the desired GP.",Anthony Stephenson|Robert Allison|Edward Pyzer-Knapp,,https://arxiv.org/abs/2211.08036v3,https://arxiv.org/pdf/2211.08036v3,,"Main article 4 pages + 14 pages of supplementary material. To be published in NeurIPS 2022 Proceedings Workshop on ""Gaussian Processes, Spatiotemporal Modeling, and Decision-making Systems""",,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2211.08036v3.pdf
2211.07645v1,2022-11-14T01:29:09Z,2022-11-14 01:29:09,Evaluating Distribution System Reliability with Hyperstructures Graph Convolutional Nets,"Nowadays, it is broadly recognized in the power system community that to meet the ever expanding energy sector's needs, it is no longer possible to rely solely on physics-based models and that reliable, timely and sustainable operation of energy systems is impossible without systematic integration of artificial intelligence (AI) tools. Nevertheless, the adoption of AI in power systems is still limited, while integration of AI particularly into distribution grid investment planning is still an uncharted territory. We make the first step forward to bridge this gap by showing how graph convolutional networks coupled with the hyperstructures representation learning framework can be employed for accurate, reliable, and computationally efficient distribution grid planning with resilience objectives. We further propose a Hyperstructures Graph Convolutional Neural Networks (Hyper-GCNNs) to capture hidden higher order representations of distribution networks with attention mechanism. Our numerical experiments show that the proposed Hyper-GCNNs approach yields substantial gains in computational efficiency compared to the prevailing methodology in distribution grid planning and also noticeably outperforms seven state-of-the-art models from deep learning (DL) community.",Yuzhou Chen|Tian Jiang|Miguel Heleno|Alexandre Moreira|Yulia R. Gel,,https://arxiv.org/abs/2211.07645v1,https://arxiv.org/pdf/2211.07645v1,,IEEE BigData 2022,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2211.07645v1.pdf
2211.05764v1,2022-11-09T17:32:09Z,2022-11-09 17:32:09,DC-Check: A Data-Centric AI checklist to guide the development of reliable machine learning systems,"While there have been a number of remarkable breakthroughs in machine learning (ML), much of the focus has been placed on model development. However, to truly realize the potential of machine learning in real-world settings, additional aspects must be considered across the ML pipeline. Data-centric AI is emerging as a unifying paradigm that could enable such reliable end-to-end pipelines. However, this remains a nascent area with no standardized framework to guide practitioners to the necessary data-centric considerations or to communicate the design of data-centric driven ML systems. To address this gap, we propose DC-Check, an actionable checklist-style framework to elicit data-centric considerations at different stages of the ML pipeline: Data, Training, Testing, and Deployment. This data-centric lens on development aims to promote thoughtfulness and transparency prior to system development. Additionally, we highlight specific data-centric AI challenges and research opportunities. DC-Check is aimed at both practitioners and researchers to guide day-to-day development. As such, to easily engage with and use DC-Check and associated resources, we provide a DC-Check companion website (https://www.vanderschaar-lab.com/dc-check/). The website will also serve as an updated resource as methods and tooling evolve over time.",Nabeel Seedat|Fergus Imrie|Mihaela van der Schaar,,https://arxiv.org/abs/2211.05764v1,https://arxiv.org/pdf/2211.05764v1,https://doi.org/10.1109/TAI.2023.3345805,"Main paper: 11 pages, supplementary & case studies follow","IEEE Transactions on Artificial Intelligence, 2023",10.1109/TAI.2023.3345805,cs.LG,cs.LG|cs.AI|cs.CY|cs.SE|stat.ML,https://arxiv.org/pdf/2211.05764v1.pdf
2210.17497v1,2022-10-31T17:21:15Z,2022-10-31 17:21:15,Leveraging Pre-trained Models for Failure Analysis Triplets Generation,"Pre-trained Language Models recently gained traction in the Natural Language Processing (NLP) domain for text summarization, generation and question-answering tasks. This stems from the innovation introduced in Transformer models and their overwhelming performance compared with Recurrent Neural Network Models (Long Short Term Memory (LSTM)). In this paper, we leverage the attention mechanism of pre-trained causal language models such as Transformer model for the downstream task of generating Failure Analysis Triplets (FATs) - a sequence of steps for analyzing defected components in the semiconductor industry. We compare different transformer models for this generative task and observe that Generative Pre-trained Transformer 2 (GPT2) outperformed other transformer model for the failure analysis triplet generation (FATG) task. In particular, we observe that GPT2 (trained on 1.5B parameters) outperforms pre-trained BERT, BART and GPT3 by a large margin on ROUGE. Furthermore, we introduce Levenshstein Sequential Evaluation metric (LESE) for better evaluation of the structured FAT data and show that it compares exactly with human judgment than existing metrics.",Kenneth Ezukwoke|Anis Hoayek|Mireille Batton-Hubert|Xavier Boucher|Pascal Gounet|Jerome Adrian,,https://arxiv.org/abs/2210.17497v1,https://arxiv.org/pdf/2210.17497v1,,"33 pages, 11 figures, 9 tables",,,cs.CL,cs.CL|cs.LG|stat.AP,https://arxiv.org/pdf/2210.17497v1.pdf
2210.14759v2,2022-10-26T14:41:31Z,2022-10-27 00:44:03,An analysis of degradation in low-cost particulate matter sensors,"Low-cost sensors (LCS) are increasingly being used to measure fine particulate matter (PM2.5) concentrations in cities around the world. One of the most commonly deployed LCS is the PurpleAir with about 15,000 sensors deployed in the United States. However, the change in sensor performance over time has not been well studied. It is important to understand the lifespan of these sensors to determine when they should be replaced, and when measurements from these devices should or should not be used for various applications. This paper fills in this gap by leveraging the fact that: 1) Each PurpleAir sensor is comprised of two identical sensors and the divergence between their measurements can be observed, and 2) There are numerous PurpleAir sensors within 50 meters of regulatory monitors allowing for the comparison of measurements between these two instruments. We propose empirically-derived degradation outcomes for the PurpleAir sensors and evaluate how these outcomes change over time. On average, we find that the number of 'flagged' measurements, where the two sensors within each PurpleAir disagree, increases in time to 4 percent after 4 years of operation. Approximately, 2 percent of all PurpleAir sensors were permanently degraded. The largest fraction of permanently degraded PurpleAir sensors appeared to be in the hot and humid climate zone, suggesting that the sensors in this zone may need to be replaced sooner. We also find that the bias of PurpleAir sensors, or the difference between corrected PM2.5 levels and the corresponding reference measurements, changed over time by -0.12 ug/m3 (95% CI: -0.13 ug/m3, -0.11 ug/m3) per year. The average bias increases dramatically after 3.5 years. Climate zone is a significant modifier of the association between degradation outcomes and time.",Priyanka deSouza|Karoline Barkjohn|Andrea Clements|Jenny Lee|Ralph Kahn|Ben Crawford|Patrick Kinney,,https://arxiv.org/abs/2210.14759v2,https://arxiv.org/pdf/2210.14759v2,,"28 pages, 5 figures, 4 tables",,,stat.AP,stat.AP,https://arxiv.org/pdf/2210.14759v2.pdf
2210.07521v1,2022-10-14T05:03:58Z,2022-10-14 05:03:58,Reliability-Based Robust Design Optimization Method for Engineering Systems with Uncertainty Quantification,"Robust optimization is a method for optimization under uncertainties in engineering systems and designs for applications ranging from aeronautics to nuclear. In a robust design process, parameter variability (or uncertainty) is incorporated into the engineering systems' optimization process to assure the systems' quality and reliability. This chapter focuses on a robust optimization approach for developing robust and reliable advanced systems and explains the framework for using uncertainty quantification and optimization techniques. For the uncertainty analysis, a polynomial chaos-based approach is combined with the optimization algorithms MOSA (Multi-Objective Simulated Annealing), and the process is discussed with a simplified test function. For the optimization process, gradient-free genetic algorithms are considered as the optimizer scans the whole design space, and the optimal values are not always dependent on the initial values.",Richa Verma|Dinesh Kumar|Kazuma Kobayashi|Syed Alam,,https://arxiv.org/abs/2210.07521v1,https://arxiv.org/pdf/2210.07521v1,,,"Handbook of Smart Energy Systems, 2022",,stat.CO,stat.CO|stat.AP,https://arxiv.org/pdf/2210.07521v1.pdf
2209.13496v1,2022-09-21T16:48:12Z,2022-09-21 16:48:12,On the Comparison between the Reliability of Units Produced by Different Production Lines,"The paper discusses how to evaluate the reliability of units produced by different production lines. The procedure is based on selecting independent random samples of units produced by different production lines and then evaluating reliability functions for each group of units. The comparison between these reliability functions at a given time allows manufacturing experts to evaluate the effectiveness of production lines. A statistical methodology has been taken based on the assumption that the lifetime of units produced by each product line has a Weibull Gamma distribution. Then, real-world data is used to illustrate the study's contribution to reliability theory applications.",Rashad M. EL-Sagheer|Mahmoud A. W. Mahmoud|Mahmoud M. M. Mansour|Mohamed S. Aboshady,,https://arxiv.org/abs/2209.13496v1,https://arxiv.org/pdf/2209.13496v1,https://doi.org/10.1002/qre.3336,,,10.1002/qre.3336,stat.AP,stat.AP,https://arxiv.org/pdf/2209.13496v1.pdf
2209.09168v1,2022-09-19T16:44:44Z,2022-09-19 16:44:44,Application of Neural Network in the Prediction of NOx Emissions from Degrading Gas Turbine,"This paper is aiming to apply neural network algorithm for predicting the process response (NOx emissions) from degrading natural gas turbines. Nine different process variables, or predictors, are considered in the predictive modelling. It is found out that the model trained by neural network algorithm should use part of recent data in the training and validation sets accounting for the impact of the system degradation. R-Square values of the training and validation sets demonstrate the validity of the model. The residue plot, without any clear pattern, shows the model is appropriate. The ranking of the importance of the process variables are demonstrated and the prediction profile confirms the significance of the process variables. The model trained by using neural network algorithm manifests the optimal settings of the process variables to reach the minimum value of NOx emissions from the degrading gas turbine system.",Zhenkun Zheng|Alan Rezazadeh,,https://arxiv.org/abs/2209.09168v1,https://arxiv.org/pdf/2209.09168v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2209.09168v1.pdf
2209.06704v2,2022-09-14T15:15:29Z,2024-03-28 10:52:49,Causal chain event graphs for remedial maintenance,"The analysis of system reliability has often benefited from graphical tools such as fault trees and Bayesian networks. In this article, instead of conventional graphical tools, we apply a probabilistic graphical model called the chain event graph (CEG) to represent the failures and processes of deterioration of a system. The CEG is derived from an event tree and can flexibly represent the unfolding of asymmetric processes. For this application we need to define a new class of formal intervention we call remedial to model causal effects of remedial maintenance. This fixes the root causes of a failure and returns the status of the system to as good as new. We demonstrate that the semantics of the CEG are rich enough to express this novel type of intervention. Furthermore through the bespoke causal algebras the CEG provides a transparent framework with which guide and express the rationale behind predictive inferences about the effects of various different types of remedial intervention. A back-door theorem is adapted to apply to these interventions to help discover when a system is only partially observed.",Xuewen Yu|Jim Q. Smith,,https://arxiv.org/abs/2209.06704v2,https://arxiv.org/pdf/2209.06704v2,https://doi.org/10.1111/risa.14308,,,10.1111/risa.14308,stat.ME,stat.ME,https://arxiv.org/pdf/2209.06704v2.pdf
2209.02678v1,2022-09-06T17:47:39Z,2022-09-06 17:47:39,An Optimized and Safety-aware Maintenance Framework: A Case Study on Aircraft Engine,"The COVID-19 pandemic has recently exacerbated the fierce competition in the transportation businesses. The airline industry took one of the biggest hits as the closure of international borders forced aircraft operators to suspend their international routes, keeping aircraft on the ground without generating revenues while at the same time still requiring adequate maintenance. To maintain their operational sustainability, finding a good balance between cost reductions measure and safety standards fulfillment, including its maintenance procedure, becomes critical. This paper proposes an AI-assisted predictive maintenance scheme that synthesizes prognostics modeling and simulation-based optimization to help airlines decide their optimal engine maintenance approach. The proposed method enables airlines to utilize their diagnostics measurements and operational settings to design a more customized maintenance strategy that takes engine operations conditions into account. Our numerical experiments on the proposed approach resulted in significant cost savings without compromising the safety standards. The experiments also show that maintenance strategies tailored to the failure mode and operational settings (that our framework enables) yield 13% more cost savings than generic optimal maintenance strategies. The generality of our proposed framework allows the extension to other intelligent, safety-critical transportation systems.",Muhammad Ziyad|Kenrick Tjandra| Zulvah|Mushonnifun Faiz Sugihartanto|Mansur Arief,,https://arxiv.org/abs/2209.02678v1,https://arxiv.org/pdf/2209.02678v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2209.02678v1.pdf
2208.13624v1,2022-08-29T14:13:55Z,2022-08-29 14:13:55,Towards Reliable Simulation-Based Inference with Balanced Neural Ratio Estimation,"Modern approaches for simulation-based inference rely upon deep learning surrogates to enable approximate inference with computer simulators. In practice, the estimated posteriors' computational faithfulness is, however, rarely guaranteed. For example, Hermans et al. (2021) show that current simulation-based inference algorithms can produce posteriors that are overconfident, hence risking false inferences. In this work, we introduce Balanced Neural Ratio Estimation (BNRE), a variation of the NRE algorithm designed to produce posterior approximations that tend to be more conservative, hence improving their reliability, while sharing the same Bayes optimal solution. We achieve this by enforcing a balancing condition that increases the quantified uncertainty in small simulation budget regimes while still converging to the exact posterior as the budget increases. We provide theoretical arguments showing that BNRE tends to produce posterior surrogates that are more conservative than NRE's. We evaluate BNRE on a wide variety of tasks and show that it produces conservative posterior surrogates on all tested benchmarks and simulation budgets. Finally, we emphasize that BNRE is straightforward to implement over NRE and does not introduce any computational overhead.",Arnaud Delaunoy|Joeri Hermans|François Rozet|Antoine Wehenkel|Gilles Louppe,,https://arxiv.org/abs/2208.13624v1,https://arxiv.org/pdf/2208.13624v1,,Code available at https://github.com/montefiore-ai/balanced-nre,,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2208.13624v1.pdf
2208.09431v4,2022-08-19T16:24:22Z,2023-07-15 09:35:24,Simulation and inference on purely observational methods of monitoring vaccine effectiveness post-deployment: none is reliable without precise information on population behaviour,"Two observational methods are currently being used to monitor post-deployment vaccine effectiveness: the obvious crude method comparing rate testing positive per head of vaccinated population with that rate per head of unvaccinated population; and the test-negative case control (TNCC) method. The two methods give very different results. We want to know whether either method is reliable.
  We assume either a homogeneous population or one partitioned into two homogeneous subsets which differ only in their not-directly-observable healthcare-seeking behaviour including probability of getting vaccinated. We first consider uniform independent priors on the probabilities of being hospitalised conditional on subset, vaccination status, and infection status. We simulate from the resulting model and observe the TNCC estimate, the crude estimate, and the Bayesian central 95% confidence interval on vaccine effectiveness represented as log ratio of odds ratios for infection with and without vaccination.
  With these wide open priors, even when the population is homogeneous, the Bayesian 95% confidence interval typically has a width of nearly 4 nats (55-fold), implying too much uncertainty for the data collected to be of any use in monitoring effectiveness. There do exist some tight priors under which the data is useful: some lead to TNCC being more accurate while with others the crude estimate is more accurate.
  Thus using only data from those spontaneously choosing to be tested, we find that neither method is reliably better than the other, and indeed that the desired information is not present in this data. We conclude that effective monitoring of vaccine effectiveness and side-effects requires either strong information on the population's behaviour, or ongoing randomised controlled trials (RCTs), rather than just choosing whichever of TNCC and crude estimate gives the result we prefer to find.",Roger F. Sewell,,https://arxiv.org/abs/2208.09431v4,https://arxiv.org/pdf/2208.09431v4,,12 pages with 6 figures and 2 tables; plus 12 pages of appendices with 2 figures and 9 tables; 16 references. v2 has typos fixed at starts of sections 3 and B4 and grammar fixed elsewhere. v3 has work on partial real-world covid-related data added. v4 adds conditional independence relationships and another way of resolving the problem,,,stat.ME,stat.ME|q-bio.QM|stat.AP,https://arxiv.org/pdf/2208.09431v4.pdf
2208.06727v3,2022-08-13T20:47:14Z,2024-01-30 18:29:44,Reliable emulation of complex functionals by active learning with error control,"A statistical emulator can be used as a surrogate of complex physics-based calculations to drastically reduce the computational cost. Its successful implementation hinges on an accurate representation of the nonlinear response surface with a high-dimensional input space. Conventional ""space-filling"" designs, including random sampling and Latin hypercube sampling, become inefficient as the dimensionality of the input variables increases, and the predictive accuracy of the emulator can degrade substantially for a test input distant from the training input set. To address this fundamental challenge, we develop a reliable emulator for predicting complex functionals by active learning with error control (ALEC). The algorithm is applicable to infinite-dimensional mapping with high-fidelity predictions and a controlled predictive error. The computational efficiency has been demonstrated by emulating the classical density functional theory (cDFT) calculations, a statistical-mechanical method widely used in modeling the equilibrium properties of complex molecular systems. We show that ALEC is much more accurate than conventional emulators based on the Gaussian processes with ""space-filling"" designs and alternative active learning methods. Besides, it is computationally more efficient than direct cDFT calculations. ALEC can be a reliable building block for emulating expensive functionals owing to its minimal computational cost, controllable predictive error, and fully automatic features.",Xinyi Fang|Mengyang Gu|Jianzhong Wu,,https://arxiv.org/abs/2208.06727v3,https://arxiv.org/pdf/2208.06727v3,https://doi.org/10.1063/5.0121805,"15 pages, 10 figures",,10.1063/5.0121805,physics.chem-ph,physics.chem-ph|stat.AP,https://arxiv.org/pdf/2208.06727v3.pdf
2208.03915v2,2022-08-08T04:40:20Z,2024-02-13 19:59:56,Dynamic Maintenance of Kernel Density Estimation Data Structure: From Practice to Theory,"Kernel density estimation (KDE) stands out as a challenging task in machine learning. The problem is defined in the following way: given a kernel function $f(x,y)$ and a set of points $\{x_1, x_2, \cdots, x_n \} \subset \mathbb{R}^d$, we would like to compute $\frac{1}{n}\sum_{i=1}^{n} f(x_i,y)$ for any query point $y \in \mathbb{R}^d$. Recently, there has been a growing trend of using data structures for efficient KDE. However, the proposed KDE data structures focus on static settings. The robustness of KDE data structures over dynamic changing data distributions is not addressed. In this work, we focus on the dynamic maintenance of KDE data structures with robustness to adversarial queries. Especially, we provide a theoretical framework of KDE data structures. In our framework, the KDE data structures only require subquadratic spaces. Moreover, our data structure supports the dynamic update of the dataset in sublinear time. Furthermore, we can perform adaptive queries with the potential adversary in sublinear time.",Jiehao Liang|Zhao Song|Zhaozhuo Xu|Junze Yin|Danyang Zhuo,,https://arxiv.org/abs/2208.03915v2,https://arxiv.org/pdf/2208.03915v2,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2208.03915v2.pdf
2207.13676v2,2022-07-27T17:42:09Z,2023-01-10 19:46:46,Open Source Vizier: Distributed Infrastructure and API for Reliable and Flexible Blackbox Optimization,"Vizier is the de-facto blackbox and hyperparameter optimization service across Google, having optimized some of Google's largest products and research efforts. To operate at the scale of tuning thousands of users' critical systems, Google Vizier solved key design challenges in providing multiple different features, while remaining fully fault-tolerant. In this paper, we introduce Open Source (OSS) Vizier, a standalone Python-based interface for blackbox optimization and research, based on the Google-internal Vizier infrastructure and framework. OSS Vizier provides an API capable of defining and solving a wide variety of optimization problems, including multi-metric, early stopping, transfer learning, and conditional search. Furthermore, it is designed to be a distributed system that assures reliability, and allows multiple parallel evaluations of the user's objective function. The flexible RPC-based infrastructure allows users to access OSS Vizier from binaries written in any language. OSS Vizier also provides a back-end (""Pythia"") API that gives algorithm authors a way to interface new algorithms with the core OSS Vizier system. OSS Vizier is available at https://github.com/google/vizier.",Xingyou Song|Sagi Perel|Chansoo Lee|Greg Kochanski|Daniel Golovin,,https://arxiv.org/abs/2207.13676v2,https://arxiv.org/pdf/2207.13676v2,,Published as a conference paper for the systems track at the 1st International Conference on Automated Machine Learning (AutoML-Conf 2022). Code can be found at https://github.com/google/vizier,,,cs.LG,cs.LG|cs.DC|stat.ML,https://arxiv.org/pdf/2207.13676v2.pdf
2207.12957v2,2022-07-26T15:01:48Z,2022-07-31 07:22:29,Reliability analysis of K-out-of-N system for Weibull components based on generalized progressive hybrid censored data,"In this paper, we have investigated the reliability of a K-out-of-N system for the components following Weibull distribution based on the generalized progressive hybrid censored data. We have obtained the maximum likelihood estimates (MLEs) of the unknown parameters and the reliability function of the system. Using asymptotic normality property of MLEs, the corresponding asymptotic confidence intervals are constructed. Furthermore, Bayes estimates are derived under squared error loss function with informative prior by using Markov Chain Monte Carlo (MCMC) technique. Highest posterior density (HPD) credible intervals are obtained. A Monte Carlo simulation study is carried out to compare performance of the established estimates. Finally, a real data set is considered for illustrative purposes.",Subhankar Dutta|Suchandan Kayal,,https://arxiv.org/abs/2207.12957v2,https://arxiv.org/pdf/2207.12957v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2207.12957v2.pdf
2207.11640v3,2022-07-24T02:38:54Z,2023-01-18 18:41:22,Reliable amortized variational inference with physics-based latent distribution correction,"Bayesian inference for high-dimensional inverse problems is computationally costly and requires selecting a suitable prior distribution. Amortized variational inference addresses these challenges via a neural network that approximates the posterior distribution not only for one instance of data, but a distribution of data pertaining to a specific inverse problem. During inference, the neural network -- in our case a conditional normalizing flow -- provides posterior samples at virtually no cost. However, the accuracy of amortized variational inference relies on the availability of high-fidelity training data, which seldom exists in geophysical inverse problems due to the Earth's heterogeneity. In addition, the network is prone to errors if evaluated over out-of-distribution data. As such, we propose to increase the resilience of amortized variational inference in the presence of moderate data distribution shifts. We achieve this via a correction to the latent distribution that improves the posterior distribution approximation for the data at hand. The correction involves relaxing the standard Gaussian assumption on the latent distribution and parameterizing it via a Gaussian distribution with an unknown mean and (diagonal) covariance. These unknowns are then estimated by minimizing the Kullback-Leibler divergence between the corrected and the (physics-based) true posterior distributions. While generic and applicable to other inverse problems, by means of a linearized seismic imaging example, we show that our correction step improves the robustness of amortized variational inference with respect to changes in the number of seismic sources, noise variance, and shifts in the prior distribution. This approach provides a seismic image with limited artifacts and an assessment of its uncertainty at approximately the same cost as five reverse-time migrations.",Ali Siahkoohi|Gabrio Rizzuti|Rafael Orozco|Felix J. Herrmann,,https://arxiv.org/abs/2207.11640v3,https://arxiv.org/pdf/2207.11640v3,,,,,stat.ML,stat.ML|cs.LG|physics.geo-ph,https://arxiv.org/pdf/2207.11640v3.pdf
2207.09101v3,2022-07-19T06:34:36Z,2024-04-11 14:22:59,Assessing quality of selection procedures: Lower bound of false positive rate as a function of inter-rater reliability,"Inter-rater reliability (IRR) is one of the commonly used tools for assessing the quality of ratings from multiple raters. However, applicant selection procedures based on ratings from multiple raters usually result in a binary outcome; the applicant is either selected or not. This final outcome is not considered in IRR, which instead focuses on the ratings of the individual subjects or objects. We outline the connection between the ratings' measurement model (used for IRR) and a binary classification framework. We develop a simple way of approximating the probability of correctly selecting the best applicants which allows us to compute error probabilities of the selection procedure (i.e., false positive and false negative rate) or their lower bounds. We draw connections between the inter-rater reliability and the binary classification metrics, showing that binary classification metrics depend solely on the IRR coefficient and proportion of selected applicants. We assess the performance of the approximation in a simulation study and apply it in an example comparing the reliability of multiple grant peer review selection procedures. We also discuss possible other uses of the explored connections in other contexts, such as educational testing, psychological assessment, and health-related measurement and implement the computations in IRR2FPR R package.",František Bartoš|Patrícia Martinková,,https://arxiv.org/abs/2207.09101v3,https://arxiv.org/pdf/2207.09101v3,https://doi.org/10.1111/bmsp.12343,,,10.1111/bmsp.12343,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/2207.09101v3.pdf
2207.07411v1,2022-07-15T11:39:37Z,2022-07-15 11:39:37,Plex: Towards Reliability using Pretrained Large Model Extensions,"A recent trend in artificial intelligence is the use of pretrained models for language and vision tasks, which have achieved extraordinary performance but also puzzling failures. Probing these models' abilities in diverse ways is therefore critical to the field. In this paper, we explore the reliability of models, where we define a reliable model as one that not only achieves strong predictive performance but also performs well consistently over many decision-making tasks involving uncertainty (e.g., selective prediction, open set recognition), robust generalization (e.g., accuracy and proper scoring rules such as log-likelihood on in- and out-of-distribution datasets), and adaptation (e.g., active learning, few-shot uncertainty). We devise 10 types of tasks over 40 datasets in order to evaluate different aspects of reliability on both vision and language domains. To improve reliability, we developed ViT-Plex and T5-Plex, pretrained large model extensions for vision and language modalities, respectively. Plex greatly improves the state-of-the-art across reliability tasks, and simplifies the traditional protocol as it improves the out-of-the-box performance and does not require designing scores or tuning the model for each task. We demonstrate scaling effects over model sizes up to 1B parameters and pretraining dataset sizes up to 4B examples. We also demonstrate Plex's capabilities on challenging tasks including zero-shot open set recognition, active learning, and uncertainty in conversational language understanding.",Dustin Tran|Jeremiah Liu|Michael W. Dusenberry|Du Phan|Mark Collier|Jie Ren|Kehang Han|Zi Wang|Zelda Mariet|Huiyi Hu|Neil Band|Tim G. J. Rudner|Karan Singhal|Zachary Nado|Joost van Amersfoort|Andreas Kirsch|Rodolphe Jenatton|Nithum Thain|Honglin Yuan|Kelly Buchanan|Kevin Murphy|D. Sculley|Yarin Gal|Zoubin Ghahramani|Jasper Snoek|Balaji Lakshminarayanan,,https://arxiv.org/abs/2207.07411v1,https://arxiv.org/pdf/2207.07411v1,,Code available at https://goo.gle/plex-code,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2207.07411v1.pdf
2207.02855v1,2022-07-06T10:30:41Z,2022-07-06 10:30:41,Coding Reliability with Aclus -- Did I correctly characterize my observations?,Describing observations or objects in non-mathematical disciplines can often be accomplished by answering a list of questions. These questions can be formulated in such a way that the only possible answers always are ``yes'' or ``no''. This article is about automatically checking such given binary data sets for inconsistencies and about finding possible logical rules valid for the analyzed objects.,Marcus Weber|Oguzhan Yürük,,https://arxiv.org/abs/2207.02855v1,https://arxiv.org/pdf/2207.02855v1,,21 pages,,,math.AC,math.AC|stat.ME,https://arxiv.org/pdf/2207.02855v1.pdf
2207.02071v2,2022-07-05T14:20:15Z,2023-02-16 14:21:42,Assessing inter-rater reliability with heterogeneous variance components models: Flexible approach accounting for contextual variables,"Inter-rater reliability (IRR), which is a prerequisite of high-quality ratings and assessments, may be affected by contextual variables such as the rater's or ratee's gender, major, or experience. Identification of such heterogeneity sources in IRR is important for implementation of policies with the potential to decrease measurement error and to increase IRR by focusing on the most relevant subgroups. In this study, we propose a flexible approach for assessing IRR in cases of heterogeneity due to covariates by directly modeling differences in variance components. We use Bayes factors to select the best performing model, and we suggest using Bayesian model-averaging as an alternative approach for obtaining IRR and variance component estimates, allowing us to account for model uncertainty. We use inclusion Bayes factors considering the whole model space to provide evidence for or against differences in variance components due to covariates. The proposed method is compared with other Bayesian and frequentist approaches in a simulation study, and we demonstrate its superiority in some situations. Finally, we provide real data examples from grant proposal peer-review, demonstrating the usefulness of this method and its flexibility in the generalization of more complex designs.",Patrícia Martinková|František Bartoš|Marek Brabec,,https://arxiv.org/abs/2207.02071v2,https://arxiv.org/pdf/2207.02071v2,https://doi.org/10.3102/10769986221150517,,,10.3102/10769986221150517,stat.ME,stat.ME|stat.AP|stat.CO,https://arxiv.org/pdf/2207.02071v2.pdf
2207.01609v1,2022-07-04T17:49:25Z,2022-07-04 17:49:25,Recommendation Systems with Distribution-Free Reliability Guarantees,"When building recommendation systems, we seek to output a helpful set of items to the user. Under the hood, a ranking model predicts which of two candidate items is better, and we must distill these pairwise comparisons into the user-facing output. However, a learned ranking model is never perfect, so taking its predictions at face value gives no guarantee that the user-facing output is reliable. Building from a pre-trained ranking model, we show how to return a set of items that is rigorously guaranteed to contain mostly good items. Our procedure endows any ranking model with rigorous finite-sample control of the false discovery rate (FDR), regardless of the (unknown) data distribution. Moreover, our calibration algorithm enables the easy and principled integration of multiple objectives in recommender systems. As an example, we show how to optimize for recommendation diversity subject to a user-specified level of FDR control, circumventing the need to specify ad hoc weights of a diversity loss against an accuracy loss. Throughout, we focus on the problem of learning to rank a set of possible recommendations, evaluating our methods on the Yahoo! Learning to Rank and MSMarco datasets.",Anastasios N. Angelopoulos|Karl Krauth|Stephen Bates|Yixin Wang|Michael I. Jordan,,https://arxiv.org/abs/2207.01609v1,https://arxiv.org/pdf/2207.01609v1,,,,,cs.IR,cs.IR|cs.LG|stat.ML,https://arxiv.org/pdf/2207.01609v1.pdf
2206.12892v1,2022-06-26T14:47:25Z,2022-06-26 14:47:25,A Model for Censored Reliability Data with Two Dependent Failure Modes and Prediction of Future Failures,"Quite often, we observe reliability data with two failure modes that may influence each other, resulting in a setting of dependent failure modes. Here, we discuss modelling of censored reliability data with two dependent failure modes by using a bivariate Weibull model with distinct shape parameters which we construct as an extension of the well-known Marshall-Olkin bivariate exponential model in reliability. Likelihood inference for modelling censored reliability data with two dependent failure modes by using the proposed bivariate Weibull distribution with distinct shape parameters is discussed. Bayesian analysis for this issue is also discussed. Through a Monte Carlo simulation study, the proposed methods of inference are observed to provide satisfactory results. A problem of practical interest for reliability engineers is to predict field failures of units at a future time. Frequentist and Bayesian methods for prediction of future failures are developed in this setting of censored reliability data with two dependent failure modes. An illustrative example based on a real data on device failure with two failure modes is presented. The model and methodology presented in this article provide a complete and comprehensive treatment of modelling censored reliability data with two dependent failure modes, and address some practical prediction issues.",Aakash Agrawal|Debanjan Mitra|Ayon Ganguly,,https://arxiv.org/abs/2206.12892v1,https://arxiv.org/pdf/2206.12892v1,https://doi.org/10.1080/08982112.2024.2321839,,,10.1080/08982112.2024.2321839,stat.ME,stat.ME,https://arxiv.org/pdf/2206.12892v1.pdf
2206.01562v1,2022-06-03T13:35:57Z,2022-06-03 13:35:57,Prescriptive maintenance with causal machine learning,"Machine maintenance is a challenging operational problem, where the goal is to plan sufficient preventive maintenance to avoid machine failures and overhauls. Maintenance is often imperfect in reality and does not make the asset as good as new. Although a variety of imperfect maintenance policies have been proposed in the literature, these rely on strong assumptions regarding the effect of maintenance on the machine's condition, assuming the effect is (1) deterministic or governed by a known probability distribution, and (2) machine-independent. This work proposes to relax both assumptions by learning the effect of maintenance conditional on a machine's characteristics from observational data on similar machines using existing methodologies for causal inference. By predicting the maintenance effect, we can estimate the number of overhauls and failures for different levels of maintenance and, consequently, optimize the preventive maintenance frequency to minimize the total estimated cost. We validate our proposed approach using real-life data on more than 4,000 maintenance contracts from an industrial partner. Empirical results show that our novel, causal approach accurately predicts the maintenance effect and results in individualized maintenance schedules that are more accurate and cost-effective than supervised or non-individualized approaches.",Toon Vanderschueren|Robert Boute|Tim Verdonck|Bart Baesens|Wouter Verbeke,,https://arxiv.org/abs/2206.01562v1,https://arxiv.org/pdf/2206.01562v1,,,,,econ.GN,econ.GN|cs.LG|stat.ML,https://arxiv.org/pdf/2206.01562v1.pdf
2205.12131v1,2022-05-24T15:08:02Z,2022-05-24 15:08:02,Detecting Deforestation from Sentinel-1 Data in the Absence of Reliable Reference Data,"Forests are vital for the wellbeing of our planet. Large and small scale deforestation across the globe is threatening the stability of our climate, forest biodiversity, and therefore the preservation of fragile ecosystems and our natural habitat as a whole. With increasing public interest in climate change issues and forest preservation, a large demand for carbon offsetting, carbon footprint ratings, and environmental impact assessments is emerging. Most often, deforestation maps are created from optical data such as Landsat and MODIS. These maps are not typically available at less than annual intervals due to persistent cloud cover in many parts of the world, especially the tropics where most of the world's forest biomass is concentrated. Synthetic Aperture Radar (SAR) can fill this gap as it penetrates clouds. We propose and evaluate a novel method for deforestation detection in the absence of reliable reference data which often constitutes the largest practical hurdle. This method achieves a change detection sensitivity (producer's accuracy) of 96.5% in the study area, although false positives lead to a lower user's accuracy of about 75.7%, with a total balanced accuracy of 90.4%. The change detection accuracy is maintained when adding up to 20% noise to the reference labels. While further work is required to reduce the false positive rate, improve detection delay, and validate this method in additional circumstances, the results show that Sentinel-1 data have the potential to advance the timeliness of global deforestation monitoring.",Johannes N. Hansen|Edward T. A. Mitchard|Stuart King,,https://arxiv.org/abs/2205.12131v1,https://arxiv.org/pdf/2205.12131v1,,,,,stat.ME,stat.ME|cs.CV,https://arxiv.org/pdf/2205.12131v1.pdf
2205.02024v1,2022-05-04T12:25:33Z,2022-05-04 12:25:33,Angular Control Charts: A New Perspective for Monitoring Reliability of Multi-State Systems,"Control charts, as had been used traditionally for quality monitoring, were applied alternatively to monitor systems' reliability. In other words, they can be applied to detect changes in the failure behavior of systems. Such purpose imposed modifying traditional control charts in addition to developing charts that are more compatible with reliability monitoring. The latter developed category is known as probability limits control charts. The existing reliability monitoring control charts were only dedicated to binary-state systems, and they can't be used to monitor several states simultaneously. Therefore, this paper develops a design of control charts that accommodates multi-state systems, called here as the Angular Control Chart, which represents a new version of the probability limits control charts. This design is able to monitor state transitions simultaneously and individually in addition. Illustrative system examples are implemented to explore the monitoring procedure of the new design and to demonstrate its efficiency, effectiveness, and limitations.",Khaled Janada|Hassan Soltan|Mohamed-Sobeih Hussein|Ahmad Abdel-Shafi,,https://arxiv.org/abs/2205.02024v1,https://arxiv.org/pdf/2205.02024v1,https://doi.org/10.1016/j.cie.2022.108621,18 pages; 13 figures,Computers & Industrial Engineering 172PA (2022) 108621,10.1016/j.cie.2022.108621,stat.AP,stat.AP|eess.SY,https://arxiv.org/pdf/2205.02024v1.pdf
2204.14130v1,2022-04-29T14:32:42Z,2022-04-29 14:32:42,Reliability in Time: Evaluating the Web Sources of Information on COVID-19 in Wikipedia across Various Language Editions from the Beginning of the Pandemic,"There are over a billion websites on the Internet that can potentially serve as sources of information on various topics. One of the most popular examples of such an online source is Wikipedia. This public knowledge base is co-edited by millions of users from all over the world. Information in each language version of Wikipedia can be created and edited independently. Therefore, we can observe certain inconsistencies in the statements and facts described therein - depending on language and topic. In accordance with the Wikipedia content authoring guidelines, information in Wikipedia articles should be based on reliable, published sources. So, based on data from such a collaboratively edited encyclopedia, we should also be able to find important sources on specific topics. This effect can be potentially useful for people and organizations.
  The reliability of a source in Wikipedia articles depends on the context. So the same source (website) may have various degrees of reliability in Wikipedia depending on topic and language version. Moreover, reliability of the same source can change over the time. The purpose of this study is to identify reliable sources on a specific topic - the COVID-19 pandemic. Such an analysis was carried out on real data from Wikipedia within selected language versions and within a selected time period.",Włodzimierz Lewoniewski|Krzysztof Węcel|Witold Abramowicz,,https://arxiv.org/abs/2204.14130v1,https://arxiv.org/pdf/2204.14130v1,,,,,cs.IR,cs.IR|stat.AP,https://arxiv.org/pdf/2204.14130v1.pdf
2204.09462v1,2022-04-20T13:52:00Z,2022-04-20 13:52:00,Quantity vs Quality: Investigating the Trade-Off between Sample Size and Label Reliability,"In this paper, we study learning in probabilistic domains where the learner may receive incorrect labels but can improve the reliability of labels by repeatedly sampling them. In such a setting, one faces the problem of whether the fixed budget for obtaining training examples should rather be used for obtaining all different examples or for improving the label quality of a smaller number of examples by re-sampling their labels. We motivate this problem in an application to compare the strength of poker hands where the training signal depends on the hidden community cards, and then study it in depth in an artificial setting where we insert controlled noise levels into the MNIST database. Our results show that with increasing levels of noise, resampling previous examples becomes increasingly more important than obtaining new examples, as classifier performance deteriorates when the number of incorrect labels is too high. In addition, we propose two different validation strategies; switching from lower to higher validations over the course of training and using chi-square statistics to approximate the confidence in obtained labels.",Timo Bertram|Johannes Fürnkranz|Martin Müller,,https://arxiv.org/abs/2204.09462v1,https://arxiv.org/pdf/2204.09462v1,,Preliminary work under review for ICML 2022,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2204.09462v1.pdf
2204.06099v2,2022-04-12T22:01:08Z,2022-10-25 21:54:25,Specifying Prior Distributions in Reliability Applications,"Especially when facing reliability data with limited information (e.g., a small number of failures), there are strong motivations for using Bayesian inference methods. These include the option to use information from physics-of-failure or previous experience with a failure mode in a particular material to specify an informative prior distribution. Another advantage is the ability to make statistical inferences without having to rely on specious (when the number of failures is small) asymptotic theory needed to justify non-Bayesian methods. Users of non-Bayesian methods are faced with multiple methods of constructing uncertainty intervals (Wald, likelihood, and various bootstrap methods) that can give substantially different answers when there is little information in the data. For Bayesian inference, there is only one method of constructing equal-tail credible intervals-but it is necessary to provide a prior distribution to fully specify the model. Much work has been done to find default prior distributions that will provide inference methods with good (and in some cases exact) frequentist coverage properties. This paper reviews some of this work and provides, evaluates, and illustrates principled extensions and adaptations of these methods to the practical realities of reliability data (e.g., non-trivial censoring).",Qinglong Tian|Colin Lewis-Beck|Jarad Niemi|William Meeker,,https://arxiv.org/abs/2204.06099v2,https://arxiv.org/pdf/2204.06099v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2204.06099v2.pdf
2203.15945v2,2022-03-29T23:05:40Z,2024-05-16 16:06:36,A Framework for Improving the Reliability of Black-box Variational Inference,"Black-box variational inference (BBVI) now sees widespread use in machine learning and statistics as a fast yet flexible alternative to Markov chain Monte Carlo methods for approximate Bayesian inference. However, stochastic optimization methods for BBVI remain unreliable and require substantial expertise and hand-tuning to apply effectively. In this paper, we propose Robust and Automated Black-box VI (RABVI), a framework for improving the reliability of BBVI optimization. RABVI is based on rigorously justified automation techniques, includes just a small number of intuitive tuning parameters, and detects inaccurate estimates of the optimal variational approximation. RABVI adaptively decreases the learning rate by detecting convergence of the fixed--learning-rate iterates, then estimates the symmetrized Kullback--Leibler (KL) divergence between the current variational approximation and the optimal one. It also employs a novel optimization termination criterion that enables the user to balance desired accuracy against computational cost by comparing (i) the predicted relative decrease in the symmetrized KL divergence if a smaller learning were used and (ii) the predicted computation required to converge with the smaller learning rate. We validate the robustness and accuracy of RABVI through carefully designed simulation studies and on a diverse set of real-world model and data examples.",Manushi Welandawe|Michael Riis Andersen|Aki Vehtari|Jonathan H. Huggins,,https://arxiv.org/abs/2203.15945v2,https://arxiv.org/pdf/2203.15945v2,,,"Journal of Machine Learning Research, 25(219):1-71, 2024",,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/2203.15945v2.pdf
2203.12913v1,2022-03-24T08:05:06Z,2022-03-24 08:05:06,k-Rater Reliability: The Correct Unit of Reliability for Aggregated Human Annotations,"Since the inception of crowdsourcing, aggregation has been a common strategy for dealing with unreliable data. Aggregate ratings are more reliable than individual ones. However, many natural language processing (NLP) applications that rely on aggregate ratings only report the reliability of individual ratings, which is the incorrect unit of analysis. In these instances, the data reliability is under-reported, and a proposed k-rater reliability (kRR) should be used as the correct data reliability for aggregated datasets. It is a multi-rater generalization of inter-rater reliability (IRR). We conducted two replications of the WordSim-353 benchmark, and present empirical, analytical, and bootstrap-based methods for computing kRR on WordSim-353. These methods produce very similar results. We hope this discussion will nudge researchers to report kRR in addition to IRR.",Ka Wong|Praveen Paritosh,,https://arxiv.org/abs/2203.12913v1,https://arxiv.org/pdf/2203.12913v1,,,,,cs.AI,cs.AI|stat.ML,https://arxiv.org/pdf/2203.12913v1.pdf
2203.11681v1,2022-03-19T07:18:26Z,2022-03-19 07:18:26,Discussion on a new extension of the FGM copula with application in reliability,"A new extended Farlie-Gumbel-Morgenstern copula recently studied by Ebaid et al. [Comm. Statist. Theory Methods, (2020)] is reviewed. The reported admissible range for the copula parameter a is incorrect and some typos are also found in their paper. Corrections to the admissible range of the copula parameter a and typos are presented here.",Ashok Kumar Pathak|Mohd. Arshad,,https://arxiv.org/abs/2203.11681v1,https://arxiv.org/pdf/2203.11681v1,,2 pages,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2203.11681v1.pdf
2203.03259v1,2022-03-07T10:26:05Z,2022-03-07 10:26:05,Predicting Bearings' Degradation Stages for Predictive Maintenance in the Pharmaceutical Industry,"In the pharmaceutical industry, the maintenance of production machines must be audited by the regulator. In this context, the problem of predictive maintenance is not when to maintain a machine, but what parts to maintain at a given point in time. The focus shifts from the entire machine to its component parts and prediction becomes a classification problem. In this paper, we focus on rolling-elements bearings and we propose a framework for predicting their degradation stages automatically. Our main contribution is a k-means bearing lifetime segmentation method based on high-frequency bearing vibration signal embedded in a latent low-dimensional subspace using an AutoEncoder. Given high-frequency vibration data, our framework generates a labeled dataset that is used to train a supervised model for bearing degradation stage detection. Our experimental results, based on the FEMTO Bearing dataset, show that our framework is scalable and that it provides reliable and actionable predictions for a range of different bearings.",Dovile Juodelyte|Veronika Cheplygina|Therese Graversen|Philippe Bonnet,,https://arxiv.org/abs/2203.03259v1,https://arxiv.org/pdf/2203.03259v1,https://doi.org/10.1145/3534678.3539057,Submitted to the KDD Applied Data Science track,,10.1145/3534678.3539057,stat.ML,stat.ML|cs.CV|cs.LG,https://arxiv.org/pdf/2203.03259v1.pdf
2203.02658v2,2022-03-05T04:57:20Z,2022-03-14 03:39:47,Koopman operator for time-dependent reliability analysis,"Time-dependent structural reliability analysis of nonlinear dynamical systems is non-trivial; subsequently, scope of most of the structural reliability analysis methods is limited to time-independent reliability analysis only. In this work, we propose a Koopman operator based approach for time-dependent reliability analysis of nonlinear dynamical systems. Since the Koopman representations can transform any nonlinear dynamical system into a linear dynamical system, the time evolution of dynamical systems can be obtained by Koopman operators seamlessly regardless of the nonlinear or chaotic behavior. Despite the fact that the Koopman theory has been in vogue a long time back, identifying intrinsic coordinates is a challenging task; to address this, we propose an end-to-end deep learning architecture that learns the Koopman observables and then use it for time marching the dynamical response. Unlike purely data-driven approaches, the proposed approach is robust even in the presence of uncertainties; this renders the proposed approach suitable for time-dependent reliability analysis. We propose two architectures; one suitable for time-dependent reliability analysis when the system is subjected to random initial condition and the other suitable when the underlying system have uncertainties in system parameters. The proposed approach is robust and generalizes to unseen environment (out-of-distribution prediction). Efficacy of the proposed approached is illustrated using three numerical examples. Results obtained indicate supremacy of the proposed approach as compared to purely data-driven auto-regressive neural network and long-short term memory network.",Navaneeth N.|Souvik Chakraborty,,https://arxiv.org/abs/2203.02658v2,https://arxiv.org/pdf/2203.02658v2,,,,,stat.ML,stat.ML|cs.LG|eess.SY,https://arxiv.org/pdf/2203.02658v2.pdf
2202.08566v1,2022-02-17T10:29:32Z,2022-02-17 10:29:32,Efficient and Reliable Probabilistic Interactive Learning with Structured Outputs,"In this position paper, we study interactive learning for structured output spaces, with a focus on active learning, in which labels are unknown and must be acquired, and on skeptical learning, in which the labels are noisy and may need relabeling. These scenarios require expressive models that guarantee reliable and efficient computation of probabilistic quantities to measure uncertainty. We identify conditions under which a class of probabilistic models -- which we denote CRISPs -- meet all of these conditions, thus delivering tractable computation of the above quantities while preserving expressiveness. Building on prior work on tractable probabilistic circuits, we illustrate how CRISPs enable robust and efficient active and skeptical learning in large structured output spaces.",Stefano Teso|Antonio Vergari,,https://arxiv.org/abs/2202.08566v1,https://arxiv.org/pdf/2202.08566v1,,Accepted at the AAAI-22 Workshop on Interactive Machine Learning,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2202.08566v1.pdf
2202.08107v3,2022-02-16T14:44:46Z,2022-04-20 12:58:44,Estimating Software Reliability Using Size-biased Modelling,"  Software reliability estimation is one of the most active areas of research in software testing. Since time between failures (TBF) has often been challenging to record, software testing data are commonly recorded as test-case-wise in a discrete set up. We have developed a Bayesian generalised linear mixed model (GLMM) based on software testing detection data and a size-biased strategy which not only estimates the software reliability, but also estimates the total number of bugs present in the software. Our approach provides a flexible, unified modelling framework and can be adopted to various real-life situations. We have assessed the performance of our model via simulation study and found that each of the key parameters could be estimated with a satisfactory level of accuracy. We have also applied our model to two empirical software testing data sets. While there can be other fields of study for application of our model (e.g., hydrocarbon exploration), we anticipate that our novel modelling approach to estimate software reliability could be very useful for the users and can potentially be a key tool in the field of software reliability estimation.",Soumen Dey|Ashis Kumar Chakraborty,,https://arxiv.org/abs/2202.08107v3,https://arxiv.org/pdf/2202.08107v3,,14 pages. This work has been submitted to the IEEE for possible publication,,,stat.AP,stat.AP,https://arxiv.org/pdf/2202.08107v3.pdf
2201.13145v1,2022-01-31T11:41:08Z,2022-01-31 11:41:08,Assessment of DeepONet for reliability analysis of stochastic nonlinear dynamical systems,"Time dependent reliability analysis and uncertainty quantification of structural system subjected to stochastic forcing function is a challenging endeavour as it necessitates considerable computational time. We investigate the efficacy of recently proposed DeepONet in solving time dependent reliability analysis and uncertainty quantification of systems subjected to stochastic loading. Unlike conventional machine learning and deep learning algorithms, DeepONet learns is a operator network and learns a function to function mapping and hence, is ideally suited to propagate the uncertainty from the stochastic forcing function to the output responses. We use DeepONet to build a surrogate model for the dynamical system under consideration. Multiple case studies, involving both toy and benchmark problems, have been conducted to examine the efficacy of DeepONet in time dependent reliability analysis and uncertainty quantification of linear and nonlinear dynamical systems. Results obtained indicate that the DeepONet architecture is accurate as well as efficient. Moreover, DeepONet posses zero shot learning capabilities and hence, a trained model easily generalizes to unseen and new environment with no further training.",Shailesh Garg|Harshit Gupta|Souvik Chakraborty,,https://arxiv.org/abs/2201.13145v1,https://arxiv.org/pdf/2201.13145v1,,21 pages,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2201.13145v1.pdf
2201.05666v1,2022-01-14T20:52:30Z,2022-01-14 20:52:30,Reliable Causal Discovery with Improved Exact Search and Weaker Assumptions,"Many of the causal discovery methods rely on the faithfulness assumption to guarantee asymptotic correctness. However, the assumption can be approximately violated in many ways, leading to sub-optimal solutions. Although there is a line of research in Bayesian network structure learning that focuses on weakening the assumption, such as exact search methods with well-defined score functions, they do not scale well to large graphs. In this work, we introduce several strategies to improve the scalability of exact score-based methods in the linear Gaussian setting. In particular, we develop a super-structure estimation method based on the support of inverse covariance matrix which requires assumptions that are strictly weaker than faithfulness, and apply it to restrict the search space of exact search. We also propose a local search strategy that performs exact search on the local clusters formed by each variable and its neighbors within two hops in the super-structure. Numerical experiments validate the efficacy of the proposed procedure, and demonstrate that it scales up to hundreds of nodes with a high accuracy.",Ignavier Ng|Yujia Zheng|Jiji Zhang|Kun Zhang,,https://arxiv.org/abs/2201.05666v1,https://arxiv.org/pdf/2201.05666v1,,NeurIPS 2021. The code is available at https://github.com/ignavierng/local-astar,,,cs.LG,cs.LG|stat.ME|stat.ML,https://arxiv.org/pdf/2201.05666v1.pdf
2201.02172v1,2022-01-06T18:35:56Z,2022-01-06 18:35:56,"Reliability Estimation of an Advanced Nuclear Fuel using Coupled Active Learning, Multifidelity Modeling, and Subset Simulation","Tristructural isotropic (TRISO)-coated particle fuel is a robust nuclear fuel and determining its reliability is critical for the success of advanced nuclear technologies. However, TRISO failure probabilities are small and the associated computational models are expensive. We used coupled active learning, multifidelity modeling, and subset simulation to estimate the failure probabilities of TRISO fuels using several 1D and 2D models. With multifidelity modeling, we replaced expensive high-fidelity (HF) model evaluations with information fusion from two low-fidelity (LF) models. For the 1D TRISO models, we considered three multifidelity modeling strategies: only Kriging, Kriging LF prediction plus Kriging correction, and deep neural network (DNN) LF prediction plus Kriging correction. While the results across these multifidelity modeling strategies compared satisfactorily, strategies employing information fusion from two LF models consistently called the HF model least often. Next, for the 2D TRISO model, we considered two multifidelity modeling strategies: DNN LF prediction plus Kriging correction (data-driven) and 1D TRISO LF prediction plus Kriging correction (physics-based). The physics-based strategy, as expected, consistently required the fewest calls to the HF model. However, the data-driven strategy had a lower overall simulation time since the DNN predictions are instantaneous, and the 1D TRISO model requires a non-negligible simulation time.",Somayajulu L. N. Dhulipala|Michael D. Shields|Promit Chakroborty|Wen Jiang|Benjamin W. Spencer|Jason D. Hales|Vincent M. Laboure|Zachary M. Prince|Chandrakanth Bolisetti|Yifeng Che,,https://arxiv.org/abs/2201.02172v1,https://arxiv.org/pdf/2201.02172v1,,,,,stat.AP,stat.AP|stat.CO|stat.ML,https://arxiv.org/pdf/2201.02172v1.pdf
2112.15246v1,2021-12-31T00:02:18Z,2021-12-31 00:02:18,When are Iterative Gaussian Processes Reliably Accurate?,"While recent work on conjugate gradient methods and Lanczos decompositions have achieved scalable Gaussian process inference with highly accurate point predictions, in several implementations these iterative methods appear to struggle with numerical instabilities in learning kernel hyperparameters, and poor test likelihoods. By investigating CG tolerance, preconditioner rank, and Lanczos decomposition rank, we provide a particularly simple prescription to correct these issues: we recommend that one should use a small CG tolerance ($ε\leq 0.01$) and a large root decomposition size ($r \geq 5000$). Moreover, we show that L-BFGS-B is a compelling optimizer for Iterative GPs, achieving convergence with fewer gradient updates.",Wesley J. Maddox|Sanyam Kapoor|Andrew Gordon Wilson,,https://arxiv.org/abs/2112.15246v1,https://arxiv.org/pdf/2112.15246v1,,ICML 2021 OPTML Workshop,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2112.15246v1.pdf
2112.13111v1,2021-12-24T17:26:07Z,2021-12-24 17:26:07,Measuring Quality of DNA Sequence Data via Degradation,"We propose and apply a novel paradigm for characterization of genome data quality, which quantifies the effects of intentional degradation of quality. The rationale is that the higher the initial quality, the more fragile the genome and the greater the effects of degradation. We demonstrate that this phenomenon is ubiquitous, and that quantified measures of degradation can be used for multiple purposes. We focus on identifying outliers that may be problematic with respect to data quality, but might also be true anomalies or even attempts to subvert the database.",Alan F. Karr|Jason Hauzel|Adam A. Porter|Marcel Schaefer,,https://arxiv.org/abs/2112.13111v1,https://arxiv.org/pdf/2112.13111v1,https://doi.org/10.1371/journal.pone.0271970,,,10.1371/journal.pone.0271970,stat.ML,stat.ML|cs.LG|stat.AP,https://arxiv.org/pdf/2112.13111v1.pdf
2112.03277v2,2021-12-06T16:30:43Z,2022-12-19 15:19:37,Automatic quality control framework for more reliable integration of machine learning-based image segmentation into medical workflows,"Machine learning algorithms underpin modern diagnostic-aiding software, which has proved valuable in clinical practice, particularly in radiology. However, inaccuracies, mainly due to the limited availability of clinical samples for training these algorithms, hamper their wider applicability, acceptance, and recognition amongst clinicians. We present an analysis of state-of-the-art automatic quality control (QC) approaches that can be implemented within these algorithms to estimate the certainty of their outputs. We validated the most promising approaches on a brain image segmentation task identifying white matter hyperintensities (WMH) in magnetic resonance imaging data. WMH are a correlate of small vessel disease common in mid-to-late adulthood and are particularly challenging to segment due to their varied size, and distributional patterns. Our results show that the aggregation of uncertainty and Dice prediction were most effective in failure detection for this task. Both methods independently improved mean Dice from 0.82 to 0.84. Our work reveals how QC methods can help to detect failed segmentation cases and therefore make automatic segmentation more reliable and suitable for clinical practice.",Elena Williams|Sebastian Niehaus|Janis Reinelt|Alberto Merola|Paul Glad Mihai|Kersten Villringer|Konstantin Thierbach|Evelyn Medawar|Daniel Lichterfeld|Ingo Roeder|Nico Scherf|Maria del C. Valdés Hernández,,https://arxiv.org/abs/2112.03277v2,https://arxiv.org/pdf/2112.03277v2,,19 pages,,,eess.IV,eess.IV|cs.AI|cs.CV|stat.ML,https://arxiv.org/pdf/2112.03277v2.pdf
2112.01594v1,2021-12-02T20:23:41Z,2021-12-02 20:23:41,On the Reliability of Multiple Systems Estimation for the Quantification of Modern Slavery,"The quantification of modern slavery has received increased attention recently as organizations have come together to produce global estimates, where multiple systems estimation (MSE) is often used to this end. Echoing a long-standing controversy, disagreements have re-surfaced regarding the underlying MSE assumptions, the robustness of MSE methodology, and the accuracy of MSE estimates in this application. Our goal is to help address and move past these controversies. To do so, we review MSE, its assumptions, and commonly used models for modern slavery applications. We introduce all of the publicly available modern slavery datasets in the literature, providing a reproducible analysis and highlighting current issues. Specifically, we utilize an internal consistency approach that constructs subsets of data for which ground truth is available, allowing us to evaluate the accuracy of MSE estimators. Next, we propose a characterization of the large sample bias of estimators as a function of misspecified assumptions. Then, we propose an alternative to traditional (e.g., bootstrap-based) assessments of reliability, which allows us to visualize trajectories of MSE estimates to illustrate the robustness of estimates. Finally, our complementary analyses are used to provide guidance regarding the application and reliability of MSE methodology.",Olivier Binette|Rebecca C. Steorts,,https://arxiv.org/abs/2112.01594v1,https://arxiv.org/pdf/2112.01594v1,,,"Journal of the Royal Statistical Society: Series A (Statistics in Society), 1 - 37 (2022)",,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2112.01594v1.pdf
2111.14368v2,2021-11-29T07:56:07Z,2022-03-09 20:24:04,A Multi-state Markov Model to Infer the Latent Deterioration Process From the Maintenance Effect on Reliability Engineering of Ships,"Maintenance optimization of naval ship equipment is crucial in terms of national defense. However, the mixed effect of the maintenance and the pure deterioration processes in the observed data hinders an exact comparison between candidate maintenance policies. That is, the observed data-annual failure counts of naval ships reflect counteracting actions between the maintenance and deterioration. The inference of the latent deteriorating process is needed in advance for choosing an optimal maintenance policy to be carried out. This study proposes a new framework for the separation of the true deterioration effect by predicting it from the current maintenance effect through the multi-state Markov model. Using an annual engine failure count of 99 ships in the Korean navy, we construct the framework consisting of imputation, transition matrix design, optimization, and validation. The hierarchical Gaussian process model is used for the imputation and the three-state Markov model is applied for the estimation of parameters in the deterioration and maintenance effect. To consider the natural (deterioration) and artificial (maintenance) effect respectively, the Bayesian HMM model with a categorical distribution is employed. Computational experiments under multiple settings showed the robustness of the estimated parameters, as well as an accurate recovery of the observed data, thereby confirming the credibility of our model. The framework could further be employed to establish a reliable maintenance system and to reduce an overall maintenance cost.",Hyunji Moon|Jungin Choi|Seoyeon Cha,,https://arxiv.org/abs/2111.14368v2,https://arxiv.org/pdf/2111.14368v2,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2111.14368v2.pdf
2111.05391v1,2021-11-09T20:00:14Z,2021-11-09 20:00:14,Statistical Perspectives on Reliability of Artificial Intelligence Systems,"Artificial intelligence (AI) systems have become increasingly popular in many areas. Nevertheless, AI technologies are still in their developing stages, and many issues need to be addressed. Among those, the reliability of AI systems needs to be demonstrated so that the AI systems can be used with confidence by the general public. In this paper, we provide statistical perspectives on the reliability of AI systems. Different from other considerations, the reliability of AI systems focuses on the time dimension. That is, the system can perform its designed functionality for the intended period. We introduce a so-called SMART statistical framework for AI reliability research, which includes five components: Structure of the system, Metrics of reliability, Analysis of failure causes, Reliability assessment, and Test planning. We review traditional methods in reliability data analysis and software reliability, and discuss how those existing methods can be transformed for reliability modeling and assessment of AI systems. We also describe recent developments in modeling and analysis of AI reliability and outline statistical research challenges in this area, including out-of-distribution detection, the effect of the training set, adversarial attacks, model accuracy, and uncertainty quantification, and discuss how those topics can be related to AI reliability, with illustrative examples. Finally, we discuss data collection and test planning for AI reliability assessment and how to improve system designs for higher AI reliability. The paper closes with some concluding remarks.",Yili Hong|Jiayi Lian|Li Xu|Jie Min|Yueyao Wang|Laura J. Freeman|Xinwei Deng,,https://arxiv.org/abs/2111.05391v1,https://arxiv.org/pdf/2111.05391v1,,40 pages,,,cs.SE,cs.SE|cs.AI|stat.AP,https://arxiv.org/pdf/2111.05391v1.pdf
2110.11896v1,2021-10-22T16:26:02Z,2021-10-22 16:26:02,Multimodel Bayesian Analysis of Load Duration Effects in Lumber Reliability,"This paper evaluates the reliability of lumber, accounting for the duration-of-load (DOL) effect under different load profiles based on a multimodel Bayesian approach. Three individual DOL models previously used for reliability assessment are considered: the US model, the Canadian model, and the Gamma process model. Procedures for stochastic generation of residential, snow, and wind loads are also described. We propose Bayesian model-averaging (BMA) as a method for combining the reliability estimates of individual models under a given load profile that coherently accounts for statistical uncertainty in the choice of model and parameter values. The method is applied to the analysis of a Hemlock experimental dataset, where the BMA results are illustrated via estimated reliability indices together with 95% interval bands.",Yunfeng Yang|Martin Lysy|Samuel W. K. Wong,,https://arxiv.org/abs/2110.11896v1,https://arxiv.org/pdf/2110.11896v1,,"15 pages, 2 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/2110.11896v1.pdf
2110.08882v1,2021-10-17T18:12:57Z,2021-10-17 18:12:57,Building Degradation Index with Variable Selection for Multivariate Sensory Data,"The modeling and analysis of degradation data have been an active research area in reliability and system health management. As the senor technology advances, multivariate sensory data are commonly collected for the underlying degradation process. However, most existing research on degradation modeling requires a univariate degradation index to be provided. Thus, constructing a degradation index for multivariate sensory data is a fundamental step in degradation modeling. In this paper, we propose a novel degradation index building method for multivariate sensory data. Based on an additive nonlinear model with variable selection, the proposed method can automatically select the most informative sensor signals to be used in the degradation index. The penalized likelihood method with adaptive group penalty is developed for parameter estimation. We demonstrate that the proposed method outperforms existing methods via both simulation studies and analyses of the NASA jet engine sensor data.",Yueyao Wang|I-Chen Lee|Yili Hong|Xinwei Deng,,https://arxiv.org/abs/2110.08882v1,https://arxiv.org/pdf/2110.08882v1,,28 pages,,,stat.AP,stat.AP,https://arxiv.org/pdf/2110.08882v1.pdf
2110.08331v1,2021-10-15T19:33:46Z,2021-10-15 19:33:46,A New Approach for Interpretability and Reliability in Clinical Risk Prediction: Acute Coronary Syndrome Scenario,"We intend to create a new risk assessment methodology that combines the best characteristics of both risk score and machine learning models. More specifically, we aim to develop a method that, besides having a good performance, offers a personalized model and outcome for each patient, presents high interpretability, and incorporates an estimation of the prediction reliability which is not usually available. By combining these features in the same approach we expect that it can boost the confidence of physicians to use such a tool in their daily activity. In order to achieve the mentioned goals, a three-step methodology was developed: several rules were created by dichotomizing risk factors; such rules were trained with a machine learning classifier to predict the acceptance degree of each rule (the probability that the rule is correct) for each patient; that information was combined and used to compute the risk of mortality and the reliability of such prediction. The methodology was applied to a dataset of patients admitted with any type of acute coronary syndromes (ACS), to assess the 30-days all-cause mortality risk. The performance was compared with state-of-the-art approaches: logistic regression (LR), artificial neural network (ANN), and clinical risk score model (Global Registry of Acute Coronary Events - GRACE). The proposed approach achieved testing results identical to the standard LR, but offers superior interpretability and personalization; it also significantly outperforms the GRACE risk model and the standard ANN model. The calibration curve also suggests a very good generalization ability of the obtained model as it approaches the ideal curve. Finally, the reliability estimation of individual predictions presented a great correlation with the misclassifications rate. Those properties may have a beneficial application in other clinical scenarios as well. [abridged]",Francisco Valente|Jorge Henriques|Simão Paredes|Teresa Rocha|Paulo de Carvalho|João Morais,,https://arxiv.org/abs/2110.08331v1,https://arxiv.org/pdf/2110.08331v1,https://doi.org/10.1016/j.artmed.2021.102113,Accepted for publication in the Artificial Intelligence in Medicine journal. Abstract abridged to respect the arXiv's characters limit,"Artificial Intelligence in Medicine, Volume 117, 2021",10.1016/j.artmed.2021.102113,cs.LG,cs.LG|stat.AP|stat.ME,https://arxiv.org/pdf/2110.08331v1.pdf
2110.07531v2,2021-10-14T16:50:37Z,2022-04-22 14:15:00,Deep learning models for predicting RNA degradation via dual crowdsourcing,"Messenger RNA-based medicines hold immense potential, as evidenced by their rapid deployment as COVID-19 vaccines. However, worldwide distribution of mRNA molecules has been limited by their thermostability, which is fundamentally limited by the intrinsic instability of RNA molecules to a chemical degradation reaction called in-line hydrolysis. Predicting the degradation of an RNA molecule is a key task in designing more stable RNA-based therapeutics. Here, we describe a crowdsourced machine learning competition (""Stanford OpenVaccine"") on Kaggle, involving single-nucleotide resolution measurements on 6043 102-130-nucleotide diverse RNA constructs that were themselves solicited through crowdsourcing on the RNA design platform Eterna. The entire experiment was completed in less than 6 months, and 41% of nucleotide-level predictions from the winning model were within experimental error of the ground truth measurement. Furthermore, these models generalized to blindly predicting orthogonal degradation data on much longer mRNA molecules (504-1588 nucleotides) with improved accuracy compared to previously published models. Top teams integrated natural language processing architectures and data augmentation techniques with predictions from previous dynamic programming models for RNA secondary structure. These results indicate that such models are capable of representing in-line hydrolysis with excellent accuracy, supporting their use for designing stabilized messenger RNAs. The integration of two crowdsourcing platforms, one for data set creation and another for machine learning, may be fruitful for other urgent problems that demand scientific discovery on rapid timescales.",Hannah K. Wayment-Steele|Wipapat Kladwang|Andrew M. Watkins|Do Soon Kim|Bojan Tunguz|Walter Reade|Maggie Demkin|Jonathan Romano|Roger Wellington-Oguri|John J. Nicol|Jiayang Gao|Kazuki Onodera|Kazuki Fujikawa|Hanfei Mao|Gilles Vandewiele|Michele Tinti|Bram Steenwinckel|Takuya Ito|Taiga Noumi|Shujun He|Keiichiro Ishi|Youhan Lee|Fatih Öztürk|Anthony Chiu|Emin Öztürk|Karim Amer|Mohamed Fares|Eterna Participants|Rhiju Das,,https://arxiv.org/abs/2110.07531v2,https://arxiv.org/pdf/2110.07531v2,,,,,stat.ML,stat.ML|cs.LG|physics.bio-ph|q-bio.BM,https://arxiv.org/pdf/2110.07531v2.pdf
2110.06114v1,2021-10-12T15:59:32Z,2021-10-12 15:59:32,Optimal Time Plan in Accelerated Degradation Testing,"Many highly reliable products are designed to function for years without failure. For such systems accelerated degradation testing may provide significance information about the reliability properties of the system. In this paper, we propose the $c$-optimality criterion for obtaining optimal designs of constant-stress accelerated degradation tests where the degradation path follow a linear mixed effects model. The present work is mainly concerned with developing optimal desings in terms of the time variable rather than considering an optimal design of the stress variable which is usually considered in the majority of the literature. Finally, numerical examples are presented and sensitivity analysis procedures are conducted for evaluating the robustness of the optimal as well as standard designs against misspecifications of the nominal values.",Helmi Shat|Rainer Schwabe,,https://arxiv.org/abs/2110.06114v1,https://arxiv.org/pdf/2110.06114v1,,arXiv admin note: substantial text overlap with arXiv:2102.09446,,,stat.AP,stat.AP,https://arxiv.org/pdf/2110.06114v1.pdf
2110.04137v2,2021-10-08T13:54:57Z,2022-04-19 01:27:00,A surrogate-based reliability analysis method of the motion of large flexible space structures,"Satellites and their instruments are subject to the motion stability throughout their lifetimes. The reliability of the large flexible space structures (LFSS) is particularly important for the motion stability of satellites and their instruments. In this paper, the reliability analysis of large flexible space structures is conducted based on Bayesian support vector regression (SVR). The kinematic model of a typical large flexible space structure is first established. Based on the kinematic model, the surrogate model of the motion of the large flexible space structure is then developed to further reduce the computational cost. Finally, the reliability analysis is conducted using the surrogate model. The proposed method shows high accuracy and efficiency for the reliability assessments of the typical large flexible space structure and can be further developed for other LFSS.",Dongyu Zhao,,https://arxiv.org/abs/2110.04137v2,https://arxiv.org/pdf/2110.04137v2,,The paper is not completed and should be revised,,,stat.AP,stat.AP,https://arxiv.org/pdf/2110.04137v2.pdf
2110.03321v2,2021-10-07T10:30:20Z,2022-05-12 14:45:56,Robustness and Reliability When Training With Noisy Labels,"Labelling of data for supervised learning can be costly and time-consuming and the risk of incorporating label noise in large data sets is imminent. When training a flexible discriminative model using a strictly proper loss, such noise will inevitably shift the solution towards the conditional distribution over noisy labels. Nevertheless, while deep neural networks have proven capable of fitting random labels, regularisation and the use of robust loss functions empirically mitigate the effects of label noise. However, such observations concern robustness in accuracy, which is insufficient if reliable uncertainty quantification is critical. We demonstrate this by analysing the properties of the conditional distribution over noisy labels for an input-dependent noise model. In addition, we evaluate the set of robust loss functions characterised by noise-insensitive, asymptotic risk minimisers. We find that strictly proper and robust loss functions both offer asymptotic robustness in accuracy, but neither guarantee that the final model is calibrated. Moreover, even with robust loss functions, overfitting is an issue in practice. With these results, we aim to explain observed robustness of common training practices, such as early stopping, to label noise. In addition, we aim to encourage the development of new noise-robust algorithms that not only preserve accuracy but that also ensure reliability.",Amanda Olmin|Fredrik Lindsten,,https://arxiv.org/abs/2110.03321v2,https://arxiv.org/pdf/2110.03321v2,,Accepted at AISTATS 2022,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2110.03321v2.pdf
2109.14688v1,2021-09-29T19:50:06Z,2021-09-29 19:50:06,Reliable Estimation of KL Divergence using a Discriminator in Reproducing Kernel Hilbert Space,"Estimating Kullback Leibler (KL) divergence from samples of two distributions is essential in many machine learning problems. Variational methods using neural network discriminator have been proposed to achieve this task in a scalable manner. However, we noted that most of these methods using neural network discriminators suffer from high fluctuations (variance) in estimates and instability in training. In this paper, we look at this issue from statistical learning theory and function space complexity perspective to understand why this happens and how to solve it. We argue that the cause of these pathologies is lack of control over the complexity of the neural network discriminator function and could be mitigated by controlling it. To achieve this objective, we 1) present a novel construction of the discriminator in the Reproducing Kernel Hilbert Space (RKHS), 2) theoretically relate the error probability bound of the KL estimates to the complexity of the discriminator in the RKHS space, 3) present a scalable way to control the complexity (RKHS norm) of the discriminator for a reliable estimation of KL divergence, and 4) prove the consistency of the proposed estimator. In three different applications of KL divergence : estimation of KL, estimation of mutual information and Variational Bayes, we show that by controlling the complexity as developed in the theory, we are able to reduce the variance of KL estimates and stabilize the training",Sandesh Ghimire|Aria Masoomi|Jennifer Dy,,https://arxiv.org/abs/2109.14688v1,https://arxiv.org/pdf/2109.14688v1,,"27 pages, 3 figures. arXiv admin note: text overlap with arXiv:2002.11187",Advances in Neural Information Processing Systems 2021,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2109.14688v1.pdf
2109.14526v2,2021-09-29T16:23:04Z,2023-03-13 03:55:43,On the reliability of published findings using the regression discontinuity design in political science,"The regression discontinuity (RD) design offers identification of causal effects under weak assumptions, earning it a position as a standard method in modern political science research. But identification does not necessarily imply that causal effects can be estimated accurately with limited data. In this paper, we highlight that estimation under the RD design involves serious statistical challenges and investigate how these challenges manifest themselves in the empirical literature in political science. We collect all RD-based findings published in top political science journals in the period 2009-2018. The distribution of published results exhibits pathological features; estimates tend to bunch just above the conventional level of statistical significance. A reanalysis of all studies with available data suggests that researcher discretion is not a major driver of these features. However, researchers tend to use inappropriate methods for inference, rendering standard errors artificially small. A retrospective power analysis reveals that most of these studies were underpowered to detect all but large effects. The issues we uncover, combined with well-documented selection pressures in academic publishing, cause concern that many published findings using the RD design may be exaggerated.",Drew Stommes|P. M. Aronow|Fredrik Sävje,,https://arxiv.org/abs/2109.14526v2,https://arxiv.org/pdf/2109.14526v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2109.14526v2.pdf
2109.10219v1,2021-09-21T14:42:58Z,2021-09-21 14:42:58,Adaptive Reliability Analysis for Multi-fidelity Models using a Collective Learning Strategy,"In many fields of science and engineering, models with different fidelities are available. Physical experiments or detailed simulations that accurately capture the behavior of the system are regarded as high-fidelity models with low model uncertainty, however, they are expensive to run. On the other hand, simplified physical experiments or numerical models are seen as low-fidelity models that are cheaper to evaluate. Although low-fidelity models are often not suitable for direct use in reliability analysis due to their low accuracy, they can offer information about the trend of the high-fidelity model thus providing the opportunity to explore the design space at a low cost. This study presents a new approach called adaptive multi-fidelity Gaussian process for reliability analysis (AMGPRA). Contrary to selecting training points and information sources in two separate stages as done in state-of-the-art mfEGRA method, the proposed approach finds the optimal training point and information source simultaneously using the novel collective learning function (CLF). CLF is able to assess the global impact of a candidate training point from an information source and it accommodates any learning function that satisfies a certain profile. In this context, CLF provides a new direction for quantifying the impact of new training points and can be easily extended with new learning functions to adapt to different reliability problems. The performance of the proposed method is demonstrated by three mathematical examples and one engineering problem concerning the wind reliability of transmission towers. It is shown that the proposed method achieves similar or higher accuracy with reduced computational costs compared to state-of-the-art single and multi-fidelity methods. A key application of AMGPRA is high-fidelity fragility modeling using complex and costly physics-based computational models.",Chi Zhang|Chaolin Song|Abdollah Shafieezadeh,,https://arxiv.org/abs/2109.10219v1,https://arxiv.org/pdf/2109.10219v1,https://doi.org/10.1016/j.strusafe.2021.102141,,"Structural Safety, Volume 94, January 2022, 102141",10.1016/j.strusafe.2021.102141,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2109.10219v1.pdf
2109.08213v2,2021-09-16T20:32:58Z,2022-10-13 03:32:17,Reliable Neural Networks for Regression Uncertainty Estimation,"While deep neural networks are highly performant and successful in a wide range of real-world problems, estimating their predictive uncertainty remains a challenging task. To address this challenge, we propose and implement a loss function for regression uncertainty estimation based on the Bayesian Validation Metric (BVM) framework while using ensemble learning. The proposed loss reproduces maximum likelihood estimation in the limiting case. A series of experiments on in-distribution data show that the proposed method is competitive with existing state-of-the-art methods. Experiments on out-of-distribution data show that the proposed method is robust to statistical change and exhibits superior predictive capability.",Tony Tohme|Kevin Vanslette|Kamal Youcef-Toumi,,https://arxiv.org/abs/2109.08213v2,https://arxiv.org/pdf/2109.08213v2,https://doi.org/10.1016/j.ress.2022.108811,,"Reliability Engineering & System Safety, January 2023, Volume 229, 108811",10.1016/j.ress.2022.108811,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2109.08213v2.pdf
2108.10828v2,2021-08-24T16:24:46Z,2021-09-05 16:06:51,Physics-Informed Deep Learning: A Promising Technique for System Reliability Assessment,"Considerable research has been devoted to deep learning-based predictive models for system prognostics and health management in the reliability and safety community. However, there is limited study on the utilization of deep learning for system reliability assessment. This paper aims to bridge this gap and explore this new interface between deep learning and system reliability assessment by exploiting the recent advances of physics-informed deep learning. Particularly, we present an approach to frame system reliability assessment in the context of physics-informed deep learning and discuss the potential value of physics-informed generative adversarial networks for the uncertainty quantification and measurement data incorporation in system reliability assessment. The proposed approach is demonstrated by three numerical examples involving a dual-processor computing system. The results indicate the potential value of physics-informed deep learning to alleviate computational challenges and combine measurement data and mathematical models for system reliability assessment.",Taotao Zhou|Enrique Lopez Droguett|Ali Mosleh,,https://arxiv.org/abs/2108.10828v2,https://arxiv.org/pdf/2108.10828v2,,"29 pages, 15 figures",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2108.10828v2.pdf
2108.03210v3,2021-08-06T17:21:48Z,2022-10-20 20:33:04,"Regression Diagnostics meets Forecast Evaluation: Conditional Calibration, Reliability Diagrams, and Coefficient of Determination","Model diagnostics and forecast evaluation are two sides of the same coin. A common principle is that fitted or predicted distributions ought to be calibrated or reliable, ideally in the sense of auto-calibration, where the outcome is a random draw from the posited distribution. For binary responses, this is the universal concept of reliability. For real-valued outcomes, a general theory of calibration has been elusive, despite a recent surge of interest in distributional regression and machine learning. We develop a framework rooted in probability theory, which gives rise to hierarchies of calibration, and applies to both predictive distributions and stand-alone point forecasts. In a nutshell, a prediction - distributional or single-valued - is conditionally T-calibrated if it can be taken at face value in terms of the functional T. Whenever T is defined via an identification function - as in the cases of threshold (non) exceedance probabilities, quantiles, expectiles, and moments - auto-calibration implies T-calibration. We introduce population versions of T-reliability diagrams and revisit a score decomposition into measures of miscalibration (MCB), discrimination (DSC), and uncertainty (UNC). In empirical settings, stable and efficient estimators of T-reliability diagrams and score components arise via nonparametric isotonic regression and the pool-adjacent-violators algorithm. For in-sample model diagnostics, we propose a universal coefficient of determination, $$\text{R}^\ast = \frac{\text{DSC}-\text{MCB}}{\text{UNC}},$$ that nests and reinterprets the classical $\text{R}^2$ in least squares (mean) regression and its natural analogue $\text{R}^1$ in quantile regression, yet applies to T-regression in general, with MCB $\geq 0$, DSC $\geq 0$, and $\text{R}^\ast \in [0,1]$ under modest conditions.",Tilmann Gneiting|Johannes Resin,,https://arxiv.org/abs/2108.03210v3,https://arxiv.org/pdf/2108.03210v3,https://doi.org/10.1214/23-EJS2180,,Electron. J. Stat. 17 (2023) 3226-3286,10.1214/23-EJS2180,stat.ME,stat.ME,https://arxiv.org/pdf/2108.03210v3.pdf
2107.11449v1,2021-07-23T20:26:14Z,2021-07-23 20:26:14,Applying Inter-rater Reliability and Agreement in Grounded Theory Studies in Software Engineering,"In recent years, the qualitative research on empirical software engineering that applies Grounded Theory is increasing. Grounded Theory (GT) is a technique for developing theory inductively e iteratively from qualitative data based on theoretical sampling, coding, constant comparison, memoing, and saturation, as main characteristics. Large or controversial GT studies may involve multiple researchers in collaborative coding, which requires a kind of rigor and consensus that an individual coder does not. Although many qualitative researchers reject quantitative measures in favor of other qualitative criteria, many others are committed to measuring consensus through Inter-Rater Reliability (IRR) and/or Inter-Rater Agreement (IRA) techniques to develop a shared understanding of the phenomenon being studied. However, there are no specific guidelines about how and when to apply IRR/IRA during the iterative process of GT, so researchers have been using ad hoc methods for years. This paper presents a process for systematically applying IRR/IRA in GT studies that meets the iterative nature of this qualitative research method, which is supported by a previous systematic literature review on applying IRR/RA in GT studies in software engineering. This process allows researchers to incrementally generate a theory while ensuring consensus on the constructs that support it and, thus, improving the rigor of qualitative research. This formalization helps researchers to apply IRR/IRA to GT studies when various raters are involved in coding. Measuring consensus among raters promotes communicability, transparency, reflexivity, replicability, and trustworthiness of the research.",Jessica Díaz|Jorge Pérez|Carolina Gallardo|Ángel González-Prieto,,https://arxiv.org/abs/2107.11449v1,https://arxiv.org/pdf/2107.11449v1,,"20 pages, 5 figures, 8 tables",,,cs.SE,cs.SE|stat.ME,https://arxiv.org/pdf/2107.11449v1.pdf
2107.08461v2,2021-07-18T14:37:07Z,2023-02-18 19:09:53,"Differentially Private Bayesian Neural Networks on Accuracy, Privacy and Reliability","Bayesian neural network (BNN) allows for uncertainty quantification in prediction, offering an advantage over regular neural networks that has not been explored in the differential privacy (DP) framework. We fill this important gap by leveraging recent development in Bayesian deep learning and privacy accounting to offer a more precise analysis of the trade-off between privacy and accuracy in BNN. We propose three DP-BNNs that characterize the weight uncertainty for the same network architecture in distinct ways, namely DP-SGLD (via the noisy gradient method), DP-BBP (via changing the parameters of interest) and DP-MC Dropout (via the model architecture). Interestingly, we show a new equivalence between DP-SGD and DP-SGLD, implying that some non-Bayesian DP training naturally allows for uncertainty quantification. However, the hyperparameters such as learning rate and batch size, can have different or even opposite effects in DP-SGD and DP-SGLD.
  Extensive experiments are conducted to compare DP-BNNs, in terms of privacy guarantee, prediction accuracy, uncertainty quantification, calibration, computation speed, and generalizability to network architecture. As a result, we observe a new tradeoff between the privacy and the reliability. When compared to non-DP and non-Bayesian approaches, DP-SGLD is remarkably accurate under strong privacy guarantee, demonstrating the great potential of DP-BNN in real-world tasks.",Qiyiwen Zhang|Zhiqi Bu|Kan Chen|Qi Long,,https://arxiv.org/abs/2107.08461v2,https://arxiv.org/pdf/2107.08461v2,,,,,cs.LG,cs.LG|cs.CR|stat.ML,https://arxiv.org/pdf/2107.08461v2.pdf
2107.07483v1,2021-07-15T17:36:24Z,2021-07-15 17:36:24,Personalized and Reliable Decision Sets: Enhancing Interpretability in Clinical Decision Support Systems,"In this study, we present a novel clinical decision support system and discuss its interpretability-related properties. It combines a decision set of rules with a machine learning scheme to offer global and local interpretability. More specifically, machine learning is used to predict the likelihood of each of those rules to be correct for a particular patient, which may also contribute to better predictive performances. Moreover, the reliability analysis of individual predictions is also addressed, contributing to further personalized interpretability. The combination of these several elements may be crucial to obtain the clinical stakeholders' trust, leading to a better assessment of patients' conditions and improvement of the physicians' decision-making.",Francisco Valente|Simão Paredes|Jorge Henriques,,https://arxiv.org/abs/2107.07483v1,https://arxiv.org/pdf/2107.07483v1,,Accepted to the ICML 2021 Workshop on Interpretable Machine Learning in Healthcare,,,stat.ME,stat.ME|cs.LG,https://arxiv.org/pdf/2107.07483v1.pdf
2107.03920v10,2021-07-08T15:52:18Z,2024-11-26 19:47:51,Likelihood-Free Frequentist Inference: Bridging Classical Statistics and Machine Learning for Reliable Simulator-Based Inference,"Many areas of science rely on simulators that implicitly encode intractable likelihood functions of complex systems. Classical statistical methods are poorly suited for these so-called likelihood-free inference (LFI) settings, especially outside asymptotic and low-dimensional regimes. At the same time, popular LFI methods - such as Approximate Bayesian Computation or more recent machine learning techniques - do not necessarily lead to valid scientific inference because they do not guarantee confidence sets with nominal coverage in general settings. In addition, LFI currently lacks practical diagnostic tools to check the actual coverage of computed confidence sets across the entire parameter space. In this work, we propose a modular inference framework that bridges classical statistics and modern machine learning to provide (i) a practical approach for constructing confidence sets with near finite-sample validity at any value of the unknown parameters, and (ii) interpretable diagnostics for estimating empirical coverage across the entire parameter space. We refer to this framework as likelihood-free frequentist inference (LF2I). Any method that defines a test statistic can leverage LF2I to create valid confidence sets and diagnostics without costly Monte Carlo or bootstrap samples at fixed parameter settings. We study two likelihood-based test statistics (ACORE and BFF) and demonstrate their performance on high-dimensional complex data. Code is available at https://github.com/lee-group-cmu/lf2i.",Niccolò Dalmasso|Luca Masserano|David Zhao|Rafael Izbicki|Ann B. Lee,,https://arxiv.org/abs/2107.03920v10,https://arxiv.org/pdf/2107.03920v10,,"46 pages, 8 figures, code available at https://github.com/lee-group-cmu/lf2i, supplementary material available at https://lucamasserano.github.io/data/LF2I_supplementary_material.pdf",Electronic Journal of Statistics Vol. 18 | No. 2 (2024),,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2107.03920v10.pdf
2107.07881v2,2021-07-04T17:05:42Z,2021-08-18 13:19:25,Estimation of Li-ion degradation test sample sizes required to understand cell-to-cell variability,"Ageing of lithium-ion batteries results in irreversible reduction in performance. Intrinsic variability between cells, caused by manufacturing differences, occurs throughout life and increases with age. Researchers need to know the minimum number of cells they should test to give an accurate representation of population variability, since testing many cells is expensive. In this paper, empirical capacity versus time ageing models were fitted to various degradation datasets for commercially available cells assuming the model parameters could be drawn from a larger population distribution. Using a hierarchical Bayesian approach, we estimated the number of cells required to be tested. Depending on the complexity, ageing models with 1, 2 or 3 parameters respectively required data from at least 9, 11 or 13 cells for a consistent fit. This implies researchers will need to test at least these numbers of cells at each test point in their experiment to capture manufacturing variability.",Philipp Dechent|Samuel Greenbank|Felix Hildenbrand|Saad Jbabdi|Dirk Uwe Sauer|David A. Howey,,https://arxiv.org/abs/2107.07881v2,https://arxiv.org/pdf/2107.07881v2,https://doi.org/10.1002/batt.202100148,"13 pages, 9 figures",,10.1002/batt.202100148,stat.AP,stat.AP|q-bio.QM,https://arxiv.org/pdf/2107.07881v2.pdf
2106.14345v2,2021-06-28T00:21:05Z,2024-08-08 14:43:13,"More on verification of probability forecasts for football outcomes: score decompositions, reliability, and discrimination analyses","Forecast of football outcomes in terms of Home Win, Draw and Away Win relies largely on ex ante probability elicitation of these events and ex post verification of them via computation of probability scoring rules (Brier, Ranked Probability, Logarithmic, Zero-One scores). Usually, appraisal of the quality of forecasting procedures is restricted to reporting mean score values. The purpose of this article is to propose additional tools of verification, such as score decompositions into several components of special interest. Graphical and numerical diagnoses of reliability and discrimination and kindred statistical methods are presented using different techniques of binning (fixed thresholds, quantiles, logistic and iso regression). These procedures are illustrated on probability forecasts for the outcomes of the UEFA Champions League (C1) at the end of the group stage based on typical Poisson regression models with reasonably good results in terms of reliability as compared to those obtained from bookmaker odds and whatever the technique used. Links with research in machine learning and different areas of application (meteorology, medicine) are discussed.",Jean-Louis Foulley,,https://arxiv.org/abs/2106.14345v2,https://arxiv.org/pdf/2106.14345v2,,"10 pages, 2 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/2106.14345v2.pdf
2106.13540v1,2021-06-25T10:20:34Z,2021-06-25 10:20:34,Optimal Accelerated Degradation Testing Based on Bivariate Gamma Process with Dependent Components,"Accelerated degradation testing (ADT) is one of the major approaches in reliability engineering which allows accurate estimation of reliability characteristics of highly reliable systems within a relatively short time. The testing data are extrapolated through a physically reasonable statistical model to obtain estimates of lifetime quantiles at normal use conditions. The Gamma process is a natural model for degradation, which exhibits a monotone and strictly increasing degradation path. In this work, optimal experimental designs are derived for ADT with two response components. We consider the situations of independent as well as dependent marginal responses where the observational times are assumed to be fixed and known. The marginal degradation paths are assumed to follow a Gamma process where a copula function is utilized to express the dependence between both components. For the case of independent response components the optimal design minimizes the asymptotic variance of an estimated quantile of the failure time distribution at the normal use conditions. For the case of dependent response components the $D$-criterion is adopted to derive $D$-optimal designs. Further, $D$- and $c$-optimal designs are developed when the copula-based models are reduced to bivariate binary outcomes.",Helmi Shat|Norbert Gaffke,,https://arxiv.org/abs/2106.13540v1,https://arxiv.org/pdf/2106.13540v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2106.13540v1.pdf
2106.09379v2,2021-06-17T10:52:25Z,2021-09-22 14:34:43,Optimal Design of Stress Levels in Accelerated Degradation Testing for Multivariate Linear Degradation Models,"In recent years, more attention has been paid prominently to accelerated degradation testing in order to characterize accurate estimation of reliability properties for systems that are designed to work properly for years of even decades. %In this regard, degradation data from particular testing levels of the stress variable(s) are extrapolated with an appropriate statistical model to obtain estimates of lifetime quantiles at normal use levels. In this paper we propose optimal experimental designs for repeated measures accelerated degradation tests with competing failure modes that correspond to multiple response components. The observation time points are assumed to be fixed and known in advance. The marginal degradation paths are expressed using linear mixed effects models. The optimal design is obtained by minimizing the asymptotic variance of the estimator of some quantile of the failure time distribution at the normal use conditions. Numerical examples are introduced to ensure the robustness of the proposed optimal designs and compare their efficiency with standard experimental designs.",Helmi Shat,,https://arxiv.org/abs/2106.09379v2,https://arxiv.org/pdf/2106.09379v2,,arXiv admin note: substantial text overlap with arXiv:2102.09446,,,stat.AP,stat.AP,https://arxiv.org/pdf/2106.09379v2.pdf
2106.06669v3,2021-06-12T02:42:51Z,2021-10-27 15:05:12,Spatial Bayesian GLM on the cortical surface produces reliable task activations in individuals and groups,"The general linear model (GLM) is a widely popular and convenient tool for estimating the functional brain response and identifying areas of significant activation during a task or stimulus. However, the classical GLM is based on a massive univariate approach that does not explicitly leverage the similarity of activation patterns among neighboring brain locations. As a result, it tends to produce noisy estimates and be underpowered to detect significant activations, particularly in individual subjects and small groups. A recent alternative, a cortical surface-based spatial Bayesian GLM, leverages spatial dependencies among neighboring cortical vertices to produce more accurate estimates and areas of functional activation. The spatial Bayesian GLM can be applied to individual and group-level analysis. In this study, we assess the reliability and power of individual and group-average measures of task activation produced via the surface-based spatial Bayesian GLM. We analyze motor task data from 45 subjects in the Human Connectome Project (HCP) and HCP Retest datasets. We also extend the model to multi-run analysis and employ subject-specific cortical surfaces rather than surfaces inflated to a sphere for more accurate distance-based modeling. Results show that the surface-based spatial Bayesian GLM produces highly reliable activations in individual subjects and is powerful enough to detect trait-like functional topologies. Additionally, spatial Bayesian modeling enhances reliability of group-level analysis even in moderately sized samples (n=45). The power of the spatial Bayesian GLM to detect activations above a scientifically meaningful effect size is nearly invariant to sample size, exhibiting high power even in small samples (n=10). The spatial Bayesian GLM is computationally efficient in individuals and groups and is convenient to implement with the open-source BayesfMRI R package.",Daniel Spencer| Yu| Yue|David Bolin|Sarah Ryan|Amanda F. Mejia,Ryan|Ryan||||,https://arxiv.org/abs/2106.06669v3,https://arxiv.org/pdf/2106.06669v3,,"44 pages, 24 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/2106.06669v3.pdf
2106.07393v1,2021-06-11T16:15:46Z,2021-06-11 16:15:46,Cross-replication Reliability -- An Empirical Approach to Interpreting Inter-rater Reliability,"We present a new approach to interpreting IRR that is empirical and contextualized. It is based upon benchmarking IRR against baseline measures in a replication, one of which is a novel cross-replication reliability (xRR) measure based on Cohen's kappa. We call this approach the xRR framework. We opensource a replication dataset of 4 million human judgements of facial expressions and analyze it with the proposed framework. We argue this framework can be used to measure the quality of crowdsourced datasets.",Ka Wong|Praveen Paritosh|Lora Aroyo,,https://arxiv.org/abs/2106.07393v1,https://arxiv.org/pdf/2106.07393v1,,,,,stat.AP,stat.AP|cs.AI|cs.SI,https://arxiv.org/pdf/2106.07393v1.pdf
2106.03195v2,2021-06-06T18:07:49Z,2022-01-11 14:56:11,Meta-Learning Reliable Priors in the Function Space,"When data are scarce meta-learning can improve a learner's accuracy by harnessing previous experience from related learning tasks. However, existing methods have unreliable uncertainty estimates which are often overconfident. Addressing these shortcomings, we introduce a novel meta-learning framework, called F-PACOH, that treats meta-learned priors as stochastic processes and performs meta-level regularization directly in the function space. This allows us to directly steer the probabilistic predictions of the meta-learner towards high epistemic uncertainty in regions of insufficient meta-training data and, thus, obtain well-calibrated uncertainty estimates. Finally, we showcase how our approach can be integrated with sequential decision making, where reliable uncertainty quantification is imperative. In our benchmark study on meta-learning for Bayesian Optimization (BO), F-PACOH significantly outperforms all other meta-learners and standard baselines.",Jonas Rothfuss|Dominique Heyn|Jinfan Chen|Andreas Krause,,https://arxiv.org/abs/2106.03195v2,https://arxiv.org/pdf/2106.03195v2,,In Advances of Neural Information Processing Systems (NeurIPS) 2021,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2106.03195v2.pdf
2106.01713v2,2021-06-03T09:33:59Z,2022-02-07 17:30:19,"Active learning for structural reliability: survey, general framework and benchmark","Active learning methods have recently surged in the literature due to their ability to solve complex structural reliability problems within an affordable computational cost. These methods are designed by adaptively building an inexpensive surrogate of the original limit-state function. Examples of such surrogates include Gaussian process models which have been adopted in many contributions, the most popular ones being the efficient global reliability analysis (EGRA) and the active Kriging Monte Carlo simulation (AK-MCS), two milestone contributions in the field. In this paper, we first conduct a survey of the recent literature, showing that most of the proposed methods actually span from modifying one or more aspects of the two aforementioned methods. We then propose a generalized modular framework to build on-the-fly efficient active learning strategies by combining the following four ingredients or modules: surrogate model, reliability estimation algorithm, learning function and stopping criterion. Using this framework, we devise 39 strategies for the solution of $20$ reliability benchmark problems. The results of this extensive benchmark (more than $12,000$ reliability problems solved) are analyzed under various criteria leading to a synthesized set of recommendations for practitioners. These may be refined with a priori knowledge about the feature of the problem to solve, i.e. dimensionality and magnitude of the failure probability. This benchmark has eventually highlighted the importance of using surrogates in conjunction with sophisticated reliability estimation algorithms as a way to enhance the efficiency of the latter.",M. Moustapha|S. Marelli|B. Sudret,,https://arxiv.org/abs/2106.01713v2,https://arxiv.org/pdf/2106.01713v2,https://doi.org/10.1016/j.strusafe.2021.102174,,"Structural Safety, 102174 (2022)",10.1016/j.strusafe.2021.102174,stat.CO,stat.CO|stat.AP|stat.ML,https://arxiv.org/pdf/2106.01713v2.pdf
2105.11357v2,2021-05-24T15:41:15Z,2021-10-24 20:17:54,Entropy-based adaptive design for contour finding and estimating reliability,"In reliability analysis, methods used to estimate failure probability are often limited by the costs associated with model evaluations. Many of these methods, such as multifidelity importance sampling (MFIS), rely upon a computationally efficient, surrogate model like a Gaussian process (GP) to quickly generate predictions. The quality of the GP fit, particularly in the vicinity of the failure region(s), is instrumental in supplying accurately predicted failures for such strategies. We introduce an entropy-based GP adaptive design that, when paired with MFIS, provides more accurate failure probability estimates and with higher confidence. We show that our greedy data acquisition strategy better identifies multiple failure regions compared to existing contour-finding schemes. We then extend the method to batch selection, without sacrificing accuracy. Illustrative examples are provided on benchmark data as well as an application to an impact damage simulator for National Aeronautics and Space Administration (NASA) spacesuits.",D. Austin Cole|Robert B. Gramacy|James E. Warner|Geoffrey F. Bomarito|Patrick E. Leser|William P. Leser,,https://arxiv.org/abs/2105.11357v2,https://arxiv.org/pdf/2105.11357v2,,"28 pages, 11 figures",,,stat.ME,stat.ME|stat.ML,https://arxiv.org/pdf/2105.11357v2.pdf
2105.05373v1,2021-05-12T00:12:13Z,2021-05-12 00:12:13,Estimation of population size based on capture recapture designs and evaluation of the estimation reliability,"We propose a modern method to estimate population size based on capture-recapture designs of K samples. The observed data is formulated as a sample of n i.i.d. K-dimensional vectors of binary indicators, where the k-th component of each vector indicates the subject being caught by the k-th sample, such that only subjects with nonzero capture vectors are observed. The target quantity is the unconditional probability of the vector being nonzero across both observed and unobserved subjects. We cover models assuming a single constraint (identification assumption) on the K-dimensional distribution such that the target quantity is identified and the statistical model is unrestricted. We present solutions for linear and non-linear constraints commonly assumed to identify capture-recapture models, including no K-way interaction in linear and log-linear models, independence or conditional independence. We demonstrate that the choice of constraint has a dramatic impact on the value of the estimand, showing that it is crucial that the constraint is known to hold by design. For the commonly assumed constraint of no K-way interaction in a log-linear model, the statistical target parameter is only defined when each of the $2^K - 1$ observable capture patterns is present, and therefore suffers from the curse of dimensionality. We propose a targeted MLE based on undersmoothed lasso model to smooth across the cells while targeting the fit towards the single valued target parameter of interest. For each identification assumption, we provide simulated inference and confidence intervals to assess the performance on the estimator under correct and incorrect identifying assumptions. We apply the proposed method, alongside existing estimators, to estimate prevalence of a parasitic infection using multi-source surveillance data from a region in southwestern China, under the four identification assumptions.",Yue You|Mark van der Laan|Philip Collender|Qu Cheng|Alan Hubbard|Nicholas P Jewell|Zhiyue Tom Hu|Robin Mejia|Justin Remais,,https://arxiv.org/abs/2105.05373v1,https://arxiv.org/pdf/2105.05373v1,,,,,math.ST,math.ST|stat.ME|stat.ML,https://arxiv.org/pdf/2105.05373v1.pdf
2105.04979v2,2021-05-11T12:29:01Z,2021-05-12 14:14:40,Surrogate assisted active subspace and active subspace assisted surrogate -- A new paradigm for high dimensional structural reliability analysis,"Performing reliability analysis on complex systems is often computationally expensive. In particular, when dealing with systems having high input dimensionality, reliability estimation becomes a daunting task. A popular approach to overcome the problem associated with time-consuming and expensive evaluations is building a surrogate model. However, these computationally efficient models often suffer from the curse of dimensionality. Hence, training a surrogate model for high-dimensional problems is not straightforward. Henceforth, this paper presents a framework for solving high-dimensional reliability analysis problems. The basic premise is to train the surrogate model on a low-dimensional manifold, discovered using the active subspace algorithm. However, learning the low-dimensional manifold using active subspace is non-trivial as it requires information on the gradient of the response variable. To address this issue, we propose using sparse learning algorithms in conjunction with the active subspace algorithm; the resulting algorithm is referred to as the sparse active subspace (SAS) algorithm. We project the high-dimensional inputs onto the identified low-dimensional manifold identified using SAS. A high-fidelity surrogate model is used to map the inputs on the low-dimensional manifolds to the output response. We illustrate the efficacy of the proposed framework by using three benchmark reliability analysis problems from the literature. The results obtained indicate the accuracy and efficiency of the proposed approach compared to already established reliability analysis methods in the literature.",Navaneeth N.|Souvik Chakraborty,,https://arxiv.org/abs/2105.04979v2,https://arxiv.org/pdf/2105.04979v2,https://doi.org/10.1016/j.cma.2021.114374,19 pages,,10.1016/j.cma.2021.114374,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2105.04979v2.pdf
2105.02625v1,2021-05-06T13:01:42Z,2021-05-06 13:01:42,Evaluating the Effect of Longitudinal Dose and INR Data on Maintenance Warfarin Dose Predictions,"Warfarin, a commonly prescribed drug to prevent blood clots, has a highly variable individual response. Determining a maintenance warfarin dose that achieves a therapeutic blood clotting time, as measured by the international normalized ratio (INR), is crucial in preventing complications. Machine learning algorithms are increasingly being used for warfarin dosing; usually, an initial dose is predicted with clinical and genotype factors, and this dose is revised after a few days based on previous doses and current INR. Since a sequence of prior doses and INR better capture the variability in individual warfarin response, we hypothesized that longitudinal dose response data will improve maintenance dose predictions. To test this hypothesis, we analyzed a dataset from the COAG warfarin dosing study, which includes clinical data, warfarin doses and INR measurements over the study period, and maintenance dose when therapeutic INR was achieved. Various machine learning regression models to predict maintenance warfarin dose were trained with clinical factors and dosing history and INR data as features. Overall, dose revision algorithms with a single dose and INR achieved comparable performance as the baseline dose revision algorithm. In contrast, dose revision algorithms with longitudinal dose and INR data provided maintenance dose predictions that were statistically significantly much closer to the true maintenance dose. Focusing on the best performing model, gradient boosting (GB), the proportion of ideal estimated dose, i.e., defined as within $\pm$20% of the true dose, increased from the baseline (54.92%) to the GB model with the single (63.11%) and longitudinal (75.41%) INR. More accurate maintenance dose predictions with longitudinal dose response data can potentially achieve therapeutic INR faster, reduce drug-related complications and improve patient outcomes with warfarin therapy.",Anish Karpurapu|Adam Krekorian|Ye Tian|Leslie M. Collins|Ravi Karra|Aaron Franklin|Boyla O. Mainsah,,https://arxiv.org/abs/2105.02625v1,https://arxiv.org/pdf/2105.02625v1,,,,,cs.LG,cs.LG|stat.AP,https://arxiv.org/pdf/2105.02625v1.pdf
2104.05449v3,2021-04-09T16:48:04Z,2023-01-25 15:30:18,Current Overview of Statistical Fiber Bundles Model and Its Application to Physics-based Reliability Analysis of Thin-film Dielectrics,"In this paper, we present a critical overview of statistical fiber bundles models. We discuss relevant aspects, like assumptions and consequences stemming from models in the literature and propose new ones. This is accomplished by concentrating on both the physical and statistical aspects of a specific load-sharing example, the breakdown (BD) for circuits of capacitors and related dielectrics. For series and parallel/series circuits (series/parallel reliability systems) of ordinary capacitors, the load-sharing rules are derived from the electrical laws. This with the BD formalism is then used to obtain the BD distribution of the circuit. The BD distribution and Gibbs measure are given for a series circuit and the size effects are illustrated for simulations of series and parallel/series circuits. This is related to the finite weakest link adjustments for the BD distribution that arise in large series/parallel reliability load-sharing systems, such as dielectric BD, from their extreme value approximations.
  An elementary but in-depth discussion of the physical aspects of SiO$_2$ and HfO$_2$ dielectrics and cell models is given. This is used to study a load-sharing cell model for the BD of HfO$_2$ dielectrics and the BD formalism. The latter study is based on an analysis of Kim and Lee (2004)'s data for such dielectrics. Here, several BD distributions are compared in the analysis and proportional hazard regression models are used to study the BD formalism. In addition, some areas of open research are discussed.",James U. Gleaton|David Han|James D. Lynch|Hon Keung Tony Ng|Fabrizio Ruggeri,,https://arxiv.org/abs/2104.05449v3,https://arxiv.org/pdf/2104.05449v3,,The majority of the materials in the paper has been published as a book,,,cond-mat.mtrl-sci,cond-mat.mtrl-sci|physics.data-an|stat.AP,https://arxiv.org/pdf/2104.05449v3.pdf
2104.03613v1,2021-04-08T08:50:44Z,2021-04-08 08:50:44,Uncertainty-aware Remaining Useful Life predictor,"Remaining Useful Life (RUL) estimation is the problem of inferring how long a certain industrial asset can be expected to operate within its defined specifications. Deploying successful RUL prediction methods in real-life applications is a prerequisite for the design of intelligent maintenance strategies with the potential of drastically reducing maintenance costs and machine downtimes. In light of their superior performance in a wide range of engineering fields, Machine Learning (ML) algorithms are natural candidates to tackle the challenges involved in the design of intelligent maintenance systems. In particular, given the potentially catastrophic consequences or substantial costs associated with maintenance decisions that are either too late or too early, it is desirable that ML algorithms provide uncertainty estimates alongside their predictions. However, standard data-driven methods used for uncertainty estimation in RUL problems do not scale well to large datasets or are not sufficiently expressive to model the high-dimensional mapping from raw sensor data to RUL estimates. In this work, we consider Deep Gaussian Processes (DGPs) as possible solutions to the aforementioned limitations. We perform a thorough evaluation and comparison of several variants of DGPs applied to RUL predictions. The performance of the algorithms is evaluated on the N-CMAPSS (New Commercial Modular Aero-Propulsion System Simulation) dataset from NASA for aircraft engines. The results show that the proposed methods are able to provide very accurate RUL predictions along with sensible uncertainty estimates, providing more reliable solutions for (safety-critical) real-life industrial applications.",Luca Biggio|Alexander Wieland|Manuel Arias Chao|Iason Kastanis|Olga Fink,,https://arxiv.org/abs/2104.03613v1,https://arxiv.org/pdf/2104.03613v1,,,,,cs.LG,cs.LG|cs.AI|stat.AP,https://arxiv.org/pdf/2104.03613v1.pdf
2103.14593v1,2021-03-23T20:17:53Z,2021-03-23 20:17:53,Reliability of MST identification in correlation-based market networks,"Maximum spanning tree (MST) is a popular tool in market network analysis. Large number of publications are devoted to the MST calculation and it's interpretation for particular stock markets. However, much less attention is payed in the literature to the analysis of uncertainty of obtained results. In the present paper we suggest a general framework to measure uncertainty of MST identification. We study uncertainty in the framework of the concept of random variable network (RVN). We consider different correlation based networks in the large class of elliptical distributions. We show that true MST is the same in three networks: Pearson correlation network, Fechner correlation network, and Kendall correlation network. We argue that among different measures of uncertainty the FDR (False Discovery Rate) is the most appropriated for MST identification. We investigate FDR of Kruskal algorithm for MST identification and show that reliability of MST identification is different in these three networks. In particular, for Pearson correlation network the FDR essentially depends on distribution of stock returns. We prove that for market network with Fechner correlation the FDR is non sensitive to the assumption on stock's return distribution. Some interesting phenomena are discovered for Kendall correlation network. Our experiments show that FDR of Kruskal algorithm for MST identification in Kendall correlation network weakly depend on distribution and at the same time the value of FDR is almost the best in comparison with MST identification in other networks. These facts are important in practical applications.",V. A. Kalyagin|A. P. Koldanov|P. A. Koldanov,,https://arxiv.org/abs/2103.14593v1,https://arxiv.org/pdf/2103.14593v1,https://doi.org/10.1016/j.physa.2022.127482,,,10.1016/j.physa.2022.127482,q-fin.ST,q-fin.ST|stat.ME,https://arxiv.org/pdf/2103.14593v1.pdf
2103.11598v1,2021-03-22T06:00:42Z,2021-03-22 06:00:42,Adaptive Degradation Process with Deep Learning-Driven Trajectory,"Remaining useful life (RUL) estimation is a crucial component in the implementation of intelligent predictive maintenance and health management. Deep neural network (DNN) approaches have been proven effective in RUL estimation due to their capacity in handling high-dimensional non-linear degradation features. However, the applications of DNN in practice face two challenges: (a) online update of lifetime information is often unavailable, and (b) uncertainties in predicted values may not be analytically quantified. This paper addresses these issues by developing a hybrid DNN-based prognostic approach, where a Wiener-based-degradation model is enhanced with adaptive drift to characterize the system degradation. An LSTM-CNN encoder-decoder is developed to predict future degradation trajectories by jointly learning noise coefficients as well as drift coefficients, and adaptive drift is updated via Bayesian inference. A computationally efficient algorithm is proposed for the calculation of RUL distributions. Numerical experiments are presented using turbofan engines degradation data to demonstrate the superior accuracy of RUL prediction of our proposed approach.",Li Yang,,https://arxiv.org/abs/2103.11598v1,https://arxiv.org/pdf/2103.11598v1,,This work will be submitted to the IEEE for possible publication,,,stat.ML,stat.ML|cs.LG|stat.AP,https://arxiv.org/pdf/2103.11598v1.pdf
2103.02774v2,2021-03-04T00:47:24Z,2023-07-14 20:11:49,Non-Asymptotic Guarantees for Reliable Identification of Granger Causality via the LASSO,"Granger causality is among the widely used data-driven approaches for causal analysis of time series data with applications in various areas including economics, molecular biology, and neuroscience. Two of the main challenges of this methodology are: 1) over-fitting as a result of limited data duration, and 2) correlated process noise as a confounding factor, both leading to errors in identifying the causal influences. Sparse estimation via the LASSO has successfully addressed these challenges for parameter estimation. However, the classical statistical tests for Granger causality resort to asymptotic analysis of ordinary least squares, which require long data duration to be useful and are not immune to confounding effects. In this work, we address this disconnect by introducing a LASSO-based statistic and studying its non-asymptotic properties under the assumption that the true models admit sparse autoregressive representations. We establish fundamental limits for reliable identification of Granger causal influences using the proposed LASSO-based statistic. We further characterize the false positive error probability and test power of a simple thresholding rule for identifying Granger causal effects and provide two methods to set the threshold in a data-driven fashion. We present simulation studies and application to real data to compare the performance of our proposed method to ordinary least squares and existing LASSO-based methods in detecting Granger causal influences, which corroborate our theoretical results.",Proloy Das|Behtash Babadi,,https://arxiv.org/abs/2103.02774v2,https://arxiv.org/pdf/2103.02774v2,https://doi.org/10.1109/TIT.2023.3296336,,,10.1109/TIT.2023.3296336,stat.ME,stat.ME|cs.IT,https://arxiv.org/pdf/2103.02774v2.pdf
2102.09446v2,2021-02-18T16:08:41Z,2021-10-12 15:53:16,Experimental Designs for Accelerated Degradation Tests Based on Linear Mixed Effects Models,"Accelerated degradation tests are used to provide accurate estimation of lifetime properties of highly reliable products within a relatively short testing time. There data from particular tests at high levels of stress (e.\,g.\ temperature, voltage, or vibration) are extrapolated, through a physically meaningful model, to obtain estimates of lifetime quantiles under normal use conditions. In this work, we consider repeated measures accelerated degradation tests with multiple stress variables, where the degradation paths are assumed to follow a linear mixed effects model which is quite common in settings when repeated measures are made. We derive optimal experimental designs for minimizing the asymptotic variance for estimating the median failure time under normal use conditions when the time points for measurements are either fixed in advance or are also to be optimized.",Helmi Shat|Rainer Schwabe,,https://arxiv.org/abs/2102.09446v2,https://arxiv.org/pdf/2102.09446v2,,arXiv admin note: text overlap with arXiv:2106.09379,,,stat.AP,stat.AP,https://arxiv.org/pdf/2102.09446v2.pdf
2102.08111v2,2021-02-16T12:26:23Z,2021-11-29 13:01:27,Multivariable Fractional Polynomials for lithium-ion batteries degradation models under dynamic conditions,"Longevity and safety of lithium-ion batteries are facilitated by efficient monitoring and adjustment of the battery operating conditions. Hence, it is crucial to implement fast and accurate algorithms for State of Health (SoH) monitoring on the Battery Management System. The task is challenging due to the complexity and multitude of the factors contributing to the battery degradation, especially because the different degradation processes occur at various timescales and their interactions play an important role. Data-driven methods bypass this issue by approximating the complex processes with statistical or machine learning models. This paper proposes a data-driven approach which is understudied in the context of battery degradation, despite its simplicity and ease of computation: the Multivariable Fractional Polynomial (MFP) regression. Models are trained from historical data of one exhausted cell and used to predict the SoH of other cells. The data are characterised by varying loads simulating dynamic operating conditions. Two hypothetical scenarios are considered: one assumes that a recent capacity measurement is known, the other is based only on the nominal capacity. It was shown that the degradation behaviour of the batteries under examination is influenced by their historical data, as supported by the low prediction errors achieved (root mean squared errors from 1.2% to 7.22% when considering data up to the battery End of Life). Moreover, we offer a multi-factor perspective where the degree of impact of each different factor is analysed. Finally, we compare with a Long Short-Term Memory Neural Network and other works from the literature on the same dataset. We conclude that the MFP regression is effective and competitive with contemporary works, and provides several additional advantages e.g. in terms of interpretability, generalisability, and implementability.",Clara Bertinelli Salucci|Azzeddine Bakdi|Ingrid K. Glad|Erik Vanem|Riccardo De Bin,,https://arxiv.org/abs/2102.08111v2,https://arxiv.org/pdf/2102.08111v2,,"45 pages, 7 figures, 3 tables",,,stat.AP,stat.AP|cs.LG|eess.SY,https://arxiv.org/pdf/2102.08111v2.pdf
2102.06016v1,2021-02-10T08:59:25Z,2021-02-10 08:59:25,Optimal adaptive inspection and maintenance planning for deteriorating structural systems,"Optimizing inspection and maintenance (I&M) plans for a large deteriorating structure is a computationally challenging task, in particular if one considers interdependences among its components. This is due to the sheer number of possible decision alternatives over the lifetime of the structure and the uncertainty surrounding the deterioration processes, the structural performance and the outcomes of inspection and maintenance actions. To address this challenge, Luque and Straub (2019) proposed a heuristic approach in which I&M plans for structural systems are defined through a set of simple decision rules. Here, we formalize the optimization of these decision rules and extend the approach to enable adaptive planning. The initially optimal I&M plan is successively adapted throughout the service life, based on past inspection and monitoring results. The proposed methodology uses stochastic deterioration models and accounts for the interdependence among structural components. The heuristic-based adaptive planning is illustrated for a structural frame subjected to fatigue.",Elizabeth Bismut|Daniel Straub,,https://arxiv.org/abs/2102.06016v1,https://arxiv.org/pdf/2102.06016v1,https://doi.org/10.1016/j.ress.2021.107891,,Reliab.Eng.Syst.Saf. 215 (2021) 107891,10.1016/j.ress.2021.107891,eess.SY,eess.SY|math.OC|stat.AP,https://arxiv.org/pdf/2102.06016v1.pdf
2102.01740v1,2021-02-02T20:25:23Z,2021-02-02 20:25:23,Reliability Analysis of Artificial Intelligence Systems Using Recurrent Events Data from Autonomous Vehicles,"Artificial intelligence (AI) systems have become increasingly common and the trend will continue. Examples of AI systems include autonomous vehicles (AV), computer vision, natural language processing, and AI medical experts. To allow for safe and effective deployment of AI systems, the reliability of such systems needs to be assessed. Traditionally, reliability assessment is based on reliability test data and the subsequent statistical modeling and analysis. The availability of reliability data for AI systems, however, is limited because such data are typically sensitive and proprietary. The California Department of Motor Vehicles (DMV) oversees and regulates an AV testing program, in which many AV manufacturers are conducting AV road tests. Manufacturers participating in the program are required to report recurrent disengagement events to California DMV. This information is being made available to the public. In this paper, we use recurrent disengagement events as a representation of the reliability of the AI system in AV, and propose a statistical framework for modeling and analyzing the recurrent events data from AV driving tests. We use traditional parametric models in software reliability and propose a new nonparametric model based on monotonic splines to describe the event process. We develop inference procedures for selecting the best models, quantifying uncertainty, and testing heterogeneity in the event process. We then analyze the recurrent events data from four AV manufacturers, and make inferences on the reliability of the AI systems in AV. We also describe how the proposed analysis can be applied to assess the reliability of other AI systems.",Yili Hong|Jie Min|Caleb B. King|William Q. Meeker,,https://arxiv.org/abs/2102.01740v1,https://arxiv.org/pdf/2102.01740v1,,"30 pages, 9 figures",,,cs.AI,cs.AI|stat.AP,https://arxiv.org/pdf/2102.01740v1.pdf
2101.08198v1,2021-01-20T16:12:03Z,2021-01-20 16:12:03,Towards reliable projections of global mean surface temperature,"Quantifying the risk of global warming exceeding critical targets such as 2.0 K requires reliable projections of uncertainty as well as best estimates of Global Mean Surface Temperature (GMST). However, uncertainty bands on GMST projections are often calculated heuristically and have several potential shortcomings. In particular, the uncertainty bands shown in IPCC plume projections of GMST are based on the distribution of GMST anomalies from climate model runs and so are strongly determined by model characteristics with little influence from observations of the real-world. Physically motivated time-series approaches are proposed based on fitting energy balance models (EBMs) to climate model outputs and observations in order to constrain future projections. It is shown that EBMs fitted to one forcing scenario will not produce reliable projections when different forcing scenarios are applied. The errors in the EBM projections can be interpreted as arising due to a discrepancy in the effective forcing felt by the model. A simple time-series approach to correcting the projections is proposed based on learning the evolution of the forcing discrepancy so that it can be projected into the future. These approaches give reliable projections of GMST when tested in a perfect model setting, and when applied to observations lead to well constrained projections with lower mean warming and narrower projection bands than previous estimates. Despite the reduced uncertainty, the lower warming leads to a greatly reduced probability of exceeding the 2.0 K warming target.",Philip G. Sansom|Donald Cummins|Stephan Siegert|David B. Stephenson,,https://arxiv.org/abs/2101.08198v1,https://arxiv.org/pdf/2101.08198v1,,"21 pages, 7 figures, 4 tables",,,stat.AP,stat.AP,https://arxiv.org/pdf/2101.08198v1.pdf
2101.08083v3,2021-01-20T11:39:24Z,2021-05-19 08:43:25,Developments and applications of Shapley effects to reliability-oriented sensitivity analysis with correlated inputs,"Reliability-oriented sensitivity analysis methods have been developed for understanding the influence of model inputs relative to events which characterize the failure of a system (e.g., a threshold exceedance of the model output). In this field, the target sensitivity analysis focuses primarily on capturing the influence of the inputs on the occurrence of such a critical event. This paper proposes new target sensitivity indices, based on the Shapley values and called ""target Shapley effects"", allowing for interpretable sensitivity measures under dependent inputs. Two algorithms (one based on Monte Carlo sampling, and a given-data algorithm based on a nearest-neighbors procedure) are proposed for the estimation of these target Shapley effects based on the $\ell^2$ norm. Additionally, the behavior of these target Shapley effects are theoretically and empirically studied through various toy-cases. Finally, the application of these new indices in two real-world use-cases (a river flood model and a COVID-19 epidemiological model) is discussed.",Marouane Il Idrissi|Vincent Chabridon|Bertrand Iooss,,https://arxiv.org/abs/2101.08083v3,https://arxiv.org/pdf/2101.08083v3,https://doi.org/10.1016/j.envsoft.2021.105115,,,10.1016/j.envsoft.2021.105115,math.ST,math.ST|stat.AP|stat.ME,https://arxiv.org/pdf/2101.08083v3.pdf
2101.03671v1,2021-01-11T02:29:05Z,2021-01-11 02:29:05,A Degradation Performance Model With Mixed-type Covariates and Latent Heterogeneity,"Successful modeling of degradation performance data is essential for accurate reliability assessment and failure predictions of highly reliable product units. The degradation performance measurements over time are highly heterogeneous. Such heterogeneity can be partially attributed to external factors, such as accelerated/environmental conditions, and can also be attributed to internal factors, such as material microstructure characteristics of product units. The latent heterogeneity due to the unobserved/unknown factors shared within each product unit may also exists and need to be considered as well. Existing degradation models often fail to consider (i) the influence of both external accelerated/environmental conditions and internal material information, (ii) the influence of unobserved/unknown factors within each unit. In this work, we propose a generic degradation performance modeling framework with mixed-type covariates and latent heterogeneity to account for both influences of observed internal and external factors as well as unobserved factors. Effective estimation algorithm is also developed to jointly quantify the influences of mixed-type covariates and individual latent heterogeneity, and also to examine the potential interaction between mixed-type covariates. Functional data analysis and data augmentation techniques are employed to address a series of estimation issues. A real case study is further provided to demonstrate the superior performance of the proposed approach over several alternative modeling approaches. Besides, the proposed degradation performance modeling framework also provides interpretable findings.",Xuxue Sun|Wenjun Cai|Qiong Zhang|Mingyang Li,,https://arxiv.org/abs/2101.03671v1,https://arxiv.org/pdf/2101.03671v1,https://doi.org/10.1016/j.ress.2021.107928,24 pages,,10.1016/j.ress.2021.107928,stat.AP,stat.AP,https://arxiv.org/pdf/2101.03671v1.pdf
2101.01366v2,2021-01-05T06:25:47Z,2023-06-05 23:49:35,A Symmetric Loss Perspective of Reliable Machine Learning,"When minimizing the empirical risk in binary classification, it is a common practice to replace the zero-one loss with a surrogate loss to make the learning objective feasible to optimize. Examples of well-known surrogate losses for binary classification include the logistic loss, hinge loss, and sigmoid loss. It is known that the choice of a surrogate loss can highly influence the performance of the trained classifier and therefore it should be carefully chosen. Recently, surrogate losses that satisfy a certain symmetric condition (aka., symmetric losses) have demonstrated their usefulness in learning from corrupted labels. In this article, we provide an overview of symmetric losses and their applications. First, we review how a symmetric loss can yield robust classification from corrupted labels in balanced error rate (BER) minimization and area under the receiver operating characteristic curve (AUC) maximization. Then, we demonstrate how the robust AUC maximization method can benefit natural language processing in the problem where we want to learn only from relevant keywords and unlabeled documents. Finally, we conclude this article by discussing future directions, including potential applications of symmetric losses for reliable machine learning and the design of non-symmetric losses that can benefit from the symmetric condition.",Nontawat Charoenphakdee|Jongyeong Lee|Masashi Sugiyama,,https://arxiv.org/abs/2101.01366v2,https://arxiv.org/pdf/2101.01366v2,,Invited article preprint,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2101.01366v2.pdf
2012.03830v1,2020-12-07T16:26:48Z,2020-12-07 16:26:48,Bearings degradation monitoring indicators based on discarded projected space information and piecewise linear representation,"Condition-based maintenance of rotating mechanics requests efficient bearings degradation monitoring. The accuracy of bearings degradation measure depends largely on degradation indicators. To extract efficient indicators, in this paper we propose a method based on the discarded projected space information and piecewise linear representation (PLR) to build three bearings degradation monitoring indicators which are named SDHT2, VSDHT2 and NVSDHT2. The discarded projected space information is measured by the segmented discarded Hotelling T square we propose in this paper. For illustration, the IEEE PHM 2012 benchmark dataset is used in this paper. The results show that the three new indicators are all sensitive and monotonic during the bearings whole lifecycle. They describe the whole degradation process history and carry the real-time information of bearings degradation. And NVSDHT2 is the generalised version of VSDHT2, which is promising to monitor bearings degradation.",Fei Huang|Alexandre Sava|Kondo H. Adjallah|Wang Zhouhang,"LCOMS, HYIT|LCOMS|LCOMS|LCOMS",https://arxiv.org/abs/2012.03830v1,https://arxiv.org/pdf/2012.03830v1,https://doi.org/10.1504/IJMA.2020.108185,,"International Journal of Mechatronics and Automation, 2020, 7 (1), pp.23-31",10.1504/IJMA.2020.108185,cs.IR,cs.IR|stat.AP,https://arxiv.org/pdf/2012.03830v1.pdf
2011.15001v1,2020-11-30T17:06:28Z,2020-11-30 17:06:28,Variance based sensitivity analysis for Monte Carlo and importance sampling reliability assessment with Gaussian processes,"Running a reliability analysis on engineering problems involving complex numerical models can be computationally very expensive, requiring advanced simulation methods to reduce the overall numerical cost. Gaussian process based active learning methods for reliability analysis have emerged as a promising way for reducing this computational cost. The learning phase of these methods consists in building a Gaussian process surrogate model of the performance function and using the uncertainty structure of the Gaussian process to enrich iteratively this surrogate model. For that purpose a learning criterion has to be defined. Then, the estimation of the probability of failure is typically obtained by a classification of a population evaluated on the final surrogate model. Hence, the estimator of the probability of failure holds two different uncertainty sources related to the surrogate model approximation and to the sampling based integration technique. In this paper, we propose a methodology to quantify the sensitivity of the probability of failure estimator to both uncertainty sources. This analysis also enables to control the whole error associated to the failure probability estimate and thus provides an accuracy criterion on the estimation. Thus, an active learning approach integrating this analysis to reduce the main source of error and stopping when the global variability is sufficiently low is introduced. The approach is proposed for both a Monte Carlo based method as well as an importance sampling based method, seeking to improve the estimation of rare event probabilities. Performance of the proposed strategy is then assessed on several examples.",Morgane Menz|Sylvain Dubreuil|Jérôme Morio|Christian Gogu|Nathalie Bartoli|Marie Chiron,,https://arxiv.org/abs/2011.15001v1,https://arxiv.org/pdf/2011.15001v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2011.15001v1.pdf
2011.11740v1,2020-11-23T21:28:03Z,2020-11-23 21:28:03,Remaining Useful Life Estimation Under Uncertainty with Causal GraphNets,"In this work, a novel approach for the construction and training of time series models is presented that deals with the problem of learning on large time series with non-equispaced observations, which at the same time may possess features of interest that span multiple scales. The proposed method is appropriate for constructing predictive models for non-stationary stochastic time series.The efficacy of the method is demonstrated on a simulated stochastic degradation dataset and on a real-world accelerated life testing dataset for ball-bearings. The proposed method, which is based on GraphNets, implicitly learns a model that describes the evolution of the system at the level of a state-vector rather than of a raw observation. The proposed approach is compared to a recurrent network with a temporal convolutional feature extractor head (RNN-tCNN) which forms a known viable alternative for the problem context considered. Finally, by taking advantage of recent advances in the computation of reparametrization gradients for learning probability distributions, a simple yet effective technique for representing prediction uncertainty as a Gamma distribution over remaining useful life predictions is employed.",Charilaos Mylonas|Eleni Chatzi,,https://arxiv.org/abs/2011.11740v1,https://arxiv.org/pdf/2011.11740v1,,A preprint,,,cs.LG,cs.LG|stat.AP,https://arxiv.org/pdf/2011.11740v1.pdf
2011.04102v3,2020-11-08T23:16:19Z,2022-11-03 15:59:59,Reliable Off-policy Evaluation for Reinforcement Learning,"In a sequential decision-making problem, off-policy evaluation estimates the expected cumulative reward of a target policy using logged trajectory data generated from a different behavior policy, without execution of the target policy. Reinforcement learning in high-stake environments, such as healthcare and education, is often limited to off-policy settings due to safety or ethical concerns, or inability of exploration. Hence it is imperative to quantify the uncertainty of the off-policy estimate before deployment of the target policy. In this paper, we propose a novel framework that provides robust and optimistic cumulative reward estimates using one or multiple logged trajectories data. Leveraging methodologies from distributionally robust optimization, we show that with proper selection of the size of the distributional uncertainty set, these estimates serve as confidence bounds with non-asymptotic and asymptotic guarantees under stochastic or adversarial environments. Our results are also generalized to batch reinforcement learning and are supported by empirical analysis.",Jie Wang|Rui Gao|Hongyuan Zha,,https://arxiv.org/abs/2011.04102v3,https://arxiv.org/pdf/2011.04102v3,https://doi.org/10.1287/opre.2022.2382,"46 pages, 7 figures",,10.1287/opre.2022.2382,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2011.04102v3.pdf
2011.03274v1,2020-11-06T10:41:39Z,2020-11-06 10:41:39,Trust Issues: Uncertainty Estimation Does Not Enable Reliable OOD Detection On Medical Tabular Data,"When deploying machine learning models in high-stakes real-world environments such as health care, it is crucial to accurately assess the uncertainty concerning a model's prediction on abnormal inputs. However, there is a scarcity of literature analyzing this problem on medical data, especially on mixed-type tabular data such as Electronic Health Records. We close this gap by presenting a series of tests including a large variety of contemporary uncertainty estimation techniques, in order to determine whether they are able to identify out-of-distribution (OOD) patients. In contrast to previous work, we design tests on realistic and clinically relevant OOD groups, and run experiments on real-world medical data. We find that almost all techniques fail to achieve convincing results, partly disagreeing with earlier findings.",Dennis Ulmer|Lotta Meijerink|Giovanni Cinà,,https://arxiv.org/abs/2011.03274v1,https://arxiv.org/pdf/2011.03274v1,,,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2011.03274v1.pdf
2011.03140v3,2020-11-05T23:40:46Z,2021-04-11 15:16:21,Prediction of Future Failures for Heterogeneous Reliability Field Data,"This article introduces methods for constructing prediction bounds or intervals for the number of future failures from heterogeneous reliability field data. We focus on within-sample prediction where early data from a failure-time process is used to predict future failures from the same process. Early data from high-reliability products, however, often have limited information due to some combination of small sample sizes, censoring, and truncation. In such cases, we use a Bayesian hierarchical model to model jointly multiple lifetime distributions arising from different subpopulations of similar products. By borrowing information across subpopulations, our method enables stable estimation and the computation of corresponding prediction intervals, even in cases where there are few observed failures. Three applications are provided to illustrate this methodology, and a simulation study is used to validate the coverage performance of the prediction intervals.",Colin Lewis-Beck|Qinglong Tian|William Q. Meeker,,https://arxiv.org/abs/2011.03140v3,https://arxiv.org/pdf/2011.03140v3,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/2011.03140v3.pdf
2011.02696v2,2020-11-05T08:04:34Z,2021-03-01 21:03:05,Amortized Conditional Normalized Maximum Likelihood: Reliable Out of Distribution Uncertainty Estimation,"While deep neural networks provide good performance for a range of challenging tasks, calibration and uncertainty estimation remain major challenges, especially under distribution shift. In this paper, we propose the amortized conditional normalized maximum likelihood (ACNML) method as a scalable general-purpose approach for uncertainty estimation, calibration, and out-of-distribution robustness with deep networks. Our algorithm builds on the conditional normalized maximum likelihood (CNML) coding scheme, which has minimax optimal properties according to the minimum description length principle, but is computationally intractable to evaluate exactly for all but the simplest of model classes. We propose to use approximate Bayesian inference technqiues to produce a tractable approximation to the CNML distribution. Our approach can be combined with any approximate inference algorithm that provides tractable posterior densities over model parameters. We demonstrate that ACNML compares favorably to a number of prior techniques for uncertainty estimation in terms of calibration on out-of-distribution inputs.",Aurick Zhou|Sergey Levine,,https://arxiv.org/abs/2011.02696v2,https://arxiv.org/pdf/2011.02696v2,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2011.02696v2.pdf
2011.01263v2,2020-11-02T19:10:15Z,2021-03-15 22:46:35,Assessing the Reliability of Wind Power Operations under a Changing Climate with a Non-Gaussian Bias Correction,"Facing increasing societal and economic pressure, many countries have established strategies to develop renewable energy portfolios, whose penetration in the market can alleviate the dependence on fossil fuels. In the case of wind, there is a fundamental question related to the resilience, and hence profitability of future wind farms to a changing climate, given that current wind turbines have lifespans of up to thirty years. In this work, we develop a new non-Gaussian method data to simulations and to estimate future wind, predicated on a trans-Gaussian transformation and a cluster-wise minimization of the Kullback-Leibler divergence. Future winds abundance will be determined for Saudi Arabia, a country with a recently established plan to develop a portfolio of up to 16 GW of wind energy. Further, we estimate the change in profits over future decades using additional high-resolution simulations, an improved method for vertical wind extrapolation, and power curves from a collection of popular wind turbines. We find an overall increase in the daily profit of $272,000 for the wind energy market for the optimal locations for wind farming in the country.",Jiachen Zhang|Paola Crippa|Marc G. Genton|Stefano Castruccio,,https://arxiv.org/abs/2011.01263v2,https://arxiv.org/pdf/2011.01263v2,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2011.01263v2.pdf
2010.15651v1,2020-10-29T14:55:20Z,2020-10-29 14:55:20,Reliable Graph Neural Networks via Robust Aggregation,"Perturbations targeting the graph structure have proven to be extremely effective in reducing the performance of Graph Neural Networks (GNNs), and traditional defenses such as adversarial training do not seem to be able to improve robustness. This work is motivated by the observation that adversarially injected edges effectively can be viewed as additional samples to a node's neighborhood aggregation function, which results in distorted aggregations accumulating over the layers. Conventional GNN aggregation functions, such as a sum or mean, can be distorted arbitrarily by a single outlier. We propose a robust aggregation function motivated by the field of robust statistics. Our approach exhibits the largest possible breakdown point of 0.5, which means that the bias of the aggregation is bounded as long as the fraction of adversarial edges of a node is less than 50\%. Our novel aggregation function, Soft Medoid, is a fully differentiable generalization of the Medoid and therefore lends itself well for end-to-end deep learning. Equipping a GNN with our aggregation improves the robustness with respect to structure perturbations on Cora ML by a factor of 3 (and 5.5 on Citeseer) and by a factor of 8 for low-degree nodes.",Simon Geisler|Daniel Zügner|Stephan Günnemann,,https://arxiv.org/abs/2010.15651v1,https://arxiv.org/pdf/2010.15651v1,,"23 pages, 9 figures, 6 Tables, Neural Information Processing Systems, NeurIPS, 2020",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2010.15651v1.pdf
2010.14986v2,2020-10-28T13:53:43Z,2021-06-11 09:32:11,Evaluating Robustness of Predictive Uncertainty Estimation: Are Dirichlet-based Models Reliable?,"Dirichlet-based uncertainty (DBU) models are a recent and promising class of uncertainty-aware models. DBU models predict the parameters of a Dirichlet distribution to provide fast, high-quality uncertainty estimates alongside with class predictions. In this work, we present the first large-scale, in-depth study of the robustness of DBU models under adversarial attacks. Our results suggest that uncertainty estimates of DBU models are not robust w.r.t. three important tasks: (1) indicating correctly and wrongly classified samples; (2) detecting adversarial examples; and (3) distinguishing between in-distribution (ID) and out-of-distribution (OOD) data. Additionally, we explore the first approaches to make DBU models more robust. While adversarial training has a minor effect, our median smoothing based approach significantly increases robustness of DBU models.",Anna-Kathrin Kopetzki|Bertrand Charpentier|Daniel Zügner|Sandhya Giri|Stephan Günnemann,,https://arxiv.org/abs/2010.14986v2,https://arxiv.org/pdf/2010.14986v2,,Published at ICML 2021,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2010.14986v2.pdf
2010.10922v1,2020-10-19T00:17:38Z,2020-10-19 00:17:38,Reliability of meta-analysis of an association between ambient air quality and development of asthma later in life,"Claims from observational studies often fail to replicate. A study was undertaken to assess the reliability of cohort studies used in a highly cited meta-analysis of the association between ambient nitrogen dioxide, NO2, and fine particulate matter, PM2.5, concentrations early in life and development of asthma later in life. The numbers of statistical tests possible were estimated for 19 base papers considered for the meta-analysis. A p-value plot for NO2 and PM2.5 was constructed to evaluate effect heterogeneity of p-values used from the base papers. The numbers of statistical tests possible in the base papers were large - median 13,824, interquartile range 1,536-221,184; range 96-42M, in comparison to statistical test results presented. Statistical test results drawn from the base papers are unlikely to provide unbiased measures for meta-analysis. The p-value plot indicated that heterogeneity of the NO2 results across the base papers is consistent with a two-component mixture. First, it makes no sense to average across a mixture in meta-analysis. Second, the shape of the p-value plot for NO2 appears consistent with the possibility of analysis manipulation to obtain small p-values in several of the cohort studies. As for PM2.5, all corresponding p-values fall on a 45-degree line indicating complete randomness rather than a true association. Our interpretation of the meta-analysis is that the random p-values indicating no cause-effect associations are more plausible and that their meta-analysis will not likely replicate in the absence of bias. We conclude that claims made in the base papers used for meta-analysis are unreliable due to bias induced by multiple testing and multiple modelling, MTMM. We also show there is evidence that the heterogeneity across the base papers used for meta-analysis is more complex than simple sampling from a normal process.",S. Stanley Young|Kai-Chieh Cheng|Jin Hua Chen|Shu-Chuan Chen|Warren B. Kindzierski,,https://arxiv.org/abs/2010.10922v1,https://arxiv.org/pdf/2010.10922v1,,68 pages including supplemental material. arXiv admin note: text overlap with arXiv:2010.08628,,,stat.AP,stat.AP,https://arxiv.org/pdf/2010.10922v1.pdf
2010.01979v5,2020-10-05T13:13:21Z,2022-10-12 09:50:36,"BayesAdapter: Being Bayesian, Inexpensively and Reliably, via Bayesian Fine-tuning","Despite their theoretical appealingness, Bayesian neural networks (BNNs) are left behind in real-world adoption, mainly due to persistent concerns on their scalability, accessibility, and reliability. In this work, we develop the BayesAdapter framework to relieve these concerns. In particular, we propose to adapt pre-trained deterministic NNs to be variational BNNs via cost-effective Bayesian fine-tuning. Technically, we develop a modularized implementation for the learning of variational BNNs, and refurbish the generally applicable exemplar reparameterization trick through exemplar parallelization to efficiently reduce the gradient variance in stochastic variational inference. Based on the lightweight Bayesian learning paradigm, we conduct extensive experiments on a variety of benchmarks, and show that our method can consistently induce posteriors with higher quality than competitive baselines, yet significantly reducing training overheads. Code is available at https://github.com/thudzj/ScalableBDL.",Zhijie Deng|Jun Zhu,,https://arxiv.org/abs/2010.01979v5,https://arxiv.org/pdf/2010.01979v5,,14th Asian Conference on Machine Learning (ACML 2022),,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2010.01979v5.pdf
2009.14448v1,2020-09-30T05:19:56Z,2020-09-30 05:19:56,Ask-n-Learn: Active Learning via Reliable Gradient Representations for Image Classification,"Deep predictive models rely on human supervision in the form of labeled training data. Obtaining large amounts of annotated training data can be expensive and time consuming, and this becomes a critical bottleneck while building such models in practice. In such scenarios, active learning (AL) strategies are used to achieve faster convergence in terms of labeling efforts. Existing active learning employ a variety of heuristics based on uncertainty and diversity to select query samples. Despite their wide-spread use, in practice, their performance is limited by a number of factors including non-calibrated uncertainties, insufficient trade-off between data exploration and exploitation, presence of confirmation bias etc. In order to address these challenges, we propose Ask-n-Learn, an active learning approach based on gradient embeddings obtained using the pesudo-labels estimated in each iteration of the algorithm. More importantly, we advocate the use of prediction calibration to obtain reliable gradient embeddings, and propose a data augmentation strategy to alleviate the effects of confirmation bias during pseudo-labeling. Through empirical studies on benchmark image classification tasks (CIFAR-10, SVHN, Fashion-MNIST, MNIST), we demonstrate significant improvements over state-of-the-art baselines, including the recently proposed BADGE algorithm.",Bindya Venkatesh|Jayaraman J. Thiagarajan,,https://arxiv.org/abs/2009.14448v1,https://arxiv.org/pdf/2009.14448v1,,,,,stat.ML,stat.ML|cs.CV|cs.LG,https://arxiv.org/pdf/2009.14448v1.pdf
2009.07530v1,2020-09-16T07:59:15Z,2020-09-16 07:59:15,m-arcsinh: An Efficient and Reliable Function for SVM and MLP in scikit-learn,"This paper describes the 'm-arcsinh', a modified ('m-') version of the inverse hyperbolic sine function ('arcsinh'). Kernel and activation functions enable Machine Learning (ML)-based algorithms, such as Support Vector Machine (SVM) and Multi-Layer Perceptron (MLP), to learn from data in a supervised manner. m-arcsinh, implemented in the open source Python library 'scikit-learn', is hereby presented as an efficient and reliable kernel and activation function for SVM and MLP respectively. Improvements in reliability and speed to convergence in classification tasks on fifteen (N = 15) datasets available from scikit-learn and the University California Irvine (UCI) Machine Learning repository are discussed. Experimental results demonstrate the overall competitive classification performance of both SVM and MLP, achieved via the proposed function. This function is compared to gold standard kernel and activation functions, demonstrating its overall competitive reliability regardless of the complexity of the classification tasks involved.",Luca Parisi,,https://arxiv.org/abs/2009.07530v1,https://arxiv.org/pdf/2009.07530v1,,"20 pages, 4 listings/Python code snippets, 2 figures, 15 tables",,,cs.LG,cs.LG|cs.CV|cs.MS|stat.ML,https://arxiv.org/pdf/2009.07530v1.pdf
2009.05501v1,2020-09-11T15:51:52Z,2020-09-11 15:51:52,Towards a More Reliable Interpretation of Machine Learning Outputs for Safety-Critical Systems using Feature Importance Fusion,"When machine learning supports decision-making in safety-critical systems, it is important to verify and understand the reasons why a particular output is produced. Although feature importance calculation approaches assist in interpretation, there is a lack of consensus regarding how features' importance is quantified, which makes the explanations offered for the outcomes mostly unreliable. A possible solution to address the lack of agreement is to combine the results from multiple feature importance quantifiers to reduce the variance of estimates. Our hypothesis is that this will lead to more robust and trustworthy interpretations of the contribution of each feature to machine learning predictions. To assist test this hypothesis, we propose an extensible Framework divided in four main parts: (i) traditional data pre-processing and preparation for predictive machine learning models; (ii) predictive machine learning; (iii) feature importance quantification and (iv) feature importance decision fusion using an ensemble strategy. We also introduce a novel fusion metric and compare it to the state-of-the-art. Our approach is tested on synthetic data, where the ground truth is known. We compare different fusion approaches and their results for both training and test sets. We also investigate how different characteristics within the datasets affect the feature importance ensembles studied. Results show that our feature importance ensemble Framework overall produces 15% less feature importance error compared to existing methods. Additionally, results reveal that different levels of noise in the datasets do not affect the feature importance ensembles' ability to accurately quantify feature importance, whereas the feature importance quantification error increases with the number of features and number of orthogonal informative features.",Divish Rengasamy|Benjamin Rothwell|Grazziela Figueredo,,https://arxiv.org/abs/2009.05501v1,https://arxiv.org/pdf/2009.05501v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2009.05501v1.pdf
2009.04547v2,2020-09-09T20:03:42Z,2021-11-28 14:37:08,Optimal Inspection and Maintenance Planning for Deteriorating Structural Components through Dynamic Bayesian Networks and Markov Decision Processes,"Civil and maritime engineering systems, among others, from bridges to offshore platforms and wind turbines, must be efficiently managed as they are exposed to deterioration mechanisms throughout their operational life, such as fatigue or corrosion. Identifying optimal inspection and maintenance policies demands the solution of a complex sequential decision-making problem under uncertainty, with the main objective of efficiently controlling the risk associated with structural failures. Addressing this complexity, risk-based inspection planning methodologies, supported often by dynamic Bayesian networks, evaluate a set of pre-defined heuristic decision rules to reasonably simplify the decision problem. However, the resulting policies may be compromised by the limited space considered in the definition of the decision rules. Avoiding this limitation, Partially Observable Markov Decision Processes (POMDPs) provide a principled mathematical methodology for stochastic optimal control under uncertain action outcomes and observations, in which the optimal actions are prescribed as a function of the entire, dynamically updated, state probability distribution. In this paper, we combine dynamic Bayesian networks with POMDPs in a joint framework for optimal inspection and maintenance planning, and we provide the formulation for developing both infinite and finite horizon POMDPs in a structural reliability context. The proposed methodology is implemented and tested for the case of a structural component subject to fatigue deterioration, demonstrating the capability of state-of-the-art point-based POMDP solvers for solving the underlying planning optimization problem. Within the numerical experiments, POMDP and heuristic-based policies are thoroughly compared, and results showcase that POMDPs achieve substantially lower costs as compared to their counterparts, even for traditional problem settings.",P. G. Morato|K. G. Papakonstantinou|C. P. Andriotis|J. S. Nielsen|P. Rigo,,https://arxiv.org/abs/2009.04547v2,https://arxiv.org/pdf/2009.04547v2,https://doi.org/10.1016/j.strusafe.2021.102140,,"Structural Safety, Volume 94, 2022,",10.1016/j.strusafe.2021.102140,cs.AI,cs.AI|eess.SY|stat.AP,https://arxiv.org/pdf/2009.04547v2.pdf
2009.03091v1,2020-09-07T13:24:47Z,2020-09-07 13:24:47,Iterative Correction of Sensor Degradation and a Bayesian Multi-Sensor Data Fusion Method,"We present a novel method for inferring ground-truth signal from multiple degraded signals, affected by different amounts of sensor exposure. The algorithm learns a multiplicative degradation effect by performing iterative corrections of two signals solely from the ratio between them. The degradation function d should be continuous, satisfy monotonicity, and d(0) = 1. We use smoothed monotonic regression method, where we easily incorporate the aforementioned criteria to the fitting part. We include theoretical analysis and prove convergence to the ground-truth signal for the noiseless measurement model. Lastly, we present an approach to fuse the noisy corrected signals using Gaussian processes. We use sparse Gaussian processes that can be utilized for a large number of measurements together with a specialized kernel that enables the estimation of noise values of all sensors. The data fusion framework naturally handles data gaps and provides a simple and powerful method for observing the signal trends on multiple timescales(long-term and short-term signal properties). The viability of correction method is evaluated on a synthetic dataset with known ground-truth signal.",Luka Kolar|Rok Šikonja|Lenart Treven,,https://arxiv.org/abs/2009.03091v1,https://arxiv.org/pdf/2009.03091v1,,,,,cs.LG,cs.LG|cs.IR|stat.ML,https://arxiv.org/pdf/2009.03091v1.pdf
2009.02728v2,2020-09-06T13:08:20Z,2020-09-08 07:53:40,Discovering Reliable Causal Rules,"We study the problem of deriving policies, or rules, that when enacted on a complex system, cause a desired outcome. Absent the ability to perform controlled experiments, such rules have to be inferred from past observations of the system's behaviour. This is a challenging problem for two reasons: First, observational effects are often unrepresentative of the underlying causal effect because they are skewed by the presence of confounding factors. Second, naive empirical estimations of a rule's effect have a high variance, and, hence, their maximisation can lead to random results.
  To address these issues, first we measure the causal effect of a rule from observational data---adjusting for the effect of potential confounders. Importantly, we provide a graphical criteria under which causal rule discovery is possible. Moreover, to discover reliable causal rules from a sample, we propose a conservative and consistent estimator of the causal effect, and derive an efficient and exact algorithm that maximises the estimator. On synthetic data, the proposed estimator converges faster to the ground truth than the naive estimator and recovers relevant causal rules even at small sample sizes. Extensive experiments on a variety of real-world datasets show that the proposed algorithm is efficient and discovers meaningful rules.",Kailash Budhathoki|Mario Boley|Jilles Vreeken,,https://arxiv.org/abs/2009.02728v2,https://arxiv.org/pdf/2009.02728v2,,Poster presented in NeurIPS 2018 Workshop on Causal Learning,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2009.02728v2.pdf
2009.00351v1,2020-09-01T11:10:13Z,2020-09-01 11:10:13,Advancing from Predictive Maintenance to Intelligent Maintenance with AI and IIoT,"As Artificial Intelligent (AI) technology advances and increasingly large amounts of data become readily available via various Industrial Internet of Things (IIoT) projects, we evaluate the state of the art of predictive maintenance approaches and propose our innovative framework to improve the current practice. The paper first reviews the evolution of reliability modelling technology in the past 90 years and discusses major technologies developed in industry and academia. We then introduce the next generation maintenance framework - Intelligent Maintenance, and discuss its key components. This AI and IIoT based Intelligent Maintenance framework is composed of (1) latest machine learning algorithms including probabilistic reliability modelling with deep learning, (2) real-time data collection, transfer, and storage through wireless smart sensors, (3) Big Data technologies, (4) continuously integration and deployment of machine learning models, (5) mobile device and AR/VR applications for fast and better decision-making in the field. Particularly, we proposed a novel probabilistic deep learning reliability modelling approach and demonstrate it in the Turbofan Engine Degradation Dataset.",Haining Zheng|Antonio R. Paiva|Chris S. Gurciullo,,https://arxiv.org/abs/2009.00351v1,https://arxiv.org/pdf/2009.00351v1,,The 3rd International Workshop on Artificial Intelligence of Things (AIoT) In conjunction with the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD 2020),,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2009.00351v1.pdf
2008.08221v1,2020-08-19T02:08:56Z,2020-08-19 02:08:56,Machine Learning for Reliability Engineering and Safety Applications: Review of Current Status and Future Opportunities,"Machine learning (ML) pervades an increasing number of academic disciplines and industries. Its impact is profound, and several fields have been fundamentally altered by it, autonomy and computer vision for example; reliability engineering and safety will undoubtedly follow suit. There is already a large but fragmented literature on ML for reliability and safety applications, and it can be overwhelming to navigate and integrate into a coherent whole. In this work, we facilitate this task by providing a synthesis of, and a roadmap to this ever-expanding analytical landscape and highlighting its major landmarks and pathways. We first provide an overview of the different ML categories and sub-categories or tasks, and we note several of the corresponding models and algorithms. We then look back and review the use of ML in reliability and safety applications. We examine several publications in each category/sub-category, and we include a short discussion on the use of Deep Learning to highlight its growing popularity and distinctive advantages. Finally, we look ahead and outline several promising future opportunities for leveraging ML in service of advancing reliability and safety considerations. Overall, we argue that ML is capable of providing novel insights and opportunities to solve important challenges in reliability and safety applications. It is also capable of teasing out more accurate insights from accident datasets than with traditional analysis tools, and this in turn can lead to better informed decision-making and more effective accident prevention.",Zhaoyi Xu|Joseph Homer Saleh,,https://arxiv.org/abs/2008.08221v1,https://arxiv.org/pdf/2008.08221v1,,50 pages,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2008.08221v1.pdf
2008.08197v3,2020-08-18T23:53:11Z,2020-12-01 19:43:21,Incorporation of frailties into a non-proportional hazard regression model and its diagnostics for reliability modeling of downhole safety valves,"In this paper, our proposal consists of incorporating frailty into a statistical methodology for modeling time-to-event data, based on non-proportional hazards regression model. Specifically, we use the generalized time-dependent logistic (GTDL) model with a frailty term introduced in the hazard function to control for unobservable heterogeneity among the sampling units. We also add a regression in the parameter that measures the effect of time, since it can directly reflect the influence of covariates on the effect of time-to-failure. The practical relevance of the proposed model is illustrated in a real problem based on a data set for downhole safety valves (DHSVs) used in offshore oil and gas production wells. The reliability estimation of DHSVs can be used, among others, to predict the blowout occurrence, assess the workover demand and aid decision-making actions.",Francisco Louzada|José A. Cuminato|Oscar M. H. Rodriguez|Vera L. D. Tomazella|Eder A. Milani|Paulo H. Ferreira|Pedro L. Ramos|Gustavo Bochio|Ivan C. Perissini|Oilson A. Gonzatto Junior|Alex L. Mota|Luis F. A. Alegría|Danilo Colombo|Paulo G. O. Oliveira|Hugo F. L. Santos|Marcus V. C. Magalhães,,https://arxiv.org/abs/2008.08197v3,https://arxiv.org/pdf/2008.08197v3,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/2008.08197v3.pdf
2008.06729v1,2020-08-15T15:03:46Z,2020-08-15 15:03:46,Reliable Uncertainties for Bayesian Neural Networks using Alpha-divergences,"Bayesian Neural Networks (BNNs) often result uncalibrated after training, usually tending towards overconfidence. Devising effective calibration methods with low impact in terms of computational complexity is thus of central interest. In this paper we present calibration methods for BNNs based on the alpha divergences from Information Geometry. We compare the use of alpha divergence in training and in calibration, and we show how the use in calibration provides better calibrated uncertainty estimates for specific choices of alpha and is more efficient especially for complex network architectures. We empirically demonstrate the advantages of alpha calibration in regression problems involving parameter estimation and inferred correlations between output uncertainties.",Hector J. Hortua|Luigi Malago|Riccardo Volpi,,https://arxiv.org/abs/2008.06729v1,https://arxiv.org/pdf/2008.06729v1,,Accepted at the ICML 2020: Workshop on Uncertainty and Robustness in Deep Learning,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2008.06729v1.pdf
2008.06130v1,2020-08-13T23:23:25Z,2020-08-13 23:23:25,An estimator for predictive regression: reliable inference for financial economics,"Estimating linear regression using least squares and reporting robust standard errors is very common in financial economics, and indeed, much of the social sciences and elsewhere. For thick tailed predictors under heteroskedasticity this recipe for inference performs poorly, sometimes dramatically so. Here, we develop an alternative approach which delivers an unbiased, consistent and asymptotically normal estimator so long as the means of the outcome and predictors are finite. The new method has standard errors under heteroskedasticity which are easy to reliably estimate and tests which are close to their nominal size. The procedure works well in simulations and in an empirical exercise. An extension is given to quantile regression.",Neil Shephard,,https://arxiv.org/abs/2008.06130v1,https://arxiv.org/pdf/2008.06130v1,,,,,stat.ME,stat.ME|q-fin.ST,https://arxiv.org/pdf/2008.06130v1.pdf
2008.05030v4,2020-08-11T22:52:21Z,2021-11-06 18:35:28,Reliable Post hoc Explanations: Modeling Uncertainty in Explainability,"As black box explanations are increasingly being employed to establish model credibility in high-stakes settings, it is important to ensure that these explanations are accurate and reliable. However, prior work demonstrates that explanations generated by state-of-the-art techniques are inconsistent, unstable, and provide very little insight into their correctness and reliability. In addition, these methods are also computationally inefficient, and require significant hyper-parameter tuning. In this paper, we address the aforementioned challenges by developing a novel Bayesian framework for generating local explanations along with their associated uncertainty. We instantiate this framework to obtain Bayesian versions of LIME and KernelSHAP which output credible intervals for the feature importances, capturing the associated uncertainty. The resulting explanations not only enable us to make concrete inferences about their quality (e.g., there is a 95% chance that the feature importance lies within the given range), but are also highly consistent and stable. We carry out a detailed theoretical analysis that leverages the aforementioned uncertainty to estimate how many perturbations to sample, and how to sample for faster convergence. This work makes the first attempt at addressing several critical issues with popular explanation methods in one shot, thereby generating consistent, stable, and reliable explanations with guarantees in a computationally efficient manner. Experimental evaluation with multiple real world datasets and user studies demonstrate that the efficacy of the proposed framework.",Dylan Slack|Sophie Hilgard|Sameer Singh|Himabindu Lakkaraju,,https://arxiv.org/abs/2008.05030v4,https://arxiv.org/pdf/2008.05030v4,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2008.05030v4.pdf
2008.03961v1,2020-08-10T08:34:20Z,2020-08-10 08:34:20,Automatic Remaining Useful Life Estimation Framework with Embedded Convolutional LSTM as the Backbone,"An essential task in predictive maintenance is the prediction of the Remaining Useful Life (RUL) through the analysis of multivariate time series. Using the sliding window method, Convolutional Neural Network (CNN) and conventional Recurrent Neural Network (RNN) approaches have produced impressive results on this matter, due to their ability to learn optimized features. However, sequence information is only partially modeled by CNN approaches. Due to the flatten mechanism in conventional RNNs, like Long Short Term Memories (LSTM), the temporal information within the window is not fully preserved. To exploit the multi-level temporal information, many approaches are proposed which combine CNN and RNN models. In this work, we propose a new LSTM variant called embedded convolutional LSTM (ECLSTM). In ECLSTM a group of different 1D convolutions is embedded into the LSTM structure. Through this, the temporal information is preserved between and within windows. Since the hyper-parameters of models require careful tuning, we also propose an automated prediction framework based on the Bayesian optimization with hyperband optimizer, which allows for efficient optimization of the network architecture. Finally, we show the superiority of our proposed ECLSTM approach over the state-of-the-art approaches on several widely used benchmark data sets for RUL Estimation.",Yexu Zhou|Yuting Gao|Yiran Huang|Michael Hefenbrock|Till Riedel|Michael Beigl,,https://arxiv.org/abs/2008.03961v1,https://arxiv.org/pdf/2008.03961v1,,"16 pages, 5 figures",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2008.03961v1.pdf
2008.03033v1,2020-08-07T08:22:26Z,2020-08-07 08:22:26,Evaluating probabilistic classifiers: Reliability diagrams and score decompositions revisited,"A probability forecast or probabilistic classifier is reliable or calibrated if the predicted probabilities are matched by ex post observed frequencies, as examined visually in reliability diagrams. The classical binning and counting approach to plotting reliability diagrams has been hampered by a lack of stability under unavoidable, ad hoc implementation decisions. Here we introduce the CORP approach, which generates provably statistically Consistent, Optimally binned, and Reproducible reliability diagrams in an automated way. CORP is based on non-parametric isotonic regression and implemented via the Pool-adjacent-violators (PAV) algorithm - essentially, the CORP reliability diagram shows the graph of the PAV- (re)calibrated forecast probabilities. The CORP approach allows for uncertainty quantification via either resampling techniques or asymptotic theory, furnishes a new numerical measure of miscalibration, and provides a CORP based Brier score decomposition that generalizes to any proper scoring rule. We anticipate that judicious uses of the PAV algorithm yield improved tools for diagnostics and inference for a very wide range of statistical and machine learning methods.",Timo Dimitriadis|Tilmann Gneiting|Alexander I. Jordan,,https://arxiv.org/abs/2008.03033v1,https://arxiv.org/pdf/2008.03033v1,https://doi.org/10.1073/pnas.2016191118,,"Stable reliability diagrams for probabilistic classifiers, Proceedings of the National Academy of Sciences 118 (2021) e2016191118",10.1073/pnas.2016191118,stat.ME,stat.ME|cs.LG|stat.ML,https://arxiv.org/pdf/2008.03033v1.pdf
2008.01377v4,2020-08-04T07:21:36Z,2024-10-08 09:01:08,Reliable Part-of-Speech Tagging of Historical Corpora through Set-Valued Prediction,"Syntactic annotation of corpora in the form of part-of-speech (POS) tags is a key requirement for both linguistic research and subsequent automated natural language processing (NLP) tasks. This problem is commonly tackled using machine learning methods, i.e., by training a POS tagger on a sufficiently large corpus of labeled data. While the problem of POS tagging can essentially be considered as solved for modern languages, historical corpora turn out to be much more difficult, especially due to the lack of native speakers and sparsity of training data. Moreover, most texts have no sentences as we know them today, nor a common orthography. These irregularities render the task of automated POS tagging more difficult and error-prone. Under these circumstances, instead of forcing the POS tagger to predict and commit to a single tag, it should be enabled to express its uncertainty. In this paper, we consider POS tagging within the framework of set-valued prediction, which allows the POS tagger to express its uncertainty via predicting a set of candidate POS tags instead of guessing a single one. The goal is to guarantee a high confidence that the correct POS tag is included while keeping the number of candidates small. In our experimental study, we find that extending state-of-the-art POS taggers to set-valued prediction yields more precise and robust taggings, especially for unknown words, i.e., words not occurring in the training data.",Stefan Heid|Marcel Wever|Eyke Hüllermeier,,https://arxiv.org/abs/2008.01377v4,https://arxiv.org/pdf/2008.01377v4,https://doi.org/10.46298/jdmdh.6696,"14 pages, 8 figures","Journal of Data Mining & Digital Humanities, 2024 (October 21, 2024) jdmdh:6696",10.46298/jdmdh.6696,cs.CL,cs.CL|cs.IR|cs.LG|stat.ML,https://arxiv.org/pdf/2008.01377v4.pdf
2008.00977v2,2020-07-31T17:10:51Z,2021-01-10 18:15:52,Reliability in Software Engineering Qualitative Research through Inter-Coder Agreement: A guide using Krippendorff's $α$ & Atlas.ti,"In recent years, the research on empirical software engineering that uses qualitative data analysis (e.g., cases studies, interview surveys, and grounded theory studies) is increasing. However, most of this research does not deep into the reliability and validity of findings, specifically in the reliability of coding in which these methodologies rely on, despite there exist a variety of statistical techniques known as Inter-Coder Agreement (ICA) for analyzing consensus in team coding. This paper aims to establish a novel theoretical framework that enables a methodological approach for conducting this validity analysis. This framework is based on a set of coefficients for measuring the degree of agreement that different coders achieve when judging a common matter. We analyze different reliability coefficients and provide detailed examples of calculation, with special attention to Krippendorff's $α$ coefficients. We systematically review several variants of Krippendorff's $α$ reported in the literature and provide a novel common mathematical framework in which all of them are unified through a universal $α$ coefficient. Finally, this paper provides a detailed guide of the use of this theoretical framework in a large case study on DevOps culture. We explain how $α$ coefficients are computed and interpreted using a widely used software tool for qualitative analysis like Atlas.ti. We expect that this work will help empirical researchers, particularly in software engineering, to improve the quality and trustworthiness of their studies.",Ángel González-Prieto|Jorge Perez|Jessica Diaz|Daniel López-Fernández,,https://arxiv.org/abs/2008.00977v2,https://arxiv.org/pdf/2008.00977v2,https://doi.org/10.1016/j.jss.2023.111707,"35 pages, 15 figures, 12 tables",Journal of Systems and Software (2023),10.1016/j.jss.2023.111707,cs.SE,cs.SE|stat.ME,https://arxiv.org/pdf/2008.00977v2.pdf
2007.14919v1,2020-07-29T15:52:49Z,2020-07-29 15:52:49,Error Correction Codes for COVID-19 Virus and Antibody Testing: Using Pooled Testing to Increase Test Reliability,"We consider a novel method to increase the reliability of COVID-19 virus or antibody tests by using specially designed pooled testings. Instead of testing nasal swab or blood samples from individual persons, we propose to test mixtures of samples from many individuals. The pooled sample testing method proposed in this paper also serves a different purpose: for increasing test reliability and providing accurate diagnoses even if the tests themselves are not very accurate. Our method uses ideas from compressed sensing and error-correction coding to correct for a certain number of errors in the test results. The intuition is that when each individual's sample is part of many pooled sample mixtures, the test results from all of the sample mixtures contain redundant information about each individual's diagnosis, which can be exploited to automatically correct for wrong test results in exactly the same way that error correction codes correct errors introduced in noisy communication channels. While such redundancy can also be achieved by simply testing each individual's sample multiple times, we present simulations and theoretical arguments that show that our method is significantly more efficient in increasing diagnostic accuracy. In contrast to group testing and compressed sensing which aim to reduce the number of required tests, this proposed error correction code idea purposefully uses pooled testing to increase test accuracy, and works not only in the ""undersampling"" regime, but also in the ""oversampling"" regime, where the number of tests is bigger than the number of subjects. The results in this paper run against traditional beliefs that, ""even though pooled testing increased test capacity, pooled testings were less reliable than testing individuals separately.""",Jirong Yi|Myung Cho|Xiaodong Wu|Weiyu Xu|Raghu Mudumbai,,https://arxiv.org/abs/2007.14919v1,https://arxiv.org/pdf/2007.14919v1,,"14 pages, 15 figures",,,q-bio.QM,q-bio.QM|stat.ME,https://arxiv.org/pdf/2007.14919v1.pdf
2007.14132v1,2020-07-28T11:23:40Z,2020-07-28 11:23:40,Toward Reliable Models for Authenticating Multimedia Content: Detecting Resampling Artifacts With Bayesian Neural Networks,"In multimedia forensics, learning-based methods provide state-of-the-art performance in determining origin and authenticity of images and videos. However, most existing methods are challenged by out-of-distribution data, i.e., with characteristics that are not covered in the training set. This makes it difficult to know when to trust a model, particularly for practitioners with limited technical background.
  In this work, we make a first step toward redesigning forensic algorithms with a strong focus on reliability. To this end, we propose to use Bayesian neural networks (BNN), which combine the power of deep neural networks with the rigorous probabilistic formulation of a Bayesian framework. Instead of providing a point estimate like standard neural networks, BNNs provide distributions that express both the estimate and also an uncertainty range.
  We demonstrate the usefulness of this framework on a classical forensic task: resampling detection. The BNN yields state-of-the-art detection performance, plus excellent capabilities for detecting out-of-distribution samples. This is demonstrated for three pathologic issues in resampling detection, namely unseen resampling factors, unseen JPEG compression, and unseen resampling algorithms. We hope that this proposal spurs further research toward reliability in multimedia forensics.",Anatol Maier|Benedikt Lorch|Christian Riess,,https://arxiv.org/abs/2007.14132v1,https://arxiv.org/pdf/2007.14132v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2007.14132v1.pdf
2007.09868v1,2020-07-20T03:40:51Z,2020-07-20 03:40:51,Attention Sequence to Sequence Model for Machine Remaining Useful Life Prediction,"Accurate estimation of remaining useful life (RUL) of industrial equipment can enable advanced maintenance schedules, increase equipment availability and reduce operational costs. However, existing deep learning methods for RUL prediction are not completely successful due to the following two reasons. First, relying on a single objective function to estimate the RUL will limit the learned representations and thus affect the prediction accuracy. Second, while longer sequences are more informative for modelling the sensor dynamics of equipment, existing methods are less effective to deal with very long sequences, as they mainly focus on the latest information. To address these two problems, we develop a novel attention-based sequence to sequence with auxiliary task (ATS2S) model. In particular, our model jointly optimizes both reconstruction loss to empower our model with predictive capabilities (by predicting next input sequence given current input sequence) and RUL prediction loss to minimize the difference between the predicted RUL and actual RUL. Furthermore, to better handle longer sequence, we employ the attention mechanism to focus on all the important input information during training process. Finally, we propose a new dual-latent feature representation to integrate the encoder features and decoder hidden states, to capture rich semantic information in data. We conduct extensive experiments on four real datasets to evaluate the efficacy of the proposed method. Experimental results show that our proposed method can achieve superior performance over 13 state-of-the-art methods consistently.",Mohamed Ragab|Zhenghua Chen|Min Wu|Chee-Keong Kwoh|Ruqiang Yan|Xiaoli Li,,https://arxiv.org/abs/2007.09868v1,https://arxiv.org/pdf/2007.09868v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2007.09868v1.pdf
2007.08128v3,2020-07-16T06:02:18Z,2021-11-01 05:50:15,Detecting Out-of-distribution Samples via Variational Auto-encoder with Reliable Uncertainty Estimation,"Variational autoencoders (VAEs) are influential generative models with rich representation capabilities from the deep neural network architecture and Bayesian method. However, VAE models have a weakness that assign a higher likelihood to out-of-distribution (OOD) inputs than in-distribution (ID) inputs. To address this problem, a reliable uncertainty estimation is considered to be critical for in-depth understanding of OOD inputs. In this study, we propose an improved noise contrastive prior (INCP) to be able to integrate into the encoder of VAEs, called INCPVAE. INCP is scalable, trainable and compatible with VAEs, and it also adopts the merits from the INCP for uncertainty estimation. Experiments on various datasets demonstrate that compared to the standard VAEs, our model is superior in uncertainty estimation for the OOD data and is robust in anomaly detection tasks. The INCPVAE model obtains reliable uncertainty estimation for OOD inputs and solves the OOD problem in VAE models.",Xuming Ran|Mingkun Xu|Lingrui Mei|Qi Xu|Quanying Liu,,https://arxiv.org/abs/2007.08128v3,https://arxiv.org/pdf/2007.08128v3,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2007.08128v3.pdf
2007.05857v1,2020-07-11T21:24:48Z,2020-07-11 21:24:48,Reliability of decisions based on tests: Fourier analysis of Boolean decision functions,"Items in a test are often used as a basis for making decisions and such tests are therefore required to have good psychometric properties, like unidimensionality. In many cases the sum score is used in combination with a threshold to decide between pass or fail, for instance. Here we consider whether such a decision function is appropriate, without a latent variable model, and which properties of a decision function are desirable. We consider reliability (stability) of the decision function, i.e., does the decision change upon perturbations, or changes in a fraction of the outcomes of the items (measurement error). We are concerned with questions of whether the sum score is the best way to aggregate the items, and if so why. We use ideas from test theory, social choice theory, graphical models, computer science and probability theory to answer these questions. We conclude that a weighted sum score has desirable properties that (i) fit with test theory and is observable (similar to a condition like conditional association), (ii) has the property that a decision is stable (reliable), and (iii) satisfies Rousseau's criterion that the input should match the decision. We use Fourier analysis of Boolean functions to investigate whether a decision function is stable and to figure out which (set of) items has proportionally too large an influence on the decision. To apply these techniques we invoke ideas from graphical models and use a pseudo-likelihood factorisation of the probability distribution.",Lourens Waldorp|Maarten Marsman|Denny Borsboom,,https://arxiv.org/abs/2007.05857v1,https://arxiv.org/pdf/2007.05857v1,,"41 pages, 4 figures",,,stat.ME,stat.ME|stat.OT,https://arxiv.org/pdf/2007.05857v1.pdf
2007.05304v7,2020-07-10T11:09:00Z,2020-12-17 09:43:00,To BAN or not to BAN: Bayesian Attention Networks for Reliable Hate Speech Detection,"Hate speech is an important problem in the management of user-generated content. To remove offensive content or ban misbehaving users, content moderators need reliable hate speech detectors. Recently, deep neural networks based on the transformer architecture, such as the (multilingual) BERT model, achieve superior performance in many natural language classification tasks, including hate speech detection. So far, these methods have not been able to quantify their output in terms of reliability. We propose a Bayesian method using Monte Carlo dropout within the attention layers of the transformer models to provide well-calibrated reliability estimates. We evaluate and visualize the results of the proposed approach on hate speech detection problems in several languages. Additionally, we test if affective dimensions can enhance the information extracted by the BERT model in hate speech classification. Our experiments show that Monte Carlo dropout provides a viable mechanism for reliability estimation in transformer networks. Used within the BERT model, it ofers state-of-the-art classification performance and can detect less trusted predictions. Also, it was observed that affective dimensions extracted using sentic computing methods can provide insights toward interpretation of emotions involved in hate speech. Our approach not only improves the classification performance of the state-of-the-art multilingual BERT model but the computed reliability scores also significantly reduce the workload in an inspection of ofending cases and reannotation campaigns. The provided visualization helps to understand the borderline outcomes.",Kristian Miok|Blaz Skrlj|Daniela Zaharie|Marko Robnik-Sikonja,,https://arxiv.org/abs/2007.05304v7,https://arxiv.org/pdf/2007.05304v7,,,,,stat.AP,stat.AP|stat.ML,https://arxiv.org/pdf/2007.05304v7.pdf
2007.04470v3,2020-07-08T23:05:18Z,2021-07-07 15:38:13,Finite mixture models do not reliably learn the number of components,"Scientists and engineers are often interested in learning the number of subpopulations (or components) present in a data set. A common suggestion is to use a finite mixture model (FMM) with a prior on the number of components. Past work has shown the resulting FMM component-count posterior is consistent; that is, the posterior concentrates on the true, generating number of components. But consistency requires the assumption that the component likelihoods are perfectly specified, which is unrealistic in practice. In this paper, we add rigor to data-analysis folk wisdom by proving that under even the slightest model misspecification, the FMM component-count posterior diverges: the posterior probability of any particular finite number of components converges to 0 in the limit of infinite data. Contrary to intuition, posterior-density consistency is not sufficient to establish this result. We develop novel sufficient conditions that are more realistic and easily checkable than those common in the asymptotics literature. We illustrate practical consequences of our theory on simulated and real data.",Diana Cai|Trevor Campbell|Tamara Broderick,,https://arxiv.org/abs/2007.04470v3,https://arxiv.org/pdf/2007.04470v3,,"Proceedings of the 38th International Conference on Machine Learning (ICML), to appear. 25 pages, 4 figures",,,math.ST,math.ST|stat.ME|stat.ML,https://arxiv.org/pdf/2007.04470v3.pdf
2007.03313v1,2020-07-07T10:00:32Z,2020-07-07 10:00:32,Predictive Maintenance for Edge-Based Sensor Networks: A Deep Reinforcement Learning Approach,"Failure of mission-critical equipment interrupts production and results in monetary loss. The risk of unplanned equipment downtime can be minimized through Predictive Maintenance of revenue generating assets to ensure optimal performance and safe operation of equipment. However, the increased sensorization of the equipment generates a data deluge, and existing machine-learning based predictive model alone becomes inadequate for timely equipment condition predictions. In this paper, a model-free Deep Reinforcement Learning algorithm is proposed for predictive equipment maintenance from an equipment-based sensor network context. Within each equipment, a sensor device aggregates raw sensor data, and the equipment health status is analyzed for anomalous events. Unlike traditional black-box regression models, the proposed algorithm self-learns an optimal maintenance policy and provides actionable recommendation for each equipment. Our experimental results demonstrate the potential for broader range of equipment maintenance applications as an automatic learning framework.",Kevin Shen Hoong Ong|Dusit Niyato|Chau Yuen,,https://arxiv.org/abs/2007.03313v1,https://arxiv.org/pdf/2007.03313v1,,"6 pages, 5 figures, accepted in IEEE WF-IoT 2020",,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2007.03313v1.pdf
2007.01659v4,2020-07-03T12:54:08Z,2021-03-22 06:23:53,Diagnostic Uncertainty Calibration: Towards Reliable Machine Predictions in Medical Domain,"We propose an evaluation framework for class probability estimates (CPEs) in the presence of label uncertainty, which is commonly observed as diagnosis disagreement between experts in the medical domain. We also formalize evaluation metrics for higher-order statistics, including inter-rater disagreement, to assess predictions on label uncertainty. Moreover, we propose a novel post-hoc method called $alpha$-calibration, that equips neural network classifiers with calibrated distributions over CPEs. Using synthetic experiments and a large-scale medical imaging application, we show that our approach significantly enhances the reliability of uncertainty estimates: disagreement probabilities and posterior CPEs.",Takahiro Mimori|Keiko Sasada|Hirotaka Matsui|Issei Sato,,https://arxiv.org/abs/2007.01659v4,https://arxiv.org/pdf/2007.01659v4,,"31 pages, 6 figures",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2007.01659v4.pdf
2007.01380v1,2020-07-02T20:44:07Z,2020-07-02 20:44:07,Deep reinforcement learning driven inspection and maintenance planning under incomplete information and constraints,"Determination of inspection and maintenance policies for minimizing long-term risks and costs in deteriorating engineering environments constitutes a complex optimization problem. Major computational challenges include the (i) curse of dimensionality, due to exponential scaling of state/action set cardinalities with the number of components; (ii) curse of history, related to exponentially growing decision-trees with the number of decision-steps; (iii) presence of state uncertainties, induced by inherent environment stochasticity and variability of inspection/monitoring measurements; (iv) presence of constraints, pertaining to stochastic long-term limitations, due to resource scarcity and other infeasible/undesirable system responses. In this work, these challenges are addressed within a joint framework of constrained Partially Observable Markov Decision Processes (POMDP) and multi-agent Deep Reinforcement Learning (DRL). POMDPs optimally tackle (ii)-(iii), combining stochastic dynamic programming with Bayesian inference principles. Multi-agent DRL addresses (i), through deep function parametrizations and decentralized control assumptions. Challenge (iv) is herein handled through proper state augmentation and Lagrangian relaxation, with emphasis on life-cycle risk-based constraints and budget limitations. The underlying algorithmic steps are provided, and the proposed framework is found to outperform well-established policy baselines and facilitate adept prescription of inspection and intervention actions, in cases where decisions must be made in the most resource- and risk-aware manner.",C. P. Andriotis|K. G. Papakonstantinou,,https://arxiv.org/abs/2007.01380v1,https://arxiv.org/pdf/2007.01380v1,,,,,cs.AI,cs.AI|cs.LG|math.OC|stat.ML,https://arxiv.org/pdf/2007.01380v1.pdf
2007.01171v1,2020-07-02T14:56:54Z,2020-07-02 14:56:54,Pricing service maintenance contracts using predictive analytics,"As more manufacturers shift their focus from selling products to end solutions, full-service maintenance contracts gain traction in the business world. These contracts cover all maintenance related costs during a predetermined horizon in exchange for a fixed service fee and relieve customers from uncertain maintenance costs. To guarantee profitability, the service fees should at least cover the expected costs during the contract horizon. As these expected costs may depend on several machine-dependent characteristics, e.g. operational environment, the service fees should also be differentiated based on these characteristics. If not, customers that are less prone to high maintenance costs will not buy into or renege on the contract. The latter can lead to adverse selection and leave the service provider with a maintenance-heavy portfolio, which may be detrimental to the profitability of the service contracts. We contribute to the literature with a data-driven tariff plan based on the calibration of predictive models that take into account the different machine profiles. This conveys to the service provider which machine profiles should be attracted at which price. We demonstrate the advantage of a differentiated tariff plan and show how it better protects against adverse selection.",Laurens Deprez|Katrien Antonio|Robert Boute,,https://arxiv.org/abs/2007.01171v1,https://arxiv.org/pdf/2007.01171v1,https://doi.org/10.1016/j.ejor.2020.08.022,,,10.1016/j.ejor.2020.08.022,stat.AP,stat.AP,https://arxiv.org/pdf/2007.01171v1.pdf
2007.00402v1,2020-07-01T11:47:15Z,2020-07-01 11:47:15,Sequential Bayesian optimal experimental design for structural reliability analysis,"Structural reliability analysis is concerned with estimation of the probability of a critical event taking place, described by $P(g(\textbf{X}) \leq 0)$ for some $n$-dimensional random variable $\textbf{X}$ and some real-valued function $g$. In many applications the function $g$ is practically unknown, as function evaluation involves time consuming numerical simulation or some other form of experiment that is expensive to perform. The problem we address in this paper is how to optimally design experiments, in a Bayesian decision theoretic fashion, when the goal is to estimate the probability $P(g(\textbf{X}) \leq 0)$ using a minimal amount of resources. As opposed to existing methods that have been proposed for this purpose, we consider a general structural reliability model given in hierarchical form. We therefore introduce a general formulation of the experimental design problem, where we distinguish between the uncertainty related to the random variable $\textbf{X}$ and any additional epistemic uncertainty that we want to reduce through experimentation. The effectiveness of a design strategy is evaluated through a measure of residual uncertainty, and efficient approximation of this quantity is crucial if we want to apply algorithms that search for an optimal strategy. The method we propose is based on importance sampling combined with the unscented transform for epistemic uncertainty propagation. We implement this for the myopic (one-step look ahead) alternative, and demonstrate the effectiveness through a series of numerical experiments.",Christian Agrell|Kristina Rognlien Dahl,,https://arxiv.org/abs/2007.00402v1,https://arxiv.org/pdf/2007.00402v1,https://doi.org/10.1007/s11222-021-10000-2,"27 pages, 13 figures","Statistics and Computing, vol. 31, no. 27 (2021)",10.1007/s11222-021-10000-2,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/2007.00402v1.pdf
2007.00306v1,2020-07-01T07:59:35Z,2020-07-01 07:59:35,Interference Distribution Prediction for Link Adaptation in Ultra-Reliable Low-Latency Communications,"The strict latency and reliability requirements of ultra-reliable low-latency communications (URLLC) use cases are among the main drivers in fifth generation (5G) network design. Link adaptation (LA) is considered to be one of the bottlenecks to realize URLLC. In this paper, we focus on predicting the signal to interference plus noise ratio at the user to enhance the LA. Motivated by the fact that most of the URLLC use cases with most extreme latency and reliability requirements are characterized by semi-deterministic traffic, we propose to exploit the time correlation of the interference to compute useful statistics needed to predict the interference power in the next transmission. This prediction is exploited in the LA context to maximize the spectral efficiency while guaranteeing reliability at an arbitrary level. Numerical results are compared with state of the art interference prediction techniques for LA. We show that exploiting time correlation of the interference is an important enabler of URLLC.",Alessandro Brighente|Jafar Mohammadi|Paolo Baracca,,https://arxiv.org/abs/2007.00306v1,https://arxiv.org/pdf/2007.00306v1,,IEEE VTC-Spring 2020; minor notation changes in Sections III-V,,,eess.SP,eess.SP|cs.IT|stat.ML,https://arxiv.org/pdf/2007.00306v1.pdf
2006.16556v1,2020-06-30T06:38:08Z,2020-06-30 06:38:08,Graph Neural Networks for Leveraging Industrial Equipment Structure: An application to Remaining Useful Life Estimation,"Automated equipment health monitoring from streaming multisensor time-series data can be used to enable condition-based maintenance, avoid sudden catastrophic failures, and ensure high operational availability. We note that most complex machinery has a well-documented and readily accessible underlying structure capturing the inter-dependencies between sub-systems or modules. Deep learning models such as those based on recurrent neural networks (RNNs) or convolutional neural networks (CNNs) fail to explicitly leverage this potentially rich source of domain-knowledge into the learning procedure. In this work, we propose to capture the structure of a complex equipment in the form of a graph, and use graph neural networks (GNNs) to model multi-sensor time-series data. Using remaining useful life estimation as an application task, we evaluate the advantage of incorporating the graph structure via GNNs on the publicly available turbofan engine benchmark dataset. We observe that the proposed GNN-based RUL estimation model compares favorably to several strong baselines from literature such as those based on RNNs and CNNs. Additionally, we observe that the learned network is able to focus on the module (node) with impending failure through a simple attention mechanism, potentially paving the way for actionable diagnosis.",Jyoti Narwariya|Pankaj Malhotra|Vishnu TV|Lovekesh Vig|Gautam Shroff,,https://arxiv.org/abs/2006.16556v1,https://arxiv.org/pdf/2006.16556v1,,Accepted at AAAI workshop DLGMA'20,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2006.16556v1.pdf
2006.15568v2,2020-06-28T10:39:39Z,2021-02-08 17:56:38,Reliable Categorical Variational Inference with Mixture of Discrete Normalizing Flows,"Variational approximations are increasingly based on gradient-based optimization of expectations estimated by sampling. Handling discrete latent variables is then challenging because the sampling process is not differentiable. Continuous relaxations, such as the Gumbel-Softmax for categorical distribution, enable gradient-based optimization, but do not define a valid probability mass for discrete observations. In practice, selecting the amount of relaxation is difficult and one needs to optimize an objective that does not align with the desired one, causing problems especially with models having strong meaningful priors. We provide an alternative differentiable reparameterization for categorical distribution by composing it as a mixture of discrete normalizing flows. It defines a proper discrete distribution, allows directly optimizing the evidence lower bound, and is less sensitive to the hyperparameter controlling relaxation.",Tomasz Kuśmierczyk|Arto Klami,,https://arxiv.org/abs/2006.15568v2,https://arxiv.org/pdf/2006.15568v2,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2006.15568v2.pdf
2006.10255v2,2020-06-18T03:38:12Z,2020-10-28 03:38:24,Calibrated Reliable Regression using Maximum Mean Discrepancy,"Accurate quantification of uncertainty is crucial for real-world applications of machine learning. However, modern deep neural networks still produce unreliable predictive uncertainty, often yielding over-confident predictions. In this paper, we are concerned with getting well-calibrated predictions in regression tasks. We propose the calibrated regression method using the maximum mean discrepancy by minimizing the kernel embedding measure. Theoretically, the calibration error of our method asymptotically converges to zero when the sample size is large enough. Experiments on non-trivial real datasets show that our method can produce well-calibrated and sharp prediction intervals, which outperforms the related state-of-the-art methods.",Peng Cui|Wenbo Hu|Jun Zhu,,https://arxiv.org/abs/2006.10255v2,https://arxiv.org/pdf/2006.10255v2,,Accepted to NeurIPS'2020. Full version with appendix,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2006.10255v2.pdf
2006.07107v3,2020-06-12T12:12:12Z,2021-09-13 15:11:26,Understanding and Resolving Performance Degradation in Graph Convolutional Networks,"A Graph Convolutional Network (GCN) stacks several layers and in each layer performs a PROPagation operation (PROP) and a TRANsformation operation (TRAN) for learning node representations over graph-structured data. Though powerful, GCNs tend to suffer performance drop when the model gets deep. Previous works focus on PROPs to study and mitigate this issue, but the role of TRANs is barely investigated. In this work, we study performance degradation of GCNs by experimentally examining how stacking only TRANs or PROPs works. We find that TRANs contribute significantly, or even more than PROPs, to declining performance, and moreover that they tend to amplify node-wise feature variance in GCNs, causing variance inflammation that we identify as a key factor for causing performance drop. Motivated by such observations, we propose a variance-controlling technique termed Node Normalization (NodeNorm), which scales each node's features using its own standard deviation. Experimental results validate the effectiveness of NodeNorm on addressing performance degradation of GCNs. Specifically, it enables deep GCNs to outperform shallow ones in cases where deep models are needed, and to achieve comparable results with shallow ones on 6 benchmark datasets. NodeNorm is a generic plug-in and can well generalize to other GNN architectures. Code is publicly available at https://github.com/miafei/NodeNorm.",Kuangqi Zhou|Yanfei Dong|Kaixin Wang|Wee Sun Lee|Bryan Hooi|Huan Xu|Jiashi Feng,,https://arxiv.org/abs/2006.07107v3,https://arxiv.org/pdf/2006.07107v3,,CIKM 2021,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2006.07107v3.pdf
2006.07021v2,2020-06-12T09:06:16Z,2020-07-01 03:18:38,A benchmark study on reliable molecular supervised learning via Bayesian learning,"Virtual screening aims to find desirable compounds from chemical library by using computational methods. For this purpose with machine learning, model outputs that can be interpreted as predictive probability will be beneficial, in that a high prediction score corresponds to high probability of correctness. In this work, we present a study on the prediction performance and reliability of graph neural networks trained with the recently proposed Bayesian learning algorithms. Our work shows that Bayesian learning algorithms allow well-calibrated predictions for various GNN architectures and classification tasks. Also, we show the implications of reliable predictions on virtual screening, where Bayesian learning may lead to higher success in finding hit compounds.",Doyeong Hwang|Grace Lee|Hanseok Jo|Seyoul Yoon|Seongok Ryu,,https://arxiv.org/abs/2006.07021v2,https://arxiv.org/pdf/2006.07021v2,,"To be appeared in ICML 2020 Workshop ""Uncertainty and Robustness in Deep Learning""",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2006.07021v2.pdf
2006.04910v3,2020-06-08T19:58:35Z,2020-10-30 14:50:33,"Variational Variance: Simple, Reliable, Calibrated Heteroscedastic Noise Variance Parameterization","Brittle optimization has been observed to adversely impact model likelihoods for regression and VAEs when simultaneously fitting neural network mappings from a (random) variable onto the mean and variance of a dependent Gaussian variable. Previous works have bolstered optimization and improved likelihoods, but fail other basic posterior predictive checks (PPCs). Under the PPC framework, we propose critiques to test predictive mean and variance calibration and the predictive distribution's ability to generate sensible data. We find that our attractively simple solution, to treat heteroscedastic variance variationally, sufficiently regularizes variance to pass these PPCs. We consider a diverse gamut of existing and novel priors and find our methods preserve or outperform existing model likelihoods while significantly improving parameter calibration and sample quality for regression and VAEs.",Andrew Stirn|David A. Knowles,,https://arxiv.org/abs/2006.04910v3,https://arxiv.org/pdf/2006.04910v3,,"17 pages, 6 figures, 10 tables",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2006.04910v3.pdf
2006.04461v2,2020-06-08T10:35:27Z,2021-07-20 11:18:08,Bayesian beta nonlinear models with constrained parameters to describe ruminal degradation kinetics,"The models used to describe the kinetics of ruminal degradation are usually nonlinear models where the dependent variable is the proportion of degraded food. The method of least squares is the standard approach used to estimate the unknown parameters but this method can lead to unacceptable predictions. To solve this issue, a beta nonlinear model and the Bayesian perspective is proposed in this article. The application of standard methodologies to obtain prior distributions, such as the Jeffreys prior or the reference priors, involves serious difficulties here because this model is a nonlinear non-normal regression model, and the constrained parameters appear in the log-likelihood function through the Gamma function. This paper proposes an objective method to obtain the prior distribution, which can be applied to other models with similar complexity, can be easily implemented in OpenBUGS, and solves the problem of unacceptable predictions. The model is generalized to a larger class of models. The methodology was applied to real data with three models that were compared using the Deviance Information Criterion and the root mean square prediction error. A simulation study was performed to evaluate the coverage of the credible intervals.",Diego Salmerón,,https://arxiv.org/abs/2006.04461v2,https://arxiv.org/pdf/2006.04461v2,https://doi.org/10.1080/02664763.2021.1913105,,Diego Salmerón (2021) Bayesian beta nonlinear models with constrained parameters to describe ruminal degradation kinetics. Journal of Applied Statistics,10.1080/02664763.2021.1913105,stat.AP,stat.AP,https://arxiv.org/pdf/2006.04461v2.pdf
2006.03729v1,2020-06-05T23:02:10Z,2020-06-05 23:02:10,Health Indicator Forecasting for Improving Remaining Useful Life Estimation,"Prognostics is concerned with predicting the future health of the equipment and any potential failures. With the advances in the Internet of Things (IoT), data-driven approaches for prognostics that leverage the power of machine learning models are gaining popularity. One of the most important categories of data-driven approaches relies on a predefined or learned health indicator to characterize the equipment condition up to the present time and make inference on how it is likely to evolve in the future. In these approaches, health indicator forecasting that constructs the health indicator curve over the lifespan using partially observed measurements (i.e., health indicator values within an initial period) plays a key role. Existing health indicator forecasting algorithms, such as the functional Empirical Bayesian approach, the regression-based formulation, a naive scenario matching based on the nearest neighbor, have certain limitations. In this paper, we propose a new `generative + scenario matching' algorithm for health indicator forecasting. The key idea behind the proposed approach is to first non-parametrically fit the underlying health indicator curve with a continuous Gaussian Process using a sample of run-to-failure health indicator curves. The proposed approach then generates a rich set of random curves from the learned distribution, attempting to obtain all possible variations of the target health condition evolution process over the system's lifespan. The health indicator extrapolation for a piece of functioning equipment is inferred as the generated curve that has the highest matching level within the observed period. Our experimental results show the superiority of our algorithm over the other state-of-the-art methods.",Qiyao Wang|Ahmed Farahat|Chetan Gupta|Haiyan Wang,,https://arxiv.org/abs/2006.03729v1,https://arxiv.org/pdf/2006.03729v1,,Accepted by IEEE International Conference on Prognostics and Health Management 2020,,,cs.LG,cs.LG|stat.AP|stat.ML,https://arxiv.org/pdf/2006.03729v1.pdf
2006.03481v6,2020-06-05T14:24:27Z,2022-03-04 12:51:36,Providing reliability in Recommender Systems through Bernoulli Matrix Factorization,"Beyond accuracy, quality measures are gaining importance in modern recommender systems, with reliability being one of the most important indicators in the context of collaborative filtering. This paper proposes Bernoulli Matrix Factorization (BeMF), which is a matrix factorization model, to provide both prediction values and reliability values. BeMF is a very innovative approach from several perspectives: a) it acts on model-based collaborative filtering rather than on memory-based filtering, b) it does not use external methods or extended architectures, such as existing solutions, to provide reliability, c) it is based on a classification-based model instead of traditional regression-based models, and d) matrix factorization formalism is supported by the Bernoulli distribution to exploit the binary nature of the designed classification model. The experimental results show that the more reliable a prediction is, the less liable it is to be wrong: recommendation quality improves after the most reliable predictions are selected. State-of-the-art quality measures for reliability have been tested, which shows that BeMF outperforms previous baseline methods and models.",Fernando Ortega|Raúl Lara-Cabrera|Ángel González-Prieto|Jesús Bobadilla,,https://arxiv.org/abs/2006.03481v6,https://arxiv.org/pdf/2006.03481v6,https://doi.org/10.1016/j.ins.2020.12.001,"28 pages, 8 figures, 8 tables","Information Sciences, 2020",10.1016/j.ins.2020.12.001,cs.LG,cs.LG|cs.IR|stat.ML,https://arxiv.org/pdf/2006.03481v6.pdf
2006.01586v1,2020-06-02T13:25:36Z,2020-06-02 13:25:36,"Stochastic Modeling of an Infectious Disease, Part I: Understand the Negative Binomial Distribution and Predict an Epidemic More Reliably","Why are the epidemic patterns of COVID-19 so different among different cities or countries which are similar in their populations, medical infrastructures, and people's behavior? Why are forecasts or predictions made by so-called experts often grossly wrong, concerning the numbers of people who get infected or die? The purpose of this study is to better understand the stochastic nature of an epidemic disease, and answer the above questions. Much of the work on infectious diseases has been based on ""SIR deterministic models,"" (Kermack and McKendrick:1927.) We will explore stochastic models that can capture the essence of the seemingly erratic behavior of an infectious disease. A stochastic model, in its formulation, takes into account the random nature of an infectious disease.
  The stochastic model we study here is based on the ""birth-and-death process with immigration"" (BDI for short), which was proposed in the study of population growth or extinction of some biological species. The BDI process model ,however, has not been investigated by the epidemiology community. The BDI process is one of a few birth-and-death processes, which we can solve analytically. Its time-dependent probability distribution function is a ""negative binomial distribution"" with its parameter $r$ less than $1$. The ""coefficient of variation"" of the process is larger than $\sqrt{1/r} > 1$. Furthermore, it has a long tail like the zeta distribution. These properties explain why infection patterns exhibit enormously large variations. The number of infected predicted by a deterministic model is much greater than the median of the distribution. This explains why any forecast based on a deterministic model will fail more often than not.",Hisashi Kobayashi,,https://arxiv.org/abs/2006.01586v1,https://arxiv.org/pdf/2006.01586v1,,"28 pages, 14 figures",,,q-bio.PE,q-bio.PE|stat.ME,https://arxiv.org/pdf/2006.01586v1.pdf
2005.14181v2,2020-05-28T17:52:26Z,2020-09-26 14:11:51,Bayesian Restoration of Audio Degraded by Low-Frequency Pulses Modeled via Gaussian Process,"A common defect found when reproducing old vinyl and gramophone recordings with mechanical devices are the long pulses with significant low-frequency content caused by the interaction of the arm-needle system with deep scratches or even breakages on the media surface. Previous approaches to their suppression on digital counterparts of the recordings depend on a prior estimation of the pulse location, usually performed via heuristic methods. This paper proposes a novel Bayesian approach capable of jointly estimating the pulse location; interpolating the almost annihilated signal underlying the strong discontinuity that initiates the pulse; and also estimating the long pulse tail by a simple Gaussian Process, allowing its suppression from the corrupted signal. The posterior distribution for the model parameters as well for the pulse is explored via Markov-Chain Monte Carlo (MCMC) algorithms. Controlled experiments indicate that the proposed method, while requiring significantly less user intervention, achieves perceptual results similar to those of previous approaches and performs well when dealing with naturally degraded signals.",Hugo Tremonte de Carvalho|Flávio Rainho Ávila|Luiz Wagner Pereira Biscainho,,https://arxiv.org/abs/2005.14181v2,https://arxiv.org/pdf/2005.14181v2,https://doi.org/10.1109/JSTSP.2020.3033410,"14 pages, 7 figures, 4 tables. Submitted to IEEE Journal of Selected Topics in Signal Processing - Special Issue ""Reconstruction of audio from incomplete or highly degraded observations""",,10.1109/JSTSP.2020.3033410,eess.AS,eess.AS|cs.SD|eess.SP|stat.AP|stat.ML,https://arxiv.org/pdf/2005.14181v2.pdf
2005.07788v1,2020-05-15T21:17:06Z,2020-05-15 21:17:06,Reliable Local Explanations for Machine Listening,"One way to analyse the behaviour of machine learning models is through local explanations that highlight input features that maximally influence model predictions. Sensitivity analysis, which involves analysing the effect of input perturbations on model predictions, is one of the methods to generate local explanations. Meaningful input perturbations are essential for generating reliable explanations, but there exists limited work on what such perturbations are and how to perform them. This work investigates these questions in the context of machine listening models that analyse audio. Specifically, we use a state-of-the-art deep singing voice detection (SVD) model to analyse whether explanations from SoundLIME (a local explanation method) are sensitive to how the method perturbs model inputs. The results demonstrate that SoundLIME explanations are sensitive to the content in the occluded input regions. We further propose and demonstrate a novel method for quantitatively identifying suitable content type(s) for reliably occluding inputs of machine listening models. The results for the SVD model suggest that the average magnitude of input mel-spectrogram bins is the most suitable content type for temporal explanations.",Saumitra Mishra|Emmanouil Benetos|Bob L. Sturm|Simon Dixon,,https://arxiv.org/abs/2005.07788v1,https://arxiv.org/pdf/2005.07788v1,,8 pages plus references. Accepted at the IJCNN 2020 Special Session on Explainable Computational/Artificial Intelligence. Camera-ready version,,,eess.AS,eess.AS|cs.LG|cs.SD|stat.ML,https://arxiv.org/pdf/2005.07788v1.pdf
2005.04089v2,2020-05-08T15:06:36Z,2021-11-11 13:51:54,How Reliable are Bootstrap-based Heteroskedasticity Robust Tests?,"We develop theoretical finite-sample results concerning the size of wild bootstrap-based heteroskedasticity robust tests in linear regression models. In particular, these results provide an efficient diagnostic check, which can be used to weed out tests that are unreliable for a given testing problem in the sense that they overreject substantially. This allows us to assess the reliability of a large variety of wild bootstrap-based tests in an extensive numerical study.",Benedikt M. Pötscher|David Preinerstorfer,,https://arxiv.org/abs/2005.04089v2,https://arxiv.org/pdf/2005.04089v2,,"59 pages, 1 figure","Econometric Theory 39 (2023), 789-847",,math.ST,math.ST|econ.EM|stat.ME,https://arxiv.org/pdf/2005.04089v2.pdf
2005.01302v3,2020-05-04T07:19:50Z,2020-06-14 05:22:59,Simulation free reliability analysis: A physics-informed deep learning based approach,"This paper presents a simulation free framework for solving reliability analysis problems. The method proposed is rooted in a recently developed deep learning approach, referred to as the physics-informed neural network. The primary idea is to learn the neural network parameters directly from the physics of the problem. With this, the need for running simulation and generating data is completely eliminated. Additionally, the proposed approach also satisfies physical laws such as invariance properties and conservation laws associated with the problem. The proposed approach is used for solving three benchmark reliability analysis problems. Results obtained illustrates that the proposed approach is highly accurate. Moreover, the primary bottleneck of solving reliability analysis problems, i.e., running expensive simulations to generate data, is eliminated with this method.",Souvik Chakraborty,,https://arxiv.org/abs/2005.01302v3,https://arxiv.org/pdf/2005.01302v3,,,,,stat.ML,stat.ML,https://arxiv.org/pdf/2005.01302v3.pdf
2004.14480v1,2020-04-27T22:15:17Z,2020-04-27 22:15:17,Calibrating Healthcare AI: Towards Reliable and Interpretable Deep Predictive Models,"The wide-spread adoption of representation learning technologies in clinical decision making strongly emphasizes the need for characterizing model reliability and enabling rigorous introspection of model behavior. While the former need is often addressed by incorporating uncertainty quantification strategies, the latter challenge is addressed using a broad class of interpretability techniques. In this paper, we argue that these two objectives are not necessarily disparate and propose to utilize prediction calibration to meet both objectives. More specifically, our approach is comprised of a calibration-driven learning method, which is also used to design an interpretability technique based on counterfactual reasoning. Furthermore, we introduce \textit{reliability plots}, a holistic evaluation mechanism for model reliability. Using a lesion classification problem with dermoscopy images, we demonstrate the effectiveness of our approach and infer interesting insights about the model behavior.",Jayaraman J. Thiagarajan|Prasanna Sattigeri|Deepta Rajan|Bindya Venkatesh,,https://arxiv.org/abs/2004.14480v1,https://arxiv.org/pdf/2004.14480v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2004.14480v1.pdf
2004.12782v1,2020-04-24T17:39:49Z,2020-04-24 17:39:49,How Reliable are Test Numbers for Revealing the COVID-19 Ground Truth and Applying Interventions?,"The number of confirmed cases of COVID-19 is often used as a proxy for the actual number of ground truth COVID-19 infected cases in both public discourse and policy making. However, the number of confirmed cases depends on the testing policy, and it is important to understand how the number of positive cases obtained using different testing policies reveals the unknown ground truth. We develop an agent-based simulation framework in Python that can simulate various testing policies as well as interventions such as lockdown based on them. The interaction between the agents can take into account various communities and mobility patterns. A distinguishing feature of our framework is the presence of another `flu'-like illness with symptoms similar to COVID-19, that allows us to model the noise in selecting the pool of patients to be tested. We instantiate our model for the city of Bengaluru in India, using census data to distribute agents geographically, and traffic flow mobility data to model long-distance interactions and mixing. We use the simulation framework to compare the performance of three testing policies: Random Symptomatic Testing (RST), Contact Tracing (CT), and a new Location Based Testing policy (LBT). We observe that if a sufficient fraction of symptomatic patients come out for testing, then RST can capture the ground truth quite closely even with very few daily tests. However, CT consistently captures more positive cases. Interestingly, our new LBT, which is operationally less intensive than CT, gives performance that is comparable with CT. In another direction, we compare the efficacy of these three testing policies in enabling lockdown, and observe that CT flattens the ground truth curve maximally, followed closely by LBT, and significantly better than RST.",Aditya Gopalan|Himanshu Tyagi,,https://arxiv.org/abs/2004.12782v1,https://arxiv.org/pdf/2004.12782v1,,,,,cs.SI,cs.SI|cs.LG|q-bio.PE|stat.AP|stat.ML,https://arxiv.org/pdf/2004.12782v1.pdf
2004.10824v1,2020-04-22T19:57:34Z,2020-04-22 19:57:34,Assessing the Reliability of Visual Explanations of Deep Models with Adversarial Perturbations,"The interest in complex deep neural networks for computer vision applications is increasing. This leads to the need for improving the interpretable capabilities of these models. Recent explanation methods present visualizations of the relevance of pixels from input images, thus enabling the direct interpretation of properties of the input that lead to a specific output. These methods produce maps of pixel importance, which are commonly evaluated by visual inspection. This means that the effectiveness of an explanation method is assessed based on human expectation instead of actual feature importance. Thus, in this work we propose an objective measure to evaluate the reliability of explanations of deep models. Specifically, our approach is based on changes in the network's outcome resulting from the perturbation of input images in an adversarial way. We present a comparison between widely-known explanation methods using our proposed approach. Finally, we also propose a straightforward application of our approach to clean relevance maps, creating more interpretable maps without any loss in essential explanation (as per our proposed measure).",Dan Valle|Tiago Pimentel|Adriano Veloso,,https://arxiv.org/abs/2004.10824v1,https://arxiv.org/pdf/2004.10824v1,,Accepted for publication at IJCNN 2020,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2004.10824v1.pdf
2004.04565v3,2020-04-09T14:37:43Z,2020-05-23 15:16:31,CovidSens: A Vision on Reliable Social Sensing for COVID-19,"With the spiraling pandemic of the Coronavirus Disease 2019 (COVID-19), it has becoming inherently important to disseminate accurate and timely information about the disease. Due to the ubiquity of Internet connectivity and smart devices, social sensing is emerging as a dynamic AI-driven sensing paradigm to extract real-time observations from online users. In this paper, we propose CovidSens, a vision of social sensing based risk alert systems to spontaneously obtain and analyze social data to infer COVID-19 propagation. CovidSens can actively help to keep the general public informed about the COVID-19 spread and identify risk-prone areas. The CovidSens concept is motivated by three observations: 1) people actively share their experience of COVID-19 via online social media, 2) official warning channels and news agencies are relatively slower than people reporting on social media, and 3) online users are frequently equipped with powerful mobile devices that can perform data processing and analytics. We envision unprecedented opportunities to leverage posts generated by ordinary people to build real-time sensing and analytic system for gathering and circulating COVID-19 propagation data. Specifically, the vision of CovidSens attempts to answer the questions: How to distill reliable information on COVID-19 with prevailing rumors and misinformation? How to inform the general public about the state of the spread timely and effectively? How to leverage the computational power on edge devices to construct fully integrated edge-based social sensing platforms? In this vision paper, we discuss the roles of CovidSens and identify potential challenges in developing reliable social sensing based risk alert systems. We envision that approaches originating from multiple disciplines can be effective in addressing the challenges. Finally, we outline a few research directions for future work in CovidSens.",Md Tahmid Rashid|Dong Wang,,https://arxiv.org/abs/2004.04565v3,https://arxiv.org/pdf/2004.04565v3,,Artificial Intelligence Review (accepted for publication),,,cs.SI,cs.SI|physics.soc-ph|q-bio.PE|stat.ML,https://arxiv.org/pdf/2004.04565v3.pdf
2004.04402v3,2020-04-09T07:58:02Z,2023-01-06 13:57:16,Reliable Time Prediction in the Markov Stochastic Block Model,"We introduce the Markov Stochastic Block Model (MSBM): a growth model for community based networks where node attributes are assigned through a Markovian dynamic. We rely on HMMs' literature to design prediction methods that are robust to local clustering errors. We focus specifically on the link prediction and collaborative filtering problems and we introduce a new model selection procedure to infer the number of hidden clusters in the network. Our approaches for reliable prediction in MSBMs are not algorithm-dependent in the sense that they can be applied using your favourite clustering tool.  In this paper, we use a recent SDP method to infer the hidden communities and we provide theoretical guarantees. In particular, we identify the relevant signal-to-noise ratio (SNR) in our framework and we prove that the misclassification error decays exponentially fast with respect to this SNR.",Quentin Duchemin,LAMA,https://arxiv.org/abs/2004.04402v3,https://arxiv.org/pdf/2004.04402v3,,,"ESAIM: Probability and Statistics, In press",,cs.SI,cs.SI|math.ST|stat.ME|stat.ML,https://arxiv.org/pdf/2004.04402v3.pdf
2005.04156v1,2020-04-08T14:08:50Z,2020-04-08 14:08:50,Comparison of Evolving Granular Classifiers applied to Anomaly Detection for Predictive Maintenance in Computing Centers,"Log-based predictive maintenance of computing centers is a main concern regarding the worldwide computing grid that supports the CERN (European Organization for Nuclear Research) physics experiments. A log, as event-oriented adhoc information, is quite often given as unstructured big data. Log data processing is a time-consuming computational task. The goal is to grab essential information from a continuously changeable grid environment to construct a classification model. Evolving granular classifiers are suited to learn from time-varying log streams and, therefore, perform online classification of the severity of anomalies. We formulated a 4-class online anomaly classification problem, and employed time windows between landmarks and two granular computing methods, namely, Fuzzy-set-Based evolving Modeling (FBeM) and evolving Granular Neural Network (eGNN), to model and monitor logging activity rate. The results of classification are of utmost importance for predictive maintenance because priority can be given to specific time intervals in which the classifier indicates the existence of high or medium severity anomalies.",Leticia Decker|Daniel Leite|Fabio Viola|Daniele Bonacorsi,,https://arxiv.org/abs/2005.04156v1,https://arxiv.org/pdf/2005.04156v1,,"8 pages, 8 figures, IEEE Conference on Evolving and Adaptive Intelligent Systems (EAIS 2020)",,,cs.NE,cs.NE|cs.LG|stat.ML,https://arxiv.org/pdf/2005.04156v1.pdf
2004.02988v2,2020-04-06T20:32:35Z,2020-04-15 19:12:24,Probabilistic Diagnostic Tests for Degradation Problems in Supervised Learning,"Several studies point out different causes of performance degradation in supervised machine learning. Problems such as class imbalance, overlapping, small-disjuncts, noisy labels, and sparseness limit accuracy in classification algorithms. Even though a number of approaches either in the form of a methodology or an algorithm try to minimize performance degradation, they have been isolated efforts with limited scope. Most of these approaches focus on remediation of one among many problems, with experimental results coming from few datasets and classification algorithms, insufficient measures of prediction power, and lack of statistical validation for testing the real benefit of the proposed approach. This paper consists of two main parts: In the first part, a novel probabilistic diagnostic model based on identifying signs and symptoms of each problem is presented. Thereby, early and correct diagnosis of these problems is to be achieved in order to select not only the most convenient remediation treatment but also unbiased performance metrics. Secondly, the behavior and performance of several supervised algorithms are studied when training sets have such problems. Therefore, prediction of success for treatments can be estimated across classifiers.",Gustavo A. Valencia-Zapata|Carolina Gonzalez-Canas|Michael G. Zentner|Okan Ersoy|Gerhard Klimeck,,https://arxiv.org/abs/2004.02988v2,https://arxiv.org/pdf/2004.02988v2,,,,,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2004.02988v2.pdf
2003.12542v1,2020-03-27T17:14:46Z,2020-03-27 17:14:46,Post-sampling crowdsourced data to allow reliable statistical inference: the case of food price indices in Nigeria,"Sound policy and decision making in developing countries is often limited by the lack of timely and reliable data. Crowdsourced data may provide a valuable alternative for data collection and analysis, e. g. in remote and insecure areas or of poor accessibility where traditional methods are difficult or costly. However, crowdsourced data are not directly usable to draw sound statistical inference. Indeed, its use involves statistical problems because data do not obey any formal sampling design and may also suffer from various non-sampling errors. To overcome this, we propose the use of a special form of post-stratification with which crowdsourced data are reweighted prior their use in an inferential context. An example in Nigeria illustrates the applicability of the method.",Giuseppe Arbia|Gloria Solano-Hermosilla|Fabio Micale|Vincenzo Nardelli|Giampiero Genovese,,https://arxiv.org/abs/2003.12542v1,https://arxiv.org/pdf/2003.12542v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/2003.12542v1.pdf
2003.07611v1,2020-03-17T10:13:31Z,2020-03-17 10:13:31,A comprehensive study on the prediction reliability of graph neural networks for virtual screening,"Prediction models based on deep neural networks are increasingly gaining attention for fast and accurate virtual screening systems. For decision makings in virtual screening, researchers find it useful to interpret an output of classification system as probability, since such interpretation allows them to filter out more desirable compounds. However, probabilistic interpretation cannot be correct for models that hold over-parameterization problems or inappropriate regularizations, leading to unreliable prediction and decision making. In this regard, we concern the reliability of neural prediction models on molecular properties, especially when models are trained with sparse data points and imbalanced distributions. This work aims to propose guidelines for training reliable models, we thus provide methodological details and ablation studies on the following train principles. We investigate the effects of model architectures, regularization methods, and loss functions on the prediction performance and reliability of classification results. Moreover, we evaluate prediction reliability of models on virtual screening scenario. Our result highlights that correct choice of regularization and inference methods is evidently important to achieve high success rate, especially in data imbalanced situation. All experiments were performed under a single unified model implementation to alleviate external randomness in model training and to enable precise comparison of results.",Soojung Yang|Kyung Hoon Lee|Seongok Ryu,,https://arxiv.org/abs/2003.07611v1,https://arxiv.org/pdf/2003.07611v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2003.07611v1.pdf
2003.05070v1,2020-03-11T01:30:08Z,2020-03-11 01:30:08,Multi-Objective Variational Autoencoder: an Application for Smart Infrastructure Maintenance,"Multi-way data analysis has become an essential tool for capturing underlying structures in higher-order data sets where standard two-way analysis techniques often fail to discover the hidden correlations between variables in multi-way data. We propose a multi-objective variational autoencoder (MVA) method for smart infrastructure damage detection and diagnosis in multi-way sensing data based on the reconstruction probability of autoencoder deep neural network (ADNN). Our method fuses data from multiple sensors in one ADNN at which informative features are being extracted and utilized for damage identification. It generates probabilistic anomaly scores to detect damage, asses its severity and further localize it via a new localization layer introduced in the ADNN.
  We evaluated our method on multi-way datasets in the area of structural health monitoring for damage diagnosis purposes. The data was collected from our deployed data acquisition system on a cable-stayed bridge in Western Sydney and from a laboratory based building structure obtained from Los Alamos National Laboratory (LANL). Experimental results show that the proposed method can accurately detect structural damage. It was also able to estimate the different levels of damage severity, and capture damage locations in an unsupervised aspect. Compared to the state-of-the-art approaches, our proposed method shows better performance in terms of damage detection and localization.",Ali Anaissi|Seid Miad Zandavi,,https://arxiv.org/abs/2003.05070v1,https://arxiv.org/pdf/2003.05070v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2003.05070v1.pdf
2003.08749v1,2020-03-06T20:49:20Z,2020-03-06 20:49:20,Toward Enabling a Reliable Quality Monitoring System for Additive Manufacturing Process using Deep Convolutional Neural Networks,"Additive Manufacturing (AM) is a crucial component of the smart industry. In this paper, we propose an automated quality grading system for the AM process using a deep convolutional neural network (CNN) model. The CNN model is trained offline using the images of the internal and surface defects in the layer-by-layer deposition of materials and tested online by studying the performance of detecting and classifying the failure in AM process at different extruder speeds and temperatures. The model demonstrates the accuracy of 94% and specificity of 96%, as well as above 75% in three classifier measures of the Fscore, the sensitivity, and precision for classifying the quality of the printing process in five grades in real-time. The proposed online model adds an automated, consistent, and non-contact quality control signal to the AM process that eliminates the manual inspection of parts after they are entirely built. The quality monitoring signal can also be used by the machine to suggest remedial actions by adjusting the parameters in real-time. The proposed quality predictive model serves as a proof-of-concept for any type of AM machines to produce reliable parts with fewer quality hiccups while limiting the waste of both time and materials.",Yaser Banadaki|Nariman Razaviarab|Hadi Fekrmandi|Safura Sharifi,,https://arxiv.org/abs/2003.08749v1,https://arxiv.org/pdf/2003.08749v1,,,,,cs.CV,cs.CV|cond-mat.mtrl-sci|cs.LG|eess.IV|stat.ML,https://arxiv.org/pdf/2003.08749v1.pdf
2003.01690v2,2020-03-03T18:15:55Z,2020-08-04 18:31:08,Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks,"The field of defense strategies against adversarial attacks has significantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufficient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difficult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we first propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than $10\%$, identifying several broken defenses.",Francesco Croce|Matthias Hein,,https://arxiv.org/abs/2003.01690v2,https://arxiv.org/pdf/2003.01690v2,,In ICML 2020,,,cs.LG,cs.LG|cs.CV|stat.ML,https://arxiv.org/pdf/2003.01690v2.pdf
2003.01412v3,2020-03-03T09:49:30Z,2020-10-15 13:12:23,CRATOS: Cognition of Reliable Algorithm for Time-series Optimal Solution,"Anomaly detection of time series plays an important role in reliability systems engineering. However, in practical application, there is no precisely defined boundary between normal and anomalous behaviors in different application scenarios. Therefore, different anomaly detection algorithms and processes ought to be adopted for time series in different situation. Although such strategy improve the accuracy of anomaly detection, it takes a lot of time for practitioners to configure various algorithms to millions of series, which greatly increases the development and maintenance cost of anomaly detection processes. In this paper, we propose CRATOS which is a self-adapt algorithms that extract features from time series, and then cluster series with similar features into one group. For each group we utilize evolutionary algorithm to search the best anomaly detection methods and processes. Our methods can significantly reduce the cost of development and maintenance of anomaly detection. According to experiments, our clustering methods achieves the state-of-art results. The accuracy of the anomaly detection algorithms in this paper is 85.1%.",Ziling Wu|Ping Liu|Zheng Hu|Bocheng Li|Jun Wang,,https://arxiv.org/abs/2003.01412v3,https://arxiv.org/pdf/2003.01412v3,,,,,cs.LG,cs.LG|cs.NE|stat.ML,https://arxiv.org/pdf/2003.01412v3.pdf
2002.09797v2,2020-02-23T00:50:01Z,2020-06-28 20:37:50,Reliable Fidelity and Diversity Metrics for Generative Models,"Devising indicative evaluation metrics for the image generation task remains an open problem. The most widely used metric for measuring the similarity between real and generated images has been the Fréchet Inception Distance (FID) score. Because it does not differentiate the fidelity and diversity aspects of the generated images, recent papers have introduced variants of precision and recall metrics to diagnose those properties separately. In this paper, we show that even the latest version of the precision and recall metrics are not reliable yet. For example, they fail to detect the match between two identical distributions, they are not robust against outliers, and the evaluation hyperparameters are selected arbitrarily. We propose density and coverage metrics that solve the above issues. We analytically and experimentally show that density and coverage provide more interpretable and reliable signals for practitioners than the existing metrics. Code: https://github.com/clovaai/generative-evaluation-prdc.",Muhammad Ferjad Naeem|Seong Joon Oh|Youngjung Uh|Yunjey Choi|Jaejun Yoo,,https://arxiv.org/abs/2002.09797v2,https://arxiv.org/pdf/2002.09797v2,,First two authors have contributed equally; ICML 2020 accepted,,,cs.CV,cs.CV|cs.LG|stat.ML,https://arxiv.org/pdf/2002.09797v2.pdf
2002.11045v1,2020-02-22T14:38:11Z,2020-02-22 14:38:11,Deep Learning for Ultra-Reliable and Low-Latency Communications in 6G Networks,"In the future 6th generation networks, ultra-reliable and low-latency communications (URLLC) will lay the foundation for emerging mission-critical applications that have stringent requirements on end-to-end delay and reliability. Existing works on URLLC are mainly based on theoretical models and assumptions. The model-based solutions provide useful insights, but cannot be directly implemented in practice. In this article, we first summarize how to apply data-driven supervised deep learning and deep reinforcement learning in URLLC, and discuss some open problems of these methods. To address these open problems, we develop a multi-level architecture that enables device intelligence, edge intelligence, and cloud intelligence for URLLC. The basic idea is to merge theoretical models and real-world data in analyzing the latency and reliability and training deep neural networks (DNNs). Deep transfer learning is adopted in the architecture to fine-tune the pre-trained DNNs in non-stationary networks. Further considering that the computing capacity at each user and each mobile edge computing server is limited, federated learning is applied to improve the learning efficiency. Finally, we provide some experimental and simulation results and discuss some future directions.",Changyang She|Rui Dong|Zhouyou Gu|Zhanwei Hou|Yonghui Li|Wibowo Hardjawana|Chenyang Yang|Lingyang Song|Branka Vucetic,,https://arxiv.org/abs/2002.11045v1,https://arxiv.org/pdf/2002.11045v1,,The manuscript contains 4 figures 2 tables. It has been submitted to IEEE Network (in the second round of revision),,,eess.SP,eess.SP|cs.LG|cs.NI|stat.ML,https://arxiv.org/pdf/2002.11045v1.pdf
2002.08289v2,2020-02-19T17:05:32Z,2020-10-17 13:51:37,Variational Encoder-based Reliable Classification,"Machine learning models provide statistically impressive results which might be individually unreliable. To provide reliability, we propose an Epistemic Classifier (EC) that can provide justification of its belief using support from the training dataset as well as quality of reconstruction. Our approach is based on modified variational auto-encoders that can identify a semantically meaningful low-dimensional space where perceptually similar instances are close in $\ell_2$-distance too. Our results demonstrate improved reliability of predictions and robust identification of samples with adversarial attacks as compared to baseline of softmax-based thresholding.",Chitresh Bhushan|Zhaoyuan Yang|Nurali Virani|Naresh Iyer,,https://arxiv.org/abs/2002.08289v2,https://arxiv.org/pdf/2002.08289v2,https://doi.org/10.1109/ICIP40778.2020.9190836,Published in ICIP 2020. Typos fixed in revision,IEEE International Conference on Image Processing (2020) 1941-1945,10.1109/ICIP40778.2020.9190836,cs.LG,cs.LG|cs.CV|stat.ML,https://arxiv.org/pdf/2002.08289v2.pdf
2003.04980v1,2020-02-14T07:10:18Z,2020-02-14 07:10:18,Improving Reliability of Latent Dirichlet Allocation by Assessing Its Stability Using Clustering Techniques on Replicated Runs,"For organizing large text corpora topic modeling provides useful tools. A widely used method is Latent Dirichlet Allocation (LDA), a generative probabilistic model which models single texts in a collection of texts as mixtures of latent topics. The assignments of words to topics rely on initial values such that generally the outcome of LDA is not fully reproducible. In addition, the reassignment via Gibbs Sampling is based on conditional distributions, leading to different results in replicated runs on the same text data. This fact is often neglected in everyday practice. We aim to improve the reliability of LDA results. Therefore, we study the stability of LDA by comparing assignments from replicated runs. We propose to quantify the similarity of two generated topics by a modified Jaccard coefficient. Using such similarities, topics can be clustered. A new pruning algorithm for hierarchical clustering results based on the idea that two LDA runs create pairs of similar topics is proposed. This approach leads to the new measure S-CLOP ({\bf S}imilarity of multiple sets by {\bf C}lustering with {\bf LO}cal {\bf P}runing) for quantifying the stability of LDA models. We discuss some characteristics of this measure and illustrate it with an application to real data consisting of newspaper articles from \textit{USA Today}. Our results show that the measure S-CLOP is useful for assessing the stability of LDA models or any other topic modeling procedure that characterize its topics by word distributions. Based on the newly proposed measure for LDA stability, we propose a method to increase the reliability and hence to improve the reproducibility of empirical findings based on topic modeling. This increase in reliability is obtained by running the LDA several times and taking as prototype the most representative run, that is the LDA run with highest average similarity to all other runs.",Jonas Rieger|Lars Koppers|Carsten Jentsch|Jörg Rahnenführer,,https://arxiv.org/abs/2003.04980v1,https://arxiv.org/pdf/2003.04980v1,,"16 pages, 2 figures",,,cs.CL,cs.CL|cs.AI|cs.LG|stat.ML,https://arxiv.org/pdf/2003.04980v1.pdf
2002.03875v3,2020-02-10T15:42:36Z,2020-09-30 05:17:25,Calibrate and Prune: Improving Reliability of Lottery Tickets Through Prediction Calibration,"The hypothesis that sub-network initializations (lottery) exist within the initializations of over-parameterized networks, which when trained in isolation produce highly generalizable models, has led to crucial insights into network initialization and has enabled efficient inferencing. Supervised models with uncalibrated confidences tend to be overconfident even when making wrong prediction. In this paper, for the first time, we study how explicit confidence calibration in the over-parameterized network impacts the quality of the resulting lottery tickets. More specifically, we incorporate a suite of calibration strategies, ranging from mixup regularization, variance-weighted confidence calibration to the newly proposed likelihood-based calibration and normalized bin assignment strategies. Furthermore, we explore different combinations of architectures and datasets, and make a number of key findings about the role of confidence calibration. Our empirical studies reveal that including calibration mechanisms consistently lead to more effective lottery tickets, in terms of accuracy as well as empirical calibration metrics, even when retrained using data with challenging distribution shifts with respect to the source dataset.",Bindya Venkatesh|Jayaraman J. Thiagarajan|Kowshik Thopalli|Prasanna Sattigeri,,https://arxiv.org/abs/2002.03875v3,https://arxiv.org/pdf/2002.03875v3,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/2002.03875v3.pdf
2002.01110v1,2020-02-04T03:47:20Z,2020-02-04 03:47:20,REAK: Reliability analysis through Error rate-based Adaptive Kriging,"As models in various fields are becoming more complex, associated computational demands have been increasing significantly. Reliability analysis for these systems when failure probabilities are small is significantly challenging, requiring a large number of costly simulations. To address this challenge, this paper introduces Reliability analysis through Error rate-based Adaptive Kriging (REAK). An extension of the Central Limit Theorem based on Lindeberg condition is adopted here to derive the distribution of the number of design samples with wrong sign estimate and subsequently determine the maximum error rate for failure probability estimates. This error rate enables optimal establishment of effective sampling regions at each stage of an adaptive scheme for strategic generation of design samples. Moreover, it facilitates setting a target accuracy for failure probability estimation, which is used as stopping criterion for reliability analysis. These capabilities together can significantly reduce the number of calls to sophisticated, computationally demanding models. The application of REAK for four examples with varying extent of nonlinearity and dimension is presented. Results indicate that REAK is able to reduce the computational demand by as high as 50% compared to state-of-the-art methods of Adaptive Kriging with Monte Carlo Simulation (AK-MCS) and Improved Sequential Kriging Reliability Analysis (ISKRA).",Zeyu Wang|Abdollah Shafieezadeh,,https://arxiv.org/abs/2002.01110v1,https://arxiv.org/pdf/2002.01110v1,https://doi.org/10.1016/j.ress.2018.10.004,,,10.1016/j.ress.2018.10.004,stat.AP,stat.AP|cs.LG,https://arxiv.org/pdf/2002.01110v1.pdf
2002.00427v1,2020-02-02T16:29:31Z,2020-02-02 16:29:31,Dynamic maintenance policy for systems with repairable components subject to mutually dependent competing failure processes,"In this paper, a repairable multi-component system is studied where all the components can be repaired individually within the system. The whole system is inspected at inspection intervals and the failed components are detected and replaced with a new one, while the other components continue functioning. Replacing components individually within the system makes their initial age to be different at each inspection time. Different initial age of all the components have affect on the system reliability and probability of failure and consequently the optimal inspection time, which is for the whole system not individual components. A dynamic maintenance policy is proposed to find the next inspection time based on the initial age of all the components. Two competing failure processes of degradation and a shock process are considered for each component. In our paper, there is a mutual dependency between the degradation process and shock process. Each incoming shock adds additional abrupt damages on the cumulative degradation path of all the components, and the shock arrival process is affected by the system degradation process. A realistic numerical example is presented to illustrate the proposed reliability and maintenance model.",Nooshin Yousefi|David W. Coit|Zhu Xiaoyan,,https://arxiv.org/abs/2002.00427v1,https://arxiv.org/pdf/2002.00427v1,,,,,eess.SP,eess.SP|math.OC|stat.AP,https://arxiv.org/pdf/2002.00427v1.pdf
2002.00351v1,2020-02-02T08:34:32Z,2020-02-02 08:34:32,Bayesian Reliability Analysis of the Power Law Process with Respect to the Higgins-Tsokos Loss Function for Modeling Software Failure Times,"The Power Law Process, also known as Non-Homogeneous Poisson Process, has been used in various aspects, one of which is the software reliability assessment. Specifically, by using its intensity function to compute the rate of change of a software reliability as time-varying function. Justification of Bayesian analysis applicability to the Power Law Process was shown using real data. The probability distribution that best characterizes the behavior of the key parameter of the intensity function was first identified, then the likelihood-based Bayesian reliability estimate of the Power Law Process under the Higgins-Tsokos loss function was obtained. As a result of a simulation study and using real data, the Bayesian estimate shows an outstanding performance compared to the maximum likelihood estimate using different sample sizes. In addition, a sensitivity analysis was performed, resulting in the Bayesian estimate being sensitive to the prior selection; whether parametric or non-parametric.",Freeh Alenezi|Chris. Tsokos,,https://arxiv.org/abs/2002.00351v1,https://arxiv.org/pdf/2002.00351v1,,,,,stat.CO,stat.CO|stat.AP,https://arxiv.org/pdf/2002.00351v1.pdf
2001.11757v2,2020-01-31T10:39:46Z,2020-11-12 09:30:09,Statistical stability indices for LIME: obtaining reliable explanations for Machine Learning models,"Nowadays we are witnessing a transformation of the business processes towards a more computation driven approach. The ever increasing usage of Machine Learning techniques is the clearest example of such trend.
  This sort of revolution is often providing advantages, such as an increase in prediction accuracy and a reduced time to obtain the results. However, these methods present a major drawback: it is very difficult to understand on what grounds the algorithm took the decision.
  To address this issue we consider the LIME method. We give a general background on LIME then, we focus on the stability issue: employing the method repeated times, under the same conditions, may yield to different explanations.
  Two complementary indices are proposed, to measure LIME stability. It is important for the practitioner to be aware of the issue, as well as to have a tool for spotting it. Stability guarantees LIME explanations to be reliable, therefore a stability assessment, made through the proposed indices, is crucial.
  As a case study, we apply both Machine Learning and classical statistical techniques to Credit Risk data. We test LIME on the Machine Learning algorithm and check its stability. Eventually, we examine the goodness of the explanations returned.",Giorgio Visani|Enrico Bagli|Federico Chesani|Alessandro Poluzzi|Davide Capuzzo,,https://arxiv.org/abs/2001.11757v2,https://arxiv.org/pdf/2001.11757v2,https://doi.org/10.1080/01605682.2020.1865846,,,10.1080/01605682.2020.1865846,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/2001.11757v2.pdf
2001.08583v1,2020-01-18T19:12:51Z,2020-01-18 19:12:51,Intelligent Road Inspection with Advanced Machine Learning; Hybrid Prediction Models for Smart Mobility and Transportation Maintenance Systems,"Prediction models in mobility and transportation maintenance systems have been dramatically improved through using machine learning methods. This paper proposes novel machine learning models for intelligent road inspection. The traditional road inspection systems based on the pavement condition index (PCI) are often associated with the critical safety, energy and cost issues. Alternatively, the proposed models utilize surface deflection data from falling weight deflectometer (FWD) tests to predict the PCI. Machine learning methods are the single multi-layer perceptron (MLP) and radial basis function (RBF) neural networks as well as their hybrids, i.e., Levenberg-Marquardt (MLP-LM), scaled conjugate gradient (MLP-SCG), imperialist competitive (RBF-ICA), and genetic algorithms (RBF-GA). Furthermore, the committee machine intelligent systems (CMIS) method was adopted to combine the results and improve the accuracy of the modeling. The results of the analysis have been verified through using four criteria of average percent relative error (APRE), average absolute percent relative error (AAPRE), root mean square error (RMSE), and standard error (SD). The CMIS model outperforms other models with the promising results of APRE=2.3303, AAPRE=11.6768, RMSE=12.0056, and SD=0.0210.",Nader Karballaeezadeh|Farah Zaremotekhases|Shahaboddin Shamshirband|Amir Mosavi|Narjes Nabipour|Peter Csiba|Annamaria R. Varkonyi-Koczy,,https://arxiv.org/abs/2001.08583v1,https://arxiv.org/pdf/2001.08583v1,,"23 pages, 10 figures",,,eess.SP,eess.SP|cs.LG|stat.ML,https://arxiv.org/pdf/2001.08583v1.pdf
2001.05497v1,2020-01-15T19:00:00Z,2020-01-15 19:00:00,"Noise-tolerant, Reliable Active Classification with Comparison Queries","With the explosion of massive, widely available unlabeled data in the past years, finding label and time efficient, robust learning algorithms has become ever more important in theory and in practice. We study the paradigm of active learning, in which algorithms with access to large pools of data may adaptively choose what samples to label in the hope of exponentially increasing efficiency. By introducing comparisons, an additional type of query comparing two points, we provide the first time and query efficient algorithms for learning non-homogeneous linear separators robust to bounded (Massart) noise. We further provide algorithms for a generalization of the popular Tsybakov low noise condition, and show how comparisons provide a strong reliability guarantee that is often impractical or impossible with only labels - returning a classifier that makes no errors with high probability.",Max Hopkins|Daniel Kane|Shachar Lovett|Gaurav Mahajan,,https://arxiv.org/abs/2001.05497v1,https://arxiv.org/pdf/2001.05497v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/2001.05497v1.pdf
1912.07137v2,2019-12-15T23:48:14Z,2020-01-17 19:49:13,Generalized reliability based on distances,"The intraclass correlation coefficient (ICC) is a classical index of measurement reliability. With the advent of new and complex types of data for which the ICC is not defined, there is a need for new ways to assess reliability. To meet this need, we propose a new distance-based intraclass correlation coefficient (dbICC), defined in terms of arbitrary distances among observations. We introduce a bias correction to improve the coverage of bootstrap confidence intervals for the dbICC, and demonstrate its efficacy via simulation. We illustrate the proposed method by analyzing the test-retest reliability of brain connectivity matrices derived from a set of repeated functional magnetic resonance imaging scans. The Spearman-Brown formula, which shows how more intensive measurement increases reliability, is extended to encompass the dbICC.",Meng Xu|Philip T. Reiss|Ivor Cribben,,https://arxiv.org/abs/1912.07137v2,https://arxiv.org/pdf/1912.07137v2,https://doi.org/10.1111/biom.13287,,,10.1111/biom.13287,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/1912.07137v2.pdf
1912.05663v2,2019-12-10T17:50:33Z,2020-02-12 19:23:15,Measuring the Reliability of Reinforcement Learning Algorithms,"Lack of reliability is a well-known issue for reinforcement learning (RL) algorithms. This problem has gained increasing attention in recent years, and efforts to improve it have grown substantially. To aid RL researchers and production users with the evaluation and improvement of reliability, we propose a set of metrics that quantitatively measure different aspects of reliability. In this work, we focus on variability and risk, both during training and after learning (on a fixed policy). We designed these metrics to be general-purpose, and we also designed complementary statistical tests to enable rigorous comparisons on these metrics. In this paper, we first describe the desired properties of the metrics and their design, the aspects of reliability that they measure, and their applicability to different scenarios. We then describe the statistical tests and make additional practical recommendations for reporting results. The metrics and accompanying statistical tools have been made available as an open-source library at https://github.com/google-research/rl-reliability-metrics. We apply our metrics to a set of common RL algorithms and environments, compare them, and analyze the results.",Stephanie C. Y. Chan|Samuel Fishman|John Canny|Anoop Korattikara|Sergio Guadarrama,,https://arxiv.org/abs/1912.05663v2,https://arxiv.org/pdf/1912.05663v2,,Accepted for publication at ICLR 2020 (spotlight),,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/1912.05663v2.pdf
1912.04202v2,2019-12-09T17:35:46Z,2021-06-04 09:13:19,Optimal Stress Levels in Accelerated Degradation Testing for Various Degradation Models,"Accelerated degradation tests are used to provide accurate estimation of lifetime characteristics of highly reliable products within a relatively short testing time. Data from particular tests at high levels of stress (e.g., temperature, voltage, or vibration) are extrapolated, through a physically meaningful statistical model, to attain estimates of lifetime quantiles at normal use conditions. The gamma process is a natural model for estimating the degradation increments over certain degradation paths, which exhibit a monotone and strictly increasing degradation pattern. In this work, we derive first an algorithm-based optimal design for a repeated measures degradation test with single failure mode that corresponds to a single response component. The univariate degradation process is expressed using a gamma model where a generalized linear model is introduced to facilitate the derivation of an optimal design. Consequently, we extend the univariate model and characterize optimal designs for accelerated degradation tests with bivariate degradation processes. The first bivariate model includes two gamma processes as marginal degradation models. The second bivariate models is expressed by a gamma process along with a mixed effects linear model. We derive optimal designs for minimizing the asymptotic variance for estimating some quantile of the failure time distribution at the normal use conditions. Sensitivity analysis is conducted to study the behavior of the resulting optimal designs under misspecifications of adopted nominal values.",Helmi Shat|Rainer Schwabe,,https://arxiv.org/abs/1912.04202v2,https://arxiv.org/pdf/1912.04202v2,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1912.04202v2.pdf
1912.04175v1,2019-12-09T16:50:35Z,2019-12-09 16:50:35,How much is optimal reinsurance degraded by error?,The literature on optimal reinsurance does not deal with how much the effectiveness of such solutions is degraded by errors in parameters and models. The issue is investigated through both asymptotics and numerical studies. It is shown that the rate of degradation is often $O(1/n)$ as the sample size $n$ of historical observations becomes infinite. Criteria based on Value at Risk are exceptions that may achieve only $O(1/\sqrt{n})$. These theoretical results are supported by numerical studies. A Bayesian perspective on how to integrate risk caused by parameter error is offered as well.,Yinzhi Wang|Erik Bølviken,,https://arxiv.org/abs/1912.04175v1,https://arxiv.org/pdf/1912.04175v1,,,,,stat.AP,stat.AP|q-fin.RM,https://arxiv.org/pdf/1912.04175v1.pdf
1912.00662v1,2019-12-02T10:03:19Z,2019-12-02 10:03:19,An Attribute Oriented Induction based Methodology for Data Driven Predictive Maintenance,"Attribute Oriented Induction (AOI) is a data mining algorithm used for extracting knowledge of relational data, taking into account expert knowledge. It is a clustering algorithm that works by transforming the values of the attributes and converting an instance into others that are more generic or ambiguous. In this way, it seeks similarities between elements to generate data groupings. AOI was initially conceived as an algorithm for knowledge discovery in databases, but over the years it has been applied to other areas such as spatial patterns, intrusion detection or strategy making. In this paper, AOI has been extended to the field of Predictive Maintenance. The objective is to demonstrate that combining expert knowledge and data collected from the machine can provide good results in the Predictive Maintenance of industrial assets. To this end we adapted the algorithm and used an LSTM approach to perform both the Anomaly Detection (AD) and the Remaining Useful Life (RUL). The results obtained confirm the validity of the proposal, as the methodology was able to detect anomalies, and calculate the RUL until breakage with considerable degree of accuracy.",Javier Fernandez-Anakabe|Ekhi Zugasti Uriguen|Urko Zurutuza Ortega,,https://arxiv.org/abs/1912.00662v1,https://arxiv.org/pdf/1912.00662v1,,"Submitted to Journal of Intelligent Manufacturing, Springer",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1912.00662v1.pdf
1912.03359v1,2019-11-27T12:50:13Z,2019-11-27 12:50:13,Ultra-Reliable and Low-Latency Vehicular Communication: An Active Learning Approach,"In this letter, an age of information (AoI)-aware transmission power and resource block (RB) allocation technique for vehicular communication networks is proposed. Due to the highly dynamic nature of vehicular networks, gaining a prior knowledge about the network dynamics, i.e., wireless channels and interference, in order to allocate resources, is challenging. Therefore, to effectively allocate power and RBs, the proposed approach allows the network to actively learn its dynamics by balancing a tradeoff between minimizing the probability that the vehicles' AoI exceeds a predefined threshold and maximizing the knowledge about the network dynamics. In this regard, using a Gaussian process regression (GPR) approach, an online decentralized strategy is proposed to actively learn the network dynamics, estimate the vehicles' future AoI, and proactively allocate resources. Simulation results show a significant improvement in terms of AoI violation probability, compared to several baselines, with a reduction of at least 50%.",Mohamed K. Abdel-Aziz|Sumudu Samarakoon|Mehdi Bennis|Walid Saad,,https://arxiv.org/abs/1912.03359v1,https://arxiv.org/pdf/1912.03359v1,https://doi.org/10.1109/LCOMM.2019.2956929,Accepted for publication in IEEE Communication Letters with 4 pages and 4 figures,,10.1109/LCOMM.2019.2956929,cs.NI,cs.NI|cs.LG|stat.ML,https://arxiv.org/pdf/1912.03359v1.pdf
1911.08682v2,2019-11-20T03:29:05Z,2019-11-22 01:55:59,Ensuring Reliable Monte Carlo Estimates of Network Properties,"The literature in social network analysis has largely focused on methods and models which require complete network data; however there exist many networks which can only be studied via sampling methods due to the scale or complexity of the network, access limitations, or the population of interest is hard to reach. In such cases, the application of random walk-based Markov chain Monte Carlo (MCMC) methods to estimate multiple network features is common. However, the reliability of these estimates has been largely ignored. We consider and further develop multivariate MCMC output analysis methods in the context of network sampling to directly address the reliability of the multivariate estimation. This approach yields principled, computationally efficient, and broadly applicable methods for assessing the Monte Carlo estimation procedure. In particular, with respect to two random-walk algorithms, a simple random walk and a Metropolis-Hastings random walk, we construct and compare network parameter estimates, effective sample sizes, coverage probabilities, and stopping rules, all of which speaks to the estimation reliability.",Haema Nilakanta|Zack W. Almquist|Galin L. Jones,,https://arxiv.org/abs/1911.08682v2,https://arxiv.org/pdf/1911.08682v2,,27 pages,,,stat.AP,stat.AP|cs.SI|stat.ME,https://arxiv.org/pdf/1911.08682v2.pdf
1911.07391v3,2019-11-18T01:15:24Z,2021-11-14 16:58:11,Justification-Based Reliability in Machine Learning,"With the advent of Deep Learning, the field of machine learning (ML) has surpassed human-level performance on diverse classification tasks. At the same time, there is a stark need to characterize and quantify reliability of a model's prediction on individual samples. This is especially true in application of such models in safety-critical domains of industrial control and healthcare. To address this need, we link the question of reliability of a model's individual prediction to the epistemic uncertainty of the model's prediction. More specifically, we extend the theory of Justified True Belief (JTB) in epistemology, created to study the validity and limits of human-acquired knowledge, towards characterizing the validity and limits of knowledge in supervised classifiers. We present an analysis of neural network classifiers linking the reliability of its prediction on an input to characteristics of the support gathered from the input and latent spaces of the network. We hypothesize that the JTB analysis exposes the epistemic uncertainty (or ignorance) of a model with respect to its inference, thereby allowing for the inference to be only as strong as the justification permits. We explore various forms of support (for e.g., k-nearest neighbors (k-NN) and l_p-norm based) generated for an input, using the training data to construct a justification for the prediction with that input. Through experiments conducted on simulated and real datasets, we demonstrate that our approach can provide reliability for individual predictions and characterize regions where such reliability cannot be ascertained.",Nurali Virani|Naresh Iyer|Zhaoyuan Yang,,https://arxiv.org/abs/1911.07391v3,https://arxiv.org/pdf/1911.07391v3,,"Extended version of paper accepted at AAAI 2020 with supplementary materials, update remark and fix typo",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1911.07391v3.pdf
1911.06256v2,2019-11-14T17:31:04Z,2019-12-20 15:12:36,A Comparative Study between Bayesian and Frequentist Neural Networks for Remaining Useful Life Estimation in Condition-Based Maintenance,"In the last decade, deep learning (DL) has outperformed model-based and statistical approaches in predicting the remaining useful life (RUL) of machinery in the context of condition-based maintenance. One of the major drawbacks of DL is that it heavily depends on a large amount of labeled data, which are typically expensive and time-consuming to obtain, especially in industrial applications. Scarce training data lead to uncertain estimates of the model's parameters, which in turn result in poor prognostic performance. Quantifying this parameter uncertainty is important in order to determine how reliable the prediction is. Traditional DL techniques such as neural networks are incapable of capturing the uncertainty in the training data, thus they are overconfident about their estimates. On the contrary, Bayesian deep learning has recently emerged as a promising solution to account for uncertainty in the training process, achieving state-of-the-art performance in many classification and regression tasks. In this work Bayesian DL techniques such as Bayesian dense neural networks and Bayesian convolutional neural networks are applied to RUL estimation and compared to their frequentist counterparts from the literature. The effectiveness of the proposed models is verified on the popular C-MAPSS dataset. Furthermore, parameter uncertainty is quantified and used to gain additional insight into the data.",Luca Della Libera,,https://arxiv.org/abs/1911.06256v2,https://arxiv.org/pdf/1911.06256v2,,Withdrawn to resolve an authorship dispute,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1911.06256v2.pdf
1911.03264v2,2019-11-01T16:50:48Z,2020-10-14 01:23:10,Experienced Deep Reinforcement Learning with Generative Adversarial Networks (GANs) for Model-Free Ultra Reliable Low Latency Communication,"In this paper, a novel experienced deep reinforcement learning (deep-RL) framework is proposed to provide model-free resource allocation for ultra reliable low latency communication (URLLC). The proposed, experienced deep-RL framework can guarantee high end-to-end reliability and low end-to-end latency, under explicit data rate constraints, for each wireless without any models of or assumptions on the users' traffic. In particular, in order to enable the deep-RL framework to account for extreme network conditions and operate in highly reliable systems, a new approach based on generative adversarial networks (GANs) is proposed. This GAN approach is used to pre-train the deep-RL framework using a mix of real and synthetic data, thus creating an experienced deep-RL framework that has been exposed to a broad range of network conditions. Formally, the URLLC resource allocation problem is posed as a power minimization problem under reliability, latency, and rate constraints. To solve this problem using experienced deep-RL, first, the rate of each user is determined. Then, these rates are mapped to the resource block and power allocation vectors of the studied wireless system. Finally, the end-to-end reliability and latency of each user are used as feedback to the deep-RL framework. It is then shown that at the fixed-point of the deep-RL algorithm, the reliability and latency of the users are near-optimal. Moreover, for the proposed GAN approach, a theoretical limit for the generator output is analytically derived. Simulation results show how the proposed approach can achieve near-optimal performance within the rate-reliability-latency region, depending on the network and service requirements. The results also show that the proposed experienced deep-RL framework is able to remove the transient training time that makes conventional deep-RL methods unsuitable for URLLC.",Ali Taleb Zadeh Kasgari|Walid Saad|Mohammad Mozaffari|H. Vincent Poor,,https://arxiv.org/abs/1911.03264v2,https://arxiv.org/pdf/1911.03264v2,,,,,cs.IT,cs.IT|cs.LG|cs.NI|stat.ML,https://arxiv.org/pdf/1911.03264v2.pdf
1911.00515v1,2019-11-01T15:52:16Z,2019-11-01 15:52:16,The reliability of a deep learning model in clinical out-of-distribution MRI data: a multicohort study,"Deep learning (DL) methods have in recent years yielded impressive results in medical imaging, with the potential to function as clinical aid to radiologists. However, DL models in medical imaging are often trained on public research cohorts with images acquired with a single scanner or with strict protocol harmonization, which is not representative of a clinical setting. The aim of this study was to investigate how well a DL model performs in unseen clinical data sets---collected with different scanners, protocols and disease populations---and whether more heterogeneous training data improves generalization. In total, 3117 MRI scans of brains from multiple dementia research cohorts and memory clinics, that had been visually rated by a neuroradiologist according to Scheltens' scale of medial temporal atrophy (MTA), were included in this study. By training multiple versions of a convolutional neural network on different subsets of this data to predict MTA ratings, we assessed the impact of including images from a wider distribution during training had on performance in external memory clinic data. Our results showed that our model generalized well to data sets acquired with similar protocols as the training data, but substantially worse in clinical cohorts with visibly different tissue contrasts in the images. This implies that future DL studies investigating performance in out-of-distribution (OOD) MRI data need to assess multiple external cohorts for reliable results. Further, by including data from a wider range of scanners and protocols the performance improved in OOD data, which suggests that more heterogeneous training data makes the model generalize better. To conclude, this is the most comprehensive study to date investigating the domain shift in deep learning on MRI data, and we advocate rigorous evaluation of DL models on clinical data prior to being certified for deployment.",Gustav Mårtensson|Daniel Ferreira|Tobias Granberg|Lena Cavallin|Ketil Oppedal|Alessandro Padovani|Irena Rektorova|Laura Bonanni|Matteo Pardini|Milica Kramberger|John-Paul Taylor|Jakub Hort|Jón Snædal|Jaime Kulisevsky|Frederic Blanc|Angelo Antonini|Patrizia Mecocci|Bruno Vellas|Magda Tsolaki|Iwona Kłoszewska|Hilkka Soininen|Simon Lovestone|Andrew Simmons|Dag Aarsland|Eric Westman,,https://arxiv.org/abs/1911.00515v1,https://arxiv.org/pdf/1911.00515v1,https://doi.org/10.1016/j.media.2020.101714,"11 pages, 3 figures",,10.1016/j.media.2020.101714,physics.med-ph,physics.med-ph|cs.CV|cs.LG|eess.IV|q-bio.QM|stat.ML,https://arxiv.org/pdf/1911.00515v1.pdf
1910.12043v1,2019-10-26T10:30:45Z,2019-10-26 10:30:45,Bayesian Experimental Design for Finding Reliable Level Set under Input Uncertainty,"In the manufacturing industry, it is often necessary to repeat expensive operational testing of machine in order to identify the range of input conditions under which the machine operates properly. Since it is often difficult to accurately control the input conditions during the actual usage of the machine, there is a need to guarantee the performance of the machine after properly incorporating the possible variation in input conditions. In this paper, we formulate this practical manufacturing scenario as an Input Uncertain Reliable Level Set Estimation (IU-rLSE) problem, and provide an efficient algorithm for solving it. The goal of IU-rLSE is to identify the input range in which the outputs smaller/greater than a desired threshold can be obtained with high probability when the input uncertainty is properly taken into consideration. We propose an active learning method to solve the IU-rLSE problem efficiently, theoretically analyze its accuracy and convergence, and illustrate its empirical performance through numerical experiments on artificial and real data.",Shogo Iwazaki|Yu Inatsu|Ichiro Takeuchi,,https://arxiv.org/abs/1910.12043v1,https://arxiv.org/pdf/1910.12043v1,,"18 pages, 8 figures",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1910.12043v1.pdf
1910.02497v5,2019-10-06T18:37:12Z,2021-09-23 21:38:49,mfEGRA: Multifidelity Efficient Global Reliability Analysis through Active Learning for Failure Boundary Location,"This paper develops mfEGRA, a multifidelity active learning method using data-driven adaptively refined surrogates for failure boundary location in reliability analysis. This work addresses the issue of prohibitive cost of reliability analysis using Monte Carlo sampling for expensive-to-evaluate high-fidelity models by using cheaper-to-evaluate approximations of the high-fidelity model. The method builds on the Efficient Global Reliability Analysis (EGRA) method, which is a surrogate-based method that uses adaptive sampling for refining Gaussian process surrogates for failure boundary location using a single-fidelity model. Our method introduces a two-stage adaptive sampling criterion that uses a multifidelity Gaussian process surrogate to leverage multiple information sources with different fidelities. The method combines expected feasibility criterion from EGRA with one-step lookahead information gain to refine the surrogate around the failure boundary. The computational savings from mfEGRA depends on the discrepancy between the different models, and the relative cost of evaluating the different models as compared to the high-fidelity model. We show that accurate estimation of reliability using mfEGRA leads to computational savings of $\sim$46% for an analytic multimodal test problem and 24% for a three-dimensional acoustic horn problem, when compared to single-fidelity EGRA. We also show the effect of using a priori drawn Monte Carlo samples in the implementation for the acoustic horn problem, where mfEGRA leads to computational savings of 45% for the three-dimensional case and 48% for a rarer event four-dimensional case as compared to single-fidelity EGRA.",Anirban Chaudhuri|Alexandre N. Marques|Karen E. Willcox,,https://arxiv.org/abs/1910.02497v5,https://arxiv.org/pdf/1910.02497v5,https://doi.org/10.1007/s00158-021-02892-5,,"Structural and Multidisciplinary Optimization 64, 797-811, 2021",10.1007/s00158-021-02892-5,stat.ML,stat.ML|cs.LG|physics.data-an|stat.CO,https://arxiv.org/pdf/1910.02497v5.pdf
1909.07053v3,2019-09-16T08:31:08Z,2019-09-30 18:00:37,Transfer learning for Remaining Useful Life Prediction Based on Consensus Self-Organizing Models,"The traditional paradigm for developing machine prognostics usually relies on generalization from data acquired in experiments under controlled conditions prior to deployment of the equipment. Detecting or predicting failures and estimating machine health in this way assumes that future field data will have a very similar distribution to the experiment data. However, many complex machines operate under dynamic environmental conditions and are used in many different ways. This makes collecting comprehensive data very challenging, and the assumption that pre-deployment data and post-deployment data follow very similar distributions is unlikely to hold. Transfer Learning (TL) refers to methods for transferring knowledge learned in one setting (the source domain) to another setting (the target domain). In this work, we present a TL method for predicting Remaining Useful Life (RUL) of equipment, under the assumption that labels are available only for the source domain and not the target domain. This setting corresponds to generalizing from a limited number of run-to-failure experiments performed prior to deployment into making prognostics with data coming from deployed equipment that is being used under multiple new operating conditions and experiencing previously unseen faults. We employ a deviation detection method, Consensus Self-Organizing Models (COSMO), to create transferable features for building the RUL regression model. These features capture how different target equipment is in comparison to its peers. The efficiency of the proposed TL method is demonstrated using the NASA Turbofan Engine Degradation Simulation Data Set. Models using the COSMO transferable features show better performance than other methods on predicting RUL when the target domain is more complex than the source domain.",Yuantao Fan|Sławomir Nowaczyk|Thorsteinn Rögnvaldsson,,https://arxiv.org/abs/1909.07053v3,https://arxiv.org/pdf/1909.07053v3,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1909.07053v3.pdf
1909.03575v2,2019-09-09T00:41:54Z,2019-09-10 16:24:52,A direct Hamiltonian MCMC approach for reliability estimation,"Accurate and efficient estimation of rare events probabilities is of significant importance, since often the occurrences of such events have widespread impacts. The focus in this work is on precisely quantifying these probabilities, often encountered in reliability analysis of complex engineering systems, by introducing a gradient-based Hamiltonian Markov Chain Monte Carlo (HMCMC) framework, termed Approximate Sampling Target with Post-processing Adjustment (ASTPA). The basic idea is to construct a relevant target distribution by weighting the high-dimensional random variable space through a one-dimensional likelihood model, using the limit-state function. To sample from this target distribution we utilize HMCMC algorithms that produce Markov chain samples based on Hamiltonian dynamics rather than random walks. We compare the performance of typical HMCMC scheme with our newly developed Quasi-Newton based mass preconditioned HMCMC algorithm that can sample very adeptly, particularly in difficult cases with high-dimensionality and very small failure probabilities. To eventually compute the probability of interest, an original post-sampling step is devised at this stage, using an inverse importance sampling procedure based on the samples. The involved user-defined parameters of ASTPA are then discussed and general default values are suggested. Finally, the performance of the proposed methodology is examined in detail and compared against Subset Simulation in a series of static and dynamic low- and high-dimensional benchmark problems.",Hamed Nikbakht|Konstantinos G. Papakonstantinou,,https://arxiv.org/abs/1909.03575v2,https://arxiv.org/pdf/1909.03575v2,,UNCECOMP 2019; 3rd ECCOMAS Thematic Conference on Uncertainty Quantification in Computational Sciences and Engineering,,,stat.CO,stat.CO|stat.AP,https://arxiv.org/pdf/1909.03575v2.pdf
1908.11682v1,2019-08-30T12:18:29Z,2019-08-30 12:18:29,Discovering Reliable Correlations in Categorical Data,"In many scientific tasks we are interested in discovering whether there exist any correlations in our data. This raises many questions, such as how to reliably and interpretably measure correlation between a multivariate set of attributes, how to do so without having to make assumptions on distribution of the data or the type of correlation, and, how to efficiently discover the top-most reliably correlated attribute sets from data. In this paper we answer these questions for discovery tasks in categorical data.
  In particular, we propose a corrected-for-chance, consistent, and efficient estimator for normalized total correlation, by which we obtain a reliable, naturally interpretable, non-parametric measure for correlation over multivariate sets. For the discovery of the top-k correlated sets, we derive an effective algorithmic framework based on a tight bounding function. This framework offers exact, approximate, and heuristic search. Empirical evaluation shows that already for small sample sizes the estimator leads to low-regret optimization outcomes, while the algorithms are shown to be highly effective for both large and high-dimensional data. Through two case studies we confirm that our discovery framework identifies interesting and meaningful correlations.",Panagiotis Mandros|Mario Boley|Jilles Vreeken,,https://arxiv.org/abs/1908.11682v1,https://arxiv.org/pdf/1908.11682v1,,Accepted to the IEEE International Conference on Data Mining 2019 (ICDM'19),,,cs.LG,cs.LG|cs.DB|cs.IT|stat.ML,https://arxiv.org/pdf/1908.11682v1.pdf
1908.10928v1,2019-08-28T20:08:41Z,2019-08-28 20:08:41,Low Cost Sensor Networks; How Do We Know the Data are Reliable?,Plausibility of data from networks of low-cost measurement devices is a growing and important contentious issue. Informal networks of low-cost devices have particularly come to prominence for air quality monitoring. The contentious point is the believability of data without regular on-site calibration since that is a specialist task and the costs very quickly become very much larger than the cost of installation in the first place. This article suggests that approaches to the problem that involve appropriate use of independent information have the potential to resolve the contention. Ideas are illustrated particularly with reference to low-cost sensor networks for air quality measurement.,David E Williams,,https://arxiv.org/abs/1908.10928v1,https://arxiv.org/pdf/1908.10928v1,https://doi.org/10.1021/acssensors.9b01455,14 pages,,10.1021/acssensors.9b01455,stat.AP,stat.AP,https://arxiv.org/pdf/1908.10928v1.pdf
1909.02115v1,2019-08-28T01:29:52Z,2019-08-28 01:29:52,Artificial Neural Networks and Adaptive Neuro-fuzzy Models for Prediction of Remaining Useful Life,"The U.S. water distribution system contains thousands of miles of pipes constructed from different materials, and of various sizes, and age. These pipes suffer from physical, environmental, structural and operational stresses, causing deterioration which eventually leads to their failure. Pipe deterioration results in increased break rates, reduced hydraulic capacity, and detrimental impacts on water quality. Therefore, it is crucial to use accurate models to forecast deterioration rates along with estimating the remaining useful life of the pipes to implement essential interference plans in order to prevent catastrophic failures. This paper discusses a computational model that forecasts the RUL of water pipes by applying Artificial Neural Networks (ANNs) as well as Adaptive Neural Fuzzy Inference System (ANFIS). These models are trained and tested acquired field data to identify the significant parameters that impact the prediction of RUL. It is concluded that, on average, with approximately 10\% of wall thickness loss in existing cast iron, ductile iron, asbestos-cement, and steel water pipes, the reduction of the remaining useful life is approximately 50%",Razieh Tavakoli|Mohammad Najafi|Ali Sharifara,,https://arxiv.org/abs/1909.02115v1,https://arxiv.org/pdf/1909.02115v1,,,,,cs.LG,cs.LG|cs.NE|stat.ML,https://arxiv.org/pdf/1909.02115v1.pdf
1908.09729v1,2019-08-26T15:23:13Z,2019-08-26 15:23:13,Statistical Analysis of Modern Reliability Data,"Traditional reliability analysis has been using time to event data, degradation data, and recurrent event data, while the associated covariates tend to be simple and constant over time. Over the past years, we have witnessed the rapid development of sensor and wireless technology, which enables us to track how the product has been used and under which environmental conditions it has been used. Nowadays, we are able to collect richer information on covariates which provides opportunities for better reliability predictions. In this chapter, we first review recent development on statistical methods for reliability analysis. We then focus on introducing several specific methods that were developed for different types of reliability data with covariate information. Illustrations of those methods are also provided using examples from industry. Test planning is also an important part of reliability analysis. In addition to data analysis, we also provide a briefly review on recent developments of test planning and then focus on illustrating the sequential Bayesian design with an example of fatigue testing for polymer composites. The paper is concluded with some discussions and remarks.",Yueyao Wang|I-Chen Lee|Lu Lu|Yili Hong,,https://arxiv.org/abs/1908.09729v1,https://arxiv.org/pdf/1908.09729v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1908.09729v1.pdf
1908.09238v1,2019-08-25T00:11:34Z,2019-08-25 00:11:34,On Accurate and Reliable Anomaly Detection for Gas Turbine Combustors: A Deep Learning Approach,"Monitoring gas turbine combustors health, in particular, early detecting abnormal behaviors and incipient faults, is critical in ensuring gas turbines operating efficiently and in preventing costly unplanned maintenance. One popular means of detecting combustor abnormalities is through continuously monitoring exhaust gas temperature profiles. Over the years many anomaly detection technologies have been explored for detecting combustor faults, however, the performance (detection rate) of anomaly detection solutions fielded is still inadequate. Advanced technologies that can improve detection performance are in great need. Aiming for improving anomaly detection performance, in this paper we introduce recently-developed deep learning (DL) in machine learning into the combustors anomaly detection application. Specifically, we use deep learning to hierarchically learn features from the sensor measurements of exhaust gas temperatures. And we then use the learned features as the input to a neural network classifier for performing combustor anomaly detection. Since such deep learned features potentially better capture complex relations among all sensor measurements and the underlying combustor behavior than handcrafted features do, we expect the learned features can lead to a more accurate and robust anomaly detection. Using the data collected from a real-world gas turbine combustion system, we demonstrated that the proposed deep learning based anomaly detection significantly indeed improved combustor anomaly detection performance.",Weizhong Yan|Lijie Yu,,https://arxiv.org/abs/1908.09238v1,https://arxiv.org/pdf/1908.09238v1,,8 pages,PHM 2015 Conference,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1908.09238v1.pdf
1907.12961v1,2019-07-30T14:09:58Z,2019-07-30 14:09:58,Modeling long-term capacity degradation of lithium-ion batteries,"Capacity degradation of lithium-ion batteries under long-term cyclic aging is modelled via a flexible sigmoidal-type regression set-up, where the regression parameters can be interpreted. Different approaches known from the literature are discussed and compared with the new proposal. Statistical procedures, such as parameter estimation, confidence and prediction intervals are presented and applied to real data. The long-term capacity degradation model may be applied in second-life scenarios of batteries. Using some prior information or training data on the complete degradation path, the model can be fitted satisfactorily even if only short-term degradation data is available. The training data may arise from a single battery.",Marcus Johnen|Simon Pitzen|Udo Kamps|Maria Kateri|Dirk Uwe Sauer,,https://arxiv.org/abs/1907.12961v1,https://arxiv.org/pdf/1907.12961v1,,"15 pages, 9 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/1907.12961v1.pdf
1907.08120v1,2019-07-18T15:50:37Z,2019-07-18 15:50:37,Automating concept-drift detection by self-evaluating predictive model degradation,"A key aspect of automating predictive machine learning entails the capability of properly triggering the update of the trained model. To this aim, suitable automatic solutions to self-assess the prediction quality and the data distribution drift between the original training set and the new data have to be devised. In this paper, we propose a novel methodology to automatically detect prediction-quality degradation of machine learning models due to class-based concept drift, i.e., when new data contains samples that do not fit the set of class labels known by the currently-trained predictive model. Experiments on synthetic and real-world public datasets show the effectiveness of the proposed methodology in automatically detecting and describing concept drift caused by changes in the class-label data distributions.",Tania Cerquitelli|Stefano Proto|Francesco Ventura|Daniele Apiletti|Elena Baralis,,https://arxiv.org/abs/1907.08120v1,https://arxiv.org/pdf/1907.08120v1,,"5 pages, 4 figures",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1907.08120v1.pdf
1907.07476v1,2019-07-17T12:38:04Z,2019-07-17 12:38:04,Achieving Ultra-Reliable Communication via CRAN-Enabled Diversity Schemes,"Internet of things is in progress to build a smart society, and wireless networks are critical enablers for many of its use cases. In this paper, we present a multi-coordinated transmission scheme to achieve ultra-reliability for critical machine-type wireless communication networks. We take advantage of diversity, which is fundamental for dealing with fading channel impairments, and for achieving ultra-reliable region of operation in order of five 9's as defined by 3GPP standardization bodies. We evaluate an interference-limited network composed of multiple remote radio heads that are allowed to cooperate, by keeping silence thus reducing interference, or by performing more elaborated strategies such as maximal ratio transmission, in order to serve a user equipment with ultra-reliability. We provide extensive numerical analysis and discuss the gains of cooperation by the centralized radio access network.",Binod Kharel|Onel L. Alcaraz López|Hirley Alves|Matti Latva-aho,,https://arxiv.org/abs/1907.07476v1,https://arxiv.org/pdf/1907.07476v1,,"5 pages,6 figures, conference",,,cs.NI,cs.NI|eess.SP|stat.AP,https://arxiv.org/pdf/1907.07476v1.pdf
1907.05146v2,2019-07-11T12:33:45Z,2019-07-19 14:46:08,Forecasting remaining useful life: Interpretable deep learning approach via variational Bayesian inferences,"Predicting the remaining useful life of machinery, infrastructure, or other equipment can facilitate preemptive maintenance decisions, whereby a failure is prevented through timely repair or replacement. This allows for a better decision support by considering the anticipated time-to-failure and thus promises to reduce costs. Here a common baseline may be derived by fitting a probability density function to past lifetimes and then utilizing the (conditional) expected remaining useful life as a prognostic. This approach finds widespread use in practice because of its high explanatory power. A more accurate alternative is promised by machine learning, where forecasts incorporate deterioration processes and environmental variables through sensor data. However, machine learning largely functions as a black-box method and its forecasts thus forfeit most of the desired interpretability. As our primary contribution, we propose a structured-effect neural network for predicting the remaining useful life which combines the favorable properties of both approaches: its key innovation is that it offers both a high accountability and the flexibility of deep learning. The parameters are estimated via variational Bayesian inferences. The different approaches are compared based on the actual time-to-failure for aircraft engines. This demonstrates the performance and superior interpretability of our method, while we finally discuss implications for decision support.",Mathias Kraus|Stefan Feuerriegel,,https://arxiv.org/abs/1907.05146v2,https://arxiv.org/pdf/1907.05146v2,https://doi.org/10.1016/j.dss.2019.113100,,,10.1016/j.dss.2019.113100,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1907.05146v2.pdf
1907.03814v1,2019-07-08T18:59:23Z,2019-07-08 18:59:23,Road Maintenance Operation Start Time Optimization Based on Real-time Traffic Map Data,"Optimizing the maintenance operation start time can greatly reduce the delays caused by the maintenance operations. A real-time traffic status data acquisition method based on real-time traffic map was first proposed, and then a method that can convert real-time traffic status into real-time traffic volume was put forward. Based on this real-time traffic volume data and the classic delay calculation method based on queuing theory, the delays caused by maintenance operations at different start time can be calculated and compared, and therefore the optimal maintenance operation start time can be obtained. The feasibility of the real-time traffic status data to real-time traffic volume data conversion method and the feasibility of optimizing the maintenance operation start time based on real-time traffic map data are verified by actual cases.",Zhepu Xu|Qun Yang,,https://arxiv.org/abs/1907.03814v1,https://arxiv.org/pdf/1907.03814v1,,"14 pages, in Chinese, 12 figures, 7 tables",,,stat.AP,stat.AP,https://arxiv.org/pdf/1907.03814v1.pdf
1907.01003v2,2019-07-01T18:18:10Z,2019-12-12 18:32:51,"Accurate, reliable and fast robustness evaluation","Throughout the past five years, the susceptibility of neural networks to minimal adversarial perturbations has moved from a peculiar phenomenon to a core issue in Deep Learning. Despite much attention, however, progress towards more robust models is significantly impaired by the difficulty of evaluating the robustness of neural network models. Today's methods are either fast but brittle (gradient-based attacks), or they are fairly reliable but slow (score- and decision-based attacks). We here develop a new set of gradient-based adversarial attacks which (a) are more reliable in the face of gradient-masking than other gradient-based attacks, (b) perform better and are more query efficient than current state-of-the-art gradient-based attacks, (c) can be flexibly adapted to a wide range of adversarial criteria and (d) require virtually no hyperparameter tuning. These findings are carefully validated across a diverse set of six different models and hold for L0, L1, L2 and Linf in both targeted as well as untargeted scenarios. Implementations will soon be available in all major toolboxes (Foolbox, CleverHans and ART). We hope that this class of attacks will make robustness evaluations easier and more reliable, thus contributing to more signal in the search for more robust machine learning models.",Wieland Brendel|Jonas Rauber|Matthias Kümmerer|Ivan Ustyuzhaninov|Matthias Bethge,,https://arxiv.org/abs/1907.01003v2,https://arxiv.org/pdf/1907.01003v2,,Accepted at the 2019 Conference on Neural Information Processing Systems,,,stat.ML,stat.ML|cs.CR|cs.CV|cs.LG|cs.NE,https://arxiv.org/pdf/1907.01003v2.pdf
1906.08421v1,2019-06-20T02:36:25Z,2019-06-20 02:36:25,Reliable data from low cost ozone sensors in a hierarchical network,"We demonstrate how a hierarchical network comprising a number of compliant reference stations and a much larger number of low-cost sensors can deliver reliable high temporal-resolution ozone data at neighbourhood scales. The framework, demonstrated originally for a smaller scale regional network deployed in the Lower Fraser Valley, BC was tested and refined using two much more extensive networks of gas-sensitive semiconductor-based (GSS) sensors deployed at neighbourhood scales in Los Angeles: one of ~20 and one of ~45 GSS ozone sensors. Of these, ten sensors were co-located with different regulatory measurement stations, allowing a rigorous test of the accuracy of the algorithms used for off-site calibration and adjustment of low cost sensors. The method is based on adjusting the gain and offset of the low-cost sensor to match the first two moments of the probability distribution of the sensor result to that of a proxy: a calibrated independent measurement (usually derived from regulatory monitors) whose probability distribution evaluated over a time that emphasizes diurnal variations is similar to that at the test location. The regulatory measurement station physically closest to the low-cost sensor was a good proxy for most sites. The algorithms developed were successful in detecting and correcting sensor drift, and in identifying locations where geographical features resulted in significantly different patterns of ozone variation due to the relative dominance of different dispersion, emission and chemical processes. The entire network results show very large variations in ozone concentration that take place on short time- and distance scales across the Los-Angeles region. Such patterns were not captured by the more sparsely distributed stations of the existing regulatory network and demonstrate the need for reliable data from dense networks of monitors.",Georgia Miskell|Kyle Alberti|Brandon Feenstra|Geoff S Henshaw|Vasileios Papapostolou|Hamesh Patel|Andrea Polidori|Jennifer A Salmond|Lena Weissert|David E Williams,,https://arxiv.org/abs/1906.08421v1,https://arxiv.org/pdf/1906.08421v1,https://doi.org/10.1016/j.atmosenv.2019.116870,"28 pages, 12 figures, Supplementray information appended has 14 pages",Atmospheric Environment 214 (2019) 116870,10.1016/j.atmosenv.2019.116870,stat.AP,stat.AP,https://arxiv.org/pdf/1906.08421v1.pdf
1906.07294v1,2019-06-17T22:40:32Z,2019-06-17 22:40:32,Template Independent Component Analysis: Targeted and Reliable Estimation of Subject-level Brain Networks using Big Data Population Priors,"Large brain imaging databases contain a wealth of information on brain organization in the populations they target, and on individual variability. While such databases have been used to study group-level features of populations directly, they are currently underutilized as a resource to inform single-subject analysis. Here, we propose leveraging the information contained in large functional magnetic resonance imaging (fMRI) databases by establishing population priors to employ in an empirical Bayesian framework. We focus on estimation of brain networks as source signals in independent component analysis (ICA). We formulate a hierarchical ""template"" ICA model where source signals---including known population brain networks and subject-specific signals---are represented as latent variables. For estimation, we derive an expectation maximization (EM) algorithm having an explicit solution. However, as this solution is computationally intractable, we also consider an approximate subspace algorithm and a faster two-stage approach. Through extensive simulation studies, we assess performance of both methods and compare with dual regression, a popular but ad-hoc method. The two proposed algorithms have similar performance, and both dramatically outperform dual regression. We also conduct a reliability study utilizing the Human Connectome Project and find that template ICA achieves substantially better performance than dual regression, achieving 75-250% higher intra-subject reliability.",Amanda F. Mejia|Mary Beth Nebel|Yikai Wang|Brian S. Caffo|Ying Guo,,https://arxiv.org/abs/1906.07294v1,https://arxiv.org/pdf/1906.07294v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1906.07294v1.pdf
1906.03118v1,2019-06-07T14:15:55Z,2019-06-07 14:15:55,Reliable Estimation of Individual Treatment Effect with Causal Information Bottleneck,"Estimating individual level treatment effects (ITE) from observational data is a challenging and important area in causal machine learning and is commonly considered in diverse mission-critical applications. In this paper, we propose an information theoretic approach in order to find more reliable representations for estimating ITE. We leverage the Information Bottleneck (IB) principle, which addresses the trade-off between conciseness and predictive power of representation. With the introduction of an extended graphical model for causal information bottleneck, we encourage the independence between the learned representation and the treatment type. We also introduce an additional form of a regularizer from the perspective of understanding ITE in the semi-supervised learning framework to ensure more reliable representations. Experimental results show that our model achieves the state-of-the-art results and exhibits more reliable prediction performances with uncertainty information on real-world datasets.",Sungyub Kim|Yongsu Baek|Sung Ju Hwang|Eunho Yang,,https://arxiv.org/abs/1906.03118v1,https://arxiv.org/pdf/1906.03118v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1906.03118v1.pdf
1906.03260v2,2019-06-04T10:14:58Z,2019-11-04 12:29:52,Reliable training and estimation of variance networks,"We propose and investigate new complementary methodologies for estimating predictive variance networks in regression neural networks. We derive a locally aware mini-batching scheme that result in sparse robust gradients, and show how to make unbiased weight updates to a variance network. Further, we formulate a heuristic for robustly fitting both the mean and variance networks post hoc. Finally, we take inspiration from posterior Gaussian processes and propose a network architecture with similar extrapolation properties to Gaussian processes. The proposed methodologies are complementary, and improve upon baseline methods individually. Experimentally, we investigate the impact on predictive uncertainty on multiple datasets and tasks ranging from regression, active learning and generative modeling. Experiments consistently show significant improvements in predictive uncertainty estimation over state-of-the-art methods across tasks and datasets.",Nicki S. Detlefsen|Martin Jørgensen|Søren Hauberg,,https://arxiv.org/abs/1906.03260v2,https://arxiv.org/pdf/1906.03260v2,,Appeared at NeurIPS 2019,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1906.03260v2.pdf
1906.01127v1,2019-06-03T23:43:16Z,2019-06-03 23:43:16,Proximal Reliability Optimization for Reinforcement Learning,"Despite the numerous advances, reinforcement learning remains away from widespread acceptance for autonomous controller design as compared to classical methods due to lack of ability to effectively tackle the reality gap. The reliance on absolute or deterministic reward as a metric for optimization process renders reinforcement learning highly susceptible to changes in problem dynamics. We introduce a novel framework that effectively quantizes the uncertainty of the design space and induces robustness in controllers by switching to a reliability-based optimization routine. The data efficiency of the method is maintained to match reward based optimization methods by employing a model-based approach. We prove the stability of learned neuro-controllers in both static and dynamic environments on classical reinforcement learning tasks such as Cart Pole balancing and Inverted Pendulum.",Narendra Patwardhan|Zequn Wang,,https://arxiv.org/abs/1906.01127v1,https://arxiv.org/pdf/1906.01127v1,,"12 pages, 6 figures",,,cs.LG,cs.LG|eess.SY|stat.ML,https://arxiv.org/pdf/1906.01127v1.pdf
1905.10713v3,2019-05-26T02:41:33Z,2020-01-27 12:21:37,Field-aware Calibration: A Simple and Empirically Strong Method for Reliable Probabilistic Predictions,"It is often observed that the probabilistic predictions given by a machine learning model can disagree with averaged actual outcomes on specific subsets of data, which is also known as the issue of miscalibration. It is responsible for the unreliability of practical machine learning systems. For example, in online advertising, an ad can receive a click-through rate prediction of 0.1 over some population of users where its actual click rate is 0.15. In such cases, the probabilistic predictions have to be fixed before the system can be deployed.
  In this paper, we first introduce a new evaluation metric named field-level calibration error that measures the bias in predictions over the sensitive input field that the decision-maker concerns. We show that existing post-hoc calibration methods have limited improvements in the new field-level metric and other non-calibration metrics such as the AUC score. To this end, we propose Neural Calibration, a simple yet powerful post-hoc calibration method that learns to calibrate by making full use of the field-aware information over the validation set. We present extensive experiments on five large-scale datasets. The results showed that Neural Calibration significantly improves against uncalibrated predictions in common metrics such as the negative log-likelihood, Brier score and AUC, as well as the proposed field-level calibration error.",Feiyang Pan|Xiang Ao|Pingzhong Tang|Min Lu|Dapeng Liu|Lei Xiao|Qing He,,https://arxiv.org/abs/1905.10713v3,https://arxiv.org/pdf/1905.10713v3,https://doi.org/10.1145/3366423.3380154,WWW 2020,,10.1145/3366423.3380154,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1905.10713v3.pdf
1905.08659v1,2019-05-21T14:08:27Z,2019-05-21 14:08:27,Assurance for sample size determination in reliability demonstration testing,"Manufacturers are required to demonstrate products meet reliability targets. A typical way to achieve this is with reliability demonstration tests (RDTs), in which a number of products are put on test and the test is passed if a target reliability is achieved. There are various methods for determining the sample size for RDTs, typically based on the power of a hypothesis test following the RDT or risk criteria. Bayesian risk criteria approaches can conflate the choice of sample size and the analysis to be undertaken once the test has been conducted and rely on the specification of somewhat artificial acceptable and rejectable reliability levels. In this paper we offer an alternative approach to sample size determination based on the idea of assurance. This approach chooses the sample size to answer provide a certain probability that the RDT will result in a successful outcome. It separates the design and analysis of the RDT, allowing different priors for each. We develop the assurance approach for sample size calculations in RDTs for binomial and Weibull likelihoods and propose appropriate prior distributions for the design and analysis of the test. In each case, we illustrate the approach with an example based on real data.",Kevin James Wilson|Malcolm Farrow,"School of Mathematics, Statistics & Physics, Newcastle University, UK|School of Mathematics, Statistics & Physics, Newcastle University, UK",https://arxiv.org/abs/1905.08659v1,https://arxiv.org/pdf/1905.08659v1,,"22 pages, 9 figures",,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/1905.08659v1.pdf
1905.05918v1,2019-05-15T02:31:45Z,2019-05-15 02:31:45,A Neural Network-Evolutionary Computational Framework for Remaining Useful Life Estimation of Mechanical Systems,"This paper presents a framework for estimating the remaining useful life (RUL) of mechanical systems. The framework consists of a multi-layer perceptron and an evolutionary algorithm for optimizing the data-related parameters. The framework makes use of a strided time window to estimate the RUL for mechanical components. Tuning the data-related parameters can become a very time consuming task. The framework presented here automatically reshapes the data such that the efficiency of the model is increased. Furthermore, the complexity of the model is kept low, e.g. neural networks with few hidden layers and few neurons at each layer. Having simple models has several advantages like short training times and the capacity of being in environments with limited computational resources such as embedded systems. The proposed method is evaluated on the publicly available C-MAPSS dataset, its accuracy is compared against other state-of-the art methods for the same dataset.",David Laredo|Zhaoyin Chen|Oliver Schütze|Jian-Qiao Sun,,https://arxiv.org/abs/1905.05918v1,https://arxiv.org/pdf/1905.05918v1,https://doi.org/10.1016/j.neunet.2019.04.016,"Published at Neural Networks 116, (2019) 178-187","Neural Networks 116, (2019) 178-187",10.1016/j.neunet.2019.04.016,cs.LG,cs.LG|cs.NE|stat.ML,https://arxiv.org/pdf/1905.05918v1.pdf
1905.00630v2,2019-05-02T09:13:12Z,2019-11-13 16:00:30,Reliability of relational event model estimates under sampling: how to fit a relational event model to 360 million dyadic events,"We assess the reliability of relational event model parameters estimated under two sampling schemes: (1) uniform sampling from the observed events and (2) case-control sampling which samples non-events, or null dyads (""controls""), from a suitably defined risk set. We experimentally determine the variability of estimated parameters as a function of the number of sampled events and controls per event, respectively. Results suggest that relational event models can be reliably fitted to networks with more than 12 million nodes connected by more than 360 million dyadic events by analyzing a sample of some tens of thousands of events and a small number of controls per event. Using data that we collected on the Wikipedia editing network, we illustrate how network effects commonly included in empirical studies based on relational event models need widely different sample sizes to be estimated reliably. For our analysis we use an open-source software which implements the two sampling schemes, allowing analysts to fit and analyze relational event models to the same or other data that may be collected in different empirical settings, varying sample parameters or model specification.",Jürgen Lerner|Alessandro Lomi,,https://arxiv.org/abs/1905.00630v2,https://arxiv.org/pdf/1905.00630v2,https://doi.org/10.1017/nws.2019.57,,,10.1017/nws.2019.57,cs.SI,cs.SI|stat.ME,https://arxiv.org/pdf/1905.00630v2.pdf
1905.13014v2,2019-04-26T03:15:59Z,2019-06-05 08:39:01,Unsupervised Deep Learning for Ultra-reliable and Low-latency Communications,"In this paper, we study how to solve resource allocation problems in ultra-reliable and low-latency communications by unsupervised deep learning, which often yield functional optimization problems with quality-of-service (QoS) constraints. We take a joint power and bandwidth allocation problem as an example, which minimizes the total bandwidth required to guarantee the QoS of each user in terms of the delay bound and overall packet loss probability. The global optimal solution is found in a symmetric scenario. A neural network was introduced to find an approximated optimal solution in general scenarios, where the QoS is ensured by using the property that the optimal solution should satisfy as the ""supervision signal"". Simulation results show that the learning-based solution performs the same as the optimal solution in the symmetric scenario, and can save around 40% bandwidth with respect to the state-of-the-art policy.",Chengjian Sun|Chenyang Yang,,https://arxiv.org/abs/1905.13014v2,https://arxiv.org/pdf/1905.13014v2,,"6 pages, 1 figure, submitted to IEEE for possible publication. arXiv admin note: text overlap with arXiv:1905.11017",,,cs.NI,cs.NI|eess.SP|stat.ML,https://arxiv.org/pdf/1905.13014v2.pdf
1904.09743v1,2019-04-22T06:50:39Z,2019-04-22 06:50:39,Reliable Weakly Supervised Learning: Maximize Gain and Maintain Safeness,"Weakly supervised data are widespread and have attracted much attention. However, since label quality is often difficult to guarantee, sometimes the use of weakly supervised data will lead to unsatisfactory performance, i.e., performance degradation or poor performance gains. Moreover, it is usually not feasible to manually increase the label quality, which results in weakly supervised learning being somewhat difficult to rely on. In view of this crucial issue, this paper proposes a simple and novel weakly supervised learning framework. We guide the optimization of label quality through a small amount of validation data, and to ensure the safeness of performance while maximizing performance gain. As validation set is a good approximation for describing generalization risk, it can effectively avoid the unsatisfactory performance caused by incorrect data distribution assumptions. We formalize this underlying consideration into a novel Bi-Level optimization and give an effective solution. Extensive experimental results verify that the new framework achieves impressive performance on weakly supervised learning with a small amount of validation data.",Lan-Zhe Guo|Yu-Feng Li|Ming Li|Jin-Feng Yi|Bo-Wen Zhou|Zhi-Hua Zhou,,https://arxiv.org/abs/1904.09743v1,https://arxiv.org/pdf/1904.09743v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1904.09743v1.pdf
1904.09235v2,2019-04-19T15:33:06Z,2020-01-24 08:49:16,Reliable Multi-label Classification: Prediction with Partial Abstention,"In contrast to conventional (single-label) classification, the setting of multilabel classification (MLC) allows an instance to belong to several classes simultaneously. Thus, instead of selecting a single class label, predictions take the form of a subset of all labels. In this paper, we study an extension of the setting of MLC, in which the learner is allowed to partially abstain from a prediction, that is, to deliver predictions on some but not necessarily all class labels. We propose a formalization of MLC with abstention in terms of a generalized loss minimization problem and present first results for the case of the Hamming loss, rank loss, and F-measure, both theoretical and experimental.",Vu-Linh Nguyen|Eyke Hüllermeier,,https://arxiv.org/abs/1904.09235v2,https://arxiv.org/pdf/1904.09235v2,,"19 pages, 12 figures","Proceedings AAAI-20, Thirty-Fourth AAAI Conference on Artificial Intelligence, New York, USA, 2020",,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1904.09235v2.pdf
1904.06442v1,2019-04-12T22:48:49Z,2019-04-12 22:48:49,Remaining Useful Life Estimation Using Functional Data Analysis,"Remaining Useful Life (RUL) of an equipment or one of its components is defined as the time left until the equipment or component reaches its end of useful life. Accurate RUL estimation is exceptionally beneficial to Predictive Maintenance, and Prognostics and Health Management (PHM). Data driven approaches which leverage the power of algorithms for RUL estimation using sensor and operational time series data are gaining popularity. Existing algorithms, such as linear regression, Convolutional Neural Network (CNN), Hidden Markov Models (HMMs), and Long Short-Term Memory (LSTM), have their own limitations for the RUL estimation task. In this work, we propose a novel Functional Data Analysis (FDA) method called functional Multilayer Perceptron (functional MLP) for RUL estimation. Functional MLP treats time series data from multiple equipment as a sample of random continuous processes over time. FDA explicitly incorporates both the correlations within the same equipment and the random variations across different equipment's sensor time series into the model. FDA also has the benefit of allowing the relationship between RUL and sensor variables to vary over time. We implement functional MLP on the benchmark NASA C-MAPSS data and evaluate the performance using two popularly-used metrics. Results show the superiority of our algorithm over all the other state-of-the-art methods.",Qiyao Wang|Shuai Zheng|Ahmed Farahat|Susumu Serita|Chetan Gupta,,https://arxiv.org/abs/1904.06442v1,https://arxiv.org/pdf/1904.06442v1,,Accepted by IEEE International Conference on Prognostics and Health Management 2019,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1904.06442v1.pdf
1904.06330v1,2019-04-12T17:18:08Z,2019-04-12 17:18:08,Reliable Prediction Errors for Deep Neural Networks Using Test-Time Dropout,"While the use of deep learning in drug discovery is gaining increasing attention, the lack of methods to compute reliable errors in prediction for Neural Networks prevents their application to guide decision making in domains where identifying unreliable predictions is essential, e.g. precision medicine. Here, we present a framework to compute reliable errors in prediction for Neural Networks using Test-Time Dropout and Conformal Prediction. Specifically, the algorithm consists of training a single Neural Network using dropout, and then applying it N times to both the validation and test sets, also employing dropout in this step. Therefore, for each instance in the validation and test sets an ensemble of predictions were generated. The residuals and absolute errors in prediction for the validation set were then used to compute prediction errors for test set instances using Conformal Prediction. We show using 24 bioactivity data sets from ChEMBL 23 that dropout Conformal Predictors are valid (i.e., the fraction of instances whose true value lies within the predicted interval strongly correlates with the confidence level) and efficient, as the predicted confidence intervals span a narrower set of values than those computed with Conformal Predictors generated using Random Forest (RF) models. Lastly, we show in retrospective virtual screening experiments that dropout and RF-based Conformal Predictors lead to comparable retrieval rates of active compounds. Overall, we propose a computationally efficient framework (as only N extra forward passes are required in addition to training a single network) to harness Test-Time Dropout and the Conformal Prediction framework, and to thereby generate reliable prediction errors for deep Neural Networks.",Isidro Cortes-Ciriano|Andreas Bender,,https://arxiv.org/abs/1904.06330v1,https://arxiv.org/pdf/1904.06330v1,https://doi.org/10.1021/acs.jcim.9b00297,,,10.1021/acs.jcim.9b00297,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1904.06330v1.pdf
1904.04636v3,2019-04-08T17:24:42Z,2020-07-28 12:13:08,Tipping point analysis of electrical resistance data with early warning signals of failure for predictive maintenance,"We apply tipping point analysis to measurements of electronic components commonly used in applications in the automotive or aviation industries and demonstrate early warning signals based on scaling properties of resistance time series. The analysis utilises the statistical physics framework with stochastic modelling by representing the measured time series as a composition of deterministic and stochastic components estimated from measurements. The early warning signals are observed much earlier than those estimated from conventional techniques, such as threshold-based failure detection, or bulk estimates used in Weibull failure analysis. The introduced techniques may be useful for predictive maintenance of power electronics, with industrial applications. We suggest that this approach can be applied to various electromagnetic measurements in power systems and energy applications.",Valerie Livina|Adam Lewis|Martin Wickham,,https://arxiv.org/abs/1904.04636v3,https://arxiv.org/pdf/1904.04636v3,,"13 pages, 5 figures. Accepted for publication in Journal of Electronic Testing (Springer)",,,physics.soc-ph,physics.soc-ph|physics.data-an|stat.AP,https://arxiv.org/pdf/1904.04636v3.pdf
1904.01128v1,2019-04-01T22:13:27Z,2019-04-01 22:13:27,Analysis of Large Heterogeneous Repairable System Reliability Data with Static System Attributes and Dynamic Sensor Measurement in Big Data Environment,"In Big Data environment, one pressing challenge facing engineers is to perform reliability analysis for a large fleet of heterogeneous repairable systems with covariates. In addition to static covariates, which include time-invariant system attributes such as nominal operating conditions, geo-locations, etc., the recent advances of sensing technologies have also made it possible to obtain dynamic sensor measurement of system operating and environmental conditions. As a common practice in the Big Data environment, the massive reliability data are typically stored in some distributed storage systems. Leveraging the power of modern statistical learning, this paper investigates a statistical approach which integrates the Random Forests algorithm and the classical data analysis methodologies for repairable system reliability, such as the nonparametric estimator for the Mean Cumulative Function and the parametric models based on the Nonhomogeneous Poisson Process. We show that the proposed approach effectively addresses some common challenges arising from practice, including system heterogeneity, covariate selection, model specification and data locality due to the distributed data storage. The large sample properties as well as the uniform consistency of the proposed estimator is established. Two numerical examples and a case study are presented to illustrate the application of the proposed approach. The strengths of the proposed approach are demonstrated by comparison studies.",Xiao Liu|Rong Pan,,https://arxiv.org/abs/1904.01128v1,https://arxiv.org/pdf/1904.01128v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1904.01128v1.pdf
1903.05347v3,2019-03-13T07:54:58Z,2023-01-18 18:10:10,What relations are reliably embeddable in Euclidean space?,"We consider the problem of embedding a relation, represented as a directed graph, into Euclidean space. For three types of embeddings motivated by the recent literature on knowledge graphs, we obtain characterizations of which relations they are able to capture, as well as bounds on the minimal dimensionality and precision needed.",Robi Bhattacharjee|Sanjoy Dasgupta,,https://arxiv.org/abs/1903.05347v3,https://arxiv.org/pdf/1903.05347v3,,Published at ALT 2020,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1903.05347v3.pdf
1902.08068v1,2019-02-21T14:30:59Z,2019-02-21 14:30:59,"Towards Reliable, Automated General Movement Assessment for Perinatal Stroke Screening in Infants Using Wearable Accelerometers","Perinatal stroke (PS) is a serious condition that, if undetected and thus untreated, often leads to life-long disability, in particular Cerebral Palsy (CP). In clinical settings, Prechtl's General Movement Assessment (GMA) can be used to classify infant movements using a Gestalt approach, identifying infants at high risk of developing PS. Training and maintenance of assessment skills are essential and expensive for the correct use of GMA, yet many practitioners lack these skills, preventing larger-scale screening and leading to significant risks of missing opportunities for early detection and intervention for affected infants. We present an automated approach to GMA, based on body-worn accelerometers and a novel sensor data analysis method-Discriminative Pattern Discovery (DPD)-that is designed to cope with scenarios where only coarse annotations of data are available for model training. We demonstrate the effectiveness of our approach in a study with 34 newborns (21 typically developing infants and 13 PS infants with abnormal movements). Our method is able to correctly recognise the trials with abnormal movements with at least the accuracy that is required by newly trained human annotators (75%), which is encouraging towards our ultimate goal of an automated PS screening system that can be used population-wide.",Yan Gao|Yang Long|Yu Guan|Anna Basu|Jessica Baggaley|Thomas Ploetz,,https://arxiv.org/abs/1902.08068v1,https://arxiv.org/pdf/1902.08068v1,,Gao and Long share equal contributions; This work has been accepted for publication in ACM IMWUT (Ubicomp) 2019;,,,cs.HC,cs.HC|cs.LG|stat.ML,https://arxiv.org/pdf/1902.08068v1.pdf
1902.04337v1,2019-02-12T11:38:25Z,2019-02-12 11:38:25,Winning the Big Data Technologies Horizon Prize: Fast and reliable forecasting of electricity grid traffic by identification of recurrent fluctuations,"This paper provides a description of the approach and methodology I used in winning the European Union Big Data Technologies Horizon Prize on data-driven prediction of electricity grid traffic. The methodology relies on identifying typical short-term recurrent fluctuations, which is subsequently refined through a regression-of-fluctuations approach. The key points and strategic considerations that led to selecting or discarding different methodological aspects are also discussed. The criteria include adaptability to changing conditions, reliability with outliers and missing data, robustness to noise, and efficiency in implementation.",Jose M. G. Vilar,,https://arxiv.org/abs/1902.04337v1,https://arxiv.org/pdf/1902.04337v1,,Approach and methodology used in winning the European Union Big Data Technologies Horizon Prize (https://ec.europa.eu/research/horizonprize/index.cfm?pg=prizes),,,stat.AP,stat.AP|cond-mat.stat-mech|cs.CE|cs.LG,https://arxiv.org/pdf/1902.04337v1.pdf
1902.00770v1,2019-02-02T19:09:44Z,2019-02-02 19:09:44,"The reliability of an environmental epidemiology meta-analysis, a case study","Summary
  Background Claims made in science papers are coming under increased scrutiny with many claims failing to replicate. Meta-analysis studies that use unreliable observational studies should be in question. We examine the reliability of the base studies used in an air quality/heart attack meta-analysis and the resulting meta-analysis.
  Methods A meta-analysis study that includes 14 observational air quality/heart attack studies is examined for its statistical reliability. We use simple counting to evaluate the reliability of the base papers and a p-value plot of the p-values from the base studies to examine study heterogeneity.
  Findings We find that the based papers have massive multiple testing and multiple modeling with no statistical adjustments. Statistics coming from the base papers are not guaranteed to be unbiased, a requirement for a valid meta-analysis. There is study heterogeneity for the base papers with strong evidence for so called p-hacking.
  Interpretation We make two observations: there are many claims at issue in each of the 14 base studies so uncorrected multiple testing is a serious issue. We find the base papers and the resulting meta-analysis are unreliable.",S. Stanley Young|Mithun Kumar Acharjee|Kumer Das,,https://arxiv.org/abs/1902.00770v1,https://arxiv.org/pdf/1902.00770v1,https://doi.org/10.1016/j.yrtph.2018.12.013,"19 pages, 3 tables, 3 figures","Regulatory Toxicology and Pharmacology, 2019",10.1016/j.yrtph.2018.12.013,stat.AP,stat.AP,https://arxiv.org/pdf/1902.00770v1.pdf
1901.10984v1,2019-01-30T18:25:48Z,2019-01-30 18:25:48,Critical states in Political Trends. How much reliable is a poll on Twitter? A study by means of the Potts Model,"In recent years, Twitter data related to political trends have tentatively been used to make predictions (poll) about several electoral events. Given $q$ candidates for an election and a time-series of Twitts (short messages), one can extract the $q$ mean trends and the $q(q+1)/2$ Twitt-to-Twitt correlations, and look for the statistical models that reproduce these data. On the base of several electoral events and assuming a stationary regime, we find out the following: i) the maximization of the entropy singles out a microscopic model (single-Twitt-level) that coincides with a $q$-state Potts model having suitable couplings and external fields to be determined via an inverse problem from the two sets of data; ii) correlations decay as $1/N_{eff}$, where $N_{eff}$ is a small fraction of the mean number of Twitts; iii) the simplest statistical models that reproduce these correlations are the multinomial distribution (MD), characterized by $q$ external fields, and the mean-field Potts model (MFP), characterized by one coupling; iv) remarkably, this coupling turns out to be always close to its critical value. This results in a MD or MFP model scenario that discriminates between cases in which polls are reliable and not reliable, respectively. More precisely, predictions based on polls should be avoided whenever the data maps to a MFP because anomalous large fluctuations (if $q=2$) or sudden jumps (if $q\geq 3$) in the trends might take place as a result of a second-order or a first-order phase transition of the MFP, respectively.",Lucas Nicolao|Massimo Ostilli,,https://arxiv.org/abs/1901.10984v1,https://arxiv.org/pdf/1901.10984v1,https://doi.org/10.1016/j.physa.2019.121920,"14 pages, 14 figures","Physica A 533, 121920 (2019)",10.1016/j.physa.2019.121920,cond-mat.stat-mech,cond-mat.stat-mech|cond-mat.dis-nn|stat.AP,https://arxiv.org/pdf/1901.10984v1.pdf
1901.10622v2,2019-01-30T00:19:24Z,2019-06-03 04:56:53,Reliable Smart Road Signs,"In this paper, we propose a game theoretical adversarial intervention detection mechanism for reliable smart road signs. A future trend in intelligent transportation systems is ``smart road signs"" that incorporate smart codes (e.g., visible at infrared) on their surface to provide more detailed information to smart vehicles. Such smart codes make road sign classification problem aligned with communication settings more than conventional classification. This enables us to integrate well-established results in communication theory, e.g., error-correction methods, into road sign classification problem. Recently, vision-based road sign classification algorithms have been shown to be vulnerable against (even) small scale adversarial interventions that are imperceptible for humans. On the other hand, smart codes constructed via error-correction methods can lead to robustness against small scale intelligent or random perturbations on them. In the recognition of smart road signs, however, humans are out of the loop since they cannot see or interpret them. Therefore, there is no equivalent concept of imperceptible perturbations in order to achieve a comparable performance with humans. Robustness against small scale perturbations would not be sufficient since the attacker can attack more aggressively without such a constraint. Under a game theoretical solution concept, we seek to ensure certain measure of guarantees against even the worst case (intelligent) attackers that can perturb the signal even at large scale. We provide a randomized detection strategy based on the distance between the decoder output and the received input, i.e., error rate. Finally, we examine the performance of the proposed scheme over various scenarios.",Muhammed O. Sayin|Chung-Wei Lin|Eunsuk Kang|Shinichi Shiraishi|Tamer Basar,,https://arxiv.org/abs/1901.10622v2,https://arxiv.org/pdf/1901.10622v2,,,,,cs.LG,cs.LG|cs.CR|cs.IT|stat.ML,https://arxiv.org/pdf/1901.10622v2.pdf
1901.10855v1,2019-01-29T16:06:22Z,2019-01-29 16:06:22,Predictive Maintenance in Photovoltaic Plants with a Big Data Approach,"This paper presents a novel and flexible solution for fault prediction based on data collected from SCADA system. Fault prediction is offered at two different levels based on a data-driven approach: (a) generic fault/status prediction and (b) specific fault class prediction, implemented by means of two different machine learning based modules built on an unsupervised clustering algorithm and a Pattern Recognition Neural Network, respectively. Model has been assessed on a park of six photovoltaic (PV) plants up to 10 MW and on more than one hundred inverter modules of three different technology brands. The results indicate that the proposed method is effective in (a) predicting incipient generic faults up to 7 days in advance with sensitivity up to 95% and (b) anticipating damage of specific fault classes with times ranging from few hours up to 7 days. The model is easily deployable for on-line monitoring of anomalies on new PV plants and technologies, requiring only the availability of historical SCADA and fault data, fault taxonomy and inverter electrical datasheet. Keywords: Data Mining, Fault Prediction, Inverter Module, Key Performance Indicator, Lost Production",Alessandro Betti|Maria Luisa Lo Trovato|Fabio Salvatore Leonardi|Giuseppe Leotta|Fabrizio Ruffini|Ciro Lanzetta,,https://arxiv.org/abs/1901.10855v1,https://arxiv.org/pdf/1901.10855v1,https://doi.org/10.4229/EUPVSEC20172017-6DP.2.4,"Preprint of the 33rd EUPVSEC Proceeding, 25-29 September 2017, Amsterdam. Plenary Presentation","33rd European Photovoltaic Solar Energy Conference and Exhibition (EUPVSEC), pages 1895-1900 (2017)",10.4229/EUPVSEC20172017-6DP.2.4,cs.LG,cs.LG|eess.SY|stat.ML,https://arxiv.org/pdf/1901.10855v1.pdf
1901.09779v1,2019-01-28T16:27:20Z,2019-01-28 16:27:20,"Shannon's entropy and its Generalizations towards Statistics, Reliability and Information Science during 1948-2018","Starting from the pioneering works of Shannon and Weiner in 1948, a plethora of works have been reported on entropy in different directions. Entropy-related review work in the direction of statistics, reliability and information science, to the best of our knowledge, has not been reported so far. Here we have tried to collect all possible works in this direction during the period 1948-2018 so that people interested in entropy, specially the new researchers, get benefited.",Asok K Nanda|Shovan Chowdhury,,https://arxiv.org/abs/1901.09779v1,https://arxiv.org/pdf/1901.09779v1,,,,,stat.OT,stat.OT|stat.ME,https://arxiv.org/pdf/1901.09779v1.pdf
1901.10399v4,2019-01-15T21:39:33Z,2020-03-27 09:51:45,Optimal Replacement Policy under Cumulative Damage Model and Strength Degradation with Applications,"In many real-life scenarios, system failure depends on dynamic stress-strength interference, where strength degrades and stress accumulates concurrently over time. In this paper, we consider the problem of finding an optimal replacement strategy that balances the cost of replacement with the cost of failure and results in a minimum expected cost per unit time under cumulative damage model with strength degradation. The existing recommendations are applicable only under restricted distributional assumptions and/or with fixed strength. As theoretical evaluation of the expected cost per unit time turns out to be very complicated, a simulation-based algorithm is proposed to evaluate the expected cost rate and find the optimal replacement strategy. The proposed method is easy to implement having wider domain of application. For illustration, the proposed method is applied to real case studies on mailbox and cell-phone battery experiments.",Phalguni Nanda|Prajamitra Bhuyan|Anup Dewanji,,https://arxiv.org/abs/1901.10399v4,https://arxiv.org/pdf/1901.10399v4,https://doi.org/10.1007/s10479-021-04080-6,,,10.1007/s10479-021-04080-6,stat.AP,stat.AP|stat.CO|stat.ME,https://arxiv.org/pdf/1901.10399v4.pdf
1901.03311v1,2019-01-10T18:30:13Z,2019-01-10 18:30:13,Surrogate-assisted reliability-based design optimization: a survey and a new general framework,"Reliability-based design optimization (RBDO) is an active field of research with an ever increasing number of contributions. Numerous methods have been proposed for the solution of RBDO, a complex problem that combines optimization and reliability analysis. Classical approaches are based on approximation methods and have been classified in review papers. In this paper, we first review classical approaches based on approximation methods such as FORM, and also more recent methods that rely upon surrogate modelling and Monte Carlo simulation. We then propose a general framework for the solution of RBDO problems that includes three independent blocks, namely adaptive surrogate modelling, reliability analysis and optimization. These blocks are non-intrusive with respect to each other and can be plugged independently in the framework. After a discussion on numerical considerations that require attention for the framework to yield robust solutions to various types of problems, the latter is applied to three examples (using two analytical functions and a finite element model). Kriging and support vector machines together with their own active learning schemes are considered in the surrogate model block. In terms of reliability analysis, the proposed framework is illustrated using both crude Monte Carlo and subset simulation. Finally, the covariance-matrix adaptation - evolution scheme (CMA-ES), a global search algorithm, or sequential quadratic programming (SQP), a local gradient-based method, are used in the optimization block. The comparison of the results to benchmark studies show the effectiveness and efficiency of the proposed framework.",M. Moustapha|B. Sudret,,https://arxiv.org/abs/1901.03311v1,https://arxiv.org/pdf/1901.03311v1,,,,,stat.ME,stat.ME|stat.CO,https://arxiv.org/pdf/1901.03311v1.pdf
1901.02717v2,2019-01-05T02:17:57Z,2019-03-08 22:30:38,Reliable and Explainable Machine Learning Methods for Accelerated Material Discovery,"Material scientists are increasingly adopting the use of machine learning (ML) for making potentially important decisions, such as, discovery, development, optimization, synthesis and characterization of materials. However, despite ML's impressive performance in commercial applications, several unique challenges exist when applying ML in materials science applications. In such a context, the contributions of this work are twofold. First, we identify common pitfalls of existing ML techniques when learning from underrepresented/imbalanced material data. Specifically, we show that with imbalanced data, standard methods for assessing quality of ML models break down and lead to misleading conclusions. Furthermore, we found that the model's own confidence score cannot be trusted and model introspection methods (using simpler models) do not help as they result in loss of predictive performance (reliability-explainability trade-off). Second, to overcome these challenges, we propose a general-purpose explainable and reliable machine-learning framework. Specifically, we propose a novel pipeline that employs an ensemble of simpler models to reliably predict material properties. We also propose a transfer learning technique and show that the performance loss due to models' simplicity can be overcome by exploiting correlations among different material properties. A new evaluation metric and a trust score to better quantify the confidence in the predictions are also proposed. To improve the interpretability, we add a rationale generator component to our framework which provides both model-level and decision-level explanations. Finally, we demonstrate the versatility of our technique on two applications: 1) predicting properties of crystalline compounds, and 2) identifying novel potentially stable solar cell materials.",Bhavya Kailkhura|Brian Gallagher|Sookyung Kim|Anna Hiszpanski|T. Yong-Jin Han,,https://arxiv.org/abs/1901.02717v2,https://arxiv.org/pdf/1901.02717v2,,,,,physics.comp-ph,physics.comp-ph|cond-mat.mtrl-sci|stat.ML,https://arxiv.org/pdf/1901.02717v2.pdf
1901.00403v2,2019-01-02T14:53:33Z,2019-02-28 21:33:19,Can You Trust This Prediction? Auditing Pointwise Reliability After Learning,"To use machine learning in high stakes applications (e.g. medicine), we need tools for building confidence in the system and evaluating whether it is reliable. Methods to improve model reliability often require new learning algorithms (e.g. using Bayesian inference to obtain uncertainty estimates). An alternative is to audit a model after it is trained. In this paper, we describe resampling uncertainty estimation (RUE), an algorithm to audit the pointwise reliability of predictions. Intuitively, RUE estimates the amount that a prediction would change if the model had been fit on different training data. The algorithm uses the gradient and Hessian of the model's loss function to create an ensemble of predictions. Experimentally, we show that RUE more effectively detects inaccurate predictions than existing tools for auditing reliability subsequent to training. We also show that RUE can create predictive distributions that are competitive with state-of-the-art methods like Monte Carlo dropout, probabilistic backpropagation, and deep ensembles, but does not depend on specific algorithms at train-time like these methods do.",Peter Schulam|Suchi Saria,,https://arxiv.org/abs/1901.00403v2,https://arxiv.org/pdf/1901.00403v2,,To appear in the proceedings of Artificial Intelligence and Statistics (AISTATS) 2019,,,stat.ML,stat.ML|cs.LG|stat.ME,https://arxiv.org/pdf/1901.00403v2.pdf
1901.02063v5,2018-12-20T11:01:05Z,2022-12-29 17:23:37,Reliable Agglomerative Clustering,"Standard agglomerative clustering suggests establishing a new reliable linkage at every step. However, in order to provide adaptive, density-consistent and flexible solutions, we study extracting all the reliable linkages at each step, instead of the smallest one. Such a strategy can be applied with all common criteria for agglomerative hierarchical clustering. We also study that this strategy with the single linkage criterion yields a minimum spanning tree algorithm. We perform experiments on several real-world datasets to demonstrate the performance of this strategy compared to the standard alternative.",Morteza Haghir Chehreghani,,https://arxiv.org/abs/1901.02063v5,https://arxiv.org/pdf/1901.02063v5,https://doi.org/10.1109/IJCNN52387.2021.9534228,This works is published by IEEE IJCNN,"International Joint Conference on Neural Networks (IJCNN), pp. 1-8, 2021",10.1109/IJCNN52387.2021.9534228,cs.LG,cs.LG|cs.AI|stat.ML,https://arxiv.org/pdf/1901.02063v5.pdf
1812.04446v1,2018-12-11T14:57:57Z,2018-12-11 14:57:57,Data Strategies for Fleetwide Predictive Maintenance,"For predictive maintenance, we examine one of the largest public datasets for machine failures derived along with their corresponding precursors as error rates, historical part replacements, and sensor inputs. To simplify the time and accuracy comparison between 27 different algorithms, we treat the imbalance between normal and failing states with nominal under-sampling. We identify 3 promising regression and discriminant algorithms with both higher accuracy (96%) and twenty-fold faster execution times than previous work. Because predictive maintenance success hinges on input features prior to prediction, we provide a methodology to rank-order feature importance and show that for this dataset, error counts prove more predictive than scheduled maintenance might imply solely based on more traditional factors such as machine age or last replacement times.",David Noever,,https://arxiv.org/abs/1812.04446v1,https://arxiv.org/pdf/1812.04446v1,,"3 pages, 3 figures",,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1812.04446v1.pdf
1812.03315v2,2018-12-08T12:56:37Z,2022-08-30 10:49:46,A deep learning-based remaining useful life prediction approach for bearings,"In industrial applications, nearly half the failures of motors are caused by the degradation of rolling element bearings (REBs). Therefore, accurately estimating the remaining useful life (RUL) for REBs are of crucial importance to ensure the reliability and safety of mechanical systems. To tackle this challenge, model-based approaches are often limited by the complexity of mathematical modeling. Conventional data-driven approaches, on the other hand, require massive efforts to extract the degradation features and construct health index. In this paper, a novel online data-driven framework is proposed to exploit the adoption of deep convolutional neural networks (CNN) in predicting the RUL of bearings. More concretely, the raw vibrations of training bearings are first processed using the Hilbert-Huang transform (HHT) and a novel nonlinear degradation indicator is constructed as the label for learning. The CNN is then employed to identify the hidden pattern between the extracted degradation indicator and the vibration of training bearings, which makes it possible to estimate the degradation of the test bearings automatically. Finally, testing bearings' RULs are predicted by using a $ε$-support vector regression model. The superior performance of the proposed RUL estimation framework, compared with the state-of-the-art approaches, is demonstrated through the experimental results. The generality of the proposed CNN model is also validated by transferring to bearings undergoing different operating conditions.",Cheng Cheng|Guijun Ma|Yong Zhang|Mingyang Sun|Fei Teng|Han Ding|Ye Yuan,,https://arxiv.org/abs/1812.03315v2,https://arxiv.org/pdf/1812.03315v2,https://doi.org/10.1109/TMECH.2020.2971503,,,10.1109/TMECH.2020.2971503,cs.LG,cs.LG|eess.SP|stat.ML,https://arxiv.org/pdf/1812.03315v2.pdf
1811.10947v5,2018-11-27T12:54:46Z,2019-10-24 13:43:24,Reliable Semi-Supervised Learning when Labels are Missing at Random,"Semi-supervised learning methods are motivated by the availability of large datasets with unlabeled features in addition to labeled data. Unlabeled data is, however, not guaranteed to improve classification performance and has in fact been reported to impair the performance in certain cases. A fundamental source of error arises from restrictive assumptions about the unlabeled features, which result in unreliable classifiers that underestimate their prediction error probabilities. In this paper, we develop a semi-supervised learning approach that relaxes such assumptions and is capable of providing classifiers that reliably quantify the label uncertainty. The approach is applicable using any generative model with a supervised learning algorithm. We illustrate the approach using both handwritten digit and cloth classification data where the labels are missing at random.",Xiuming Liu|Dave Zachariah|Johan Wågberg|Thomas B. Schön,,https://arxiv.org/abs/1811.10947v5,https://arxiv.org/pdf/1811.10947v5,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1811.10947v5.pdf
1811.08633v1,2018-11-21T08:37:40Z,2018-11-21 08:37:40,Compensated Integrated Gradients to Reliably Interpret EEG Classification,"Integrated gradients are widely employed to evaluate the contribution of input features in classification models because it satisfies the axioms for attribution of prediction. This method, however, requires an appropriate baseline for reliable determination of the contributions. We propose a compensated integrated gradients method that does not require a baseline. In fact, the method compensates the attributions calculated by integrated gradients at an arbitrary baseline using Shapley sampling. We prove that the method retrieves reliable attributions if the processes of input features in a classifier are mutually independent, and they are identical like shared weights in convolutional neural networks. Using three electroencephalogram datasets, we experimentally demonstrate that the attributions of the proposed method are more reliable than those of the original integrated gradients, and its computational complexity is much lower than that of Shapley sampling.",Kazuki Tachikawa|Yuji Kawai|Jihoon Park|Minoru Asada,,https://arxiv.org/abs/1811.08633v1,https://arxiv.org/pdf/1811.08633v1,,Machine Learning for Health (ML4H) Workshop at NeurIPS 2018 arXiv:1811.07216,,,cs.LG,cs.LG|eess.SP|stat.ML,https://arxiv.org/pdf/1811.08633v1.pdf
1810.08985v3,2018-10-21T16:24:46Z,2019-06-17 00:41:38,Mechanisms for Integrated Feature Normalization and Remaining Useful Life Estimation Using LSTMs Applied to Hard-Disks,"With emerging smart communities, improving overall system availability is becoming a major concern. In order to improve the reliability of the components in a system we propose an inference model to predict Remaining Useful Life (RUL) of those components. In this paper we work with components of backend data servers such as hard disks, that are subject to degradation. A Deep Long-Short Term Memory (LSTM) Network is used as the backbone of this fast, data-driven decision framework and dynamically captures the pattern of the incoming data. In the article, we discuss the architecture of the neural network and describe the mechanisms to choose the various hyper-parameters. Further, we describe the challenges faced in extracting effective training sets from highly unorganized and class-imbalanced big data and establish methods for online predictions with extensive data pre-processing, feature extraction and validation through online simulation sets with unknown remaining useful lives of the hard disks. Our algorithm performs especially well in predicting RUL near the critical zone of a device approaching failure. With the proposed approach we are able to predict whether a disk is going to fail in next ten days with an average precision of 0.8435. We also show that the architecture trained on a particular model can be used to predict RUL for devices in different models from same manufacturer through transfer learning.",Sanchita Basak|Saptarshi Sengupta|Abhishek Dubey,,https://arxiv.org/abs/1810.08985v3,https://arxiv.org/pdf/1810.08985v3,,"9 pages, 13 figures, 2 tables",Proceedings of IEEE Smartcomp 2019,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1810.08985v3.pdf
1810.05644v2,2018-10-12T08:00:33Z,2018-12-10 10:11:11,Temporal Convolutional Memory Networks for Remaining Useful Life Estimation of Industrial Machinery,"Accurately estimating the remaining useful life (RUL) of industrial machinery is beneficial in many real-world applications. Estimation techniques have mainly utilized linear models or neural network based approaches with a focus on short term time dependencies. This paper, introduces a system model that incorporates temporal convolutions with both long term and short term time dependencies. The proposed network learns salient features and complex temporal variations in sensor values, and predicts the RUL. A data augmentation method is used for increased accuracy. The proposed method is compared with several state-of-the-art algorithms on publicly available datasets. It demonstrates promising results, with superior results for datasets obtained from complex environments.",Lahiru Jayasinghe|Tharaka Samarasinghe|Chau Yuen|Jenny Chen Ni Low|Shuzhi Sam Ge,,https://arxiv.org/abs/1810.05644v2,https://arxiv.org/pdf/1810.05644v2,,accepted to IEEE International Conference on Industrial Technology (ICIT2019),,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1810.05644v2.pdf
1810.05004v1,2018-10-03T19:12:03Z,2018-10-03 19:12:03,Hybrid integration of multilayer perceptrons and parametric models for reliability forecasting in the smart grid,"The reliable power system operation is a major goal for electric utilities, which requires the accurate reliability forecasting to minimize the duration of power interruptions. Since weather conditions are usually the leading causes for power interruptions in the smart grid, especially for its distribution networks, this paper comprehensively investigates the combined effect of various weather parameters on the reliability performance of distribution networks. Specially, a multilayer perceptron (MLP) based framework is proposed to forecast the daily numbers of sustained and momentary power interruptions in one distribution management area using time series of common weather data. First, the parametric regression models are implemented to analyze the relationship between the daily numbers of power interruptions and various common weather parameters, such as temperature, precipitation, air pressure, wind speed, and lightning. The selected weather parameters and corresponding parametric models are then integrated as inputs to formulate a MLP neural network model to predict the daily numbers of power interruptions. A modified extreme learning machine (ELM) based hierarchical learning algorithm is introduced for training the formulated model using realtime reliability data from an electric utility in Florida and common weather data from National Climatic Data Center (NCDC). In addition, the sensitivity analysis is implemented to determine the various impacts of different weather parameters on the daily numbers of power interruptions.",Longfei Wei|Arif I. Sarwat,,https://arxiv.org/abs/1810.05004v1,https://arxiv.org/pdf/1810.05004v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1810.05004v1.pdf
1809.10979v1,2018-09-28T12:08:51Z,2018-09-28 12:08:51,Cost-Sensitive Learning for Predictive Maintenance,"In predictive maintenance, model performance is usually assessed by means of precision, recall, and F1-score. However, employing the model with best performance, e.g. highest F1-score, does not necessarily result in minimum maintenance cost, but can instead lead to additional expenses. Thus, we propose to perform model selection based on the economic costs associated with the particular maintenance application. We show that cost-sensitive learning for predictive maintenance can result in significant cost reduction and fault tolerant policies, since it allows to incorporate various business constraints and requirements.",Stephan Spiegel|Fabian Mueller|Dorothea Weismann|John Bird,,https://arxiv.org/abs/1809.10979v1,https://arxiv.org/pdf/1809.10979v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1809.10979v1.pdf
1809.09060v1,2018-09-24T17:08:08Z,2018-09-24 17:08:08,Deep Confidence: A Computationally Efficient Framework for Calculating Reliable Errors for Deep Neural Networks,"Deep learning architectures have proved versatile in a number of drug discovery applications, including the modelling of in vitro compound activity. While controlling for prediction confidence is essential to increase the trust, interpretability and usefulness of virtual screening models in drug discovery, techniques to estimate the reliability of the predictions generated with deep learning networks remain largely underexplored. Here, we present Deep Confidence, a framework to compute valid and efficient confidence intervals for individual predictions using the deep learning technique Snapshot Ensembling and conformal prediction. Specifically, Deep Confidence generates an ensemble of deep neural networks by recording the network parameters throughout the local minima visited during the optimization phase of a single neural network. This approach serves to derive a set of base learners (i.e., snapshots) with comparable predictive power on average, that will however generate slightly different predictions for a given instance. The variability across base learners and the validation residuals are in turn harnessed to compute confidence intervals using the conformal prediction framework. Using a set of 24 diverse IC50 data sets from ChEMBL 23, we show that Snapshot Ensembles perform on par with Random Forest (RF) and ensembles of independently trained deep neural networks. In addition, we find that the confidence regions predicted using the Deep Confidence framework span a narrower set of values. Overall, Deep Confidence represents a highly versatile error prediction framework that can be applied to any deep learning-based application at no extra computational cost.",Isidro Cortes-Ciriano|Andreas Bender,,https://arxiv.org/abs/1809.09060v1,https://arxiv.org/pdf/1809.09060v1,https://doi.org/10.1021/acs.jcim.8b00542,,,10.1021/acs.jcim.8b00542,cs.LG,cs.LG|cs.AI|q-bio.QM|stat.ML,https://arxiv.org/pdf/1809.09060v1.pdf
1808.03253v1,2018-08-09T17:35:52Z,2018-08-09 17:35:52,Counterfactual Normalization: Proactively Addressing Dataset Shift and Improving Reliability Using Causal Mechanisms,"Predictive models can fail to generalize from training to deployment environments because of dataset shift, posing a threat to model reliability and the safety of downstream decisions made in practice. Instead of using samples from the target distribution to reactively correct dataset shift, we use graphical knowledge of the causal mechanisms relating variables in a prediction problem to proactively remove relationships that do not generalize across environments, even when these relationships may depend on unobserved variables (violations of the ""no unobserved confounders"" assumption). To accomplish this, we identify variables with unstable paths of statistical influence and remove them from the model. We also augment the causal graph with latent counterfactual variables that isolate unstable paths of statistical influence, allowing us to retain stable paths that would otherwise be removed. Our experiments demonstrate that models that remove vulnerable variables and use estimates of the latent variables transfer better, often outperforming in the target domain despite some accuracy loss in the training domain.",Adarsh Subbaswamy|Suchi Saria,,https://arxiv.org/abs/1808.03253v1,https://arxiv.org/pdf/1808.03253v1,,"Proceedings of the 34th Conference on Uncertainty in Artificial Intelligence (UAI), 2018. Revised from print version",,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1808.03253v1.pdf
1807.09616v1,2018-07-24T13:05:08Z,2018-07-24 13:05:08,Reliability analysis of general phased mission systems with a new survival signature,"It is often difficult for a phased mission system (PMS) to be highly reliable, because this entails achieving high reliability in every phase of operation. Consequently, reliability analysis of such systems is of critical importance. However, efficient and interpretable analysis of PMSs enabling general component lifetime distributions, arbitrary structures, and the possibility that components skip phases has been an open problem.
  In this paper, we show that the survival signature can be used for reliability analysis of PMSs with similar types of component in each phase, providing an alternative to the existing limited approaches in the literature. We then develop new methodology addressing the full range of challenges above. The new method retains the attractive survival signature property of separating the system structure from the component lifetime distributions, simplifying computation, insight into, and inference for system reliability.",Xianzhen Huang|Louis J. M. Aslett|Frank P. A. Coolen,,https://arxiv.org/abs/1807.09616v1,https://arxiv.org/pdf/1807.09616v1,https://doi.org/10.1016/j.ress.2019.04.019,21 pages,"Reliability Engineering & System Safety (2019), 189, pp.416-422",10.1016/j.ress.2019.04.019,stat.ME,stat.ME,https://arxiv.org/pdf/1807.09616v1.pdf
1807.02235v1,2018-07-06T03:22:29Z,2018-07-06 03:22:29,Towards more Reliable Transfer Learning,"Multi-source transfer learning has been proven effective when within-target labeled data is scarce. Previous work focuses primarily on exploiting domain similarities and assumes that source domains are richly or at least comparably labeled. While this strong assumption is never true in practice, this paper relaxes it and addresses challenges related to sources with diverse labeling volume and diverse reliability. The first challenge is combining domain similarity and source reliability by proposing a new transfer learning method that utilizes both source-target similarities and inter-source relationships. The second challenge involves pool-based active learning where the oracle is only available in source domains, resulting in an integrated active transfer learning framework that incorporates distribution matching and uncertainty sampling. Extensive experiments on synthetic and two real-world datasets clearly demonstrate the superiority of our proposed methods over several baselines including state-of-the-art transfer learning methods.",Zirui Wang|Jaime Carbonell,,https://arxiv.org/abs/1807.02235v1,https://arxiv.org/pdf/1807.02235v1,,ECML-PKDD 2018,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1807.02235v1.pdf
1807.00431v2,2018-07-02T01:57:38Z,2018-07-13 01:07:41,Confounding variables can degrade generalization performance of radiological deep learning models,"Early results in using convolutional neural networks (CNNs) on x-rays to diagnose disease have been promising, but it has not yet been shown that models trained on x-rays from one hospital or one group of hospitals will work equally well at different hospitals. Before these tools are used for computer-aided diagnosis in real-world clinical settings, we must verify their ability to generalize across a variety of hospital systems. A cross-sectional design was used to train and evaluate pneumonia screening CNNs on 158,323 chest x-rays from NIH (n=112,120 from 30,805 patients), Mount Sinai (42,396 from 12,904 patients), and Indiana (n=3,807 from 3,683 patients). In 3 / 5 natural comparisons, performance on chest x-rays from outside hospitals was significantly lower than on held-out x-rays from the original hospital systems. CNNs were able to detect where an x-ray was acquired (hospital system, hospital department) with extremely high accuracy and calibrate predictions accordingly. The performance of CNNs in diagnosing diseases on x-rays may reflect not only their ability to identify disease-specific imaging findings on x-rays, but also their ability to exploit confounding information. Estimates of CNN performance based on test data from hospital systems used for model training may overstate their likely real-world performance.",John R. Zech|Marcus A. Badgeley|Manway Liu|Anthony B. Costa|Joseph J. Titano|Eric K. Oermann,,https://arxiv.org/abs/1807.00431v2,https://arxiv.org/pdf/1807.00431v2,https://doi.org/10.1371/journal.pmed.1002683,,PLoS Med 15(11):e1002683 (2019),10.1371/journal.pmed.1002683,cs.CV,cs.CV|cs.LG|stat.ML,https://arxiv.org/pdf/1807.00431v2.pdf
1806.10179v1,2018-06-26T19:23:45Z,2018-06-26 19:23:45,Multi-Merge Budget Maintenance for Stochastic Gradient Descent SVM Training,Budgeted Stochastic Gradient Descent (BSGD) is a state-of-the-art technique for training large-scale kernelized support vector machines. The budget constraint is maintained incrementally by merging two points whenever the pre-defined budget is exceeded. The process of finding suitable merge partners is costly; it can account for up to 45% of the total training time. In this paper we investigate computationally more efficient schemes that merge more than two points at once. We obtain significant speed-ups without sacrificing accuracy.,Sahar Qaadan|Tobias Glasmachers,,https://arxiv.org/abs/1806.10179v1,https://arxiv.org/pdf/1806.10179v1,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1806.10179v1.pdf
1807.01269v2,2018-06-20T03:29:04Z,2019-05-01 18:06:57,Estimation of component reliability from superposed renewal processes with masked cause of failure by means of latent variables,"In a system, there are identical replaceable components working for a given task and a failed component is replaced by a functioning one in the corresponding position, which characterizes a repairable system. Assuming that a replaced component lifetime has the same lifetime distribution as the old one, a single component position can be represented by a renewal process and the multiple components positions for a single system form a superposed renewal process. When the interest consists in estimating the component lifetime distribution, there are a considerable amount of works that deal with estimation methods for this kind of problem. However, the information about the exact position of the replaced component is not available, that is, a masked cause of failure. In this work, we propose two methods, a Bayesian and a maximum likelihood function approaches, for estimating the failure time distribution of components in a repairable system with a masked cause of failure. As our proposed estimators consider latent variables, they yield better performance results compared to commonly used estimators from the literature. The proposed models are generic and straightforward for any probability distribution. Aside from point estimates, interval estimates are presented for both approaches. Using several simulations, the performances of the proposed methods are illustrated and their efficiency and applicability are shown based on the so-called cylinder problem.",Agatha Rodrigues|Pascal Kerschke|Carlos Alberto de B. Pereira|Heike Trautmann|Carolin Wagner|Bernd Hellingrath|Adriano Polpo,,https://arxiv.org/abs/1807.01269v2,https://arxiv.org/pdf/1807.01269v2,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1807.01269v2.pdf
1806.01551v3,2018-06-05T08:26:53Z,2019-11-25 03:50:19,Deep Mixed Effect Model using Gaussian Processes: A Personalized and Reliable Prediction for Healthcare,"We present a personalized and reliable prediction model for healthcare, which can provide individually tailored medical services such as diagnosis, disease treatment, and prevention. Our proposed framework targets at making personalized and reliable predictions from time-series data, such as Electronic Health Records (EHR), by modeling two complementary components: i) a shared component that captures global trend across diverse patients and ii) a patient-specific component that models idiosyncratic variability for each patient. To this end, we propose a composite model of a deep neural network to learn complex global trends from the large number of patients, and Gaussian Processes (GP) to probabilistically model individual time-series given relatively small number of visits per patient. We evaluate our model on diverse and heterogeneous tasks from EHR datasets and show practical advantages over standard time-series deep models such as pure Recurrent Neural Network (RNN).",Ingyo Chung|Saehoon Kim|Juho Lee|Kwang Joon Kim|Sung Ju Hwang|Eunho Yang,,https://arxiv.org/abs/1806.01551v3,https://arxiv.org/pdf/1806.01551v3,,AAAI 2020,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1806.01551v3.pdf
1806.00048v2,2018-05-31T18:50:58Z,2018-10-31 18:53:10,Cutting the Double Loop: Theory and Algorithms for Reliability-Based Design Optimization with Statistical Uncertainty,"Statistical uncertainties complicate engineering design -- confounding regulated design approaches, and degrading the performance of reliability efforts. The simplest means to tackle this uncertainty is double loop simulation; a nested Monte Carlo method that, for practical problems, is intractable. In this work, we introduce a flexible, general approximation technique that obviates the double loop. This approximation is constructed in the context of a novel theory of reliability design under statistical uncertainty: We introduce metrics for measuring the efficacy of RBDO strategies (effective margin and effective reliability), minimal conditions for controlling uncertain reliability (precision margin), and stricter conditions that guarantee the desired reliability at a designed confidence level. We provide a number of examples with open-source code to demonstrate our approaches in a reproducible fashion.",Zachary del Rosario|Richard W. Fenrich|Gianluca Iaccarino,,https://arxiv.org/abs/1806.00048v2,https://arxiv.org/pdf/1806.00048v2,,"25 pages, 16 figures, linked GitHub repo",,,stat.ME,stat.ME,https://arxiv.org/pdf/1806.00048v2.pdf
1805.10627v3,2018-05-27T13:51:48Z,2018-12-13 16:58:22,Reliability and Learnability of Human Bandit Feedback for Sequence-to-Sequence Reinforcement Learning,"We present a study on reinforcement learning (RL) from human bandit feedback for sequence-to-sequence learning, exemplified by the task of bandit neural machine translation (NMT). We investigate the reliability of human bandit feedback, and analyze the influence of reliability on the learnability of a reward estimator, and the effect of the quality of reward estimates on the overall RL task. Our analysis of cardinal (5-point ratings) and ordinal (pairwise preferences) feedback shows that their intra- and inter-annotator $α$-agreement is comparable. Best reliability is obtained for standardized cardinal feedback, and cardinal feedback is also easiest to learn and generalize from. Finally, improvements of over 1 BLEU can be obtained by integrating a regression-based reward estimator trained on cardinal feedback for 800 translations into RL for NMT. This shows that RL is possible even from small amounts of fairly reliable human feedback, pointing to a great potential for applications at larger scale.",Julia Kreutzer|Joshua Uyheng|Stefan Riezler,,https://arxiv.org/abs/1805.10627v3,https://arxiv.org/pdf/1805.10627v3,,ACL 2018,,,cs.CL,cs.CL|stat.ML,https://arxiv.org/pdf/1805.10627v3.pdf
1805.10540v1,2018-05-26T21:15:20Z,2018-05-26 21:15:20,Reliability Estimation in Coherent Systems,"Usually, methods evaluating system reliability require engineers to quantify the reliability of each of the system components. For series and parallel systems, there are some options to handle the estimation of each component's reliability. We will treat the reliability estimation of complex problems of two classes of coherent systems: series-parallel, and parallel-series. In both of the cases, the component reliabilities may be unknown. We will present estimators for reliability functions at all levels of the system (component and system reliabilities). Nonparametric Bayesian estimators of all sub-distribution and distribution functions are derived, and a Dirichlet multivariate process as a prior distribution is presented. Parametric estimator of the component's reliability based on Weibull model is presented for any kind of system. Also, some ideas in systems with masked data are discussed.",Agatha Rodrigues|Carlos Alberto Pereira|Adriano Polpo,,https://arxiv.org/abs/1805.10540v1,https://arxiv.org/pdf/1805.10540v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1805.10540v1.pdf
1805.09653v1,2018-05-24T13:17:08Z,2018-05-24 13:17:08,Uncertainty-Aware Attention for Reliable Interpretation and Prediction,"Attention mechanism is effective in both focusing the deep learning models on relevant features and interpreting them. However, attentions may be unreliable since the networks that generate them are often trained in a weakly-supervised manner. To overcome this limitation, we introduce the notion of input-dependent uncertainty to the attention mechanism, such that it generates attention for each feature with varying degrees of noise based on the given input, to learn larger variance on instances it is uncertain about. We learn this Uncertainty-aware Attention (UA) mechanism using variational inference, and validate it on various risk prediction tasks from electronic health records on which our model significantly outperforms existing attention models. The analysis of the learned attentions shows that our model generates attentions that comply with clinicians' interpretation, and provide richer interpretation via learned variance. Further evaluation of both the accuracy of the uncertainty calibration and the prediction performance with ""I don't know"" decision show that UA yields networks with high reliability as well.",Jay Heo|Hae Beom Lee|Saehoon Kim|Juho Lee|Kwang Joon Kim|Eunho Yang|Sung Ju Hwang,,https://arxiv.org/abs/1805.09653v1,https://arxiv.org/pdf/1805.09653v1,,,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/1805.09653v1.pdf
1805.07569v2,2018-05-19T11:09:27Z,2018-11-16 09:09:15,Reliable counting of weakly labeled concepts by a single spiking neuron model,"Making an informed, correct and quick decision can be life-saving. It's crucial for animals during an escape behaviour or for autonomous cars during driving. The decision can be complex and may involve an assessment of the amount of threats present and the nature of each threat. Thus, we should expect early sensory processing to supply classification information fast and accurately, even before relying the information to higher brain areas or more complex system components downstream. Today, advanced convolutional artificial neural networks can successfully solve visual detection and classification tasks and are commonly used to build complex decision making systems. However, in order to perform well on these tasks they require increasingly complex, ""very deep"" model structure, which is costly in inference run-time, energy consumption and number of training samples, only trainable on cloud-computing clusters. A single spiking neuron has been shown to be able to solve recognition tasks for homogeneous Poisson input statistics, a commonly used model for spiking activity in the neocortex. When modeled as leaky integrate and fire with gradient decent learning algorithm it was shown to posses a variety of complex computational capabilities. Here we improve its implementation. We also account for more natural stimulus generated inputs that deviate from this homogeneous Poisson spiking. The improved gradient-based local learning rule allows for significantly better and stable generalization. We also show that with its improved capabilities it can count weakly labeled concepts by applying our model to a problem of multiple instance learning (MIL) with counting where labels are only available for collections of concepts. In this counting MNIST task the neuron exploits the improved implementation and outperforms conventional ConvNet architecture under similar condtions.",Hannes Rapp|Martin Paul Nawrot|Merav Stern,,https://arxiv.org/abs/1805.07569v2,https://arxiv.org/pdf/1805.07569v2,,,,,cs.NE,cs.NE|cs.LG|q-bio.NC|stat.ML,https://arxiv.org/pdf/1805.07569v2.pdf
1805.09253v1,2018-05-11T11:31:42Z,2018-05-11 11:31:42,Federated Learning for Ultra-Reliable Low-Latency V2V Communications,"In this paper, a novel joint transmit power and resource allocation approach for enabling ultra-reliable low-latency communication (URLLC) in vehicular networks is proposed. The objective is to minimize the network-wide power consumption of vehicular users (VUEs) while ensuring high reliability in terms of probabilistic queuing delays. In particular, a reliability measure is defined to characterize extreme events (i.e., when vehicles' queue lengths exceed a predefined threshold with non-negligible probability) using extreme value theory (EVT). Leveraging principles from federated learning (FL), the distribution of these extreme events corresponding to the tail distribution of queues is estimated by VUEs in a decentralized manner. Finally, Lyapunov optimization is used to find the joint transmit power and resource allocation policies for each VUE in a distributed manner. The proposed solution is validated via extensive simulations using a Manhattan mobility model. It is shown that FL enables the proposed distributed method to estimate the tail distribution of queues with an accuracy that is very close to a centralized solution with up to 79\% reductions in the amount of data that need to be exchanged. Furthermore, the proposed method yields up to 60\% reductions of VUEs with large queue lengths, without an additional power consumption, compared to an average queue-based baseline. Compared to systems with fixed power consumption and focusing on queue stability while minimizing average power consumption, the reduction in extreme events of the proposed method is about two orders of magnitude.",Sumudu Samarakoon|Mehdi Bennis|Walid Saad|Merouane Debbah,,https://arxiv.org/abs/1805.09253v1,https://arxiv.org/pdf/1805.09253v1,,"7 pages, 5 figures, 2 tables, 1 algorithm, conference submission",,,cs.NI,cs.NI|cs.LG|stat.ML,https://arxiv.org/pdf/1805.09253v1.pdf
1805.00726v1,2018-05-02T10:57:04Z,2018-05-02 10:57:04,Emulation of utility functions over a set of permutations: sequencing reliability growth tasks,"We consider Bayesian design of experiments problems in which we maximise the prior expectation of a utility function over a set of permutations, for example when sequencing a number of tasks to perform. When the number of tasks is large and the expected utility is expensive to compute, it may be unreasonable or infeasible to evaluate the expected utility of all permutations. We propose an approach to emulate the expected utility using a surrogate function based on a parametric probabilistic model for permutations. The surrogate function is fitted by maximising the correlation with the expected utility over a set of training points. We propose a suitable transformation of the expected utility to improve the fit. We provide results linking the correlation between the two functions and the number of expected utility evaluations to undertake. The approach is applied to the sequencing of reliability growth tasks in the development of hardware systems, in which there is a large number of potential tasks to perform and engineers are interested in meeting a reliability target subject to minimising costs and time. An illustrative example shows how the approach can be used and a simulation study demonstrates the performance of the approach more generally.",Kevin J Wilson|Daniel A Henderson|John Quigley,,https://arxiv.org/abs/1805.00726v1,https://arxiv.org/pdf/1805.00726v1,https://doi.org/10.1080/00401706.2017.1377637,Article accepted at Technometrics. The official journal version is given here: https://doi.org/10.1080/00401706.2017.1377637,,10.1080/00401706.2017.1377637,stat.ME,stat.ME,https://arxiv.org/pdf/1805.00726v1.pdf
1804.10820v1,2018-04-28T15:25:34Z,2018-04-28 15:25:34,"On a bivariate Birnbaum-Saunders distribution parameterized by its means: features, reliability analysis and application","Birnbaum-Saunders models have been widely used to model positively skewed data. In this paper, we introduce a bivariate Birnbaum-Saunders distribution which has the means as parameters. We present some properties of the univariate and bivariate Birnbaum-Saunders models. We discuss the maximum likelihood and modified moment estimation of the model parameters and associated inference. A simulation study is conducted to evaluate the performance of the maximum likelihood and modified moment estimators. The probability coverages of confidence intervals are also discussed. Finally, a real-world data analysis is carried out for illustrating the proposed model.",Helton Saulo|Jeremias Leão|Roberto Vila|Victor Leiva|Vera Tomazella,,https://arxiv.org/abs/1804.10820v1,https://arxiv.org/pdf/1804.10820v1,,"29 pages, 4 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/1804.10820v1.pdf
1804.10527v1,2018-04-27T14:15:42Z,2018-04-27 14:15:42,Detecting and modeling worst-case dependence structures between random inputs of computational reliability models,"Uncertain information on input parameters of reliability models is usually modeled by considering these parameters as random, and described by marginal distributions and a dependence structure of these variables. In numerous real-world applications, while information is mainly provided by marginal distributions, typically from samples , little is really known on the dependence structure itself. Faced with this problem of incomplete or missing information, risk studies are often conducted by considering independence of input variables, at the risk of including irrelevant situations. This approach is especially used when reliability functions are considered as black-box computational models. Such analyses remain weakened in absence of in-depth model exploration, at the possible price of a strong risk misestimation. Considering the frequent case where the reliability output is a quantile, this article provides a methodology to improve risk assessment, by exploring a set of pessimistic dependencies using a copula-based strategy. In dimension greater than two, a greedy algorithm is provided to build input regular vine copulas reaching a minimum quantile to which a reliability admissible limit value can be compared, by selecting pairwise components of sensitive influence on the result. The strategy is tested over toy models and a real industrial case-study. The results highlight that current approaches can provide non-conservative results, and that a nontrivial dependence structure can be exhibited to define a worst-case scenario.",Nazih Benoumechiara|Bertrand Michel|Philippe Saint-Pierre|Nicolas Bousquet,"LPSM, EDF R and D PRISME|LMJL|IMT|LPSM",https://arxiv.org/abs/1804.10527v1,https://arxiv.org/pdf/1804.10527v1,,,,,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/1804.10527v1.pdf
1804.05497v1,2018-04-16T03:55:42Z,2018-04-16 03:55:42,Deep Learning on Key Performance Indicators for Predictive Maintenance in SAP HANA,"With a new era of cloud and big data, Database Management Systems (DBMSs) have become more crucial in numerous enterprise business applications in all the industries. Accordingly, the importance of their proactive and preventive maintenance has also increased. However, detecting problems by predefined rules or stochastic modeling has limitations, particularly when analyzing the data on high-dimensional Key Performance Indicators (KPIs) from a DBMS. In recent years, Deep Learning (DL) has opened new opportunities for this complex analysis. In this paper, we present two complementary DL approaches to detect anomalies in SAP HANA. A temporal learning approach is used to detect abnormal patterns based on unlabeled historical data, whereas a spatial learning approach is used to classify known anomalies based on labeled data. We implement a system in SAP HANA integrated with Google TensorFlow. The experimental results with real-world data confirm the effectiveness of the system and models.",Jaekoo Lee|Byunghan Lee|Jongyoon Song|Jaesik Yoon|Yongsik Lee|Donghun Lee|Sungroh Yoon,,https://arxiv.org/abs/1804.05497v1,https://arxiv.org/pdf/1804.05497v1,,This version withdrawn by arXiv administrators because the author did not have the right to agree to our license at the time of submission,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1804.05497v1.pdf
1803.10791v1,2018-03-28T18:13:54Z,2018-03-28 18:13:54,A systematic approach to improving the reliability and scale of evidence from health care data,"Concerns over reproducibility in science extend to research using existing healthcare data; many observational studies investigating the same topic produce conflicting results, even when using the same data. To address this problem, we propose a paradigm shift. The current paradigm centers on generating one estimate at a time using a unique study design with unknown reliability and publishing (or not) one estimate at a time. The new paradigm advocates for high-throughput observational studies using consistent and standardized methods, allowing evaluation, calibration, and unbiased dissemination to generate a more reliable and complete evidence base. We demonstrate this new paradigm by comparing all depression treatments for a set of outcomes, producing 17,718 hazard ratios, each using methodology on par with state-of-the-art studies. We furthermore include control hypotheses to evaluate and calibrate our evidence generation process. Results show good transitivity and consistency between databases, and agree with four out of the five findings from clinical trials. The distribution of effect size estimates reported in literature reveals an absence of small or null effects, with a sharp cutoff at p = 0.05. No such phenomena were observed in our results, suggesting more complete and more reliable evidence.",Martijn J. Schuemie|Patrick B. Ryan|George Hripcsak|David Madigan|Marc A. Suchard,,https://arxiv.org/abs/1803.10791v1,https://arxiv.org/pdf/1803.10791v1,,"24 pages, 6 figures, 2 tables, 28 pages supplementary materials",,,stat.AP,stat.AP,https://arxiv.org/pdf/1803.10791v1.pdf
1803.06206v1,2018-03-16T13:08:25Z,2018-03-16 13:08:25,Big Data and Reliability Applications: The Complexity Dimension,"Big data features not only large volumes of data but also data with complicated structures. Complexity imposes unique challenges in big data analytics. Meeker and Hong (2014, Quality Engineering, pp. 102-116) provided an extensive discussion of the opportunities and challenges in big data and reliability, and described engineering systems that can generate big data that can be used in reliability analysis. Meeker and Hong (2014) focused on large scale system operating and environment data (i.e., high-frequency multivariate time series data), and provided examples on how to link such data as covariates to traditional reliability responses such as time to failure, time to recurrence of events, and degradation measurements. This paper intends to extend that discussion by focusing on how to use data with complicated structures to do reliability analysis. Such data types include high-dimensional sensor data, functional curve data, and image streams. We first provide a review of recent development in those directions, and then we provide a discussion on how analytical methods can be developed to tackle the challenging aspects that arise from the complexity feature of big data in reliability applications. The use of modern statistical methods such as variable selection, functional data analysis, scalar-on-image regression, spatio-temporal data models, and machine learning techniques will also be discussed.",Yili Hong|Man Zhang|William Q. Meeker,,https://arxiv.org/abs/1803.06206v1,https://arxiv.org/pdf/1803.06206v1,https://doi.org/10.1080/00224065.2018.1438007,"28 pages, 7 figures","Journal of Quality Technology, 2018",10.1080/00224065.2018.1438007,stat.AP,stat.AP,https://arxiv.org/pdf/1803.06206v1.pdf
1804.04586v1,2018-03-16T12:45:35Z,2018-03-16 12:45:35,Reliability Analysis of Polymeric Materials,"Polymeric materials are widely used in many applications and are especially useful when combined with other polymers to make polymer composites. The appealing features of these materials come from their having comparable levels of strength and endurance to what one would find in metal alloys while being more lightweight and economical. However, these materials are still susceptible to degradation over time and so it is of great importance to manufacturers to assess their product's lifetime. Because these materials are meant to last over a span of several years or even decades, accelerated testing is often the method of choice in assessing product lifetimes in a more feasible time frame. In this article, a brief introduction is given to the methods of accelerated testing and analysis used with polymer materials. Special attention is given to degradation testing and modeling due to the growing popularity of these techniques along with a brief discussion of fatigue testing. References are provided for further reading in each of these areas.",Caleb King|Zhibing Xu|I-Chen Lee|Yili Hong,,https://arxiv.org/abs/1804.04586v1,https://arxiv.org/pdf/1804.04586v1,,"14 pages, 3 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/1804.04586v1.pdf
1803.04475v1,2018-03-12T19:24:37Z,2018-03-12 19:24:37,Accuracy-Reliability Cost Function for Empirical Variance Estimation,"In this paper we focus on the problem of assigning uncertainties to single-point predictions. We introduce a cost function that encodes the trade-off between accuracy and reliability in probabilistic forecast. We derive analytic formula for the case of forecasts of continuous scalar variables expressed in terms of Gaussian distributions. The Accuracy-Reliability cost function can be used to empirically estimate the variance in heteroskedastic regression problems (input dependent noise), by solving a two-objective optimization problem. The simple philosophy behind this strategy is that predictions based on the estimated variances should be both accurate and reliable (i.e. statistical consistent with observations). We show several examples with synthetic data, where the underlying hidden noise function can be accurately recovered, both in one and multi-dimensional problems. The practical implementation of the method has been done using a Neural Network and, in the one-dimensional case, with a simple polynomial fit.",Enrico Camporeale,,https://arxiv.org/abs/1803.04475v1,https://arxiv.org/pdf/1803.04475v1,,under review for ICML 2018,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1803.04475v1.pdf
1803.00464v1,2018-03-01T15:46:04Z,2018-03-01 15:46:04,Mortality data reliability in an internal model,"In this paper, we discuss the impact of some mortality data anomalies on an internal model capturing longevity risk in the Solvency 2 framework. In particular, we are concerned with abnormal cohort effects such as those for generations 1919 and 1920, for which the period tables provided by the Human Mortality Database show particularly low and high mortality rates respectively. To provide corrected tables for the three countries of interest here (France, Italy and West Germany), we use the approach developed by Boumezoued (2016) for countries for which the method applies (France and Italy), and provide an extension of the method for West Germany as monthly fertility histories are not sufficient to cover the generations of interest. These mortality tables are crucial inputs to stochastic mortality models forecasting future scenarios, from which the extreme 0,5% longevity improvement can be extracted, allowing for the calculation of the Solvency Capital Requirement (SCR). More precisely, to assess the impact of such anomalies in the Solvency II framework, we use a simplified internal model based on three usual stochastic models to project mortality rates in the future combined with a closure table methodology for older ages. Correcting this bias obviously improves the data quality of the mortality inputs, which is of paramount importance today, and slightly decreases the capital requirement. Overall, the longevity risk assessment remains stable, as well as the selection of the stochastic mortality model. As a collateral gain of this data quality improvement, the more regular estimated parameters allow for new insights and a refined assessment regarding longevity risk.",Fabrice Balland|Alexandre Boumezoued|Laurent Devineau|Marine Habart|Tom Popa,,https://arxiv.org/abs/1803.00464v1,https://arxiv.org/pdf/1803.00464v1,https://doi.org/10.1017/S1748499520000081,,Ann. actuar. sci. 14 (2020) 420-444,10.1017/S1748499520000081,q-fin.RM,q-fin.RM|math.PR|stat.AP,https://arxiv.org/pdf/1803.00464v1.pdf
1802.08286v1,2018-02-05T19:22:23Z,2018-02-05 19:22:23,Reliability and Market Price of Energy in the Presence of Intermittent and Non-Dispatchable Renewable Energies,"The intermittent nature of the renewable energies increases the operation costs of conventional generators. As the share of energy supplied by renewable sources increases, these costs also increase. In this paper, we quantify these costs by developing a market clearing price of energy in the presence of renewable energy and congestion constraints. We consider an electricity market where generators propose their asking price per unit of energy to an independent system operator (ISO). The ISO solve an optimization problem to dispatch energy from each generator to minimize the total cost of energy purchased on behalf of the consumers.
  To ensure that the generators are able to meet the load within a desired confidence level, we incorporate the notion of load variance using the Conditional Value-at-Risk (CVAR) measure in an electricity market and we derive the amount of committed power and market clearing price of energy as a function of CVAR. It is shown that a higher penetration of renewable energies may increase the committed power, market clearing price of energy and consumer cost of energy due to renewable generation uncertainties. We also obtain an upper-bound on the amount that congestion constraints can affect the committed power. We present descriptive simulations to illustrate the impact of renewable energy penetration and reliability levels on committed power by the non-renewable generators, difference between the dispatched and committed power, market price of energy and profit of renewable and non-renewable generators.",Ashkan Zeinalzadeh|Donya Ghavidel|Vijay Gupta,,https://arxiv.org/abs/1802.08286v1,https://arxiv.org/pdf/1802.08286v1,,11 pages,,,eess.SY,eess.SY|stat.AP,https://arxiv.org/pdf/1802.08286v1.pdf
1801.08881v5,2018-01-26T16:12:07Z,2019-01-20 21:15:39,Correlated Components Analysis - Extracting Reliable Dimensions in Multivariate Data,"How does one find dimensions in multivariate data that are reliably expressed across repetitions? For example, in a brain imaging study one may want to identify combinations of neural signals that are reliably expressed across multiple trials or subjects. For a behavioral assessment with multiple ratings, one may want to identify an aggregate score that is reliably reproduced across raters. Correlated Components Analysis (CorrCA) addresses this problem by identifying components that are maximally correlated between repetitions (e.g. trials, subjects, raters). Here we formalize this as the maximization of the ratio of between-repetition to within-repetition covariance. We show that this criterion maximizes repeat-reliability, defined as mean over variance across repeats, and that it leads to CorrCA or to multi-set Canonical Correlation Analysis, depending on the constraints. Surprisingly, we also find that CorrCA is equivalent to Linear Discriminant Analysis for zero-mean signals, which provides an unexpected link between classic concepts of multivariate analysis. We present an exact parametric test of statistical significance based on the F-statistic for normally distributed independent samples, and present and validate shuffle statistics for the case of dependent samples. Regularization and extension to non-linear mappings using kernels are also presented. The algorithms are demonstrated on a series of data analysis applications, and we provide all code and data required to reproduce the results.",Lucas C. Parra|Stefan Haufe|Jacek P. Dmochowski,,https://arxiv.org/abs/1801.08881v5,https://arxiv.org/pdf/1801.08881v5,https://doi.org/10.51628/001c.7125,,"Neurons, Behavior, Data Analysis and Theory (NBDT), Vol. 2, Issue 1, 2019",10.51628/001c.7125,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1801.08881v5.pdf
1801.01748v1,2018-01-05T13:09:49Z,2018-01-05 13:09:49,Transformation of arbitrary distributions to the normal distribution with application to EEG test-retest reliability,"Many variables in the social, physical, and biosciences, including neuroscience, are non-normally distributed. To improve the statistical properties of such data, or to allow parametric testing, logarithmic or logit transformations are often used. Box-Cox transformations or ad hoc methods are sometimes used for parameters for which no transformation is known to approximate normality. However, these methods do not always give good agreement with the Gaussian. A transformation is discussed that maps probability distributions as closely as possible to the normal distribution, with exact agreement for continuous distributions. To illustrate, the transformation is applied to a theoretical distribution, and to quantitative electroencephalographic (qEEG) measures from repeat recordings of 32 subjects which are highly non-normal. Agreement with the Gaussian was better than using logarithmic, logit, or Box-Cox transformations. Since normal data have previously been shown to have better test-retest reliability than non-normal data under fairly general circumstances, the implications of our transformation for the test-retest reliability of parameters were investigated. Reliability was shown to improve with the transformation, where the improvement was comparable to that using Box-Cox. An advantage of the general transformation is that it does not require laborious optimization over a range of parameters or a case-specific choice of form.",Sacha Jennifer van Albada|Peter A. Robinson,,https://arxiv.org/abs/1801.01748v1,https://arxiv.org/pdf/1801.01748v1,https://doi.org/10.1016/j.jneumeth.2006.11.004,,J. Neurosci. Methods 161: 205-211 (2007),10.1016/j.jneumeth.2006.11.004,stat.ME,stat.ME|q-bio.NC,https://arxiv.org/pdf/1801.01748v1.pdf
1712.04248v2,2017-12-12T11:36:26Z,2018-02-16 14:40:42,Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at https://github.com/bethgelab/foolbox .",Wieland Brendel|Jonas Rauber|Matthias Bethge,,https://arxiv.org/abs/1712.04248v2,https://arxiv.org/pdf/1712.04248v2,,Published as a conference paper at the Sixth International Conference on Learning Representations (ICLR 2018) https://openreview.net/forum?id=SyZI0GWCZ,,,stat.ML,stat.ML|cs.CR|cs.CV|cs.LG|cs.NE,https://arxiv.org/pdf/1712.04248v2.pdf
1712.04135v1,2017-12-12T05:12:44Z,2017-12-12 05:12:44,Deep Learning for Reliable Mobile Edge Analytics in Intelligent Transportation Systems,"Intelligent transportation systems (ITSs) will be a major component of tomorrow's smart cities. However, realizing the true potential of ITSs requires ultra-low latency and reliable data analytics solutions that can combine, in real-time, a heterogeneous mix of data stemming from the ITS network and its environment. Such data analytics capabilities cannot be provided by conventional cloud-centric data processing techniques whose communication and computing latency can be high. Instead, edge-centric solutions that are tailored to the unique ITS environment must be developed. In this paper, an edge analytics architecture for ITSs is introduced in which data is processed at the vehicle or roadside smart sensor level in order to overcome the ITS latency and reliability challenges. With a higher capability of passengers' mobile devices and intra-vehicle processors, such a distributed edge computing architecture can leverage deep learning techniques for reliable mobile sensing in ITSs. In this context, the ITS mobile edge analytics challenges pertaining to heterogeneous data, autonomous control, vehicular platoon control, and cyber-physical security are investigated. Then, different deep learning solutions for such challenges are proposed. The proposed deep learning solutions will enable ITS edge analytics by endowing the ITS devices with powerful computer vision and signal processing functions. Preliminary results show that the proposed edge analytics architecture, coupled with the power of deep learning algorithms, can provide a reliable, secure, and truly smart transportation environment.",Aidin Ferdowsi|Ursula Challita|Walid Saad,,https://arxiv.org/abs/1712.04135v1,https://arxiv.org/pdf/1712.04135v1,,5 figures,,,cs.IT,cs.IT|stat.ML,https://arxiv.org/pdf/1712.04135v1.pdf
1712.03304v1,2017-12-08T22:51:02Z,2017-12-08 22:51:02,Reliability-centered maintenance: analyzing failure in harvest sugarcane machine using some generalizations of the Weibull distribution,"In this study we considered five generalizations of the standard Weibull distribution to describe the lifetime of two important components of harvest sugarcane machines. The harvesters considered in the analysis does the harvest of an average of 20 tons of sugarcane per hour and their malfunction may lead to major losses, therefore, an effective maintenance approach is of main interesting for cost savings. For the considered distributions, the mathematical background is presented. Maximum likelihood is used for parameter estimation. Further, different discrimination procedures were used to obtain the best fit for each component. At the end, we propose a maintenance scheduling for the components of the harvesters using predictive analysis.",Pedro Luiz Ramos|Diego Nascimento|Camila Cocolo|Márcio José Nicola|Carlos Alonso|Luiz Gustavo Ribeiro|André Ennes|Francisco Louzada,,https://arxiv.org/abs/1712.03304v1,https://arxiv.org/pdf/1712.03304v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1712.03304v1.pdf
1712.00131v1,2017-11-30T23:59:17Z,2017-11-30 23:59:17,Benford's law first significant digit and distribution distances for testing the reliability of financial reports in developing countries,"We discuss a common suspicion about reported financial data, in 10 industrial sectors of the 6 so called ""main developing countries"" over the time interval [2000-2014]. These data are examined through Benford's law first significant digit and through distribution distances tests. It is shown that several visually anomalous data have to be a priori removed. Thereafter, the distributions much better follow the first digit significant law, indicating the usefulness of a Benford's law test from the research starting line. The same holds true for distance tests. A few outliers are pointed out.",Jing Shi|Marcel Ausloos|Tingting Zhu,,https://arxiv.org/abs/1712.00131v1,https://arxiv.org/pdf/1712.00131v1,https://doi.org/10.1016/j.physa.2017.11.017,"22 pages, 34 references, 4 figures, 7 tables; to be published in Physica A",,10.1016/j.physa.2017.11.017,q-fin.ST,q-fin.ST|cond-mat.stat-mech|stat.AP,https://arxiv.org/pdf/1712.00131v1.pdf
1711.09761v1,2017-11-23T14:06:09Z,2017-11-23 14:06:09,Mitigating Blackout Risk via Maintenance : Inference from Simulation Data,"Whereas maintenance has been recognized as an important and effective means for risk management in power systems, it turns out to be intractable if cascading blackout risk is considered due to the extremely high computational complexity. In this paper, based on the inference from the blackout simulation data, we propose a methodology to efficiently identify the most influential component(s) for mitigating cascading blackout risk in a large power system. To this end, we first establish an analytic relationship between maintenance strategies and blackout risk estimation by inferring from the data of cascading outage simulations. Then we formulate the component maintenance decision-making problem as a nonlinear 0-1 programming. Afterwards, we quantify the credibility of blackout risk estimation, leading to an adaptive method to determine the least required number of simulations, which servers as a crucial parameter of the optimization model. Finally, we devise two heuristic algorithms to find approximate optimal solutions to the model with very high efficiency. Numerical experiments well manifest the efficacy and high efficiency of our methodology.",Jinpeng Guo|Feng Liu|Xuemin Zhang|Yunhe Hou|Shengwei Mei,,https://arxiv.org/abs/1711.09761v1,https://arxiv.org/pdf/1711.09761v1,,,,,math.OC,math.OC|stat.AP,https://arxiv.org/pdf/1711.09761v1.pdf
1711.05150v1,2017-11-14T15:41:10Z,2017-11-14 15:41:10,Fast and reliable inference algorithm for hierarchical stochastic block models,"Network clustering reveals the organization of a network or corresponding complex system with elements represented as vertices and interactions as edges in a (directed, weighted) graph. Although the notion of clustering can be somewhat loose, network clusters or groups are generally considered as nodes with enriched interactions and edges sharing common patterns. Statistical inference often treats groups as latent variables, with observed networks generated from latent group structure, termed a stochastic block model. Regardless of the definitions, statistical inference can be either translated to modularity maximization, which is provably an NP-complete problem.
  Here we present scalable and reliable algorithms that recover hierarchical stochastic block models fast and accurately. Our algorithm scales almost linearly in number of edges, and inferred models were more accurate that other scalable methods.",Yongjin Park|Joel S. Bader,,https://arxiv.org/abs/1711.05150v1,https://arxiv.org/pdf/1711.05150v1,,,,,stat.ML,stat.ML,https://arxiv.org/pdf/1711.05150v1.pdf
1711.00867v1,2017-11-02T18:01:30Z,2017-11-02 18:01:30,The (Un)reliability of saliency methods,"Saliency methods aim to explain the predictions of deep neural networks. These methods lack reliability when the explanation is sensitive to factors that do not contribute to the model prediction. We use a simple and common pre-processing step ---adding a constant shift to the input data--- to show that a transformation with no effect on the model can cause numerous methods to incorrectly attribute. In order to guarantee reliability, we posit that methods should fulfill input invariance, the requirement that a saliency method mirror the sensitivity of the model with respect to transformations of the input. We show, through several examples, that saliency methods that do not satisfy input invariance result in misleading attribution.",Pieter-Jan Kindermans|Sara Hooker|Julius Adebayo|Maximilian Alber|Kristof T. Schütt|Sven Dähne|Dumitru Erhan|Been Kim,,https://arxiv.org/abs/1711.00867v1,https://arxiv.org/pdf/1711.00867v1,,,,,stat.ML,stat.ML|cs.LG,https://arxiv.org/pdf/1711.00867v1.pdf
1711.00303v1,2017-11-01T12:05:46Z,2017-11-01 12:05:46,Assessing the reliability polynomial based on percolation theory,"In this paper, we study the robustness of network topologies. We use the concept of percolation as measuring tool to assess the reliability polynomial of those systems which can be modeled as a general inhomogeneous random graph as well as scale-free random graph.",Farkhondeh A. Sajadi,,https://arxiv.org/abs/1711.00303v1,https://arxiv.org/pdf/1711.00303v1,,,,,math.ST,math.ST|physics.soc-ph|stat.AP,https://arxiv.org/pdf/1711.00303v1.pdf
1710.02219v1,2017-10-05T21:08:15Z,2017-10-05 21:08:15,The reliability of a nutritional meta-analysis study,"Background: Many researchers have studied the relationship between diet and health. There are papers showing an association between the consumption of sugar-sweetened beverages and Type 2 diabetes. Many meta-analyses use individual studies that do not adjust for multiple testing or multiple modeling and thus provide biased estimates of effect. Hence the claims reported in a meta-analysis paper may be unreliable if the primary papers do not ensure unbiased estimates of effect. Objective: Determine the statistical reliability of 10 papers and indirectly the reliability of the meta-analysis study. Method: Ten primary papers used in a meta-analysis paper and counted the numbers of outcomes, predictors, and covariates. We estimated the size of the potential analysis search space available to the authors of these papers; i.e. the number of comparisons and models available. Since we noticed that there were differences between predictors and covariates cited in the abstract and in the text, we applied this formula to information found in the abstracts, Space A, as well as the text, Space T, of each primary paper. Results: The median and range of the number of comparisons possible across the primary papers are 6.5 and (2-12,288) for abstracts, and 196,608 and (3,072-117,117,952) the texts. Note that the median of 6.5 for Space A is misleading as each primary study has 60-165 foods not mentioned in the abstract. Conclusion: Given that testing is at the 0.05 level and the number of comparisons is very large, nominal statistical significance is very weak support for a claim. The claims in these papers are not statistically supported and hence are unreliable. Thus, the claims of the meta-analysis paper lack evidentiary confirmation.",Karl E. Peace|JingJing Yin|Haresh Rochani|Sarbesh Pandeya|S. Stanley Young,,https://arxiv.org/abs/1710.02219v1,https://arxiv.org/pdf/1710.02219v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1710.02219v1.pdf
1710.02101v3,2017-10-05T16:22:27Z,2019-06-16 04:55:27,Reliable Clustering of Bernoulli Mixture Models,"A Bernoulli Mixture Model (BMM) is a finite mixture of random binary vectors with independent dimensions. The problem of clustering BMM data arises in a variety of real-world applications, ranging from population genetics to activity analysis in social networks. In this paper, we analyze the clusterability of BMMs from a theoretical perspective, when the number of clusters is unknown. In particular, we stipulate a set of conditions on the sample complexity and dimension of the model in order to guarantee the Probably Approximately Correct (PAC)-clusterability of a dataset. To the best of our knowledge, these findings are the first non-asymptotic bounds on the sample complexity of learning or clustering BMMs.",Amir Najafi|Abolfazl Motahari|Hamid R. Rabiee,,https://arxiv.org/abs/1710.02101v3,https://arxiv.org/pdf/1710.02101v3,,22 pages,,,cs.LG,cs.LG|cs.IT|stat.ML,https://arxiv.org/pdf/1710.02101v3.pdf
1710.01614v2,2017-10-04T14:23:04Z,2017-10-05 20:52:35,Constructing multi-modality and multi-classifier radiomics predictive models through reliable classifier fusion,"Radiomics aims to extract and analyze large numbers of quantitative features from medical images and is highly promising in staging, diagnosing, and predicting outcomes of cancer treatments. Nevertheless, several challenges need to be addressed to construct an optimal radiomics predictive model. First, the predictive performance of the model may be reduced when features extracted from an individual imaging modality are blindly combined into a single predictive model. Second, because many different types of classifiers are available to construct a predictive model, selecting an optimal classifier for a particular application is still challenging. In this work, we developed multi-modality and multi-classifier radiomics predictive models that address the aforementioned issues in currently available models. Specifically, a new reliable classifier fusion strategy was proposed to optimally combine output from different modalities and classifiers. In this strategy, modality-specific classifiers were first trained, and an analytic evidential reasoning (ER) rule was developed to fuse the output score from each modality to construct an optimal predictive model. One public data set and two clinical case studies were performed to validate model performance. The experimental results indicated that the proposed ER rule based radiomics models outperformed the traditional models that rely on a single classifier or simply use combined features from different modalities.",Zhiguo Zhou|Zhi-Jie Zhou|Hongxia Hao|Shulong Li|Xi Chen|You Zhang|Michael Folkert|Jing Wang,,https://arxiv.org/abs/1710.01614v2,https://arxiv.org/pdf/1710.01614v2,,,,,cs.LG,cs.LG|physics.med-ph|stat.ML,https://arxiv.org/pdf/1710.01614v2.pdf
1710.01494v1,2017-10-04T08:03:41Z,2017-10-04 08:03:41,Strengths and Weaknesses of Deep Learning Models for Face Recognition Against Image Degradations,"Deep convolutional neural networks (CNNs) based approaches are the state-of-the-art in various computer vision tasks, including face recognition. Considerable research effort is currently being directed towards further improving deep CNNs by focusing on more powerful model architectures and better learning techniques. However, studies systematically exploring the strengths and weaknesses of existing deep models for face recognition are still relatively scarce in the literature. In this paper, we try to fill this gap and study the effects of different covariates on the verification performance of four recent deep CNN models using the Labeled Faces in the Wild (LFW) dataset. Specifically, we investigate the influence of covariates related to: image quality -- blur, JPEG compression, occlusion, noise, image brightness, contrast, missing pixels; and model characteristics -- CNN architecture, color information, descriptor computation; and analyze their impact on the face verification performance of AlexNet, VGG-Face, GoogLeNet, and SqueezeNet. Based on comprehensive and rigorous experimentation, we identify the strengths and weaknesses of the deep learning models, and present key areas for potential future research. Our results indicate that high levels of noise, blur, missing pixels, and brightness have a detrimental effect on the verification performance of all models, whereas the impact of contrast changes and compression artifacts is limited. It has been found that the descriptor computation strategy and color information does not have a significant influence on performance.",Klemen Grm|Vitomir Štruc|Anais Artiges|Matthieu Caron|Hazim Kemal Ekenel,,https://arxiv.org/abs/1710.01494v1,https://arxiv.org/pdf/1710.01494v1,https://doi.org/10.1049/iet-bmt.2017.0083,,,10.1049/iet-bmt.2017.0083,stat.ML,stat.ML,https://arxiv.org/pdf/1710.01494v1.pdf
1710.00324v1,2017-10-01T09:40:12Z,2017-10-01 09:40:12,Mutual Information based Bayesian Analysis of Power System Reliability,"This paper aims at assessing the power system reliability by estimating loss of load (LOL) index using mutual information based Bayesian approach. Reliability analysis is a key component in the design, analysis and tuning of complex structure like electrical power system. Consideration is given to rare events while constructing the Bayesian network, which provides reliable estimates of probability distribution function of LOL with lesser computing effort. Also, the ranking of load components due to loss of load is evaluated. The RBTS and IEEE RTS-24 systems are used as test cases.",Swasti R. Khuntia|Jose L. Rueda|Mart A. M. M. van der Meijden,,https://arxiv.org/abs/1710.00324v1,https://arxiv.org/pdf/1710.00324v1,https://doi.org/10.1109/PTC.2015.7232592,"6 pages, 9 figures, 2 tables, PowerTech, 2015 IEEE Eindhoven",,10.1109/PTC.2015.7232592,stat.AP,stat.AP,https://arxiv.org/pdf/1710.00324v1.pdf
1709.01589v2,2017-09-05T20:44:32Z,2018-08-09 16:52:07,An active-learning algorithm that combines sparse polynomial chaos expansions and bootstrap for structural reliability analysis,"Polynomial chaos expansions (PCE) have seen widespread use in the context of uncertainty quantification. However, their application to structural reliability problems has been hindered by the limited performance of PCE in the tails of the model response and due to the lack of local metamodel error estimates. We propose a new method to provide local metamodel error estimates based on bootstrap resampling and sparse PCE. An initial experimental design is iteratively updated based on the current estimation of the limit-state surface in an active learning algorithm. The greedy algorithm uses the bootstrap-based local error estimates for the polynomial chaos predictor to identify the best candidate set of points to enrich the experimental design. We demonstrate the effectiveness of this approach on a well-known analytical benchmark representing a series system, on a truss structure and on a complex realistic frame structure problem.",S. Marelli|B. Sudret,,https://arxiv.org/abs/1709.01589v2,https://arxiv.org/pdf/1709.01589v2,https://doi.org/10.1016/j.strusafe.2018.06.003,,"Structural Safety, 75, pp. 67-74 (2018)",10.1016/j.strusafe.2018.06.003,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/1709.01589v2.pdf
1708.08551v1,2017-08-28T22:41:11Z,2017-08-28 22:41:11,Deep Learning for Accelerated Reliability Analysis of Infrastructure Networks,"Natural disasters can have catastrophic impacts on the functionality of infrastructure systems and cause severe physical and socio-economic losses. Given budget constraints, it is crucial to optimize decisions regarding mitigation, preparedness, response, and recovery practices for these systems. This requires accurate and efficient means to evaluate the infrastructure system reliability. While numerous research efforts have addressed and quantified the impact of natural disasters on infrastructure systems, typically using the Monte Carlo approach, they still suffer from high computational cost and, thus, are of limited applicability to large systems. This paper presents a deep learning framework for accelerating infrastructure system reliability analysis. In particular, two distinct deep neural network surrogates are constructed and studied: (1) A classifier surrogate which speeds up the connectivity determination of networks, and (2) An end-to-end surrogate that replaces a number of components such as roadway status realization, connectivity determination, and connectivity averaging. The proposed approach is applied to a simulation-based study of the two-terminal connectivity of a California transportation network subject to extreme probabilistic earthquake events. Numerical results highlight the effectiveness of the proposed approach in accelerating the transportation system two-terminal reliability analysis with extremely high prediction accuracy.",Mohammad Amin Nabian|Hadi Meidani,,https://arxiv.org/abs/1708.08551v1,https://arxiv.org/pdf/1708.08551v1,https://doi.org/10.1111/mice.12359,,"Nabian, M. A. and Meidani, H. (2018), Deep Learning for Accelerated Seismic Reliability Analysis of Transportation Networks. Computer Aided Civil and Infrastructure Engineering, 33: 443-458",10.1111/mice.12359,cs.CE,cs.CE|cs.AI|stat.ML,https://arxiv.org/pdf/1708.08551v1.pdf
1708.07213v1,2017-08-23T23:24:52Z,2017-08-23 23:24:52,The duration of load effect in lumber as stochastic degradation,"This paper proposes a gamma process for modelling the damage that accumulates over time in the lumber used in structural engineering applications when stress is applied. The model separates the stochastic processes representing features internal to the piece of lumber on the one hand, from those representing external forces due to applied dead and live loads. The model applies those external forces through a time-varying population level function designed for time-varying loads. The application of this type of model, which is standard in reliability analysis, is novel in this context, which has been dominated by accumulated damage models (ADMs) over more than half a century. The proposed model is compared with one of the traditional ADMs. Our statistical results based on a Bayesian analysis of experimental data highlight the limitations of using accelerated testing data to assess long-term reliability, as seen in the wide posterior intervals. This suggests the need for more comprehensive testing in future applications, or to encode appropriate expert knowledge in the priors used for Bayesian analysis.",Samuel W. K. Wong|James V. Zidek,,https://arxiv.org/abs/1708.07213v1,https://arxiv.org/pdf/1708.07213v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1708.07213v1.pdf
1708.06990v1,2017-08-23T13:32:24Z,2017-08-23 13:32:24,On the interpretability and computational reliability of frequency-domain Granger causality,"This is a comment to the paper 'A study of problems encountered in Granger causality analysis from a neuroscience perspective'. We agree that interpretation issues of Granger Causality in Neuroscience exist (partially due to the historical unfortunate use of the name 'causality', as nicely described in previous literature). On the other hand we think that the paper uses a formulation of Granger causality which is outdated (albeit still used), and in doing so it dismisses the measure based on a suboptimal use of it. Furthermore, since data from simulated systems are used, the pitfalls that are found with the used formulation are intended to be general, and not limited to neuroscience. It would be a pity if this paper, even written in good faith, became a wildcard against all possible applications of Granger Causality, regardless of the hard work of colleagues aiming to seriously address the methodological and interpretation pitfalls. In order to provide a balanced view, we replicated their simulations used the updated State Space implementation, proposed already some years ago, in which the pitfalls are mitigated or directly solved.",Luca Faes|Sebastiano Stramaglia|Daniele Marinazzo,,https://arxiv.org/abs/1708.06990v1,https://arxiv.org/pdf/1708.06990v1,,,,,stat.ME,stat.ME|math.ST|stat.AP,https://arxiv.org/pdf/1708.06990v1.pdf
1708.04757v1,2017-08-16T03:27:25Z,2017-08-16 03:27:25,Scalable Joint Models for Reliable Uncertainty-Aware Event Prediction,"Missing data and noisy observations pose significant challenges for reliably predicting events from irregularly sampled multivariate time series (longitudinal) data. Imputation methods, which are typically used for completing the data prior to event prediction, lack a principled mechanism to account for the uncertainty due to missingness. Alternatively, state-of-the-art joint modeling techniques can be used for jointly modeling the longitudinal and event data and compute event probabilities conditioned on the longitudinal observations. These approaches, however, make strong parametric assumptions and do not easily scale to multivariate signals with many observations. Our proposed approach consists of several key innovations. First, we develop a flexible and scalable joint model based upon sparse multiple-output Gaussian processes. Unlike state-of-the-art joint models, the proposed model can explain highly challenging structure including non-Gaussian noise while scaling to large data. Second, we derive an optimal policy for predicting events using the distribution of the event occurrence estimated by the joint model. The derived policy trades-off the cost of a delayed detection versus incorrect assessments and abstains from making decisions when the estimated event probability does not satisfy the derived confidence criteria. Experiments on a large dataset show that the proposed framework significantly outperforms state-of-the-art techniques in event prediction.",Hossein Soleimani|James Hensman|Suchi Saria,,https://arxiv.org/abs/1708.04757v1,https://arxiv.org/pdf/1708.04757v1,,To appear in IEEE Transaction on Pattern Analysis and Machine Intelligence,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/1708.04757v1.pdf
1707.03173v2,2017-07-11T08:37:40Z,2018-01-06 16:25:48,Reliability of components of coherent systems: estimates in presence of masked data,"The reliability of a system of components depends on reliability of each component. Thus, the initial statistical work should be the estimation of the reliability of each component of the system. This is not an easy task because when the system fails, the failure time of a given component can not be observed, that is, censored data. Rodrigues et al. (2017) presented a solution for reliability estimation of components when it is avaliable the system failure time and the status of each component at the time of system failure (if it had failed before, after or it is responsible for system failure). However, there are situations it may be difficult to identify the status of components at the moment of system failure.
  Such cases are systems with masked causes of failure. Since parallel and series systems are the simplest systems, innumerous alternative solutions for these two systems have been appeared in the literature. To the best of our knowledge, this seems to be the first work that considers the general case of coherent systems. The three-parameter Weibull distribution is considered as the component failure time model. Identically distributed failure times is not required restrictions. Furthermore, there is no restriction on the subjective choice of prior distributions but preference has been given to continuous prior distributions; these priors represent well the nuances of the environment that the system operates. The statistical work of obtaining quantities of the posterior distribution is supported by the Metropolis within Gibbs algorithm. With several simulations, the excellent performance of the model was evaluated. We also consider a computer hard-drives real dataset in order to present the practical relevance of the proposed model.",Agatha Sacramento Rodrigues|Carlos Alberto de Braganca Pereira|Adriano Polpo,,https://arxiv.org/abs/1707.03173v2,https://arxiv.org/pdf/1707.03173v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1707.03173v2.pdf
1707.03119v1,2017-07-11T03:19:11Z,2017-07-11 03:19:11,Estimation of Component Reliability in Coherent Systems,"The first step in statistical reliability studies of coherent systems is the estimation of the reliability of each system component. For the cases of parallel and series systems the literature is abundant. It seems that the present paper is the first that presents the general case of component inferences in coherent systems. The failure time model considered here is the three-parameter Weibull distribution. Furthermore, neither independence nor identically distributed failure times are required restrictions. The proposed model is general in the sense that it can be used for any coherent system, from the simplest to the more complex structures. It can be considered for all kinds of censored data; including interval-censored data. An important property obtained for the Weibull model is the fact that the posterior distributions are proper, even for non-informative priors. Using several simulations, the excellent performance of the model is illustrated. As a real example, boys first use of marijuana is considered to show the efficiency of the solution even when censored data occurs.",Agatha S. Rodrigues|Felipe Bhering|Carlos Alberto de Braganca Pereira|Adriano Polpo,,https://arxiv.org/abs/1707.03119v1,https://arxiv.org/pdf/1707.03119v1,https://doi.org/10.1109/ACCESS.2018.2821102,,,10.1109/ACCESS.2018.2821102,stat.ME,stat.ME,https://arxiv.org/pdf/1707.03119v1.pdf
1706.04643v1,2017-06-14T19:19:57Z,2017-06-14 19:19:57,Bayesian analysis of accumulated damage models in lumber reliability,"Wood products that are subjected to sustained stress over a period of long duration may weaken, and this effect must be considered in models for the long-term reliability of lumber. The damage accumulation approach has been widely used for this purpose to set engineering standards. In this article, we revisit an accumulated damage model and propose a Bayesian framework for analysis. For parameter estimation and uncertainty quantification, we adopt approximation Bayesian computation (ABC) techniques to handle the complexities of the model. We demonstrate the effectiveness of our approach using both simulated and real data, and apply our fitted model to analyze long-term lumber reliability under a stochastic live loading scenario.",Chun-Hao Yang|James V. Zidek|Samuel W. K. Wong,,https://arxiv.org/abs/1706.04643v1,https://arxiv.org/pdf/1706.04643v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1706.04643v1.pdf
1706.10129v1,2017-06-09T21:18:56Z,2017-06-09 21:18:56,Electric propulsion reliability: statistical analysis of on-orbit anomalies and comparative analysis of electric versus chemical propulsion failure rates,"With a few hundred spacecraft launched to date with electric propulsion (EP), it is possible to conduct an epidemiological study of EP on orbit reliability. The first objective of the present work was to undertake such a study and analyze EP track record of on orbit anomalies and failures by different covariates. The second objective was to provide a comparative analysis of EP failure rates with those of chemical propulsion. After a thorough data collection, 162 EP-equipped satellites launched between January 1997 and December 2015 were included in our dataset for analysis. Several statistical analyses were conducted, at the aggregate level and then with the data stratified by severity of the anomaly, by orbit type, and by EP technology. Mean Time To Anomaly (MTTA) and the distribution of the time to anomaly were investigated, as well as anomaly rates. The important findings in this work include the following: (1) Post-2005, EP reliability has outperformed that of chemical propulsion; (2) Hall thrusters have robustly outperformed chemical propulsion, and they maintain a small but shrinking reliability advantage over gridded ion engines. Other results were also provided, for example the differentials in MTTA of minor and major anomalies for gridded ion engines and Hall thrusters. It was shown that: (3) Hall thrusters exhibit minor anomalies very early on orbit, which might be indicative of infant anomalies, and thus would benefit from better ground testing and acceptance procedures; (4) Strong evidence exists that EP anomalies (onset and likelihood) and orbit type are dependent, a dependence likely mediated by either the space environment or differences in thrusters duty cycles; (5) Gridded ion thrusters exhibit both infant and wear-out failures, and thus would benefit from a reliability growth program that addresses both these types of problems.",Joseph Homer Saleh|Fan Geng|Michelle Ku|Mitchell L. R. Walker,,https://arxiv.org/abs/1706.10129v1,https://arxiv.org/pdf/1706.10129v1,,"36 pages, 18 figures",,,physics.space-ph,physics.space-ph|stat.AP,https://arxiv.org/pdf/1706.10129v1.pdf
1706.02690v5,2017-06-08T17:43:56Z,2020-08-30 16:50:36,Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks,"We consider the problem of detecting out-of-distribution images in neural networks. We propose ODIN, a simple and effective method that does not require any change to a pre-trained neural network. Our method is based on the observation that using temperature scaling and adding small perturbations to the input can separate the softmax score distributions between in- and out-of-distribution images, allowing for more effective detection. We show in a series of experiments that ODIN is compatible with diverse network architectures and datasets. It consistently outperforms the baseline approach by a large margin, establishing a new state-of-the-art performance on this task. For example, ODIN reduces the false positive rate from the baseline 34.7% to 4.3% on the DenseNet (applied to CIFAR-10) when the true positive rate is 95%.",Shiyu Liang|Yixuan Li|R. Srikant,,https://arxiv.org/abs/1706.02690v5,https://arxiv.org/pdf/1706.02690v5,,ICLR 2018,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1706.02690v5.pdf
1704.08444v2,2017-04-27T06:16:46Z,2018-03-13 11:25:52,A note on Quantile curves based bivariate reliability concepts,We extend the univariate quantile based reliability concepts to the bivariate case using quantile curves. We propose quantile curves based bivariate hazard rate and bivariate mean residual life function and establish a relationship between them. We study the uniqueness properties of these concepts to determine the underlying quantile curve. We also study the quantile curves based reliability concepts in reverse time.,Sreelakshmi N,,https://arxiv.org/abs/1704.08444v2,https://arxiv.org/pdf/1704.08444v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1704.08444v2.pdf
1704.00023v1,2017-03-31T18:55:48Z,2017-03-31 18:55:48,On the Reliable Detection of Concept Drift from Streaming Unlabeled Data,"Classifiers deployed in the real world operate in a dynamic environment, where the data distribution can change over time. These changes, referred to as concept drift, can cause the predictive performance of the classifier to drop over time, thereby making it obsolete. To be of any real use, these classifiers need to detect drifts and be able to adapt to them, over time. Detecting drifts has traditionally been approached as a supervised task, with labeled data constantly being used for validating the learned model. Although effective in detecting drifts, these techniques are impractical, as labeling is a difficult, costly and time consuming activity. On the other hand, unsupervised change detection techniques are unreliable, as they produce a large number of false alarms. The inefficacy of the unsupervised techniques stems from the exclusion of the characteristics of the learned classifier, from the detection process. In this paper, we propose the Margin Density Drift Detection (MD3) algorithm, which tracks the number of samples in the uncertainty region of a classifier, as a metric to detect drift. The MD3 algorithm is a distribution independent, application independent, model independent, unsupervised and incremental algorithm for reliably detecting drifts from data streams. Experimental evaluation on 6 drift induced datasets and 4 additional datasets from the cybersecurity domain demonstrates that the MD3 approach can reliably detect drifts, with significantly fewer false alarms compared to unsupervised feature based drift detectors. The reduced false alarms enables the signaling of drifts only when they are most likely to affect classification performance. As such, the MD3 approach leads to a detection scheme which is credible, label efficient and general in its applicability.",Tegjyot Singh Sethi|Mehmed Kantardzic,,https://arxiv.org/abs/1704.00023v1,https://arxiv.org/pdf/1704.00023v1,,,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/1704.00023v1.pdf
1703.10651v4,2017-03-30T19:51:03Z,2018-02-01 13:40:16,Reliable Decision Support using Counterfactual Models,"Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and ""what if?"" reasoning for individualized treatment planning.",Peter Schulam|Suchi Saria,,https://arxiv.org/abs/1703.10651v4,https://arxiv.org/pdf/1703.10651v4,,Published in the proceedings of Neural Information Processing Systems (NIPS) 2017,,,stat.ML,stat.ML|cs.AI|cs.LG,https://arxiv.org/pdf/1703.10651v4.pdf
1702.00584v1,2017-02-02T08:53:59Z,2017-02-02 08:53:59,Ultra Reliable Short Message Relaying with Wireless Power Transfer,"We consider a dual-hop wireless network where an energy constrained relay node first harvests energy through the received radio-frequency signal from the source, and then uses the harvested energy to forward the source's information to the destination node. The throughput and delay metrics are investigated for a decode-and-forward relaying mechanism at finite blocklength regime and delay-limited transmission mode. We consider ultra-reliable communication scenarios under discussion for the next fifth-generation of wireless systems, with error and latency constraints. The impact on these metrics of the blocklength, information bits, and relay position is investigated.",Onel L. Alcaraz López|Hirley Alves|Richard Demo Souza|Evelio Martín García Fernández,,https://arxiv.org/abs/1702.00584v1,https://arxiv.org/pdf/1702.00584v1,,Accepted in ICC 2017,,,cs.IT,cs.IT|stat.AP,https://arxiv.org/pdf/1702.00584v1.pdf
1701.03462v1,2017-01-12T16:16:41Z,2017-01-12 16:16:41,"Adversarial and Amiable Inference in Medical Diagnosis, Reliability, and Survival Analysis","In this paper, we develop a family of bivariate beta distributions that encapsulate both positive and negative correlations, and which can be of general interest for Bayesian inference. We then invoke a use of these bivariate distributions in two contexts. The first is diagnostic testing in medicine, threat detection, and signal processing. The second is system survivability assessment, relevant to engineering reliability, and to survival analysis in biomedicine. In diagnostic testing one encounters two parameters that characterize the efficacy of the testing mechanism, {\it test sensitivity}, and {\it test specificity}. These tend to be adversarial when their values are interpreted as utilities. In system survivability, the parameters of interest are the component reliabilities, whose values when interpreted as utilities tend to exhibit co-operative (amiable) behavior. Besides probability modeling and Bayesian inference, this paper has a foundational import. Specifically, it advocates a conceptual change in how one may think about reliability and survival analysis. The philosophical writings of de Finetti, Kolmogorov, Popper, and Savage, when brought to bear on these topics constitute the essence of this change. Its consequence is that we have at hand a defensible framework for invoking Bayesian inferential methods in diagnostics, reliability, and survival analysis. Another consequence is a deeper appreciation of the judgment of independent lifetimes. Specifically, we make the important point that independent lifetimes entail at a minimum, a two-stage hierarchical construction.",Nozer D. Singpurwalla|Barry C. Arnold|Joseph L. Gastwirth|Anna S. Gordon|Hon Keung Tony Ng,,https://arxiv.org/abs/1701.03462v1,https://arxiv.org/pdf/1701.03462v1,https://doi.org/10.1111/insr.12104,,"International Statistical Review (2016), Vol. 84, pp. 390-412",10.1111/insr.12104,stat.ME,stat.ME,https://arxiv.org/pdf/1701.03462v1.pdf
1612.02572v1,2016-12-08T09:26:08Z,2016-12-08 09:26:08,Predicting brain age with deep learning from raw imaging data results in a reliable and heritable biomarker,"Machine learning analysis of neuroimaging data can accurately predict chronological age in healthy people and deviations from healthy brain ageing have been associated with cognitive impairment and disease. Here we sought to further establish the credentials of ""brain-predicted age"" as a biomarker of individual differences in the brain ageing process, using a predictive modelling approach based on deep learning, and specifically convolutional neural networks (CNN), and applied to both pre-processed and raw T1-weighted MRI data. Firstly, we aimed to demonstrate the accuracy of CNN brain-predicted age using a large dataset of healthy adults (N = 2001). Next, we sought to establish the heritability of brain-predicted age using a sample of monozygotic and dizygotic female twins (N = 62). Thirdly, we examined the test-retest and multi-centre reliability of brain-predicted age using two samples (within-scanner N = 20; between-scanner N = 11). CNN brain-predicted ages were generated and compared to a Gaussian Process Regression (GPR) approach, on all datasets. Input data were grey matter (GM) or white matter (WM) volumetric maps generated by Statistical Parametric Mapping (SPM) or raw data. Brain-predicted age represents an accurate, highly reliable and genetically-valid phenotype, that has potential to be used as a biomarker of brain ageing. Moreover, age predictions can be accurately generated on raw T1-MRI data, substantially reducing computation time for novel data, bringing the process closer to giving real-time information on brain health in clinical settings.",James H Cole|Rudra PK Poudel|Dimosthenis Tsagkrasoulis|Matthan WA Caan|Claire Steves|Tim D Spector|Giovanni Montana,,https://arxiv.org/abs/1612.02572v1,https://arxiv.org/pdf/1612.02572v1,,,,,stat.ML,stat.ML|cs.CV|cs.LG|q-bio.NC,https://arxiv.org/pdf/1612.02572v1.pdf
1611.10258v1,2016-11-30T16:42:23Z,2016-11-30 16:42:23,Reliably Learning the ReLU in Polynomial Time,"We give the first dimension-efficient algorithms for learning Rectified Linear Units (ReLUs), which are functions of the form $\mathbf{x} \mapsto \max(0, \mathbf{w} \cdot \mathbf{x})$ with $\mathbf{w} \in \mathbb{S}^{n-1}$. Our algorithm works in the challenging Reliable Agnostic learning model of Kalai, Kanade, and Mansour (2009) where the learner is given access to a distribution $\cal{D}$ on labeled examples but the labeling may be arbitrary. We construct a hypothesis that simultaneously minimizes the false-positive rate and the loss on inputs given positive labels by $\cal{D}$, for any convex, bounded, and Lipschitz loss function.
  The algorithm runs in polynomial-time (in $n$) with respect to any distribution on $\mathbb{S}^{n-1}$ (the unit sphere in $n$ dimensions) and for any error parameter $ε= Ω(1/\log n)$ (this yields a PTAS for a question raised by F. Bach on the complexity of maximizing ReLUs). These results are in contrast to known efficient algorithms for reliably learning linear threshold functions, where $ε$ must be $Ω(1)$ and strong assumptions are required on the marginal distribution. We can compose our results to obtain the first set of efficient algorithms for learning constant-depth networks of ReLUs.
  Our techniques combine kernel methods and polynomial approximations with a ""dual-loss"" approach to convex programming. As a byproduct we obtain a number of applications including the first set of efficient algorithms for ""convex piecewise-linear fitting"" and the first efficient algorithms for noisy polynomial reconstruction of low-weight polynomials on the unit sphere.",Surbhi Goel|Varun Kanade|Adam Klivans|Justin Thaler,,https://arxiv.org/abs/1611.10258v1,https://arxiv.org/pdf/1611.10258v1,,,,,cs.LG,cs.LG|cs.CC|stat.ML,https://arxiv.org/pdf/1611.10258v1.pdf
1611.09083v1,2016-11-28T11:43:31Z,2016-11-28 11:43:31,Prediction of Video Popularity in the Absence of Reliable Data from Video Hosting Services: Utility of Traces Left by Users on the Web,"With the growth of user-generated content, we observe the constant rise of the number of companies, such as search engines, content aggregators, etc., that operate with tremendous amounts of web content not being the services hosting it. Thus, aiming to locate the most important content and promote it to the users, they face the need of estimating the current and predicting the future content popularity.
  In this paper, we approach the problem of video popularity prediction not from the side of a video hosting service, as done in all previous studies, but from the side of an operating company, which provides a popular video search service that aggregates content from different video hosting websites. We investigate video popularity prediction based on features from three primary sources available for a typical operating company: first, the content hosting provider may deliver its data via its API, second, the operating company makes use of its own search and browsing logs, third, the company crawls information about embeds of a video and links to a video page from publicly available resources on the Web. We show that video popularity prediction based on the embed and link data coupled with the internal search and browsing data significantly improves video popularity prediction based only on the data provided by the video hosting and can even adequately replace the API data in the cases when it is partly or completely unavailable.",Alexey Drutsa|Gleb Gusev|Pavel Serdyukov,"Yandex, Moscow, Russia|Yandex, Moscow, Russia|Yandex, Moscow, Russia",https://arxiv.org/abs/1611.09083v1,https://arxiv.org/pdf/1611.09083v1,,"23 pages, 4 figures",,,cs.IR,cs.IR|cs.HC|cs.MM|stat.ML,https://arxiv.org/pdf/1611.09083v1.pdf
1611.07412v1,2016-11-22T16:52:53Z,2016-11-22 16:52:53,Statistical Methods for Thermal Index Estimation Based on Accelerated Destructive Degradation Test Data,"Accelerated destructive degradation test (ADDT) is a technique that is commonly used by industries to access material's long-term properties. In many applications, the accelerating variable is usually the temperature. In such cases, a thermal index (TI) is used to indicate the strength of the material. For example, a TI of 200C may be interpreted as the material can be expected to maintain a specific property at a temperature of 200C for 100,000 hours. A material with a higher TI possesses a stronger resistance to thermal damage. In literature, there are three methods available to estimate the TI based on ADDT data, which are the traditional method based on the least-squares approach, the parametric method, and the semiparametric method. In this chapter, we provide a comprehensive review of the three methods and illustrate how the TI can be estimated based on different models. We also conduct comprehensive simulation studies to show the properties of different methods. We provide thorough discussions on the pros and cons of each method. The comparisons and discussion in this chapter can be useful for practitioners and future industrial standards.",Yimeng Xie|Zhongnan Jin|Yili Hong|Jennifer H. Van Mullekom,,https://arxiv.org/abs/1611.07412v1,https://arxiv.org/pdf/1611.07412v1,,21 pages,,,stat.AP,stat.AP,https://arxiv.org/pdf/1611.07412v1.pdf
1611.04067v2,2016-11-13T02:14:01Z,2017-01-11 17:05:07,Error Metrics for Learning Reliable Manifolds from Streaming Data,"Spectral dimensionality reduction is frequently used to identify low-dimensional structure in high-dimensional data. However, learning manifolds, especially from the streaming data, is computationally and memory expensive. In this paper, we argue that a stable manifold can be learned using only a fraction of the stream, and the remaining stream can be mapped to the manifold in a significantly less costly manner. Identifying the transition point at which the manifold is stable is the key step. We present error metrics that allow us to identify the transition point for a given stream by quantitatively assessing the quality of a manifold learned using Isomap. We further propose an efficient mapping algorithm, called S-Isomap, that can be used to map new samples onto the stable manifold. We describe experiments on a variety of data sets that show that the proposed approach is computationally efficient without sacrificing accuracy.",Frank Schoeneman|Suchismit Mahapatra|Varun Chandola|Nils Napp|Jaroslaw Zola,,https://arxiv.org/abs/1611.04067v2,https://arxiv.org/pdf/1611.04067v2,https://doi.org/10.1137/1.9781611974973.84,,,10.1137/1.9781611974973.84,stat.ML,stat.ML,https://arxiv.org/pdf/1611.04067v2.pdf
1610.07472v3,2016-10-24T16:13:56Z,2017-04-02 13:32:25,Distilling Information Reliability and Source Trustworthiness from Digital Traces,"Online knowledge repositories typically rely on their users or dedicated editors to evaluate the reliability of their content. These evaluations can be viewed as noisy measurements of both information reliability and information source trustworthiness. Can we leverage these noisy evaluations, often biased, to distill a robust, unbiased and interpretable measure of both notions?
  In this paper, we argue that the temporal traces left by these noisy evaluations give cues on the reliability of the information and the trustworthiness of the sources. Then, we propose a temporal point process modeling framework that links these temporal traces to robust, unbiased and interpretable notions of information reliability and source trustworthiness. Furthermore, we develop an efficient convex optimization procedure to learn the parameters of the model from historical traces. Experiments on real-world data gathered from Wikipedia and Stack Overflow show that our modeling framework accurately predicts evaluation events, provides an interpretable measure of information reliability and source trustworthiness, and yields interesting insights about real-world events.",Behzad Tabibian|Isabel Valera|Mehrdad Farajtabar|Le Song|Bernhard Schölkopf|Manuel Gomez-Rodriguez,,https://arxiv.org/abs/1610.07472v3,https://arxiv.org/pdf/1610.07472v3,https://doi.org/10.1145/3038912.3052672,Accepted at 26th World Wide Web conference (WWW-17),,10.1145/3038912.3052672,cs.SI,cs.SI|stat.ML,https://arxiv.org/pdf/1610.07472v3.pdf
1610.07222v1,2016-10-23T19:24:59Z,2016-10-23 19:24:59,Robust Bayesian Reliability for Complex Systems under Prior-Data Conflict,"In reliability engineering, data about failure events is often scarce. To arrive at meaningful estimates for the reliability of a system, it is therefore often necessary to also include expert information in the analysis, which is straightforward in the Bayesian approach by using an informative prior distribution. A problem called prior-data conflict then can arise: observed data seem very surprising from the viewpoint of the prior, i.e., information from data is in conflict with prior assumptions. Models based on conjugate priors can be insensitive to prior-data conflict, in the sense that the spread of the posterior distribution does not increase in case of such a conflict, thus conveying a false sense of certainty. An approach to mitigate this issue is presented, by considering sets of prior distributions to model limited knowledge on Weibull distributed component lifetimes, treating systems with arbitrary layout using the survival signature. This approach can be seen as a robust Bayesian procedure or imprecise probability method that reflects surprisingly early or late component failures by wider system reliability bounds.",Gero Walter|Frank P. A. Coolen,,https://arxiv.org/abs/1610.07222v1,https://arxiv.org/pdf/1610.07222v1,,"21 pages, 9 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/1610.07222v1.pdf
1610.07108v1,2016-10-23T00:36:12Z,2016-10-23 00:36:12,Fast and Reliable Parameter Estimation from Nonlinear Observations,"In this paper we study the problem of recovering a structured but unknown parameter ${\bfθ}^*$ from $n$ nonlinear observations of the form $y_i=f(\langle {\bf{x}}_i,{\bfθ}^*\rangle)$ for $i=1,2,\ldots,n$. We develop a framework for characterizing time-data tradeoffs for a variety of parameter estimation algorithms when the nonlinear function $f$ is unknown. This framework includes many popular heuristics such as projected/proximal gradient descent and stochastic schemes. For example, we show that a projected gradient descent scheme converges at a linear rate to a reliable solution with a near minimal number of samples. We provide a sharp characterization of the convergence rate of such algorithms as a function of sample size, amount of a-prior knowledge available about the parameter and a measure of the nonlinearity of the function $f$. These results provide a precise understanding of the various tradeoffs involved between statistical and computational resources as well as a-prior side information available for such nonlinear parameter estimation problems.",Samet Oymak|Mahdi Soltanolkotabi,,https://arxiv.org/abs/1610.07108v1,https://arxiv.org/pdf/1610.07108v1,,"23 pages, 4 figures",,,stat.ML,stat.ML|cs.IT|math.OC,https://arxiv.org/pdf/1610.07108v1.pdf
1609.07662v1,2016-09-24T19:20:18Z,2016-09-24 19:20:18,Detecting Performance Degradation of Software-Intensive Systems in the Presence of Trends and Long-Range Dependence,"As contemporary software-intensive systems reach increasingly large scale, it is imperative that failure detection schemes be developed to help prevent costly system downtimes. A promising direction towards the construction of such schemes is the exploitation of easily available measurements of system performance characteristics such as average number of processed requests and queue size per unit of time. In this work, we investigate a holistic methodology for detection of abrupt changes in time series data in the presence of quasi-seasonal trends and long-range dependence with a focus on failure detection in computer systems. We propose a trend estimation method enjoying optimality properties in the presence of long-range dependent noise to estimate what is considered ""normal"" system behaviour. To detect change-points and anomalies, we develop an approach based on the ensembles of ""weak"" detectors. We demonstrate the performance of the proposed change-point detection scheme using an artificial dataset, the publicly available Abilene dataset as well as the proprietary geoinformation system dataset.",Alexey Artemov|Evgeny Burnaev,,https://arxiv.org/abs/1609.07662v1,https://arxiv.org/pdf/1609.07662v1,,"8 pages, 7 figures",,,stat.AP,stat.AP|stat.CO|stat.ME,https://arxiv.org/pdf/1609.07662v1.pdf
1609.07217v2,2016-09-23T03:16:10Z,2017-12-27 21:27:33,Statistical Modeling for Spatio-Temporal Degradation Data,"This paper investigates the modeling of an important class of degradation data, which are collected from a spatial domain over time; for example, the surface quality degradation. Like many existing time-dependent stochastic degradation models, a special random field is constructed for modeling the spatio-temporal degradation process. In particular, we express the degradation at any spatial location and time as an additive superposition of two stochastic components: a dynamic spatial degradation generation process, and a spatio-temporal degradation propagation process. Some unique challenges are addressed, including the spatial heterogeneity of the degradation process, the spatial propagation of degradation to neighboring areas, the anisotropic and space-time non-separable covariance structure often associated with a complex spatio-temporal degradation process, and the computational issue related to parameter estimation. When the spatial dependence is ignored, we show that the proposed spatio-temporal degradation model incorporates some existing pure time-dependent degradation processes as its special cases. We also show the connection, under special conditions, between the proposed model and general physical degradation processes which are often defined by stochastic partial differential equations. A numerical example is presented to illustrate the modeling approach and model validation.",Xiao Liu|Kyongmin Yeo|Jayant Kalagnanam,,https://arxiv.org/abs/1609.07217v2,https://arxiv.org/pdf/1609.07217v2,,"30 pages, 7 figures. Manuscript prepared for submission",,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/1609.07217v2.pdf
1607.07745v1,2016-07-26T15:19:12Z,2016-07-26 15:19:12,Leveraging Unstructured Data to Detect Emerging Reliability Issues,"Unstructured data refers to information that does not have a predefined data model or is not organized in a pre-defined manner. Loosely speaking, unstructured data refers to text data that is generated by humans. In after-sales service businesses, there are two main sources of unstructured data: customer complaints, which generally describe symptoms, and technician comments, which outline diagnostics and treatment information. A legitimate customer complaint can eventually be tracked to a failure or a claim. However, there is a delay between the time of a customer complaint and the time of a failure or a claim. A proactive strategy aimed at analyzing customer complaints for symptoms can help service providers detect reliability problems in advance and initiate corrective actions such as recalls. This paper introduces essential text mining concepts in the context of reliability analysis and a method to detect emerging reliability issues. The application of the method is illustrated using a case study.",Deovrat Kakde|Arin Chaudhuri,,https://arxiv.org/abs/1607.07745v1,https://arxiv.org/pdf/1607.07745v1,https://doi.org/10.1109/RAMS.2015.7105093,,,10.1109/RAMS.2015.7105093,cs.AI,cs.AI|stat.AP|stat.ME|stat.ML,https://arxiv.org/pdf/1607.07745v1.pdf
1607.02801v2,2016-07-11T01:08:14Z,2016-08-02 17:59:32,Bounds on the Number of Measurements for Reliable Compressive Classification,"This paper studies the classification of high-dimensional Gaussian signals from low-dimensional noisy, linear measurements. In particular, it provides upper bounds (sufficient conditions) on the number of measurements required to drive the probability of misclassification to zero in the low-noise regime, both for random measurements and designed ones. Such bounds reveal two important operational regimes that are a function of the characteristics of the source: i) when the number of classes is less than or equal to the dimension of the space spanned by signals in each class, reliable classification is possible in the low-noise regime by using a one-vs-all measurement design; ii) when the dimension of the spaces spanned by signals in each class is lower than the number of classes, reliable classification is guaranteed in the low-noise regime by using a simple random measurement design. Simulation results both with synthetic and real data show that our analysis is sharp, in the sense that it is able to gauge the number of measurements required to drive the misclassification probability to zero in the low-noise regime.",Hugo Reboredo|Francesco Renna|Robert Calderbank|Miguel R. D. Rodrigues,,https://arxiv.org/abs/1607.02801v2,https://arxiv.org/pdf/1607.02801v2,https://doi.org/10.1109/TSP.2016.2599496,"16 pages, 5 figures, 4 tables. Submitted to the IEEE Transactions on Signal Processing",,10.1109/TSP.2016.2599496,cs.IT,cs.IT|cs.CV|stat.ML,https://arxiv.org/pdf/1607.02801v2.pdf
1606.06284v1,2016-06-19T15:34:11Z,2016-06-19 15:34:11,Effects of Scan Length and Shrinkage on Reliability of Resting-State Functional Connectivity in the Human Connectome Project,"In this paper, we use data from the Human Connectome Project (N=461) to investigate the effect of scan length on reliability of resting-state functional connectivity (rsFC) estimates produced from resting-state functional magnetic resonance imaging (rsfMRI). Additionally, we study the benefits of empirical Bayes shrinkage, in which subject-level estimates borrow strength from the population average by trading a small increase in bias for a greater reduction in variance. For each subject, we compute raw and shrinkage estimates of rsFC between 300 regions identified through independent components analysis (ICA) based on rsfMRI scans varying from 3 to 30 minutes in length. The time course for each region is determined using dual regression, and rsFC is estimated as the Pearson correlation between each pair of time courses. Shrinkage estimates for each subject are computed as a weighted average between the raw subject-level estimate and the population average estimate, where the weight is determined for each connection by the relationship of within-subject variance to between-subject variance. We find that shrinkage estimates exhibit greater reliability than raw estimates for most connections, with 30-40% improvement using scans less than 10 minutes in length and 10-20% improvement using scans of 20-30 minutes. We also observe significant spatial variability in reliability of both raw and shrinkage estimates, with connections within the default mode and motor networks exhibiting the greatest reliability and between-network connections exhibiting the poorest reliability. We conclude that the scan length required for reliable estimation of rsFC depends on the specific connections of interest, and shrinkage can be used to increase reliability of rsFC, even when produced from long, high-quality rsfMRI scans.",Amanda F. Mejia|Mary Beth Nebel|Anita D. Barber|Ann S. Choe|Martin A. Lindquist,,https://arxiv.org/abs/1606.06284v1,https://arxiv.org/pdf/1606.06284v1,,,,,stat.AP,stat.AP|q-bio.NC,https://arxiv.org/pdf/1606.06284v1.pdf
1606.02275v1,2016-06-07T19:39:02Z,2016-06-07 19:39:02,Measuring the reliability of MCMC inference with bidirectional Monte Carlo,"Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We present Bounding Divergences with REverse Annealing (BREAD), a protocol for validating the relevance of simulated data experiments to real datasets, and integrate it into two probabilistic programming languages: WebPPL and Stan. As an example of how BREAD can be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in both WebPPL and Stan.",Roger B. Grosse|Siddharth Ancha|Daniel M. Roy,,https://arxiv.org/abs/1606.02275v1,https://arxiv.org/pdf/1606.02275v1,,,,,cs.LG,cs.LG|stat.CO|stat.ML,https://arxiv.org/pdf/1606.02275v1.pdf
1604.05180v1,2016-04-18T14:32:57Z,2016-04-18 14:32:57,Cryptographically secure multiparty evaluation of system reliability,"The precise design of a system may be considered a trade secret which should be protected, whilst at the same time component manufacturers are sometimes reluctant to release full test data (perhaps only providing mean time to failure data). In this situation it seems impractical to both produce an accurate reliability assessment and satisfy all parties' privacy requirements. However, we present recent developments in cryptography which, when combined with the recently developed survival signature in reliability theory, allows almost total privacy to be maintained in a cryptographically strong manner in precisely this setting. Thus, the system designer does not have to reveal their trade secret design and the component manufacturer can retain component test data in-house.",Louis J. M. Aslett,,https://arxiv.org/abs/1604.05180v1,https://arxiv.org/pdf/1604.05180v1,,13 pages; supplemental material at http://www.louisaslett.com/,,,cs.CR,cs.CR|stat.ME,https://arxiv.org/pdf/1604.05180v1.pdf
1603.07775v2,2016-03-24T22:54:24Z,2016-04-27 19:32:08,The Impact of Operators' Performance in the Reliability of Cyber-Physical Power Distribution Systems,"Cyber-Physical Systems are the result of integrating information and communication technologies into physical systems. One particular case are Cyber-Physical Power Systems (CPPS), which use communication technologies to perform real-time monitoring and operation. These kinds of systems have become more complex, impacting on the systems' characteristics, such as their reliability. In addition, it is already known that in terms of the reliability of Cyber-Physical Power Distribution Systems (CPPDS), the failures of the communication network are just as relevant as the electrical network failures. However, some of the operators' performances, such as response time and decision quality, during CPPDS contingencies have not been investigated yet. In this paper, we introduce a model to the operator response time, present a Sequential Monte Carlo Simulation methodology that incorporates the response time in CPPDS reliability indices estimation, and evaluate the impact of the operator response time in reliability indices. Our method is tested on a CPPDS using different values for the average response time of operators. The results show that the response time of the operators affects the reliability indices that are related to the durations of the failure, indicating that a fast decision directly contributes to the system performance. We conclude that the improvement of CPPDS reliability is not only dependent on the electric and communication components, but also dependent on operators' performance.",Michel Bessani|Rodrigo Z. Fanucchi|Alexandre C. C. Delbem|Carlos D. Maciel,,https://arxiv.org/abs/1603.07775v2,https://arxiv.org/pdf/1603.07775v2,https://doi.org/10.1049/iet-gtd.2015.1062,,,10.1049/iet-gtd.2015.1062,eess.SY,eess.SY|stat.AP,https://arxiv.org/pdf/1603.07775v2.pdf
1603.05587v5,2016-03-17T17:39:12Z,2016-07-12 17:39:50,Reliable Prediction Intervals for Local Linear Regression,"This paper introduces two methods for estimating reliable prediction intervals for local linear least-squares regressions, named Bounded Oscillation Prediction Intervals (BOPI). It also proposes a new measure for comparing interval prediction models named Equivalent Gaussian Standard Deviation (EGSD). The experimental results compare BOPI to other methods using coverage probability, Mean Interval Size and the introduced EGSD measure. The results were generally in favor of the BOPI on considered benchmark regression datasets. It also, reports simulation studies validating the BOPI method's reliability.",Mohammad Ghasemi Hamed|Masoud Ebadi Kivaj,,https://arxiv.org/abs/1603.05587v5,https://arxiv.org/pdf/1603.05587v5,,"40 pages,11 figures, 10 tables and 1 algorithm. arXiv admin note: text overlap with arXiv:1402.5874",,,stat.ME,stat.ME|cs.LG,https://arxiv.org/pdf/1603.05587v5.pdf
1603.01397v1,2016-03-04T09:44:21Z,2016-03-04 09:44:21,Latent class analyisis for reliable measure of inflation expectation in the indian public,"The main aim of this paper is to inspect the properties of survey based on households inflation expectations, conducted by Reserve Bank of India. It is theorized that the respondents answers are exaggerated by extreme response bias. Latent class analysis has been hailed as a promising technique for studying measurement errors in surveys, because the model produces estimates of the error rates associated with a given question of the questionnaire. I have identified a model with optimum performance and hence categorize the objective as well as reliable classifiers or otherwise.",Sunil Kumar,,https://arxiv.org/abs/1603.01397v1,https://arxiv.org/pdf/1603.01397v1,,"16 pages, 04 Tables and 03 figures",,,stat.AP,stat.AP|q-fin.GN|q-fin.ST,https://arxiv.org/pdf/1603.01397v1.pdf
1602.08355v1,2016-02-25T18:24:56Z,2016-02-25 18:24:56,On short-term traffic flow forecasting and its reliability,"Recent advances in time series, where deterministic and stochastic modelings as well as the storage and analysis of big data are useless, permit a new approach to short-term traffic flow forecasting and to its reliability, i.e., to the traffic volatility. Several convincing computer simulations, which utilize concrete data, are presented and discussed.",Hassane Abouaïssa|Michel Fliess|Cédric Join,,https://arxiv.org/abs/1602.08355v1,https://arxiv.org/pdf/1602.08355v1,,"8th IFAC Conference on Manufacturing Modeling, Management & Control (Troyes, France, June 2016)",,,stat.AP,stat.AP|cs.OH,https://arxiv.org/pdf/1602.08355v1.pdf
1602.07290v1,2016-02-23T20:37:24Z,2016-02-23 20:37:24,Reliability estimates for three factor score predictors,"Estimates for the reliability of Thurstone's regression factor score predictor, Bartlett's factor score predictor, and McDonald's factor score predictor were proposed. As in Kuder-Richardson's formula, the reliability estimates are based on a hypothetical set of equivalent items. The reliability estimates were compared by means of simulation studies. Overall, the reliability estimates were largest for the regression score predictor, so that the reliability estimates for Bartlett's and McDonald's factor score predictor should be compared with the reliability of the regression score predictor, whenever Bartlett's or McDonald's factor score predictor are to be computed. An R-script and an SPSS-script for the computation of the respective reliability estimates is presented.",Andre Beauducel|Christopher Harms|Norbert Hilger,,https://arxiv.org/abs/1602.07290v1,https://arxiv.org/pdf/1602.07290v1,https://doi.org/10.5539/ijsp.v5n6p94,2 figures,,10.5539/ijsp.v5n6p94,stat.AP,stat.AP,https://arxiv.org/pdf/1602.07290v1.pdf
1602.05454v1,2016-02-17T15:48:52Z,2016-02-17 15:48:52,Some Reliability Properties of Transformed-Transformer Family of Distributions,"The Transformed-Transformer family of distributions are the resulting family of distributions as transformed from a random variable $T$ through another transformer random variable $X$ using a weight function $ω$ of the cumulative distribution function of $X$. In this paper, we study different stochastic ageing properties, as well as different stochastic orderings of this family of distributions. We discuss the results with several well known distributions.",Nil Kamal Hazra|Pradip Kundu|Asok K. Nanda,,https://arxiv.org/abs/1602.05454v1,https://arxiv.org/pdf/1602.05454v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1602.05454v1.pdf
1602.01650v1,2016-02-04T12:02:45Z,2016-02-04 12:02:45,Bayesian Nonparametric System Reliability using Sets of Priors,An imprecise Bayesian nonparametric approach to system reliability with multiple types of components is developed. This allows modelling partial or imperfect prior knowledge on component failure distributions in a flexible way through bounds on the functioning probability. Given component level test data these bounds are propagated to bounds on the posterior predictive distribution for the functioning probability of a new system containing components exchangeable with those used in testing. The method further enables identification of prior-data conflict at the system level based on component level test data. New results on first-order stochastic dominance for the Beta-Binomial distribution make the technique computationally tractable. Our methodological contributions can be immediately used in applications by reliability practitioners as we provide easy to use software tools.,Gero Walter|Louis J. M. Aslett|Frank P. A. Coolen,,https://arxiv.org/abs/1602.01650v1,https://arxiv.org/pdf/1602.01650v1,https://doi.org/10.1016/j.ijar.2016.08.005,,"International Journal of Approximate Reasoning (2017), 80, pp.67-88",10.1016/j.ijar.2016.08.005,stat.ME,stat.ME,https://arxiv.org/pdf/1602.01650v1.pdf
1602.00207v1,2016-01-31T06:39:07Z,2016-01-31 06:39:07,Binomial and Multinomial Proportions: Accurate Estimation and Reliable Assessment of Accuracy,"Misestimates of $σ_{P_o}$, the \emph{uncertainty} in $P_o$ from a 2-state Bayes equation used for binary classification, apparently arose from $\hatσ_{p_i}$, the uncertainty in underlying pdfs estimated from experimental $b$-bin histograms. To address this, several Bayesian estimator pairs $(\hat{p}_i, \hatσ_{p_i})$ were compared for agreement between nominal confidence level ($ξ$) and calculated coverage values ($C$). Large $ξ$-to-$C$ inconsistency for large $b$ and $ p_i \gg \frac{1}{b}$ arises for all multinomial estimators since priors downweight low likelihood, high $p_i$ values. To improve $ξ$-to-$C$ matching, $(ξ-C)^2$ was minimized against $α_0$ in a more general prior pdf ($\mathcal{B}[α_0,(b-1)α_0;x]$) to obtain $(\hat{p_i})_{ξ\leftrightarrow C}$. This improved matching for $b=2$, but for $b>2$, $ξ$-to-$C$ matching by $(\hat{p_i})_{ξ\leftrightarrow C}$ required an effective value ""$b=2$"" and renormalization, and this reduced $\hat{p}_i$-to-$p_i$ matching. Better $\hat{p}_i$-to-$p_i$ matching came from the original multinomial estimators, a new discrete-domain estimator $\hat{p}(n_i,N)$, or an earlier \emph{joint} estimator, $(\hat{p_i})_{\bowtie}$ that co-adjusted all estimates $p_i$ for James-Stein shrinkage to a mean vector. Best simultaneous $ξ$-to-$C$ and $\hat{p}_i$-to-$p_i$ matching came by \emph{de-noising} initial estimates of underlying pdfs. For $b=100$, $N<12800$, de-noised $\hat{p}$ needed $\approx 10\times$ fewer observations to achieve $\hat{p}_i$-to-$p_i$ matching equivalent to that found for $\hat{p}(n_i,N)$, $(\hat{p_i})_{\bowtie}$ or the original multinomial $\hat{p}_i$. De-noising each different type of initial estimate yielded similarly high accuracy in Monte-Carlo tests.",Jonathan Malcolm Friedman,,https://arxiv.org/abs/1602.00207v1,https://arxiv.org/pdf/1602.00207v1,,"61 pages, 24 figures; Small changes occurred (Figs 13-18, A1 & A2, Tables 1, S1) after fixing a slight bug in the the source code. For comparison, version (N-1) prior to fixing the bug is at: http://www.researchgate.net/profile/Jonathan_Friedman",,,stat.CO,stat.CO|stat.ME,https://arxiv.org/pdf/1602.00207v1.pdf
1601.08034v1,2016-01-29T10:29:40Z,2016-01-29 10:29:40,"The latent state hazard model, with application to wind turbine reliability","We present a new model for reliability analysis that is able to distinguish the latent internal vulnerability state of the equipment from the vulnerability caused by temporary external sources. Consider a wind farm where each turbine is running under the external effects of temperature, wind speed and direction, etc. The turbine might fail because of the external effects of a spike in temperature. If it does not fail during the temperature spike, it could still fail due to internal degradation, and the spike could cause (or be an indication of) this degradation. The ability to identify the underlying latent state can help better understand the effects of external sources and thus lead to more robust decision-making. We present an experimental study using SCADA sensor measurements from wind turbines in Italy.",Ramin Moghaddass|Cynthia Rudin,,https://arxiv.org/abs/1601.08034v1,https://arxiv.org/pdf/1601.08034v1,https://doi.org/10.1214/15-AOAS859,Published at http://dx.doi.org/10.1214/15-AOAS859 in the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2015, Vol. 9, No. 4, 1823-1863",10.1214/15-AOAS859,stat.AP,stat.AP,https://arxiv.org/pdf/1601.08034v1.pdf
1512.07060v2,2015-12-22T12:42:47Z,2016-05-03 14:32:06,Stochastic simulators based optimization by Gaussian process metamodels -- Application to maintenance investments planning issues,"This paper deals with the optimization of industrial asset management strategies, whose profitability is characterized by the Net Present Value (NPV) indicator which is assessed by a Monte Carlo simulator. The developed method consists in building a metamodel of this stochastic simulator, allowing to get, for a given model input, the NPV probability distribution without running the simulator. The present work is concentrated on the emulation of the quantile function of the stochastic simulator by interpolating well chosen basis functions and metamodeling their coefficients (using the Gaussian process metamodel). This quantile function metamodel is then used to treat a problem of strategy maintenance optimization (four systems installed on different plants), in order to optimize an NPV quantile. Using the Gaussian process framework, an adaptive design method (called QFEI) is defined by extending in our case the well known EGO algorithm. This allows to obtain an ""optimal"" solution using a small number of simulator runs.",Thomas Browne|Bertrand Iooss|Loïc Le Gratiet|Jérôme Lonchampt|Emmanuel Remy,"UPD5|IMT, GdR MASCOT-NUM|||",https://arxiv.org/abs/1512.07060v2,https://arxiv.org/pdf/1512.07060v2,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/1512.07060v2.pdf
1512.03036v1,2015-12-09T20:38:19Z,2015-12-09 20:38:19,Semi-parametric Models for Accelerated Destructive Degradation Test Data Analysis,"Accelerated destructive degradation tests (ADDT) are widely used in industry to evaluate materials' long term properties. Even though there has been tremendous statistical research in nonparametric methods, the current industrial practice is still to use application-specific parametric models to describe ADDT data. The challenge of using a nonparametric approach comes from the need to retain the physical meaning of degradation mechanisms and also perform extrapolation for predictions at the use condition. Motivated by this challenge, we propose a semi-parametric model to describe ADDT data. We use monotonic B-splines to model the degradation path, which not only provides flexible models with few assumptions, but also retains the physical meaning of degradation mechanisms (e.g., the degradation path is monotonically decreasing). Parametric models, such as the Arrhenius model, are used for modeling the relationship between the degradation and accelerating variable, allowing for extrapolation to the use conditions. We develop an efficient procedure to estimate model parameters. We also use simulation to validate the developed procedures and demonstrate the robustness of the semi-parametric model under model misspecification. Finally, the proposed method is illustrated by multiple industrial applications.",Yimeng Xie|Caleb B. King|Yili Hong|Qingyu Yang,,https://arxiv.org/abs/1512.03036v1,https://arxiv.org/pdf/1512.03036v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1512.03036v1.pdf
1508.01720v2,2015-08-07T15:16:39Z,2016-02-18 18:59:48,Mismatch in the Classification of Linear Subspaces: Sufficient Conditions for Reliable Classification,"This paper considers the classification of linear subspaces with mismatched classifiers. In particular, we assume a model where one observes signals in the presence of isotropic Gaussian noise and the distribution of the signals conditioned on a given class is Gaussian with a zero mean and a low-rank covariance matrix. We also assume that the classifier knows only a mismatched version of the parameters of input distribution in lieu of the true parameters. By constructing an asymptotic low-noise expansion of an upper bound to the error probability of such a mismatched classifier, we provide sufficient conditions for reliable classification in the low-noise regime that are able to sharply predict the absence of a classification error floor. Such conditions are a function of the geometry of the true signal distribution, the geometry of the mismatched signal distributions as well as the interplay between such geometries, namely, the principal angles and the overlap between the true and the mismatched signal subspaces. Numerical results demonstrate that our conditions for reliable classification can sharply predict the behavior of a mismatched classifier both with synthetic data and in a motion segmentation and a hand-written digit classification applications.",Jure Sokolic|Francesco Renna|Robert Calderbank|Miguel R. D. Rodrigues,,https://arxiv.org/abs/1508.01720v2,https://arxiv.org/pdf/1508.01720v2,https://doi.org/10.1109/TSP.2016.2537272,"17 pages, 7 figures, submitted to IEEE Transactions on Signal Processing",,10.1109/TSP.2016.2537272,cs.IT,cs.IT|cs.CV|stat.ML,https://arxiv.org/pdf/1508.01720v2.pdf
1507.06119v3,2015-07-22T10:17:17Z,2020-11-12 05:37:04,Reliability study of a coherent system with single general standby component,"The properties of a coherent system with a single general standby component is investigated. Here three different switch over viz. perfect switching, imperfect switching and random worm up period of the standby component are considered with some numerical examples.",Pradip Kundu|Nil Kamal Hazra|Asok K. Nanda,,https://arxiv.org/abs/1507.06119v3,https://arxiv.org/pdf/1507.06119v3,https://doi.org/10.1016/j.spl.2015.11.023,,Statistics & Probability Letters 110 (2016) 25-33,10.1016/j.spl.2015.11.023,stat.AP,stat.AP,https://arxiv.org/pdf/1507.06119v3.pdf
1507.04143v1,2015-07-15T09:52:00Z,2015-07-15 09:52:00,A Shock Model Based Approach to Network Reliability,"We consider a network consisting of $n$ components (links or nodes) and assume that the network has two states, up and down. We further suppose that the network is subject to shocks that appear according to a counting process and that each shock may lead to the component failures. Under some assumptions on the shock occurrences, we present a new variant of the notion of signature which we call it t-signature. Then t-signature based mixture representations for the reliability function of the network are obtained. Several stochastic properties of the network lifetime are investigated. In particular, under the assumption that the number of failures at each shock follows a binomial distribution and the process of shocks is non-homogeneous Poisson process, explicit form of the network reliability is derived and its aging properties are explored. Several examples are also provided",S. Zarezadeh|S. Ashrafi|M. Asadi,,https://arxiv.org/abs/1507.04143v1,https://arxiv.org/pdf/1507.04143v1,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1507.04143v1.pdf
1506.09060v1,2015-06-30T12:34:11Z,2015-06-30 12:34:11,Nonlinear Distortion Reduction in OFDM from Reliable Perturbations in Data Carriers,"A novel method for correcting the effect of nonlinear distortion in orthogonal frequency division multiplexing signals is proposed. The method depends on adaptively selecting the distortion over a subset of the data carriers, and then using tools from compressed sensing and sparse Bayesian recovery to estimate the distortion over the other carriers. Central to this method is the fact that carriers (or tones) are decoded with different levels of confidence, depending on a coupled function of the magnitude and phase of the distortion over each carrier, in addition to the respective channel strength. Moreover, as no pilots are required by this method, a significant improvement in terms of achievable rate can be achieved relative to previous work.",Ebrahim B. Al-Safadi|Tareq Y. Al-Naffouri|Mudassir Masood|Anum Ali,,https://arxiv.org/abs/1506.09060v1,https://arxiv.org/pdf/1506.09060v1,,"27 pages, 11 Figures",,,cs.IT,cs.IT|stat.AP,https://arxiv.org/pdf/1506.09060v1.pdf
1504.00865v2,2015-04-03T15:37:56Z,2016-02-15 16:54:24,A lower bound on the expected optimal value of certain random linear programs and application to shortest paths and reliability,"The paper studies the expectation of the inspection time in complex aging systems. Under reasonable assumptions, this problem is reduced to studying the expectation of the length of the shortest path in the directed degradation graph of the systems where the parameters are given by a pool of experts. The expectation itself being sometimes out of reach, in closed form or even through Monte Carlo simulations in the case of large systems, we propose an easily computable lower bound. The proposed bound applies to a rather general class of linear programs with random nonnegative costs and is directly inspired from the upper bound of Dyer, Frieze and McDiarmid [Math.Programming {\bf 35} (1986), no.1,3--16].",Stephane Chretien|Franck Corset,,https://arxiv.org/abs/1504.00865v2,https://arxiv.org/pdf/1504.00865v2,,,,,math.ST,math.ST|stat.ME,https://arxiv.org/pdf/1504.00865v2.pdf
1503.03297v2,2015-03-11T12:19:40Z,2015-03-12 13:03:38,"Uncertainty in Test Score Data and Classically Defined Reliability of Tests and Test Batteries, using a New Method for Test Dichotomisation","As with all measurements, the measurement of examinee ability, in terms of scores that the examinee obtains in a test, is also error-ridden. The quantification of such error or uncertainty in the test score data--or rather the complementary test reliability--is pursued within the paradigm of Classical Test Theory in a variety of ways, with no existing method of finding reliability, isomorphic to the theoretical definition that parametrises reliability as the ratio of the true score variance and observed (i.e. error-ridden) score variance. Thus, multiple reliability coefficients for the same test have been advanced. This paper describes a much needed method of obtaining reliability of a test as per its theoretical definition, via a single administration of the test, by using a new fast method of splitting of a given test into parallel halves, achieving near-coincident empirical distributions of the two halves. The method has the desirable property of achieving splitting on the basis of difficulty of the questions (or items) that constitute the test, thus allowing for fast computation of reliability even for very large test data sets, i.e. test data obtained by a very large examinee sample. An interval estimate for the true score is offered, given an examinee score, subsequent to the determination of the test reliability. This method of finding test reliability as per the classical definition can be extended to find reliability of a set or battery of tests; a method for determination of the weights implemented in the computation of the weighted battery score is discussed. We perform empirical illustration of our method on real and simulated tests, and on a real test battery comprising two constituent tests.",Satyendra Nath Chakrabartty|Kangrui Wang|Dalia Chakrabarty,,https://arxiv.org/abs/1503.03297v2,https://arxiv.org/pdf/1503.03297v2,,30 pages,,,stat.AP,stat.AP,https://arxiv.org/pdf/1503.03297v2.pdf
1502.04765v2,2015-02-17T01:46:59Z,2015-12-14 07:09:02,Reliable inference for complex models by discriminative composite likelihood estimation,"Composite likelihood estimation has an important role in the analysis of multivariate data for which the full likelihood function is intractable. An important issue in composite likelihood inference is the choice of the weights associated with lower-dimensional data sub-sets, since the presence of incompatible sub-models can deteriorate the accuracy of the resulting estimator. In this paper, we introduce a new approach for simultaneous parameter estimation by tilting, or re-weighting, each sub-likelihood component called discriminative composite likelihood estimation (D-McLE). The data-adaptive weights maximize the composite likelihood function, subject to moving a given distance from uniform weights; then, the resulting weights can be used to rank lower-dimensional likelihoods in terms of their influence in the composite likelihood function. Our analytical findings and numerical examples support the stability of the resulting estimator compared to estimators constructed using standard composition strategies based on uniform weights. The properties of the new method are illustrated through simulated data and real spatial data on multivariate precipitation extremes.",Davide Ferrari|Chao Zheng,,https://arxiv.org/abs/1502.04765v2,https://arxiv.org/pdf/1502.04765v2,,"29 pages, 4 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/1502.04765v2.pdf
1501.05072v1,2015-01-21T06:49:52Z,2015-01-21 06:49:52,Estimation of Reliability in the Two-Parameter Geometric Distribution,"In this article, the reliabilities $R(t)=P(X\geq t)$, when $X$ follows two-parameter geometric distribution and $R=P(X\leq Y)$, arises under stress-strength setup, when X and Y assumed to follow two-parameter geometric independently have been found out. Maximum Likelihood Estimator (MLE) and an Unbiased Estimator (UE) of these have been derived. MLE and UE of the reliability of k-out-of-m system have also been derived. The estimators have been compared through simulation study.",Sudhansu S. Maiti|Sudhir Murmu|G. Chattopadhyay,,https://arxiv.org/abs/1501.05072v1,https://arxiv.org/pdf/1501.05072v1,,"31 pages, 1 figure",,,stat.AP,stat.AP,https://arxiv.org/pdf/1501.05072v1.pdf
1501.04070v1,2015-01-16T18:01:31Z,2015-01-16 18:01:31,An Information-Theoretic Alternative to the Cronbach's Alpha Coefficient of Item Reliability,"We propose an information-theoretic alternative to the popular Cronbach alpha coefficient of reliability. Particularly suitable for contexts in which instruments are scored on a strictly nonnumeric scale, our proposed index is based on functions of the entropy of the distributions of defined on the sample space of responses. Our reliability index tracks the Cronbach alpha coefficient uniformly while offering several other advantages discussed in great details in this paper.",Ernest Fokoue|Necla Gunduz,,https://arxiv.org/abs/1501.04070v1,https://arxiv.org/pdf/1501.04070v1,,"8 pages, 2 tables, 1 figure",,,math.ST,math.ST|stat.ME,https://arxiv.org/pdf/1501.04070v1.pdf
1412.6921v2,2014-12-22T10:55:20Z,2015-01-07 11:27:15,Combination Mean Residual Life Order with Reliability Applications,"The purpose of this paper is to introduce, study and analyze a new stochastic order which lies in the framework of the mean residual life and the combination convexity orders. Several preservation properties of the new order under reliability operations of monotone transformation, mixture, weighted distributions and shock models are discussed. In addition, two characterization properties of the new order based on the concept of residual life at random time and the concept of excess lifetime in renewal processes are given. Finally, we highlight some new applications of this order in the context of reliability and survival analysis.",M. Kayid|S. Izadkhah|H. Alhalees,,https://arxiv.org/abs/1412.6921v2,https://arxiv.org/pdf/1412.6921v2,,This paper has been withdrawn by the author due to a crucial sign error in equation 1,,,stat.ME,stat.ME,https://arxiv.org/pdf/1412.6921v2.pdf
1412.4260v2,2014-12-13T17:25:47Z,2022-03-21 14:42:19,A Bayesian Nonparametric System Reliability Model which Integrates Multiple Sources of Lifetime Information,"We present a Bayesian nonparametric system reliability model which scales well and provides a great deal of flexibility in modeling. The Bayesian approach naturally handles the disparate amounts of component and subsystem data that may exist. However, traditional Bayesian reliability models are quite computationally complex, relying on MCMC techniques. Our approach utilizes the conjugate properties of the beta-Stacy process, which is the fundamental building block of our model. These individual models are linked together using a method of moments estimation approach. This model is computationally fast, allows for right-censored data, and is used for estimating and predicting system reliability.",Richard L. Warr|Jeremy M. Meyer|Jackson T. Curtis,,https://arxiv.org/abs/1412.4260v2,https://arxiv.org/pdf/1412.4260v2,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1412.4260v2.pdf
1412.1315v1,2014-12-03T13:22:18Z,2014-12-03 13:22:18,Degradation-based residual life prediction under different environments,"Degradation modeling has traditionally relied on historical signals to estimate the behavior of the underlying degradation process. Many models assume that these historical signals are acquired under the same environmental conditions and can be observed along the entire lifespan of a component. In this paper, we relax these assumptions and present a more general statistical framework for modeling degradation signals that may have been collected under different types of environmental conditions. In addition, we consider applications where the historical signals are not necessarily observed continuously, that is, historical signals are sparse or fragmented. We consider the case where historical degradation signals are collected under known environmental states and another case where the environmental conditions are unknown during the acquisition of these historical data. For the first case, we use a classification algorithm to identify the environmental state of the units operating in the field. In the second case, a clustering step is required for clustering the historical degradation signals. The proposed model can provide accurate predictions of the lifetime or residual life distributions of engineering components that are still operated in the field. This is demonstrated by using simulated degradation signals as well as vibration-based degradation signals acquired from a rotating machinery setup.",Rensheng Zhou|Nicoleta Serban|Nagi Gebraeel,,https://arxiv.org/abs/1412.1315v1,https://arxiv.org/pdf/1412.1315v1,https://doi.org/10.1214/14-AOAS749,Published in at http://dx.doi.org/10.1214/14-AOAS749 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2014, Vol. 8, No. 3, 1671-1689",10.1214/14-AOAS749,stat.AP,stat.AP,https://arxiv.org/pdf/1412.1315v1.pdf
1411.0408v1,2014-11-03T10:06:07Z,2014-11-03 10:06:07,On the practical interest of discrete Inverse Polya and Weibull-1 models in industrial reliability studies,"Engineers often cope with the problem of assessing the lifetime of industrial components, under the basis of observed industrial feedback data. Usually, lifetime is modelled as a continuous random variable, for instance exponentially or Weibull distributed. However, in some cases, the features of the piece of equipment under investigation rather suggest the use of discrete probabilistic models. This happens for an equipment which only operates on cycles or on demand. In these cases, the lifetime is rather measured in number of cycles or number of demands before failure, therefore, in theory, discrete models should be more appropriate. This article aims at bringing some light to the practical interest for the reliability engineer in using two discrete models among the most popular: the Inverse Polya distribution (IPD), based on a Polya urn scheme, and the so-called Weibull-1 (W1) model. It is showed that, for different reasons, the practical use of both models should be restricted to specific industrial situations. In particular, when nothing is a priori known over the nature of ageing and/or data are heavily right-censored, they can remain of limited interest with respect to more flexible continuous lifetime models such as the usual Weibull distribution. Nonetheless, the intuitive meaning given to the IPD distribution favors its use by engineers in low (decelerated) ageing situations.",Alberto Pasanisi|Côme Roero|Nicolas Bousquet|Emmanuel Remy,,https://arxiv.org/abs/1411.0408v1,https://arxiv.org/pdf/1411.0408v1,,"18 pages, 7 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/1411.0408v1.pdf
1409.5450v2,2014-09-18T20:13:21Z,2015-10-28 18:56:02,Improving Reliability of Subject-Level Resting-State fMRI Parcellation with Shrinkage Estimators,"A recent interest in resting state functional magnetic resonance imaging (rsfMRI) lies in subdividing the human brain into anatomically and functionally distinct regions of interest. For example, brain parcellation is often used for defining the network nodes in connectivity studies. While inference has traditionally been performed on group-level data, there is a growing interest in parcellating single subject data. However, this is difficult due to the low signal-to-noise ratio of rsfMRI data, combined with typically short scan lengths. A large number of brain parcellation approaches employ clustering, which begins with a measure of similarity or distance between voxels. The goal of this work is to improve the reproducibility of single-subject parcellation using shrinkage estimators of such measures, allowing the noisy subject-specific estimator to ""borrow strength"" in a principled manner from a larger population of subjects. We present several empirical Bayes shrinkage estimators and outline methods for shrinkage when multiple scans are not available for each subject. We perform shrinkage on raw intervoxel correlation estimates and use both raw and shrinkage estimates to produce parcellations by performing clustering on the voxels. Our proposed method is agnostic to the choice of clustering method and can be used as a pre-processing step for any clustering algorithm. Using two datasets---a simulated dataset where the true parcellation is known and is subject-specific and a test-retest dataset consisting of two 7-minute rsfMRI scans from 20 subjects---we show that parcellations produced from shrinkage correlation estimates have higher reliability and validity than those produced from raw estimates. Application to test-retest data shows that using shrinkage estimators increases the reproducibility of subject-specific parcellations of the motor cortex by up to 30%.",Amanda F. Mejia|Mary Beth Nebel|Haochang Shou|Ciprian M. Crainiceanu|James J. Pekar|Stewart Mostofsky|Brian Caffo|Martin A. Lindquist,,https://arxiv.org/abs/1409.5450v2,https://arxiv.org/pdf/1409.5450v2,,"body 21 pages, 11 figures",,,stat.ME,stat.ME,https://arxiv.org/pdf/1409.5450v2.pdf
1406.6288v3,2014-06-24T16:03:32Z,2015-09-02 15:19:41,Reliable ABC model choice via random forests,"Approximate Bayesian computation (ABC) methods provide an elaborate approach to Bayesian inference on complex models, including model choice. Both theoretical arguments and simulation experiments indicate, however, that model posterior probabilities may be poorly evaluated by standard ABC techniques. We propose a novel approach based on a machine learning tool named random forests to conduct selection among the highly complex models covered by ABC algorithms. We thus modify the way Bayesian model selection is both understood and operated, in that we rephrase the inferential goal as a classification problem, first predicting the model that best fits the data with random forests and postponing the approximation of the posterior probability of the predicted MAP for a second stage also relying on random forests. Compared with earlier implementations of ABC model choice, the ABC random forest approach offers several potential improvements: (i) it often has a larger discriminative power among the competing models, (ii) it is more robust against the number and choice of statistics summarizing the data, (iii) the computing effort is drastically reduced (with a gain in computation efficiency of at least fifty), and (iv) it includes an approximation of the posterior probability of the selected model. The call to random forests will undoubtedly extend the range of size of datasets and complexity of models that ABC can handle. We illustrate the power of this novel methodology by analyzing controlled experiments as well as genuine population genetics datasets. The proposed methodologies are implemented in the R package abcrf available on the CRAN.",Pierre Pudlo|Jean-Michel Marin|Arnaud Estoup|Jean-Marie Cornuet|Mathieu Gautier|Christian P. Robert,"IMAG and IBC, Universite de Montpellier|IMAG and IBC, Universite de Montpellier|CBGP, INRA, Montpellier|CBGP, INRA, Montpellier|CBGP, INRA, Montpellier|Universite Paris-Dauphine and University of Warwick",https://arxiv.org/abs/1406.6288v3,https://arxiv.org/pdf/1406.6288v3,,"39 pages, 15 figures, 6 tables",,,stat.ML,stat.ML|q-bio.PE|stat.CO|stat.ME,https://arxiv.org/pdf/1406.6288v3.pdf
1405.6268v1,2014-05-24T05:08:09Z,2014-05-24 05:08:09,The inverse Lindley distribution: A stress-strength reliability model,"In this article, we proposed an inverse Lindley distribution and studied its fundamental properties such as quantiles, mode, stochastic ordering and entropy measure. The proposed distribution is observed to be a heavy-tailed distribution and has a upside-down bathtub shape for its failure rate. Further, we proposed its applicability as a stress-strength reliability model for survival data analysis. The estimation of stress-strength parameters and $R=P[X>Y]$, the stress-strength reliability has been approached by both classical and Bayesian paradigms. Under Bayesian set-up, non-informative (Jeffrey) and informative (gamma) priors are considered under a symmetric (squared error) and a asymmetric (entropy) loss functions, and a Lindley-approximation technique is used for Bayesian computation. The proposed estimators are compared in terms of their mean squared errors through a simulation study. Two real data sets representing survival of Head and Neck cancer patients are fitted using the inverse Lindley distribution and used to estimate the stress-strength parameters and reliability.",Vikas Kumar Sharma|Sanjay Kumar Singh|Umesh Singh|Varun Agiwal,,https://arxiv.org/abs/1405.6268v1,https://arxiv.org/pdf/1405.6268v1,,"17 pages, 4 figures, 9 tables",,,stat.AP,stat.AP,https://arxiv.org/pdf/1405.6268v1.pdf
1404.3806v3,2014-04-15T03:23:54Z,2014-12-17 12:01:00,Optimal design for step-stress accelerated test with random discrete stress elevating times based on gamma degradation process,"Recently, a step-stress accelerated degradation test (SSADT) plan, in which the stress level is elevated when the degradation value of a product crosses a pre-specified value, was proposed. The times of stress level elevating are random and vary from product to product. In this paper we extend this model to a more economic plan. The proposed extended model has two economical advantages compared with the previous one. The first is that the times of stress level elevating in the new model are identical for all products, which enable us to use only one chamber (oven) for testing all test units. The second is that, the new method does not require continuous inspection and to elevate the stress level, it is not necessary for the experimenter to inspect the value of the degradation continually. The new method decrease the cost of measurement and also there is no need to use electronic sensors to detect the first passage time of the degradation to the threshold value in the new method. We assume that the degradation path follows a gamma process. The stress level is elevated as soon as the measurement of the degradation of one of the test units, at one of the specified times, exceeds the threshold value. Under the constraint that the total experimental cost does not exceed a pre-specified budget, the optimal settings including the optimal threshold value, sample size, measurement frequency and termination time are obtained by minimizing the asymptotic variance of an estimated quantile of the lifetime distribution of the product. A case study is presented to illustrate the proposed method.",Morteza Amini|Soudabeh Shemehsavar|Zhengqiang Pan,,https://arxiv.org/abs/1404.3806v3,https://arxiv.org/pdf/1404.3806v3,,,,,stat.AP,stat.AP,https://arxiv.org/pdf/1404.3806v3.pdf
1304.6643v2,2013-04-23T17:42:41Z,2013-05-12 20:46:08,Resampling Approach to the Estimation of the Aircraft Circulation Plan Reliability,"The paper illustrates an application of the Resampling approach [2] for the estimation of the aircraft circulation plan reliability. Resampling is an intensive computer statistical method, which can be used effectively in the case of small samples. Algorithm of the Resampling method for the given task is illustrated and variance of obtained estimators is calculated, which is the measure of the method effectiveness.",Maxim Fioshin,,https://arxiv.org/abs/1304.6643v2,https://arxiv.org/pdf/1304.6643v2,,,"In Proc. of the 5-th International Conference ""Transport Systems Telematics"", Katowice-Ustron, Poland, 2005, pp. 13-21",,stat.AP,stat.AP,https://arxiv.org/pdf/1304.6643v2.pdf
1304.6670v1,2013-04-23T17:37:15Z,2013-04-23 17:37:15,Resampling Approach to the Estimation of Reliability Systems,"The article is devoted to the resampling approach application to the reliability problems. This approach to reliability problems was first proposed by Ivnitsky (1967). Resampling is intensive statistical computer method, which is non-parametrical, that uses initial samples data in different combinations to simulate the process many times and get finally the estimator of the characteristics of interest. At the present paper simple resampling, hierarchical resampling, the case of one sample for several variables, the case of partially known distributions, analysis of degradation flow, analysis of degradation-renewal process, construction of confidence intervals are described. All those resampling application cases can be applied successfully to solve the reliability problems as an alternative to classical methods.",Maxim Fioshin|Helen Fioshina,,https://arxiv.org/abs/1304.6670v1,https://arxiv.org/pdf/1304.6670v1,,,"Proc. of the International Conference ""Mathematical Methods in Reliability"", Glasgow, UK, 2007, CD Proc",,stat.AP,stat.AP,https://arxiv.org/pdf/1304.6670v1.pdf
1302.3053v1,2013-02-13T11:21:42Z,2013-02-13 11:21:42,Reliability estimators for the components of series and parallel systems: The Weibull model,"This paper presents a hierarchical Bayesian approach to the estimation of components' reliability (survival) using a Weibull model for each of them. The proposed method can be used to estimation with general survival censored data, because the estimation of a component's reliability in a series (parallel) system is equivalent to the estimation of its survival function with right- (left-) censored data. Besides the Weibull parametric model for reliability data, independent gamma distributions are considered at the first hierarchical level for the Weibull parameters and independent uniform distributions over the real line as priors for the parameters of the gammas. In order to evaluate the model, an example and a simulation study are discussed.",Felipe L. Bhering|Carlos A. de B. Pereira|Adriano Polpo,,https://arxiv.org/abs/1302.3053v1,https://arxiv.org/pdf/1302.3053v1,https://doi.org/10.4236/am.2014.511157,,"Applied Mathematics, v. 05, p. 1633-1640, 2014",10.4236/am.2014.511157,stat.ME,stat.ME,https://arxiv.org/pdf/1302.3053v1.pdf
1212.2358v1,2012-12-11T09:56:34Z,2012-12-11 09:56:34,Hidden Markov Model for the detection of a degraded operating mode of optronic equipment,"As part of optimizing the reliability, Thales Optronics now includes systems that examine the state of its equipment. The aim of this paper is to use hidden Markov Model to detect as soon as possible a change of state of optronic equipment in order to propose maintenance before failure. For this, we carefully observe the dynamic of a variable called ""cool down time"" and noted Tmf, which reflects the state of the cooling system. Indeed, the Tmf is an indirect observation of the hidden state of the system. This one is modelled by a Markov chain and the Tmf is a noisy function of it. Thanks to filtering equations, we obtain results on the probability that an appliance is in degraded state at time $t$, knowing the history of the Tmf until this moment. We have evaluated the numerical behavior of our approach on simulated data. Then we have applied this methodology on our real data and we have checked that the results are consistent with the reality. This method can be implemented in a HUMS (Health and Usage Monitoring System). This simple example of HUMS would allow the Thales Optronics Company to improve its maintenance system. This company will be able to recall appliances which are estimated to be in degraded state and do not control to soon those estimated in stable state.",Camille Baysse|Didier Bihannic|Anne Gégout-Petit|Michel Prenat|Jérôme Saracco,,https://arxiv.org/abs/1212.2358v1,https://arxiv.org/pdf/1212.2358v1,,"11 pages, 9 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/1212.2358v1.pdf
1210.5095v1,2012-10-18T11:20:05Z,2012-10-18 11:20:05,Application of Bayesian Methods for Age-dependent Reliability Analysis,"In this paper authors present a general methodology for age dependent reliability analysis of degrading or ageing systems, structures and components.The methodology is based on Bayesian methods and inference, its ability to incorporate prior information and on idea that ageing can be thought as age dependent change of believes about reliability parameters, when change of belief occurs not just due to new failure data or other information which becomes available in time, but also it continuously changes due to flow of time and beliefs evolution. The main objective of this paper is to present the clear way of how Bayesian methods can be applied by practitioners to deal with risk and reliability analysis considering ageing phenomena. The methodology describes step by step failure rate analysis of ageing systems: from the Bayesian model building to its verification and generalization with Bayesian model averaging which, as authors suggest in this paper, could serve as alternative for various goodness of fit assessment tools and as universal tool to cope with various sources of uncertainty.",Robertas Alzbutas|Tomas Iešmantas,,https://arxiv.org/abs/1210.5095v1,https://arxiv.org/pdf/1210.5095v1,,Accepted draft for publication in Quality and Reliability Engineering International,,,stat.AP,stat.AP,https://arxiv.org/pdf/1210.5095v1.pdf
1205.1971v2,2012-05-09T13:07:47Z,2012-10-16 19:16:02,Linked Ego Networks: Improving Estimate Reliability and Validity with Respondent-driven Sampling,"Respondent-driven sampling (RDS) is currently widely used for the study of HIV/AIDS-related high risk populations. However, recent studies have shown that traditional RDS methods are likely to generate large variances and may be severely biased since the assumptions behind RDS are seldom fully met in real life. To improve estimation in RDS studies, we propose a new method to generate estimates with ego network data, which is collected by asking RDS respondents about the composition of their personal networks, such as ""what proportion of your friends are married?"". By simulations on an extracted real-world social network of gay men as well as on artificial networks with varying structural properties, we show that the new estimator, RDSI^{ego} shows superior performance over traditional RDS estimators. Importantly, RDSI^{ego} exhibits strong robustness to the preference of peer recruitment and variations in network structural properties, such as homophily, activity ratio, and community structure. While the biases of traditional RDS estimators can sometimes be as large as 10%~20%, biases of all RDSI^{ego} estimates are well restrained to be less than 2%. The positive results henceforth encourage researchers to collect ego network data for variables of interests by RDS, for both hard-to-access populations and general populations when random sampling is not applicable. The limitation of RDSI^{ego} is evaluated by simulating RDS assuming different level of reporting error.",Xin Lu,,https://arxiv.org/abs/1205.1971v2,https://arxiv.org/pdf/1205.1971v2,,"22 pages, 16 figures, 3 tables",,,stat.ME,stat.ME|physics.data-an,https://arxiv.org/pdf/1205.1971v2.pdf
1204.5963v2,2012-04-26T15:49:24Z,2013-06-26 22:34:16,On a Reliable Peer-Review Process,"We propose an enhanced peer-review process where the reviewers are encouraged to truthfully disclose their reviews. We start by modelling that process using a Bayesian model where the uncertainty regarding the quality of the manuscript is taken into account. After that, we introduce a scoring function to evaluate the reported reviews. Under mild assumptions, we show that reviewers strictly maximize their expected scores by telling the truth. We also show how those scores can be used in order to reach consensus.",Arthur Carvalho|Kate Larson,,https://arxiv.org/abs/1204.5963v2,https://arxiv.org/pdf/1204.5963v2,,This paper has been withdrawn by the author due to some errors in the basic model,,,cs.GT,cs.GT|stat.AP|stat.OT,https://arxiv.org/pdf/1204.5963v2.pdf
1204.0549v1,2012-04-02T22:51:25Z,2012-04-02 22:51:25,Bayesian sequential estimation of the reliability of a parallel-series system,"We give a risk-averse solution to the problem of estimating the reliability of a parallel-series system. We adopt a beta-binomial model for components reliabilities, and assume that the total sample size for the experience is fixed. The allocation at subsystems or components level may be random. Based on the sampling schemes for parallel and series systems separately, we propose a hybrid sequential scheme for the parallel-series system. Asymptotic optimality of the Bayes risk associated with quadratic loss is proved with the help of martingale convergence properties.",Zohra Benkamra|Mekki Terbeche|Mounir Tlemcani,,https://arxiv.org/abs/1204.0549v1,https://arxiv.org/pdf/1204.0549v1,https://doi.org/10.1016/j.amc.2013.05.010,12 pages,"Applied Mathematics and Computation 2013, Volume 219, Issue 23, 1 August 2013, Pages 10842--10852",10.1016/j.amc.2013.05.010,stat.AP,stat.AP,https://arxiv.org/pdf/1204.0549v1.pdf
1203.5986v1,2012-03-27T14:50:56Z,2012-03-27 14:50:56,Bayesian Network Enhanced with Structural Reliability Methods: Methodology,"We combine Bayesian networks (BNs) and structural reliability methods (SRMs) to create a new computational framework, termed enhanced Bayesian network (eBN), for reliability and risk analysis of engineering structures and infrastructure. BNs are efficient in representing and evaluating complex probabilistic dependence structures, as present in infrastructure and structural systems, and they facilitate Bayesian updating of the model when new information becomes available. On the other hand, SRMs enable accurate assessment of probabilities of rare events represented by computationally demanding, physically-based models. By combining the two methods, the eBN framework provides a unified and powerful tool for efficiently computing probabilities of rare events in complex structural and infrastructure systems in which information evolves in time. Strategies for modeling and efficiently analyzing the eBN are described by way of several conceptual examples. The companion paper applies the eBN methodology to example structural and infrastructure systems.",Daniel Straub|Armen Der Kiureghian,,https://arxiv.org/abs/1203.5986v1,https://arxiv.org/pdf/1203.5986v1,https://doi.org/10.1061/(ASCE)EM.1943-7889.0000173,,"Journal of Engineering Mechanics, Trans. ASCE, 2010, 136(10): 1248-1258",10.1061/(ASCE)EM.1943-7889.0000173,stat.AP,stat.AP|stat.ME|stat.ML,https://arxiv.org/pdf/1203.5986v1.pdf
1203.5985v1,2012-03-27T14:50:45Z,2012-03-27 14:50:45,Bayesian Network Enhanced with Structural Reliability Methods: Application,"The enhanced Bayesian network (eBN) methodology described in the companion paper facilitates the assessment of reliability and risk of engineering systems when information about the system evolves in time. We present the application of the eBN (a) to the assessment of the life-cycle reliability of a structural system, (b) to the optimization of a decision on performing measurements in that structural system, and (c) to the risk assessment of an infrastructure system subject to natural hazards and deterioration of constituent structures. In all applications, observations of system performances or the hazards are made at various points in time and the eBN efficiently includes these observations in the analysis to provide an updated probabilistic model of the system at all times.",Daniel Straub|Armen Der Kiureghian,,https://arxiv.org/abs/1203.5985v1,https://arxiv.org/pdf/1203.5985v1,https://doi.org/10.1061/(ASCE)EM.1943-7889.0000170,,"Journal of Engineering Mechanics, Trans. ASCE, 2010, 136(10): 1259-1270",10.1061/(ASCE)EM.1943-7889.0000170,stat.AP,stat.AP,https://arxiv.org/pdf/1203.5985v1.pdf
1203.5405v1,2012-03-24T11:06:22Z,2012-03-24 11:06:22,Reliability updating with equality information,"In many instances, information on engineering systems can be obtained through measurements, monitoring or direct observations of system performances and can be used to update the system reliability estimate. In structural reliability analysis, such information is expressed either by inequalities (e.g. for the observation that no defect is present) or by equalities (e.g. for quantitative measurements of system characteristics). When information Z is of the equality type, the a-priori probability of Z is zero and most structural reliability methods (SRM) are not directly applicable to the computation of the updated reliability. Hitherto, the computation of the reliability of engineering systems conditional on equality information was performed through first- and second order approximations. In this paper, it is shown how equality information can be transformed into inequality information, which enables reliability updating by solving a standard structural system reliability problem. This approach enables the use of any SRM, including those based on simulation, for reliability updating with equality information. It is demonstrated on three numerical examples, including an application to fatigue reliability.",Daniel Straub,,https://arxiv.org/abs/1203.5405v1,https://arxiv.org/pdf/1203.5405v1,https://doi.org/10.1016/j.probengmech.2010.08.003,,"Probabilistic Engineering Mechanics, 2011, 26(2): 254-258",10.1016/j.probengmech.2010.08.003,stat.ME,stat.ME|stat.AP|stat.ML,https://arxiv.org/pdf/1203.5405v1.pdf
1203.2062v1,2012-03-09T12:49:35Z,2012-03-09 12:49:35,Meta-models for structural reliability and uncertainty quantification,"A meta-model (or a surrogate model) is the modern name for what was traditionally called a response surface. It is intended to mimic the behaviour of a computational model M (e.g. a finite element model in mechanics) while being inexpensive to evaluate, in contrast to the original model which may take hours or even days of computer processing time. In this paper various types of meta-models that have been used in the last decade in the context of structural reliability are reviewed. More specifically classical polynomial response surfaces, polynomial chaos expansions and kriging are addressed. It is shown how the need for error estimates and adaptivity in their construction has brought this type of approaches to a high level of efficiency. A new technique that solves the problem of the potential biasedness in the estimation of a probability of failure through the use of meta-models is finally presented.",Bruno Sudret,,https://arxiv.org/abs/1203.2062v1,https://arxiv.org/pdf/1203.2062v1,,"Keynote lecture Fifth Asian-Pacific Symposium on Structural Reliability and its Applications (5th APSSRA) May 2012, Singapore",,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/1203.2062v1.pdf
1202.5334v4,2012-02-23T21:52:57Z,2012-12-24 23:09:29,An allocation scheme for estimating the reliability of a parallel-series system,"We give a hybrid two stage design which can be useful to estimate the reliability of a parallel-series and/or by duality a series-parallel system, when the component reliabilities are unknown as well as the total numbers of units allowed to be tested in each subsystem. When a total sample size is fixed large, asymptotic optimality is proved systematically and validated via Monte Carlo simulation.",Zohra Benkamra|Mekki Terbeche|Mounir Tlemcani,,https://arxiv.org/abs/1202.5334v4,https://arxiv.org/pdf/1202.5334v4,https://doi.org/10.1155/2012/289035,"16 pages, 4 figures","Advances in Decision Sciences Volume 2012 (2012), Article ID 289035",10.1155/2012/289035,stat.AP,stat.AP,https://arxiv.org/pdf/1202.5334v4.pdf
1201.3935v2,2012-01-18T21:51:41Z,2012-04-24 16:06:39,Reliability-based design optimization of imperfect shells using adaptive kriging meta-models,"The optimal and robust design of structures has gained much attention in the past ten years due to the ever increasing need for manufacturers to build robust systems at the lowest cost. Reliability-based design optimization (RBDO) allows the analyst to minimize some cost function while ensuring some minimal performances cast as admissible probabilities of failure for a set of performance functions. In order to address real-world problems in which the performance is assessed through computational models (e.g. large scale finite element models) meta-modelling techniques have been developed in the past decade. This paper introduces adaptive kriging surrogate models to solve the RBDO problem. The latter is cast in an augmented space that ""sums up"" the range of the design space and the aleatory uncertainty in the design parameters and the environmental conditions. Thus the surrogate model is used (i) for evaluating robust estimates of the probabilities of failure (and for enhancing the computational experimental design by adaptive sampling) in order to achieve the requested accuracy and (ii) for applying the gradient-based optimization algorithm. The approach is applied to the optimal design of imperfect stiffened cylinder shells used in submarine engineering. For this application the performance of the structure is related to buckling which is addressed here by means of the asymptotic numerical method.",Vincent Dubourg|Jean-Marc Bourinet|Bruno Sudret,,https://arxiv.org/abs/1201.3935v2,https://arxiv.org/pdf/1201.3935v2,,This paper has been withdrawn by the author in favor of arXiv:	1104.3479,,,stat.AP,stat.AP,https://arxiv.org/pdf/1201.3935v2.pdf
1110.4198v3,2011-10-19T07:34:19Z,2013-07-12 03:28:17,A Reliable Effective Terascale Linear Learning System,"We present a system and a set of techniques for learning linear predictors with convex losses on terascale datasets, with trillions of features, {The number of features here refers to the number of non-zero entries in the data matrix.} billions of training examples and millions of parameters in an hour using a cluster of 1000 machines. Individually none of the component techniques are new, but the careful synthesis required to obtain an efficient implementation is. The result is, up to our knowledge, the most scalable and efficient linear learning system reported in the literature (as of 2011 when our experiments were conducted). We describe and thoroughly evaluate the components of the system, showing the importance of the various design choices.",Alekh Agarwal|Olivier Chapelle|Miroslav Dudik|John Langford,,https://arxiv.org/abs/1110.4198v3,https://arxiv.org/pdf/1110.4198v3,,,,,cs.LG,cs.LG|stat.ML,https://arxiv.org/pdf/1110.4198v3.pdf
1110.3564v4,2011-10-17T02:52:20Z,2013-03-26 07:28:04,Budget-Optimal Task Allocation for Reliable Crowdsourcing Systems,"Crowdsourcing systems, in which numerous tasks are electronically distributed to numerous ""information piece-workers"", have emerged as an effective paradigm for human-powered solving of large scale problems in domains such as image classification, data entry, optical character recognition, recommendation, and proofreading. Because these low-paid workers can be unreliable, nearly all such systems must devise schemes to increase confidence in their answers, typically by assigning each task multiple times and combining the answers in an appropriate manner, e.g. majority voting.
  In this paper, we consider a general model of such crowdsourcing tasks and pose the problem of minimizing the total price (i.e., number of task assignments) that must be paid to achieve a target overall reliability. We give a new algorithm for deciding which tasks to assign to which workers and for inferring correct answers from the workers' answers. We show that our algorithm, inspired by belief propagation and low-rank matrix approximation, significantly outperforms majority voting and, in fact, is optimal through comparison to an oracle that knows the reliability of every worker. Further, we compare our approach with a more general class of algorithms which can dynamically assign tasks. By adaptively deciding which questions to ask to the next arriving worker, one might hope to reduce uncertainty more efficiently. We show that, perhaps surprisingly, the minimum price necessary to achieve a target reliability scales in the same manner under both adaptive and non-adaptive scenarios. Hence, our non-adaptive approach is order-optimal under both scenarios. This strongly relies on the fact that workers are fleeting and can not be exploited. Therefore, architecturally, our results suggest that building a reliable worker-reputation system is essential to fully harnessing the potential of adaptive designs.",David R. Karger|Sewoong Oh|Devavrat Shah,,https://arxiv.org/abs/1110.3564v4,https://arxiv.org/pdf/1110.3564v4,,"38 pages, 4 figure",,,cs.LG,cs.LG|cs.DS|cs.HC|stat.ML,https://arxiv.org/pdf/1110.3564v4.pdf
1108.5185v1,2011-08-25T20:07:08Z,2011-08-25 20:07:08,Function Based Nonlinear Least Squares and Application to Jelinski--Moranda Software Reliability Model,"A function based nonlinear least squares estimation (FNLSE) method is proposed and investigated in parameter estimation of Jelinski-Moranda software reliability model. FNLSE extends the potential fitting functions of traditional least squares estimation (LSE), and takes the logarithm transformed nonlinear least squares estimation (LogLSE) as a special case. A novel power transformation function based nonlinear least squares estimation (powLSE) is proposed and applied to the parameter estimation of Jelinski-Moranda model. Solved with Newton-Raphson method, Both LogLSE and powLSE of Jelinski-Moranda models are applied to the mean time between failures (MTBF) predications on six standard software failure time data sets. The experimental results demonstrate the effectiveness of powLSE with optimal power index compared to the classical least--squares estimation (LSE), maximum likelihood estimation (MLE) and LogLSE in terms of recursively relative error (RE) index and Braun statistic index.",Jingwei Liu|Meizhi Xu,,https://arxiv.org/abs/1108.5185v1,https://arxiv.org/pdf/1108.5185v1,,,,,stat.ME,stat.ME|cs.SE,https://arxiv.org/pdf/1108.5185v1.pdf
1107.5712v1,2011-07-28T13:26:29Z,2011-07-28 13:26:29,Degradation modeling applied to residual lifetime prediction using functional data analysis,"Sensor-based degradation signals measure the accumulation of damage of an engineering system using sensor technology. Degradation signals can be used to estimate, for example, the distribution of the remaining life of partially degraded systems and/or their components. In this paper we present a nonparametric degradation modeling framework for making inference on the evolution of degradation signals that are observed sparsely or over short intervals of times. Furthermore, an empirical Bayes approach is used to update the stochastic parameters of the degradation model in real-time using training degradation signals for online monitoring of components operating in the field. The primary application of this Bayesian framework is updating the residual lifetime up to a degradation threshold of partially degraded components. We validate our degradation modeling approach using a real-world crack growth data set as well as a case study of simulated degradation signals.",Rensheng R. Zhou|Nicoleta Serban|Nagi Gebraeel,,https://arxiv.org/abs/1107.5712v1,https://arxiv.org/pdf/1107.5712v1,https://doi.org/10.1214/10-AOAS448,Published in at http://dx.doi.org/10.1214/10-AOAS448 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 2B, 1586-1610",10.1214/10-AOAS448,stat.AP,stat.AP,https://arxiv.org/pdf/1107.5712v1.pdf
1105.2433v1,2011-05-12T11:44:07Z,2011-05-12 11:44:07,Rejoinder: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Rejoinder to ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Blakeley B. McShane|Abraham J. Wyner,,https://arxiv.org/abs/1105.2433v1,https://arxiv.org/pdf/1105.2433v1,https://doi.org/10.1214/10-AOAS398REJ,Published in at http://dx.doi.org/10.1214/10-AOAS398REJ the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 99-123",10.1214/10-AOAS398REJ,stat.AP,stat.AP,https://arxiv.org/pdf/1105.2433v1.pdf
1105.2145v1,2011-05-11T10:18:25Z,2011-05-11 10:18:25,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Gavin A. Schmidt|Michael E. Mann|Scott D. Rutherford,,https://arxiv.org/abs/1105.2145v1,https://arxiv.org/pdf/1105.2145v1,https://doi.org/10.1214/10-AOAS398D,Published in at http://dx.doi.org/10.1214/10-AOAS398D the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 65-70",10.1214/10-AOAS398D,stat.AP,stat.AP,https://arxiv.org/pdf/1105.2145v1.pdf
1105.0562v2,2011-05-03T12:16:58Z,2011-05-07 09:54:02,Metamodel-based importance sampling for structural reliability analysis,"Structural reliability methods aim at computing the probability of failure of systems with respect to some prescribed performance functions. In modern engineering such functions usually resort to running an expensive-to-evaluate computational model (e.g. a finite element model). In this respect simulation methods, which may require $10^{3-6}$ runs cannot be used directly. Surrogate models such as quadratic response surfaces, polynomial chaos expansions or kriging (which are built from a limited number of runs of the original model) are then introduced as a substitute of the original model to cope with the computational cost. In practice it is almost impossible to quantify the error made by this substitution though. In this paper we propose to use a kriging surrogate of the performance function as a means to build a quasi-optimal importance sampling density. The probability of failure is eventually obtained as the product of an augmented probability computed by substituting the meta-model for the original performance function and a correction term which ensures that there is no bias in the estimation even if the meta-model is not fully accurate. The approach is applied to analytical and finite element reliability problems and proves efficient up to 100 random variables.",V. Dubourg|F. Deheeger|B. Sudret,,https://arxiv.org/abs/1105.0562v2,https://arxiv.org/pdf/1105.0562v2,,"20 pages, 7 figures, 2 tables. Preprint submitted to Probabilistic Engineering Mechanics",,,stat.ME,stat.ME|stat.ML,https://arxiv.org/pdf/1105.0562v2.pdf
1105.0524v1,2011-05-03T09:36:45Z,2011-05-03 09:36:45,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Stephen McIntyre|Ross McKitrick,,https://arxiv.org/abs/1105.0524v1,https://arxiv.org/pdf/1105.0524v1,https://doi.org/10.1214/10-AOAS398L,Published in at http://dx.doi.org/10.1214/10-AOAS398L the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 56-60",10.1214/10-AOAS398L,stat.AP,stat.AP|physics.ao-ph,https://arxiv.org/pdf/1105.0524v1.pdf
1105.0522v1,2011-05-03T09:28:10Z,2011-05-03 09:28:10,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Jonathan Rougier,,https://arxiv.org/abs/1105.0522v1,https://arxiv.org/pdf/1105.0522v1,https://doi.org/10.1214/10-AOAS409,Published in at http://dx.doi.org/10.1214/10-AOAS409 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 96-98",10.1214/10-AOAS409,stat.AP,stat.AP,https://arxiv.org/pdf/1105.0522v1.pdf
1105.0519v1,2011-05-03T09:02:21Z,2011-05-03 09:02:21,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Doug Nychka|Bo Li,,https://arxiv.org/abs/1105.0519v1,https://arxiv.org/pdf/1105.0519v1,https://doi.org/10.1214/10-AOAS398K,Published in at http://dx.doi.org/10.1214/10-AOAS398K the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 80-82",10.1214/10-AOAS398K,stat.AP,stat.AP,https://arxiv.org/pdf/1105.0519v1.pdf
1104.4195v1,2011-04-21T07:42:03Z,2011-04-21 07:42:03,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Eugene R. Wahl|Caspar M. Ammann,,https://arxiv.org/abs/1104.4195v1,https://arxiv.org/pdf/1104.4195v1,https://doi.org/10.1214/10-AOAS398J,Published in at http://dx.doi.org/10.1214/10-AOAS398J the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 91-95",10.1214/10-AOAS398J,stat.AP,stat.AP|physics.ao-ph,https://arxiv.org/pdf/1104.4195v1.pdf
1104.4193v1,2011-04-21T07:32:45Z,2011-04-21 07:32:45,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Peter Craigmile|Bala Rajaratnam,,https://arxiv.org/abs/1104.4193v1,https://arxiv.org/pdf/1104.4193v1,https://doi.org/10.1214/10-AOAS398F,Published in at http://dx.doi.org/10.1214/10-AOAS398F the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 88-90",10.1214/10-AOAS398F,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4193v1.pdf
1104.4191v1,2011-04-21T07:25:43Z,2011-04-21 07:25:43,Spurious predictions with random time series: The Lasso in the context of paleoclimatic reconstructions. Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Martin P. Tingley,,https://arxiv.org/abs/1104.4191v1,https://arxiv.org/pdf/1104.4191v1,https://doi.org/10.1214/10-AOAS398E,Published in at http://dx.doi.org/10.1214/10-AOAS398E the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 83-87",10.1214/10-AOAS398E,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4191v1.pdf
1104.4188v1,2011-04-21T07:04:22Z,2011-04-21 07:04:22,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Jason E. Smerdon,,https://arxiv.org/abs/1104.4188v1,https://arxiv.org/pdf/1104.4188v1,https://doi.org/10.1214/10-AOAS398B,Published in at http://dx.doi.org/10.1214/10-AOAS398B the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 76-79",10.1214/10-AOAS398B,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4188v1.pdf
1104.4185v1,2011-04-21T06:53:57Z,2011-04-21 06:53:57,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Lasse Holmström,,https://arxiv.org/abs/1104.4185v1,https://arxiv.org/pdf/1104.4185v1,https://doi.org/10.1214/10-AOAS398H,Published in at http://dx.doi.org/10.1214/10-AOAS398H the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 71-75",10.1214/10-AOAS398H,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4185v1.pdf
1104.4178v1,2011-04-21T06:30:32Z,2011-04-21 06:30:32,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Murali Haran|Nathan M. Urban,,https://arxiv.org/abs/1104.4178v1,https://arxiv.org/pdf/1104.4178v1,https://doi.org/10.1214/10-AOAS398G,Published in at http://dx.doi.org/10.1214/10-AOAS398G the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 61-64",10.1214/10-AOAS398G,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4178v1.pdf
1104.4176v1,2011-04-21T06:07:08Z,2011-04-21 06:07:08,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Richard A. Davis|Jingchen Liu,,https://arxiv.org/abs/1104.4176v1,https://arxiv.org/pdf/1104.4176v1,https://doi.org/10.1214/10-AOAS398C,Published in at http://dx.doi.org/10.1214/10-AOAS398C the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 52-55",10.1214/10-AOAS398C,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4176v1.pdf
1104.4174v1,2011-04-21T05:48:55Z,2011-04-21 05:48:55,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",Alexey Kaplan,,https://arxiv.org/abs/1104.4174v1,https://arxiv.org/pdf/1104.4174v1,https://doi.org/10.1214/10-AOAS398M,Published in at http://dx.doi.org/10.1214/10-AOAS398M the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 47-51",10.1214/10-AOAS398M,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4174v1.pdf
1104.4171v1,2011-04-21T05:36:21Z,2011-04-21 05:36:21,Discussion of: A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Discussion of ""A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?"" by B.B. McShane and A.J. Wyner [arXiv:1104.4002]",L. Mark Berliner,,https://arxiv.org/abs/1104.4171v1,https://arxiv.org/pdf/1104.4171v1,https://doi.org/10.1214/10-AOAS398I,Published in at http://dx.doi.org/10.1214/10-AOAS398I the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 45-46",10.1214/10-AOAS398I,stat.AP,stat.AP,https://arxiv.org/pdf/1104.4171v1.pdf
1104.4002v1,2011-04-20T11:42:28Z,2011-04-20 11:42:28,A statistical analysis of multiple temperature proxies: Are reconstructions of surface temperatures over the last 1000 years reliable?,"Predicting historic temperatures based on tree rings, ice cores, and other natural proxies is a difficult endeavor. The relationship between proxies and temperature is weak and the number of proxies is far larger than the number of target data points. Furthermore, the data contain complex spatial and temporal dependence structures which are not easily captured with simple models. In this paper, we assess the reliability of such reconstructions and their statistical significance against various null models. We find that the proxies do not predict temperature significantly better than random series generated independently of temperature. Furthermore, various model specifications that perform similarly at predicting temperature produce extremely different historical backcasts. Finally, the proxies seem unable to forecast the high levels of and sharp run-up in temperature in the 1990s either in-sample or from contiguous holdout blocks, thus casting doubt on their ability to predict such phenomena if in fact they occurred several hundred years ago. We propose our own reconstruction of Northern Hemisphere average annual land temperature over the last millennium, assess its reliability, and compare it to those from the climate science literature. Our model provides a similar reconstruction but has much wider standard errors, reflecting the weak signal and large uncertainty encountered in this setting.",Blakeley B. McShane|Abraham J. Wyner,,https://arxiv.org/abs/1104.4002v1,https://arxiv.org/pdf/1104.4002v1,https://doi.org/10.1214/10-AOAS398,Published in at http://dx.doi.org/10.1214/10-AOAS398 the Annals of Applied Statistics (http://www.imstat.org/aoas/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Annals of Applied Statistics 2011, Vol. 5, No. 1, 5-44",10.1214/10-AOAS398,stat.AP,stat.AP|physics.ao-ph,https://arxiv.org/pdf/1104.4002v1.pdf
1104.3667v1,2011-04-19T08:07:38Z,2011-04-19 08:07:38,Reliability-based design optimization using kriging surrogates and subset simulation,"The aim of the present paper is to develop a strategy for solving reliability-based design optimization (RBDO) problems that remains applicable when the performance models are expensive to evaluate. Starting with the premise that simulation-based approaches are not affordable for such problems, and that the most-probable-failure-point-based approaches do not permit to quantify the error on the estimation of the failure probability, an approach based on both metamodels and advanced simulation techniques is explored. The kriging metamodeling technique is chosen in order to surrogate the performance functions because it allows one to genuinely quantify the surrogate error. The surrogate error onto the limit-state surfaces is propagated to the failure probabilities estimates in order to provide an empirical error measure. This error is then sequentially reduced by means of a population-based adaptive refinement technique until the kriging surrogates are accurate enough for reliability analysis. This original refinement strategy makes it possible to add several observations in the design of experiments at the same time. Reliability and reliability sensitivity analyses are performed by means of the subset simulation technique for the sake of numerical efficiency. The adaptive surrogate-based strategy for reliability estimation is finally involved into a classical gradient-based optimization algorithm in order to solve the RBDO problem. The kriging surrogates are built in a so-called augmented reliability space thus making them reusable from one nested RBDO iteration to the other. The strategy is compared to other approaches available in the literature on three academic examples in the field of structural mechanics.",V. Dubourg|B. Sudret|J. -M. Bourinet,,https://arxiv.org/abs/1104.3667v1,https://arxiv.org/pdf/1104.3667v1,https://doi.org/10.1007/s00158-011-0653-8,"20 pages, 6 figures, 5 tables. Preprint submitted to Springer-Verlag",Structural Multisciplinary Optimization. 2011,10.1007/s00158-011-0653-8,stat.ME,stat.ME|stat.ML,https://arxiv.org/pdf/1104.3667v1.pdf
1104.3503v1,2011-04-18T14:59:54Z,2011-04-18 14:59:54,RESID: A Practical Stochastic Model for Software Reliability,"A new approach called RESID is proposed in this paper for estimating reliability of a software allowing for imperfect debugging. Unlike earlier approaches based on counting number of bugs or modelling inter-failure time gaps, RESID focuses on the probability of ""bugginess"" of different parts of a program buggy. This perspective allows an easy way to incorporate the structure of the software under test, as well as imperfect debugging. One main design objective behind RESID is ease of implementation in practical scenarios.",Arnab Chakraborty,,https://arxiv.org/abs/1104.3503v1,https://arxiv.org/pdf/1104.3503v1,,"13 pages, 4 figures",,,stat.AP,stat.AP|cs.SE,https://arxiv.org/pdf/1104.3503v1.pdf
1104.3479v2,2011-04-18T13:33:58Z,2017-04-12 14:19:34,Reliability-based design optimization of shells with uncertain geometry using adaptive Kriging metamodels,"Optimal design under uncertainty has gained much attention in the past ten years due to the ever increasing need for manufacturers to build robust systems at the lowest cost. Reliability-based design optimization (RBDO) allows the analyst to minimize some cost function while ensuring some minimal performances cast as admissible failure probabilities for a set of performance functions. In order to address real-world engineering problems in which the performance is assessed through computational models (e.g., finite element models in structural mechanics) metamodeling techniques have been developed in the past decade. This paper introduces adaptive Kriging surrogate models to solve the RBDO problem. The latter is cast in an augmented space that ""sums up"" the range of the design space and the aleatory uncertainty in the design parameters and the environmental conditions. The surrogate model is used (i) for evaluating robust estimates of the failure probabilities (and for enhancing the computational experimental design by adaptive sampling) in order to achieve the requested accuracy and (ii) for applying a gradient-based optimization algorithm to get optimal values of the design parameters. The approach is applied to the optimal design of ring-stiffened cylindrical shells used in submarine engineering under uncertain geometric imperfections. For this application the performance of the structure is related to buckling which is addressed here by means of a finite element solution based on the asymptotic numerical method.",V. Dubourg|J. -M. Bourinet|B. Sudret,,https://arxiv.org/abs/1104.3479v2,https://arxiv.org/pdf/1104.3479v2,,,,,stat.ME,stat.ME|stat.AP,https://arxiv.org/pdf/1104.3479v2.pdf
1011.2929v1,2010-11-12T14:52:23Z,2010-11-12 14:52:23,Intrinsic Geometric Analysis of the Network Reliability and Voltage Stability,"This paper presents the intrinsic geometric model for the solution of power system planning and its operation. This problem is large-scale and nonlinear, in general. Thus, we have developed the intrinsic geometric model for the network reliability and voltage stability, and examined it for the IEEE 5 bus system. The robustness of the proposed model is illustrated by introducing variations of the network parameters. Exact analytical results show the accuracy as well as the efficiency of the proposed solution technique.",N. Gupta|B. N. Tiwari|S. Bellucci,,https://arxiv.org/abs/1011.2929v1,https://arxiv.org/pdf/1011.2929v1,,"8 pages, 4 figures, 2 tables, Index Terms -- Circuit modeling, geometric modeling, parameter space method, power system reliability, power system stability, transmission planning, nonlinear methods, geometric controls, components optimization",,,stat.AP,stat.AP|math-ph|physics.data-an,https://arxiv.org/pdf/1011.2929v1.pdf
1007.0562v2,2010-07-04T15:27:23Z,2010-07-20 01:13:46,"Cliophysics: Socio-political Reliability Theory, Polity Duration and African Political (In)stabilities","Quantification of historical sociological processes have recently gained attention among theoreticians in the effort of providing a solid theoretical understanding of the behaviors and regularities present in sociopolitical dynamics. Here we present a reliability theory of polity processes with emphases on individual political dynamics of African countries. We found that the structural properties of polity failure rates successfully capture the risk of political vulnerability and instabilities in which 87.50%, 75%, 71.43%, and 0% of the countries with monotonically increasing, unimodal, U-shaped and monotonically decreasing polity failure rates, respectively, have high level of state fragility indices. The quasi-U-shape relationship between average polity duration and regime types corroborates historical precedents and explains the stability of the autocracies and democracies.",Alhaji Cherif|Kamal Barley,,https://arxiv.org/abs/1007.0562v2,https://arxiv.org/pdf/1007.0562v2,https://doi.org/10.1371/journal.pone.0015169,"4 pages, 3 figures, 1 table",,10.1371/journal.pone.0015169,physics.soc-ph,physics.soc-ph|physics.data-an|stat.AP,https://arxiv.org/pdf/1007.0562v2.pdf
1005.1214v1,2010-05-07T14:18:17Z,2010-05-07 14:18:17,Parametric inference in a perturbed gamma degradation process,"We consider the gamma process perturbed by a Brownian motion (independent of the gamma process) as a degradation model. Parameters estimation is studied here. We assume that $n$ independent items are observed at irregular instants. From these observations, we estimate the parameters using the moments method. Then, we study the asymptotic properties of the estimators. Furthermore we derive some particular cases of items observed at regular or non-regular instants. Finally, some numerical simulations and two real data applications are provided to illustrate our method.",Laurent Bordes|Christian Paroissin|Ali Salami,LMA-PAU|LMA-PAU|LMA-PAU,https://arxiv.org/abs/1005.1214v1,https://arxiv.org/pdf/1005.1214v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/1005.1214v1.pdf
0908.3962v1,2009-08-27T09:33:41Z,2009-08-27 09:33:41,New approaches for increasing the reliability of the h index research performance measurement,"  In the year 2005 Jorge Hirsch introduced the h index for quantifying the research output of scientists. Today, the h index is a widely accepted indicator of research performance. The h index has been criticized for its insufficient reliability - the ability to discriminate reliably between meaningful amounts of research performance. Taking as an example an extensive data set with bibliometric data on scientists working in the field of molecular biology, we compute h2 lower, h2 upper, and sRM values and present them as complementary approaches that improve the reliability of the h index research performance measurement.",Lutz Bornmann|Ruediger Mutz|Hans-Dieter Daniel,,https://arxiv.org/abs/0908.3962v1,https://arxiv.org/pdf/0908.3962v1,,"20 pages, 2 figures",,,stat.AP,stat.AP,https://arxiv.org/pdf/0908.3962v1.pdf
0907.3944v1,2009-07-22T21:50:20Z,2009-07-22 21:50:20,The Utility of Reliability and Survival,"  Reliability (survival analysis, to biostatisticians) is a key ingredient for mak- ing decisions that mitigate the risk of failure. The other key ingredient is utility. A decision theoretic framework harnesses the two, but to invoke this framework we must distinguish between chance and probability. We describe a functional form for the utility of chance that incorporates all dispositions to risk, and pro- pose a probability of choice model for eliciting this utility. To implement the model a subject is asked to make a series of binary choices between gambles and certainty. These choices endow a statistical character to the problem of utility elicitation. The workings of our approach are illustrated via a live example in- volving a military planner. The material is general because it is germane to any situation involving the valuation of chance.",Nozer D. Singpurwalla,,https://arxiv.org/abs/0907.3944v1,https://arxiv.org/pdf/0907.3944v1,,,,,stat.ME,stat.ME|math.ST,https://arxiv.org/pdf/0907.3944v1.pdf
0905.2864v1,2009-05-18T11:24:05Z,2009-05-18 11:24:05,Designing a Bayesian Network for Preventive Maintenance from Expert Opinions in a Rapid and Reliable Way,"  In this study, a Bayesian Network (BN) is considered to represent a nuclear plant mechanical system degradation. It describes a causal representation of the phenomena involved in the degradation process. Inference from such a BN needs to specify a great number of marginal and conditional probabilities. As, in the present context, information is based essentially on expert knowledge, this task becomes very complex and rapidly impossible. We present a solution which consists of considering the BN as a log-linear model on which simplification constraints are assumed. This approach results in a considerable decrease in the number of probabilities to be given by experts. In addition, we give some simple rules to choose the most reliable probabilities. We show that making use of those rules allows to check the consistency of the derived probabilities. Moreover, we propose a feedback procedure to eliminate inconsistent probabilities. Finally, the derived probabilities that we propose to solve the equations involved in a realistic Bayesian network are expected to be reliable. The resulting methodology to design a significant and powerful BN is applied to a reactor coolant sub-component in EDF Nuclear plants in an illustrative purpose.",Gilles Celeux|Franck Corset|A. Lannoy|Benoit Ricard,INRIA Futurs|LJK||,https://arxiv.org/abs/0905.2864v1,https://arxiv.org/pdf/0905.2864v1,,,,,stat.ME,stat.ME,https://arxiv.org/pdf/0905.2864v1.pdf
0710.2740v1,2007-10-15T08:55:10Z,2007-10-15 08:55:10,Reliability of Module Based Software System,  This paper consider the problem of determining the reliability of a software system which can be decomposed in a number of modules. We have derived the expression of the reliability of a system using the Markovian model for the transfer of control between modules in order. We have given the expression of reliability by considering both benign and catastrophic failure. The expression of reliability presented in this work is applicable for some control software which are designed to detect its own internal errors.,Rudrani Banerjee|Angshuman Sarkar,,https://arxiv.org/abs/0710.2740v1,https://arxiv.org/pdf/0710.2740v1,,Submitted to the Electronic Journal of Statistics (http://www.i-journals.org/ejs/) by the Institute of Mathematical Statistics (http://www.imstat.org),,,stat.AP,stat.AP,https://arxiv.org/pdf/0710.2740v1.pdf
0708.1566v1,2007-08-11T13:36:44Z,2007-08-11 13:36:44,Markov Chain Modelling for Reliability Estimation of Engineering Systems at Different Scales - Some Considerations,"  The concepts of probability, statistics and stochastic theory are being successfully used in structural engineering. Markov Chain modelling is a simple stochastic process model that has found its application in both describing stochastic evolution of system and in system reliability estimation. The recent developments in Markov Chain Monte Carlo and the possible integration of Bayesian theory within Markov Chain theory have enhanced its application possibilities. However, the application possibility can be furthered to range over wider scales of application (perhaps from nano- to macro-) by considering the developments in Physics (in particular Quantum Physics). This paper tries to present the results of quantum physics that would help in interpretation of transition probability matrix. However, care has to be taken in the choice of densities in computing the transition probability matrix. The paper is based on available literature, and the aim is only to make an attempt to show how Markov Chain can be used to model systems at various scales.",K. Balaji Rao,,https://arxiv.org/abs/0708.1566v1,https://arxiv.org/pdf/0708.1566v1,,"10 pages (including cover page) International Conference on Civil Engineering in the New Millennium: Opportunities and Challenges, January 11-14, 2007",,,stat.AP,stat.AP|stat.ME,https://arxiv.org/pdf/0708.1566v1.pdf
0708.0355v1,2007-08-02T14:42:06Z,2007-08-02 14:42:06,"Advances in Data Combination, Analysis and Collection for System Reliability Assessment","  The systems that statisticians are asked to assess, such as nuclear weapons, infrastructure networks, supercomputer codes and munitions, have become increasingly complex. It is often costly to conduct full system tests. As such, we present a review of methodology that has been proposed for addressing system reliability with limited full system testing. The first approaches presented in this paper are concerned with the combination of multiple sources of information to assess the reliability of a single component. The second general set of methodology addresses the combination of multiple levels of data to determine system reliability. We then present developments for complex systems beyond traditional series/parallel representations through the use of Bayesian networks and flowgraph models. We also include methodological contributions to resource allocation considerations for system relability assessment. We illustrate each method with applications primarily encountered at Los Alamos National Laboratory.",Alyson G. Wilson|Todd L. Graves|Michael S. Hamada|C. Shane Reese,,https://arxiv.org/abs/0708.0355v1,https://arxiv.org/pdf/0708.0355v1,https://doi.org/10.1214/088342306000000439,Published at http://dx.doi.org/10.1214/088342306000000439 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Statistical Science 2006, Vol. 21, No. 4, 514-531",10.1214/088342306000000439,stat.ME,stat.ME,https://arxiv.org/pdf/0708.0355v1.pdf
0708.0295v1,2007-08-02T08:29:46Z,2007-08-02 08:29:46,Reliability,"  This special volume of Statistical Sciences presents some innovative, if not provocative, ideas in the area of reliability, or perhaps more appropriately named, integrated system assessment. In this age of exponential growth in science, engineering and technology, the capability to evaluate the performance, reliability and safety of complex systems presents new challenges. Today's methodology must respond to the ever-increasing demands for such evaluations to provide key information for decision and policy makers at all levels of government and industry--problems ranging from international security to space exploration. We, the co-editors of this volume and the authors, believe that scientific progress in reliability assessment requires the development of processes, methods and tools that combine diverse information types (e.g., experiments, computer simulations, expert knowledge) from diverse sources (e.g., scientists, engineers, business developers, technology integrators, decision makers) to assess quantitative performance metrics that can aid decision making under uncertainty. These are highly interdisciplinary problems. The principal role of statistical sciences is to bring statistical rigor, thinking and methodology to these problems.",Sallie Keller-McNulty|Alyson Wilson|Christine Anderson-Cook,,https://arxiv.org/abs/0708.0295v1,https://arxiv.org/pdf/0708.0295v1,https://doi.org/10.1214/088342306000000664,Published at http://dx.doi.org/10.1214/088342306000000664 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Statistical Science 2006, Vol. 21, No. 4, 427-427",10.1214/088342306000000664,stat.ME,stat.ME,https://arxiv.org/pdf/0708.0295v1.pdf
0708.0279v1,2007-08-02T08:12:24Z,2007-08-02 08:12:24,Expert Elicitation for Reliable System Design,"  This paper reviews the role of expert judgement to support reliability assessments within the systems engineering design process. Generic design processes are described to give the context and a discussion is given about the nature of the reliability assessments required in the different systems engineering phases. It is argued that, as far as meeting reliability requirements is concerned, the whole design process is more akin to a statistical control process than to a straightforward statistical problem of assessing an unknown distribution. This leads to features of the expert judgement problem in the design context which are substantially different from those seen, for example, in risk assessment. In particular, the role of experts in problem structuring and in developing failure mitigation options is much more prominent, and there is a need to take into account the reliability potential for future mitigation measures downstream in the system life cycle. An overview is given of the stakeholders typically involved in large scale systems engineering design projects, and this is used to argue the need for methods that expose potential judgemental biases in order to generate analyses that can be said to provide rational consensus about uncertainties. Finally, a number of key points are developed with the aim of moving toward a framework that provides a holistic method for tracking reliability assessment through the design process.",Tim Bedford|John Quigley|Lesley Walls,,https://arxiv.org/abs/0708.0279v1,https://arxiv.org/pdf/0708.0279v1,https://doi.org/10.1214/088342306000000510,"This paper commented in: [arXiv:0708.0285], [arXiv:0708.0287], [arXiv:0708.0288]. Rejoinder in [arXiv:0708.0293]. Published at http://dx.doi.org/10.1214/088342306000000510 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org)","Statistical Science 2006, Vol. 21, No. 4, 428-450",10.1214/088342306000000510,stat.ME,stat.ME,https://arxiv.org/pdf/0708.0279v1.pdf
0708.0293v1,2007-08-02T08:02:18Z,2007-08-02 08:02:18,Rejoinder: Expert Elicitation for Reliable System Design,  Rejoinder: Expert Elicitation for Reliable System Design [arXiv:0708.0279],Tim Bedford|John Quigley|Lesley Walls,,https://arxiv.org/abs/0708.0293v1,https://arxiv.org/pdf/0708.0293v1,https://doi.org/10.1214/088342306000000556,Published at http://dx.doi.org/10.1214/088342306000000556 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Statistical Science 2006, Vol. 21, No. 4, 460-462",10.1214/088342306000000556,stat.ME,stat.ME,https://arxiv.org/pdf/0708.0293v1.pdf
0708.0288v1,2007-08-02T07:48:24Z,2007-08-02 07:48:24,Comment: Expert Elicitation for Reliable System Design,  Comment: Expert Elicitation for Reliable System Design [arXiv:0708.0279],Wenbin Wang,,https://arxiv.org/abs/0708.0288v1,https://arxiv.org/pdf/0708.0288v1,https://doi.org/10.1214/088342306000000547,Published at http://dx.doi.org/10.1214/088342306000000547 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Statistical Science 2006, Vol. 21, No. 4, 456-459",10.1214/088342306000000547,stat.ME,stat.ME,https://arxiv.org/pdf/0708.0288v1.pdf
0708.0287v1,2007-08-02T07:36:31Z,2007-08-02 07:36:31,Comment: Expert Elicitation for Reliable System Design,  Comment: Expert Elicitation for Reliable System Design [arXiv:0708.0279],Andrew Koehler,,https://arxiv.org/abs/0708.0287v1,https://arxiv.org/pdf/0708.0287v1,https://doi.org/10.1214/088342306000000538,Published at http://dx.doi.org/10.1214/088342306000000538 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Statistical Science 2006, Vol. 21, No. 4, 454-455",10.1214/088342306000000538,stat.ME,stat.ME,https://arxiv.org/pdf/0708.0287v1.pdf
0708.0285v1,2007-08-02T07:13:09Z,2007-08-02 07:13:09,Comment: Expert Elicitation for Reliable System Design,  Comment: Expert Elicitation for Reliable System Design [arXiv:0708.0279],Norman Fenton|Martin Neil,,https://arxiv.org/abs/0708.0285v1,https://arxiv.org/pdf/0708.0285v1,https://doi.org/10.1214/088342306000000529,Published at http://dx.doi.org/10.1214/088342306000000529 in the Statistical Science (http://www.imstat.org/sts/) by the Institute of Mathematical Statistics (http://www.imstat.org),"Statistical Science 2006, Vol. 21, No. 4, 451-453",10.1214/088342306000000529,stat.ME,stat.ME,https://arxiv.org/pdf/0708.0285v1.pdf
physics/0702148v1,2007-02-17T04:19:57Z,2007-02-17 04:19:57,Reliability of rank order in sampled networks,"  In complex scale-free networks, ranking the individual nodes based upon their importance has useful applications, such as the identification of hubs for epidemic control, or bottlenecks for controlling traffic congestion. However, in most real situations, only limited sub-structures of entire networks are available, and therefore the reliability of the order relationships in sampled networks requires investigation. With a set of randomly sampled nodes from the underlying original networks, we rank individual nodes by three centrality measures: degree, betweenness, and closeness. The higher-ranking nodes from the sampled networks provide a relatively better characterisation of their ranks in the original networks than the lower-ranking nodes. A closeness-based order relationship is more reliable than any other quantity, due to the global nature of the closeness measure. In addition, we show that if access to hubs is limited during the sampling process, an increase in the sampling fraction can in fact decrease the sampling accuracy. Finally, an estimation method for assessing sampling accuracy is suggested.",Pan-Jun Kim|Hawoong Jeong,,https://arxiv.org/abs/physics/0702148v1,https://arxiv.org/pdf/physics/0702148v1,https://doi.org/10.1140/epjb/e2007-00033-7,,"Eur. Phys. J. B 55, 109-114 (2007)",10.1140/epjb/e2007-00033-7,physics.soc-ph,physics.soc-ph|cond-mat.stat-mech|physics.data-an|stat.AP,https://arxiv.org/pdf/physics/0702148v1.pdf
