is_reliability_paper,reliability_topic,modeling_approach,data_type,maintenance_policy,application_domain,evaluation_type,code_used,software_platform,code_availability_source,software_urls,summary,key_equations,key_results,limitations_stated,limitations_unstated,future_work_stated,future_work_unstated,id,pdf_url,pdf_path,llm_provider,llm_model,repeat_id,extracted_at
TRUE,Failure mode analysis|System reliability|Maintenance optimization|Reliability growth|Other,Bayesian|Nonparametric/Semi-parametric|Stochastic process|Simulation-based|Other,Right-censored|Degradation measurements|Event/count data|Sensor/condition monitoring|Mixture of types|Other,Condition-based|Predictive|Not applicable|Other,Manufacturing (general)|Semiconductor/electronics|Energy/utilities|Transportation/logistics|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a review of how structured expert judgement (expert elicitation) can be used to support reliability, availability, and maintainability (RAM) assessment throughout the systems engineering design life cycle (concept/definition through operation, maintenance, and disposal). It argues that meeting reliability requirements is better viewed as a control/feedback process—iteratively structuring models, quantifying uncertainties, and revising decisions—rather than as a one-shot statistical estimation problem. The authors synthesize elicitation roles (decision-maker, expert, analyst), common judgement biases, and differences between elicitation for probabilistic risk assessment versus engineering design (especially the importance of qualitative problem structuring and downstream mitigation potential). The paper surveys reliability-relevant modeling frameworks and tools that can be populated/updated with expert inputs (e.g., FMEA/FTA/RBD/Markov and Bayesian belief networks, reliability growth, accelerated testing, maintenance and warranty models, and condition monitoring/degradation). It highlights challenges in combining experts, transforming elicited beliefs into model parameters, and integrating expert opinion with historical/generic reliability databases (e.g., Mil-Hdbk-217, IEEE-500/OREDA/EIREDA), emphasizing uncertainty ranges and model-change effects rather than naïve Bayesian updating of old databases.","The paper frames a reliability metric as a function of life-cycle decisions: $r = r(d,p,u,m,c)$, where $d,p,u,m,c$ denote design, production, usage, maintenance, and changes/modifications. It introduces “tolerance uncertainty” for a parameter such as a failure rate $\lambda(e)$ over a design envelope $E$ as an interval $\left[\min_{e\in E}\lambda(e),\ \max_{e\in E}\lambda(e)\right]$, noting that it may not be meaningful to place a probability distribution on $E$ when variables reflect choices by stakeholders. These expressions are used to motivate elicitation that captures dependence of reliability on controllable decisions and future mitigation actions.","No new control-chart/ARL-type quantitative performance results are reported; the paper is a conceptual and literature review rather than a new method with numerical benchmarking. Key takeaways are qualitative: expert elicitation in design must emphasize problem structuring and the impact of future decisions, and stakeholders often require methods that expose/mitigate biases to achieve “rational consensus.” The authors also stress that generic reliability databases (e.g., Mil-Hdbk-217) can be misleading if treated as precise point estimates, and that uncertainty bands/ranges and explicit modeling of changes from prior systems are crucial when leveraging historical data.",None stated.,"Because the article is a broad review and position piece, it does not provide a worked, end-to-end elicitation protocol with validated performance metrics (e.g., calibration scoring results, predictive accuracy, or decision impact) across multiple real design projects. Many claims (e.g., what is common in industry practice, or which approaches tend to be overly optimistic) are supported by citations and examples but are not backed by systematic empirical studies or reproducible comparative evaluations. The discussion spans many model classes (FMEA/FTA/BBNs, accelerated tests, maintenance, warranty, condition monitoring), but offers limited technical guidance on selecting among them under specific data/assumption violations (dependence, nonstationarity, organizational constraints).","The authors call for research toward a holistic framework for tracking reliability assessment through the design process, integrating qualitative structuring, quantification, and revision across life-cycle phases. They emphasize the need for methods that can support rational consensus across stakeholders, better represent dependence of reliability on design/production/usage/maintenance/change decisions, and improve elicitation and learning/feedback for expert calibration in longitudinal design programs.","Develop and validate standardized, domain-specific elicitation workflows for RAM design (including training, calibration scoring, and feedback loops) and report results across multiple industrial case studies to quantify benefits versus ad hoc practice. Create practical methods to combine expert judgement with heterogeneous evidence streams (legacy databases, test data, field returns, sensor/degradation signals) under model uncertainty, including robustness to dependence, changing environments, and decision-driven nonstationarity. Provide open-source tooling (e.g., templates, scoring utilities, Bayesian/BBN implementations) to make transparent stakeholder-facing analyses feasible, and benchmark elicitation-driven reliability forecasts against alternatives (pure-data, physics-only, ML-only, and hybrid approaches).",0708.0279v1,https://arxiv.org/pdf/0708.0279v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:44:39Z
TRUE,System reliability|Other,Bayesian|Other,Other,Not applicable,Transportation/logistics|Other,Other,NA,None / Not applicable,Not applicable (No code used),www.agenarisk.com|www.dcs.qmw.ac.uk/~norman/papers/ranked_nodes%20v01.004.pdf,"This is a discussion/commentary on Bedford, Quigley and Walls’ paper on expert elicitation for reliable system design, emphasizing the practical importance of combining expert judgment with empirical reliability evidence. The authors argue Bayesian methods provide a coherent uncertainty calculus for fusing elicited probabilities with reliability test data and updating assessments over a system life cycle. They describe their experience using Bayesian networks and hierarchical Bayesian modeling for system dependability evaluation, including the TRACS framework used to assess military vehicle reliability across procurement, design, test, and operational stages. TRACS is described as estimating component-family failure rates via a Bayesian hierarchical model, aggregating to a system-level reliability distribution, then updating using Bayes’ rule with likelihood information from successive testing stages while adjusting predictions using expert assessments of design/manufacturing/process factors. The commentary also highlights organizational challenges (e.g., cultural resistance to priors) and positions elicitation as a knowledge management opportunity, not only a technical step in reliability quantification.",No explicit equations are provided in the commentary; methods are described conceptually (Bayesian hierarchical modeling of component failure rates; system-level aggregation to a reliability distribution; updating with Bayes’ rule using likelihood from test/trial data; Bayesian network fusion of expert qualitative factors with quantitative failure-rate evidence).,"No numerical performance results (e.g., ARL/coverage/accuracy) are reported in this commentary. The main applied claim is that TRACS has been used routinely by QinetiQ to assess military vehicle reliability through multiple life-cycle stages and that commercial Bayesian-network software availability has made model construction faster and easier than in early implementations. The paper also asserts (qualitatively) that traditional reliability prediction can be over-optimistic when design/process factors are ignored and that Bayesian methods improve transparency by making assumptions and uncertainties explicit.",None stated.,"As a commentary, it does not provide enough methodological detail to assess modeling assumptions (e.g., independence/conditional-independence structure in Bayesian networks, prior sensitivity, calibration of expert adjustments) or reproducibility. There is no quantitative evaluation, benchmark comparison, or case-study data presented to substantiate the claimed benefits of TRACS or the elicitation techniques. The discussion of data relevance/heterogeneity is high-level and does not specify how model misspecification or conflicting sources are diagnosed and handled in practice.",The authors identify research issues including: how to persuade engineering experts to express Bayesian priors in data-driven SPC cultures; identifying universal organizational/process drivers affecting reliability across industries; and assessing process-factor effects quantitatively or encouraging methodical data collection and sharing to support such modeling.,Empirical studies comparing Bayesian elicitation-plus-data approaches against standard reliability-growth/testing-only approaches on shared benchmark problems would strengthen the evidence base. Sensitivity analyses and calibration studies for elicited priors (and expert-based adjustment factors) could clarify robustness and help operationalize best practices. Public release of reference Bayesian-network/TRACS-style templates and accompanying datasets (even anonymized) would improve reproducibility and accelerate adoption across domains beyond defense vehicles and software.,0708.0285v1,https://arxiv.org/pdf/0708.0285v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:44:56Z
TRUE,System reliability|Other,Bayesian|Other,Other,Not applicable,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This short discussion/commentary addresses expert elicitation as an input to reliable engineered system design and reliability prediction, emphasizing the distinctive challenges of eliciting information in complex systems engineering contexts. It highlights two advances of the main article it comments on: synthesizing probability elicitation literature relevant to engineered systems and reframing elicitation around practical systems-engineering constraints. The commentary stresses how elicitation differs from “single distribution” settings because system reliability depends on evolving uncertainties, system dependencies, and life-cycle realities (e.g., spiral vs. waterfall development). It identifies additional elicitation challenges for waterfall programs, including loss of expertise continuity between design and operations and “forward casting” (operating outside original assumptions), both of which increase reliance on subjective judgment for reliability estimates. It also raises epistemic and verification issues in modeling large dependency structures for “systems of systems,” suggesting reliability may require maintaining and combining multiple competing structural models rather than a single coherent model.","The commentary references a conceptual relationship linking reliability to multiple systems factors, written as $r = r(d, p, u, m, c)$ (reliability as a function of design parameters and other program/usage/model/context variables). No new charting statistic, life distribution, or closed-form reliability/ARL equations are derived in this piece; it is primarily qualitative and conceptual.","No quantitative reliability results, parameter estimates, or performance metrics are reported in the commentary. There are no ARL/RUL/lifetime comparisons, simulation tables, or empirical estimates; the contribution is conceptual—identifying practical elicitation complications (expertise discontinuity and forward casting) and emphasizing the need to capture system dependency structure uncertainty in reliability assessments.",None stated.,"As a brief discussion piece, it does not provide a formal elicitation protocol, mathematical model, or validation demonstrating how the proposed considerations improve reliability estimates in practice. It also does not specify how to operationalize or quantify “forward casting” impacts, how to assess elicitation quality, or how to combine multiple competing system-structure models into a joint reliability distribution. Domain scope is discussed anecdotally (e.g., weapons/long-lead facilities) without case-study data.",The author suggests it would be interesting in follow-up work to learn more about the types of systems studied by the main-article authors and to develop additions that could lead to a technical system elicitation taxonomy. The discussion also points to the need for more research on tracking an expert’s span of expertise and on capturing/verifying large dependency structures in complex systems and systems-of-systems for reliability prediction.,"Develop and evaluate concrete, repeatable elicitation workflows tailored to spiral vs. waterfall life cycles, including methods to model expertise attrition/hand-off between phases. Create quantitative frameworks (e.g., Bayesian model averaging over competing dependency structures) for combining multiple structural system models and propagating that uncertainty into reliability metrics. Provide empirical case studies with real engineering programs to compare elicitation-driven reliability predictions against operational outcomes and to establish diagnostics for elicitation bias, coherence, and calibration.",0708.0287v1,https://arxiv.org/pdf/0708.0287v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:45:15Z
TRUE,System reliability|Maintenance optimization|Other,Bayesian|Nonparametric/Semi-parametric|Other,Mixture of types|Other,Not applicable,Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a discussion/commentary on Bedford, Quigley and Walls about expert elicitation for reliability assessment during system design. It highlights two subjective-information aggregation approaches not emphasized in the main paper: empirical Bayes (estimating prior hyperparameters from elicited/observed data via marginal likelihood) and evidential reasoning (Dempster–Shafer theory) for combining multiple component-level beliefs into an overall system-level assessment. The note argues empirical Bayes can reduce computational burden versus fully hierarchical Bayes and can help avoid arbitrary/conjugate prior choices when incorporating expert information. It also motivates evidential reasoning as useful when elicited information is imprecise/incomplete (e.g., interval or qualitative grades), illustrating with a decomposed system reliability example (motorcycle components and qualitative grades). Overall, it positions EB and ER as practical tools for reliability assessment with subjective data at the design stage rather than proposing a new reliability model with formal performance evaluation.","Empirical Bayes is framed by a marginal likelihood over hyperparameters: $p(x\mid\Phi)=\int p(x\mid\Theta)\,p(\Theta\mid\Phi)\,d\Theta$, followed by plugging in $\hat\Phi$ to compute posteriors such as $p(\Theta\mid x,\hat\Phi) \propto p(x\mid\Theta) p(\Theta\mid\hat\Phi)$ and design-reliability quantities via $p(d\mid\hat\Phi)=\int p(d\mid\Theta)\,p(\Theta\mid x,\hat\Phi)\,d\Theta$. Evidential reasoning is summarized with basic-attribute probability masses $P_{n,i}=\omega_i\beta_{n,i}$ and a recursive aggregation rule $P_{n,I(i+1)}=K_{I(i+1)}P_{n,I(i)}P_{n,i+1}$, yielding combined beliefs $\beta_n=P_{n,I(L)}$ over grades.","No numerical reliability results, ARL/coverage comparisons, or empirical performance tables are reported; the article is primarily conceptual. The main “results” are qualitative: (i) EB can provide more context-sensitive priors than arbitrary/conjugate choices by estimating hyperparameters from elicited/observed information, and (ii) EB can reduce computational complexity relative to fully hierarchical Bayes when many unknowns exist. The discussion also asserts ER can combine multiple, possibly qualitative and incomplete, component-level assessments into a system-level reliability grade, which is argued to match common design-stage elicitation realities.","The author notes that Bedford, Quigley and Walls do not fully explore the technical statistical techniques for expert elicitation, implying practitioners must consult other sources for practical solutions. No additional explicit limitations of EB/ER (e.g., bias, calibration, sensitivity) are directly stated as limitations in this commentary.","As a short discussion piece, it does not provide a worked reliability case study, simulation, or quantitative validation demonstrating when EB or ER improves decision quality or calibration in design reliability assessment. The EB plug-in approach can understate uncertainty by treating $\hat\Phi$ as fixed, which may lead to overconfident reliability inferences compared with full hierarchical Bayes, but this is not analyzed. The ER presentation is high-level and omits practical elicitation details (how to elicit/calibrate $\beta_{n,i}$ and weights $\omega_i$, dependence between attributes, and sensitivity/robustness of conclusions to these inputs).",None stated.,"Provide empirical or simulation-based studies in reliability design settings comparing EB, full hierarchical Bayes, and alternative elicitation schemes on calibration, decision loss, and robustness to mis-specified priors/experts. Develop practical guidance and diagnostics for eliciting and validating ER inputs (belief degrees and weights), including handling dependence among components and performing sensitivity/uncertainty analysis. Extend these ideas to explicit system reliability structures (fault trees/Bayesian networks) with mixed objective test data and expert judgement, and supply reproducible software implementations for practitioners.",0708.0288v1,https://arxiv.org/pdf/0708.0288v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:45:38Z
TRUE,System reliability|Other,Bayesian|Other,Other,Not applicable,Transportation/logistics|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a rejoinder to discussions of the authors’ main article on expert elicitation for reliable system design, focusing on how expert judgment can be structured and used within engineering design when empirical failure data are limited. It discusses practical elicitation challenges in complex engineered systems (including aerospace, rail, and naval contexts), and emphasizes the need for problem structuring, stakeholder roles, and managing bias/trust when combining inputs from different parties. The authors comment on methodological options including empirical Bayes approaches (using expert judgment to define exchangeable “pools” of events to increase effective data) and Bayesian network/meta-modeling for system reliability assessment. They critique nonprobabilistic uncertainty frameworks and caution against arbitrary multicriteria probability-weighting schemes, while noting the potential of imprecise/interval probabilities for bounding risks. Overall, the rejoinder positions probability elicitation and Bayesian reliability modeling as decision-support tools whose value depends heavily on context, model structure, and organizational processes rather than solely on statistical technique.",Not applicable,Not applicable,"The authors note that their purpose was not to provide a survey of expert judgment methodologies, relying instead on existing surveys (e.g., Jenkinson, 2005). They also indicate that assessing commercial tools (e.g., TRACS) is difficult for academics because internal workings are not available. They further acknowledge that achieving stakeholder conceptual acceptance and successful use in practice remains scarce, with evidence largely piecemeal/anecdotal in their experience.","As a rejoinder/commentary, it provides limited methodological detail and no reproducible empirical evaluation, so it is difficult to assess performance or generalizability of the advocated elicitation/EB pooling strategies. The discussion remains high-level on how to operationalize elicitation protocols (e.g., calibration, aggregation rules, validation of expert pools) and does not provide concrete guidance for handling dependence, nonstationarity, or model misspecification in complex systems. The application claims (aerospace/rail/naval) are not backed here by case-study data or quantified impacts on reliability decisions.","The authors suggest that more research is needed on developing a taxonomy for technical system elicitation, with the possibility that international standards could emerge based on such a taxonomy. They also highlight open research questions around cultural conflict, organizational drivers, and process drivers that affect acceptance and use of elicitation-based reliability modeling in industry.","Develop and test standardized elicitation workflows with measurable quality criteria (expert calibration, coherence checks, sensitivity/robustness analysis) tailored to reliability models such as fault trees and Bayesian networks. Provide comparative empirical studies showing how different pooling/EB strategies affect reliability parameter estimates and downstream decisions under sparse data, including guidance for model validation with limited operational feedback. Create open-source implementations and benchmark datasets to improve transparency and facilitate adoption beyond proprietary tools.",0708.0293v1,https://arxiv.org/pdf/0708.0293v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:45:53Z
TRUE,System reliability|Software reliability|Accelerated testing|Maintenance optimization|Life distribution modeling|Other,Other,Mixture of types|Other,Not applicable,Network/cybersecurity|Transportation/logistics|Energy/utilities|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This article is an editorial introduction to a special issue of Statistical Science focused on reliability (also framed as “integrated system assessment”) for complex systems. It argues that modern reliability and safety assessment must integrate heterogeneous information sources—physical experiments, computer simulations, and expert judgment—to produce quantitative performance metrics for decision making under uncertainty. The editors briefly summarize the contributions in the special issue, including expert-judgment frameworks for systems engineering design, monitoring of networked software/application health, recurrent-event and first-hitting-time models (with regression structures), methods for system reliability with limited full-system testing and resource allocation, repairable-systems modeling with imperfect repair and trend-renewal processes, and accelerated testing for failure-time distribution and long-term performance estimation. The piece positions statistical science as providing rigor and methodology to interdisciplinary reliability problems spanning domains from security to space exploration. No new reliability model, inferential method, or empirical study is introduced in this editorial itself.",Not applicable,Not applicable (editorial overview; no new quantitative results are reported).,None stated.,"As an editorial, it does not provide methodological details, assumptions, or validation (e.g., run-length/ARL results, estimation performance, or case-study outcomes) for any specific reliability approach; it only summarizes other papers. The broad framing (“integrated system assessment”) is not operationalized into a specific workflow, metrics, or decision-analytic structure, which limits direct reproducibility or implementation guidance.",None stated.,"A natural extension would be to formalize the proposed integration of experiments, simulations, and expert judgment into a concrete statistical framework (e.g., Bayesian evidence synthesis) with guidance on uncertainty propagation to decision metrics. Additional work could develop standardized benchmarks and reporting practices for integrated system assessments so that competing reliability methods can be compared consistently across application domains.",0708.0295v1,https://arxiv.org/pdf/0708.0295v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:46:04Z
TRUE,Life distribution modeling|Degradation modeling|Failure mode analysis|System reliability|Reliability growth|Network/infrastructure reliability|Other,"Parametric (Weibull, etc.)|Stochastic process|Bayesian|Simulation-based|Other",Complete lifetime data|Right-censored|Degradation measurements|Event/count data|Mixture of types|Other,Not applicable,Energy/utilities|Network/cybersecurity|Transportation/logistics|Other,Other,TRUE,R|None / Not applicable|Other,Not provided,yadas.lanl.gov|www.r-project.org,"This paper is a review of statistical methodology for assessing reliability of complex systems when full system testing is limited or infeasible. It emphasizes hierarchical Bayesian modeling to combine heterogeneous evidence sources, including failure time data with censoring, degradation measurements, pass/fail (Bernoulli/binomial) tests, quality assurance/specification measurements, and biased (convenience) samples. For system-level assessment it discusses multilevel data fusion (component and system tests) under series/parallel-like structures, extensions to repairable systems via nonhomogeneous Poisson process (NHPP) models (including reliability growth), and richer system representations beyond fault trees/reliability block diagrams through Bayesian networks and flowgraph models. The paper also reviews prior elicitation issues and presents a computational approach to test resource allocation using repeated pre-posterior analyses and optimization (genetic algorithms), with implementation described using R coupled to the YADAS MCMC software. Applications are motivated by Los Alamos National Laboratory contexts such as nuclear weapons, infrastructure networks, supercomputer clusters, and munitions.","Key modeling examples include: (i) degradation + lifetime linkage with $Y_j\sim\mathcal N(\alpha-\beta_j^{-1}t_j,\sigma_y^2)$, $\log\beta_j\sim\mathcal N(\mu,\sigma_b^2)$, and failure time $T_j=(\alpha-L)\beta_j$ yielding lognormal lifetimes and a joint Bayesian posterior for $(\alpha,\{\beta_j\},\mu,\sigma_b,\sigma_y,L)$. (ii) pass/fail + specification (surrogate) modeling with $\Pr(Y=1\mid Z,t)=\prod_{j=1}^J \Phi\big((Z_j-\theta_j)/\sigma_j\big)$ and the integrated reliability function $R(t\mid\Theta)=\prod_{j=1}^J \Phi\left(\frac{\alpha_j+\delta_j t-\theta_j}{\sqrt{\gamma_j^2+\sigma_j^2}}\right)$. (iii) repairable-system NHPP intensity $\nu(t)=\frac{\phi}{\eta}(t/\eta)^{\phi-1}+\rho$ and (series) cluster reliability $R(l,s\mid\Theta)=\prod_{i=1}^{48}\exp\{(s/\eta_i)^{\phi_i}-((s+l)/\eta_i)^{\phi_i}-\rho_i l\}$.","In a simulated degradation+failure-time example (population at 20 years with 4 observed failures, 76 survivors, annual degradation data), the posterior mean reliability curve closely tracks the true survivor function and substantially reduces uncertainty compared with using heavily censored failure times alone; reported posterior means (90% intervals) include $\alpha\approx99.2\,(92.9,105.1)$, $L\approx17.6\,(2.3,34.6)$, $\mu\approx-1.00\,(-1.21,-0.76)$, $\sigma_y\approx6.57\,(3.8,10.3)$, and $\sigma_b\approx0.24\,(0.14,0.35)$. For multilevel system modeling, plots show component-specific uncertainty bands and how different components dominate system unreliability at different ages in a three-component series example combining logistic-regression pass/fail, Weibull lifetimes with right censoring, and degradation-linked lognormal lifetimes. For a 48-node supercomputer (repairable components in series), posterior quantiles of six-hour job reliability increase with start time, indicating inferred reliability growth under the hierarchical NHPP model. In resource allocation (binomial data) with possible structure bias parameter $\beta$, a worked example with budget 2500 and costs $TC_1=30,TC_2=TC_3=1$ yields an optimal allocation spending essentially all budget on system tests (e.g., $(n_1,n_2,n_3)=(83,10,0)$) and reducing an uncertainty criterion (90% interval-length upper quantile) to about 0.160 in the reported simulation setting.","The authors note that the convenience-sample bias parameterization (extended hypergeometric with bias factor $\theta$) can be hard to interpret and that their data were inconclusive about the direction of bias; they caution that if the biasing mechanism is better understood it should be modeled explicitly rather than via the generic bias parameter. They also state that more study is needed to characterize optimal mixtures of system vs. component tests in resource allocation when system tests are cheaper than testing all components together but structure uncertainty remains. They emphasize that elicitation of system structure/dependencies is an open research area and that many problems (richer representations, inference, and allocation beyond binomial cases) remain unresolved. They additionally highlight computational intensity as a practical limitation for large systems and for resource allocation in particular, raising the need for approximations or substantial computing power.","As a review paper, it synthesizes many methods but provides limited head-to-head benchmarking across competing approaches under standardized scenarios; many illustrated examples are simulated or context-specific, so general performance claims are hard to quantify. Several models rely on strong assumptions (conditional independence, parametric forms like lognormal/Weibull, deterministic threshold crossing for degradation-to-failure, and series-system independence) and robustness to misspecification is not systematically analyzed. The resource-allocation approach depends heavily on prior choices and on computationally expensive nested simulation/MCMC; the paper does not provide scalable alternatives (e.g., surrogate modeling of the utility, variational approximations) or runtime guidance. Some domains mentioned (e.g., nuclear weapons, infrastructure) likely involve dependence and common-cause effects, but practical elicitation/validation of dependence structures is only briefly discussed.","The paper explicitly calls for more research on (i) eliciting system structure and dependencies among components and failure modes, (ii) statistical inference methods for richer system representations such as Bayesian networks and flowgraphs, and (iii) resource allocation for more complicated systems and for experiments beyond the binomial setting (e.g., accelerated degradation studies, including choosing stress levels, sample sizes, and inspection schedules). It also highlights the need for improved implementation tools and computational strategies, including approximations that preserve accuracy and addressing the heavy computational burden of large-system assessments and allocation studies. The authors mention ongoing tool development (GROMIT, YADAS, and interfaces) while noting many remaining challenges.","Develop robust/self-starting variants of the hierarchical Bayesian reliability models that explicitly handle autocorrelation in degradation/sensor data, model discrepancy between component tests and field conditions, and incorporate model checking/validation (posterior predictive checks) as a standard step. Create scalable resource-allocation methods using Bayesian optimization or surrogate utilities to reduce the nested simulation/MCMC cost, and study sensitivity of optimal allocations to priors and structural uncertainty. Extend the multilevel/system models to explicitly incorporate common-cause failures, dynamic/repair policies, and time-varying environments, and assess identifiability when system tests are sparse. Provide open-source, reproducible implementations (e.g., modern probabilistic programming) and shared benchmark datasets for systematic comparisons across fault-tree, BN, and flowgraph-based reliability assessment pipelines.",0708.0355v1,https://arxiv.org/pdf/0708.0355v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:47:00Z
TRUE,System reliability|Other,"Stochastic process|Parametric (Weibull, etc.)|Other",Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),http://www.fineprint.com|http://philsci-archive.pitt.edu,"The paper discusses Markov Chain (MC) modelling as a stochastic-process framework for estimating the reliability of engineering/structural systems whose response evolves with time or loading, framing reliability estimation as a first-passage problem. It summarizes homogeneous and nonhomogeneous Markov chain formulations via the transition probability matrix (TPM), including steady-state behavior via the invariant distribution vector. For nonlinear/nonhomogeneous evolution, it outlines construction of stage-dependent TPMs and propagation via products of one-step matrices. A key practical element is computation of TPM entries from assumed joint distributions of response at successive stages; the paper presents formulas for TPM elements under a bivariate normal assumption (maximum-entropy given means/variances/correlation). It then explains how to aggregate states into safe vs. failed sets and compute failure probability at a given loading stage from unconditional state probabilities. The latter part is conceptual, relating “transition probability” notions to a metric/overlap interpretation inspired by quantum/statistical mechanics to guide TPM interpretation across scales.","The transition probability matrix is defined by $p_{ij}=P(X_{k+1}=j\mid X_k=i)$, with $n$-step transitions $P^n$ (homogeneous) or $P^{(n)}=P_1P_2\cdots P_{n-1}$ (nonhomogeneous). TPM elements can be computed as conditional probabilities using joint and marginal densities, e.g., $p_{ij}(Y_k,Y_{k+1})=\frac{\int_{x_{k+1}\in j}\int_{x_k\in i} f_{k,k+1}(x_k,x_{k+1})\,dx_k\,dx_{k+1}}{\int_{x_k\in i} f_k(x_k)\,dx_k}$ with $f_{k,k+1}$ taken as bivariate normal. Unconditional state probabilities at stage $Y_k$ yield failure probability via aggregation: $P_f(Y_k)=\sum_{i\in \text{failed}} p_i(Y_k)$ (the text presents an equivalent sum over the complement depending on aggregation convention). Steady-state probabilities satisfy $\pi P=\pi$ and $\sum_i \pi_i=1$.","No numerical or comparative performance results are reported; the contribution is primarily methodological/formulative. The paper provides closed-form integral expressions for TPM entries under normal/bivariate-normal assumptions and shows how to derive response mean/variance from discretized state probabilities. It presents a state-aggregation procedure where the probability mass in the “failure” aggregate state gives stage-wise failure probability, enabling reliability estimation as a first-passage-type assessment. The later sections provide a conceptual reinterpretation of transition/overlap measures (via a metric/“statistical distance”) to motivate TPM interpretation across scales rather than quantitative reliability gains.","The author states the paper is based on available literature and aims only to show how Markov Chains can be used to model systems at various scales, explicitly noting that no examples are presented because examples exist in cited references. It also notes that in the presented TPM computation, distributions are assumed (normal at a stage and bivariate normal across successive stages), implying reliance on these distributional choices. The paper cautions that “care has to be taken in the choice of densities in computing the transition probability matrix.”","The approach relies on discretizing the response/state space into finitely many states, and results (including $P_f$) can be sensitive to binning/aggregation choices; guidance for selecting the number of states and boundaries is limited. The TPM estimation assumes knowledge/estimation of means, variances, and correlations between successive stages; in practice these may be difficult to estimate robustly with limited inspection/test data, and uncertainty in TPM parameters is not propagated to reliability. The normal/bivariate-normal assumption may be inappropriate for strongly non-Gaussian nonlinear structural responses, and dependence beyond one-step (non-Markov memory) is not deeply treated despite being mentioned as an issue. No validation, benchmarking against alternative reliability methods, or computational cost analysis is provided in this paper.","The paper suggests that integration of Bayesian theory with Markov Chain theory (and developments such as MCMC) enhances application possibilities, and indicates that further studies are being carried out at SERC, Chennai, in the direction of using quantum-interpretation concepts (e.g., density operators/QISP) for interpreting transition probabilities and modelling across scales. It also implies further work is needed on careful selection of densities for TPM construction and on extending applicability from micro- to macro-scales.","A useful extension would be a full Bayesian TPM estimation framework (including credible intervals for $P_f$) to handle sparse/limited data and to quantify epistemic uncertainty in transition probabilities. Robust/nonparametric alternatives to the bivariate-normal assumption (e.g., copula-based or kernel-estimated joint densities) could improve applicability to non-Gaussian nonlinear responses. Empirical validation on real structural monitoring/inspection datasets and head-to-head comparisons with standard structural reliability methods (FORM/SORM, simulation-based time-dependent reliability, stochastic process degradation models) would clarify practical benefits. Developing software/tooling and providing reproducible implementations (e.g., discretization schemes, TPM estimation, first-passage reliability computation) would improve adoption by practitioners.",0708.1566v1,https://arxiv.org/pdf/0708.1566v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:47:44Z
TRUE,Software reliability|System reliability|Reliability growth|Other,"Stochastic process|Parametric (Weibull, etc.)|Other",Event/count data|Other,Not applicable,Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops an analytic expression for the reliability of a module-based software system whose execution transfers control among modules, modeling the control flow as a Markov chain. Reliability is defined as the expected probability of failure-free operation over the input distribution, and is computed via the probability of correct output for a given input, $\pi_t(x)$. The method expresses system reliability in terms of module-level quantities based on testability (probability a module fails for an input, given at least one fault) and a Bayesian-style update of the probability a module still contains an error after successful tests. Two failure severities are modeled: (i) catastrophic failure only, and (ii) both benign and catastrophic failures, where benign failures can persist up to a threshold $n_c$ before being treated as catastrophic. The approach yields closed-form matrix expressions for $\pi_t(x)$ and suggests maximum-likelihood estimation of transition probabilities from module-to-module execution counts during testing, enabling practical reliability estimation from modular test data.","Module testability is defined as $p_i=\Pr(\text{incorrect output of module }i\mid \text{at least one fault, input distribution})$. After $n_i$ successful tests, the posterior probability the module still contains an error is updated as $\alpha_i(t)=\frac{\alpha_i(0)(1-p_i)^{n_i}}{\alpha_i(0)(1-p_i)^{n_i}+1-\alpha_i(0)}$, and input-specific failure probability is $\alpha_i^x(t)=q_i\alpha_i(t)$ where $q_i$ is revealability. For catastrophic-failure-only modeling, with Markov transition matrix $Q$ augmented by absorbing success $S$ and failure $F$, the correct-output probability is $\pi_t(x)=\sum_{i=1}^N[(I_N-\hat Q)^{-1}]_{1i}\,p_{iS}(1-\alpha_i^x(t))$. With benign and catastrophic failures, the state space is expanded to include benign levels $B_1,\dots,B_{n_c}$ and $\pi_t(x)=\sum_{i=1}^N[(I_{Nn_c}-\hat Q)^{-1}]_{1i}\,p_{iS}$.","The paper’s primary results are closed-form matrix expressions for $\pi_t(x)$ (and hence $R_t$) under (a) catastrophic failures only and (b) combined benign/catastrophic failures with a truncation threshold $n_c$ for benign duration. It provides an estimation procedure: transition probabilities $p_{ij}$ can be estimated by MLE from observed transfer counts between modules during testing, and module failure probabilities can be estimated as $\hat\alpha_i^x(t)=x_{iF}/n_i$ (with $x_{iF}$ failures in $n_i$ tests). System reliability over a finite test input set $W$ is estimated by $\hat R_t=\frac{1}{|W|}\sum_{x\in W}\hat\pi_t(x)$. No numerical ARL-style performance tables or empirical benchmark comparisons are reported in the provided text; the contribution is primarily analytical/model-formulation plus estimation guidance.","The benign-failure model assumes benign failures lasting more than a fixed threshold $n_c$ transition into a catastrophic failure region; the authors note this “take[s] the model a little away from reality” but is adopted to simplify calculations and improve practical applicability. They also remark that, ideally, benign vs. catastrophic splitting should be based on system-specific consequence knowledge, which may not be available.","The approach requires specifying/estimating many control-transfer probabilities (especially with benign-failure levels), which may be data-hungry and unstable when some module transitions are rare. The method relies on a Markovian control-flow assumption and implicitly treats transitions and failure behavior as stationary with respect to time/testing stage, which may not hold as software changes during debugging. The testability/revealability quantities ($p_i,q_i$) and the Bayesian-style update for $\alpha_i(t)$ depend on assumptions about fault presence and test representativeness; misspecification of the input distribution or non-representative test suites can bias $R_t$. The benign-failure modeling restricts transitions among benign states (only $B_k\to B_{k-1}$) to reduce parameters, which may not capture realistic recovery/escalation dynamics.",None stated.,"Extend the model to allow arbitrary-length benign failures without a hard threshold $n_c$, e.g., via an explicit duration model or semi-Markov formulation. Develop robustness variants that relax the Markov and stationarity assumptions (e.g., nonhomogeneous transition probabilities across versions/testing stages) and handle uncertainty in $p_i,q_i$ via Bayesian inference with credible intervals for $R_t$. Provide empirical validation on real modular software projects and compare against established SRGMs and architecture-based reliability models, including sensitivity analyses to test-suite representativeness. Create an implementable software tool/package to estimate the Markov parameters from execution traces and to compute $\pi_t(x)$ efficiently for large module graphs.",0710.2740v1,https://arxiv.org/pdf/0710.2740v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:48:10Z
TRUE,Degradation modeling|Maintenance optimization|System reliability|Other,Bayesian|Other,Mixture of types|Other,Condition-based|Predictive|Not applicable|Other,Energy/utilities|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),http://www.norsys.com/download.html|http://www.hugin.com/Products Services/,"The paper proposes a practical methodology for building Bayesian Networks (BNs) to model degradation and preventive maintenance of an industrial (nuclear plant) mechanical system when information comes primarily from expert elicitation. To reduce the infeasible burden of specifying large conditional probability tables, the BN is treated as an unsaturated log-linear model and higher-order interaction terms (order > 2) are initially constrained to zero, effectively adding conditional-independence assumptions among parents given a child. The authors elicit all marginal probabilities plus only first-order conditional probabilities, yielding redundant equations (via total probability) that enable consistency checking; they provide heuristic rules and a feedback procedure to resolve inconsistencies by retaining the “most reliable” probabilities. In a case study on an EDF reactor coolant pump sub-component with 22 discrete variables, the required probabilities drop from 381 (classical BN specification) to 69 under the proposed simplification, and inference highlights influential variables used to motivate maintenance actions. The approach is positioned as a fast, expert-friendly BN construction process that supports diagnosis/decision help and simulation of maintenance strategies under limited data.","The BN joint distribution is factorized as $P(X_1,\ldots,X_n)=\prod_{i=1}^n P(X_i\mid pa(X_i))$. Consistency checks use the law of total probability, e.g., for a node $D$ with parent $A$: $P(D)=\sum_A P(D\mid A)P(A)$ (similarly for other parents), producing redundant equations across different parent sets. Under the added assumption that parents are conditionally independent given the child (log-linear interactions of order $>2$ set to zero), multi-parent conditionals are computed from first-order conditionals, e.g. $P(M6\mid Ad,Ab)=\frac{P(M6\mid Ab)P(M6\mid Ad)}{P(M6)}$ and analogous product-form expressions for nodes with more parents.","In the EDF nuclear mechanical-system BN (22 discrete variables), classical BN parameterization would require 381 probabilities for inference, while the proposed log-linear/conditional-independence simplification reduces this to 69 elicited probabilities. For a specific node ($O2'$), the number of conditional probabilities drops from 192 (full CPT) to 7 (first-order conditionals plus marginals) under the method. After initial inference, experts added nine additional conditional dependencies (selected higher-order associations), and inference indicated three variables (Ab, Ad, PI3) as particularly influential on degradation, motivating targeted maintenance tasks whose effects were then incorporated as additional BN variables.","The authors note that setting all association terms of order greater than two to zero (equivalently, assuming parents are conditionally independent given their child) can be “too restrictive,” and they propose a second stage where experts add selected higher-order (e.g., three-way) associations they consider useful and reliable. They also indicate practical difficulty/instability in heavy expert information acquisition and motivate their approach as a way to avoid an overly burdensome elicitation process.","The heuristic rules for selecting/adjusting probabilities to resolve inconsistencies are not validated against ground truth and may introduce subjective bias or non-unique solutions depending on which constraints are dropped. The strong conditional-independence/product-form approximations can distort the joint distribution and downstream maintenance decisions, especially with common-cause dependencies or interacting degradation mechanisms typical in complex equipment. The paper provides limited quantitative validation (e.g., predictive accuracy, sensitivity to elicitation error, or robustness analyses) beyond counts of parameters and qualitative case-study outcomes, and it does not benchmark against alternative expert-elicitation/BN-parameter learning methods.",None stated.,A valuable extension would be a formal sensitivity/uncertainty analysis quantifying how elicitation errors and the imposed conditional-independence constraints affect posterior inferences and maintenance recommendations. Another direction is to integrate limited operational data via Bayesian updating/parameter learning (with priors from experts) to reduce reliance on heuristics and to compare against structured elicitation protocols. Extending the approach to handle temporal dynamics explicitly (dynamic Bayesian networks) would better capture degradation evolution and maintenance effects over time.,0905.2864v1,https://arxiv.org/pdf/0905.2864v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:48:42Z
TRUE,Life distribution modeling|Other,"Bayesian|Parametric (Weibull, etc.)|Other",Other,Not applicable,Transportation/logistics|Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"This paper argues that reliability (survival) should be used within a decision-theoretic framework that combines an unknown reliability/survival function (treated as “chance/propensity”) with a decision maker’s utility for that chance. The author distinguishes the survival function $F(x\mid\theta)=P(X\ge x\mid\theta)$ (chance/propensity) from the decision maker’s personal probability about survival (survivability) using de Finetti-style exchangeability as motivation. Methodologically, it proposes a functional family for “utility of reliability” (including potential disutility due to cost) and introduces a probability-of-choice (item response/choice model) approach to elicit utilities via repeated binary choices between a sure reliability level $c$ and a gamble that yields 1 vs 0 with chance $p$. A new two-parameter choice-probability model (indexed by $(p-c)$ and parameters governing discrimination and risk attitude) is fit to the binary choice data via MLE and via a Bayesian approach with gamma priors to quantify uncertainty. The approach is illustrated in a live military procurement example (combat vehicle reliability), showing utility saturates around reliability ~0.8 and providing an argument against arbitrary extreme reliability requirements (e.g., 0.999).","The elicitation is based on binary choices between a sure reliability level $c\in(0,1)$ and a $p$-gamble paying 1 with chance $p$ and 0 with chance $1-p$, where utility is anchored by $U(0)=0$ and $U(1)=1$ and indifference corresponds to $p=U(c)$. A proposed choice-probability model for selecting the gamble is (final form):
$$P(Y=1\mid\alpha,\beta;c,p)=\tfrac12\,[1+\operatorname{sgn}(p-c)|p-c|^{\alpha}]^{\beta}\quad\text{(with boundary cases for }p,c\in\{0,1\}).$$
An illustrative “omnibus utility” combines a power utility and a cost disutility: $U(F) = F^{\beta_x}-\left[1-\exp\{-\delta F/(1-F)\}\right]$ for $F\in[0,1]$.","In the military planner example, elicitation was carried out for reliability levels $c\in\{0.5,0.6,0.7,0.8,0.9\}$ using multiple gamble probabilities $p$ per $c$ and fitting the model by MLE and Bayes (gamma priors). Using end-point gambles, the paper reports essentially identical MLE and Bayesian utilities: $U(0.5)=0.5$, $U(0.6)=0.6$, $U(0.7)=0.7$, $U(0.8)=0.93$, and $U(0.9)=0.92$, indicating a utility jump near 0.8 and little/no gain from 0.8 to 0.9. Using adjacent-point gambles, MLE utilities are reported as uniformly somewhat higher than Bayesian utilities for the chosen priors, but both indicate low utility at or below 0.5 and diminishing returns above about 0.8. The substantive conclusion is that very high reliability specifications (e.g., 0.999 or “0 failures in 1000 hours”) may be strategically unwarranted relative to the decision maker’s utility.","The author notes that end-point gambles can yield elicited utilities that are not monotone in $c$ (violating the monotonicity requirement), motivating adjacent-point gambles and post-processing (e.g., isotonic regression) to temper inconsistencies. It is also stated that the approach can still fail invariance with respect to anchor points, and that resolving invariance inconsistencies is difficult. The paper additionally acknowledges it may have missed prior work, so aspects of the model/approach might not be novel.","The reliability quantity being valued is treated abstractly as $F(x\mid\theta)$ at a fixed mission time; the paper does not address how uncertainty in estimating $F$ from field/test data (censoring, covariates, heterogeneity, dependence) propagates into decisions—utility is elicited conditional on hypothetical reliability levels. The choice model assumes conditional independence of responses across gambles (memoryless choices) and stable preferences, which can be violated by framing, learning, fatigue, or inconsistency over the elicitation session. The proposed functional form in (3.3) is ad hoc relative to standard logistic/probit IRT links; sensitivity to link choice and identifiability/fit diagnostics are not systematically explored. The “cost disutility” term is only illustrative and not integrated with the elicitation example, limiting guidance for real procurement where cost/reliability tradeoffs are central.","The conclusion suggests developing more sophisticated utility-elicitation models than the one proposed, pointing to the Grade of Membership (GOM) model as a potentially richer alternative that encompasses aspects of Rasch/IRT structures. It also implies broader application of the choice-model elicitation ideas beyond reliability to any context involving valuation of chance.","Empirically validating the elicitation method across multiple decision makers and domains (with test–retest studies) would clarify robustness and interpersonal variability of utility-of-reliability curves. Extending the framework to jointly elicit utility and willingness-to-pay (or explicit multi-attribute utility including cost, availability, maintainability) would better support real engineering trade studies. Using standard generalized linear IRT links (logit/probit) with hierarchical Bayesian structure could improve interpretability, handle heterogeneity across subjects, and provide better-calibrated uncertainty. Finally, integrating elicited utilities with reliability demonstration testing and life/distribution models (including censored data) would connect preference elicitation to actionable reliability requirements and test plans.",0907.3944v1,https://arxiv.org/pdf/0907.3944v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:49:18Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,SAS,In text/Appendix,http://www.embo.org/yip/index.html|http://embc.embo.org/,"The paper critiques the bibliometric h-index as having insufficient reliability for discriminating among scientists’ research performance because it compresses a full citation distribution into a single value. Using publication/citation data from 297 EMBO Young Investigator Programme applicants in molecular biology, it proposes two complementary descriptive measures, $h^2_{lower}$ and $h^2_{upper}$, which quantify (as percentages of total citations) the portions of the citation distribution ignored by the $h^2$ square captured by the h-index. It also introduces an sRM (segmented regression model) approach that fits a two-part model (quadratic then linear) to cumulative citations vs. paper rank to estimate a breakpoint interpreted as a scientist’s ‘true core’ size (the sRM value). The authors show that scientists with the same h-index can have very different $h^2_{lower}/h^2/h^2_{upper}$ profiles and widely varying sRM values, indicating heterogeneous performance types not reflected by h alone. An example figure and subgroup tables illustrate that $h^2$ typically covers only about a quarter of total citation area, with most area in $h^2_{upper}$, and that sRM values can differ substantially within the same h-index band.","The paper defines citation-area shares relative to total citations: $h^2_{upper}=100\cdot\frac{\sum_{j=1}^{h}(cit_j-h)}{\sum_{j=1}^{n} cit_j}$, $h^2=100\cdot\frac{h\cdot h}{\sum_{j=1}^{n} cit_j}$, and $h^2_{lower}=100\cdot\frac{\sum_{j=h+1}^{n} cit_j}{\sum_{j=1}^{n} cit_j}=100-(h^2+h^2_{upper})$. For the sRM, cumulative citations $y_j$ are modeled as a quadratic for ranks $x_j<z_0$ and a linear continuation for $x_j\ge z_0$, with Gaussian errors; the breakpoint is set to the quadratic maximum $z_0=-b_1/(2b_2)$ (notation as in the paper). Parameters are estimated via nonlinear least squares (e.g., SAS NLIN with Gauss–Newton iteration).","For the full sample (n=297), Table 1 reports average area shares of approximately $h^2_{lower}=7\%$, $h^2=23\%$, and $h^2_{upper}=70\%$, indicating most citation mass lies above the h-square and is not represented by the h-index. For three example applicants with the same $h=14$, the profiles differ markedly: Applicant A ($h^2_{lower}=3\%$, $h^2=15\%$, $h^2_{upper}=82\%$), Applicant B (13%, 48%, 39%), and Applicant C (57%, 33%, 10%). sRM values could be computed for 241/297 applicants; across these, the mean sRM is 14.75 with SD 8.45 and range 2.83–60.73 (Table 2). In an illustrated example with $h=14$, the fitted sRM value is 25.15 with $R^2\approx0.99$, suggesting the h-index underestimates the ‘core’ size by about 10 publications for that case.","The authors state that sRM values cannot be computed when certain requirements are not met: clear two-part citation distribution, algorithm convergence, high fit ($R^2>0.90$), breakpoint within the publication-rank range, and having roughly 15–20+ publications. They also note a drawback of the sRM value: it does not convey the absolute citation counts of the most visible publications, unlike the h-index’s implicit “at least h citations” interpretation.","The proposed measures are specific to bibliometric evaluation and do not address engineering reliability (failure behavior, lifetime/degradation data, or maintenance decisions), so their applicability to reliability engineering is essentially none. The sRM approach assumes a particular functional form (quadratic then linear) and normal-error least squares on cumulative citations; robustness to heavy tails, discreteness, heteroscedasticity, and dependence typical in citation data is not developed. Comparisons focus on descriptive variability rather than decision-theoretic or inferential calibration (e.g., uncertainty intervals for $h^2$ shares or breakpoint estimates, sensitivity to database coverage and author disambiguation).",None stated.,"A useful extension would be to provide uncertainty quantification for $h^2_{lower}/h^2/h^2_{upper}$ and for sRM breakpoint estimates (e.g., bootstrap confidence intervals) and to study robustness under common citation-data issues (outliers, database errors, and author-name ambiguity). Additional work could compare sRM against alternative breakpoint or mixture models (e.g., piecewise-linear, power-law/Zipf fits) and evaluate stability over time as citation counts accrue. Packaging the method as reproducible code (e.g., an R/Python package) and validating on multiple fields with differing productivity/citation norms would improve practical adoption.",0908.3962v1,https://arxiv.org/pdf/0908.3962v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:49:57Z
TRUE,Degradation modeling|Accelerated testing,"Stochastic process|Parametric (Weibull, etc.)",Degradation measurements|Sensor/condition monitoring,Not applicable,Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,http://www.itl.nist.gov/div898/handbook/apr/section4/apr423.htm,"The paper proposes a degradation model where the latent monotone degradation is a gamma process and the observed degradation is perturbed by an independent Brownian motion, i.e., $D_t=Y_t+\tau B_t$, allowing non-monotone observations due to measurement error or minor repairs. It develops method-of-moments estimators for the model parameters $(\xi,\alpha,\tau^2)$ under general sampling with multiple independent items observed at irregular and item-specific time points. The authors establish strong consistency and asymptotic normality of the estimators under summability/bounded-interinspection assumptions, and derive simplified conditions for several common observation schemes (regular and shared inspection times). Performance is illustrated via Monte Carlo simulation (bias/MSE/SD versus sample size) and via two real degradation datasets, including an accelerated setting with multiple temperatures. The analysis also enables asymptotic confidence intervals and hypothesis tests such as testing $\tau^2=0$ (pure gamma process) and testing conditions related to a Brownian-with-drift limit.","The degradation model is $D_t = Y_t + \tau B_t$ with $Y_t$ a gamma process (scale $\xi$, shape rate governed by $\alpha$) and $B_t$ standard Brownian motion, independent. For increments $\Delta D_{ij}$ over $\Delta t_{ij}$, the normalized moments satisfy $m^{(1)}/\Delta t=\alpha/\xi$, $m^{(2)}/\Delta t=\alpha/\xi^2+\tau^2$, and $m^{(3)}/\Delta t=2\alpha/\xi^3$, yielding the moment map $f(\xi,\alpha,\tau^2)=(\alpha/\xi,\alpha/\xi^2+\tau^2,2\alpha/\xi^3)$. Using empirical counterparts $\hat m^{(1)},\hat m^{(2)},\hat m^{(3)}$ computed from irregular increments, the method-of-moments estimators are $\hat\xi=\sqrt{2\hat m^{(1)}/\hat m^{(3)}}$, $\hat\alpha=\hat m^{(1)}\sqrt{2\hat m^{(1)}/\hat m^{(3)}}$, and $\hat\tau^2=\hat m^{(2)}-\sqrt{2\hat m^{(1)}\hat m^{(3)}}\,/2$.","In simulation with true parameters $(\xi,\alpha,\tau^2)=(1,0.02,0.02)$ and three inspections over $T=1000$ (increments 200, 300, 500), empirical bias and MSE decrease as the number of items increases from $n=50$ to 200 (e.g., bias for $\xi$ drops from $2.22\times 10^{-1}$ to $6.25\times 10^{-2}$; MSE for $\xi$ drops from $8.29\times 10^{-1}$ to $7.07\times 10^{-2}$). For the NIST accelerated degradation dataset (15 components at 65/85/105°C measured at 200/500/1000h), estimates show $\xi$ decreasing with temperature while $\tau^2$ and $\alpha/\xi^2$ increase; at 65°C the 95% CI includes $\tau^2=0$, suggesting the pure gamma submodel may suffice. For heating cable accelerated data (15 items at 200/240/260°C, failure threshold $\ln(2)$), estimated standard deviations are very large and the authors caution that it becomes difficult to discriminate between the gamma-only and perturbed models, indicating possible model misfit for that dataset. Theoretical results provide asymptotic normality and confidence intervals/tests for each parameter under assumptions (H1–H3), and discuss when these assumptions hold under several practical sampling schemes.","For the heating cable data application, the authors report very large standard deviations for parameter estimates and state that it is difficult to choose between the two sub-models (gamma-only vs. perturbed), suggesting potential bad fitting of the model in that example. They also note that in some sampling schemes (notably their “case 3” and “case 5”), asymptotic normality and/or consistency cannot be established using their results.","The model assumes independent increments and independence across items, which may be violated when degradation is autocorrelated beyond the Lévy/Brownian structure or when shared environments induce cross-item dependence. The Brownian perturbation is treated as an additive Gaussian term with variance proportional to time, which may not match real measurement systems where error variance is state-dependent, heteroscedastic, or dominated by inspection noise rather than continuous-time diffusion. The estimation approach is method-of-moments; efficiency relative to maximum likelihood is not quantified, and finite-sample behavior may be sensitive to irregular sampling designs and small numbers of inspections per unit (e.g., $N=3$). The paper focuses on parameter inference for degradation paths and does not develop explicit reliability quantities such as first-passage time distributions, RUL prediction, or maintenance decision rules under the perturbed process.",The conclusion states it would be interesting to extend the model to incorporate covariates (since degradation is influenced by environment) and that such a covariate-integrated model will be studied in a forthcoming paper.,"Develop likelihood-based and Bayesian inference for the perturbed gamma model (including prior structure for $(\xi,\alpha,\tau^2)$) and compare efficiency/robustness against moment estimators under realistic inspection designs. Extend the framework to derive and validate reliability metrics (first-hitting-time to a failure threshold) for non-monotone observed degradation, enabling RUL prediction and decision-oriented maintenance optimization. Consider more realistic measurement-error models (discrete inspection noise, heteroscedastic errors, calibration drift) and dependence structures (random effects, shared frailty, covariate-driven Lévy intensity). Provide reproducible software (e.g., an R/Python package) and benchmark studies across multiple public degradation datasets to assess generalizability.",1005.1214v1,https://arxiv.org/pdf/1005.1214v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:50:35Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Other",Complete lifetime data|Other,Not applicable,Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a “socio-political reliability theory” that models polity/regime duration and the risk of polity change using reliability concepts (survival function and hazard/failure rate). Using Polity IV historical data for 48 African countries (1946–2008), it fits a modified Weibull-type survival model with a scale parameter (σ) and two shape parameters (α, β), then classifies countries by the functional shape of their estimated hazard rate (monotonically increasing, monotonically decreasing, U-shaped/bathtub, unimodal). The authors introduce the “Cliometric Number” $C_p=\sqrt{\alpha\beta}$ and use $(\alpha,\beta)$ regions to interpret early-life adaptation vs cumulative-damage phases in political vulnerability. Empirically, they report strong associations between hazard-shape categories and high State Fragility Index levels (e.g., 87.5% of monotonically increasing; 75% of unimodal; 71.43% of U-shaped; and 0% of monotonically decreasing countries have high fragility). They also show a quasi-U-shaped relationship between average polity duration and regime type, with autocracies and democracies having longer average durations than anocracies.","They model polity duration $\Delta t$ via a modified Weibull survival function (up to proportionality):
$$S(\Delta t;\sigma,\alpha,\beta)\propto \left[1-\exp\left(-\left(\frac{\Delta t}{\sigma}\right)^{\alpha}\right)\right]^{\beta}.$$
The hazard (failure) rate is defined from the survival as $h(\Delta t;\sigma,\alpha,\beta)=\frac{S'(\Delta t;\sigma,\alpha,\beta)}{1-S(\Delta t;\sigma,\alpha,\beta)}$, and shape classification is based on parameter-region conditions involving $\alpha$ and $C_p=\sqrt{\alpha\beta}$ relative to 1.","From Polity IV data for 48 African countries, the estimated hazard-shape distribution is reported as: U-shaped/bathtub 29.2% (14 countries) and unimodal 50% (24 countries), with fewer countries in monotone-increasing and monotone-decreasing categories. The paper reports that 87.50% (monotonically increasing), 75% (unimodal), 71.43% (U-shaped), and 0% (monotonically decreasing) of countries in those respective categories have high State Fragility Index levels. Average polity durations by regime type (1946–2008) are reported as: Autocracies 10.65 years (95% CI 9.75–11.56; N=164), Anocracies 5.04 (5.02–5.07; N=53), Democracies 10.20 (8.42–11.99; N=78). Parameter estimates for example countries are provided (e.g., Chad $\sigma=14.81,\alpha=1.946,\beta=0.2602$; Gambia $\sigma=20.9,\alpha=10.22,\beta=0.05641$; South Africa $\sigma=3.11\times 10^{-6},\alpha=0.1042,\beta=71.07$).",None stated.,"The modeling treats polity changes as i.i.d. duration events within each country and does not explicitly address dependence/autocorrelation across successive regimes, time-varying covariates, or unobserved heterogeneity that can strongly affect hazard-shape inference. The survival function is presented “up to proportionality,” leaving ambiguity about normalization/identifiability and how exactly the hazard is computed from the stated form. Estimation is described (least absolute regression with trust-region/Levenberg–Marquardt), but model checking, uncertainty quantification for hazard-shape classification, and sensitivity analyses (e.g., to censoring, missingness, alternative distributions) are not fully developed. The reported associations with State Fragility Index are correlational and may be confounded by shared measurement or omitted variables.","The authors suggest studying instantaneous behaviors of polity change jointly with regime type and other socio-political/economic factors to inform preventive policy design and instability mitigation. They also propose extending the model using repairable-systems reliability theory combined with decision models to develop policies that minimize the costs of de-consolidating political institutions and reduce risks of state failure, civil conflict, and unrest.","A natural extension is to incorporate time-varying covariates (economic indicators, conflict events, institutional measures) in a survival framework (e.g., Cox or parametric AFT models) to separate baseline hazard shape from drivers of change. Bayesian or hierarchical (country-level) models could quantify uncertainty in $(\alpha,\beta)$ and propagate it into the hazard-shape region assignment, improving robustness. Methods for dependent/recurrent event data (renewal/Markov-modulated hazards, frailty models) could better reflect successive regime changes within a country. Broader empirical validation could compare predictive performance against alternative duration models (log-logistic, Gompertz, piecewise exponential) and test out-of-sample forecasting of instability/fragility.",1007.0562v2,https://arxiv.org/pdf/1007.0562v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:51:17Z
TRUE,Network/infrastructure reliability|System reliability|Other,Physics-based|Other,Simulated only,Not applicable,Energy/utilities,Exact distribution theory|Simulation study|Other,NA,None / Not applicable,Not applicable (No code used),NA,"The paper proposes an intrinsic geometric (Riemannian) framework to analyze power-system network reliability and voltage stability during planning/operation under nonlinear power-flow behavior and parameter fluctuations. Network “reliability” and “stability” are mapped to geometric properties of a state-space manifold whose metric tensor is defined as the Hessian of the effective (complex) power with respect to circuit parameters (e.g., r, L, C or impedance variables). Local reliability/stability are assessed via positivity/definiteness conditions (principal minors and determinant of the metric), while global behavior is characterized by the scalar curvature (interpreted as a correlation volume), where curvature divergence signals critical behavior/breakdown risk. The method is illustrated on an IEEE 5-bus system with robustness discussed via variations in line/component parameters; tables provide quantitative checks for RL and RLC components and suggest capacitor ranges for stable operation. The contribution advances power-system reliability/stability assessment by providing an analytic, nonlinear, geometry-based criterion rather than iterative linearized load-flow approximations.","The core definition sets the manifold metric as the Hessian of effective power with respect to parameters: \(g_{ij}=\partial_i\partial_j S(r,X_L,X_C)\) (and similarly \(g_{ij}=\partial_i\partial_j P(r,L,C)\) for real/complex power forms). For RL components, effective power is simplified using \(|V|=1\) and impedance \(Z=\sqrt{r^2+(\omega L)^2}\), yielding closed-form expressions for \(g_{rr}, g_{rL}, g_{LL}\) (Eq. 14) and the determinant \(\det(g)\) used as a reliability test. For RLC components, the full metric is again obtained from the Hessian (Eq. 21), with voltage stability assessed by principal minors (e.g., \(P_2\)) and \(\det(g)\); global behavior is assessed via the scalar curvature \(R\), where divergence indicates instability/criticality.","For the IEEE 5-bus example, the RL-component verification (Table I) reports negative \(\det(g)\) for all listed transmission lines (e.g., \(-8.5\times 10^5\), \(-2.1\times 10^2\), \(-5.46\times 10^7\)), interpreted as indicating inadequate reliability without strengthening/parameter adjustments. For RLC components (Table II), computed \(P_2\), \(\det(g)\), and curvature \(R\) vary by line; one case shows \(\det(g)\approx -11519.5\) (unstable), while others have positive determinants (e.g., 0.68, 0.27, 0.79), and curvature values ranging roughly from \(-4.92\) to \(95.40\). From surface/volume/global stability plots and discussion for the limiting case \(r=0\), the paper recommends a capacitor range approximately \(0.1 < C < 0.5\) p.u. to maintain stability (with instability/curvature blow-ups outside this range).","The authors note that some derived expressions are too cumbersome to present, stating that the general expression of the LC surface minor is “not very elegant to present” and that the general expression for the scalar curvature is “rather intricate.” They also frame the work primarily as a bootstrap/modeling step demonstrated on the IEEE 5-bus system, implying limited empirical validation beyond this illustrative test case.","The reliability notion is defined geometrically (metric positivity/curvature regularity) rather than via standard power-system reliability indices (e.g., LOLE, EENS) or component failure/repair models, limiting comparability to established reliability engineering practice. The evaluation appears largely analytic with an illustrative small test network and parameter sweeps; there is no calibration/validation against real outage, failure, or operational disturbance data. Assumptions such as equilibrium \(|V|=1\) and simplified parameterizations may reduce applicability under realistic operating constraints (uncertain demand, contingencies, protection actions, dynamics, and autocorrelated loads). The paper does not provide implementation details (e.g., computational procedure, numerical stability, sensitivity to parameter estimation error) that would be needed for deployment on large-scale systems.","The paper explicitly states that extending the approach toward “future optimal electricity market designs and planning” is left for future study. More generally, it sketches an outlook toward broader planning applications (e.g., siting generation plants and improving operating capability) but does not detail a concrete methodological extension plan beyond these directions.","A valuable extension would be to connect the geometric stability/reliability criteria to standard utility reliability metrics and to probabilistic contingency analysis (N-1/N-k) with stochastic failure/repair processes. The framework could be tested on larger benchmark systems (IEEE 14/39/118 bus) with realistic operating constraints and compared against established voltage stability and security assessment tools. Developing a numerically robust, scalable algorithm (and releasing software) for computing the Hessian metric, minors, and curvature under uncertainty would improve practical uptake. Incorporating measurement noise and online parameter estimation (e.g., PMU-based) could enable condition monitoring and real-time risk alerts consistent with the model’s intent.",1011.2929v1,https://arxiv.org/pdf/1011.2929v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:52:00Z
TRUE,Maintenance optimization|System reliability|Other,Simulation-based|ML-based|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,http://www.jcss.byg.dtu.dk/,"The paper proposes an efficient reliability-based design optimization (RBDO) strategy for structures whose performance is evaluated by expensive finite-element (FE) models, using adaptive Kriging surrogate models built in an “augmented space” that combines design-variable ranges and aleatory uncertainties. Reliability constraints are imposed via small target failure probabilities, with failure probability estimation performed using subset simulation while the adaptive Kriging model is refined near the limit-state surface using a margin-of-uncertainty criterion based on the Kriging predictive variance. The approach is coupled with a gradient-based optimizer (Polak–He) requiring sensitivity/gradient information of the failure probability with respect to design variables. A detailed application optimizes a ring-stiffened cylindrical shell bay representative of a submarine pressure hull under uncertain geometric imperfections and uncertain material/geometry parameters, where failure corresponds to buckling/collapse pressure falling below the accidental diving depth pressure. Results compare a “worst-case” deterministic design practice against the probabilistic RBDO solution and show that the RBDO approach achieves the prescribed reliability level (e.g., reliability index around 6) with feasible computational cost (reported convergence within ~850 model calls).","The RBDO problem is posed as minimizing cost $c(d)$ subject to deterministic bounds $b_i(d)\le 0$ and reliability constraints $\mathbb{P}[g_l(X(d))\le 0]\le p_{f,l}^0$ (Eq. 14). Kriging models the performance as a Gaussian process $Y(x)=f(x)^T a + Z(x)$ with correlation $R(x-x',\theta)=\exp\{-\sum_k[(x_k-x'_k)/\theta_k]^2\}$, yielding predictor mean $\tilde g(x)=\mu_{\hat Y}(x)=f(x)^T\hat a + r(x)^T R^{-1}(y-F\hat a)$ (Eq. 18). Adaptive refinement targets the margin of uncertainty $\mathcal{M}=\{x:-k\sigma_{\hat Y}(x)\le \mu_{\hat Y}(x)\le k\sigma_{\hat Y}(x)\}$ (Eq. 24) and stops when $\log(p_f^+/p_f^-)\le \varepsilon_{p_f}$.","For the submarine ring-stiffened shell bay example, the FE-based designs are consistently lighter (lower cost) than semi-numerical (SN) designs; in the RBDO setting, the FE-based cost is 0.2356 versus 0.2847 for SN (about 17% reduction). Under the worst-case approach, the optimized FE-based design achieves reliability indices of about $\beta(I_{acc})=4.99$ at accidental depth and $\beta(I_{des})=1.40$ at destruction depth; the SN-based counterpart gives $\beta(I_{acc})=3.81$ and $\beta(I_{des})=2.00$. Under RBDO with target $\beta_0=6$ (i.e., $p_f\approx 10^{-9}$), the optimized FE-based design achieves $\beta(I_{acc})=6.06$ (meeting the target) and $\beta(I_{des})=4.42$, while SN-based yields $\beta(I_{acc})=6.11$ and $\beta(I_{des})=4.99$. The metamodel-based RBDO convergence is reported within roughly 850 calls to the buckling-strength models, with an FE run time noted as about 10 minutes CPU per analysis.","The authors note that approximate semi-numerical code-based solutions embed safety factors and may be conservative, and that their imperfection modeling uses only two selected imperfection modes (e.g., $n=2$, $m=14$), whereas a finer study would consider a broader spectrum that may change with the design during optimization. They also state that addressing geometric uncertainties modeled as random fields would increase stochastic dimension and require more advanced surrogate-fitting algorithms than presented. In the FE model, plasticity is not included; instead a nonlinear elastic Ramberg–Osgood law is used, justified by the claim of no significant unloading until collapse.","The RBDO algorithm relies on accurate gradients of failure probability for a gradient-based optimizer; in practice, gradient estimation can be noisy for very small probabilities (even with subset simulation) and may affect optimizer robustness or convergence guarantees. The augmented-space global Kriging approach can become challenging as input dimension grows (design variables + uncertainties), potentially requiring many more points than the “few hundred” target and risking surrogate miscalibration far from sampled regions. The probabilistic models for imperfection amplitudes and several uncertainties are largely assumed (limited/no data), so the resulting optimized reliability level is sensitive to these modeling choices; this epistemic uncertainty is not propagated into the final reliability statements. Comparisons to other modern surrogate-based reliability/RBDO approaches (e.g., AK-MCS variants, PCE-based methods) are limited, which makes it harder to generalize the claimed efficiency gains.","The paper suggests improving the method to handle geometric uncertainties modeled by random fields, noting that this leads to higher stochastic dimension and would require more advanced surrogate-modeling algorithms. It also mentions that for larger target failure probabilities, the RBDO formulation can be transformed to evaluate quantiles of the limit-state function at each optimization step (citing related work), implying extensions toward quantile-based optimization formulations.","A valuable extension would be to incorporate explicit treatment of model-form uncertainty (e.g., discrepancy between FE/ANM predictions and physical tests) and assess how surrogate and FE modeling errors affect achieved reliability. Developing self-adaptive or multi-fidelity strategies (combining semi-numerical codes and FE runs within a unified surrogate/active-learning framework) could reduce computational cost further while improving robustness. Robust/adaptive designs that update the selected imperfection modes (or use reduced-basis/random-field representations) as the design changes could better capture mode interaction and reduce bias from fixed modal assumptions. Providing open-source implementations and benchmark comparisons on standard RBDO test problems would improve reproducibility and facilitate adoption in engineering practice.",1104.3479v2,https://arxiv.org/pdf/1104.3479v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:52:51Z
TRUE,Software reliability|Reliability growth|Other,"Parametric (Weibull, etc.)|Other",Event/count data|Other,Not applicable,Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes RESID (Reliability Estimation for Software under Imperfect Debugging), a stochastic software reliability model that estimates a software’s overall “unreliability” parameter by modeling the probability that different code chunks are buggy rather than modeling inter-failure times or counting bugs. Software is decomposed into straight-line “chunks,” each initially buggy with probability p, and after each detected bug in a chunk its remaining bugginess probability is reduced multiplicatively by a known inefficiency factor α (imperfect debugging). Given execution logs from repeated test runs (which chunks were visited, whether the run failed, and which chunk was identified as buggy), the paper derives a likelihood for p that incorporates program structure (branching and loops via truncation rules) and estimates p via maximum likelihood. The log-likelihood is shown to be strictly concave, yielding uniqueness of the MLE when it exists, and a sufficient condition for existence is provided. Performance is illustrated via simulation on toy flowcharts, and several practical variants are discussed (chunk-specific probabilities based on lines of code, chunk classification with different α, multi-chunk debugging, and bugs detected but not removed).","Core model: each chunk has initial bugginess probability p; after k debugging events on that chunk, its bugginess becomes p\alpha^k with known \(\alpha\in(0,1)\). For aggregated debugging data, the likelihood takes the form \(L(p)\propto p^m\prod_{i=0}^k (1-p\alpha^i)^{n_i}\), where m is the number of detected bugs and \(n_i\) counts successful (bug-free) executions of chunks having exactly i prior debugging attempts. The log-likelihood is \(\ell(p)=m\log p+\sum_{i=0}^k n_i\log(1-p\alpha^i)\), and the score equation solved numerically is \(\ell'(p)=\frac{m}{p}-\sum_{i=0}^k \frac{n_i\alpha^i}{1-p\alpha^i}=0\).","A key theoretical result is that \(\ell(p)\) is strictly concave for any program structure and debugging outcomes, implying uniqueness of the MLE when it exists. A sufficient condition for existence and uniqueness of the MLE over \((0,1)\) is \(m>0\) and \(n_0>0\) (at least one bug observed and at least one initially-successful chunk execution). Simulation on a 4-chunk toy flowchart (with \(\alpha=0.9\)) shows the log-likelihood peaks near the true p values for p = 0.2, 0.4, 0.6, 0.8 after 100 runs. In additional simulations (50 runs repeated 100 times) across p = 0.3, 0.6, 0.9 and \(\alpha\in\{0.3,0.6,0.9\}\), the reported mean MLEs are close to the true p (e.g., for p=0.9 the mean estimates are 0.9178, 0.9083, 0.9006) with variances on the order of 0.002–0.015 depending on p and \(\alpha\).","The authors note that proper evaluation of RESID should be done in an industrial setup on a large, complex software system; the paper only provides results on a simulated toy example. They also highlight practical difficulty in pinpointing which pass through a loop first triggered a bug, motivating truncation of loop execution logs. Additionally, they acknowledge that assuming identical a priori bug probability for all chunks may be unrealistic and discuss variants to address this.","The approach treats the debugging inefficiency factor \(\alpha\) as known, but in practice \(\alpha\) is typically unknown and may vary by developer, module, or time; fixing it risks biased p estimates. Independence assumptions between chunks’ bugginess (and between bug-triggering events) may be violated in real code due to shared design/implementation patterns, copy-paste, and coupled logic, which could distort likelihood-based inference. The method relies on accurately identifying the “buggy chunk” per failure; mislocalization (common in debugging) is not modeled and could materially affect estimates. Finally, validation is limited to small synthetic control-flow graphs; robustness to complex architectures, concurrency, non-determinism, and changing operational profiles is not assessed.","The paper suggests that full evaluation should be carried out in an industrial environment with large, complex software. It also proposes practical extensions/variants to suit real needs: allowing chunk-specific bug probabilities (e.g., via lines-of-code), classifying chunks with different debugging inefficiency factors, handling cases where multiple chunks are corrected after a failure, and accommodating bugs that are detected but not removed.","A natural extension is to estimate \(\alpha\) jointly with p (or to model \(\alpha\) hierarchically by chunk/category) rather than assuming it known, enabling data-driven quantification of debugging effectiveness. Developing a self-starting/online version with uncertainty quantification (confidence/credible intervals for p and per-chunk bugginess) would make the method more actionable for release decisions. Robustness to dependent chunks, misidentified fault locations, and correlated test inputs could be studied via sensitivity analyses or alternative dependence models. Finally, packaging the approach into a usable toolchain (e.g., instrumentation plus an estimator library) and benchmarking on public defect datasets would strengthen practical adoption.",1104.3503v1,https://arxiv.org/pdf/1104.3503v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:53:33Z
TRUE,Maintenance optimization|System reliability|Other,Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"This paper proposes an efficient strategy for reliability-based design optimization (RBDO) when performance/limit-state functions are expensive to evaluate (e.g., finite-element structural models). It builds kriging (Gaussian process) surrogate models of limit-state functions, and adaptively refines the design of experiments near uncertain regions of the limit-state surface using a population-based scheme (MCMC sampling from a refinement criterion plus K-means clustering). Failure probabilities and reliability sensitivities (gradients of failure probability/reliability index w.r.t. design variables) are then estimated using subset simulation to reduce Monte Carlo cost, and embedded in a gradient-based nested optimization loop. A key contribution is constructing surrogates in an “augmented reliability space” so the same kriging models can be reused across RBDO iterations, and propagating surrogate uncertainty to an empirical error measure (via upper/lower “shifted” limit states) to decide when refinement is sufficient. The approach is demonstrated on structural mechanics benchmarks, achieving orders-of-magnitude reductions in true model evaluations compared with direct simulation-based nested RBDO, while providing an explicit surrogate-induced reliability-estimation error indicator.","RBDO is posed as minimizing cost $c(\theta)$ subject to deterministic constraints and probabilistic constraints $P_f(\theta)=P(g(X(\theta))\le 0)\le P_f^0$ (Eq. 1–2). The kriging surrogate assumes $Y(x)=f(x)^T\beta+Z(x)$ with stationary covariance $\sigma_Y^2 R(\|x-x'\|,\ell)$ (Eq. 3–5), giving predictive mean $\mu_{\hat Y}(x)$ and variance $\sigma^2_{\hat Y}(x)$ (Eq. 11–12). Refinement targets the “margin of uncertainty” $\mathcal M=\{x:-k\sigma_{\hat G}(x)\le \mu_{\hat G}(x)\le k\sigma_{\hat G}(x)\}$ (Eq. 17) using $P(x\in\mathcal M)$ (Eq. 18) and checks accuracy via reliability indices of shifted failure sets $\hat F_i=\{x: \mu_{\hat G}(x)+ik\sigma_{\hat G}(x)\le 0\}$ and $\hat\beta_i=-\Phi^{-1}(P(X\in\hat F_i))$ (Eq. 21–24). Reliability sensitivity uses $\partial P_f/\partial\theta = \mathbb E[\mathbf 1_{g\le 0}(X)\, (\partial f_X/\partial\theta)/f_X]$ with a sample estimator (Eq. 29–33).","On an elastic buckling column example with an analytical reference, the method converges to the correct optimum (reported around $\mu_b=\mu_h\approx 231\,\text{mm}$ for target $\beta_0=3$) using only about 20 true performance-function evaluations via kriging refinement, versus about $4\times 10^6$ evaluations for the same nested scheme using subset simulation on the true model. On the Royset et al. short-column benchmark, the proposed approach with kriging attains an optimum near $b\approx 379\,\text{mm}$, $h\approx 547\,\text{mm}$ with estimated $\beta_{\text{sim}}\approx 3.32$ using about 140 true model calls, compared with about $19\times 10^6$ calls without kriging. On the bracket structure benchmark with two constraints and target $\beta_0=2$, the kriging-based approach achieves approximately $\beta_{\text{sim},1}\approx 2.01$, $\beta_{\text{sim},2}\approx 2.03$ with roughly 100 and 150 true model calls for the two limit states, whereas direct simulation-based nesting required on the order of $5\times 10^6$ calls per limit state.","The authors note that the number of required experiments (DOE size) increases with the number of variables, and that kriging loses numerical efficiency when the DOE contains more than a few thousand experiments. They indicate this scalability issue requires further investigation, especially for problems with higher dimension and expensive nonlinear finite-element models.","The approach relies on kriging model-form choices (trend and kernel) and MLE-fitted hyperparameters; the predictive variance used for refinement can underestimate true surrogate uncertainty (model misspecification and hyperparameter uncertainty), which may affect the empirical error bounds on reliability. The refinement and stopping criterion based on shifted limit states ($\mu\pm k\sigma$) is heuristic and does not guarantee bracketing of the true failure domain, especially for non-Gaussian surrogate errors or poorly calibrated kriging variance. Comparisons focus on a few academic examples; broader validation on real industrial datasets/models, higher-dimensional problems, and sensitivity to tuning parameters (e.g., $k$, $\varepsilon_\beta$, K-means cluster count, MCMC settings) is limited.","They explicitly state that the scalability issue (growing DOE size with number of variables and reduced kriging efficiency beyond a few thousand experiments) needs further investigation. They also mention an ongoing study on a problem involving a nonlinear finite-element-based performance function with about 10 variables, to be reported in a forthcoming paper.","A promising extension is to incorporate Bayesian or fully probabilistic treatment of kriging hyperparameters (or calibration techniques) so the refinement/stopping rule reflects both surrogate and hyperparameter uncertainty. Developing robust/self-starting versions for cases with model bias, nonstationarity, or strong discontinuities in limit-state behavior would improve practical reliability. Extending the method to multiple competing failure modes with joint/system failure constraints (series/parallel systems) and to correlated or time-dependent reliability problems would broaden applicability. Providing open-source implementations and standardized benchmarks, plus guidance on automatic selection of refinement batch size and MCMC settings, would help adoption in engineering workflows.",1104.3667v1,https://arxiv.org/pdf/1104.3667v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:54:24Z
FALSE,NA,Nonparametric/Semi-parametric|ML-based|Bayesian|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Environmental monitoring,Simulation study|Case study (real dataset)|Other,TRUE,R,Supplementary material (Journal/Publisher),http://www.meteo.psu.edu/~mann/supplements/MultiproxyMeans07/|http://www.cru.uea.ac.uk/cru/data/temperature/|http://www.image.ucar.edu/~boli/research.html|http://www.uoguelph.ca/~rmckitri/research/StupakResponse.pdf|http://republicans.energycommerce.house.gov/108/home/07142006_Wegman_Report.pdf|http://www.blakemcshane.com|http://www.adiwyner.com,"This paper evaluates the statistical reliability of millennial-scale temperature reconstructions built from large collections of climate proxies (tree rings, ice cores, etc.) in a high-dimensional, autocorrelated setting (p≫n). The authors benchmark proxy-based predictive skill via block cross-validation and compare against multiple null “pseudo-proxy” models, including highly autocorrelated and nonstationary noise processes, finding that sophisticated noise can match or outperform real proxies in predicting 30-year holdout blocks. They show that many reconstruction methods have similar instrumental-period cross-validated RMSE yet yield sharply different historical backcasts, implying that predictive accuracy alone is insufficient for selecting a unique climate history. They develop a Bayesian reconstruction model combining principal components of proxies with an AR(2)-type structure and produce pathwise uncertainty bands that account for parameter uncertainty, which are substantially wider than typical pointwise intervals in the climate literature. Overall, they conclude that proxy data provide weak predictive power for decadal gradients/levels (e.g., the 1990s run-up) and that reconstruction uncertainty is often underestimated.","The paper uses Lasso regression for proxy-to-temperature prediction: \(\hat\beta=\arg\min_\beta \sum_i (y_i-\beta_0-\sum_j x_{ij}\beta_j)^2+\lambda\sum_j |\beta_j|\), with \(\lambda\) chosen by repeated K-fold CV. For reconstruction with uncertainty, they fit a Bayesian regression with proxy principal components and an AR(2)-style term: \(y_t=\beta_0+\sum_{i=1}^{10}\beta_i x_{t,i}+\beta_{11}y_{t+1}+\beta_{12}y_{t+2}+\varepsilon_t\), \(\varepsilon_t\sim N(0,\sigma^2)\), with weakly-informative priors \(\beta\sim N(0,1000I)\), \(\sigma\sim\text{Unif}(0,100)\). Backcasts are generated iteratively “one-step-behind,” propagating parameter and residual uncertainty to obtain pathwise credible bands.","Across 30-year contiguous holdout blocks (1850–1998), proxy-based Lasso predictions are only marginally better than simple baselines and are outperformed by ARMA temperature-only forecasts on most blocks (reported as ARMA beating proxies 86% of the time in their setup). When compared to null pseudo-proxies, proxies beat weak AR(1) nulls but are not statistically significant against empirical-AR(1) and Brownian-motion pseudo-proxies, which often have lower holdout RMSE despite being independent of temperature. In variable-selection tests where true proxies are augmented with pseudo-proxies, pseudo-proxies are selected by Lasso at substantial rates (about 28%–53% depending on pseudo-proxy type), indicating weak distinguishable signal. In the Bayesian reconstruction, uncertainty bands widen markedly when parameter uncertainty is propagated, and the model still fails to capture the high level and sharp run-up of 1990s temperatures even in-sample and in contiguous holdout forecasting.","They note that validation on overlapping/interior contiguous blocks yields highly dependent RMSE values because adjacent blocks share years and the series is autocorrelated, limiting the effective amount of new information per block. They also acknowledge that predicting instrumental-period blocks only evaluates short-range predictive ability and “says little” about the legitimacy of extrapolating centuries back in time. They caution that smoothing and other preprocessing choices can inflate correlations and complicate valid uncertainty quantification, with tuning choices difficult to select blindly.","The work is not reliability engineering; its conclusions about “reliability” pertain to statistical validity of climate reconstructions, so the factsheet categories only partially fit. Their main Bayesian model includes future-temperature terms (\(y_{t+1},y_{t+2}\)) for backcasting, which may complicate interpretation and could leak information in some validation setups unless carefully handled; the paper’s validation focuses on blocks but does not fully explore sensitivity to this structural choice. The null-model comparisons hinge on particular pseudo-proxy generators and the chosen CV/blocking scheme; different proxy preprocessing, hierarchical spatial models, or non-linear proxy–temperature relationships could change relative performance. The paper provides limited direct mechanistic/physics-based modeling of proxy formation, so “weak signal” may partly reflect model mismatch rather than purely data limitations.","They suggest exploring different holdout block lengths (explicitly mentioning smaller blocks such as 15 years as a possible extension) while noting potential concerns about dependence and overfitting. They discuss the possibility that smoothing could enhance signal but emphasize that selecting smoothing methods/tuning parameters raises additional statistical difficulties and would require careful treatment. They also note outstanding questions related to proxy data quality and the need for replication and open inquiry, though they do not develop specific methodological extensions in detail.","Develop hierarchical spatiotemporal Bayesian models that explicitly link local proxies to local temperatures with measurement-error models and physically informed proxy forward models, rather than relying primarily on PCA/linear regression. Add robustness checks for nonstationarity/regime changes in proxy–temperature relationships and evaluate with more rigorous time-series cross-validation (e.g., rolling-origin, non-overlapping blocked CV with effective sample size adjustments). Provide open-source implementations (e.g., an R package) to standardize pseudo-proxy benchmark generation and reconstruction comparisons across studies, and expand comparisons to modern regularized/ensemble learners while guarding against leakage and autocorrelation-driven spurious skill.",1104.4002v1,https://arxiv.org/pdf/1104.4002v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:55:08Z
FALSE,NA,Bayesian|Other,Other,Not applicable,Environmental monitoring,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This short discussion critiques statistical approaches used in paleoclimate temperature reconstructions, arguing that uncertainty has often been underestimated and that richer process-based statistical modeling is needed. It questions the common assumption that proxy observations are linearly related to climate variables and is skeptical of dimension-reduction approaches (principal components/EOFs) used without sufficient underlying physical/statistical modeling. The author also critiques reliance on stationary time-series structure (e.g., AR(2)) after regression on principal components with time-constant coefficients, suggesting spatially distributed and proxy-dependent regression coefficients as a more modern Bayesian alternative. The discussion notes that important climate drivers (e.g., atmospheric CO2, solar variability, ENSO/PDO) are omitted from the analyzed models, limiting interpretability of back-casts. It concludes that climate policy questions cannot be resolved by statistics alone and should combine statistical analysis with climate science and climate-model evidence under different forcing scenarios.",Not applicable,NA,"The author argues that key modeling assumptions used in the target article are questionable: linear proxy–climate relationships, heavy reliance on principal components/EOF summaries, and stationarity assumptions (e.g., AR(2)) after regression on PCs with time-constant coefficients. He also states that ignoring known climate controllers (CO2, solar, ENSO/PDO) undermines the usefulness of such reconstructions for back-casting.","Because this is a brief discussion rather than a full methodological paper, it does not provide a concrete alternative model specification with estimation details, validation, or quantitative comparison to the criticized approaches. The critique is largely qualitative and does not demonstrate, via sensitivity analysis or reanalysis, how much each assumption (linearity, PC reduction, stationarity, omitted forcings) changes reconstruction uncertainty or conclusions. No reproducible analysis, benchmark datasets, or implementation guidance is provided for practitioners.","The author suggests pursuing richer statistical analyses that model the underlying processes and data (rather than small fixes), including considering spatially distributed and proxy-dependent regression coefficients within a modern Bayesian framework. He also suggests incorporating additional relevant data/forcings (e.g., CO2, solar variability, ENSO/PDO) and combining statistical reconstructions with climate-system model information under anthropogenic and natural forcing scenarios, with uncertainty quantification.","Develop and evaluate full hierarchical Bayesian proxy–climate models that allow nonlinear proxy response, time-varying coefficients, and nonstationary spatiotemporal dependence, then compare against PC/EOF-based regressions using standardized out-of-sample skill metrics. Add robustness checks for proxy selection, measurement error, temporal autocorrelation, and missingness, and quantify contributions of individual forcings through causal-inference-aware frameworks (e.g., state-space models with forcing covariates). Provide open-source software and reproducible workflows to enable independent verification and routine uncertainty reporting.",1104.4171v1,https://arxiv.org/pdf/1104.4171v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:55:24Z
FALSE,NA,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Stochastic process|Other",Other,Not applicable,Environmental monitoring|Other,Simulation study|Other,TRUE,MATLAB,Supplementary material (Journal/Publisher),http://rainbow.ldeo.columbia.edu/˜alexeyk,"This paper is a discussion of McShane and Wyner (2011) on paleoclimate temperature reconstructions and argues that the “reliability” at issue is statistical predictive skill, not reliability engineering. It reproduces MW2011-style cross-validation experiments using ridge regression (instead of the Lasso) on a large proxy set (p=1138, n=149 with nc=119 calibration years) and compares performance against several classes of synthetic “noise” predictors (white noise, AR(1) with high persistence, Brownian motion). The main finding is that highly persistent noise predictors (e.g., AR(1) with ϕ≥0.9) can outperform real proxies in holdout RMSE, and this persists under ridge regression. The author explains this via a large-p limit: when predictors are i.i.d. draws from a fixed covariance structure, the ridge regression reconstruction converges in probability to a deterministic linear smoother equivalent to a kriging/objective analysis estimator in time driven by the noise autocovariance. Consequently, using persistent noise as a null benchmark effectively pits proxy regression against a different skilled model (temporal interpolation), so concluding proxies are “useless” from that comparison is not justified; instead, proxy models should be combined with explicit temporal dependence constraints (e.g., AR structure) and then tested for incremental contribution.","The reconstruction is based on the linear ridge-regression mapping for a holdout block: $\hat y_v = R[S_p,\lambda,e]y_c$, where $S_p = X_e X_e^T/p$ (with standardized predictors $X_e$), $e=n_c^{-1}\mathbf{1}_{n_c}$, and $R[S,\lambda,w]=S_{vc}(S_{cc}+\lambda I)^{-1}W[w]+\mathbf{1}_{n_v}w^T$ with $W[w]=I-\mathbf{1}_{n_c}w^T$. For i.i.d. noise columns $x\sim N(0,\Phi)$, $S_p\xrightarrow{P}\Psi=E(\tilde x\tilde x^T)$ and with GCV-selected $\lambda_{\min}=\ell[S,e]$, the mapping converges: $\hat y_v\xrightarrow{P}B[\Psi,e]y_c$; in a simplified setting this reduces to a kriging-like predictor $\Phi_{vc}(\Phi_{cc}+\ell(\Phi,0)I)^{-1}y_c$.","Cross-validated RMSE for ridge regression is smaller than the corresponding Lasso RMSEs reported by MW2011, while preserving the qualitative ranking across experiments. As in MW2011, temporally persistent noise (Brownian motion or AR(1) with $\phi\ge 0.9$) outperforms real proxies in holdout RMSE. For AR(1) noise with $\phi=0.99$, the p→∞ probability-limit RMSE is extremely close to the ensemble mean RMSE (reported RMS difference ≈ $1.3\times 10^{-3}\,^{\circ}$C), indicating the null benchmark becomes nearly deterministic at large p. A simple-kriging analogue using an exponential semivariogram with GCV-selected nugget yields RMSE close to the AR(1) ensemble mean (RMS difference ≈ $5.4\times 10^{-3}\,^{\circ}$C), supporting the interpretation that the “noise” null behaves like temporal interpolation rather than a meaningless baseline.",None stated.,"This is a methodological commentary focused on explaining MW2011’s null behavior; it does not develop or validate a general-purpose reconstruction framework, nor does it provide systematic sensitivity analyses across alternative proxy preprocessing, dependence structures, or calibration/validation schemes beyond the MW2011 design. The theoretical argument relies on assumptions about GCV behavior (existence/uniqueness/continuity of the minimizer) and i.i.d. noise columns; departures from these conditions could change the large-p convergence and the kriging analogy. The work is not framed for engineering reliability contexts (failure/degradation/maintenance), so its findings do not transfer directly to reliability engineering applications.","The paper suggests that multivariate regressions on proxy data would benefit from incorporating additional constraints on temporal variability of the target series (e.g., an AR model) and that, after combining proxies with such a temporal model, one should test the significance of the proxies’ contributions to the combined reconstruction.","A natural extension would be to formalize and compare hybrid models that explicitly combine proxy regression with time-series state-space/AR structures, including principled hypothesis tests for incremental proxy value under dependence. Additional work could study robustness of the null comparison under alternative pseudo-proxy constructions, nonstationary covariance, and parameter-uncertainty in the temporal dependence model, and provide open, reusable software implementations (e.g., in R/Python) to standardize benchmarking across reconstruction studies.",1104.4174v1,https://arxiv.org/pdf/1104.4174v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:56:06Z
FALSE,NA,Other,Other,Not applicable,Environmental monitoring,Other,TRUE,R,Supplementary material (Journal/Publisher),https://doi.org/10.1214/10-AOAS398CSUPP,"This paper is a discussion of McShane and Wyner’s climate proxy reconstruction article, focusing on time series modeling considerations for annual mean Northern Hemisphere land temperatures (1850–1999) and proxy-derived covariates. The authors use differencing and autocorrelation diagnostics to argue the differenced temperature series appears stationary with an ACF pattern consistent with a moving-average-like structure (suggesting a unit-root-type behavior in levels but not a random walk). They highlight risks of spurious regression when covariates mimic the response dependence structure, and they recommend exploring lagged relationships (time synchronization) between proxies and temperature rather than using contemporaneous covariates only. They illustrate whitening a principal component (from PCA of proxies), examining residual outliers/variance changes, and computing cross-correlations with temperature to identify potentially meaningful lag structure (e.g., peaks at lags around 14 and 28 years). R code to reproduce the figure diagnostics is provided as journal supplementary material.","The discussion proposes an additive decomposition $Y_t = X_t + Z_t$, where $\{Z_t\}$ is IID with mean 0 and variance $\sigma^2$, and the signal $X_t$ evolves slowly with small increments $\nabla X_t = X_t - X_{t-1}$ having little temporal dependence. Differencing is defined as $\nabla Y_t = Y_t - Y_{t-1}$ and is used for ACF-based diagnostics; cross-correlation is examined between the temperature series and whitened PCA-factor residuals $\hat u_t$ across lags.","The differenced temperature series ACF reportedly shows a prominent spike around −0.5 at lag 1, small values at lags 2–3, and near-zero beyond lag 4, which the authors interpret as resembling a classical moving-average signature (with a unit-root-like feature in levels) and as evidence against a random walk model. After whitening the leading PCA component (via an ARMA fit), the contemporaneous cross-correlation with temperature at lag 0 is described as virtually zero. The largest statistically significant cross-correlations are reported at lags $h=14$ and $h=28$, suggesting a periodicity of about 14 years and motivating consideration of lagged covariates rather than contemporaneous ones. The whitened PCA residual series shows two large outliers (around 1930 and 1970) and a possible variance increase in the last ~30 years, suggesting nonstationary features/outlier handling could matter for modeling.",None stated.,"This is a short discussion piece rather than a full methodological reliability/forecasting paper; it does not provide a fully specified fitted model, formal estimation details, or systematic performance evaluation (e.g., out-of-sample error/ARL-style metrics) for the proposed modeling ideas. The interpretations based on visual ACF/cross-correlation diagnostics may be sensitive to preprocessing choices (differencing, whitening model selection) and multiple-comparisons issues when scanning many lags and many proxies/components. The suggested lagged proxy effects raise identifiability/physical-plausibility questions that are not resolved here (e.g., proxy dating uncertainty, lead–lag ambiguity, and confounding).","The authors suggest more fully exploring lagged effects with covariates (time synchronization of proxies) and considering more sophisticated time series/regression approaches, including transfer-function-style models using a small set of strategically chosen covariates. They also suggest investigating connections between outliers/structural breaks in covariate series and features such as slope changes in the temperature series, and exploring intervention/nonlinear effects as potential improvements.","A natural extension would be a systematic model comparison framework (e.g., rolling-origin validation) that evaluates lagged-transfer-function models against proxy regression and pure time-series baselines under consistent tuning and uncertainty quantification. Robust methods that explicitly accommodate dating uncertainty in proxies, outliers, and heteroskedasticity/regime changes could be developed to stabilize lag inference. Broader sensitivity analyses (different whitening models, alternative differencing/detrending, and multiple-testing control across lags/components) would strengthen the evidentiary basis for proposed lead–lag structure.",1104.4176v1,https://arxiv.org/pdf/1104.4176v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:56:36Z
FALSE,NA,Other,Other,Not applicable,Environmental monitoring,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This discussion paper critiques statistical methods used for paleoclimate temperature reconstructions from multiple proxy records, focusing on model formulation and validation rather than reliability engineering. The authors argue that reconstruction approaches should better exploit spatiotemporal proxy data (instead of spatially aggregated summaries) and advocate hierarchical spatiotemporal modeling to represent dependence and multiple error sources. They recommend reframing proxy–temperature regression as a calibration/measurement-error problem (temperature predicting proxies), caution against proxy-selection practices that discard incomplete series, and note substantial heterogeneity among proxy types that should be modeled explicitly. They also express concerns about PCA/LASSO-style dimension reduction being used in a “data-mining” fashion without incorporating scientific constraints, and suggest process-based proxy models may improve realism. Validation via comparisons to random (unrelated) proxy series is discussed as potentially unsurprising for short-horizon interpolation under dependent errors, with performance depending on proxy temporal characteristics.",Not applicable,Not applicable,None stated,"As a short discussion piece, it does not present a fully specified alternative reconstruction/validation model, formal diagnostics, or quantitative comparisons demonstrating the impact of the proposed improvements. It does not provide implementation details, computational strategies, or software/code to operationalize the recommended hierarchical spatiotemporal calibration framework. The piece is not focused on engineering reliability, so reliability metrics (failure times, hazard/repair models, maintenance policies) and associated empirical validation are outside its scope.",None stated,"Develop and benchmark hierarchical spatiotemporal calibration models that explicitly represent proxy-specific measurement error, temporal smoothing, and spatial dependence, and compare them quantitatively against regression/PCA-based approaches under multiple validation regimes. Study robustness to missing proxy records and heterogeneous proxy frequencies via principled missing-data models and multi-resolution spatiotemporal frameworks. Provide reproducible software implementations to facilitate broader adoption and transparent evaluation across proxy networks and reconstruction targets.",1104.4178v1,https://arxiv.org/pdf/1104.4178v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:56:51Z
FALSE,NA,Bayesian|Nonparametric/Semi-parametric|Other,Other,Not applicable,Environmental monitoring|Other,Other,TRUE,None / Not applicable,Supplementary material (Journal/Publisher),NA,"This discussion critiques a paper on the reliability of millennial surface-temperature reconstructions from multiple climate proxy series using linear regression methods (including Lasso-style approaches). It argues that, for the Mann et al. (2008) proxy network considered in the main article, the data do not support reliable temperature prediction and that random “pseudo-proxies” can perform similarly. The author emphasizes that other established paleoclimate reconstruction traditions (e.g., organism assemblage data from sediment cores with space-for-time substitution) can be more local, ecologically informed, and potentially more predictive than the global regression approach. The discussion highlights Bayesian reconstruction as advantageous because it yields posterior distributions enabling joint (pathwise) uncertainty statements about past climate features. It also promotes “scale space” (multi-level) smoothing and credibility mapping to interpret reconstructed temperature variation across multiple time scales, noting that code and data to reproduce an example figure are provided as supplementary material.","The discussion frames reconstructions as observations of an underlying true temperature curve: $y_i = \mu(t_i) + \varepsilon_i$, for $i=1,\ldots,n$, where $\mu(t)$ is the true past temperature and $\varepsilon_i$ are reconstruction errors. After specifying priors for $\mu$ and $\varepsilon_i$, one can obtain the posterior distribution of the derivative $p(\mu'\mid y_1,\ldots,y_n)$, then apply smoothing at multiple levels to assess credible features (e.g., credible positive/negative trends) across time scales. It is noted that the framework can be extended to correlated errors and uncertainty in time points $t_i$.","No numerical run-length/ARL-type results are reported because this is not an SPC/reliability paper; it is a discussion/critique in applied statistics/paleoclimate. The main empirical claim relayed from the main article is that, for the Mann et al. (2008) dataset and the studied regression approach, prediction is not reliable and random artificial proxy records can match or exceed predictive performance. The discussion’s concrete applied output is a scale-space credibility map (Figure 1) for a diatom-based reconstruction over ~800 years, qualitatively indicating credible cooling (Little Ice Age) at multiple scales and credible recent warming across scales. Code and data to reproduce Figure 1 are stated to be available in the supplement.",None stated.,"Because this is a discussion rather than a full methods paper, methodological and computational details needed for replication (priors, smoothing operators, credibility thresholding, and diagnostics) are not provided in the text itself and are deferred to cited work/supplement. The piece focuses on qualitative critique and illustrative scale-space visualization rather than systematic benchmarking across multiple datasets, proxy types, or reconstruction methods, limiting generalizability of performance claims. It does not address how sensitive the scale-space credibility conclusions are to choices like prior specification, error model, or smoothing family in the illustrative example.",None stated.,"A natural extension would be a systematic comparative study of local, ecologically informed Bayesian reconstructions versus global proxy-network regression methods under matched validation protocols (e.g., hindcasting with proper temporal dependence handling). Additional work could formalize sensitivity/robustness analyses for scale-space credibility maps (priors, correlated errors, dating uncertainty) and provide open, reusable software implementing the full workflow for broader adoption. Another direction is integrating hierarchical ecological response models with modern regularization/latent-factor proxy models to bridge local mechanistic plausibility with larger-scale reconstructions.",1104.4185v1,https://arxiv.org/pdf/1104.4185v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:57:18Z
FALSE,Other,Other,Other,Not applicable,Environmental monitoring,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Supplementary material (Journal/Publisher),http://www.ldeo.columbia.edu/˜jsmerdon,"This paper is a discussion of McShane and Wyner (2011) on statistical reconstruction of Northern Hemisphere temperatures from paleoclimate proxy networks, focusing on the interpretation of cross-validation results. Smerdon reproduces parts of the original cross-validation experiments and adds new experiments where instrumental temperature grid-cell series are perturbed with varying levels of white noise (0%, 50%, 80%, 94% by variance) and red noise (86% by variance with ρ=0.32) to mimic proxy-like noise. Using Lasso-based prediction and 30-year holdout blocks, he shows that even predictors containing true temperature signals can perform comparably to or worse than highly persistent null models (e.g., AR(1) and Brownian motion) under the MW11 testing setup. He argues this indicates the MW11 hypothesis test is prone to Type II errors and is therefore not suitable for concluding that proxies are “severely limited” as temperature predictors. The discussion also notes that results may depend on predictor-count and sampling design, and calls for further testing under alternative sampling scenarios.","The discussion describes creating predictor series by adding noise to instrumental temperature time series, with white-noise variance fractions (e.g., 94%) and a red-noise component specified by an AR(1) structure with ρ = 0.32 and 86% variance contribution. Predictive skill is evaluated via cross-validated RMSE using 30-year holdout blocks, and additional composite-plus-scale (CPS) reconstructions are compared to the CRU Northern Hemisphere mean index using correlations (e.g., r = 0.73 and r = 0.62). No new closed-form run-length/ARL or reliability-theory equations are derived, as this is not an SPC/reliability charting paper.","Cross-validation performance (RMSE) degrades as noise is added to instrumental predictors; with 86% red noise and 94% white noise the perturbed instrumental predictors perform comparable to or worse than the proxy network in the reproduced MW11 setup. Simple area-weighted CPS reconstructions using these noisy predictor sets still track the CRU NH target well, with reported correlations of 0.73 (86% red-noise predictors) and 0.62 (94% white-noise predictors). Despite containing temperature signal, these noisy predictors also fail to outperform the AR1(Emp) and Brownian-motion null models in MW11-style cross-validation, supporting the claim that the test can suffer Type II errors. The paper highlights that persistence plus short validation windows and selection among many noise draws can make null models appear superior.","The author notes several caveats due to the brief discussion format and acknowledges dependencies that are not explored. He states that using a more realistic sampling pattern matching true proxy locations (including reduced ocean sampling and regional clustering) could worsen cross-validation skill relative to his random sampling, while the NH-concentrated proxy distribution could improve NH index prediction. He also notes he sampled each grid cell once rather than allowing multiple proxies per grid cell, which would reduce effective noise and complicate interpretation of noise–skill dependence.","As a discussion, the work relies heavily on one validation paradigm (block cross-validation RMSE) and does not systematically explore alternative skill metrics, different block lengths, or other reconstruction frameworks beyond the reproduced MW11/Lasso and CPS illustrations. The red-noise and white-noise perturbation scheme is a simplified proxy-noise model and may not capture proxy-specific nonstationarity, age-model uncertainty, calibration errors, or spatially varying error structures. Details of hyperparameter choices for Lasso, preprocessing, and the sensitivity of conclusions to these choices are not fully elaborated in the discussion text. Because the goal is to critique a test, the evidence is illustrative rather than a comprehensive benchmark across competing reconstruction methods and datasets.","The author explicitly suggests that dependencies of cross-validation skill on sampling design should be tested in future work, including using field sampling that reflects the true proxy locations (with ocean undersampling and regional clustering) and allowing multiple samples per grid cell to reflect multiple proxies in a cell. He also indicates it remains unclear how MW11 cross-validation results would change when using far fewer predictors (tens rather than over a thousand), implying further experiments varying predictor count are needed.","A useful extension would be a systematic power analysis of the MW11-style hypothesis test under controlled signal-to-noise, persistence, predictor-count, and sampling-cluster scenarios to quantify Type II error rates directly. Evaluating robustness to temporal autocorrelation in both predictors and targets via alternative validation schemes (e.g., longer blocks, nested CV, or hindcast periods with different regimes) could clarify when persistence-driven nulls dominate. Incorporating more realistic proxy error models (e.g., proxy-specific calibration uncertainty, non-Gaussian noise, time-varying noise levels) would better connect the perturbation experiments to physical proxy processes. Publishing a packaged, reproducible workflow (e.g., in R/Python) with standardized data prep and tuning choices would facilitate broader benchmarking across reconstruction methods.",1104.4188v1,https://arxiv.org/pdf/1104.4188v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:57:59Z
FALSE,NA,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Supplementary material (Journal/Publisher),http://www-stat.stanford.edu/~tibs/glmnet-matlab/,"This discussion paper critiques the use of the LASSO for paleoclimate reconstructions, arguing it can yield spurious predictive performance when predictors are temporally structured but unrelated to the target. Using a surrogate-data simulation, the author generates a target series consisting of a linear trend plus AR(1) noise and compares prediction RMSE from (i) LASSO regression on many predictors versus (ii) composite regression (averaging predictors then OLS). Pseudo-proxy predictors are created by adding white noise to the target at varying signal-to-noise ratios, and are contrasted with independent AR(1) predictors with varying autoregressive coefficients. Across experiments, composite regression outperforms LASSO for pseudo-proxies at all tested SNRs, while LASSO can appear to perform well on highly persistent random AR(1) predictors due to trend/interpolation behavior. The paper concludes LASSO’s sparsity-inducing prior (shrinkage to zero) is scientifically implausible in this context and advocates hierarchical/Bayesian spatiotemporal models that encode proxy–climate relationships and correlation structure.","The surrogate target time series is defined as $y(t)=0.25\,t+\varepsilon(t)$ for $t=1,\ldots,149$, where $\varepsilon(t)$ follows an AR(1) process with coefficient 0.4 and innovation variance 1. Pseudo-proxy predictors are generated as $x_j(t)=y(t)+\eta_j(t)$ where $\eta_j(t)$ is additive white noise tuned to achieve specified SNR values. LASSO is fit across all predictors with a small penalty: $\lambda$ is set to 0.05 times the smallest value of $\lambda$ for which all coefficients are zero (as in MW2011). Performance is evaluated via out-of-sample RMSE using 30 withheld observations.","In 1000 simulation runs with 30 points withheld at the end, composite regression yields lower out-of-sample RMSE than LASSO for all tested pseudo-proxy SNR levels. For SNR = 1/4, the LASSO RMSE is reported to be about 7.5 times larger than the composite regression RMSE. When predictors are independent AR(1) series, LASSO can achieve lower RMSE for sufficiently large AR coefficient (notably $\alpha\ge 0.8$), sometimes outperforming even composite regression on white-noise predictors. The paper explains this as LASSO selecting strongly trending random-walk-like predictors, which interpolate/extrapolate well over short validation intervals despite being unrelated to the target.",None stated.,"This is a short discussion based on a specific surrogate-data design (linear trend + AR(1) noise), so the conclusions about LASSO may depend on that structure and on the particular tuning choice ($\lambda$ fixed as a fraction of the all-zero threshold) rather than cross-validated or otherwise optimally tuned LASSO. The evaluation focuses on RMSE over a short withheld block (30 of 149), which emphasizes short-horizon extrapolation/interpolation and may not reflect longer-horizon reconstruction objectives. Comparisons are limited mainly to composite regression versus LASSO, omitting other common high-dimensional time-series regression alternatives (e.g., ridge/elastic net, PCR/PLS, Bayesian shrinkage with nonzero-centered priors) under the same simulation settings.","The author suggests using more scientifically grounded models for paleoclimate reconstruction, specifically hierarchical statistical models that encode spatial and temporal correlation in the climate process and forward models for proxies given climate. The discussion also suggests modifying LASSO-style priors to shrink regression coefficients toward a common, data-determined value rather than toward zero.","A direct extension would be a systematic sensitivity analysis over LASSO/elastic-net tuning (including time-series-aware cross-validation) and alternative validation schemes with varying holdout lengths to quantify when temporally structured noise dominates. Another useful direction is benchmarking against ridge regression and Bayesian global-local shrinkage priors (e.g., horseshoe) and group/structured sparsity that better match proxy families, using both surrogate and real proxy datasets. Finally, incorporating autocorrelation-aware error models and explicitly modeling nonstationary trends could clarify whether the observed LASSO behavior is driven by temporal dependence, nonstationarity, or penalty selection.",1104.4191v1,https://arxiv.org/pdf/1104.4191v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:58:34Z
FALSE,NA,Other,Other,Not applicable,Environmental monitoring,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This short discussion paper critiques the use of LASSO regression for paleoclimate (multiproxy) temperature reconstruction, focusing on theoretical and practical concerns rather than proposing a reliability engineering method. The authors argue LASSO’s sparsity-inducing selection may be ill-suited when there are many weak signals (small coefficients) rather than a few strong predictors, and note constraints such as selecting at most n nonzero coefficients. They highlight additional drawbacks including coefficient shrinkage bias, lack of oracle guarantees in some settings, and the need for methods like elastic net or adaptive LASSO as possible remedies. A key concern is that standard LASSO assumptions (uncorrelated errors) are violated because proxy series and residuals are highly autocorrelated, with additional complications from spatial correlation. The piece serves as methodological commentary within climate reconstruction rather than reliability engineering.",Not applicable,Not applicable,None stated,"As a brief discussion, it does not provide new methodology, formal proofs, or quantitative simulation/case-study comparisons demonstrating the magnitude of the alleged issues (e.g., ARL/forecast error impacts under autocorrelation). It also does not specify concrete alternative modeling workflows (e.g., time-series regularization with explicit error models) or provide implementation guidance for practitioners working with dependent and spatially correlated proxy data.","The authors indicate that further research is needed for paleoclimatic variable selection, particularly methods that account for autocorrelation (time-series versions of LASSO) and potentially spatial correlation; they also suggest exploring alternatives such as elastic net and adaptive LASSO in this context.","Empirically benchmarking regularization methods under realistic proxy-generating mechanisms (joint temporal autocorrelation plus spatial dependence) would clarify when LASSO fails and what alternatives work best. Developing self-contained Bayesian or state-space formulations that incorporate proxy-specific noise, calibration uncertainty, and spatiotemporal structure could provide more principled uncertainty quantification and variable selection than iid-error LASSO variants.",1104.4193v1,https://arxiv.org/pdf/1104.4193v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:58:48Z
FALSE,NA,Other,Other,Not applicable,Environmental monitoring,Other,NA,None / Not applicable,Not applicable (No code used),http://www.cgd.ucar.edu/ccr/ammann/millennium/AW%20supplement.html|http://republicans.energycommerce.house.gov/108/letters/062305_pachauri.pdf|http://energycommerce.house.gov/108/home/07142006_Wegman_Report.pdf|http://www.ncdc.noaa.gov/paleo/paleo.html|http://www.cgd.ucar.edu/ccr/ammann/millennium/MBH-reevaluation.html,"This paper is a discussion/commentary on McShane and Wyner (2011) about statistical reconstructions of Northern Hemisphere temperatures over the last millennium using climate proxy data. It argues that MW’s review of prior work is incomplete and that MW mischaracterize issues surrounding principal component (PC) summarization used in Mann, Bradley and Hughes (1998/1999), especially the effects of centering/standardizing choices on extracted PCs and downstream reconstructions. The authors cite prior analyses (Ammann & Wahl 2007; Wahl & Ammann 2007) showing that using the first two PCs under different centering conventions yields nearly identical reconstructions, while alternative nonstandardized covariance-based PC extraction can shift “hockey-stick”-like structure to later PCs and affect validation unless additional PCs are included. They also emphasize that pseudo-proxy significance testing that preserves full AR structure can be overly conservative and note reported validation significance levels for reconstruction segments. Overall, the piece addresses scientific reliability of climate reconstructions in a statistical sense, not engineering system/component reliability.",Not applicable,"The discussion reports that using the first two PCs from different centering/standardization methods produces nearly identical reconstructed series for 1400–1449, with an average warming difference of about 0.05°C when “common centered” PCs are used (per Wahl & Ammann 2007). It states that using only the first 2–3 PCs from nonstandardized covariance-based PCA yields a reconstruction that fails validation, but adding the 4th or 5th PC makes reconstructions converge and pass validation. For Ammann & Wahl (2007) significance testing, the authors report 10 of 12 reconstruction segments significant at the 95% level, with the remaining two at 89% and 94%; the 1400–1449 and 1450–1499 segments are reported significant at 99% and 96%, respectively.",None stated,"As a short discussion piece, it does not present a full standalone methodology, detailed algorithmic specification, or reproducible computational workflow; the reader must rely on cited prior papers for derivations and complete evidence. The claims about validation significance and conservativeness of pseudo-proxy nulls are asserted without providing sensitivity analyses in this document (e.g., varying AR structures, alternative null models, or robustness to proxy selection). Because it focuses on correcting perceived misstatements, it provides limited comparative evaluation against the broader set of modern reconstruction approaches and uncertainty quantification frameworks.",None stated,"A natural extension would be a systematic, reproducible benchmark comparing centering/standardization choices, PCA variants (covariance vs correlation), and alternative regularization methods (e.g., LASSO variants) across multiple proxy networks with openly shared code and data. Further work could formalize and compare pseudo-proxy null models (AR-only, AR+spatial dependence, climate-model-based nulls) to quantify how conservative each is for different reconstruction targets and validation metrics.",1104.4195v1,https://arxiv.org/pdf/1104.4195v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:59:10Z
FALSE,NA,Bayesian,Sensor/condition monitoring|Mixture of types,Not applicable,Environmental monitoring,Other,FALSE,None / Not applicable,Not applicable (No code used),http://www.image.ucar.edu/~nychka/Paleo/BartonHearingsJUL2006.pdf,"This is a discussion article about statistical methods for paleoclimate temperature reconstructions, not a reliability engineering paper. The authors argue that a Bayesian hierarchical model (BHM) provides a more scientifically grounded “indirect” approach than direct regression of temperature on proxies, because it models proxies conditional on the latent temperature field and then inverts via Bayes’ theorem. They outline a hierarchical structure with a proxy data model, a space–time process model for the temperature field (including autoregressive dynamics and spatial covariance), and a model for hemispheric mean temperature driven by external forcings (solar, volcanic, CO2). The discussion emphasizes practical benefits of BHMs for handling missing/irregular proxy records and for mitigating issues such as proxy centering and measurement-error attenuation that can bias direct approaches. No new control/decision policies, life models, maintenance optimization, or other reliability-engineering contributions are presented.","A sketched Bayesian hierarchical model is given: (i) data level for proxies $x_{t,i}=\gamma_i h_i T_t + u_{t,i}$; (ii) process level for the temperature field $T_t=y_t\mathbf{1}+v_t$ with $v_t=Av_{t-1}+e_t$, $e_t\sim N(0,\Sigma)$; (iii) hemispheric mean model $y_t=\mu+S_t\omega_S+V_t\omega_V+C_t\omega_C+w_t$; with priors on $(\gamma,\omega,A,\Sigma,\ldots)$ and posterior sampling via MCMC.","Not applicable (the piece is a short discussion and does not report new quantitative performance metrics, ARL/ATS-type results, or numerical comparisons).",None stated.,"As a discussion/commentary, the article does not provide a full specification of priors, identifiability conditions, or computational details (e.g., MCMC diagnostics), so the practical reproducibility of the proposed BHM sketch cannot be assessed from this text alone. The assumed independence of proxy noise between proxies and the linear proxy–temperature relationship may be unrealistic for some proxy types and could affect reconstruction uncertainty if violated. No empirical validation, sensitivity analyses, or comparative benchmarks are presented within the discussion itself.",None stated.,"A natural extension would be to provide full implementation details (priors, computation, convergence checks) and systematic sensitivity analyses to key modeling choices (e.g., proxy error structure, spatial covariance, forcing covariates). Another direction is to relax linearity/normality assumptions in the proxy data model and innovations (e.g., heavy tails, non-Gaussian observation models) and to test robustness on multiple proxy networks with differing missingness patterns. Additional work could develop standardized software and reproducible workflows for hierarchical paleoclimate reconstructions to facilitate broader adoption and independent verification.",1105.0519v1,https://arxiv.org/pdf/1105.0519v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:59:25Z
FALSE,NA,Other,Other,Not applicable,Environmental monitoring,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This discussion piece critiques a statistical analysis of paleoclimate temperature reconstructions from multiple proxy time series. It argues that certain in-sample comparisons are not informative for proxy assessment and that hold-out strategies (especially at the end of the calibration period) would be more powerful. It disputes the characterization of the original paper’s approach as “fully Bayesian,” noting the absence of a joint probabilistic model accounting for autocorrelated reconstruction error and calibration conditioning. It suggests that an apparent poor fit in the 1990s may be an artifact of reconstruction initialization choices and calls for robustness checks. It highlights the broader methodological issue of using inverse regression (climate on proxies) versus classical calibration (proxies on climate), arguing classical approaches tend to extrapolate better for historical reconstructions.",Not applicable.,"No new quantitative reliability/SPC results are presented; the piece provides qualitative methodological critique (e.g., recommending alternative hold-out validation and questioning initialization sensitivity) rather than reporting new ARL/run-length, failure-rate, or lifetime-model estimates.",None stated.,"As a brief discussion article, it does not provide original empirical evaluation, simulation studies, or formal theoretical results to substantiate its critiques (e.g., no demonstrated sensitivity analysis for initialization or alternative validation designs). The commentary is specific to paleoclimate proxy reconstruction and does not generalize to reliability engineering contexts (failures, degradation, maintenance) despite using the word “reliable” in a colloquial sense.",None stated.,"A useful extension would be to operationalize the critiques via reproducible experiments: implement alternative validation schemes (end-of-period holdouts), explicit Bayesian hierarchical models with autocorrelated errors, and sensitivity analyses to initialization choices, then quantify effects on reconstruction uncertainty and predictive performance.",1105.0522v1,https://arxiv.org/pdf/1105.0522v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T08:59:36Z
FALSE,NA,Other,Other,Not applicable,Environmental monitoring,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This document is a discussion/commentary on McShane and Wyner (2011) about statistical challenges in reconstructing past surface temperatures from paleoclimate proxy networks. It interprets the paper’s cross-validation/holdout RMSE comparisons in terms familiar to paleoclimate scientists, particularly the “RE” (reduction of error) statistic and its benchmarking against pseudoproxy simulations. The authors highlight findings such as endpoint holdouts performing better than interior holdouts and white-noise pseudoproxies outperforming low-order AR(1) pseudoproxies, arguing these warrant further investigation. They discuss methodological choices (e.g., using Lasso, principal components regression) and emphasize that different reconstructions can yield similar cross-validation scores, complicating uncertainty quantification. Overall, it is about statistical validation of climate reconstructions, not reliability engineering (despite the word “reliable” in the title context).","Defines the paleoclimate “RE” statistic in terms of holdout RMSE: $\mathrm{RE}_{\text{proxy}} = 1 - \frac{\mathrm{RMSE}_{\text{holdout}}(\text{proxy reconstruction})}{\mathrm{RMSE}_{\text{holdout}}(\text{intercept/in-sample mean})}$. An analogous benchmark is computed for pseudoproxies: $\mathrm{RE}_{\text{pseudoproxy}} = 1 - \frac{\mathrm{RMSE}_{\text{holdout}}(\text{pseudoproxy reconstruction})}{\mathrm{RMSE}_{\text{holdout}}(\text{intercept})}$, with significance assessed via upper percentiles of the pseudoproxy RE distribution.","The discussion reports qualitative comparative findings attributed to McShane and Wyner (2011): (i) reconstructions validated using holdout periods at endpoints perform noticeably better than reconstructions validated on interior 30-year holdouts; (ii) white-noise pseudoproxy networks can outperform low-order AR(1) pseudoproxy networks; and (iii) “empirical AR1” pseudoproxies can match or outperform actual proxy networks (reported as particularly surprising for the Mann et al. (2008) network). It also notes that reconstructions using different numbers of principal components can look very different while having very similar cross-validation statistics, undermining simple uncertainty interpretations. No new numeric ARL/RMSE tables are provided in this discussion itself beyond referencing figures in the main paper.",None stated.,"As a discussion piece, it does not provide full reproducible methodological details, sensitivity analyses, or an independent empirical re-analysis; most claims rely on interpreting figures/results from McShane and Wyner (2011). The focus is narrow (paleoclimate proxy reconstruction validation) and does not translate to engineering reliability settings (failure/degradation/maintenance) despite overlapping terminology (e.g., “reliable”). It also does not systematically evaluate robustness to alternative reconstruction methods beyond brief speculation (e.g., CPS/RegEM) and does not provide implementation guidance or software artifacts.","The discussion explicitly notes that some findings (e.g., endpoint vs interior holdout behavior; white-noise pseudoproxies outperforming low-order AR1 pseudoproxies) “warrant further investigation,” and suggests it would be worth checking whether key proxy vs pseudoproxy comparisons are sensitive to methodological variations (e.g., using other reconstruction methods than Lasso).","A useful extension would be a fully reproducible re-analysis with shared code and data-processing provenance, including systematic sensitivity to holdout design, proxy selection, and alternative reconstruction algorithms (CPS, RegEM variants, Bayesian hierarchical models). More formal uncertainty quantification that accounts for model-selection/multiverse effects (many reconstructions with similar CV scores) could be developed, along with diagnostic tools to detect proxy inconsistency or spurious correlation. Additional benchmarking using realistic autocorrelation/nonstationarity in pseudoproxies and multiple skill metrics (beyond RMSE/RE) would strengthen conclusions.",1105.0524v1,https://arxiv.org/pdf/1105.0524v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:00:03Z
TRUE,Other,Simulation-based|ML-based|Other,Simulated only,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper proposes a metamodel-based importance sampling (meta-IS) method for estimating small structural failure probabilities when the limit-state/performance function g(x) is expensive (e.g., finite-element analysis). A kriging (Gaussian process) surrogate is used to define a probabilistic classification function π(x)=P(Ĝ(x)≤0) that reflects epistemic uncertainty in the surrogate prediction. This π(x) is used to construct a quasi-optimal importance sampling instrumental density \(\hat h^*(x) \propto \pi(x) f_X(x)\), leading to a failure-probability identity \(p_f = p_f^{\""}\,\alpha_{\mathrm{corr}}\) where \(p_f^{\""}=E[\pi(X)]\) is an “augmented” probability and \(\alpha_{\mathrm{corr}}=E_{\hat h^*}[\mathbf{1}_{g\le0}(X)/\pi(X)]\) corrects any surrogate-induced bias. The approach includes an adaptive refinement scheme that samples candidate points from \(\hat h^*\) (via MCMC slice sampling), clusters them (k-means), and enriches the design of experiments (DOE), with a leave-one-out estimate of \(\alpha_{\mathrm{corr}}\) used as a stopping criterion. Examples (including up to 100 random variables and a 93-variable nonlinear shell-roof buckling problem) show agreement with crude Monte Carlo/subset simulation while requiring far fewer evaluations of the expensive model.","Failure probability is \(p_f = P(g(X)\le 0)=\int \mathbf{1}_{g\le 0}(x) f_X(x)\,dx\). Kriging yields \(\hat G(x)\sim \mathcal N(\mu_{\hat G}(x),\sigma_{\hat G}(x))\) and the probabilistic classification \(\pi(x)=P(\hat G(x)\le0)=\Phi\big((0-\mu_{\hat G}(x))/\sigma_{\hat G}(x)\big)\). The quasi-optimal IS density is \(\hat h^*(x)=\pi(x)f_X(x)/p_f^{\""}\) with \(p_f^{\""}=E[\pi(X)]\), giving \(p_f=p_f^{\""}\alpha_{\mathrm{corr}}\) where \(\alpha_{\mathrm{corr}}=E_{\hat h^*}[\mathbf{1}_{g\le0}(X)/\pi(X)]\) and estimator \(\hat p_f^{\mathrm{metaIS}}=\hat p_f^{\""}\,\hat\alpha_{\mathrm{corr}}\).","For the Rackwitz (2001) lognormal-sum example, meta-IS matched crude Monte Carlo estimates while drastically reducing expensive function calls: for n=100, crude MC used N=1,450,000 runs to reach CoV ≤2%, whereas meta-IS used NDOE+Ncorr=2,700 total model calls and obtained \(\hat p_f\approx 1.70\times10^{-3}\) with CoV ≤2%. In that same case, the augmented estimate was \(\hat p_f^{\""}=1.83\times 10^{-3}\) and the correction factor \(\hat\alpha_{\mathrm{corr}}=0.93\), showing increasing surrogate misclassification with dimension. In the 93-variable shell-roof buckling problem, meta-IS produced \(p_f\approx 1.32\times10^{-4}\) (CoV 13.75%), consistent with subset simulation (\(1.27\times10^{-4}\), CoV 12.36%) and a Multi-FORM approximation (\(1.22\times10^{-4}\)). The reported components were \(\hat p_f^{\""}=2.06\times10^{-4}\) (CoV 5.70%) and \(\hat\alpha_{\mathrm{corr}}=0.641\) (CoV 12.49%).",None stated.,"The method’s efficiency depends on the quality of the kriging surrogate near the limit-state; in very high dimensions or with highly non-smooth/discontinuous responses, the DOE size (capped at mmax=1000) may be insufficient and performance may degrade. Sampling from \(\hat h^*\) relies on MCMC (slice sampling) and thinning, so practical efficiency and estimator variance can be sensitive to chain mixing and tuning choices, which are not fully benchmarked across problem classes. The paper focuses on independent input variables (and KL-based constructions) and does not provide a general treatment of strong dependence, heavy tails, or model-form uncertainty beyond kriging epistemic uncertainty.",The authors state that further work is in progress to include the proposed algorithm within a reliability-based design optimization (RBDO) framework.,"Developing robustness extensions for non-Gaussian/heteroscedastic surrogates and for autocorrelated or strongly dependent inputs (e.g., copulas) would broaden applicability. A systematic study of MCMC efficiency (mixing diagnostics, adaptive MCMC, alternative samplers) and its impact on \(\hat\alpha_{\mathrm{corr}}\) variance would strengthen guidance for practitioners. Providing an open-source implementation (e.g., as a UQLab/FERUM-compatible module) and additional industrial case studies would improve reproducibility and adoption.",1105.0562v2,https://arxiv.org/pdf/1105.0562v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:00:46Z
FALSE,NA,Bayesian|Simulation-based|Other,Other|Simulated only,Not applicable,Environmental monitoring,Simulation study|Other,TRUE,Other,Personal website|Not provided,http://www.meteo.psu.edu/~mann/supplements/AOAS/|http://probabilitynotes.wordpress.com/,"This discussion paper critiques McShane and Wyner (2011) on statistical reconstructions of past temperatures from climate proxy data, focusing on data-quality screening and appropriate validation. It argues MW included many tree-ring proxies that failed replication/reliability criteria and that removing low-quality and potentially contaminated proxies materially changes the inferred Medieval warmth. The authors also contend MW’s methodology choices (e.g., mixing annual and decadal proxies, OLS PC regression, and Lasso) lead to bias—especially variance underestimation—and overfitting when too many principal components are retained. They emphasize “pseudoproxy” simulation tests (synthetic proxy networks from climate model simulations with AR(1) red noise) as a benchmark for method skill, reporting that hybrid RegEM EIV-style methods outperform MW’s OLS/Lasso variants in these tests. They provide posterior probabilities that recent (1997–2006) warmth is unprecedented over the last millennium (reported as 80% by MW; recalculated as 86% with a screened dataset; and up to 99% using fewer PCs), while cautioning these probabilities are sensitive to methodological choices and do not capture systematic data issues.","Key methodological elements discussed include OLS regression of instrumental temperatures on the first K principal components of proxy data (e.g., “OLS PC10” and “OLS PC4”), and Bayesian/MCMC estimation (implemented via JAGS/rjags) for posterior probabilities of recent decadal warmth being unprecedented. Validation is framed via “pseudoproxy” simulations where proxy noise is modeled as red noise AR(1) with parameter ρ = 0.32 and signal-to-noise amplitude ratio SNR = 0.4, then reconstruction skill is assessed by comparing reconstructed vs. known model-simulated target climate histories. Specific control-limit/run-length-type SPC formulas are not part of this paper.","The authors state MW’s featured “OLS PC10” reconstruction yields inflated peak Medieval warmth when poor-quality proxies are included; screening to a more appropriate proxy set reduces that apparent warmth. They report MW’s estimate of an 80% probability that 1997–2006 is warmer than any other decade in the past 1000 years increases to 86% when using a screened 55-proxy dataset with K=10 PCs, and can rise to as high as 99% when using K=4 PCs. They argue K=10 is likely too large and leads to overfitting, while K=4 is favored by objective criteria and pseudoproxy analyses. In pseudoproxy tests with AR(1) red noise (ρ=0.32) and SNR=0.4, they report dramatic performance differences: OLS variants and especially Lasso exhibit serious underestimation bias relative to hybrid RegEM EIV methods, with bias diminishing for larger (e.g., 104-location) networks for the hybrid method.","They note that the reported posterior probabilities do not account for potential systematic issues in the underlying source data, are sensitive to methodological choices (e.g., number of PCs retained), and can vary by a few percent depending on the MCMC realization. They therefore caution against over-interpreting very high probability statements (e.g., 99%) despite obtaining them under certain modeling choices.","As a discussion/commentary, it does not fully specify or reproduce all computational details in the text itself (e.g., full preprocessing pipeline and all parameter settings), making independent replication depend on supplementary materials. The critique leans heavily on particular proxy-screening criteria and selected pseudoproxy configurations (specific AR(1) and SNR values), and conclusions could vary under alternative, equally plausible proxy-noise models or screening rules. The paper is not about engineering reliability and does not translate its “reliability” terminology (proxy reliability/replication) into formal reliability-engineering constructs (failure/degradation, censoring, RUL, maintenance).",They argue that progress is most likely through continued collaboration between statistics and climate science communities and by focusing more on reconstructing and analyzing underlying spatial patterns of past climate changes rather than only hemispheric mean temperature series.,"A useful extension would be a more systematic sensitivity analysis across a broader range of proxy-noise models, SNR settings, and proxy-screening criteria to quantify robustness of conclusions. Another direction would be standardized, open, end-to-end reproducible workflows (containerized environments, versioned datasets, and fully archived code) to reduce ambiguity about preprocessing and model choices. Finally, expanding validation beyond pseudoproxies to multiple independent instrumental targets or out-of-sample spatial field validation could better characterize generalization performance.",1105.2145v1,https://arxiv.org/pdf/1105.2145v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:01:20Z
FALSE,NA,Other,Other,Not applicable,Environmental monitoring,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Supplementary material (Journal/Publisher),http://www.meteo.psu.edu/˜mann/PseudoproxyJGR06/|http://www.meteo.psu.edu/˜mann/supplements/MultiproxyMeans07/|http://www.nap.edu/catalog.php?record_id=11676|http://www.blakemcshane.com|http://statistics.wharton.upenn.edu,"This paper is a rejoinder in a discussion of proxy-based paleoclimate temperature reconstructions, focusing on statistical validation, uncertainty quantification, and reproducibility rather than reliability engineering. It critiques competing reconstruction methods (notably RegEM EIV) and argues that many methods achieve similar cross-validated RMSE in the instrumental period yet yield very different historical reconstructions, implying large model uncertainty. The authors reanalyze both real proxy networks and climate-model pseudoproxy simulations, identifying issues such as nonreproducible simulation details and an improper centering step that biased OLS/Lasso results in a discussant’s figure. They emphasize that uncertainty bands are typically underestimated in the climate literature and show (via a Bayesian AR2 + PC model) that accounting for temporal dependence materially widens posterior intervals, especially after smoothing. Code and data for the rejoinder’s analyses are stated to be available via the Annals of Applied Statistics supplementary materials.","Key methods discussed include Lasso regression with tuning parameter selected by cross-validation, principal-components regression (OLS on selected PCs), and a Bayesian time-series regression described as an AR(2) model for Northern Hemisphere temperature with principal components of proxies as predictors (""Bayesian AR2 + PC10""). Performance is assessed primarily via cross-validated RMSE on holdout blocks (e.g., 30-year blocks) and through comparisons of reconstructions and posterior predictive uncertainty bands; specific closed-form SPC-style run-length equations are not part of the paper.","The authors report that, after correcting a discussant’s improper centering of simulated temperature anomalies, RMSEs for OLS/Lasso in the pseudoproxy experiments drop by about 15–20% and the apparent performance differences among methods are dramatically reduced. They state that their Bayesian AR2 + PC10 model produces reconstructions nearly indistinguishable from RegEM EIV on simulated data, with posterior bands that generally cover the target series (and always when unsmoothed). They also state their Bayesian models outperform RegEM EIV in holdout RMSE in the supplementary information, including beating a RegEM “hybrid” method in 2 of 4 simulations. They show that smoothing can greatly reduce uncertainty if the model omits temporal dependence, whereas including an AR structure keeps smoothed uncertainty bands wide.","The authors state they could not reproduce certain published RegEM EIV results within the publication time constraints due to layered, non-straightforward code and incomplete/unusable repositories from one discussant. They also state they cannot properly assess how model fits vary from draw to draw in the pseudoproxy simulation framework, and that this unaccounted variation is likely large. They further note their own uncertainty intervals may still be optimistically narrow because they do not include model uncertainty and do not account for uncertainties/biases in data selection, processing, infilling, smoothing, or potential “snooping.”","Because this is a rejoinder (not a full methods paper), many details of the Bayesian AR2+PC model specification, priors, diagnostics, and sensitivity checks are referenced but not fully developed in the short form, limiting standalone reproducibility without consulting supplements. The work is centered on climate reconstruction validation and does not address engineering reliability concepts (failure/degradation) or provide transferable reliability metrics beyond general predictive validation ideas. Several claims about comparative performance (e.g., Bayesian outperforming RegEM EIV) are deferred to supplementary materials, so key quantitative comparisons are not directly verifiable from the short text alone. The software platforms used for computation are not explicitly stated in the excerpt, making implementation details unclear without the supplements.","They explicitly call for additional research to quantify variability of reconstruction performance across repeated draws of climate-model simulations (not just conditional on one draw). They argue for more rigorous evaluation of whether climate model simulations used for pseudoproxy experiments match key features of real proxy and temperature data, and suggest this is a fertile area for investigation. They also call for rigorous testing/diagnosis of assumption-laden spatio-temporal and hierarchical models on real data using holdout RMSE, calibration of posterior intervals, and posterior predictive checks, including stress-testing against pseudoproxies.","A useful extension would be to provide a fully reproducible, containerized workflow (data + code + exact environment) for all reconstruction and simulation comparisons, including standardized benchmarks for competing methods and centering/processing steps. More systematic sensitivity analyses to alternative priors, PC selection schemes, and autocorrelation structures (e.g., ARMA/long-memory) could clarify robustness of uncertainty inflation claims. Applying the same validation framework to multiple independent observational temperature products and proxy compilations would help separate data-set-specific effects from methodological effects. Developing standardized reporting for uncertainty decomposition (process noise vs parameter uncertainty vs model uncertainty) could improve comparability across reconstruction studies.",1105.2433v1,https://arxiv.org/pdf/1105.2433v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:01:58Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,Nonparametric/Semi-parametric|Bayesian|Other,Degradation measurements|Sensor/condition monitoring|Right-censored|Mixture of types|Simulated only,Condition-based|Predictive|Not applicable,Transportation/logistics|Energy/utilities|Network/cybersecurity|Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a nonparametric degradation modeling framework for sensor-based degradation signals that may be complete, sparsely observed, or observed only over short time fragments. The degradation signal is modeled as $S_i(t)=\mu(t)+X_i(t)+\sigma\varepsilon_i(t)$ with unknown mean and covariance functions estimated nonparametrically; the covariance is represented via a truncated Karhunen–Loève expansion and estimated using FPCA for sparse longitudinal functional data. An empirical Bayes updating scheme is then derived to update the latent FPCA scores in real time for a fielded (partially observed) component, yielding a posterior Gaussian process for its degradation trajectory. Using a soft-failure threshold crossing definition, the method produces an updated residual life distribution (and bootstrap confidence intervals) for online remaining useful life prediction. The approach is validated on real crack growth data and simulation studies, showing similar accuracy when trained on incomplete signals versus complete signals and improved robustness relative to misspecified parametric degradation models; a nonuniform sampling design is also proposed to improve estimation near the end of the time domain.","The degradation model is $S_i(t)=\mu(t)+X_i(t)+\sigma\varepsilon_i(t)$, with $\mathrm{cov}(t,t')=\sum_{k\ge1}\lambda_k\phi_k(t)\phi_k(t')$ and truncated representation $X_i(t_{ij})\approx\sum_{k=1}^K\xi_{ik}\phi_k(t_{ij})$. Empirical Bayes updating for a new component observed at times $\mathbf t$ gives posterior scores $(\xi_1^*,\dots,\xi_K^*)'\sim N(Cd,C)$ where $C=(\sigma^{-2}P(\mathbf t)'P(\mathbf t)+\Lambda^{-1})^{-1}$ and $d=\sigma^{-2}P(\mathbf t)'(S(\mathbf t)-\mu(\mathbf t))$. The (soft) failure time is $T=\inf\{t:S^*(t)\ge D\}$; under a no-recrossing approximation, the residual life CDF has closed form (Prop. 2): $P(T-t^*\le y\mid T\ge t^*)=\frac{\Phi(g^*(y\mid t^*))-\Phi(g^*(0\mid t^*))}{1-\Phi(g^*(0\mid t^*))}$ with $g^*(y\mid t^*)=(\mu^*(t^*+y)-D)/\sqrt{V^*(t^*+y)}$.","On the Virkler et al. crack growth dataset (59 signals; 50 train/9 validation, repeated 100 times), the nonparametric FPCA-based method achieves similar residual-life prediction accuracy when trained on sparse signals (e.g., $m=6$ points per signal) as when trained on complete signals (about 50 points per signal), with sparse generally outperforming fragmented in prediction error. In benchmark comparisons on sparse crack signals, parametric random-effects models using log-linear or log–log-linear transformations yield noticeably larger residual-life prediction errors than the proposed nonparametric model due to trend misspecification. In simulations (Model 1), the proposed nonparametric method performs close to the true parametric benchmark, and a nonuniform (increasing-frequency) sampling plan reduces median prediction errors versus uniform sampling, especially at late-life percentiles (e.g., at 90% percentile: 3.11 nonuniform vs 3.95 uniform; Table 1). Bootstrap 90% residual-life confidence intervals show coverage close to nominal (≈0.9) for both complete and sparse scenarios, with interval length shrinking as the latest observation time approaches failure.","The authors note key assumptions: (i) degradation signals follow a Gaussian process (normality of FPCA scores and errors); (ii) observation time points across training signals must cumulatively cover $[0,M]$ densely (uniform sparse sampling can under-sample near $M$); and (iii) the degradation path does not cross back below the threshold after failure (a no-recrossing/monotonicity-like approximation). They also remark that the sampling plan requires specifying an upper time limit $M$ in advance, even though true maximum lifetime may be unknown, with the option to revise $M$ during experimentation.","The residual-life distribution formula relies on the threshold no-recrossing approximation, which can be inaccurate for highly noisy/nonmonotone degradation and may bias tail probabilities without diagnostics to detect recrossing risk. The empirical Bayes update treats $\mu(t)$, $\phi_k(t)$, and $\lambda_k$ (estimated from training data) as fixed when computing the fielded-component posterior, so uncertainty in these estimated functions/parameters is not propagated into the RLD beyond the bootstrap scheme described. The approach presumes independence across components and i.i.d. measurement errors; practical CM data often exhibit autocorrelation, drift changes, and sensor faults that could degrade FPCA estimation and updating. Practical deployment would benefit from guidance on selecting/validating $K$, bandwidths, and threshold $D$ under changing environments, but these aspects are largely treated as given.",None stated.,"Extend the framework to explicitly handle temporal dependence and non-i.i.d. noise in sensor streams (e.g., autoregressive errors or state-space formulations) and to provide robustness to outliers/sensor faults via robust FPCA or heavy-tailed Bayesian models. Develop self-starting/online FPCA updating so that the eigenfunctions and eigenvalues can adapt as more field data arrive and operating conditions shift. Provide principled methods for threshold selection ($D$) and for checking/relaxing the no-recrossing approximation, potentially using first-passage time methods for stochastic processes. Add scalable software implementations and comparative benchmarks against modern degradation/RUL methods (e.g., Gaussian process regression with learned kernels, particle filters, and neural sequence models) on multiple real CM datasets.",1107.5712v1,https://arxiv.org/pdf/1107.5712v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:02:50Z
TRUE,Software reliability|Reliability growth|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Complete lifetime data|Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.sportsci.org/ resource/ stats/ logtrans.html,"The paper proposes a function-based nonlinear least squares estimation (FNLSE) framework for parameter estimation in the Jelinski–Moranda (JM) software reliability growth model, extending classical least squares by applying a transformation function to both observations and model predictions. It studies two special cases: logarithm-transformed nonlinear least squares (LogLSE) and a newly proposed power-transformed least squares (powLSE), and shows that FNLSE is equivalent to a weighted nonlinear least squares estimator via the mean-value theorem. Parameters (N, \phi) of the JM model (exponential inter-failure times with hazard \lambda_i=\phi(N-i+1)) are solved numerically (Newton–Raphson) and used for recursive one-step-ahead MTBF prediction. The methods are evaluated on six benchmark software inter-failure-time datasets using two predictive criteria—recursive relative error (RE) and Braun statistic—showing powLSE with an optimized power index improves prediction accuracy over MLE, LSE, and LogLSE on these datasets. The paper also discusses heteroscedasticity in the failure-time data via variance trends across recursive segments and motivates transformed/weighted fitting as a practical response.","Jelinski–Moranda model assumes independent exponential inter-failure times with hazard \(\lambda_i=\phi(N-i+1)\) and mean \(\mathrm{MTBF}_i=1/[\phi(N-i+1)]\). FNLSE minimizes \(S_H=\sum_{i=1}^n\big(H(x_i)-H(1/[\phi(N-i+1)])\big)^2\), which is shown equivalent to weighted NLS since \(H(x_i)-H(f_i)=H'(\xi_i)(x_i-f_i)\). LogLSE uses \(H(x)=\log(x)\) leading to estimating equations (paper Eq. 3.6) and \(\phi=\exp\{-\tfrac{1}{n}\sum_{i=1}^n[\log x_i+\log(N-i+1)]\}\). powLSE uses \(H(x)=x^\alpha\) (\(\alpha\neq 0\)) minimizing \(\sum_{i=1}^n\big(x_i^\alpha-(1/[\phi(N-i+1)])^\alpha\big)^2\) and yields estimating equations (paper Eq. 3.8), with \(N\) solved from a scalar root equation via Newton–Raphson and \(\phi^\alpha=\frac{\sum (1/(N-i+1))^{2\alpha}}{\sum (x_i/(N-i+1))^\alpha}\).","Across six benchmark datasets (NTDS, JDM-I–IV, AT&T), powLSE with optimized \(\alpha\) substantially reduces average one-step-ahead RE versus MLE/LSE/LogLSE (Table 7): e.g., NTDS RE drops from 162.829 (MLE) and 125.966 (LogLSE) to 92.476 (powLSE, \(\hat\alpha=-2\)); JDM-III drops from 536.269 (MLE) and 208.453 (LogLSE) to 101.031 (powLSE, \(\hat\alpha=-2\)); AT&T drops from 2680.787 (MLE) and 1511.177 (LogLSE) to 706.623 (powLSE, \(\hat\alpha=-2\)). Under the Braun statistic criterion (Table 8), powLSE matches LSE on four datasets and improves on JDM-II (0.612 vs 0.847) and JDM-IV (0.918 vs 0.994) with criterion-specific optimized \(\alpha\). The paper reports all six datasets exhibit heteroscedasticity when examining variance of original and residual series across recursive segments (Figs. 25–30).","The authors note that selecting the optimal power index \(\alpha\) is difficult to determine directly from theoretical sufficient conditions and is instead chosen by a grid search/optimization over candidate \(\alpha\) values based on training criteria (TE or TBS). They also state that more complex shifted transformations (e.g., \(\log(x+K)\), \((x+K)^\alpha\)) are omitted because the resulting \((N,\phi)\) estimation becomes more complex. They further imply evaluation is limited to the included six benchmark datasets and the JM model setting, motivating broader evaluation as future work.","The approach is tightly coupled to the JM model assumptions (independent exponential inter-failure times, perfect debugging, constant per-fault detection rate), and improvements may reflect transformation/weighting compensating for model misspecification rather than universally better estimation. The power-index optimization uses in-sample (recursive training) criteria, which can overfit and may not generalize without a separate validation scheme or penalties for complexity. Comparisons are restricted largely to MLE/LSE/LogLSE within JM; there is no benchmark against modern SRGMs (e.g., NHPP-based Goel–Okumoto) or time-series/ML approaches mentioned in the introduction. The paper does not report computational details (software, convergence diagnostics, initialization) for Newton–Raphson, which can materially affect solutions in JM estimation (e.g., feasibility constraints like \(N>n\)).","The authors propose evaluating FNLSE on more failure datasets, comparing FNLSE (time-independent JM estimation) against time-dependent software reliability models, and applying the FNLSE framework to other software reliability models beyond Jelinski–Moranda to generalize the LSE-based estimation algorithm.","A natural extension is to develop a principled selection method for \(\alpha\) (and potential shift \(K\)) using out-of-sample validation, information criteria, or Bayesian/empirical Bayes formulations to reduce overfitting risk. Robust/self-starting implementations could enforce JM constraints (e.g., \(N\ge n\), \(\phi>0\)) and handle nonconvergence in Newton–Raphson with alternative solvers. It would also be valuable to study statistical properties (bias/variance, consistency) of powLSE under model misspecification and under heteroscedastic errors, and to compare against alternative weighted/robust estimators (e.g., LAD, Huber loss) and NHPP SRGMs on common benchmark suites with standardized protocols.",1108.5185v1,https://arxiv.org/pdf/1108.5185v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:03:40Z
FALSE,NA,Bayesian|Stochastic process|Other,Other,Not applicable,Service industry|Other,Exact distribution theory|Simulation study|Case study (real dataset),TRUE,Other,Public repository (GitHub/GitLab),http://www.mturk.com|http://turkopticon.differenceengines.com|http://turkernation.com,"The paper studies how to allocate redundant labels to binary crowdsourcing tasks at minimum cost (number of task assignments) while achieving a target overall accuracy when workers have unknown, heterogeneous reliability and are transient (non-reusable). It proposes a non-adaptive task allocation based on (ℓ,r)-regular random bipartite graphs and an iterative inference algorithm inspired by belief propagation and low-rank/spectral methods, with message updates that weight worker responses by inferred reliability. It derives finite-sample/large-system error bounds showing the average task error decays exponentially in ℓq (with q = E[(2p−1)^2] capturing the crowd’s collective quality) above a phase transition, and provides matching minimax lower bounds (up to constants) showing Θ((1/q)log(1/ε)) repetitions per task are necessary and sufficient in worst case. It further proves that allowing adaptive task assignment (without worker reuse) does not improve the order of the required budget, though adaptivity can help for particular worker distributions (instance-optimality gap). Empirical evaluations include simulations (spammer–hammer and beta-like models) and Mechanical Turk color-comparison tasks, showing the iterative/spectral methods outperform majority vote and EM and approach an oracle estimator that knows worker reliabilities.","Worker response model: for binary task i with truth t_i∈{±1}, worker j answers A_{ij}=t_i w.p. p_j and A_{ij}=−t_i otherwise (A_{ij}=0 if unassigned). Collective quality is q = E[(2p_j−1)^2]. Iterative messages on edges (i,j): x^{(k)}_{i→j}=∑_{j'∈∂i\{j}} A_{ij'} y^{(k−1)}_{j'→i} and y^{(k)}_{j→i}=∑_{i'∈∂j\{i}} A_{i'j} x^{(k)}_{i'→j}, with final estimate \hat t_i=sign(∑_{j∈∂i} A_{ij} y^{(k−1)}_{j→i}). Key performance scaling: to reach error ε, replicate each task ℓ = Θ((1/q)log(1/ε)) times (order-optimal), with a phase transition governed by (ℓ−1)(r−1)q^2 ≷ 1.","For random (ℓ,r)-regular assignments and the proposed iterative algorithm, the average error probability is bounded by exp(−ℓq/(2σ_k^2)) plus a vanishing (in m) non-tree term; for sufficiently large k and m this simplifies to ≤ 2 exp(−ℓq/(4σ_∞^2)). With r≥1+1/q, a sufficient budget is (32/q)log(2/ε) queries per task to ensure error ≤ ε (Corollary 2.3), and more generally (24+8/(rq̂))·(1/q)log(2/ε) (Corollary 2.4). A minimax non-adaptive lower bound shows no method can beat ~ (C/q)log(1/ε) queries per task under worst-case worker distributions, and an adaptive minimax lower bound of the same order implies adaptivity does not improve scaling when workers are fleeting. Simulations show iterative/spectral methods substantially outperform majority voting and EM and approach an oracle estimator; real Mechanical Turk experiments on color similarity show improvements over majority voting with an observed phase transition around the predicted threshold.","The authors note the model simplifies worker behavior by assuming (i) worker reliability does not depend on the true label (no worker bias) and (ii) all tasks have equal difficulty; extending to more general models with bias and heterogeneous task difficulty (as in Section 2.7) remains challenging. They also state their optimality is minimax (worst-case over worker distributions) and may not be instance-optimal; there exist distributions where adaptive strategies can outperform any non-adaptive method. Finally, they acknowledge their constant-factor optimality may be improvable, and that better algorithms (e.g., modified EM/BP) might achieve smaller constants.","Although the method is order-optimal in query complexity, practical deployment depends on estimating or adapting to q; the proposed workaround (doubling/replicas) may increase latency and operational complexity beyond the asymptotic constant-factor discussion. The theory assumes conditional independence of responses given p_j and ignores correlated workers, strategic/adversarial behavior beyond simple spammers, and nonstationary worker quality, which can be important in real platforms. The main guarantees are derived for random regular bipartite graphs (locally tree-like); performance and tuning on real task-assignment constraints (e.g., worker dropout, partial batch completion, platform-imposed task routing) are not fully addressed. Code details for simulations/experiments are not clearly documented in the provided text (the EM Java implementation is referenced but not concretely linked), limiting reproducibility.","They propose extending the approach and analysis to more general crowdsourcing models that include worker bias (label-dependent accuracies) and heterogeneous task difficulty, aiming for algorithms with performance guarantees under those richer models. They also highlight improving the constant factors in the optimality bounds, potentially via modified EM or belief propagation variants. Additionally, they call out the open question of instance-optimality for non-adaptive schemes (either proving instance-wise lower bounds or finding counterexamples) and suggest formally characterizing the observed phase transition around (ℓ−1)(r−1)q^2=1.","Developing robust/self-calibrating versions that estimate q online with minimal overhead and provide stopping rules with finite-sample guarantees would improve practical usability. Extending the framework to multiclass labels and structured outputs (e.g., ordinal labels, bounding boxes) and to settings with worker/task features (contextual reliability) would broaden applicability. Incorporating incentives and strategic behavior (e.g., payment/bonus design, adversaries) into the reliability model and allocation policy could align accuracy guarantees with mechanism design. Providing open-source reference implementations and benchmarks (including reproducible Mechanical Turk protocols) would strengthen empirical validation and adoption.",1110.3564v4,https://arxiv.org/pdf/1110.3564v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:04:36Z
FALSE,NA,Other,Other,Not applicable,Other,Other,NA,Other,Not applicable (No code used),http://www.mcs.anl.gov/research/projects/mpi/|http://hadoop.apache.org/|https://github.com/jpatanooga/KnittingBoar/wiki/IterativeReduce|http://hunch.net/~large_scale_survey/|http://users.soe.ucsc.edu/~niejiazhong/slides/chandra.pdf|http://www.magicbroom.info/Papers/Ladis10.pdf|http://mahout.apache.org/,"This paper presents a terascale distributed system for training linear predictors with convex losses (e.g., L2-regularized logistic regression) on Hadoop clusters, targeting datasets with billions of examples and trillions of nonzero feature entries. The core systems contribution is a Hadoop-compatible AllReduce communication layer (tree-based reduce+broadcast with pipelining) designed to be robust in practice via speculative execution and delayed tree initialization to avoid slow/failed nodes. Methodologically, the learning approach is a hybrid strategy: a single asynchronous adaptive-gradient online pass on each node followed by a weighted parameter average, which warm-starts distributed L-BFGS where gradients are aggregated via AllReduce each iteration. Extensive experiments on display advertising CTR prediction and splice site recognition show high throughput (reported up to ~470M features/s overall) and that the online warm-start saves roughly 10–15 L-BFGS iterations while achieving strong test performance. The paper also compares against MapReduce-based iteration, oversampled SGD averaging, and distributed minibatch methods, arguing that AllReduce plus the hybrid optimization yields superior runtime/communication tradeoffs on large clusters.","The objective optimized is regularized empirical risk minimization: $\min_{w\in\mathbb{R}^d}\sum_{i=1}^n \ell(w^\top x_i,y_i)+\lambda R(w)$. After one adaptive-gradient online pass per node $k$ producing $(w_k,G_k)$ (with diagonal $G_k$ accumulating gradient squares), the system computes a non-uniform weighted average $\bar w = (\sum_{k=1}^m G_k)^{-1}(\sum_{k=1}^m G_k w_k)$ using AllReduce. L-BFGS is then initialized at $\bar w$; each iteration computes local gradients $g_k$, aggregates $g=\sum_k g_k$ via AllReduce, adds regularization terms, and takes an L-BFGS update.","On an 8× larger display-advertising dataset (16B examples, ~125 nonzero features/example), training with 1000 nodes and 10 passes took 70 minutes, implying ~4.7M features/node/s and ~470M features/s overall throughput. For splice site recognition, the authors report that one online pass plus 14 L-BFGS iterations achieved test auPRC 0.581 in 1960 seconds on 500 machines (claimed 68× speedup vs a reported single-machine baseline), with explicit features totaling ~3TB. Speculative execution greatly reduced straggler impact: in a 1000-node splice experiment, max per-iteration compute time dropped from 758s (no spec) to 63s (with spec), and estimated communication time from 26s to 10s. Warm-starting L-BFGS with the online-averaged solution saved about 10–15 L-BFGS iterations in objective suboptimality convergence plots, and AllReduce reduced per-iteration training time versus MapReduce substantially (e.g., 670s vs 1690s on a full dataset subset; 59s vs 1322s on a 10% sample).","The authors note that they do not analyze two practical overhead sources: data loading and Hadoop node scheduling; they state these can affect performance but are typically amortized in the AllReduce approach compared to per-iteration MapReduce overheads. They also acknowledge uncertainty/variability in timing due to cluster utilization (e.g., some timing experiments were not repeated and runtimes can vary substantially). For splice site recognition, they mention their explicit feature representation introduces significant overhead relative to prior work that avoids explicit feature computation.","The paper focuses on scaling linear models with convex losses; the approach may not extend straightforwardly to non-convex objectives or models requiring more complex parameter synchronization (beyond vector AllReduce). Reliability/fault tolerance is addressed mainly via Hadoop speculative execution and delayed tree setup, but the single-tree socket-based AllReduce still appears vulnerable to mid-iteration node failures once the tree is formed, and no quantitative failure-rate evaluation is provided. Comparisons to competing systems (e.g., Sibyl) are limited by lack of direct reproducible benchmarks and differences in problem setup, data, and infrastructure. The work reports an open-source implementation but does not provide explicit reproducibility artifacts (exact configs, scripts, datasets), which limits independent verification of throughput claims.","The authors suggest that improvements in Hadoop scheduling algorithms could further improve overall system performance, especially by reducing one-time overheads and mitigating cluster-related inefficiencies. They also imply (in the context of their discussion) that better handling of data loading and scheduling overheads would be beneficial though not explored in the paper.","A natural extension would be a more robust AllReduce design that tolerates node failures during communication (e.g., multi-tree/overlay or checkpointed reductions) with quantified reliability under fault injection. Additional work could broaden applicability to settings with unknown/streaming feature spaces (parameter server or sharded models) while retaining the favorable communication pattern demonstrated here. More comprehensive benchmarking on public large-scale datasets, with released experiment scripts and cluster configuration details, would strengthen reproducibility and allow fairer comparison to alternative distributed optimization frameworks. Finally, extending the hybrid warm-start idea to other solvers (e.g., accelerated methods, proximal/quasi-Newton with sparsity, or adaptive second-order methods) and to non-i.i.d./heterogeneous data partitions would improve generalizability to modern federated and multi-tenant cluster environments.",1110.4198v3,https://arxiv.org/pdf/1110.4198v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:05:22Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1201.3935v2,https://arxiv.org/pdf/1201.3935v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:05:22Z
TRUE,System reliability|Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only,Not applicable,Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper develops an asymptotically optimal hybrid two-stage sampling/allocation design to estimate the reliability of a parallel–series (and by duality, series–parallel) system when component reliabilities and subsystem sample sizes are unknown but a total test budget $T$ is fixed and large. The system is modeled with independent Bernoulli component outcomes; subsystem reliabilities are products/sums consistent with parallel subsystems in series, and the estimator is the plug-in product of component sample means. Using Lagrange’s identity, the authors derive allocation-independent lower bounds on the variance of the subsystem reliability estimator and on the overall system reliability estimator. They then propose a two-stage design within each parallel subsystem to allocate component-level samples approximately proportional to $c_{ij}^{-1}$ (where $c_{ij}=\sqrt{1/R_{ij}-1}$) and a system-level two-stage rule to allocate subsystem budgets $T_j$ to approach the global variance lower bound. Monte Carlo simulations on small and larger synthetic systems validate that the proposed hybrid scheme approaches the derived lower bound as $T$ increases and yields better allocations than balanced sampling in the illustrated examples.","System reliability for a parallel–series system: $R=\prod_{j=1}^n R_j$ with parallel-subsystem reliability $R_j=1-\prod_{i=1}^{n_j}(1-R_{ij})$. The estimator uses component sample means $\hat R_{ij}=\frac{1}{M_{ij}}\sum_{l=1}^{M_{ij}}X^{(l)}_{ij}$ and $\hat R_j=1-\prod_{i}(1-\hat R_{ij})$, hence $\hat R=\prod_j \hat R_j$. For a fixed subsystem budget $T_j=\sum_i M_{ij}$, the asymptotically optimal component allocation is $M_{ij}=T_j\,\frac{c_{ij}^{-1}}{\sum_{k=1}^{n_j}c_{kj}^{-1}}$; for the full system, the asymptotically optimal subsystem allocation is $T_j=T\,\frac{\frac{1-R_j}{R_j}\sum_{k=1}^{n_j}c_{kj}^{-1}}{\sum_{k=1}^n \frac{1-R_k}{R_k}\sum_{i=1}^{n_k}c_{ik}^{-1}}$. They derive lower bounds $\mathrm{Var}(\hat R_j)\ge Q_j=(1-R_j)^2T_j^{-1}(\sum_i c_{ij}^{-1})^2$ and $\mathrm{Var}(\hat R)\ge Q=T^{-1}R^2\left[\sum_{j=1}^n \frac{1-R_j}{R_j}\sum_{i=1}^{n_j}c_{ij}^{-1}\right]^2$, and prove the hybrid two-stage design satisfies $\lim_{T\to\infty}T(\mathrm{Var}(\hat R)-Q)=0$.","Theoretical results include explicit allocation-independent lower bounds for the variance of the subsystem estimator ($Q_j$) and of the system estimator ($Q$), and first-order asymptotic optimality proofs showing the proposed two-stage (subsystem) and hybrid two-stage (system) allocations achieve these bounds in the sense that the variance gap is $o(1/T)$. Simulation evidence is provided for (i) a two-subsystem example with $T=20$, where scanning partitions of $T_1$ shows the minimum variance occurs at the allocation suggested by the hybrid scheme, and (ii) a larger synthetic system with 4 subsystems (2,3,4,5 components) where $T(\mathrm{Var}(\hat R)-Q)$ decreases toward 0 as $T$ increases from 100 to 10000. The paper also illustrates (in the introduction, for a 4-component parallel system with $T=100$) that sequential allocation can substantially reduce variance versus balanced allocation (reported as roughly a tenfold reduction in variance in that example). Overall, the quantitative takeaway is convergence of the hybrid design’s achieved variance to the lower bound $Q$ as $T$ grows, empirically visible on a log-scale plot of the excess variance term.",None stated.,"The approach assumes independence of Bernoulli trials within and across components/subsystems and effectively identical test conditions, which may be violated by common-cause failures, dependence, or heterogeneous environments. Results are asymptotic (large $T$); performance for small to moderate budgets may be sensitive to the choice of first-stage sizes (e.g., $L=\lfloor\sqrt{T}\rfloor$, $L_j=\lfloor\sqrt{T_j}\rfloor$) and to boundary effects from integer rounding and the max-corrections. The study validates mainly via synthetic Monte Carlo examples and does not provide real-data case studies or robustness checks (e.g., misspecified independence or non-Bernoulli outcomes such as time-to-failure). No implementation details (software, runtime, reproducibility artifacts) are provided, making practical adoption and verification harder.","The authors suggest extending the study (with straightforward but tedious adaptation) to more complex systems and to multi-criteria optimization under additional constraints such as risk, system weight, cost, and performance, within either fixed-sample or Bayesian frameworks.","Develop self-starting/robust variants that handle dependence (common-cause failures) and model uncertainty, and study sensitivity of allocations to estimation error in stage 1. Extend the allocation framework to time-to-failure or censored lifetime testing (rather than Bernoulli pass/fail), and to settings with unequal test costs or constraints per component/subsystem. Provide broader empirical validation on benchmark reliability datasets and release reference software to facilitate adoption and reproducibility.",1202.5334v4,https://arxiv.org/pdf/1202.5334v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:06:04Z
TRUE,System reliability|Other,Simulation-based|Nonparametric/Semi-parametric|Stochastic process|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.jcss.byg.dtu.dk,"This keynote-style review surveys surrogate (meta-)modeling methods for structural reliability and uncertainty quantification when the true computational model (e.g., finite elements) is expensive to run. It covers classical polynomial response surfaces, polynomial chaos expansions (PCE, including sparse/adaptive regression-based PCE with cross-validation error estimates), and kriging/Gaussian-process surrogates with sequential (active learning) enrichment strategies such as EGRA and AK-MCS. The paper emphasizes estimating small failure probabilities $P_f=\mathbb{P}[g(X)\le 0]$ and shows why crude Monte Carlo becomes infeasible for rare events, motivating surrogate-based workflows. A key contribution is the presentation of “meta-model-based importance sampling” (meta-IS), where kriging is used to construct a near-optimal instrumental density and a correction factor is applied so the final estimator of $P_f$ remains unbiased (avoiding bias from direct surrogate substitution). Example applications (a frame structure using sparse PCE and a system reliability benchmark using meta-IS) illustrate large reductions in expensive model evaluations while maintaining target accuracy.","Structural reliability is defined via the limit-state function $g$ and probability of failure $P_f=\mathbb{P}[g(X)\le 0]=\int_{\{g\le 0\}} f_X(x)\,dx$. Surrogates include (i) quadratic response surfaces $\tilde g(x)=a_0+\sum_i a_i x_i+\sum_i a_{ii}x_i^2+\sum_{i<j} a_{ij}x_ix_j$ fit by least squares, (ii) polynomial chaos expansions $g(X)\approx g_{PC}(X)=\sum_{\alpha\in\mathcal A} a_\alpha \Psi_\alpha(X)$ with coefficients via regression/projection, and (iii) kriging with mean predictor $\mu_{\hat Y}(x)=f(x)^T\hat a+r(x)^T R^{-1}(\Gamma-F\hat a)$ and predictive variance $\sigma^2_{\hat Y}(x)$. The meta-IS scheme defines a smoothed failure classifier $\pi(x)=\Phi\big(-\mu_{\hat Y}(x)/\sigma_{\hat Y}(x)\big)$ and instrumental density $\tilde h(x)=\pi(x)f_X(x)/P_{f\varepsilon}$, yielding $P_f=\alpha_{corr}P_{f\varepsilon}$ with $\alpha_{corr}=\mathbb{E}_{\tilde h}[\mathbf{1}_{\{g\le 0\}}(X)/\pi(X)]$.","For the frame-structure example (21 correlated inputs), an adaptive sparse PCE (target error $10^{-3}$) achieved reliability-index estimates up to about $\beta\approx 4$ with <5% relative error using ~450 finite-element runs (versus tens of thousands for full PCE and ~500,000 runs for FORM+IS reference sampling). The paper notes PCE-based substitution is generally robust down to roughly $P_f\approx 10^{-4}$–$10^{-5}$ but does not guarantee tail accuracy for extremely small probabilities. For a 2D system-reliability benchmark, meta-IS achieved $\hat P_f\approx 2.38\times 10^{-3}$ with coefficient of variation <5% at a cost of about 40 true-model runs to build the kriging surrogate plus ~200 additional true evaluations for the correction factor, compared to ~172,000 true evaluations for crude Monte Carlo (and ~284,195 for subset simulation in the cited comparison). Overall, the meta-IS approach is highlighted as providing unbiased rare-event probability estimates while dramatically reducing expensive model calls.","Polynomial chaos expansions provide global (least-squares) accuracy control but do not guarantee accuracy in the tail of the limit-state distribution; consequently, direct PCE-substitution should not be used for very small probabilities of failure. The paper also notes a practical trade-off in meta-IS between the number of true model calls needed to build an accurate kriging surrogate and the additional calls needed to estimate the correction factor (total cost $N+N_{corr}$).","As a review/keynote, the paper does not provide a unified, head-to-head benchmark protocol across many problem classes (high dimension, strong nonlinearity/discontinuity, multiple disjoint failure regions, correlated/non-Gaussian inputs) to quantify robustness of each surrogate method under comparable budgets. The meta-IS framework is presented primarily with kriging; extensions to other surrogate classes and guidance on hyperparameter optimization stability (e.g., likelihood multimodality, nugget/regularization, numerical conditioning of $R$) are not deeply examined. Practical implementation details (software/tooling, reproducibility artifacts, and computational overhead for sequential design optimization/MCMC sampling) are not provided, which may affect adoption for large-scale industrial FEM workflows. Dependence/auto-correlation in model evaluations or time-dependent reliability/degradation settings are not addressed, limiting scope to static structural reliability problems.","The paper explicitly points to the need to balance surrogate-construction cost versus correction-sampling cost in meta-IS (stopping criteria for enrichment), citing cross-validation-based procedures to stop iterative kriging improvement when $P_{f\varepsilon}$ is sufficiently close to $P_f$. It also emphasizes (at a high level) the need for engineers to adopt and further develop modern tools from computer science/applied mathematics/statistics to innovate in reliability and uncertainty quantification.","Extend the meta-IS/unbiased rare-event estimation idea beyond kriging to other adaptive surrogates (e.g., sparse PCE, SVM classifiers, neural operators) and compare efficiency under identical evaluation budgets. Develop robust, scalable strategies for correlated/high-dimensional inputs (including dimension reduction, active subspaces, or vine-copula/Nataf-informed designs) and for problems with multiple separated failure regions where sequential enrichment can miss modes. Provide self-starting/online versions for situations with unknown input distributions or streaming data, and broaden to time-dependent reliability and degradation/RUL settings where surrogates must capture trajectories. Release reference implementations and standardized benchmark suites to improve reproducibility and practitioner guidance on design-of-experiments, hyperparameter tuning, and stopping rules.",1203.2062v1,https://arxiv.org/pdf/1203.2062v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:06:50Z
TRUE,System reliability|Other,Simulation-based|Bayesian|Other,Sensor/condition monitoring|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper addresses reliability updating when new information is of the equality type (e.g., exact-valued measurements/monitoring), which has zero prior probability and therefore cannot be handled directly by standard structural reliability methods (SRM) that operate on probability integrals over domains. Straub reformulates equality information as a likelihood function and then transforms that likelihood into an equivalent inequality event in an augmented variable space by introducing auxiliary standard Normal variable(s). This converts conditional reliability with equality information into a standard (inequality-based) structural reliability problem, enabling the use of any SRM, including simulation approaches such as importance sampling, subset simulation, or crude Monte Carlo. Three examples (a Weibull capacity measurement case, a linear-Normal analytic case, and a fatigue crack growth monitoring case) demonstrate the method and show that Monte Carlo with the transformed inequality yields unbiased updated reliability with quantifiable sampling error. The work advances SRM-based Bayesian updating by providing a general, simulation-compatible route to incorporate equality/measurement information without relying on FORM/SORM surface-integration approximations.","The conditional reliability is recast by expressing the likelihood as an equivalent inequality event using an auxiliary variable: for likelihood $L(x_g)$, define $h_e(x_g,u)=u-\Phi^{-1}(c\,L(x_g))$ and domain $\Omega_{Z_e}=\{h_e\le 0\}$, with $U\sim\mathcal N(0,1)$ and $c>0$ chosen so $0\le cL\le 1$. Then $\Pr(E\mid Z)=\dfrac{\int_{\Omega_E\cap\Omega_{Z_e}} f_{X^+}(x^+)\,dx^+}{\int_{\Omega_{Z_e}} f_{X^+}(x^+)\,dx^+}$ in the augmented space $X^+=[X_g;U]$ (or $[X_g;U_1,\ldots,U_n]$ for multiple measurements). For additive measurement error with measured value $m$, a typical likelihood is $L(r)=\varphi(m-r)$, leading to $h_e(r,u)=u-\Phi^{-1}(\varphi(m-r))$ (with $c=1$ here).","Example 1 (Weibull capacity with measurement $m_r=6$ and $s=2$): APIS importance sampling with 500 line searches gives conditional reliability index $\beta\in[4.47,4.53]$ across repeated runs, matching the numerical-integration “exact” value $\beta=4.49$; FORM and SORM on the transformed limit state give $\beta_{FORM}=4.69$ and $\beta_{SORM}=4.60$, illustrating approximation error due to nonlinearity. Example 2 (linear/Normal with 3 equality observations): APIS yields $\beta\in[3.02,3.08]$ vs analytical $\beta=3.07$; FORM and SORM give $\beta_{FORM}=3.51$ and $\beta_{SORM}=2.95$. Example 3 (fatigue crack growth with two crack-depth measurements): crude MCS with $10^6$ samples using the equivalent inequality representation provides unbiased results with a small 95% confidence interval; a DBN approximation matches closely with small deviations after the second measurement, while a second-order surface-integral approach underestimates reliability and fails for larger cycle counts due to design-point search issues.","FORM/SORM should be applied only with due attention to the shape of the limit state surfaces around the joint design points, because the equivalent inequality limit state surfaces can be considerably non-linear and may yield inaccurate FORM/SORM approximations. The paper also notes that when target probabilities are small, crude Monte Carlo becomes inefficient and advanced simulation methods (importance sampling, subset simulation) are recommended. In the fatigue example, the second-order surface-integral comparison method encounters practical algorithmic difficulties in design-point search (not fundamental, but common in implementations).","Selecting the scaling constant $c$ to ensure $0\le cL(x_g)\le 1$ for all $x_g$ can be nontrivial for unbounded likelihoods or poorly specified measurement models; the paper does not provide a systematic, numerically robust procedure beyond simple cases. The approach increases problem dimension by adding one auxiliary variable per equality observation, which can degrade efficiency for high-dimensional updating with many measurements unless carefully combined with specialized rare-event simulation. The method assumes likelihood evaluation is feasible and accurate; model-form error and dependence between measurement errors and state variables (violations of independence assumptions used in deriving some likelihood expressions) are not explored. Comparisons are limited and do not benchmark against modern sequential/particle filtering style Bayesian updating approaches for monitoring data.",None stated.,"Develop practical guidance/algorithms for choosing and bounding the constant $c$ (or alternative transformations that avoid global bounding) to improve numerical robustness for complex likelihoods. Extend and evaluate the approach under autocorrelated monitoring data, model-form uncertainty, and dependence between measurement error and system state, including hierarchical Bayesian formulations. Provide self-starting/efficient rare-event simulation schemes tailored to the augmented-space formulation when many equality observations are assimilated over time (e.g., adaptive importance sampling or subset simulation variants). Release an implementation (e.g., in an SRM toolbox) and validate on larger real-world monitoring datasets to quantify computational scaling and practitioner usability.",1203.5405v1,https://arxiv.org/pdf/1203.5405v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:07:30Z
TRUE,System reliability|Network/infrastructure reliability|Maintenance optimization|Other,Bayesian|Stochastic process|Simulation-based|Other,Right-censored|Degradation measurements|Sensor/condition monitoring|Other,Condition-based|Predictive|Not applicable,Transportation/logistics|Network/cybersecurity|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://dx.doi.org/10.1061/(ASCE)EM.1943-7889.0000170,"This paper applies the enhanced Bayesian network (eBN) framework (Bayesian networks combined with structural reliability methods, SRMs) to time-evolving reliability and risk assessment problems where information arrives sequentially. For a ductile frame system, the eBN supports Bayesian updating of failure probability using imperfect capacity measurements and observations of past load/performance, and it enables life-cycle reliability assessment under temporally dependent annual maxima hazards via a latent common factor. The authors also extend the rBN to a decision graph to optimize repair/replacement actions and to compute value-of-information (VOI) for measurement campaigns under uncertainty. A second application develops an object-oriented BN (OOBN) infrastructure framework for spatio-temporal risk assessment, including hazard fields, element deterioration (Markovian capacity evolution), component fragilities, and network connectivity, with near-real-time updating during/after hazard events. Performance is demonstrated via FORM-based SRM computations embedded in BN inference, with Monte Carlo simulation used to verify conditional reliability estimates in the structural example.","Structural system failure is defined by three limit-state functions (frame failure modes): $g_1=r_1+r_2+r_4+r_5-5h$, $g_2=r_2+2r_3+r_4-5v$, $g_3=r_1+2r_2+2r_4+r_5-5h-5v$, with failure domain $\Omega_F=\{\min_{i=1,2,3} g_i(\mathbf{x})\le 0\}$. Imperfect measurements are modeled as $M_i=R_i+\varepsilon_i$ (for $i=4,5$) with $\varepsilon_i\sim\mathcal{N}(0,15^2)$, and correlated annual maxima loads are modeled via a random Gumbel location parameter $U_H$ so $H(t)\mid U_H$ are conditionally independent. VOI is computed as $\mathrm{VOI}(a_i)=\mathbb{E}[U\mid a_i]-\mathbb{E}[U]$ with preposterior expected utility $\mathbb{E}[U\mid a_i]=\sum_j \mathbb{E}[U\mid M_i=j]\,\Pr(M_i=j)$.","For the frame example, conditional reliability indices computed by the eBN–rBN approach match Monte Carlo simulation (10^6 samples): e.g., no measurement $\beta\approx1.94$ ($P_f\approx2.6\times10^{-2}$); with $M_4=50, M_5=100$ kNm $\beta\approx0.70$ ($P_f\approx0.24$); with $M_4=150, M_5=200$ kNm $\beta\approx2.45$ ($P_f\approx0.71\times10^{-2}$). Building the measurement-updating rBN uses discretization of $R_4,R_5$ into 21 states (441 SRM/FORM evaluations); the life-cycle model additionally discretizes the system capacity proxy $Q$ into 31 states leading to $2\times 21^2\times 31=19{,}251$ FORM computations (about one CPU hour reported), after which updating queries take about one CPU second. In the decision/VOI study, the individual VOIs are reported as $\mathrm{VOI}(a_4)=1{,}802$ and $\mathrm{VOI}(a_5)=1{,}168$ utility units, with joint $\mathrm{VOI}(a_4,a_5)=2{,}763$, implying measurement selection depends on per-measurement cost. In the infrastructure example (transportation network with bridges and control systems plus deterioration and hazard dependence), sequential evidence (capacity measurements, low-load years, partial post-event observations, and final hazard/performance data) produces substantial real-time updates to annual reliability indices for both the network and individual structures (shown in the paper’s Figures 15–16), and updates the posterior of the hazard uncertainty parameter $U_H$ (Figure 17).","The authors note computational limitations: Markov envelope sizes in the eBN and the resulting rBN complexity must be restricted for exact inference to remain feasible, and some evidence patterns can make exact evaluation too demanding. They also acknowledge modeling limitations from simplifying structural systems after time zero into single capacity variables $Q_j(t)$, which prevents detailed element-level deterioration modeling and direct inclusion of inspections/measurements at times $t>0$. They further state that multiple-hazard settings would require $Q_j(t)$ to represent joint hazard effects and that efficient representations for this require further work.","The approach relies heavily on discretization choices (number of states and cutpoints) that can materially affect accuracy and computational cost; the paper provides limited guidance or adaptive error control for discretization design beyond fixed grids. Many models assume conditional independences (e.g., hazards conditionally independent given $U_H$, Markov deterioration, component conditional independence given parents) and simplified replacement/repair rules; robustness to model misspecification (dependence structure, nonstationarity, autocorrelation) is not systematically studied. Practical deployment would also require scalable implementation/software integration; the paper references tools (e.g., CalREL) but does not provide reusable code or workflows for large networks with real data streams.","They suggest developing strategies for cases where exact inference becomes too demanding, including switching to approximate inference algorithms or hybrid exact/approximate approaches. They also call for further development to enable computationally feasible modeling of more general/realistic systems, including element-level deterioration over time and incorporation of inspections/measurements at $t>0$. They note the need for efficient representations when structures are subjected to multiple hazard types so that time-dependent capacity variables can capture joint hazard effects.","Develop principled discretization/adaptive refinement schemes (e.g., error-bounded state aggregation, dynamic discretization) to balance accuracy and runtime automatically as evidence changes. Extend the framework to handle unknown/learned model parameters more explicitly (hierarchical Bayesian calibration for hazard and deterioration models) and assess sensitivity/robustness to dependence assumptions. Provide open software implementations and benchmarking on real infrastructure monitoring datasets (e.g., SHM sensor streams) to validate near-real-time performance at scale, including comparisons against particle filtering / dynamic BN approaches for hybrid continuous-discrete systems.",1203.5985v1,https://arxiv.org/pdf/1203.5985v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:08:18Z
TRUE,System reliability|Network/infrastructure reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Other,Not applicable,Network/cybersecurity|Transportation/logistics|Energy/utilities|Other,Other,NA,None / Not applicable,Not applicable (No code used),http://dx.doi.org/10.1061/(ASCE)EM.1943-7889.0000173,"This paper proposes an “enhanced Bayesian network” (eBN) framework that combines Bayesian networks (BNs) with structural reliability methods (SRMs) to support reliability and risk analysis of engineering structures and infrastructure with evolving information. The key idea is to represent hybrid models with both discrete and continuous variables (including deterministic nodes defined via limit-state domains) and then eliminate continuous nodes to obtain a reduced BN (rBN) containing only discrete nodes. Conditional probability tables in the rBN are computed via SRMs (e.g., FORM/SORM and simulation methods) to accurately estimate small/rare-event probabilities arising from physically based limit-state models. The paper introduces the concept of Markov envelopes to characterize computational bottlenecks (number of SRM evaluations and rBN clique size) and provides modeling strategies—especially selective discretization and maintaining causal structure—to keep inference tractable. It also describes how to incorporate evidence on continuous variables by introducing discrete children representing observation domains, enabling Bayesian updating through exact inference on the rBN.","The reliability event probability is formulated as $\Pr(E)=\int_{\Omega_E} f(\mathbf{x})\,d\mathbf{x}$ with $\Omega_E$ defined via limit-state functions (e.g., system cut sets using $\min/\max$ of $g_i(\mathbf{x})\le 0$). The eBN joint measure factors like a BN: $\pi(\mathbf{z})=\prod_i \pi(z_i\mid \mathrm{pa}(Z_i))$, with discrete-child-of-continuous nodes defined by domains in parent space. To build the rBN, continuous nodes are eliminated by link reversals; the resulting rBN potentials are computed by SRM evaluations of integrals like $\Pr(\cap_i\{\mathbf{X}\in \Omega_{i,k}(\cdot)\})=\int_{\cap_i \Omega_{i,k}} f(\mathbf{x}\mid \cdot)\,d\mathbf{x}$ (system/component reliability), and then marginalized to obtain conditional probability tables.","The paper’s main results are methodological rather than numeric: it proves/argues that eliminating continuous nodes yields an rBN whose required SRM computations and a lower bound on inference complexity are governed by “Markov envelopes” of continuous variables (one node per envelope can end up with all other discrete variables in the envelope as parents). It shows that different link-reversal orders can change rBN sparsity (illustrated with an example where one ordering introduces an extra link). It demonstrates via conceptual examples that discretizing selected continuous variables can reduce SRM evaluations dramatically (e.g., from $m^4(m-1)$ system SRM calculations to $5(m-1)$ component SRM calculations in a hierarchical structure with $m$ states per discrete variable). It provides constructive procedures for representing evidence on continuous variables by adding discrete nodes whose states correspond to observation domains, enabling exact BN updating on the rBN.","The authors note that the approach does not offer advantages for problems lacking exploitable conditional independence, highlighting discretized random fields (non-Markovian dependence) as a key difficulty: observations at many locations can force very large parent sets and conditional tables (scaling like $m^n$). They also indicate that computational feasibility depends critically on keeping Markov envelopes small (roughly limited to ~15–20 binary discrete variables). They mention approximate inference (e.g., MCMC) as an alternative but emphasize limitations for rare-event probabilities and near-real-time decision support.","The methodology relies heavily on discretization choices (binning, tail handling, and the conditional distribution assumed within bins), which can introduce bias and understate dependence (especially when “splitting” a single continuous variable into multiple conditional copies to separate envelopes). Practical implementation may require substantial expert effort to design a tractable eBN and to run many SRM computations for conditional probability tables, yet guidance on error control/accuracy vs. computational cost is mostly qualitative. The framework’s performance for highly nonlinear, high-dimensional limit-state models (where SRMs themselves may struggle) is not benchmarked with systematic numerical studies in this methodology paper.","The paper points to the need for further work to address complex dependence structures (notably random fields) within the eBN framework, mentioning that ideas like principal component analysis have been explored but are not yet sufficient. It also suggests that approximate inference directly on the hybrid eBN (e.g., MCMC) could handle more general dependence, though challenges remain for rare-event probabilities and real-time use. The companion paper is referenced as demonstrating applications to structural and infrastructure systems.","Develop formal discretization and model-error quantification procedures (e.g., adaptive discretization with accuracy guarantees, sensitivity of posterior/reliability estimates to binning and tail models). Extend the framework to better handle spatial/random-field dependence (e.g., sparse latent-factor or Gaussian-process surrogates coupled to eBNs) while preserving rare-event accuracy. Provide open-source implementations integrating BN inference with reliability solvers and include standardized numerical benchmarks comparing eBN/rBN vs. hybrid MCMC/SMC approaches on rare-event updating problems.",1203.5986v1,https://arxiv.org/pdf/1203.5986v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:10:00Z
TRUE,System reliability|Other,Bayesian|Other,Event/count data|Other,Not applicable,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper develops Bayesian sequential (two-stage / hybrid) sampling schemes to estimate the reliability of a parallel-series (and by duality series-parallel) system when the total test budget/sample size is fixed but allocation across subsystems/components is to be chosen. Component reliabilities are modeled as independent Bernoulli parameters with independent Beta priors, leading to Beta-Binomial updating and posterior-mean plug-in estimators for subsystem and system reliability polynomials. The authors derive large-sample (first-order) approximations to the Bayes risk under squared-error loss and use Lagrange-identity decompositions to identify allocations that asymptotically minimize this risk. For a parallel subsystem they propose a two-stage design: an initial pilot sample of size L=⌊√m⌋ per component to estimate allocation weights, followed by allocating the remaining tests proportionally to estimated √U_i terms. For the parallel-series architecture they combine subsystem-level allocation based on √(B_i w_i) with component-level two-stage allocations within each parallel subsystem, and prove first-order optimality of the resulting Bayes risk using martingale convergence and Doob’s inequality.","Parallel system reliability is modeled as $p = 1-\prod_{i=1}^n (1-p_i)$ with estimator $\hat p=1-\prod_{i=1}^n(1-\hat p_i)$, where $\hat p_i$ is the Beta-posterior mean from Beta-Binomial updating: $a_{i,m_i}=a_i+\sum_{k=1}^{m_i}x_i^{(k)}$, $b_{i,m_i}=b_i+m_i-\sum_{k=1}^{m_i}x_i^{(k)}$. The large-sample Bayes-risk approximation is $\tilde R_m(p)=\mathbb E[\sum_i U_i/(m_i+r_i)]$ and yields the asymptotically optimal allocation $m_i \propto \sqrt{U_i}$, implemented via a two-stage rule using $L=\lfloor\sqrt m\rfloor$ and $\hat m_i = m\,\sqrt{U_{iL}}/\sum_j\sqrt{U_{jL}}$. For a parallel-series system with series across subsystems and parallel within each subsystem, $\hat p=\prod_{i=1}^n \hat p_i$ with $\hat p_i=1-\prod_{j=1}^{n_i}(1-\hat p_{ij})$, and the subsystem-level allocation targets $m_i \propto \sqrt{B_i w_i}$ (equation (3.9)), with component-level allocations within subsystem $i$ using the same two-stage $m_{ij}\propto \sqrt{U_{ij}}$ rule.","For a parallel system, the proposed two-stage sequential allocation achieves first-order asymptotic optimality: $\lim_{m\to\infty} m\,R_m(p)=\mathbb E\big[(\sum_{i=1}^n \sqrt{V_i})^2\big]$, with the realized allocations satisfying $m_i/m \to \sqrt{V_i}/\sum_j\sqrt{V_j}$ almost surely (Lemma 2.1). For the parallel-series system, the hybrid two-stage design similarly yields $\lim_{m\to\infty} m\,R_m(p)=\mathbb E\big[(\sum_{i=1}^n \sqrt{B_i Z_i})^2\big]$ and subsystem allocations $m_i/m \to \sqrt{B_i Z_i}/\sum_k\sqrt{B_k Z_k}$ a.s. (Lemma 3.1, Theorem 3.1). Optimality proofs rely on martingale convergence (posterior expectations converging to latent quantities) and uniform integrability via Doob’s inequality. The paper is primarily theoretical and does not report numeric ARL-style performance tables or empirical comparisons.","The conclusion notes that extending the techniques beyond the studied structures may be “tediously” complex, indicating practical difficulty in adapting the method to more complex systems and multi-criteria constraints. The work is framed in an asymptotic (large-sample) setting with first-order optimality results, implying reliance on large total sample sizes for the theoretical guarantees. No other explicit limitations are stated.","The method assumes conditional independence of component reliabilities and Beta priors with Bernoulli testing, which may be unrealistic with common-cause failures, dependence within subsystems, or more complex failure mechanisms. Optimality is only first-order asymptotic under squared-error Bayes risk; finite-sample performance and robustness to prior misspecification or model misspecification (e.g., overdispersion beyond Binomial) are not demonstrated. The approach requires computing weight terms ($U_{ij}$, $w_i$, etc.) from pilot data; for large systems this may be nontrivial and sensitivity to the pilot size choice $L=\lfloor\sqrt m\rfloor$ is not explored. The paper does not provide simulation studies or real datasets to validate practical gains versus simpler allocation heuristics or dynamic programming benchmarks.","The authors suggest that, with minor changes, series-parallel systems can be treated similarly using duality. They also state that the techniques can be adapted (though tediously) to more complex systems involving multi-criteria optimization under constraints such as risk, system weight, cost, and performance.","A natural extension is to study finite-sample performance via Monte Carlo and provide practical guidance on pilot sample sizing and sensitivity to prior hyperparameters. Extending the framework to dependent components (e.g., common-cause failure, hierarchical priors, or copula models) would increase realism for engineered systems. Incorporating imperfect test information (censoring, varying test times, accelerated testing) and moving beyond Bernoulli pass/fail outcomes to lifetime/degradation data would broaden applicability. Providing open-source implementations (e.g., R/Python) and computational comparisons against dynamic programming or approximate DP allocation methods would improve adoption.",1204.0549v1,https://arxiv.org/pdf/1204.0549v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:10:30Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1204.5963v2,https://arxiv.org/pdf/1204.5963v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:10:30Z
FALSE,NA,Simulation-based|Other,Simulated only|Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper is about improving statistical estimation in respondent-driven sampling (RDS) studies by incorporating linked ego-network composition data (e.g., proportions of alters with a trait). It proposes a new estimator, RDS Iego, which replaces the observed recruitment matrix in the classic RDS I estimator with an ego-network–based estimator of cross-group link proportions using Hansen–Hurwitz weighting. The method is evaluated primarily via large-scale simulations (10,000 runs) on an empirical MSM social network and on synthetic networks generated by the KOSKK model under varying homophily, activity ratio, and differential recruitment. Results show substantially reduced bias and variance compared with standard RDS estimators, especially under differential recruitment; reported biases of traditional estimators can reach ~10%–20% while RDS Iego biases are typically <2% when ego composition is accurately reported. The paper also studies sensitivity to degree undercounting and alter misclassification, finding robustness to degree underreporting but higher sensitivity to misclassification of alter traits.","The proposed estimator builds an ego-network estimator of cross-group link proportions: \(\hat{s}^{ego}_{XY} = \frac{1}{n_X}\sum_{v_i\in X\cap U} \frac{n_i^Y}{d_i}\), derived from Hansen–Hurwitz weighting under degree-proportional inclusion in RDS. This \(\hat{s}^{ego}\) replaces the observed recruitment matrix in RDS I, giving \(\hat{P}_A = \frac{\hat{s}^{ego}_{BA}\,\widehat{\bar{D}}_B}{\hat{s}^{ego}_{AB}\,\widehat{\bar{D}}_A + \hat{s}^{ego}_{BA}\,\widehat{\bar{D}}_B}\) (RDS Iego), where \(\widehat{\bar{D}}_X\) is an estimated mean degree for group \(X\) from reported degrees.","In simulations on an empirical MSM network, under differential recruitment (group A twice as likely to be recruited), the raw recruitment-based estimate of \(s^*_{AB}\) can have large bias (e.g., for ‘ct’ Bias≈0.09) while the ego-based \(\hat{s}^{ego}_{AB}\) remains near-unbiased (Bias≈0.01) with lower SD (≈0.02 vs 0.03–0.04). For estimating population proportions \(P_A^*\), under differential recruitment the sample proportion and RDS I can have large bias (e.g., for ‘ct’ Bias≈0.20 and 0.17) whereas RDS Iego has much smaller bias (≈0.02) and RMSE (≈0.06). Across variables under differential recruitment, RDS Iego biases are reported in roughly [0.00, 0.02] with RMSE about [0.04, 0.07], versus substantially larger errors for sample proportions and RDS I. Robustness studies over synthetic KOSKK networks show RDS Iego maintains low bias across varying homophily/activity ratio even when RDS I bias can reach ~0.20.","The authors state that RDS Iego requires collecting ego-network composition data, which may be difficult for hidden/stigmatized populations and for sensitive traits (respondents may not know or may not report alter characteristics accurately). They explicitly note that RDS Iego is sensitive to alter-trait misclassification in ego reports; when misclassification is substantial and asymmetric between groups, estimate bias can exceed 0.1. They also note that even the improved bootstrap confidence intervals “rarely approach required coverage rates” on simulated networks, indicating remaining challenges in uncertainty quantification.","The evaluation is largely simulation-based and relies on a single empirical online MSM friendship network plus a particular synthetic network model (KOSKK); performance may differ in other real-world network types (e.g., offline contact networks, directed/weighted ties, temporally evolving networks). The estimator assumes respondents can provide unbiased estimates of ego composition proportions and that reported degrees/ego counts map cleanly onto network ties; in practice, name generators/boundary specification and recall biases could create systematic, not random, errors. The work does not provide an implementation package or guidance for practical survey instrument design (question wording, validation procedures) needed to achieve the low misclassification regime where RDS Iego excels. Comparisons exclude other modern RDS estimators that could be competitive in some regimes, so the benchmark set is incomplete for decision-making.","The authors explicitly state that future work is needed to develop confidence-interval estimation methods with improved precision, because even the proposed ego-based bootstrap often underachieves nominal coverage, especially on networks with community structure. They also encourage integrating ego-network questions and the improved bootstrap procedure into future RDS studies and suggest using discrepancies between \(\hat{S}^{ego}\) and the observed recruitment matrix \(S\) to assess the severity of differential recruitment. They note applicability beyond public health (e.g., sampling internet content) where ego-network data may be more reliable/easier to obtain.","Developing a self-starting/Phase-I calibration approach that jointly models recruitment bias and ego-reporting error (e.g., via Bayesian measurement-error models) could make RDS Iego more robust when misclassification is non-negligible. Extending the estimator and theory to directed, weighted, and multiplex ties (and to time-varying networks) would broaden applicability to realistic social-contact settings. Providing open-source software and standardized survey modules (with validation studies) would improve reproducibility and practical adoption, and enable routine sensitivity analyses for misclassification/degree errors. Empirical validation on multiple real RDS datasets with known ground truth (or strong external benchmarks) would strengthen evidence beyond simulation settings.",1205.1971v2,https://arxiv.org/pdf/1205.1971v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:11:21Z
TRUE,Life distribution modeling|Maintenance optimization|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based",Event/count data|Mixture of types,Not applicable,Energy/utilities|Other,Simulation study|Other,TRUE,Other,In text/Appendix,NA,"The paper presents a practitioner-oriented Bayesian methodology for age-dependent reliability analysis of ageing/degrading systems using failure-count data observed over consecutive time periods. Age-dependent failure rates are modeled with several parametric trend functions (e.g., linear, log-linear/exponential, power-law/Weibull, Xie–Lai additive Weibull, and generalized Makeham) embedded in a piecewise homogeneous Poisson regression/jump-process framework. Bayesian inference is performed via MCMC in WinBUGS, with emphasis on prior selection (warning that diffuse Gamma priors with very small parameters can unintentionally concentrate mass near zero and bias inferences) and model checking (posterior predictive p-values) alongside DIC. Because model selection criteria can be ambiguous, the authors advocate Bayesian Model Averaging (BMA) using marginal likelihoods estimated via power posteriors, and show in a case study of electrical instrumentation & control components that BMA can yield improved predictive uncertainty quantification versus adopting a single trend model. The work advances reliability practice by providing a step-by-step Bayesian workflow for sparse/rare-event failure data with explicit treatment of model uncertainty.","Failure counts in each interval follow a Poisson model with rate $\lambda_i\tau_i$, where $N_i\sim\text{Poisson}(\lambda_i\,\tau_i)$ and likelihood $L(\Theta)=\prod_{i=1}^N \exp\{-\lambda(t_i,\Theta)\tau_i\}\,\frac{\{\lambda(t_i,\Theta)\tau_i\}^{N_i}}{N_i!}$. The age-dependent failure rate is represented as a jump process $\lambda(t)=\sum_{i=1}^N \mathbf{1}_{\{t_i<t\le t_{i+1}\}}\lambda_i$ with parametric trend forms such as log-linear $\log\lambda(t)=\theta_1+\theta_2 t$, power-law $\lambda(t)=\theta_1 t^{\theta_2}$, and more flexible bathtub-capable forms (Xie–Lai additive Weibull and generalized Makeham). Bayesian updating is via $\pi(\Theta\mid Y)\propto f(Y\mid d(t,\Theta))\,\pi(\Theta)$, and model uncertainty is handled using BMA: $p(A(t)\mid Y)=\sum_{j=1}^r p(A(t)\mid Y,M_j)\,p(M_j\mid Y)$ with $p(M_j\mid Y)\propto p(Y\mid M_j)p(M_j)$. Marginal likelihoods $p(Y\mid M_j)$ are estimated using power posteriors (thermodynamic integration) as described by Friel & Pettitt (2008).","In the I&C component case study (15 yearly age bins), DIC favored the exponential/log-linear trend (DIC 86.48) over power-law (88.42), Xie–Lai (88.00), linear (91.39), and generalized Makeham (94.00). Posterior predictive checks gave mixed conclusions: p(D1) values were moderate (e.g., linear 0.5458; exponential 0.6333), while chi-square-based p(D2) were very small for all models (e.g., exponential 0.0278), illustrating discrepancy-measure sensitivity. Using power-posterior marginal likelihoods, BMA posterior model probabilities were reported as: linear 0.107, exponential 0.422, power-law 0.108, generalized Makeham 0.216, Xie–Lai 0.147. The authors report that BMA yields different (typically more conservative/appropriately wider) predictive intervals than the single best model and argue it is superior for reliability prediction under model uncertainty.","The authors note that Bayesian Model Averaging cannot handle an infinite set of candidate models; if the true/best model is not among the finite set considered, BMA cannot recover it. They also state that BMA may fail to “emit an alert signal” when all candidate models fit the data very poorly, in which case averaging does not guarantee better predictive performance. They further emphasize that common model-checking tools (posterior predictive p-values with a single discrepancy measure, and DIC) can be misleading/ambiguous and require careful practitioner judgment.","The case study uses aggregated annual failure counts with varying exposure times, which can hide within-year heterogeneity, maintenance/operational changes, and reporting/inspection effects; these could induce non-Poisson behavior (overdispersion, dependence) not modeled. The approach appears to assume conditional independence of counts and correct specification of the exposure offsets; no robust/overdispersed alternatives (e.g., negative binomial, frailty, or random effects) are evaluated. Prior ranges are chosen heuristically (e.g., uniform on large intervals); sensitivity analysis to these bounds and to alternative weakly-informative priors is limited, even though small-sample behavior is a core motivation. The evaluation is largely based on a single dataset, so generalizability across component types and operating contexts is not empirically established.","The authors suggest the methodology can serve as groundwork for broader ageing/degradation assessments beyond failure counts, including stochastic crack growth where the modeled characteristic is crack growth rate, and degradation modeled as transitions through Markovian states where $d(t)$ would be (time-homogeneous or time-inhomogeneous) transition rates. They indicate the general idea of ageing as time-evolving beliefs about reliability parameters enables application to a wide spectrum of degradation and state-transition problems.","Extending the framework to handle overdispersion and dependence (e.g., hierarchical Poisson-gamma/negative binomial models, shared frailty, or autoregressive random effects) would improve realism for field failure data. Developing robust, weakly-informative prior recommendations with principled scaling (and formal prior-sensitivity workflows) would make the method easier to apply safely in sparse-data settings. A self-starting/online updating implementation for streaming operational data, plus diagnostic tools for identifying when all candidate models are inadequate (model expansion or nonparametric hazard components), would strengthen practice. Validation on multiple real datasets and inclusion of covariates (environment, maintenance actions, reactor/unit effects) would better support adoption in PRA and reliability engineering.",1210.5095v1,https://arxiv.org/pdf/1210.5095v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:12:04Z
TRUE,Degradation modeling|Maintenance optimization|RUL prediction|Other,Stochastic process|Other,Sensor/condition monitoring|Right-censored|Degradation measurements|Mixture of types,Condition-based|Predictive,Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Hidden Markov Model (HMM) framework to detect as early as possible the transition of optronic equipment from a stable operating mode to a degraded mode so that maintenance can be performed before failure. The hidden health state is modeled as a continuous-time finite-state Markov chain, while the observed signal (cool down time, Tmf; after preprocessing and smoothing, Tmfl) is treated as a noisy (Brownian) integral of a state-dependent drift. Using continuous-time filtering (unnormalized filter and a change of measure), the authors compute the posterior probability of being in the degraded state at each startup given the history of the observed signal. The method is evaluated via simulations (including sensitivity to misspecification of the transition matrix and drift parameters) and then applied to real logbook data from 28 appliances (5 observed cooling-system failures, 23 censored/non-failure cases). A simple maintenance decision rule based on the degraded-state probability reaching 1 for three consecutive uses yields 0 false alarms among 23 non-failed appliances and detects 3 of 5 observed failures before breakdown.","The model uses a continuous-time Markov chain state process $X_t$ with semimartingale form $X_t = X_0 + \int_0^t A X_r\,dr + V_t$, where $A$ is the generator (Q-matrix). Observations follow $Y_t = \int_0^t c(X_r)\,dr + W_t$ with $c(X_t)=\langle X_t,c\rangle$ and Brownian noise $W_t$. The posterior state probability is obtained from the filter $\mathbb{E}[X_t\mid \mathcal{Y}_t] = \sigma(X_t)/\sigma(1)$, and (when estimable from long trajectories) parameters are given by $\hat a_{ij}(t)=\sigma(\varsigma_{ij,t})/\sigma(\vartheta_{i,t})$ and $\hat c_i(t)=\sigma(T_{i,t})/\sigma(\vartheta_{i,t})$.","On real data (28 appliances), five appliances experienced observed cooling-system-related failure and 23 were treated as censored/no-failure with respect to the modeled degradation mechanism. Using an expert/data-informed parameter choice $\hat c=(0,1)$ and $\hat a_{12}=1/100$ (inflated from a survival-based estimate $1/1000$ to favor earlier detection) with $\hat a_{21}=1/1000$, the filtered degraded-state probability stayed near 0 for a healthy unit and rose sharply toward 1 for a degrading unit. A maintenance decision rule of “probability equals 1 for three consecutive uses” produced 26/28 correct classifications: 3/5 failures detected before failure with 0/23 false positives among non-failed appliances (2/5 failures not detected). Simulation sensitivity studies indicate that moderate deviations in $A$ or $c$ do not strongly affect the degraded-state probability estimate.","The authors note that, unlike the simulation setting with many state transitions, the industrial process is effectively stopped at its first transition to the degraded state, so the long-horizon estimation procedure for $A$ and $c$ (based on repeated transitions) is not applicable to the real data. Consequently, they adopt a practical selection of $A$ and $c$ combining expert opinion and limited estimation (including deliberately increasing $a_{12}$ to detect earlier). They also suggest the two missed detections may correspond to failures not related to the modeled cooling-system degradation mechanism.","The observation model assumes a continuous-time Brownian-noise formulation, while the real measurements are discrete by startup count and include preprocessing (linear regression correction and moving-average smoothing) that can distort noise properties and induce dependence; the impact on filtering optimality is not quantified. The decision rule (probability equals 1 for three consecutive uses) is tuned on the same small dataset (28 units) without an explicit validation protocol, so generalization and expected false-alarm/detection rates are uncertain. The model structure is essentially two-state (stable/degraded) in application, with hand-set transition rates and no explicit failure state modeling or cost-based maintenance optimization; it does not produce an explicit lead-time/RUL distribution. Independence across appliances and a common $(A,c)$ for all units may be unrealistic if there is unit-to-unit heterogeneity (random effects) or covariate effects beyond initial temperature.","The authors state that Thales is working on implementing the model in a HUMS to support a new maintenance algorithm, with two deployment options: onboard assessment by the embedded calculator or periodic assessment via a maintenance laptop connected to the system. They also emphasize that the approach supports a shift from preventive/corrective to predictive maintenance and enables a degraded operational mode selection based on estimated health state.","A natural extension is to develop a discrete-time/state-space formulation aligned to startup-indexed observations, including explicit measurement-noise modeling after temperature correction and smoothing, and to analyze robustness to autocorrelation and non-Gaussian noise. Incorporating unit heterogeneity (hierarchical/Bayesian HMM or random-effects transition intensities) could improve accuracy across fleets and provide calibrated uncertainty for maintenance decisions. The decision criterion could be optimized via ROC/cost-sensitive analysis and validated with cross-validation or prospective trials, potentially producing expected detection lead time and false-alarm rates. Extending the state model to include an absorbing failure state and estimating a time-to-failure/RUL distribution would make the method more directly actionable for maintenance planning.",1212.2358v1,https://arxiv.org/pdf/1212.2358v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:12:45Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Right-censored|Left-censored|Mixture of types|Other,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a hierarchical Bayesian method to estimate component reliability functions in series and parallel systems, modeling each component lifetime with a Weibull distribution. It leverages the fact that, from system-level data, component lifetimes are observed under right censoring for series systems (system fails at the minimum component time) and left censoring for parallel systems (system fails at the maximum component time). Priors are set hierarchically: Weibull shape and scale parameters receive independent Gamma priors (parameterized by mean and variance), while the Gamma means have flat (improper uniform) hyperpriors and the Gamma variances are fixed as prior-precision constants. Estimation uses an EM algorithm to obtain posterior modes of the hyperparameters and MCMC to sample from the posterior of component Weibull parameters, from which posterior-mean reliability curves and HPD credible bands are constructed. Performance is demonstrated via a challenging simulated example with heavy censoring and an extensive simulation study (108 scenarios × 100 replications) showing small bias/MSE for estimating mean reliability time even under model misspecification (gamma/lognormal generating distributions).","Component reliability is Weibull: $R_j(x\mid\beta_j,\eta_j)=\exp\{-(x/\eta_j)^{\beta_j}\}$ with likelihood built from system observations $(T,\delta)$: for series systems contributions are $f_j(t_i\mid\theta_j)^{\mathbb{I}(\delta_i=j)}R_j(t_i\mid\theta_j)^{1-\mathbb{I}(\delta_i=j)}$ (right-censoring for non-failed components), while for parallel systems they are $f_j(t_i\mid\theta_j)^{\mathbb{I}(\delta_i=j)}(1-R_j(t_i\mid\theta_j))^{1-\mathbb{I}(\delta_i=j)}$ (left-censoring). Priors: $\beta_j\sim\text{Gamma}(m_{\beta j},v_{\beta j})$, $\eta_j\sim\text{Gamma}(m_{\eta j},v_{\eta j})$ (mean/variance parameterization), with flat hyperpriors $\pi(m_{\beta j})\propto 1$, $\pi(m_{\eta j})\propto 1$ and fixed $v$’s. Reliability estimate is posterior mean computed from MCMC draws: $\hat R(t)=1-\frac{1}{n_p}\sum_{\ell=1}^{n_p}F(t\mid\beta_{j\ell},\eta_{j\ell})$, and mean lifetime uses $E[T\mid\beta,\eta]=\eta\,\Gamma(1+1/\beta)$.","In Example 1 with $n=100$ systems and heavy censoring, posterior means (sd) for series systems were (Comp1) $\beta=1.26(0.17)$, $\eta=2.27(0.41)$, $E[T]=2.13(0.45)$; (Comp2) $\beta=3.98(0.53)$, $\eta=2.06(0.12)$, $E[T]=1.87(0.11)$; (Comp3) $\beta=1.40(0.17)$, $\eta=1.83(0.22)$, $E[T]=1.68(0.22)$. For parallel systems the corresponding posterior means (sd) were (Comp1) $\beta=1.23(0.15)$, $\eta=2.60(0.27)$, $E[T]=2.45(0.22)$; (Comp2) $\beta=2.47(0.32)$, $\eta=2.25(0.14)$, $E[T]=2.00(0.13)$; (Comp3) $\beta=0.87(0.11)$, $\eta=1.98(0.33)$, $E[T]=2.17(0.29)$, with 95% credible bands generally covering the true reliability curves. In the broader simulation study (108 scenarios varying $n\in\{30,100,1000\}$, censoring 0/20/40%, mean 2 or 7, generating distribution Weibull/gamma/lognormal, left/right censoring), reported biases and MSEs for estimated mean reliability time were typically near 0 and remained reasonable even for small $n=30$ and 40% censoring; when the data were truly Weibull, performance was strongest, but results were still good under gamma/lognormal generation.",None stated.,"The method assumes independent component lifetimes and fits Weibull models to each component; robustness is assessed via simulations, but there is no formal robustness analysis or model-checking strategy for real applications (e.g., posterior predictive checks). Hyperprior choices include improper flat priors on Gamma means and fixed prior variances (precision parameters), and sensitivity to these hyperparameters (beyond suggesting $v=4$) is not systematically studied. The approach requires MCMC and an EM-within-MCMC workflow; computational cost and convergence diagnostics are discussed only informally, and no reproducible implementation details (software, tuning specifics, acceptance rates) are provided.","The authors suggest developing hypothesis tests for component properties, such as testing equality of component mean lifetimes (for all components or subsets). They also propose extending the approach beyond series/parallel structures to more general coherent systems.","Provide a self-contained, reproducible software implementation (e.g., an R/Python package) with convergence diagnostics and recommended default tuning for the EM–MCMC procedure. Extend the model to handle dependent components (shared frailty/coplanar random effects or copula models) and covariates, which are common in system reliability. Add principled prior-sensitivity analyses for the fixed Gamma-variance hyperparameters and consider alternative hierarchical priors (e.g., half-t priors on scales) to reduce reliance on fixed “precision” settings.",1302.3053v1,https://arxiv.org/pdf/1302.3053v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:13:32Z
TRUE,System reliability|Other,Nonparametric/Semi-parametric|Simulation-based|Other,Mixture of types|Other,Not applicable,Transportation/logistics,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper applies a resampling (computer-intensive) statistical approach to estimate the reliability of an aircraft circulation plan, defined as the probability that a sequence of scheduled departures can be executed without departure delays (thus avoiding use of a reserve aircraft). The model treats flight arrival delays $X_i$ and post-flight service times $Y_i$ as independent random variables, and defines plan reliability as $R(T)=P(X_1+Y_1\le t_1,\dots,X_k+Y_k\le t_k)=\prod_i R_i(t_i)$. Because the underlying distributions are unknown and only small samples are available for each $X_i$ and $Y_i$, the method estimates $R(T)$ via repeated resampling from the observed samples and averaging an indicator function of plan feasibility. The paper derives an explicit variance expression for the resampling estimator using a pair-overlap (\omega-pair) framework to account for dependence between resampling replicates due to reusing sample observations. A numerical example (assuming exponential delays/service times) illustrates how the estimator variance changes with available turnaround time between flights and demonstrates feasibility of the approach under small-sample conditions.","Plan reliability is defined as $R(T)=P(X_1+Y_1\le t_1,\dots,X_k+Y_k\le t_k)=\prod_{i=1}^k R_i(t_i)$ with $R_i(t)=P(X_i+Y_i\le t)=\int_0^{\infty} F_i(t-x)\,dG_i(x)$. The resampling estimator draws one observation from each empirical sample of $X_i$ and $Y_i$ per replicate and averages the indicator $\phi_T(X,Y)=\mathbf{1}\{x_1+y_1\le t_1,\dots,x_k+y_k\le t_k\}$: $\hat\theta_T^*=\frac{1}{r}\sum_{l=1}^r \phi_T(X^{(l)},Y^{(l)})$. Its variance is expressed as $D\hat\theta_T^*=\frac{1}{r}\mu_2+\frac{r-1}{r}\mu_{11}-\mu^2$ with $\mu_2=\mu=\theta_T$ and $\mu_{11}=\sum_{\omega_1,\omega_2}\mu_{11}(\omega_1\omega_2)P(\omega_1\omega_2)$ using the \omega-pair overlap probabilities.","In the numerical example with $k=5$ flights, exponential delays and service times with parameters $\lambda_i=0.05$ and $\mu_i=0.02$, sample sizes $n_i^X=n_i^Y=20$, and $r=50$ resamples, the variance of the resampling estimator depends strongly on the common slack time $t$. Reported values include $D\hat\theta_T^*\approx 6.9\times10^{-7}$ at $t=20$, increasing to about $0.0124$ at $t=140$, then decreasing to about $0.0011$ at $t=300$. The paper explains this non-monotone pattern as uncertainty being highest when the probability of meeting schedule constraints is near intermediate values (neither near 0 nor near 1).",None stated.,"The core model assumes mutual independence across flights for both delays $X_i$ and service times $Y_i$, which may be unrealistic because delays propagate and operational conditions are correlated across legs. The resampling scheme is described at a high level, but practical implementation choices (with/without replacement, handling ties, and finite-sample bias/coverage) are not fully detailed, and no sensitivity analysis is provided. Evaluation is limited to a single parametric numerical example (exponential assumption) and variance behavior; there is no comparison to alternative estimators (e.g., plug-in convolution with fitted distributions, bootstrap confidence intervals) or validation on real airline operational data.",None stated.,"A natural extension is to relax independence by modeling delay/service sequences with dependence (e.g., via copulas, Markov structure, or block/bootstrap resampling) to reflect delay propagation across legs. The method could be expanded to provide confidence intervals for $R(T)$ (percentile/basic/bootstrap-t) and to incorporate parameter uncertainty and model selection when some distributional structure is assumed. Empirical validation on real operational datasets and benchmarking against parametric convolution and Bayesian approaches would clarify accuracy and robustness under realistic data limitations and censoring/recording practices.",1304.6643v2,https://arxiv.org/pdf/1304.6643v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:14:04Z
TRUE,Life distribution modeling|System reliability|Maintenance optimization|Degradation modeling|Other,Nonparametric/Semi-parametric|Simulation-based|Other,Complete lifetime data|Degradation measurements|Event/count data|Mixture of types,Condition-based|Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper surveys and develops resampling (nonparametric, simulation-based) estimators for reliability characteristics when component lifetime distributions are unknown and sample sizes are small. It presents simple and hierarchical resampling to estimate expectations of system performance functions for logical/structured systems (e.g., k-out-of-n, series/parallel, cold standby) and derives variance expressions using overlap-structure accounting via ω-, β-, and α-pairs. It extends the framework to settings with (i) one sample shared across multiple identical components (sampling without replacement) and (ii) partially known component distributions by conditioning on unknown components and simulating known-distribution variables. It also applies resampling to stochastic-process reliability models including a Poisson damage-arrival/degeneration (initial-to-terminal failure) model and a degradation–maintenance renewal-process comparison relevant to threshold crossing/failure-absence probabilities. Finally, it describes resampling-based confidence interval construction for logical systems and provides a method to compute the true coverage probability accounting for dependence among resampling estimates, with numerical illustrations comparing bias/variance/MSE against plug-in estimators.","The core resampling estimator repeatedly draws a resample $X^{*q}$ from empirical samples (typically with replacement; without replacement when one sample feeds multiple arguments), computes $\Theta^{*q}=\phi(X^{*q})$, and averages $\Theta^* = \frac{1}{r}\sum_{q=1}^r \Theta^{*q}$. Its variance is decomposed as $\mathrm{Var}(\Theta^*)=\frac{1}{r}\mu_2+\frac{r-1}{r}(\mu_{11}-\mu^2)$ where $\mu_{11}=E[\Theta^{*q}\Theta^{*q'}]$ depends on overlap of resamples; overlap probabilities are computed via ω-pairs with $P\{\omega\}=\prod_{i\in\omega}\frac{1}{n_i}\prod_{i\notin\omega}(1-\frac{1}{n_i})$ (and generalized via β/α-pairs when a single sample supplies multiple arguments). For the damage-accumulation model, $E[X_t]=\lambda\int_0^t(1-F(x))dx$ and $X_t$ is Poisson with mean $E[X_t]$; resampling simulates arrival intervals and degeneration times to estimate $E[X_t]$ and $P\{X_t=i\}$.","In the damage-accumulation example with $\lambda=0.5$, triangular degeneration time (parameter $a=2$), and $t=5$, the paper tabulates expectations of plug-in vs resampling estimators for $P\{X_5=i\}$ for sample sizes $n_A=n_B\in\{4,6,8\}$ and shows resampling expectations closer to the true Poisson probabilities as $n_A$ grows (bias decreasing with $n_A$). For estimating $E[X_5]$ where the true value is 1, Table 2 reports resampling having substantially lower variance/MSE than plug-in for small samples (e.g., at $n_A=4$: $\mathrm{Var}(\hat E[X_5])=0.79$ vs $\mathrm{Var}(E^*X_5)=0.55$; $\mathrm{MSE}(\hat E[X_5])=0.89$ vs $\mathrm{MSE}(E^*X_5)=0.55$). In the renewal degradation–maintenance simulation (normal increments, $r=1000$), Table 3 compares classical estimator variance/bias/MSE to resampling variance across thresholds $K=0..3$ and finds resampling is unbiased and has variance of similar magnitude to classical MSE depending on $(n,m,K)$. For resampling-based upper confidence intervals in a 3-component exponential example (true $\Theta=0.25$, $k=10$, $r=16$), Table 4 shows actual coverage probabilities exceeding nominal levels (e.g., requested $\gamma=0.9$ yields actual $R\approx0.77$–$0.83$ across sample-size settings), highlighting dependence effects.","The authors note that a disadvantage of the resampling approach is the inability to obtain good estimators of probabilities $P\{X_t=i\}$ for counts larger than the available sample size (e.g., $i>n_A$ in the damage-accumulation model). They suggest combining approaches: use resampling estimators for $i<n_A$ and plug-in estimators for $i>n_A$, followed by normalization of the probability mass.","The work largely assumes independence between component lifetimes/samples and between process components (e.g., i.i.d. inter-arrival and degeneration times), which may be violated in practical reliability/condition-monitoring data with dependence or covariates. Although computation is central to resampling, the paper does not provide implementation details, complexity guidance (choice of $r$, $N$ in nested simulation), or reproducible code, which can affect practical adoption. Comparisons to alternative modern methods (e.g., Bayesian hierarchical models, parametric frailty/dependence models, or variance-reduction techniques) are limited, so performance claims are mainly illustrated on selected examples. The confidence-interval section focuses on an upper interval for binary logical-function expectations and may not directly extend to continuous-valued reliability metrics or to multi-parameter interval estimation without additional development.",None stated.,"Develop self-starting/adaptive resampling procedures with automatic selection of $r$ (and nested $N$ for partially known distributions) to meet target precision at minimal computational cost. Extend the resampling variance/coverage analysis to dependent data settings common in reliability (shared environments, common-cause failures, autocorrelated degradation signals) and to multivariate systems with correlated component lifetimes. Provide software implementations (e.g., an R/Python package) with templates for common system structures (series/parallel/k-out-of-n/cold standby) and stochastic-process models, plus benchmarking against parametric/Bayesian alternatives. Investigate resampling-based interval estimation methods with improved finite-sample calibration (e.g., studentized, BCa-style corrections) to reduce the observed mismatch between nominal and actual coverage under dependence.",1304.6670v1,https://arxiv.org/pdf/1304.6670v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:14:57Z
TRUE,Degradation modeling|Accelerated testing,"Stochastic process|Parametric (Weibull, etc.)|Other",Degradation measurements,Not applicable,Semiconductor/electronics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,NA,"The paper proposes an economical step-stress accelerated degradation test (SSADT) design where stress is elevated based on degradation crossing a threshold, but elevations occur at discrete inspection times and are common across all test units (enabling use of a single chamber and avoiding continuous monitoring/sensors). Degradation increments are modeled via a gamma process with stress affecting the gamma shape rate through an Arrhenius model, and product lifetime under use stress is defined by first passage of cumulative degradation to a critical level. For a budget-constrained experiment, the authors optimize the threshold value, sample size, measurement frequency, and number of measurements/termination time by minimizing the asymptotic variance of an estimated lifetime quantile (via Fisher information and delta method). The design is illustrated on carbon-film resistor degradation data, producing optimal settings (e.g., n=13, f=52, M=7) and quantile-dependent optimal thresholds, and is supplemented with sensitivity and Monte Carlo stability analyses. Overall, it advances SSADT planning by integrating discrete inspections, common stress switching, and joint optimization of threshold and sampling plan under cost constraints for gamma-process degradation models.","Degradation increments under stress level $S_j$ follow a gamma process: $\Delta L(t\mid S_j) \sim \mathrm{Ga}(\alpha_j\Delta t,\,\beta)$, with Arrhenius stress linkage $\alpha_j=\exp\{a+b/(273+S_j)\}$. Under discrete inspections at times $kf$, stress switches at $\kappa_j=\min\{M,\lceil \tau_{(1),j}/f\rceil\}$ and the common stress schedule is $S_1$ on $[0,\kappa_1 f)$ then $S_2$ on $[\kappa_1 f, Mf)$. The likelihood combines the (approximate) pmf of $\kappa_1$ with gamma densities for increments: $L(\theta)=P_\theta(\kappa_1=k_1)\prod_i\prod_k \frac{g_{ki}^{f\alpha_j-1}e^{-g_{ki}/\beta}}{\Gamma(f\alpha_j)\beta^{f\alpha_j}}$ (with $j$ determined by $k$ relative to $\kappa_1$), and the design criterion minimizes $\mathrm{Avar}(\hat\xi_p)=\frac{1}{\hat f_0(\hat\xi_p)^2}h^T I(\hat\theta)^{-1}h$ subject to a cost budget.","In the carbon-film resistor illustration (failure threshold $D=5$ at use temperature $S_0=50^{\circ}C$, with $S_1=83^{\circ}C$ and $S_2=133^{\circ}C$, budget $\$1500), the optimized sampling plan was $(n^*,f^*,M^*)=(13,52,7)$ for all examined quantiles $p\in\{0.1,\ldots,0.9\}$. The corresponding total test duration was reported as $T^*=4 f M=1456$ (unit time = 4 hours), and the optimal threshold $\omega_1^*$ decreased slightly with larger $p$ (about 0.0507 at $p=0.1$ down to 0.0497 at $p=0.9$). The probability that stress is elevated before the experiment ends was extremely high (e.g., $G_{\tau_{(1),1}}(T^*;\hat\theta)\approx 0.999995$ across $p$ values). A Monte Carlo study (10,000 iterations) showed small bias/MSE for parameter estimates near the optimal plan, indicating stability to small deviations in $(n,f,M,\omega_1)$.","The authors note that the optimal design depends on initial parameter estimates (typically from a pilot study), motivating their sensitivity analysis for departures from the assumed true parameter vector. They also develop and present the optimization algorithm explicitly for the case of two stress levels ($m=2$), stating that extension to larger $m$ is straightforward but not fully detailed in the main algorithm.","The method relies on approximate distributions for stress-elevation/first-passage times (e.g., Birnbaum–Saunders approximations), and the impact of approximation error on optimal designs and inference is not thoroughly quantified. The model assumes independent gamma-process increments (no autocorrelation or unit-to-unit random effects beyond iid behavior), which may be violated in real degradation settings with heterogeneity or measurement error. Optimization is performed by enumerating integer design variables and using asymptotic variance (Fisher information/delta method), which may be inaccurate for small samples or short tests, and no guidance is given on finite-sample calibration. Implementation details are limited (e.g., numerical stability of MLE under mixed discrete/continuous likelihood and constraints), and no public code is provided to ensure reproducibility.","The paper states that the proposed optimization algorithm can be extended to larger numbers of stress levels (e.g., $m=3$ by optimizing over $(\omega_1,\omega_2)$ instead of a single threshold) and indicates that similar formulas can be obtained for joint pmfs of $(\kappa_1,\ldots,\kappa_{m-1})$ for $m\ge 4$.","A valuable extension would be to study robustness and redesign under model misspecification, including non-gamma degradation, autocorrelated increments, unit heterogeneity (random effects), and explicit measurement error. Developing self-starting or Bayesian designs that propagate uncertainty in pilot parameter estimates into the design choice would better address dependence on initial values. More scalable optimization (beyond brute-force enumeration) and providing open-source software for design computation would improve practical uptake. Finally, broader empirical validation across multiple real degradation datasets and comparisons to alternative SSADT/ADT designs (including continuous-monitoring schemes) would clarify when the discrete-inspection common-chamber design yields the largest benefit.",1404.3806v3,https://arxiv.org/pdf/1404.3806v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:15:43Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian",Complete lifetime data|Simulated only|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a new one-parameter lifetime model, the inverse Lindley distribution (ILD), obtained by transforming a Lindley random variable via X=1/Y, and studies its statistical properties (pdf/cdf, quantiles via Lambert W, mode, stochastic ordering, Renyi entropy, and hazard-rate shape). The ILD is characterized as heavy-tailed with an upside-down bathtub (unimodal) hazard rate, targeting non-monotone hazard lifetime data. A stress–strength reliability model is developed for independent X~ILD(θ1) (strength) and Y~ILD(θ2) (stress), deriving a closed-form expression for R=P[X>Y]. Estimation of (θ1,θ2) and R is addressed using maximum likelihood (with explicit MLEs) and Bayesian methods under Jeffreys and gamma priors with both squared-error and entropy loss, using Lindley’s approximation for Bayes computations. Performance is compared via Monte Carlo simulation (MSE) and the model is illustrated on two real survival datasets for head and neck cancer patients, where ILD is reported to fit better than a competing inverse Rayleigh distribution using KS/AIC/BIC.","The ILD density and CDF are f(x;θ)=\frac{\theta^2}{1+\theta}\left(\frac{1+x}{x^3}\right)e^{-\theta/x},\; x>0 and F(x;θ)=\left(1+\frac{\theta}{1+\theta}\frac{1}{x}\right)e^{-\theta/x}. The hazard rate is h(x)=\frac{f(x)}{1-F(x)}=\frac{\theta^2(1+x)}{x^2\{\theta+x(1+\theta)(e^{-\theta/x}-1)\}} (as presented). For stress–strength with X~ILD(θ1), Y~ILD(θ2), R=P(Y<X)=\frac{\theta_1^2\left[(\theta_1+\theta_2)^2(1+\theta_2)+(\theta_1+\theta_2)(1+2\theta_2)+2\theta_2\right]}{(1+\theta_1)(1+\theta_2)(\theta_1+\theta_2)^3}, and MLEs are \hat\theta_1=\frac{-(s_1-n)+\sqrt{(s_1-n)^2+8ns_1}}{2s_1}, \hat\theta_2=\frac{-(s_2-m)+\sqrt{(s_2-m)^2+8ms_2}}{2s_2} with s1=\sum_i 1/x_i, s2=\sum_j 1/y_j.","A Monte Carlo study with 5000 replications compares MLE and Bayes estimators (Jeffreys and gamma priors; SELF and ELF) across sample sizes and parameter settings chosen to yield R≈0.25, 0.50, and 0.75. The authors report that MSE decreases with increasing (n,m), Bayes under non-informative priors performs similarly to MLE, and informative gamma priors can reduce MSE relative to MLE. They also report that (for their setups) Bayes under entropy loss yields smaller MSE than under squared-error loss, and that MSE for estimating R is larger when true R is near 0.25 or 0.75 than when R≈0.5. In the real-data application (head & neck cancer survival), ILD shows better fit than inverse Rayleigh by KS/AIC/BIC (e.g., Data 2 KS 0.0799 for ILD vs 0.3783 for IRD), and the estimated stress–strength reliability is about R≈0.585 (MLE 0.5847).",None stated.,"The approach assumes independence between stress and strength samples and exact (non-censored) observations; the paper does not develop estimation for right/interval-censored survival data common in medical reliability contexts. Bayesian computation relies on Lindley’s approximation rather than exact posterior sampling/integration, and the accuracy of this approximation is not validated against MCMC or numerical quadrature. Model comparison in the case study is limited (primarily ILD vs one-parameter inverse Rayleigh) and does not benchmark against broader families (e.g., lognormal, Weibull variants, generalized Lindley/inverse-gamma mixtures) that also capture unimodal hazards.","The authors state that “further extensions of this distribution are under progress,” mentioning development of evolved versions with effective shape parameter(s) to be presented in future work.","Develop inference procedures for censored and truncated lifetime data (right/interval censoring) and assess robustness under model misspecification and dependence between stress and strength. Validate Bayesian estimators computed via Lindley approximation against MCMC-based posterior estimates, and provide guidance on when the approximation is accurate. Extend the framework to multicomponent/system settings (e.g., series/parallel systems) and to regression/accelerated-life formulations where θ depends on covariates.",1405.6268v1,https://arxiv.org/pdf/1405.6268v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:16:24Z
FALSE,NA,ML-based|Bayesian|Simulation-based,Simulated only|Other,Not applicable,Other,Simulation study|Other,TRUE,R|C/C++|Python|Other,Package registry (CRAN/PyPI)|Personal website,http://www1.montpellier.inra.fr/CBGP/diyabc|http://www.1000genomes.org/data,"This paper proposes a new approach for Approximate Bayesian Computation (ABC) model choice that reframes model selection as a supervised classification problem using random forests (RF). Instead of estimating model posterior probabilities directly via standard ABC (e.g., k-nearest neighbors on summary statistics), it predicts the most suitable model via an RF trained on an ABC reference table of simulated datasets, then estimates the posterior probability of the selected model using a second RF regression on the out-of-bag misclassification indicator. The method is argued and demonstrated to be more robust to the number/choice of summary statistics, to yield lower prior error rates (misclassification risk under the prior predictive), and to drastically reduce required simulation effort (reported typical speed gains around 50×). Performance is illustrated on controlled experiments (MA(1) vs MA(2), simulated population-genetics scenarios) and real population-genetics datasets (Harlequin ladybird invasion routes; 50,000-SNP human history example). The methodology is implemented in the R package abcrf on CRAN and leverages simulation/reference-table generation via DIYABC for genetic applications.","Model choice is cast as predicting the model index $m\in\{1,\dots,M\}$ from summary statistics $s=S(x)$ using an RF classifier $\hat m(s)$ trained on an ABC reference table sampled from $\pi(m)\,\pi(\theta\mid m)\,f(x\mid m,\theta)$. The global performance criterion is the prior error rate (integrated 0–1 loss) $\sum_m \pi(m)\int \mathbf{1}\{\hat m(S(y))\neq m\}\, f(y\mid\theta,m)\,\pi(\theta\mid m)\,dy\,d\theta$. The posterior probability of the selected model is estimated via a second RF regression $\varrho(s)\approx P[m\neq \hat m(s)\mid s]$ fit to out-of-bag errors, returning $1-\varrho(S(x_0))\approx P[m=\hat m(S(x_0))\mid S(x_0)]$.","On the toy MA(1) vs MA(2) example, the Bayes classifier using the full data achieves about 12% prior error, while the best summary-statistic-based method is RF with about 16.15% prior error (better than k-NN at ~17–18% and LDA/logistic at ~26–27%). For the Harlequin ladybird (10 scenarios, 130 summaries + 9 LDA axes), RF on summaries+LDA achieves the lowest reported prior error rate, decreasing from 36.86% (Nref=10k) to 34.44% (Nref=50k), outperforming standard ABC k-NN on summaries (~57% to ~51%) and LDA-only approaches (~39%). For the human 50k-SNP example (6 scenarios, 112 summaries + 5 LDA axes), RF on summaries+LDA attains ~5.01% (Nref=10k) down to ~4.18% (Nref=50k), and the estimated posterior probability for the selected scenario is reported as 0.998. The authors report that RF can provide a minimum computation speed gain around a factor of 50 compared with standard ABC model-choice treatments requiring far larger reference tables.","The paper emphasizes that (i) standard ABC posterior probabilities can be unreliable when conditioning on insufficient/poor summaries, and (ii) there is no direct connection between RF vote proportions and Bayesian posterior model probabilities, motivating their secondary RF-based probability approximation. It also notes that model choice is only meaningful when the simulated prior predictive produces datasets reasonably close to the observed data; otherwise models/priors should be reformulated. The approximation of the posterior probability via Algorithm 3 is acknowledged to be an approximation (and in the toy example it can underestimate high true posterior probabilities).","The approach still depends on the quality and coverage of the prior predictive simulations; if priors are misspecified or place little mass near the data, both the classifier and the probability regression can be misleading even if the RF machinery is robust. The proposed posterior probability estimate is for $P(m=\hat m\mid S)$ rather than $P(m=\hat m\mid x)$ and thus inherits any information loss from summaries; the paper demonstrates discrepancies but does not provide formal calibration/coverage guarantees for the RF probability estimate. Comparisons are largely within ABC-style summary-statistic pipelines; results may be sensitive to how summaries are scaled/constructed and to fairness of computational budgets across methods. The method focuses on model-index prediction, offering limited diagnostics for why models are confused beyond variable-importance heuristics and does not address decision-theoretic costs beyond 0–1 loss.","The authors suggest that ABC-RF will extend the feasible size/complexity of datasets and models handled by ABC (notably massive SNP datasets). They also state that once a model is chosen and confidence evaluated by ABC-RF, parameter inference under the selected model can proceed using standard ABC parameter estimation (e.g., Beaumont et al. 2002) or alternative methods (e.g., Excoffier et al. 2013).","Developing theoretical guarantees for the second-stage RF posterior-probability approximation (e.g., calibration, consistency, uncertainty quantification) would strengthen the method’s Bayesian interpretability. Extensions to handle dependent/structured summaries (time series, spatial dependence) and to incorporate model-misspecification checks directly into the RF workflow (beyond LDA visualization) would improve robustness. Providing open, end-to-end reproducible code and benchmark suites (including non-genetics domains) would facilitate adoption and fair comparison. Extending ABC-RF to sequential/adaptive simulation schemes (active learning to enrich the reference table near decision boundaries) could further reduce simulation cost while improving accuracy.",1406.6288v3,https://arxiv.org/pdf/1406.6288v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:22:11Z
FALSE,Other,Bayesian|Other,Sensor/condition monitoring|Simulated only|Other,Not applicable,Healthcare/medical|Theoretical/simulation only,Simulation study|Other,TRUE,R|MATLAB,Public repository (GitHub/GitLab)|Personal website,http://www.mathworks.com/matlabcentral/fileexchange/48453-mandymejia-shrinkit|https://github.com/mandymejia/shrinkR,"The paper proposes empirical Bayes shrinkage estimators for subject-specific resting-state fMRI voxel-by-voxel connectivity (correlation) matrices to improve reproducibility of single-subject brain parcellations obtained via clustering. It models observed connectivity as a noisy measurement of a subject-specific latent connectivity value, with between-subject (signal) variance and within-subject (noise) variance, and derives shrinkage weights that optimally combine the raw subject estimate with the group mean. Four practical variance-component estimators are developed (common, individual, scaled, and global), including strategies for when only one scan per subject is available via pseudo test–retest splitting and scan-length adjustment. Performance is evaluated on simulated data with known ground-truth parcellations and on a 20-subject test–retest dataset of two 7-minute rsfMRI scans, using MSE for connectivity reliability and Dice similarity for parcellation reproducibility. Results show substantial reductions in connectivity-estimation MSE and increases in parcellation test–retest Dice similarity, with reproducibility improvements reported up to ~30% for motor-cortex parcellations using shrinkage correlations (especially with the global noise-variance estimator).","The measurement-error model is $W_{ij}(v,v')=X_i(v,v')+U_{ij}(v,v')$ with $X_i\sim N(\mu_X,\sigma_X^2)$ and $U_{ij}\sim N(0,\sigma_{U,i}^2)$. The shrinkage estimator is $\tilde W_{ij}(v,v')=\lambda_i(v,v')\,\bar W_j(v,v')+(1-\lambda_i(v,v'))W_{ij}(v,v')$, where $\lambda_i(v,v')=\sigma^2_{U,i}(v,v')/(\sigma_X^2(v,v')+\sigma^2_{U,i}(v,v'))$. Parcellation reliability is quantified by Dice similarity $S(A_i,\hat A_i)=2|A_i\cap \hat A_i|/(|A_i|+|\hat A_i|)$.","In simulations under default settings (e.g., $I=20, T=200$), shrinkage reduced median MSE of correlation estimates from 0.00498 to about 0.00118–0.00150 (roughly 70–76% lower depending on variance estimator and whether single-session or test–retest variance estimation was used) and increased median Dice similarity of parcellations from 0.750 to about 0.923–0.962 (about 23–28% higher). On real test–retest rsfMRI (20 subjects, two 7-minute scans), shrinkage reduced median MSE versus a held-out test set from 0.0434 to ~0.0316–0.0344 (about 21–27% lower) in the single-session setup, and from 0.0491 to ~0.0344–0.0359 (about 27–30% lower) in the test–retest setup when shrinking correlations directly. Parcellation reproducibility (median Dice vs test-set parcellations) increased from 0.401 to up to 0.521 (30.0% higher) in the single-session setup and from 0.403 to up to 0.479 (18.8% higher) in the test–retest setup, with the global noise-variance estimator typically best or among the best.","The authors note that their real-data analysis is limited to the motor cortex (precentral gyrus) and to a population of healthy adults, and that benefits of shrinkage may vary by similarity metric, ROI, and population. They also caution that subject-specific shrinkage (e.g., scaled/individual noise variance) may not preserve subject ranking, so care is needed when interpreting subject differences. They mention that constrained clustering can artificially inflate reliability metrics, motivating their approach, but do not claim their results generalize to all parcellation methods or settings.","The shrinkage model assumes (approximate) normality and signal–noise independence at each voxel-pair; real rsfMRI correlations can exhibit non-Gaussianity, heavy tails, and structured dependence across voxel-pairs and time, which may affect optimality of the empirical Bayes weights. The approach shrinks toward a population mean, which can reduce sensitivity to true subject-specific structure when between-subject heterogeneity is large or when the cohort is not representative (dataset shift). Evaluation uses Dice similarity against a proxy “truth” (held-out scan segment/session), which conflates biological variability with measurement noise and may overstate or understate true reliability depending on state changes between sessions. Computational scalability is implicitly challenging (tens of millions of voxel pairs) and practical implementations may require approximations or memory-efficient strategies that are not fully detailed here.","The authors suggest extending evaluation to other brain regions, whole-brain parcellation, and more diverse populations to quantify how benefits vary by ROI, metric, and cohort. They also propose adapting the shrinkage ideas to parcellation methods that use other subject-level inputs beyond pairwise similarity/distance matrices, such as methods based on entire time series or principal components. They note that studying shrinkage benefits on longer scans using real rsfMRI data is important to understand interactions between scan length and shrinkage.","A useful extension would be robust or distribution-free shrinkage (e.g., heavy-tailed or nonparametric Bayes) and explicit handling of temporal autocorrelation and spatial dependence across voxel pairs to better match rsfMRI structure. Another direction is developing self-starting/online versions that update shrinkage parameters as more data accrue and providing uncertainty quantification for parcellations (e.g., posterior over adjacency/labels). Broad benchmarking against modern individual-parcellation methods (including spatially constrained approaches) on multiple public datasets and clinical populations would clarify generalizability. Finally, scalable implementations (GPU/parallel, sparse/low-rank approximations) and user-friendly software packaging with reproducible pipelines would improve adoption.",1409.5450v2,https://arxiv.org/pdf/1409.5450v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:23:02Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Other",Right-censored|Mixture of types|Other,Not applicable,Energy/utilities|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"This paper studies when discrete lifetime models are practically useful in industrial reliability problems where lifetime is measured in cycles/demands rather than calendar time. It focuses on two popular discrete models: the Inverse Pólya distribution (IPD), motivated by a Pólya urn ageing mechanism, and the Weibull-1 (Type I discrete Weibull) model derived by discretizing the continuous Weibull survival function. The authors show that IPD’s hazard rate is always concave (i.e., it can only represent decelerated ageing), which restricts its applicability and can lead to overly optimistic extrapolation under accelerated ageing. For Weibull-1, they analyze ageing/concavity properties and show that, unlike the continuous Weibull, hazard concavity can depend on both shape and scale parameters and may include an inflection point. They prove and empirically demonstrate that Weibull-1 is often well-approximated by the continuous Weibull—especially for reliable components (large scale parameter) and heavily right-censored feedback data—leading to similar MLEs and reliability quantities; they illustrate this on simulations and two EDF industrial datasets with high censoring rates (96% and 81%).","Inverse Pólya (IPD) hazard is $\lambda(n)=P(N=n\mid N>n-1)=\dfrac{\alpha+(n-1)\zeta}{1+(n-1)\zeta}$, with $\alpha$ interpretable as failure probability at the first demand and $\zeta$ controlling ageing; the discrete second difference $\lambda''(n)=\lambda(n)-2\lambda(n-1)+\lambda(n-2)=\dfrac{2(\alpha-1)\zeta^2}{\prod_{k=1}^3(1+(n-k)\zeta)}<0$ for $\alpha<1,\zeta>0$ (always decelerated ageing). Weibull-1 hazard is $\lambda(n)=1-\exp\{-(n/\eta)^\beta+((n-1)/\eta)^\beta\}$ with survival $S(n)=\exp(-(n/\eta)^\beta)$; also $p_{W1}(n)=S(n-1)-S(n)=\int_{n-1}^{n} f_W(t)\,dt$ links the discrete pmf to the continuous Weibull density and underpins the approximation results (e.g., $E_W[T]\le E_{W1}[N]\le 1+E_W[T]$).","For IPD, the hazard’s second discrete derivative is strictly negative under $\alpha<1,\zeta>0$, proving IPD cannot model accelerated ageing (only concave/decelerated ageing). For Weibull-1 and continuous Weibull with the same $(\eta,\beta)$, they prove $E_W[T]\le E_{W1}[N]\le 1+E_W[T]$ and (for $\beta\ge1$) uniform convergence of the discrete pmf to the continuous density as $\eta\to\infty$. Extensive simulations (5000 replications across $(\eta,\beta)$ grids, sample sizes 50 and 100, censoring 0–75%) show MLE errors and plug-in MTTF errors under Weibull-1 vs continuous Weibull are very similar (scatter close to the first bisector), implying little practical gain from using W1 in typical industrial regimes. On two EDF datasets with heavy right-censoring (96% and 81%), Weibull-1 and Weibull MLEs are nearly identical (e.g., sample 1: $\hat\beta\approx2.32$ for both; sample 2: $\hat\beta\approx1.12$ for both), while IPD can yield implausible parameter ratios and less conservative extrapolation at high solicitation counts in the accelerated-ageing case.","The authors note that IPD is intrinsically limited because it assumes decelerated ageing, which may be hard to justify a priori and leads to poor predictive performance when the true hazard is convex/accelerating. They also state that for Weibull-1 the familiar Weibull interpretation is not fully preserved: the type of ageing/concavity depends on both $\beta$ and $\eta$, and the hazard cannot be strictly convex since it must tend to 1 as $n\to\infty$. They emphasize that, in the industrial contexts considered (reliable components with high right-censoring), Weibull-1 brings low practical improvement over the continuous Weibull approximation.","The empirical study of Weibull-1 hazard inflection behavior is described as approximate and based on simplifying assumptions; the paper does not provide a full analytical characterization of concavity/inflection for $\beta>2$, leaving practitioners without closed-form guidance in accelerated-ageing regimes. The paper largely focuses on i.i.d. lifetime/demand counts and standard right-censoring; it does not address common industrial complications such as heterogeneous operating profiles, covariates, frailty, dependence across units, or repair/recurrence processes. While MLE is used, there is limited discussion of estimator robustness, identifiability issues under extreme censoring, uncertainty quantification (CIs), or model selection diagnostics beyond qualitative comparisons (e.g., no AIC/BIC calibration is highlighted in the excerpt).",They suggest extending the IPD model to discard the forced decelerated-ageing property by letting the number of added black balls $z$ depend on solicitation count $n$ (an increasing pattern rather than constant). They propose defining and comparing several such patterns from both analytical and computational viewpoints to preserve the model’s intuitive appeal while improving versatility in reliability analysis.,"Develop a self-starting/robust inference workflow for discrete demand-based lifetimes under heavy censoring that includes uncertainty intervals and goodness-of-fit diagnostics tailored to discrete hazards. Extend the discrete models to include covariates (e.g., discrete-time proportional hazards) and heterogeneous populations (frailty/mixtures), which are common in feedback data. Provide an analytical study or practical approximations for Weibull-1 hazard inflection/concavity regions for $\beta>2$, enabling easier engineering interpretation. Release reference implementations (e.g., R/Python) for IPD sampling and MLE under censoring to facilitate adoption and reproducibility.",1411.0408v1,https://arxiv.org/pdf/1411.0408v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:23:45Z
TRUE,Degradation modeling|RUL prediction|Accelerated testing|Other,Nonparametric/Semi-parametric|Bayesian|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Not applicable,Transportation/logistics|Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a degradation-signal modeling framework for predicting lifetime/residual life distributions of fielded components operating under different (time-invariant) environmental conditions. Degradation signals are modeled nonparametrically via functional data analysis: each signal is represented using spline basis functions with unit-specific coefficients, and environmental effects are captured through a mixture model over discrete environment types (clusters) with environment-specific coefficient distributions and noise variances. The authors treat environment labels as either known (supervised classification for fielded units) or unknown (unsupervised clustering), and estimate parameters via an EM algorithm with covariance regularization (regularized discriminant analysis shrinkage). Residual life prediction is performed by computing the posterior of basis coefficients given partial observations and then using a parametric bootstrap to generate the residual life distribution implied by the failure threshold crossing. Performance is demonstrated on simulated truncated/sparse signals and a rotating machinery bearing vibration dataset, showing classification/clustering variants outperform a no-environment (single-population) benchmark, especially earlier in life and under sparsity.","Failure time is defined by threshold crossing: $T=\inf\{t: S(t)\ge D\}$. The observed degradation signal is modeled as $S_l(t)=B(t)\gamma_l+\varepsilon_l(t)$ with spline basis vector $B(t)$, random coefficients $\gamma_l$, and Gaussian measurement error. Environmental type $Z_l\in\{1,\dots,K\}$ follows a multinomial with probabilities $\pi$, and conditionally $\gamma_l\mid Z_l=k\sim\mathcal N(\mu_k,\Lambda_k)$ and $\varepsilon\mid Z_l=k\sim\mathcal N(0,\sigma_k^2)$. For a fielded unit, posterior environment probabilities use a Gaussian mixture form: $P(Z^*=k\mid S^*)\propto f(S^*\mid Z^*=k)\pi_k$ (equation (4)), and the residual life distribution is approximated via parametric bootstrap by sampling $\gamma\mid S^*$, generating $S_b(t)=B(t)\gamma$, and computing $RL_b=\inf\{t:S_b(t)>D\}-t^*$.","In simulation with two environment clusters (signals truncated at $D=1000$), the proposed clustering method perfectly recovered training cluster memberships (Rand index = 1) under both complete and sparse observation scenarios. Residual-life point prediction accuracy (mean squared prediction error) was consistently better for the environment-aware methods (“classification” or “clustering”) than the “no clustering” benchmark; e.g., for complete signals at the 10% life percentile MSE was 3.24 vs 4.58, and for sparse signals 6.18–6.41 vs 7.52. The gap between methods is largest at early life percentiles (when prior information dominates) and narrows near end-of-life. In the bearing case study (two rotational speeds: 2200 and 2600 rpm; failure threshold $D=0.02$ v.r.m.s.), classification and clustering variants produced similar error distributions, and prediction error decreased as more of the bearing life was observed (10%→90% percentiles).","The authors note reliance on several assumptions: (i) the underlying degradation process is smooth, which is implicit in using cubic B-splines and may not hold in all applications; (ii) degradation signals are modeled with Gaussian assumptions (Gaussian process errors/coefficients), and while sensitivity analysis suggests trend estimation matters more, departures from Gaussianity can affect results; (iii) environmental conditions are assumed to fall into a discrete number of groups and (iv) are constant over time, which may not be realistic in some real-world settings.","The framework assumes time-invariant environment labels for each unit; if environments vary within a unit’s life (regime switching), the model may misattribute changes in slope/variance to unit effects rather than environment changes. Prediction uses a parametric bootstrap based on $S_b(t)=B(t)\gamma$ (i.e., without explicitly simulating measurement noise), which may understate predictive uncertainty for future observations depending on how residual life is operationalized. The method requires selecting tuning parameters (basis dimension, number of clusters, shrinkage) via cross-validation, which can be computationally heavy and may be unstable with small historical datasets or highly truncated signals; robustness to misspecifying $K$ or basis choice is only partially explored. Comparisons are mainly against a “no clustering” variant rather than against other established prognostics/RUL approaches (e.g., gamma/Wiener process ALT models, state-space models, modern ML prognostics), limiting evidence of broader competitiveness.","The authors explicitly point to extending the framework to (a) continuous environmental covariates instead of discrete clusters (by modeling relationships between coefficients $\gamma_l$ and environment variables, akin to ADT modeling ideas), and (b) time-varying environmental conditions where the cluster membership $Z_l$ can change over time, potentially inducing shocks and changing degradation rates.","Develop a regime-switching/hidden Markov version where $Z(t)$ evolves over time and couple it with online filtering to update RUL under changing environments. Add robust/self-starting variants that relax Gaussianity and i.i.d. error assumptions (e.g., heavy-tailed noise, autocorrelated measurement errors common in vibration signals) and provide calibrated predictive intervals. Provide open-source implementation (R/Python) and computational benchmarks for EM + cross-validation to support adoption, and extend validation to additional real datasets and head-to-head baselines (Wiener/gamma process models, deep learning prognostics) under common metrics (CRPS, calibration, timeliness).",1412.1315v1,https://arxiv.org/pdf/1412.1315v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:24:31Z
TRUE,Life distribution modeling|System reliability,Nonparametric/Semi-parametric|Bayesian|Simulation-based,Right-censored|Mixture of types,Not applicable,Manufacturing (general)|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian nonparametric (BNP) framework for system reliability that integrates component-, subsystem-, and system-level lifetime data (including right-censoring) without relying on MCMC. Each component or subsystem lifetime CDF is modeled with a Beta–Stacy process (BSP), exploiting its structural and parametric conjugacy under i.i.d. right-censored data to obtain fast posterior updates. To propagate information upward through a reliability block diagram (series/parallel structures), the authors compute the first two moments of the implied subsystem/system CDF from its children and then approximate the result by a BSP via a method-of-moments precision construction; a key contribution is a derivation of the second moment for a discrete BSP enabling this step. Performance is demonstrated via (i) a computational scaling study (showing large speedups when rounding reduces support size) and (ii) a simulation comparison against Bayesian parametric models, where the BNP method is competitive and more robust under misspecification while being far faster than MCMC. An industrial application (six components in series) illustrates using legacy-component data to form priors (via time-scaling) and quickly updating with new field-test data to obtain system reliability estimates and credible intervals.","Component/subsystem lifetimes are modeled as random CDFs: $F(t)\sim \mathrm{BSP}(\alpha(t),G(t))$ with $\mathbb{E}[F(t)]=G(t)$. Given right-censored data with risk set $M(t)=\sum I(T_i\ge t)$ and failures $J(t)=\sum C_i I(T_i=t)$, the BSP posterior remains BSP with centering measure $G^*(t)=1-\prod_{t_i\le t}\left(1-\frac{\alpha(t_i)(G(t_i)-G(t_i^-))+J(t_i)}{\alpha(t_i)(1-G(t_i^-))+M(t_i)}\right)$ and precision $\alpha^*(t)=\frac{\alpha(t)(1-G(t))+M(t)-J(t)}{1-G^*(t)}$. For series/parallel composition under independence, subsystem mean-CDFs are combined as $G_S(t)=G_1(t)G_2(t)$ (parallel) and $G_S(t)=G_1(t)+G_2(t)-G_1(t)G_2(t)$ (series), and a BSP precision for the subsystem is recovered by a recursive method-of-moments formula (their Eq. (7)) matching the second moment derived for the discrete BSP.","In the simulation study (hybrid-electric aircraft block diagram with 9 components plus subsystems/system; 30 observations per unit; ~15% right-censoring), aggregated mean absolute error (MAE) for the system CDF was 0.0311 for the proposed BNP/BSP method, 0.0179 for a correctly specified Bayesian parametric model, and 0.0391 for a mildly misspecified parametric model; average bias was 0.0021 (BNP), 0.0048 (correct parametric), and 0.0280 (misspecified). Runtime for the full BSP posterior was reported as under 2 seconds versus ~45 minutes for 10,000 MCMC draws in each parametric model. In a computational scaling study for a 3-component parallel system, no-rounding computation appeared roughly $O(N^2)$ with times growing to 28.1 hours at $N=10^6$, while rounding to 3 decimals reduced that to 5.8 hours and rounding to 2 decimals to 3.6 minutes (Table 1). In the industrial six-component series case study, the end-to-end workflow (posterior + Monte Carlo credible intervals + plots) took ~1.88 seconds and produced system lifetime posterior summaries including mean 716.7 and median 330.0 (Table 5).","The authors note the model’s primary assumption is component independence; if violated, accuracy degrades depending on the strength of dependence. They also state that extending to other censoring mechanisms (e.g., interval censoring) is nontrivial because there are no known conjugate models in that setting, implying a potentially severe computational cost.","The approach approximates the composed subsystem/system distribution by a BSP matched only on first two moments; this moment-matching may not capture higher-order features (tail behavior, multimodality) and its approximation error is not theoretically bounded. The framework assumes i.i.d. lifetimes within components and does not address covariates, time-varying operating conditions, or within-unit dependence (e.g., load sharing, common-cause failures) beyond a brief mention of copulas. Reported computational claims depend strongly on discretization/support size (and rounding choices), which can affect inference yet are not treated as a formal part of the statistical model. Code and implementation details (data structures, numerical stability, sampling-from-BSP routines) are not provided, which limits reproducibility and practical adoption.","They propose extending the framework to handle dependent components, suggesting copulas as a natural way to incorporate dependence. They also mention exploring incorporation of other reliability data types such as interval-censored data, noting this would likely sacrifice conjugacy and increase computational cost.","Develop a principled approximation analysis for the BSP moment-matching step (e.g., bounds on approximation error or alternative projections such as KL-minimizing neutral-to-the-right processes) and study sensitivity to rounding/discretization. Extend the model to handle unknown/estimated component priors in a fully hierarchical way (rather than engineering time-scaling) and to incorporate covariates or environmental/stress factors. Provide self-starting/online updating algorithms and diagnostic tools to detect violations of independence or i.i.d. assumptions. Release an open-source reference implementation (e.g., R/Python) with benchmarks and reproducible examples to facilitate uptake in reliability practice.",1412.4260v2,https://arxiv.org/pdf/1412.4260v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:25:26Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1412.6921v2,https://arxiv.org/pdf/1412.6921v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:25:27Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an information-theoretic alternative to Cronbach’s alpha for measuring internal consistency (item reliability) of questionnaire instruments, especially for strictly non-numeric/ordinal response scales (e.g., Likert items). It represents each respondent’s response pattern as an empirical probability vector over response categories and defines an Information Consistency Ratio (ICR) based on normalized entropy (with variants using min/max respondent entropy and an alternative normalization using the entropy of the distribution of respondents’ modal responses). The authors analyze limiting behavior: the index tends to 0 for instruments with mutually independent items as the number of items grows, and equals 1 for perfectly identical items. A simulation study (n=1000, p=50) compares multiple ICR variants against Cronbach’s alpha across increasing fractions of perfectly identical items, showing ICR can be more stringent than alpha. The work is focused on psychometric/measurement reliability rather than engineering reliability (failure/degradation/maintenance).","Cronbach’s alpha is given as $\alpha=\frac{p}{p-1}\left(1-\frac\sum_{j=1}^p \mathrm{Var}(X_j)}{\mathrm{Var}(\sum_{\ell=1}^p X_\ell)}\right)$. The proposed Information Consistency Ratio is $\phi = 1- \frac{\min_i H(\hat z_i)}{\log_2 K}$ where $\hat z_{ik}=\frac{1}{p}\sum_{j=1}^p \mathbf{1}(x_{ij}=k)$ and $H(\hat z_i)=-\sum_{k=1}^K \hat z_{ik}\log_2 \hat z_{ik}$; alternative normalizations use $H(\hat w)$ with $\hat w_k=\frac1n\sum_i \mathbf{1}(Y_i=k)$ and $Y_i$ the respondent’s modal category, and a more stringent variant replaces $\min_i$ by $\max_i$.","In simulation with $p=50$ items and $n=1000$ respondents, the authors vary the fraction of perfectly identical (constant) items from 10% to 100%. Reported Cronbach’s alpha increases from 0.38 (10%) to 0.70 (20%) to 0.82 (30%) to 0.94 (50%) to 1.00 (90–100%). One ICR variant (\u03d5\u2081/\u03d5\u2083) increases from 0.23 (10%) to 0.52 (50%) to 0.90 (80%) to 1.00 (90–100%), while a stricter variant (\u03d5\u2082/\u03d5\u2084) stays at 0.00 through 20% and rises to 0.14 (50%), 0.52 (80%), 1.00 (100%). The figure and table indicate Cronbach’s alpha is less stringent (higher) than the information consistency ratio for partially reliable instruments.",None stated.,"The proposed index uses the minimum (or maximum) respondent entropy, which can be highly sensitive to outliers and sample size (e.g., one highly consistent respondent can inflate \u03d5 when using $\min_i$). The simulation setup replaces columns with constant values (perfectly identical items), which is a narrow notion of “reliability” and may not reflect realistic psychometric structures (e.g., multiple correlated factors, graded item difficulties, or response biases). No real questionnaire datasets are analyzed, and there is limited discussion of statistical uncertainty (e.g., variance/CI) for \u03d5 or how to calibrate it for finite p,n.","The authors state they intend to use the other probability-vector similarity measures (e.g., VI/KL/Bhattacharyya/Hellinger) to further refine the proposed information consistency ratio. They also plan to conduct a larger simulation study to place the measure on a stronger footing and to compare the predictive power of ICR to Cronbach’s alpha on various real and simulated datasets.","Develop robust/self-starting versions of \u03d5 that are less sensitive to extreme respondents (e.g., using quantiles or trimmed minima of $H(\hat z_i)$) and provide inferential procedures (standard errors/bootstrapped CIs) for finite samples. Extend evaluation to realistic psychometric data-generating models (IRT, multidimensional factor models, missing data) and benchmark against additional ordinal reliability measures (e.g., ordinal alpha/polychoric-based coefficients, McDonald’s omega). Provide open-source implementations (R/Python) to facilitate adoption and reproducibility.",1501.04070v1,https://arxiv.org/pdf/1501.04070v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:26:01Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)",Complete lifetime data|Right-censored,Not applicable,Theoretical/simulation only,Simulation study,TRUE,R,Not provided,NA,"The paper studies reliability inference when lifetimes are recorded on a discrete scale and modeled by the two-parameter geometric distribution Geo(r,θ). It derives maximum likelihood estimators (MLEs) and Rao–Blackwellized unbiased estimators for the component survival/reliability function R(t)=P(X≥t), for k-out-of-m system reliability Rs(t), and for stress–strength reliability R=P(X≤Y) when X and Y are independent two-parameter geometric variables. MLEs are provided for both complete samples and Type-I right-censored samples (censoring at a fixed cycle count c). Because the sufficient statistic (X(1),S) is not complete in this model, UMVUEs are generally not obtainable via Lehmann–Scheffé; instead the authors construct improved unbiased estimators via conditioning. Monte Carlo simulations (implemented in R) compare MLEs and unbiased estimators in terms of MSE/efficiency and show MLEs typically have smaller MSE, while the unbiased estimators can have better coverage in some reliability ranges.","Two-parameter geometric pmf: P(X=x)=(1-\theta)\,\theta^{x-r},\ x=r,r+1,\ldots. Component reliability (survival): R(t)=P(X\ge t)=\theta^{t-r}. For a k-out-of-m system with iid components, Rs(t)=\sum_{i=k}^{m}\binom{m}{i}R(t)^i[1-R(t)]^{m-i}. With sample X_1,\ldots,X_n, MLEs are \hat r=X_{(1)} and \hat\theta=S/(n+S) where S=\sum_{i=1}^n(X_i-X_{(1)}), giving \hat R_M(t)=1\ (t\le X_{(1)}) and (S/(n+S))^{t-X_{(1)}}\ (t>X_{(1)}); analogous plug-in yields \hat R_{sM}(t). Stress–strength reliability for X\sim Geo(r_1,\theta_1), Y\sim Geo(r_2,\theta_2): R=P(X\le Y)=\rho\,\theta_2^{\delta} (\delta>0) or 1-(1-\rho)\theta_1^{-\delta} (\delta<0), with \rho=(1-\theta_1)/(1-\theta_1\theta_2), \delta=r_1-r_2; MLE obtained by plug-in estimates of \rho,\delta,\theta_1,\theta_2.","Across extensive simulations (often 10,000 replications for R(t) and Rs(t), and 1,000–10,000 for stress–strength R), the MLEs generally show lower MSE than the constructed unbiased estimators for component reliability, system reliability, and stress–strength reliability. For Type-I censoring, the MLE under censoring approaches the complete-sample MLE as the censoring time (cycle limit) c increases, with relative efficiency improving toward 100%. For confidence interval coverage in one setup (n=20,r=15,t=25), the unbiased estimator and MLE both show near-nominal coverage for many \theta values, with the paper noting MLE can be better for moderate reliabilities (roughly 0.02<R(t)<0.5) while the unbiased estimator can be better outside that range. The authors also empirically observe (via simulation) that the Rao–Blackwellized unbiased estimator of R(t) is not UMVUE due to lack of completeness and nonzero covariance with certain unbiased estimators of zero.","Because the sufficient statistic (X(1),S) is not complete for the two-parameter geometric model, the authors state they are “handicapped” in obtaining UMVUEs via the Lehmann–Scheffé theorem and therefore focus on unbiased estimators via Rao–Blackwell improvement. They also note that verifying UMVUE status analytically (via uncorrelatedness with all unbiased estimators of zero) is intractable and they resort to simulation for particular choices.","The work is restricted to the two-parameter geometric distribution and independence assumptions; robustness to model misspecification (e.g., overdispersion, hurdle/zero-inflation, dependence, or other discrete lifetime laws) is not studied. The censoring scheme is limited to Type-I right-censoring at a fixed cycle count; other common reliability censoring/truncation schemes (Type-II, progressive censoring, left truncation) are not covered. Practical guidance on selecting between MLE and unbiased estimators is based mainly on simulated scenarios and may not generalize without broader benchmarking; no real industrial dataset is analyzed. Interval estimation is reported in simulations but a general, fully specified inferential procedure (e.g., analytic CI construction or likelihood-based intervals for all settings) is not systematically developed.",None stated.,"Extend inference to additional censoring/truncation mechanisms used in life testing (Type-II/progressive censoring and left truncation) and to dependent stress–strength settings. Develop likelihood-based or Bayesian interval estimation for R(t), Rs(t), and P(X\le Y) with unknown parameters, including small-sample calibrated confidence/credible intervals. Study robustness and model selection among competing discrete lifetime models (e.g., discrete Weibull, negative binomial, Poisson mixtures) and assess performance on real cycle-to-failure datasets. Provide reproducible software (e.g., an R package) implementing the estimators and interval procedures for practitioners.",1501.05072v1,https://arxiv.org/pdf/1501.05072v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:26:41Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Simulated only|Other,Not applicable,Environmental monitoring|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,http://wwwc.bom.gov.au/tas/|http://stsda.kaust.edu.sa/Documents/2015.GH.JRSSB.pdf,"The paper proposes discriminative composite likelihood estimation (D-McLE), which chooses data-adaptive weights for sub-likelihood components by maximizing the composite log-likelihood subject to a fixed Kullback–Leibler divergence 
 distance 
 from uniform weights. The resulting weights have an exponential-tilting form and down-weight sub-models that are incompatible with the majority of the data subsets, aiming to improve estimator stability and bound worst-case bias under misspecification. The authors derive consistency and asymptotic normality for the estimator, highlighting how random, 
 parameter-dependent weights affect the sandwich variance structure. Performance is assessed via Monte Carlo simulations for correlation and heterogeneous-normal-location examples, and via a real spatial extremes application (Tasmanian rainfall maxima) using pairwise likelihood for a Gaussian max-stable model. The fitted weights are also used diagnostically through compatibility profile plots (CPPs) to identify influential or incompatible sub-likelihood components.","The weighted composite likelihood is $CL(\theta|w,x)=\prod_{j=1}^m f_j(y_j|\theta)^{w_j}$, with weights chosen by solving $\max_w\,\ell_n(\theta|w)$ subject to $D_{KL}(w,w_{unif})=\xi$ and $\sum_j w_j=1$, where $D_{KL}(w,w_{unif})=\sum_j w_j\log(mw_j)$. The 
 optimal weights have exponential-tilting form $w_{nj}(\theta)=\alpha_2\exp\{\alpha_1\ell_{nj}(\theta)\}$, with $\alpha_1$ determined by the KL constraint, and the estimator solves the weighted score equation $u_n(\theta)=\sum_j w_{nj}(\theta)u_{nj}(\theta)=0$.","In the misspecified correlation example (multivariate normal with a subset of pair correlations perturbed), the D-McLE with small positive \
 $\xi$ sharply reduces bias relative to the MLE and the uniform-weight McLE; for example with $\varepsilon=3$ and $n=50$, the reported squared-bias (\
 $\times100$) drops from 1.75 (uniform weights) to near 0.00 at $\xi=0.2$. For stronger misspecification ($\varepsilon=5$), squared-bias (\
 $\times100$) drops from 4.53 (uniform) to near 0.00 at $\xi=0.2$. In the heterogeneous-normal location example with two misspecified components, the D-McLE reduces the very large bias from the uniform-weight composite estimator and achieves low variance for \ 
 $0<\xi\le 0.3$ in the reported settings. In the Tasmanian rainfall max-stable application, CPPs indicate low compatibility for pairs involving King Island, and CLIC decreases from 156.6 at $\xi=0$ to 155.9 at $\xi=0.3$, suggesting preference for $\xi>0$.","The paper does not provide general theoretical convergence guarantees for the proposed iterative algorithm, noting only empirically fast convergence and suggesting that results could be derived via 
 analogies to existing reweighted procedures. It also notes that alternative divergences/penalties could be used (e.g., to promote sparsity when $m$ is very large) but are not pursued. The authors acknowledge limited existing theory for the large-$m$ behavior of composite likelihood estimators and position this as outside their current theoretical 
 development.","The method’s behavior depends on the choice of tuning parameter $\xi$, and while a heuristic stability-based selection rule is proposed, there is no formal optimality criterion (e.g., minimizing MSE) or theoretical guarantee of selection performance. Asymptotic variance estimation may be sensitive because the weights are random and depend on estimated quantities; the paper suggests sandwich/jackknife/bootstrap but does not systematically evaluate finite-sample coverage. The approach presumes a meaningful “majority” of compatible sub-models; if incompatibility is diffuse or if sub-models are strongly dependent, the weight-tilting may not cleanly separate misspecified components. No software implementation details (language, computational complexity benchmarks) are given, which can hinder reproducibility for practitioners.","The authors suggest developing new sparsity-inducing penalty schemes for likelihood selection within their regularization framework, particularly for high-dimensional settings with many sub-likelihood components. They also propose further theoretical study of composite likelihood estimators when the number of sub-likelihoods $m$ grows large, including understanding the interplay between the regularization constraint and the estimator’s mean squared error as $m$ increases.","Develop principled, data-driven selection of $\xi$ (e.g., cross-validation, information-criteria calibrated specifically for tilted weights, or risk minimization) and study its statistical properties. Extend the approach to settings with temporal/spatial dependence within observations and to composite likelihoods built from dependent data blocks where standard CLT assumptions are strained. Provide robust/diagnostic variants that explicitly account for sub-model dependence (e.g., clustering sub-likelihoods, group-tilting) and evaluate coverage of confidence intervals under misspecification. Release a reference implementation (e.g., R/Python) and benchmark runtime/scalability for very large $m$ to support adoption.",1502.04765v2,https://arxiv.org/pdf/1502.04765v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:27:23Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,None / Not applicable,Not provided,NA,"This paper is about psychometric (Classical Test Theory) test-score reliability, not reliability engineering of physical systems. It proposes an algorithm to split a multiple-choice dichotomous test into two near-parallel halves using item total scores (difficulty), enabling estimation of the classically defined test reliability (true-score variance / observed-score variance) from a single test administration. The method derives formulas for error variance and reliability based on the geometry of score vectors and properties of parallel tests, and provides interval estimation of an examinee’s true score via linear regression using the estimated reliability. The approach is empirically illustrated on simulated item-response data and real exam datasets, and extended to composite (battery) reliability including weighted batteries with weights chosen by minimizing composite-score variance under a sum-to-one constraint. Performance evidence is presented via reported reliabilities, correlations between split halves, and runtime observations for very large examinee cohorts.","The classically defined reliability is $r_{tt}=S_T^2/S_X^2=1-(S_e^{(test)})^2/S_X^2$. For two (near) parallel halves with score vectors $X_g, X_h$, the paper estimates error variance via $ (S_e^{(test)})^2=\{\|X_g\|^2+\|X_h\|^2-2\sum_{i=1}^N X_i^{(g)}X_i^{(h)}\}/N$ and then computes reliability as $r_{tt}=1-\{\|X_g\|^2+\|X_h\|^2-2\sum_i X_i^{(g)}X_i^{(h)}\}/(N S_X^2)$ (or the simplified form when $\|X_g\|=\|X_h\|$). For a battery score $Y=\sum_{i=1}^K W_i X_i$, battery reliability is given as a ratio of weighted true-score variance to weighted observed variance (Equation 5.7), and weights are chosen as $W=D^{-1}e/(e^T D^{-1}e)$ where $D$ is the observed variance–covariance matrix.","On four simulated 50-item tests with $N=999$, reported reliabilities were: $D1\approx0.9662$, $D2\approx0.0205$, $D3\approx0.8994$, $D4\approx0.0361$, matching expectations for realistic vs unrealistic generating mechanisms. For a real 50-item test with $N=912$ (DATA-I), the classically defined reliability is reported as $r_{tt}\approx0.66$, while the split-half correlation is $r_{gh}\approx0.9941$. For a real two-test battery (DATA-II(a), DATA-II(b)), the constituent reliabilities are reported as about $0.66$ and $0.35$, computed weights about $0.7028$ and $0.2972$, and the resulting battery reliability about $0.7112$. The paper also reports runtime examples (e.g., ~0.8s for 50k examinees and ~6.2s for 500k examinees on a 50-item test) and notes quadratic scaling in the number of items for the splitting algorithm.","The authors note that their simulation models are restrictive (e.g., in some simulations item-to-item variation is ignored). They also acknowledge that their chosen true-score interval (±1 standard deviation of the modeled error distribution) is a model choice and is pessimistic/overestimates uncertainty relative to the standard error from the regression-based prediction error.","The work is not in reliability engineering; its ‘reliability’ concerns educational/psychometric measurement, so it does not generalize to failure-time, degradation, or maintenance contexts. The method relies on Classical Test Theory assumptions (e.g., parallel halves, uncorrelated errors, additive decomposition of observed score into true + error, and normal error modeling for intervals), which may be violated in real testing (item dependence, multidimensionality, speededness, guessing). The “near-parallel” construction is heuristic and may not guarantee true parallelism; reliability estimates may be sensitive to the splitting algorithm, item ordering/ties, and the stopping criterion for minimizing the sum-difference. Empirical evaluation largely reports point results and correlations without comprehensive uncertainty quantification (e.g., confidence intervals for $r_{tt}$) or broad benchmarking against established estimators under varied violations (nonparallel forms, heteroscedastic errors).","The paper suggests the methodology can be extended from single tests to batteries of tests, including weighted batteries, and discusses alternative constraints (e.g., nonnegative weights via quadratic programming) when variance-minimizing weights can become negative. It also mentions using the computed reliability to support comparisons between tests via hypothesis testing of equality of error variances (e.g., an F-test framework).","For broader adoption, it would be useful to develop formal statistical properties of the proposed estimator (consistency/bias under CTT/IRT models) and provide confidence intervals via bootstrap or asymptotic theory. Robust extensions could address nonparallel halves, correlated errors, multidimensional constructs, and polytomous (non-binary) item scoring, and compare performance systematically against Cronbach’s alpha, McDonald’s omega, and IRT-based reliability across realistic data-generating processes. Providing an openly available software implementation (e.g., R/Python) with clear stopping rules and tie-handling would improve reproducibility. Further validation on diverse real datasets (different test lengths, distributions, and item difficulties) and sensitivity analyses to algorithmic choices would clarify practical reliability and failure modes.",1503.03297v2,https://arxiv.org/pdf/1503.03297v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:28:05Z
TRUE,Degradation modeling|Maintenance optimization|System reliability|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Other,Condition-based|Not applicable,Energy/utilities|Theoretical/simulation only|Other,Exact distribution theory|Approximation methods|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper models inspection-time planning for complex aging systems by representing degradation evolution as a directed acyclic “degradation graph,” where edge weights are random transition times between degradation states. It reduces the expected acceptable inspection time to the expected shortest-path length from “new” (node 1) to “unacceptable degradation” (node n), and then further to the expected optimal value of a linear program with random nonnegative costs. Because the expectation is difficult to compute exactly (and may be impractical to estimate by Monte Carlo for large graphs), it derives an easily computable lower bound on the expected optimal LP value, inspired by (but complementary to) the Dyer–Frieze–McDiarmid (DFM) bound. The work specializes the bound to shortest-path LP formulations and discusses that the DFM upper bound can be trivial/bad for shortest paths under exponential costs. In the reliability application, it shows Weibull transition times (with shape parameter in roughly the reliability-relevant range) satisfy the condition needed for the proposed lower-bound technique and provides an explicit bound for the Weibull case.","Degradation/inspection time is modeled as the shortest-path length in a DAG with random edge transition times, formulated as an LP: minimize $c^T x$ subject to $Ax=b$, $x\ge 0$, where $A$ is an (extended) incidence matrix for the degradation graph and $b=[-1,0,\dots,0,1]^T$. A key assumption for the lower bound is a mean residual life-type inequality for costs: $\mathbb{E}[c_i\mid c_i\ge h]\le \mathbb{E}[c_i]+\beta h$ (for Weibull with certain shapes, $\beta=1$). The derived lower bound has the form $\mathbb{E}[z]\ge \frac{1}{\beta}\sum_{B\in\mathcal{B}} p_B\, \mathbb{E}[c_B]^T x_B$, with $p_B$ expressed/approximated via conditional tail probabilities $p_B=\mathbb{E}[\prod_{i\in B^c} \mathbb{P}(c_i\ge \sum_{j\in B} \alpha_{ji}c_j\mid c_B)]$ for independent costs (and further bounded explicitly for Weibull).","For Weibull transition times, the paper derives the mean residual time to failure (MRTF) expression $G_X(h)=\mathbb{E}[X\mid X\ge h]=\eta e^{(h/\eta)^\gamma}\,\Gamma(1+1/\gamma,(h/\eta)^\gamma)$ and analyzes its derivatives to establish required inequalities. It proves that for Weibull $\text{Weib}(\eta,\gamma)$ with shape parameter in a stated range (notably $(1,2)$ in the reliability section), the key condition $\mathbb{E}[X\mid X\ge h]\le \mathbb{E}[X]+h$ holds for all $h\ge 0$, enabling the lower-bound framework with $\beta=1$. It shows that for shortest paths with independent exponential costs, the DFM upper bound equals the trivial bound obtained by replacing random costs with their expectations (i.e., it can be “as bad as possible” for this problem class). It provides an explicit computable Weibull-based lower bound on $p_B$ (hence on $\mathbb{E}[z]$) in terms of basis-dependent coefficients $\alpha_{ji}$ and Weibull moments involving gamma functions (e.g., $\Gamma(1+1/\gamma_j)$ and $\Gamma(1+2/\gamma_j)$).","The authors state the results are mainly theoretical and indicate that refinements and real-data applications will be developed in a subsequent paper. They also motivate the work by noting that closed-form expectations and even Monte Carlo estimation can be out of reach for large systems, which is why they focus on lower bounds rather than exact computation.","The degradation-evolution rule assumes the system always moves from a node to the outgoing neighbor with the minimum realized transition time, which may not match physical degradation dynamics (often semi-Markov with competing risks, dependence, and constraints). Independence of edge transition times and a fixed DAG structure may be unrealistic in practice (shared stressors, common-cause effects, and feedback can induce dependence and cycles). The bound depends on LP-basis quantities ($\alpha_{ji}$) and on (potentially many) bases; for large graphs, computing or selecting a useful subset of bases and ensuring the bound is numerically tight may still be challenging. The method is developed for nonnegative costs and specific Weibull-shape regimes; robustness to misspecification, censoring, and parameter-estimation uncertainty (beyond brief mention of Bayesian ideas) is not analyzed.",They explicitly state that further refinements and applications to real data will be proposed in a subsequent paper.,"A natural extension would incorporate dependence between transition times (shared frailty/common-cause or copula models) and assess how the lower bound changes or can be made robust. Another direction is to integrate parameter-estimation uncertainty (including censored degradation/transition data) directly into the bound, yielding Bayesian posterior bounds on expected inspection time. Empirical validation on real multi-state assets (e.g., nuclear plant subsystems or industrial fleets) and benchmarking versus simulation-based estimators would clarify practical tightness. Extending from a shortest-path (single trajectory) abstraction to richer multi-state reliability models (semi-Markov processes, competing risks at each node, imperfect maintenance effects) would improve realism for inspection/maintenance optimization.",1504.00865v2,https://arxiv.org/pdf/1504.00865v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:28:34Z
FALSE,NA,Bayesian|Other,Simulated only|Other,Not applicable,Semiconductor/electronics|Network/cybersecurity|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,http://cvxr.com/cvx,"This paper proposes a pilotless method to mitigate nonlinear distortion (clipping) in OFDM by exploiting the time-domain sparsity of the clipping signal and recovering it via compressed sensing and sparse Bayesian recovery. The key idea is to select a subset of data carriers as measurements based on a newly derived per-tone reliability metric that depends on the magnitude and phase of the perturbation and the channel gain, avoiding any dedicated reserved tones or pilots. The authors derive a truncated Bayesian reliability function and analyze its behavior, including a closed-form transition point (using the Lambert W function) separating regimes where circular vs. square-like reliability approximations are appropriate. A dual-stage subset selection is introduced: first minimize the probability of incorrect measurements (reliability-based), then refine using a clipping-to-noise ratio (CNR) criterion to improve recovery. Simulations on 256-subcarrier, 64-QAM OFDM over Rayleigh channels show improved clipping mitigation and achievable rate compared to prior CS-based approaches, with runtime comparisons for WPAL and phase-augmented FBMP variants.","The OFDM clipping model is $\bar{x}(i)=\gamma e^{j\theta_{x(i)}}$ if $|x(i)|>\gamma$ else $x(i)$, equivalently $\bar{x}=x+c$ with sparse $c$ in time and dense $C=Fc$ in frequency. After equalization, $\hat{\mathbf{X}}=\mathbf{X}+\mathbf{C}+\Lambda^{-1}\mathbf{Z}=\mathbf{X}+\mathbf{D}$, and a pilotless CS measurement model is formed from reliable tones: $\mathbf{Y}'_{\Omega_m}=\Psi_{\Omega_m}\mathbf{c}+\mathbf{Z}'_{\Omega_m}$ where $\mathbf{Y}'=S_{\Omega_m}(\hat{\mathbf{X}}-\langle\hat{\mathbf{X}}\rangle)$ and $\Psi=S_{\Omega_m}F$. Reliability-based tone selection uses a (truncated) likelihood ratio $R_{\text{trunc}}(r,\theta)=\left[\beta(\alpha^{\cos\theta}+\alpha^{\sin\theta}+\beta\alpha^{\cos\theta+\sin\theta})\right]^{-1}$ with $\alpha=e^{2d_{\min}r/\sigma_D^2}$, $\beta=e^{-d_{\min}^2/\sigma_D^2}$, and the convex/concave transition magnitude is $\tilde r=-\frac{\sqrt{2}\sigma_D^2}{2d_{\min}}\left(W_0(-e^{1-d_{\min}^2/\sigma_D^2})-1\right)\approx \frac{\sqrt{2}}{2}\frac{\sigma_D^2}{d_{\min}}$.","Simulations use 256 subcarriers with 64-QAM under block-fading frequency-selective Rayleigh channels, evaluating performance versus clipping ratio (CR) and $E_b/N_0$. For reliability criteria comparison at $E_b/N_0=20$ dB with $|\Omega_m|=64$, the truncated reliability $R_{\text{trunc}}$ performs close to the exact Bayesian reliability $R_{\text{exact}}$ in normalized success rate (NSR), indicating little loss from truncation. In achievable-rate experiments, corrective second-stage methods (C-WPAL and C-PAFBMP) using larger reliable subsets (reported as about 39% of carriers in the figure caption) improve rate relative to single-stage counterparts; C-PAFBMP is shown as strongest overall, and C-WPAL is best around CR \approx 1.5 in the plotted results. Runtime comparisons indicate PAFBMP has the lowest relative computational cost and its runtime decreases as CR increases, while WPAL/C-WPAL are more expensive in the reported scaling.",None stated.,"The approach assumes the channel frequency response is known at the receiver for equalization (the paper calls this a “practical assumption”), so performance under channel-estimation error is not fully characterized. The distortion-plus-noise term $D$ is modeled as circularly symmetric Gaussian with variance $\sigma_D^2$, which may be mismatched for clipping distortion (non-Gaussian, signal-dependent) and could affect reliability scoring and subset selection. Results are primarily simulation-based on a specific OFDM/QAM setup (e.g., N=256, 64-QAM, Rayleigh fading), with limited evidence of robustness across modulation orders, coding, PA models beyond hard clipping, or real measured RF impairments.",None stated.,"Extend the framework to explicitly handle unknown/estimated channels (joint channel + clipping estimation) and quantify robustness to estimation errors and phase noise. Develop reliability/tone-selection rules that are robust to non-Gaussian, signal-dependent clipping statistics or that learn $f_D$ online. Provide reproducible implementations (e.g., MATLAB/Python code) and validate on hardware or over-the-air datasets with practical PA nonlinearity models and coded OFDM systems.",1506.09060v1,https://arxiv.org/pdf/1506.09060v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:29:16Z
TRUE,Network/infrastructure reliability|System reliability|Life distribution modeling|Other,"Stochastic process|Parametric (Weibull, etc.)|Other",Simulated only,Not applicable,Network/cybersecurity|Transportation/logistics|Energy/utilities|Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops a shock-model framework for two-state network reliability where shocks arrive according to a counting process and each shock can cause multiple component failures (ties). To accommodate simultaneous component failures, it introduces the t-signature (tie-signature), a structural probability vector for the minimum number of component failures required to bring the network down, and derives mixture representations for the network survival function in terms of the shock counting process and the t-signature. It further defines a shock t-signature (ST-signature), giving the distribution of which shock epoch causes network failure, enabling survival representations as mixtures over shock arrival times. Under a binomial damage mechanism (each component fails with probability p at each shock, conditional on remaining components) and shocks following a nonhomogeneous Poisson process (NHPP), the paper derives explicit reliability expressions and studies aging properties, including conditions under which the network lifetime is IHRA. A separate “fatal shock” model (each shock kills at least one component) yields analogous mixture representations based on a structural vector s* for failure-at-shock-index probabilities, and the stochastic comparison results extend to this case.","The network fails when the cumulative number of failed components by time t, $N(t)=\sum_{i=0}^{\xi(t)} W_i$, reaches the random threshold $M$ (minimum number of component failures that cause network failure). With t-signature $\mathbf{s}^\tau$ where $s_i^\tau=P(M=i)$, the survival is $P(T>t)=\sum_{i=1}^n s_i^\tau\,P(N(t)\le i-1)=\sum_{k\ge0}\beta_{k,n}P(\xi(t)=k)$, where $\beta_{k,n}=\sum_{i=1}^n s_i^\tau H_k(i-1)$ and $H_k$ is the CDF of $\sum_{i=0}^k W_i$. Using shock epochs $\vartheta_k$, another representation is $P(T>t)=\sum_{k\ge1} b_{k,n}P(\vartheta_k>t)$ with $b_{k,n}=P(T=\vartheta_k)=\beta_{k-1,n}-\beta_{k,n}$ (ST-signature). In the binomial damage model, $P(\sum_{i=1}^k W_i=j)=\binom{n}{j}(1-q^k)^j q^{k(n-j)}$ (with $q=1-p$), giving $\beta^*_{k,n}=\sum_{j=0}^{n-1}\bar S_j^\tau\binom{n}{j}(1-q^k)^j q^{k(n-j)}$; for fatal shocks, $P(T>t)=\sum_{i=1}^n s_i^* P(\varrho_i>t)$.","The paper proves that $\beta_{k,n}=P(T>\vartheta_k)$ is a discrete survival function in k, and the induced ST-signature $b_{k,n}=P(T=\vartheta_k)$ gives a mixture representation of network survival over shock arrival times. Under the binomial-per-shock failure mechanism, it derives a closed form for the distribution of the cumulative number of failed components after k shocks: $P(\sum_{i=1}^k W_i=j)=\binom{n}{j}(1-q^k)^j q^{k(n-j)}$, yielding explicit expressions for $\beta^*_{k,n}$ and hence $P(T>t)$. For a series network under NHPP shocks, it obtains $\beta^*_{k,n}=q^{kn}$ and $P(T>t)=(\bar G(t))^{1-q^n}$ where $\bar G$ is the survival of time-to-first-shock. It shows $\beta^*_{k,n}$ is always IHRA (in k), but provides a bridge-network example where $\beta^*_{k,n}$ is not IHR. Finally, it proves that if shocks follow an NHPP and the time-to-first-shock distribution is IHRA, then the network lifetime T is IHRA, and establishes stochastic/hazard-rate ordering results comparing two networks via their (t-/ST-) signatures, shock processes, and per-shock failure probabilities.",None stated.,"The framework largely assumes exchangeability/structural symmetry (e.g., “all orders of component failures are equally likely”), which may not hold in real networks with heterogeneous components, dependencies, or location-specific shock impacts. The binomial per-shock damage model assumes conditional independence and a common failure probability p for all surviving components at a shock, which can be unrealistic for spatially correlated or component-specific vulnerability. Results are primarily theoretical with illustrative examples; there is no empirical calibration to real shock data, no sensitivity analysis for model misspecification (e.g., overdispersion vs. binomial), and no implementation guidance for computing t-signatures/ST-signatures for large networks where enumeration is infeasible.",None stated.,"Extend the model to heterogeneous components (component-specific p, covariate-dependent shock severity) and dependent failures (common-cause/spatial correlation) to better match real network shock behavior. Develop scalable algorithms to compute t-signatures and ST-signatures for large networks (e.g., using Monte Carlo, recursive decomposition, BDDs, or minimal cut/path set methods). Incorporate parameter estimation and uncertainty quantification from observed shock/failure data (Bayesian or likelihood-based), and validate on real infrastructure datasets. Generalize beyond two-state networks to multi-state performance and to maintenance/recovery processes where shocks can cause repairable damage rather than absorbing failure.",1507.04143v1,https://arxiv.org/pdf/1507.04143v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:30:01Z
TRUE,System reliability|Other,"Parametric (Weibull, etc.)|Stochastic process|Other",Simulated only,Not applicable,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper derives reliability (survival) functions for a binary coherent system equipped with a single general standby component that can transition through cold, warm, and active states. The standby is assumed to remain cold initially and switch to warm after a fixed time $u$ (during which the system is known not to fail), then switch to active at the time of the $s$-th component failure that would otherwise cause system failure; three switching regimes are treated: perfect switching, imperfect switching (standby may fail at switch-over with probability $1-p$), and random warm-up time (non-instantaneous cold-to-warm transition with warm-up distribution $H$ on $[u_1,u]$). The main method expresses system survival via the system signature $\mathbf{p}=(p_1,\dots,p_n)$ and integrates conditional survival of the post-failure residual structure using order statistics and the general standby model of Cha et al. (virtual age $\omega$ and acceleration function $\gamma$). Closed-form integral expressions are obtained for general coherent systems and specialized to $k$-out-of-$n$ systems, showing known cold-standby and warm-standby results as special cases (e.g., by setting $u=0$ or $\gamma(t)=\omega(t)=0$). A numerical illustration plots reliability with/without standby for an example coherent system under exponential lifetimes and specific $\gamma,\omega$ choices, demonstrating the reliability gain and the effect of $u$.","The general-standby survival for a single active component is given by $R(t)=\bar F(t)+\int_0^t \frac{\bar G(\omega(x)+t-x)}{\bar G(\omega(x))\,\bar G(\gamma(x))}\,dF(x)$, and with an initial cold period $u$ by $R(t)=\bar F(t)+\int_u^t \frac{\bar G(\omega(x-u)+t-x)}{\bar G(\omega(x-u))\,\bar G(\gamma(x-u))}\,dF(x)$ for $t>u$. For a coherent system with signature $p_s=P\{T=X_{s:n}\}$, the perfect-switching reliability is $P\{T_{gs}>t\}=\sum_{s=k_\phi}^{z_\phi+1} p_s\Big(P\{X_{s:n}>t\}+\sum_{c=1}^nP\{V_s=c\}\sum_{b_1<\cdots<b_{s-1}}P\{B_{s,c}=(b_1,\dots,b_{s-1})\}\int_u^t P\{\phi_s(\cdot)>t-x\mid X_{s:n}=x\}\,dF_{s:n}(x)\Big)$. In the imperfect switching case, the integral term is multiplied by the perfect-switch probability $p\in[0,1]$; with random warm-up, an additional averaging $\int_{u_1}^u (\cdot)\,dH(r)$ is introduced.","The paper provides explicit integral expressions for $P\{T_{gs}>t\}$ for (i) perfect switching, (ii) imperfect warm-to-active switching with success probability $p$, and (iii) random warm-up time with cdf $H$ on $[u_1,u]$. For a $k$-out-of-$n$ system, it derives a simplified form: $P\{T_{gs}>t\}=P\{X_{n-k+1:n}>t\}+\frac{\bar F^{k-1}(t)}{B(n-k+1,k)}\int_u^t \frac{\bar G(\omega(x-u)+t-x)}{\bar G(\omega(x-u))\,\bar G(\gamma(x-u))}F^{n-k}(x)\,dF(x)$. It shows that setting $u=0$ recovers warm-standby results (e.g., Eryilmaz, 2013) and setting $\gamma=\omega=0$ (with $u=0$) recovers cold-standby coherent-system results (Franko et al., 2015). A worked example for $T=\min(X_1,\max(X_2,X_3))$ with signature $(1/3,2/3,0)$ and exponential lifetimes plots reliability curves, illustrating improvement with standby and the effect of delaying warm-up to $u=0.5$ vs $u=0$.","The authors note that because their results are general, the resulting mathematical expressions are complicated, and they provide only one worked example to illustrate practical computation. They also assume an initial period $[0,u]$ during which the system is known not to fail and treat switching as instantaneous in the perfect/imperfect cases. For random warm-up, they assume the warm-up time is independent of the standby lifetime.","The development assumes independent component lifetimes and (for the main coherent-system results) identical distributions for the active components, which may limit applicability to heterogeneous or dependent components. The reliability expressions rely on knowledge of the system signature and conditional distributions tied to order statistics; in practice these may be difficult to obtain for complex systems or when component parameters are unknown/estimated from limited data (no Phase I/estimation uncertainty is treated). The numerical illustration is limited (exponential case and specific $\gamma,\omega$), and there is no broader sensitivity analysis or validation on real reliability datasets. The standby model uses virtual-age/acceleration functions $\omega$ and $\gamma$ but does not discuss how to identify or estimate these functions from engineering data or physics.",None stated.,"Extend the results to non-identically distributed active components and to dependent lifetimes (e.g., via copulas or shared frailty) to better reflect common-cause or load-sharing effects. Develop inference/estimation procedures for $\gamma(\cdot)$ and $\omega(\cdot)$ (and for signature-related probabilities) under censored life-test data, including uncertainty propagation to system reliability. Provide computational tools (e.g., an R/Python package) and scalable algorithms for evaluating the derived integrals for large $n$ and complex coherent structures. Validate and benchmark the models on real standby-system datasets and explore optimal design choices for $u$, warm-up distributions, or switching reliability $p$ under cost/availability constraints.",1507.06119v3,https://arxiv.org/pdf/1507.06119v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:30:53Z
FALSE,Other,"Parametric (Weibull, etc.)|Other",Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies classification of signals that lie approximately in low-dimensional linear subspaces when the classifier uses mismatched class parameters (mismatched priors and low-rank Gaussian covariances) instead of the true ones. It derives an analytically tractable upper bound on the MMAP (mismatched MAP) classifier’s error probability and then develops a low-noise (\(\sigma^2\to 0\)) asymptotic expansion of this bound. From the expansion, it provides sufficient geometric conditions (involving subspace intersections and principal angles between true and mismatched subspaces) that predict when the classifier will or will not exhibit an error floor. The conditions are validated via simulations on synthetic examples and are applied to real datasets (Hopkins 155 motion segmentation and MNIST digit classification) to estimate the training sample sizes needed to achieve reliable classification behavior. Overall, the contribution is theoretical performance characterization for mismatched subspace/Gaussian classification rather than reliability engineering of physical systems.","The observation model is \(y=x+n\) with isotropic Gaussian noise \(n\sim\mathcal N(0,\sigma^2 I)\) and class-conditional signals \(x|c=i\sim\mathcal N(0,\Sigma_i)\) (possibly low-rank). The mismatched MAP decision rule uses \(\tilde p_i,\tilde\Sigma_i\) and compares \(\tilde p_i\,\tilde p(y|c=i)\), where \(\tilde p(y|c=i)\propto |\tilde\Sigma_i+\sigma^2 I|^{-1/2}\exp\{-\tfrac12 y^T(\tilde\Sigma_i+\sigma^2 I)^{-1}y\}. A key bound uses \(\Sigma_{ij}=(\Sigma_i+\sigma^2 I)^{-1}+\alpha_{ij}(\tilde\Sigma_j+\sigma^2 I)^{-1}-\alpha_{ij}(\tilde\Sigma_i+\sigma^2 I)^{-1}\) and yields \(\bar P(e_{ij})= (\tfrac{\tilde p_j}{\tilde p_i})^{\alpha_{ij}}\Big(\tfrac{|\tilde\Sigma_i+\sigma^2 I|}{|\tilde\Sigma_j+\sigma^2 I|}\Big)^{\alpha_{ij}/2}(|\Sigma_i+\sigma^2 I|\,|\Sigma_{ij}|)^{-1/2}\) when \(\Sigma_{ij}\succ 0\).","The main asymptotic result is that under geometric conditions on true vs. mismatched subspaces, the upper bound obeys \(\bar P(e)=A\,(\sigma^2)^d+o((\sigma^2)^d)\) as \(\sigma^2\to 0\) with \(A>0\), predicting no error floor when \(d>0\). The exponent is \(d=\min_{i\neq j} d_{ij}\) with \(d_{ij}=\tfrac12\big(s^{V}_{ij}+\alpha_{ij}(\tilde r_j-\tilde r_i)\big)\), linking decay rate to subspace dimensions and mismatch ranks. If certain inclusion/angle conditions fail for any class pair, the bound remains \(O(1)\) as \(\sigma^2\to 0\), predicting an error floor. Simulations on synthetic constructions (Examples/Table III) show the bound sharply predicts whether an error floor occurs, and additional experiments on Hopkins 155 and MNIST show the theory can approximate “phase transitions” in training-set size needed to reach best-achievable performance.","The authors note that real data are not drawn from exact Gaussian distributions nor from perfect linear subspaces (the core assumptions of the analysis), yet they observe the derived bound can still have practical value even when these assumptions do not hold strictly. They also remark that some comparisons involve upper bounds rather than the true error probabilities.","The work is not about engineering reliability (failures, lifetimes, maintenance) despite using the term “reliable classification”; it addresses statistical classification reliability in a low-noise asymptotic sense. The key guarantees are sufficient (not necessary) and depend on low-noise asymptotics, so their practical accuracy at moderate/high noise or with strong model misspecification is not ensured. The analysis assumes known noise variance and Gaussian class-conditionals with low-rank covariances; robustness to heavy tails, outliers, autocorrelation, or unknown \(\sigma^2\) is not developed. Code and reproducibility details (implementation, exact parameter settings, dataset splits beyond brief descriptions) are not provided, limiting replication fidelity.","The authors suggest their conditions could serve as a proxy to develop linear feature extraction methods robust to mismatch, arguing that feature transforms should aim to orthogonalize classes (increase principal angles) to tolerate mismatch. They also motivate using the framework to gauge training sample requirements for reliable classification in practical tasks.","Extending the analysis beyond the low-noise regime (finite \(\sigma^2\)) and providing tighter bounds (or approximations) to actual error probability would improve practical utility. Developing robust/self-starting variants that handle unknown noise level and non-Gaussian or contaminated distributions (e.g., heavy-tailed noise, outliers) would broaden applicability. A multivariate/structured extension to affine subspaces, manifold models, or time-correlated observations (e.g., video trajectories with temporal dependence) would better match real datasets. Providing open-source code and standardized experimental protocols (including more baselines) would strengthen reproducibility and benchmarking.",1508.01720v2,https://arxiv.org/pdf/1508.01720v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:31:34Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling|RUL prediction,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)",Degradation measurements,Not applicable,Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a semi-parametric modeling framework for accelerated destructive degradation test (ADDT) data, combining a nonparametric monotone B-spline model for the baseline degradation path with a parametric acceleration relationship (e.g., Arrhenius) to enable extrapolation to use conditions. The approach enforces physically meaningful monotone degradation via linear inequality constraints on spline coefficients and fits the model using likelihood-based estimation with quadratic programming for constrained spline coefficients and REML/profile likelihood for variance/correlation and acceleration parameters. The method supports inference through a nonparametric bootstrap tailored to the compound-symmetry correlation structure within each (temperature, time) cell. Simulation studies assess estimator bias/MSE, confidence interval coverage, and robustness under model misspecification, showing good overall performance and improved robustness relative to misspecified parametric models. The method is demonstrated on multiple industrial ADDT datasets (adhesive bond strength, seal strength with batch variability, and adhesive formulation data), often achieving lower AIC than application-specific parametric models and producing different MTTF estimates at use conditions.","The degradation measurement model is $y_{ijk}=D(t_{ij},x_i;\theta)+\varepsilon_{ijk}$ with Arrhenius-transformed temperature $x_i=-11605/(\mathrm{Temp}_i+273.16)$. The proposed scale-acceleration form is $D(t_{ij},x_i;\theta)=g(\eta_i(t_{ij};\beta);\gamma)$ with time rescaling $\eta_i(t;\beta)=t/\exp(\beta s_i)$ and $s_i=x_{\max}-x_i$; the baseline path is modeled as $g(z)=\sum_{l=1}^p \gamma_l B_{q,l}(z)$ using monotone B-splines. Monotone decreasing degradation is imposed via coefficient constraints $\gamma_l\le \gamma_{l-1}$ (sufficient) and errors follow $\varepsilon_{ijk}\sim N(0,\sigma^2)$ with compound-symmetry correlation $\mathrm{Corr}(\varepsilon_{ijk},\varepsilon_{ijk'})=\rho$ for replicates within the same $(i,j)$ cell.","In simulations (500 datasets per scenario; bootstrap with $B=1000$), estimator MSE for parameters and the baseline degradation curve decreases as the number of temperature levels or time points increases; bias for $\beta$ and $\sigma$ is small even in small designs, while $\rho$ shows larger bias when designs are very small. Under model misspecification (data generated from a parametric model but fit with different models), the semi-parametric method achieved near-zero integrated bias for the baseline path (IBias $\approx 0.0003$) with higher variance (RIVar $\approx 0.0091$) than the correctly specified parametric model, and far lower bias than an incorrect parametric model (IBias $\approx 0.0267$). For MTTF at 30°C in the misspecification study, the semi-parametric estimator had mean 82.77 (bias 0.16, SD 4.22) versus the true model 82.60 (bias 0.01, SD 2.99) and the incorrect parametric model 85.82 (bias 3.20, SD 3.75). In three real ADDT applications, the semi-parametric model had lower AIC than the respective parametric alternatives (e.g., Adhesive Bond B AIC −67.44 vs −61.93; Seal Strength −387.49 vs −379.98; Formulation K −311.98 vs −309.90) and produced notably different use-condition MTTF estimates (weeks): 306 vs 270 (Bond B, 30°C), 127 vs 222 (Seal Strength, 100°C), and 92 vs 68 (Formulation K, 30°C) at a 70% failure threshold.","The authors note that extrapolation is limited by the nonparametric baseline path: the model can extrapolate degradation down to the lowest degradation level observed in the data ($y_M$), but extrapolation beyond $y_M$ is not possible due to the nonparametric construction of $g(\cdot)$. They also restrict attention to scale-acceleration models in this work, acknowledging that some products may require both scale and shape acceleration to adequately describe degradation.","The approach assumes normal errors and a compound-symmetry correlation structure within each temperature–time cell, which may be inadequate when variability changes with time/stress (heteroscedasticity) or when correlation is more complex than exchangeable. Knot/degree selection is AIC-driven and may be unstable in very small-sample ADDT settings, potentially affecting extrapolated MTTF substantially. The monotonicity constraint via $\gamma_l\le \gamma_{l-1}$ is only sufficient (not necessary for higher-order splines), which can overly restrict flexibility and introduce bias if the true monotone function requires coefficient patterns outside this sufficient condition. No implementation details (software/package) are provided, and the constrained QP + profile likelihood workflow may be nontrivial to reproduce and tune (e.g., convergence, boundary solutions) in routine industrial analyses.","The paper suggests developing test planning (optimal ADDT design) procedures based on the proposed semi-parametric model so that designs are broadly applicable across materials and allow model checking rather than committing to a specific parametric form upfront. The authors also propose extending the framework beyond pure scale-acceleration to models that include both scale and shape acceleration, noting that estimation and inference would become more complex but could better represent some degradation behaviors.","A natural extension is to allow heteroscedastic error variance as a function of time/stress and to consider alternative correlation structures (e.g., hierarchical random effects for batch/lot and time) to better match industrial destructive testing. Developing robust/self-starting versions that handle non-normal errors or outliers (common in material testing) could improve reliability of MTTF estimates. Extending to multiple accelerating factors (e.g., temperature and humidity/voltage) with semi-parametric interaction structure would broaden applicability. Providing open-source software (e.g., an R/Python package) with reproducible examples and guidance on knot selection sensitivity and uncertainty propagation to MTTF would increase practical adoption.",1512.03036v1,https://arxiv.org/pdf/1512.03036v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:32:24Z
TRUE,Maintenance optimization|System reliability|Other,Bayesian|Simulation-based|Other,Simulated only|Other,Age-based|Not applicable|Other,Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper addresses optimization of maintenance investment strategies for industrial assets where the economic outcome (Net Present Value, NPV) is evaluated by a computationally expensive Monte Carlo simulator (EDF’s VME tool). Instead of repeatedly running the stochastic simulator during optimization, the authors propose a surrogate modeling approach that emulates the full output distribution by modeling the simulator’s quantile function rather than only its mean/variance. They represent each output quantile function as a linear combination of basis quantile functions selected from training runs via a greedy “Modified Magic Points” (MMP) algorithm, and then model the basis coefficients with independent Gaussian process (kriging) metamodels. For optimization of a target quantile (risk-averse objective), they introduce an adaptive sampling method, Quantile Function Expected Improvement (QFEI), extending EGO/EI ideas to quantiles by exploiting the GP predictive distribution of the quantile value. On a toy stochastic function and on the VME maintenance-planning case (five discrete decision dates for replacements and supply recovery across four plants), QFEI improves over direct optimization on a fixed surrogate and finds solutions close to the best observed points using relatively few additional simulator runs.","The stochastic simulator is modeled as $G: E\times\Omega\to\mathbb{R}$ with random output $\mathrm{NPV}(x,\omega)$ and associated quantile function $Q_x(p)$ defined by $\int_a^{Q_x(p)} f_x(\varepsilon)\,d\varepsilon=p$. Each quantile function is approximated by a basis expansion $\hat Q_x(p)=\sum_{j=1}^k \psi_j(x)R_j(p)$ where $R_j$ are basis quantile functions chosen by MMP and $\psi_j(x)$ are coefficients emulated by GP models. For quantile optimization at level $p$, the induced GP for $U_x=\sum_{j=1}^k \psi_j(x)R_j(p)$ yields an EI-style criterion $\mathbb{E}[I(x)] = \sigma_{U|D}(x)\big(u(x)\phi(u(x)) + \varphi(u(x))\big)$ with $u(x)=(\hat U_x-\max(U_D))/\sigma_{U|D}(x)$, and the next point is $x_{new}=\arg\max_{x\in E}\mathbb{E}[I(x)]$.","For the toy stochastic function, a 4-function MMP basis gave very small projection errors (reported $\mathrm{err}_1=0.09\%$ on the training set and $\mathrm{err}_2=0.13\%$ over all $E$), and the full metamodel error was $\mathrm{err}_3=1.42\%$ in relative $L^2$ over $E$. Directly maximizing the surrogate quantile could select a suboptimal point (example: true optimum 40%-quantile 0.884 vs surrogate-selected point with true 0.739), while QFEI with 20 iterations found the best point 22/30 times and otherwise the second-best (example achieved 0.878). In the VME case, with $k=5$ basis functions, an example projection error was about 0.2% and an example emulation error about 2.8% (relative $L^2$), but direct optimization on the fixed surrogate failed to improve over the initial design. With QFEI (50 iterations) for $p=0.4$, the reported best in the study set was $Q_{x^*}(p)=-1.72$ at $x^*=(41,47,48,45,18)$ and QFEI found $\hat x^*=(41,47,45,46,19)$ with $Q_{\hat x^*}(p)=-1.74$, consistently yielding one of the five best points over repeated trials.","The authors note that they restrict optimization to a reduced candidate set $E$ (a random subsample) rather than the full discrete space because computing the metamodel over the full space is too costly, and they plan to extend to the full space later. They also state that QFEI has “no guarantee on convergence” and that many tests remain (varying quantile level $p$, increasing $|E|$, increasing input dimension $d$). They mention that their implementation neglects the Monte Carlo estimation error of empirical quantiles by taking $N_{MC}=10^4$ as sufficiently large.","The approach depends on strong modeling assumptions: independent GP models for the basis coefficients and approximate Gaussianity of the induced quantile-at-$p$ process, which may be violated (especially in tails or with discrete/integer inputs). The basis construction (MMP) is data-dependent and may not generalize well outside the explored region; re-computing the basis each iteration can introduce instability and complicate reproducibility. The method is demonstrated on a restricted discrete design space and does not fully address scalability to the original $|E|=10^5$ (or larger) nor optimization over mixed discrete/continuous decision variables. Practical deployment would also need guidance on choosing $k$, stopping rules, and handling Monte Carlo noise when $N_{MC}$ cannot be as large as $10^4$ per design point.","They propose several extensions: allow variable $N_{MC}$ (including decreasing it to reduce simulator cost), improve the initial design by using space-filling designs instead of random sampling, and make algorithmic improvements to reduce metamodel-evaluation cost and allow larger study sets $E$. They suggest multi-objective optimization (optimizing multiple quantiles), incorporating parameter-estimation uncertainty from $(\hat h,\hat\beta,\hat\theta)$ into a more precise QFEI, applying the approach to more complex real cases, and extending to robust optimization with additional environmental (uncertain) inputs.","A natural extension is a noise-aware version of QFEI that explicitly models Monte Carlo error in estimated quantiles (e.g., heteroscedastic “stochastic kriging” for quantiles or Bayesian quantile regression surrogates) to reduce the need for very large $N_{MC}$. The method could be broadened to constrained and mixed-integer Bayesian optimization directly on the full discrete space (e.g., using categorical kernels or combinatorial BO) rather than subsampling $E$. Additional reliability-specific integration—linking decision variables to explicit failure-time/reliability models (Weibull/process-based degradation) and validating against real maintenance/failure datasets—would strengthen engineering credibility beyond the economic simulator. Finally, providing open-source implementations and benchmarking against alternative risk-averse BO methods (e.g., CVaR optimization, quantile EGO variants, or distributional surrogates) would clarify robustness and practical performance.",1512.07060v2,https://arxiv.org/pdf/1512.07060v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:33:11Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Life distribution modeling|Other,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|ML-based|Other",Complete lifetime data|Right-censored|Sensor/condition monitoring|Mixture of types|Simulated only,Condition-based|Predictive|Imperfect maintenance|Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes the Latent State Hazard Model (LSHM) for reliability analysis of non-self-healing equipment, separating failure risk into (i) a monotonically nondecreasing latent degradation component and (ii) a transient, covariate-driven component reflecting short-term external stressors. The hazard is modeled as an additive mixed hazard structure that generalizes time-dependent Cox models while compactly encoding the full covariate history through an integral/recursion for the latent term, enabling convex maximum-likelihood fitting with ℓ2 regularization. The method is applied to SCADA condition-monitoring data from 28 wind turbines (95 run-to-failure lifetimes) to estimate hazards, interpret covariate effects, and generate maintenance warnings using a cost-sensitive threshold policy targeting a desired lead time. Empirically, LSHM yields higher hazard-rank percentiles at failure than a Cox PHM (sign test p=0.048) and slightly lower warning costs under multiple early/late cost ratios; it also demonstrates comparable performance on NASA turbofan run-to-failure data and via simulations that show parameter consistency. Overall, the work advances prognostics by providing an interpretable latent health state usable for predictive maintenance decisions without requiring explicit degradation signals, a failure threshold, or strong distributional assumptions on covariate processes.","The hazard decomposes as $\lambda(t\mid x_i^h(t)) = \mu(t\mid x_i^h(t)) + g(t\mid x_i(t))$. The latent degradation term is $\mu(t\mid x_i^h(t))=\int_0^t \mu_0(\tau)\exp(\beta_0+\beta^\top x_i(\tau))\,d\tau\approx\sum_{l\le t}\mu_0(l)\exp(\beta_0+\beta^\top x_i(l))\Delta$, yielding recursion $\mu(t)=\mu(t-\Delta)+\mu_0(t)\exp(\beta_0+\beta^\top x_i(t))\Delta$ and monotonicity. The transient term is $g(t\mid x_i(t))=g_0(t)\exp(\alpha_0+\alpha^\top x_i(t))$, and parameters are fit by (regularized) maximum likelihood using the discrete-time likelihood in Eq. (6) with objective $W_N(\theta)=-\log L_N(\theta\mid X)+C_1\|\alpha\|_2^2+C_2\|\beta\|_2^2$, which is strictly convex for $C_1,C_2>0$.","On the wind-turbine SCADA study (28 turbines; 95 lifetimes; 5-fold CV), Cox–Snell residual K–S test p-values were 0.14, 0.26, 0.30, 0.31, 0.19, supporting adequate model fit. In comparing hazard-rank percentiles at failure between LSHM and a Weibull-baseline Cox PHM, LSHM significantly outperformed via a sign test (p=0.0480). For warning generation with desired lead time $d=5$ days, total costs across folds were lower on average for LSHM than Cox PHM (means: 92.6 vs 94.0 for $c_1=c_2$; 245.0 vs 268.2 for $c_1=5c_2$; 303.6 vs 305.2 for $c_1=10c_2$), and both beat “warn at failure.” In simulation, average percent error of estimated $\lambda$ decreased with sample size (about 19.06% at N=50 down to 1.40% at N=800), and parameter estimates converged with narrowing 95% intervals.","The authors note that the decision framework relies on an application-chosen cost function (e.g., hinge/quadratic/logistic) and that too much averaging of SCADA data could potentially remove transient effects (they check 24h vs 12h/6h to mitigate this). They also state that work-order data were insufficient to distinguish specific shutdown reasons or failed components, so the model predicts “any failure mode” rather than component-specific failures. The model assumes independent lifetimes between restarts and treats maintenance implicitly as restoring the turbine to a similar initial condition; they acknowledge true post-repair state is not fully known.","The latent degradation term $\mu$ is defined as an accumulated integral of exponentials of covariates rather than an explicit stochastic degradation process; while monotone, it may not correspond to physical damage accumulation or allow straightforward uncertainty quantification for the latent state. The discrete-time likelihood assumes piecewise-constant hazards and effectively conditional independence given covariates; SCADA features are often autocorrelated and may violate modeling assumptions, potentially affecting calibration of warnings. The method is evaluated on relatively small numbers of lifetimes per fold (19), and comparisons focus mainly on Cox PHM; broader benchmarks (e.g., random survival forests, deep survival, joint models, recurrent event/repair models) and sensitivity analyses for feature engineering choices (M1/M2, averaging window) are limited.","They propose incorporating additional prior knowledge about how external factors influence the latent state into an extended model. They also suggest adding a third hazard term that can reduce hazard to accommodate cases with self-healing due to external factors. They emphasize broader applicability beyond wind turbines (e.g., healthcare) and note the model can be modified to include an initial degradation level when more post-repair information is available (referencing supplementary material).","Develop a self-starting/online updating version (streaming convex optimization) with principled uncertainty estimates for $\mu(t)$ and warning thresholds, enabling risk-aware decisions with confidence bounds. Extend to explicit repair/renewal modeling (imperfect maintenance, dependence between successive lifetimes, component-level competing risks) rather than treating lifetimes as independent resets. Add robustness to autocorrelation and nonstationarity in SCADA covariates (e.g., frailty terms, random effects by turbine, or state-space extensions) and perform larger-scale benchmarking against modern survival ML and joint latent-state models on multiple industrial datasets.",1601.08034v1,https://arxiv.org/pdf/1601.08034v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:34:06Z
FALSE,NA,Bayesian|Simulation-based|Other,Event/count data|Simulated only|Other,Not applicable,Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Exact distribution theory|Other,TRUE,R|Fortran|Other,Supplementary material (Journal/Publisher),http://xte.mit.edu/ASM_lc.html|http://cran.r-project.org/web/packages/Rgbp/Rgbp.pdf|http://doi.acm.org/10.1145/1186822.1073227,"The paper studies Bayesian estimation of binomial and multinomial proportions and, crucially, the reliability of associated uncertainty/interval estimates via agreement between nominal confidence level (ξ) and empirical or exact coverage (C). It compares common Bayesian priors (uniform/Laplace “rule of succession” and Jeffreys) and posterior approximations (normal vs beta), showing that multinomial settings with large numbers of bins can yield severe ξ–C mismatches (often C≪ξ) for moderate-to-large true bin probabilities due to constraints induced by the multinomial structure. To improve probability matching, it proposes tuning a hyperparameter α0 in a generalized beta/Dirichlet prior by minimizing (ξ−C)^2, finding this works well for b=2 but fails for b>2 unless an effective b=2 workaround with renormalization is used. The work then introduces a de-noising (smoothing) approach for histogram-based pdf estimates, demonstrating via Monte Carlo that smoothed estimates can achieve similar p̂-to-p accuracy with roughly ~10× fewer observations for b=100, and develops an empirical correction to single-histogram variance estimates to avoid overconfidence caused by variability in σ̂ itself. The paper also discusses discrete-domain (integer-lattice) Bayesian estimators and compares them against multinomial, joint shrinkage (James–Stein) estimators and smoothed variants.","Key estimators include the multinomial “rule of succession” mean and variance under a uniform prior: \(\hat p_i=(n_i+1)/(N+b)\) and \(\widehat{\sigma^2}_{p_i}=\hat p_i(1-\hat p_i)/(N+1+b)\); under Jeffreys: \(\hat p_i=(n_i+1/2)/(N+b/2)\) with \(\widehat{\sigma^2}_{p_i}=\hat p_i(1-\hat p_i)/(N+1+b/2)\). A generalized Dirichlet/Beta prior parameterized by \(\alpha_0\) yields \(\hat p_i=(n_i+\alpha_0)/(N+b\alpha_0)\) and \(\widehat{\sigma^2}_{p_i}=\hat p_i(1-\hat p_i)/(N+1+b\alpha_0)\); \(\alpha_0\) is optimized by minimizing \((\xi-C)^2\). For smoothed histograms, a parametric single-run uncertainty model is fit as \(\hat\sigma_{p_i}=\sqrt{\hat p_i(1-\hat p_i)/(A_0 N+B_0)}\) and then rescaled by a lower-tail factor \(\rho_{0.99}(N)\) to guard against underestimation: \(\hat\sigma_{p_i}=\rho_{0.99}(N)^{-1}\sqrt{\hat p_i(1-\hat p_i)/(\bar A_0 N+\bar B_0)}\).","For multinomial histograms with many bins (e.g., b=100) and larger true bin probabilities (e.g., p_true≈0.2–0.5), both Jeffreys and uniform priors can give very poor coverage, with C dropping to ~0.4–0.6 around N≈10^2 and remaining well below 0.95 even at N≈10^3 in the displayed examples. Optimizing \(\alpha_0\) for ξ↔C improves probability matching for the binomial case (b=2), and the optimized \(\alpha_0(N)\) for the normal-posterior approximation is well fit by an exponential-like form (e.g., for ξ=0.95: k≈0.991, B0≈5.863 in \(\alpha_0=\exp(k)\,N/(N+B_0)\)). For smoothed/de-noised b=100 histograms, the paper reports roughly an order-of-magnitude reduction in required sample size to reach comparable p̂-to-p accuracy: for b=100 and N<12800, de-noised estimates often need ≈10× fewer observations (sometimes 4–25× depending on N and the test pdf) to match the S/N of unsmoothed estimators. The fitted single-run σ̂ model parameters for smoothed histograms are around \(\bar A_0\approx 10\) and \(\bar B_0\approx 560–655\) depending on the starting estimator, and an additional upward correction based on run-to-run variation (via \(\rho_{0.99}(N)\)) is recommended to avoid overconfident intervals.","The authors note that for b>2, optimizing \(\alpha_0\) in the multinomial Dirichlet prior often drives \(\alpha_0\to 0\) with large variability, and ξ↔C matching remains poor unless an ad hoc effective “b=2” workaround with renormalization is used, which then harms p̂-to-p matching. They explicitly report edge effects in the de-noising procedure: coverage can drop sharply in sparse bins near histogram boundaries, likely due to fewer local data segments for smoothing and histogram-range truncation artifacts. They also state that the scaling/baseline correction after smoothing is heuristic and strongly affects S/N, and that the choice of the best starting estimator for smoothing remains unsettled.","The work targets “reliable assessment of accuracy” for interval coverage, but most evaluations are simulation-based and tuned to the same simulation setup (e.g., fixed b=100, fixed smoothing parameters, specific synthetic pdf families), which may limit generalization to real histogram shapes, multimodality, discontinuities, or autocorrelated samples. The proposed variance correction \(\rho_{0.99}(N)\) is empirically fit and assumed broadly invariant across bins and underlying pdfs; robustness to different b, different smoothing settings, and real-data departures (non-IID, measurement error, discretization) is not fully established. Computational burden is significant (week-long optimization cycles are mentioned), and no packaged implementation is identified, which may impede practical uptake despite the methodological promise.","The authors suggest extending the de-noised histogram approach to downstream tasks such as estimating means/medians, downweighting outliers, and “idealizing” raw measurements by adjustment to the nearest smooth distribution; they note preliminary routines are included with the accompanying source code. They also mention that, for the binomial case, it may be possible to derive exact expressions to account for run-to-run variation in single-run \(\hat\sigma_p\) (rather than relying on Monte Carlo-based corrections). They indicate that improved scaling/baseline procedures and broader testing of Bayes classification with updated procedures are ongoing.","A valuable extension would be a systematic robustness study across varying numbers of bins b, different binning schemes (adaptive bins), and different smoothing kernels/parameters, with principled selection (e.g., cross-validation) rather than heuristic tuning. It would also help to provide theoretical guidance or approximate formulas for ξ–C behavior in large-b multinomial settings (including when and why C collapses), and to connect the \(\alpha_0\) probability-matching objective to established matching-prior theory for discrete multinomial models. Finally, releasing a reproducible software package (R/Python) with documented pipelines for smoothing, scaling, and variance correction would materially improve practical adoption and allow benchmarking against modern empirical Bayes/shrinkage and Bayesian hierarchical multinomial-Dirichlet models.",1602.00207v1,https://arxiv.org/pdf/1602.00207v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:35:03Z
TRUE,System reliability|Life distribution modeling|Other,Bayesian|Nonparametric/Semi-parametric|Other,Complete lifetime data,Not applicable,Transportation/logistics|Other,Other,TRUE,R,Personal website,http://www.louisaslett.com,"The paper develops an imprecise Bayesian nonparametric framework for system reliability with multiple heterogeneous component types using the survival signature to separate system structure from component failure behavior. Component survival at each time point on a user-chosen grid is modeled via Bernoulli indicators with a Beta prior on the functioning probability, yielding Beta-Binomial posterior predictive distributions for counts of functioning components; imprecision is introduced by using sets of Beta priors (rectangular sets in canonical parameters: prior strength and prior mean bounds). The main methodological contribution is propagation of component-level prior imprecision and test data to lower/upper bounds on the system survival probability $R_{sys}(t)=P(T_{sys}>t)$, along with a practical diagnostic for prior–data conflict that manifests as widened posterior predictive bounds. New first-order stochastic dominance results for the Beta-Binomial distribution enable efficient computation of the system-level bounds in most time points without exhaustive optimization. The approach is implemented in the R package ReliabilityTheory and demonstrated on a toy bridge system and a simplified automotive brake system, illustrating how conflict at the component level may or may not be consequential at the system level.","System reliability is computed via the survival signature: $R_{sys}(t)=\sum_{l_1=0}^{m_1}\cdots\sum_{l_K=0}^{m_K} \Phi(l_1,\ldots,l_K)\,P(\cap_{k=1}^K\{C_t^k=l_k\})$. For each component type $k$ and time $t$, with a Beta prior on functioning probability $p_{k,t}$ and Binomial data summary $s_{k,t}$ from lifetime tests, the posterior predictive for $C_t^k$ is Beta-Binomial: $P(C_t^k=l_k\mid s_{k,t})=\binom{m_k}{l_k}\,\frac{B(l_k+n^{(n)}_{k,t}y^{(n)}_{k,t},\,m_k-l_k+n^{(n)}_{k,t}(1-y^{(n)}_{k,t}))}{B(n^{(n)}_{k,t}y^{(n)}_{k,t},\,n^{(n)}_{k,t}(1-y^{(n)}_{k,t}))}$ with canonical update $n^{(n)}=n^{(0)}+n$ and $y^{(n)}=\frac{n^{(0)}}{n^{(0)}+n}y^{(0)}+\frac{n}{n^{(0)}+n}\frac{s}{n}$. Bounds on $R_{sys}(t)$ are obtained by minimizing/maximizing the above expression over prior parameter rectangles $[\underline n^{(0)},\overline n^{(0)}]\times[\underline y^{(0)},\overline y^{(0)}]$, aided by stochastic dominance results for Beta-Binomial in $y$ (and sufficient conditions in $n$).","The paper proves (Theorem 1) that for fixed $(n,m,s,N)$, the Beta-Binomial predictive distribution is first-order stochastically increasing in the prior mean parameter $y$ (larger $y$ yields stochastically larger counts of functioning components), enabling system-bound extrema to occur at $\underline y^{(0)}$ or $\overline y^{(0)}$. It also provides sufficient conditions (Theorem 2) and a tail-check lemma (Lemma 3) to determine stochastic dominance with respect to the prior strength parameter $n$, substantially reducing the need for exhaustive optimization across time points. In the bridge-system example, when component-type T3 data conflict with priors, posterior component survival bounds widen markedly and this imprecision propagates to wider system reliability bounds; however, the method can show that some component-level conflict is not consequential for system reliability beyond certain mission times. The approach is implemented in R (ReliabilityTheory), with a main function that computes lower/upper posterior predictive system survival probabilities and parallelizes computations across CPU cores.","The authors note that the current model does not yet incorporate right-censored observations, which are common in reliability studies. They also highlight that scaling survival-signature computation to very large systems or networks (thousands of components) is a major challenge, even though the signature needs to be derived only once and monotonicity can help when only partial signatures are available.","The nonparametric time-grid approach treats component functioning at each grid time separately; without additional constraints, ensuring global coherence/monotonicity across time (e.g., $p(t_j)\ge p(t_{j+1})$) depends on prior elicitation choices and may be sensitive to grid selection and rounding of failure times to grid points. The method assumes exchangeability/conditional i.i.d. within component types and (in the main closed-form development) independence across types; strong dependencies (e.g., common-cause failures) are discussed as possible but are not developed in the core inference algorithm. Practical use requires specifying prior rectangles at multiple time points, which can be elicitation-heavy and may produce conservative bounds if not carefully structured. Empirical evaluation is limited to illustrative examples rather than a broad simulation benchmark comparing bound tightness or computational cost versus alternative robust/imprecise Bayesian system reliability methods.","They propose extending the model to handle right-censored data so that observations from a running system can be used to compute remaining useful life. Two approaches are suggested: (i) a minimal-assumption method treating censored components as failing immediately after censoring or surviving through the horizon (simple but highly imprecise), and (ii) assuming exchangeability with other surviving components at censoring time (more complex but less imprecise, akin to Kaplan–Meier and related imprecise-probability work). They also mention the challenge of upscaling survival signatures to very large systems and networks and note possible generalizations of the structure function to probabilistic/imprecise probabilistic system functioning given component states.","A valuable extension would be a self-starting/online version that updates system reliability bounds sequentially as new component data arrive, including efficient reuse of previously computed dominance decisions across time. Robustness to model misspecification could be studied more systematically, e.g., sensitivity of bounds to time-grid resolution, dependence violations, and heterogeneity within a nominal component type. Developing explicit support for common-cause failures and other cross-type dependence structures within the imprecise Bayesian updating (rather than treating them only conceptually) would broaden applicability. Packaging additional tooling for prior elicitation (constraints enforcing monotone survival curves, spline/shape-restricted priors for $y^{(0)}(t)$, and diagnostics explaining which components/time points drive system-level imprecision) would improve practitioner usability.",1602.01650v1,https://arxiv.org/pdf/1602.01650v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:35:52Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Other",Other,Not applicable,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper studies reliability (ageing) properties and stochastic ordering results for the Transformed-Transformer (T-X) family of distributions, where a random variable R is generated via a weight function ω applied to the cdf/survival of a transformer variable X and then mapped through a base variable T. It derives sufficient conditions on the weight function (e.g., monotonicity of xω2′(x) on (0,1]) under which ageing properties such as IFR (increasing failure rate) and NBU (new better than used) are inherited by R when they hold for both T and X. It also establishes how stochastic orders between (T1,T2) and (X1,X2) transfer to the induced variables (R1,R2), including usual stochastic order, hazard rate order, up-shifted hazard rate order, and reversed hazard rate order, under additional regularity conditions on ω1 or ω2. Several examples/special cases are discussed (e.g., gamma-X and gamma–Weibull families), and counterexamples show that the stated conditions on ω cannot generally be dropped. Overall, it provides a general theoretical toolbox to infer reliability-relevant ageing/ordering behavior of many distributions that are special cases of the T-X family (including beta-generated and other common lifetime models).","The T-X cdf is defined by \(F_R(x)=F_T(\omega_1(F_X(x)))\) for \(x\in[c,d]\), and the survival function by \(\bar F_R(x)=\bar F_T(\omega_2(\bar F_X(x)))\) with \(\omega_2(u)=\omega_1(1-u)\). Differentiating gives the density \(f_R(x)= f_T(\omega_2(\bar F_X(x)))\,\frac{d}{dx}\omega_2(\bar F_X(x))\), and hence the failure rate can be expressed via \(r_T\) and \(r_X\) multiplied by \(\bar F_X(x)\omega_2'(\bar F_X(x))\). Key sufficient-condition templates include: IFR inheritance when \(x\omega_2'(x)\) is increasing on \((0,1]\); NBU inheritance when \(\omega_2(xy)\ge \omega_2(x)+\omega_2(y)\) for \(x,y\in(0,1]\); and reversed-hazard ordering results using conditions such as \(x\omega_1'(x)\) decreasing on \([0,1)\) or \((1-x)\omega_1'(x)\) increasing on \([0,1)\).","Main theoretical results include: (1) If \(x\omega_2'(x)\) is increasing on \((0,1]\) and both \(T\) and \(X\) are IFR, then \(R\) is IFR (Theorem 2.1), with explicit counterexamples showing the condition is not removable. (2) If \(\omega_2(xy)\ge \omega_2(x)+\omega_2(y)\) for all \(x,y\in(0,1]\) and both \(T\) and \(X\) are NBU, then \(R\) is NBU (Theorem 2.2), again with a counterexample when the condition fails. (3) Order-preservation results: if \(T_1\le_{st}T_2\) and \(X_1\le_{st}X_2\) then \(R_1\le_{st}R_2\) (Theorem 3.1); under \(x\omega_2'(x)\) increasing plus hazard-rate/up-shifted hazard-rate assumptions on \(T_i,X_i\), one gets \(R_1\le_{hr}R_2\) and \(R_1\le_{hr\uparrow}R_2\) (Theorems 3.2–3.3); and under conditions on \(\omega_1\) plus reversed-hazard assumptions one gets \(R_1\le_{rhr}R_2\) (Theorems 3.4–3.5). The paper also tabulates sufficient parameter conditions for IFR in gamma-X families (e.g., exponential/weibull/half-normal/gompertz/makeham/rayleigh transformers) and gives multiple admissible \(\omega_2\) choices (e.g., \(-\ln x\), \((1-x)/x\), \((1-x)^2/x\)) that satisfy the inheritance conditions.",None stated.,"The work is primarily theoretical and does not provide a systematic simulation study or real-data case studies to quantify practical detection/fit improvements across competing lifetime models. Many results rely on strong shape/monotonicity conditions on the weight functions (e.g., monotonicity of \(x\omega_2'(x)\), superadditivity-like inequalities), which may be hard to verify or may exclude useful transformations. The paper focuses on univariate continuous distributions and standard ageing/order concepts under idealized assumptions (e.g., well-defined densities), without addressing censoring, parameter estimation, or robustness to model misspecification that arise in applied reliability analyses.",None stated.,"Extend the inheritance/ordering results to settings with censoring and parameter uncertainty (Phase I estimation) to make the theory directly usable for reliability data analysis. Develop computational tools (e.g., checkable sufficient conditions, automated verification for common \(\omega\) families) and software implementations for practitioners constructing T-X models. Provide empirical validation via simulation and real lifetime datasets comparing fit, tail behavior, and inferred ageing class (IFR/NBU/DFR, etc.) across special cases of the T-X family.",1602.05454v1,https://arxiv.org/pdf/1602.05454v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:36:36Z
FALSE,NA,Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study,TRUE,R|SAS|Other,In text/Appendix,NA,"The paper proposes reliability estimates for three common factor score predictors in factor analysis: Thurstone’s regression predictor, Bartlett’s predictor, and McDonald’s correlation-preserving predictor. The estimates adapt the Kuder–Richardson equivalent-items idea by defining reliability as the correlation between factor score predictors computed from an observed item set and a hypothetical equivalent item set under the same factor model (true score variance attributed only to common factors; no cross-occasion covariance in unique errors). Closed-form matrix expressions are derived for each predictor’s reliability under these assumptions, and conditions are provided (e.g., one-factor models; orthogonal perfect simple structure) under which the three predictors have equal reliability. Simulation studies (population-level and sample-level; additionally with model error via minor factors) compare the reliabilities across design factors such as loading size, secondary loadings, factor intercorrelations, and number of variables. Across conditions, the regression predictor generally yields the largest reliability, with Bartlett typically close and McDonald sometimes lower when factors are correlated and secondary loadings are present; appendices provide R and SPSS scripts to compute the estimates.","Reliability is defined as the diagonal of the correlation matrix between factor score predictors from two equivalent item sets: \(R_{ttf}=\mathrm{diag}(\mathrm{Cor}(\hat f_1,\hat f_2))\), with \(\hat f = B' x\) and identical \(B\) and \(\Sigma\) under equivalence. For Thurstone regression scores, the derived estimator is (Eq. 9): \(R_{ttr}=D^{-1/2}\,\mathrm{diag}(\Phi\Lambda'\Sigma^{-1}\Lambda\Phi\Lambda'\Sigma^{-1}\Lambda\Phi)\,D^{-1/2}\) where \(D=\mathrm{diag}(\Phi\Lambda'\Sigma^{-1}\Lambda\Phi)\). For Bartlett scores, the simplified form is (Eq. 14): \(R_{ttb}=\mathrm{diag}((\Lambda'\Psi^{-2}\Lambda)^{-1}+\Phi)^{-1}\). For McDonald’s correlation-preserving scores, reliability is computed via an \(N\) such that \(N'N=\Phi\) and a normalization term involving \(\Lambda'\Psi^{-2}\Sigma\Psi^{-2}\Lambda\) (Eq. 15).","In population simulations (q=6; p/q=5 or 10; main loadings 0.40–0.80; secondary loadings 0 or 0.10; factor intercorrelations 0 or 0.30), the regression factor score predictor produced the largest reliability estimates overall. Differences among the three predictors were small when secondary loadings were zero, particularly for reliabilities above about 0.70; larger gaps appeared when secondary loadings were present and factors were correlated (r=0.30), favoring regression over Bartlett and McDonald. In sample-based simulations (n=500 and n=1000; 1000 replications per condition), the same qualitative pattern held; additionally, Bartlett became “substantially more reliable” than McDonald when factor intercorrelations and secondary loadings were substantial. Adding model error via many minor factors produced results “extremely similar” to the sample-based study, indicating little sensitivity of these reliability estimates to the specific model-imperfection mechanism tested.","The authors note the reliability estimates rest on strong assumptions: the two item sets are equivalent and share the same population factor model (same \(\Lambda,\Phi,\Psi,\Sigma\)); all true variance/reliability is attributed only to common factors; and unique/error variance does not contribute to reliability (including assuming zero covariance of unique errors across occasions, \(E[e_1 e_2']=0\)). They acknowledge that other perspectives (e.g., hierarchical factor models where some unique variance might be considered true-score variance) are possible but are not treated as the “standard case” addressed here.","The work is psychometric rather than reliability engineering; applicability to engineered systems reliability is indirect at best. The proposed ‘reliability’ definition hinges on a hypothetical equivalent-form replication and assumes stability of the factor model and scoring weights; in practical applications, factor model misspecification, rotation indeterminacy, and sampling variability in \(\Lambda\) and \(\Phi\) can materially affect score reliability beyond what is captured by the equivalent-item construct. The simulations cover a limited design space (e.g., fixed q=6, specific rotation choices, and Gaussian-generation assumptions implied by common-factor score simulation), and do not benchmark against alternative empirical reliability estimators (e.g., split-half with refitting, bootstrap-based approaches) for factor scores.",None stated.,"Extend the reliability derivations to settings with correlated unique errors across occasions (allowing \(E[e_1 e_2']\neq 0\)) and to hierarchical/bifactor models where portions of uniqueness might be treated as true-score variance. Develop robust/self-starting or bootstrap-based estimators that account for uncertainty in estimated \(\Lambda,\Phi\) (including rotation variability) and evaluate coverage/uncertainty intervals for factor-score reliability. Provide validated, packaged implementations (e.g., an R package) and empirical case studies demonstrating behavior under non-normality and model misspecification common in applied factor analysis.",1602.07290v1,https://arxiv.org/pdf/1602.07290v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:37:16Z
FALSE,Other,Other,Other,Not applicable,Transportation/logistics,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://hal.archives-ouvertes.fr/hal-00711847/en/|https://hal.archives-ouvertes.fr/inria-00352834/en/|https://hal.archives-ouvertes.fr/hal-00991942/en/|https://hal.archives-ouvertes.fr/hal-01208171/en/|https://hal.archives-ouvertes.fr/hal-00562488/en/|https://hal.archives-ouvertes.fr/hal-00585152/en/|https://hal.archives-ouvertes.fr/inria-00158855/en/|https://hal.archives-ouvertes.fr/hal-01272152/en/|https://hal.archives-ouvertes.fr/hal-01068569/en/|https://hal.archives-ouvertes.fr/hal-01093635/en/,"The paper proposes a short-term traffic flow forecasting approach based on a nonstandard-analysis time-series decomposition into a smooth trend and quickly fluctuating component, avoiding explicit stochastic/deterministic modeling and big-data storage. Forecasts target the trend via an algebraic estimation of the trend and its time derivative, yielding a basic predictor $X_{\text{forecast}}(t+\Delta T)=E(X)(t)+\widehat{dE(X)(t)/dt}\,\Delta T$. It compares persistence, scaled persistence, an algebraic trend-based forecast, and a mixed method designed to reduce over/undershoot by selecting the smaller-magnitude slope estimate. Reliability is treated in a forecasting-uncertainty sense through “volatility,” defined as a standard-deviation-type measure of deviations from the trend at multiple smoothing time scales. Experiments on 1-minute traffic volume data from the A25 highway (June 2014) show the mixed forecast improves squared error over the baselines for 5/15/60-minute horizons and provides reasonable volatility forecasting in an illustrative example.","Time-series decomposition: $X(t)=E(X)(t)+X_{\text{fluctuation}}(t)$. Trend-based forecast: $X_{\text{forecast}}(t+\Delta T)=E(X)(t)+\widehat{\frac{dE(X)(t)}{dt}}\,\Delta T$ (implemented with a causal moving average $E_{100}$). Scaled persistence: $X_{Pe}(t+\Delta T)=E_{100}(X)(t)\times\frac{E_{100}(X)(t-1\text{day}+\Delta T)}{E_{100}(X)(t-1\text{day})}$. Volatility (reliability proxy): $\mathrm{vol}(X)(t)=\sqrt{E\big((X-E(X))^2\big)}\approx\sqrt{E(X^2)-E(X)^2}$.","Using A25 highway traffic volume (veh/min) sampled every minute over 1–30 June 2014, the mixed forecast achieves the lowest squared errors among compared methods. Reported squared errors (Table 1) for persistence (Pe) vs algebraic (Al) vs mixed (Mi) are: 5 min horizon: $2.08\times10^6$ (Pe), $1.01\times10^6$ (Al), $8.75\times10^5$ (Mi); 15 min: $2.64\times10^6$, $1.7335\times10^6$, $1.23\times10^6$; 60 min: $1.15\times10^7$, $8.47\times10^6$, $4.29\times10^6$. The authors also show volatility increases and varies with the trend-smoothing window (100/250/500 minutes) and present an example of 15-minute-ahead volatility forecasting judged “rather good.”","The authors state the results are preliminary and need further development. They also explicitly note that the approach should be compared with other existing forecasting approaches (i.e., broader benchmarking is still needed).","Although termed “reliability,” the uncertainty treatment is largely descriptive (volatility around a moving-average trend) and does not provide calibrated predictive intervals, coverage guarantees, or decision-oriented risk metrics. The evaluation appears limited to one site/time period (A25, June 2014) and relies on a non-causal moving average to define the “trend” used for error computation, which can advantage methods tuned to similar smoothing. Implementation details (parameter choices beyond window sizes, filtering/derivative-estimation settings) and reproducibility artifacts (code, data access procedure) are not provided, making independent verification difficult.","The authors propose further developing the preliminary results and performing comparisons with other existing traffic forecasting approaches. They also point to deeper study of reliability/risk via confidence bands, referring to prior work where confidence bands could be extended to traffic management.","Extend the method to provide formal uncertainty quantification (e.g., prediction intervals/confidence bands with empirical coverage validation) rather than volatility-only plots. Test robustness under nonstationarity, incidents, weather, missing data, and sensor faults, and evaluate across multiple road segments/cities and seasons. Provide open-source reference implementations and standardized benchmarks to enable fair comparisons with modern baselines (e.g., state-space/Kalman, ARIMA, gradient boosting, LSTM/TCN) under identical preprocessing and loss functions.",1602.08355v1,https://arxiv.org/pdf/1602.08355v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:37:57Z
FALSE,NA,Other,Other,Not applicable,Finance/economics|Other,Other,TRUE,R,Not provided,NA,"The paper studies measurement error and extreme response bias in the Reserve Bank of India (RBI) Inflation Expectations Survey of Households using latent class analysis (LCA). Using three survey rounds (25th–27th; 2011Q3–2012Q1) and 12 categorical indicators about expected price changes (3-month ahead and 1-year ahead across product groups), the author fits LCA models without covariates and selects an optimal 5-class solution via information criteria (notably minimum BIC). The resulting classes are interpreted behaviorally (e.g., “Simulators,” “Revenue Rocketers,” “Dormants/Neutralizers”), and class-conditional response probabilities are used to identify inconsistent/extreme responses. The paper reports notable rates of extreme misclassification for several indicators (especially for 3-month-ahead expectations), concluding that a substantial fraction of respondents exhibit extreme-response bias that can distort survey-based measures of inflation expectations.","The latent class model expresses the joint probability of categorical responses as a mixture over classes: $f(Y_i\mid \pi,p)=\sum_{r=1}^R p_r\prod_{j=1}^J\prod_{k=1}^{K_j}(\pi_{jrk})^{Y_{ijk}}$. Posterior class membership is computed by Bayes’ rule: $\hat P(r_i\mid Y_i)=\frac{\hat p_r f(Y_i;\hat\pi_r)}{\sum_{q=1}^R \hat p_q f(Y_i;\hat\pi_q)}$. Model selection uses information criteria: $AIC=L^2-2\,df$ and $BIC=L^2-df\ln(n)$, with the preferred model having the smallest criterion value.","Among LCA models with 2–6 classes, the 5-class model yields the lowest BIC (BIC ≈ 199595.7), and is selected as optimal. Estimated class population shares are approximately: 0.094, 0.387, 0.282, 0.133, and 0.103. The author highlights substantial “extreme false negative” response probabilities for 3-month-ahead questions—about 27% (general), 27% (food), 31% (non-food), 18% (durables), 34% (housing), and 25% (services)—and an “extreme false positive” issue for 1-year-ahead housing (about 16%). The paper also reports average probabilities of consistent classification ≈ 0.1152 versus consistent misclassification ≈ 0.0179 across indicators, with general 3-month-ahead being a notable exception where misclassification exceeds classification.",The study is explicitly limited to only three survey rounds (25th–27th; 2011Q3–2012Q1) due to non-availability of all-round data. The conclusion notes that additional insight would come from panel data to study how expectations change over time (implying the current cross-sectional/limited-time data restricts dynamic inference).,"This is not a reliability-engineering study; it addresses survey measurement error in an economics context, so the findings do not generalize to engineering reliability without adaptation. The LCA relies on the local independence assumption (conditional independence of indicators within class), which can be violated in multi-item survey batteries and may bias class definitions and misclassification estimates if not tested/relaxed. Model selection is based primarily on AIC/BIC across a small set of class counts and appears to use one best-fitting run; LCA can be sensitive to starting values/local maxima, so multiple random starts and stability checks would strengthen the evidence. No external validation (e.g., holdout sample, reinterview/ground truth, or predictive validity against realized inflation) is provided to confirm that identified “bias” classes correspond to true response styles rather than genuine heterogeneity in beliefs.",The paper suggests extending the analysis using panel data to identify the impact of time on changes in consumer inflation expectations and related response behavior.,"A natural extension is to test robustness to violations of local independence by fitting models with direct effects/residual associations or using item-response/latent trait hybrids. Incorporating covariates (demographics, city, education, income) into latent class membership could clarify drivers of extreme response bias and improve interpretability. Validating the latent classes against external benchmarks (e.g., subsequent realized inflation, professional forecasts, or repeated-measures reinterviews) would help distinguish response style from true belief heterogeneity. Providing reproducible code (e.g., poLCA scripts and model-selection workflow) and sensitivity analyses (different numbers of starts, alternative class counts, alternative indicators) would improve transparency and replicability.",1603.01397v1,https://arxiv.org/pdf/1603.01397v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:38:32Z
FALSE,NA,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|Other",Complete lifetime data|Right-censored|Interval-censored|Left-censored|Degradation measurements|Event/count data|Sensor/condition monitoring|Mixture of types|Simulated only|Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Not provided,http://dx.doi.org/10.1023/A:1018054314350|http://www.cs.toronto.edu/~delve/data/datasets.html|http://books.google.com.au/books?id=T1Jrv88TuH8C|http://www.jstor.org/stable/2290637|http://www.jstor.org/stable/3035587|http://www.jstor.org/stable/2242378|http://www.jstor.org/stable/1390794|http://archive.ics.uci.edu/ml|http://dx.doi.org/10.2307/2241837|http://www.jstor.org/stable/2241566|http://books.google.fr/books?id=qa29r1Ze1coC|http://www.ams.org/mathscinet-getitem?mr=1045442|http://www.jstor.org/stable/2242229,"The paper proposes two methods—Farness BOPI (F-BOPI) and Adaptable BOPI (A-BOPI)—to construct high-confidence prediction intervals for local linear regression/loess that are more reliable than a conventional Wald-type interval based on global RMSE. The approach assumes locally linear mean structure and locally homoscedastic, approximately normal prediction errors; it estimates prediction-error distributions in a local “LHNPE” neighborhood (distinct from the regression neighborhood) using cross-validated leave-one-out prediction errors. Prediction intervals for the response are formed by adding the point prediction \(\hat f(x^*)\) to a tolerance interval for local prediction errors, using tolerance intervals as conservative upper bounds to avoid under-coverage when neighborhoods are estimated. A new interval-comparison metric, Equivalent Gaussian Standard Deviation (EGSD), is introduced to trade off mean interval size and achieved coverage. Simulation studies (Friedman #1/#2) and experiments on 11 benchmark datasets show BOPI intervals generally improve coverage at high nominal levels (e.g., \(\beta\ge 0.9\)) while keeping intervals relatively tight compared with alternatives such as conventional loess intervals, OLS intervals, and SVM-based methods.","Conventional intervals: \(\hat f(x) \pm Z_{1-\beta/2}\,\mathrm{RMSE}\). BOPI construction: \(I^{\mathrm{Pred}}_\beta(x^*) \approx \hat f(x^*) + \hat I^{\mathrm{Tol}}_{\gamma,\beta}(\varepsilon^{\mathrm{pred}}_{x^*})\), where \(\hat I^{\mathrm{Tol}}_{\gamma,\beta}\) is a normal-theory tolerance interval computed from cross-validated local prediction errors in the estimated LHNPE neighborhood. The tolerance interval uses \(\hat\theta \pm c\hat\sigma\) with \(c = \sqrt{\frac{(K-1)(1+1/K)Z^2_{1-(1-\beta)/2}}{\chi^2_{1-\gamma,K-1}}}\) for local neighborhood size \(K\).","In simulations (e.g., Friedman #1 with \(\beta=0.95\)), the conventional loess-based intervals show substantial under-coverage (e.g., around 0.88 coverage), while F-BOPI and A-BOPI achieve coverages much closer to nominal (often \(\approx 0.93\)–0.96 depending on \(\gamma\)). On 11 real benchmark datasets and for \(\beta\in\{0.8,0.9,0.95\}\), the authors report BOPI methods are generally not rejected by a Wilson-score-based reliability test while conventional methods are sometimes rejected at higher \(\beta\) (notably on larger datasets). A-BOPI frequently yields the smallest mean interval size among methods that pass the reliability test, whereas F-BOPI tends to be slightly more conservative (higher coverage). EGSD-based comparisons across datasets typically rank A-BOPI and F-BOPI as most efficient overall relative to alternatives such as OLS and LS-SVM conventional intervals.","The authors note BOPI is limited to local linear regression and has higher computational complexity than conventional and quantile-regression approaches. They state BOPI may perform similarly to alternatives for extremely high desired content (around 0.99 or more) or when the dataset is nearly identically distributed in feature space. They also state BOPI is not suited when there are substantially better regression models than nonparametric regression, or when prediction errors deviate strongly from normality.","The method relies on local normality and local homoscedasticity of prediction errors; in practice, heavy tails, skewness, or strong heteroscedasticity can make normal-theory tolerance intervals miscalibrated, especially in small local neighborhoods. The approach uses cross-validated/LOO prediction errors and nearest-neighbor neighborhoods; results may be sensitive to distance metric choice, feature scaling, and the presence of categorical predictors (mixed data) without a fully specified treatment. Hyperparameter tuning enforces an empirical coverage constraint on the training set, which can risk overfitting the interval width to the validation scheme and may not transfer under dataset shift. No reproducible code or standardized benchmark protocol is provided, which limits independent verification and fair comparison against more recent conformal prediction baselines.","The authors suggest extending the approach beyond local linear regression to other regression functions (e.g., support vector machines). They also propose generalizing BOPI to one-sided interval prediction and applying the method to interval prediction for time series models.","A natural extension is to connect BOPI with distribution-free conformal prediction to relax normality assumptions while preserving finite-sample coverage guarantees, potentially using locally weighted or adaptive conformal scores. Robust or nonparametric tolerance-interval constructions (e.g., based on empirical quantiles or bootstrap) could replace normal-theory tolerance intervals in the LHNPE neighborhood. A principled procedure for selecting neighborhood size under mixed continuous/categorical predictors (and under autocorrelation) would improve practicality. Providing an open-source implementation and benchmarking against modern interval methods (e.g., split conformal, quantile random forests, gradient-boosted quantile regression) would strengthen evidence and adoption.",1603.05587v5,https://arxiv.org/pdf/1603.05587v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:39:19Z
TRUE,Maintenance optimization|System reliability|Network/infrastructure reliability|Other,"Stochastic process|Simulation-based|Parametric (Weibull, etc.)|Other",Simulated only|Other,Condition-based|Not applicable|Other,Energy/utilities|Network/cybersecurity,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.dejazzer.com/reds.html,"The paper studies reliability of cyber-physical power distribution systems (CPPDS) by explicitly incorporating human operators’ response time into reliability estimation. It proposes a stochastic model for operator response time (RTO) and embeds it within a Sequential Monte Carlo Simulation (SMCS) that also simulates electrical-component failures and ICT/communication-network failures (including RSTP-based reconfiguration delay). Using the simulated failure/repair histories, it estimates standard distribution reliability indices (Failure Rate, Availability, SAIDI, SAIFI) and examines how these indices change as the average RTO varies. A case study on the Civanlar distribution network augmented with a ring ICT network shows that larger RTO primarily degrades duration-based indices (Availability and SAIDI), while occurrence-based indices (SAIFI and Failure Rate) change only when moving from instantaneous decision-making to nonzero RTO. The work advances CPPS reliability analysis by highlighting operator performance as a third reliability driver alongside electrical and communication components.","Electrical/communication component time-to-failure is sampled assuming a constant failure rate with exponential reliability: $R(t)=e^{-\lambda t}$ and inverse transform sampling $t=-\frac{1}{\lambda}\ln(U)$. Repair time is modeled as normal, $t_r\sim \mathcal{N}(\mu_r,\sigma_r^2)$, sampled via Box–Muller. Operator response time is modeled as $t_{RTO}\sim \mathcal{N}(\mu_{RTO},\sigma_{RTO}^2)$ and likewise sampled via Box–Muller, then added as an outage-duration contribution for customers affected during restoration decisions. Reliability indices computed include Availability $A=\frac{\text{time functioning}}{\text{time simulated}}$, SAIDI $=\frac{\sum_i U_i N_i}{\sum_i N_i}$, SAIFI $=\frac{\sum_i \lambda_i N_i}{\sum_i N_i}$, and “number of nines” $N9=-\log_{10}(1-A)$.","On the Civanlar network (simulated with $T=1000$ years and $N=1000$ runs), adding communication-network failures (vs. fully reliable comms, both with $\mu_{RTO}=0$) worsened indices: Availability fell from 3.386 to 3.203 “nines”, SAIDI rose from 0.457 to 0.953 h/customer-year, SAIFI rose from 0.152 to 0.318 int/customer-year, and Failure Rate rose from 1.200 to 1.831 failures/year. With comms failures present, increasing $\mu_{RTO}$ from 0 to 60 minutes increased SAIDI from 0.953 to 1.136 h/customer-year (about +19.2%) and decreased Availability from 3.2027 to 3.1506 nines (about 1.63% change in nines). For any $\mu_{RTO}>0$, SAIFI and Failure Rate shifted from 0.3178/1.8314 to about 0.3356/1.8998 and then remained essentially constant across larger $\mu_{RTO}$ values, consistent with these being occurrence-focused indices.","The authors state they only model operator decision-making time and do not model decision quality; the simulation assumes the restoration procedure always leads to the best configuration during contingencies. They also note the observed linear RTO effect is a consequence of the small test system, where failures are well separated and there are no multiple simultaneous failures; larger systems may exhibit chained effects when failures occur while operators are still deciding. They further mention results would differ for systems without automated switches, because manual switching/crew travel time would reduce the relative impact of operator decision time.","RTO and repair times are modeled as (unbounded) normal distributions, which can yield negative sampled times unless explicitly truncated—this can bias results if not handled. Component failures are treated with constant rates and independent two-state Markov behavior; real distribution and ICT assets may show aging, common-cause events (storms), and dependencies/cascades beyond the modeled coupling. The case study assumes feeders, fiber-optic cables, servers, and distribution feeders are “fully reliable,” which likely understates cyber/physical contributions and may inflate the apparent marginal impact of operator time. No real outage/SCADA dataset validation or calibration for $\mu_{RTO},\sigma_{RTO}$ is provided, limiting practical interpretability of the chosen RTO scenarios.","The authors propose incorporating the quality of the operator’s decision (not only decision time) into the simulation, noting it may lead to additional findings on human performance impacts. They also suggest applying the methodology to real CPPDS to better estimate reliability indices and help direct investment decisions for improving overall reliability.","A useful extension would be to adopt bounded/positive distributions (e.g., lognormal/gamma) or truncated normals for RTO and repair times and to quantify sensitivity to that modeling choice. Extending the framework to model correlated failures and extreme events (weather-driven outages, shared telecom dependencies) would improve realism for distribution reliability studies. Another direction is integrating operator workload/queueing (multiple concurrent contingencies) and DSS interventions as explicit policies to evaluate training/decision-support investments. Providing open-source implementation and benchmarking against established CPPS reliability models on multiple standard test feeders would strengthen reproducibility and generalizability.",1603.07775v2,https://arxiv.org/pdf/1603.07775v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:39:57Z
TRUE,System reliability|Other,Other,Complete lifetime data|Right-censored|Mixture of types|Other,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Other,Supplementary material (Journal/Publisher),http://www.zlib.net/pigz/|http://cran.r-project.org/package=ReliabilityTheory|http://www.louisaslett.com/HomomorphicEncryption/|http://doi.org/10.1111/risa.12228|http://arxiv.org/abs/1508.06574|http://arxiv.org/abs/1508.06845|http://www.i-like.org.uk|http://crypto.stanford.edu/craig/|https://www.R-project.org/|http://arxiv.org/abs/1602.01650,"The paper proposes a privacy-preserving protocol to evaluate a system’s survival curve when the system design is a trade secret and component manufacturers will not disclose full lifetime test data. It combines homomorphic encryption with the survival signature, which decomposes system reliability into a polynomial in (encrypted) survival-signature values and component-type count probabilities, enabling computation directly on ciphertexts. The protocol lets manufacturers locally infer component reliability (e.g., nonparametrically) and then multiply encrypted lookup-table entries to produce an encrypted system survival function over a time grid, which only the system designer can decrypt. A practical demonstration on an automotive braking system (4 component types) shows feasibility across globally distributed servers, with decrypted results closely matching the unencrypted baseline aside from small rounding error from integer encoding. The work advances system reliability practice by enabling full reliability evaluation under strong cryptographic privacy constraints for both structure and test data.","The survival signature is defined as $\Phi(\mathbf{l})=\left[\prod_{k=1}^K {M_k \choose l_k}^{-1}\right]\sum_{\mathbf{x}\in S_{\mathbf{l}}}\varphi(\mathbf{x})$, i.e., the average of the structure function over states with exactly $l_k$ working components of each type. The system survival function decomposes as $P(T_S>t)=\sum_{l_1=0}^{M_1}\cdots\sum_{l_K=0}^{M_K}\Phi(l_1,\ldots,l_K)\prod_{k=1}^K P(C_t^k=l_k)$ (under independence of types). The encrypted protocol updates a lookup table by homomorphically multiplying each encrypted $\Phi(\mathbf{l})$ entry by encrypted component-count probabilities: $\Xi^{(\Phi)}_{i,t}\leftarrow \Xi^{(\Phi)}_{i,t}\otimes \text{Enc}(k_p,\mu_\kappa(P(C_t^k=\Xi^{(l)}_{i,k})))$, then homomorphically sums rows to obtain encrypted survival values for each time point.","In the braking-system example with $K=4$ component types and a 100-point time grid on $[0,5]$, the end-to-end encrypted analysis across five globally distributed servers completed in 2 hr 18 min 38.4 s, with data transfers between parties ranging from under 7 minutes to over 20 minutes. The encrypted lookup table $\Xi^{(\Phi)}$ reached ~12 GB in-memory and 5.5 GB on disk (compressed), while the final encrypted output vector $\xi$ was 58.4 MB on disk. The decrypted survival curve closely matched the unencrypted baseline; the paper reports an empirical total variation distance of 0.029 attributable to integer-encoding/rounding error at precision $\kappa=5$. Key generation took 0.3 s and final decryption 8.6 s on the designer’s server.","The security model assumes parties are “honest but curious,” do not collude, and follow the protocol correctly; extending to malicious parties is noted as an open question. The authors also note that the system designer cannot validate the manufacturers’ component-reliability inference approach from private test data, motivating specification of a shared nonparametric method in the protocol. Practical homomorphic-encryption constraints are acknowledged (e.g., high computational cost, inability to do comparisons/division directly, and rounding error from integer encoding).","The protocol appears to leak meta-information such as the number of component types $K$, the counts $M_k$, and the evaluation time grid, which may still be commercially sensitive in some settings. It relies on independence across component types to factor probabilities as a product in the survival-function decomposition; correlated failures/common-cause effects would require modification. The scalability can become challenging because the lookup table has $\prod_k (M_k+1)$ rows, which can grow rapidly for larger systems. Communication overhead (moving multi-GB ciphertext tables) may dominate runtime in real supply chains with limited bandwidth or stricter latency constraints.","The paper explicitly identifies extending the protocol from the “honest but curious” setting to malicious parties (who may deviate from the protocol) as an open research question, requiring methods to dynamically establish trust in the final answer.","A valuable extension would be to incorporate dependence/common-cause failure models (e.g., shared frailty or copula-based component dependence) while retaining encrypted computability. Developing scalable variants that avoid the full $\prod_k (M_k+1)$ lookup-table enumeration (e.g., sparsity-aware computation or hierarchical aggregation) would broaden applicability to larger systems. A robust, standardized in-protocol component inference step (including censored data handling and uncertainty quantification) could reduce strategic/manipulation risk when manufacturers choose inference methods. Packaging the protocol as a maintained open-source workflow (with reproducible benchmarks and security parameter guidance) would improve adoption by practitioners.",1604.05180v1,https://arxiv.org/pdf/1604.05180v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:40:39Z
FALSE,NA,Other,Simulated only|Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Not provided,http://dippl.org|http://cdn1.sph.harvard.edu/wp-content/uploads/sites/1268/2015/07/nhefs_book.xlsx|https://github.com/probmods/webppl/issues/473,"The paper extends bidirectional Monte Carlo (BDMC) to evaluate the accuracy of MCMC-based approximate posterior inference algorithms that can be expressed as importance sampling over an extended state space (notably annealed importance sampling and sequential Monte Carlo). By running AIS forward (prior→posterior) and reverse (posterior→prior) on simulated data where an exact posterior draw is available, it derives an estimator whose forward/reverse log normalizing-constant bound gap upper-bounds (in expectation) the Jeffreys (symmetrized KL) divergence between the approximate-sample distribution and the true posterior. It introduces BREAD (Bounding Divergences with REverse Annealing), a protocol for making simulated-data BDMC diagnostics relevant to real datasets by fitting hyperparameters on real data, simulating data at those hyperparameters, and validating similar forward-chain behavior across real vs simulated data. The approach is integrated into WebPPL and Stan toolchains and demonstrated on toy discrete distributions, Bayesian regression, and matrix factorization, including using bound violations to uncover a WebPPL implementation bug. Overall, the work provides a scalar, bound-based measure of approximate inference quality for certain AIS/SMC-style samplers and a practical workflow for diagnosing inference reliability in probabilistic programming systems.","AIS weight update: $w_t \leftarrow w_{t-1}\,\frac{f_t(x_{t-1})}{f_{t-1}(x_{t-1})}$ with MCMC transition $x_t\sim T_t(\cdot\mid x_{t-1})$; geometric annealing path in Bayesian inference: $f_t(\theta,z)=p(\theta,z)\,p(y\mid\theta,z)^{\beta_t}$. BDMC produces a forward stochastic lower bound $\log \hat R$ and a reverse-chain stochastic upper bound $\log \hat R^{-1}_{\mathrm{rev}}$ on $\log(Z_T/Z_1)$; the bound gap $\hat B=\log \hat R^{-1}_{\mathrm{rev}}-\log \hat R$ satisfies $\mathbb E[\hat B]\ge J$, where $J= D_{\mathrm{KL}}(p_T\|q_{\mathrm{fwd}})+D_{\mathrm{KL}}(q_{\mathrm{fwd}}\|p_T)$ is the Jeffreys divergence between true target/posterior $p_T$ and approximate-sample distribution $q_{\mathrm{fwd}}$.","On 7×7 discrete toy targets, the proposed expected upper bound $B=\mathbb E[\hat B]$ tracked the true Jeffreys divergence $J$ well in harder multimodal settings: for HARD RANDOM with 1000 intermediates, $J\approx1.840$ vs $B\approx2.309$ (≈26% relative error), and for BARRIER, $J\approx1.085$ vs $B\approx1.184$ (≈9% relative error). For an easy random target, the bound was conservative: with 1000 intermediates, $J\approx0.00518$ while $B\approx0.0705$ (>10× larger), though still indicating good performance. In a Stan Bayesian regression example, forward AIS curves on real (NHEFS) vs simulated data showed similar convergence shapes, supporting the simulated-data proxy check in BREAD. In the fixed-hyperparameter initialization test, reverse-AIS upper-bound curves were indistinguishable when starting from 10, 100, 1000, or 10,000 MCMC steps, suggesting only a small number of steps was sufficient in that setup.","The paper notes BDMC/BREAD yields rigorous bounds only on simulated data because it requires an exact posterior sample to initialize the reverse chain. It also emphasizes the method is directly applicable only to inference algorithms that can be viewed as importance sampling over an extended state space (e.g., AIS/SMC), not arbitrary MCMC samplers. For real-world data, it acknowledges the validation step comparing real vs simulated forward-chain behavior is heuristic and that stronger validation is desirable.","The Jeffreys-divergence bound is in expectation; any single-run gap $\hat B$ can be noisy, and the paper does not fully characterize variance, confidence intervals, or stopping rules for practitioners. The approach depends on access to (or approximation of) exact posterior samples for reverse initialization; the proposed fixed-hyperparameter workaround may fail in hierarchical models with poorly identified hyperparameters or strong prior-data conflict. Practical integration details (e.g., computational cost overhead for reverse chains, robustness to autocorrelation in transitions, or sensitivity to annealing schedule design) are not systematically explored across a wide benchmark suite. The method measures global distributional discrepancy but does not provide localized diagnostics (which parameters/latent variables are problematic) without additional analysis.","The authors state they would prefer a stronger, more rigorous method for validating that inference behaves similarly on real and simulated datasets, and identify this as a topic for future work.","Develop principled statistical tests or calibration diagnostics to compare real vs simulated forward-chain behavior beyond qualitative curve-shape inspection, and quantify uncertainty on the inferred divergence bounds. Extend the framework to handle broader classes of MCMC (e.g., standard HMC/NUTS outputs) via coupling, regeneration, or other techniques that do not require explicit AIS/SMC extended-space representations. Study robustness to model misspecification and hierarchical hyperparameter uncertainty, potentially via Bayesian model averaging or posterior predictive checks integrated with BREAD. Provide standardized software implementations (packages) and benchmarks to assess computational overhead, tuning sensitivity (number of intermediates, annealing schedule), and practical decision rules for when inference is “good enough.”",1606.02275v1,https://arxiv.org/pdf/1606.02275v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:41:20Z
FALSE,Other,Bayesian|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Other,NA,None / Not applicable,Not applicable (No code used),NA,"Using Human Connectome Project resting-state fMRI data (N=461), the paper studies test–retest (intersession) reliability of resting-state functional connectivity (rsFC) estimates as scan length increases from ~3.6 to 28.8 minutes and as ICA parcellation granularity varies (Q=25–300). It proposes/uses an empirical Bayes shrinkage estimator that pulls subject-level correlations toward the group mean with connection-specific weights based on within-subject vs between-subject variance, and compares raw vs shrinkage rsFC using absolute percent error (APE) as the reliability metric. Results show longer scans improve reliability but intersession error remains substantial even near 30 minutes, and coarser parcellations (fewer/larger ICs) are generally more reliable. Shrinkage improves reliability for most connections, with roughly 30–40% improvement for scans under 10 minutes and about 10–20% improvement for 20–30 minute scans; within-network connections (e.g., default mode and motor) are more reliable than between-network connections. The paper also demonstrates that “end-point reliability” (comparing short segments to the full same-session estimate) substantially overestimates true intersession reliability because the compared estimates are not independent.","A measurement-error model is assumed for each connection: $W^{(\ell)}_{ij}(q,q') = X_i(q,q') + U^{(\ell)}_{ij}(q,q')$, with $X_i \sim N(\mu,\sigma_x^2)$ (between-subject variance) and $U \sim N(0,\sigma_{u}^{2(\ell)})$ (within-subject/noise variance). The empirical Bayes shrinkage estimator is $\tilde W^{(\ell)}_{ij}(q,q') = \lambda(q,q')\,\bar W^{(\ell)}_{\cdot j}(q,q') + (1-\lambda(q,q'))\,W^{(\ell)}_{ij}(q,q')$, where $\lambda(q,q') = \sigma_{u}^{2(\ell)}/(\sigma_{u}^{2(\ell)}+\sigma_x^2)$. Reliability is quantified by absolute percent error for intersession comparison: $\text{APE}^{(\ell)}_{\text{raw},i}(q,q')=\left|\frac{W^{(\ell)}_{i1}(q,q')-W^{(L)}_{i2}(q,q')}{W^{(L)}_{i2}(q,q')}\right|$ (and similarly using $\tilde W$ for shrinkage).","End-point reliability (within-session reference) substantially underestimates true intersession error; at $\ell=300$ volumes (~3.6 min), end-point error is ~25–30% lower than intersession error depending on ICA model order. Increasing scan length from 300 to 2400 volumes (~3.6 to 28.8 min) reduces omnibus intersession APE by ~30%, but intersession APE remains high (~60–100% depending on model order), indicating large session-to-session variability in rsFC. Shrinkage improves reliability for most connections: about 30–40% improvement for scans <10 minutes and about 10–20% improvement for scans of 20–30 minutes. Within-network connections (notably within default mode and motor networks) show higher reliability than between-network connections, and shrinkage tends to help broadly across connections (often more for less reliable edges, but also for some highly reliable ones).","The authors note that the Human Connectome Project protocol changes phase-encoding direction halfway through each visit, inflating single-session within-subject variance estimates and causing over-shrinkage, limiting evaluation of the proposed single-session shrinkage method on long segments. They also state they can only assess scan lengths up to ~30 minutes per visit in HCP, so benefits of shrinkage for longer scans are uncertain. They report results largely via median performance, which may obscure subjects who do not benefit, and their shrinkage is connection-specific but not subject-specific. They further acknowledge a modeling limitation: approximating across-session variance in true rsFC by within-session variance can induce under-shrinkage, and they only examine Pearson correlation as the connectivity metric.","Reliability is measured via APE relative to a single-visit raw estimate as “reference,” which itself contains estimation error; this conflates measurement error with true change and makes comparisons sensitive to small denominators when $W^{(L)}$ is near zero. The shrinkage model assumes normality and independence (across subjects/visits and in error terms) and treats each edge separately, ignoring dependence structure across connections that could inform multivariate shrinkage. The work is not in a reliability-engineering (failure-time/maintenance) sense; conclusions about “reliability” pertain to psychometric/test–retest reproducibility and may not transfer to engineering reliability metrics (e.g., hazard, MTBF). Practical reproducibility would benefit from shared code and fully specified implementation details (e.g., exact variance-estimation procedure in appendix for rsFC setting) to enable replication across datasets.","They suggest exploring subject-specific (or group-specific) degrees of shrinkage, noting that different subjects may have different rsFC reliability and could benefit from tailored shrinkage. They also propose further study of the relationship between intersession and intrasession variance in true rsFC to correct potential under-shrinkage when within-session variability is smaller than across-session variability. Additionally, they note that for state-level (rather than trait-level) analyses, the shrinkage method could be modified to target sampling variance only, preserving state-related differences. They also imply examining other connectivity metrics beyond Pearson correlation (e.g., partial correlation or coherence) as an extension.","A natural extension is to develop and evaluate robust reliability/shrinkage approaches that handle near-zero correlations (stabilized denominators) and alternative reproducibility metrics (e.g., ICC, concordance correlation, Fisher-z scale error) with uncertainty quantification. Extending shrinkage to a multivariate or graph-structured prior that borrows strength across edges (e.g., network-regularized Bayes) could improve estimation while respecting dependence. Testing robustness under common rsfMRI complications (autocorrelation, motion artifacts, site/session effects) and validating on non-HCP datasets with different acquisition protocols would strengthen generalizability. Providing an open-source implementation and standardized benchmarking across scan lengths/parcellations would improve practical uptake and reproducibility.",1606.06284v1,https://arxiv.org/pdf/1606.06284v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:42:17Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies compressive classification of high-dimensional signals drawn from a zero-mean Gaussian mixture model (each class supported on a low-rank subspace) using low-dimensional noisy linear measurements. It derives low-noise (σ^2→0) asymptotic bounds on the probability of misclassification of the MAP classifier via a Bhattacharyya/union upper bound, yielding an explicit decay exponent d depending on ranks of projected class covariances. Using this exponent, it provides sufficient conditions (upper bounds) on the minimum number of measurements M for “reliable classification” (misclassification probability driven to zero) under random sensing matrices and under designed measurement kernels. For random left-rotation-invariant measurements, it shows a phase transition at M = r_Σ + 1, while for designed measurements it provides constructions that can reduce required M (e.g., M=1 for two classes; M=min{L−1, r_Σ+1} for multiple classes) and characterizes regimes where one-vs-all designs are optimal. Simulations with synthetic data and real datasets (motion segmentation and face recognition) validate sharpness of the bounds and compare against IDA and mutual-information/Rényi-based designs.","Measurement model: y = Φx + n with n ~ N(0, σ^2 I). MAP rule: Ĉ = argmax_i p(y|C=i) p_i with p(y|C=i) Gaussian of covariance ΦΣ_iΦ^T + σ^2 I. Misclassification is upper-bounded by a Bhattacharyya/union bound \(\bar P_e = \sum_{i}\sum_{j\neq i} \sqrt{p_i p_j} e^{-K_{ij}}\) where \(K_{ij}=\tfrac14 \log \frac{\det(\Phi(\Sigma_i+\Sigma_j)\Phi^T+2\sigma^2 I/2)^2}{\det(\Phi\Sigma_i\Phi^T+\sigma^2 I)\det(\Phi\Sigma_j\Phi^T+\sigma^2 I)}\). Low-noise expansion: \(\bar P_e = g (\sigma^2)^d + o((\sigma^2)^d)\) with \(d = \min_{i\neq j} (2r_{ij}-r_i-r_j)/4\), where \(r_i=\mathrm{rank}(\Phi\Sigma_i\Phi^T)\), \(r_{ij}=\mathrm{rank}(\Phi(\Sigma_i+\Sigma_j)\Phi^T)\).","Random measurements: with probability 1, reliable classification in the low-noise regime is guaranteed with M = r_Σ + 1 measurements (Proposition 1). More generally, to ensure a target decay rate exponent d0 (with d0 < R/4), a sufficient number is M = \lfloor 2d0 + r_Σ \rfloor + 1 for random measurements (Proposition 2). Designed measurements (two classes): reliable classification can be achieved with M = 1 measurement by choosing Φ along a vector in N1 or N2 but not in N1∩N2 (Proposition 3), and achieving exponent d0 requires M = \lfloor 4d0 \rfloor + 1 (Proposition 4). Designed measurements (multi-class): reliable classification is guaranteed with M = min{L−1, r_Σ+1} using a one-vs-all style construction selecting rows from class null-spaces (Proposition 5). Synthetic and real-data simulations (motion segmentation; Yale B faces) show phase-transition behavior consistent with these bounds and comparable measurement requirements to IDA and MI/Rényi designs (Tables I, III, IV).","The authors note that their main analysis assumes the MAP classifier is provided with the true model parameters (priors and covariances), whereas in practice these are learned from training data, inducing model mismatch. They also acknowledge that many real signals are only “approximately” low-rank, not exactly low-rank, and that this affects perfect phase-transition behavior in real data; they model mismatch partly via added noise. They emphasize that extending the measurement bound analysis to explicitly incorporate mismatch between true and learned models is not immediate.","The theoretical guarantees are asymptotic (σ^2→0) and are derived for an upper bound (Bhattacharyya + union bound), not the exact MAP misclassification probability, so the required M may be conservative outside the low-noise regime. The core multi-class design relies on assumptions that class subspaces are independently drawn from continuous distributions on the Grassmann manifold and share equal dimension r_Σ, which may not hold in engineered systems with structured or correlated subspaces. Practical implementability is not fully addressed: the designed kernels require knowledge of class null spaces/covariances, which may be costly or unstable to estimate when N is large and training data are limited, and no software implementation details are provided.","The paper suggests that a proper derivation of the number of measurements required for reliable classification in practical settings would require an in-depth analysis that accounts for model mismatch due to learning from training data, including (i) expressions linking misclassification probability to true vs learned models and (ii) analysis of how compressive random/designed measurements affect the resulting phase transition. It points to existing mismatch analyses for non-compressive settings and implies extending their framework to the compressive setting as a direction for further work.","A useful extension would be to derive non-asymptotic (finite-σ^2) guarantees or tight approximations for the true MAP error (or alternative bounds) to better guide measurement selection in realistic SNR regimes. Another direction is to relax the equal-rank and independent-Grassmann subspace assumptions, handling heterogeneous class subspace dimensions, correlated subspaces, or approximately low-rank covariances with robustness guarantees. Developing self-starting/adaptive measurement designs that operate with uncertainty in Σ_i (Bayesian or distributionally robust formulations) and providing open-source implementations for reproducibility would also strengthen practical impact.",1607.02801v2,https://arxiv.org/pdf/1607.02801v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:42:55Z
TRUE,Failure mode analysis|Other,Other,Event/count data|Other,Not applicable,Transportation/logistics,Case study (real dataset),TRUE,Other,Not provided,http://ftp.sas.com/techsup/download/EMiner/TamingTextwiththeSVD.pdf,"The paper presents a text-mining-driven approach for early detection of emerging product reliability issues using unstructured after-sales data, primarily customer complaints and technician comments. It first extracts and organizes complaint themes via standard text mining steps (bag-of-words/TF-IDF representations and dimensionality reduction, including SVD) and topic identification in SAS Enterprise Miner. After topics are established, it monitors topic frequency over time using an expectation-based Poisson log-likelihood ratio (scan-statistic style) to flag statistically significant increases (emerging issues). A case study on 5,259 automotive complaints (1992–2014) demonstrates the workflow, including creation of custom topics and identification of an emerging “power steering locking” issue with a clear upward trend beginning around May 2008. The method is positioned as proactive reliability surveillance that can trigger earlier corrective actions (e.g., investigations/recalls), but it requires human/domain input to define and interpret topics.","Topic counts over a monitoring window are aggregated as $C_j=\sum_{t=1}^n C_{jt}$ and expected counts as $B_j=\sum_{t=1}^n B_{jt}$ based on a baseline topic rate. The expectation-based Poisson log-likelihood ratio is computed as $F_{jn}=(C_j\log(C_j/B_j)+B_j-C_j)$ when $C_j>B_j$, and $F_{jn}=0$ otherwise. Topics with $F_{jn}>1$ are treated as emerging topics.","In the automotive complaint case study (5,259 complaints), the first 36 months were used to establish baseline topic rates and subsequent monitoring was performed in three-month increments. A newly discovered topic related to power steering failure with locking (key terms including power/steer/lock/indicator/light/illuminate) was promoted to a custom topic for continued surveillance. The monitored Poisson likelihood ratio for “Power Steering Locking” showed a sudden upward trend beginning around May 2008, indicating an emerging reliability issue affecting a significant number of vehicles. The paper also illustrates trend plots for broader custom topics (e.g., Air Bags, Engine, Fuel System, Hazardous Conditions).","The authors state the method cannot be completely automated because human input is required to define stop lists, synonyms, and custom topics and to interpret discovered topics. They also note that building topics with symptom-indicative terms can be difficult when symptoms are new or domain expertise is insufficient, requiring manual examination of discovered topics.","The alert threshold $F_{jn}>1$ is presented without calibration for false-alarm control (e.g., in-control ARL, multiple-testing adjustment across many topics, or sensitivity to varying exposure/denominators), which can lead to unstable signaling in practice. The approach assumes Poisson count behavior and (implicitly) comparable reporting intensity over time; changes in complaint volume, reporting channels, or media attention could confound signals unless explicitly modeled. Topic modeling quality and assignment errors (documents mapped to multiple topics, synonym/stoplist choices) can materially affect detection, but robustness to these text-processing choices is not quantified. Implementation is demonstrated using SAS Enterprise Miner; reproducibility is limited because no code, parameter settings (beyond a few node choices), or benchmark comparisons against alternative detectors (e.g., CUSUM/EWMA on rates, Bayesian surveillance) are provided.","The authors suggest augmenting the Poisson log-likelihood statistic with a symptom-severity measure (e.g., average cost of repairs) to better prioritize detected issues for further investigation and root cause analysis.","Future work could formalize alerting by controlling false-alarm rates (e.g., selecting thresholds to target ARL, accounting for multiple topics, and modeling exposure with an offset for units-in-field and complaint volume). Extending the framework to handle overdispersion, autocorrelation, and reporting bias (e.g., negative binomial or state-space models) would improve robustness. More modern topic/embedding methods (e.g., LDA variants, transformer embeddings) and systematic validation of topic stability over time could reduce dependence on manual custom-topic curation. Providing open, reproducible implementations and broader comparative studies against established surveillance methods would clarify when this approach yields practical gains.",1607.07745v1,https://arxiv.org/pdf/1607.07745v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:43:26Z
TRUE,Degradation modeling|RUL prediction,"Stochastic process|Parametric (Weibull, etc.)|Simulation-based|Other",Degradation measurements|Simulated only,Not applicable,Manufacturing (general)|Pharmaceutical|Transportation/logistics|Other,Simulation study|Other,TRUE,R|None / Not applicable,Not provided,NA,"The paper proposes a statistical modeling framework for spatio-temporal degradation data observed over a spatial domain across time (e.g., surface quality degradation), representing degradation as the sum of (i) a spatial degradation generation process and (ii) a spatio-temporal propagation process. Propagation is modeled via a convolution (kernel smoothing) operator with a Gaussian kernel, inducing an anisotropic, space–time non-separable covariance structure; the model is linked to convection–diffusion–decay stochastic PDEs under special conditions. The framework nests common purely time-dependent degradation models as special cases when spatial propagation is removed (e.g., multivariate Wiener-type degradation). Parameters (including decay, propagation direction/speed, kernel spread, and spatial covariance parameters) are estimated via maximum likelihood using a computational trick that leverages a white-in-time transformed field to reduce complexity. The paper demonstrates model validation via variogram matching and normality checks, and uses simulation to approximate first-passage time (FPT) and first-passage location (FPL), enabling remaining useful life (RUL) style inference for spatio-temporal degradation.","The core model is additive: $Y(s,t)=G_\Delta(s,t)+Z(s,t)$ with $G_\Delta(s,t)=g_\Delta(s,t)+\varepsilon_\Delta(s,t)$. Propagation is defined by convolution: $Z(s,t)=\zeta_\Delta\{\omega_\Delta*Y(s,t-\Delta)\}$ where $\zeta_\Delta=\exp(-\lambda\Delta)$ and $\omega_\Delta(x)=\phi(x;\mu_\Delta,\Sigma_\Delta)$ is a Gaussian kernel with $\mu_\Delta=v\Delta$ and anisotropic $\Sigma_\Delta$ (rotated and scaled by parameters $\rho_1,\rho_2$). The induced spatio-temporal covariance is given (for lag $\jmath\delta$) by $\mathrm{cov}(Y(s_1,t_1),Y(s_2,t_2))=\sum_{i\ge 0}(\tilde\Psi_i*\Psi_{\jmath+i,t_2}*c_\delta)(d)+\mathbb{I}\{\jmath=0\}c_\delta(d)$, where $d=s_2-s_1$ and $c_\delta$ is the spatial covariance of $\varepsilon_\delta$.","Using a motivating 2D surface-degradation dataset (21×21 grid over 20 times), the Gaussian spatial covariance for the generation-noise field $c(\cdot)$ provided the best variogram and chi-square Q–Q fit versus Exponential and Matérn alternatives. Under the selected Gaussian $c(\cdot)$, ML estimates were reported as: $\hat\lambda=0.09$ (SE 0.012; 90% CI (0.071, 0.109)), propagation vector components $\hat v_1=-0.004$ (SE 0.018) and $\hat v_2=0.793$ (SE 0.039), and kernel spread parameters $\hat\rho_1=2.247$ (SE 0.117) and $\hat\rho_2=0.301$ (SE 0.024). The covariate effect estimate was $\hat\beta=1.251$ (SE 0.040; 90% CI (1.184, 1.317)). A simulation study (500 replicated datasets) showed small bias and reported MSEs for parameters on the order of $10^{-3}$ to $10^{-1}$ (and 3.58 for $\theta_2$) under the authors’ specified true values; additional simulations illustrated empirical RUL/FPT distributions and FPL hotspot regions.","The paper assumes the degradation propagation field is uniform in space and invariant in time (constant direction and speed), which is required for the stationarity approximation and the covariance structure derived. In the conclusion, the authors note this assumption may be violated when propagation is affected by dynamic environmental conditions. They also indicate that, in general, first-passage time distributions are not available in closed form and thus require numerical simulation for their spatio-temporal setting.","The worked example uses a confidential/unspecified real application, limiting reproducibility and external validation across domains with different sensing noise, boundary conditions, and spatial resolutions. The model relies on Gaussian assumptions (Gaussian kernel propagation and Gaussian spatial noise) and white-in-time generation noise; robustness to non-Gaussian measurement errors, heavy tails, or temporal dependence in the generation term is not fully explored. Practical implementation details (e.g., discretization choices, handling boundaries/edge effects in convolution, sensitivity to grid size, and identifiability between decay $\lambda$ and kernel spread parameters) are not deeply investigated. Despite discussing scalability, the approach can still be expensive for very large $N_s$ (spatial sites) without FFT/spectral acceleration or sparse approximations.",The authors suggest extending the framework to incorporate dynamic environments in which degradation propagation is not time-invariant (direction and speed vary with time) and may be influenced by time-varying covariates. They motivate this by pointing to recent work on time-dependent degradation under dynamic operating conditions and argue analogous extensions are important for spatio-temporal degradation.,"Developing fast approximate inference (e.g., FFT-based convolutions, sparse/low-rank Gaussian process approximations, or state-space/spectral methods) would better support truly large spatio-temporal datasets. A self-starting/online updating version for streaming condition-monitoring data could enable real-time FPT/RUL updating. Extending the model to handle nonstationary propagation fields (spatially varying $v(s)$ and diffusivity) and incorporating boundary conditions explicitly would improve physical realism for finite domains (e.g., pipes, pavements). Providing open-source software and benchmark datasets would substantially improve adoption and allow fair comparisons against alternative spatio-temporal GP/SPDE models.",1609.07217v2,https://arxiv.org/pdf/1609.07217v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:44:09Z
TRUE,Software reliability|Failure mode analysis|Other,"Stochastic process|ML-based|Parametric (Weibull, etc.)|Other",Sensor/condition monitoring|Event/count data|Mixture of types|Simulated only,Not applicable,Network/cybersecurity|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online failure/anomaly detection methodology for software-intensive systems where performance metrics exhibit quasi-seasonal trends and long-range dependence (LRD). It introduces a trend estimation approach based on the theoretically optimal maximum-likelihood filter for a polynomial drift observed in fractional Brownian motion noise, with a practical sliding-window implementation that estimates and corrects for the Hurst exponent. After detrending, the method forms standardized residuals and detects transient change-points using an ensemble of multiple “weak” sequential detectors aggregated via a logistic-regression classifier, trained by minimizing a differentiable surrogate of an average relative error rate objective. The approach is evaluated on Monte Carlo simulated datasets with injected transient shifts under fractional Gaussian noise, on the public Abilene backbone traffic dataset, and on a proprietary Yandex geoinformation-system request-rate dataset. Results show improved trend extraction versus EWMA and competitive or better change-point detection compared with EWMA/CUSUM and PCA/SSA baselines under violated i.i.d. assumptions and LRD effects, with PCA benefiting when additional pretraining history is available but at higher computational cost online.","The observation model is $X_t=\sum_{i=0}^n \theta_i\,\phi_i(t)+\sigma B_t^H$ (continuous time), specialized to a cubic polynomial drift with $\phi_i(t)=t^i$; the ML estimator is $\hat\theta_{\mathrm{ML}}=R_H(t)^{-1}\,\psi_t^H$ where $R_H$ and $\psi_t^H$ are defined via fractional-noise-specific kernels and a martingale transform $M_t^H$. In discrete-time implementation, local cubic fits are computed on windows and corrected after estimating $\hat H$ from standardized residuals. Change-points are modeled in the noise as $\eta^H(t)=\mu\,\mathbf{1}_{[\theta,\theta+\Delta t]}(t)+\sigma Z_t^H$, and detection operates on residuals $R_t=\sigma^{-1}(X_t-\hat X_t)$. The ensemble aggregation is logistic: $a_t=\sigma\big(\sum_{j=0}^p\sum_{k=1}^n \lambda_{kj}s^k_{t-j}-\lambda_0\big)$, thresholded to alarm.","On the ARTIFICIAL datasets (1000 train + 1000 test replications with injected transient shifts under fGn with $H=0.95$), the proposed trend filter achieves lower RRMSE than EWMA for both trend approximation and one-step-ahead forecasting (Table I: EWMA 7.84/7.34 vs Ours 5.72/3.06 in %). In precision–recall and average relative error-rate comparisons, the proposed ensemble-based detector is competitive with an (oracle-tuned) optimal CUSUM on ARTIFICIAL-EASY and outperforms the other baselines on ARTIFICIAL-HARD under equal-information conditions, while PCA can surpass it when given extra historical pretraining. On real data (Abilene traffic and Yandex request-rate series sampled at 5-minute intervals), the method produces smoother trend estimates and residuals that better match the transient-change model, and the ensemble statistic shows strong temporal alignment with labeled anomalies in illustrated examples.","The authors note that standard sequential detection assumptions (Gaussian i.i.d. pre/post-change with known parameters) are heavily violated due to local polynomial trend approximation error, trend-estimation error, unknown change signature, and modeling error from approximating real noise with fractional Brownian motion/fGn; these violations reduce the effectiveness of classical methods. They also state PCA-based methods can require retraining and are computationally expensive when performed online across many time series.","The approach relies on accurate detrending and reasonable estimation of the Hurst exponent; misspecification of LRD structure, non-Gaussian heavy tails, or strong nonstationary variance changes could degrade both filtering and detection. The ensemble requires labeled abnormal segments for training, which may be scarce or biased in operational reliability settings, and the paper does not fully characterize performance under label noise or concept drift. Comparisons include tuned baselines, but the “optimal CUSUM” comparison on simulated data may use oracle knowledge not available in practice (e.g., shift size/distributions), complicating fairness; similarly, real-data evaluation appears largely qualitative (figures) rather than reporting standardized detection-delay/false-alarm metrics across many series.",None stated.,"Develop self-starting/unsupervised or weakly supervised ensemble training to reduce dependence on labeled anomalies, and add mechanisms for concept drift and seasonality changes (adaptive windowing, time-varying $H$ and variance). Extend the method to multivariate correlated metrics with joint change-point localization and attribution, which is common in software-intensive system monitoring. Provide more rigorous real-world benchmarking (average detection delay at fixed false-alarm rate, per-service stratification) and release reference implementations to support adoption and reproducibility.",1609.07662v1,https://arxiv.org/pdf/1609.07662v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:44:52Z
FALSE,NA,Other,Other,Not applicable,Theoretical/simulation only,Other,TRUE,None / Not applicable,Not provided,NA,"The paper analyzes estimation of an unknown structured parameter \(\theta^*\) from nonlinear single-index observations \(y_i=f(\langle x_i,\theta^*\rangle)\) when the nonlinearity \(f\) is unknown. It develops a geometric framework that characterizes sample–computation tradeoffs for iterative algorithms (projected gradient descent, projected stochastic gradient, and a resampled proximal-gradient variant) by linking convergence rates to a “minimal sample size” based on Gaussian width (or Gaussian distance for penalized formulations). For projected gradient descent with Gaussian design, the authors prove linear convergence to a neighborhood of \(\mu\theta^*\), where \(\mu=\mathbb{E}[f(g)g]\), with error proportional to an effective-noise level derived from \(\sigma^2=\mathbb{E}[(f(g)-\mu g)^2]\) and \(\gamma^2=\mathbb{E}[g^2(f(g)-\mu g)^2]\). Similar (but weaker) geometric convergence bounds are provided for projected stochastic gradient, and additional results are given for a proximal-gradient method under a resampling assumption. Synthetic experiments (including 1-bit observations) corroborate that nonlinear observations behave like linear observations with matched effective noise, and an image example demonstrates recovery from quantized/subsampled transform coefficients using a denoiser-based proximal step.","The observation model is \(y_i=f(\langle x_i,\theta^*\rangle)\) and the baseline estimator is constrained least squares \(\min_\theta \tfrac12\|y-X\theta\|_2^2\ \text{s.t. }R(\theta)\le R\). Projected gradient descent uses \(\theta_{\tau+1}=P_K(\theta_\tau+\alpha X^T(y-X\theta_\tau))\) with \(\alpha\approx 1/n\), and the analysis introduces nonlinearity parameters \(\mu=\mathbb{E}[f(g)g],\ \sigma^2=\mathbb{E}[(f(g)-\mu g)^2],\ \gamma^2=\mathbb{E}[g^2(f(g)-\mu g)^2]\). The main PGD bound shows \(\|\theta_\tau-\mu\theta^*\|_2\) contracts by a factor \((\sqrt{8\kappa_R^2 n_0/n})^\tau\) plus a residual term on the order of \(\eta(\sigma\sqrt{n_0}+\gamma)/\sqrt{n}\), where \(n_0\) is a Gaussian-width-based minimal sample complexity.","For PGD, assuming Gaussian features and \(n\ge 8\kappa_R^2 n_0\), the iterates converge at a linear (geometric) rate to a neighborhood around \(\mu\theta^*\) with residual radius scaling like \(\eta(\sigma\sqrt{n_0}+\gamma)/\sqrt{n}\) (up to constants and the factor \(1/(1-\sqrt{8\kappa_R^2 n_0/n})\)). For projected stochastic gradient (PSGD) with convex regularizer and \(n>n_0\), an expected squared-error bound is shown with a contraction factor \(\big(1-\frac{(1-\sqrt{n_0/n})^2}{2p}\big)^\tau\) plus an asymptotic error term proportional to \(\eta^2\sigma^2/(1-\sqrt{n_0/n})^2\). Simulations for 1-bit nonlinear measurements indicate PGD’s behavior closely matches an equivalent noisy linear model with matched effective noise, and PSGD empirically reaches similar final error to PGD (suggesting the theorem’s residual term may be improvable). An image reconstruction from quantized/subsampled DCT coefficients reports PSNR \(\approx 27.5944\) with relative reconstruction error \(0.0757\) in the presented example.","For the proximal-gradient analysis, the paper notes the guarantees require additional modeling assumptions and are not as strong as those for projected gradient methods. The proximal-gradient theorem is proved under an impractical “resampling” setup (fresh mini-batches each iteration), which the authors acknowledge is not useful in practice but provides insight. The authors also remark that computational benefits of more samples can be offset when matrix–vector multiplies are not fast, since iteration cost grows with \(n\).","The main theory assumes i.i.d. Gaussian design matrices and (implicitly) independence/idealized randomness; extensions to correlated, heavy-tailed, or dependent features typical in engineering sensing/monitoring are not developed here. The PGD result assumes the constraint level is perfectly tuned to \(R(\mu\theta^*)\), which is generally unknown in practice and may affect robustness when misspecified. The treatment of the unknown nonlinearity is summarized via moments \((\mu,\sigma,\gamma)\) and a concentration function, but practical estimation/validation of these quantities (or robustness to violations such as heteroskedasticity) is not addressed. Empirical validation is limited and not benchmarked against a broad set of modern nonconvex/robust alternatives beyond illustrative comparisons.","The authors suggest proving “precise” statistical constants (e.g., showing the PGD error constant is exactly 1 times the effective-noise level as suggested by experiments) and tightening the PSGD residual term (closing an apparent \(\sqrt{n/n_0}\) gap). They propose studying parallel and lock-free stochastic schemes for nonlinear parameter estimation and characterizing their time–data tradeoffs. They also highlight improving guarantees for proximal gradient methods (removing extra assumptions and achieving optimal bounds) as an important direction.","Developing self-tuning or adaptive procedures that avoid needing \(R(\mu\theta^*)\) (and possibly estimate \(\mu\) online) would increase practical usability. Extending the analysis to non-Gaussian and dependent designs (e.g., sub-Gaussian, heavy-tailed, time-series/autocorrelated features) and to measurement noise in addition to unknown nonlinearity would broaden applicability. Providing open-source reference implementations and standardized benchmarks (including real datasets) would help assess practical performance and sensitivity to hyperparameters. It would also be valuable to derive finite-sample guarantees under model misspecification where \(f\) is not entrywise or the single-index assumption is violated.",1610.07108v1,https://arxiv.org/pdf/1610.07108v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:45:41Z
TRUE,Life distribution modeling|System reliability|RUL prediction|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Right-censored|Complete lifetime data|Mixture of types,Not applicable,Transportation/logistics|Other,Other,TRUE,R,Not provided,https://www.R-project.org/|http://edoc.ub.uni-muenchen.de/17059/|http://www.sipta.org/isipta11/proceedings/046.html|http://dx.doi.org/10.1007/978-3-319-40596-4_14,"The paper proposes a robust Bayesian (imprecise-probability) method for reliability assessment of complex systems under prior–data conflict when component failure data are scarce. Component lifetimes are modeled as Weibull with known shape parameter $\beta_k$ and unknown scale $\lambda_k$, using an inverse-gamma conjugate prior; robustness is achieved by using sets of priors indexed by ranges of hyperparameters $(n_k^{(0)},y_k^{(0)})$ rather than a single prior. The approach uses the survival signature to compute system reliability for arbitrary system layouts with multiple component types, combining posterior predictive distributions of the numbers of functioning components per type. Right-censored component observations from a running system are explicitly handled, enabling computation of conditional reliability beyond the current time $t_{now}$ and supporting remaining useful life (RUL) assessments. The central result is that prior–data conflict yields wider posterior reliability bounds (more cautious predictions), while agreement between prior and data yields narrower bounds; an automotive brake system example illustrates early/late failure scenarios and resulting system reliability imprecision changes.","System reliability is computed via the survival signature as $R_{sys}(t)=\sum_{l_1=0}^{n_1}\cdots\sum_{l_K=0}^{n_K}\Phi(l_1,\ldots,l_K)\prod_{k=1}^K P(C_t^k=l_k)$, where $C_t^k$ is the number of working components of type $k$ at time $t$. For Weibull lifetimes with known $\beta_k$ and inverse-gamma prior on $\lambda_k$, posterior updating with right-censoring gives $n_k^{(n)}+1=n_k^{(0)}+e_k+1$ and $n_k^{(n)}y_k^{(n)}=n_k^{(0)}y_k^{(0)}+(n_k-e_k)t_{now}^{\beta_k}+\sum_{i=1}^{e_k}(t_i^k)^{\beta_k}$, yielding a closed-form posterior predictive pmf for $P(C_t^k=l_k\mid\cdot)$ (Eq. 17). Robustness/prior–data conflict sensitivity is obtained by optimizing lower/upper system reliability over hyper-rectangles $\Pi_k^{(0)}=[\underline n_k^{(0)},\overline n_k^{(0)}]\times[\underline y_k^{(0)},\overline y_k^{(0)}]$ to produce bounds $\underline R_{sys}(t\mid t>t_{now})$ and $\overline R_{sys}(t\mid t>t_{now})$.","Using a single conjugate prior can mask prior–data conflict: in an illustrative example with prior mean lifetime 9 weeks (via $y^{(0)}=103.13$, $\beta=2$, $n^{(0)}=2$) and two surprisingly early failures $t_1=1,t_2=2$, the posterior mean corresponds to 6.44 weeks while the posterior spread shrinks (posterior sd 60.99 vs prior sd 145.85), creating false certainty. With sets of priors (e.g., $y^{(0)}\in[103.13,154.06]$, $n^{(0)}\in[2,5]$), posterior sets remain wide under conflict but contract under agreement (demonstrated by contrasting $(1,2)$ vs $(10,11)$ observations). In the automotive brake system example (4 component types, survival signature), observations aligned with prior expectations yield noticeably narrower posterior component and system reliability bounds than the prior, while surprisingly early or late failures yield wider posterior bounds (largest for late-failure scenario). System-level results show posterior reliability is 1 at $t_{now}$ and then drops faster than the prior (aging effect), with imprecision plots highlighting increased uncertainty under conflict and reduced uncertainty when data agree with prior beliefs.",None stated.,"The method assumes Weibull lifetimes with known shape parameters $\beta_k$ and independence between component types; mis-specification (unknown/variable shapes, dependence, or autocorrelation in operating conditions) could materially affect bounds. Prior imprecision is restricted to rectangular sets in $(n^{(0)},y^{(0)})$, which may not capture more structured expert beliefs; results can be sensitive to the chosen set shape/limits. Practical deployment may require careful computational handling for large $K$ or large $n_k$ (survival signature summations and repeated box optimizations over time), and the paper does not report runtime/scalability benchmarks or provide released code.","The authors suggest extending the model to also reflect very strong agreement between prior and data, exploring other component lifetime distributions within canonical exponential families using analogous conjugate-set constructions, and analyzing effects of replacing failed components on system reliability bounds. They also propose jointly estimating/handling uncertainty in $\beta_k$ (e.g., via discrete priors) together with sets of priors for $\lambda_k$. Another stated direction is incorporating common-cause failures by combining their approach with robust common-cause modeling frameworks and exploring more general (non-rectangular) prior-parameter set shapes.","Develop self-starting/online implementations that update bounds sequentially with streaming condition-monitoring data and quantify computational costs for real-time RUL use. Add robustness to dependence across component types and shared environmental covariates (frailty/shared random effects), and assess sensitivity to informative censoring. Provide publicly available software with standardized benchmarks, plus guidance on eliciting prior sets from domain experts and on selecting prior-set shapes that control conflict sensitivity without over-widening bounds.",1610.07222v1,https://arxiv.org/pdf/1610.07222v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:46:23Z
FALSE,NA,"Stochastic process|Parametric (Weibull, etc.)|Other",Event/count data|Right-censored|Mixture of types|Other,Not applicable,Service industry|Other,Exact distribution theory|Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Personal website,http://btabibian.com/projects/reliability,"The paper proposes a temporal point process framework to infer (i) the evolving “reliability” of online knowledge items (e.g., Wikipedia articles, Stack Overflow questions) and (ii) the “trustworthiness” of their sources, using the timing of additions and subsequent evaluations (refutations or verifications). Statement additions are modeled as a counting process whose intensity decreases as an item becomes more reliable, while statement evaluations are modeled via a survival process whose hazard depends on item intrinsic reliability and source-specific parameters. Model parameters are learned by a jointly convex maximum-likelihood formulation that decomposes into parallel subproblems, enabling estimation at large scale. Experiments on synthetic data show parameter recovery improves with more items, and real-data studies on Wikipedia and Stack Overflow show improved prediction of refutation/verification events versus baselines while yielding interpretable source and item measures. Overall, this is reliability in the information-quality/trust sense rather than reliability engineering of physical systems.","The statement addition intensity for item $d$ is $\lambda_d(t)=\sum_j \phi_{d,j}k(t-t_j)+\sum_{e_i\in \mathcal{H}_d(t)} \mathbf{w}_d^\top\gamma_{s_i}\, g(t-\tau_i)$, where the second term captures effects of past evaluations (refutation increases additions; verification inhibits). The statement evaluation (survival) intensity is $\mu_i(t)=(1-N_i(t))\left[\sum_j \beta_{d,j}k(t+t_i-t_j)+\mathbf{w}_d^\top\alpha_{s_i}\right]$, combining item intrinsic reliability and source trustworthiness. Parameters are fit by maximizing a decomposable MLE objective over additions, evaluations, and source-topic popularity (Eq. 11), using standard point-process likelihood $\sum_i\log \lambda(t_i)-\int_0^T\lambda(t)dt$.","On Wikipedia removal prediction, the proposed model achieves AUC values above 0.69 across tested horizons and outperforms baselines that use only intrinsic item reliability or only source effects, with performance improving for longer horizons. On Stack Overflow, selecting the most likely accepted answer based on predicted verification times yields success rates above 0.47 and consistently beats user-only baselines; the random baseline is reported as 0.41. Synthetic experiments show RMSE for shared source parameters decreases as the number of knowledge items increases, while item-specific parameter errors flatten once source parameters are well-estimated. The method scales to about 19 million Wikipedia events and about 1 million Stack Overflow events with ~4 hours runtime on a 10-core machine (64GB RAM).",None stated.,"The work’s “reliability” constructs are specific to online content dynamics (edit/addition and refutation/verification timing) and do not map directly to engineering reliability measures (failure probability, lifetime distributions) for physical assets. The framework assumes the specified intensity forms (kernel mixtures, topic-weight linear effects) are adequate and that event histories sufficiently capture confounding factors (e.g., moderation bursts, external attention shocks), which may violate model assumptions. Evaluation is largely predictive/interpretive and does not provide robustness checks under strong temporal dependence misspecification beyond the chosen kernels and regularization. Code is promised via a website but may not be archived/reproducible long-term, and full implementation details (hyperparameters, preprocessing) may affect replicability.","The authors suggest extending the model to allow source trustworthiness to vary over time, supporting non-binary evaluation signals (partial refutations/ratings), incorporating trustworthiness of the evaluators (refuters/verifiers), and reducing parameters by clustering sources and knowledge items. They also propose applying the approach to other repositories (e.g., Quora, GitHub) and using inferred trustworthiness for credit assignment and identifying trustworthy editors/users.","A valuable extension would be to incorporate exogenous covariates capturing attention/traffic shocks (news cycles) to separate popularity-driven edits from reliability-driven edits. Another direction is to develop nonparametric or Bayesian versions with uncertainty quantification for trustworthiness/reliability estimates, especially for sparse sources/items. Robustness to bots, coordinated campaigns, and adversarial manipulation could be studied via anomaly-aware intensities or change-point models. Finally, releasing a maintained open-source package (with end-to-end preprocessing) would improve adoption and reproducibility.",1610.07472v3,https://arxiv.org/pdf/1610.07472v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:47:00Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies how to learn a stable low-dimensional manifold from streaming high-dimensional data using Isomap, arguing that only an initial fraction of the stream is needed to learn a high-quality manifold. It defines “collective error” measures for Isomap based on Procrustes analysis: a direct ground-truth Procrustes error when true low-dimensional coordinates are known, and a reference-sample Procrustes error that works without ground truth by comparing embeddings of a shared reference set. Empirical results on a proposed Euler Isometric Swiss Roll dataset and on MNIST/Corel benchmarks show that these errors decrease rapidly and then stabilize after a transition point (on the order of ~1.5k to ~2–2.5k samples in their experiments). Using this transition point, the authors propose a streaming out-of-sample extension algorithm (S-Isomap) that maps new samples to the learned manifold without recomputing the full geodesic matrix or eigendecomposition. Experiments indicate S-Isomap achieves similar embedding quality to full Isomap after the transition point while substantially reducing runtime, with per-point cost depending only on the batch size rather than total stream length.","The core error metric uses Procrustes distance: $d_{Proc}(A,B)=\min_{R,t,s}\|sRB+t-A\|_F$. With ground-truth embedding $Y$ and Isomap embedding $\hat{Y}$, the direct error is $\epsilon=d_{Proc}(Y,\hat{Y})$. Without ground truth, a reference-sample error is defined by choosing a shared reference set $F$ and two sampled sets $R_1,R_2$, learning two embeddings $\hat{f}^{-1}_1,\hat{f}^{-1}_2$, and computing $\epsilon=d_{Proc}(\hat{f}^{-1}_1(F),\hat{f}^{-1}_2(F))$; this error is shown empirically to have similar asymptotic behavior to the direct error.","On the Euler Isometric Swiss Roll, the direct Procrustes error decreases quickly and begins converging at roughly ~1,500 samples, suggesting a stable-manifold transition point. For MNIST digit subsets and the Corel Image dataset, the reference-sample Procrustes error similarly stabilizes when sample sizes reach about ~2,000–2,500 points. Using S-Isomap, the Procrustes error of the combined (batch + streamed) embedding closely tracks the error from the batch-only manifold after the transition point, indicating little added error from streaming mappings. Runtime experiments (log-scale plots) show S-Isomap is substantially faster than running full Isomap on all points, and cumulative streaming runtime grows approximately linearly with stream size once the batch is fixed.","The conclusions about convergence/transition rely on the assumption of sufficiently dense and (implicitly) uniform sampling of the underlying manifold; the paper notes stability is expected “under the assumption of uniform sampling.” The direct Procrustes error requires access to low-dimensional ground truth, which is often unavailable, motivating the reference-sample method instead. The reference-sample approach introduces a design choice of a fixed reference set $F$ and sampling scheme, which may affect measured error (noted implicitly by the protocol description rather than as a formal limitation).","The work is not reliability engineering; “reliable” refers to stable/accurate manifold learning rather than system/component reliability, so its metrics do not translate directly to failure-time, degradation, or maintenance decision contexts. The method assumes a stable manifold exists and remains stationary; concept drift or evolving manifolds in real streams could break the transition-point logic and degrade S-Isomap mappings. The evaluation is primarily based on embedding-error curves and runtime; downstream task performance (e.g., classification/regression impact) and robustness to noise/outliers/autocorrelation in streams is not comprehensively quantified. Implementation details (software, parameter tuning, and reproducibility artifacts) are not provided here, which limits practical adoption and independent verification.",None stated.,"Extend the transition-point/error monitoring to nonstationary streams with concept drift (detecting when the manifold changes and triggering re-batching/relearning). Develop robust variants of the Procrustes-based error metrics and S-Isomap mapping under noise, outliers, missing data, and temporally correlated streaming observations. Provide theoretical guarantees for the reference-sample error (e.g., concentration bounds and sample complexity for stability detection) and for S-Isomap approximation quality. Release an open-source implementation (e.g., Python/R) with benchmarks to support reproducibility and adoption.",1611.04067v2,https://arxiv.org/pdf/1611.04067v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:47:29Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Bayesian|Other",Degradation measurements|Other,Not applicable,Manufacturing (general)|Semiconductor/electronics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Package registry (CRAN/PyPI),http://CRAN.R-project.org/package=ADDT,"This paper/chapter reviews and compares three statistical approaches for estimating a material Thermal Index (TI) from accelerated destructive degradation test (ADDT) data, where temperature is the accelerating variable and each unit is destroyed at measurement. The three methods are: (i) the UL 746B traditional two-step least-squares approach using per-temperature polynomial fits and Arrhenius temperature–life regression, (ii) a fully parametric maximum-likelihood degradation-path model (with Arrhenius acceleration and within-batch correlation), and (iii) a semiparametric model using monotone splines for the baseline degradation path plus an Arrhenius temperature effect. It derives TI estimators for each method in a common framework where TI is the temperature corresponding to a target life (often 100,000 hours) at a specified failure threshold proportion (often 50% of initial strength). The chapter includes an application to the Adhesive Bond B dataset and extensive simulation studies under correct specification and misspecification to compare bias/SD/RMSE across design scenarios (numbers of temperatures and time points). Overall, the traditional method can discard data if the threshold is not reached and tends to perform worse with sparse designs, while parametric and semiparametric methods use all data and show comparable performance, with semiparametric offering flexibility via monotone splines.","Arrhenius temperature transform uses $x=1/(A+273.16)$ (equivalently $h(A)=-11605/(A+273.16)$). TI is defined by solving $m(x_d)=t_d$ and converting $x_d$ back to temperature: $R=1/m^{-1}(t_d)-273.16$. Traditional/parametric/semiparametric approaches all yield an Arrhenius life line $\log_{10} m(x)=\beta_0+\beta_1 x$ and TI formula $R=\beta_1/(\log_{10}(t_d)-\beta_0)-273.16$ (as shown in Eqs. (5), (9), (15)). Parametric degradation path example: $\mu(t;x)=\alpha\,[1+(t/\eta(x))^\gamma]^{-1}$ with $\eta(x)=\exp(\nu_0+\nu_1 x)$ and Gaussian errors with within-batch correlation; semiparametric model uses $\mu(t;x)=g(t\exp(-\beta(x-x_{\max}));\gamma)$ where $g(\cdot)$ is a monotone spline baseline at the highest temperature.","In the Adhesive Bond B example (with $t_d=100{,}000$ hours and failure threshold $p=50\%$), the estimated TI values are 39°C (traditional), 33°C (parametric ML), and 34°C (semiparametric). In simulations with true TI 181°C, increasing the number of temperature levels (3→5) and measurement times (4→5) reduces SD/RMSE for all methods; the traditional method typically has higher RMSE when designs are sparse. Under correct parametric specification (Setting I), representative RMSEs include scenario 1 (3 temps, 4 times): TM 18 vs PM 9 vs SPM 9, and scenario 8 (5 temps, 5 times): TM 4 vs PM 4 vs SPM 4. Under model misspecification (Setting II), PM and SPM remain comparable and generally outperform TM; e.g., scenario 1 RMSE: TM 17 vs PM 11 vs SPM 12, scenario 8 RMSE: TM 6 vs PM 4 vs SPM 5.","The authors note practical and methodological limitations mainly for the traditional method: it cannot use temperature levels where degradation has not reached the failure threshold (so data may be discarded), it fits the Arrhenius life line using only a few temperature points (typically ~4), it may yield non-monotone estimated failure times across temperatures due to polynomial flexibility and noise, and its two-step procedure makes uncertainty quantification challenging. They also state the semiparametric method is the most computationally intensive, with the parametric method intermediate in computational cost. For parametric methods, an appropriate functional form for the degradation path $\mu(t;x)$ must be chosen.","All three approaches rely on the Arrhenius acceleration structure (linear in transformed temperature for log-life), which may be inadequate if the mechanism changes across temperatures or exhibits non-Arrhenius behavior. The error model assumes Gaussian errors and a simple exchangeable within-batch correlation structure, which may be misspecified for heteroscedastic degradation variability or more complex batch/lot effects. TI is tied to a fixed failure threshold proportion (often 50%), but sensitivity to threshold choice and uncertainty in the initial level (baseline) is not deeply explored; in practice, threshold selection can dominate TI differences. The chapter’s comparisons focus on TI point-estimation performance (bias/SD/RMSE) and do not fully address robustness to outliers, censoring/LOQ issues, or practical design optimization beyond a limited scenario grid.",None stated.,"Develop robust and/or self-starting variants that relax distributional assumptions (non-normal errors, heteroscedasticity, heavier tails) and incorporate richer random-effects structures for batch/lot and specimen-to-specimen variability. Extend the semiparametric framework to allow non-Arrhenius or piecewise acceleration models to accommodate mechanism changes at high temperatures. Provide broader guidance on optimal ADDT design (temperature/time allocation and sample sizes) explicitly targeting TI precision under the semiparametric model. Release reproducible scripts/vignettes for the simulation study and examples (beyond the R package implementation) to facilitate benchmarking and extension by practitioners.",1611.07412v1,https://arxiv.org/pdf/1611.07412v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:48:16Z
FALSE,NA,ML-based|Other,Event/count data|Sensor/condition monitoring|Mixture of types|Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable|Other,Not provided,http://www.youtube.com/t/rss_feeds|https://support.google.com/youtube/troubleshooter/2991876|http://www.bing.com/videos/browse,"This paper studies video popularity prediction from the perspective of an operating company (e.g., a video search engine) that may not have reliable access to a hosting provider’s API. It proposes using three evidence sources—(1) hosting-provider API data, (2) internal search and browsing logs, and (3) publicly crawled Web traces such as embeds and links—to predict current and near-future view counts (cumulative and daily), often in log-transformed form. Models include gradient-boosted decision trees and a Linear Influence Model (LIM) that leverages non-aggregated host-level embed/link events; evaluation uses RMSE (normalized/averaged over days 1–14) and NDCG@100 for ranking popular videos. Experiments on large-scale YouTube datasets (millions of videos) show Web embed/link data plus internal logs improve over API-only prediction, and that Web/log data can substitute for missing API components and can even outperform API data for ranking the most popular videos. The work’s main contribution is demonstrating that publicly observable web traces and internal logs can meaningfully replace or supplement hosting-provider data when API data are delayed, noisy, or unavailable.","Targets are defined as cumulative and daily view counts (and their log transforms): $\Theta=\{\mathrm{Views}[c],\mathrm{Views}[d],\log(\mathrm{Views}[c]),\log(\mathrm{Views}[d])\}$ with $\log(x)=\log_2(x+1)$. The learning objective is expressed as choosing an optimal predictor within a model class to minimize a loss/metric (Eq. 1), and performance is primarily measured by $\mathrm{RMSE}$ (Eq. 3) and its normalized variant $\mathrm{nRMSE}(\Psi,t_c;\theta,t_t)=\mathrm{RMSE}(\Psi,t_c;\theta,t_t)/\mathrm{RMSE}(\mathrm{BASE.avg},t_c;\theta,t_t)$. The LIM models host influence via per-host influence functions $I_h(t)$ (number of induced views $t$ days after host infection via an embed/link), learned via a least-squares formulation over sparse matrices.","Using all sources (ALL = API+WEB+LOG) improves average normalized RMSE vs API-only by about 2.1–2.4% for log targets and about 4.3–10.7% for non-transformed targets over days 1–14 (Table 4). For ranking, ALL improves NDCG@100 over API by +38% (daily, non-transformed) and +53% (cumulative, non-transformed); Web+Log alone also outperforms API for cumulative ranking (+36% NDCG@100 non-transformed) (Table 6). Ablation shows removing user-feedback API features hurts most (+27.6% log-cumulative RMSE), but adding Web/Log partially recovers performance; other API groups (temporal context, static video properties, social environment) can be removed and replaced with little/no loss (Table 5). The paper also reports that Web/Log features can compensate for missing API starting around day 8 for predicting exact view counts, and that non-aggregated Web features via LIM provide measurable gains over aggregated Web features for some targets.",None stated.,"This is not a reliability engineering study; results are specific to video popularity dynamics and the availability/quality of Yandex internal logs and its crawler coverage, which may not generalize to other organizations. The boosted-tree implementation and MapReduce/LIM training are described as proprietary, limiting reproducibility and making it hard to assess sensitivity to hyperparameters, feature leakage, or platform-specific biases. Evaluation focuses on a specific period and YouTube-centric data collection; shifts in platform behavior (API deprecations, counting rules, anti-spam measures) could change feature utility over time.","The authors propose extending feature sources by using APIs of other operating companies, training different predictors for different topical video categories, and reframing the task as classification (e.g., predicting whether a video is among the most/least popular) instead of RMSE-optimized regression. They also suggest using online user feedback from a displayed ‘most popular’ list to adjust predictions in real time.","Provide a fully reproducible pipeline (open feature extraction, model code, and splits) and benchmark against additional modern baselines (e.g., temporal deep models or calibrated count models) to validate robustness. Study domain shift and concept drift explicitly (changes in API behavior, embed ecosystems, bot/abuse filtering) and evaluate how models degrade over time. Explore privacy-preserving or less organization-specific variants that do not require proprietary search/browsing logs, to improve portability to other operating companies.",1611.09083v1,https://arxiv.org/pdf/1611.09083v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:48:54Z
FALSE,NA,ML-based|Other,Other,Not applicable,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper presents the first dimension-efficient polynomial-time algorithms for learning a single Rectified Linear Unit (ReLU) function under the Reliable Agnostic learning model, allowing arbitrary (adversarial) labeling noise. The method reduces learning to convex optimization over low-degree polynomial approximations of the ReLU, implemented implicitly via an RKHS using a multinomial kernel, and employs a dual-loss + clipping/thresholding strategy to control false positives while remaining competitive on positive-labeled points. The main result achieves runtime roughly $2^{O(1/\varepsilon)}\,\mathrm{poly}(n)$ for distributions over the unit sphere and for losses that are convex, bounded, Lipschitz (and typically monotone). The paper also extends the approach to learn constant-depth ReLU networks, and provides related applications (convex piecewise-linear regression, noisy polynomial reconstruction) plus hardness results over the Boolean hypercube. Overall, it advances learning-theory foundations for ReLUs but is not about reliability engineering of physical/engineered systems.","Reliable-learning objectives are defined as $L_{=0}(h;D)=\Pr_{(x,y)\sim D}[h(x)\neq 0\wedge y=0]$ (false-positive rate on zero labels) and $L_{>0}(h;D)=\mathbb{E}_{(x,y)\sim D}[\ell(h(x),y)\,\mathbf{1}(y>0)]$ (loss on positive-labeled points). The learning algorithm solves a convex program in an RKHS induced by the multinomial kernel $\mathrm{MK}_d(x,x')=\sum_{j=0}^d (x\cdot x')^j$, optimizing coefficients $\alpha$ with constraints $\alpha^T K \alpha\le B$ and (for reliability) $\sum_j \alpha_j\mathrm{MK}_d(x_j,x_i)\le \varepsilon$ for samples with $y_i=0$. The final predictor is a clipped/thresholded version of $f(x)=\sum_i \alpha_i^*\mathrm{MK}_d(x_i,x)$: output 0 if $\mathrm{clip}_{0,1}(f(x))\le 2\varepsilon$, else output $\mathrm{clip}_{0,1}(f(x))$.","Theorem 1.3 / Theorem 3.2: ReLU$(n,1)$ is reliably learnable (for convex, monotone, bounded, Lipschitz losses on $[0,1]$) in time polynomial in $n$, $\log(1/\delta)$, and $2^{O(L/\varepsilon)}$; in particular it is polynomial-time for accuracies down to $\varepsilon=\Theta(L/\log n)$. Theorem 1.5: depth-2 ReLU networks with $k$ hidden units are reliably learnable in time $2^{O(\sqrt{k}/\varepsilon)}\,\mathrm{poly}(n)$. Theorem 1.6 / Theorem 3.5: bounded-weight degree-$d$ polynomials on the sphere are agnostically learnable in time $\mathrm{poly}(n,d,B,1/\varepsilon)$ under similar loss assumptions. Hardness: reliably learning ReLUs over $\{0,1\}^n$ in $g(\varepsilon)\,\mathrm{poly}(n)$ time would imply polynomial-time learning of sparse parities with noise.","The authors state that their runtime dependence on accuracy is exponential, $2^{O(1/\varepsilon)}$, and explicitly leave open the problem of improving the dependence on $\varepsilon$ (they have “no reason to believe” it is optimal). They also note that their key positive results focus on distributions over the unit sphere (and similarly bounded domains), while learning over Boolean domains encounters hardness barriers. They further remark that monotonicity of the loss is used due to clipping/thresholding for reliability; handling non-monotone losses can worsen sample complexity.","The work is theoretical and does not provide empirical validation on real datasets or practical implementation guidance, so practical performance and numerical stability of the convex programs are unclear. The reliable-learning formulation is about controlling false positives for zero labels in a learning-theory sense, not about engineering reliability metrics (failure rates, hazard models), so direct applicability to reliability engineering is limited. Computationally, solving the kernelized convex programs may be heavy for large sample sizes despite polynomial-time guarantees, and the constants hidden in $2^{O(1/\varepsilon)}$ and kernel computations could be prohibitive. The results assume i.i.d. samples and do not address common real-world issues such as dependence/autocorrelation or distribution shift.","They explicitly leave open improving the dependence of the main learning algorithm on $1/\varepsilon$ (seeking better than $2^{O(1/\varepsilon)}$). They also discuss examining scenarios (especially in standard agnostic learning) where the exponential dependence on the loss Lipschitz constant $L$ can be avoided or reduced. More broadly, they frame as open the quest for sharper complexity bounds for learning ReLUs compared to known lower bounds for threshold functions in related settings.","Developing self-contained software and scalable optimization methods (e.g., stochastic/online variants, approximate kernel methods) would help assess real-world feasibility of the proposed convex programs. Extending the reliable-learning guarantees beyond the unit sphere/ball to more general domains or to distribution shift settings would broaden applicability. Empirically benchmarking against modern gradient-based training for ReLU networks could clarify when the theory-driven approach is competitive. Robust variants that handle correlated samples or heavy-tailed data, and tighter lower bounds/complexity results to determine whether $2^{O(1/\varepsilon)}$ is inherent, would strengthen the theoretical picture.",1611.10258v1,https://arxiv.org/pdf/1611.10258v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:49:41Z
FALSE,Other,ML-based|Nonparametric/Semi-parametric|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Case study (real dataset)|Other,TRUE,Other,Not provided,www.mlnl.cs.ucl.ac.uk/pronto,"The paper develops and evaluates a deep-learning (3D convolutional neural network) approach to predict ‘brain-predicted age’ from T1-weighted MRI, using both pre-processed tissue volume maps (grey matter, white matter) and minimally processed ‘raw’ images, and compares performance to Gaussian Process Regression implemented in PRoNTo. Using a large multi-source cohort (BANC, N=2001), the CNN achieves high age-prediction accuracy (best MAE 4.16 years with GM; raw MAE 4.65 years), broadly comparable to GPR on processed maps (e.g., GM MAE 4.66 years), while GPR performs poorly on raw images. The study then demonstrates external validity by estimating heritability in a female twin sample (N=62), finding substantial heritability across models and inputs (h^2 roughly 0.50–0.84 depending on age correction and input). It also quantifies measurement reproducibility using intraclass correlation coefficients for test–retest (within-scanner; N=20) and multi-centre (between-scanner; N=11) settings, showing high within-scanner reliability (ICC ~0.90–0.99) and more variable between-scanner reliability (ICC as low as ~0.51–0.77 for some inputs, especially raw/WM). The key contribution is showing accurate, fast inference of brain age directly from minimally processed MRI, while characterizing reliability and heritability of the derived biomarker for research/clinical translation.","CNN is trained to predict a scalar age from a 3D MRI volume by minimizing mean absolute error (MAE) via stochastic gradient descent with momentum; the network consists of repeated 3D convolution blocks followed by a final fully connected layer producing the age estimate. Heritability is computed from AE structural equation models as $h^2 = \frac{a^2}{a^2+e^2}$, where $a$ and $e$ are the additive genetic and unique-environment path coefficients. Reliability is assessed with intraclass correlation coefficient ICC[2,1] (Shrout & Fleiss) on Brain-PAD, where Brain-PAD = (brain-predicted age) − (chronological age at scan).","On the BANC test set (N=200), CNN achieved MAE 4.16 years (r=0.96) using GM maps and MAE 4.65 years (r=0.94) using raw images; GPR achieved MAE 4.66 (r=0.95) with GM maps but much worse performance on raw images (MAE 11.81; r=0.57). Heritability estimates (AE models) were high: unadjusted $h^2$ ranged from 0.62–0.84 for CNN and 0.64–0.82 for GPR depending on input; with age correction, CNN ranged 0.50–0.66 and GPR ranged 0.55–0.64. Within-scanner Brain-PAD reliability was high (CNN ICC 0.90–0.97 for GM/WM/GM+WM and 0.94 for raw; GPR ICC 0.96–0.99 across inputs). Between-scanner reliability was more variable (CNN ICC 0.83–0.85 for GM/GM+WM, 0.51 for WM, 0.66 for raw; GPR ICC 0.96 for GM, 0.92 for GM+WM, 0.77 for WM, 0.56 for raw).","The authors note the heritability sample is small, especially for dizygotic twins, and includes only females, limiting generalization (e.g., to males) and precision despite reporting confidence intervals. They also state that between-scanner reliability was assessed using only two scanners (both 3T), so broader conclusions about scanner/vendor effects require additional scanner varieties.","The ‘raw’ pipeline still includes rigid registration to MNI space and resampling, so results may depend on those preprocessing choices and may not reflect truly scanner-native inference. The evaluation uses a specific random split of a heterogeneous multi-source dataset; without repeated splits or external held-out cohorts per site/vendor, generalization claims may be sensitive to dataset composition and site effects. Reliability estimates are based on small N (20 within, 11 between), making ICCs and CIs potentially unstable and limiting subgroup analyses (e.g., by age range, sex, scanner order/interval). The work focuses on reliability of the derived biomarker (Brain-PAD) but does not connect to engineering reliability modeling (failure/degradation/maintenance), so “reliability” is measurement repeatability rather than reliability engineering.",The paper suggests expanding between-scanner analyses to include more scanner systems to build a comprehensive picture of how scanner variability affects brain-predicted age. It also implies that larger twin samples (including more dizygotic pairs and broader demographics) would improve precision and enable deeper analysis of how heritability changes with age.,"Develop domain-adaptation or harmonization methods (e.g., ComBat-style, adversarial invariance, or calibration layers) to improve CNN performance and reliability across scanners/vendors when using minimally processed images. Provide open, reproducible implementations and standardized evaluation protocols (multiple random splits, external validation by site, and sensitivity analyses to registration/resampling choices). Extend analyses to explicitly model longitudinal change and minimal detectable change (MDC) for Brain-PAD to support clinical trial design, including robustness to artifacts and pathology. Evaluate uncertainty quantification for predicted age (e.g., Bayesian deep learning) to improve interpretability and reliability of individual-level biomarker use.",1612.02572v1,https://arxiv.org/pdf/1612.02572v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:50:24Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Simulated only|Other,Not applicable,Healthcare/medical|Network/cybersecurity|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a family of bivariate beta distributions on the unit square that can represent both negative dependence (for “adversarial” parameters) and positive dependence (for “amiable” parameters), motivated by Bayesian inference problems. It applies negatively correlated bivariate beta priors to medical diagnostic testing, modeling test sensitivity (η) and specificity (θ) as adversarial propensities and performing Bayesian posterior inference using confirmatory-test sampling; posterior computation is done numerically via a grid approximation. It also applies positively dependent bivariate beta priors to engineering system survivability (a two-component series system), treating component reliabilities/propensities as amiable and showing how dependence assumptions change system survivability. A key conceptual contribution is the argument that “independent lifetimes” require a two-stage hierarchical construction (independence of propensities plus conditional independence of lifetimes), contrasting exchangeability vs hierarchical independence. Simulation with synthetic diagnostic data compares independent beta priors against negatively dependent (OL)− and AN(5) priors, finding that dependence-aware priors can improve inference for small samples and that AN(5) tends to yield sharper posteriors.","Diagnostic-test likelihood with confirmatory sampling: for data d=(n,n1,k1,k2),
\[L(\pi,\eta,\theta;d) \propto \theta^{k_2}(1-\theta)^{(n-n_1-k_2)}\,\eta^{k_1}(1-\eta)^{(n_1-k_1)}\,\pi^{n_1}(1-\pi)^{(n-n_1)}.\]
(OL)− bivariate beta prior density (up to proportionality):
\[g(\eta,\theta;\alpha_1,\alpha_2,\alpha_3) \propto \frac{\eta^{\alpha_1-1}(1-\eta)^{\alpha_2+\alpha_3-1}\,\theta^{\alpha_1+\alpha_3-1}(1-\theta)^{\alpha_2-1}}{[1-\eta(1-\theta)]^{\alpha_1+\alpha_2+\alpha_3}}.\]
Two-component series-system survivability under exchangeability with \(\theta\sim \text{Beta}(\alpha,\beta)\): \(\Pr(Y^*=1)=E(\theta^2)=\frac{\alpha\beta+\alpha^2(\alpha+\beta+1)}{(\alpha+\beta)^2(\alpha+\beta+1)}\); under hierarchical independence: \(\Pr(Y^*=1)=E(\theta_1)E(\theta_2)\).","Synthetic diagnostic example sets true (η,θ)=(0.773, 0.599) using two normal-class model with thresholding; priors are calibrated to induce negative dependence: (OL)− with (α1,α2,α3)=(10,2.5,5) has prior corr ρ≈−0.45, while AN(5) with α1=…=α4=5, α5=0.0001 has ρ≈−0.65. From the posterior contour/marginal comparisons across n=15,30,50,100, the AN(5) prior’s posterior centroid/modes approach the true values fastest (noted convergence for n as small as 30) and yields the sharpest uncertainty bands for small n; for n≥50, differences among priors largely diminish. In system survivability, for exchangeable components with θ~Beta(1,1), series-system survivability is 1/3 (0.333) vs the conventional 1/4 that arises under hierarchical independence; this highlights the impact of dependence/exchangeability assumptions. Using positively dependent (OL)+ and AN(5) priors for component propensities, the paper illustrates that stronger positive dependence increases series-system survivability, and that matched AN(5) dependence can yield more conservative survivability than (OL)+ in examples (e.g., for E(θi)=0.5 with ρ≈0.484, AN(5) gives 0.255 vs (OL)+ example 0.290).","The authors note that they restrict attention to the bivariate case; extending to multi-component systems/hypercube dependence is unclear and remains challenging. They also state that joint posteriors are not available in closed form (especially for AN(5)), requiring numerical approximation (grid-based) which increases computational burden. For diagnostic validation they avoid field data because η and θ are not known with certainty, so proof-of-principle relies on synthetic data. They explicitly mention that decision-theoretic aspects (utilities, optimal sampling plan, sample size choice) are not addressed and remain open.","The diagnostic-test model assumes conditional independence of K1 and K2 given N1,η,θ and a simplified confirmatory-then-screening design; real diagnostic studies often involve verification bias, imperfect confirmatory tests, and covariate effects that could break these assumptions. Posterior computation via a fixed m×m grid on (η,θ) does not scale well and may be inaccurate for sharp posteriors or higher-dimensional extensions; MCMC/variational methods and adaptive quadrature could be more robust. The system-survivability illustration is limited to two-component binary survival at a single mission time (Bernoulli at t), rather than full lifetime distributions/hazard-rate modeling typical in reliability and survival analysis. Comparisons between (OL)+ and AN(5) are based on selected parameter matches; a broader, systematic sensitivity analysis over correlation and marginal shapes would be needed to generalize claims about “conservatism.”","They propose extending the approach from bivariate dependence to multivariate distributions on the unit hypercube for multi-component systems and networks, or alternatively modularizing systems into dependent pairs. They also indicate that diagnostic assessment should ultimately be treated decision-theoretically (utilities for correct/incorrect diagnosis), and that optimal sampling plan and sample size selection remain to be addressed. The paper also points to further development/usage of a more general eight-parameter bivariate beta family (AN(8)/GBB) with closure under complementation as a flexible prior class.","Develop scalable Bayesian computation (e.g., MCMC with efficient parameterizations of AN(5)/AN(8), or copula-based alternatives) to avoid coarse grid approximations and to enable higher-dimensional extensions. Extend the diagnostic model to include imperfect gold standards, verification bias, and covariate-dependent sensitivity/specificity, then evaluate on real diagnostic datasets. For reliability, move beyond single-time Bernoulli survivability to continuous-time lifetime models (e.g., Weibull/lognormal with shared frailty) and study how dependence in component propensities maps to dependence in failure times and system-level survival functions. Provide principled elicitation guidance for the bivariate beta prior parameters (matching expert beliefs about marginals and dependence) and perform robustness/sensitivity analysis to prior misspecification.",1701.03462v1,https://arxiv.org/pdf/1701.03462v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:51:24Z
FALSE,NA,Other,Simulated only,Not applicable,Network/cybersecurity|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies ultra-reliable short-packet communications in a dual-hop wireless network where an energy-constrained relay harvests energy from the source’s RF signal (time-switching relaying, TSR) and then forwards data using decode-and-forward (DF). Performance is analyzed in the finite-blocklength regime under delay-limited transmission, focusing on throughput (effective rate) and delay under stringent target block error probabilities (e.g., 10^{-5}). The authors derive numerical-integration-based analytical approximations for the average error probabilities on S→R and R→D links (including their correlation via harvested energy), yielding an approximation to end-to-end DF error probability and thus throughput/delay. They validate the approximation against Monte Carlo simulations, showing strong agreement for ultra-reliable regimes where the relay-decoding error probability is very small. Results highlight trade-offs between reliability and latency, show that optimal relay placement tends to be closer to the source under WPT, and that very short messages (e.g., 64 bits) can meet very low error targets with acceptable latency under certain resource allocations.","The TSR time allocation is parameterized by $\alpha=\frac{v}{2n+v}$, where $v$ channel uses are for power transfer and $n$ per hop for information (total blocklength $2n+v$). Finite-blocklength decoding error for SNR $\gamma$ at rate $r=k/n$ is approximated as $\epsilon\approx Q\!\left(\frac{C(\gamma)-r}{\sqrt{V(\gamma)/n}}\right)$ with $C(\gamma)=\log_2(1+\gamma)$ and $V(\gamma)=\left(1-(1+\gamma)^{-2}\right)(\log_2 e)^2$. End-to-end DF block error is $\epsilon_{DF}=\epsilon_r+(1-\epsilon_r)\epsilon_d=\epsilon_r+\epsilon_d-\epsilon_r\epsilon_d$, and throughput is $\tau=(1-\mathbb{E}[\epsilon_{DF}])\,\frac{k}{2n+v}$ with delay $\delta=\frac{k}{\tau}=\frac{2n+v}{1-\mathbb{E}[\epsilon_{DF}]}$. The harvested-energy-induced relay power is approximated by $P_r\approx \frac{\eta P_s v h}{d_1^{\omega} n}$, leading to $\gamma_d=\frac{\eta P_s h g v}{d_1^{\omega} d_2^{\omega} \sigma_d^2 n}$.","Simulations confirm the analytical approximation that neglects energy accumulated during consecutively failed decoding blocks (setting $L=0$) is accurate when targeting very small error probabilities (URC regime). There exists an optimal information blocklength $n^*$ and optimal power-transfer fraction $\alpha^*$ that maximize throughput; as the target error probability tightens (e.g., from $10^{-2}$ to $10^{-5}$), both $n^*$ and $\alpha^*$ increase. For a stringent target such as $\epsilon_0=10^{-5}$ with $k=160$ bits, the paper reports approximately $v^*\approx 6000$ and $n^*\approx 1000$, giving delay $\delta^*\approx 8001$ channel uses (about 16 ms if $T_c=2\,\mu$s). With a fixed total budget $2n+v=2000$, very short packets (e.g., $k=64$) can achieve about $\epsilon_{DF}\approx 3\times 10^{-5}$ at roughly $v\approx 1500$ and $n\approx 500$. Optimal relay placement for minimizing delay/maximizing throughput is closer to the source (unlike conventional relaying without WPT).","The authors note that channel uses required to acquire CSI are not included; therefore, results for systems where CSI acquisition overhead is non-negligible are upper bounds. They also neglect other relay energy consumption sources besides transmission energy, arguing transmit energy typically dominates processing power. Analytically, they approximate relay transmit power by ignoring energy harvested during blocks that were incorrectly decoded at the relay (setting $L=0$), and then validate this approximation via simulation for URC regimes.","The work is not about reliability engineering (failure/maintenance/life data); it addresses communication reliability (packet error probability), so results do not translate directly to engineering reliability metrics like hazard/MTBF. The finite-blocklength approximation used (normal approximation via $Q(\cdot)$) is stated to be accurate for sufficiently large $n$ (e.g., $n\ge 100$), but performance for very small $n$ may deviate without tighter bounds. The analysis assumes quasi-static block fading with i.i.d. blocks and perfect CSI at receivers; real URC systems may face temporal correlation, imperfect estimation, interference, and protocol overheads that change optimal allocations. Code/software details are not provided, which limits reproducibility of the numerical integration and simulation settings.",None stated.,"Extend the analysis to include explicit CSI acquisition/estimation overhead and imperfect CSI within the latency budget, which is critical for URC. Consider more realistic energy-harvesting and circuit-power models (e.g., non-linear rectifier efficiency, processing power, storage leakage) and investigate robustness of the optimal $\alpha$, $n$, and relay placement. Generalize to interference-limited/multi-user settings and to correlated fading or mobility, and provide self-contained open-source implementations to support reproducible benchmarks.",1702.00584v1,https://arxiv.org/pdf/1702.00584v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:52:00Z
FALSE,Other,ML-based|Bayesian|Stochastic process|Other,Sensor/condition monitoring|Mixture of types|Simulated only|Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Supplementary material (Journal/Publisher),NA,"The paper argues that standard supervised-learning prediction models can be unreliable for decision support when training data is generated under an action policy (e.g., treatment decisions), because learned relationships may not generalize when the policy changes. It proposes a counterfactual learning objective and introduces the Counterfactual Gaussian Process (CGP) to predict counterfactual continuous-time outcome trajectories under hypothetical future action sequences. The CGP is learned from irregularly sampled observational traces by modeling actions and observations jointly as a marked point process and maximizing an adjusted likelihood that accounts for action-selection effects. In simulation, CGP-based risk scores are stable across different training action policies when assumptions hold, while a baseline GP trained with a standard objective is policy-sensitive; when assumptions are violated, CGP performance degrades. A real-data case study using ICU creatinine measurements and dialysis events (MIMIC-II) demonstrates “what-if” individualized treatment planning and shows improved factual predictive accuracy versus simpler GP baselines.","The CGP target is the counterfactual trajectory distribution $P(\{Y_s[a]: s>t\}\mid H_t)$ under a specified future action sequence $a$. The observational trace is modeled as an MPP with intensity factorization $\lambda^*(t,y,a,z_y,z_a)=\lambda^*(t)\,p^*(z_y,z_a\mid t)\,p^*(y\mid t,z_y)\,p^*(a\mid y,t,z_a)$, where $p^*(y\mid t,z_y)$ is the GP outcome model. Parameters are learned by maximizing the adjusted log-likelihood $\ell(\theta)=\sum_j\log p^*_{\theta}(y_j\mid t_j,z_{y_j})+\sum_j\log \lambda^*_{\theta}(t_j)p^*_{\theta}(a_j,z_{y_j},z_{a_j}\mid t_j,y_j)-\int_0^{\tau}\lambda^*_{\theta}(s)ds$.","In simulated risk prediction, the baseline GP’s risk scores change substantially when trained under a different action policy (e.g., average risk-score difference vs regime A: $\Delta=0.083$ in regime B), while CGP remains nearly invariant when assumptions hold ($\Delta=0.001$ in regime B). Rank stability similarly favors CGP (Kendall’s $\tau=0.998$ for CGP vs $0.857$ for baseline in regime B relative to regime A), and CGP AUC is unchanged across regimes A and B (AUC $=0.872$) while baseline varies (AUC $0.853$ vs $0.832$). When assumptions are violated (regime C), CGP also becomes unstable ($\Delta=0.128$, $\tau=0.562$, AUC $0.829$). On real ICU creatinine/dialysis data, the CGP outcome model achieves MAE 0.39 (95% CI 0.38–0.40) for 24h and 0.62 (0.60–0.64) for 24–48h, improving over two baselines by mean absolute-error differences of about 0.07–0.09 and 0.03–0.04 depending on horizon.","The authors note that the CGP’s validity depends on structural/causal assumptions (consistency, continuous-time no unmeasured confounding, and non-informative measurement times) that are generally not testable from data; reliability therefore depends on the plausibility of these assumptions given domain knowledge. They also state they cannot quantitatively evaluate counterfactual predictions on the ICU case study without prospective experimental data, so the “what-if” results are qualitative. They emphasize that when assumptions are violated (their regime C), CGP predictions can become unstable and degrade.","The method requires specifying and fitting an action/event (marked point process) model; misspecification of action timing/mark models or unmodeled feedback/autocorrelation could bias the learned counterfactual outcome model. The approach is developed for univariate continuous outcomes and discrete action types; extensions to multivariate/high-dimensional outcomes or complex, continuous-valued interventions may be nontrivial in practice. Computationally, mixture-of-GPs with nonconvex likelihood and BFGS optimization can be sensitive to initialization and may be hard to scale to large datasets or long horizons without sparse/approximate GP methods. The evaluation emphasizes policy-invariance in simulation and predictive MAE on factual outcomes; it does not provide calibration or decision-analytic evaluation of downstream decision quality under realistic misspecification or measurement noise patterns.","They propose developing formal procedures such as sensitivity analyses to detect when causal assumptions conflict with a dataset and to improve practical applicability. They also suggest exploring alternative sets of structural assumptions (e.g., Pearl’s back-door/front-door criteria analogs) that could enable learning counterfactual GPs from non-experimental data. More broadly, they point to further work on safety/accountability/transparency in ML systems by identifying and removing nuisance factors (like action policy) that harm reliability.","Develop robust/self-starting variants that relax assumptions (e.g., informative visit processes, unmeasured confounding) via joint modeling, instrumental variables, or partial identification bounds. Extend CGP to multivariate outcomes and heterogeneous intervention representations (dose, duration, continuous actions) with scalable sparse GP/variational inference. Add diagnostic tools for assumption checking (e.g., negative controls, residualized action-model checks) and provide calibrated uncertainty for counterfactual trajectories and derived risk scores. Provide open-source reference implementations and benchmark studies across multiple real clinical and non-clinical datasets to assess generalizability and operational impact.",1703.10651v4,https://arxiv.org/pdf/1703.10651v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:52:41Z
FALSE,NA,ML-based,Sensor/condition monitoring|Other,Not applicable,Network/cybersecurity|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,http://archive.ics.uci.edu/ml,"This paper proposes Margin Density Drift Detection (MD3), an unsupervised and incremental method for detecting concept drift in streaming unlabeled data by tracking the fraction of samples that fall in a classifier’s uncertainty region (its margin). The core idea is that changes in margin density are more aligned with changes that affect classification performance (i.e., changes relevant to P(Y|X)) than generic feature-distribution change detectors that track P(X), thereby reducing false alarms. MD3 is described for classifiers with explicit margins (e.g., linear SVM) and extended to classifiers without explicit margins via a feature-bagged random subspace ensemble that uses vote-disagreement as a pseudo-margin. Drift is suspected when the online margin-density estimate deviates from a reference distribution learned via cross-validation, and then a bounded batch of labels is requested only to confirm performance degradation and retrain if needed. Experiments on drift-induced UCI datasets, real cybersecurity streams (spam, phishing, intrusion detection), and benchmark drift datasets show accuracy close to a fully-labeled accuracy-tracking detector while requiring far fewer labels and producing fewer false alarms than an unlabeled Hellinger-distance feature monitor.","MD3 defines margin density as the proportion of unlabeled samples in a model’s uncertainty region. For a linear SVM, $\mathrm{MD}=\frac{1}{|X|}\sum_{x\in X}\mathbb{1}(|w\cdot x+b|\le 1)$; for an ensemble pseudo-margin it uses vote-probability closeness, $\mathrm{MD}=\frac{1}{|X|}\sum_{x\in X}\mathbb{1}(|p_E(y=+1|x)-p_E(y=-1|x)|\le \theta_{\text{margin}})$. The online MD estimate is updated incrementally with an EWMA-style recursion $\mathrm{MD}_t=\lambda\mathrm{MD}_{t-1}+(1-\lambda)S(X_t)$ and drift is suspected when $|\mathrm{MD}_t-\mathrm{MD}_{\text{ref}}|>\Theta\,\sigma_{\text{ref}}$; drift is confirmed if accuracy on a queried labeled batch drops by more than $\Theta\,\sigma_{\text{Acc}}$.","Across drift-induced UCI datasets, MD3 variants achieve stream accuracies close to a fully-labeled accuracy-tracking baseline (reported average difference <1% in those experiments) while substantially reducing false alarms compared with an unlabeled Hellinger-distance detector (HDDDM). On cybersecurity streams, MD3-SVM and MD3-RS accuracies are close to the fully labeled AccTr baseline (examples: spam 87.3–89.2% vs 90.6%; spamassassin 92.8% vs 92.8%; phishing 90.8–91.7% vs 92.9%; nsl-kdd 89.4–89.9% vs 90.1%) with much lower labeling rates (about 5.3%–18.9% vs 100% for AccTr). HDDDM tends to signal more drifts and incurs more false alarms (e.g., spamassassin: 4 drifts with 3 false alarms; phishing: 4 drifts with 2 false alarms), increasing labeling cost relative to MD3. Sensitivity analyses suggest MD3 is relatively robust to detection-model choice and to pseudo-margin width settings except when the margin is set extremely narrow (e.g., $\theta_{\text{margin}}=0.05$ causing a failure case on phishing).","The authors note that MD3’s effectiveness relies on using a robust classifier that distributes importance across features; they show that a non-robust sparse model (e.g., logistic regression with L1 penalty) can fail to detect drift (e.g., no drift detected on phishing) and degrade performance. They also highlight an extreme drift scenario where changes occur largely away from the classifier margin (a drastic global shift), in which case margin-density may miss the change even though feature-distribution measures and labeled error tracking would detect it. They suggest that such rare cases are better handled by specialized novelty detection methods.","The approach depends on assumptions about independence/temporal structure: the EWMA-style update and thresholding do not explicitly address autocorrelation, seasonality, or recurrent/temporary drifts common in real streams, which can affect false alarm rates. MD3 also requires an initial labeled training segment and a cross-validation-based reference distribution; in truly label-scarce cold-start settings this may be impractical, and the reference may be unstable for small initial samples. Confirming drift still requires querying a fixed batch of labels ($N_{train}$), which may be expensive or delayed in operational settings; the paper does not deeply analyze detection delay vs. batch size trade-offs. Finally, comparisons focus on a limited set of baselines (AccTr/EWMA and HDDDM); broader evaluation against additional state-of-the-art unsupervised/active drift detectors could better position performance claims.","The authors propose focusing on label-efficient relearning after drift detection, including integrating margin density with active learning strategies to selectively label samples for confirmation and retraining. They also motivate further work on improving reliability of unlabeled drift detection and on creating/analyzing real-world datasets that mirror the synthetic scenarios used for understanding false alarms and detectability in robust models.","A natural extension is to develop adaptive/self-starting versions of MD3 that update reference distributions without cross-validation and with minimal initial labels, while controlling in-control false alarm rates. Extending MD3 to handle temporal dependence explicitly (e.g., modeling MD as an autocorrelated process with robust control limits) could improve performance on seasonal or bursty streams. Multiclass and high-dimensional settings could be treated more directly (beyond binary reductions) by defining class-conditional or vector-valued margin-density signals and studying their detection power. Providing an open-source implementation with standardized benchmarks and calibrated label-cost/delay models would improve reproducibility and practical adoption, especially for cybersecurity operations.",1704.00023v1,https://arxiv.org/pdf/1704.00023v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:53:26Z
TRUE,Life distribution modeling|Other,Nonparametric/Semi-parametric|Other,Other,Not applicable,Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper extends univariate quantile-based reliability concepts to the bivariate case by using direction-specific bivariate quantile curves (level curves) defined via orthant probabilities. It proposes quantile-curve-based bivariate hazard rate and bivariate mean residual life (MRL) as vectors whose components are constructed from marginal quantile functions and conditional quantile functions (e.g., of $Y\mid X\le x$). The authors establish uniqueness/characterization results showing that the proposed bivariate hazard-rate vector uniquely determines the underlying bivariate quantile curve, and similarly for the bivariate MRL (with constants involving means of relevant conditional variables). A key contribution is deriving an explicit relationship between the quantile-based bivariate hazard rate and the quantile-based bivariate MRL, and providing analogous concepts in reversed time (bivariate reversed hazard rate and reversed MRL) with corresponding characterization formulas. The work is primarily theoretical and positions quantile curves as an alternative to copula-based formulations for bivariate reliability concepts, relying on conditional quantiles rather than explicit copula specification.","Bivariate quantile curve in direction $\varepsilon$ is $Q_{\mathbf X}(p,\varepsilon)=\{(x,y)\in\mathbb R^2: F_\varepsilon(x,y)=p\}$, and for $\varepsilon=--$ it can be represented as $(Q_X(u),\,Q_{Y\mid X\le Q_X(u)}(p/u))$ for $u>p$. The proposed quantile-curve bivariate hazard rate vector for $\varepsilon=--$ is $h^-_{\varepsilon--}(u,P_X)=(h_1(u),h_2(P_X))$ with $h_1(u)=\{(1-u)Q_X'(u)\}^{-1}$ and $h_2(P_X)=\{(1-P_X)\phi'(P_X)\}^{-1}$ where $\phi$ is the conditional-quantile mapping. The bivariate MRL vector is $m^-_{\varepsilon--}(u,P_X)=(m_1(u),m_2(P_X))$ with $m_1(u)=\frac{1}{1-u}\int_u^1 Q_X(z)\,dz-Q_X(u)$ and $m_2(P_X)=\frac{1}{1-P_X}\int_{P_X}^1 \phi(z)\,dz-\phi(P_X)$. A derived link is $(1-u)m_1(u)=\int_u^1 \frac{dz}{h_1(z)}$ and $(1-P_X)m_2(P_X)=\int_{P_X}^1 \frac{dz}{h_2(z)}$.","The paper proves characterization/uniqueness: the quantile-curve-based bivariate hazard vector uniquely determines the underlying quantile curve via $Q_X(u)=\int_0^u \frac{dz}{(1-z)h_1(z)}$ and $\phi(P_X)=\int_0^{P_X} \frac{dz}{(1-z)h_2(z)}$. It also shows the quantile curve is uniquely determined by the bivariate MRL through $Q_X(u)=\mu_X-m_1(u)+\int_0^u \frac{m_1(z)}{1-z}\,dz$ and $\phi(P_X)=\mu_{Y_X}-m_2(P_X)+\int_0^{P_X} \frac{m_2(z)}{1-z}\,dz$. A relationship between hazard and MRL components is established: $(1-u)m_1(u)=\int_u^1 1/h_1(z)\,dz$ and $(1-P_X)m_2(P_X)=\int_{P_X}^1 1/h_2(z)\,dz$. In reversed time, analogous vectors are defined with $r_1(u)=\{uQ_X'(u)\}^{-1}$ and $r_2(P_X)=\{P_X\phi'(P_X)\}^{-1}$, and the reversed-MRL components characterize the quantile curve without requiring conditional means.",None stated.,"The development is largely restricted to absolutely continuous bivariate distributions and differentiability of quantile/conditional-quantile mappings, which may not hold for mixed/discrete or heavy-tailed models with irregular quantiles. The framework is theoretical and does not provide estimation procedures, finite-sample properties, or empirical performance studies for estimating conditional quantiles and their derivatives (which can be statistically unstable). The proposed bivariate hazard/MRL are direction-specific orthant-based vectors; interpretation and comparison across directions (and implications for practical system reliability metrics) are not fully developed. No applied case study is provided to illustrate implementation challenges (bandwidth/quantile regression choices, smoothing for derivatives, etc.).",None stated.,"Develop nonparametric or semiparametric estimators for the proposed bivariate hazard/MRL vectors based on estimated conditional quantiles, including practical smoothing/regularization for derivative estimation and uncertainty quantification (confidence bands). Study robustness to dependence misspecification, ties, censoring (right/left/interval), and autocorrelation, and extend the framework to censored bivariate lifetime data commonly seen in reliability. Provide simulation and real-data case studies comparing quantile-curve-based measures to copula-based bivariate hazard/MRL approaches, including guidance on selecting direction(s) relevant for specific engineering interpretations. Extend the approach to higher dimensions and to system-level functionals (e.g., series/parallel system reliability) expressed via multivariate quantile surfaces.",1704.08444v2,https://arxiv.org/pdf/1704.08444v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:53:49Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/facebookresearch/odin,"This paper studies the reliability-related problem of detecting out-of-distribution (OOD) inputs for image classifiers, proposing ODIN, a post-hoc method that requires no retraining or architectural changes. ODIN combines temperature scaling in the softmax with a small, adversarial-style input perturbation that increases the maximum softmax score more for in-distribution than OOD samples, improving separability. The detector thresholds the calibrated maximum softmax probability to decide in- vs. out-of-distribution, selecting parameters to achieve 95% true positive rate on in-distribution data. Experiments across DenseNet and Wide ResNet models trained on CIFAR-10/100 and tested against multiple OOD datasets (TinyImageNet, LSUN, Gaussian/Uniform noise) show large improvements over the Hendrycks & Gimpel baseline; e.g., FPR at 95% TPR drops from 34.7% to 4.3% on DenseNet (CIFAR-10 vs. TinyImageNet-crop). The work advances practical OOD detection by showing strong gains with a simple, easily deployable preprocessing/calibration procedure and provides analysis explaining the effect of temperature and gradient-norm differences.","ODIN uses (1) temperature-scaled softmax $S_i(x;T)=\exp(f_i(x)/T)/\sum_j \exp(f_j(x)/T)$ and the detection score $S(x;T)=\max_i S_i(x;T)$. It applies input preprocessing $\tilde{x}=x-\varepsilon\,\mathrm{sign}(-\nabla_x \log S_{\hat{y}}(x;T))$ and then thresholds $S(\tilde{x};T)$ against $\delta$ to decide OOD. The paper also gives a large-$T$ Taylor approximation $S_{\hat{y}}(x;T)\approx \frac{1}{N}-\frac{1}{T}\sum_i(f_{\hat{y}}-f_i)+\frac{1}{2T^2}\sum_i(f_{\hat{y}}-f_i)^2$ to analyze why large temperatures help until gains saturate.","On DenseNet trained on CIFAR-10 and evaluated with TinyImageNet (crop) as OOD, ODIN reduces FPR@95%TPR from 34.7% (baseline) to 4.3% and improves AUROC from 95.3% to 99.1%. For CIFAR-10 vs. LSUN (resize), FPR@95%TPR drops from 33.6% to 3.8% with AUROC rising from 95.4% to 99.2%. The paper reports using $T=1000$ for all experiments, with tuned perturbation magnitudes (e.g., $\varepsilon=0.0014$ for CIFAR-10 models and $\varepsilon=0.002$ for CIFAR-100 models). It also shows parameter transferability across validation OOD sets with similar performance, indicating limited sensitivity to the specific tuning dataset.","The authors note that very large perturbation magnitudes $\varepsilon$ can degrade performance because higher-order terms in the Taylor expansion are no longer negligible. They also highlight that conventional OOD methods can be unreliable in high-dimensional spaces, motivating their approach, but do not present extensive guarantees beyond the provided analysis and experiments. They mention diminishing returns as temperature $T$ becomes very large, with performance converging as $T\to\infty$ when $\varepsilon=0$.","The method relies on access to input gradients (backpropagation) at inference time, which can be costly or infeasible in some deployment settings and complicates use with black-box models. ODIN’s decision rule is still based on maximum softmax probability, which can fail for certain near-OOD or adversarially crafted OOD inputs; robustness to adaptive adversaries is not established. Threshold/parameter selection assumes availability of a representative OOD validation set (e.g., iSUN) and fixes TPR=95%, which may not align with real operational risk/cost tradeoffs. The evaluation focuses on vision benchmarks (CIFAR/TinyImageNet/LSUN/noise) and may not generalize to more realistic distribution shifts (e.g., corruptions, domain shifts, class-incremental settings) without further validation.",The authors suggest exploring ODIN in other application areas such as speech recognition and natural language processing. They also indicate that better understanding of the observed phenomena (larger variance across class logits and larger gradient norms on in-distribution inputs) could lead to further insights and improvements for OOD detection.,"A valuable extension would be to develop a self-calibrating or self-starting variant that reduces dependence on an explicit OOD validation dataset and provides principled threshold selection under uncertainty. Another direction is to evaluate and harden ODIN under adaptive, worst-case OOD/adversarial settings, including attackers that optimize against the ODIN score. Extending the approach to structured outputs and high-dimensional multivariate settings (e.g., segmentation, detection) and to correlated/streaming data would improve operational applicability. Finally, providing standardized, reproducible benchmarks (including non-i.i.d. shifts and real deployment logs) and packaging the method as a maintained library would strengthen practical adoption.",1706.02690v5,https://arxiv.org/pdf/1706.02690v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:54:29Z
TRUE,Degradation modeling|Life distribution modeling|Accelerated testing|System reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Right-censored|Complete lifetime data|Simulated only|Mixture of types|Other,Not applicable,Manufacturing (general)|Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,C/C++|Other,Not provided,NA,"The paper develops a Bayesian inference framework for accumulated damage models (ADMs) used in long-term reliability assessment of lumber subject to duration-of-load (DOL) effects (creep and creep rupture). Focusing on a dimensionally consistent reparameterization of the Canadian ADM (a differential-equation degradation model with a stress threshold), the authors show the likelihood for failure-time data is intractable because failure times are only implicitly defined via numerical solutions of the ADM under ramp/constant load tests. They adopt approximate Bayesian computation with MCMC (ABC‑MCMC) to perform likelihood-free posterior sampling and propose a modification that explicitly accounts for right-censoring (truncated constant-load tests) by combining ABC on uncensored summaries with a separate factor based on the simulated censoring proportion. The approach is validated on simulated datasets and then applied to classic Western Hemlock duration-of-load experiments involving both ramp-load and constant-load tests with substantial censoring. Posterior samples are propagated through a stochastic live-loading model to estimate time-to-failure distributions, reliability index curves, and the DOL adjustment factor K_D with posterior intervals, enabling uncertainty quantification not available in earlier point-estimate approaches.","The core degradation/accumulated-damage model is the (reparameterized) Canadian ADM ODE: $\frac{d}{dt}\alpha(t)\,\mu = [(a\tau_s)(\tau(t)/\tau_s-\sigma_0)_+]^b + [(c\tau_s)(\tau(t)/\tau_s-\sigma_0)_+]^n\,\alpha(t)$, with failure at $\alpha(t)=1$ and a stress threshold $\sigma_0$. Failure time under constant-load tests is defined implicitly: solve numerically for $T_s$ (ramp failure time) and then set $T=T_s$ if $T_s\le T_0$ else $T=T_c$ where $T_c$ is a closed-form expression involving $H(t)=\exp\{-\frac{1}{\mu}(c k T_s)^n\frac{T_s}{n+1}(t/T_s-\sigma_0)^{n+1}\}$. Random effects are modeled lognormally (e.g., $a\sim\text{LogNormal}(\mu_a,\sigma_a)$, etc.) with $\sigma_0=\eta/(1+\eta)$ and $\eta\sim\text{LogNormal}(\mu_{\sigma_0},\sigma_{\sigma_0})$, and inference is performed via ABC‑MCMC using quantile summaries and a kernel $K_\delta$; for censored data the acceptance ratio is adjusted by factors based on the simulated censoring proportion at truncation time $t_c$.","For the real Western Hemlock dataset (constant-load 4500 psi for 1 year; constant-load 3000 psi for 4 years; plus a ramp-load sample), the ABC‑MCMC fit produced high-likelihood parameter vectors around (e.g.) $\hat\theta_1=(\mu_a,\sigma_a,\mu_b,\sigma_b,\mu_c,\sigma_c,\mu_n,\sigma_n,\mu_{\sigma_0},\sigma_{\sigma_0})\approx(-7.76,0.48,3.21,0.18,-21.96,0.29,-1.00,0.20,0.15,0.07)$ with log-likelihood about −1120. Using posterior propagation through a stochastic residential live-load model over a 30‑year horizon, the paper estimates reliability-index vs performance-factor curves and quantifies DOL via the adjustment factor $K_D=\phi_2/\phi_1$. Posterior-mean $K_D$ is about 0.71 with 95% posterior intervals (0.56, 0.81) at $\beta=2.5$, (0.53, 0.81) at $\beta=3.0$, and (0.49, 0.82) at $\beta=3.5$, compared to Foschi et al. (1989) point estimates around 0.75–0.76. The authors report their Bayesian approach yields slightly more conservative reliability estimates while placing prior results within posterior uncertainty bands.","The authors note that some ADM parameters remain highly uncertain (posterior likelihood surface is relatively flat), motivating exploration of alternative or reduced parameterizations that still fit the data well. They also indicate that the live-load modeling used is illustrative and limited to the residential live-load scenario adopted from Foschi et al. (1989), implying broader loading environments were not addressed in the present analysis. They further acknowledge the need to extend the framework to other forms of accumulated damage models beyond the Canadian ADM considered.","The ABC approach relies on a chosen set of summary statistics (19 quantiles of uncensored times) and a tuned bandwidth to target ~1% acceptance, so posterior accuracy can be sensitive to these choices and may not approximate the true posterior well if summaries are weakly informative for some parameters. The method requires extensive simulation and numerical ODE solving (and, for their reliability results, very large replication counts), which may be computationally heavy and could limit routine engineering use without optimized software and benchmarking. Model assumptions such as independent piece-specific random effects, lognormal random-effects distributions, and the particular stochastic live-load process may be misspecified; robustness to these assumptions is not thoroughly explored. The paper does not provide a full posterior predictive check across all datasets (e.g., joint fit to ramp and constant-load outcomes under varying truncation times) beyond visual density/CDF overlays and likelihood comparisons.","The authors propose extending the Bayesian/ABC framework to other ADM functional forms and investigating alternative or reduced parameterizations because several parameters are highly uncertain in the posterior. They also suggest incorporating additional types of future live loads—such as snow, wind, and earthquakes—to obtain more realistic stochastic loading profiles for $\tau(t)$ and broaden the reliability assessment. Finally, they highlight ABC‑MCMC for censored data as a promising computational tool for similarly complex likelihood-intractable models.","Developing more principled or automated summary-statistic construction (e.g., semi-automatic ABC regression summaries tailored to the ADM parameters) and performing calibration/coverage studies could strengthen posterior validity. A self-contained, reproducible software implementation (with documented ODE solver settings, convergence diagnostics, and parallel simulation) and sensitivity analysis to the ODE discretization and ABC bandwidth would improve practical adoption. Extending the model to handle dependence between ramp strength $\tau_s$ and ADM random effects, or incorporating hierarchical structure for lumber grades/species, could better reflect real production variability. Comparing against alternative likelihood-free or approximate-likelihood methods (e.g., synthetic likelihood, particle MCMC, variational Bayes) and evaluating computational trade-offs would clarify when ABC‑MCMC is preferable.",1706.04643v1,https://arxiv.org/pdf/1706.04643v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:55:17Z
TRUE,Life distribution modeling|Failure mode analysis|Other,"Parametric (Weibull, etc.)|Other",Right-censored|Other,Not applicable,Transportation/logistics|Other,Other,NA,None / Not applicable,Not applicable (No code used),http://seradata.com/products/spacetrak.html,"This paper conducts an observational (epidemiological-style) reliability analysis of on-orbit electric propulsion (EP) anomalies and compares EP anomaly rates to chemical propulsion (CP) for GEO spacecraft. Using SpaceTrak data (validated against an insurer dataset and manual checks), the authors analyze 162 EP-equipped spacecraft launched 1997–2015 (39 EP anomalies) and 482 CP-equipped GEO spacecraft (37 CP anomalies). They estimate Mean Time To Anomaly (MTTA) and fit Weibull models to time-to-anomaly, finding evidence of infant-type behavior for minor EP anomalies (shape < 1) and wear-out behavior for major EP anomalies (shape > 1). Stratified analyses show strong dependence between anomaly likelihood/onset and orbit type, and much higher anomaly risk for gridded ion engines than Hall thrusters. For GEO, anomaly rates are compared across two eras (1997–2004 vs 2005–2015), showing a large post-2005 improvement for EP and concluding that post-2005 EP (notably Hall thrusters and improved gridded ion engines) outperforms CP when both anomaly frequency and severity are considered (with major-anomaly rates not significantly different).","The paper models time-to-anomaly with a Weibull survival function: $P(T>t)=\exp\{-(t/\theta)^\beta\}$, where $\beta$ is the shape and $\theta$ the scale. Mean Time To Anomaly is estimated both empirically and via the Weibull mean $\mathrm{MTTA}=\theta\,\Gamma(1+1/\beta)$. Confidence intervals for MTTA are computed using a $t$-interval: $\mathrm{MTTA}\pm t_{1-\alpha/2}\,(\mathrm{stdev}/\sqrt{n})$. Relative risk of anomaly by thruster type is computed as $RR = P(\text{anomaly}|\text{GIE})/P(\text{anomaly}|\text{Hall})$ with an approximate normal CI on $\ln(RR)$.","Across all orbits, EP minor anomalies have Weibull shape $\beta=0.86$ (infant-type), scale $\theta=699$ days, and MTTA \~758 days; major anomalies have $\beta=1.14$ (wear-out), $\theta=1442$ days, and MTTA \~1378 days. Orbit type and EP anomaly occurrence are strongly associated (chi-squared statistic 30.4; p-value reported < 1e-6; Fisher exact p < 0.0001), with earlier onset in interplanetary/trans-lunar (onset 17 days; 1st quartile 79 days) vs GEO (onset 60 days; 1st quartile 297 days). By technology, anomaly risk for gridded ion engine-equipped spacecraft is 19/59=0.322 vs 1/102=0.0098 for Hall, giving RR=32.8 with 90% CI [6.2, 172.4] (p=0.0002). In GEO, EP anomaly rate drops from 3.23e-4 per spacecraft-day (1997–2004) to 9.14e-6 (2005–2015), a ~35× reduction; post-2005, EP shows lower fatal and minor anomaly rates than CP while major anomaly rates are not statistically different.","The authors note limitations inherent to retrospective observational studies: analyses are constrained by the resolution and completeness of recorded anomaly classifications, and they cannot reveal physics-of-failure mechanisms. They also acknowledge potential confounding by differences in EP duty cycle and space environment across orbit types, and state that duty-cycle data are impractical to obtain across operators. In the EP vs CP GEO comparison, they acknowledge functional differences: CP historically bears most orbit-raising risk, while EP has mostly been used for station-keeping, so phase-of-mission could confound comparisons; limited EP orbit-raising experience prevents stratification by phase.","The anomaly-rate comparisons rely on reported anomalies in a commercial database; underreporting or inconsistent classification across operators, eras, and countries could bias rates, especially for non-Western missions. The analysis largely treats anomalies as independent events (and uses spacecraft-level counting to mitigate dependence), but recurrent-event modeling or frailty effects could better account for multiple thrusters per spacecraft and correlated failures. Right-censoring and differing follow-up times are present; while person-time style rates help, a unified survival/regression model (e.g., Cox/Poisson with covariates) could more rigorously adjust for confounders (technology, launch year, manufacturer, orbit, usage). Parameter uncertainty for Weibull fits and goodness-of-fit diagnostics are only briefly addressed (probability plots), with limited quantitative fit assessment given small samples in some strata.","They propose further investigation to disentangle the causal drivers behind the dependence of EP anomalies on orbit type, highlighting space environment effects versus duty-cycle differences as competing hypotheses. They also identify the need for physics-of-failure understanding of EP anomaly mechanisms beyond what observational data can provide. Finally, they suggest that as more missions perform EP orbit raising, future analyses should stratify anomaly data by mission phase (orbit raising vs station-keeping) to reduce confounding in EP vs CP comparisons.","Develop a multivariable time-to-event framework (e.g., Cox/parametric AFT or piecewise-exponential Poisson regression) that simultaneously adjusts for calendar time, orbit, technology, manufacturer/heritage, and censoring to better quantify post-2005 improvements. Incorporate recurrent-event models at the thruster level (with clustering by spacecraft) to use all anomaly events without bias from dependence. Perform robustness checks to misclassification/underreporting (sensitivity analysis) and explicitly model reporting heterogeneity across operators/regions. Release a reproducible analysis pipeline and, where possible, a de-identified event dataset or code to enable independent verification and updates as new on-orbit EP orbit-raising data accumulate.",1706.10129v1,https://arxiv.org/pdf/1706.10129v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:56:02Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based|Other",Right-censored|Interval-censored|Left-censored|Mixture of types|Other,Not applicable,Manufacturing (general)|Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a general Bayesian framework to estimate component reliabilities in arbitrary coherent systems using only system failure-time observations plus each component’s status at system failure, accommodating heavy censoring induced by the system structure. Component lifetimes are modeled with a three-parameter Weibull distribution, allowing different components to have different distributions and permitting dependence and tied failure times (no IID/independence requirement). The likelihood is formulated via component-specific observation intervals (exact, right-, left-, and interval-censored), and posterior inference is obtained using adaptive Metropolis–Hastings MCMC. A key theoretical contribution is proving posterior propriety for a class of improper non-informative priors when sample size n>1. Simulation studies on 2-out-of-3 and bridge coherent systems show lower mean absolute error for the proposed Weibull posterior-mean reliability estimator versus the Bhattacharya–Samaniego nonparametric estimator, especially as n increases, and a real interval-censored dataset (boys’ first marijuana use) illustrates applicability beyond engineering reliability.","Component-j likelihood uses interval notation $(L_{ji},U_{ji})$: $L(\theta_j\mid \ell_j,u_j)=\prod_{i=1}^n f(L_{ji}\mid\theta_j)^{\mathbb{I}(L_{ji}=U_{ji})}\,[R(L_{ji}\mid\theta_j)-R(U_{ji}\mid\theta_j)]^{1-\mathbb{I}(L_{ji}=U_{ji})}$. The component reliability model is 3-parameter Weibull: $R(x\mid\beta,\eta,\mu)=\exp\{-((x-\mu)/\eta)^\beta\}$ (for $x>\mu$), with prior $\pi(\beta,\eta,\mu)\propto (1/\eta)(1/\beta)$ (more generally $1/(\eta\beta^b)$). Posterior mean reliability is approximated from MCMC draws: $\mathbb{E}[R(x\mid\theta)\mid\text{data}]\approx \frac{1}{n_p}\sum_{k=1}^{n_p} R(x\mid\theta^{(k)})$.","Across six simulation scenarios (2-out-of-3 and bridge systems) with high censoring (total censoring typically >50% and up to 90% in one component), the proposed Weibull 3-parameter posterior-mean estimator (W3PM) generally yields lower MAE than the Bhattacharya–Samaniego nonparametric estimator (BSNP), with the advantage increasing as sample size grows from n=25 to n=1000. The notable exceptions reported are component 3 in scenarios 2 and 4, where W3PM and BSNP behave similarly. In the real interval/left/right-censored marijuana-use dataset (n=191), posterior means are reported as $\beta\approx2.4$, $\eta\approx6.19$, $\mu\approx9.54$, implying mean time to first use $\mathbb{E}(X\mid\theta)\approx15.05$ years, and a posterior mean reliability curve with a 95% HPD band is presented.","The authors note that their approach requires choosing a parametric family (they use the three-parameter Weibull), whereas BSNP is nonparametric and avoids this choice. They also remark that the posterior propriety result does not hold for n=1 with at least one exact failure under the considered improper priors, so the method’s non-informative-prior justification relies on having more than one observed system.","Although the method allows dependence in principle, the paper does not specify an explicit joint dependence model among components; inference is effectively component-wise via censored intervals induced by system outcomes, so complex dependence may not be identifiable from the available data without additional assumptions. The approach relies on MCMC (adaptive Metropolis–Hastings) but does not provide implementation details (tuning, diagnostics criteria, computational cost) sufficient for straightforward reproduction. The comparisons focus on MAE of reliability curves over simulated grids; additional operating characteristics (e.g., coverage of credible bands, sensitivity to prior choice, robustness to Weibull misspecification and small n) are not extensively reported.","The authors suggest extending the framework beyond the Weibull family to other distributions (with proper priors) and even to a pure likelihood approach. They also propose applying the methodology to “reverse engineering,” using Bayesian model selection techniques (including mixture-based approaches) to choose among alternative coherent system structures.",A natural extension is a fully hierarchical/multilevel model linking component parameters across similar component types to borrow strength under extreme censoring and small samples. Another direction is developing a copula- or frailty-based joint model to quantify component dependence explicitly and study identifiability from system-level observations. Providing an open-source software implementation and standardized benchmark datasets (with reproducible simulation settings) would improve adoption and allow broader comparisons to modern Bayesian survival/reliability methods.,1707.03119v1,https://arxiv.org/pdf/1707.03119v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:56:38Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based|Other",Right-censored|Left-censored|Mixture of types|Other,Not applicable,Semiconductor/electronics|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Bayesian framework to estimate component reliability functions in general coherent systems when failure causes are partially or fully masked, extending beyond the well-studied series/parallel cases. Component lifetimes are modeled with a three-parameter Weibull distribution (allowing a location/threshold parameter) and do not need to be identically distributed across components; component lifetimes are assumed mutually independent. Masking is represented via component- and censor-type-specific masking probabilities (with an optional symmetric masking special case) and handled via data augmentation with latent censoring indicators for masked components. Posterior inference is computed using a Metropolis-within-Gibbs MCMC sampler, and posterior reliability curves and HPD intervals are produced. Performance is evaluated through simulations on complex coherent structures (including bridge and k-out-of-m examples) and a real hard-drive competing-risks dataset with 38% masked causes; results show improved mean absolute error versus the Bhattacharya–Samaniego nonparametric estimator which assumes i.i.d. component lifetimes.","Component reliability under the three-parameter Weibull is modeled as $R(t\mid\theta_j)=\exp\{-((t-\mu_j)/\eta_j)^{\beta_j}\}$ for $t>\mu_j$, with $\theta_j=(\beta_j,\eta_j,\mu_j)$. The component likelihood combines contributions from observed censoring types and masked observations using masking probabilities $(\lambda_{1j},\lambda_{2j},\lambda_{3j})$ and latent indicators $d_{lji}$ for the (unknown) censoring type of masked data, yielding the augmented likelihood in Eq. (1). Posterior inference is $\pi(\theta_j,\lambda_{1j},\lambda_{2j},\lambda_{3j},d_j\mid डेटा)\propto \pi(\cdot)\,L(\cdot)$ (Eq. (2)), and posterior mean reliability is approximated by Monte Carlo averaging $\frac{1}{n_p}\sum_{k=1}^{n_p}R(t\mid\theta_{jk})$ (Eq. (3)).","Across simulated coherent-system examples, the proposed posterior-mean estimator (W3PM) generally achieves lower MAE than the Bhattacharya–Samaniego nonparametric estimator (BSNP): e.g., in a 2-out-of-3 system MAE for components (1,2,3) is W3PM (0.0350, 0.0282, 0.0259) vs BSNP (0.1356, 0.0176, 0.1332). In a 5-component coherent structure (System 2), W3PM MAE is (0.0426, 0.0581, 0.0180, 0.0381, 0.0390) vs BSNP (0.0583, 0.0343, 0.0750, 0.0538, 0.0321), showing especially large gains for component 3. In the simulated bridge system, W3PM outperforms BSNP for all components (e.g., component 5: 0.0565 vs 0.1777). In the hard-drive dataset (n=172; 38% masked), posterior means for the Weibull location parameter are near 0 for all three causes, consistent with testing from time origin; reliability curves differ by component whereas BSNP yields the same curve for all components.",None stated.,"The method assumes mutual independence of component lifetimes, which can be violated in many coherent systems due to shared environment/load, common-cause failures, or dependent competing risks. Masking probabilities are modeled as time-invariant (and either type-specific or symmetric); if masking depends on failure time, diagnostic effort, or operating conditions, the model may be misspecified. The paper does not provide shared code/software, which limits reproducibility and makes implementation details (e.g., proposals/tuning and convergence diagnostics) harder to verify.","The authors note the approach is straightforward to extend to other lifetime distributions and even to a purely likelihood-based approach. They also state that accelerated life testing (ALT) settings can be handled by incorporating covariates/stress variables through an accelerated life model parameterization (e.g., via the Weibull scale).","Developing versions that relax independence (e.g., frailty/shared random effects, copula models, or common-cause components) would broaden applicability to realistic engineered systems. Adding time- or covariate-dependent masking models (e.g., logistic regression for masking probabilities) could improve fit when diagnosis quality changes over time or with conditions. Providing an open implementation (e.g., an R/Python package) and benchmarking against additional masked-data methods for series/parallel/hybrid systems would strengthen practical adoption and comparative evidence.",1707.03173v2,https://arxiv.org/pdf/1707.03173v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:57:20Z
TRUE,Life distribution modeling|Other,Stochastic process|Bayesian|Hybrid/Ensemble,Right-censored|Interval-censored|Sensor/condition monitoring|Mixture of types,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset),TRUE,Python|Other,Not provided,https://github.com/GPflow/GPflow,"The paper proposes a scalable Bayesian joint modeling framework (J-LTM) for uncertainty-aware event prediction from irregularly sampled multivariate longitudinal data with missingness and outliers. The longitudinal submodel uses sparse multiple-output Gaussian processes with a linear model of coregionalization and Student-t observation noise to flexibly model multiple clinical signals and quantify uncertainty. The time-to-event submodel is a survival model with an exponentially parameterized hazard function that depends on an exponentially weighted integral of the latent trajectories, enabling computation of a full predictive distribution over event (failure) probability within a horizon. A decision-theoretic prediction policy is derived that can abstain; it uses quantiles of the event-probability distribution to trade off false alarms, missed detections, and abstention cost. Experiments on the MIMIC-II ICU dataset for septic shock prediction (with right and interval censoring) show improved discrimination and alert quality over baselines, reporting AUC 0.84 vs ~0.78–0.80 and substantially better PPV at comparable TPR.","Survival/hazard are defined as $\lambda(t)=p(t)/S(t)=-\frac{\partial}{\partial t}\log S(t)$ and event probability within horizon $\Delta$ as $H(\Delta\mid\cdot)=1-\exp\{-\int_t^{t+\Delta}\lambda(s;t)\,ds\}$. The longitudinal model is $y_d(t)=f_d(t)+\epsilon_d(t)$ with $f_d(t)=\sum_{r=1}^R w_{dr}g_r(t)+\kappa_d v_d(t)$, where $g_r, v_d$ are GPs and $\epsilon_d(t)\sim t_3(0,\sigma_d)$. The hazard used is $\lambda(s;t)=\exp\{b+a(s-t)+\gamma^T x_t + \bar f(t)\}$ with $\bar f(t)=\alpha^T\int_0^t \rho_c(t';t) f(t')dt'$ and $\rho_c$ an exponentially decaying kernel; this yields closed-form $H(\Delta\mid\bar f,t)=1-\exp\{-\lambda(t;t)\frac{1}{a}(e^{a\Delta}-1)\}$. The abstention policy minimizes risk quantiles using event-probability quantiles $h^{(q)}$, leading to decision rule (Eq. 17) with thresholds depending on confidence width $c_q=h^{(q)}-h^{(1-q)}$ and costs $L_{01},L_{10},L_a$.","On septic shock prediction using MIMIC-II (3151 patients; ~12% shock; right- and interval-censoring handled), J-LTM achieves AUC 0.84 (SE 0.005), outperforming MoGP 0.79 (0.006), JM 0.78 (0.008), LR 0.80 (0.005), SVM 0.79 (0.007), and RNN 0.80 (0.006). At FPR = 0.2, J-LTM attains TPR 0.75 (SE 0.004) vs MoGP 0.62, JM 0.61, LR 0.57, SVM 0.62, RNN 0.59. For PPV-focused operation, J-LTM shows 13%–23% PPV improvement over MoGP and 18%–36% over standard baselines (JM/LR) in the reported TPR range (0.4–0.6). With PPV > 0.5, the best TPR for J-LTM is 0.68 (SE 0.01), exceeding MoGP 0.51, JM 0.40, LR 0.18, SVM 0.21, and RNN 0.12.","The authors assume missing-at-random (MAR) for the longitudinal missingness mechanism, noting that under MAR the missing-data process can be ignored for inference. They also avoid hazard models that depend on instantaneous features because this would require accurate long-horizon extrapolation (12–48 hours) of latent trajectories, which they describe as challenging for their domain. They note that their main computational bottleneck is per-individual local optimization, though it is parallelizable.","The survival component uses a specific parametric baseline hazard form (log-linear in time since prediction) and a particular exponentially weighted history summary; if the true risk depends on more complex or nonstationary temporal effects, calibration may degrade. The model’s validity relies on conditional independence assumptions (e.g., given latent functions) and on correct specification of correlation structure (LMC with fixed small $R=2$), which may underfit complex cross-signal dependencies. Evaluation is restricted to a specific clinical task and a fixed evaluation protocol (5 points per patient over a 2-day window), which may not reflect continuous real-world alerting performance or lead-time utility. No ablation isolating the abstention policy vs. the joint uncertainty propagation is fully quantified beyond the MoGP two-stage comparison, and external validation on other datasets/domains is absent.","They state that a distributed version of the algorithm could enable scaling to larger datasets with more patients and longitudinal signals, leveraging the embarrassingly parallel local optimization step. They also mention that more flexible longitudinal kernel formulations are possible (e.g., adding individual-specific length-scale random effects), though they did not see significant gains in their experiments. They note that non-linear feature representations for the time-to-event component could be incorporated as needed, citing related work on more complex covariate-to-hazard links.","Extending the framework to handle missing-not-at-random (MNAR) sampling processes (e.g., clinician-driven measurement frequency) via explicit observation models could improve realism and robustness. Developing self-starting/online updating and real-time computational optimizations (e.g., amortized inference for local parameters) would be valuable for deployment. Broadening to competing risks, recurrent events, and multi-event types (e.g., multiple adverse events) would align with many reliability/health monitoring settings. Providing open-source reference implementations and standardized benchmarking (including calibration, decision-curve analysis, and lead-time dependent utility) would improve reproducibility and operational relevance.",1708.04757v1,https://arxiv.org/pdf/1708.04757v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:58:07Z
FALSE,Other,Stochastic process|Simulation-based|Other,Simulated only,Not applicable,Healthcare/medical|Other,Simulation study,TRUE,MATLAB,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher),https://github.com/danielemarinazzo/GC_SS_PNAS,"This paper is a short letter/commentary addressing interpretability and computational reliability issues in frequency-domain Granger-Geweke causality (GGC) raised by Stokes and Purdon (2017). The authors replicate the simulations from the target critique and show that the standard approach (separate full and reduced finite-order VAR fits) can produce biased or highly variable spectral GGC estimates due to misspecification of the reduced model. They argue that this can be overcome using a state-space (SS) estimation approach that permits closed-form computation of spectral GGC from SS parameters, yielding estimates close to the theoretical profiles and negligible values in uncoupled directions. They also discuss why invariance of frequency-domain causality measures to receiver dynamics can be meaningful when interpreted via directed coherence as frequency-specific transmitted power contributions. Code for the analyses is provided as supplementary material and on GitHub, based on MATLAB scripts from prior referenced works.","The letter contrasts spectral GGC computed via fitting separate full and reduced VAR models (with model order p) against computing spectral GGC in closed form from a state-space representation of the underlying VAR process. It also notes that directed coherence (DC) is analytically related to pairwise spectral GGC, and uses the spectral decomposition $S_{2\mid 1}(f)=S_{22}(f)\cdot DC_{1\to 2}(f)$ to quantify the transmitter’s frequency-specific causal contribution to the receiver’s power spectrum.","In replicated simulations (100 runs), the classical full/reduced VAR approach shows a strong bias when using the true model order (e.g., p=3) and very large variability when using a higher order (e.g., p=20). The state-space estimation approach yields accurate spectral GGC profiles that closely match the theoretical causality curves in coupled directions and are near-zero in an uncoupled direction. In the second simulation, directed coherence $DC_{1\to2}$ can remain identical across systems with different receiver resonance properties, but the causal power contribution on the receiver differs through $S_{2\mid1}=S_{22}\cdot DC_{1\to2}$, clarifying interpretation.",None stated.,"This is not reliability engineering work; “computational reliability” is discussed in the sense of numerical/estimation stability rather than reliability of engineered systems. Results are based on specific simulated linear VAR/state-space setups, so robustness to nonlinearity, nonstationarity, measurement noise typical of real recordings, and model-selection uncertainty is not fully established here. The contribution is primarily comparative/interpretive and depends on previously developed SS-GC methodology rather than proposing a new reliability-related model.",None stated.,"Assess performance under more realistic data issues (finite-sample effects, nonstationarity, noise, downsampling, and model mismatch) and provide guidance for practical model order/SS dimension selection in applied studies. Extend the comparison beyond the specific examples to broader benchmarks and include additional frequency-domain directed measures to clarify when interpretational invariances are helpful versus misleading. Provide packaged, reproducible implementations and standardized validation tests to improve computational robustness across toolboxes and datasets.",1708.06990v1,https://arxiv.org/pdf/1708.06990v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:58:31Z
TRUE,Degradation modeling|Life distribution modeling|RUL prediction|Accelerated testing|Other,Stochastic process|Bayesian,Right-censored|Mixture of types|Other,Not applicable,Other,Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a specimen-level stochastic degradation model for the duration-of-load (DOL) effect in lumber using a gamma process, with failure defined as the degradation crossing a threshold (scaled to 1). It separates internal stochastic degradation in an individual piece from external, time-varying applied loads by making the gamma-process shape parameter a deterministic function of the load history, including a possible stress threshold below which degradation does not progress. A flexible time function for the shape parameter is introduced (a mixture of two power terms) to better capture long-term behavior than the usual single power-law form. Parameters are estimated via Bayesian inference using accelerated testing data (constant-load tests with right censoring and a ramp-load test), yielding wide posterior intervals that highlight the difficulty of extrapolating long-term reliability from short-duration accelerated tests. In an illustrative 50-year residential load-profile prediction, the model estimates a higher failure probability than a traditional accumulated-damage model (ADM), and it produces highly uncertain, long right-tailed residual-life predictions for survivors under sustained loads.","Degradation is modeled as a gamma process: increments satisfy $Y_{t_i}-Y_{t_{i-1}}\sim \text{Ga}(\text{scale}=\xi,\ \text{shape}=\eta_{t_i}-\eta_{t_{i-1}})$, with failure time $T=\inf\{t: Y_t\ge 1\}$. The survival function is $P(T>t\mid \xi,\eta_t)=P(Y_t\le 1)=1-\Gamma(\eta_t,1/\xi)/\Gamma(\eta_t)$. The load-history-driven shape parameter is discretized over load levels $\tau_1<\cdots<\tau_m$ as $\eta_t=\sum_{i=1}^m(\tilde t_i^{\,a}+b\tilde t_i^{\,c})\big[(u\tau_i-v)_+-(u\tau_{i-1}-v)_+\big]$, where $\tilde t_i=\int_0^t \mathbb{I}(\tau(t')\ge \tau_i)dt'$ and the population stress threshold is $\tau^*=v/u$.","Posterior means (gamma-process model) are reported as $a\approx 0.019$, $b\approx 0.01026$, $c\approx 0.40$, $u\approx 8.8\times 10^{-4}$, $v\approx 0.359$, and $\xi\approx 0.21$, with particularly large uncertainty for the long-term term $b$ (central 95% interval roughly $0.00071$ to $0.03732$). The inferred population stress threshold is weakly supported: posterior mean $\tau^*=v/u\approx 413$ psi with a wide 95% posterior interval (43, 642) psi. For a simulated 50-year residential dynamic load profile, the posterior mean probability of failure by 50 years is 0.090 with a central 95% posterior interval (0.055, 0.150), compared with 0.015 reported using the Canadian ADM on the same scenario. For residual life after surviving accelerated constant-load tests, the median remaining life under indefinite continuation has a 95% posterior interval of (21.9, 333.5) years at 3000 psi after 4 years survived, and (5.2, 24.3) years at 4500 psi after 1 year survived.","The authors state that accelerated testing data truncated at 1 and 4 years provide weak information about degradation over much longer horizons, leading to wide posterior/credibility intervals for long-term reliability and residual-life predictions. They note uncertainty is especially high for parameters governing longer-term degradation behavior (e.g., $b$), implying poor precision when extrapolating to 30–50 year service lives. They also mention they do not have data to incorporate other environmental degradation contributors (e.g., moisture content and temperature) as additional components in the degradation model.","The load-history model discretizes load levels (e.g., 20 psi bins), which can introduce approximation artifacts (jagged $\eta_t$) and may affect inference/predictions unless sensitivity to discretization is studied. The analysis is based on a single lumber species/grade dataset with no specimen covariates recorded, limiting generalizability and preventing assessment of variability explained by measurable properties (e.g., modulus of elasticity). The computational approach (parallel tempering over 120 cores) suggests nontrivial compute demands, yet no reproducible implementation details (software, code, diagnostics) are provided, making replication and practical adoption harder.","The paper suggests that more comprehensive testing is needed (larger accelerated tests and/or longer test durations) to better estimate long-term reliability under low sustained loads and to reduce posterior uncertainty. It also suggests encoding appropriate expert knowledge in Bayesian priors to better constrain long-term degradation behavior when data are insufficient. As a modeling extension, it notes that other external factors (e.g., time-varying moisture and temperature) could be incorporated as additional additive gamma-process components when suitable data become available.","Develop and validate a continuous-load formulation for $\eta_t$ (avoiding discretized thresholds) and quantify sensitivity of predictions to load-bin resolution and smoothing choices. Extend to incorporate autocorrelated or stochastic future load processes directly (hierarchical model for occupancy/live loads) and propagate that uncertainty jointly with degradation-parameter uncertainty. Add covariates and random effects (e.g., stiffness/strength surrogates) and evaluate transferability across wood products (including engineered composites) via multi-population hierarchical priors and out-of-sample validation on additional datasets.",1708.07213v1,https://arxiv.org/pdf/1708.07213v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T09:59:17Z
TRUE,System reliability|Network/infrastructure reliability|Other,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://doi.org/10.5281/zenodo.846898,"This paper proposes deep-learning surrogate models to accelerate simulation-based hazard reliability analysis of infrastructure networks, focusing on two-terminal connectivity of transportation networks under earthquake hazards. Two surrogates are developed: (1) a classifier surrogate that replaces repeated depth-first-search (DFS) connectivity checks for each Monte Carlo (MC) network realization, and (2) an end-to-end surrogate that maps roadway failure probabilities directly to expected two-terminal connectivity, bypassing MC sampling and averaging. The reliability pipeline includes earthquake scenario sampling, ground-motion prediction (GK15 GMPE), bridge fragility (HAZUS-MH) to obtain component failure probabilities, and network reliability estimation via MC. In a San Jose–Mountain View network case study, the classifier achieves >99.9% classification accuracy and about an order-of-magnitude speedup versus DFS-based evaluation, while the end-to-end surrogate accelerates expected-connectivity estimation by over four orders of magnitude compared to MC+DFS. The work demonstrates how surrogate modeling can enable rapid reliability evaluation for sensitivity analysis and planning tasks in large infrastructure systems subject to natural hazards.","Two-terminal connectivity for MC sample j is defined as a binary indicator $g_j(\mathbf{x}^{(j)})\in\{0,1\}$ for whether source and terminal nodes are connected (Eq. 5), and expected connectivity is estimated by Monte Carlo averaging $P_c=\frac{1}{N}\sum_{j=1}^N g_j(\mathbf{x}^{(j)})$ (Eq. 6). Roadway survival probability aggregates bridge survival probabilities via $\ln(p_i)=\sum_{j=1}^k \ln(p_{ij})$ (Eq. 4), with roadway state modeled as a Bernoulli variable (Eq. 3). The classifier DNN is trained with binary cross-entropy loss (Eq. 9) and the end-to-end surrogate with MSE loss (Eq. 8); QoI accuracy is summarized by $\alpha_{QoI}=1-|P_c-\hat P_c|/P_c$ (Eq. 11).","For fixed-magnitude earthquake scenarios (Mw 6.7–7.9) with 100,000 topology samples, the classifier surrogate matches DFS estimates closely (e.g., $P_c$ 0.9769–0.6263) and reduces runtime from ~5.2–6.1 s (DFS) to ~0.62–0.65 s (surrogate), with $\alpha_{binary}\approx 0.9995$–1.0000. For a probabilistic-magnitude event with 10,000,000 network realizations, expected connectivity is 0.9002 (DFS) vs 0.9001 (surrogate) with runtimes 603.97 s vs 53.89 s and $\alpha_{binary}=0.9997$. When including GMPE residual variability, expected connectivity drops to 0.6853 (both DFS and surrogate) with runtimes 574.48 s vs 53.08 s and $\alpha_{binary}=0.9990$. The end-to-end surrogate reproduces expected connectivity (0.9001) while reducing computation from 7,857.92 s (DFS+MC) to 0.71 s, i.e., >4 orders of magnitude speedup; it also enables rapid one-at-a-time sensitivity analysis (retrofit +10% survival) with per-run times ~0.72–0.75 s versus ~8,000+ s using DFS.",None stated.,"The approach is demonstrated on a single, relatively small transportation network (14 roadway links with bridges), so scalability and accuracy on much larger and more heterogeneous networks are not fully validated. Training data for the end-to-end surrogate is generated from the classifier surrogate (not exact DFS labels), which can propagate systematic bias and may be problematic when accuracies are lower (e.g., class imbalance for rare disconnection events). The framework largely relies on model assumptions (bridge-only vulnerability, specific GMPE/fragility models, sampling choices, independence structure) and does not deeply assess robustness to misspecification, correlated failures, or temporal recovery dynamics.",The authors suggest extending the framework via training-data augmentation to improve prediction accuracy (especially improving TNR by generating additional disconnected topologies from known no-connectivity cases). They also propose using GPUs to accelerate deep neural network training and prediction due to the parallelizable matrix operations in deep learning.,"Extend the surrogate framework to handle correlated component failures and spatially correlated ground-motion fields explicitly, and quantify robustness to alternative GMPEs/fragility models. Develop uncertainty-calibrated surrogates (e.g., Bayesian or conformal methods) to provide prediction intervals for $P_c$ and support risk-informed decisions. Generalize from two-terminal connectivity to richer resilience metrics (multi-terminal reliability, travel-time/flow-based performance, and post-event restoration with time-dependent reliability) and validate on multiple real networks and hazard types.",1708.08551v1,https://arxiv.org/pdf/1708.08551v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:00:00Z
TRUE,System reliability|Other,Hybrid/Ensemble|Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"This paper proposes an active-learning structural reliability method that combines sparse Polynomial Chaos Expansions (PCE) with bootstrap resampling to obtain local (pointwise) surrogate error estimates, addressing PCE’s typical lack of tail accuracy and uncertainty quantification near failure boundaries. The resulting algorithm, termed active bootstrap-PCE Monte Carlo simulation (A-bPCE), iteratively enriches an experimental design by selecting new model evaluations near the estimated limit-state surface, guided by a misclassification-probability learning function computed from bootstrap PCE replicates. Failure probability is estimated at each iteration via an inner Monte Carlo simulation over a large fixed sample, and convergence is assessed using the spread between the minimum and maximum bootstrap-based failure-probability estimates. The method is demonstrated on a multi-failure-region series-system benchmark (four-branch function) and two finite-element structural examples (a 2D truss and a 21D frame with dependent inputs), showing comparable or improved efficiency versus AK-MCS (Kriging-based active learning) at similar accuracy. Implementation and experiments are carried out in UQLab (MATLAB), but the paper does not provide released code.","Structural failure probability is defined by $P_F=\int_{\{x:g(x)\le0\}} f_X(x)\,dx$. The surrogate uses a truncated sparse PCE $\hat{Y}(x)=\sum_{\alpha\in A} y_\alpha\,\Psi_\alpha(x)$ with coefficients fitted by least squares; bootstrap resampling of the experimental design yields replicate PCE predictors $\hat{Y}^{(b)}(x)=\sum_{\alpha\in A} y^{(b)}_\alpha\Psi_\alpha(x)$ used to form local uncertainty bands. The active-learning acquisition is based on bootstrap classification disagreement: $U_{\mathrm{FBR}}(x)=\left|\frac{B_S(x)-B_F(x)}{B}\right|$, and the next point is chosen as $x^*=\arg\min_{x\in X_{\mathrm{MCS}}} U_{\mathrm{FBR}}(x)$; convergence uses $\frac{P_F^+-P_F^-}{\hat P_F}\le \varepsilon_{\hat P_F}$ with $P_F^+=\max_b P_F^{(b)}$ and $P_F^-=\min_b P_F^{(b)}$.","On the four-branch (series system) benchmark with reference $P_F=4.460\times10^{-3}$ (from $10^8$ MCS), A-bPCE converged with about $N_{tot}\approx167$ model evaluations to $\hat P_F=4.62\times10^{-3}$ with bounds $[4.5,4.7]\times10^{-3}$, while AK-MCS required about 200 evaluations for similar accuracy. On the 2D truss example (reference $P_F=1.52\times10^{-3}$ from $10^6$ MCS), A-bPCE achieved $\hat P_F=1.48\times10^{-3}$ with bounds $[1.43,1.54]\times10^{-3}$ using 129 model evaluations, versus AK-MCS at 300 evaluations; FORM underestimates ($0.76\times10^{-3}$) and SORM is accurate but costlier (372 runs). On the 21D frame example with importance-sampling reference $P_F\approx1.54\times10^{-3}$ (40,241 runs), A-bPCE gave $\hat P_F=1.49\times10^{-3}$ with bounds $[1.42,1.62]\times10^{-3}$ using 235 evaluations, whereas AK-MCS did not converge within 300 evaluations and retained very wide bounds.",None stated.,"The method’s performance depends on the quality and size of the fixed inner Monte Carlo sample ($N_{MCS}$ often set to $10^6$), which can be computationally heavy for repeated evaluations and may be inefficient for extremely small failure probabilities without variance-reduction. The bootstrap-based local uncertainty is heuristic for model-form error and may be sensitive to ED size, LARS basis selection stability, and dependence structures (e.g., via isoprobabilistic transforms) in high dimensions. Comparisons are mainly against AK-MCS with specific kernel choices and settings; broader benchmarking (different Kriging variants, subset simulation/IS within the loop, self-starting/unknown distributional settings) is limited. Practical guidance for choosing bootstrap size $B$, enrichment batch size $K$, and convergence tolerance $\varepsilon_{\hat P_F}$ beyond the tested ranges is not fully developed.","The authors suggest extending the inner simulation step beyond plain Monte Carlo by using variance-reduction reliability methods such as importance sampling, line sampling, or subset simulation to improve $\hat P_F$ estimation for very low failure probabilities. They also propose using remote/parallel computing during enrichment when adding multiple points per iteration for expensive models. Finally, they note that bootstrap-enabled local error estimation could be applied to other regression-based surrogates (e.g., low-rank tensor approximations).","Developing a self-adaptive strategy to select $N_{MCS}$, $B$, and enrichment batch size $K$ based on targeted uncertainty in $\hat P_F$ could reduce wasted computation and improve robustness across problems. Extending A-bPCE to explicitly handle autocorrelated or time-dependent limit-state evaluations (e.g., stochastic dynamics) and to system reliability with many correlated components/failure modes would broaden applicability. Providing theoretical guarantees or empirical calibration on how bootstrap misclassification relates to true surrogate classification error near $g(x)=0$ (especially under sparse/basis-selection instability) would strengthen reliability of the stopping rule. Open-source implementations (e.g., a reproducible UQLab script set) and standardized benchmarks would facilitate adoption and fair comparison.",1709.01589v2,https://arxiv.org/pdf/1709.01589v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:00:46Z
TRUE,System reliability|Network/infrastructure reliability|Other,Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Energy/utilities,Simulation study|Other,TRUE,MATLAB|Other,Not provided,http://www.garpur-project.eu|https://code.google.com/p/bnt,"The paper proposes a mutual-information-based Bayesian network approach to assess power system reliability, focusing on estimating the loss of load (LOL) index and ranking critical components/load buses that contribute to LOL. Training data are generated via non-sequential Monte Carlo simulation of component up/down states (generators, lines, buses), with importance sampling used to better capture rare outage events and improve estimation efficiency. Mutual information is computed between nodes to retain stronger dependencies and prune weaker edges, yielding a more informative Bayesian network structure for inference. The approach is demonstrated on the RBTS and IEEE RTS-24 test systems, where the learned Bayesian networks enable posterior analysis such as identifying the most probable component outages given LOL at specific buses and ranking load buses by conditional probabilities given LOL. Implementation is done in MATLAB using the Bayes Net Toolbox (BNT), and the learned structures are reported to be constructed quickly (sub-second) for the studied systems.","The Bayesian update rule is used as $P(A\mid B)=\frac{P(B\mid A)P(A)}{P(B)}$. Mutual information between discrete variables $X$ and $Y$ is computed as $MI(X,Y)=\sum_{x,y} P(x,y)\log\left(\frac{P(x,y)}{P(x)P(y)}\right)$ and is used to decide which dependencies (edges) to keep in the Bayesian network. The LOL-related operating policy is embedded via an optimization that minimizes weighted load curtailment $\min \sum_{i\in NC} W_i C_i$ subject to power flow balance/limits and bounds on generation and curtailment (as written in the paper).","The RBTS Bayesian network structure is reported to be constructed in 0.82 s, and the IEEE RTS-24 Bayesian network structure in 0.95 s (MATLAB/BNT implementation). For RBTS, a load-bus ranking table reports conditional probabilities $P(B=F\mid LOL=T)$ with buses ranked (Bus 3: 0.65; Bus 5: 0.53; Bus 6: 0.30; Bus 4: 0.22). For IEEE RTS-24, a load-bus ranking table reports (Bus 13: 0.66; Bus 20: 0.54; Bus 18: 0.33; Bus 15: 0.25; Bus 7: 0.20; Bus 2: 0.19; Bus 1: 0.12; Rest: <0.1). Component-ranking plots illustrate that dominant contributors differ by system and loading condition (e.g., lines more influential in RBTS, generators more influential in RTS-24; rankings shift between full and half load for Bus 13).","The authors note a downside that for a highly reliable system the Bayesian network structure would be complex with a large number of components. They also observe that, due to local generation or multiple transmission paths, some load buses may be missing from the learned structure when mutual information retains only critical components.","The method’s learned structure and parameters depend heavily on the quality and representativeness of Monte Carlo/importance-sampled simulated data; limited discussion is provided on sensitivity to sampling choices, mutual-information thresholds, or model-selection criteria. Components are modeled as binary up/down with (implicitly) independence assumptions in sampling, which may miss common-cause failures, weather-driven dependencies, and temporal/autocorrelation effects important in real grids. The evaluation is limited to standard test systems and does not quantify estimation error/uncertainty in LOL or provide systematic benchmarking against established reliability methods (e.g., variance reduction baselines, analytical approximations) under comparable computational budgets.",The authors state that future work is to extend the approach to a real-time system.,"A valuable extension would be to formalize structure-learning choices (e.g., MI thresholds, scoring/regularization) and report uncertainty bounds on LOL estimates and rankings (e.g., via Bayesian model averaging or bootstrap over simulations). Incorporating dependent failures (common-cause, weather, cascading) and time-sequential dynamics would improve realism, potentially via dynamic Bayesian networks. Releasing a reusable implementation (e.g., scripts/notebooks) and evaluating on real operational datasets would strengthen reproducibility and practical adoption.",1710.00324v1,https://arxiv.org/pdf/1710.00324v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:01:21Z
FALSE,NA,ML-based,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/kgrm/face-recog-eval,"This paper systematically evaluates how image degradations and certain model-related choices affect deep CNN face-verification performance on the LFW benchmark. Using four architectures (AlexNet, VGG-Face, GoogLeNet/Inception-v3, and SqueezeNet) trained on the VGG Face dataset, the authors degrade probe images with controlled levels of blur, JPEG compression, Gaussian and salt-and-pepper noise, brightness/contrast changes, and missing-pixel occlusions, and measure verification accuracy under the standard 10-fold LFW protocol. The main finding is that noise, blur, missing pixels (especially periocular/eye regions), and increased brightness significantly degrade performance across all models, while contrast reduction and JPEG compression generally have limited impact except at extreme compression. They also test descriptor computation via multi-patch averaging (1/5/10/30 patches) and color vs grayscale inputs, finding only marginal, generally not statistically significant gains from patch averaging and minimal accuracy loss when switching to grayscale. Trained models and evaluation code are made available in a public GitHub repository, and the paper identifies future research directions in image enhancement, better use (or omission) of color information, and recognition from partial data.","The feature extraction is defined as $\mathbf{y}=f(\mathbf{x})$, where $f(\cdot)$ is a pretrained CNN and $\mathbf{y}$ is the resulting face descriptor. Verification uses cosine similarity $\delta(\mathbf{y}_1,\mathbf{y}_2)$ and a threshold $T$: decide match if $\delta(f(x_1),f(x_2))\ge T$, otherwise non-match; $T$ is selected per fold following the LFW protocol.","Across degradations, high blur, high noise (Gaussian and salt-and-pepper), increased brightness, and missing pixels substantially reduce verification accuracy for all four CNNs; contrast reduction and JPEG artifacts have comparatively small effects until very severe compression. VGG-Face is generally most robust to noise and maintains the highest absolute accuracy across many conditions, while the tested GoogLeNet variant is often the least robust to blur/noise but appears relatively robust to missing data (smallest performance variation). Occluding/removing the periocular region is the most detrimental missing-data scenario for all models, followed by eye, nose, then mouth regions. Multi-patch descriptor averaging provides at most a small (~1–2%) and not statistically significant improvement (mainly for VGG-Face and SqueezeNet), and converting probe/target images to grayscale causes only marginal, not statistically significant drops for most models (except AlexNet).",None stated.,"The study is not a reliability-engineering analysis; it focuses on biometric verification robustness rather than modeling failure processes, lifetime, or maintenance decisions. Degradations are synthetically generated and applied primarily to probe images, which may not reflect real capture pipelines (camera noise models, motion blur, compression settings, and correlated covariates) or symmetric degradation of both images. Results are reported mainly as LFW mean accuracy (10-fold) without broader operational metrics (e.g., ROC/DET at fixed FAR) or evaluations across additional datasets/domains, limiting generalizability.","The authors suggest (i) applying image enhancement methods for blurred/low-resolution imagery to improve automated recognition, (ii) developing models that better exploit color information or alternatively discard it to build more compact models, and (iii) researching deep CNN approaches capable of recognition from partial/missing data (e.g., robust to occluded periocular regions).","Extend robustness evaluation to more operational biometric criteria (ROC/DET, TAR at fixed FAR) and include calibration/threshold stability under degradation. Study combined and correlated degradations (e.g., blur+noise+compression) and more realistic sensor-specific noise/blur models, including video and motion blur. Develop degradation-aware or uncertainty-aware models (e.g., quality-conditioned embeddings, domain adaptation) and validate across multiple unconstrained benchmarks beyond LFW for stronger external validity.",1710.01494v1,https://arxiv.org/pdf/1710.01494v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:01:58Z
FALSE,Other,ML-based|Other,Mixture of types|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,http://archive.ics.uci.edu/ml,"This paper proposes a “reliable classifier fusion” (RCF) strategy for radiomics prediction that fuses outputs from multiple imaging modalities and/or multiple classifiers. The method defines a per-classifier reliability based on agreement with other classifiers’ predicted labels and score similarity, and then combines reliability, classifier weights, and output scores using an evidential reasoning (ER) framework. To ease training, the authors derive an analytic evidential reasoning (AER) fusion rule (closed-form aggregation) under the no-local-ignorance setting. The approach is evaluated on five UCI binary datasets via cross-validation and on two clinical radiomics case studies (lung SBRT distant failure prediction using PET/CT + clinical parameters, and cervical cancer distant failure prediction using PET/CT with multiple classifiers). Across experiments, RCF yields more discriminative fused scores and higher AUC than weighted fusion and ER/DS-based fusion baselines, with best AUC reported in most datasets and both clinical studies.","RCF computes a reliability for classifier $i$ using label agreement $SL$ (number of other classifiers sharing the same label) and a score-similarity term $S_i$: $r_i = \frac{SL}{N-1} S_i$, where $S_i = 1-\prod_{j\neq i}(1-p_j)$ (as presented in Eqs. 15–16). Final fused class belief/probability is obtained via the analytic ER (AER) rule (Eqs. 35–36), which combines per-classifier output scores $p_{h,i}$ with learned weights $w_i$ and reliabilities $r_i$ into an explicit closed-form aggregate $p_h$; the final label is $\arg\max_h p_h$ (Eqs. 10–11).","On five UCI datasets, RCF achieves the highest AUC in all reported cases (Heart 0.88±0.01; Ionosphere 0.96±0.01; Mask 0.93±0.01; Sonar 0.85±0.01; Spambase 0.98±0.00), outperforming WF, DSF, and ERF. In lung SBRT multi-modality radiomics (n=52), RCF improves AUC to 0.86±0.01 versus 0.82–0.83 for WF/DSF/ERF and 0.82 for a single combined-feature model; specificity also rises to 0.95±0.01. In cervical cancer multi-classifier radiomics (n=75), RCF achieves AUC 0.83±0.02 versus ~0.80–0.81 for other fusion methods and 0.72–0.76 for individual classifiers; specificity reaches 0.84±0.03. A toy numerical example shows RCF yields more discriminative fused scores than simple weighted fusion (e.g., Group 1 fused Class I score 0.8393 in RCF vs 0.7 in WF).","The authors note that the presented RCF strategy is used only for binary classification and that extensions are needed for multi-class problems. They also state that the meaning and calculation of “reliability” may differ by application, implying that alternative reliability definitions/formulas may be required in different settings.","The reliability definition depends on agreement with other classifiers, which can reinforce correlated errors when classifiers share data, features, or inductive biases; the independence assumption required by the ER framework may be violated in typical radiomics pipelines. The clinical case studies have relatively small sample sizes (e.g., 52 and 75 patients), increasing risk of overfitting and optimistic performance estimates, especially with repeated feature selection and model tuning. The paper does not report calibrated probability assessment or decision-analytic impact (e.g., calibration curves, net benefit), which is important for clinical deployment. Code and implementation details for the full pipeline (feature extraction, IMIA settings, optimization/training of weights) are not provided, limiting reproducibility.","The authors suggest developing more suitable reliability calculation methods tailored to different applications, since reliability meaning/formula may vary. They also propose extending the RCF strategy beyond binary classification to multi-class classification problems. More broadly, they point toward a unified “multifaceted radiomics” model to address multiple ‘multi-’ challenges (devices/acquisition variability, segmentation, feature extraction/selection, and modeling).","Develop self-starting/robust variants of RCF that explicitly model dependence between classifiers (e.g., via copulas, stacking with correlation penalties, or Bayesian hierarchical fusion) rather than assuming independence. Evaluate robustness to label noise, class imbalance, and probability miscalibration, and add calibration objectives or post-hoc calibration (Platt/Isotonic) before fusion. Provide external validation on multi-institutional cohorts and prospective evaluation to assess generalizability. Release an open-source implementation (e.g., Python/R package) with reproducible pipelines for radiomics feature extraction, optimization, and fusion.",1710.01614v2,https://arxiv.org/pdf/1710.01614v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:02:31Z
FALSE,NA,Other,Other,Not applicable,Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is not about reliability engineering; it is a theoretical machine-learning/statistics study of clustering data generated from Bernoulli Mixture Models (BMMs) when the number of clusters is unknown. It introduces a new purity-testing statistic called Maximal Total Correlation (MTC), derived from empirical total correlation (a KL-divergence between the empirical joint distribution and the product of empirical marginals), to distinguish “pure” clusters (single Bernoulli model) from impure clusters (mixtures). Using MTC, the authors propose an exhaustive-search clustering procedure and prove non-asymptotic sufficient conditions on sample size n and dimension/informative-dimensions L that guarantee Probably Approximately Correct (PAC) clustering with error at most ε and confidence at least 1−ζ. Theoretical results provide exponential concentration bounds for the proposed statistics and yield polynomial sample-complexity upper bounds in ε^{-1} and ζ^{-1}, but the algorithm is exponential-time and thus not computationally efficient. The work is positioned as the first non-asymptotic sample-complexity bounds for PAC-clusterability of BMMs under separability and minimum-mixture-weight assumptions.","The BMM distribution is $P_{\mathrm{BMM}}(X)=\sum_{k=1}^K w_k \prod_{\ell=1}^L p^{(k)}_{\ell}{}^{X_\ell}(1-p^{(k)}_{\ell})^{1-X_\ell}$. Empirical total correlation for a submatrix $Q\in\{0,1\}^{m\times d}$ is $D(Q)=D_{\mathrm{KL}}(\hat P_{1,Q}\|\hat P_{2,Q})$, where $\hat P_{2,Q}(x)=\prod_{\ell=1}^d \hat p_\ell^{x_\ell}(1-\hat p_\ell)^{1-x_\ell}$. Maximal Total Correlation is $D_{\max}(Y;d)=\max_{Q\in\mathrm{Col}(Y;d)} D(Q)$, maximizing over all $d$-column submatrices of a cluster’s data matrix $Y$.","Under an $(\mathcal L,\delta)$-separability condition on component frequency vectors and a minimum mixture weight $w_k\ge \alpha$, the paper proves sufficient non-asymptotic conditions for PAC clustering: $\mathcal L\ge B\,\log^3(1/\varepsilon)\,/\,\varepsilon^{\,2+\frac{1-\alpha}{2(\alpha\delta)^2}}$ and $n\ge C\,\log^3(1/\varepsilon)\,/\,\varepsilon^{\,2+\frac{1-\alpha}{2(\alpha\delta)^2}}\cdot \log(L/\zeta)$. It establishes exponential tail bounds showing (i) for impure (mixture) data, $\Pr\{D(Q)\le \tau\}\le 2^{d+1}e^{-\beta n}$, while (ii) for pure (single Bernoulli) data, $\Pr\{D(Q)\ge \tau\}\le 2^{d+1}e^{-\beta d^2 n}$ with the same threshold form $\tau=\tfrac{\varepsilon}{2}(1+\log(K/\varepsilon))$. For MTC, the probability of failing to detect impurity decays exponentially in $n\times L$ (up to polynomial factors in $\binom{L}{d}$). The proposed clustering algorithm is information-theoretically valid but relies on exhaustive search over clusterings and is exponential in $n$.","The authors state that the paper does not focus on computational efficiency and that the proposed Algorithm 1 is exponential-time in $n$, so efficient PAC-learnability remains open. They also state that they are not aware of sample-complexity lower bounds for clustering/learning BMMs, so it is unclear whether their sufficient upper bounds are tight.","The guarantees rely on strong modeling assumptions: i.i.d. sampling, conditional independence of dimensions within each Bernoulli component, a known lower bound $\alpha$ on all mixture weights, and a separability condition requiring many coordinates to differ by at least $\delta$ between every pair of components. The main algorithm is not implementable for realistic $n$ due to exhaustive enumeration of clusterings and (implicitly) of many column subsets for MTC, so the practical value is mainly in the information-theoretic bounds. No real-data case study is provided to validate whether the separability/informative-dimension assumptions and required sample sizes are realistic in applied domains. The work provides only sufficient conditions; practitioners cannot readily diagnose whether a given dataset satisfies the separability parameters $(\mathcal L,\delta)$ or how to estimate them robustly.","They propose developing an efficient (polynomial-time) clustering/learning algorithm for BMMs as an important open problem, noting similar open questions even for Gaussian mixtures. They also explicitly suggest deriving sample-complexity lower bounds for BMM clustering/learning to assess whether their upper bounds in Theorem 1 are tight.","A natural extension is to replace exhaustive search with tractable algorithms (e.g., spectral/tensor methods or EM variants) and then prove comparable finite-sample PAC-style clustering guarantees under similar separability conditions. Another direction is robustness: analyze performance when dimensions are not independent within components, when samples are dependent (temporal/spatial correlation), or under model misspecification/noise. It would also be valuable to provide data-driven procedures to estimate or relax $(\mathcal L,\delta,\alpha)$ assumptions (e.g., adaptive thresholds for MTC) and to validate the theory on real binary datasets (genetics, clickstreams, network activity) with benchmarking against standard model-selection/clustering pipelines.",1710.02101v3,https://arxiv.org/pdf/1710.02101v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:02:58Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical|Food/agriculture|Other,Other,TRUE,None / Not applicable,Not provided,http://www.theheart.org/article/813719.do|https://en.wikipedia.org/wiki/Replication_crisis,"This paper evaluates the statistical reliability of claims made in 10 observational nutrition studies that were used as the evidentiary base for a published meta-analysis on sugar-sweetened beverages and type 2 diabetes/metabolic syndrome risk. The authors quantify each primary study’s potential analysis “search space” (outcomes × predictors × 2^c covariate-model choices) using counts extracted from both abstracts (Space A) and full text (Space T), highlighting the extent of multiple testing and multiple modeling opportunities. Reported search spaces are very large (median 6.5; range 2–12,288 in abstracts; median 196,608; range 3,072–117,117,952 in texts), and none of the 10 primary papers report multiplicity corrections. They further illustrate why small p-values can arise by chance via expected order statistics, and they recompute (Bonferroni) adjusted p-values from effect estimates in the meta-analysis table, concluding that adjusted significance largely disappears. The paper argues that biased/selected estimates from exploratory observational analyses undermine the assumptions (unbiasedness/independence) required for valid meta-analysis, so the meta-analysis conclusions lack evidentiary confirmation.","The core “analysis search space” is defined as $S = (\#\text{outcomes})\times(\#\text{predictors})\times 2^{c}$, where $c$ is the number of covariates (modeling choices) available. The paper also discusses z-tests derived from reported risk ratios and confidence limits (computing $Z$ from estimated log-effect $\beta$ and its standard error) and applies Bonferroni-style adjustments by multiplying nominal p-values by an adjustment factor related to the estimated search space.","Across the 10 primary studies, sample sizes range from 4,304 to 91,249 (median 28,897; total 332,357), and the smallest reported nominal p-values range from 0.0001 to 0.001 with largest reported RR ranging 1.23–5.06 (median 2.07). Estimated search-space sizes are extremely large: Space A median 6.5 (range 2–12,288) and Space T median 196,608 (range 3,072–117,117,952). The authors report that none of the 10 primary papers mention adjusting for multiple testing or multiple modeling. Using Bonferroni adjustment applied to p-values recomputed from the meta-analysis table, they state that there are no statistically significant results after adjustment, implying reported effects are plausibly biased/selection-driven.","The authors state that counting covariates is difficult because covariates can appear throughout a paper, and the number of potential covariates may exceed what can be counted because some may not be mentioned. They also note that without access to the raw datasets, it is not possible to properly adjust analyses for multiple testing and multiple modeling, limiting what can be definitively concluded from published summaries alone.","The search-space formula $\#O\times\#P\times 2^c$ is a rough upper bound and may substantially misrepresent the effective number of hypotheses actually tested (due to correlation among outcomes/predictors, hierarchical modeling choices, and analyst constraints), so multiplicity severity could be over- or under-estimated. The use of Bonferroni-style adjustment based on large, estimated search spaces can be overly conservative and is not aligned with typical dependency structures in epidemiologic analyses; the paper does not systematically compare alternative multiplicity-control methods (e.g., FDR, permutation/maxT) using actual data. Finally, the work audits reporting and analysis flexibility rather than performing a full reanalysis of each primary study’s modeling pipeline, so conclusions about bias magnitude remain suggestive rather than quantified.","The authors suggest that with access to raw data, resampling-based methods could be used to compute more accurate multiplicity adjustments (citing Westfall and Young). They also recommend greater transparency, including making datasets and analysis code available, and propose using tools like p-value plots to evaluate many tests simultaneously in FFQ-style studies.","A useful extension would be to obtain at least a subset of the primary datasets (or realistic synthetic replicas) and empirically estimate the effective number of tests under realistic dependence, comparing Bonferroni, Holm, maxT/permutation, and FDR controls. Another direction is to develop standardized, preregistered analysis protocols for FFQ observational studies (including fixed covariate sets and modeling decisions) and evaluate how preregistration changes replicability and effect-size bias. Finally, building an automated text-mining pipeline to extract outcomes/predictors/covariates from papers could scale the reliability audit to hundreds of nutrition meta-analyses.",1710.02219v1,https://arxiv.org/pdf/1710.02219v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:03:37Z
TRUE,Network/infrastructure reliability|System reliability|Life distribution modeling|Other,"Stochastic process|Parametric (Weibull, etc.)|Other",Simulated only|Other,Not applicable,Network/cybersecurity|Theoretical/simulation only|Other,Exact distribution theory|Approximation methods|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a percolation-theory-based approach to assess (approximate) the all-terminal network reliability polynomial for systems modeled as random graphs, including general inhomogeneous random graphs and scale-free graphs. It uses the bond percolation threshold to identify a critical edge-operational probability $p_c=\langle k\rangle/(\langle k^2\rangle-\langle k\rangle)$ (with scale-free variants via polylogarithms/zeta functions) at which the giant component disappears, linking this phase transition to loss of connectivity. Based on this threshold, it defines an assessed/critical reliability $\widehat{\mathrm{Rel}}_c(G,p_e)(t)$ as a tail probability over the number of operational edges, with a simplified binomial form under identical edge reliabilities. It connects this assessed reliability to a network lifetime concept (time when $\widehat{\mathrm{Rel}}_c= p_c$) and provides a Poisson approximation (with Le Cam error bound) to make computation feasible for large graphs. Examples include an exact match for $K_4$, a small inhomogeneous graph with exponentially decaying edge reliabilities, and a scale-free (internet-like) graph illustrating an abrupt reliability drop around the predicted threshold.","All-terminal reliability is defined as $\mathrm{Rel}(G,p_e)=\sum_{S\in O}\prod_{e\in S}p_e\prod_{e\notin S}(1-p_e)$, and for identical edge probabilities $p$ as the reliability polynomial $\mathrm{Rel}(G,p)=\sum_{i=0}^{|E|}F_i(1-p)^ip^{|E|-i}$. The percolation (giant-component) threshold is derived from a fixed-point condition and yields $p_c=\langle k\rangle/(\langle k^2\rangle-\langle k\rangle)$ (with scale-free forms using $\mathrm{Li}_\gamma$ or $\zeta$). The paper’s assessed reliability at time $t$ is defined as a tail sum over operational-edge counts, $\widehat{\mathrm{Rel}}_c(G,p_e)(t)=\sum_{i=[M_c]+1}^{N}\sum_{A\in[N]_i}\prod_{e\in A}p_e(t)\prod_{e\in A^c}(1-p_e(t))$, reducing to a binomial tail when all $p_e(t)=p(t)$; it also suggests a Poisson tail approximation using $\mu(t)=\sum_e p_e(t)$.","For $K_4$ with uniform edge reliability $p(t)$, the assessed reliability from the percolation-threshold tail sum matches the exact reliability polynomial, with $p_c=1/3$ and $[M_c]+1=3$ (so terms with 3 or more operating edges reproduce the exact expression). For an example inhomogeneous graph with 5 nodes and 8 edges, the computed threshold is reported as $p_c=0.421053$, and the plotted $\widehat{\mathrm{Rel}}_c$ drops abruptly at this level, implying a network lifetime between $t\approx 4$ and $5$ for the given exponential edge reliabilities. For scale-free graphs, the paper states that for $2<\gamma<3$ the transition occurs only at $p_c=0$ (no finite percolation threshold asymptotically), while for $\gamma>3$ a finite threshold exists; it notes $p_c$ is between 0 and 1 only for roughly $3<\gamma<3.48$ in the zeta-based example. In an internet-like power-law example with $\gamma=2.5$, $k_{\min}=1$, $k_{\max}=11$, $|E|=250$, and $p_e(t)=e^{-0.25t}$, the plotted lifetime is reported around $t\approx 3.36$.",None stated.,"The approach equates practical all-terminal connectivity reliability with a percolation/giant-component threshold phenomenon; however, existence of a giant component is weaker than full connectivity, so the approximation may be inaccurate for graphs where connectivity and giant-component emergence are well separated (especially finite graphs). The assessed reliability $\widehat{\mathrm{Rel}}_c$ is largely driven by the count of operational edges (a tail probability), which can ignore the actual topology/which edges fail; different edge sets of the same cardinality can have very different connectivity outcomes. The paper provides plots/examples but limited systematic validation (e.g., Monte Carlo comparisons across graph families/sizes) quantifying approximation error beyond the $K_4$ case and the Poisson-approximation bound. Implementation details (algorithms/software) and computational complexity for evaluating the proposed sums/approximations in realistic large networks are not provided.",None stated.,"A natural extension is to quantify accuracy by benchmarking $\widehat{\mathrm{Rel}}_c$ against exact or simulation-based all-terminal reliability across diverse finite graph families (including sparse graphs near the connectivity threshold) and heterogeneous edge reliabilities. Developing topology-aware approximations that condition on degree sequence/community structure (rather than only edge-count tails) could improve predictive fidelity for real networks. Extending the framework to dependent failures (common-cause, spatial correlation) and to node failures (beyond uniform $q_v$) would broaden applicability. Providing reproducible software (e.g., efficient Poisson-binomial tail computation and percolation-threshold estimation) would facilitate practical adoption.",1711.00303v1,https://arxiv.org/pdf/1711.00303v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:04:19Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"This paper studies the reliability of neural-network saliency/explanation methods rather than reliability engineering of physical systems. It introduces an “input invariance” axiom: if a transformation of the input does not change the model’s predictions (e.g., a constant shift compensated by biases), then a reliable saliency method should not change its attribution. Using MNIST and ImageNet/VGG16 examples, the authors show several attribution methods (e.g., gradient×input, Integrated Gradients with certain baselines, and Deep Taylor Decomposition/LRP with certain reference points) can be manipulated by constant shifts to produce misleading explanations while leaving the model function unchanged. They also show some methods or reference choices (e.g., PatternAttribution, Integrated Gradients with a “black image” baseline under mean-shift) can satisfy input invariance for the tested transformations, and that SmoothGrad inherits the invariance/sensitivity of its underlying method. The work argues that reference/baseline selection and preprocessing/normalization materially affect explanation reliability and calls for more systematic guarantees beyond case-by-case fixes.","Key constructions include the model-invariance to constant input shift: for first-layer pre-activation $z=w^T x + b$, shifting inputs by $m$ and adjusting bias as $b_2=b_1-w^T m$ yields identical activations and outputs, and preserves input gradients. The “gradient×input” attribution is $s_j=\frac{\partial f(x)_j}{\partial x}\odot x$, which changes under input shifts because $x$ changes. Integrated Gradients attributes relative to a baseline $x_0$: $s=(x-x_0)\odot \int_0^1 \frac{\partial f(x_0+\alpha(x-x_0))}{\partial x}\,d\alpha$ (approximated by a finite sum). PatternAttribution’s root point is $x_0=x-a\,w^T x$ with $a^T w=1$ and (linear case) $a=\frac{\mathrm{cov}[x,y]}{w^T\mathrm{cov}[x,y]}$, making it invariant to mean shifts in the tested setting.","Under a constant mean shift of MNIST inputs (from [0,1] to [-1,0]) with a compensating bias change, raw gradients, Guided Backprop, and PatternNet produce identical saliency maps (input invariant), while gradient×input changes and is not input invariant. Integrated Gradients and Deep Taylor Decomposition can be invariant or non-invariant depending on the chosen baseline/reference: IG with a “black image” baseline is invariant to the tested mean shift, whereas IG with a zero baseline is not; DTD with a PatternAttribution reference is invariant, while DTD/LRP (zero-root) is not. The authors demonstrate deliberate manipulation (“cat” constant-shift attack) where multiple methods show a cat-shaped attribution despite identical model predictions, with PA remaining robust in that example. SmoothGrad does not fix invariance issues; it inherits the underlying method’s sensitivity (non-invariant when wrapping GI, IG-zero, or DTD/LRP; invariant when wrapping PA or IG-black for the tested shift).","The authors state their treatment of input invariance is restricted to showing at least one transformation (a constant vector shift) that breaks many saliency methods, and that their proposed mitigation via data normalization is not systematic and does not guarantee reliable attribution for all possible transformations. They also note their MNIST classifier is not state-of-the-art (98.3% accuracy) but claim this gap does not significantly influence their findings.","The evaluation is largely qualitative/illustrative (visual heatmaps) and does not provide standardized quantitative robustness metrics over a wide range of transformations, datasets, and architectures, limiting generalizability. The invariance analysis focuses on transformations that can be exactly compensated by bias adjustments; many real preprocessing changes (e.g., scaling, whitening, clipping, feature engineering) may not preserve functional equivalence in the same way, and the paper does not systematically cover these. The work does not provide implementation details, code, or reproducible experimental settings (e.g., IG step counts, SmoothGrad parameters) in the provided text, which can affect attribution behavior materially.","They call for wider research to systematically understand and choose reference/baseline points that guarantee reliability, noting normalization can help for the specific transformation studied but is not a general solution. They emphasize the need to evaluate which methods and/or reference points consistently guarantee reliability across possible transformations, especially in domains (e.g., language/audio) where visual inspection is difficult and failures are harder to detect.","Develop formal robustness criteria and benchmark suites for explanation methods that quantify invariance/sensitivity under families of function-preserving transformations (e.g., shifts, scalings, affine transforms, invertible preprocessing pipelines). Create self-starting or data-adaptive baseline selection procedures for reference-based methods (IG/DTD) with theoretical guarantees and practical diagnostics to detect baseline-induced artifacts. Provide reproducible open-source implementations and standardized parameter settings to enable fair cross-method comparisons and to assess computational cost/feasibility in real deployments.",1711.00867v1,https://arxiv.org/pdf/1711.00867v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:04:58Z
FALSE,NA,Bayesian|Stochastic process|Other,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://code.google.com/p/hsblock/,"This paper proposes fast, scalable variational Bayes inference algorithms for hierarchical stochastic block models (hSBM) and hierarchical degree-corrected stochastic block models (hDSBM) for network clustering. The key modeling idea is to share block-to-block interaction parameters hierarchically via a binary dendrogram, reducing parameter complexity from O(K^2) to O(K). For inference, the authors develop mean-field and locally-collapsed variational updates, and introduce dynamic-programming/lazy-evaluation schemes plus optional deterministic hard-assignments to speed convergence. Performance is evaluated on synthetic LFR benchmark graphs using normalized mutual information and on six real-world networks via link-prediction (AUPRC), where hDSB/hSB are consistently top-ranked among several scalable baselines. Runtime experiments show near-linear scaling in number of edges when using a restricted/local computation strategy.","The (hierarchical) SBM likelihood ties edge probabilities to the lowest common ancestor (LCA) of group labels in a dendrogram: $p(A\mid g,\theta)=\prod_{i<j}\theta_{\mathrm{LCA}(g_i,g_j)}^{A_{ij}}(1-\theta_{\mathrm{LCA}(g_i,g_j)})^{1-A_{ij}}$ with Beta priors on $\theta$. The hierarchical degree-corrected SBM uses a Poisson likelihood with degree-based scaling $\rho_{ij}$: $p(A\mid g,\lambda)=\prod_{i<j}\frac{(\lambda_{\mathrm{LCA}(g_i,g_j)}\rho_{ij})^{A_{ij}}}{A_{ij}!}e^{-\lambda_{\mathrm{LCA}(g_i,g_j)}\rho_{ij}}$ with Gamma priors on $\lambda$. Variational inference uses a factorized surrogate $q(z,\lambda)=\prod_i q(z_i\mid\mu_i)\prod_r \mathrm{Gam}(\lambda_r\mid\alpha_r,\beta_r)$ and updates global parameters via aggregated sufficient statistics (e.g., $\alpha_r=a_0+\sum_{a,b}\sum_{i<j}\mathbf{1}\{r=\mathrm{LCA}(a,b)\}\mu_{ia}\mu_{jb}A_{ij}$; similarly for $\beta_r$).","On LFR benchmark networks (e.g., average degree 10; n=5,000 and 10,000; varying noise and max group size), hDSB (both mean-field and locally-collapsed VI) achieves higher normalized mutual information than Metis, hSB, and CNM across most conditions. In link-prediction cross-validation on six real networks (NCAA, NetSci, Hep-Th, Cond2005, Reactome, CoExp), either hDSB or hSB is consistently top-ranked across missing-link fractions, outperforming HRG sampling and other baselines in many cases. Runtime experiments show the restricted/local computation variant scales approximately linearly with number of edges, while the full computation grows much faster, with comparable accuracy reported between restricted and full variants.","The authors note that variational inference does not guarantee convergence to the global optimum and can get trapped in bad local optima, motivating careful initialization (iterative bisections) and/or restarts. They also indicate that fully evaluating updates can become expensive for large K, and introduce restricted/local computation as an approximation to improve scalability.","Despite using the term “reliable,” the work is not reliability engineering; it focuses on statistical inference for network community detection, so there is no connection to lifetime/failure/degradation data or maintenance decisions. The evaluation emphasizes clustering accuracy (NMI) and link prediction (AUPRC) but provides limited guidance on uncertainty calibration, sensitivity to model misspecification (e.g., non-Poisson edges, temporal dependence), or robustness to correlated/attributed networks. Implementation details (language, dependencies, reproducibility) are not described in the provided excerpt beyond a code link, and the Google Code host is legacy/obsolete which may hinder long-term reproducibility.","The paper suggests improving robustness against poor local optima via better initialization (iterative bisection) rather than relying on many random restarts, and proposes locality-based approximations (restricting computation to a small subtree) to further speed up inference when K is large. It also describes final pruning via Bayes factors to automatically determine the number of groups from data, implying continued refinement of model selection and scalability.","A useful extension would be to handle temporal/dynamic networks (time-varying communities) and quantify uncertainty in the inferred hierarchy (e.g., credible intervals over dendrogram structure or assignments). Additional robustness studies could examine heavy-tailed weights, degree-degree correlations beyond simple correction, and sensitivity to preprocessing choices (e.g., removing degree≤2 nodes). Providing a maintained, modern repository (e.g., GitHub) with scripts to reproduce all benchmark and real-data results, plus standardized datasets/splits, would materially improve reproducibility and uptake.",1711.05150v1,https://arxiv.org/pdf/1711.05150v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:05:34Z
TRUE,Maintenance optimization|System reliability|Network/infrastructure reliability|Other,Simulation-based|Other,Simulated only|Other,Condition-based|Other,Energy/utilities,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an efficient methodology to mitigate cascading blackout risk in large power systems by optimally selecting components for maintenance under a resource constraint. It derives an explicit, data-driven analytic relationship between maintenance decisions (which change component failure probability functions) and an unbiased estimator of blackout risk using a fixed set of cascading outage simulation samples via a likelihood-ratio/importance reweighting scheme. The resulting maintenance selection problem is formulated as a nonlinear 0–1 program that minimizes estimated blackout risk subject to a maximum number of maintained components. To ensure the optimization uses a credible risk estimate, the authors develop a variance-based credibility analysis (CLT-based) and an adaptive rule to determine the minimum required Monte Carlo sample size. Two heuristic solvers (a sensitivity-based scenario reduction plus enumeration, and a greedy successive selection algorithm) are proposed and demonstrated on IEEE 57-bus and 300-bus systems, showing large computational savings versus resimulating for each maintenance strategy and meaningful blackout risk reductions.","Blackout risk is defined as $R_g(Y_0)=\mathbb{E}[Y\,\mathbf{1}\{Y\ge Y_0\}] = \sum_{z\in\mathcal{Z}} g(z)h(z)\mathbf{1}\{h(z)\ge Y_0\}$. Using samples $z^i\sim g$, the maintained-system risk under distribution $f$ is estimated by importance reweighting: $\hat R_f(Y_0)=\frac1N\sum_{i=1}^N w(z^i)h(z^i)\mathbf{1}\{h(z^i)\ge Y_0\}$ with $w(z)=f(z)/g(z)$. With binary maintenance decisions $m_k\in\{0,1\}$ and per-component path terms $\Gamma(\phi_k,z)$, they express $f(z^i)=\prod_{k\in K}[m_k\Gamma(\bar\phi_k,z^i)+(1-m_k)\Gamma(\phi_k,z^i)]$ and obtain an explicit objective (Eq. 12/13) optimized subject to $\sum_{k\in K^*} m_k\le M_{\max}$. Credibility uses CLT and an estimated variance $\hat D_f(Y_0)=\frac{1}{(N-1)N}\sum_{i=1}^N\left[w(z^i)h(z^i)\mathbf{1}\{h(z^i)\ge Y_0\}-\hat R_f(Y_0)\right]^2$, leading to an adaptive required sample size formula (Eq. 23).","On the IEEE 57-bus system (17 transformers), maintaining certain individual transformers yields outsized benefits (e.g., transformer 7 reduces estimated blackout risk by about 2.8% for $Y_0=0$ vs an average ~0.9% across components; for $Y_0=100$, transformer 7 gives ~5.4% reduction). For maintaining four transformers simultaneously, the best strategies achieve about 3.5% reduction for $Y_0=0$ and about 6.2% reduction for $Y_0=200$, illustrating strong nonlinearity (the combined reduction is much smaller than the sum of individual reductions). In a heuristic comparison on the 57-bus case with $|Z_g|=35{,}000$ and $M_{\max}=4$, both heuristics found the global optimum for $Y_0=200$ with only 62–210 evaluated scenarios versus 2380 for full enumeration. On the IEEE 300-bus system (107 transformers) with target relative error 10% and 95% confidence, the adaptive procedure increased sample size from 5,000 to ~95,000 to meet credibility, and the final 8-transformer maintenance strategy achieved about 21.5% blackout-risk reduction for $Y_0=0$; total runtime was dominated by sampling (~107 minutes) while optimization itself was under a minute.","The authors note that the most time-consuming step is generating the cascading-outage sample set using conventional Monte Carlo, which limits scalability for large-scale systems. They also state that the chosen blackout simulation model and component failure probability functions are simplified and used mainly to demonstrate the method; more realistic models/settings may be needed for practical deployment. They further acknowledge that the heuristic algorithms may yield sub-optimal solutions in some cases due to nonlinearity and sample uncertainty.","The approach assumes the base sample set $Z_g$ generated under $g(Z)$ provides sufficient support for reweighting to the post-maintenance distribution $f(Z)$; large probability changes can cause weight degeneracy/high-variance estimates, potentially undermining optimization quality. The CLT-based credibility analysis presumes a “large enough” $N$ and i.i.d. samples; cascading outage simulators and variance estimates may violate independence (e.g., common random numbers, adaptive sampling) or have heavy-tailed outcomes, affecting coverage. Maintenance is modeled only through changes in component failure probability functions and (in case studies) restricted largely to transformers, omitting costs, durations, logistics, imperfect maintenance effects, and time/age dynamics that are central in real maintenance optimization. The optimization objective focuses on expected load shedding beyond a threshold and does not incorporate multi-criteria operational constraints or risk measures (e.g., CVaR) that might better reflect tail risk of rare blackouts.",They propose replacing conventional Monte Carlo with higher-efficiency sampling methods (explicitly mentioning Sequential Importance Sampling and splitting) to further improve scalability and efficiency. They also indicate ongoing work to include a more practical formulation of maintenance strategy optimization and corresponding solution algorithms.,"Develop variance-reduction/robust reweighting techniques (e.g., weight clipping, control variates, stratification) and diagnostics for effective sample size to prevent instability when maintenance causes large distribution shifts. Extend the model to include maintenance cost, resource schedules, outage constraints, imperfect maintenance, and time-dependent/age-based failure probability evolution to enable multi-period planning. Validate the methodology on real utility data or high-fidelity industry simulators and compare against alternative risk-based maintenance frameworks (e.g., risk-averse objectives like CVaR, N-1 security constraints). Provide an open-source implementation (or reproducible experiment scripts) and benchmark datasets to facilitate adoption and fair comparison.",1711.09761v1,https://arxiv.org/pdf/1711.09761v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:06:16Z
FALSE,NA,Other,Other,Not applicable,Finance/economics,Other,TRUE,None / Not applicable,Not provided,NA,"The paper evaluates the trustworthiness of reported/collected financial market data (FTSE Global monthly price indices transformed to monthly log-returns) for 10 industry sectors across six major developing countries over 2000–2014 using Benford’s Law (first significant digit, FSD) and complementary distribution-distance measures. It computes FSD frequencies for each sector and compares them to Benford’s expected logarithmic distribution using a chi-square goodness-of-fit statistic with 8 degrees of freedom. The authors identify visually anomalous, repetitious values in the raw Datastream-sourced series, delete these suspected error values, and then re-run the Benford and distance analyses. After cleaning, sectoral FSD distributions generally move closer to Benford’s distribution (smaller distances and often improved correlations), though many chi-square tests still reject exact Benford conformity at 1% significance. The work positions Benford-based screening as an early-stage data quality/reliability check for financial reporting/market datasets rather than definitive fraud proof.","Benford’s Law for the first significant digit: expected count/frequency for digit $d\in\{1,\dots,9\}$ is $P(d)=\log_{10}(1+1/d)$ (equivalently $N_d = N\log_{10}(1+1/d)$). Goodness-of-fit uses $\chi^2=\sum_{i=1}^9 (e_i-b_i)^2/b_i$, where $e_i$ are observed FSD frequencies and $b_i$ are Benford-expected frequencies. A complementary Euclidean distance is defined as $d^*=(1/M)\sqrt{\sum_{i=1}^9 (e_i-b_i)^2}$ with $M=\max_i |e_i-b_i|$, and an additional mean-digit-based distance $a^*$ (difference of means normalized by the maximum possible difference).","Using raw data, all 10 sectors show large chi-square statistics with 99% significant departures from Benford (e.g., FIN $\chi^2\approx 25.18$, HEA $\chi^2\approx 90.18$; others mostly \~36–57). After removing abnormal repetitious values, distances generally decrease and correlations often improve (e.g., OIL correlation rises to \~0.985 with $d^*\approx 0.0395$), indicating better agreement with Benford; however, chi-square rejections at 1% remain for all sectors (e.g., GDS $\chi^2\approx 30.94$, HEA $\chi^2\approx 78.92$). The health care (HEA) sector is a persistent outlier with low correlation (\~0.44 raw, \~0.55 cleaned) and relatively large distance measures (raw $a^*\approx 0.278$, cleaned $a^*\approx 0.217$). The financials (FIN) sector slightly worsens in Benford similarity after cleaning per the authors, though the change is small and FIN retains the lowest chi-square among sectors (\~25–26).","The authors stress that Benford conformity is not definitive proof of “clean” data and Benford nonconformity is not conclusive proof of fraud/manipulation; deviations can arise for reasons unrelated to data quality (e.g., structural shifts). They note limitations of the FTSE index as a market proxy (it uses selected portfolios) and missing sector coverage for some country-sector pairs (e.g., no Mexico technology index), which constrains the country set to six. They also acknowledge that anomalous repetitions have no certified interpretation; removal is motivated by suspicion of Datastream technical issues rather than confirmed error labeling.","The cleaning step (deleting repeated values) is heuristic and may introduce selection bias; the paper does not formalize an objective anomaly-detection rule or quantify sensitivity of results to the cleaning criterion. The approach implicitly assumes independence/appropriateness of applying Benford’s Law to (percentage) log-returns; returns can be bounded/skewed and centered near zero, conditions under which Benford behavior is not guaranteed, yet this is not deeply analyzed. The study aggregates across countries and time, which can mask heterogeneous digit behavior by country/period and may confound interpretation of Benford deviations as “data reliability” issues.","The paper concludes that complementary accounting techniques should be considered before deciding whether data are faked or erroneous, implying use of additional/alternative tests beyond first-digit Benford analysis. It also argues that reliability checks should be performed before further modeling and analysis of such financial datasets, suggesting broader adoption of early-stage data-quality screening.","A useful extension would be to pre-register and validate an automated anomaly-detection procedure (e.g., rule-based or robust statistical detection) for repetitious/erroneous values and assess how Benford metrics change under alternative cleaning strategies. The analysis could be stratified by country and subperiod (e.g., crisis vs. non-crisis) to separate structural shifts from data-quality artifacts. It would also be valuable to compare Benford-based diagnostics on price levels versus returns and to evaluate higher-order digit tests (second digit, first-two digits) with appropriate corrections for multiple testing and dependence.",1712.00131v1,https://arxiv.org/pdf/1712.00131v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:06:59Z
TRUE,Life distribution modeling|Maintenance optimization|Other,"Parametric (Weibull, etc.)",Complete lifetime data,Condition-based|Predictive,Food/agriculture|Transportation/logistics,Other,TRUE,None / Not applicable,Not provided,NA,"The paper analyzes time-between-failure data for two key components (pricker and transmission) of two sugarcane harvesters to support reliability-centered maintenance. It fits five 3-parameter generalizations of the Weibull distribution (generalized gamma/Gamma-Weibull, generalized Weibull, exponentiated Weibull, Marshall–Olkin Weibull, and extended Poisson–Weibull) using maximum likelihood estimation. Model selection is performed using TTT-plots to assess hazard-shape flexibility and discrimination via AIC/AICc, with goodness-of-fit checked by Kolmogorov–Smirnov tests. Based on the best-fitting parametric model for each component/machine, the authors propose predictive maintenance schedules using a chosen percentile (25%) of the fitted lifetime distribution, with uncertainty for the percentile estimate obtained by bootstrap. The resulting recommendations are roughly 3 days for pricker (both machines), 3 days for transmission (machine A), and 7 days for transmission (machine B).","The work models component lifetimes with parametric generalized Weibull-family distributions, estimating parameters by maximizing the log-likelihood (e.g., for generalized gamma $f(t)=\frac{\alpha}{\Gamma(\phi)}\mu^{\alpha\phi} t^{\alpha\phi-1} e^{-(\mu t)^\alpha}$; for exponentiated Weibull $f(t)=\alpha\phi\sigma^{-1}(t/\sigma)^{\alpha-1}e^{-(t/\sigma)^\alpha}(1-e^{-(t/\sigma)^\alpha})^{\phi-1}$; and for extended Poisson–Weibull $f(t)=\frac{\alpha\lambda\beta t^{\alpha-1} e^{-\beta t^\alpha-\lambda e^{-\beta t^\alpha}}}{1-e^{-\lambda}}$). The predictive maintenance time is taken as a quantile (25th percentile) $y^*=Q(0.25\mid\hat\theta)$ using the fitted model’s closed-form quantile when available, with bootstrap used for a CI on $y^*$. Model discrimination uses $\mathrm{AIC}=-2\ell(\hat\theta)+2k$ and $\mathrm{AICc}=\mathrm{AIC}+\frac{2k(k+1)}{n-k-1}$; KS statistic is $D_n=\sup_t |F_n(t)-F(t;\theta)|$.","For pricker (machine A), exponentiated Weibull is selected (min AIC/AICc: AIC 340.435; AICc 334.981) and the 25% predictive time is $y^*=3.093$ days with 95% bootstrap CI (1.7374, 4.0236). For pricker (machine B), exponentiated Weibull is again selected (AIC 381.790; AICc 376.226) with $y^*=2.497$ days and CI (1.8212, 3.5760). For transmission (machine A), generalized gamma is selected (AIC 368.074; AICc 362.563; generalized Weibull rejected by KS p=0.0035) with $y^*=2.807$ days and CI (1.8487, 4.3526). For transmission (machine B), extended Poisson–Weibull is selected (AIC 201.997; AICc 203.140) with $y^*=6.748$ days and CI (3.8716, 9.5585).","The authors note that further work is needed to go beyond the fitted parametric models by incorporating the structure of recurrent event data and evaluating forecast accuracy. They also suggest implementation aspects (e.g., turning the approach into an application) are not covered in the current study.","The analysis appears to treat observed times between failures as i.i.d. complete lifetime observations, which may be questionable for repairable systems where dependence, changing operating conditions, and imperfect repairs can induce non-stationarity. Using the KS test with parameters estimated from the same data can yield miscalibrated p-values unless adjusted (e.g., via parametric bootstrap), which is not discussed. The maintenance rule is based on a fixed 25th percentile without an explicit cost/downtime optimization model, so the recommended intervals may not be economically optimal. No software, implementation details, or reproducibility materials are provided, limiting practical adoption and verification.","They propose extending the modeling to account for recurrent event data structure, implementing the methods, and analyzing forecast accuracy. They also suggest implementing the approach as an app to assist maintenance personnel with individualized scheduling.","A natural extension is to model the two machines/components with hierarchical or frailty models to share information while allowing unit-level heterogeneity. Incorporating covariates from onboard computer data (usage intensity, environment, load) in accelerated failure-time or proportional hazards frameworks could improve prediction and interpretability. Developing an economic/availability-based optimization that selects the percentile/threshold to minimize expected cost (preventive vs corrective, downtime, spare parts) would make the maintenance recommendation decision-theoretic. Robustness checks for censoring, imperfect maintenance (virtual age), and dependency/autocorrelation in failure sequences would strengthen validity for real repairable-system settings.",1712.03304v1,https://arxiv.org/pdf/1712.03304v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:07:42Z
FALSE,Other,ML-based,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Transportation/logistics,Simulation study,TRUE,None / Not applicable,Not provided,http://papers.nips.cc/paper/4683-multimodal-learning-with-deep-boltzmann-machines.pdf,"The paper proposes a mobile edge analytics architecture for intelligent transportation systems (ITS) in which data processing is pushed to passengers’ mobile devices, intra-vehicle processors, and roadside sensors to improve latency and operational reliability compared to cloud-only analytics. It surveys key ITS edge analytics challenges (heterogeneous multimodal sensing, autonomous control/path planning, vehicular platooning, driver behavior prediction in mixed autonomy, and cyber-physical security) and maps each challenge to suitable deep learning methods. Specifically, it advocates multimodal RBMs for heterogeneous sensor fusion, CNN+LSTM pipelines for perception and sequential decision-making in path planning, CNN-based similarity detection plus V2V/V2X sharing for platoon information reuse, RNNs for driver behavior prediction, and GAN/LSTM-based mechanisms for attack detection and message authentication. A small illustrative simulation shows that sharing decisions from a platoon leader can substantially reduce total data generation compared to every vehicle sensing/processing independently, supporting the edge-sharing motivation. Overall, the contribution is primarily an ITS edge computing/deep learning perspective paper rather than a reliability engineering model of component failure or lifetime.",Not applicable,"In a three-vehicle platoon simulation, total generated data is about 250 Mbps when all vehicles sense/process independently; with leader decision sharing, it drops to about 150 Mbps once the leader and first follower reach steady state (after ~75 s), and further to about 75 Mbps once the second follower reaches steady state (after ~225 s). For the proposed LSTM-based fingerprinting security check performed at the cloud, the reported added delay is about 0.1 seconds in the illustrated example. These results are presented as preliminary/illustrative benefits of edge analytics and information sharing rather than as a comprehensive benchmark.","The authors note multiple practical challenges: deep learning needs large accurate training datasets and edge-processed results can be more error-prone due to limited edge device capabilities. They highlight difficulty handling unpredictable situations (random driver behavior/environmental effects) which can increase reaction delay and cloud queries due to storage/security constraints. They also state resilience concerns (accidents can damage edge devices, requiring fallback to neighbors/cloud), robustness to adversarial perturbations/noise, and that DNN training can be delay-intensive.","The paper frames “reliability” at the system/service level (latency/secure operation) but does not formalize reliability metrics (e.g., packet error rate, safety risk, or probabilistic guarantees) nor connect to standard dependability/reliability engineering models. The simulation evidence is limited (single illustrative scenario, unclear assumptions and variability) and does not provide statistical comparisons across baselines, environments, or workload/network conditions. The proposed methods are largely conceptual mappings to deep learning architectures without implementation details needed for reproducibility (model sizes, training regimes, datasets, edge hardware constraints, or communication overhead of model sharing).","The authors explicitly list open problems for future research: ensuring training data accuracy and supervision; accounting for unpredictable situations and driver/environment randomness; improving resilience to accidents and enabling rapid recovery when edge devices fail; modeling interactions between human drivers and self-driving vehicles (suggesting combining game theory with RNNs); improving robustness against adversarial manipulation; and reducing delay-intensive DNN training (e.g., via federated learning).","A valuable extension would be to define and evaluate quantitative dependability metrics (end-to-end reliability/availability, safety constraint violation probability, attack detection ROC/latency tradeoffs) under realistic wireless channel and mobility models. Another direction is to develop communication-efficient collaborative learning/sharing schemes (compressed model updates, uncertainty-aware sharing, formal privacy guarantees) and to benchmark them on public ITS datasets with standardized scenarios. Finally, integrating formal verification or runtime assurance layers with edge deep learning could help provide safety/reliability guarantees for autonomy decisions under distribution shift and adversarial conditions.",1712.04135v1,https://arxiv.org/pdf/1712.04135v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:08:12Z
FALSE,Other,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/bethgelab/foolbox,"This ICLR 2018 conference paper introduces the Boundary Attack, a decision-based adversarial attack that requires only the final model decision (top-1 label) rather than gradients or confidence scores, enabling reliable black-box attacks in realistic deployment settings. The method starts from an already-adversarial example and performs a rejection-sampling random walk along the decision boundary to reduce the perturbation magnitude while staying adversarial, with dynamic step-size adjustment inspired by trust-region ideas. The authors benchmark the attack on MNIST, CIFAR-10, and ImageNet (VGG-19, ResNet-50, Inception-v3), reporting median squared L2 perturbation scores and showing competitiveness with strong gradient-based baselines (FGSM, DeepFool, Carlini–Wagner) in both untargeted and targeted settings, albeit with many more model queries. They also demonstrate that decision-based attacks bypass gradient-masking defenses such as defensive distillation, achieving similar perturbation sizes on distilled vs. standard networks. Finally, they apply the attack to real-world black-box models from Clarifai (brand and celebrity recognition), showing practical misclassification results with generally larger perturbations than on ImageNet models. Implementation is provided via the Foolbox toolbox.","The attack seeks an adversarial \(\tilde{o}\) minimizing the squared L2 distance \(d(o,\tilde{o})=\|o-\tilde{o}\|_2^2\) while satisfying an adversarial criterion \(c(\tilde{o})\) based only on the model decision. Each iteration proposes a random perturbation \(\eta_k\sim P(\tilde{o}_{k-1})\) and accepts it if \(\tilde{o}_{k-1}+\eta_k\) remains adversarial (rejection sampling). The proposal is constrained heuristically to (i) remain in the input box constraints (e.g., pixel range \([0,255]\)), (ii) have relative step size \(\|\eta_k\|_2=\delta\, d(o,\tilde{o}_{k-1})\), and (iii) reduce distance toward the original by a fraction \(\epsilon\): \(d(o,\tilde{o}_{k-1})-d(o,\tilde{o}_{k-1}+\eta_k)=\epsilon\, d(o,\tilde{o}_{k-1})\). Performance is summarized by \(S_A(M)=\mathrm{median}_i\big(\tfrac{1}{N}\|\eta_{A,M}(o_i)\|_2^2\big)\).","Untargeted attacks (median per-pixel squared L2; lower is better): MNIST—Boundary 3.6e-03 vs DeepFool 4.3e-03 and C&W 2.2e-03; CIFAR—Boundary 5.6e-06 vs DeepFool 5.8e-06; ImageNet—Boundary 2.9e-07 (VGG-19), 1.0e-07 (ResNet-50), 6.5e-08 (Inception-v3), comparable to DeepFool/C&W. Targeted setting: Boundary 6.5e-03 (MNIST), 3.3e-05 (CIFAR), 9.9e-06 (VGG-19) vs C&W 4.8e-03, 3.0e-05, 5.7e-06 respectively. Query cost example on ResNet-50 (avg over 20 samples): DeepFool ~7 forward and 37 backward passes; C&W ~16,000 forward + 16,000 backward; Boundary ~1,200,000 forward and 0 backward passes. Defensive distillation: FGSM fails, while Boundary achieves similar perturbations on standard vs distilled networks (MNIST 3.6e-03 vs 4.2e-03; CIFAR 5.6e-06 vs 1.3e-05).",None stated.,"The method is highly query-intensive (often requiring orders of magnitude more model evaluations than gradient-based methods), which can be impractical for rate-limited APIs or real-time systems. It assumes the attacker can obtain at least one initial adversarial example (or a correctly classified sample from the target class in targeted attacks), which may be nontrivial in some domains beyond vision. The evaluation focuses largely on L2 distortion and image classifiers; robustness and efficiency under common black-box constraints (noise, randomized outputs, hard query budgets, detection/monitoring) are not systematically explored.",The authors suggest improving the Boundary Attack by learning a more suitable proposal distribution for a given model and/or conditioning the proposal distribution on the recent history of successful and unsuccessful proposals to make the search more effective.,"Develop explicit query-budgeted variants (anytime algorithms) with principled stopping rules and performance guarantees under strict API limits. Extend and validate the approach under realistic black-box conditions such as stochastic/rounded outputs, input preprocessing defenses, and detection mechanisms, and broaden experiments to non-vision modalities (speech/text) with task-specific perceptual metrics. Provide standardized, reproducible benchmarks (including code/scripts and fixed query budgets) comparing against modern decision-based and score-based black-box attacks.",1712.04248v2,https://arxiv.org/pdf/1712.04248v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:08:52Z
FALSE,Other,Nonparametric/Semi-parametric|Other,Other,Not applicable,Healthcare/medical,Other,TRUE,MATLAB|Other,Not provided,http://www.brainresource.com,"The paper presents a general probability-integral-transform-based method to map any continuous distribution to the standard normal by applying the variable’s CDF followed by the inverse standard normal CDF (expressed via the inverse error function). For empirical data, the authors use an adjusted empirical distribution function based on normalized ranks that avoids mapping sample minima/maxima to 0/1, then apply the inverse-normal transform. The method is demonstrated on highly non-normal quantitative EEG (qEEG) spectral measures from repeat recordings of 32 subjects, and compared to common ad hoc transformations (log for absolute power, logit for relative power) and Box–Cox. Normality is assessed primarily with Anderson–Darling (and also KS) tests; the proposed transform yields p-values of 1 across all tested EEG measures and is reported to outperform log/logit and slightly outperform Box–Cox in normality. The paper also examines test–retest reliability (Pearson correlations over six weeks) and finds average reliability improves after normalization (from 0.59 to 0.64; paired t-test p=0.013), with performance comparable to Box–Cox but without parameter optimization.","The core transform is the probability integral transform composed with the inverse normal CDF: for a continuous variable $v$ with CDF $P_v(v)$, $y(v)=\sqrt{2}\,\mathrm{erf}^{-1}(2P_v(v)-1)$ (standard normal), or $y(v)=\mu+\sigma\sqrt{2}\,\mathrm{erf}^{-1}(2P_v(v)-1)$ for target mean/SD. For a Weibull example with $P(x)=1-e^{-(x/b)^a}$, the induced normalizing transform is $y(x)=\sqrt{2}\,\mathrm{erf}^{-1}(1-2e^{-(x/b)^a})$. For empirical data, $P_v$ is approximated by an EDF based on normalized ranks with an offset so values range from $1/(2N)$ to $1-1/(2N)$.","On 193 eyes-closed EEGs (32 subjects), untransformed band powers are strongly non-normal (AD p-values often <2.2e−16); log/logit improves but several measures remain non-normal (e.g., log delta p=0.0039; log alpha p=0.00061; logit rel alpha p=0.0016; logit rel gamma p=0.0024). Box–Cox improves normality for many measures (e.g., deltaBC p=0.99), but some remain marginal/non-normal (e.g., alphaBC p=0.056; entropyBC p=0.082; rel alphaBC p=0.0080). The proposed CDF→inverse-normal transform yields AD and KS p-values of 1 for all reported spectral measures (optimal agreement with Gaussian). Test–retest reliability (Pearson r over first six weeks) increases on average from 0.59 (original) to 0.64 (normalized), with paired t-test p=0.013; Box–Cox results are similar (comparison vs proposed p=0.25). Spearman correlations average 0.61 after normalization (vs Pearson 0.64), with reduction p=0.0025.","The authors caution that normalizing is not a panacea: other assumptions (independence, identical distribution, homoscedasticity, linear relations to predictors) may still be violated and may require compromise transformations. They note that for small sample sizes the empirical transformation may vary substantially across repeat studies because the underlying distribution is incompletely represented, complicating cross-study comparability; they suggest using a single transformation based on combined datasets as one remedy. They also note that test power/specificity for comparing means does not depend directly on normality and alternative methods (bootstrapping, moment-based CI adjustments) may be preferable in some settings.","Because the empirical transform is rank/EDF-based, ties and discretization (common in quantized or rounded measurements) can create non-unique mappings and may distort variance structure unless tie-handling is specified. The approach can obscure physically meaningful units and effect sizes, potentially complicating engineering/clinical interpretability and back-transformation of confidence intervals. Using the same dataset to estimate the EDF and evaluate normality/reliability can inflate apparent gains (a form of overfitting); out-of-sample validation or split-sample EDF estimation would strengthen claims. The method guarantees marginal normality (for continuous distributions) but does not in general preserve multivariate dependence structure across multiple features unless extended via a multivariate copula/Rosenblatt transform, which is not implemented for the EEG feature set.",None stated.,"Evaluate the method with explicit out-of-sample protocols (e.g., estimate the EDF on a training cohort and apply to a held-out cohort) to quantify generalization of normality and reliability gains. Extend the approach to multivariate normalization that preserves dependence structure (e.g., Rosenblatt transform/copula-based Gaussianization) for joint modeling of multiple EEG features. Develop robust procedures for discrete/rounded data and tied ranks, and provide guidance on uncertainty propagation and back-transformation for interpretable effect sizes. Provide open-source implementations (R/Python/MATLAB) and practical recommendations for Phase I/Phase II style usage (reference EDF vs ongoing monitoring) in applied workflows.",1801.01748v1,https://arxiv.org/pdf/1801.01748v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:09:29Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical|Other,Exact distribution theory|Simulation study|Other,TRUE,Other,Personal website|Public repository (GitHub/GitLab)|Not provided,http://parralab.org/corrca|https://figshare.com/articles/Correlated_Component_Analysis/7609145,"The paper proposes Correlated Components Analysis (CorrCA), a multivariate method to extract linear dimensions that are maximally correlated across repeated measurements (e.g., subjects, trials, or raters), formalized as maximizing a between-repetition to within-repetition covariance ratio. It shows that this objective maximizes a repeat-reliability/SNR measure (variance of the mean across repeats divided by mean within-repeat variance) and establishes links to multi-set CCA and an equivalence to LDA for zero-mean signals. The authors derive an exact parametric significance test for the resulting inter-subject/inter-rater correlation using an F-statistic under normal IID assumptions, and propose shuffle-based surrogate tests (phase scrambling and circular shifts) for dependent samples. They demonstrate the method on EEG inter-subject correlation, inter-rater motor assessment data, parent–child questionnaire agreement, and trial-to-trial EEG reliability, and validate performance and testing procedures with extensive simulations. Code and data to reproduce figures and experiments are stated to be provided online.","CorrCA defines inter-subject correlation as $\rho=\frac{1}{N-1}\frac{r_B}{r_W}$, which becomes the generalized Rayleigh quotient $\rho=\frac{1}{N-1}\frac{\mathbf v^\top R_B\mathbf v}{\mathbf v^\top R_W\mathbf v}$. Maximization leads to the generalized eigenproblem $R_B\mathbf v=\rho\,R_W\mathbf v$ (and $R_BV=R_WV\Lambda$ for multiple uncorrelated components). For IID normal data, significance is tested via an F-statistic linked to $\rho$: $F=\frac{T(N-1)}{T-1}S=\frac{T((N-1)\rho+1)}{(T-1)(1-\rho)}$ with $d_1=T(N-1)$ and $d_2=T-1$ degrees of freedom.","On EEG data from $N=18$ subjects watching a 197s video (64 channels, $T=50344$), CorrCA finds 3 statistically significant correlated components (via circular-shift shuffle), whereas regularized MCCA finds 1, and CorrCA generalizes better to held-out time samples. On Zurich Neuromotor Assessment ratings ($T=30$ children, $D=12$ tasks, $N=2$ raters), CorrCA yields 5 significant inter-rater-reliable components and produces uncorrelated aggregate scores compared to strongly intercorrelated raw task scores. On the Alabama Parenting Questionnaire ($T=616$ parent–child pairs, $D=42$), CorrCA identifies a first component with test-set parent–child correlation about $\rho\approx0.61$ (with shrinkage regularization $\gamma=0.5$) and estimates 8 significant components, while MCCA finds fewer significant components (reported $K=4$). In simulations, CorrCA approaches ideal mean ISC as measurement SNR increases and surrogate-based tests estimate the correct correlated dimensionality for autocorrelated (pink-noise) data, while the parametric F-test overestimates when IID assumptions are violated.","The authors note that the exact parametric F-test is valid only for normally distributed, independent samples and is not appropriate for autocorrelated time series; for dependent data they recommend surrogate/shuffle statistics. They state that CorrCA assumes identical signal (and typically noise) spatial directions across repeats/subjects; if this assumption is violated, performance drops and MCCA or approaches interpolating between CorrCA and MCCA may be preferable. They also caution that individual components may be non-identifiable when eigenvalues/ISC values are similar (degeneracy), so components can mix within a correlated subspace.","The work’s “reliability” is statistical repeatability across repetitions (subjects/raters/trials) rather than reliability engineering (failure-time, degradation, maintenance), so it does not translate directly to engineering reliability decisions. Many real applications (EEG/ratings) can have nonstationarity and structured dependencies across repetitions (e.g., subject heterogeneity, rater drift) that are only partially addressed by mean-centering/regularization and may still bias eigen-solutions and p-values. The surrogate tests control significance empirically but their calibration can depend on the chosen surrogate construction (phase scrambling vs circular shift) and on preprocessing choices (filtering, detrending), which may affect false positive/negative rates in practice.","They propose extending CorrCA to non-linear mappings via kernels (kernel-CorrCA) and discuss leveraging the equivalence to LDA for that purpose. They suggest that when CorrCA’s shared-direction assumption is violated, methods that smoothly move from CorrCA to MCCA (cited work) may be beneficial. They also emphasize releasing optimized, faster code and data to encourage new applications of correlated component analysis.","Developing robust/self-starting variants that explicitly model subject/rater-specific scaling (multiplicative bias) and heterogeneity could improve performance when repeats are not strictly in the same measurement space. A principled multiple-testing framework beyond max-statistic shuffles (e.g., FDR control for correlated components) and power analyses for choosing $T,N,D$ would aid practitioners. Providing packaged implementations (e.g., an R/Python package with documented APIs and reproducible pipelines) and benchmarks across diverse domains could improve adoption and facilitate fair comparisons with modern alternatives (e.g., representation learning / contrastive methods) for extracting reproducible dimensions.",1801.08881v5,https://arxiv.org/pdf/1801.08881v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:10:12Z
FALSE,Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only,Not applicable,Energy/utilities,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies how intermittent, non-dispatchable renewable generation affects committed non-renewable generation and the market clearing price of electricity under reliability requirements. Reliability is modeled as a confidence level that net load can be met, implemented by enforcing a Conditional Value-at-Risk (CVaR) constraint on unserved net load; this yields an analytic expression linking required committed power to the tail of the net-load distribution. The authors derive market clearing prices (and, with congestion, locational marginal prices) from the KKT conditions of the ISO’s cost-minimizing dispatch problem, showing prices are non-decreasing in net-load uncertainty and in the demanded reliability level. For a radial network with line-flow limits, they provide an upper bound on excess committed power induced by congestion using CVaR subadditivity. Simulations on a small generator system illustrate that as renewable penetration increases, committed power and price can decrease up to a threshold and then increase due to uncertainty; deviation and cost-recovery terms increase with higher reliability and higher renewable penetration.","Reliability is enforced by setting CVaR of nonserved net load to zero: with $s_i^t=p_{di}^t-p_{ri}^t$ and $n^t=(\sum_i(s_i^t-p_{gi}^t))_+$, the constraint $\mathrm{CVaR}_\alpha(n^t)=0$ implies $\sum_i p_{gi}^t=\mathrm{CVaR}_\alpha(\sum_i s_i^t)$. The ISO minimizes expected procurement cost subject to generator limits and this CVaR balance, leading to a market clearing price $\lambda^t$ equal to the marginal unit’s bid price (from KKT conditions). With congestion in a radial grid, an upper bound on planned power follows from subadditivity: $\mathrm{CVaR}_\alpha(\sum_i (p_{di}^t-p_{ri}^t))\le \sum_i \mathrm{CVaR}_\alpha(p_{di}^t-p_{ri}^t)$.","Analytically, the market clearing price is shown to be non-decreasing in (i) the demanded reliability level $\alpha$ and (ii) the tail-heaviness/uncertainty of net load captured through $\mathrm{CVaR}_\alpha(\sum_i s_i^t)$. With congestion, the paper derives an explicit upper bound on the additional committed non-renewable power attributable to congestion and uncertainty via CVaR subadditivity. In simulations (7-generator example), committed power and price increase with higher reliability levels, and as renewable penetration rises they decrease initially then increase after a threshold due to renewable uncertainty. The deviation cost (difference between profit under committed dispatch vs. expected profit under scenario dispatch) increases with renewable penetration and rises sharply beyond a high-penetration regime (reported as >~60% in the narrative).","The authors explicitly note simplifications made to obtain analytic solutions: they assume a radial grid topology and (in parts of the analysis) ignore congestion constraints, later treating congestion only to derive an upper bound rather than a full analytic solution. They also assume renewables have zero marginal cost and are paid at the market price, and use linearized power-flow assumptions (small-angle approximation, voltage magnitudes near 1).","The paper’s notion of “reliability” is a risk-constraint on net-load shortfall (CVaR) rather than component/system failure reliability (e.g., forced outage rates, MTBF/MTTR), so it does not address classical reliability engineering metrics or maintenance actions. The stochastic models for load/renewables are assumed known with continuous PDFs; parameter uncertainty and forecast model error are not treated. Numerical results are based on a small illustrative system and do not benchmark against alternative stochastic UC/ED formulations (e.g., chance constraints, robust optimization, scenario-based stochastic programming) under comparable assumptions. Code and reproducibility details (distributions, scenario generation, random seeds, implementation) are not provided, limiting replication.",The paper suggests extensions to more complex (non-radial) grid topologies with congestion constraints and using the framework to assess the profitability/benefit of removing congestion at specific buses. It also indicates the analysis could be extended to identify where congestion relief most reduces the economic inefficiency introduced by renewable uncertainty.,"A natural extension is to incorporate conventional-generator outage models (forced outages) and reserve/commitment decisions in a full unit commitment setting so that reliability includes both net-load uncertainty and supply failures. Another direction is to develop robust/self-starting versions where load/renewable distributions are estimated online, and to test robustness under autocorrelated renewable errors. Empirical validation on real market data (e.g., ISO day-ahead/real-time prices, wind/solar forecasts) and comparison against chance-constrained and robust UC baselines would strengthen practical relevance. Providing open-source implementations and standardized test cases would also improve reproducibility and adoption.",1802.08286v1,https://arxiv.org/pdf/1802.08286v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:10:49Z
FALSE,Other,Nonparametric/Semi-parametric|Other,Other,Not applicable,Finance/economics|Other,Other,TRUE,None / Not applicable,Not provided,https://hal.archives-ouvertes.fr/hal-01270565|http://mortality.org|www.mortality.org|www.humanmortality.de|www.humanfertility.org,"The paper studies how anomalies in Human Mortality Database (HMD) period mortality tables—especially isolated cohort effects for birth years 1919 (abnormally low mortality) and 1920 (abnormally high mortality)—affect a Solvency II longevity-risk internal model. It corrects HMD period mortality rates using a fertility-based exposure adjustment derived from monthly births in the Human Fertility Database (HFD) for France and Italy, and proposes an extension for West Germany by predicting the correction indicator via stepwise-selected multivariate regression on correction indicators from other countries. The corrected mortality tables remove artificial diagonal cohort patterns and reduce excess volatility in historical mortality improvement rates, while preserving overall mortality characteristics (e.g., period life expectancy trends). The authors then calibrate three standard stochastic mortality models (Lee–Carter/M1, APC/M3, and CBD/M5), compare fit (BIC) and qualitative diagnostics, and assess downstream impacts on projected life expectancy and the 99.5% quantile longevity shock used to compute the Solvency Capital Requirement (SCR). Empirically, the correction slightly reduces longevity trend SCR (notably for France) and improves stability of longevity risk assessment over time, while leaving overall conclusions and model selection largely stable.","The core mortality quantities are the period death rate $m(x,t)=D(x,t)/E(x,t)$ and the death probability $q(x,t)=1-\exp(-m(x,t))$. The fertility-based correction applies a cohort-specific indicator $I(b)$ (year of birth $b=t-x$) to obtain corrected rates $\tilde m(x,t)=m(x,t)/I(t-x)$. Mortality improvement rates are defined as $r(x,t)=m(x,t+1)/m(x,t)-1$ (and analogously for $\tilde r$), and the West Germany correction indicator for early cohorts is predicted via regression $\hat I_{WG}(t)=\hat\mu+\sum_{C\in S^*}\hat\alpha_C I_C(t)$.","For France and Italy, the HFD-based correction indicator shows the 1919 cohort mortality is underestimated and 1920 overestimated in HMD, with error magnitude around \(\pm 6\%\) for those cohorts. For West Germany, where monthly births are only available from 1946, the paper reconstructs earlier correction indicators using stepwise regression; the preferred Adjusted-$R^2$ model uses Finland, France, and Italy as predictors with estimated coefficients $\hat\mu\approx-0.14$, $\hat\alpha_{Finland}\approx0.21$, $\hat\alpha_{France}\approx0.63$, $\hat\alpha_{Italy}\approx0.29$. BIC fit improves for models without cohort terms after correction (e.g., France females: M1 BIC from -13,585 to -11,907; France males: -16,767 to -15,128; Germany females: -16,410 to -13,360), while M3 BIC is essentially unchanged. Portfolio-level longevity trend SCR decreases modestly after correction: France about -2.87%, Germany about -0.84%, and Italy about -0.92%; total Life SCR impact is small (France about -0.38%, Germany about -0.02%, Italy about -0.04%).","The authors note that for West Germany the HFD monthly fertility history is insufficient to directly correct the key 1919–1920 cohorts, motivating an extrapolation via regression that could raise stability concerns in other contexts (they mention potential non-stationarity issues). They also acknowledge that for West Germany the 1945 cohort anomaly is not fully corrected and that improving correction for such generations is left for further research. The internal model used is described as a simplified prototype intended to assess business impact rather than a full production internal model.","Although the paper argues regression extrapolation is reasonable using “mirror” countries, it does not provide out-of-sample validation (e.g., backtesting on held-out years/countries) of the reconstructed correction indicator for West Germany or uncertainty bounds on the corrected rates. The work focuses on data anomalies from exposure approximation (births uniformity) and may not address other HMD/HFD inconsistencies (migration, census revisions, war-time reporting errors) that can also generate cohort artifacts. The SCR impact results are portfolio- and model-selection-dependent; sensitivity to alternative closure methods at old ages, alternative calibration windows/age ranges, and parameter uncertainty treatment is not fully explored.",The authors explicitly state that developing an improved correction method for generations such as the 1945 cohort anomaly (not fully corrected for West Germany) is left for further research. They also indicate that significant theoretical work remains to be done on the regression-based extrapolation approach used to reconstruct historical correction indicators outside the observed monthly-fertility period.,"A useful extension would be to quantify uncertainty from the correction step (especially the regression-predicted indicator for West Germany) and propagate it through mortality model calibration and SCR computation, producing confidence/credibility intervals for SCR impacts. Additional work could test robustness across more stochastic mortality models, different time-series dynamics for period/cohort factors, and alternative data-quality corrections beyond birth-seasonality (e.g., migration adjustments). Implementing and publishing reproducible code (e.g., an R/Python package) and performing cross-country out-of-sample validation would strengthen practical adoption and credibility.",1803.00464v1,https://arxiv.org/pdf/1803.00464v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:11:36Z
FALSE,NA,"ML-based|Parametric (Weibull, etc.)|Other",Simulated only,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Personal website,www.anonymized.com,"The paper proposes an Accuracy–Reliability (AR) scalarized cost function to estimate predictive variances for Gaussian probabilistic forecasts in heteroskedastic regression, given point predictions (means) from a black-box model. Accuracy is measured by the Continuous Ranked Probability Score (CRPS) for Gaussian forecasts, and reliability is measured by a new Reliability Score (RS) that quantifies the deviation of scaled residuals from a standard normal distribution. The method estimates per-sample variances by minimizing a weighted sum of CRPS and RS, with an automatic weighting based on the respective minima. Variance functions are learned parametrically via either a neural network constrained to output [0,1] or a low-order polynomial fit, and demonstrated on multiple synthetic datasets (1D and 5D). Results show improved negative log predictive density compared to a homoskedastic Gaussian process baseline, and the method can recover the hidden noise function when the mean model is adequate.","CRPS for Gaussian forecast: $\mathrm{CRPS}(\mu,\sigma,y^o)=\sigma\left[\frac{y^o-\mu}{\sigma}\,\mathrm{erf}\left(\frac{y^o-\mu}{\sqrt{2}\sigma}\right)+\sqrt{\frac{2}{\pi}}\exp\left(-\frac{(y^o-\mu)^2}{2\sigma^2}\right)-\frac{1}{\sqrt{\pi}}\right]$; for fixed error $\varepsilon=y^o-\mu$, its minimizer is $\sigma^{\mathrm{CRPS}}_{\min}=\varepsilon/\sqrt{\log 2}$. Reliability Score: $\mathrm{RS}=\int_{-\infty}^{\infty}[\Phi(\eta)-C(\eta)]^2 d\eta$ with $\eta_i=\varepsilon_i/(\sqrt{2}\sigma_i)$ and an analytic telescoping-series form (Eq. 7). The proposed scalarized objective is $\mathrm{AR}=\beta\,\overline{\mathrm{CRPS}}+(1-\beta)\,\mathrm{RS}$ with $\beta=\mathrm{RS}_{\min}/(\mathrm{CRPS}_{\min}+\mathrm{RS}_{\min})$.","Across 100 runs on synthetic datasets, the AR-based variance estimation improves Negative Log Estimated Predictive Density (NLPD) versus a homoskedastic GP baseline. For the Y dataset, median NLPD drops from 1.56 (GP) to 0.57 (AR-NN) and 0.44 (AR-POLY). For the W dataset, median NLPD drops from 1.68 (GP) to -0.03 (AR-NN) and 0.17 (AR-POLY). For the G dataset, median NLPD improves modestly from 1.28 (GP) to 1.26 (AR-NN) and 1.17 (AR-POLY). A 5D synthetic test shows predicted vs. true standard deviations closely aligned in density plots using a model trained on 10,000 points and evaluated over 10^7 samples.","The authors note that the method is fully parametric and therefore subject to model-selection choices (e.g., neural network architecture or polynomial order). They also caution that estimated variances are optimal only relative to the provided approximation of the mean function $f(x)$, and the method may compensate mean-model inaccuracies by inflating variances. They mention that minimizing RS alone is ill-posed because RS depends only on relative errors $\eta$, so RS is best treated as a regularization term rather than a standalone objective.","The work is not focused on reliability engineering (lifetime/failure/degradation) and does not evaluate on real reliability datasets; demonstrations are synthetic and geared to probabilistic forecasting calibration. The approach assumes Gaussian predictive distributions and independence of errors; performance under non-Gaussian noise, heavy tails, outliers, or autocorrelated data is not analyzed. Implementation details (e.g., optimization stability, sensitivity to the weighting choice for $\beta$, and constraints to keep $\sigma>0$ beyond the chosen architectures) are not systematically stress-tested. Code is said to be provided later but no reproducible repository or exact software environment is specified in the paper.",They suggest incorporating the Accuracy–Reliability cost function into a non-parametric Bayesian method for heteroskedastic regression.,"Extend the AR objective to non-Gaussian predictive families (e.g., Student-t) or distribution-free calibration measures to improve robustness to outliers and heavy tails. Analyze theoretical properties such as consistency of variance estimation and sensitivity to mean-model misspecification, including guidance on selecting/learning the weight $\beta$. Validate on real-world datasets with correlated errors and/or covariate shift, and provide an open-source, reproducible implementation (e.g., Python/R package) with standardized benchmarks.",1803.04475v1,https://arxiv.org/pdf/1803.04475v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:12:10Z
TRUE,Degradation modeling|RUL prediction|Warranty analysis|Maintenance optimization|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Stochastic process|ML-based|Bayesian|Hybrid/Ensemble|Simulation-based|Other",Right-censored|Degradation measurements|Event/count data|Sensor/condition monitoring|Mixture of types|Other,Not applicable,Manufacturing (general)|Semiconductor/electronics|Energy/utilities|Transportation/logistics|Other,Other,NA,None / Not applicable,Not applicable (No code used),https://www.diva-portal.org/smash/get/diva2:990005/FULLTEXT01.pdf,"This paper is a broad review/discussion of how the “complexity” dimension of big data (high-dimensional sensor streams, functional curves, images, and other unstructured data) creates new opportunities and challenges for reliability analysis. It surveys recent reliability-relevant developments in degradation modeling (general path and stochastic-process models; inverse Gaussian/Wiener/gamma processes), including dynamic covariates and multivariate degradation, and highlights methodological gaps for complex data structures. The authors outline concrete modeling frameworks for: (i) constructing a composite degradation/health index from multistream sensor data via additive nonlinear modeling with variable selection and monotonicity constraints, (ii) multivariate degradation via copula-based random effects in stochastic-process models, and (iii) Bayesian hierarchical spatio-temporal degradation models for spatial fields evolving over time. The paper also discusses integrating complex covariates into reliability prediction (functional covariates, scalar-on-image/tensor regression) and the role of machine learning (clustering, deep learning, text mining) for reliability tasks such as early warning from warranty/field data streams. Overall, it positions modern statistical learning and spatio-temporal/functional/image methods as key tools to extend classical reliability modeling to complex big-data settings.","The paper proposes example formulations rather than a single new chart/model. A key degradation-index construction is $z_i(t)=\sum_{j=1}^p f_j(x_{ij}(t);\beta_j)$, estimated by minimizing event-level variation with penalties for group sparsity and monotonicity: $\min_{\{\beta_j\}}\sum_{\{i:\delta_i=1\}}(z_i(t_i)-\bar z)^2+\lambda_1\sum_{j=1}^p\|\beta_j\|+\lambda_2\sum_{i=1}^n\sum_{k=1}^{n_i}[z_i(t_{i,k-1})-z_i(t_{i,k})]_+$. For multivariate degradation, a copula random-effects Wiener structure is illustrated: $D_j(t)=\omega_j\Lambda_j(t;x)+\sigma_j B(\Lambda_j(t;x))$ with $(\omega_1,\ldots,\omega_p)'\sim C(F_1(\omega_1),\ldots,F_p(\omega_p))$. For spatio-temporal degradation fields, a Bayesian state-space form is described: $\mathbf D_t=\mathbf U_t+\varepsilon_t$ and $\mathbf U_t=M\mathbf U_{t-1}+M^{(b)}\mathbf U^{(b)}_{t-1}+\eta_t$, with posterior inference via MCMC.","No dedicated performance study (e.g., ARL/coverage/accuracy numbers) or quantitative benchmarking results are reported; the paper is primarily a review plus illustrative modeling formulations. It identifies key practical challenges—degradation index construction from multistream sensors, dependence modeling for multivariate degradation without losing process properties, monotonicity/extrapolation in longitudinal functional degradation, and censoring/correlation issues for image covariates in reliability regression. It also points to early-warning monitoring of warranty/field streams (e.g., CUSUM-based approaches in prior work) and individualized warranty prediction using spatio-temporal weather/usage covariates as important application directions.","The paper is positioned as a review and discussion piece rather than a comprehensive methodological development, focusing mostly on data modeling and analysis for reliability prediction. It notes open challenges such as enforcing monotonicity and enabling extrapolation for longitudinal functional degradation models, preserving well-defined stochastic-process properties when introducing dependence across multiple degradation characteristics, and handling censoring (for lifetime regression with image covariates) and temporal correlation among images in degradation settings.","Because the article is largely conceptual, it does not provide end-to-end validated workflows (data preprocessing, model fitting, diagnostics, and decision rules) on real industrial datasets demonstrating measurable gains in prediction or decision quality. The proposed/outlined methods may require substantial tuning (penalty parameters, basis choices, copula families, priors) and may be computationally heavy for truly massive streams; scalability and robustness (outliers, missingness, autocorrelation, nonstationarity, dataset shift) are not systematically assessed. The discussion spans many problem types (degradation, warranty monitoring, ML) but does not deeply address identifiability and uncertainty quantification when combining high-dimensional learned representations with reliability extrapolation.","The authors explicitly frame multiple areas as opportunities for future research: developing analytical methods for complex structured data (high-dimensional sensors, functional data, image streams), advancing degradation modeling via better degradation-index construction and multivariate dependence structures, extending methods to address monotonicity and extrapolation for longitudinal functional degradation, and creating more advanced early-warning systems for reliability and warranty data streams. They also highlight broader opportunities in maintenance analytics enabled by big data (e.g., condition monitoring and fault detection) and emerging domains such as renewable energy and IoT where reliability metrics, testing, data collection, and prediction methods need development.","Promising extensions include: (i) scalable self-starting/online Bayesian or streaming variational inference versions of the proposed spatio-temporal and copula-process models for real-time monitoring, (ii) robust/semiparametric formulations that relax normality/independent-noise assumptions and handle autocorrelated sensor errors, and (iii) principled integration of deep-learning feature extractors with survival/degradation models that preserves extrapolation and yields calibrated uncertainty (e.g., Bayesian deep survival, conformal prediction for RUL). Additional work could provide open-source implementations and reproducible benchmarks across standard PHM datasets (e.g., C-MAPSS variants) and industrial case studies to quantify improvements in RUL/warranty forecasts and maintenance decisions.",1803.06206v1,https://arxiv.org/pdf/1803.06206v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:12:53Z
FALSE,NA,Nonparametric/Semi-parametric|ML-based|Other,Right-censored|Mixture of types|Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),http://data.ohdsi.org/SystematicEvidence|https://github.com/OHDSI/FeatureExtraction|https://github.com/OHDSI/CommonDataModel|https://github.com/OHDSI/ETL-CDMBuilder/tree/master/man/TRUVEN_CCAE_MDCR|https://github.com/OHDSI/ETL-CDMBuilder/tree/master/man/TRUVEN_MDCD|https://github.com/OHDSI/ETL-CDMBuilder/tree/master/man/OPTUM_EXTENDED|https://github.com/OHDSI/StudyProtocols/tree/master/LargeScalePopEst,"The paper proposes a standardized, high-throughput paradigm for generating more reliable evidence from observational healthcare databases by running many comparative effectiveness studies using a consistent design and publishing all results to reduce publication bias. For each target–comparator–outcome question, it estimates hazard ratios using propensity score stratification with a large, data-driven covariate set (tens of thousands of baseline features) and a Cox proportional hazards model. To quantify and correct residual systematic error (e.g., unmeasured confounding), the authors use negative controls (true HR=1) and synthetic positive controls (true HR>1 via injected outcomes) to empirically calibrate confidence intervals and p-values. In a depression-treatment example across four claims databases, they generate 17,718 calibrated hazard ratios (out of 23,936 possible) and report improved between-database consistency after calibration (83% with I²<0.25 vs 58% uncalibrated) and strong transitivity (96% of tested triplets). They provide an online explorer for results and state that the software to execute the study is open source within the OHDSI ecosystem.","The primary effect estimator is a Cox proportional hazards model conditioned on propensity-score strata (10 strata), producing a propensity score–adjusted hazard ratio for each target–comparator–outcome. Propensity scores are fit via L1-regularized logistic regression with 10-fold cross-validation over a large, predefined covariate set (dropping covariates with <100 non-zero entries). Empirical calibration models systematic error as a Gaussian distribution whose mean and log standard deviation are linear functions of the true log effect size, estimated from negative (HR=1) and synthetic positive controls (target HRs 1.5, 2, 4), and then used to produce calibrated confidence intervals incorporating random + systematic error.","In the worked example (duloxetine vs sertraline for stroke in CCAE), the uncalibrated PS-adjusted HR is 1.13 (95% CI 0.81–1.61) and the calibrated HR is 1.11 (95% CI 0.77–1.62). Across the full study, 17,718 effect estimates are produced after requiring ≥2,500 persons per treatment arm (out of 23,936 possible across 4 databases). Transitivity holds for 722/755 (96%) statistically significant A–B–C triplets. Between-database heterogeneity improves with calibration: among 2,570 triplets observed in all four databases, 83% of calibrated estimates have I²<0.25 vs 58% without calibration.","The authors note that negative controls may not be truly negative because definitive evidence of no causal relationship is rare; some controls could later prove positive. Their calibration assumes controls and hypotheses of interest are exchangeable in key aspects (biases drawn from the same distribution), which may not hold if negative controls have different biases than outcomes of interest. Synthetic positive controls may not reflect bias from unmeasured confounding beyond what is present in the negative controls used as the base. They also emphasize general limitations of observational healthcare data: only treatments/outcomes captured in the data can be studied, and unmeasured confounding remains a threat (e.g., channeling bias in sexual dysfunction findings).","The approach is not a reliability-engineering methodology; it addresses reproducibility/validity of observational effect estimates rather than engineering failure/degradation processes. The results may depend strongly on design choices (new-user definitions, time-at-risk, PS stratification into 10 bins, L1 model specification), and the paper provides limited sensitivity/robustness analysis across alternative causal estimators (e.g., doubly robust methods) or PS diagnostics beyond balance thresholds. Calibration quality hinges on the availability and suitability of control outcomes; domains where controls are scarce or systematically different may see degraded performance. The work largely relies on claims/EHR coding fidelity; misclassification of exposures/outcomes and temporal misalignment could be substantial but is not deeply quantified.","They suggest future studies can replace their current state-of-the-art components for confounding adjustment (data-driven propensity score methods) and residual bias handling (confidence-interval calibration) with improved methods. They encourage the research community to propose and objectively evaluate modifications to the standardized design using control hypotheses, and incorporate proven improvements systematically into the overall high-throughput process. They also emphasize refining the shared evidence-generation process collaboratively rather than conducting isolated one-off studies.","Extending the framework to handle autocorrelation/longitudinal treatment strategies (time-varying confounding) via g-methods (MSMs, g-formula) would broaden applicability. Developing guidance for selecting/validating negative controls (and diagnosing non-exchangeability) and for domains with sparse controls would strengthen calibration reliability. Adding robust outcome/exposure misclassification modeling and quantitative bias analysis could better characterize systematic error beyond confounding. Packaging end-to-end reproducible workflows (containerized pipelines, standardized reporting templates) and benchmarking across additional international EHR/registry sources would improve adoption and external validity.",1803.10791v1,https://arxiv.org/pdf/1803.10791v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:13:45Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling|Maintenance optimization|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Stochastic process|Bayesian|Simulation-based|Other",Degradation measurements|Complete lifetime data|Mixture of types|Other,Not applicable,Semiconductor/electronics|Energy/utilities|Transportation/logistics|Manufacturing (general)|Other,Other,TRUE,R|None / Not applicable,Not provided,NA,"This paper is an introductory reliability-focused review of accelerated testing and statistical analysis methods used for polymeric materials and polymer composites whose field lifetimes can span years or decades. It describes repeated measures degradation testing (RMDT) and presents two major modeling families: general degradation path (mixed-effects) models and stochastic process models (e.g., Wiener/gamma/inverse Gaussian) for characterizing degradation trajectories and deriving implied time-to-failure distributions via threshold crossing. It also covers accelerated destructive degradation testing (ADDT), highlighting batch correlation structure and contrasting the industry “traditional” two-step method (e.g., UL746B) with one-step parametric maximum-likelihood and semi-parametric monotone-spline approaches, including thermal indexing as a key application. For polymer composite fatigue, it summarizes accelerated life testing (ALT) using stress-life (S–N) relationships, commonly within log-location-scale lifetime families (e.g., Weibull/lognormal), and outlines balanced, Fisher-information-based optimal, Bayesian, and sequential Bayesian test planning ideas. The article primarily synthesizes established methods and references key sources and standards rather than introducing a new reliability model, but it emphasizes practical modeling choices and test-planning considerations for polymer reliability assessment.","For RMDT, the general degradation path model is presented as $y_i(t_{ij}) = f(t_{ij};\theta,\phi_i)+\varepsilon_{ij}$ with random effects $\phi_i\sim N(0,\Sigma)$ and measurement error $\varepsilon_{ij}\sim N(0,\sigma^2)$; lifetimes are obtained by solving $D_f=f(t_f;\theta,\phi)$ for $t_f$ (often via Monte Carlo when no closed form exists). As an example stochastic process model, the gamma-process increment is given by $\Delta y(t)\sim \mathrm{Gamma}\{\xi,\eta(t+\Delta t)-\eta(t)\}$ with pdf $f[\Delta y(t);\Delta\eta(t),\xi]=\{\Gamma[\Delta\eta(t)]\xi^{\Delta\eta(t)}\}^{-1}(\Delta y(t))^{\Delta\eta(t)-1}\exp[-\Delta y(t)/\xi]$. For ADDT, an adapted path model $y_i(t_{ij})=f(t_{ij};\theta,\rho)+\varepsilon_{ij}$ is described to account for within-batch correlation. For fatigue ALT, the log-location-scale family is written as $F(t)=\Phi\big((\log t-\mu)/\nu\big)$ and $f(t)=\nu^{-1}\phi\big((\log t-\mu)/\nu\big)$, with stress typically modeled through $\mu(\text{stress})$.","The paper does not report new benchmark ARL/coverage/estimation results; it is a methods overview with illustrative figures (e.g., degradation paths with a failure threshold, an ADDT thermal-indexing example plot, and an S–N fatigue curve example). It provides qualitative pros/cons: traditional ADDT two-step polynomial-then-acceleration regression is simple but lacks an explicit uncertainty quantification method and ignores batch correlation, whereas parametric MLE enables uncertainty quantification and extrapolation but can be sensitive to model misspecification. The semi-parametric monotone-spline approach reduces degradation-form misspecification risk but may not support extrapolation in time if data do not reach the failure threshold. For fatigue test planning, it notes balanced designs are easy but often suboptimal, and optimal/Bayesian/sequential Bayesian designs can reduce estimator variance and better allocate samples toward lower stress levels near use conditions.","The authors note the article is intentionally introductory and that it is challenging to cover all topics in depth within a short article; instead they provide references for further reading in each area. No additional explicit methodological limitations (e.g., restrictive assumptions, missing scenarios) are emphasized beyond this scope limitation.","As a largely narrative overview, the paper does not provide systematic simulation studies or real-data re-analyses comparing competing methods under common conditions, so practitioners get limited quantitative guidance on when each approach is superior. Many discussed models rely on common assumptions (e.g., normal measurement error, independence/structure of random effects, correct Arrhenius/power-law acceleration, monotone degradation, and appropriateness of threshold-based ‘soft failure’) whose robustness is not assessed here. The discussion of ADDT mentions an R package but the paper itself does not provide reproducible code, worked examples, or implementation details sufficient for immediate application without consulting cited sources. Application coverage is broad (polymers, composites, PV components, LEDs) but domain-specific nuances (e.g., competing degradation mechanisms, model validation diagnostics, and handling changing mechanisms across stress levels) are not developed.",None stated.,"A useful extension would be a unified comparative study (simulation + multiple real polymer datasets) evaluating GDP vs stochastic-process vs semi-/parametric ADDT models under misspecification, sparse sampling, and incomplete threshold crossing, with guidance on model selection and diagnostics. Developing robust/self-starting methods for time-varying outdoor covariates (including autocorrelation and measurement error in covariates) and for mechanism changes across stress levels would improve field generalizability. Providing an end-to-end open-source workflow (e.g., R notebooks) demonstrating thermal indexing, uncertainty quantification, and test-plan optimization (including sequential Bayesian) would materially increase practical impact. Extending the fatigue planning discussion to incorporate competing risks or mixed failure modes (e.g., degradation + fatigue) and multilevel coupon/batch variability would better match composite testing practice.",1804.04586v1,https://arxiv.org/pdf/1804.04586v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:14:28Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1804.05497v1,https://arxiv.org/pdf/1804.05497v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:14:28Z
TRUE,System reliability|Other,"Simulation-based|Parametric (Weibull, etc.)|Other",Simulated only|Other,Not applicable,Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper addresses reliability/risk assessment when the joint dependence structure among uncertain model inputs is unknown, a common weakness in computational (black-box) reliability studies. It proposes a copula-based approach to find conservative (worst-case) dependence structures by minimizing an output risk indicator defined as an output quantile of the computer model response. In dimensions >2, the authors construct worst-case dependencies using regular vine (R-vine) copulas and introduce a greedy heuristic that iteratively selects the most influential input pairs (and copula families) to reduce computational burden under a sparsity assumption. They provide consistency results for grid-search-based estimation of the minimal quantile over increasingly fine parameter grids, and demonstrate via toy examples and an industrial case study that assuming independence can be non-conservative and that worst cases may require nontrivial (not necessarily Fr","The output risk measure is an b1-quantile of the model output distribution: $G^{-1}(\alpha)=\inf\{y: G(y)\ge \alpha\}$, with worst-case over admissible joint inputs $G^{-1*}(\alpha)=\min_{P\in\mathcal{D}}G^{-1}(\alpha)$. Dependence is modeled via copulas using Sklar: $F_\theta(x)=C_\theta(F_1(x_1),\dots,F_d(x_d))$ and density factorization $f_\theta(x)=c_\theta(F_1(x_1),\dots,F_d(x_d))\prod_j f_j(x_j)$. The copula-family worst quantile is $G^{-1*}_C(\alpha)=\inf_{\theta\in\Theta}G^{-1}_\theta(\alpha)$, estimated on a grid by minimizing the empirical quantile: $\hat\theta_N=\arg\min_{\theta\in\Theta_N}\hat G^{-1}_\theta(\alpha)$.","Simulation studies show the output quantile can vary substantially with dependence even when marginals are fixed, and the minimizer need not occur at perfect dependence (Fr","The authors note the approach relies on several hypotheses and simplifying approximations, including assumptions needed for the convergence result (regularity/continuity and increase conditions) that may need to be checked or relaxed for real codes. They emphasize computational cost as a key limitation, motivating the sparsity assumption and greedy heuristic, and acknowledge that the basic grid-search strategy is used because convexity/gradients of $\theta\mapsto G^{-1}_\theta(\alpha)$ are unknown. They also mention difficulty incorporating expert knowledge beyond concordance measures and the need for better diagnostic/visual tools for multivariate laws and vine structures.","The worst-case search is restricted to chosen parametric copula families (and often single-parameter pair-copulas), so the identified ""worst"" dependence may be far from the true worst case over all admissible joint distributions with the given marginals. The greedy algorithm is heuristic and can get trapped in local minima or depend on vine structure/orderings and grid resolution; results may be sensitive to discretization (grid size Nk) and Monte Carlo error in quantile estimation. The framework largely assumes i.i.d. sampling and does not directly address temporal/spatial dependence in inputs, nor robustness to model mis-specification in marginals or numerical noise/discontinuities in the computer code beyond brief discussion. No publicly accessible, archived code link is provided in the paper itself, limiting reproducibility verification.","The authors propose improving and stress-testing the statistical estimation by checking/relaxing assumptions behind Theorem 1 using expert knowledge and numerical exploration of code regularity. They suggest replacing grid search with more powerful stochastic approximation methods such as RobbinsdMonro, and reducing computational cost via better exploration criteria and incorporation of expert guidance. They mention potential use of nonparametric bootstrap within the iterative procedure to quantify selection/estimation quality at each iteration, and exploring Bayesian global optimization with surrogate models (e.g., kriging) for expensive simulators. They also point to developing better visualization/diagnostic tools for vine structures and considering an optimal-transport (multi-marginal) formulation for the fully general worst-case problem.","A useful extension would be formal uncertainty quantification for the identified worst-case quantile (e.g., confidence bounds that account jointly for Monte Carlo noise and optimization over dependence), and robustness analysis to grid design and vine truncation choices. Developing self-adaptive or Bayesian optimization over copula/vine structure (structure learning under a worst-case objective rather than likelihood) could improve reliability and reduce sensitivity to greedy choices. Extending the methodology to handle autocorrelated inputs, mixed discrete/continuous marginals, and constraints from partial multivariate data (e.g., known pairwise rank correlations or bounds) would broaden applicability. Providing an open-source, versioned implementation with benchmarks and standardized test cases would materially improve reproducibility and adoption in reliability engineering practice.",1804.10527v1,https://arxiv.org/pdf/1804.10527v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:15:16Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)",Complete lifetime data|Other,Not applicable,Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,http://dx.doi.org/10.1007/s00362--017--0888--6,"The paper introduces a new bivariate reparameterized Birnbaum–Saunders distribution (BRBS) whose marginal means are explicit parameters, extending the Birnbaum–Saunders family widely used for positively skewed lifetime/fatigue data. It derives the joint CDF/PDF via a standard bivariate normal construction with correlation parameter ρ and shows shape properties including unimodality under stated conditions. Reliability analysis is developed for a stress–strength-type measure R = P(T1 < T2), and bivariate survival and hazard-rate expressions are derived; monotonicity results are provided for conditional hazard rate, conditional survival, and mean residual functions under dependence (sign of ρ and regions relative to medians/βk). Parameter estimation is addressed through maximum likelihood (profile likelihood with closed-form estimator for ρ given other parameters) and modified-moment estimators with explicit formulas; asymptotic distributions and confidence intervals are discussed. Performance is evaluated via Monte Carlo simulation (bias/MSE and CI coverage) and an application to real reliability-related data (paired stiffness measurements: shock and vibration on printed boards), where the model fits well according to PP/TTT plots and a transformed Mahalanobis-distance KS test.","The BRBS joint CDF is defined as $F_{T_1,T_2}(t_1,t_2)=\Phi_2\big(\sqrt{\delta_1/2}(a_1-b_1),\sqrt{\delta_2/2}(a_2-b_2);\rho\big)$ with $a_k=\sqrt{((\delta_k+1)t_k)/(\delta_k\mu_k)}$ and $b_k=\sqrt{(\delta_k\mu_k)/((\delta_k+1)t_k)}$. The joint PDF is $f_{T_1,T_2}(t_1,t_2)=\phi_2(\cdot;\rho)\prod_{k=1}^2 \sqrt{\delta_k/2}\, (a_k+b_k)/\sqrt{2t_k}$. The bivariate survival function is $S(t_1,t_2)=\int_{t_1}^\infty f(w;\mu_1,\delta_1)\{1-\Phi(c_{2,1}(t_2,w;\rho))\}\,dw$ and Basu’s bivariate hazard is $h_{T_1,T_2}(t_1,t_2)=f_{T_1,T_2}(t_1,t_2)/S(t_1,t_2)$, yielding the explicit form in Eq. (9). The stress–strength reliability is expressed as $R=P(T_1<T_2)=\mathbb{E}[\Phi(c_{2,1}(T_1,T_1;\rho))]$ with $c_{j,k}(t,w;\rho)=(a_j(t)-\rho a_k(w))/\sqrt{1-\rho^2}$.","Monte Carlo experiments use n ∈ {10, 50, 100}, (µ1,µ2) = (2,2), δk ∈ {0.25, 2.0}, ρ ∈ {0, 0.25, 0.50, 0.95}, with 5,000 replications; biases and MSEs of ML and modified-moment estimators both decrease with n and are broadly similar. Reported CI coverages show poor asymptotic ML coverage for some parameters at n=10 (e.g., for δ/µ under high ρ, coverages can be far below nominal), improving by n=50–100; modified-moment-based intervals for ρ (Fisher z and Krishnamoorthy–Xia methods) generally provide better coverage than ML-based intervals. In the real-data example (n=30 printed boards, paired stiffness measures: shock T1 and vibration T2), ML estimates are µ1=1906.100, µ2=1749.533, δ1=77.030, δ2=69.134, ρ=0.908 with log-likelihood ℓ(θ)=−400.648, and the BRBS fit is supported by PP plots and a transformed Mahalanobis-distance KS test p-value ≈ 0.4433. The paper also provides theoretical monotonicity results for conditional hazard/survival/mean residual functions depending on the sign of ρ and regions relative to βk, and establishes positive association and correlation structure (e.g., Cov(T1,T2)= (ρ^2/2)∏_{k=1}^2 βk α_k^2).","None stated explicitly as limitations; however, the conclusion notes that future work should extend likelihood inference to censored data and develop regression and time-series versions of the proposed bivariate model with diagnostic tools, implying the current work focuses on uncensored i.i.d. settings without regression/time dependence.","The model and inference rely on a specific Gaussian-copula-like construction (bivariate normal on transformed margins), which may be restrictive for tail dependence or asymmetric dependence structures in bivariate lifetime data. The numerical study is limited to a small grid of parameters and does not benchmark against alternative bivariate lifetime models (e.g., bivariate lognormal, Clayton/Gumbel copulas with BS margins, or other bivariate BS variants) on detection/prediction tasks. Practical implementation details (software, numerical stability for optimization, and diagnostic workflow) are not provided, which may hinder adoption despite the derived likelihood; also, the real-data example is stiffness measurements rather than actual failure-time data, so reliability interpretation may be indirect.","The authors suggest developing likelihood-based inferential methods for censored data, implementing the BRBS distribution in a regression context, and building time-series models based on the proposed bivariate distribution along with influence diagnostic tools; they note work on these problems is in progress.","Develop robust and semiparametric variants (e.g., robust estimation under outliers/heavy tails, or alternative copulas with RBS margins) and study sensitivity of reliability measures R=P(T1<T2) to dependence misspecification. Extend to higher dimensions (multivariate BRBS) with scalable estimation and to condition-monitoring settings with covariates (accelerated life/PH models) and censoring/truncation typical in reliability datasets. Provide open-source implementations and reproducible simulation code, and validate on multiple real failure-time datasets with right/interval censoring and model comparison using predictive checks and information criteria.",1804.10820v1,https://arxiv.org/pdf/1804.10820v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:16:06Z
TRUE,Reliability growth|Maintenance optimization|Other,"Bayesian|Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Not provided,https://cran.r-project.org/web/packages/CEGO/CEGO.pdf,"The paper addresses a Bayesian decision/design problem where the design space is all permutations of a set of tasks, motivated by sequencing reliability growth activities for hardware system development to meet a reliability target while minimizing cost and time. Because computing the prior expected utility for every permutation is infeasible when the number of tasks is large, the authors propose a surrogate (emulator) for expected utility based on the Benter probabilistic model for permutations, with parameters fitted by maximizing correlation with logit-transformed expected utilities on a training sample. They further improve fit via a regression-adjusted surrogate that maps surrogate outputs to the logit-utility scale and can provide prediction intervals. For the reliability growth application, they use a concern–fault framework with Bayesian updating of fault probabilities based on task efficacies (an efficacy matrix), assume faults found are designed out, and approximate the distribution of (log) system reliability as Normal via a rare-event/CLT-based approximation. An illustrative aerospace-motivated example and simulation studies (varying surrogate families and correlation objectives) show the Benter+Pearson-correlation surrogate is most effective, and provide guidance on splitting a fixed evaluation budget between training (N) and candidate evaluation (M), with a practical rule-of-thumb of N≈M.","The surrogate for a permutation x=(x1,...,xJ) is derived from the Benter model log-likelihood: $f(x;\theta,\alpha)=\sum_{j=1}^J \alpha_j\log(\theta_{x_j})-\log\Big(\sum_{m=j}^J \theta_{x_m}^{\alpha_j}\Big)$ with $\alpha_J=0$. The emulator fits $(\theta,\alpha)$ by maximizing correlation between $f(x;\theta,\alpha)$ and the logit-transformed expected utilities $\eta=\log(u/(1-u))$ on training permutations, and then applies a cubic regression adjustment $f^*(x)=\hat\beta_0+\hat\beta_1\hat f(x)+\hat\beta_2\hat f(x)^2+\hat\beta_3\hat f(x)^3$, optionally mapped back to utility via $f^\dagger(x)=\exp\{f^*(x)\}/(1+\exp\{f^*(x)\})$. For reliability growth, system reliability is $R(t,z)=\prod_{i=1}^I [1-z_i(1-R_i(t))]$ with concern-fault indicators $z_i$, updated fault probabilities using task efficacies $p_{i,j}$ and performed-task indicators $\kappa_j$; they approximate $g(t,z)=\log R(t,z)$ as Normal $N(m(t),v(t))$ using a rare-event approximation and CLT arguments to compute $\Pr(R\ge R_0)$ at each stage of a sequence.","In the illustrative example with 15 concerns and 9 candidate tasks (9!=362,880 sequences), exhaustive evaluation of all expected utilities took about 102 seconds in R; with a total budget of B=100 utility evaluations, using N=60 training and M=40 top-emulator candidates achieved a fitted Pearson correlation of 0.984 for the initial surrogate and 0.993 after regression adjustment, and successfully recovered the true optimal sequence. The identified optimal sequence was (8, 6, 4, 3, 1, 7, 9, 2, 5) with expected utility about 0.9185 (reported as 0.919). In simulation comparisons of surrogates, the Benter-based surrogate fitted by maximizing Pearson correlation substantially outperformed Plackett–Luce and reverse Plackett–Luce surrogates and also outperformed Spearman/Kendall correlation objectives; e.g., for J=9 and N=100, M=50, the estimated probability of finding the global optimum was ~0.94 (Benter+Pearson) versus ~0.48 (PL+Pearson). A broader simulation study led to a rule-of-thumb budget split of N≈M (i.e., N=B/2) as a near-optimal default.","The authors note that their surrogate emulator is effectively unimodal; for genuinely multi-modal utility landscapes it may emulate only one mode, motivating mixture-of-Benter-model extensions. They also emphasize that their reported performance and N/M guidance are based largely on using a simple random training sample from all permutations, and that more intelligent exploration/training designs could change optimal choices of N and M. They further acknowledge that for larger numbers of tasks it may be infeasible even to enumerate the surrogate over all J! permutations, requiring sampling-based search under the fitted probabilistic surrogate.","The reliability growth model assumes (i) independence of concerns/failures, (ii) independent test outcomes across activities, and (iii) that any confirmed fault is designed out with probability one and immediately, which can be optimistic in practice and can bias expected utilities and stopping probabilities. The approximation that log reliability is Normal relies on a rare-event approximation and asymptotic CLT arguments; its accuracy for moderate numbers of activities/strongly heterogeneous concerns is not fully characterized. The emulator fitting maximizes correlation rather than directly optimizing top-rank identification; high global correlation does not guarantee correct identification of the extreme optimum, especially when utilities are tightly clustered. Finally, no public implementation of the proposed Benter-correlation fitting and search workflow is provided, which may hinder reproducibility and uptake.","The authors propose extending the framework to allow some tasks to be carried out in parallel. They suggest developing a multi-modal surrogate based on mixtures of Benter models to better handle multi-modal utility functions. They also propose designing the training set more informatively (rather than uniform random sampling) to focus on regions of high expected utility, potentially using benefit-to-cost metrics, and exploring applications beyond reliability growth such as optimal sequencing of actions in project risk management.","Develop self-starting/robust variants that handle uncertain or learned efficacy matrices (pi,j) and dependence between concerns and between task outcomes, including sensitivity analysis to elicited inputs. Provide theoretical/empirical calibration of the Normal approximation for log reliability and quantify its impact on decision quality (e.g., via posterior predictive checks or importance-sampling benchmarks). Integrate sequential/adaptive design (Bayesian optimization over permutations) where emulator-guided exploration updates iteratively instead of a single N/M split. Release an open-source reference implementation (e.g., an R package) of the proposed emulator fitting, candidate generation (including sampling from fitted Benter), and diagnostics to improve reproducibility and practitioner adoption.",1805.00726v1,https://arxiv.org/pdf/1805.00726v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:16:58Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Simulated only|Other,Not applicable,Other,Simulation study|Other,TRUE,MATLAB|Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher),https://github.com/fomorians/counting-mnist|https://blog.openai.com/ai-and-compute/,"This paper proposes an improved gradient-based learning implementation for the Multi‑Spike Tempotron (a single leaky integrate‑and‑fire spiking neuron) by replacing Momentum updates with an adaptive per-synapse learning-rate method similar to RMSprop with gradient smoothing. The method is evaluated via simulations on synthetic spike-train datasets with multiple input spike statistics (including non‑Poisson renewal processes) and with increasing background-noise complexity (including inhomogeneous Poisson noise), showing reduced variance and improved generalization relative to Momentum. The model is then applied to a weakly labeled multiple-instance learning counting task on images built from MNIST digits, where images are encoded into spike trains via either a naive pixelwise encoding or a FoCal front-end. In the Counting MNIST setup (800 train / 800 test, 30 epochs), the MST with adaptive learning and FoCal encoding achieves the best reported RMSE among compared models, outperforming a ConvNet of similar parameter scale under the same epoch budget. The work is positioned as computational neuroscience / spiking ML rather than reliability engineering.","The neuron membrane potential is defined by a leaky integrate-and-fire sum of weighted synaptic kernels plus a reset term: $V(t)=V_{rest}+\sum_{i=1}^N \omega_i \sum_{t_i^j<t} K(t-t_i^j) - \vartheta \sum_{t_{spike}^j<t} e^{-(t-t_{spike}^j)/\tau_m}$. The synaptic kernel is a difference-of-exponentials $K(\Delta t)=V_{norm}(e^{-\Delta t/\tau_m}-e^{-\Delta t/\tau_s})$ for $\Delta t\ge 0$. Learning uses a spike-threshold-surface gradient update $\Delta \omega = \eta\lambda \nabla_{\omega} \vartheta_k^*$, with Momentum $\Delta\omega(t)=\alpha\Delta\omega(t-1)+\eta\lambda\nabla_{\omega}\vartheta_k^*$. The proposed adaptive update is RMSprop-like per synapse: $v_i(t)=\gamma v_i(t-1)+(1-\gamma)(\Delta\omega_i(t))^2$, then $\Delta\omega_i(t)=\eta\lambda\, \nabla_{\omega_i}\vartheta_k^*/\sqrt{v_i(t)}$.","Across synthetic spike-train datasets with non-Poisson input statistics and added background noise, the adaptive learning rule yields substantially lower variance in training and validation/generalization error than Momentum (variance comparisons shown after ~10 epochs when convergence is reached). In Counting MNIST, the best result is MST with adaptive learning + FoCal encoding: RMSE 1.22 with 10,000 parameters, compared with ConvNet RMSE 1.49 with 26,471 parameters (both trained 30 epochs on 800 images and tested on 800 images). MSTMomentum + FoCal achieves RMSE 1.28; MSTadaptive + naive encoding RMSE 2.34; MSTMomentum + naive encoding RMSE 1.87. Baselines reported: always-zero RMSE 1.65 and random guessing RMSE 2.50.","The authors note the spike-train statistics in visual cortex are still unknown, motivating evaluation across several point-process statistics rather than claiming one is correct. They also state that encoding choice impacts MST performance and that exploring additional efficient encoding algorithms could further improve results, which they leave open. They describe the work as a “Preprint. Work in progress.”","The work does not establish any connection to reliability engineering constructs (failure times, degradation, maintenance), so it is not directly applicable as a reliability method without substantial reframing. Comparisons to the ConvNet are constrained to a small training regime (800 samples, 30 epochs); broader benchmarking (multiple seeds, larger datasets, standardized hyperparameter tuning budgets) would be needed to confirm superiority. The paper implies code is public but does not provide an explicit repository URL for the MST implementation in the excerpt (only a Zenodo DOI is mentioned), limiting immediate reproducibility from the provided text alone.",The authors explicitly leave exploration of different spike-encoding front-ends (beyond naive and FoCal) for future research. They also state that studying more complex and layered networks of multiple interconnected Multi-Spike Tempotrons is left for future work.,"A useful extension would be to test robustness under realistic temporal correlations and nonstationarities beyond sinusoidally modulated Poisson background, and to quantify sensitivity to hyperparameters (e.g., learning-rate schedules, RMSprop decay). Providing a fully self-contained, versioned software release with scripts to reproduce all figures/tables (and reporting compute cost) would strengthen reproducibility. If the method were to be bridged to reliability applications, future work could explore whether spike-count regression can model event-counting processes (e.g., recurrent failures) and how uncertainty calibration could be incorporated.",1805.07569v2,https://arxiv.org/pdf/1805.07569v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:17:34Z
FALSE,NA,Other,Other,Not applicable,Transportation/logistics,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a distributed joint transmit-power and radio resource allocation framework for ultra-reliable low-latency (URLLC) vehicle-to-vehicle (V2V) communications. Reliability is defined in terms of probabilistic queue-length/queuing-delay constraints and, critically, the control of rare ‘extreme’ queue events; the tail of excess queue lengths over a high threshold is modeled using extreme value theory (EVT) via a generalized Pareto distribution (GPD). To estimate GPD tail parameters without exchanging raw queue state information, the authors use federated learning (FL) with a distributed maximum-likelihood estimation (MLE) procedure (SVRG-based), coordinated by a roadside unit that aggregates model parameters. Lyapunov optimization then yields per-vehicle decentralized power/RB allocation policies. Simulations (Manhattan mobility) show FL achieves near-centralized tail estimation accuracy while reducing exchanged data up to 79%, and the proposed control reduces the fraction of vehicles with large queues (extreme events) by up to 60% versus an average-queue baseline without extra power, and by ~two orders of magnitude versus fixed-power stability-focused approaches.","Extreme-event (excess queue) modeling uses a generalized Pareto distribution for exceedances over a high threshold $q_0$: $M(t)=q(t)-q_0$ and $G_{\boldsymbol d}(m)=\frac{1}{\sigma}(1+\xi m/\sigma)^{-1-1/\xi}$ (or exponential when $\xi=0$), with $\boldsymbol d=[\sigma,\xi]$. The mean exceedance is $\mathbb{E}[M]=\sigma/(1-\xi)$ for $\xi<1$, turning the extreme-queue constraint into a bound using $\sigma/(1-\xi)$. Queue dynamics are $q_u(t{+}1)=[q_u(t)+a_u(t)-r_u(t)]_+$ and the per-VUE power allocation is solved via a water-filling form $p_{u}^{n*}(t)=\left[\frac{\alpha_u(t)}{V+\lambda_u^*(t)}-\frac{1}{\gamma_u^n(t)}\right]_+$ under a sum-power budget.","Simulation results using a 250 m×250 m Manhattan mobility model show the FL-based GPD tail-parameter estimates are “almost equivalent” to a centralized SVRG estimator. Compared to centralized learning, FL reduces RSU–VUE data exchange by ~27% at $U{=}28$ and up to 79% at $U{=}100$ vehicles while achieving nearly identical reliability $\Pr(q\le q_0)$. For extreme-event mitigation, the proposed method reduces the fraction of VUEs exceeding $q_0$ by about 60% (U=20), 8% (U=60), and 9% (U=100) versus a probabilistic-queue baseline, with no additional average transmit power; compared to fixed-power and a queue-stability-only baseline, reductions reach 97–99%. The paper also reports about two orders of magnitude reduction in extreme events relative to approaches focusing on stability/average power with fixed power consumption.",None stated.,"The work targets URLLC ‘reliability’ in a communications/queueing sense (probability of queue exceedance and extreme tails), not reliability engineering of physical component lifetimes, degradation, or failure processes, limiting relevance to classical reliability-engineering datasets and decisions. EVT modeling assumes i.i.d. queue samples and a sufficiently high threshold $q_0$; in practice, queue processes can be autocorrelated/nonstationary under mobility and scheduling, which may bias GPD fits and the implied constraint $\sigma/(1-\xi)$. The evaluation is simulation-based (Manhattan model) without real vehicular traces or field tests, and no sensitivity study is provided for key EVT/FL design choices (threshold selection, block size $w$, FL participation, non-IID data) that affect tail estimation robustness. Implementation details (computational cost on VUEs, convergence time under fast fading/mobility, and robustness to packet loss in model exchange) are not fully quantified.",None stated.,"Validate the EVT+FL tail-learning and Lyapunov control on real V2V measurement datasets or high-fidelity network simulators with realistic PHY/MAC stacks to confirm tail modeling and latency reliability under nonstationarity. Extend the method to handle correlated/non-IID queue samples explicitly (e.g., declustering/peaks-over-threshold methods with dependence, robust GPD fitting, or Bayesian/online EVT) and to include uncertainty in estimated $\sigma,\xi$ within the control policy. Study communication-efficient and robust FL variants (partial participation, quantization/compression, secure aggregation, and resilience to dropouts) tailored to vehicular connectivity constraints. Provide open-source implementations and reproducibility artifacts (simulation scripts, parameter settings, and FL training code) and compare against additional URLLC tail-risk baselines (e.g., CVaR-based latency control) under varying channel/interference regimes.",1805.09253v1,https://arxiv.org/pdf/1805.09253v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:18:10Z
FALSE,NA,ML-based|Bayesian,Sensor/condition monitoring|Mixture of types,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Upon request,NA,"The paper proposes Uncertainty-aware Attention (UA), an attention mechanism that models attention logits as Gaussian random variables with input-dependent (heteroscedastic) variance to represent feature-level uncertainty. UA is trained in a Bayesian/variational inference framework using MC dropout for weight uncertainty (epistemic) and reparameterized sampling for attention noise, yielding calibrated predictive probabilities and uncertainty-aware interpretations. The method is instantiated on the RETAIN attentional RNN for EHR time-series and evaluated on multiple clinical risk prediction tasks (PhysioNet outcomes, pancreatic cancer onset, and MIMIC-III sepsis). Across tasks, UA and UA+ improve AUROC over deterministic and prior stochastic attention baselines and substantially reduce Expected Calibration Error, enabling safer “I don’t know” deferral based on uncertainty. The authors also provide qualitative interpretation analyses showing attention patterns and uncertainty that align better with clinician expectations than baselines.","UA models attention logits as $p(\omega)=\mathcal{N}(0,\tau^{-1}I)$ and $p_\theta(z\mid x,\omega)=\mathcal{N}(\mu(x,\omega;\theta),\mathrm{diag}(\sigma^2(x,\omega;\theta)))$, with attention $a=\pi(z)$ (e.g., sigmoid/tanh/softmax). Training maximizes an ELBO that reduces to $\mathcal{L}(\theta,M)=\sum_n \log p_\theta(y^{(n)}\mid \tilde z^{(n)},x^{(n)})-\lambda\|M\|_2^2$ under dropout variational approximation, using reparameterization $\tilde z=g(x,\tilde\epsilon,\tilde\omega)$ with $\tilde\epsilon\sim\mathcal{N}(0,I)$. Prediction uses MC integration $p(y^*\mid x^*)\approx\frac1S\sum_{s=1}^S p(y^*\mid x^*,\tilde z^{(s)})$. Calibration is summarized by Expected Calibration Error $\mathrm{ECE}=\mathbb{E}_{\text{confidence}}\,|p(\text{correct}\mid\text{confidence})-\text{confidence}|$.","On PhysioNet mortality, AUROC improves from 0.7652 (RETAIN-DA) to 0.7827 (UA) and 0.7770 (UA+); on MIMIC sepsis, AUROC improves from 0.7965 (RETAIN-DA) to 0.8017 (UA) and 0.8114 (UA+). Calibration improves markedly: for PhysioNet mortality, ECE drops from 7.23 (RETAIN-DA) and 7.70 (RETAIN-SA) to 4.22 (UA) (and 4.41 for UA+); for MIMIC sepsis ECE drops from 3.05 (RETAIN-DA) to 1.78 (UA). In “I don’t know” deferral experiments, UA/UA+ achieve lower incorrect prediction rates at the same correct prediction rate than the MC-dropout baseline, indicating improved decision reliability under uncertainty-based abstention.","The paper is labeled “Preprint. Work in progress.” Beyond this, explicit limitations are not clearly stated in the provided text excerpt; performance and calibration results are primarily demonstrated on EHR risk prediction tasks with RETAIN as the backbone.","The work is not reliability engineering in the classical sense (no component/system lifetime or maintenance modeling); its “reliability” refers to predictive calibration and abstention in ML. The approach depends on Bayesian approximations (MC dropout and variational assumptions) whose calibration quality can be sensitive to hyperparameters (dropout rates, sampling count) and may not generalize under distribution shift. The evaluation largely focuses on AUROC/ECE and selective prediction curves; additional metrics relevant to clinical deployment (e.g., PPV/NPV at operating points, temporal generalization, subgroup robustness) are not systematically covered here, nor is there a prospective/real-time clinical validation.",They plan to apply the uncertainty-aware attention mechanism to other attention-based domains such as image annotation and machine translation.,"Extend UA to handle distribution shift explicitly (domain adaptation/OOD detection) and autocorrelated or irregularly sampled time series with principled uncertainty propagation. Provide a self-contained software release (e.g., a PyPI/CRAN package) and standardized benchmarks for selective prediction/abstention in clinical tasks, including clinical utility/cost-based evaluation. Explore robustness to missingness mechanisms and unknown label noise, and extend to multivariate/multi-task settings with calibrated uncertainty decomposition (aleatoric vs epistemic) at both feature and time-step levels.",1805.09653v1,https://arxiv.org/pdf/1805.09653v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:18:46Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Bayesian|Simulation-based",Right-censored|Interval-censored|Left-censored|Mixture of types|Simulated only|Other,Not applicable,Semiconductor/electronics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This manuscript develops Bayesian methods to estimate component and system reliabilities in coherent systems when only system failure times and component status at system failure are observed (including censoring and, in a later chapter, masked causes). For series-parallel (SPS) and parallel-series (PSS) coherent structures, it derives Bayesian nonparametric estimators for component distribution/reliability functions using a multivariate Dirichlet process prior on sub-distribution functions, along with inversion formulas that map sub-distributions to marginal component distributions. For general coherent systems (including k-out-of-m and bridge structures where components can be left-, right-, or interval-censored), it proposes a Bayesian parametric approach assuming mutually independent component lifetimes following a three-parameter Weibull model, with posterior inference obtained via MCMC (adaptive Metropolis–Hastings). The manuscript also extends the Weibull Bayesian framework to masked-data settings where the failure-causing component is only known to lie in a subset, using data augmentation and Metropolis-within-Gibbs sampling including masking probabilities. Performance and applicability are illustrated through multiple simulated studies and two real datasets (Device-G competing risks data; computer hard-drive masked-cause data).","For a general coherent-system observation expressed as intervals $(l_{ji},u_{ji})$, the per-component likelihood is $L(\theta_j\mid l_j,u_j)=\prod_{i=1}^n f(l_{ji}\mid\theta_j)^{\mathbb{I}(l_{ji}=u_{ji})}\,[R(l_{ji}\mid\theta_j)-R(u_{ji}\mid\theta_j)]^{1-\mathbb{I}(l_{ji}=u_{ji})}$ (Eq. 1.3). Under a 3-parameter Weibull, $R(t\mid\theta_j)=\exp\{-[((t-\mu_j)/\eta_j)^{\beta_j}]\}$ and $f(t\mid\theta_j)=\frac{\beta_j}{\eta_j}\left(\frac{t-\mu_j}{\eta_j}\right)^{\beta_j-1}\exp\{-[((t-\mu_j)/\eta_j)^{\beta_j}]\}$ (Eqs. 3.1–3.2), yielding the Weibull interval-censoring likelihood in Eq. (3.3) and posterior in Eq. (3.4) with prior $\pi(\beta_j,\eta_j,\mu_j)=1/(\eta_j\beta_j)$ (Eq. 3.5). For SPS/PSS nonparametrics, a Dirichlet multivariate process prior is placed on sub-distributions $F_j^*(t)=P(T\le t,\delta=j)$, giving posterior means $\hat F_j^*(t)=p_\alpha\,\alpha_j(0,t]/\sum_{\ell}\alpha_\ell(0,\infty)+(1-p_\alpha)F_{jn}^*(t)$ (Eq. 2.17) and component DFs via inversion maps (Theorem 6). In masked data, the augmented likelihood (Eq. 4.1) introduces latent indicators for whether a masked component is uncensored/right/left-censored and masking probabilities $\lambda_{1j},\lambda_{2j},\lambda_{3j}$.","In the SPS simulated example with $n=100$ (Chapter 2), the estimated conditional probabilities of being the failure-causing component were approximately $\hat\rho_1\approx0.2294$, $\hat\rho_2\approx0.3581$, $\hat\rho_3\approx0.2690$, $\hat\rho_4\approx0.1403$. In the PSS simulated example with $n=100$, the corresponding estimates were $\hat\rho_1\approx\hat\rho_2\approx\hat\rho_3\approx0.2887$ and $\hat\rho_4\approx0.1303$. For the Weibull-MCMC parallel-system simulation ($n=30$), posterior means of reliability at $t=10$ were about 0.083 (component 1), 0.045 (component 2), and 0.024 (component 3), with reported 95% HPD intervals covering/near the generating curves. For the Device-G dataset ($n=30$, two competing causes in series), posterior mean reliabilities at $t\approx250$ were about 0.741 for mode W and 0.408 for mode S (Table 3.8). For the masked-cause hard-drive dataset ($n=172$, three competing causes), example posterior mean reliabilities at $t=4.48$ were about 0.610 (component 1: electronic), 0.724 (component 2: head flyability), and 0.113 (component 3: head/disc magnetics) (Table 4.10).","For the nonparametric SPS/PSS estimator, the method assumes mutually independent component lifetimes and that two or more components cannot fail at the same instant of time (Chapter 2). The SPS/PSS inversion theory also relies on components having no common discontinuities (disjoint jump points) in the relevant theorems. For the parametric Weibull approach, mutual independence of component lifetimes is required and emphasized as necessary for the approach (Chapter 3).","The nonparametric approach is restricted to SPS/PSS representations and hinges on finding a representation that avoids repeated components and simultaneous failures; many coherent systems (e.g., bridge/k-out-of-m) violate these assumptions, pushing users to the parametric model. The parametric method assumes independent component lifetimes and a 3-parameter Weibull family; dependence, shared frailty, or misspecification could materially bias component-level inference, especially under heavy censoring. MCMC implementation details (proposal tuning, diagnostics beyond brief statements, reproducibility) are not fully specified, and no public code is provided, limiting practical uptake. Real-data validation is limited to two datasets and lacks comparison to alternative competing-risks/masked-cause estimators (e.g., EM-based likelihood, frailty or copula models, semiparametric approaches).","The manuscript notes that it is a first version and that improvements/new versions are expected, but it does not clearly enumerate specific methodological future research directions beyond presenting “some ideas” for masked data and referring to related articles for more detail.","Extend the parametric framework to allow dependent component lifetimes (e.g., copulas, shared frailty, or hierarchical Bayes) to handle common-cause effects typical in coherent systems. Develop robust or semiparametric alternatives (e.g., piecewise exponential/B-spline hazard, Bayesian nonparametric priors on lifetimes) to reduce sensitivity to the Weibull assumption under heavy censoring/masking. Provide self-contained, reusable software (R/Python) with documented MCMC defaults, convergence checks, and examples for common structures (k-out-of-m, bridge) and masked-cause settings. Add systematic benchmarking against established competing-risks and masked-data methods (EM/MLE, Bayesian hierarchical models) across varying censoring/masking rates and component heterogeneity.",1805.10540v1,https://arxiv.org/pdf/1805.10540v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:19:47Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/juliakreutzer/bandit-neuralmonkey/tree/acl2018,"This paper studies reinforcement learning (RL) for sequence-to-sequence learning (neural machine translation) using human bandit feedback, focusing on the reliability of human ratings and how that affects learning reward estimators and downstream RL performance. It compares cardinal feedback (5-point ratings) and ordinal feedback (pairwise preferences) collected on 800 translations (with repeats for intra-rater analysis), using Krippendorff’s alpha to quantify inter- and intra-annotator agreement and showing that standardized 5-point ratings yield the best inter-rater reliability. The authors then train reward estimators from human feedback using (i) regression on normalized cardinal ratings and (ii) a Bradley–Terry preference model for pairwise data, evaluating reward estimators via Spearman correlation with TER on held-out translations. Finally, they integrate learned reward estimators into off-policy RL for NMT and report that using a regression-based estimator trained on roughly 800 human-rated translations can yield over +1 BLEU improvement over the baseline, demonstrating RL gains from small but reasonably reliable human feedback datasets.","The cardinal-feedback reward estimator is trained by minimizing mean squared error: $L_{\mathrm{MSE}}(\psi)=\frac{1}{n}\sum_{i=1}^{n}(r(y_i)-\hat r_\psi(y_i))^2$. For pairwise preferences, a Bradley–Terry model defines $\hat P_\psi[y_1\succ y_2]=\frac{\exp(\hat r_\psi(y_1))}{\exp(\hat r_\psi(y_1))+\exp(\hat r_\psi(y_2))}$ and trains with a cross-entropy-style loss over preference probabilities. Downstream NMT RL uses a REINFORCE objective maximizing expected reward $R_{\mathrm{RL}}(\theta)=\mathbb{E}_{p(x)p_\theta(y|x)}[r(y)]$, approximated with sampled translations and optionally temperature-controlled sampling, while off-policy learning from a deterministic log uses a self-normalized reweighted objective over logged $(x,y,r)$ tuples.","Inter-rater reliability (Krippendorff’s $\alpha$) is similar for raw 5-point vs pairwise feedback (about 0.23–0.24), but improves for participant-standardized 5-point ratings (reported $\alpha\approx 0.282$). Intra-rater reliability averages are higher for pairwise than for 5-point ratings (mean $\alpha\approx 0.51$ vs $\approx 0.40$), though the difference is not statistically significant at the stated test. Reward estimators trained on human cardinal feedback (regression/MSE) show better usefulness than pairwise (Bradley–Terry) models when evaluated by rank correlation with TER, and filtering consistent participants slightly improves the cardinal model. In RL experiments, integrating a regression-based reward estimator trained on about 800 human-rated translations yields a significant improvement of roughly +1.1 BLEU over the baseline on TED test data, whereas pairwise-based reward estimation yields smaller gains.","The authors note the restricted nature of real-world exploration in human feedback settings (users typically rate only one displayed translation), and highlight that the amount of human feedback they use is tiny compared to typical NMT training corpora. They also point out that the overall reward-estimation correlations are relatively low (especially for pairwise models), which they suspect is due to overfitting given the small training set. They further mention that pairwise judgments cannot distinguish ties that arise from both translations being high quality versus both being low quality, which can harm agreement.","The work centers on human rating reliability for MT quality rather than reliability engineering; its reliability conclusions may not transfer to engineering measurement systems or failure/degradation contexts. Human raters are limited to fluent/natural bilingual university students, so reliability and learnability may differ substantially for broader populations or real production users. The off-policy RL setup uses a relatively small logged dataset and specific NMT/tooling choices (e.g., temperature, sampling k, model architecture), so gains may be sensitive to hyperparameters and may not generalize across languages/domains. Comparisons for reward estimation are largely indirect (correlation with TER) and do not deeply analyze calibration or uncertainty of reward estimates, which would matter for safe deployment of learned rewards.","The paper concludes that because this type of feedback is fast and cheap to elicit from non-professionals, the demonstrated gains from a very small dataset suggest strong potential for future applications at larger scale. It implies scaling up data collection and applying the approach more broadly beyond the small experimental setting.","A natural extension is to model rater-specific bias and uncertainty explicitly (e.g., Bayesian or hierarchical models) rather than relying mainly on z-score normalization and filtering, and to propagate reward uncertainty into RL updates. The approach could be extended to interactive or online learning settings to study exploration–feedback tradeoffs and how reliability evolves with rater training or interface changes. Broader validation on different language pairs, domains, and more diverse rater pools would test generalizability of the reliability findings. Providing a packaged implementation of the reward estimator and training pipeline (beyond a research code fork) would improve reproducibility and practical adoption.",1805.10627v3,https://arxiv.org/pdf/1805.10627v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:20:30Z
TRUE,System reliability|Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Right-censored|Degradation measurements|Mixture of types|Other,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/zdelrosario/bv-questionable,"The paper develops theory and practical algorithms for reliability-based design optimization (RBDO) when the input distribution parameters are themselves uncertain due to finite-sample estimation (“statistical uncertainty”), where naive nested (double-loop) Monte Carlo is computationally intractable. It introduces performance metrics—effective margin (cost slack/deficit relative to the ideal optimum) and effective reliability (true achieved reliability under the ground-truth parameters)—to evaluate RBDO strategies under parameter uncertainty. The key methodological contribution is the concept of precision margin (PM), defined as a reliability-conservatizing margin that is tied to realized reliability discrepancy and decays to zero with increasing estimation precision; two implementations are proposed: margin-in-limit-state (MIL) and margin-in-probability (MIP). MIP is shown to have a “confidently conservative” (C2) interpretation—achieving the target reliability with a chosen confidence level under the randomness of the estimated parameters—when the modified constraint is met. To avoid double-loop simulation, the paper applies the delta method plus likelihood-ratio (score-function) gradient estimation to approximate the distribution of the margin terms using samples already generated for reliability estimation, adding negligible extra cost; results on a uniaxial tension sizing example and a cantilever beam RBDO benchmark show PM reduces over/under-design compared to basis-value and plug-in designs and can yield weight savings (e.g., ~2–5% vs A-basis designs) while maintaining conservative reliability.","The core RBDO constraint is $\mathbb{P}[g(d,X)>0]\ge R$ with limit state (e.g., uniaxial tension) $g(t,X)=U-\frac{F}{A(t)}$. Precision margin is added either as MIL: $\mathbb{P}[g(d,X)>g_m]\ge R$ with $g_{\mathrm{MIL},C}$ defined by a confidence condition on the mean discrepancy (Eq. 13), or as MIP: $R(\hat\theta)=\mathbb{P}_{X(\hat\theta)}[g(d,X)>0]\ge R+p$ with $p$ set by $\mathbb{P}_{\hat\theta}[p>R(\hat\theta)-R(\theta)]=C$ (Eqs. 14–15). Margin distributions are approximated via the delta method using parameter gradients estimated by the likelihood-ratio identity $\nabla_{\hat\theta}\mathbb{E}_{X(\hat\theta)}[f(X)] = \mathbb{E}_{X(\hat\theta)}\big[f(X)\nabla_{\hat\theta}\log \rho(X;\hat\theta)\big]$ (Eq. 22), yielding approximate closed forms like $\tilde g_{\mathrm{MIL},C}=\Phi^{-1}(C)\tilde\tau_D$ (Eq. 29) and $\hat p_C\approx \Phi^{-1}(C)\tilde\tau_r$ (Eq. 32).","In the uniaxial tension example, basis-value (regulated/mixed) approaches can be either overly conservative at moderate reliability (e.g., $R=0.99$) or fail by orders of magnitude at extreme reliability (e.g., $R=1-10^{-7}$), while plug-in designs yield an unacceptable fraction of under-performing (negative effective margin) outcomes at realistic sample sizes. MIL and especially MIP precision-margin designs approach the requested reliability as material sample size $m$ increases, with MIP exhibiting the intended “confidently conservative” behavior in the semi-analytic study, though it can become infeasible when $R+p>1$ at low $m$ for very high reliability targets. For the cantilever beam benchmark with two constraints (stress and displacement) and $R=0.99865$ per constraint at $C=0.95$, designs using basis values are overly conservative; PM designs move achieved reliabilities closer to the target while remaining conservative. Reported average objective (area) savings versus the basis-value approach are about 2.4% (MIP) and 5.9% (MIL) at $m=100$, and about 4.7% (MIP) and 5.2% (MIL) at $m=1000$, while maintaining high reliabilities (Tables 3–4).","The authors note that the presented PM approximations are restricted to cases of “modeled randomness,” where a specific analytic joint PDF is assumed for uncertain quantities, and that MIP can be numerically unstable (e.g., noisy estimation can produce infeasible constraints with $R+p\ge 1$). They also state that using simple Monte Carlo makes MIP extremely expensive for high-reliability cases, limiting demonstrations in that regime. Finally, they emphasize PM targets statistical (sampling) uncertainty only and cannot replace safety factors meant to cover unknown-unknowns.","Empirical validation is limited to illustrative/benchmark problems (uniaxial tension and a cantilever beam) and does not demonstrate performance on complex high-dimensional industrial systems with expensive simulations or strongly non-Gaussian dependence structures. The delta-method normal approximation and score-function gradient estimation can perform poorly for small sample sizes, highly nonlinear limit states, rare-event probabilities, or when parameter estimators are biased/heavy-tailed, yet robustness across these regimes is not comprehensively quantified. Practical implementation details (e.g., optimizer stability, constraint qualification when using MC-based probability estimates, variance reduction/rare-event methods) are not fully developed, which may affect real-world adoption for stringent reliability targets.","They propose integrating precision margin with faster reliability estimators/integrators (e.g., quadrature/accelerated methods) to address the computational burden—especially for high-reliability targets where crude Monte Carlo is too slow. They suggest reducing dependence on strict parametric distribution assumptions by using more flexible distribution families (e.g., Johnson distributions) and developing nonparametric/empirical PM implementations, potentially leveraging ambiguity sets. They also suggest jointly optimizing design and the sampling/characterization plan (linking sample size and design via margin), citing multi-objective approaches using stochastic dominance as a promising direction.","Develop self-starting or adaptive PM schemes that tune the confidence level or margin online based on observed estimator diagnostics (bias/variance) and feasibility (e.g., preventing $R+p>1$ via constrained margin calibration). Extend PM to correlated/multilevel parameter uncertainty (e.g., hierarchical pooling across batches or suppliers) and to high-dimensional/multivariate RBDO settings with multiple failure modes and dependence. Combine PM with rare-event simulation (subset simulation, importance sampling, cross-entropy) and surrogate modeling (GP/PC expansions) to make MIP practical for $10^{-6}$–$10^{-9}$ failure probabilities while controlling approximation error in both reliability and margin estimates.",1806.00048v2,https://arxiv.org/pdf/1806.00048v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:21:23Z
FALSE,Other,ML-based|Bayesian|Hybrid/Ensemble|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/jik0730/Deep-Mixed-Effect-Model-using-Gaussian-Processes,"The paper proposes Deep Mixed Effect Model using Gaussian Processes (DME-GP) for personalized prediction from heterogeneous patient time-series (EHR). It decomposes each patient’s function as a shared global component learned by a deep network (e.g., RNN/MLP mean and embedding) plus a patient-specific Gaussian Process component that captures idiosyncratic variability and provides predictive uncertainty. By sharing only the deep mean/embedding across patients and keeping per-patient GPs independent, the resulting covariance is block-diagonal, reducing training complexity from the multi-task GP style O(P^3 T^3) to O(P T^3). The method is evaluated on vital-sign regression (Physionet 2012 heart rate) and disease-risk classification (NHIS medical checkups), showing improved RMSE/AUC over population deep models and personalized GP baselines, and includes a reliability/uncertainty-oriented study contrasting performance on “easy” vs “hard” patients. Overall, the work advances personalized healthcare prediction by combining scalable global representation learning with per-patient probabilistic modeling and uncertainty estimates.","The model decomposes patient i’s latent function as $f^{(i)}(x_t)=g(x_t)+\ell^{(i)}(x_t)$, with DME-GP setting $g(x_t)=\mu(h_t\mid w)$ from a shared deep network and $\ell^{(i)}(\cdot)\sim \mathcal{GP}(0,k^{(i)}(\cdot,\cdot\mid\theta_i))$. This yields per-patient GP priors $f^{(i)}(x_t)\sim \mathcal{GP}(\mu(h_t\mid w),k^{(i)}(h_t,h_{t'}\mid\theta_i))$ and a block-diagonal joint covariance across patients (independence across i). For Gaussian likelihood, training maximizes per-patient marginal log-likelihood (Eq. 7) and prediction uses standard GP posterior mean/variance: $\bar y_t=\mu(h_t)+k_t^\top K^{-1}(y-\mu(H))$ and $\sigma_t^2=k(h_t,h_t)-k_t^\top K^{-1}k_t$ (Eq. 9).","On the Physionet 2012 heart-rate step-ahead regression task, test RMSE is reported as 0.150 for DME-GP versus 0.243 for personalized GPs (p-GPs) and 0.194 for an MLP baseline. On 12 NHIS disease risk prediction tasks, DME-GP achieves the best average AUC (task average 0.778 for DME-GP(RNN) and 0.753 for DME-GP(MLP)) compared to RNN (0.720), RETAIN (0.715), MTGP-RNN (0.732), MLP (0.743), MAML (0.732), and LM (0.580). In the reliability study excluding “hard” positives, DME-GP attains near-perfect AUCs for several diseases (e.g., Atherosclerosis 1.0; Emphysema 0.991; Arrhythmia 0.992), while deep baselines remain substantially lower. When only hard positives are included (appendix table), DME-GP performance degrades but remains competitive with deep baselines across diseases.","The paper notes computational intractability of exact multi-task GP formulations (ME-GP/MTGP) for large numbers of patients due to inversion of a huge covariance matrix, motivating the block-diagonal personalized formulation. For classification with non-Gaussian likelihood, it states that inference is analytically intractable and requires approximation methods such as Laplace, variational inference, or MCMC. It also indicates that while mixture-of-experts mean functions can help some diseases, further investigation is required to understand cohort formation and when it helps.","The “reliability” evaluation is indirect (confidence separation between easy vs hard patient groups) and does not include standard uncertainty calibration metrics (e.g., ECE/Brier score, calibration curves) or decision-theoretic evaluation tied to clinical actions. The approach requires per-patient GP hyperparameter adaptation for new patients, which may be computationally heavy in online settings and sensitive to short histories; the paper does not quantify runtime/latency for adaptation or address stability under severe sparsity. Handling of EHR issues like irregular sampling, informative missingness, and temporal confounding is limited (e.g., some missing values are filled with zeros), which can bias both predictions and uncertainty estimates. Comparisons exclude scalable GP variants (e.g., inducing-point variational GPs) as primary baselines, so it is unclear how much gain is due to per-patient exact GP vs modern approximate GP alternatives.","The appendix proposes extending the shared global mean function to a mixture-of-experts form, $\mu(x_t)=\sum_j g(j\mid x_t)\mu_j(x_t)$, motivated by patient cohort structure (e.g., age/region groups). It also suggests that additional investigation is needed to understand how such cohorts are formed and when mixtures help across diseases.","Evaluate and improve probabilistic calibration explicitly (ECE, reliability diagrams, proper scoring rules) and study how uncertainty should trigger clinician override/abstention policies. Extend the model to better handle irregular time grids and informative missingness (e.g., jointly modeling observation processes or using time-aware kernels) and quantify robustness under distribution shift across hospitals or years. Develop faster/self-starting adaptation for new patients (e.g., amortized inference for GP hyperparameters or shared priors over $\theta_i$) to reduce online compute. Explore multivariate/multi-output extensions that jointly model multiple vital signs or codes with structured patient-level correlations while retaining scalability.",1806.01551v3,https://arxiv.org/pdf/1806.01551v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:22:10Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://www.csie.ntu.edu.tw/~cjlin/libsvm,"This paper proposes a computational improvement to Budgeted Stochastic Gradient Descent (BSGD) for training large-scale kernel SVMs under a fixed support-vector budget. Standard BSGD maintains the budget by merging two support vectors when the budget is exceeded; the authors generalize this to “multi-merge” maintenance that merges M>2 points, reducing how often the expensive merge-partner search is triggered. They introduce partner selection that reuses pairwise merge evaluations (against the smallest-|α| support vector) to choose the best M−1 additional mergees, then implement merging either via gradient descent over the preimage (MM-GD) or via a cascade of M−1 standard binary merges (MM-BSGD). Experiments on several binary classification datasets (PHISHING, WEB, ADULT, IJCNN, SKIN) show large training-time reductions (e.g., merging 3 points yields ~30–50% speedups; merging 10 points can yield up to ~5× speedup) with little or no accuracy loss for moderate M. The work is machine-learning optimization focused rather than reliability engineering; “maintenance” refers to model budget maintenance, not equipment maintenance.","The SVM primal objective minimized is $P(w,b)=\frac{\lambda}{2}\|w\|^2+\frac{1}{n}\sum_{i=1}^n \max\{0,1-y_i(\langle w,\phi(x_i)\rangle+b)\}$. Budget maintenance is formulated as minimizing weight degradation $E=\|\Delta\|^2=\|w'-w\|^2$ when replacing multiple support-vector terms $\sum_i \alpha_i\phi(x_i)$ with a single term $\alpha_z\phi(z)$; for Gaussian kernels, binary merging reduces to 1-D optimization over $z=hx_i+(1-h)x_j$ with optimal $\alpha_z$ in closed form, solved via golden section search.","Empirically, the merge-partner search/merge step can consume a large fraction of total training time (reported up to ~45% in the baseline two-point merge setting). Merging three points at once reduces total training time by roughly 30–50% while preserving similar test accuracy across studied budgets, and larger merges (e.g., M=10) can speed training by about a factor of five in their reported experiments. For ADULT with M=3, a direct 3→1 gradient-descent merge had slightly lower merging time than two cascaded binary merges (e.g., at B=120: 6.042s vs 10.562s) with essentially identical accuracy (~76.32%). Overall accuracy trends are dominated by budget size B, while moderate M (recommended M∈{3,4,5}) provides stable speedups with minimal accuracy impact.","The authors note that for too large M, merging many points can cause large approximation error (weight degradation), potentially harming performance. They also discuss that when the Gaussian kernel is very peaked (large $\gamma$), merging can become unavoidable/removal-like for distant points, which is known to cause oscillatory behavior and poor models; thus multi-merge could be detrimental in such regimes. They additionally remark that results for small budgets can be unstable due to the stochastic nature of BSGD.","The evaluation is limited to a small set of binary classification benchmarks and focuses on wall-clock training time and test accuracy, without deeper analysis of robustness across random seeds, convergence criteria, or statistical significance. The multi-merge selection heuristic (anchoring on smallest-|α| and using pairwise degradations) may be sensitive to data scaling/kernel parameters and may not generalize to other kernels or structured outputs; this is not thoroughly stress-tested. Implementation and reproducibility details (hardware, exact stopping conditions, and full code) are not provided in the paper excerpt, making it hard to validate the reported speedups independently.",None stated.,"Evaluate multi-merge BSGD under broader conditions (other kernels, multi-class settings, non-i.i.d. streams, and more datasets) with rigorous statistical reporting over multiple runs. Develop adaptive strategies to choose M online based on observed degradation or kernel length-scale (e.g., $\gamma$) to avoid detrimental merges, and study principled selection rules beyond the smallest-|α| anchor heuristic. Provide an open-source implementation and benchmarks to facilitate reproducibility and comparison against newer scalable kernel/approximation methods.",1806.10179v1,https://arxiv.org/pdf/1806.10179v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:22:41Z
FALSE,NA,ML-based|Other,Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,R|Python|Other,Not provided,https://lukeoakdenrayner.wordpress.com/2017/12/18/the-chestxray14-dataset-problems/,"The paper evaluates how radiology CNNs for pneumonia detection generalize across hospital systems and shows that internal test performance can substantially overestimate external performance due to confounding variables. Using 158,323 chest radiographs from NIH, Mount Sinai (MSH), and Indiana University (IU), the authors train DenseNet-121 models and compare internal vs external AUCs, finding significant degradation in 3/5 natural cross-site comparisons (e.g., joint MSH+NIH model AUC 0.931 internally vs 0.815 on external IU). They demonstrate that models can exploit site-specific cues correlated with disease prevalence: a CNN predicts hospital system with ~99.95–99.98% accuracy (NIH/MSH) and identifies MSH department (inpatient vs ED) with 100% accuracy, indicating strong learnable acquisition/source signals. An engineered-prevalence experiment shows that increasing prevalence imbalance between sites improves internal AUC but harms external IU performance, supporting the hypothesis that models calibrate to site prevalence rather than pathology. The work highlights dataset shift and confounding as key risks for deployment of radiological deep learning and motivates rigorous multi-site external validation.","The paper defines a subregion-based hospital-system probability from activation maps: $P(\text{hospital}=\text{NIH}\mid \text{radiograph}_{i,j})=\frac{e^{Y^{\text{NIH}}_{i,j}}}{e^{Y^{\text{NIH}}_{i,j}}+e^{Y^{\text{MSH}}_{i,j}}+e^{Y^{\text{IU}}_{i,j}}}$, where $Y^{\text{Hospital}}_{i,j}=\sum_{k=1}^{K}(\beta^{\text{Hospital}}_{k}X_{k,i,j})+\beta^{\text{Hospital}}_{0}$ uses final-layer activations $X_{k,i,j}$. Model performance is primarily evaluated via ROC AUC with statistical comparisons using DeLong’s test.","Cross-site generalization often degrades: NIH-trained pneumonia model AUC 0.750 internally (NIH) vs 0.695 on external MSH (p<0.001); MSH-trained model AUC 0.802 internally (MSH) vs 0.717 on external NIH (p<0.001). A jointly trained MSH+NIH model achieves AUC 0.931 on the joint internal test set but drops to AUC 0.815 on external IU (p=0.001). A trivial prevalence-only ranker achieves AUC 0.861 on the joint MSH-NIH test set, showing hospital system prevalence alone is highly predictive in pooled data. Site is directly detectable: hospital system classification accuracy is 99.95% (NIH), 99.98% (MSH), 95.59% (IU), and MSH department (inpatient vs ED) is predicted at 100% accuracy; engineered prevalence imbalance increases internal AUC up to 0.899 but reduces external IU AUC to as low as 0.641.","The authors state that, given limitations of available public data and lack of granular patient-population details, they cannot fully assess which factors contribute to hospital system–specific biases. They also note that the relatively small size and low pneumonia case count in the IU dataset yields wide confidence intervals and may reduce power to detect some external degradation.","The study focuses on AUC and does not deeply evaluate calibration, decision-curve utility, or clinical operating points beyond setting thresholds for 95% sensitivity, which can mask practical harms under prevalence shift. Ground-truth labels differ by site (curator labels, proprietary NLP, and site-trained NLP), introducing label shift/noise confounding that may inflate or distort measured generalization gaps. The approach does not include domain adaptation/robustness baselines (e.g., reweighting, adversarial debiasing, invariant risk minimization) that would clarify how mitigations compare to naive pooling. The IU split for some tasks is not patient-level (accession-based) due to missing identifiers, leaving potential leakage/correlation that could affect reported accuracies and AUCs.",None stated.,"Evaluate and report calibration and clinical utility under explicit prevalence and acquisition shifts, including site-stratified calibration methods and prospective multi-site validation. Add robustness methods (e.g., domain generalization/adversarial removal of site signals, harmonization, causal/invariant learning) and compare to naive pooling using standardized benchmarks. Improve labeling consistency across sites (e.g., unified adjudication or standardized NLP pipelines) to disentangle true domain shift from label shift. Explore higher-resolution and radiology-specific architectures and conduct ablations to quantify how resolution/downsampling influences reliance on confounders.",1807.00431v2,https://arxiv.org/pdf/1807.00431v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:23:20Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Right-censored|Event/count data|Mixture of types|Other,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,NA,"The paper studies estimation of an identical component lifetime distribution in repairable series systems observed as superposed renewal processes (SRPs) when the socket/cause of each failure is masked (latent). It proposes two estimation approaches that explicitly introduce latent socket indicators: (i) a maximum-likelihood method using an EM algorithm with Monte Carlo E-steps, and (ii) a Bayesian model fit via Metropolis-within-Gibbs MCMC with noninformative priors. The methods are presented generically for any positive-support parametric lifetime distribution; simulations focus on Weibull lifetimes and show comparable or improved accuracy versus the existing Zhang et al. likelihood approach, while remaining computationally feasible in high-failure/high-component settings where Zhang et al. becomes intractable. Performance is assessed mainly by mean absolute error (MAE) of the estimated reliability curve over a time grid across many simulated scenarios. A real diesel-engine cylinder dataset (120 engines, 16 cylinders each, masked cylinder identity) is analyzed with several candidate parametric models, selecting lognormal as best by AIC-family criteria (frequentist) and LPML/CPO (Bayesian).","The masked-data likelihood for a fleet of SRPs is a product over systems of sums over all socket-assignment configurations: $L(\theta\mid T)=\prod_{i=1}^n\left[\sum_{v=1}^{V_i} L_{iv}\right]$, with $V_i=m^{r_i}$. Introducing latent socket indicators $d_i$ yields an augmented likelihood $L(\theta\mid T,d)=\prod_{i=1}^n L_i(\theta\mid T^i,d_i)$, where each socket contributes a renewal-product of inter-failure densities $\prod_k f(\Delta x_{ilk})$ times survival terms $R(\tau_i-x_{iln_l})$, and non-failing sockets contribute $R(\tau_i)$. The EM method maximizes the Monte-Carlo approximation $Q_m(\theta\mid\theta^{(r)})=\frac1L\sum_{l=1}^L \log L(\theta\mid T,d^{(l)})$ with $d^{(l)}\sim f(d\mid T;\theta^{(r)})$; the Bayesian approach targets $\pi(\theta,d\mid T)\propto \pi(\theta,d)L(\theta,d\mid T)$ via Metropolis-within-Gibbs.","In simulated Example 1 (n=100, m=16, censor mean mc=4), MAE for the reliability curve was 0.0117 (Bayesian), 0.0057 (EM-ML), and 0.0057 (Zhang et al. MLE via SRPML). In simulated Example 2 (n=100, m=16, mc=8), Z-ML could not be computed due to the high number of failures/configurations, while MAE remained low and similar for Bayesian (0.0080) and EM-ML (0.0079). Across 32 simulation scenarios (n∈{10,50,100,200}, m∈{4,8,16,32}, mc∈{4,8}), the proposed EM-ML and Bayesian methods produced comparable MAE distributions, and Z-ML failed to return results in the higher-failure settings (notably m∈{16,32} with mc=8) due to excessive computation time/errors. In the cylinder case study (120 engines, 16 cylinders), lognormal was selected as best model; estimated mean component life was about 11.05 years (Bayesian posterior mean) and 10.93 years (EM-ML).","The authors note that their modeling assumes component failure times are independent and identically distributed (i.i.d.) across sockets and over time, and they acknowledge this assumption may not hold in other applications. They also discuss that the existing Z-ML approach is computationally limited for large numbers of failures/components; their work is positioned to remove that restriction.","The methods rely on correct specification of a parametric lifetime family; misspecification (e.g., unmodeled heterogeneity across sockets/systems) could bias reliability estimates, especially under masking. The Monte Carlo EM (with fixed L=1000) and MCMC implementations may have nontrivial computational cost and tuning sensitivity (proposal choices, mixing), yet systematic runtime/mixing diagnostics and scalability benchmarks beyond anecdotal comparisons to Z-ML are limited. The approach assumes independence of sockets within a system conditional on parameters; common-cause effects or shared environment covariates are not modeled and could affect both inference and uncertainty quantification.","They propose extending the methodology to settings where the i.i.d. assumption for component failure times is violated. They also plan to investigate using the approach for assessing system reliability (not only component/cylinder reliability), which was the focus of this work.","Extending the framework to incorporate covariates and random effects (frailty) could address heterogeneity across systems and dependence between sockets due to shared operating conditions. Developing robust/semi-parametric variants (or goodness-of-fit diagnostics) could reduce sensitivity to parametric misspecification under masking. Providing an open-source implementation (e.g., an R package) and exploring more efficient inference (e.g., variational Bayes or particle EM) would improve practical adoption for very large fleets and long observation windows.",1807.01269v2,https://arxiv.org/pdf/1807.01269v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:23:58Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,http://ecmlpkdd2006.org/challenge.html|http://www.cs.jhu.edu/~mdredze/datasets/sentiment,"This paper proposes methods for multi-source transfer learning when source domains have unequal labeled-data volume and labeling noise (“source reliability divergences”). The first method, Peer-Weighted Multi-Source Transfer Learning (PW-MSTL), combines source–target proximity (estimated via MMD/KMM-style distribution matching) with a learned inter-source relationship matrix that quantifies how well models trained on one source perform on other sources. The second method, Adaptive Multi-Source Active Transfer (AMSAT), performs pool-based active learning in the source domains (where an oracle exists) to improve transfer to an unlabeled target, combining an adaptive source-selection strategy with an instance-selection criterion that mixes uncertainty sampling with representativeness weights. The authors evaluate on synthetic data and two real datasets (spam detection and Amazon sentiment) and report consistent accuracy improvements over baselines such as KMM, KMM-Aggregate, A-SVM, and DAM, especially when source labeled fractions are highly uneven. Overall, the contribution advances transfer learning robustness rather than reliability engineering in the classical sense (failure/degradation/maintenance).","PW-MSTL learns an inter-source relationship matrix $R\in\mathbb{R}^{K\times K}$ with off-diagonal entries $R_{i,j}\propto \exp(\beta_1\,\hat\epsilon_{S_i}(\hat h_j))$ (diagonal set to 0), where $\hat\epsilon_{S_i}(\hat h_j)$ is empirical error of a classifier trained on source $j$ when evaluated on source $i$. Combined source weights are $\omega = \delta\cdot[\mu I_K + (1-\mu)R]$, where $\delta$ encodes source–target proximity and $\mu$ trades off proximity vs. reliability. Proximity weights are obtained by solving a reweighted MMD/KMM quadratic program: $\min_{\alpha_k}\|\frac{1}{n_k}\sum_i \alpha^k_i\Phi(x_i^{S_k})-\frac{1}{n_T}\sum_i\Phi(x_i^T)\|_\mathcal{H}^2$ with $\alpha^k_i\ge 0$, and AMSAT selects queries by maximizing an uncertainty term times the stored matching weight $\alpha_i^{k(t)}$.","On unevenly labeled sources (labeled fractions drawn from {1%, 5%, 15%, 30%}), PW-MSTL achieves higher target accuracy than all compared methods across synthetic, spam, and many sentiment domains; e.g., on sentiment ‘electronics’ PW-MSTL reports 79.3% vs 77.6% (KMM), 74.6% (KMM-A), 70.8% (A-SVM), 71.3% (DAM). Under balanced labeling (e.g., 10% and 50% labeled per source), PW-MSTL remains best or near-best; for sentiment ‘dvd’ at 50% labeled, PW-MSTL reports 87.2% vs 82.2% (KMM) and 84.4% (DAM). Sensitivity plots show accuracy typically increases then decreases as $\mu$ varies from 0 to 1, supporting the need to combine proximity and reliability rather than using proximity alone. In active learning experiments (budget = 10% of source examples), AMSAT produces consistently better accuracy-vs-queries curves than Random, Uncertainty, Representative, and Proximity baselines on sentiment domains, with reported significance p < 0.0001 across datasets.",None stated.,"The work is not a reliability-engineering study; “reliability” refers to label quality/quantity, so results do not translate to engineering reliability metrics (failure rates, MTBF, etc.). The empirical evaluation focuses on accuracy on selected datasets and synthetic Gaussian setups, with limited discussion of robustness to strong conditional-shift violations or severe model misspecification. Implementation details are insufficient to reproduce results (no released code and incomplete hyperparameter reporting across all experiments).",None stated.,"It would be valuable to study robustness when the assumption of similar conditional probabilities $P(Y\mid X)$ across sources/target is violated (i.e., stronger label/conditional shift), and to derive or estimate reliability under such shifts. Extensions to multi-class/structured prediction with calibrated confidence measures, and to settings with unknown/limited oracles (imperfect labels) would improve practicality. Releasing reference implementations and adding broader benchmarks (more domains, varying noise models, and stronger baselines) would strengthen reproducibility and comparative conclusions.",1807.02235v1,https://arxiv.org/pdf/1807.02235v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:24:34Z
TRUE,System reliability|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Other",Simulated only,Not applicable,Energy/utilities|Transportation/logistics|Network/cybersecurity|Other,Other,TRUE,R|None / Not applicable,Not provided,http://dx.doi.org/10.1111/risa.12228|http://dx.doi.org/10.1016/j.ress.2017.03.003|http://www.louisaslett.com/,"The paper develops a new survival-signature-based methodology for reliability analysis of general phased mission systems (PMSs), where the system structure changes across phases and component failures across phases are dependent through non-repairability and ageing. It extends the standard survival signature by defining a phase-indexed family of survival signatures that applies up to the current phase and accommodates arbitrary phase structures and general component lifetime distributions, including situations where components skip phases. To handle heterogeneous components across phases, it introduces “meta-types” (same physical type and same phase participation pattern) so that conditional lifetime distributions remain coherent within phases, preserving the survival signature’s separation of structure from component lifetime models. The resulting PMS reliability function is expressed as a sum over counts of functioning components by (meta-)type and phase, multiplied by binomial probabilities derived from conditional reliabilities within each phase. Numerical examples (including Weibull lifetimes and a space-mission benchmark with multiple component types) demonstrate agreement with prior BDD-based methods and highlight that PMS reliability curves can exhibit jump discontinuities at phase boundaries due to non-critical failures becoming critical after reconfiguration.","Mission reliability is computed via phase-indexed survival signatures: for phase p, $\Phi_p(\cdot)=\""\prod_{i=1}^p\prod_{k=1}^K {m_{ik}\choose l_{ik}}^{-1}\""\sum_{X\in S}\prod_{i=1}^p\varphi_i(X)$ (Eq. 11), summarizing structure up to phase p. System reliability at time t is then $R(t)=\sum \Phi_{\rho(t)}(\{l_{ik}\})\prod_{i=1}^{\rho(t)}\prod_{k=1}^K {m_{ik}\choose l_{ik}} (R_{ik}(t))^{l_{ik}}(1-R_{ik}(t))^{m_{ik}-l_{ik}}$ (Eq. 14), where $\rho(t)$ maps time to the current phase. Component conditional reliabilities are $R_{ik}(t)=1-\frac{F_k(\min\{t,\tau_{i+1}\})-F_k(\tau_i)}{1-F_k(\tau_i)}$ (Eq. 13), enabling general lifetime distributions.","Across three worked examples, the proposed survival-signature PMS method matches reliability results previously reported using independent BDD/DSPN approaches (e.g., Zang et al.). In Example 1 (three 10-hour phases, exponential failure rate $10^{-4}$/hour), the reliability shows a jump at the phase-3 boundary: $R(20^-)=0.997700$ vs $R(20^+)=0.99601$. In Example 2 (Weibull lifetimes with phase-dependent parameters), the reliability has a jump at $t=10$: $R(10^-)=0.999768$ and $R(10^+)=0.999536$, with the jump size reported as $2.3\times10^{-4}$. In Example 3 (space mission benchmark with five phases), tabulated values include $R(45864)=0.98943$, with multiple phase-boundary discontinuities consistent with reconfiguration effects.","The authors state that reliability analysis of PMSs with multiple failure-mode components is not studied in this paper, noting that practical components may have more than one failure mode. They also discuss modeling constraints motivating their extension, emphasizing that without their “meta-type” construction, identical physical types that skip phases can violate identical conditional lifetime assumptions unless exponential lifetimes are assumed.","The approach assumes conditional independence and exchangeability within each (meta-)type in each phase; common-cause failures, within-phase dependence, and shared environmental shocks are not incorporated in the core formulation. Practical computation of the required (phase-indexed) survival signatures for large PMSs with many types/phases may still be challenging, but algorithmic complexity and scalability benchmarks are not quantified in the paper. The numerical validation is limited to illustrative examples with tabulated outputs; there is no broad simulation study over varied structures/parameters to characterize performance, robustness, or runtime compared to BDD/Markov/Petri-net implementations.","The authors indicate ongoing work on component importance analysis for PMSs, extending survival-signature-based importance methods (e.g., prior work cited as [23, 31]) to the phased-mission setting. They also point to extending the framework to handle components with multiple failure modes.","Extending the methodology to explicitly model dependent failures (e.g., common-cause failures, phase-dependent covariates, or shared loads) within the survival-signature framework would broaden applicability. Developing and releasing efficient software to compute PMS survival signatures (including automated meta-type construction) and providing complexity/runtimes on large benchmarks would improve practical adoption. Additional work could address imperfect repair/maintenance and partially observable condition monitoring to integrate PMS reliability with maintenance decision-making.",1807.09616v1,https://arxiv.org/pdf/1807.09616v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:25:10Z
FALSE,NA,ML-based|Bayesian|Other,Sensor/condition monitoring|Mixture of types|Simulated only|Other,Not applicable,Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,NA,"This paper proposes Counterfactual Normalization (CFN), a causal-graph-based approach to proactively improve model reliability under dataset shift without access to target-domain samples during training. CFN identifies “vulnerable” observed variables that have active paths to the target label passing through unstable mechanisms (selection bias S or unobserved, domain-dependent confounders U) and removes them from the conditioning set to prevent learning non-transportable associations. To retain useful stable information that would otherwise be discarded, the method introduces a node-splitting operation that augments the DAG with latent counterfactual variables (e.g., $Y(C=\emptyset)$ or $Y(A=\emptyset, C=\emptyset)$) which sever unstable backdoor paths while preserving stable causal signal paths to the target. The paper provides algorithms to construct stable conditioning sets and decide which counterfactuals to estimate, assuming additive structural equations (no parent interactions) and fitting structural equations by maximum likelihood/MAP. Simulation studies and a real EHR sepsis prediction task show that CFN yields more stable cross-domain performance and can outperform baseline models in the target domain despite sometimes reduced source-domain accuracy.","Key components are (i) the node-splitting operation that creates a latent counterfactual node $Y(P=\emptyset)$ and rewires parent edges so that $Y(P=\emptyset)$ inherits the non-intervened parents and becomes a parent of factual $Y$. In the real-data experiment, INR is counterfactually normalized by subtracting estimated parent contributions: $Y_i(\emptyset,\emptyset,\emptyset)=Y_i-\hat\beta_2^\top A_i-\hat\beta_3^\top C_i-\hat\beta_4 X_i$ (Eq. 1). Prediction is then performed with discriminative models of the form $p(T\mid Z')$ where $Z'$ includes stable observed variables and/or counterfactual features.","In the linear Gaussian simulated regression, the CFN estimator’s test MSE remains stable as the confounded mechanism parameter varies, while a naive model’s error can worsen substantially as shift increases (Figure 3). In a simulated cross-hospital classification setting, AUROC (source/target) is Baseline 0.95/0.80, CFN 0.96/0.97, and CFN with vulnerable variables 0.97/0.92 (Table 1), showing markedly better transfer for CFN. In the real EHR sepsis task with induced selection bias, on biased data average AUROC/AUPRC are ~0.98/0.45 (baseline with vulnerable vars) vs 0.96/0.38 (CFN), but on unbiased data CFN achieves higher AUPRC (0.30) than vulnerable models (0.24) with similar AUROC (~0.96 vs ~0.95) (Figures 6–7). A noise-sensitivity experiment shows CFN’s target performance advantage depends on accurate counterfactual estimation; high counterfactual MSE can eliminate gains and make CFN worse than vulnerable-variable models (Figure 4).","The authors note that prior approaches require ignorability/no-unobserved-confounders assumptions that often do not hold, motivating their method; CFN still relies on structural/functional assumptions to estimate counterfactuals. They also acknowledge that CFN may incur accuracy loss in the training/source domain because it removes informative but unstable relationships. In the real-data sepsis experiment, they explicitly simplify the problem to a small set of variables (a simplified cross-sectional setting) to demonstrate the technical ideas rather than modeling the full complexity of sepsis.","CFN assumes access to a correctly specified causal DAG (including where selection bias and unobserved confounding may occur); misspecification could lead to removing the wrong variables or retaining unstable paths. Counterfactual estimation requires additive (no-interaction) structural equations for node-splitting to isolate parent effects cleanly; real clinical mechanisms often include interactions and nonlinearities, making stability sensitive to modeling choices. The evaluation focuses mainly on AUROC/AUPRC stability under constructed shifts (simulated mechanism changes and induced selection bias); broader real-world shifts (temporal drift, measurement changes, coding changes, autocorrelation) and calibration/decision-curve impacts are not deeply assessed. No implementation artifacts (runtime, scalability, reproducible code) are provided, which may hinder practical adoption and independent verification.","They suggest testing whether counterfactual features are more intelligible to human experts than standard adjustment methods (e.g., via a future user study).","Develop robust/self-starting variants that do not require a fully known DAG, e.g., incorporating causal discovery uncertainty or sensitivity analysis over plausible graphs. Extend counterfactual normalization beyond additive SEMs to allow interactions and richer nonlinear structural models (with identifiability/robustness guarantees). Evaluate CFN on additional real dataset-shift types (temporal shifts, site-to-site coding differences, sensor drift) and include calibration and downstream decision utility analyses, not just discrimination metrics.",1808.03253v1,https://arxiv.org/pdf/1808.03253v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:25:50Z
FALSE,NA,ML-based|Other,Other,Not applicable,Pharmaceutical,Simulation study|Other,TRUE,Python|Other,Not provided,https://github.com/flatkinson/standardiser,"The paper proposes “Deep Confidence,” a framework to produce per-prediction confidence intervals for deep neural network (DNN) regression models by combining Snapshot Ensembling (extracting multiple DNN “snapshots” from a single training run using cyclical/annealed learning rates) with conformal prediction. The method constructs nonconformity scores from validation residuals normalized by the ensemble’s predictive dispersion and then forms prediction intervals for test points based on the selected confidence level. Experiments on 24 ChEMBL IC50 (pIC50) datasets show Snapshot Ensembles achieve predictive RMSE comparable to Random Forests and ensembles of independently trained DNNs, while Deep Confidence yields valid conformal intervals whose distributions are typically less spread than RF-based conformal predictors. The work emphasizes computational efficiency because Snapshot Ensembles reuse a single training trajectory to obtain many base learners at essentially no extra training cost. The study also evaluates local validity across activity (pIC50) bins and notes higher error rates in bins with fewer samples, especially in high-activity regions.","The conformal nonconformity score for validation instance $i$ is defined as $\alpha_i = \frac{|y_i-\hat{y}_i|}{\exp(\sigma_i)}$, where $\hat{y}_i$ is the ensemble mean prediction and $\sigma_i$ is the standard deviation across ensemble members for that instance. For a desired confidence level (CL), the corresponding quantile $\alpha_{CL}$ of the sorted $\{\alpha_i\}$ is selected. The prediction interval for test instance $j$ is then $\hat{y}_j \pm \exp(\sigma_j)\,\alpha_{CL}$, using the ensemble mean $\hat{y}_j$ and ensemble standard deviation $\sigma_j$.","Across 24 ChEMBL IC50 datasets, RF, independently trained DNN ensembles, and Snapshot Ensembles show comparable test performance (Kruskal–Wallis P > 0.05) with mean test RMSE values in the ~0.6–0.9 pIC50 range. Conformal predictors built from DNN ensembles and Snapshot Ensembles are globally valid, showing strong agreement between nominal confidence level and empirical coverage (reported $R^2 > 0.99$, P < 0.001). Average 80% interval widths are broadly similar across methods (typically ~0.8–1.2 pIC50), but RF-based conformal predictors sometimes produce much wider intervals (reported outliers > 6 pIC50) due to larger ensemble variance. Local validity varies across pIC50 bins, with higher error rates in sparsely populated (often high-activity) bins; the authors note this as an area needing improvement.","The authors note that Snapshot Ensemble parameterization (learning-rate schedule and related settings) may need to be optimized per dataset to obtain tighter and robust intervals, and that further tuning would be advisable for other tasks, datasets, or descriptors. They also state that local validity across particular bioactivity ranges (especially the high-activity region with fewer instances) is not guaranteed and requires new methods. They add that for deeper architectures, sampling multiple local minima may require more sophisticated learning-rate annealing schedules and that additional computation might be needed if obtaining 100 high-quality snapshots requires longer training than expected.","The approach relies on exchangeability/independence assumptions underlying conformal prediction, which may be violated under common drug-discovery data issues such as scaffold/time splits, dataset shift, or strong chemical-series clustering; the paper mainly uses random splits. Interval quality depends on ensemble standard deviation as a proxy for uncertainty; correlated snapshots may underestimate epistemic uncertainty in regions far from training data, and the exponential scaling choice is heuristic. Comparisons focus on RF and DNN ensembles; other modern uncertainty baselines (e.g., MC-dropout, deep evidential regression, Bayesian last-layer/GP heads) are not systematically benchmarked. No released implementation is provided, which limits reproducibility and adoption.",The authors state that future work is needed to develop methods that guarantee local validity in the high-activity range (where data are scarce). They also propose more comprehensive evaluation of how exhaustively sampling local minima (versus sampling fewer or more correlated minima) affects conformal efficiency. They further mention that deeper architectures may require more sophisticated cyclical/annealing schedules and that this should be explored.,"Evaluate Deep Confidence under non-exchangeable and more realistic splitting schemes for chemistry (scaffold splits, temporal splits, and external validation) to assess robustness under dataset shift. Develop self-calibrating or adaptive conformal methods (e.g., Mondrian/conditional conformal regression or covariate-shift-aware conformal) tailored to chemical space density and activity cliffs. Extend the framework to multitask or multi-output settings common in drug discovery and assess interval calibration jointly across tasks. Provide an open-source reference implementation (e.g., PyTorch + RDKit pipeline) and standardized benchmarks to improve reproducibility and facilitate practitioner uptake.",1809.09060v1,https://arxiv.org/pdf/1809.09060v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:26:27Z
TRUE,Maintenance optimization|RUL prediction|Degradation modeling|Other,ML-based|Other,Sensor/condition monitoring|Event/count data|Mixture of types,Predictive|Condition-based,Other,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper argues that standard predictive-maintenance (PdM) model selection using precision/recall/F1 can be economically suboptimal, and proposes selecting model/hyperparameters using an application-specific economic cost (savings) function derived from confusion-matrix outcomes. It constructs a business-cost model that assigns explicit monetary costs to ticketing/triage, service/parts, and downtime, yielding an affine savings function of the form S(TP,FP)=a·TP−b·FP (and a generalized form where coefficients depend on gap and prediction-window lengths). Using a real-world PdM dataset (150,924 devices over 6 months) and a random-forest classifier tuned via grid search (ntree, mtry, sampling, cutoff), it demonstrates that optimizing F1 can increase total cost versus reactive maintenance, while optimizing the economic savings function reduces cost. In the reported base case, the F1-optimized model produced negative savings (extra cost) while the savings-optimized model produced positive savings and a “fail-safe” behavior by selecting high cutoffs when predictions are unreliable. The work advances PdM evaluation by tying model selection directly to business impact and by showing how to incorporate additional operational constraints (gap, prediction interval) into the economic objective.","Binary-classification performance measures are expressed in terms of confusion-matrix counts: recall $RE=TP/P$, precision $PR=TP/(TP+FP)$, and $F1=2TP/(TP+FP+P)$. The paper’s basic savings objective derived from a cost table is an affine function of $(TP,FP)$, e.g. $S=44\,TP-55\,FP$ (example use case). A generalized affine family is proposed: $\pi(TP,FP)=a(T_G,T_P)\,TP+b(T_G,T_P)\,FP+c(T_G,T_P)$, with downtime modeled as $\hat{T}_D(T_G,T_P)=\max(0, T_T-T_G-T_P/2)+T_R$, leading to the generalized savings equation (12): $S(TP,FP)=[32+2(\hat{T}_D(0,0)-\hat{T}_D(T_G,T_P))]TP-[51+2\hat{T}_D(T_G,T_P)+0.06T_P]FP$.","On the real dataset (train weeks 1–12, test weeks 13–24; 10-week observation, 1-week transition, 1-week prediction; test positives 5,445 and negatives 10,479), F1-optimization selected cutoff 0.36 and achieved precision 0.4451, recall 0.8033, F1 0.5729, but increased cost versus reactive maintenance (reactive $539,055 vs predictive $646,459; savings $-107,404). Savings-optimization selected cutoff 0.62 with precision 0.6359, recall 0.3153, F1 0.4216, and reduced cost (predictive $517,572; savings $+21,483) under the paper’s example cost structure. The ROC analysis shows the savings-optimal point occurs where the model ROC intersects the savings upper-bound line, and highlights a “passive mode” (very high cutoff ~0.95) that avoids negative savings when predictions are unreliable. Across multiple (gap, prediction-interval) settings using the generalized cost (Table 1), choosing the model via the economic objective consistently reduced costs relative to reactive (e.g., for $T_G=7, T_P=7$: reactive $539,055; PdM chosen by F1 $648,749 (120.35%); PdM chosen by S $517,985 (96.09%)).","The example business case uses “realistic but artificial costs” inspired by a prior PdM case, implying results depend on the assumed cost structure. The paper notes that savings depend strongly on factors like class imbalance/failure rate, prediction-window length, and the transition (gap) interval, which can make outages harder to predict and reduce achievable savings. It also remarks that larger prediction intervals can lead to early repairs/replacements and less accurate failure timing, affecting practical benefit.","The evaluation is centered on a single classifier family (random forest) and one dataset, so it is unclear how robust the conclusions are across different model types (e.g., calibrated probabilistic models, temporal deep learning) and other domains. The savings objective is largely reduced to an affine function of (TP,FP); real maintenance operations often involve non-linearities (capacity constraints, batching, routing/travel coupling, inventory limits) that can break additivity. The approach assumes stable cost parameters and stationarity between training/test; concept drift or changing service processes could invalidate the tuned cutoff and weights without explicit adaptation/monitoring. No implementation details (software, code, reproducibility artifacts) are provided, limiting replicability and sensitivity analysis (e.g., uncertainty in cost coefficients).","The authors plan to deploy the solution in a production environment and extend it to different PdM use cases. They propose investigating additional cost factors, such as differing costs for different service personnel involved in the maintenance process. They also identify the influence of sliding-window step size and overlapping prediction windows as an “interesting and untouched” direction for further study.","A useful extension would be to incorporate operational constraints explicitly (technician capacity, routing/travel optimization, spare-part inventory) so the objective reflects end-to-end maintenance decisions rather than per-incident additive costs. Another direction is robust or Bayesian cost-sensitive tuning that accounts for uncertainty in the cost coefficients and drift over time, with online recalibration of probability outputs and cutoff selection. The framework could be generalized to time-to-failure/RUL outputs and lead-time-dependent utility, linking classification decisions to prognostic horizons. Finally, providing an open-source reference implementation and benchmarking against other cost-sensitive methods (e.g., weighted loss, threshold-moving with calibrated probabilities, constrained optimization) would strengthen practical adoption.",1809.10979v1,https://arxiv.org/pdf/1809.10979v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:27:05Z
TRUE,Maintenance optimization|Network/infrastructure reliability|Other,"Parametric (Weibull, etc.)|ML-based|Hybrid/Ensemble|Other",Event/count data|Sensor/condition monitoring|Mixture of types,Not applicable,Energy/utilities,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a hybrid reliability-forecasting framework for smart-grid distribution networks that predicts daily sustained (N) and momentary (M) interruption counts from common weather time series. It first fits parametric regression models (polynomial up to 3rd degree and two-term exponential) to model the nonlinear relationship between interruptions and individual weather variables (temperature-related metrics, wind speed metrics, precipitation, air pressure, and lightning). Outputs from these fitted parametric models, together with the raw weather variables, are then used as inputs to a multilayer perceptron (MLP) with one hidden layer (10 neurons) to learn combined effects and improve forecasting accuracy. Training uses a modified extreme learning machine (ELM) algorithm that introduces a self-adjusting regularization parameter in the pseudo-inverse solution, with a convergence claim provided. Using Florida utility interruption data (2015-01-01 to 2017-04-30) and weather data (NCDC plus utility lightning data), the hybrid MLP reduces mean squared error versus a prior statistical baseline and includes sensitivity analysis showing lightning as the most influential predictor.","The interruption–weather relationship is modeled per variable via parametric regression: polynomial $f(x,\beta^{pol})=\beta_0+\beta_1 x+\beta_2 x^2+\cdots+\beta_n x^n$ (with $n=1,2,3$) and two-term exponential $f(x,\beta^{ex})=\beta_0+\beta_1 e^{\beta_2 x}+\beta_3 e^{\beta_4 x}$. The MLP output is given by $Y=F\!
\left(b+\sum_{j=1}^m v_j\left[\sum_{i=1}^n G(w_{ij}x_i+b_j)\right]\right)$. The modified ELM computes output weights using a regularized pseudo-inverse $v=H^{\dagger}Y$ with $H^{\dagger}=(H^T H+\lambda)^{-1}H^T$ and $\lambda=\|Y\|^{\delta}$, converting learning to a regularized least-squares objective.","On a real utility management area in Florida (daily data from Jan 1, 2015 to Apr 30, 2017), the proposed hybrid regression+MLP framework improves forecasting accuracy relative to a referenced statistical model baseline. For sustained interruptions (N), it reports MSE = 315.4, an 8.77% reduction versus the statistical model. For momentary interruptions (M), it reports MSE = 31.3, a 61.37% reduction versus the statistical model. Sensitivity analysis indicates lightning strike count is the most influential weather input, while heat degree days have the least impact for the studied area.","The authors note that the current framework primarily uses common weather parameters as inputs and suggest that other factors (e.g., equipment failure rates and aging of distribution components) are not included yet. They also restrict the dataset to days without exclusion data to avoid extreme weather conditions and transmission outages, implying the presented model is focused on normal-operation conditions rather than extreme-event outage forecasting.","The evaluation appears to be based on a single utility management area and a single time period, so generalizability across utilities, climates, and network designs is unclear. The approach relies on fitting separate univariate parametric regressions per weather variable and then feeding those derived predictions into the MLP, which may introduce redundancy/collinearity and potential information leakage if regressions are fit using the full dataset rather than within each training fold (the paper does not clearly state how this is handled). Baseline comparisons are limited (primarily to one prior statistical model), and uncertainty quantification (prediction intervals) for interruption forecasts is not addressed, which is important for operational reliability planning.",The paper explicitly proposes integrating additional non-weather drivers—specifically power system equipment failure rates and aging of distribution network components—as additional inputs to the forecasting framework in future work.,"A valuable extension would be to validate the model across multiple management areas and utilities with cross-area transfer testing to assess robustness and portability. Incorporating temporal dependence explicitly (e.g., autoregressive terms, recurrent networks, or state-space models) and handling seasonality/holiday effects could improve performance beyond purely weather-driven inputs. Providing probabilistic forecasts (e.g., predictive distributions for N and M) and evaluating with proper scoring rules would better support reliability operations and risk-based decision-making.",1810.05004v1,https://arxiv.org/pdf/1810.05004v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:27:40Z
TRUE,RUL prediction|Degradation modeling|Maintenance optimization,ML-based|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Predictive,Transportation/logistics|Manufacturing (general)|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes a deep learning architecture, termed Temporal Convolutional Memory Networks (TCMN), for remaining useful life (RUL) estimation of industrial machinery from multivariate sensor time series. The method combines 1D temporal convolutional layers (for local temporal feature extraction per sensor) with stacked LSTM layers (to capture longer-term dependencies), connected via fully connected layers with dropout, and is trained as a regression model to predict RUL. A training-time data augmentation strategy is introduced that creates additional partial run-to-failure trajectories by truncating each training trajectory at random points after degradation begins, making training samples more similar to test trajectories. The approach is benchmarked on the NASA C-MAPSS turbofan engine datasets (FD001–FD004) using RMSE and the standard asymmetric scoring function that penalizes overestimation more heavily. Results indicate the augmentation significantly improves performance, and the proposed architecture is particularly strong on the more complex operating-condition datasets (FD002/FD004), achieving the best (lowest) reported RMSE/score among the compared baselines in their experiments.","RUL error is defined as $E_n=\mathrm{RUL}_{\mathrm{Estimated}}-\mathrm{RUL}_{\mathrm{True}}$. Performance is evaluated by an asymmetric exponential score $S=\sum_i e^{-E_i/13}$ if $E_i<0$ and $S=\sum_i e^{-E_i/10}$ if $E_i\ge 0$, and by $\mathrm{RMSE}=\sqrt{\frac{1}{N}\sum_i E_i^2}$. The temporal convolution feature map is computed as $d^{(l)}_j=f\big(\sum_i d^{(l-1)}_i * w^{(l)}_{i,j}+b^{(l)}_j\big)$ with 1D max-pooling, and the recurrent component uses standard LSTM gate equations (input/forget/output/cell) to update $(c_t,h_t)$ over time.","Data augmentation yields large gains: for FD002 score improves from $3.55\times 10^4$ to $4.19\times 10^3$ (88.19% gain) and RMSE from 31.06 to 21.03 (40.76% gain); for FD004 score improves from $5.91\times 10^4$ to $7.96\times 10^3$ (86.53% gain) and RMSE from 36.85 to 23.57 (36.03% gain). In benchmarking (their Table IV), the proposed architecture achieves the best RMSE on FD002 (20.45) and FD004 (21.03) versus LSTM baseline RMSEs 24.49 (FD002) and 28.17 (FD004). For FD001 and FD003, the proposed model’s scores are $1.22\times 10^3$ and $1.30\times 10^3$, respectively, which are not better than the cited LSTM baseline scores ($3.38\times 10^2$ and $8.52\times 10^2$), indicating mixed results depending on dataset. The authors also report that removing either the LSTM stack or the temporal convolution stack degrades tracking of the target RUL curve (qualitatively shown in Fig. 5).","The authors note open areas including the need to examine on-line learning, to test newer deep architectures for RUL estimation, and to improve performance on FD001 and FD003. They also state they did not perform operating-condition clustering during normalization (treating all sub-datasets equally), which may affect results across datasets.","The work relies on a specific piecewise-linear capped RUL target construction, which can materially influence training and comparability; robustness to alternative labeling schemes or uncapped/physics-based targets is not analyzed. Evaluation is confined to the C-MAPSS benchmark (simulation-based) and does not demonstrate transfer to real industrial deployments, sensor faults, missing data, or distribution shift across fleets. Implementation details (optimizer, learning rate schedule, training epochs, early stopping, random seeds) and statistical significance/uncertainty over multiple runs are not provided, which limits reproducibility and confidence in reported improvements. The paper frames the augmentation as beneficial, but does not analyze potential leakage/overfitting to the target-shape prior introduced by truncation, nor sensitivity to augmentation size $\lambda$ and window length choices.","They propose examining on-line learning, exploring newer deep architectures for RUL estimation, improving performance on FD001 and FD003, and evaluating the approach on other publicly available datasets for further improvements.","A valuable extension would be to incorporate uncertainty quantification (e.g., Bayesian deep learning or prediction intervals) so maintenance decisions can account for risk, not just point RUL estimates. Another direction is to handle realistic data issues—sensor dropouts, outliers, variable sampling rates, and domain shifts—via robust training, self-supervised pretraining, and/or domain adaptation across operating conditions and assets. The model could also be integrated into explicit maintenance policy optimization (e.g., cost/risk-based decision rules) to connect RUL outputs to actionable CBM/predictive maintenance schedules. Finally, providing an open-source reference implementation and standardized evaluation protocol (multiple seeds, confidence intervals, fair baselines) would improve reproducibility and facilitate broader adoption.",1810.05644v2,https://arxiv.org/pdf/1810.05644v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:28:16Z
TRUE,RUL prediction|Degradation modeling|Maintenance optimization,ML-based|Other,Sensor/condition monitoring|Event/count data|Mixture of types,Predictive,Network/cybersecurity|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://www.backblaze.com/blog/hard-drive-smart-stats/,"The paper proposes a data-driven framework for online remaining useful life (RUL) estimation of hard disks using a stacked Long Short-Term Memory (LSTM) network, targeting settings where failure-indicative feature ranges vary substantially across devices. The key methodological contribution is an integrated, device-specific feature-normalization mechanism for online (test-time) data where the failure point is unknown, using a historical two-month window and a 75th-percentile-based scaling to reduce outlier influence. The approach is trained on 150-day pre-failure sequences (look-back window of 25 days to predict RUL) using selected SMART attributes and is evaluated on Backblaze operational drive data in an online-simulation setup that avoids “future information” leakage. The method performs particularly well near end-of-life and achieves an average precision of 0.8435, recall of 0.72, and F1 score of 0.77 for predicting whether a disk will fail within the next 10 days. The authors also show transferability: a model trained on one Seagate drive model can be applied to another model from the same manufacturer with acceptable performance given compatible normalization thresholds.","Feature selection uses the Fisher score for feature n: $F_n = \frac{\sum_{k=1}^L d_k(\mu_n^k-\mu_n)^2}{\sum_{k=1}^L d_k(\sigma_n^k)^2}$. For online normalization (Prediction Strategy 2), each feature column $p$ of simulation data $B_i$ is scaled as $\tilde{B}_i(:,p)=\frac{B_i(:,p)-\min(B_i(:,p))}{\phi_{75}-\min(B_i(:,p))}$, where $\phi_{75}$ is the 75th percentile of the past two months’ historical feature distribution (after sorting). The LSTM consumes the most recent $ts=25$ time steps of $\tilde{B}_i$ to output predicted RUL.","Using the proposed online normalization (75th-percentile strategy), the model predicts imminent failures (within 10 days) with average precision 0.8435, recall 0.72, and F1 score 0.77 over seven consecutive days of evaluation. The authors report the method is more accurate (lower uncertainty) as a device approaches end-of-life, with Strategy 2 outperforming an optimistic historical-maximum normalization strategy (Strategy 1) especially for near-failure devices. The LSTM inference time is reported as approximately 0.005 seconds per device for online prediction once preprocessing is done. A model trained on Seagate ST4000DM000 is shown to transfer to Seagate ST8000DM002 with acceptable RUL prediction performance under the same normalization strategy.","The authors note that transfer across different disk models may require adjusting the normalization threshold if the historical feature distributions differ drastically from the model used for training. They also emphasize that using cross-validation-style splits with global normalization can leak future failure information into the test data, and that their online simulation setup is designed to avoid this unrealistic advantage.","The approach relies on assumptions that selected SMART features exhibit a consistent increasing trend toward failure and that a two-month historical distribution is representative; these may fail under regime changes (firmware updates, workload shifts) or non-monotonic degradation. The evaluation focuses on a single manufacturer/model for training and demonstrates transfer only within-manufacturer; broader generalization across manufacturers or environments is not established. The paper reports classification metrics for a 10-day failure horizon and qualitative RUL plots but provides limited standard RUL regression metrics (e.g., RMSE/MAE over a large test set) and uncertainty quantification. Implementation details (optimizer, learning rate schedule, random seeds) and reproducibility artifacts (code, exact preprocessing scripts) are not provided, which can materially affect results on large operational datasets.","They plan to extend the architecture to create an adaptive job scheduler that considers hardware failures, workload forecasts, and application interface models. They also intend to apply the generalized normalization-and-inference framework to other related applications with data-specific modifications for online decision relaying.","A valuable extension would be a self-starting/online-updating version that adapts normalization and model parameters under concept drift (changing SMART distributions over time). Robustness studies under autocorrelation, missing data, and different sampling rates (not strictly daily) would improve practical applicability. Adding calibrated uncertainty estimates (e.g., Bayesian LSTMs, ensembles) could better support maintenance decision-making and risk-aware scheduling. Broader benchmarking against modern sequence models (e.g., temporal CNNs/Transformers) and across multiple manufacturers/datasets would clarify generalizability and strengthen conclusions.",1810.08985v3,https://arxiv.org/pdf/1810.08985v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:28:55Z
FALSE,NA,ML-based,Sensor/condition monitoring,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://www.physionet.org/physiobank/database/sleep-edfx/|https://archive.ics.uci.edu/ml/datasets/eeg+database|https://www.physionet.org/pn6/chbmit/,"This paper proposes Compensated Integrated Gradients (C-IG), an interpretability method for EEG deep-learning classifiers that removes the need to choose an appropriate baseline for Integrated Gradients (IG). The method computes IG from a user-defined baseline (e.g., zero) and adds a compensation term estimated as the difference between IG and Shapley Sampling (SS) at a small number of calibration records, aiming to approximate SS-like attributions with far lower cost. The authors provide conditions under which the compensated attributions satisfy IG’s axioms (implementation invariance, dummy, linearity, completeness) and the symmetry axiom when feature-processing pathways are independent and identical (e.g., temporal CNNs with shared weights). Experiments on three public EEG datasets (sleep staging, alcoholism/control, seizure/no-seizure) show C-IG yields attributions much more correlated with SS than standard IG under a zero baseline, particularly for temporal CNNs. They also report that C-IG’s computational cost is orders of magnitude lower than SS while improving reliability of explanations compared to baseline-dependent IG.","The core attribution is a path-integrated gradient: $\text{PathIG}^\gamma_i(x)=\int_{0}^{1} \frac{\partial f(\gamma(\alpha))}{\partial \gamma_i(\alpha)}\,\frac{\partial \gamma_i(\alpha)}{\partial \alpha}\,d\alpha$. The proposed C-IG adds a compensation vector computed on a small calibration set as the difference between SS attributions and IG attributions computed from an arbitrary (user-defined) baseline; this compensation is then added to IG attributions for target inputs to approximate the unknown “appropriate baseline” result. In the appendix, the classifier is expressed as $f(x)=\sum_i\sum_j W_{ij} g_m(x_i)$, and symmetry is shown to hold for path methods when per-feature processing functions $g_m(\cdot)$ are identical and independent across features (e.g., temporal CNNs with shared weights).","Using Spearman correlation against SS attributions as a proxy for explanation reliability, C-IG achieved very high agreement on temporal CNNs (e.g., PhysioNet classes: 0.963–0.988 for C-IG vs 0.953–0.988 for SS, while IG ranged 0.180–0.925 depending on class). On UCI EEG (temporal), C-IG correlations were ~0.994–0.995 vs IG ~0.258–0.260; on CHB-MIT (temporal), C-IG was ~0.993–0.996 vs IG ~0.293–0.817. For spatiotemporal CNNs, C-IG improved over IG but remained below SS (e.g., CHB-MIT seizure: C-IG 0.806 vs SS 0.983 vs IG 0.695; UCI EEG alcoholism: C-IG 0.793 vs SS 0.989 vs IG 0.323). Reported computational costs (UCI EEG example) were 40,000 (IG), 650,000 (C-IG), and 12,200,000 (SS); for 1000 targets the cost ratio was 20:81:6100.","The authors state that the method’s strongest theoretical guarantees (including the symmetry axiom) require classifier constraints—namely that input-feature processing pathways are mutually independent and identical (as in temporal CNNs with shared weights). They also note that these constraints can reduce classification accuracy compared with spatiotemporal CNNs, creating a trade-off between predictive accuracy and interpretation reliability.","The evaluation treats SS attributions as “true,” but SS itself is estimated via sampling and can have variance; this makes correlation-to-SS an imperfect ground truth for reliability. The compensation is estimated from only a small number of calibration records (10 in experiments), so the compensation term may be unstable across subjects, recording conditions, or distribution shift. No robustness analysis is provided for correlated inputs beyond the stated constraints, despite EEG channels/time samples often exhibiting strong dependencies that can violate assumptions.","They propose applying the compensated IG approach to other multichannel time-series domains beyond EEG, specifically mentioning acceleration signals for human activity recognition and electrocardiography (ECG).","Validate compensation stability under dataset shift (different patients, devices, montages) and quantify how many calibration records are needed for reliable compensation. Extend the approach to settings with correlated or interacting features (common in multichannel biosignals) via explicit interaction modeling or structured baselines, and provide uncertainty estimates for attributions. Provide an open-source implementation and benchmarks across additional explanation methods and model classes (e.g., transformers for time series) to improve reproducibility and adoption.",1811.08633v1,https://arxiv.org/pdf/1811.08633v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:29:33Z
FALSE,NA,ML-based|Bayesian|Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a semi-supervised learning method for classification when labels are missing at random (MAR), aiming to produce well-calibrated (reliable) uncertainty estimates for prediction error probabilities. It introduces a selective label-sampling procedure that identifies “label-informative” regions of the feature space using a likelihood-ratio test between labeled and unlabeled feature models, and avoids forcing unlabeled data outside these regions to influence class-conditional modeling. For unlabeled, non-informative regions, it imposes constraints that the feature distribution is class-independent and the class prior is uniform, preventing overconfident miscalibration common under the missing-completely-at-random (MCAR) assumption. The approach is illustrated with deep generative modeling (VAE for representation learning plus variational Bayes Gaussian mixture models for density estimation) and evaluated on MNIST and Fashion-MNIST under MAR, showing improved calibration in reliability diagrams compared to MCAR-based semi-supervised learning and purely supervised baselines. Overall, the contribution is primarily in reliable uncertainty quantification for semi-supervised classification under dataset shift induced by MAR labeling, rather than reliability engineering of physical systems.","The method defines a label-informative region $\mathcal{X}_\ast=\{x: \ln\frac{\hat q(x\mid y,\ell=1)}{\hat q(x\mid \ell=0)}>\kappa,\ \exists y\}$ using a likelihood-ratio criterion. Unlabeled points in $\mathcal{X}_\ast$ are assigned labels by sampling from $\hat q(y\mid x,\ell=0)\equiv \hat q(y\mid x,\ell=1)\propto \hat q(x\mid y,\ell=1)\hat q(y\mid \ell=1)$. For unlabeled points outside $\mathcal{X}_\ast$, constraints are imposed: $q(x\mid y,\ell=0)\equiv q(x\mid \ell=0)$ and $q(y\mid \ell=0)\equiv 1/|\mathcal{Y}|$. The resulting predictive error probability is computed as $q_e(x^*)=1-\frac{q(x^*\mid \hat y)q(\hat y)}{\sum_y q(x^*\mid y)q(y)}$.","On MAR-constructed MNIST and Fashion-MNIST settings (with $|\mathcal{D}_1|=1000$ labeled, $|\mathcal{D}_0|=59000$ unlabeled, and test sets of 10,000), the proposed MAR method produces reliability diagrams whose estimated error probabilities track empirical error probabilities closely (near the diagonal). In contrast, MCAR-based semi-supervised label-sampling and supervised-only baselines systematically underestimate empirical error in regions with few/no labeled samples, yielding overconfident (miscalibrated) predictions. Qualitatively, the MAR method assigns higher predicted error in feature regions outside the label-informative set $\mathcal{X}_\ast$, matching the observed increase in test error there. The paper’s evidence is primarily comparative calibration plots rather than reporting single-number calibration metrics.",None stated.,"The work focuses on classification calibration under MAR and does not address engineering reliability modeling (e.g., lifetime/degradation, system reliability, maintenance). Empirical evaluation is limited to two vision benchmarks with a specific pipeline (VAE features + VB-GMM); it is unclear how sensitive calibration is to representation quality, choice of density model, or the threshold $\kappa$. The paper relies on the MAR assumption $p(y\mid x,\ell=0)=p(y\mid x,\ell=1)$; if violated (MNAR), the selective procedure may still yield misleading uncertainty without an explicit detection/diagnosis mechanism. No code, hyperparameters, or computational details are provided to support reproducibility and implementation in other domains.",The authors suggest evaluating the method with more advanced models and training algorithms beyond the illustrative VAE + VB-GMM setup.,"Extend the approach to explicitly handle MNAR or to provide diagnostics/tests for MAR vs. MNAR violations in practice. Provide self-starting or fully end-to-end training that jointly learns representations and the selective label-informative region, and benchmark against modern calibration-aware semi-supervised methods using quantitative metrics (ECE, Brier score) across varied shifts. Develop robust variants for correlated/temporal data and assess performance under covariate shift with high-dimensional, non-image domains (e.g., medical records) where density estimation is challenging. Release reference implementations to enable reproducibility and broader adoption.",1811.10947v5,https://arxiv.org/pdf/1811.10947v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:30:08Z
TRUE,Degradation modeling|RUL prediction|Other,ML-based|Other,Degradation measurements|Sensor/condition monitoring,Not applicable,Manufacturing (general)|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python,Not provided,NA,"The paper proposes a data-driven framework for remaining useful life (RUL) prediction of rolling element bearings using vibration condition-monitoring data. It constructs a nonlinear degradation energy indicator (DEI) label from raw vibration signals via Hilbert–Huang transform (EMD + Hilbert transform) and marginal Hilbert spectrum values at characteristic bearing frequencies (inner race, outer race, ball). A 1-D convolutional neural network is trained to map raw vibration segments directly to the DEI, enabling automatic degradation indicator estimation for new bearings without manual feature crafting. RUL is then predicted by forecasting the estimated DEI forward in time using an ε-support vector regression (RBF kernel) model until a failure threshold (taken as the terminal DEI of the training bearing) is reached. Experiments on the PRONOSTIA accelerated bearing dataset show lower RUL errors than two ablation baselines and several published methods, and the CNN representation is reported to transfer across operating conditions with minimal SVR retuning.","The degradation label is defined from the marginal Hilbert spectrum as $L_i = \max_{f\in\{f_{inner},f_{outer},f_{ball}\}} M_i(f)$, where $M_i(f)=\sum_{p=1}^P M_i(f,t_p)$ and $M_i(f,t_p)=\sum_{j=1}^n h_{i,j}(f_{i,j},t_p)$ after EMD + Hilbert transform. The label is normalized as $L_{norm,i}=(L_i-\min L)/(\max L-\min L) \pm \epsilon$. The CNN is trained by minimizing MSE $z=\frac{1}{N}\sum_{i=1}^N (L_{norm,i}-\tilde L_{norm,i})^2$, and RUL is obtained by ε-SVR forecasting of DEI until the failure threshold $L_{ft}=L_N$ is crossed, yielding $T_{b,failure}=U\,\tau$.","On PRONOSTIA bearing set 1 tests, the proposed method predicts RULs of 340 s (bearing1_4), 1500 s (bearing1_5), and 1480 s (bearing1_6), with relative percentage errors of -0.29%, 6.83%, and -1.37%, respectively. Compared baselines show substantially larger errors (e.g., C1: 91.15%, 49.07%, 19.11%; C2: 29.19% and 26.02% on bearing1_5 and bearing1_6, with bearing1_4 failing due to early threshold crossing). In the multi-method comparison table, the proposed approach reports $S_{mean}=0.87$, MAE = 46.2, and NRMSE = 0.05 across the evaluated bearings, outperforming several referenced methods. For bearings under different operating conditions (bearing2_4 and bearing2_6), tuning only SVR penalty parameter $C$ from 5.09 to 7.09 yields reported errors of 5.75% and 1.55%.","Model-based approaches are noted as limited by the difficulty of constructing precise physical degradation models under noisy environments, motivating a data-driven approach. For transfer to different operating conditions, the authors still fine-tune the ε-SVR penalty parameter (e.g., changing $C$), implying the full pipeline is not entirely parameter-free across conditions. Future work is explicitly framed as needing wider case studies and exploration of better degradation labels, suggesting current validation scope is limited to the evaluated datasets/labels.","The failure threshold is defined as the last DEI value of the training bearing and reused for test bearings; this assumption may not hold when component-to-component variability or differing failure criteria exist, even within nominally similar conditions. The approach relies on fixed characteristic bearing frequencies and HHT-derived spectra; robustness to speed variation, slip, varying load, and strong nonstationarity (common in field data) is not thoroughly analyzed. The SVR forecasting uses hand-chosen sliding-window features (mean/variance) and step-ahead training, which may limit capturing complex degradation dynamics and uncertainty; no calibrated predictive uncertainty for RUL is provided. Reproducibility is constrained because implementation details are partial and no code is released, while comparisons to prior methods may depend on consistent preprocessing and test protocol choices.",The authors propose applying the framework to a wider range of experimental case studies in other applications. They also suggest investigating other potential degradation labels to achieve higher RUL estimation accuracy.,"Develop a self-starting/online version that estimates thresholds and normalization parameters without requiring a full run-to-failure training bearing, improving deployability. Extend the method to explicitly handle variable speed/load via order tracking or conditioning the CNN on operating covariates, and assess robustness under nonstationary field conditions. Provide uncertainty quantification for DEI forecasts and RUL (e.g., Bayesian SVR/CNN, quantile regression, or conformal prediction) to support risk-aware maintenance decisions. Release a reference implementation and standardized evaluation protocol to enable fair, reproducible benchmarking across competing RUL methods.",1812.03315v2,https://arxiv.org/pdf/1812.03315v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:30:48Z
TRUE,RUL prediction|Maintenance optimization|Other,ML-based|Other,Sensor/condition monitoring|Event/count data|Other,Predictive|Condition-based|Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,R|None / Not applicable|Other,Not provided,http://ti.arc.nasa.gov/project/prognostic-data-repository|https://gallery.azure.ai/Notebook/Predictive-Maintenance-Modelling-Guide-R-Notebook-1,"The paper studies fleetwide predictive maintenance using a large public (synthetic) dataset that combines sensor streams, maintenance/replacement logs, error counts, and failure labels across 100 machines. To handle extreme class imbalance (failures around 1/4000), it uses nominal under-sampling to rebalance normal vs failing states and then benchmarks 27 machine-learning algorithms on accuracy and execution time. It identifies three models—mixture discriminant analysis (MDA), penalized discriminant analysis (PDA), and multivariate adaptive regression splines (gcvEarth/MARS)—that achieve >96% accuracy and run roughly 20× faster than the previously highlighted gradient boosting machine (GBM). The work also uses random-forest variable importance to rank predictive features, finding precursor error counts and rotational sensor summary statistics most informative, while machine age and time since last replacement are relatively weak predictors. The contribution is positioned as a practical data/feature strategy and algorithm-screening workflow for predictive maintenance on heterogeneous fleet data rather than a new reliability model.",Not applicable (the paper describes feature engineering and ML classifiers/benchmarks but does not present defining SPC/reliability equations for a new reliability model).,"Using under-sampling to address class imbalance, the study reports identifying three algorithms (MDA, PDA, and gcvEarth/MARS) exceeding 96% accuracy, with PDA approximately 20× faster than the previously reported GBM approach. It also reports dataset scale (≈3.2 million sensor samples; 876,101 timestamps with four sensor readings) and rarity of events (errors <1/1000; failures ≈1/4000). Feature-importance results indicate error-count features rank highest, followed by rotational mean features, while machine age/model and time since last replacement rank among the least important predictors. Prior work on the same dataset is cited as achieving 88–99% recall with GBM.","The paper notes the primary dataset is synthetic/idealized, implying results may not fully represent the complexity and noise of real fleet data. It also highlights that extreme class imbalance can make naive accuracy misleading without rebalancing/appropriate metrics.","The evaluation emphasizes (rebalanced) accuracy and runtime; it does not provide a comprehensive reliability/PHM metric suite (e.g., precision/recall tradeoffs under original prevalence, cost-weighted errors, time-to-event calibration, or lead-time/early-warning utility), which limits maintenance decision relevance. Under-sampling can distort probability calibration and operational false-alarm rates in deployment unless corrected; the paper does not discuss this. The approach is largely retrospective classification on engineered summary features rather than explicit degradation/RUL stochastic modeling, so uncertainty quantification and prognostic horizons are not addressed.","Future work will expand the classifier suite to include deep learning methods and approaches for unlabeled data such as nearest neighbors and clustering, with the goal of broadening algorithm families beyond those tested.","Validate findings on real fleet HUMS/maintenance datasets and report performance under true class prevalence with cost-sensitive metrics tied to maintenance outcomes (missed-failure cost vs false-alarm cost). Add calibrated probabilistic outputs and decision thresholds for actionable maintenance policies, and evaluate early-warning lead time and prognostic horizon performance. Extend to time-series models that exploit temporal dependence directly (e.g., sequence models) and include uncertainty quantification for risk-based maintenance planning.",1812.04446v1,https://arxiv.org/pdf/1812.04446v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:31:16Z
FALSE,NA,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Manufacturing (general)|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://archive.ics.uci.edu/ml/index.php,"This paper proposes Resampling Uncertainty Estimation (RUE), a post-training auditing method to score the pointwise reliability of a fixed model’s predictions by approximating how much predictions would change under resampled training data. RUE uses the gradient matrix of per-sample losses and the (dampened) Hessian of the training objective to generate an ensemble of perturbed parameter vectors without retraining, then uses the variance of the resulting predictions as an uncertainty/reliability score. The authors derive RUE as a first-order approximation to the supervised-learning bootstrap via the implicit function theorem, yielding parameter perturbations of the form $\theta^*\approx \hat\theta-\tilde H^{-1}L(w^*-w_0)$. Empirically, on eight UCI regression benchmarks, RUE’s uncertainty scores better detect large pointwise prediction errors than Laplace-based uncertainty, a KDE novelty score, and a first-order “bootstrap SGD” baseline. When converted into Gaussian predictive distributions, RUE attains negative log likelihoods competitive with integrated uncertainty methods (e.g., MC dropout, probabilistic backprop, deep ensembles) while requiring no special training-time procedure.","RUE forms a dampened Hessian $\tilde H=H+\lambda I$ and computes $A=\tilde H^{-1}L$, where $L$ stacks per-example gradients $\nabla_\theta \ell_i(\hat\theta)$. For bootstrap-style resampled weights $w\sim\text{Multinomial}(n,p_0)$, it samples approximate parameters $\theta_i^*=\hat\theta-A(w_i-w_0)$ and computes an uncertainty score as the column-wise variance of ensemble predictions $\widehat{\mathrm{Var}}(\hat y(x))$. Under an exponential-family loss, the paper approximates $\hat\sigma^2_{\mathrm{RUE}}(x)\approx \sum_{i=1}^n r_i^2\,k_{\mathrm{RUE}}(x,x_i)$ with kernel $k_{\mathrm{RUE}}(x_1,x_2)=\big(\nabla_\theta f(x_1,\hat\theta)^\top \tilde H^{-1}\nabla_\theta f(x_2,\hat\theta)\big)^2$.","Across eight UCI regression datasets, RUE achieves higher AUC for detecting incorrect predictions over a sweep of error thresholds than Laplace, KDE-based density scoring, and a first-order bootstrap-SGD ensemble for most datasets/thresholds (power is noted as difficult with all AUCs < 0.56). For predictive negative log-likelihood (NLL), RUE beats Laplace and bootstrap-SGD on 6/8 datasets (exceptions: concrete and kin8nm where bootstrap-SGD is slightly better, and yacht where both RUE and bootstrap-SGD are much worse than integrated methods). RUE’s NLL is competitive with integrated uncertainty methods on some datasets (e.g., housing, power, wine) but generally does not surpass deep ensembles/MC dropout/PBP. Laplace performs substantially worse in NLL across all listed datasets in Table 1 (e.g., housing 4.12 vs RUE 2.54).","The authors note potential scalability concerns: storing the full Hessian requires $O(d^2)$ space, inverting it costs $O(d^3)$, and storing all training gradients costs $O(nd)$. They also note that the Hessian may not be positive definite because convergence to a local minimum is not assumed, so a dampening term is required to ensure invertibility. They limit experiments to relatively small datasets and leave evaluation of gradient-retention strategies and Hessian approximations (e.g., conjugate gradients, K-FAC) for future work.","The method assumes access to per-example gradients and (approximate) Hessian information of the trained objective, which may be infeasible or expensive for many large modern deep networks and pipelines, and can be difficult with nondifferentiable components. The RUE derivation relies on a local first-order Taylor/implicit-function approximation; reliability scores may degrade when parameter perturbations are not small (e.g., highly nonconvex losses, sharp minima, or strong regularization effects). The approach implicitly targets resampling from the empirical training distribution and may not provide meaningful uncertainty under dataset shift/out-of-distribution inputs beyond what is induced by the local geometry at $\hat\theta$. Empirical comparisons are limited to a single network architecture/hyperparameter setting and regression benchmarks; results may not generalize to classification, structured prediction, time series with dependence, or real safety-critical deployments.","They propose exploring scalability improvements, including retaining only a fraction of training gradients/data while maintaining quality of reliability scores, and using approximate Hessian techniques (Hessian-vector products with conjugate gradients, or approximations like K-FAC) instead of explicit storage/inversion. They also suggest defining new kinds of post-training audits by leveraging other reliability principles, drawing inspiration from interpretability methods and sensitivity/influence analyses.","Validate RUE on classification and calibrated confidence/abstention settings (including selective prediction metrics), and on explicit out-of-distribution and dataset-shift benchmarks to test whether the score meaningfully flags shift-induced failures. Develop robust/self-starting variants that work when the Hessian is poorly conditioned or when only stochastic approximations to curvature are available, and study sensitivity to the dampening choice $\lambda$. Provide practical implementations and complexity-reduced approximations (e.g., low-rank/diagonal/K-FAC curvature, subsampled per-example gradients) with accuracy guarantees or error bounds relative to full RUE. Extend the framework to dependent data (time series) and to domain-specific reliability auditing workflows where uncertainty estimates drive operational decisions (e.g., reject/triage policies).",1901.00403v2,https://arxiv.org/pdf/1901.00403v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:31:56Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"This paper proposes “reliable agglomerative clustering,” a modification to standard agglomerative hierarchical clustering that merges all mutually-nearest-neighbor (“reliable”) linkages at each iteration rather than only the single smallest linkage. The approach is applicable across common linkage criteria (single, complete, average, centroid, Ward) and is motivated by improved adaptivity to clusters with varying density and shape and better early-stopping behavior. For the single-linkage case, the paper argues that the procedure yields a minimum spanning tree (MST) construction (with a different edge-selection order than Kruskal/Prim). Empirical evaluation is performed on multiple real-world datasets (UCI datasets, 20 Newsgroups subsets, and proprietary scanned-document datasets) using external clustering metrics (normalized mutual information, normalized Rand score, and V-measure), generally showing improved or comparable performance vs. the standard strategy. No reliability engineering models (failure, degradation, maintenance, life testing) are addressed; “reliable” refers to linkage consistency in clustering.","The core definition is mutual-nearest-neighbor reliability: a linkage between clusters $C_p$ and $C_q$ is “reliable” iff $C_q\in nn(C_p)$ and $C_p\in nn(C_q)$, where $nn(\cdot)$ returns nearest neighbor cluster(s) under the chosen inter-cluster distance $dist(\cdot,\cdot)$. For Ward’s method, the inter-cluster distance is written as $dist(C_p,C_q)=\frac{|C_p||C_q|}{|C_p|+|C_q|}\,\|m_{C_p}-m_{C_q}\|^2$ (difference in cluster means scaled by cluster sizes). Algorithmically, at each level it computes each cluster’s minimum distance to others, forms a graph with edges only for pairs achieving each other’s minima, then merges connected components.","Across a suite of real datasets, the “reliable” strategy often increases external clustering agreement versus the standard agglomerative approach, especially for average-linkage and Ward linkage in several datasets (as reported in Tables for normalized mutual information, normalized Rand score, and V-measure). For example, on Hayes-Roth, normalized mutual information improves from 0.0161 (standard single) to 0.2336 (reliable single) and from 0.1629 (standard average) to 0.2338 (reliable average). On Real III, Ward improves in normalized mutual information from 0.7156 (standard) to 0.8697 (reliable). Reported runtimes are similar between strategies (e.g., COMP single-linkage about 0.91s vs 0.92s; COMP average-linkage about 0.534s vs 0.533s).",None stated.,"The method’s behavior depends on the chosen distance/linkage and on tie-handling when multiple nearest neighbors exist; the paper notes tie-breaking but does not deeply analyze sensitivity or stability under many equal/near-equal distances. Complexity is discussed qualitatively as similar to standard agglomeration, but no detailed complexity bounds or scalability experiments (e.g., very large n, approximate NN search) are provided. The MST claim is given as a proof sketch for single linkage; practical implications of different edge ordering (e.g., partial MST quality under early stopping) are not formally characterized.","The paper notes (in the outlier discussion) postponing details of using the level at which an object first joins a cluster as an outlier probability indicator to future work, and mentions possible parameterization by a ratio $\alpha$ of reliable linkages to select per step as a richer family of strategies.","A self-contained statistical analysis of robustness (noise/outliers), stability under perturbations, and sensitivity to distance ties would strengthen the method’s practical guidance. Extending the approach to large-scale settings via efficient nearest-neighbor data structures/approximate search, and providing open-source implementations, would improve adoption. For the MST connection, formalizing guarantees for early-stopped partial trees/forests and comparing against established MST variants under noisy distances would be valuable.",1901.02063v5,https://arxiv.org/pdf/1901.02063v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:32:22Z
FALSE,NA,ML-based,Other,Not applicable,Other,Other,TRUE,Python|Other,Supplementary material (Journal/Publisher),http://dl.acm.org/citation.cfm?id=1953048.2078195|http://doi.acm.org/10.1145/2939672.2939785,"This paper analyzes pitfalls of applying standard machine-learning workflows to materials informatics problems with severely imbalanced and distributionally skewed target-property data (e.g., OQMD bandgap values concentrated at 0). It proposes a general-purpose pipeline that partitions the target-property space into classes, balances training via sub-sampling, trains class predictors and class-conditional regressors, and then improves regression via transfer learning by augmenting features with marginal property predictions. To improve interpretability, the framework adds a rationale generator that provides decision-level explanations (prototype-based explanations using Gower distance) and model-level explanations (feature importance separated into inter-class and intra-class contributions). For reliability of predictions, it advocates class-specific evaluation rather than global metrics and introduces a “trust score” based on relative distances to samples in the predicted class versus other classes, intended to flag overconfident extrapolations. The framework is demonstrated on predicting multiple OQMD properties and on screening candidate stable solar-cell materials with target bandgaps, highlighting cases where model confidence is high but trust score is low.","The proposed trust score for a test sample $X_i$ is $T(X_i)=1-\frac{d(X_i,\{X_j\}_{j\in c_i})}{d(X_i,\{X_j\}_{j\in c_i})+d(X_i,\{X_j\}_{j\notin c_i})}$, where $d$ is the (average) Gower distance from $X_i$ to samples in the same class $c_i$ versus samples in other classes. The score lies in $[0,1]$, with larger values indicating the sample is closer to known points of its own class than to other classes (more trustworthy).","Global 5-fold CV metrics for conventional gradient-boosting regressors appear strong (e.g., Energy/atom $R^2\approx0.9915$, Volume/atom $R^2\approx0.9861$, Bandgap $R^2\approx0.8865$, Stability $R^2\approx0.8657$), but class-specific evaluation reveals large degradation on minority classes (e.g., Bandgap class-wise $R^2$ includes negative values down to about $-0.13$; Stability class-wise $R^2$ includes values as low as about $-0.86$). Using the partitioned “simpler models” without transfer learning further hurts minority-class performance (e.g., Bandgap class-wise $R^2$ down to about $-0.43$; Stability down to about $-1.5$), illustrating an explainability–accuracy trade-off. Adding the proposed transfer-learning feature augmentation recovers and often improves class-specific performance relative to the conventional approach (e.g., Energy/atom minority-class MAE improves to about $0.07$ in the best-represented class triplet; Stability minority-class MAE improves to about $0.19$ vs $0.23$ conventional). The paper also shows examples where model confidence is ~0.998–0.999 on incorrect minority-class predictions, while trust scores are substantially lower (e.g., 0.27–0.43), suggesting the trust score better flags unreliable extrapolations.","The authors note that when a class is heavily under-represented, “none of the model design strategies will improve the performance” and generating new data may be the only solution (they cite bandgap prediction in minority classes as an example). They also note that verifying ML recommendations via DFT/experiments is challenging due to crystal-structure and synthesis-recipe identification, and that feature-importance explanations still require domain expertise and can suffer from human bias.","Although framed as “reliability,” the work does not address reliability engineering in the sense of lifetime/failure/degradation modeling, maintenance policies, or system reliability; its reliability notion is predictive trustworthiness under dataset shift/imbalance. The trust score depends on having labeled, representative “experimentally known” side information and on distances in a hand-engineered feature space (Gower on heterogeneous descriptors), which may be sensitive to feature scaling/encoding choices and may not generalize across domains. Evaluation is largely via cross-validation and selected case examples; there is limited analysis of statistical significance, calibration quality of predictive intervals, or robustness to autocorrelation/duplicate compositions beyond selecting lowest-energy per composition.",They pose open questions about how many training samples are sufficient for reliable models and where to sample when data are inadequate. They suggest biasing recommendations toward compounds with favorable synthesis conditions to make predictions more practically actionable. They also call for advances in explainable ML to mitigate human bias and mention interactive ML and causal inference as potential directions.,"A natural extension is to quantify and improve probabilistic calibration explicitly (e.g., conformal prediction, calibrated uncertainty estimates) and compare the proposed trust score against standard OOD detection/uncertainty methods. The approach could be tested under more realistic distribution shifts (e.g., new chemistries, different DFT settings) and with ablations that isolate the contributions of sub-sampling, partition thresholds, and transfer-learning augmentation. Providing packaged, reproducible software (e.g., a pip-installable implementation plus documented SI artifacts) and benchmarking against modern imbalance-aware regression methods would strengthen adoption and validation.",1901.02717v2,https://arxiv.org/pdf/1901.02717v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:32:58Z
TRUE,System reliability|Other,ML-based|Simulation-based|Hybrid/Ensemble|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"This paper surveys reliability-based design optimization (RBDO) methods and introduces a new modular “surrogate-assisted” framework that couples adaptive surrogate modeling, reliability analysis, and optimization as three non-intrusive, interchangeable blocks. It emphasizes simulation-based reliability estimation (e.g., crude Monte Carlo, subset simulation) to address limitations of approximation-based reliability methods (FORM/SORM) on highly nonlinear or multimodal limit-state functions. The framework builds a single surrogate of the performance/limit-state function over an augmented space spanning design variables (deterministic and/or random) and environmental random variables, and adaptively enriches the training set via active learning focused near the limit-state surface. The authors demonstrate multiple configurations using Kriging or support vector regression (SVR) surrogates, Monte Carlo or subset simulation for failure probability estimation, and CMA-ES or SQP for optimization. Three benchmark examples (including a finite-element dome structure problem) show that the proposed approach achieves accurate optimal designs with substantially fewer true model evaluations than direct nested simulation approaches, and is competitive with other surrogate-based RBDO methods.","The RBDO problem is posed as minimizing cost $c(d)$ subject to deterministic constraints $f_j(d)\le 0$ and probabilistic constraints $P(g_k(X(d),Z)\le 0)\le \bar P_{f_k}$ (Eq. 1), with failure probability $P_{f_k}(d)=\int_{\{g_k(w)\le 0\}} f_{W|d}(w)\,dw$ (Eq. 2). A quantile-based equivalent formulation is also given: enforce $Q_{\alpha_k}(d;g_k(X(d),Z))\le 0$ where $Q_{\alpha}(\cdot)=\inf\{q:P(g\le q)\ge \alpha\}$ (Eqs. 3–4). The proposed “augmented space” for surrogate training is $\mathcal W=\mathcal X\times\mathcal Z$, with bounds computed via inverse CDFs to form design/environmental confidence regions (Eqs. 5–9), and reliability is then estimated by simulation on the surrogate within each optimization iteration.","On the 2D highly nonlinear benchmark, the median optimal costs are about 1.32–1.33 with median true-model evaluations 47.5 (SVR+MCS+SQP), 52.5 (Kriging+SubsetSim+CMA-ES), and 36.5 (Kriging+quantile MCS+CMA-ES with coupled enrichment), compared to 296 evaluations for a PMA benchmark and ~80 for a prior meta-RBDO reference (Table 2). On the short-column benchmark, the median solutions for the three configurations are close to the reference ($c(d^*)\approx 1.96\times 10^5$) with median true-model evaluations 84, 86, and 57 respectively, and achieved reliability index about $\beta\approx 3.00$ for the reported formulations (Table 4). On the 16D dome structure finite-element example, the framework (configuration #3) reached an optimal weight of 8.68 tons using 154 finite-element runs while meeting a target failure probability $\bar P_f=0.01$ via a displacement-quantile constraint. A validation Monte Carlo check at the final design indicated the surrogate slightly underestimates the quantile but remains within a small bootstrap-based confidence interval around the target threshold.","The authors note that results depend on the convergence of the enrichment (active learning) scheme and that it “requires a proper calibration.” They also caution that coupling optimization with simulation-based reliability estimates introduces stochastic noise; techniques like common random numbers/exterior sampling can reduce noise but may bias the solution unless sample sizes are increased or optimization is repeated. For the dome example, they acknowledge the final Kriging model appears to slightly underestimate the true quantile and that tighter enrichment criteria would improve the fit at the cost of more model evaluations.","The evaluation is largely based on benchmark problems and simulated sampling from specified input distributions; broader validation on diverse real industrial datasets/problems (with model-form uncertainty and non-idealities like correlation or nonstationarity) is limited. The framework’s computational advantages hinge on surrogate fidelity in rare-event regions; for very small target failure probabilities or very high-dimensional augmented spaces, active learning may still require many expensive evaluations and performance can degrade without careful design of acquisition functions. Comparisons are not fully apples-to-apples across all competing RBDO methods because formulations, constraint handling (probability vs. quantile), and stopping criteria can differ, affecting reported evaluation counts. The paper does not provide an openly released implementation or standardized benchmark scripts, which limits reproducibility and makes it harder to confirm robustness across solver/settings choices.","The authors state that the framework is integrated into the UQLab RBDO module to support dissemination of surrogate-based RBDO methods and imply further dissemination and use through that platform. They also indicate that robustness/efficiency can be improved by better calibration of enrichment convergence criteria, suggesting refinement of enrichment strategies as a natural extension. Additionally, they mention that more “tricks” could make the coupled enrichment–optimization procedure more robust and efficient (e.g., a decreasing threshold during optimization), but this is left out of scope.","Develop self-starting/robust variants that explicitly handle unknown distribution parameters, input dependence (copulas), and model-form uncertainty, and assess sensitivity of the optimum to these choices. Extend the modular framework to multi-failure-mode system reliability constraints beyond the series assumption, and to time-dependent reliability/degradation (e.g., lifecycle constraints). Provide open-source reference implementations and benchmark protocols (fixed random seeds, comparable stopping rules) to improve reproducibility and fair comparisons. Incorporate variance-reduced gradient estimators or Bayesian optimization-style acquisition functions tailored to constrained RBDO to reduce enrichment needs in high-dimensional rare-event settings.",1901.03311v1,https://arxiv.org/pdf/1901.03311v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:33:41Z
TRUE,Life distribution modeling|Degradation modeling|Other,Nonparametric/Semi-parametric|Other,Complete lifetime data|Right-censored|Other,Not applicable,Theoretical/simulation only|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is a broad review (1948–2018) of Shannon entropy and many generalizations with emphasis on developments relevant to statistics, reliability, and information science. For reliability and survival analysis, it surveys uncertainty measures for lifetimes including residual (used-item) entropy, past entropy, cumulative residual entropy (CRE), and dynamic versions (e.g., DCRE, CRRE/DCRRE), along with their relationships to hazard/reversed-hazard functions and aging classes (e.g., DURL/IURL and links to DMRL/IMRL). It summarizes characterization and ordering results where entropy-based functions can determine or compare lifetime distributions (often under monotonicity conditions) and reviews divergence measures for used items (dynamic KL divergence, discrimination between past lifetime distributions) including characterizations of proportional hazards/reversed hazards models. On the statistical inference side, it reviews nonparametric estimation of entropy (kernel estimators, spacing-based estimators, k-nearest-neighbor estimators) and entropy/KL-based goodness-of-fit tests (e.g., normality and exponentiality tests), including extensions to censored-data settings. Overall, the contribution is a curated, reliability-oriented map of entropy-related measures, their theoretical properties, and their use in inference for lifetime distributions rather than a new reliability model or maintenance method.","Key reliability-related measures reviewed include: (i) residual (used-item) Shannon entropy $H(X;t)=-\int_t^\infty \frac{f(x)}{\bar F(t)}\log\!\left(\frac{f(x)}{\bar F(t)}\right)dx$ (with links to the hazard rate $\lambda_F$), and its discrete analogue $H^d(X;k)=-\sum_{i\ge k}\frac{p_i}{P(k)}\ln\!\left(\frac{p_i}{P(k)}\right)$. (ii) past entropy $\bar H(t)=-\int_0^t \frac{f(x)}{F(t)}\ln\!\left(\frac{f(x)}{F(t)}\right)dx$. (iii) CRE $\mathrm{CRE}(X)=\int_0^\infty \bar F(x)\log \bar F(x)\,dx$ and dynamic CRE $\mathrm{CRE}(X;t)=\int_t^\infty \bar F_t(x)\log \bar F_t(x)\,dx$ with $\bar F_t(x)=\bar F(t+x)/\bar F(t)$. (iv) dynamic KL divergence between residual lives $I(X,Y;t)=\int_t^\infty \frac{f(x)}{\bar F(t)}\log\!\left(\frac{f(x)/\bar F(t)}{g(x)/\bar G(t)}\right)dx$.","As a review, the paper does not report a single new numeric benchmark; instead it consolidates established results: (1) residual entropy $H(X;t)$ is not always distribution-determining unless additional conditions hold; the review notes corrections leading to the result that an increasing residual entropy (continuous or discrete) uniquely determines the underlying distribution. (2) It records relationships between entropy-based aging classes, including that DMRL (IMRL) implies DURL (IURL). (3) It summarizes known characterizations of lifetime families (e.g., generalized Pareto and Weibull) via residual-entropy/CRE-type functionals and order-statistic versions. (4) It states model-characterization results for divergence measures, e.g., dynamic KL divergence being free of $t$ iff the compared lifetimes satisfy a proportional hazards model (and the analogous past-lifetime discrimination measure being $t$-free iff proportional reversed hazards holds).","The authors explicitly note the scope limitation that it is nearly impossible to survey all entropy literature across all fields; they therefore restrict attention to statistics, reliability, and information science and compile 106 representative articles. They also acknowledge that some relevant literature may have been unintentionally omitted and apologize for such omissions.","Because this is a narrative review, coverage and selection criteria are not fully systematic (e.g., no reproducible search strategy, inclusion/exclusion rules, or quality assessment), so completeness and balance may be uneven. Many reliability-relevant results are reported at a high level without a unifying framework for practitioners (e.g., guidance on which entropy functional to use under censoring, truncation, or dependence), and there is no implemented software artifact to operationalize the surveyed methods. The paper focuses primarily on univariate lifetime settings; modern reliability practice often requires multivariate/competing risks, repairable systems, and dependent/condition-monitoring data, which are not organized into a dedicated reliability-engineering synthesis.","The authors suggest developing postulate systems that would generate the various modified entropies proposed to address limitations of Shannon entropy, and then assessing whether such postulates are practically feasible. They also propose using modified entropies (beyond Shannon’s) for statistical tasks where Shannon entropy has been used—such as goodness-of-fit testing, hypothesis testing, and distribution estimation—anticipating potential improvements in measures like test power.","A useful extension would be a systematic reliability-focused taxonomy that maps each entropy/generalized entropy to data conditions (right/interval censoring, truncation, competing risks) with recommended estimators and uncertainty quantification. Developing open-source software (e.g., R/Python) implementing residual/past/CRE-family measures and KL-type divergences with censoring support would greatly improve practical uptake. Additional work could integrate entropy-based uncertainty measures with condition-monitoring and degradation/RUL pipelines (feature extraction, prognostics calibration), and expand characterization/ordering theory to multicomponent system reliability and dependent lifetimes.",1901.09779v1,https://arxiv.org/pdf/1901.09779v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:34:08Z
TRUE,Degradation modeling|Maintenance optimization|Other,Stochastic process|Simulation-based|Other,Complete lifetime data|Event/count data|Other,Age-based|Condition-based|Block replacement|Not applicable|Other,Energy/utilities|Transportation/logistics|Network/cybersecurity|Other,Simulation study|Other,TRUE,R|None / Not applicable,Not provided,NA,"The paper develops optimal replacement policies for systems subject to cumulative shock damage while the system strength degrades deterministically over time (dynamic stress–strength interference). Shocks arrive according to a renewal process with i.i.d. inter-arrival times, and each shock adds random damage; failure occurs when cumulative damage reaches/exceeds the (time-varying) strength, or when strength degrades to zero. Preventive replacement is considered at a planned time $T$, at shock count $N$, at cumulative damage level $Z$, or under the combined policy “replace at the first of $T$, $N$, $Z$, or failure,” with different costs for each replacement type and higher corrective (failure) cost. Because the expected cost rate expressions involve convolutions, infinite sums, and integrals (and become intractable under general distributions and degrading strength), the authors propose a simulation-based algorithm to estimate expected cost rate and optimize $(T,N,Z)$ via grid search and simulated annealing. The method is illustrated with real case studies (mailbox capacity and cell-phone battery) and generalized to non-i.i.d. and dependent shock damages (including a multivariate gamma construction).","The modeling setup is a cumulative damage shock model with strength degradation: shocks arrive at times $S_j=\sum_{i=1}^j X_i$ (renewal process), damages accumulate as $L_j=\sum_{i=1}^j W_i$, and strength is a deterministic decreasing curve $K(t)$. Failure occurs when $L_j\ge K(S_j)$ (or when $K(t)$ reaches 0 without shocks). Expected cost rates are derived for (i) age-$T$ replacement $C_1(T)$, (ii) shock-count-$N$ replacement $C_2(N)$, (iii) damage-threshold-$Z$ replacement $C_3(Z)$ using a modified threshold $\tilde K(t)=\min\{Z,K(t)\}$ up to $T_0$ where $K(T_0)=Z$, and (iv) the combined policy $C(T,N,Z)$; each equals expected replacement cost divided by mean time to replacement. The simulation estimator is $\hat C(T,N,Z)=\dfrac{c_T\bar I_T+c_Z\bar I_Z+c_N\bar I_N+c_K(1-\bar I_T-\bar I_Z-\bar I_N)}{\bar T_R}$ based on repeated simulated cycles producing time to replacement $T_R$ and indicator of replacement cause.","Across multiple numerical settings (exponential/lognormal inter-arrival times; exponential/weibull/gamma or generalized dependent/non-iid damages; exponential/linear/constant strength degradation), optimal thresholds $(\hat T,\hat N,\hat Z)$ decrease as corrective replacement cost $c_K$ increases. For scenarios where direct computation is possible (both inter-arrival and damage exponential), simulation-based estimates closely match direct/grid-search results, supporting the simulation approach. Joint optimization over $(T,N,Z)$ yields lower minimal expected cost rates than optimizing a single threshold alone (because it allows earlier preventive triggers). In the mailbox case study (strength fixed at 5 MB; Poisson arrivals; lognormal email sizes), the combined-policy optimum for $c_K=2$ and $c_T=c_N=c_Z=1$ is reported as $(\hat T,\hat N,\hat Z)=(708.89,183,3.86)$ with expected cost rate $3.82\times 10^{-3}$. In the cell-phone battery case (Poisson call arrivals; gamma shock damages; exponential strength decay), with $c_K=2$ and $c_T=c_N=1$, the joint optimum over $(T,N)$ is $(\hat T,\hat N)=(73.41\text{ hours},28)$ with expected cost rate $1.458\times 10^{-2}$.","The authors note that analytical optimization is generally infeasible because expected cost rate expressions involve convolutions, integrals, and infinite sums, especially without closure under convolution and with time-dependent strength; hence the reliance on numerical/simulation methods. They also acknowledge that the “expected cost rate” objective is exact as a long-run mean cost only under renewal/regenerative assumptions (e.g., HPP arrivals or shock process restarting with replacement); under non-HPP arrivals it may be interpreted as first-cycle average cost or an approximation. They further state that immediate replacement is assumed; incorporating delayed replacement would require modeling downtime duration and cost and would be theoretically challenging.","The approach assumes a deterministic, monotone non-increasing strength path in the core model; robustness to model misspecification (e.g., wrong degradation curve form) is not quantified, and parameter uncertainty from fitted case studies is not propagated into optimal policies. The simulation optimization (grid search/SA) may be sensitive to chosen search ranges, discretization, and Monte Carlo error; confidence intervals for the estimated cost rates and optima are not reported, which affects decision reliability. Comparisons are mainly internal (direct vs simulated) rather than against modern condition-based/predictive maintenance methods (e.g., Bayesian updating, partially observable Markov decision processes, or RUL-based policies) under comparable assumptions.","The authors suggest extending the framework to mixture damage distributions arising from multiple shock sources, where convolutions lack closed forms but the simulation algorithm can still be applied. They also propose generalizing to time-varying thresholds such as making $Z$ and/or $N$ functions of time for dynamic implementations. Further, they mention more realistic scenarios where initial strength or degradation path is random or stochastic, including non-monotone (auto-repair) strength trajectories, and note simulation as a promising tool. They additionally point to extending the simulation to estimate expected time to corrective replacement and to incorporate delayed replacement with downtime cost and duration modeling.","A valuable extension would be a formal statistical layer: jointly estimating arrival/damage/degradation parameters with uncertainty quantification (e.g., Bayesian posterior) and optimizing expected cost under that uncertainty (robust or Bayesian decision-making). Developing variance-reduced simulation or surrogate modeling (e.g., Gaussian process emulators) could speed up optimization over $(T,N,Z)$ and improve reproducibility of optima. Another direction is to relax i.i.d./independence assumptions by incorporating covariates and dependence between shock arrivals and damages (e.g., marked point processes) and evaluating policy performance under such dependencies. Finally, producing an open-source implementation (e.g., an R/Python package) with standardized inputs for $F$, $G$, and $K(t)$ and returning estimated optima with uncertainty bounds would improve practical adoption.",1901.10399v4,https://arxiv.org/pdf/1901.10399v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:35:01Z
FALSE,Other,Other,Other,Not applicable,Transportation/logistics,Simulation study|Other,TRUE,MATLAB,Not provided,https://www.fastcompany.com/90140902/smart-roads-are-coming-do-we-need-them|https://www.cnet.com/roadshow/news/3m-connected-roads-aim-to-make-life-easier-for-autonomous-vehicles/|http://advancedsourcecode.com/qrcode.asp|http://cvxr.com/cvx,"The paper proposes a game-theoretic adversarial intervention detection mechanism for “smart road signs” that embed error-correcting smart codes (e.g., infrared-readable) for machine perception. It models the interaction between a detector and a worst-case attacker as a zero-sum Stackelberg game, where the detector uses a randomized alerting rule based on the symbol error rate (Hamming distance between the received word and decoder output). To make equilibrium computation scalable, the authors reduce and then relax the attacker’s strategy space using equivalence classes and histogram-based constraints, yielding a conservative formulation solvable via a linear program. Numerical experiments on Reed–Solomon code configurations (various [n,k,d]q) under different symbol error probabilities compare conservative costs with and without detection and report false-alarm rates. Results show substantial reductions in conservative cost (relative to no detection) while keeping false alarms in a practically bounded range for low channel error regimes.","The detector’s randomized rule is a vector $\pi\in[0,1]^{d_0+1}$ where $\pi_j$ is the probability of triggering an alert given $j$ symbol errors, with $d_0=\lfloor(d-1)/2\rfloor$. The zero-sum Stackelberg objective is the expected cost $U(\alpha,\pi)$ (Eq. 4), combining missed-detection cost, decoding error/failure loss $\ell(x_0,y)$ (Eq. 3), false-alarm cost, and an attack effort term proportional to Hamming distance $H(a_0,a_x)$. The noisy reading channel is modeled as $p(y\mid x)=(1-p_e)^{n-H(y,x)}\left(\frac{p_e}{q-1}\right)^{H(y,x)}$ (Eq. 8), enabling reduction to an LP after relaxing the attacker’s space (Eqs. 45–48).","Across multiple Reed–Solomon code settings and channels with symbol error probabilities $p_e\in\{0.01,0.05,0.1,0.2\}$, the paper tabulates baseline decoding error/failure probabilities and highlights regimes where these are below 0.02. It reports large reductions in the conservative cost metric when using the proposed detection mechanism versus no detection (Tables III vs. IV), especially in low-noise regimes. False-alarm probabilities are provided (Table V) and are generally kept below about 10% in scenarios where the underlying code’s decoding error/failure probability is below 0.02. The equilibrium detection policies typically trigger alerts more often at higher observed symbol error counts, and the relaxed worst-case attacker concentrates probability mass on more aggressive perturbations in equilibrium (discussion with Figs. 5–6).","The authors note that they relax constraints on the attacker’s action space to address scalability, which yields a conservative (more powerful-than-realistic) attacker model. They explicitly state that tighter approximations may be possible by imposing tighter necessary conditions on the attacker’s actions, but this would increase computational complexity.","The work focuses on symbol-independent, symmetric noise and uses Hamming/symbol error counts; real smart-code reading errors may be spatially correlated or structured, which could change optimal policies. The evaluation is simulation-based on assumed channel models and does not include deployment or real sensor/field datasets, so practical calibration of $p_e$ and cost weights $\gamma_i$ remains uncertain. The approach centers on single-codeword decisions (per sign encounter) and does not fully model temporal correlation, repeated encounters, or system-level decision fusion beyond brief discussion.","The authors propose integrating smart codes with vision-based road-sign recognition via sensor fusion to improve resilience. They suggest leveraging a network of smart vehicles to share false-alarm costs and improve collective reliability (analogous to herd immunity). They also propose extending the framework to other signaling/classification applications that can embed visual smart codes, such as warehouse inventory management and robotic sorting.","Validate the detection framework on real-world smart-code imaging datasets under varied weather/lighting and structured occlusions to test robustness beyond the i.i.d. symmetric channel model. Extend the model to incorporate correlated/structured noise, adaptive/online estimation of channel parameters, and sequential decision-making across repeated sign observations. Provide an open-source reference implementation (e.g., CVX/MATLAB scripts) and sensitivity analyses for choosing the cost weights $\gamma_i$ and for comparing against non-game-theoretic detectors (fixed thresholds, Bayesian detectors) under matched assumptions.",1901.10622v2,https://arxiv.org/pdf/1901.10622v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:35:36Z
TRUE,RUL prediction|Maintenance optimization|Failure mode analysis|Degradation modeling|Other,ML-based|Nonparametric/Semi-parametric|Simulation-based|Other,Sensor/condition monitoring|Event/count data|Mixture of types,Condition-based|Predictive,Energy/utilities,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes a data-driven predictive maintenance approach for photovoltaic (PV) plants using SCADA data to predict inverter faults at two levels: (i) generic abnormal status/fault prediction and (ii) specific fault-class prediction. The generic module (SDM) uses an unsupervised 20×20 Self-Organizing Map trained on a normal-operation period; it detects distributional changes in mapped patterns via a KPI and triggers multi-level warnings based on control-limit and persistence rules, effectively acting as a multivariate SPC-style monitoring chart. The specific module (FPM) uses a pattern-recognition feed-forward neural network trained on labeled SCADA data and fault logbooks to predict specific fault classes at various lead times. The approach is validated on six PV plants (up to 10 MW) and over 100 inverters across three inverter technologies/brands (2014–2016), showing generic fault anticipation up to 7 days and specific fault-class anticipation ranging from hours to 7 days depending on fault frequency. The work advances PV predictive maintenance practice by combining unsupervised anomaly detection with supervised fault classification, designed to be portable across plants with limited required inputs (historical SCADA + fault taxonomy + inverter datasheet).","Fault-logbook labeling assigns fault k to SCADA timestamp $t_n$ if $t_{start,k} \le t_n \le t_{end,k}$. The SDM defines a SOM-occupancy-based KPI: $\mathrm{KPI}=\sum_{i,j} P^{\mathrm{TEST}}_{ij}\,\frac{1-|P^{\mathrm{TRAIN}}_{ij}-P^{\mathrm{TEST}}_{ij}|}{1+|P^{\mathrm{TRAIN}}_{ij}-P^{\mathrm{TEST}}_{ij}|}$, where $P_{ij}$ are normalized cell occupancies over training vs. a 24h test window; KPI=1 indicates matching distributions and KPI→0 indicates large deviations. Lost production is estimated as $LP(t)=\int_{t_0}^{t}[P_{th}(t')-P_{AC}(t')]\,dt'$, with $P_{th}$ derived from nominal power adjusted by irradiance and temperature effects.","For the SDM (generic prediction), the method anticipates high-frequency inverter failures up to ~7 days in advance with sensitivity reported up to 95% and specificity ~80% (overall accuracy often around ~80–85% in reported examples). In one plant-level overview (Plant #1), mean sensitivity is reported as ~72% at 3 days and ~61% at 7 days ahead, while average accuracy and specificity are ~80% for both horizons. For a specific inverter example, sensitivity is described as degrading from ~93% (near-term) to ~84% at 7 days ahead with overall accuracy ~85%. The predicted-maintenance service is estimated to improve energy yield by ~10–15% in one illustrated case, and ~6–7% in another plant example. For the FPM (fault-class prediction), performance depends strongly on fault-instance counts: with thousands of occurrences, sensitivity remains ~50–60% even 7 days ahead; with <100 instances, sensitivity often becomes satisfactory only over short horizons (e.g., ~2 hours) though some classes show multi-day predictability when correlations are strong.","The authors note that some fault classes cannot be predicted due to their instantaneous nature; for these, the SDM is positioned as an early-detection tool rather than a long-horizon predictor. They also indicate that FPM sensitivity can degrade quickly for low-frequency failures because of poor fault statistics (limited number of fault instances).","The approach relies on historical fault taxonomy/logbook quality and consistent SCADA-tag behavior; label noise, missing/incorrect fault timestamps, or changes in alarm coding could materially affect training and evaluation. The SDM uses a plant-specific “normal period” selection and thresholding (e.g., 3σ/5σ rules and persistence), which may require expert tuning and may not transfer without re-calibration; robustness to concept drift beyond de-trending is not fully established. Evaluation is presented as offline classification with lead-time windows; operational impacts such as false-alarm workload, maintenance scheduling constraints, and end-to-end cost/benefit under realistic response times are not rigorously optimized. The NN classifier setup appears to be mostly binary (fault vs normal per class/setting) with imputation and sampling strategies; comparisons to strong baselines (e.g., gradient-boosted trees, calibrated probabilistic models, modern deep sequence models) and uncertainty quantification are not demonstrated.","The authors propose adding a deep-learning-based automated fault-detection system using drone-based thermal images of PV modules and integrating the predictive model into a smart solar monitoring software that includes intervention management, alarm handling, and business-intelligence reporting from intervention to portfolio level.","A natural extension is a unified probabilistic framework that outputs calibrated fault probabilities and expected time-to-failure (or expected loss) to support risk-based maintenance decisions and explicit cost-sensitive thresholds. More robust, self-starting/adaptive versions could address nonstationarity (seasonality, inverter aging, firmware changes) via online learning, drift detection, or periodic re-training with safeguards. Broader benchmarking on public PV datasets and more comprehensive baselines (including temporal models such as TCN/LSTM/Transformers and classical multivariate SPC methods) would clarify generality. Finally, integrating maintenance-response constraints (crew availability, spare parts, travel time) into an optimization layer could turn predictions into executable maintenance schedules with measurable reliability and economic outcomes.",1901.10855v1,https://arxiv.org/pdf/1901.10855v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:36:19Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies whether Twitter-based “polls” of political trends are reliable by modeling tweet mentions (and in one case sentiment) for elections with q candidates. Using a maximum-entropy argument, the authors show that matching observed mean trends and pairwise correlations leads formally to a generalized q-state Potts model whose parameters are obtained via an inverse problem. Because only aggregated time-series of candidate mention shares are available, they assume approximate stationarity and estimate macroscopic correlations from the time series, finding they scale as O(1/N_eff) with an effective message count N_eff that is much smaller than the raw tweet volume due to replication effects. They compare two simple effective models: a multinomial distribution (no microscopic interactions; correlations arise only from the sum-to-one constraint) versus a mean-field Potts model (one coupling J/N_eff; genuine interactions). Across several elections, they find the inferred mean-field coupling is typically near the model’s critical value; they argue Twitter “polls” are only reliable in cases best described by the multinomial model, while mean-field Potts (near-critical) cases can exhibit large fluctuations or abrupt jumps that undermine prediction.","The maximum-entropy solution yields a Gibbs distribution $Q(\sigma)=e^{-H(\sigma)}/Z$ with a generalized Potts Hamiltonian, e.g. in the homogeneous case $H(\sigma)=-\sum_{i<j}\sum_{\sigma,\sigma'}J_{\sigma,\sigma'}\,\delta(\sigma_i,\sigma)\delta(\sigma_j,\sigma')-\sum_i\sum_\sigma h_\sigma\,\delta(\sigma_i,\sigma)$. The multinomial (uncorrelated) benchmark gives connected correlations $\operatorname{Cov}(x_\sigma,x_{\sigma'})=\frac{1}{N}(x_\sigma\delta_{\sigma\sigma'}-x_\sigma x_{\sigma'})$. The mean-field Potts model uses $H=-(J/N)\sum_{i<j}\delta(\sigma_i,\sigma_j)-\sum_{\sigma}h_\sigma\sum_i\delta(\sigma,\sigma_i)$ with saddle-point equations $x_\sigma=\frac{e^{Jx_\sigma+h_\sigma}}{\sum_{\sigma'}e^{Jx_{\sigma'}+h_{\sigma'}}}$ and a critical coupling (for $q\ge3$) $J_c=\frac{2(q-1)}{q-2}\log(q-1)$.","Empirically, the measured macroscopic connected correlations from Twitter trend time series decay approximately as $1/N_{\mathrm{eff}}$, where $N_{\mathrm{eff}}$ is inferred to be a small fraction of the mean tweet count $N$, implying substantial replication (an overall replication factor $N/N_{\mathrm{eff}}$ on the order of $10$–$10^2$). When fitting the mean-field Potts model, the inferred coupling $J$ is reported to lie close to the critical value $J_c$ across the electoral events studied. For Brazil 2014 (q=11), correlation-structure comparisons favor the multinomial model (deemed “poll reliable”) despite a near-critical fitted Potts coupling. For the UK 2010 election (q=3), correlation functions rule out the multinomial model and support the mean-field Potts description near criticality (deemed “poll unreliable” due to potential jumps). For the India 2014 case augmented with sentiment (treated as an additional state), they again conclude the multinomial model is more appropriate and note strong time-variation in Potts coupling estimates over a long observation window.","The authors note that they do not have access to instantaneous, single-tweet (microscopic) activity data or direct instantaneous correlations, and thus cannot solve the fully general inverse problem for the generalized Potts model. They rely on an approximate stationarity assumption to estimate macroscopic correlations from aggregated time-series, and they acknowledge that richer generalized Potts models would require costly Twitter data (released on demand) and heavy computation. They also point out that when the observation window is long and external news influences preferences, the stationarity assumption (and thus correlation estimation via time averages) is hard to justify.","The stationarity assumption is central but is not stress-tested (e.g., change-point tests, autocorrelation/seasonality controls, or sensitivity to window selection), so model classification (MD vs MFP) may be brittle. The mapping from inferred near-critical mean-field coupling to “unreliable polls” is suggestive but not validated against systematic out-of-sample forecasting performance or calibrated prediction intervals. The effective sample size concept $N_{\mathrm{eff}}$ is inferred from correlation scaling but is not linked to an identifiable generative model of retweets/bots/user-level dependence, which may confound interpretation. The work is about social/physics modeling rather than reliability engineering, and it does not address engineering notions such as failure mechanisms, lifetime distributions, or maintenance decisions.","They suggest investigating richer generalized Potts models that interpolate between the multinomial and mean-field Potts limits and that may incorporate features of the underlying Twitter network (e.g., scale-free structure). They also state that the empirical observation that inferred couplings sit near criticality across elections “deserves further investigations.” They note that pursuing these directions would require access to costly, single-tweet instantaneous data and substantial computational analysis.","A natural extension would be to formalize and automate the MD-vs-MFP model selection using likelihood-based criteria or Bayesian model comparison, coupled with uncertainty quantification for $J$, $N_{\mathrm{eff}}$, and correlation estimates. Another direction is to relax independence assumptions in time by explicitly modeling temporal dependence/nonstationarity (state-space models, Hawkes processes, or regime-switching Potts/MD variants) and testing robustness across window lengths. Validating the “reliability” claim would benefit from explicit forecast experiments (pre-election holdout, rolling-origin evaluation) comparing predicted outcomes and uncertainty bands under MD vs MFP. Finally, connecting $N_{\mathrm{eff}}$ to measurable platform mechanisms (retweets, quote tweets, bot detection, user-level deduplication) could make the effective-size interpretation operational.",1901.10984v1,https://arxiv.org/pdf/1901.10984v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:37:07Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Mixture of types|Other,Not applicable,Environmental monitoring|Healthcare/medical|Other,Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.1016/j.yrtph.2018.12.013,"This paper critiques the statistical reliability of a published meta-analysis (Nawrot et al., 2011) relating particulate matter (PM10/PM2.5) exposure to myocardial infarction by examining the reliability of the 14 underlying observational base studies. The authors quantify each base study’s “analysis search space” by counting outcomes, predictors, lags, and covariates, and approximating total model/specification multiplicity as Space_3 = (Outcomes×Predictors×Lags)×2^{Covariates}. They then assess heterogeneity and potential selective reporting/p-hacking using a p-value plot of p-values derived from reported confidence intervals in the meta-analysis table, arguing that a mixture of very small p-values and an approximately uniform remainder is consistent with manipulation rather than a stable effect. Reported search spaces are large (median Space_3 ≈ 6,784; IQR 2,600–94,208; max 638,976), and the paper argues that unadjusted multiple testing/multiple modeling implies biased base-study effect estimates, undermining meta-analysis validity. The authors conclude that both the base studies and the resulting meta-analysis are unreliable and that causality of PM10/PM2.5 on heart attacks is not supported by this evidence base.",The analysis search space is defined via counts from each base paper: $\text{Space}_1 = (\#\text{Outcomes})\times(\#\text{Predictors})\times(\#\text{Lags})$. Modeling multiplicity from covariate inclusion/exclusion is approximated as $\text{Space}_2 = 2^{(\#\text{Covariates})}$. The total analysis search space is then $\text{Space}_3 = \text{Space}_1\times\text{Space}_2$. P-values for base-study risk ratios are computed from reported 95% confidence intervals and assessed using a ranked p-value plot (expected ~45° line under the global null).,"Across the 14 base studies, the estimated total analysis search space $\text{Space}_3$ has median 6,784 with interquartile range 2,600 to 94,208; the maximum reported is 638,976 and the minimum is 448 (from Table 2). The p-value plot of 14 p-values shows heterogeneity: several very small p-values alongside others that align roughly with a 45-degree line consistent with no effect, which the authors interpret as a “hockey-stick” pattern suggestive of selective reporting/p-hacking. The paper notes that two studies in the meta-analysis were described as not significant but did not report p-values and thus were not plotted. As external corroboration for a null effect, the authors cite Milojevic et al. (2014) reporting PM2.5 RR = 1.003 with p = 0.36 for heart attacks, and they cite Young et al. (2017) finding no PM2.5 effect on acute deaths in California.",None stated.,"The work is a methodological critique rather than a reliability-engineering study; its conclusions depend on approximating multiplicity via simple counts and $2^{k}$ covariate combinations, which may overstate or understate the true model/search process (e.g., correlated predictors, restricted model forms, or pre-specified protocols). The p-value-plot diagnostic is suggestive but not definitive for p-hacking and can be confounded by true heterogeneity, differing exposure metrics, varying lag structures, or measurement error across studies. The paper does not re-run the original analyses or reconstruct all alternative models, so it cannot quantify how much reported effect sizes would shrink under specific multiplicity adjustments. In addition, drawing causal conclusions about PM effects from a critique of one meta-analysis may not generalize to other study designs (e.g., better-controlled cohorts, quasi-experimental approaches).",None stated.,"Reanalyze the 14 base datasets (where available) under a pre-registered analysis plan to quantify effect-size sensitivity to model/specification choices and to apply principled multiplicity control (e.g., hierarchical models, selective-inference approaches, or FDR control). Extend the diagnostic beyond p-value plots by combining selection models/publication-bias models with heterogeneity modeling (random effects with sensitivity analyses). Develop standardized reporting/checklists for observational-study meta-analyses that explicitly require disclosure of analysis search space, all tested outcomes/exposures/lags, and robustness to alternative specifications. Provide reproducible code and data pipelines to enable independent verification and to benchmark how different bias-correction methods affect pooled estimates.",1902.00770v1,https://arxiv.org/pdf/1902.00770v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:37:39Z
FALSE,Other,ML-based|Other,Sensor/condition monitoring|Mixture of types,Not applicable,Energy/utilities,Simulation study|Other,TRUE,Python,Not provided,http://ec.europa.eu/research/horizonprize/index.cfm?prize=bigdata|http://www.elia.be/|https://transparency.entsoe.eu|https://www.bpa.gov,"The paper describes a data-driven short-term forecasting method used to win the EU Big Data Technologies Horizon Prize for predicting electricity grid line flows one hour ahead at 5-minute resolution. The approach identifies recurrent daily and weekly fluctuation patterns and forecasts by summing expected stepwise recurrent increments, using exponential smoothing to adapt online. It is designed for robust performance under real-world data issues (missing values, NaNs, outliers, spikes, discontinuities, zero-runs), with safeguards that default to persistence forecasting when the model’s recent error exceeds persistence error. The author reports that naive multivariate regression over the whole grid overfit and underperformed persistence unless strongly regularized (Lasso), while univariate use of multiple past days captured most gains; adding environmental covariates did not improve accuracy. The implemented software in Python/NumPy emphasizes efficiency (limited state storage) and reliability via validity indicators, extrapolation for missing terminal values, and reinitialization on unexpected computation problems.","Forecasts use recurrent increments: $\hat y_{t+i}=y_t+\sum_{h=1}^{i}\langle\Delta y_{t+h}\rangle$, where $\langle\Delta y_t\rangle$ averages historical increments at the same time-of-day using a validity indicator for missing/outlier increments. Daily and weekly recurrent increments are updated by exponentially weighted recursion: $D_t=r_d I_{t,t-1}(y_t-y_{t-1})+(1-r_d I_{t,t-1})D_{t-\tau_d}$ and $W_t=r_w I_{t,t-1}(y_t-y_{t-1})+(1-r_w I_{t,t-1})W_{t-\tau_w}$; forecasts blend them with an adaptive weight $g_t$ chosen to minimize a moving squared error. A correction term regresses fluctuations toward a moving mean: $\hat y^{(3)}_{t+i}=\hat y^{(2)}_{t+i}+o_t(m_t-y_t)i$, and the final output applies saturation damping: $\hat y_{t+i}=y_t+\frac{\hat y^{(3)}_{t+i}-y_t}{1+\frac{1}{K}|\hat y^{(3)}_{t+i}-y_t|}$.","On the illustrative synthetic example, the recurrent-fluctuations method substantially reduces RMSE versus persistence: for 1-hour-ahead forecasts, RMSE drops from 30.3 (persistence) to 12.8 (recurrent fluctuations). For 3-hour-ahead forecasts, RMSE drops from 58.4 (persistence) to 22.8 (recurrent fluctuations). In development-phase analyses on the prize dataset, multivariate regression without regularization performed worse than persistence (overfitting), while Lasso regularization slightly improved on persistence; adding one-day-lagged changes provided a substantial additional MSE reduction. Gradient boosting offered only marginal gains over linear regression, and incorporating environmental variables (temperature, wind speed, radiation) did not improve performance.","The paper notes that the competition dataset cannot be reported in detail due to restrictions, limiting reproducibility and external verification of results. It also highlights that grid lines were anonymized and the physical grid structure was unknown, preventing the use of a physical model or explicit network connectivity in forecasting. The author further implies that complexity (e.g., full multivariate coupled models) can harm generalization and reliability if not managed, motivating simpler univariate methods.","The method is evaluated with limited quantitative benchmarking beyond a few reported RMSE values on a synthetic illustrative series; broader comparisons on multiple real datasets, horizons, and seasonal regimes are not provided. Several algorithm parameters (e.g., smoothing/persistence factors, thresholds for outliers/spikes/zeros, saturation limit $K$) appear to require tuning, but systematic sensitivity/robustness analysis and principled selection guidance are not detailed. The approach is tailored to strong daily/weekly recurrence; performance may degrade for nonstationary regimes without clear periodicity (e.g., major outages, topology changes, market shocks), and the fallback-to-persistence safeguard may mask such failures rather than correcting them.","The author suggests incorporating the structure of the grid into a predictive model that uses sufficient historical data from multiple lines to predict a given line, while doing so autonomously and adaptively on undisclosed data with changing properties. The paper also suggests that a potential improvement is to incorporate reliable forecasts of environmental variables to add information about the future not already encoded in grid flow data.","Validate the approach on multiple publicly available transmission flow datasets (e.g., ENTSO-E/Elia/BPA) with standardized train/test protocols and report accuracy-speed tradeoffs against strong baselines (seasonal naïve, SARIMA, state-space, gradient boosting, deep sequence models). Develop an automated parameter-setting or self-starting procedure (e.g., online Bayesian/robust adaptive tuning) for smoothing rates and anomaly thresholds, with documented sensitivity and stability guarantees. Extend the method to exploit spatial/network structure via graph-based features or graph neural/state-space models while preserving the paper’s reliability goals (graceful degradation under missing/outlier data and bounded computational cost).",1902.04337v1,https://arxiv.org/pdf/1902.04337v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:38:17Z
FALSE,NA,ML-based|Nonparametric/Semi-parametric|Other,Sensor/condition monitoring,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes an automated version of Prechtl’s General Movement Assessment (GMA) for screening infants at risk of perinatal stroke, using four limb-worn tri-axial accelerometers and a machine-learning pipeline designed for trial-wise (weak) labels. The core contribution is a Discriminative Pattern Discovery (DPD) framework that classifies 1-second windows into three latent categories—abnormal (wAM), typical-developing-specific (wTD), and neutral (wNM)—and suppresses neutral windows before aggregating evidence to decide a trial label (AM vs TD). DPD computes a per-window increment score via log-odds between positive and negative “bags,” estimating class-conditional densities with a kNN-based kernel density estimator to improve generalisation on small datasets. Evaluation on 161 validated trials from 34 infants (21 typically developing, 13 perinatal stroke) using cross-validation shows the proposed method achieves about 0.80 accuracy, exceeding the 0.75 pass threshold used for newly trained human GMA annotators, and improving specificity/false-positive rate over key baselines. The work is positioned as an enabling step toward scalable, objective, population-wide screening using low-cost wearable sensors.","The trial label is predicted by aggregating window-level evidence: \(\hat{y} = \arg\max_{y\in\{0,1\}} \prod_{i=1}^{\hat{n}} P(y\mid \hat{x}_i)\). A window-level increment (log-odds) is defined as \(\delta_i = \log\frac{P(B^+\mid \hat{x}_i)}{P(B^-\mid \hat{x}_i)}\), and windows are assigned to wAM/wTD/wNM via threshold \(\pi\): wAM if \(\delta_i>\pi\), wTD if \(\delta_i<-\pi\), otherwise wNM. Trial-level classification averages masked increments and thresholds by \(\lambda\): predict AM if \(\frac{1}{\sum \hat{h}_i}\sum \hat{h}_i\delta_i > \lambda\). The densities \(P(\hat{x}_i\mid B)\) are estimated nonparametrically using kNN Parzen/Gaussian kernel density estimation over the top-k neighbors in each bag, yielding an increment of the form shown in Eq. (7).","On 10-fold cross-validation, the proposed DPD method achieved accuracy 0.80 ± 0.13, sensitivity (recall) 0.70 ± 0.35, false positive rate 0.13 ± 0.11, specificity 0.87 ± 0.11, and precision 0.57 ± 0.27. The ablation “Ours (No-DPD)” reached accuracy 0.70 ± 0.10 with higher recall (0.88 ± 0.16) but substantially worse false positive rate (0.32 ± 0.13) and specificity (0.68 ± 0.13), supporting the benefit of suppressing neutral windows via \(\pi\). Baselines performed poorly or degenerated: KNN had accuracy 0.22 ± 0.12 (predicting all trials as AM), and SVM had accuracy 0.79 ± 0.11 but recall 0.00 (predicting all trials as TD). The clustering-based GMI-GEN baseline had accuracy 0.32 ± 0.17 with very high false positive rate 0.80 ± 0.23, suggesting quantisation can lose discriminative information.","The authors describe the work as “explorative” and note that the dataset is “still small-scale” in machine-learning terms, which pressures generalisation and motivates their kernel generalisation strategy. They also note practical deployment limitations, including that the sensor synchronisation procedure requires effort and may be difficult to integrate into home routines. They further point out that sensing only on limbs may be insufficient as GMA considers head/posture/whole-body synchrony, and that sensor/band form-factor may need adaptation as infants grow and become more mobile and interactive with the straps.","The study design appears to mix multiple trials per infant across folds under random trial-wise splitting; without participant-wise separation, reported accuracy may be optimistic due to within-subject correlation and potential leakage of subject-specific motion signatures. The approach relies on fixed windowing (1 s, no overlap) and PCA features; sensitivity to preprocessing choices (filtering, sampling rate changes, alternative feature learning) is not fully characterised. Clinical utility would require calibration/decision-threshold selection under realistic prevalence and cost asymmetries, plus prospective validation across sites and sensor placement variability; this is not established here. Interpretability of detected “wAM” windows is only qualitatively shown and not validated against expert temporal annotations of abnormal movement types.","The authors propose moving toward population-wide screening with more flexible, parent-at-home recording and more frequent/continuous monitoring rather than monthly sessions. They suggest leveraging increasing volumes of data to build more robust models and potentially adapt models over time using newly recorded data and clinician-confirmed predictions. They also suggest stratifying or personalising models using readily available meta-information (e.g., age/weight/gender) once case numbers are larger, and extending sensing beyond limbs (e.g., trunk) using patch-like disposable accelerometers to better capture posture/head/whole-body aspects of GMA.","A key next step would be participant-independent (leave-one-infant-out) and multi-site external validation to assess robustness to subject, clinic, and operator variability, including sensor orientation/placement errors. Developing a clinically aligned evaluation that reports calibrated risk scores, decision curves, and false-alarm burden at target sensitivity levels would better support screening deployment. Incorporating temporal structure (e.g., HMM/CRF or sequence models) while still handling weak labels could improve consistency of window-level detections and reduce false positives. Releasing code and a de-identified feature dataset (or reproducible pipeline) would materially improve reproducibility and accelerate benchmarking against newer weakly supervised representation learning methods.",1902.08068v1,https://arxiv.org/pdf/1902.08068v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:39:05Z
FALSE,NA,Other,Other,Not applicable,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is about which directed graph relations can be embedded in Euclidean space using embedding families motivated by knowledge graph methods (translational, distance/structured, and similarity/bilinear embeddings). It characterizes limits of translational embeddings (e.g., directed cycles are not translationally embeddable, while all DAGs are) and shows that all directed graphs admit distance and similarity embeddings. The authors introduce robustness (a margin parameter) as a central notion, connecting it to required numerical precision and to achievable low-dimensional embeddings (via Johnson–Lindenstrauss and embeddings into the Boolean hypercube). They relate minimum embedding dimension to sign rank of the adjacency matrix and show random dense directed graphs typically require large dimension and have poor robustness. Algorithmically, they show minimizing embedding dimension is NP-hard, while maximizing robustness can be formulated as a semidefinite program.","Translational embedding: (u,v) in E iff ||phi(v) - (phi(u)+z)|| <= t_u (uniform if all t_u equal). Distance embedding: (u,v) in E iff ||phi_out(u) - phi_in(v)|| <= t, with delta-robustness defined by non-edges satisfying ||phi_out(u)-phi_in(v)||^2 >= t^2(1+delta). Similarity embedding: (u,v) in E iff phi_L(u)·phi_R(v) >= t, with an additive delta-robustness margin for non-edges.","Directed cycles C_n (n>=3) do not admit translational embeddings, while any directed acyclic graph admits a uniform translational embedding (constructed in dimension d+1 using a sphericity-style embedding of the underlying undirected graph). For any directed graph with adjacency matrix rank k and top singular value sigma_1, there exists a (1/sigma_1)-robust distance/similarity embedding into the unit sphere in R^k (via SVD). Any delta-robust embedding implies an embedding in O((1/delta^2) log n) dimensions (Johnson–Lindenstrauss), and robust similarity embeddings imply embeddings into the Boolean hypercube {0,1}^k with k=O((1/delta^2) log n). Random dense directed graphs have high probability lower bounds on required dimension on the order of n H(p)/log n (via counting sign-rank matrices), implying poor robustness.",None stated.,"The work studies robustness primarily through worst-case margin-style definitions; it does not empirically validate whether optimizing these margins correlates with downstream link-prediction performance on real knowledge graph benchmarks. The robustness-to-precision link is established through existence results (e.g., hypercube embeddings) but practical constants and resulting embedding quality may be unfavorable for moderate n. The translational embedding model includes node-specific thresholds and excludes self-edges; these modeling choices may limit direct comparability to commonly used TransE-style scoring and training objectives. The SDP formulations optimize robustness but the paper does not provide runtime/scalability analysis for large graphs where n is large and SDP solvers become impractical.","The paper poses open problems including: characterizing the minimum dimension needed for translational embeddings of DAGs, and determining whether existence of a low-dimensional Euclidean embedding implies existence of a robust embedding (i.e., whether low dimension guarantees good robustness).","Develop scalable (non-SDP) optimization methods or approximations for maximizing robustness on large knowledge graphs, with theoretical guarantees. Extend the robustness framework to settings with noisy/incomplete edges (probabilistic relations) and to common training losses used in practice (margin ranking, negative sampling) to connect theory with learning behavior. Analyze robustness under additional realistic assumptions such as correlated entities/edges or constraints on embedding norms, and extend results to hyperbolic/complex embeddings to compare representational power and required precision. Provide benchmarked implementations and empirical studies comparing robustness-optimized embeddings with standard KG embedding methods across tasks.",1903.05347v3,https://arxiv.org/pdf/1903.05347v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:39:26Z
TRUE,Degradation modeling|Maintenance optimization|System reliability|Other,Nonparametric/Semi-parametric|Stochastic process|ML-based|Hybrid/Ensemble,Right-censored|Event/count data|Sensor/condition monitoring|Mixture of types,Not applicable,Energy/utilities,Simulation study|Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/dnncode/RF-R,"The paper proposes RF-R, a Random-Forests-based framework for analyzing large, heterogeneous repairable-system recurrence (failure/repair) data with both static system attributes and dynamic sensor measurements in a Big Data/distributed storage setting. For static covariates, each tree partitions the covariate space and estimates node-specific reliability via the nonparametric Mean Cumulative Function (MCF), with splitting rules based on maximizing differences between daughter-node MCFs (log-rank-type statistic or an L2 distance). For dynamic covariates, the method estimates node-specific NHPP intensity functions using a log-linear model with time-varying covariates and L1 (lasso) regularization, and uses an L2 distance between estimated intensity functions for splitting. The authors provide large-sample variance expressions and establish uniform consistency (under discretized covariate space assumptions), and they use out-of-bag (OOB) evaluation with Harrell’s C-index for predictive performance. Simulation studies and an oil-and-gas well case study (8232 wells) show improved predictive ranking performance and robustness to redundant covariates compared with classical MCF approaches and parametric NHPP/frailty models.","For the static-covariate version, the tree-specific MCF is $\widehat{\text{MCF}}^{(b)}(t;x)=\sum_{j=1}^M \widehat{\text{MCF}}_{h^{(b)}_j}(t)\,\mathbf{1}\{x\in h^{(b)}_j\}$ and the forest ensemble is the average over trees $\widehat{\text{MCF}}^{(*)}(t;x)=\frac{1}{B}\sum_{b=1}^B \widehat{\text{MCF}}^{(b)}(t;x)$. For distributed data, local worker-node MCFs are computed as step functions $\widehat{\text{mcf}}^{(w)}(t_j)=\sum_{k\le j} d_{\cdot}^{(w)}(t_k)/\delta_{\cdot}^{(w)}(t_k)$ and merged by averaging across workers. With dynamic covariates, node intensities are modeled by an NHPP with $\log \lambda_{\tilde h}(t)=\beta^{(\tilde h)}_0+\sum_{j=1}^q z_j(t)\beta^{(\tilde h)}_j$ estimated via lasso-penalized negative log-likelihood, and splits maximize an $L_2$ distance between daughter-node intensity functions.","Across repeated train/test splits in simulations (Datasets A–D), RF-R achieves higher prediction C-index than (i) a global nonparametric MCF, (ii) a k-nearest-neighbor MCF, and (iii) parametric HPP/NHPP models, especially when many covariates are redundant. In the oil-and-gas well case study (8232 wells, 8 static covariates and 7 dynamic sensor signals), the OOB C-index stabilizes after roughly 50 trees, and permutation importance identifies geolocation covariates (latitude/longitude) as the most influential static attributes. The extended method’s lasso fits induce sparsity at terminal nodes, indicating different sensor variables dominate failure intensity in different geographic subregions. Overall, the comparisons show parametric NHPP and gamma frailty models degrade notably with redundant covariates, while RF-R remains comparatively robust.",None stated.,"Theoretical guarantees rely on discretizing covariates and assuming a finite (discrete) covariate space, which may limit direct applicability to truly continuous, high-dimensional covariates without careful binning and sensitivity checks. The approach assumes conditional independence of systems given covariates when merging distributed estimates; unmodeled cross-system dependence (e.g., shared environment/maintenance logistics) could bias uncertainty estimates and splitting decisions. Dynamic covariates are incorporated only through node-wise parametric NHPP models with a log-linear form; complex time-lag/cumulative exposure structures and autocorrelation in sensors may not be well captured without additional feature engineering. Practical guidance for tuning (e.g., lasso penalty $\omega$, minimum failures per leaf $d_0$, and split-distance choice) is limited and could materially affect performance in field deployments.",None stated.,"Extend RF-R to explicitly handle within-system dependence structures beyond NHPP (e.g., trend-renewal or bounded-intensity processes) and to accommodate serial dependence/functional features of sensor time series (lags, cumulative damage, embeddings). Develop robust/self-starting variants that better handle unknown/distributionally-shifted censoring mechanisms, missing sensor streams, and informative censoring. Provide scalable, open-source implementations for common distributed platforms (e.g., Spark) with documented tuning heuristics and uncertainty quantification (e.g., calibrated intervals for MCF/intensity). Explore multivariate extensions for fleets with multiple failure modes and shared maintenance actions, including hierarchical or spatial random effects coupled with the tree partitions.",1904.01128v1,https://arxiv.org/pdf/1904.01128v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:40:12Z
TRUE,Degradation modeling|RUL prediction|Accelerated testing|Maintenance optimization|Other,"Stochastic process|Parametric (Weibull, etc.)|Other",Degradation measurements|Sensor/condition monitoring|Other,Condition-based|Predictive|Not applicable,Semiconductor/electronics|Transportation/logistics|Energy/utilities|Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,http://faculty.washington.edu/fscholz/Reports/weibullanalysis2002.pdf,"The paper applies tipping-point/early-warning-signal (EWS) analysis to time-series measurements of DC electrical resistance from solder-joint test vehicles to anticipate imminent failure for predictive maintenance. Nine resistance time series are collected during accelerated thermal cycling (-55°C to 125°C) of zero-ohm 2512 chip resistors on FR-4 PCBs; failures are defined by abrupt resistance increases up to open-circuit. The method monitors critical slowing down via lag-1 autocorrelation (AR(1) coefficient) and a detrended fluctuation analysis (DFA) scaling exponent computed in sliding windows, and compares these EWS to conventional threshold-based criteria (20% resistance increase and triple-nominal resistance). The EWS indicators rise substantially earlier than the threshold crossings and provide a probabilistic forecast of failure time by linearly extrapolating indicator trajectories to their critical values and summarizing projected failure times with kernel densities. The contribution is an individualized, signal-based prognostic approach intended to complement or improve upon bulk Weibull life analysis and fixed thresholds for electronics health monitoring.","The observed resistance time series is framed as a stochastic dynamical system $\dot z = D(z,t)+S(z,t)$, where $D$ is deterministic drift and $S$ is stochastic forcing; the deterministic term can be written as $D(z,t)=-U'(z,t)$ for a potential $U$. The primary EWS is lag-1 autocorrelation estimated by fitting an AR(1) model $z_{t+1}=c z_t+\sigma\eta_t$ (with $\eta_t$ unit-variance Gaussian white noise) and using the AR coefficient $c$ as the indicator, with $c=e^{-\kappa\Delta t}\to 1$ as the decay rate $\kappa\to 0$ near a tipping point. A second EWS is the DFA scaling exponent (DFA of order 2 with detrending) monitored in sliding windows; its critical reference level is taken as 1.5 (per cited prior work), while ACF’s critical reference is near 1.","Across nine thermally-cycled solder-joint resistance series, the lag-1 ACF indicator and DFA indicator provide earlier warning of impending failure than conventional resistance thresholds (20% above nominal and triple-nominal). The authors operationalize warning times by noting when ACF reaches 0.9 and by extrapolating indicator curves to critical values (ACF to 1; DFA to 1.5), yielding a distribution of projected failure times summarized via kernel density peaks. The paper reports that in most cases ACF produces earlier warnings than DFA, attributed to lag-1 (single-point) ACF estimation versus multi-scale DFA estimation (subset of DFA curve over scales 10–100). Conventional thresholds are illustrated as later than EWS in the comparative plots, with hard open-circuit occurring after these earlier EWS and after the moderate threshold in many cases.","The authors note that filtering the resistance time series can distort autocorrelations and therefore corrupt EWS indicators: low-pass filtering can artificially increase autocorrelation and mask meaningful warnings, while high-pass filtering can reduce autocorrelation so indicators may not reach nominal critical values. They also state that the power-spectrum (PS) scaling indicator may require longer datasets because spectra from short subsets can be noisy. The discussion acknowledges that industrial economic constraints may favor later maintenance actions than those suggested by early warnings, depending on safety criticality and device dynamics.","The evaluation is limited to nine units from a specific accelerated thermal-cycling setup, so generalization to other failure mechanisms, environments, or operational (non-accelerated) conditions is uncertain. Warning thresholds for indicators (e.g., ACF=0.9, critical value 1; DFA critical 1.5) and window-size choices may be sensitive hyperparameters; the paper does not fully quantify false-alarm rates or robustness across parameter settings and noise regimes. The extrapolation approach (linear projection of indicator trajectories) assumes near-linear indicator growth and may be unreliable for abrupt failures or nonstationary behavior, especially without prospective validation. There is no head-to-head benchmarking against modern predictive-maintenance baselines (e.g., state-space degradation models, particle filtering, or supervised learning where labels exist) using common metrics such as lead time vs. false-alarm tradeoff.","The authors suggest the approach is generic and can be applied to various electrical measurements and other types of components in power systems and energy applications, beyond the specific resistor/solder-joint experiments studied. They also imply that the techniques can be tuned for earlier detection if more stringent tests are required and that multiple indicators (ACF, DFA, PS) may be applied for comparative assessment depending on data length and noise.","Prospective (online) validation on larger, more diverse fleets with quantified false-alarm/false-negative rates and lead-time distributions would strengthen the case for deployment in predictive maintenance. Developing self-calibrating or adaptive control limits for EWS (accounting for changing operating conditions, drift, and autocorrelated noise) could make the method more robust in the field. Integrating EWS with explicit remaining-useful-life models (e.g., stochastic degradation processes) could translate indicator trends into actionable RUL estimates with uncertainty. Providing open-source implementation and standardized evaluation protocols would enable reproducibility and fair comparison with alternative prognostics methods.",1904.04636v3,https://arxiv.org/pdf/1904.04636v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:40:48Z
FALSE,NA,ML-based|Other,Other,Not applicable,Pharmaceutical|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://github.com/flatkinson/standardiser|https://www.rdkit.org/,"This paper proposes a framework to produce reliable, per-sample prediction error estimates (prediction intervals) for deep neural network regression models by combining test-time dropout ensembles with conformal prediction. A single dropout-trained DNN is run multiple times (100 stochastic forward passes) on the validation and test sets; the mean prediction and the across-pass standard deviation are used to form nonconformity scores on validation residuals and then construct prediction intervals for test points. The method is evaluated on 24 real bioactivity (pIC50) regression datasets from ChEMBL, with repeated random train/validation/test splits (20 runs), and compared against random-forest-based cross-conformal predictors. Results show strong calibration/validity (reported R^2 > 0.99 between confidence level and coverage) and comparable or narrower interval-size distributions than RF conformal predictors at typical confidence levels (80% used by default). A retrospective virtual screening analysis suggests dropout conformal predictors and RF conformal predictors yield similar retrieval rates of active compounds at several pIC50 cutoffs, while the dropout approach is computationally efficient since it requires only extra forward passes rather than training many separate models.","Nonconformity scores on the validation set are defined as $\alpha_i = \frac{|y_i-\hat{y}_i|}{e^{\sigma_i}}$, where $\hat{y}_i$ is the mean of 100 test-time-dropout predictions and $\sigma_i$ is their standard deviation. For a desired confidence level (CL), the corresponding percentile $\alpha_{CL}$ of the sorted nonconformity scores is selected. Prediction intervals (confidence regions) for a test point $j$ are then computed as $\hat{y}_j \pm (e^{\sigma_j}\,\alpha_{CL})$, using the test-time-dropout ensemble mean $\hat{y}_j$ and standard deviation $\sigma_j$.","Across 24 ChEMBL pIC50 regression datasets, DNN and RF predictive accuracy on held-out test sets is reported as comparable, with RMSE generally in the ~0.55–0.80 pIC50 range. Dropout conformal predictors are reported to be well-calibrated/valid, with a very high correlation between nominal confidence level and empirical coverage on test sets (R^2 > 0.99, P < 0.001). Interval efficiency is described as comparable to RF cross-conformal predictors, with dropout-based predictors showing fewer extreme (very large) prediction intervals and a smaller spread of interval sizes. In retrospective virtual screening experiments using multiple pIC50 cutoffs (5–9), dropout and RF conformal predictors yield comparable numbers/fractions of true positives, false positives, false negatives, and ‘uncertain’ (interval-crossing) predictions.","The paper notes that conformal validity guarantees rely on the exchangeability assumption, which is generally assumed for preclinical data but is not always satisfied. It also observes that test-time-dropout ensembles exhibit a narrower spread of predictive variances than RF ensembles, implying a weaker relationship between predicted variance and absolute error, so interval sizes may be less representative of true per-sample error than RF-derived intervals in some settings.","The evaluation is restricted to random splits on curated ChEMBL IC50 datasets; performance and calibration under more realistic distribution shifts (e.g., scaffold splits/temporal splits/novel chemistry) is not established and could materially affect validity and usefulness. The approach uses a fixed number of stochastic passes (100) and specific architecture/hyperparameters; sensitivity to these choices (and computational scaling for larger models) is not systematically analyzed. No public code or fully reproducible pipeline is provided, which limits verification of implementation details (e.g., exact conformal variant, tie handling, percentile selection) and comparability across studies.",None stated.,"Evaluate dropout conformal predictors under non-exchangeable settings common in drug discovery (scaffold-based splits, temporal validation, domain shifts) and consider extensions that preserve validity under covariate shift. Explore alternative uncertainty surrogates or nonconformity functions (including self-normalized or heteroscedastic models) to strengthen the link between predicted uncertainty and absolute error for DNNs. Provide an open-source reference implementation and benchmark against additional uncertainty-quantification baselines (e.g., deep ensembles, quantile regression, Bayesian last-layer, conformalized quantile regression) across standardized molecular ML benchmarks.",1904.06330v1,https://arxiv.org/pdf/1904.06330v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:41:21Z
TRUE,RUL prediction|Maintenance optimization|Degradation modeling|Other,ML-based|Other,Sensor/condition monitoring|Simulated only,Predictive,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Other,TRUE,None / Not applicable,Not provided,http://ti.arc.nasa.gov/project/prognostic-data-repository,"The paper proposes a Functional Data Analysis (FDA) approach for remaining useful life (RUL) estimation called functional Multilayer Perceptron (functional MLP), which treats multivariate sensor time series from each asset as continuous functions and learns a nonlinear functional regression from sensor curves to RUL. The method uses functional neurons that compute integrals of sensor curves against learned weight functions, followed by standard numerical neural-network layers; the weight functions are represented using data-driven bases derived from functional principal component eigenfunctions. The authors apply the method to the NASA C-MAPSS turbofan engine benchmark (simulated run-to-failure / prior-to-failure sensor trajectories) and include a preprocessing step to remove operating-condition effects by regressing each sensor on operating-condition variables and subtracting the predicted value. Performance is evaluated on four C-MAPSS subsets using RMSE and the PHM08 score metric, comparing against MLP, SVR/RVR, CNN, DW-RNN/MTL-RNN, LSTM, and bootstrap LSTM variants. Across all four subsets, functional MLP achieves the best reported results and large improvements over LSTM, particularly on the asymmetric score metric used in PHM.","The functional regression target is $Y_i = F(X_{i,1}(t),\ldots,X_{i,R}(t))$. In functional MLP, each functional neuron computes a generalized inner product $\int_{t\in T} V_{k,r}(\beta_{k,r},t)\,X_{i,r}(t)\,dt$ summed over sensors and passed through an activation, yielding first-layer output $H(X_i,\beta)=\sum_{k=1}^K a_k U_k\left(b_k+\sum_{r=1}^R\int V_{k,r}(\beta_{k,r},t)X_{i,r}(t)dt\right)$. The weight functions are expanded in eigenfunctions of the sensor covariance: $V_{k,r}(\beta_{k,r},t)=\sum_{p=1}^{P_r}\beta_{k,r,p}\,\phi_{r,p}(t)$, where $\phi_{r,p}$ satisfy $\lambda_{r,p}\phi_{r,p}(t)=\int G_r(s,t)\phi_{r,p}(s)ds$ and $G_r$ is the sample covariance function.","On NASA C-MAPSS, functional MLP (FMLP) achieves RMSEs of 13.36 (FD001), 16.62 (FD002), 12.74 (FD003), and 17.76 (FD004), outperforming all compared baselines including LSTM (16.14/24.49/16.18/28.17). Relative to LSTM, RMSE improvements are 17.22%, 32.14%, 21.26%, and 36.95% for FD001–FD004, with an average RMSE improvement of 26.89%. For the PHM score metric, FMLP reports 2.0×10^2 (FD001), 9.0×10^2 (FD002), 1.8×10^2 (FD003), and 1.0×10^3 (FD004), improving over LSTM by 41.18%, 80.00%, 78.82%, and 82.14% respectively (average score improvement 70.54%). The authors also show qualitative plots indicating estimates track the piecewise RUL more closely near end-of-life.",None stated.,"The evaluation is limited to a single benchmark (NASA C-MAPSS) with simulated data, so generalization to real industrial sensor data with missingness, calibration drift, and nonstationary operating regimes is not demonstrated. The paper does not report uncertainty quantification or prediction intervals for RUL, which are often required for decision-making in PHM. Implementation details critical for reproducibility (training hyperparameters, optimizer settings, basis/truncation sensitivity, compute budget) and any released code are not provided, making it hard to validate claims or deploy. The approach assumes functional curve recovery or alignment to common time grids when sampling differs; robustness to sparse/irregular sampling and strong autocorrelation/noise is not systematically studied.",None stated.,"Extend the functional MLP to provide calibrated uncertainty estimates (e.g., Bayesian functional networks, ensembling, or conformal prediction) to support risk-aware maintenance decisions. Validate the method on real-world run-to-failure or field-return datasets across different asset types and include ablations isolating the impact of operating-condition normalization and eigenfunction vs spline/wavelet bases. Develop variants robust to irregular sampling, missing sensors, and domain shift (changing operating conditions over fleets), potentially via self-starting or online/transfer learning FDA formulations. Provide an open-source implementation and standardized benchmarking protocol to facilitate adoption and fair comparison with newer transformer/sequence models for RUL.",1904.06442v1,https://arxiv.org/pdf/1904.06442v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:41:54Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study,TRUE,Python|Other,Not provided,http://mulan.sourceforge.net/datasets.html|http://scikit.ml/api/|http://scikit.ml/api/skmultilearn.base.problem_transformation.html|http://scikit.ml/api/skmultilearn.problem_transform.cc.html,"The paper formalizes multi-label classification (MLC) with partial abstention, allowing a predictor to output 0/1 on some labels and abstain (⊥) on others, and frames optimal prediction as Bayes risk minimization under generalized losses. It proposes an additive abstention-penalty extension of standard MLC losses and derives optimal decision rules/algorithms for three metrics: Hamming loss, rank loss, and F-measure (via a generalized F-measure). For decomposable losses (notably Hamming), the risk-minimizing partial prediction is obtained by sorting labels by expected label-wise loss/uncertainty and choosing how many labels to predict versus abstain based on the penalty. For rank loss and generalized F-measure (under conditional independence assumptions), the optimal partial prediction has a “boundary set” structure relative to labels sorted by marginal relevance probabilities, enabling efficient construction (claimed time O(m log m) for rank loss; O(m^3) for generalized F-measure). Experiments on MULAN benchmark datasets with binary relevance + logistic regression (and additional variants in the supplement) show that partial abstention can reduce expected loss compared with always predicting or always abstaining, with predictable trade-offs controlled by abstention cost/penalty.","Partial predictions take values in {0,⊥,1}^m and are chosen by minimizing expected generalized loss: \(\hat{\mathbf{y}}\in\arg\min_{\hat{\mathbf{y}}}\sum_{\mathbf{y}} \tilde\ell(\mathbf{y},\hat{\mathbf{y}})P(\mathbf{y}\mid x)\). The main generalized-loss construction is additive: \(\tilde\ell(\mathbf{y},\hat{\mathbf{y}})=\ell(\mathbf{y},\hat{\mathbf{y}}_{D})+\psi(|A|)\), with decision set \(D\) (predicted labels) and abstention set \(A\). For linear per-label abstention cost \(c\), \(\tilde\ell=\ell(\mathbf{y},\hat{\mathbf{y}}_{D})+c|A|\); for Hamming loss this yields abstain on label i when \(\min(p_i,1-p_i)>c\) (with \(p_i=P(Y_i=1\mid x)\)).","For decomposable losses with abstention penalty \(\psi(|A|)\), the Bayes-optimal partial prediction can be found by sorting labels by their label-wise expected loss and selecting an optimal prediction size (time \(O(m\log m)\)). For Hamming loss, the extension is shown uncertainty-aligned (risk-minimizers abstain on the most uncertain labels), and monotonicity holds under stated conditions on \(\psi\) (e.g., bounded increments). For rank loss, an efficient construction is given (time \(O(m\log m)\)) based on selecting a boundary set in the probability-sorted ranking, but the rank-loss extension is not uncertainty-aligned (counterexample provided). For generalized F-measure, under conditional independence a boundary-set structure is also derived and an \(O(m^3)\) algorithm is given; experiments on six MULAN datasets show partial abstention often achieves lower loss than full prediction or full abstention, with abstention size decreasing as abstention cost increases.","Several theoretical results for rank loss and generalized F-measure rely on (conditional) independence assumptions on label probabilities (e.g., factorization of \(P(\mathbf{y}\mid x)\)). The paper also notes that monotonicity as defined for label-wise predictions does not directly apply to rank loss (since outputs are rankings rather than labelings) and therefore is not analyzed there. Proofs of formal statements are deferred to supplementary material rather than included in the main paper.","The approach presumes access to reasonably calibrated conditional probabilities (marginals and, for rank loss, pairwise marginals), but the impact of miscalibration on abstention decisions is not systematically analyzed. The abstention penalty is exogenously specified (via \(c\) or \(\psi\)) and not learned or tied to an explicit operational cost model, which can limit practical deployment guidance. Experimental comparisons mainly vary the abstention mechanism while keeping the underlying base learner fixed; broader comparisons to other selective prediction/MLC-with-reject baselines could strengthen evidence. The framework is developed for i.i.d. data and does not address distribution shift or temporal dependence, which are common in reliability/monitoring contexts.",The authors plan to further elaborate the formal framework by instantiating it for other loss functions commonly used in multi-label classification. They also explicitly mention extending the analysis to cases with label dependence (beyond the independence assumptions used for some results).,"Developing self-calibrating or uncertainty-quantified probability estimators (e.g., conformal prediction, Bayesian methods) could make abstention decisions more robust and principled in practice. Extending the framework to handle distribution shift (domain adaptation) and correlated/temporal data would broaden applicability to real monitoring settings. Providing an open-source reference implementation and standardized benchmarks for selective MLC would improve reproducibility and adoption. Learning or optimizing abstention penalties from downstream utility/cost data (rather than fixed \(c\)/\(\psi\)) would better connect the method to operational decision-making.",1904.09235v2,https://arxiv.org/pdf/1904.09235v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:42:38Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes PGS (maximize Performance Gain while maintaining Safeness), a weakly supervised learning framework that uses a small clean validation set to guide label-quality optimization while enforcing that performance does not degrade relative to a supervised baseline. The method is formulated as a bi-level optimization problem in which instance weights and label-correction variables are optimized to minimize validation loss subject to safeness constraints across multiple bootstrapped validation splits. The authors provide optimization procedures for both convex settings (via KKT/implicit differentiation) and non-convex settings (via unrolled optimization with Lagrangian multipliers) and give a convergence result under Lipschitz continuity assumptions. Experiments on MNIST and CIFAR-10 for label-noise learning and semi-supervised learning, and on six UCI regression datasets, show improved accuracy/MSE compared with several WSL baselines and prior safe WSL methods, including under biased validation splits. Overall, the work targets reliability in the sense of preventing performance degradation in weak supervision, but it is not a reliability engineering study (no failure/degradation/maintenance reliability modeling).","The core formulation is a constrained optimization that maximizes validation performance while enforcing safeness: maximize over model parameters and optimized labels such that for each (bootstrapped) validation set $D^v_i$, $\mathrm{perf}(\theta,D^v_i)\ge \mathrm{perf}(\theta_0,D^v_i)$, with $\theta=\mathrm{WSL}(x,y')$ and $\theta_0=\mathrm{SL}(x,y)$. Label-quality optimization is parameterized by instance weights $w\in[0,1]^n$ and label-transition variables $Q$ (classification: simplex constraints per instance; regression: additive corrections), with the upper-level objective minimizing validation loss plus a worst-case (max over validation splits) penalty term, and the lower-level objective training $\theta$ by minimizing weighted (and label-corrected) empirical loss.","On MNIST with 50% uniform label noise, PGS achieves 83.4±0.19% accuracy (unbiased validation) versus baseline 77.4±0.45% and SafeW 77.9±0.34%; with biased validation, PGS achieves 80.3±0.39% versus baseline 76.7±0.47%. On CIFAR-10 with 50% label noise, PGS achieves 66.3±0.13% (unbiased) and 64.5±0.33% (biased), outperforming compared methods reported. For semi-supervised learning with 40% labeled data, PGS reaches 86.5±0.33% on MNIST and 68.8±0.33% on CIFAR-10 under unbiased validation. On six UCI regression datasets, PGS yields lower MSE than baseline and SVR in the reported settings (e.g., abalone noise: 0.010±0.002 vs baseline 0.017±0.004; mpg noise: 0.021±0.005 vs baseline 0.081±0.011).",None stated.,"The approach depends on having a small clean, unbiased validation set; if this set is not representative (domain shift) or contains label errors, both the gain objective and the safeness constraints can be misleading. The safeness guarantee is empirical and tied to bootstrapped validation subsets rather than a formal worst-case guarantee over the true data-generating distribution. Computational cost can be high for non-convex models due to bi-level/unrolled optimization and repeated training steps, and the paper does not provide implementation/code details to assess practicality and reproducibility.",The authors suggest follow-up work including integrating an adversarial mechanism into the proposed framework.,"Develop self-starting or robust variants that tolerate label noise or bias in the validation set, and study formal generalization guarantees for the safeness constraint beyond bootstrap validation splits. Extend the framework to settings with distribution shift, time series/autocorrelation, or structured outputs where validation representativeness is harder to ensure. Provide an open-source reference implementation and benchmarks to evaluate computational tradeoffs and facilitate adoption.",1904.09743v1,https://arxiv.org/pdf/1904.09743v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:43:09Z
FALSE,NA,Other,Event/count data,Not applicable,Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/juergenlerner/eventnet|https://github.com/juergenlerner/eventnet/wiki/Large-event-networks-(tutorial),"The paper studies the stability (“reliability”) of parameter estimates in Relational Event Models (REMs) when fitting to extremely large dyadic event streams (Wikipedia edits; 361,769,741 events; risk set up to ~3.7×10^13 dyads). It evaluates two sampling strategies for scalable estimation: (1) uniform sampling of observed events and (2) case-control sampling of non-event dyads (“controls”) from the risk set, using a sampled Cox partial likelihood. Through repeated sampling experiments, it quantifies how variability in estimated REM coefficients depends on the number of sampled events and controls per event for common network statistics (repetition, popularity, activity, four-cycle, assortativity). Results indicate that tens of thousands of sampled events with a small number of controls per event can yield stable inference for most effects, but highly sparse predictors (notably repetition among controls) can cause large across-sample coefficient variability; the authors recommend checking predictor distributions separately for events vs controls and suggest stratified sampling for degenerate covariates. The work is accompanied by open-source software (eventnet) and public preprocessed data to enable replication and further sensitivity testing.","The REM is formulated as a Cox proportional hazards model with dyadic intensity $\lambda(u,v,t;\theta)=\lambda_0(t)\exp\{\sum_{\ell=1}^k \theta_\ell s_\ell(u,v,G[E;t])\}$ and partial likelihood $L(\theta)=\prod_{e_i\in E}\frac{\lambda_1(u_i,v_i,t_i)}{\sum_{(u,v)\in R_{t_i}}\lambda_1(u,v,t_i)}$. Case-control sampling replaces the denominator risk set $R_{t_i}$ by a sampled set $SR_i(m)$ of size $m+1$ (event dyad plus $m$ controls), yielding a sampled likelihood $L^{[SR(m)]}(\theta)$. Uniform sampling of observed events keeps only $SE(p)\subseteq E$ in the product, yielding $L^{[SR(m),SE(p)]}(\theta)=\prod_{e_i\in SE(p)}\frac{\lambda_1(u_i,v_i,t_i)}{\sum_{(u,v)\in SR_i(m)}\lambda_1(u,v,t_i)}$ while still computing statistics from the full past-event network $G[E;t]$.","With initial sampling parameters $p_0=10^{-4}$ and $m_0=5$ (≈36,114 sampled events; 216,684 total observations), a fitted model on one sample yields significant coefficients: repetition 12.348 (SE 0.999), popularity 1.244 (0.035), activity 1.445 (0.030), four.cycle 0.659 (0.089), assortativity −0.120 (0.024). Over 100 repeated samples at $p=10^{-4}, m=5$, popularity and activity coefficients are very stable (SD ≈0.04 each) and four.cycle/assortativity moderately stable (SD 0.15 and 0.03), whereas repetition is highly variable (min 9.38, max 86.73; SD 27.00) despite consistently positive and significant z-values. Experiments varying $p$ show that popularity/activity can be reliably estimated with only a few hundred events, while four.cycle and assortativity typically need thousands to tens of thousands of events for consistent significance. For a fixed observation budget, sampling more events with few controls (small m, roughly 2–16) generally reduces variability compared to sampling fewer events with many controls, consistent with events contributing more information than controls.","The authors note that asymptotic consistency/normality of case-control sampling in the Cox model does not directly determine sufficient finite-sample sizes for stable estimation of specific network effects, motivating their experimental approach. They also acknowledge that extremely sparse/degenerate predictors among controls (e.g., repetition) can lead to misleadingly small apparent variability under some sampling configurations and may require alternative sampling designs (e.g., stratified sampling). The paper does not implement stratified sampling in the reported experiments, presenting it as a suggested improvement rather than evaluated evidence.","The study’s conclusions about required sample sizes are tied to a specific REM specification (five effects, log(1+x) transforms, 30-day half-life decay, and a two-mode user–article risk set); other statistics, decay choices, or risk-set definitions could materially change sparsity patterns and needed sample sizes. Evaluation focuses mainly on coefficient variability/significance across repeated samples, but does not provide calibrated error metrics (e.g., coverage of confidence intervals against a known ground truth) because no ground truth exists for the empirical data; this limits claims about estimation accuracy vs stability. The runtime/feasibility discussion is qualitative; a more detailed benchmark (wall-clock time, memory, scaling with p and m) would strengthen operational guidance. Dependence on the Cox partial likelihood with ordinal time ignores potential issues from tied events and temporal dependence beyond covariates; robustness to alternative time models (parametric baseline hazards or marked point process formulations) is not empirically demonstrated here.","The authors propose experimentally assessing additional sampling strategies, especially stratified sampling/counter-matching approaches, to address situations with highly skewed or near-degenerate covariates among controls. They also suggest exploring sampling from the input event stream used to construct the past-event network $G[E;t]$ to enable analysis when the full history is too large to store, noting that this would require sensitivity analyses akin to those in the paper. Finally, they mention that improved dynamic graph algorithms (e.g., for efficiently maintaining four-cycle counts) could reduce computation time for complex statistics.","A useful extension would be a practical diagnostic toolkit that automatically flags problematic covariate degeneracy separately for events vs controls (e.g., effective sample size per covariate stratum) and recommends adaptive sampling (stratified or importance sampling) tailored to each statistic. Another direction is to evaluate how sampling impacts downstream predictions (e.g., predictive log-likelihood on held-out events or time-to-next-event calibration) rather than only coefficient stability. Extending the empirical evaluation to additional large-scale domains (communication logs, transactions) and to alternative REM variants (marked events, actor-oriented REMs) would clarify generalizability. Providing packaged scripts/notebooks that reproduce the full experimental grid and runtime profiles would improve reproducibility and help practitioners select p and m under explicit compute budgets.",1905.00630v2,https://arxiv.org/pdf/1905.00630v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:43:56Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,ML-based|Hybrid/Ensemble|Simulation-based|Other,Degradation measurements|Sensor/condition monitoring|Simulated only,Condition-based|Predictive,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/dlaredo/NASA_RUL_-CMAPS-,"The paper proposes an ANN–EA framework for remaining useful life (RUL) estimation of mechanical systems, using a shallow multi-layer perceptron (MLP) regressor paired with differential evolution (DE) to optimize data-shaping parameters. The key idea is to generate features via a strided moving time window and to tune window size ($n_w$), window stride ($n_s$), and the early constant-RUL label ($R_e$) that defines a piecewise-linear degradation labeling scheme. The framework is evaluated on the NASA C-MAPSS run-to-failure turbofan engine dataset, using RMSE and the asymmetric NASA scoring metric (RHS) that penalizes late predictions more than early ones. DE finds near-optimal data-related parameters with far fewer function evaluations than exhaustive search, enabling competitive accuracy using a compact MLP (few layers/neurons) suitable for limited compute environments. Across the four C-MAPSS subsets, the method achieves average RMSEs of about 14.39 (FD001), 29.09 (FD002), 15.42 (FD003), and 34.74 (FD004), and is compared against several recent ML prognostics methods reported in the literature.","The method labels training examples using a piecewise-linear degradation model with an early constant RUL value $R_e$ and a later linear decrease to 0 at failure. Input features are created by unrolling a multivariate strided window of sensor data into vectors of dimension $s\cdot n_w$ (with $s=14$ selected sensors), where consecutive windows advance by stride $n_s$. Sensor readings are min–max normalized to $\hat{x}_i = 2\,\frac{x_i-\min(x_i)}{\max(x_i)-\min(x_i)}-1$. Data parameters $v=(n_w,n_s,R_e)$ are selected by solving $\min_{v\in\mathbb{Z}^3} E_{\mathrm{RMS}}(v)$ via differential evolution, where $E_{\mathrm{RMS}}=\sqrt{\frac{1}{n}\sum_{i=1}^n e_i^2}$ and RHS uses $s_i=\exp(-e_i/13)-1$ for $e_i<0$ and $s_i=\exp(e_i/10)-1$ for $e_i\ge 0$.","Exhaustive search (with 20-epoch training) found best $v$ for FD001 as $[24,1,127]$ with $\min E_{\mathrm{RMS}}=15.11$ (8160 evaluations) and for FD002 as $[16,1,138]$ with $\min E_{\mathrm{RMS}}=30.93$ (3060 evaluations). Differential evolution achieved near-optimal solutions with far fewer evaluations (372): FD001 $[24,1,129]$ with $E_{\mathrm{RMS}}=15.24$ and FD002 $[17,1,139]$ with $E_{\mathrm{RMS}}=30.95$ (both at 20 epochs). Using the DE-chosen parameters and training the MLP for 200 epochs, average test performance over 10 runs was: FD001 RMSE 14.39 and RHS 3.37; FD002 RMSE 29.09 and RHS 50.69; FD003 RMSE 15.42 and RHS 5.33; FD004 RMSE 34.74 and RHS 74.77. The authors report that larger window sizes and small strides (notably $n_s=1$) are generally favored, and that subsets with more operating conditions (FD002/FD004) show larger errors and more overestimation (late/unsafe) behavior.","The authors note that further studies are needed on the impact of the stride parameter $n_s$ on prediction performance. They also acknowledge that the current framework, while using an MLP, could be combined with other regressors and that future work will extend the framework toward model selection (constructing tailored network architectures).","The evaluation is restricted to a simulated benchmark (C-MAPSS); generalization to real fleet data with sensor noise, missingness, censoring, and dataset shift is not established. The approach relies on a specific labeling heuristic (piecewise-linear with tuned $R_e$), which can bias training and may not match true degradation trajectories; uncertainty quantification for RUL is also not provided. The DE objective optimizes RMSE only (not the asymmetric RHS), potentially misaligned with safety-critical costs where late predictions are more harmful. Computational cost is reduced versus exhaustive search, but DE still requires repeated retraining; results may be sensitive to random seeds, training protocol, and hyperparameters beyond $v$ (e.g., learning rate, regularization strength) that are not jointly optimized.","Future work will extend the framework to broader model construction/model selection, i.e., generating the best neural network architecture tailored to a given application. The authors also plan to analyze the influence of the window stride parameter $n_s$ on prediction performance.","A natural extension is to incorporate probabilistic/uncertainty-aware RUL outputs (e.g., predictive intervals) to better support maintenance decisions and risk-aware scoring (e.g., directly optimizing RHS or cost-weighted objectives). The framework could be adapted to handle real-world issues such as censored trajectories, missing sensors, and varying operating regimes via domain adaptation or covariate shift correction. Joint optimization of data parameters and model/training hyperparameters (multi-objective DE optimizing RMSE and RHS plus model size) could better balance accuracy and safety. Additional validation on real industrial datasets (beyond C-MAPSS) and robustness testing under sensor faults/noise would strengthen practical reliability claims.",1905.05918v1,https://arxiv.org/pdf/1905.05918v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:44:39Z
TRUE,Life distribution modeling|Accelerated testing|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based|Other",Complete lifetime data|Event/count data|Mixture of types|Other,Not applicable,Energy/utilities|Other,Simulation study|Other,TRUE,R|Other,Package registry (CRAN/PyPI)|Not provided,https://CRAN.R-project.org/package=rjags,"The paper proposes an assurance-based framework for sample size determination in reliability demonstration testing (RDT), defining sample size via the probability the RDT will conclude “success” rather than via classical power or (Bayesian) risk-criteria designs. It develops assurance calculations for (i) failure-on-demand RDTs with a binomial likelihood (plan defined by number tested n and allowable failures c), and (ii) time-to-failure RDTs with a Weibull model, including accelerated testing through a stress–life link function for the Weibull scale. A key contribution is the explicit separation of design and analysis stages, allowing different priors: an informative producer-oriented design prior and a conservative (e.g., sceptical or mixture) analysis prior used for the pass/fail decision. For the Weibull case, the paper describes computational approaches to estimate assurance efficiently (Monte Carlo with curve-fitting/response-surface smoothing inspired by Müller’s methods) and shows how to incorporate historical data into the design stage without forcing it into the analysis. Two worked examples (emergency diesel generator demand failures; Kevlar pressure vessel lifetimes at multiple stresses/locations) illustrate substantial reductions in required sample size when historical data inform the design prior/posterior.","For binomial RDT with survival probability \(\pi\), \(Y\mid\pi\sim\text{Bin}(n,1-\pi)\), a plan \((n,c)\) passes if \(Y\le c\); assurance is \(\Pr(\text{success})=\int_0^1 \Pr(Y\le c\mid\pi)\,p(\pi)\,d\pi = \int_0^1\sum_{y=0}^c \binom{n}{y}(1-\pi)^y\pi^{n-y}p(\pi)d\pi\), or with historical data \(p(\pi\mid x)\). For Weibull lifetimes \(T\mid\rho,\beta\sim\text{Weibull}(\rho,\beta)\) with density \(f(t\mid\rho,\beta)=\rho\beta(\rho t)^{\beta-1}\exp(-(\rho t)^\beta)\); reliable life is \(\tau_q=\rho^{-1}[-\log q]^{1/\beta}\) and accelerated testing uses \(\log\rho=g(s,\theta)\). A Bayesian pass rule is \(\Pr_A(\tau_q\ge\tau^*_{q}\mid t)\ge 1-\delta\), and assurance averages this pass event over the design prior/posterior for parameters and simulated test data.","In the binomial diesel-generator example with target \(\pi_T=0.96\), achieving 50% assurance using the design prior required sample sizes \(n=(227,279,222)\) for (exact binomial test, sceptical Bayesian prior, mixture Bayesian prior) respectively; assurance could not exceed ~80% under that design prior. Incorporating historical data from 63 generators (via a design posterior for \(\pi\)) concentrated \(\pi\) near 1 (posterior mean 0.990, median 0.992) and reduced the sample sizes needed for 50% assurance to \(n=(74,141,78)\); using the prior-based sample sizes would then yield assurances about \((0.866,0.885,0.872)\). In the Weibull pressure-vessel example (87 failures, 8 locations, stresses 25.5/27.6/29.7 MPa), with target \(\tau_{0.5}^*=4000\) hours at 25 MPa and a sceptical analysis prior set to \(\Pr_A(\tau_{0.5}\ge 4000)=0.1\), the fitted assurance curve under the design posterior reached >85% for sample sizes under 60 and the first \(n\) to achieve 80% assurance (with half at 27 MPa and half at 29 MPa) was \(n=32\). Optimizing allocation across two accelerated stresses showed 80% assurance could be achieved with total sample size as low as 22 (e.g., 20 at 27 MPa and 2 at 29 MPa; estimated assurance 0.802).","The authors note that practical adoption will require additional resources for practitioners, including tooling and step-by-step elicitation guidance to specify the design prior. They also indicate the Weibull assurance computation relies on MCMC plus a numerical scheme with curve fitting, and suggest that more efficient augmented-MCMC approaches are not yet developed in the paper.","The approach depends strongly on elicited priors (design and especially the analysis “sceptical” prior) and on the chosen success criterion (e.g., \(\Pr_A(\tau_q\ge\tau_q^*)\ge 0.95\)); sensitivity analysis is not systematically developed, so design recommendations may change materially under alternative priors/criteria. The Weibull model assumes a common shape parameter \(\beta\) across stresses/locations and a specific stress–life link form, which may be violated; model mis-specification and diagnostics are not integrated into the assurance-driven design. Computational burden and Monte Carlo error control (number of simulations, smoothing choices) could affect assurance estimates, yet uncertainty bands for the assurance curves/surfaces are not emphasized. The paper focuses on pass/fail demonstration and does not address cost/time constraints explicitly (e.g., test duration, censoring plans, or economic design), which are often central in industrial RDT planning.","They propose developing free open-source software and step-by-step guides to support prior elicitation and implementation of assurance-based RDT design in practice. They also suggest investigating augmented MCMC schemes for the Weibull assurance calculations and considering alternative analysis strategies (e.g., frequentist tests or mixture analysis priors) for time-to-failure RDTs. Finally, they note the posterior risk-criteria approach could be adapted to allow separate priors for RDT design and analysis.","Add formal robustness/sensitivity analyses (to design priors, analysis priors, pass thresholds \(\delta\), and target metrics \(\tau_q^*\)) and propagate Monte Carlo/smoothing uncertainty into assurance estimates. Extend the framework to accommodate censoring plans and time-truncated tests (common in RDT), and to jointly optimize sample size, stress allocation, and test duration under cost constraints. Broaden modeling to handle non-Weibull or semi-parametric lifetime models, stress-dependent shape parameters, and dependence/heterogeneity beyond simple random effects. Provide reusable software implementations (e.g., an R/Python package) with templates for elicitation, model checking, and design optimization across multiple stresses.",1905.08659v1,https://arxiv.org/pdf/1905.08659v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:45:24Z
FALSE,NA,"ML-based|Parametric (Weibull, etc.)|Other",Other,Not applicable,Finance/economics|Service industry|Other,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.1145/3366423.3380154|https://www.kaggle.com/wendykan/lending-club-loan-data|https://www.kaggle.com/c/criteo-display-ad-challenge|https://www.kaggle.com/c/avazu-ctr-prediction|https://www.kaggle.com/c/porto-seguro-safe-driver-prediction,"This paper addresses the unreliability of probabilistic predictions from machine learning models due to miscalibration on specific data subsets (e.g., by user group or geography). It proposes a new evaluation metric, Field-level Calibration Error (Field-ECE / Field-RCE), which measures average prediction bias across categories of a chosen input field rather than across probability bins. To improve both calibration and non-calibration performance (notably AUC), the authors introduce Neural Calibration, a post-hoc method that calibrates using both the model logit and the original features via an auxiliary neural network plus a parametric isotonic piecewise-linear mapping (Isotonic Line-Plot Scaling, ILPS). Experiments on five large-scale real datasets (loan default and CTR prediction among others) show Neural Calibration consistently improves log-loss, Brier score, AUC, and field-level calibration metrics versus uncalibrated models and common post-hoc methods (Platt scaling, isotonic regression), and is empirically more robust under dataset shift. The work advances calibration practice for large-scale data mining systems by focusing evaluation and correction on field/group-level biases relevant to deployment decisions.","Probabilistic prediction from a base classifier is \(\hat p(x)=\sigma(l)=\sigma(f(x))\). Field-level calibration error partitions by a categorical field \(z\): \(\text{Field-ECE}=\frac{1}{|D|}\sum_{z\in Z}\left|\sum_i (y_i-\hat p_i)\mathbf{1}[z_i=z]\right|\) and relative version \(\text{Field-RCE}=\frac{1}{|D|}\sum_{z\in Z} N_z\,\frac{\left|\sum_i (y_i-\hat p_i)\mathbf{1}[z_i=z]\right|}{\sum_i (y_i+\epsilon)\mathbf{1}[z_i=z]}\). Neural Calibration outputs \(q(l,x)=\sigma(\eta(l)+g(x))\), where \(\eta(l)\) is an isotonic piecewise-linear mapping (ILPS) and \(g(x)\) is an auxiliary neural network trained on the development set by minimizing log-loss.","Across five large-scale datasets, Neural Calibration improves both calibration metrics and AUC relative to uncalibrated models and standard post-hoc calibration. For Lending Club, Field-RCE drops from 49.2% (base) and 115.3% (jointly trained Model-2) to 12.0% with Neural Calibration, while AUC increases to 0.992 (vs 0.909 base and 0.985 Model-2). For Criteo, Field-RCE improves from 7.46% (base) to 4.59% with Neural Calibration while maintaining competitive AUC (0.7996 vs 0.8001 Model-2). For Avazu, where traditional post-hoc methods degrade field-level calibration under shift (Field-RCE 18.56% isotonic vs 12.88% base), Neural Calibration achieves 10.91% and the best AUC (0.7520). ILPS alone consistently outperforms Platt scaling on field-level calibration across datasets in the ablation study.","Traditional univariate post-hoc calibration methods (e.g., Platt scaling, isotonic regression) can be unreliable under dataset shift when the development set distribution differs from the test distribution, sometimes producing worse calibrated probabilities than the raw outputs. The authors also note that Field-level calibration errors are evaluation metrics for test data and are not appropriate as training loss functions. They focus experiments and formulations on binary classification, with extensions left for future work.","The work is not reliability engineering in the classical sense (failure/degradation/maintenance), but rather ML probability calibration; mapping to engineering reliability contexts (e.g., survival probabilities, hazard models) is not validated. Reported robustness to dataset shift is empirical and limited to the chosen splits/datasets; there is no formal guarantee or systematic stress testing over diverse shift types (label shift, covariate shift, concept drift). The auxiliary network can overfit the development set without careful regularization/monitoring, and the paper does not provide implementation details or public code to assess reproducibility. Field-level metrics depend on selecting and discretizing fields; performance and interpretability may degrade for high-cardinality fields, sparse groups, or continuous variables discretized arbitrarily.","The authors propose future work to better understand the sources of miscalibration in modern ML systems, especially deep learning and information retrieval. They suggest extending field-aware calibration beyond binary classification to regression and multi-class settings. They also mention extending from supervised offline learning to more general settings such as online learning and reinforcement learning.","Evaluate Neural Calibration in settings closer to reliability engineering, e.g., calibrating survival probabilities, time-to-failure risk scores, or hazard predictions under censoring and temporal drift. Develop principled mechanisms for choosing/weighting multiple sensitive fields (and handling sparse/high-cardinality groups) while controlling variance and avoiding overfitting. Provide theoretical analysis or bounds for calibration under specific shift models, plus diagnostics to detect when calibration will likely fail. Release reference implementations and benchmarks to enable reproducible comparisons and study computational/latency impacts in large-scale deployment.",1905.10713v3,https://arxiv.org/pdf/1905.10713v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:46:07Z
FALSE,Other,ML-based|Other,Simulated only,Not applicable,Network/cybersecurity,Simulation study,TRUE,None / Not applicable,Not provided,NA,"This paper studies ultra-reliable and low-latency communications (URLLC) resource allocation, proposing an unsupervised deep-learning approach to solve a constrained functional optimization problem. The example problem jointly allocates downlink transmit power (as a function of instantaneous small-scale fading) and per-user bandwidth (based on large-scale gains) to minimize total bandwidth while meeting strict QoS constraints on delay bound and packet loss probability. The QoS constraints combine finite-blocklength decoding error probability with a queueing-delay violation bound using effective capacity/effective bandwidth, and are enforced during training by using the Lagrangian of the constrained problem as the loss function (implicit supervision via KKT conditions). In a symmetric scenario the authors derive a global optimal policy analytically (power allocation in closed form; bandwidth via stochastic iteration), and show the learned policy matches it. Simulations indicate the learned policy can save roughly 40% bandwidth versus a prior heuristic multi-user diversity policy (and ~60% versus a non-diversity baseline) while satisfying URLLC constraints.","The achievable service rate uses a finite-blocklength approximation: $s_k \approx \frac{\tau W_k}{u\ln 2}\left[\ln\left(1+\frac{\alpha_k g_k P_k}{N_0 W_k}\right)-\frac{Q^{-1}(\epsilon_k^c)}{\sqrt{\tau W_k}}\right]$. Queueing-delay reliability is enforced via effective capacity: $C_k^E=-\frac{1}{\theta_k}\ln \mathbb{E}_g[e^{-\theta_k s_k}]\ge B_k^E$, equivalently $\mathbb{E}_g[e^{-\theta_k s_k}]-e^{-\theta_k B_k^E}\le 0$, and the overall packet loss is approximated by $\epsilon_k^c+\epsilon_k^q\le \epsilon_{\max}$. The unsupervised learning minimizes the Lagrangian $\hat L=\sum_k\big[W_k+\lambda_k(\mathbb{E}_g[e^{-\theta_k \hat s_k}]-e^{-\theta_k B_k^E})\big]$ with $\hat P_k(g;\omega)$ produced by a neural network and constrained via Softmax so $\sum_k \hat P_k\le P_{\max}$.","In the symmetric scenario, the learned (unsupervised) allocation achieves the same total required bandwidth as the analytically derived global optimum. Relative to a state-of-the-art heuristic multi-user diversity policy, the learned approach saves about 40% bandwidth (and about 60% compared with a policy that does not exploit multi-user diversity), while meeting $\epsilon_{\max}=10^{-5}$ and $D_{\max}=1$ ms (10 frames) in the reported setup. In an asymmetric scenario with users at varying distances (50–250 m), the learned solution shows similar performance gains over the compared baselines. Convergence in the asymmetric scenario is reported as ~10,000 frames to reach 99.99% convergence without pre-training, reduced to ~1,000 frames with pre-training (per their convergence criteria).","The authors note (in discussion around bandwidth iteration) that due to the finite-blocklength approximation (using $V_k\approx 1$), the achievable-rate expression may not be monotone in bandwidth for extremely small small-scale gains, but argue such events are rare and the impact is negligible after expectation. They also state they do not show certain validation results (effective capacity applicability) due to space limitations.","The work targets communications QoS “reliability” (packet loss/latency) rather than reliability engineering (failure/maintenance), so results do not translate directly to component/system lifetime reliability. The learning method’s guarantees depend on convergence of primal-dual SGD and on accurate estimation of expectations via mini-batches; stability/sensitivity to hyperparameters and non-stationary traffic/channel statistics is not fully characterized. Evaluation is simulation-based with Rayleigh fading and specific parameter settings; robustness to model mismatch (e.g., correlated fading, imperfect CSI, different traffic processes) and scalability beyond the tested regimes is unclear. Code and reproducibility details (implementation, seeds, training schedules) are not provided, limiting independent verification.",None stated.,"Provide theoretical convergence/constraint-violation bounds for the proposed primal-dual unsupervised training under stochastic approximation and function approximation error. Extend to more realistic settings (imperfect/partial CSI, correlated fading, non-Poisson arrivals, non-orthogonal multiple access, and multi-cell interference) and evaluate robustness to distribution shift. Release reference implementations and benchmarking scripts to support reproducibility and fair comparison across URLLC resource-allocation methods. Explore adaptive/self-tuning architectures or safe-learning mechanisms that enforce strict constraints during transient learning (not only at convergence).",1905.13014v2,https://arxiv.org/pdf/1905.13014v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:46:41Z
TRUE,Maintenance optimization|Other,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes PRO-RL (Proximal Reliability Optimization for Reinforcement Learning), a reinforcement-learning framework that replaces deterministic reward maximization with reliability-based optimization to improve robustness under uncertainty (“reality gap”). Reliability is defined at each time step as the probability that reward exceeds a specified threshold, computed by sampling many realizations of uncertain environment parameters and estimating distributions (e.g., via kernel density estimation). To keep data efficiency, the authors build a virtual environment using a neural-network surrogate model for the state transition function trained from Latin Hypercube Sampling (LHS) of state, action, and uncertainty parameters. Policy optimization is performed with a modified PPO-style actor–critic objective using reliability-based returns/advantages rather than reward-based ones. Experiments on CartPole and Inverted Pendulum with modeled physical, environmental (gravity), observation, and control noise show improved robustness/generalization across sampled parameter variations, achieving task success thresholds across the design space in their evaluations.","Environment dynamics are parameterized by an uncertainty vector/matrix $\phi_t$ with $s_{t+1}=f_s(s_t,a_t,\phi_t)$. Per-step reliability replaces reward: $rel_t=\Pr(r_t\ge r_{threshold})$; the trajectory “return” is the sum of per-step reliabilities $R(\tau)=\sum_{t=0}^T rel_t$, and the objective is to maximize $J(\pi)=\mathbb{E}[R(\tau)]$. PPO’s clipped surrogate is reused with reliability advantage: $L(\theta)=\mathbb{E}_t[\min(\rho_t A_t,\text{clip}(\rho_t,1-\epsilon,1+\epsilon)A_t)]$ where $\rho_t=\pi_\theta(a_t|s_t)/\pi_{\theta_{old}}(a_t|s_t)$.","On CartPole, the learned controller is reported to remain successful across systematic variations of key parameters (e.g., cart/pole masses, pole length, gravity) and to reliably meet the minimum reward/steps criterion of 195 over the sampled design space (shown via reward maps). Over 1000 random realizations, the controller reportedly stabilizes in about 100 time steps in the majority of cases (shown via a histogram of time-to-maximum-reward). Policies trained in the virtual environment are validated in 100 realizations of the “real” (simulated) environment with varying $\phi$ and start states drawn from $p_0(\cdot)$. Specific comparative ARL-style or baseline numerical deltas versus standard PPO/reward-optimization are not provided in the excerpted text, but the qualitative claim is improved robustness/generalization under time-invariant and time-varying parameter changes.","The authors note that surrogate model confidence is restricted to previously visited regions; many model-based methods exploit a small initial-state distribution, leading to low robustness (motivating their use of LHS for global fidelity). They also imply that the number of virtual-environment queries can be orders of magnitude larger depending on the number of uncertainty realizations used, even if wall-clock time is similar to coupled model-free RL.","The reliability definition $\Pr(r_t\ge r_{threshold})$ depends heavily on the chosen reward threshold and on how uncertainties in $\phi$ are modeled; sensitivity analysis and principled threshold selection are limited. The approach relies on generating many realizations (stated as $10^3$–$10^6$) and KDE-style distribution estimation, which may become computationally expensive or unstable in higher-dimensional state/reward settings. Validation is on classic control benchmarks with simulated uncertainty; evidence for real hardware transfer or broader classes of dynamics (e.g., strong nonstationarity, unmodeled failure modes) is not established. No implementation details on software stack or released code are given, which limits reproducibility.",None stated.,"Extend PRO-RL to settings with unknown/learned uncertainty distributions (online Bayesian updating of $\Phi$) and to partially observable problems with explicit belief-state modeling. Develop principled methods for selecting/adapting $r_{threshold}$ (or using CVaR/quantile reliability objectives) and analyze robustness to misspecified uncertainty models. Provide self-contained open-source implementations and benchmark against strong robust-RL baselines (e.g., RARL/EPOpt variants, distributional RL, risk-sensitive PPO) under standardized uncertainty suites. Evaluate on real robotic platforms or higher-dimensional continuous-control tasks to demonstrate sim-to-real transfer beyond classic benchmarks.",1906.01127v1,https://arxiv.org/pdf/1906.01127v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:47:17Z
FALSE,NA,ML-based|Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"This paper proposes Causal Information Bottleneck (CIB), an information-theoretic representation learning framework to estimate individual treatment effects (ITE) from observational data more reliably. The method extends the Information Bottleneck objective to maximize mutual information between a learned representation and potential outcomes under treatment/control while penalizing information retained about covariates, and adds two regularizers: conditional mutual information minimization to encourage treatment-invariant representations and a counterfactual predictive-variance regularizer to stabilize counterfactual predictions. The approach is implemented with stochastic encoders (Gaussian reparameterization) and neural estimators for the mutual-information-related objectives, optimized with stochastic gradient methods. Experiments on three standard causal inference benchmarks (IHDP, Jobs, Twins) show competitive or state-of-the-art performance on PEHE/policy risk/AUC and improved uncertainty-based “I don’t know” behavior for out-of-distribution-like samples. Overall, the contribution is in causal ML/uncertainty calibration rather than reliability engineering.","The core objective is an IB-style maximization for potential outcomes: $\max\; I(Y_0;Z)+I(Y_1;Z)-\beta I(X;Z)$. Additional regularizers include conditional mutual information minimization for disentangling, $I(Z;T\mid X)$ (implemented via a Donsker–Varadhan neural estimator), and counterfactual predictive variance regularization $L_V(\phi,\theta):=-\mathbb{E}_{p(x)}\,\mathrm{Var}_{q_\theta(y\mid x)}[y^{CF}\mid x]$. The final training objective combines factual-outcome likelihood bounds for both arms with the compression term and the two regularizers: $L=L_0+L_1-\beta L_C+\lambda_M L_M+\lambda_V L_V$.","On out-of-sample evaluation, CIB reports IHDP $\sqrt{\epsilon_{PEHE}}=0.613\pm0.118$ and Twins AUC $0.861\pm0.005$, outperforming or matching strong baselines reported in the paper’s comparison tables (e.g., SITE: IHDP $0.656\pm0.108$, Twins $0.853\pm0.006$). For the Jobs dataset, CIB achieves policy risk $R_{pol}=0.211\pm0.017$ (out-of-sample), comparable to leading representation-learning methods (e.g., SITE $0.219\pm0.009$). An ablation study indicates removing CPVR degrades IHDP performance (out-of-sample increases from $0.613$ to $0.649$), while both MIGDR and CPVR matter on Jobs/Twins. The uncertainty experiment (dropping top-k uncertain samples) shows CIB more effectively identifies and removes uncertain/OOD-like instances than random dropping used for other methods.",None stated.,"The work targets reliability in the sense of robustness/calibration for causal effect prediction, not reliability engineering (failure/maintenance) contexts; results do not translate directly to engineering reliability decisions. The method relies on strong causal identification assumptions (overlap and conditional ignorability) and does not address unobserved confounding, which can dominate real observational settings. Empirical comparisons partly rely on numbers taken from prior papers under potentially different experimental protocols, and the paper notes some baselines (e.g., CEVAE/GANITE) use different realization counts/settings, which complicates fairness. Implementation details indicate a score/propensity model was omitted due to no gains, raising questions about sensitivity to selection bias correction and hyperparameter stability across domains.",None stated.,"Evaluate robustness under violations of overlap and unconfoundedness (e.g., sensitivity analysis or instrumental-variable settings) and quantify how uncertainty estimates behave under hidden confounding. Extend the approach to multi-valued/continuous treatments and time-varying treatments with longitudinal outcomes. Provide an open-source implementation and standardized benchmarking across identical splits/protocols to improve reproducibility and enable broader adoption. Explore deployment-oriented calibration metrics and decision-focused evaluation (e.g., treatment policy value under uncertainty) beyond PEHE/policy risk/AUC.",1906.03118v1,https://arxiv.org/pdf/1906.03118v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:47:51Z
FALSE,NA,ML-based|Bayesian|Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/SkafteNicki/john|https://mrcc.illinois.edu/CLIMATE/Station/Daily/StnDyBTD2.jsp|https://www.tensorflow.org/|http://github.com/SheffieldML/GPy|https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_moons.html,"This paper proposes complementary methods for training regression neural networks that predict both a mean function $\mu(x)$ and a predictive variance (or uncertainty) function, addressing common underestimation and poor out-of-domain variance behavior. The contributions include (i) a locality-aware mini-batching scheme (nearest-neighbor sampling) with Horvitz–Thompson reweighting to yield unbiased stochastic gradients for the global likelihood, (ii) mean–variance split (alternating) training so variance updates are conditioned on a fixed mean and vice versa, (iii) a robust likelihood formed by modeling the variance with an inverse-Gamma distribution leading to a Student-t predictive distribution, and (iv) an extrapolation architecture that forces variance to revert toward a preset value $\eta$ far from the training data using inducing-point distances and a sigmoid gating function. The methods are evaluated on toy heteroscedastic regression, UCI regression benchmarks, an uncertainty-calibration weather dataset, active learning, and VAE generative modeling, showing improved test log-likelihood/uncertainty quality versus baselines (NN, BNN, MC-Dropout, deep ensembles, and GPs). The work is primarily about uncertainty/variance estimation in machine learning rather than reliability engineering (no failure-time/degradation/maintenance focus).","The predictive model assumes $p_\theta(y\mid x)=\mathcal N(y\mid \mu(x),\sigma^2(x))$ and introduces a local likelihood $\log \tilde p_\theta(\cdot\mid x_i)=\sum_{j=1}^N w_j(x_i)\log p_\theta(y_j\mid x_j)$. For locality-sampled mini-batches, an unbiased estimator of the global log-likelihood uses Horvitz–Thompson reweighting: $\sum_{x_j\in\mathcal O}\frac{1}{\pi_j}\left[-\tfrac12\log\sigma^2(x_j)-\tfrac{(y_j-\mu(x_j))^2}{2\sigma^2(x_j)}\right]$, with inclusion probability $\pi_j=\frac{m}{N}\sum_{i=1}^N \frac{n}{k}\mathbf 1_{j\in\mathcal O_k(i)}$. Robust variance modeling sets $\sigma_i^2\sim \text{Inv-Gamma}(\alpha_i,\beta_i)$ and integrates out $\sigma_i^2$ to yield a Student-t predictive density. The extrapolation architecture defines $\delta(x)=\min_i\|c_i-x\|$ and blends $\hat\sigma^2(x)=(1-\nu(\delta(x)))\hat\sigma^2_\theta(x)+\eta\,\nu(\delta(x))$ with $\nu$ a scaled/shifted sigmoid.","On the proposed weather-uncertainty calibration task, the mean absolute variance error reported is lowest for the Combined model (Err = 1.6), compared with GP (1.84), Ens-NN (1.86), NN (2.94), MC-Dropout (3.71), and BNN (7.6). In the UCI regression benchmark (13 datasets), the Combined approach achieves the best test log-likelihood on 10/13 datasets (as stated by the authors), while also avoiding the need to train multiple ensemble members. For generative modeling, the proposed Comb-VAE improves both training ELBO and test log-likelihood across MNIST, FashionMNIST, CIFAR10, and SVHN (e.g., MNIST test log-likelihood 2018.37 vs 1914.77 for standard VAE). Active learning experiments show faster RMSE improvement on several datasets (e.g., Boston, Superconduct, Power) when selecting points with highest predicted variance.","The authors note their methods depend on Euclidean distance computations, which can break down in high dimensions, potentially limiting suitability for high-dimensional data. They suggest nearest-neighbor computations may still work better than full distance use, but acknowledge the risk. They also speculate that variance estimation may be inherently difficult in high dimensions due to distance/metric issues and suggest a learned metric could help.","The nearest-neighbor locality sampler requires precomputing neighborhoods with stated cost $O(N^2D)$, which can be prohibitive for very large $N$ (even if pre-training) and is only partially mitigated by approximate NN methods; this limits scalability compared with standard SGD pipelines. The approach is evaluated largely on i.i.d. regression datasets; robustness to temporal dependence/autocorrelation, covariate shift, or heavy-tailed/contaminated noise beyond what the Student-t can cover is not systematically characterized. Uncertainty quality is mainly assessed via log-likelihood (and one bespoke variance-calibration metric), but additional calibration diagnostics (coverage vs nominal, reliability diagrams for intervals) are not comprehensively reported across benchmarks. The Combined method bundles multiple changes, which may complicate tuning and isolate when/why it fails in practice despite an ablation on log-likelihood.",They state that the encoder variance in the VAE setting $\sigma^2_\phi(x)$ is left unchanged and is deferred to future study. They also mention that high-dimensional issues might be addressed by learning an alternative metric (a learned distance) rather than relying on Euclidean distance.,"Develop a scalable variant of the locality sampler using streaming/online approximate nearest neighbors or minibatch-internal locality to avoid $O(N^2D)$ preprocessing, and provide complexity/accuracy trade-offs. Extend the method to settings with unknown/learned noise decomposition (separate epistemic vs aleatoric) and evaluate interval calibration (e.g., empirical coverage) across datasets. Test robustness under distribution shift and correlated data (time series, spatial data) where i.i.d. assumptions fail, potentially integrating with self-normalized importance weighting or conformal prediction. Provide a maintained software package (e.g., PyPI) with reference implementations for the sampler, HT weighting, and extrapolation architecture to ease adoption.",1906.03260v2,https://arxiv.org/pdf/1906.03260v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:48:46Z
FALSE,NA,Bayesian|Other,Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset),TRUE,MATLAB|Other,Not provided,http://openneuro.org/,"This paper proposes Template Independent Component Analysis (template ICA), an empirical Bayes hierarchical ICA framework for estimating subject-level resting-state brain networks from fMRI using population “template” priors (voxelwise mean and between-subject variance maps). The model treats template network spatial maps as latent variables with Gaussian priors centered on known population maps, while additional nuisance components are modeled with a mixture-of-Gaussians; estimation is developed via an EM algorithm, with an exact but intractable version and two practical approximations (subspace EM and a fast two-stage EM that estimates/removes nuisance ICs using Infomax). Performance is evaluated through extensive simulations (varying scan duration, nuisance components, and model-order misspecification) and through an experimental reliability study on Human Connectome Project data comparing against dual regression. The proposed algorithms yield substantially cleaner subject-level network estimates than dual regression, and the fast two-stage approach is computationally tractable while maintaining high accuracy. In the HCP reliability study, template ICA yields much higher intra-subject reliability than dual regression, reported as 75–259% higher weighted image ICC depending on component and scan duration.","Template ICA builds on probabilistic ICA: after SVD-based dimension reduction, subject data satisfy $y_i(v)=A_i s_i(v)+e_i(v)$ with Gaussian residuals. Template components follow a Gaussian prior $s_{i1}(v)=s_0(v)+\delta_i(v)$ with $\delta_i(v)\sim N(0,\Sigma_v)$ (diagonal voxelwise between-subject variances), while nuisance components follow a mixture-of-Gaussians; parameters are estimated by EM using posterior moments of $s_i(v)$ (and mixture states for nuisance ICs). Reliability in the HCP experiment is quantified by voxelwise ICC $\mathrm{ICC}(q,v)=\sigma^2_{\mathrm{bwn}}(q,v)/\sigma^2_{\mathrm{tot}}(q,v)$ and a weighted image ICC $\mathrm{wI2C2}(q)=\sum_v \lambda(q,v)\sigma^2_{\mathrm{bwn}}(q,v)\,/\,\sum_v \lambda(q,v)\sigma^2_{\mathrm{tot}}(q,v)$ with weights proportional to $|s_{0q}(v)|$.","In HCP-based test–retest analyses (two visits; scan durations 400–1200 volumes), template ICA produces markedly less noisy subject-level IC maps than dual regression and yields substantially higher within-subject reliability. For the longest scans ($T=1200$), template ICA achieves approximately 75% to 259% higher reliability (weighted image ICC) than dual regression across the 16 template ICs. Simulations show template ICA attains high correlations with true subject maps even for short scans (e.g., correlations >0.95 reported for short durations in several settings) and is robust to nuisance ICs; the fast two-stage EM is far faster than subspace EM while retaining similar template-IC accuracy. Under model-order misspecification for nuisance ICs, template-IC accuracy is largely preserved, with slight degradation mainly when nuisance order is underestimated.","The authors note that the exact EM algorithm is computationally infeasible due to enumerating all mixture-state configurations, and the approximate subspace EM is only feasible when the number of nuisance components is small (e.g., <15). They also acknowledge that the model assumes subject-level deviations from the template are independent across locations, despite real brain networks being spatially smooth, which may reduce efficiency and inflate variance estimates. They further mention that templates may be affected by site/batch effects when estimated from multi-site consortium data and that applicability across differing acquisition/processing protocols must be validated.","Reliability is evaluated primarily via test–retest ICC rather than downstream predictive/clinical utility, so improvements in reliability may not translate directly to better inference or biomarkers. The approach depends on the availability and representativeness of an external population template; population mismatch (age, disease status, scanner, preprocessing) could bias subject estimates toward inappropriate priors. The fast two-stage method relies on separate ICA (Infomax/GIFT) for nuisance removal; errors in nuisance estimation could leak into template estimates, and the method’s sensitivity to ICA algorithm choices and tuning is not fully characterized. No public implementation or reproducible pipeline is provided in the paper text, which limits practical adoption and independent verification.","The authors propose extending the template ICA model to incorporate spatial dependence/smoothness in subject deviations (e.g., via SPDE priors and Bayesian computation such as INLA) to improve efficiency and power. They also suggest developing hierarchical models for small-group or multi-group studies that leverage large external datasets either through empirical priors or as starting points for study-specific parameter estimation. They additionally plan validation work addressing site/batch effects and assessing how well templates transfer across differing acquisition and processing protocols, especially in consortium datasets.","A self-tuning or adaptive prior strategy (e.g., empirical Bayes shrinkage that varies by subject quality/SNR or by network) could reduce bias when templates are imperfect matches. Extending template ICA to multivariate/longitudinal settings with explicit within-subject temporal autocorrelation and motion/confound modeling could improve robustness for lower-quality clinical data. Benchmarking against additional subject-level reconstruction methods (e.g., GIG-ICA variants, modern ICA back-reconstruction approaches, or deep-learning-based denoisers) on multiple public datasets would strengthen generalizability claims. Providing an open-source reference implementation (e.g., a MATLAB/Python toolbox) with standardized preprocessing and template-building utilities would materially improve reproducibility and adoption.",1906.07294v1,https://arxiv.org/pdf/1906.07294v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:49:29Z
TRUE,Degradation modeling|Failure mode analysis|Maintenance optimization|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Other",Sensor/condition monitoring|Mixture of types,Condition-based|Predictive|Not applicable,Environmental monitoring,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,http://www.aqmd.gov/docs/default-source/aq-spec/field-evaluations/aeroqual-aqy-v0-5---field-evaluation.pdf?sfvrsn=4|https://www3.epa.gov/ttnamti1/files/networkplans/CASCAQMDPlan2010.pdf|https://www.aqmd.gov/home/air-quality/air-quality-data-studies/historical-data-by-year|http://www.dot.ca.gov/trafficops/census/docs/2016_aadt_volumes.pdf|http://pems.dot.ca.gov,"The paper proposes and validates a transparent management framework to deliver reliable ozone measurements from dense networks of low-cost gas-sensitive semiconductor (WO3) sensors by leveraging a hierarchical network with a smaller number of regulatory reference stations. Reliability is maintained via proxy-based monitoring: each low-cost sensor is compared against a chosen proxy (typically the closest reference station) using (i) a Kolmogorov–Smirnov (KS) distributional similarity test and (ii) a mean–variance (MV) moment-matching test that tracks drift in effective gain (slope) and offset (intercept). When alarms persist beyond a specified duration, a simple “semi-blind” recalibration is applied by updating slope and intercept so the sensor’s recent mean/variance match the proxy’s over a diurnally informative window. The framework is evaluated on two real deployments in Los Angeles (≈20 sensors) and the Inland Empire (≈45 sensors) with 10 co-locations at regulatory analyzers, showing effective drift detection/correction and improved bias versus uncorrected data. The work advances practical reliability management for low-cost sensor networks by emphasizing minimal site visits, explicit alarm rules, and proxy selection validated using reference-station cross-comparisons.","The measurement model assumes linearity between sensor output and true concentration: $X_{j,t}=a_0+a_1Y_{j,t}+\varepsilon_{j,t}$. MV moment-matching estimates calibration drift against proxy $Z_{k,t}$ over a window $(t-t_d:t)$ using $\hat a_1=\sqrt{\mathrm{var}\langle Z\rangle/\mathrm{var}\langle Y\rangle}$ and $\hat a_0=\mathrm{E}\langle Z\rangle-\hat a_1\mathrm{E}\langle Y\rangle$; alarms are based on bounds for $\hat a_0,\hat a_1$ plus a KS-test p-value $p_{KS}$ comparing empirical CDFs. Semi-blind correction applies $\hat X_{j,t}=\hat a_0+\hat a_1Y_{j,t}$ when alarms persist longer than $t_f$.","Using $t_d=3$ days and $t_f=5$ days with alarm thresholds $p_{KS}<0.05$, $0.7<\hat a_1<1.3$, and $-5\text{ ppb}<\hat a_0<5\text{ ppb}$, the closest-proximity reference station was the best-performing proxy among options tested (nearest station vs. network median vs. similar-AADT station) in terms of lower alarm time fraction, lower mean absolute bias (MAB), and higher $R^2$ when validating using only regulatory stations. For co-located low-cost sensors, framework correction kept MAB within roughly 2–8 ppb over seven months, whereas uncorrected MAB increased over time to roughly 6–16 ppb. At month 7, the root mean square deviation of corrected hourly data from regulatory instruments was reported as ±1.3 ppb (pooled across co-located sensors). Drift events were attributed in practice to inlet filter fouling that reduced airflow; the framework detected these via trends in $\hat a_0$ and $\hat a_1$ and corrected accordingly.","The authors note proxy selection can be challenging in regions with mixed land use and complex geography, and an imperfect proxy (e.g., sites with distinctly different ozone distributions such as RDLD) can trigger persistent alarms and cause overcorrection at low ozone concentrations. They also discuss that the KS test sensitivity depends on the distribution-window length and can flag small differences that may not be practically significant. They further caution that moment matching constrains only the first two moments, raising the question of whether local site information might be lost (though they argue it is largely retained in practice).","The framework relies strongly on (i) approximate linearity of sensor response and (ii) sufficiently similar diurnally-averaged distributions between proxy and target; performance may degrade under abrupt regime changes (meteorology, emissions) that alter correlations without sensor malfunction. Autocorrelation and spatial dependence are intrinsic to air-quality time series, yet the KS test and alarm persistence rules implicitly treat many observations as independent, which can affect false-alarm rates. The work focuses on ozone; cross-sensitivity and interference behavior for other gases or mixed-pollutant settings may require additional diagnostics beyond mean/variance matching. Although computations were done in R, no implementation details, reproducible pipeline, or parameter-tuning guidance (e.g., choosing $t_d,t_f$ per climate/season) are provided for practitioners deploying at scale.","The authors state that more work is required to clearly elucidate the underlying processes driving the observed fine-scale ozone patterns and hotspots revealed by the dense managed network. They also suggest the managed dense-network approach enables future ‘granularity’ studies, such as more detailed examination of correlations between urban design and microscale spatiotemporal air-quality variation.","A useful extension would be a formal, data-driven proxy-selection and switching strategy (e.g., dynamic proxy ensembles or Bayesian model averaging) to reduce overcorrection when the nearest site is not distributionally similar. Robust/self-starting variants that account for autocorrelation, missingness, and nonstationarity could improve false-alarm control and interpretability of alarms. Publishing open-source software (or an R package) implementing the end-to-end framework, including control-chart visualization and alarm triage, would materially improve uptake. Finally, extending the framework to multivariate pollutant settings (joint calibration using co-pollutant signals and meteorology) could better distinguish true local phenomena from sensor faults and interference.",1906.08421v1,https://arxiv.org/pdf/1906.08421v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:50:15Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Not provided,http://arxiv.org/abs/1802.00420|https://arxiv.org/abs/1712.04248|http://arxiv.org/abs/1608.04644|http://stanford.edu/~boyd/papers/scs.html|http://arxiv.org/abs/1707.04131|https://openreview.net/forum?id=rJzIBfZAb,"This paper proposes a new class of gradient-based adversarial attacks for evaluating the robustness of deep neural networks, designed to be both reliable under gradient masking and query-efficient. The method starts from an adversarial point far from the clean input and iteratively walks along the model’s decision boundary toward the closest adversarial example, solving a trust-region constrained optimization at each step. It supports multiple perturbation metrics ($L_0, L_1, L_2, L_\infty$) and both targeted and untargeted attack criteria, requiring minimal hyperparameter tuning (primarily a trust-region radius). The authors benchmark against common state-of-the-art attacks (PGD, Adam-PGD, C&W, DDN, EAD, JSMA, SparseFool) across six models spanning MNIST, CIFAR-10, and ImageNet (mostly defended models) using 1000 validation samples. Results show higher attack success (smaller median perturbations for $L_0/L_1/L_2$ and lower accuracy under fixed $\epsilon$ for $L_\infty$), improved query efficiency (often near convergence in ~10–20 queries for $L_2$), and greater robustness to suboptimal hyperparameters than baselines such as C&W and PGD.","The core step solves a trust-region constrained optimization: $\min_{\delta_k}\ \|x-\tilde{x}_{k-1}-\delta_k\|_p$ subject to box constraints $0\le \tilde{x}_{k-1}+\delta_k\le 1$, boundary linearization constraint $b_k^\top \delta_k=c_k$ with $b_k=\nabla_{\tilde{x}_{k-1}}\,\text{adv}(\tilde{x}_{k-1})$ and $c_k=\text{adv}(\tilde{x}_{k-1})$, and trust-region constraint $\|\delta_k\|_2^2\le r$. For targeted attacks, $\text{adv}(\tilde{x})=m(\tilde{x})_y-m(\tilde{x})_t$; for untargeted attacks, $\text{adv}(\tilde{x})=\min_{t\ne y}(m_y(\tilde{x})-m_t(\tilde{x}))$.","On Madry-MNIST (untargeted $L_2$), the proposed attack achieves median perturbation 1.15 versus 3.24 for C&W and 1.59 for DDN; in targeted $L_2$, 1.70 versus 4.79 (C&W) and 2.22 (DDN). For $L_\infty$ untargeted, reported accuracies drop to 49.1% (Ours) versus 60.1% (PGD) on Madry-MNIST and to 37.0% (Ours) versus 51.0% (PGD) on ResNet-50 at chosen $\epsilon$ thresholds. For $L_1$ (untargeted), Ours achieves 0.00377 median perturbation on Madry-MNIST compared to 0.01931 for EAD; for $L_0$ (untargeted), Ours achieves 0.07143 versus 0.22832 for SaliencyMap on Madry-MNIST (SparseFool/SaliencyMap often fail on defended models). Query-success curves indicate the proposed $L_2$ attack typically approaches convergence in ~10–20 queries, while C&W often requires hundreds of iterations; $L_\infty$ performance generally surpasses PGD/AdamPGD after ~10 queries. An ablation suggests performance degrades only modestly when reducing hyperparameter sweeps and repetitions, and the trust-region parameter is less sensitive than C&W’s learning-rate/tradeoff tuning.",None stated.,"The work is about adversarial robustness evaluation rather than reliability engineering (e.g., lifetime/degradation/maintenance), so reliability-specific validity and operational risk implications are not addressed. Empirical evaluation focuses on vision benchmarks (MNIST/CIFAR-10/ImageNet variants) and a fixed set of six models; generalization to other modalities (text, audio) or structured/tabular tasks is not demonstrated. The method assumes differentiability of the adversarial criterion and access to gradients (white-box or differentiable surrogate), which may not hold for non-differentiable preprocessing or strict black-box settings. Implementations are said to be forthcoming, but the paper as provided does not include a reproducible artifact or detailed solver code for the per-norm optimization routines.","The authors suggest extending the approach to other metrics (e.g., elastic net) as long as the trust-region optimization problem can be solved efficiently, and extending to other adversarial criteria beyond targeted/untargeted classification as long as the boundary can be expressed as a differentiable equality constraint.","Provide an open-source reference implementation (with exact per-norm solvers) and standardized evaluation scripts to improve reproducibility and adoption across toolchains. Extend the approach to settings with gradient-free constraints (true black-box) by coupling boundary-following with finite-difference or surrogate gradient estimation and quantify the query/compute trade-offs. Study robustness under correlated inputs or distribution shift (e.g., natural corruptions) and assess whether boundary-walking attacks remain reliable when model outputs are stochastic or randomized defenses are used. Develop theoretical guarantees or bounds on convergence to minimal adversarial perturbations under reasonable local boundary curvature assumptions.",1907.01003v2,https://arxiv.org/pdf/1907.01003v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:50:56Z
FALSE,NA,Other,Sensor/condition monitoring|Mixture of types|Other,Other,Transportation/logistics,Case study (real dataset)|Other,TRUE,Other,Not provided,http://its.map.baidu.com:8002/traffic/TrafficTileService?time=1527043432323&label=web2D&v=016&level=19&x=105113&y=27854|http://www.jiazhengblog.com/blog/2011/07/02/289/,"This paper proposes a method to optimize the start time of road maintenance operations to minimize traffic delay, using real-time traffic map data. It develops a workflow to (i) acquire real-time traffic status from commercial map tiles (e.g., Baidu/Google Maps), (ii) quantify categorical congestion colors into average speeds via field calibration, and (iii) convert speeds to traffic volumes using a practical S-shaped speed–flow model to handle cases where demand exceeds capacity (v/c>1). With estimated normal-road capacity and work-zone capacity (from the Highway Capacity Manual-style adjustment factors), total delay is computed via a queuing-theory-based parameter method including speed-change delays and queueing delay. Two case studies (Zurich E60 expressway and Shanghai Inner Ring elevated road) validate the traffic-status-to-speed mapping and the speed–flow conversion; the Shanghai example finds the best start time is approximately 9–11 pm, and reports an example total delay of 66,885 hours for an 8 pm start under assumed conditions. The contribution is an inexpensive, scalable way to obtain and accumulate traffic-volume inputs for classic work-zone delay models when direct detector/floating-car data are hard to access.","The method converts traffic status to speed by calibration, then uses speed–flow relations to infer volume. A classic parabolic model is given as $V = K_j\left(U - \frac{U^2}{U_0}\right)$ (Eq. 1), but the paper adopts a practical S-shaped model (Eq. 2) that expresses speed as $U=\frac{\alpha_1 U_s}{1+(V/C)^\beta}$ with $\beta=\alpha_2+\alpha_3(V/C)^3$ to cover $v/c>1$ demand. Normal-road capacity and work-zone capacity are estimated with multiplicative adjustment-factor formulas (Eqs. 4–5), and per-interval work-zone delay is computed by a queuing-theory-based formula set summarized in Table 3 (combining deceleration, running, acceleration, and queueing delays).","In the Zurich E60 validation, the classic speed–flow fit achieves $R^2=0.96$, with estimated capacity about 1577 pcu·(h·ln)$^{-1}$ and free-flow speed about 91 km/h. For E60, quantified speeds for traffic-status categories are about 57 (free), 44 (slow), 33 (congested), and 12 km/h (severely congested), and the practical model implies corresponding volumes (from the paper’s mapping) around 1453, 1593, 1687, and 1892 pcu·(h·ln)$^{-1}$. In the Shanghai case study, the work-zone capacity computed from factor adjustments is 1287 pcu·(h·ln)$^{-1}$; an example scenario with an 8 pm start and 8-hour duration yields total additional delay of 66,885 hours. Scanning start times shows the minimum total delay occurs when starting roughly between 9 pm and 11 pm.","The authors note that to improve accuracy, each road should ideally be calibrated for the mapping from traffic-status categories to speed; if using another road’s calibration, road conditions must be comparable. They also state that different map providers (or changes in the traffic map source) can lead to different status-to-speed relationships, requiring re-calibration. Additionally, they acknowledge larger error under the “free-flow” status because the method may conservatively assign a single volume level even when actual volume could be much lower.","The approach depends on proprietary, potentially changing map-tile rendering and color/RGB thresholds; small interface or styling changes could silently break data acquisition or classification without detection. The speed–volume conversion relies on a particular parametric speed–flow model and assumed/design parameters (e.g., $U_s$, $C$, $\alpha$’s), which may not generalize across weather, incidents, merging/weaving, or heterogeneous driver behavior. Delay estimation assumes simplified queuing inputs (e.g., arrivals uniform within $dt$ and capacity treated deterministically per interval), and uncertainty propagation (from status→speed→volume→delay) is not quantified. The “optimize by brute-force enumerating start times” strategy may be computationally fine, but it does not address operational constraints (crew availability, noise restrictions, safety windows) or multi-objective tradeoffs (cost, risk, service levels).","The paper’s outlook suggests that agencies should build data acquisition systems and decision-support modules leveraging ubiquitous real-time traffic maps as a low-cost data source, enabling better maintenance scheduling decisions, especially when historical maintenance data are incomplete. It implies broader deployment and integration into practical maintenance decision-making workflows.","A valuable extension would be to add robustness checks and automated monitoring for map API/tile-style changes (e.g., adaptive color clustering) and to quantify uncertainty in each conversion step with confidence intervals for delay and optimal start time. The method could be extended to incorporate recurrent and nonrecurrent factors (weather, incidents, special events) and to model autocorrelated arrivals rather than assuming uniformity within intervals. Another direction is multi-constraint optimization (crew shifts, allowable lane-closure periods, safety/visibility requirements) and multi-objective formulations balancing delay, cost, and risk. Publishing an open-source implementation (with modular support for multiple map providers) and validating across more cities/road classes would improve reproducibility and generalizability.",1907.03814v1,https://arxiv.org/pdf/1907.03814v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:51:37Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,"Parametric (Weibull, etc.)|ML-based|Bayesian|Hybrid/Ensemble",Complete lifetime data|Degradation measurements|Sensor/condition monitoring|Mixture of types,Predictive|Condition-based,Transportation/logistics,Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/MathiasKraus/PredictiveMaintenance,"The paper proposes an interpretable deep learning framework for remaining useful life (RUL) forecasting to support preemptive/predictive maintenance decisions. Its main contribution is a structured-effect neural network (SENN) that decomposes RUL into (i) an explicit probabilistic lifetime baseline (e.g., conditional Weibull/log-normal), (ii) a linear sensor-effect term for accountability, and (iii) a recurrent neural network term (LSTM) to capture residual nonlinear, history-dependent degradation patterns. Model parameters are estimated using variational Bayesian inference (maximizing the ELBO / minimizing KL divergence) to obtain posterior uncertainty for interpretable components, especially the linear sensor coefficients. Experiments on NASA’s CMAPSS turbofan engine degradation dataset (21 sensors, 200 engines) show SENN outperforms traditional ML baselines and distribution-only models; the best SENN (log-normal baseline + feature engineering in the linear component) achieves MAE 13.267, while a pure LSTM achieves MAE 11.188 but is less interpretable. The paper further demonstrates forecast decomposition and reports posterior coefficient estimates to rank sensor importance and quantify uncertainty, supporting accountable decision support for maintenance planning.","RUL is modeled as a conditional expectation of time-to-failure, with distribution baselines computed as $E[Y_t]=E_{Z\sim P}[Z\mid Z>t]-t$ and approximated via MCMC for Weibull/log-normal baselines. The proposed SENN decomposes predictions as $\text{SENN}_\Theta(t;X_t,\ldots,X_1)=\lambda(t)+\beta^\top X_t+\text{RNN}_\Theta(X_t,\ldots,X_1,t)$, where $\lambda(t)$ is the explicit lifetime-model baseline, $\beta^\top X_t$ is an interpretable linear sensor effect, and the RNN is an LSTM over recent history (e.g., last 50 timesteps). Parameters are learned with variational Bayes by maximizing the ELBO: $\lambda^*=\arg\max_\lambda \text{ELBO}(\lambda)$, yielding posterior distributions over baseline parameters and linear coefficients.","On the NASA CMAPSS turbofan dataset, the naive empirical RUL baseline yields MAE 45.060, while conditional expectation baselines achieve MAE 27.794 (Weibull) and 27.409 (log-normal). Best traditional ML baseline is random forest with feature engineering at MAE 17.793; the best SENN achieves MAE 13.267 (log-normal baseline + feature engineering), improving over the best traditional ML by about 25.44%. A pure LSTM attains the lowest MAE 11.188 (better accuracy but reduced interpretability). Variance-explained-by-component (relative to overall RUL variance) is reported as 0.175 (non-parametric baseline), 0.408 (linear component), and 0.064 (recurrent component), indicating most explanatory power is in interpretable components.","The authors note that variational Bayesian inference is computationally more expensive than simpler Bayesian approximations such as dropout-based approaches. They discuss dropout as a Bayesian approximation as having lower computational cost and fewer parameters, but reject it because it does not provide uncertainty estimates for coefficients in their structured model and does not allow incorporating prior information about coefficients.","Evaluation is limited to a single benchmark dataset (CMAPSS) with sanitized sensor semantics, so the practical interpretability of specific sensor effects in real deployments is not fully validated. The baseline component relies on parametric lifetime assumptions (Weibull/log-normal) and conditional expectation approximations (via MCMC), which may be sensitive to model mismatch and to censoring/maintenance regimes not represented in CMAPSS. The approach focuses on point-forecast MAE; operational decision quality (e.g., maintenance cost, downtime, risk constraints) is not directly optimized or evaluated, and calibration of predictive uncertainty for the overall SENN forecast is not thoroughly assessed.","The paper suggests considering dropout as a Bayesian approximation as a potential alternative direction, noting it can reduce computational cost and model complexity relative to variational Bayesian inference. They imply further work on more efficient uncertainty estimation approaches for neural networks while preserving the ability to encode priors and obtain uncertainty for interpretable coefficients.","A valuable extension would be to evaluate SENN across multiple real industrial datasets (with known sensor meanings) and under realistic censoring/maintenance policies, including right-censored and interval-censored lifetimes. Another direction is to integrate decision-aware objectives (e.g., cost-sensitive loss, maintenance scheduling optimization) and assess performance in terms of maintenance KPIs, not just MAE. Methodologically, developing robust/self-starting variants that handle distribution shift, autocorrelated sensor noise, and online updating (continual learning) would improve deployability in condition monitoring systems.",1907.05146v2,https://arxiv.org/pdf/1907.05146v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:52:09Z
FALSE,NA,"Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only,Not applicable,Network/cybersecurity|Other,Exact distribution theory|Approximation methods|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies ultra-reliable wireless communication (URLLC) in a C-RAN-enabled downlink cellular network, aiming at very low outage probabilities (e.g., “five 9’s” reliability). It analyzes an interference-limited scenario with multiple remote radio heads (RRHs) and proposes/compares coordination strategies: full interference (no coordination), silencing a subset of RRHs to reduce interference, and maximal ratio transmission (MRT) where cooperating RRHs jointly transmit to the user. The authors derive closed-form expressions/approximations for the SIR outage (CDF) under silencing and MRT (involving hypergeometric functions), and then study rate control subject to a target reliability constraint. Extensive numerical results are provided and validated via Monte Carlo simulation (reported as 10^7 runs), showing MRT achieves the ultra-reliable region with fewer cooperating RRHs and supports higher rates than silencing for the same reliability target. Overall, the contribution is in analytical reliability (outage) characterization and reliability-constrained rate design for coordinated wireless transmission rather than reliability engineering of physical components.","The SIR is modeled as $\mathrm{SIR}=h_0 d_0^{-\alpha}/I$ with Rayleigh fading $h\sim\mathrm{Exp}(1)$ and interference from non-cooperating RRHs. Under silencing, the SIR CDF is $F_{\mathrm{SIR}}^{S}(\theta)=1-(1+\delta^{\alpha}\theta)^{-(\eta-k)}$ (Eq. 2), where $k$ RRHs are coordinated/silenced and $\delta=d_0/d_j$. Under MRT, the SIR CDF is given in closed form using Gauss hypergeometric functions (Eq. 3), and reliability-constrained rate control uses $\theta=2^r-1$; for silencing this yields $r=\log_2\left(\left(\epsilon_{\mathrm{th}}^{-1/(\eta-k)}-1\right)/\delta^{\alpha}+1\right)$ (Eq. 4), while MRT rate is obtained numerically (Eq. 5).","Monte Carlo simulations with 10^7 runs corroborate the analytical SIR CDF/outage expressions for full interference, silencing, and MRT. The results show MRT produces a much smaller left-tail outage (higher reliability) than silencing for the same number of coordinated RRHs $k$, enabling operation in the “five 9’s” reliability region where silencing may not. Reliability and achievable rate both improve as the number of cooperating RRHs increases for both schemes, with MRT providing a substantially higher rate at a fixed target reliability than silencing. For an example configuration with $\eta=10$ RRHs and threshold $\theta=0.3$ dB, the paper reports that achieving five-9s reliability requires about $k_{\min}=8$ cooperating RRHs under MRT, while silencing would require at least as many (often more) resources.",The authors note that some simulated curves do not closely match the targeted reliability of $1-10^{-5}=0.99999$ (“five 9’s”) because accurately estimating such extreme tail probabilities would require longer simulations. They explicitly state that further work is needed to improve accuracy in the tail of the distribution.,"The work frames “reliability” as wireless-link outage probability rather than classical reliability engineering (lifetime/failure/maintenance), so it may not transfer to component/system reliability contexts. The analysis relies on simplifying assumptions for tractability (e.g., equidistant interferers, quasi-static independent Rayleigh fading, interference-limited regime, single-antenna UE, perfect/available CSI for MRT), which may limit realism and robustness to model mismatch (shadowing, correlated fading, noise-limited regimes, imperfect CSI, heterogeneous distances). The MRT CDF is restricted to $\delta<1$ and the reliability-constrained rate for MRT must be solved numerically, which may hinder practical closed-form design and sensitivity analysis.",They state that further work is required to improve Monte Carlo accuracy in the extreme tail (to better match very stringent reliability targets such as $10^{-5}$ outage).,"A natural extension is to incorporate imperfect CSI/estimation overhead and fronthaul/backhaul constraints in C-RAN coordination, quantifying the reliability/latency tradeoffs. Additional work could relax the equidistant-interferer and independent-fading assumptions (random spatial models, shadowing, correlated fading, inclusion of thermal noise) and provide robust/self-adaptive designs. Providing open-source implementation code (including numerical inversion/solvers for MRT rate) and validating on real channel measurements or system-level simulators would strengthen reproducibility and practical impact.",1907.07476v1,https://arxiv.org/pdf/1907.07476v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:52:44Z
FALSE,Other,ML-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Network/cybersecurity|Manufacturing (general)|Other,Simulation study|Case study (real dataset),TRUE,Python|Other,Not provided,https://doi.org/10.1109/BDCloud.2018.00136|https://doi.org/10.1145/3019612.3019661|https://doi.org/10.1109/BigDataCongress.2018.00012|https://doi.org/10.1109/TNNLS.2018.2886956|https://doi.org/10.1109/TNNLS.2017.2775225|https://doi.org/10.1109/ICDM.2006.66|https://doi.org/10.1109/BigDataCongress.2019.00033|https://arxiv.org/abs/1907.08120,"The paper proposes an unsupervised, self-evaluating methodology to detect predictive model degradation due to class-based concept drift, i.e., the arrival of samples from previously unseen class labels. It computes a baseline cluster-quality metric on the training set and periodically recomputes the same metric on newly predicted (unlabeled) data, comparing per-class “Silhouette curves” over time. Degradation per class is quantified using MAAPE between baseline and current Silhouette values, signed to indicate worsening vs improvement and weighted by the class frequency in the recent window; overall degradation is the sum over classes. Experiments on a large synthetic dataset (scikit-learn generated) and a real Wikipedia text dataset (Doc2Vec embeddings) show that degradation remains low pre-drift and increases markedly once unseen-class data is introduced, enabling automatic drift detection. The approach is positioned as scalable (MapReduce/Spark) and general across data types via appropriate distance measures.","Model degradation for class $c$ at time $t$ is defined as $\mathrm{DEG}(c,t)=\alpha\,\mathrm{MAAPE}(\mathrm{Sil}_{t_0},\mathrm{Sil}_{t})\cdot \frac{N_c}{N}$, where $\mathrm{Sil}_{t_0}$ is the baseline Silhouette on training data and $\mathrm{Sil}_{t}$ is the Silhouette on a trailing window of newly labeled/predicted data capped at the training size. The sign $\alpha$ is $+1$ if $\mathrm{Sil}_{t_0}\ge \mathrm{Sil}_{t}$ (degradation) and $-1$ otherwise (improvement). Overall model degradation is $\sum_{c\in C} \mathrm{DEG}(c,t)$.","On the synthetic dataset (D1), overall MAAPE degradation stays below 5% for time windows containing only known classes (t1–t4) and becomes consistently above 10% once unseen-class samples begin arriving (from t5), with degradation increasing as the proportion of unseen-class data increases. On the Wikipedia dataset (D2), overall MAAPE is below 15% up to t4, then rises above 28% at t5 when an unknown class starts arriving. The method detects drift even when the unseen class constitutes only ~20% of the added data. A Random Forest classifier achieved average f-measure 0.964 (D1) and 0.934 (D2) under 3-fold CV (reported as baseline predictive performance, not the unsupervised drift metric).","The paper states that selecting/assessing degradation thresholds for triggering retraining depends on domain expectations, risks/costs, and dataset characteristics, and that evaluating appropriate thresholds is out of scope. It also notes that the self-evaluation triggering mechanism is currently heuristic (e.g., based on a percentage increase in class counts) and is not fully optimized/validated. The work is presented with preliminary experiments rather than a comprehensive benchmarking study.","The method relies on distance computations needed for Silhouette (even if a scalable variant is used), which can be sensitive to feature scaling, distance choice, and high-dimensional effects; robustness to these choices is not thoroughly analyzed. It assumes predicted labels on new data are meaningful enough to compute within-class cohesion/separation; heavy misclassification could distort Silhouette-based signals and confound drift vs model error. Comparisons against established drift detectors are not provided, making it hard to attribute gains or understand relative detection delay/false-alarm rates. The approach targets class-based drift (novel/unseen classes) and may not detect other drift types (e.g., covariate shift without new classes) as effectively.","The authors propose (i) comparing against state-of-the-art techniques with a focus on drift recognition efficiency, (ii) introducing alternative unsupervised metrics beyond Silhouette, (iii) improving the self-evaluation triggering mechanism (currently based on percentage of new data), and (iv) conducting further experiments on additional real-world datasets with known concept drifts.","A natural extension is to quantify detection performance with formal metrics (false alarm rate, detection delay) and to study calibration of retraining thresholds under cost/risk models. Robust/self-starting variants that handle autocorrelation, feature drift, or changing class priors without relying on predicted labels (e.g., density-ratio or two-sample tests in representation space) could broaden applicability beyond class-based drift. Providing an open-source Spark/Python implementation and reproducible pipelines (including distance choices and scaling) would improve adoption and facilitate fair benchmarking. Exploring multivariate monitoring of per-class degradation trajectories and integrating uncertainty estimates (e.g., Bayesian or bootstrap intervals for degradation) could make triggers more reliable in practice.",1907.08120v1,https://arxiv.org/pdf/1907.08120v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:53:20Z
TRUE,Degradation modeling|RUL prediction,"Parametric (Weibull, etc.)|ML-based|Bayesian|Other",Degradation measurements|Mixture of types,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,MATLAB,Not provided,NA,"The paper proposes a five-parameter, interpretable sigmoidal (linear + logistic) parametric regression model for long-term lithium-ion battery capacity fade under cyclic aging, aimed at capturing the observed two-bend (S-shaped) degradation trajectory seen in extended experiments. A Gaussian-error nonlinear regression framework is used with maximum likelihood/nonlinear least squares estimation, leveraging conditional linearity to reduce optimization to two nonlinear parameters and using MATLAB global/local optimization routines. The authors derive asymptotic pointwise confidence and prediction intervals for capacity at a given cycle count and also provide parametric bootstrap confidence/prediction intervals. The proposed model is compared against common alternatives (double exponential, polynomial, and mixture models) and shown to fit long-term sigmoidal degradation substantially better, particularly for extrapolation beyond the first bend, while remaining competitive in short-term (single-bend) regimes. Cross-validation on real multi-cell cycling datasets demonstrates the approach for end-of-life (capacity-threshold) lifetime prediction and shows that including at least one fully observed degradation trajectory can stabilize long-term predictions when most training data are right-censored at high remaining capacity.","The proposed shifted sigmoidal degradation model is
\[f(x;\beta)=\beta_1-\beta_2 x-\frac{\beta_3}{1+\exp(-(x-\beta_4)/\beta_5)}+\frac{\beta_3}{1+\exp(\beta_4/\beta_5)},\; x\ge 0,\; \beta\in(0,\infty)^5.\]
Measurements follow an additive-error regression model \(y_j=f(x_j;\beta)+\varepsilon_j\) with i.i.d. \(\varepsilon_j\sim N(0,\sigma^2)\), and parameters are estimated by constrained nonlinear least squares/MLE. Lifetime to a capacity threshold \(y_q=q\,y_{init}\) is obtained by inversion \(\hat x_q=f^{-1}(y_q;\hat\beta)\).","Model-comparison plots on three example batteries (A from Baumhöfer et al. 2014; B from Harris et al. 2017; C from Lewerenz et al. 2017) show that double-exponential, polynomial, and mixture models can fit data up to the first bend but deviate strongly when a second bend appears, whereas the sigmoidal model fits the full long-term path. In cross-validation on the 48-cell dataset from Baumhöfer et al., average lifetime-prediction errors for EoL thresholds around 80–85% are on the order of RMSE \(\approx 0.108–0.112\) (in units of 1000 cycles) with full data, and RMSE \(\approx 0.133–0.153\) when most training curves are censored but one complete curve is included. On the 24-cell dataset from Harris et al., the sigmoidal and double-exponential models yield essentially identical prediction performance (e.g., for 85% EoL and 75% training size, both report MSE \(\approx 0.011\), RMSE \(\approx 0.104\)). Asymptotic and bootstrap pointwise confidence/prediction bands are reported to be nearly identical in the examples shown.","The paper notes that extrapolation using the fitted curve is not recommended and can be unstable outside the measurement range unless the available data extend beyond the inflection point. It also states that the asymptotic and bootstrap interval procedures rely on the validity of the regression assumptions (additive i.i.d. Gaussian errors with constant variance) and may worsen if at least one assumption is violated. The authors highlight that when training data are heavily right-censored (e.g., stopping near 70–80% capacity) and do not include curvature changes, prediction of lower-threshold EoL becomes unstable unless at least one full degradation path is available.","The error model assumes independence and homoscedastic Gaussian noise, but cycling capacity data often exhibit autocorrelation over cycles and heteroscedasticity across the degradation trajectory; robustness to these violations is not explored. The approach is purely parametric and primarily empirical (linear+logistic form); while interpretable, it is not explicitly linked to operating conditions (temperature, C-rate, DoD) or cell-to-cell random effects, limiting transferability across regimes without retraining. The optimization strategy (GA + simulated annealing + local NLS) may be computationally heavy and sensitive to tuning; no runtime/scalability discussion or convergence diagnostics are provided. Uncertainty quantification for inverted lifetime estimates \(\hat x_q\) (EoL cycle count) is not fully developed (e.g., direct confidence intervals for \(x_q\) via delta method/bootstrapping the inverse), despite being central to RUL use cases.",None stated.,"Extend the model to incorporate random effects/hierarchical structure to explicitly model cell-to-cell variability (and potentially batch effects) while sharing information across cells. Relax the i.i.d. Gaussian error assumptions by modeling autocorrelated and/or heteroscedastic residuals (e.g., GLS or state-space formulations) and assess robustness under misspecification. Develop direct uncertainty intervals for EoL/RUL predictions (for \(x_q\)) and decision-oriented metrics (coverage, calibration) under censoring. Incorporate stress covariates (temperature, C-rate, depth of discharge) to enable extrapolation across operating conditions and improve applicability for second-life screening and qualification.",1907.12961v1,https://arxiv.org/pdf/1907.12961v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:54:02Z
TRUE,Failure mode analysis|Maintenance optimization|Other,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Energy/utilities,Simulation study|Other,TRUE,MATLAB,Not provided,http://www.iclr.cc/doku.php|http://www.dlworkshop.org/,"The paper proposes a data-driven PHM approach to improve anomaly detection for heavy-duty industrial gas turbine combustors using exhaust gas temperature (thermocouple) profiles. It uses unsupervised deep feature learning via a stacked denoising autoencoder (SDAE) to learn representations from event-free sensor data, then feeds the learned features into an Extreme Learning Machine (ELM) classifier to detect combustor abnormality events. The main contribution is replacing manual, knowledge-driven feature engineering with learned features that are intended to be more robust to noise and better capture complex relationships among multiple thermocouples. Performance is evaluated on real plant data using repeated 5-fold cross-validation and ROC curves, comparing learned features against a 12-feature handcrafted baseline. The learned-feature approach shows higher detection performance and lower variability; at 1% false positive rate, mean true positive rate is reported as about 0.99±0.01 versus 0.96±0.02 for handcrafted features, suggesting improved early fault/anomaly detection capability for condition monitoring of combustors.","The SDAE/DAE feature learner is defined by an encoder $h(x)=s_f(Wx+b_h)$ and decoder $y=s_g(W'h+b_y)$ trained to minimize reconstruction loss over training data: $J_{AE}(\theta)=\sum_{x\in D} L(x, g(f(x)))$ with either squared error $L(x,y)=\sum_i (x_i-y_i)^2$ or cross-entropy $L(x,y)=-\sum_i[x_i\log y_i+(1-x_i)\log(1-y_i)]$, optionally with weight decay $\lambda\sum_{ij} W_{ij}^2$. Denoising is introduced by corrupting inputs (e.g., masking noise, Gaussian noise, salt-and-pepper) and training to reconstruct the clean $x$ from corrupted $\tilde{x}$. The classifier is an Extreme Learning Machine where input-to-hidden weights are randomly fixed and only hidden-to-output weights are solved via linear least squares / generalized inverse of the hidden-layer output matrix.","Using real turbine data (27 thermocouples; 13,791 event-free pre-event samples for unsupervised learning; 300 event samples; 47,575 post-event samples), the authors compare an ELM classifier using 12 handcrafted statistical features vs. 12 SDAE-learned features. Evaluation uses ROC curves with 5-fold cross-validation repeated 10 times. The SDAE+ELM approach produces consistently better ROC curves and less run-to-run variation than handcrafted features. At a false positive rate of 1%, the reported mean±std true positive rate is approximately 0.99±0.01 (learned features) versus 0.96±0.02 (handcrafted).","The authors note that their study is an initial exploration of deep learning for PHM and imply the need for broader validation. They also indicate they only demonstrate the approach on a limited real-world dataset/time window (several months for one turbine) and frame more thorough study as future work. No other explicit methodological limitations (e.g., assumptions, failure modes not covered) are clearly stated.","The evaluation appears limited to data from a single turbine and a small number of labeled events (10 events; 300 event samples), so generalization across turbine frames, ambient/fuel variations, and aging regimes is unclear. The approach is framed as point-wise classification on temperature profiles; temporal dependence and event lead-time/early-warning performance (e.g., time-to-detect, detection delay) are not quantified. Details on label quality/ground truth definition for “events” and how close samples are to incipient fault onset are limited, which can materially affect ROC results. Comparisons are only against one handcrafted-feature baseline with the same ELM classifier; stronger baselines (e.g., one-class methods, sequence models, calibrated probabilistic detectors) and operational thresholding/cost tradeoffs are not fully explored.",The authors state they plan to conduct more thorough studies using more real-world data. They also state interest in exploring other deep learning methods beyond SDAE for combustor anomaly detection and for other PHM applications.,"Validate the approach across multiple turbines/frames and operating regimes with rigorous train/test separation by unit and time to assess true generalization and prevent leakage. Extend the method to explicitly model time dynamics (e.g., temporal autoencoders, RNN/TCN-based encoders) and evaluate operational metrics such as detection delay and early-warning horizon. Develop robust/self-starting or domain-adaptation variants to handle changing ambient conditions, fuel type, and equipment aging, and study calibration so output probabilities map to actionable risk levels. Provide reproducible implementations and sensitivity analyses for SDAE/ELM hyperparameters (noise rate, layer sizes, class weighting) and compare against additional PHM anomaly detection baselines (e.g., isolation forest, one-class SVM, autoencoder reconstruction-error detectors, change-point methods).",1908.09238v1,https://arxiv.org/pdf/1908.09238v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:54:36Z
TRUE,Life distribution modeling|Degradation modeling|RUL prediction|Accelerated testing|Maintenance optimization|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Stochastic process|Bayesian|Simulation-based|Other",Right-censored|Degradation measurements|Event/count data|Sensor/condition monitoring|Mixture of types,Imperfect maintenance|Not applicable,Manufacturing (general)|Transportation/logistics|Energy/utilities|Other,Other,TRUE,None / Not applicable,Not provided,NA,"This chapter surveys modern statistical methods for reliability analysis when rich, time-varying (dynamic) covariates are available from sensors/connected products, covering lifetime (time-to-event), degradation, and recurrent-event settings, plus modern test planning. For failure-time data with dynamic covariates, it presents a cumulative exposure/damage model where failure occurs when accumulated exposure $u(t)=\int_0^t\exp(\beta x(s))ds$ reaches a random threshold, with Weibull (or other log-location-scale) baseline for the exposure threshold and likelihood-based estimation under censoring. For degradation with dynamic covariates, it describes a general path model in cumulative-damage form using shape-restricted spline covariate-effect functions and random effects, along with parametric time-series modeling of the covariate processes to enable reliability prediction via Monte Carlo simulation. For recurrent events in repairable systems, it outlines a multi-level trend-renewal process (MTRP/HMTRP) framework incorporating dynamic covariates and random effects, fit via Bayesian MCMC, and used to predict future event counts. For test planning, it introduces sequential Bayesian design for accelerated life tests (ALTs) that selects the next stress level by minimizing posterior-expected (weighted) asymptotic variance of a life quantile under multiple use conditions.","Time-to-event with dynamic covariates uses cumulative exposure $u(t)=\int_0^t\exp[\beta x(s)]ds$ and assumes failure when $U=u(T)$ (Eq. 1), giving $F(t)=F_0(u[t;\beta,x(t)];\theta_0)$ and $f(t)=\exp[\beta x(t)]f_0(u[t;\beta,x(t)];\theta_0)$ with Weibull (sev) baseline for $U$. Degradation modeling uses an additive general path model $y_i(t)=D[t;x_i(t)]+G(t;w_i)+\epsilon_i(t)$ with cumulative covariate effects $D[t;x(t)]=\beta_0+\sum_l\int_0^t f_l(x_l(u);\beta_l)du$ and random effects $G(t;w_i)=w_{0i}+w_{1i}t$. Recurrent events use an MTRP/HMTRP intensity for component events (Eqs. 17–20), with baseline intensity extended by dynamic covariates and random effects, e.g. $\lambda_i(t)=\lambda_b(t)\exp\{\gamma\log X_i(t)+w_i\}$. Sequential Bayesian ALT design chooses the next standardized stress $q^*_{n+1}=\arg\min_{q\in[q_L,q_U]}\phi(q)$ where $\phi(q)$ is the posterior expectation of a weighted asymptotic variance criterion (Eqs. 27–28).","For the Product D2 field-failure example (1800 units; 69 failures over 70 weeks), the fitted dynamic-covariate model yields higher predicted remaining-life risk for units with higher and increasing use-rate trajectories, and enables prediction intervals for cumulative future failures in the risk set (1731 surviving units at DFD). For the NIST outdoor coating study (36 specimens over ~5 years with UV/RH/temperature covariates), the degradation model plus simulated covariate paths produces an estimated failure-time CDF (soft-failure threshold example $D_f=-0.4$) with pointwise 95% CIs, showing most failures for a stated starting-window population occurring roughly between 50 and 150 days in service. For the Vehicle B recurrent-event fleet (203 units, 219 component and 44 subsystem events over 110 months), the Bayesian MTRP backtest predicts held-out 15-month cumulative component events closely, with observations lying within pointwise prediction intervals; a forward 30-month forecast gives a 95% prediction interval of approximately 62 to 90 component events. For sequential Bayesian ALT planning in composite fatigue testing (E-glass, 14 historical observations with censoring), simulations show the SBD concentrates allocations at boundary stress levels (e.g., 0.35 and 0.75 standardized) and dramatically improves efficiency versus local c-optimal design (mean asymptotic variance 0.6048 vs 4.0337 in 100 simulations).",None stated.,"As a review/chapter, the work does not provide comprehensive, standardized benchmarking across competing modern methods (e.g., deep learning/modern state-space models) under controlled scenarios; performance evidence is mainly via selected examples from cited papers. Many models assume conditional independence structures and specific parametric forms (e.g., Weibull/log-location-scale thresholds, Gaussian random effects, VAR covariate dynamics) that may be sensitive to misspecification and may require diagnostic tools not detailed here. Implementation details (software, convergence checks for MCMC/optimization, and computational cost) are not provided, which can be a barrier for practitioners wanting to reproduce the analyses.","The chapter notes that increasingly versatile data types (spatial, functional, image, and text data) offer strong potential for reliability modeling, and that new statistical and machine-learning methods should be adapted and integrated with reliability domain knowledge to better exploit such data for improved modeling and prediction.","Develop robust/self-starting versions of the dynamic-covariate reliability models that explicitly handle covariate measurement error, missingness, and irregular sampling typical of field sensor data. Extend the presented approaches to multivariate/high-dimensional settings (many sensors) with regularization and to autocorrelated/clustered units (fleet, hierarchical) with scalable inference. Provide open-source reference implementations and reproducible case studies to facilitate adoption and enable head-to-head comparisons on shared benchmark datasets.",1908.09729v1,https://arxiv.org/pdf/1908.09729v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:55:17Z
FALSE,Other,Bayesian|ML-based|Hybrid/Ensemble|Other,Sensor/condition monitoring|Other,Condition-based|Not applicable,Environmental monitoring,Other,FALSE,None / Not applicable,Not applicable (No code used),https://airqualityegg.com/home|http://cogdem.org.uk/newsite/?page=Industrial,"The paper is a perspective/discussion article on the plausibility and trustworthiness of measurements from low-cost sensor networks, especially for urban air-quality monitoring. It frames “reliable data” as a Bayesian/measurement-theory problem in which confidence depends on explicit observation and restitution models, calibration assumptions, and the availability of independent evidence (e.g., reference stations, satellite data, meteorology, dispersion/LUR models). It highlights how drift, cross-sensitivities, hidden preprocessing, and proprietary/black-box machine-learning models embedded in devices can prevent independent assessment of uncertainty and thus reduce believability. The article surveys classes of ‘blind’ or ‘semi-blind’ calibration approaches (e.g., consensus/PCA methods, rendezvous calibration, moment matching, chain co-location, hierarchical networks with proxies/kriging) and emphasizes that their assumptions can be restrictive and hard to verify. Overall, it argues that practical reliability for low-cost networks requires transparency to raw data and models and often a hierarchical design that includes well-maintained reference instruments plus data-fusion methods.

This work is adjacent to reliability engineering (data reliability/validity in sensing systems) but does not develop or evaluate classical reliability models of failure, lifetime, degradation, or maintenance optimization; it focuses on measurement uncertainty, calibration, and plausibility of sensor-derived data products.","The paper formulates the measurement/restitution problem probabilistically: observation model $\mathbb{P}(y\mid x,\theta)$ and measurement (inverse) model $\mathbb{P}(x\mid y,\theta)$. Restitution integrates over uncertainty in instrument parameters and information $\mathcal{I}$: $\mathbb{P}(x\mid y)=\int \mathbb{P}(x\mid y,\theta)\,\mathbb{P}(\theta)\,d\theta=\int \mathbb{P}(x\mid y,\theta)\,\mathbb{P}(\theta\mid\mathcal{I})\,\mathbb{P}(\mathcal{I})\,d\mathcal{I}$. These expressions are used to argue that independent information (reference data, domain models, covariates) can constrain $\mathbb{P}(\theta)$ and improve plausibility without frequent on-site calibration.","No control limits/ARL-style quantitative results are reported; the paper is primarily conceptual and survey-like. Key qualitative conclusions are that (i) conventional calibration is often infeasible for dense low-cost networks, (ii) data plausibility depends on explicit, transparent measurement/restitution models and independent evidence, and (iii) proprietary embedded ML models hinder uncertainty quantification and independent verification. It also argues (with air-quality sensor examples) that co-location with reference instruments and hierarchical network designs provide practical routes to maintaining believable data, while many blind/semi-blind calibration methods rely on restrictive assumptions that may be costly to validate in practice.","The author notes that a fully rigorous measurement-theory/Bayesian implementation may be computationally too demanding for high-resolution networks and may require substantial historical data. The paper also emphasizes that in practice the needed independent information (e.g., full interference PDFs, validated models) may not be available, and that increasing model complexity raises computational burden and reduces transparency of assumptions. It further highlights that proprietary/hidden algorithms and lack of access to raw data prevent independent plausibility assessment.","Because this is a narrative/perspective piece, it does not provide a systematic experimental evaluation (e.g., benchmark datasets, standardized metrics) comparing the listed calibration/data-fusion approaches under controlled conditions. The discussion treats “reliability” largely as plausibility/uncertainty of measurements rather than reliability engineering notions such as failure rate, MTBF, sensor survival, or maintenance cost optimization across fleets. Practical implementation guidance is limited (e.g., concrete algorithms, parameter choices, scalability/latency constraints, and governance for transparency/QA). The article also does not formalize decision thresholds for “useful enough” uncertainty (fitness-for-purpose criteria) beyond qualitative framing.","The paper suggests that the formal measurement-theory/Bayesian framework needs to be developed for networks (not just single instruments), including how to treat both individual nodes and spatial patterns revealed by the network. It also argues that resolving plausibility conflicts will require substantial effort in hierarchical use of low-cost sensors with reference networks plus models/supplementary data, and that transparency to raw data and restitution models must be achieved despite proprietary constraints.","A valuable extension would be to define and validate operational reliability/quality metrics for low-cost sensor networks (e.g., time-varying calibration health indicators, drift detection rates, false-alarm rates) and tie them to decision risk. Developing standardized open benchmarks (raw sensor streams + reference + meteorology) would enable fair comparisons of blind/semi-blind calibration and ML approaches. Incorporating explicit sensor fault/degradation models (e.g., filter clogging, sensitivity loss) and optimizing maintenance/verification schedules at fleet scale would connect plausibility with lifecycle reliability engineering. Finally, providing open-source reference implementations of the proposed data-fusion/restitution pipelines would improve reproducibility and adoption.",1908.10928v1,https://arxiv.org/pdf/1908.10928v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:55:52Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Mixture of types|Simulated only|Other,Not applicable,Other|Theoretical/simulation only,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/pmandros/wodiscovery,"This paper proposes a corrected-for-chance, consistent estimator of normalized total correlation to reliably measure multivariate correlation in categorical data without distributional assumptions. The method subtracts a conservative correction term derived from a permutation-model expectation (bounded and relaxed to be efficiently computable) from the plug-in normalized total correlation, yielding an estimator intended to avoid chance-inflated scores in sparse/high-dimensional settings. For discovering the top-k most correlated variable subsets, the authors develop an algorithmic framework with exact (branch-and-bound), approximate (via an 1 approximation factor), and greedy heuristic search, enabled by tight admissible bounds under a low-entropy extension ordering. Empirical evaluation includes synthetic simulations (regret curves across sample sizes and correlation regimes) and many real benchmark datasets to demonstrate efficiency and near-optimality of the search procedures. Case studies on Tic-tac-toe and European land mammals show the framework can recover interpretable correlated sets.","The normalized total correlation is defined as $w(\mathbf{X}) = W(\mathbf{X})/\bar W(\mathbf{X})$, where $W(\mathbf{X})=\sum_{X\in\mathbf{X}}H(X)-H(\mathbf{X})$ and $\bar W(\mathbf{X})=\sum_{X\in\mathbf{X}}H(X)-\max_{X\in\mathbf{X}}H(X)$, yielding $w\in[0,1]$. The corrected-for-chance estimator is $\hat w^{\,\bar\bar 0}(\mathbf{X})=\hat w(\mathbf{X})-\bar\bar t_0(\mathbf{X},n)$, where $\bar\bar t_0$ is a conservative maximized correction over variable orderings using the relaxed bound $\bar\bar m_0(X_{i-1},X_i,n)=\log\!\left(\frac{n+\left(\prod_{X\in X_{i-1}}S_X\right)S_{X_i}}{n-1}\right)$ (up to the papers notation) and the maximizing order is achieved by sorting variables by decreasing domain size.","On synthetic data, the corrected estimators ($\hat w^0$, $\hat w^{\bar 0}$, and the efficient $\hat w^{\bar\bar 0}$) yield substantially lower regret than the plug-in $\hat w$, with the paper reporting up to about a 5 reduction for some sample sizes and faster convergence toward zero regret. The efficient estimator $\hat w^{\bar\bar 0}$ performs on par with the less efficient corrected versions despite relaxations used to compute the correction term. On 49 KEEL benchmark datasets, the branch-and-bound (BNB) method finds the exact optimum (\u03b1=1) for 42/49 datasets within 30 minutes and averages about 77 seconds runtime, with very high pruning rates (often ~99.99%) on many high-dimensional datasets. The GREEDY heuristic typically runs in a few seconds on average and produces solutions that are almost always identical to BNB, with only negligible differences on a couple of datasets.","The authors note that optimizing the objective is unlikely to admit a polynomial-time algorithm and that the exact complexity (e.g., NP-hardness) of optimizing their estimator remains open. They also describe an artifact for small sample sizes in very low-dimensional dependency settings (e.g., d=2 in their synthetic setup) where the domain-size-product penalty can cause the efficient estimator $\hat w^{\bar\bar 0}$ to miss the dependent variables under certain sampling sparsity patterns. They further mention that randomly sampling distributions with very high normalized total correlation becomes difficult for higher dimensionalities, and that such regimes are less challenging for estimators.","The estimator and search framework depend on i.i.d. sampling and accurate plug-in entropy estimates; robustness to dependence (temporal/spatial autocorrelation) or strong sampling bias is not analyzed. The correction relies on conservative bounds/relaxations and assumes (for the relaxation) strictly positive distributions / product-form joint domain size reasoning, which may be mismatched in sparse real datasets with structural zeros. Comparisons are largely against variants of their own estimators rather than a broader set of alternative multivariate dependence measures (e.g., interaction information variants, log-linear modeling, Bayesian network scoring, or modern multivariate MI estimators) for the same discovery task. Practical guidance on choosing k, handling multiple testing/false discovery across many searched subsets, and interpretability/diagnostics beyond reporting top sets is limited.","They suggest developing similar discovery frameworks using other estimators (including parametric corrections) and designing efficient algorithms for those alternatives. They propose extending to conditional normalized total correlation to discover correlated sets given control variables (e.g., for fairness) and to enable iterative control using previously found top results to promote diversity. On the algorithmic side, they propose proving NP-hardness for their optimization problem and exploring newer algorithmic approaches for computing entropic measures to scale to larger k.","Extending the estimator and bounds to handle unknown/estimated discretization effects, structural zeros, and sparse high-cardinality variables more robustly (e.g., Bayesian smoothing or shrinkage for entropies) would strengthen reliability in practice. Developing statistically principled selection procedures for top-k (e.g., stability selection, FDR control over discovered subsets) would help prevent overinterpretation when searching huge hypothesis spaces. Adapting the approach to non-i.i.d. data (time series, spatial grids) via block permutation/null models or explicit dependence modeling would broaden applicability. Providing a maintained software package (e.g., PyPI/CRAN) with reproducible pipelines and benchmarks would improve adoption and allow standardized comparisons.",1908.11682v1,https://arxiv.org/pdf/1908.11682v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:56:37Z
TRUE,Degradation modeling|RUL prediction|Network/infrastructure reliability|Other,"ML-based|Parametric (Weibull, etc.)|Other",Sensor/condition monitoring|Mixture of types|Other,Not applicable,Energy/utilities|Other,Case study (real dataset)|Other,TRUE,MATLAB|Other,Not provided,www.Ep.gov/nrmrl,"The paper develops data-driven models to predict the remaining useful life (RUL) of buried water distribution pipes, defining RUL as the time remaining before a structural failure such as a pipe break. It trains and tests Artificial Neural Network (ANN) and Adaptive Neuro-Fuzzy Inference System (ANFIS) models using municipal field datasets containing pipe age, installation year, length, diameter, material, wall thickness loss, and number of breaks. The study also fits separate non-linear regression deterioration equations by pipe material (cast iron, ductile iron, asbestos-cement, steel) and reports their coefficients of determination. Results indicate that pipe age and wall thickness loss are the most influential predictors of RUL; ANFIS also highlights installation year as important. A key practical takeaway reported is that roughly a 10% wall thickness loss corresponds on average to about a 50% reduction in remaining useful life for the pipe materials studied.",The paper provides material-specific non-linear regression deterioration models where RUL $Y$ is a function of pipe age $A$ and wall thickness loss $W$: for cast iron $Y=-0.342A^2+0.0548W+48.163$; for ductile iron $Y=0.004A^3-0.025W^2+0.11AW+51$; for asbestos-cement $Y=0.0038A^2-0.49W+195.92$; for steel $Y=0.005A^3-0.012W^2-0.989AW-0.012$. The ANN models are standard feed-forward networks with one hidden layer (varying neuron counts) trained on a random split (75% train/10% validation/15% test) to map the selected inputs to RUL. The ANFIS model uses a Gaussian membership function and hybrid learning (backpropagation plus least-squares) to learn the input–output mapping.,"Using ANN, the best-performing model’s predicted vs. estimated RUL fit is reported with a line approximately $y=0.9112x+3.7663$ and $R^2\approx 0.89$. For one ANN configuration (ANN1), reported errors include training MAE 0.17 and MAPE 1.076, validation MAE 1.304 and MAPE 8.047, and testing MAE 0.88 and MAPE 5.431 (with RRSE values near 0.001–0.012 as tabulated). The paper concludes that age and wall thickness loss are the most significant variables across statistical analysis and ANN/ANFIS; ANFIS also identifies installation year as influential. It reports an average rule-of-thumb finding that about 10% wall thickness loss corresponds to roughly a 50% reduction in RUL across cast iron, ductile iron, asbestos-cement, and steel pipes. The fitted non-linear regression equations achieve $R^2$ values of 0.73–0.80 depending on material (CI 0.78, DI 0.74, AC 0.80, steel 0.73).","The authors state that limited deterioration parameters and limited data availability reduced the effectiveness of neural network training and constituted the main limitation of the work. They note that several environmental/operational parameters (e.g., overburden pressure, soil type/properties, groundwater table location, water pressure, installation depth, temperature, corrosive conditions) were omitted due to lack of monitoring data. They also state the developed model is not complete because it does not include several parameters thought to be important for water deterioration. They further note the model does not apply to new pipes with age zero and no wall thickness loss.","Model validation appears to rely largely on a random train/validation/test split across pooled municipal datasets; if pipes from the same municipality (or time period) occur in both train and test sets, performance may be optimistic due to site-specific correlations and potential leakage. The paper does not provide enough implementation detail to ensure reproducibility (network architecture/training settings, ANFIS rule base size, preprocessing/encoding choices), nor does it report uncertainty quantification (prediction intervals), which is important for maintenance and risk decisions. RUL is treated as a point target derived from available records rather than modeled with explicit censoring/inspection bias; survivorship and missing-not-at-random effects in break histories could distort learned relationships. Comparisons are limited (no benchmarking against common survival/ROF or stochastic-process degradation models on the same split), so it is hard to attribute gains specifically to ANN/ANFIS.","The authors state that further work in data collection and model development is required to confirm the model is more precise and reliable for future applications. They explicitly suggest incorporating additional detailed parameters such as soil parameters, water-table location/fluctuation, joint condition, leakage history, water pressure, installation depth, temperature, and water corrosive conditions to better model deterioration and more precisely predict RUL. They also suggest combining the ANN/ANFIS RUL prediction models with an inclusive infrastructure asset management system to aid municipal agencies in planning and budgeting.","A valuable extension would be to evaluate generalization via municipality-held-out or time-forward validation (train on earlier years/other cities, test on a new city or later period) to reflect deployment conditions. Incorporating uncertainty (e.g., probabilistic RUL with prediction intervals via Bayesian neural nets, quantile regression, or conformal prediction) would make outputs more actionable for risk-based planning. The approach could be expanded to handle censoring and recurrent events explicitly (e.g., survival/renewal models, recurrent-event hazard models) and to fuse heterogeneous inspection data with break histories. Providing open-source code and a standardized preprocessing pipeline, plus benchmarking against modern tree-based and survival ML baselines, would improve reproducibility and strengthen comparative claims.",1909.02115v1,https://arxiv.org/pdf/1909.02115v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:57:15Z
TRUE,Life distribution modeling|System reliability|Other,Simulation-based|Bayesian|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes ASTPA (Approximate Sampling Target with Post-processing Adjustment), a Hamiltonian MCMC-based framework to estimate rare-event (failure) probabilities in reliability analysis, especially for high-dimensional problems and very small probabilities. The method constructs an approximate sampling target by weighting the standard-normal input space with a 1D Gaussian likelihood of the normalized limit-state function, then draws samples using gradient-based HMCMC; a Quasi-Newton (BFGS) mass-preconditioned variant (QNp-HMCMC) is developed to improve efficiency in challenging/high-dimensional cases. A post-processing step based on inverse importance sampling is applied using a Gaussian mixture model (fit via EM) to compute an unbiased failure probability estimate without additional limit-state evaluations. Performance is evaluated on several benchmark reliability problems (static multimodal, four-branch series system, and dynamic SDOF oscillators including a 101D white-noise excitation case) and compared against component-wise Metropolis-Hastings Subset Simulation. Results from repeated simulations show ASTPA variants—particularly QNp-HMCMC—often reduce estimator coefficient of variation and improve accuracy at comparable model-call budgets, with guidance that QNp-HMCMC is especially attractive beyond ~20 dimensions when gradients are available.","Failure probability is formulated as $P_F=\int \mathbb{I}[g(\theta)\le 0]\,\pi_\theta(\theta)\,d\theta$ with $\theta$ typically in standard normal space. ASTPA defines an (unnormalized) target density proportional to a 1D Gaussian likelihood of the normalized limit-state, $\mathcal{N}(g(\theta)/g_c\mid 0,\sigma)\,\times\,\mathcal{N}(\theta\mid 0,I)$, where $g_c$ normalizes $g(\theta)$. The post-processing estimator rewrites $P_F=\int \mathbb{I}_F(\theta)\,\frac{C\,\tilde h(\theta)}{\ell(\theta)}\,d\theta$ with $C=\frac{1}{N}\sum_{i=1}^N \tilde h(\theta_i)/Q(\theta_i)$, $\ell(\theta)$ the likelihood, $\tilde h$ the non-normalized target, and $Q$ a GMM fit to samples.","Across 500 independent runs per example, HMCMC-based ASTPA methods generally achieve lower estimator variability (C.O.V.) than Subset Simulation for the same approximate model-call budget in several benchmarks, with QNp-HMCMC often best in high dimensions. Example 1 (parabolic, exact $P_F\approx3.95\times10^{-5}$): C.O.V. drops from ~0.62–0.65 (SuS) to 0.35 (HMCMC) and 0.39 (QNp-HMCMC) at ~4.4k–4.9k model calls. Example 3 (undamped SDOF, exact $P_F\approx9.09\times10^{-6}$): C.O.V. reduces from 0.67/0.51 (SuS) to 0.14 (HMCMC) and 0.11 (QNp-HMCMC) at ~5.1k calls; for the rarer case (exact $\sim1.55\times10^{-8}$), QNp-HMCMC attains C.O.V. 0.15 vs 0.77/0.70 (SuS). Example 4 (101D white-noise SDOF): for $R=1.8$ (exact $\sim2.53\times10^{-6}$) QNp-HMCMC achieves C.O.V. 0.24 vs 0.32 (SuS) and 0.30 (HMCMC) at ~11k calls; for $R=2$ (exact $\sim1.11\times10^{-7}$) QNp-HMCMC achieves C.O.V. 0.29 vs 0.41 (SuS) and 0.34 (HMCMC) at ~13.6k calls.","All analyses and numerical results assume analytical gradients of the limit-state/log-target can be computed. The authors state that if numerical differentiation is required for gradients, HMCMC-based methods (including ASTPA) will not be competitive relative to Subset Simulation. They also note that different combinations of HMCMC and QNp-HMCMC may be needed depending on problem-specific characteristics.","The method’s efficiency depends on multiple tuning choices (e.g., likelihood dispersion $\sigma$, trajectory length/stepsize selection, burn-in annealing schedule, and GMM/EM fit), but robustness to poor tuning or automated default performance is not fully characterized. The post-processing relies on fitting a Gaussian mixture model to samples, which can be unstable in very high dimensions or multimodal settings without careful model selection/regularization; the paper does not deeply analyze sensitivity to the number of mixture components or EM failures. Comparisons are primarily against a specific Subset Simulation implementation (CWMH-SuS with two proposals), omitting other strong rare-event estimators (e.g., cross-entropy, line sampling, advanced SuS proposals, surrogate-assisted methods), which limits generality of the benchmarking.",The authors state ongoing/future work will explore various ASTPA variants and will focus on estimating first-passage problems under numerous settings and high-dimensional parameter spaces.,"Developing self-tuning or adaptive schemes for selecting $\sigma$, trajectory length, and mixture-model complexity (including principled component selection) would improve practical usability and robustness. Extending ASTPA to settings with noisy/approximate gradients (e.g., adjoint error, surrogate gradients) or gradient-free variants would broaden applicability to expensive black-box simulators. Additional real-world case studies (beyond benchmarks) and broader comparisons to other rare-event methods (cross-entropy, splitting with advanced proposals, surrogate-assisted IS) would better establish when ASTPA is the preferred tool.",1909.03575v2,https://arxiv.org/pdf/1909.03575v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:57:58Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,ML-based|Hybrid/Ensemble|Other,Degradation measurements|Right-censored|Mixture of types,Predictive|Condition-based,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper proposes a transductive transfer learning (TL) approach for remaining useful life (RUL) prediction when labels are available only in a source domain (run-to-failure experiments) and not in the target domain (field/deployed setting). It uses the Consensus Self-Organizing Models (COSMO) deviation-detection framework to create transferable, sensor-wise features defined as distances from each sample to a nominal peer/reference group, aiming to reduce source–target distribution discrepancy under new operating conditions and/or unseen faults. A random forest regressor is trained on COSMO features from the labeled source domain and then transferred to predict RUL in the unlabeled target domain. The approach is evaluated on the NASA C-MAPSS turbofan engine degradation dataset across multiple cross-subset transfer scenarios, including cases with new operating conditions, new fault modes, or both. Results show COSMO-based features outperform raw sensor features and standard domain adaptation baselines (TCA, CORAL, SCL), with particularly large gains when the target domain introduces new operating conditions and faults (reported as roughly 4× lower error than the traditional approach in those scenarios).","RUL training labels use a piecewise linear target: $y_{u,t}=l(u)-t$ for $l(u)-\tau_{\max}\le t\le l(u)$ and $y_{u,t}=\tau_{\max}$ otherwise (with $\tau_{\max}=130$). COSMO features are computed sensor-wise as median kNN L1 distances to a nominal reference group: $\Delta^j(x_{u,t},\phi)=[|x^j_{u,t}-\phi^j_i|]_{i=1}^{|\phi|}$, select the $k$ smallest values $\Delta^j_{(-k)}$, then $\theta^j_{u,t}=\mathrm{median}(\Delta^j_{(-k)}(x_{u,t},\phi))$. The neighborhood size is constrained by operating-condition complexity: $k \le |\phi|/|\Omega_{oc}|$.","Across transfer scenarios on C-MAPSS where the target domain has new operating conditions and/or new fault modes, COSMO distance features (notably mode (ST,ST)) achieve the lowest MAPE compared with raw sensor inputs and domain adaptation baselines (TCA, CORAL, SCL). In the conclusion, the authors report that for scenarios with new operating conditions (C1) and new operating conditions plus new faults (D), COSMO reduces error substantially—approximately four times lower than the traditional no-transfer random-forest approach. The paper also reports that a random forest trained on raw features achieves RMSE close to prior RF results on same-subset evaluation (e.g., their RF RMSE: 19.65±0.80, 29.43±0.24, 22.40±0.52, 29.95±0.43 for FD001–FD004). Performance under mode β (right-censored test trajectories) shows lower statistical significance than mode α due to censoring, but COSMO still generally improves over raw features in the key transfer scenarios.","Results for mode β (using the predefined right-censored test trajectories) have lower statistical significance than mode α because the test data are right censored. The authors note that COSMO’s effectiveness depends on selecting a peer/reference group that is representative of nominal conditions across operating profiles; if the peer group is not representative, the transferable feature quality may degrade.","The method relies on a heuristic definition of “healthy/nominal” data (first $\tau=30$ cycles), which may not hold in real fleets with early-life anomalies or variable initial degradation, and can bias the reference group. COSMO features are computed per-sensor using L1 distances and kNN/median heuristics, potentially ignoring cross-sensor dependencies and temporal dynamics that are important for RUL. The evaluation is largely limited to the simulated C-MAPSS benchmark; generalization to real-world sensor noise, missingness, maintenance actions, and nonstationary concept drift is not demonstrated. Also, code and full hyperparameter details for all compared methods are not provided, making exact reproducibility and fairness verification harder.",None stated.,"Validate COSMO-based transferable features on real deployed industrial/fleet datasets with concept drift, missing data, and maintenance interventions, and study how to maintain/update the peer reference group online. Extend the approach to incorporate temporal modeling (e.g., sequence embeddings) and multivariate distance metrics that capture cross-sensor relationships while preserving transferability. Develop robust/self-starting strategies for selecting nominal windows and choosing $k$ (and estimating $|\Omega_{oc}|$) without prior knowledge of operating conditions. Provide an open-source implementation and standardized experimental protocol to enable reproducible comparisons with modern deep domain adaptation and uncertainty-aware RUL methods.",1909.07053v3,https://arxiv.org/pdf/1909.07053v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:58:42Z
TRUE,Other,Stochastic process|Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper proposes mfEGRA, a multifidelity active-learning method for efficient global reliability analysis focused on locating the failure boundary (the zero contour of a limit-state function) and estimating probability of failure. It extends single-fidelity EGRA by using a multifidelity Gaussian process (GP) surrogate that fuses a high-fidelity model with one or more cheaper low-fidelity models via additive discrepancy GPs. Sampling is done sequentially in two stages: the next input location is chosen by maximizing EGRA’s expected feasibility function (targeting points near the failure boundary), and the information source (fidelity level) is chosen by maximizing a cost-normalized, weighted one-step lookahead information gain based on KL divergence between current and hypothetical-updated GP predictions. A key technical result is a closed-form expression for the expected KL divergence (information gain) in the multifidelity GP setting, enabling single-loop Monte Carlo estimation of the acquisition value instead of double-loop sampling. Numerical studies on an analytic multimodal limit-state and an acoustic horn reliability problem show comparable probability-of-failure accuracy with reduced equivalent high-fidelity cost versus single-fidelity EGRA (reported savings up to ~46% on the analytic case and 24%–48% on acoustic horn variants depending on implementation and rarity).","Probability of failure is estimated by Monte Carlo as \(\hat p_F=\frac{1}{m}\sum_{i=1}^m I[g(z_i)>0]\). The next sample location is chosen by maximizing EGRA’s expected feasibility function \(E[F(z)]\) (with band \(\epsilon(z)=2\sigma(0,z)\)), i.e., \(z_{n+1}=\arg\max_{z\in\Omega}E[F(z)]\). Given \(z_{n+1}\), the fidelity \(l\) is selected by maximizing a cost-normalized weighted information gain based on KL divergence between current and one-step-ahead GP predictions: \(l_{n+1}=\arg\max_l \sum_{z\in \mathcal Z}\frac{w(z)}{c_l(z)}D(z\mid z_{n+1},l)\), where \(D\) is derived from the closed-form expected KL divergence for Gaussians.","On the analytic multimodal test problem, mfEGRA attains a median relative error below \(10^{-3}\) in probability-of-failure estimation with equivalent high-fidelity cost 26 versus 48 for single-fidelity EGRA (~46% savings). On the 3D acoustic horn case, mfEGRA reaches median relative error below \(10^{-3}\) with cost 19 versus 25 for EGRA (~24% savings); when restricting candidate locations to pre-drawn Monte Carlo samples, mfEGRA needs 12 versus 22 (~45% savings). On the 4D (rarer-event) acoustic horn case with location search restricted to Monte Carlo samples, mfEGRA requires 25 versus 48 (~48% savings) to reach median relative error below \(10^{-3}\). The paper also reports illustrative run details for one analytic DOE (e.g., 21 HF, 77 LF1, 23 LF2 evaluations to reach an EFF stopping threshold) and for one horn DOE (e.g., 31 HF and 76 LF evaluations to reach EFF < \(10^{-10}\)).","The authors note a key limitation of GP-based methods: curse of dimensionality—sample requirements grow rapidly with dimension and GP training cost scales cubically with the number of samples. They state that the increasing-cost issue for GP training is not addressed in the paper, suggesting sparsification techniques as a possible remedy. They also remark that parallel multifidelity adaptive sampling is more difficult and remains to be explored; additionally, they mention solver failures can occur in practice (though not encountered here) and suggest treating failed evaluations as system failures by setting the limit-state value to an upper limit.","Reliability performance is assessed mainly via Monte Carlo–based relative error of \(p_F\) versus a reference estimate; there is limited discussion of robustness to GP/model-mismatch assumptions (e.g., additive discrepancy structure, stationarity of kernels, and independence of discrepancy processes across fidelities). The acquisition involves numerical integration over a fixed Monte Carlo set \(\mathcal Z\); accuracy and computational burden may be sensitive to the size/quality of \(\mathcal Z\), especially for rare-event regions, but this sensitivity is not systematically analyzed. Comparisons are primarily against single-fidelity EGRA; broader benchmarking against other active-learning reliability methods (e.g., AK-MCS, SUR-based methods, subset simulation hybrids) is not comprehensive. No implementation artifacts (code) are provided, which can hinder reproducibility of optimizer choices (patternsearch/GlobalSearch), GP fitting, and stopping behavior.","They suggest addressing GP scalability via sparsification techniques to mitigate cubic training cost as sample size grows. They also propose exploring parallel adaptive sampling strategies, noting that multifidelity parallelization is more challenging and remains an open area in both global optimization and contour-location settings. Additionally, they indicate that alternative stopping criteria beyond the chosen EFF threshold could be explored.","A natural extension is to integrate mfEGRA with variance-reduction/rare-event methods (importance sampling, subset simulation) in a tightly coupled way rather than only noting the possibility, to improve efficiency for very small \(p_F\). Developing robust/self-starting variants that handle unknown/estimated input distributions, heteroscedastic observation noise, or occasional solver failures within the GP likelihood would increase practical applicability. Extending the approach to high-dimensional settings using modern scalable GP approximations (inducing points, structured kernels) and evaluating the trade-off between fidelity selection and approximation error would strengthen the method’s reach. Providing open-source implementations and standardized benchmarks across multiple reliability problems would enable more rigorous comparative evaluation and adoption.",1910.02497v5,https://arxiv.org/pdf/1910.02497v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:59:20Z
TRUE,Other,Bayesian|Other,Simulated only|Other,Not applicable,Manufacturing (general)|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper formulates an Input Uncertain Reliable Level Set Estimation (IU-rLSE) problem motivated by expensive operational testing in manufacturing, where inputs cannot be precisely controlled during real use. Reliability at an input setting is defined as the probability that the (uncertain-input) output falls below/above a threshold, and the goal is to identify the set of inputs whose reliability exceeds a specified probability level (e.g., 0.95) with as few tests as possible. Using a Gaussian process prior on the black-box function, the authors derive credible intervals for the reliability probability via integrals of normal CDF terms under the input-noise distribution, and propose an active-learning (Bayesian experimental design) acquisition function extending MILE to the input-uncertain setting with a tractable approximation to avoid expensive nested integrals. They provide theoretical guarantees on misclassification loss and almost-sure finite-time termination (convergence) under regularity conditions. Empirically, they demonstrate improved F1/precision over Straddle, MILE, and random sampling on synthetic benchmarks and a real Combined Cycle Power Plant dataset under modeled input uncertainty.","Reliability at design point $x$ is $p_x^* = \int_D \mathbf{1}[f(s)\le h] g(s\mid\theta_x)\,ds$ and the reliable set is $H=\{x\in X: p_x^*>\alpha\}$. Under a GP posterior for $f$, they define $p_{t,x}=\int_D \mathbf{1}[f_t(s)<h]g(s\mid\theta_x)ds$ with mean $\mu_t^{(p)}(x)=\int_D \Phi\big((h-\mu_t(s))/\sigma_t(s)\big)g(s\mid\theta_x)ds$ and variance proxy $\gamma_t^2(x)=\int_D \Phi_s(1-\Phi_s)g(s\mid\theta_x)ds$, yielding a credible interval $Q_t(x)=[\mu_t^{(p)}(x)\pm \beta^{1/2}\gamma_t(x)]$. The expected classification-improvement acquisition $a_t(x)$ integrates over uncertain realized inputs $s^*$ and outcomes $y^*$, and is approximated to a single-integral form (Eq. (10)) to reduce cost from $O(|X|M^3)$ to $O(|X|M)$.","The paper proves an accuracy guarantee: with $\beta^{1/2}=(\delta/|X|)^{-1/2}$, the maximum misclassification loss is bounded by $\epsilon$ with probability at least $1-\delta$ at termination (Theorem 4.1). It also proves almost-sure finite termination under regularity conditions (A1–A4), despite the inability to sample exactly at intended inputs due to input uncertainty (Theorem 4.2). In experiments, the proposed IU-rLSE achieves higher F1-score/precision than Straddle, MILE, and random sampling on 1D polynomial, sinusoidal, and Himmelblau functions under both Gamma and Gaussian input-noise models. On the Combined Cycle Power Plant dataset, IU-rLSE attains higher average F1 and precision that trends toward 1, while baselines focusing on classifying $f$ (rather than $p_x^*$) do not achieve precision approaching 1.",None stated.,"The method assumes a GP model for the latent function and relies on posterior normality and conditional independence assumptions; performance may degrade under strong nonstationarity, heavy-tailed noise, or model misspecification. The acquisition approximation (replacing integrals over $s$ with evaluations at representative points such as means/maximizers) can introduce bias and may underperform when input uncertainty is large, multimodal, or highly asymmetric. Experimental evaluation is limited to a small set of benchmarks and one real dataset with modeled (rather than empirically validated) input-uncertainty distributions, and no implementation details/software are provided for reproducibility.",None stated.,"Develop robust/self-normalizing variants that handle GP hyperparameter uncertainty, misspecified noise, and non-Gaussian observation models, and study sensitivity to the assumed input-uncertainty distribution. Extend IU-rLSE to correlated/autocorrelated input disturbances, multivariate reliability constraints (multiple thresholds/outputs), and safe/constraint-aware Bayesian optimization settings. Provide an open-source implementation and benchmark on additional real operational-testing problems where input uncertainty can be estimated from field data, including ablations quantifying the impact of the acquisition approximation.",1910.12043v1,https://arxiv.org/pdf/1910.12043v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T10:59:56Z
FALSE,NA,ML-based,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/gsmartensson/avra_public,"This paper evaluates the robustness and generalization of a deep-learning MRI model (AVRA) when applied to out-of-distribution (OOD) clinical data acquired with different scanners, protocols, and disease populations. Using 3117 T1-weighted brain MRI scans from multiple research cohorts and memory clinics labeled by a single expert neuroradiologist for Scheltens’ medial temporal atrophy (MTA) score, the authors train multiple CNN variants with different training-cohort combinations while keeping training set size and label distribution fixed. Performance is assessed primarily via Cohen’s linearly weighted kappa (with MSE as a complementary metric), showing that models trained on homogeneous research data generalize well to similar cohorts but can degrade substantially on certain clinical centers with visibly different tissue contrast. Adding heterogeneous clinical data to the training set improves performance on OOD cohorts without materially harming in-distribution performance. The work emphasizes that external multi-cohort evaluation is necessary before clinical deployment and suggests uncertainty/OOD detection as an important next step.","The model outputs a continuous MTA prediction that is rounded to the nearest integer for agreement analysis. Reliability/performance is quantified using Cohen’s linearly weighted kappa $\kappa_w$ (agreement between AVRA’s rounded ratings and the neuroradiologist’s ratings) and mean squared error (MSE) between AVRA’s continuous predictions and the radiologist ratings. The network is described as a recurrent CNN processing the 3D volume slice-by-slice with a residual attention feature extractor feeding an LSTM, but no single explicit closed-form charting/statistic equation is provided beyond these metrics.","When trained only on ADNI, agreement is high on ADNItest (Cohen’s $\kappa_w\approx0.70$) and AddNeuroMed ($\kappa_w\approx0.72$) but drops on clinical cohorts (e.g., MemClin $\kappa_w\approx0.65$, E-DLBall $\kappa_w\approx0.61$). A major failure case is one external center (E-DLBC1) where $\kappa_w$ falls to about 0.34 (with high MSE ≈ 0.75), attributed to systematic underestimation linked to protocol/intensity differences. Incorporating more heterogeneous clinical training data improves OOD performance, raising E-DLBC1 agreement up to roughly $\kappa_w\approx0.58$ and reducing MSE to about 0.36, while leaving ADNItest performance near 0.70. Test–retest analyses show generally small within-subject variability, with notable deviations primarily for one 3T scanner/protocol, indicating some scanner-dependent systematic differences.","The authors state that they trained and evaluated only a single network architecture, so results may not generalize to deep-learning models broadly. They note that using hyperparameters tuned on within-distribution data does not prevent protocol overfitting. They also acknowledge that weighted kappa can be noisy because it requires rounding continuous predictions, while MSE is sensitive to outliers. Finally, because AVRA uses unprocessed MRI inputs, they did not examine how preprocessing or intensity normalization might affect generalization.","The evaluation focuses on agreement with a single radiologist’s ratings; while this avoids inter-rater variability, it also ties “ground truth” to one reader and does not quantify model performance relative to multi-rater consensus. The work documents domain shift effects but does not provide a formal OOD detection or calibration analysis (e.g., confidence calibration, decision thresholds for “do not use” cases), which is central to operational reliability in deployment. The study is task-specific (MTA visual rating) and may not transfer to other clinical endpoints that are more sensitive to acquisition differences. Implementation details of training (software stack, reproducibility artifacts such as fixed seeds, full training code/environment) are not clearly documented in the excerpt beyond a public model/software link.","The authors suggest developing scalable methods to estimate deep-learning model uncertainty and/or detect OOD data, noting this is an active research area not explored in the current study. They also imply that studying preprocessing or intensity normalization and their effect on generalization is a remaining gap for future work. They mention that semi-supervised approaches leveraging unlabeled images may be important to improve generalization when labeled medical data are scarce.","A practical next step would be to add prospective clinical validation with pre-specified acceptance criteria and workflow integration (e.g., when to defer to human readers) across multiple new sites. Extending evaluation to robustness under common real-world artifacts (motion, incomplete brain coverage, varying resolutions) and to calibrated uncertainty estimates (e.g., conformal prediction or Bayesian/ensemble calibration) would improve deployment reliability. Providing an explicit domain adaptation strategy (e.g., intensity harmonization, style transfer, or scanner-invariant representations) and comparing it against “more heterogeneous training data” would clarify the best mitigation. Releasing full training scripts, data preprocessing pipelines, and environment specifications would improve reproducibility and enable independent verification of robustness claims.",1911.00515v1,https://arxiv.org/pdf/1911.00515v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:00:30Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Network/cybersecurity,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an “experienced” deep reinforcement learning framework for model-free resource allocation in downlink OFDMA networks targeting ultra-reliable low-latency communication (URLLC-6G). A GAN-based refiner generates a virtual training environment by refining synthetic traffic/channel samples using a limited amount of real unlabeled data, aiming to expose the RL agent to rare/extreme network conditions and reduce transient training time at deployment. The RL agent (PPO) outputs desired user rates, which are mapped to per-resource-block assignments and powers via an optimization-based action-space reducer (dual decomposition) that minimizes power while meeting desired rates. Reliability is defined in a communications QoS sense (probability that end-to-end delay exceeds a threshold) and is measured empirically from packets delivered within the latency constraint; this measurement is used in the reward. Simulation results show near-optimal latency vs. an exhaustive-search benchmark and faster recovery under extreme condition changes compared with vanilla or differently pre-trained agents.","Downlink rate: $r_i(t)=\sum_{j=1}^K \rho_{ij}(t) B\log_2\big(1+\frac{p_{ij}(t)h_{ij}(t)}{\sigma^2}\big)$. Reliability (QoS) estimated empirically: $\gamma_i(t)=1-\Pr\{D_i>D_i^{\max}\}\approx 1-\mu_i^0(t)/\mu_i(t)$. Deep-RL reward: $R(a_t,s_t)=-\sum_i w_i(t)(1-\gamma_i(t)) - \alpha P(t)$ with weight update $w_i(t{+}1)=\max\{w_i(t)+\gamma_i^* - \gamma_i(t),0\}$. Action-space reducer power solution per RB from dual decomposition: $p_{ij}^*=\big[\frac{\lambda_i B}{\log 2}-\frac{\sigma^2}{h_{ij}(t)}\big]^+$ with RB assignment via minimizing the dual subproblem cost.","In simulations, the experienced (GAN-pretrained) deep-RL agent recovers from a sudden shift to an extreme traffic environment about 60 epochs faster than baseline agents (vanilla, synthetic-only pretraining, and real-only pretraining). With bandwidth increased from 45 MHz to 50 MHz, average latency per user decreases by about 16% at comparable reliability targets; the system can reach about 99.99% reliability with <1.5 ms target latency in the illustrated setting. Compared to a greedy exhaustive-search benchmark, the proposed method achieves near-optimal average delay across tested BS power levels, with reported average delays as low as ~0.15 ms for small packets. The action-space reducer mapping error is reported as <1% per user for RB bandwidth 180 kHz (LTE-like).","The authors note that in practical deployments there are delay components the algorithm cannot directly control (e.g., processing delay at UE/BS and hardware delays), though the approach can adapt because it uses measured end-to-end delay as feedback. They also acknowledge limited real data availability and that their evaluation uses a dataset where per-session statistics required them to synthetically generate per-packet sizes and inter-arrival times from reported means.","The work uses “reliability” in a URLLC QoS sense (deadline violation probability), not reliability engineering (component/system failure over time), so conclusions do not transfer to degradation/failure modeling contexts. The paper’s performance evidence is primarily simulation-based with specific traffic/channel assumptions and limited real-data grounding, so generalization to diverse real networks, mobility, and correlated traffic/channel dynamics is uncertain. No implementation details (training time, compute requirements, hyperparameters, reproducibility artifacts) are provided, and the complexity of the action-space reducer ($O(KN^3)$) could be challenging for very large $N$ or tight real-time constraints without further optimization.",None stated.,"Validate the approach on real operational network traces (including mobility, non-stationary interference, and control-channel constraints) and report deployment-oriented metrics such as compute latency and sample efficiency. Extend the framework to multi-cell settings with inter-cell interference and coordination and to scenarios with partial observability and delayed/quantized feedback. Provide an open-source reference implementation and ablation studies to isolate the value of the GAN refiner vs. other experience replay / domain randomization strategies.",1911.03264v2,https://arxiv.org/pdf/1911.03264v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:01:03Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,1911.06256v2,https://arxiv.org/pdf/1911.06256v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:01:03Z
TRUE,Failure mode analysis|Other,ML-based|Other,Sensor/condition monitoring|Mixture of types|Simulated only|Other,Not applicable,Healthcare/medical|Energy/utilities|Network/cybersecurity|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,http://yann.lecun.com/exdb/mnist/|www.cs.ucr.edu/∼eamonn/time_series_data/,"The paper proposes “Epistemic classifiers” to quantify reliability of individual ML predictions by framing a classifier’s output as a belief and requiring a data-driven justification based on nearby training points in input and latent (hidden-layer) spaces, inspired by the Justified True Belief (JTB) theory from epistemology. For each test input, the method constructs layer-wise supports using neighborhood operators (e.g., k-NN, ε-ball, and hybrid variants) over activations, unions these supports into a justification set, and then labels the prediction as IK (“I know”), IMK (“I may know”), or IDK (“I don’t know”) depending on how the base prediction matches the justification. The approach targets reliability failures arising from epistemic uncertainty such as extrapolation/novel inputs (empty/insufficient neighborhood support) and class-overlap/confusion (impure support), and it enables abstention rather than overconfident misclassification. The method is evaluated via simulations and multiple real datasets (tabular, time-series, and images), including robustness tests under Gaussian/uniform noise and adversarial perturbations; results show ε-ball/hybrid supports can greatly reduce unreliable “IK” assertions under large perturbations compared with softmax-threshold baselines. Practical limitations are discussed around nearest-neighbor search scalability in high-dimensional embeddings and the mismatch between semantic similarity and ℓp distances in early layers.","Support in neural-network layer i is defined via neighborhood search in activation space: $S_i(x)=\{f(z): z\in X,\ h_i(z)\in N_i(h_i(x))\}$, where $N_i$ is an ε-ball or k-NN (or hybrid) operator. Justification aggregates supports across chosen layers $I$ as $J(x)=\varnothing$ if any $S_i(x)=\varnothing$, else $J(x)=\bigcup_{i\in I} S_i(x)$; the epistemic label is IK if $g(x)=J(x)$, IMK if $g(x)\subset J(x)$, and IDK otherwise. A derived bound relates ε across layers: $\varepsilon_{i+1}\le L_i\sqrt{\lambda_i^*}\,\varepsilon_i$, where $\lambda_i^*$ is the largest eigenvalue of $W_iW_i^T$ and $L_i$ is the activation Lipschitz constant; the paper also proposes a weighted-distance metric $D_i=V\Lambda^{-1}V^T$ from eigendecomposition of $W_{f,i}^TW_{f,i}$ to reuse a common ε across layers under simplifying assumptions.","Across multiple datasets, the key outcome is that accuracy on the IK subset (AIK) remains very high while coverage (FIK, fraction of IK) adapts to overlap/perturbations—indicating the method can avoid asserting high-confidence reliability where it should not. In a toy overlap experiment, maximum coverage drops as overlap/noise increases (reported example: $F_{IK}\approx 1.00$ for $\sigma=0.5$ vs. $F_{IK}\approx 0.44$ for $\sigma=1.5$), reflecting shrinking “reliably knowable” regions. Under large uniform noise or adversarial perturbations, softmax-threshold baselines can retain high FIK but exhibit very poor AIK (e.g., MNIST baseline under adversarial noise shows $F_{IK}=0.851$ with $A_{IK}=0.026$), whereas ε-ball/hybrid Epistemic classifiers tend to drive $F_{IK}$ to near zero (detect/abstain) with non-IK accuracy being largely irrelevant for rubbish/adversarial samples. On nominal data, reported AIK values are near-perfect for many datasets (often ≳0.99) with moderate coverage (e.g., MNIST ε-NN nominal: $F_{IK}=0.590$, $A_{IK}=0.997$), demonstrating high reliability when the method asserts IK.","The authors state five main challenges: (1) nearest-neighbor search in high-dimensional early-layer embeddings can be computationally expensive and impractical for large convolutional layers; they suggest approximate methods like locality-sensitive hashing. (2) Large training-set size increases memory to $O(|X|)$ and makes search/storage burdensome even with $O(\log|X|)$ average query time. (3) Semantic similarity may not align well with convenient ℓ2/ℓp distances in input/early layers, improving only deeper in the network. (4) Neighborhood sparsity due to high dimensionality or limited sample size can degrade performance. (5) The current approach is suited to neural networks; they note model-agnostic variants (e.g., via input representations and Platt scaling) are not yet addressed.","The JTB/IK–IMK–IDK labeling depends heavily on choices of layers, distance metrics, and neighborhood parameters (ε, k); while tuned via coverage, there is limited guidance on principled selection that generalizes across tasks and distribution shifts. The justification rule $J(x)=\varnothing$ if any layer’s support is empty can be overly brittle when one layer is noisy/high-dimensional, potentially causing excessive IDK without a calibration of per-layer reliability. Comparisons focus mainly on softmax-thresholding and k-NN variants; stronger uncertainty/OOD baselines (e.g., deep ensembles, MC-dropout, conformal prediction, density-based OOD detectors) are not systematically benchmarked, which limits claims of superiority. The approach stores and queries training activations, which can create privacy and compliance issues and may be costly for continual learning where the training set (and embeddings) evolve over time.","The authors state that the outlined challenges will be explored in the near future, including using approximate neighbor search (e.g., locality-sensitive hashing) to address scalability and developing a model-agnostic approach (e.g., with suitable input representations and Platt scaling) for non-neural-network models. They also note that while BIM adversarial attacks are included, a detailed treatment of adversarial robustness (including other inference-time and backdoor attacks they explored) is planned as follow-on work.","A natural extension is to formalize statistical guarantees for IK coverage/accuracy (e.g., conformal-style validity for selective classification/abstention) and to study robustness under autocorrelation and non-i.i.d. data common in industrial monitoring. The justification operator could be improved with weighted/soft evidence aggregation across layers rather than hard union/emptiness rules, enabling smoother trade-offs between IDK and IMK and better calibration. Scalability could be enhanced by learned approximate nearest-neighbor indices or prototypical summarizations (class prototypes/coresets) to avoid storing all activations. More extensive real-world validation in safety-critical settings (healthcare/industrial control) with cost-sensitive decision analysis would clarify how IK/IMK/IDK should drive downstream actions.",1911.07391v3,https://arxiv.org/pdf/1911.07391v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:01:54Z
FALSE,NA,Stochastic process|Simulation-based|Other,Simulated only|Other,Not applicable,Network/cybersecurity|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R|Other,Not provided,NA,"This paper develops and applies multivariate MCMC output analysis tools to assess the reliability (precision) of Monte Carlo estimates of network properties obtained via random-walk-based network sampling. It focuses on estimating multiple network features (e.g., mean degree, clustering coefficient, and attribute proportions) and quantifying Monte Carlo error using a multivariate CLT with asymptotic covariance estimated via multivariate batch means. The authors operationalize reliability through effective sample size (ESS), confidence region volume, coverage probability, and a relative fixed-volume sequential stopping rule that terminates sampling once a target relative precision is met. They compare a simple random walk (SRW) with importance sampling versus a Metropolis–Hastings (MH) random walk targeting a uniform node distribution across three networks (simulated high school, NYU Facebook, and Friendster). Empirically, SRW consistently achieves higher ESS and/or earlier stopping than MH for the same reliability threshold, highlighting that algorithm choice materially affects the trustworthiness of sampled-network estimates.","Key quantities are the network-feature mean $\mu_h=\frac{1}{n}\sum_{v\in V} h(v)$ and the MCMC sample mean $\mu_m=\frac{1}{m}\sum_{t=0}^{m-1} h(V_t)$, with a multivariate Markov chain CLT $\sqrt{m}(\mu_m-\mu_h)\Rightarrow N_p(0,\Sigma)$ where $\Sigma=\mathrm{Var}_\lambda(h(V_0)) + \sum_{t\ge1}(\mathrm{Cov}_\lambda(h(V_0),h(V_t)) + \mathrm{Cov}_\lambda(h(V_0),h(V_t))^T)$. The asymptotic covariance $\Sigma$ is estimated by multivariate batch means $\Sigma_m=\frac{b_m}{a_m-1}\sum_{k=0}^{a_m-1}(\bar X_k-\mu_m)(\bar X_k-\mu_m)^T$ and multivariate ESS is $\widehat{ESS}=\left\lfloor m\left(\frac{|\Lambda_m|}{|\Sigma_m|}\right)^{1/p}\right\rfloor$ with $\Lambda_m$ the sample covariance. A relative fixed-volume stopping rule $T_{SD}(\epsilon)$ stops when the confidence-ellipsoid volume is small relative to $|\Lambda_m|$, which is approximately equivalent to stopping once $\widehat{ESS}$ exceeds a computable threshold determined by $(\alpha,\epsilon,p)$.","For the faux.magnolia.high network (p=5, $\epsilon=0.05$, $\alpha=0.05$; minimum ESS 10363), SRW terminated at about 341,190 steps with ESS ≈ 10,639, while MH terminated at about 689,115 steps with ESS ≈ 10,550; both visited all 439 nodes and MH acceptance was ~0.29. For the NYU Facebook well-connected component (p=4; minimum ESS 9992), SRW terminated around 14,677 steps (ESS ≈ 10,559; coverage ≈ 0.938) while MH terminated around 85,949 steps (ESS ≈ 6,824; coverage ≈ 0.91), with unique nodes sampled ≈ 8,704 (SRW) vs ≈ 16,791 (MH). For Friendster with 10,000-step chains (p=2; minimum ESS 7530), neither method reached the ESS threshold; SRW had ESS ≈ 3,866 vs MH ≈ 463 and unique nodes ≈ 9,797 vs ≈ 2,437; at 100,000 steps for mean degree (univariate), SRW ESS ≈ 36,229 vs MH ≈ 6,002 with unique nodes ≈ 97,474 vs ≈ 24,477.","The authors note they restrict attention to binary, undirected, well-connected graphs and focus on node-sampling via random-walk MCMC rather than edge-sampling algorithms. They also indicate that applying these reliability tools to respondent-driven sampling (RDS) is not straightforward because the assumptions required for the output analysis methods are not met in RDS. For the Friendster case, they report triangle counting is computationally expensive, limiting multivariate results to shorter (10,000-step) chains.","The methodology assumes the sampled process can be treated as an irreducible, aperiodic Markov chain with well-defined stationary behavior and relies on asymptotic approximations (multivariate CLT) that may be inaccurate for short or highly correlated walks, especially on very large graphs with bottlenecks. Batch means performance depends on batch-size choices and requires $a_m>p$ and positive definiteness; finite-sample sensitivity or robustness to heavy-tailed network statistics (e.g., high-degree nodes) is not thoroughly explored. The work targets estimation reliability (Monte Carlo error) but does not directly address bias arising from nonstationary starts/burn-in choices, network changes over time, or sampling without full access to neighbor lists (a common practical constraint).","They propose extending the framework to edge-sampling algorithms for estimating edge properties, and generalizing from binary undirected networks to weighted and directed networks. They also suggest applying minimum-ESS and reliable-estimation tools in the context of respondent-driven sampling (RDS), but emphasize this will require new work because standard output-analysis assumptions do not hold for RDS.","Developing self-starting or burn-in-robust stopping rules that explicitly handle initialization bias and nonstationarity would improve practical use in network crawling. Extending the approach to streaming/dynamic networks (time-varying graphs) and to partially observed neighbor queries (API-limited crawling) would broaden applicability. Providing an open-source, end-to-end implementation (including scalable triangle estimation or approximate clustering computation) and benchmarking against additional modern graph-sampling methods (e.g., frontier sampling, non-backtracking variants, multiple-walker designs) would strengthen adoption and comparative conclusions.",1911.08682v2,https://arxiv.org/pdf/1911.08682v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:02:36Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Failure mode analysis|Other,ML-based|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Simulated only,Predictive|Condition-based,Transportation/logistics|Energy/utilities|Other,Simulation study,TRUE,None / Not applicable,Not provided,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/|http://doi.acm.org/10.1145/1541880.1541882|https://ivctechnologies.com/2017/08/29/reactive-preventive-predictive-maintenance/|https://doi.org/10.1016/j.knosys.2010.07.013|http://www.sciencedirect.com/science/article/pii/S0950705110001279,"The paper proposes a data-driven predictive maintenance methodology that extends Attribute Oriented Induction (AOI) to create weighted clusters (a “Knowledge Base”) from multivariate sensor data, using the cluster weights to build a 1D wear/health quantification function over time. Change detection is performed on this quantification via an EWMA control chart, and anomaly/failure declaration is refined using Western Electric Rules (WER). Remaining Useful Life (RUL) is estimated by forecasting the quantification function with an LSTM model from the detected change point until the chosen WER condition is met. Experiments are conducted on the NASA Ames turbofan engine degradation simulation dataset (train/test sets of 100 runs each), showing that the approach detects a pre-failure change point in all cases and that WER selection materially affects RUL error. The work contributes a hybrid AOI+SPC+LSTM pipeline that aims to combine (in principle) expert knowledge (hierarchies/weights) with operational data, though the presented case relies on percentile-based surrogate hierarchies due to lack of experts.","AOI cluster weighting is based on generalization levels: $\text{GenLvW}(i)=\sum_{j=0}^{\text{numattr}} \frac{1}{2^{\text{genlevel}_j}}\,/\,\text{numattr}$ (Eq. 1) and final cluster weight $\text{ClusterW}(x,i)=\text{GenLvW}(i)+\frac{\text{inst}(x)}{\text{outl}(i)}\cdot\text{diffw}(i)$ (Eq. 2). EWMA control limits are computed as $\text{UCL}=\mu_0+L\frac{\sigma}{\sqrt{n}}\sqrt{\frac{\lambda}{2-\lambda}(1-(1-\lambda)^{2i})}$ and $\text{LCL}=\mu_0-L\frac{\sigma}{\sqrt{n}}\sqrt{\frac{\lambda}{2-\lambda}(1-(1-\lambda)^{2i})}$ (Eqs. 3–4), with EWMA statistic update $z_i=\lambda x_i+(1-\lambda)z_{i-1}$ and $z_0=\mu_0$ (Eq. 5). RUL is determined as the number of cycles until the selected Western Electric Rule is satisfied on observed/predicted EWMA/quantification trajectories.","On the NASA turbofan degradation simulations, the method detected the change point before failure in 100% of cases. The LSTM forecasting of the quantification function reported RMSE = 0.036021 in an illustrative evaluation. Comparing Western Electric Rules for mapping change/out-of-control behavior to failure time, WER4 performed best with mean absolute error 6.26 cycles and mean squared error 72.74 (vs. WER1 MAE 56.0/MSE 8518.48; WER2 MAE 33.25/MSE 4125.35; WER3 MAE 17.55/MSE 1084.13). Overall RUL mean absolute error from change-point detection was reported as 39.02 cycles, improving to 13.30 when excluding cases where the change point was detected very early (<40 cycles).","The authors note that the use case did not include domain experts, even though expert-defined generalization hierarchies and attribute weights are a key advantage of AOI; instead, generalizations were derived from percentile binning and attribute weighting was arbitrary. They also state that the RUL error depends strongly on the predictive capability of the LSTM model, implying that insufficient LSTM tuning limits performance in some cases (especially when change points are detected very early).","The approach assumes simulated runs share a single failure mode and treats end-of-run behavior as “failure,” which may not generalize to real assets with multiple competing failure modes and censored outcomes. It relies on a univariate quantification function derived from AOI clusters; information loss and sensitivity to discretization (percentile bins, hierarchy depth, thresholds, minimum cluster size) could materially affect robustness but is not systematically analyzed. The EWMA/SPC step implicitly assumes independence/stationarity properties that may be violated in autocorrelated sensor time series, potentially inflating false alarms or distorting change-point timing. Evaluation is limited to one benchmark dataset and does not report standard PM metrics such as false-alarm rate, early/late prediction penalties (PHM scoring), or uncertainty bounds on RUL estimates.","Future work includes incorporating Root Cause Analysis (RCA) into the methodology, leveraging AOI’s descriptive power to support expert interpretation. The authors also suggest that including domain expert knowledge to define generalization hierarchies and attribute weights should improve results, and that further fine-tuning of the LSTM model is expected to improve RUL prediction accuracy.","Develop a self-starting/online version that updates the AOI Knowledge Base and control limits over time to handle concept drift and evolving operating regimes. Add uncertainty quantification for RUL (prediction intervals) and evaluate with PHM community scoring functions under early/late penalties. Extend the method to handle multiple failure modes, right-censoring (assets not yet failed), and real-world labeled maintenance events rather than assuming end-of-sequence failure. Provide an open-source implementation and sensitivity/ablation studies isolating the contribution of AOI weighting, EWMA parameters, WER choice, and LSTM architecture.",1912.00662v1,https://arxiv.org/pdf/1912.00662v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:03:26Z
FALSE,Network/infrastructure reliability|Other,ML-based|Bayesian,Sensor/condition monitoring|Simulated only|Other,Not applicable,Transportation/logistics|Network/cybersecurity|Other,Simulation study,TRUE,MATLAB|Other,Not provided,http://arxiv.org/abs/1811.12924,"The paper proposes an age-of-information (AoI)-aware transmission power and resource-block (RB) allocation method for vehicle-to-vehicle (V2V) ultra-reliable low-latency communications (URLLC) under unknown and time-varying wireless channel/interference dynamics. It models the next-slot AoI evolution as an unknown nonlinear function and learns this function online using Gaussian process regression (GPR), yielding a predictive mean and variance for future AoI. Resource allocation is then selected by minimizing a weighted objective that trades off reducing the probability that predicted AoI exceeds a threshold (reliability in an AoI-tail sense) against increasing predictive variance (exploration/active learning). The scheme is decentralized per VUE pair to avoid roadside-unit coordination overhead, and is evaluated via simulations in a Manhattan mobility scenario. Simulation results report large reductions in AoI violation probability versus baselines (e.g., at least ~50% reduction for some arrival-rate settings), and show that dataset size and exploration weight have an optimal range in fast-varying vehicular environments.","The AoI state evolves as $\Delta_k(t+1)=f_k(\Delta_k(t),\mathbf{P}_k(t))$, where $f_k(\cdot)$ is unknown and learned. Using GPR with dataset $\mathcal{D}_k=\{(\mathbf{x}_i,y_i)\}_{i=1}^M$ (with $\mathbf{x}_i=[\Delta_k(i),\mathbf{P}_k(i)]$ and $y_i=\Delta_k(i+1)$), the prediction satisfies $y_*\mid \mathbf{x}_*,\mathcal{D}_k\sim\mathcal{N}(\mu_{y_*},\sigma^2_{y_*})$ with $\mu_{y_*}=\mathbf{c}_*^T\mathbf{C}^{-1}\mathbf{y}$ and $\sigma^2_{y_*}=c(\mathbf{x}_*,\mathbf{x}_*)-\mathbf{c}_*^T\mathbf{C}^{-1}\mathbf{c}_*$. The control chooses $\mathbf{P}_k(t)$ to minimize $\alpha_c\Pr\{\hat\Delta_k(t+1)>d\}-\alpha_i\sigma^2_{\hat\Delta_k(t+1)}$, where the exceedance probability is computed from the Gaussian predictive distribution (via $\mathrm{erfc}$).","In simulations (Manhattan mobility, $K=20$ VUE pairs, $N=20$ RBs, duration 5000 slots), GPR-based allocation outperforms random allocation in complementary CDF AoI tail behavior (lower $\Pr\{\Delta(t)>d\}$). The paper reports AoI violation probability reductions of at least 52%, 85%, and 99.6% versus the random baseline for arrival rates of 2.5 Mbps, 500 kbps, and 1 Mbps, respectively. The results also show an optimal range for history size: prediction RMSE improves up to about $M\approx 100$, while very large $M$ (e.g., 1000) worsens RMSE due to outdated/uncorrelated samples; AoI-violation probability similarly improves up to about $M\approx 200$ then degrades. Increasing exploration weight $\alpha_i$ reduces both average AoI and violation probability up to about $\alpha_i\approx 10^2$, after which performance degrades as actions become too exploration-biased.","The authors note that vehicular environments are fast-varying and that using too much historical data can be harmful because outdated observations become uncorrelated, degrading GPR prediction accuracy. They also note the computational cost of GPR scales cubically with dataset size $M$, motivating a fixed-size sliding window that discards old samples. They highlight that too much exploration (large $\alpha_i$) can bias decisions away from reliability optimization, worsening AoI outcomes.","The work frames “reliability” as an AoI-threshold exceedance probability, but it does not connect this to conventional reliability engineering constructs (failure/repair processes, lifetime distributions, or dependability metrics), limiting transfer to classical reliability practice. Evaluation is simulation-only with specific mobility/channel assumptions; robustness to different traffic patterns, non-Manhattan layouts, correlated shadowing, or measurement noise/latency in AoI feedback is not demonstrated. The decentralized approach may induce game-theoretic coupling via interference; convergence/stability or regret/optimality guarantees under simultaneous learning by many VUEs are not established. No publicly released implementation is provided, which limits reproducibility.","The paper suggests as future work “finding the optimal dataset size and exploration-exploitation weights to ensure a reliability target,” motivated by observed tradeoffs between prediction accuracy, exploration, and AoI violation probability. It implies further study of how to tune active-learning parameters in dynamic vehicular settings to meet URLLC reliability constraints.","Extending the approach to handle non-stationary dynamics more explicitly (e.g., forgetting factors, change-point detection, or online hyperparameter adaptation) could improve robustness beyond a fixed window size. Incorporating communication constraints (control/signaling overhead, delayed AoI feedback) and correlated interference dynamics could make the method more deployable. Multi-agent coordination or interference-aware coupling (e.g., federated/consensus learning of shared channel structure) could reduce destabilizing interactions among independently learning VUEs. Real-world trace validation (vehicular testbeds or public V2X datasets) and release of reference code would strengthen empirical credibility and reproducibility.",1912.03359v1,https://arxiv.org/pdf/1912.03359v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:04:15Z
FALSE,NA,Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Finance/economics|Other,Simulation study|Approximation methods|Other,TRUE,R|Fortran|Other,Not provided,NA,"This paper studies how parameter and model estimation errors degrade the performance of optimal reinsurance contracts, focusing mainly on single-layer (excess-of-loss with limit) treaties that are often optimal or near-optimal. The authors define degradation as the increase in the optimized objective when a contract optimized under estimated parameters is evaluated under the true parameters. Using asymptotic analysis, they show that for smooth objective criteria (including many utility- or convex-risk-measure-based setups and often CVaR-based criteria), degradation typically shrinks at rate O(1/n) with sample size n, whereas VaR-based criteria can be non-smooth at the optimum and may only achieve O(1/sqrt(n)). These results are supported by Monte Carlo and nested bootstrap studies under several claim severity models (Gamma, Lognormal, Pareto) and different premium principles (expected premium and a mixed Esscher-type premium). A Bayesian approach using the posterior predictive distribution is also proposed to incorporate parameter uncertainty, with simulations indicating informative priors can reduce degradation when historical data are limited.","The main objective is posed as a criterion C(a,θ) (notably risk-over-expected-surplus, e.g., C=ρ_{R_I}/G_I) with degradation D(θ)=C(â,θ)−C(a,θ), where a minimizes C(·,θ) and â minimizes C(·,θ̂). For smooth criteria, Proposition 3.1 gives nD(θ) ⇒ N^T Q N with Q=(1/2)(C_{aθ})^T (C_{aa})^{-1}(C_{aθ}) under √n(θ̂−θ)⇒N(0,Σ). For VaR-based non-smooth cases, Proposition 3.2 gives √n D(θ) ⇒ {h1(−V)_+ + h2 V}·√(g^TΣg), where V~N(0,1), g=∂x_ε(θ)/∂θ, and h1,h2 are functions of C(a,θ), a1, and K(1−ε).","Asymptotically, degradation is typically O(1/n) for smooth criteria (with mean approximately tr(QΣ)/n), but for VaR-based risk-over-surplus it can be only O(1/sqrt(n)) with explicit mean and variance formulas involving h1(a,θ) and g^TΣg. In nested bootstrap experiments for VaR, the reported mean degradation E[D(θ̂)] decreases with n (e.g., under expected premium: Gamma 0.255 at n=5000 vs 0.893 at n=500; Lognormal 0.289 vs 0.786; Pareto 0.378 vs 1.187), while at very small n=50 degradation becomes large (e.g., 3–5+). The authors also find the optimized ratio is relatively insensitive to the chosen severity family when distributions are calibrated to have similar mean and variance, suggesting a Gaussian approximation to total loss may be acceptable for this optimization objective in some settings. Bayesian simulations show informative priors generally reduce E[D] and sd[D] relative to non-informative priors when the effective historical portfolio size is small (Jh≈10^3), while differences diminish when Jh is large (Jh≈10^5).","The paper explicitly notes it does not tackle model uncertainty—i.e., the impact when the true claim severity family deviates from the assumed one—and leaves that for future research. It also highlights that asymptotic approximations are primarily valid for large samples and can be unreliable when n is very small (e.g., n=50), where degradation can be large and the asymptotics may not apply well.","The study focuses largely on single-layer contracts and VaR/CVaR-style criteria; results may not generalize to more complex multi-layer or constraint-rich treaty designs without further analysis. The empirical evaluation is simulation-based with calibrated parameter settings, so sensitivity to broader ranges of tail-heaviness, dependence structures (e.g., between X and market factor Z via copulas), and premium principle specifications is not exhaustively explored. Implementation details suggest reliance on heavy Monte Carlo/nested bootstrap, which may be computationally expensive in operational settings and may require variance-reduction methods for stability.",The authors propose investigating model uncertainty: how deviations between the true claim severity family and the assumed one affect optimal solutions and degradation. They also suggest examining whether the Gaussian approximation for total loss is a sensible approximation for this reinsurance optimization problem using similar degradation analyses.,"Extending the degradation theory and numerical evaluation to multivariate or dependent portfolios (multiple lines of business, correlated losses, or explicit X–Z dependence via copulas) would improve practical relevance. Developing robust/self-starting procedures that account for unknown parameters without heavy nested simulation (e.g., analytic approximations, influence-function-based corrections, or Bayesian computation efficiency improvements) could make the methods more deployable. Additional work could compare degradation under alternative industry risk measures and regulatory capital formulations beyond VaR/CVaR, and under imperfect/transaction-cost-laden reinsurance contracts.",1912.04175v1,https://arxiv.org/pdf/1912.04175v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:04:52Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling,"Stochastic process|Parametric (Weibull, etc.)|Other",Degradation measurements,Not applicable,Theoretical/simulation only,Other,TRUE,R,Not provided,NA,"The paper develops locally c-optimal stress-level designs for constant-stress accelerated degradation tests (CSADT) with repeated measurements, targeting precise estimation of a lifetime quantile at normal-use conditions. For a univariate monotone degradation path modeled as a gamma process with stress-dependent rate (log-link GLM form), it derives Fisher information and shows the asymptotic-variance criterion for a failure-time quantile reduces to a c-optimal extrapolation problem at the normal-use stress, yielding endpoint-supported optimal designs with an analytic optimal weight. It then extends to bivariate systems with two independent degradation/failure modes: (i) two independent gamma processes, and (ii) a gamma process plus a linear mixed-effects model (random intercept) for the second degradation component; in both cases the system failure time is the minimum of component failure times and the design criterion becomes a compound (weighted-sum) criterion across components. Optimal designs are computed with the multiplicative algorithm and found to allocate most units to the lowest stress and the remainder to the highest stress (e.g., weights around 0.78/0.22 in provided examples). Sensitivity analyses investigate robustness to misspecified nominal parameters, finding high robustness to some parameters (e.g., intercept) but greater sensitivity to the gamma-process slope parameter and to shifts in which component dominates system failure.","Gamma-process rate under stress: $\gamma(x)=\exp(\beta_0+\beta_1 x)$, with increments $Y\sim\mathrm{Gamma}(\text{shape }\gamma(x)\Delta,\text{scale }\nu)$. Soft-failure time at normal use $x_u$ is first passage to threshold $z_0$, with CDF $F_T(t)=Q(\gamma(x_u)t, z_0/\nu)$ (regularized gamma). The per-increment Fisher information has form $M_\beta(x,\Delta)=q(\beta_0+\beta_1 x+\ln\Delta)\begin{pmatrix}1&x\\x&x^2\end{pmatrix}$ and sums over measurement intervals; the design criterion is $\mathrm{aVar}(\hat t_\alpha)=c^T M_\beta(\xi)^{-1}c$ which reduces (univariate case) to a c-criterion with $c=(1,x_u)^T$ leading to two-point (endpoint) optimal designs with analytic weight (Eq. 2.11).","For the univariate gamma-process example with $x_u=-0.4$, $\nu=1$, $z_0=5.16$, $k=4$ equally spaced measurements ($\Delta=0.25$), and nominal $(\beta_0,\beta_1)=(0.23,0.53)$, the median failure time is reported as $t_{0.5}=5.39$ and the numerically found optimal endpoint design assigns weight $w^*=0.79$ at $x=0$ and $0.21$ at $x=1$. In that example, efficiencies at $x_u=-0.4$ for uniform designs are reported as 75% for the uniform two-point design and 55% for the uniform three-point design (relative to the locally optimal design). For the bivariate two-gamma-process example (additional component with $\nu_2=0.88$, $z_{20}=4.60$, $(\beta_{20},\beta_{21})=(0.31,0.35)$), the system median is $t_{0.5}=3.93$ and the locally optimal endpoint weight is $w^*=0.78$ at $x=0$ (0.22 at $x=1$). For the gamma+LMEM example (LMEM threshold $y_{20}=3.73$, fixed effects $(\beta_{20},\beta_{21},\beta_{22},\beta_{23})=(2.35,0.06,0.28,0.04)$, $\sigma_0=0.08$, $\sigma_\varepsilon=0.09$), the system median is $t_{0.5}=4.99$ and the locally optimal endpoint weight is again $w^*=0.78$ at $x=0$; robustness is high (e.g., reported efficiency about 0.9936 even at $\beta_{10}=1.92$ in a misspecification study).","The authors assume constant stress within each unit (CSADT) and uncorrelated/independent marginal degradation components; correlation or interaction between failure modes is not treated. Designs are locally optimal and require nominal parameter values; sensitivity analysis is provided but the approach still depends on the correctness of nominal values (notably for the gamma-process slope). Only gamma-process and linear mixed-effects (random-intercept) marginal models are developed in detail, though extensions are suggested.","The work focuses on stress-level allocation but largely treats the measurement-time plan as fixed; jointly optimizing stress levels with sampling times, termination time, and sample size/cost constraints would be necessary for many practical ADT plans. The independence assumption between degradation modes and the use of a soft-failure definition based on a latent mean path in the LMEM (rather than stochastic paths including measurement/process noise) may limit realism and could change optimality. The designs are derived for a standardized single stress variable over $[0,1]$ and endpoint designs; constraints such as maximum allowable stress, multiple stresses, or discrete stress settings in practice may alter feasibility. No shared implementation/code is provided despite reliance on numerical optimization (multiplicative algorithm), which may hinder reproducibility.","The authors state the methods can be extended to other marginal degradation/failure modes such as Wiener processes, inverse Gaussian processes, and non-linear mixed-effects degradation models. They also suggest considering optimality criteria that account for simultaneous estimation of multiple characteristics of the failure-time distribution (rather than a single quantile).","A natural extension is to incorporate dependence between degradation components (e.g., correlated random effects or copula-linked first-passage times) and study how correlation impacts the compound optimal design. Developing cost-constrained and practically implementable designs that jointly optimize stress levels, inspection times, termination rules, and sample size (possibly with discrete constraints) would increase applicability. Robust or Bayesian optimal design approaches could reduce sensitivity to misspecified nominal parameters, especially for slope parameters governing acceleration. Providing an open-source R implementation (or package) of the design algorithms and sensitivity analysis would improve reproducibility and adoption.",1912.04202v2,https://arxiv.org/pdf/1912.04202v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:05:34Z
FALSE,Other,Nonparametric/Semi-parametric|Simulation-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/google-research/rl-reliability-metrics,"This ICLR 2020 paper proposes a suite of quantitative metrics and companion nonparametric statistical tests to measure the reliability of reinforcement learning (RL) algorithms, focusing on performance variability and downside risk during training and after learning. Reliability is decomposed along three variability axes—across time within a run, across independent training runs, and across rollouts of a fixed learned policy—and two variability notions: dispersion (primarily via robust IQR) and risk (via Conditional Value at Risk, CVaR). The authors recommend reporting practices (normalization, ranking across tasks, time-frame aggregation) and provide inference procedures using bootstrap confidence intervals and exact permutation tests with multiple-comparisons correction (Benjamini–Yekutieli). They apply the metrics to common continuous-control (MuJoCo/OpenAI Gym) and discrete-control (Atari/Dopamine) benchmarks, showing that algorithms with strong median performance can exhibit poor reliability, and that reliability patterns can differ across axes. An open-source Python library is released to compute the metrics and statistical comparisons.","The key risk measure is conditional value at risk (CVaR): $\mathrm{CVaR}_\alpha(X)=\mathbb{E}[X\mid X\le \mathrm{VaR}_\alpha(X)]$, where $\mathrm{VaR}_\alpha$ is the $\alpha$-quantile. Across-time dispersion (DT) is computed as IQR within a sliding window on detrended training curves using first differences $y'_t=y_t-y_{t-1}$. Short-term risk across time (SRT) applies CVaR to normalized first differences (drops between consecutive evaluations), while long-term risk across time (LRT) applies CVaR to drawdown $\mathrm{Drawdown}_T = R_T - \max_{t\le T} R_t$; across-run (DR/RR) and fixed-policy rollout (DF/RF) metrics apply IQR or CVaR to filtered final performance distributions across runs/rollouts.","On MuJoCo continuous control (30 runs per (algorithm, environment)), SAC and TD3 achieve the best median performance during training, but SAC is consistently more reliable than TD3 on all during-training reliability metrics; both SAC and TD3 are relatively poor on after-learning reliability despite high median performance. On Atari (60 games, 5 runs per game/algorithm using Dopamine baselines), Rainbow has significantly better median performance than IQN, yet IQN is numerically or significantly better than Rainbow on several reliability metrics. Confidence intervals are estimated with 1,000 bootstrap resamples in the presented figures, and pairwise algorithm differences are tested using permutation tests (examples shown with 1,000 permutations in figure captions; later experiments use 10,000 permutations) with $\alpha=0.05$ and Benjamini–Yekutieli correction.","The authors note that comparisons for Long-term Risk across Time (LRT) are only meaningful when evaluation frequency during training is held constant across experiments, because LRT can be biased by evaluation frequency. They also emphasize that several user-specified parameters (e.g., window size, filter thresholds, evaluation frequency, training length, number of rollouts) can affect metric values and must be clearly reported and held constant for fair comparisons. They caution that reliability can vary by environment, recommending per-environment inspection rather than relying only on aggregated rankings.","Although termed “reliability,” the work addresses statistical stability/variability of RL performance rather than reliability engineering notions like failure rates, lifetime distributions, or dependability of physical components/systems; mapping these metrics to safety-critical reliability requirements is not established. The metrics largely assume independence/exchangeability of runs for permutation testing; correlations induced by shared hyperparameter tuning, shared codebases, or clustered experimental setups could violate this. Empirical validation is limited to standard RL benchmarks; the behavior of these metrics under nonstationary deployment conditions, real-world operational constraints, or distribution shift is not evaluated.",None stated.,"Extend the metrics and tests to settings with non-exchangeable or hierarchical experimental structure (e.g., multiple implementations/labs, shared tuning budgets) using mixed-effects or hierarchical resampling/permutation schemes. Develop reliability metrics that connect to safety constraints and operational risk (e.g., constraint violations, tail risk on costs) and validate them in real-world or high-stakes RL deployments. Provide self-normalizing or adaptive versions of the metrics that reduce sensitivity to evaluation frequency and other reporting degrees of freedom, especially for long-term drawdown-based measures.",1912.05663v2,https://arxiv.org/pdf/1912.05663v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:06:14Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Mixture of types|Simulated only|Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/wtagr/dbicc,"This paper proposes a distance-based intraclass correlation coefficient (dbICC) that generalizes the classical ICC to arbitrary data objects by defining reliability via ratios of between- and within-subject mean squared distances. It develops plug-in estimators for dbICC and constructs nonparametric bootstrap confidence intervals, introducing a bias correction that removes extra negative bias caused by duplicated subjects in bootstrap resamples. Simulation studies with multivariate normal data (Euclidean distance) show the bias correction improves bootstrap CI coverage, though small-sample negative bias remains for very small numbers of subjects. The method is illustrated on test–retest reliability of resting-state fMRI functional connectivity (correlation) matrices using several distance measures, finding generally low overall reliability for whole-brain connectivity and higher reliability within specific networks. The paper also generalizes the Spearman–Brown relationship between measurement intensity and reliability to dbICC under a Hilbert-space true-score framework, and empirically examines how increasing fMRI time-series length affects reliability (with attenuation under autocorrelation).","The proposed reliability index is the distance-based ICC: $\rho=1-\frac{\mathrm{MSD}_w}{\mathrm{MSD}_b}$, where $\mathrm{MSD}_b=E\{d(X_{i_1j_1},X_{i_2j_2})^2\}$ for $i_1\neq i_2$ and $\mathrm{MSD}_w=E\{d(X_{ij_1},X_{ij_2})^2\}$ for $j_1\neq j_2$. The sample estimator is $\hat\rho=1-\frac{\widehat{\mathrm{MSD}}_w}{\widehat{\mathrm{MSD}}_b}$ with between/within averages computed over all cross-subject and within-subject pairs of replicates. For measurement intensity $m$, the generalized Spearman–Brown relation is $\frac{\rho_m}{1-\rho_m}=\frac{\Delta_T}{\Delta_\varepsilon(m)}\propto \frac{1}{\Delta_\varepsilon(m)}$; for covariance matrices under normality, $\Delta_\varepsilon(m)\propto \frac{1}{m-1}$ so $\frac{\rho_m}{1-\rho_m}\propto (m-1)$.","In simulations with $X_{ij}\in\mathbb{R}^2$, $T_i\sim N_2(0,I_2)$ and $\varepsilon_{ij}\sim N_2(0,cI_2)$, the population dbICC is $\rho=1/(c+1)$, yielding $\rho\in\{0.2,0.5,0.8\}$ for $c\in\{4,1,0.25\}$. With $J_i=4$ and bootstrap resamples $B=1200$, the proposed bootstrap bias correction markedly reduces negative bias of bootstrap dbICC estimates (especially for $I=10$ and partly for $I=40$) and improves 95% CI coverage (Table 1 shows corrected coverage around ~90–95% vs naive often mid-80s to low-90s depending on $\rho$ and $I$). In the fMRI test–retest application (25 subjects, 2 scans), dbICC for all 333 ROIs is low (~0.378–0.382 across distances), while within the default mode network it is higher (~0.487–0.493) and within the visual network ~0.434–0.451 (Table 2). In simulations of covariance/correlation matrix estimation, the log–log slope of $\hat\rho_m/(1-\hat\rho_m)$ vs $m-1$ is ~1 under independence (~0.997–1.018) but is attenuated under VAR(1) autocorrelation (e.g., slopes ~0.96 or ~0.69 depending on autocorrelation strength), consistent with reduced reliability gains when data are autocorrelated.","The authors note that small-sample negative bias (as in classical ICC) remains for dbICC point estimates, leading to poor CI coverage for very small numbers of subjects (e.g., $I=10$) even after bootstrap bias correction. They also state that theoretical slope-1 improvements in reliability with increased fMRI time-series length cannot be expected in real data due to discrepancies from assumptions (e.g., autocorrelation, non-normality, nested subsets of the same series). They further caution that if true connectivity (true score) differs between test and retest scans for some participants, reliability may level off and not improve linearly with longer series length.","The method depends on the choice of distance; different distances can yield different dbICC values and interpretations, and guidance for selecting/validating distances is limited. The dbICC collapses complex object variability into a single scalar and may mask localized instabilities (e.g., specific edges/connections in networks) that practitioners might care about. The bootstrap procedure resamples subjects only; when within-subject replicates are few (e.g., $J=2$ in the fMRI example), estimation of within-subject variability may be unstable and sensitive to preprocessing choices, yet robustness to preprocessing pipelines is not systematically assessed. Also, the approach does not directly address dependence among subjects or hierarchical/multisite designs common in biomedical imaging reliability studies.","The authors suggest extending ideas from distance correlation research (interclass correlation via distances) to the intraclass setting as a potential avenue for future work. They also mention broader applicability of dbICC beyond test–retest, such as assessing reliability of stochastic algorithm outputs (e.g., bootstrap-based procedures).","Developing principled, data-adaptive guidance for selecting distances (or learning distances) and studying sensitivity of dbICC to distance choice would improve practical adoption. Extending dbICC inference to settings with autocorrelation/temporal dependence and to more complex designs (e.g., multilevel, multisite, varying numbers of replicates per subject) would make the approach more broadly usable. Robust/self-starting interval estimation (e.g., permutation or Bayesian approaches) could mitigate small-sample bias and improve coverage when $I$ is small. Providing standardized software workflows for common object types (networks, images, curves) and diagnostic tools to localize sources of unreliability would enhance interpretability for applied users.",1912.07137v2,https://arxiv.org/pdf/1912.07137v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:07:00Z
FALSE,NA,Other,Other,Not applicable,Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper is about active learning theory for binary classification under label and comparison-query noise (Massart and Tsybakov/Generalized Tsybakov noise), introducing algorithms to learn non-homogeneous linear separators efficiently. The main contribution is a set of query- and (for Massart noise) computationally efficient algorithms that achieve an “Almost Reliable and Probably Useful (ARPU)” guarantee: with high probability, the learner makes no classification errors on labeled outputs while allowing abstentions (“⊥”) and achieving high coverage. The methods combine noisy sorting/approximate ranking (building on Braverman and Mossel’s noisy sorting) to “clean” comparisons/labels with high probability, and an inference-based linear programming framework (building on inference dimension) to infer labels without errors. The paper also provides lower bounds showing that comparison queries can be necessary (sometimes labels-only ARPU learning is impossible or exponentially worse) and analyzes settings using inference dimension, average inference dimension, margin, and distributional concentration/anti-concentration assumptions. Overall, it advances selective/reliable active learning theory rather than reliability engineering (no failure-time/degradation/maintenance modeling).","The paper’s key definitions include ARPU learning conditions: usefulness via abstention loss $L_{\tilde{D}_L}(A(S))=\mathbb{E}[\mathbf{1}(A(S)(x)=\perp)]<\varepsilon$ with probability $\ge 1-\delta_u$, and reliability via $\Pr[\forall x,\ A(S)(x)\in\{h^*(x),\perp\}]\ge 1-\delta_r$. Noise is formalized by correctness probabilities $\beta_L(x)=\Pr[Q_L(x)=\mathrm{sign}(h^*(x))\mid x]$ and $\beta_C(x_1,x_2)=\Pr[Q_C(x_1,x_2)=\mathrm{sign}(h^*(x_1)-h^*(x_2))\mid x_1,x_2]$, with Massart requiring $\beta\ge 1/2+\lambda$ and GTNC/TNC bounding $\beta$ as a monotone function of distance to the boundary or score difference. Comparison queries are $\mathrm{sign}(h(x_1)-h(x_2))$; equitability is defined by $v(x)=|\{y\in S: y <_e x\}|$ with $(1/2\pm\varepsilon)|S|$ bounds.","For Massart noise, the paper gives query bounds such as $\tilde{O}(\frac{k}{\lambda^{10}}\log\frac{1}{\varepsilon}\log^2\frac{1}{\delta_r}\log\frac{1}{\delta_u})$ for ARPU learning when inference dimension is $k$ (plus polytime/sample bounds), and extends to average inference dimension with a corresponding $\tilde{O}(\frac{f(d)^{1/a}}{\lambda^{10}}\log^{2+1/a}(1/\varepsilon)\log^2(1/\delta_r)\log(1/\delta_u))$ form. It proves labels-only ARPU learning can be impossible on $S^1$ even in the noiseless/Massart setting (infinite label-query complexity for specified parameters), and gives an exponential lower bound (e.g., $\ge 2^{d-1}$) in a margin-based GTNC setting for labels-only. Under GTNC/TNC with additional assumptions (margin or concentration/anti-concentration), it provides ARPU-learning algorithms with polynomial dependence on parameters and shows polynomial lower bounds in $\varepsilon^{-1}$ are unavoidable in that regime. Theoretical guarantees are framed in terms of coverage (abstention rate), reliability probability, and active query complexity rather than empirical performance numbers.","The authors note that their Massart-noise computational efficiency is limited by the noisy sorting subroutine: Braverman–Mossel style sorting has unfavorable dependence on the noise parameter $\lambda$, and achieving efficiency as $\lambda\to 0$ remains difficult. They also state that for GTNC/TNC (unbounded noise), their approach requires additional restrictions (e.g., focusing on linear separators and assuming either margin or distributional concentration/anti-concentration). They further remark that providing an efficient sorting scheme with good pointwise movement bounds under Massart noise for small $\lambda$ is an open problem.","The work is purely theoretical and does not validate the methods on real datasets or provide implementation guidance, so practical robustness (e.g., to model misspecification, non-s-concave distributions, or non-Bayes-consistent oracle behavior) is unclear. The oracle model assumes persistent noise (re-querying the same point/pair returns the same answer), which may not match many practical labeling workflows where repeated queries can average out noise. The GTNC algorithm described includes computationally expensive steps (e.g., testing all subsets for equitability), suggesting impractical runtime despite query efficiency; scalability to large pools is not demonstrated. Comparisons are assumed to reflect a latent real-valued score $h(x)$ (distance-to-boundary proxy), which may not hold in applications where pairwise judgments are inconsistent or context-dependent.","They explicitly highlight as an open problem the design of an efficient (polynomial-in-error/noise) noisy sorting scheme that retains sublinear pointwise movement guarantees under Massart noise as $\lambda\to 0$, noting that such an advance would immediately improve their computational and query efficiency range. They also mention that follow-up works improve time complexity but only for constant-bounded $\lambda$, leaving the small-$\lambda$ regime unresolved.","A natural extension is to relax the persistent-noise assumption by allowing repeated queries and aggregating responses, and reanalyzing ARPU guarantees under i.i.d. (non-persistent) oracle noise. Another direction is to develop practical, implementable approximations of the ARPU procedures (including efficient equitability/cluster detection and LP inference) and benchmark them empirically against modern selective classification and active learning baselines. Extending the ARPU framework and bounds to multiclass classification, non-linear separators (kernels), or modern representation learning settings would broaden applicability. Finally, robustness analyses under distribution shift or oracle bias (systematic, non-symmetric noise) could make the reliability guarantees more relevant to real-world deployments.",2001.05497v1,https://arxiv.org/pdf/2001.05497v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:07:47Z
TRUE,Degradation modeling|Maintenance optimization|RUL prediction|Other,ML-based|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Other,Not applicable,Transportation/logistics,Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper proposes an intelligent road inspection approach that predicts Pavement Condition Index (PCI) from Falling Weight Deflectometer (FWD) surface deflection measurements, aiming to reduce safety risks and human error inherent in visual distress surveys. It develops four machine-learning predictors—MLP optimized with Levenberg–Marquardt (MLP-LM) and scaled conjugate gradient (MLP-SCG), and RBF networks optimized with a genetic algorithm (RBF-GA) and an imperialist competitive algorithm (RBF-ICA)—and then combines them via a weighted Committee Machine Intelligent System (CMIS). The models are trained and tested on 236 pavement segments from the Tehran–Qom freeway (Iran), using seven deflection sensors (geophones) as inputs and PCI as the target output. Performance is evaluated using APRE, AAPRE, RMSE, and SD; CMIS achieves the best overall accuracy among the compared methods. Practically, the method supports transportation maintenance planning by enabling PCI estimation from non-destructive structural measurements, facilitating safer and potentially more cost-effective network-level inspection workflows.","The PCI target is defined by the standard PCI procedure as $\mathrm{PCI}=100-\mathrm{CDV}_{\max}$ after computing deduct values and corrected deduct values. The RBF predictor is expressed as $\widehat{\mathrm{PCI}}=\sum_{j=1}^{N} w_j\,\varphi_j(\lVert D_i-c_j\rVert)$, where $D_i$ are FWD deflection inputs, $c_j$ are RBF centers, and $\varphi_j$ is the radial basis function. The ensemble (CMIS) combines outputs of the four base learners using optimized weights (reported coefficients for $C_1\dots C_5$), i.e., a weighted averaging committee. Evaluation metrics are computed as APRE, AAPRE, RMSE, and SD per equations (7)–(10) based on observed vs. predicted PCI.","On the full dataset, the CMIS ensemble reports APRE = 2.3303%, AAPRE = 11.6768%, RMSE = 12.005653, and SD = 0.021081, outperforming the individual models. For totals, MLP-LM yields AAPRE = 14.4877% and RMSE = 14.499431; MLP-SCG yields AAPRE = 15.1187% and RMSE = 14.367255. RBF-GA and RBF-ICA show worse generalization on the reported test split (e.g., RBF-GA test RMSE = 58.919147; RBF-ICA test RMSE = 45.214386), though their training errors are comparatively lower. The authors also report that the farthest sensor (geophone D7) has the highest relevancy factor/impact on PCI among the seven deflection inputs.",None stated.,"The study is based on a single roadway corridor (Tehran–Qom freeway) with 236 segments, which may limit external validity across different climates, materials, traffic spectra, and construction practices. The work focuses on predictive accuracy metrics (APRE/AAPRE/RMSE/SD) without reporting uncertainty bounds, calibration analysis, or robustness to measurement noise and sensor faults common in field FWD data. Details of data splitting strategy, hyperparameter selection, and potential leakage controls are limited, making it hard to assess reproducibility and fairness of comparisons. The method predicts PCI (a condition index) rather than directly modeling failure time, hazard, or life distributions, so translating results into quantitative reliability targets or maintenance optimization still requires additional modeling steps.","The authors suggest making the modeling more complete by incorporating results from other non-destructive tests (e.g., GPR) alongside FWD. They also propose using newer deflectometers such as RWD and TSD to record pavement surface deflections, noting these systems can operate at traffic speed and thus interfere less with traffic flow.","Validate the approach across multiple networks and regions (different pavement structures, environmental conditions, and traffic loads) and report cross-site transfer performance. Develop uncertainty quantification (e.g., prediction intervals) and reliability-oriented outputs (e.g., probability of PCI falling below a threshold within a planning horizon) to better support risk-based maintenance decisions. Extend the framework to integrate temporal data for true deterioration/degradation trajectory modeling rather than single-time prediction, enabling forecasting and maintenance optimization. Release a reproducible implementation (code and data preprocessing pipeline) and benchmark against additional modern methods (e.g., gradient boosting, random forests, Gaussian processes) under standardized splits and reporting.",2001.08583v1,https://arxiv.org/pdf/2001.08583v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:08:22Z
FALSE,NA,"ML-based|Parametric (Weibull, etc.)|Other",Other,Not applicable,Finance/economics,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/giorgiovisani/LIME%20stability,"The paper studies the stability (repeatability) of LIME explanations, noting that repeated runs under the same conditions can yield different selected features and coefficients due to LIME’s random sampling. It proposes two stability indices: the Variables Stability Index (VSI) measuring agreement in the set of selected features across runs, and the Coefficients Stability Index (CSI) measuring agreement of coefficient estimates via overlap of coefficient confidence intervals. The authors derive the (Gaussian) sampling distribution for weighted ridge-regression coefficients used by LIME and use it to construct confidence intervals rather than formal equality tests (complicated by ridge bias). The indices are demonstrated on a real credit-risk dataset comparing logistic regression vs gradient-boosting trees, and show that poor LIME parameter choices (e.g., kernel width, ridge penalty) can lead to very low VSI/CSI and thus unreliable explanations. The code for experiments is provided in a public GitHub repository.","LIME’s local surrogate is defined by $\arg\min_{g\in G} L(f,g,\pi_x)+\Omega(g)$. VSI is based on pairwise feature-set concordance $|F_\alpha\cap F_\beta|$ averaged and normalized by $p$. For CSI, the paper derives a Gaussian law for weighted ridge coefficients: $\hat\beta(\lambda)\sim \mathcal N\big((X^T W X+\lambda I_p)^{-1}X^T W X\beta,\; \sigma^2 (X^T W X+\lambda I_p)^{-1}X^T W X[(X^T W X+\lambda I_p)^{-1}]^T\big)$, then forms 95% CIs $[\hat\beta\pm 1.96\sqrt{\mathrm{Var}(\hat\beta)}]$ and defines CSI via the fraction of overlapping CI pairs averaged across features.","In the credit-risk case study (LIME run 10 times and explaining with the top 7 features), a “stable” LIME configuration yields VSI = 89.44% and CSI = 92.7%. A less suitable configuration (kernel width 1.3 and ridge penalty 0.001) yields VSI = 14.17% and CSI = 57.46%, indicating strong instability in selected variables and moderate instability in coefficients. Gradient boosting improves predictive performance over logistic regression by more than 3 Gini points on the test set (as shown in the Lorenz/Gini comparison figure). Reported runtime for computing the indices on a 4-core Intel i7 2.90GHz laptop is about 10–12 seconds for the shown settings.","The authors note that high VSI/CSI only ensures LIME explanations are concordant across repeated runs, not that they are faithful to (close to) the underlying black-box model; LIME may still return explanations that are not really close to the ML model. They also state that formal hypothesis testing for equality of ridge coefficients is tricky because ridge estimators are biased and their expected values depend on the sampled design matrix, so they rely on confidence intervals instead.","The stability metrics depend on the choice of LIME hyperparameters (kernel width, number of samples, feature count p) and on the CI-overlap heuristic; overlap is not a calibrated statistical test and can be sensitive to CI width (which varies with sampling size/weights and multicollinearity). The derivations assume the weighted ridge model assumptions and treat LIME’s generated samples as independent draws; in practice, LIME’s sampling scheme and discretization/perturbation choices may violate these assumptions, affecting CI validity. The approach measures stability for a single explained instance x at a time; it does not directly provide population-level stability guarantees across many instances or shifts in the data distribution.","The authors state that more research is needed to assess whether LIME explanations are truly close to the underlying machine-learning model (faithfulness), beyond mere concordance across runs. They also indicate the framework can be extended beyond tabular data to images and text as long as LIME’s local model is ridge regression.","Develop calibrated statistical tests or Bayesian hierarchical models for cross-run coefficient agreement that explicitly account for ridge bias and LIME’s sampling mechanism, rather than relying on CI overlap. Extend the indices to assess stability jointly across many instances (distribution of VSI/CSI) and under dataset shift, and study robustness to correlated features and alternative perturbation distributions. Provide standardized benchmarking and software implementations (e.g., a Python package) with recommended default settings and diagnostic plots to help practitioners tune LIME for both stability and faithfulness.",2001.11757v2,https://arxiv.org/pdf/2001.11757v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:09:01Z
TRUE,Reliability growth|Software reliability|Life distribution modeling|Other,"Stochastic process|Bayesian|Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Simulation-based",Event/count data|Complete lifetime data|Mixture of types|Simulated only,Not applicable,Semiconductor/electronics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops Bayesian estimation for the Power Law Process (PLP), i.e., a non-homogeneous Poisson process (NHPP) with intensity $\nu(t;\beta,\theta)=\frac{\beta}{\theta}(t/\theta)^{\beta-1}$, targeting software reliability growth modeling from software failure time data. The key methodological contribution is a Bayes estimator of the PLP shape parameter $\beta$ under the Higgins–Tsokos (H–T) loss function, leading to a closed-form expression in terms of posterior moment-generating integrals and requiring numerical integration for practical computation. The authors compare the proposed Bayesian estimator against maximum likelihood estimation (MLE) via Monte Carlo simulation across multiple sample sizes and multiple priors for $\beta$ (Burr XII, inverted gamma, Jeffreys, and kernel density priors), and then apply the approach to Crow’s real developmental testing failure-time dataset. Empirically, the Bayesian H–T estimator yields substantially smaller MSE than MLE for estimating $\beta$ (and consequently improved estimation of the intensity/reliability functions), but the results are sensitive to prior choice; Jeffreys and kernel priors tend to perform best when prior information is weak. The work advances software reliability growth estimation by emphasizing loss-function choice (H–T vs squared error) and by providing a practical prior-sensitivity study for PLP-based reliability assessment.","The PLP/NHPP intensity is $\nu(t;\beta,\theta)=\frac{\beta}{\theta}(t/\theta)^{\beta-1}$ and the MLEs are $\hat\beta=\frac{n}{\sum_{i=1}^n\log(t_n/t_i)}$ and $\hat\theta=\frac{t_n}{n^{1/\hat\beta}}$. Under Higgins–Tsokos loss with constants $f_1,f_2>0$, the Bayes estimator of $\beta$ is $\hat\beta_{B,HT}=\frac{1}{f_1+f_2}\ln\left(\frac{\int e^{f_1\beta}h(\beta\mid t)\,d\beta}{\int e^{-f_2\beta}h(\beta\mid t)\,d\beta}\right)$, where $h(\beta\mid t)$ is the posterior from the PLP likelihood and a chosen prior (Burr XII / inverted gamma / Jeffreys / kernel). The paper also uses an adjusted scale estimate $\hat\theta_{B,HT}=\frac{t_n}{n^{1/\hat\beta_{B,HT}}}$ and defines relative efficiency of intensity estimates via an IMSE ratio $RE=\frac{\int(\hat\nu(t)-\nu(t))^2dt}{\int(\hat\nu_{MLE}(t)-\nu(t))^2dt}$.","In simulation with sample size $n=40$ and 100,000 repetitions, the MSE of $\hat\beta$ (MLE) is about 0.011 while the Bayesian H–T estimate MSE is about 0.00051 across $\theta\in\{0.5,1.7441,4\}$ (Table 2), indicating an order-of-magnitude improvement. For a fixed true $\beta=0.7054$, averaged over 10,000 repetitions, MLE overestimates (e.g., 0.7840 at $n=20$) while the Bayesian H–T estimator is much closer (0.6753 at $n=20$ and 0.7042 at $n=160$) (Table 3). Using adjusted $\hat\theta$ based on $\hat\beta_{B,HT}$ improves $\theta$ estimation versus MLE (e.g., for true $\theta=1.7441$ at $n=40$, $\hat\theta_{MLE}\approx2.731$ vs $\hat\theta_{B,HT}\approx1.581$; Table 4). For intensity estimation, the reported relative efficiency $RE(\hat\nu_{B,HT},\hat\nu_{MLE})=0.0762<1$ (Table 8), favoring the Bayesian H–T approach; on Crow’s real data, the paper reports $\hat\beta_{B,HT}=0.501199$, $\hat\theta_{B,HT}=2.07144$, yielding $\hat\nu_{B,HT}(t)=0.347933\,t^{-0.498801}$.","The authors state that for several priors (e.g., Jeffreys, inverted gamma, kernel) they cannot obtain a closed-form Bayes estimator under the H–T loss and therefore must rely on numerical methods (numerical integration) to compute $\hat\beta$. They also report that the Bayesian estimator is sensitive to prior selection, and recommend Jeffreys or kernel priors when prior knowledge of $\beta$ is lacking.","The analysis largely assumes the PLP/NHPP model is correctly specified (independent increments, no simultaneous failures, PLP form of intensity), but offers limited robustness checking for model misfit (e.g., overdispersion, time-varying testing effort, dependence). The paper focuses on point estimation/MSE and relative efficiency, with limited uncertainty quantification for practitioners (e.g., credible intervals for $\beta$, $\theta$, reliability, or predictive intervals for next failure time). Implementation details for numerical integration and kernel bandwidth selection are not fully specified to ensure reproducibility, and no software/code is provided. The real-data validation is primarily one dataset (Crow) and may not generalize across different software testing regimes or censoring/termination mechanisms.",None stated.,"Extend the approach to handle common software-testing complications such as right truncation/censoring, imperfect debugging, and nonstationary testing effort (which can violate PLP assumptions). Provide full Bayesian uncertainty quantification (credible bands for intensity/reliability and predictive distributions for future failures) and compare against other software reliability growth models (e.g., Goel–Okumoto, S-shaped models) under matched conditions. Develop and release reproducible software (e.g., an R/Python package) implementing the numerical integration, prior fitting, and kernel bandwidth selection, and validate across multiple public software failure datasets.",2002.00351v1,https://arxiv.org/pdf/2002.00351v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:09:49Z
TRUE,Degradation modeling|Maintenance optimization|System reliability|Other,"Stochastic process|Parametric (Weibull, etc.)|Simulation-based|Other",Degradation measurements|Other,Condition-based|Age-based|Block replacement|Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a reliability and maintenance model for a repairable multi-component (series) system where components can be replaced individually at periodic inspections, leading to different initial ages/health states across components at each interval. Each component is subject to competing soft failure (gamma-process degradation reaching a threshold) and hard failure (shock magnitude exceeding a threshold), with mutual dependence: shocks cause abrupt degradation jumps, and the system’s cumulative degradation increases the shock-arrival intensity (facilitation/dependence model). Based on this reliability model, the authors propose a dynamic inspection policy that selects the next inspection interval as a function of the components’ current initial ages/degeneration levels by minimizing an average cost rate that includes inspection, replacement, and downtime penalty costs. Performance is illustrated on a jet pipe servo-valve example (spool and sleeve), comparing an independent-shock (Poisson) model against the mutually dependent model; mutual dependence yields lower reliability and shorter optimal inspection intervals. Reliability and cost-rate curves are evaluated numerically via Monte Carlo simulation (10^5 replications) and numerical search for the cost-minimizing inspection interval.","Component pure degradation is modeled as a gamma process: increments satisfy $X_i(t)-X_i(s)\sim\text{Gamma}(\alpha_i(t)-\alpha_i(s),\beta_i)$ with linear shape $\alpha_i(t)=\alpha t$. Total component degradation within an inspection interval is $X_{Si}(t)=u_i+X_i(t)+\sum_{j=1}^{N(t)}Y_{ij}$, where $u_i$ is the component’s initial age/degradation at the interval start and $Y_{ij}$ are shock-induced degradation jumps (assumed Normal). Hard-failure avoidance for a single shock is $P(W_{ij}<D_i)=\Phi((D_i-\mu_{Wi})/\sigma_{Wi})$, while the shock count distribution uses a facilitation process with intensity $\lambda_i(t)=(1+\eta i)\lambda_0(t)$ and degradation-dependent baseline $\lambda_0(t)=\lambda_0+\gamma X_S(t)$ (system cumulative degradation surrogate). The dynamic inspection interval is chosen by minimizing the cost rate $CR(\tau;\mathbf{u})=(C_I+C_R(1-R(\tau;\mathbf{u}))+C_\rho\,\mathbb{E}[\rho])/\tau$.","In the jet pipe servo-valve example, reliability under the mutually dependent shock–degradation model (Model 2) is consistently lower than under the independent Poisson-shock model (Model 1), because degradation increases the shock-arrival intensity and shocks add degradation jumps. Sensitivity analysis shows higher degradation–shock dependence factor $\gamma$ decreases reliability (more shocks arrive as degradation grows). Optimal inspection intervals are shorter under mutual dependence: e.g., when initial ages are (spool,sleeve)=(0,0), $\tau^*\times10^{-4}$ is 1.432 (Model 1) vs 0.741 (Model 2); when (4,4), 0.441 (Model 1) vs 0.083 (Model 2). Across scenarios, larger initial ages/degradation levels lead to smaller optimal inspection intervals to reduce failure and downtime costs.","The authors note the model and numerical illustration assume a series system configuration; extending the dynamic inspection plan to other system configurations is left for future work. They also assume all incoming shocks are damaging (“fatal”) and always contribute to degradation and affect subsequent shock intensity; they state that incorporating fatal vs. non-fatal shocks would be a useful extension. Finally, only replacement is considered as the maintenance action, and they explicitly mention extending to imperfect/minimal repair and different shock patterns per component as future work.","The method relies on a specific mutual-dependence structure (facilitation model and linear degradation-to-intensity link using a surrogate sum of component degradations), which may be misspecified for some systems and could materially affect optimal intervals. Shock magnitudes and shock damages are assumed i.i.d. Normal, which can be unrealistic for nonnegative damages and heavy-tailed shocks; robustness to distributional misspecification is not established. The dynamic policy is evaluated primarily through simulation on a single example without benchmarking against alternative CBM/predictive policies (e.g., component-level decisions, opportunistic/group replacement, or self-starting/unknown-parameter uncertainty). Parameter estimation is described at a high level (MLE) but practical identifiability and requirements (sample size, inspection frequency, censoring) are not demonstrated.",The authors suggest extending the dynamic inspection planning approach beyond series systems to other system configurations. They propose relaxing the assumption that all shocks are fatal by modeling multiple shock types (fatal vs. non-fatal) and their differing impacts on degradation and shock intensity. They also explicitly mention considering other maintenance actions such as imperfect or minimal repair (beyond replacement) and allowing different shock patterns for each component.,"A valuable extension would be to incorporate parameter uncertainty (Bayesian or robust optimization) so inspection intervals account for estimation error in $\alpha,\beta,\gamma,\eta$ and shock/damage distributions. The framework could be generalized to partial observability/noisy condition monitoring (sensor-based CBM) rather than only periodic perfect inspections, including filtering of degradation states. Comparative studies against established multi-component CBM policies (e.g., opportunistic maintenance, group replacement, k-out-of-n structures, and component-level adaptive thresholds) would clarify practical benefits. Finally, developing and releasing software (e.g., an R/Python package) and validating on real field degradation/shock datasets would improve reproducibility and adoption.",2002.00427v1,https://arxiv.org/pdf/2002.00427v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:10:34Z
TRUE,Life distribution modeling|System reliability|Other,Simulation-based|ML-based|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,http://www.uqlab.com/userguide-reliability|http://www.uqlab.com/userguidekriging,"The paper proposes REAK (Reliability analysis through Error rate-based Adaptive Kriging), an adaptive Kriging/Gaussian-process surrogate reliability method aimed at estimating small failure probabilities with far fewer expensive model evaluations. Its key novelty is an analytically derived conservative upper bound on the failure-probability estimation error (maximum error rate $\hat\epsilon_{max}$), obtained by modeling wrong-sign classifications of the limit-state function and applying a CLT extension under Lindeberg’s condition. REAK also introduces effective sampling regions (ESR) that initially restrict candidate points to high joint PDF regions and then adaptively expand by decreasing an ESR parameter $\alpha$, balancing cost and accuracy. The algorithm uses a Kriging active-learning loop (EFF learning function) within ESR, and stops when $\hat\epsilon_{max}\le \epsilon_{thr}$ (plus a Monte Carlo coefficient-of-variation check for the candidate pool). Across four benchmark problems (2D series system, modified Rastrigin, 6D nonlinear oscillator, 9D cantilever tube), REAK reduces expensive model calls by up to ~50% versus AK-MCS and ISKRA while maintaining target relative error thresholds (e.g., 1–5%).","Reliability is defined via a limit-state function with failure event $g(\mathbf{X})\le 0$, so $P_f=P(g(\mathbf{X})\le 0)$ and crude MCS uses $\hat P_f=\#\{g(\mathbf{X})\le 0\}/N_{MCS}$. Kriging (Gaussian process) surrogate gives mean/variance $(\mu_K(\mathbf{x}),\sigma_K^2(\mathbf{x}))$ and the EFF learning function (Eq. 11) selects new training points, with a learning stop such as $\max(EFF)\le 10^{-3}$. REAK defines an effective sampling region by retaining candidate points whose joint PDF exceeds a threshold set through $P\{\rho(\mathbf{x})\le \rho_{thr}\}=\alpha\,\hat P_f$ and adaptively decreases $\alpha$. The paper derives a conservative maximum relative error bound $\hat\epsilon_{max}$ (Eq. 36) based on the distribution of wrong-sign surrogate classifications (Bernoulli sums with CLT/Lindeberg) and uses the stopping rule $\hat\epsilon_{max}\le \epsilon_{thr}$, along with a candidate-pool adequacy check $COV_{\hat P_f}=\sqrt{(1-\hat P_f)/(\hat P_f N_{MCS})}\le COV_{thr}$.","In the four-boundary series system example (2D), for $\epsilon_{thr}=0.05$ REAK used 60 total performance-function calls (12 initial + 48) versus 125 for AK-MCS and 120 for ISKRA, achieving true relative error $\epsilon=0.0216$ with estimated bound $\hat\epsilon_{max}=0.0375$. In the modified Rastrigin example, for $\epsilon_{thr}=0.05$ REAK required 230 calls versus 520 (AK-MCS) and 559 (ISKRA), with $\epsilon=0.0263$ and $\hat\epsilon_{max}=0.0438$; for $\epsilon_{thr}=0.01$, REAK achieved $\epsilon=0.0043$ and $\hat\epsilon_{max}=0.0051$. In the 6D nonlinear oscillator, for $\epsilon_{thr}=0.05$ REAK used 30 calls (12+18) versus 61 (AK-MCS) and 60 (ISKRA), with $\epsilon=0.0045$ and $\hat\epsilon_{max}=0.0162$. In the 9D cantilever tube, for $\epsilon_{thr}=0.05$ REAK used 56 calls (12+44) versus 84 (AK-MCS) and 71 (ISKRA), with $\epsilon=0.0195$ and $\hat\epsilon_{max}=0.0218$; across 50-run averages, REAK’s $\hat\epsilon_{max}$ was typically close to but above $\epsilon$ with success probabilities near the nominal 95% confidence level.","The authors note that very high-dimensional reliability problems remain challenging because Kriging requires repeated inversion of large covariance matrices; they suggest dimension-reduction methods to address this. They also state that REAK assumes wrong-sign estimation events across candidate points are uncorrelated; incorporating correlations could improve accuracy (especially for larger error thresholds) but would substantially increase computational complexity. They further mention that integrating REAK with importance sampling or subset simulation could reduce computational demand further, and that parallel/unsupervised selection of multiple training points could improve efficiency.","The method’s error bound depends on several approximations (e.g., treating candidate-point misclassification indicators as independent and applying CLT/Lindeberg), and the conservativeness/coverage may vary with surrogate kernel choice, training design, and strong spatial correlation typical in GP predictions. REAK’s ESR mechanism relies on ranking by input joint PDF, which can be less effective for problems with non-Gaussian dependencies, multiple important regions, or failures driven by low-density tails (where truncation may bias $P_f$ unless carefully controlled). Comparisons are mostly on standard benchmarks with specified sampling budgets; broader validation on real industrial models and on strongly non-stationary/heteroscedastic response surfaces would be needed. No publicly available implementation is provided, which may limit reproducibility and adoption.","The paper suggests improving performance on very high-dimensional problems via dimension reduction techniques. It proposes extending the framework to account for correlations among Kriging wrong-sign estimates to improve maximum-error estimation, particularly for higher error thresholds, while managing added computational complexity. The authors also suggest integrating REAK with importance sampling and subset simulation for further efficiency gains. Finally, they propose leveraging unsupervised learning to select multiple training points simultaneously to enable parallelization.","Developing a robust/self-starting REAK variant that explicitly handles unknown input distributions, dependent random variables, and non-Gaussian copulas would improve generality. Extending REAK to time-dependent reliability, system reliability with multiple limit states, and rare-event simulation settings where failure domains are disconnected could broaden applicability. Providing theoretical guarantees or empirical calibration procedures for $\hat\epsilon_{max}$ coverage under GP correlation (e.g., via bootstrap, conservative dependence bounds, or Bayesian posterior error quantification) would strengthen the stopping rule. Packaging the method into an open-source toolbox (e.g., MATLAB/Python) with standardized benchmarks would enhance reproducibility and practitioner uptake.",2002.01110v1,https://arxiv.org/pdf/2002.01110v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:11:25Z
FALSE,NA,ML-based,Other,Not applicable,Other,Simulation study,TRUE,None / Not applicable,Not provided,http://yann.lecun.com/exdb/mnist,"This paper studies how incorporating prediction-confidence calibration objectives during training of an over-parameterized neural network affects the quality of Lottery Ticket Hypothesis (LTH) “winning tickets” obtained via pruning and rewinding. The authors evaluate several calibration strategies—Mixup, variance-weighted confidence calibration (VWCC), a newly proposed likelihood-weighted confidence calibration with stochastic inferences (LWCC-SI), marginal distribution alignment (MDA), and a new normalized bin assignment (NBA) regularizer—and compare them to standard cross-entropy training. Winning tickets are selected via iterative magnitude pruning (local for MNIST/Fashion-MNIST; global for CIFAR-10 ResNet-18) and retrained using late resetting (rewinding after one epoch). Empirical results on MNIST/Fashion-MNIST show minor gains, while CIFAR-10 shows more consistent improvements in both accuracy and calibration quality (ECE/NLL/Brier). Ticket reusability experiments under distribution shifts (CIFAR-10a→10b and CIFAR-10→CIFAR-10-C corruptions) indicate calibrated-source tickets often generalize better and yield better-calibrated predictions than baseline LTH tickets, with particularly large gains under severe corruptions.","The paper augments cross-entropy with calibration regularizers. Examples include VWCC: $\sum_i (1-\alpha_i)\,L_{ce}^i + \alpha_i\,D_{KL}(U(y)\|p(\hat y_i\mid x_i))$, where $\alpha_i$ is derived from prediction variance across $T$ stochastic (dropout) forward passes. The proposed LWCC-SI uses $L_{ce}^i + \lambda\,\beta_i D_{KL}(U(y)\|p(\hat y_i\mid x_i))$ with $\beta_i=(1-\max(\hat y_i))\,\mathbb{I}(y_i=\hat y_i)$, emphasizing high-entropy outputs when wrong and penalizing low likelihood when correct. NBA adds a regularizer to encourage near-uniform occupancy of confidence bins: $\sum_i L_{ce}^i + \gamma_n\sum_{b=1}^B w_b\left|\frac{N_b}{N}-\frac{1}{B}\right|$, implemented via a differentiable soft histogram.","Across MNIST and Fashion-MNIST with LeNet-300-100, calibration-aware training yields similar accuracy to basic LTH with only marginal improvements in calibration metrics, consistent with these tasks/models already being relatively well-calibrated. On CIFAR-10 with ResNet-18, calibrated training (notably Mixup, VWCC, LWCC-SI) produces winning tickets with better accuracy and consistently improved calibration (lower ECE, NLL, and Brier score) across compression ratios compared to basic LTH. Under distribution shifts, calibrated-source tickets provide larger gains: for CIFAR-10a→CIFAR-10b, the paper reports about ~1% accuracy improvement over basic LTH across compressions (vs ~0.3–0.5% in the non-shift CIFAR-10 setting). For CIFAR-10→CIFAR-10-C (severity level 5 corruptions), improvements can be very large (reported up to ~10–12% for corruptions like fog/frost/snow at higher compression), alongside better calibration scores.","The authors note that for simpler tasks/models (MNIST/Fashion-MNIST with a small fully connected network), adding calibration objectives provides only minor improvements, suggesting such models may already be well-calibrated. They also observe that strengthening regularization (e.g., increasing dropout rate for VWCC/LWCC-SI) can reduce accuracy on already well-calibrated models, indicating over-regularization risk. They additionally remark that calibration is task-specific and is not guaranteed to be preserved under transfer to a different task, although they focus on same-task distribution shifts.","The study is entirely empirical and does not provide theoretical guarantees relating calibration objectives to the probability of finding better lottery tickets or to pruning optimality. Results depend on specific architectures (LeNet-300-100, ResNet-18), pruning schedules (iterative magnitude pruning; local/global choices), and hyperparameter tuning for each calibration method; sensitivity analyses and fair tuning budgets across methods are not fully characterized in the provided excerpt. The “reliability” notion is limited to probabilistic calibration metrics (ECE/NLL/Brier) rather than operational reliability outcomes, and it is unclear how robust the conclusions are under other forms of shift (label shift, adversarial shift) or with unknown/estimated control limits for calibration metrics in deployment.","The authors motivate further research to better understand the role of sub-network selection beyond changing pruning criteria, specifically by designing networks that are not only accurate but also better calibrated to meaningful priors. They also suggest the findings are relevant to efforts on identifying performant subnetworks in randomly initialized networks without training, implying calibration strategies could help identify more effective subnetworks in terms of both generalization and reliability.","It would be valuable to evaluate whether the observed gains persist under more diverse architectures (e.g., larger ResNets/ViTs) and alternative pruning methods (structured pruning, one-shot pruning, gradient-flow methods) with matched compute budgets. Extending the approach to settings with unknown class priors, label noise, or temporal/streaming shifts could clarify practical deployment value. Developing open-source, reproducible implementations and standardized benchmarks for “ticket reliability” (including uncertainty quantification beyond calibration, such as selective classification or risk-coverage) would strengthen adoption and comparability.",2002.03875v3,https://arxiv.org/pdf/2002.03875v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:12:07Z
TRUE,Other,ML-based|Hybrid/Ensemble,Other,Not applicable,Healthcare/medical|Manufacturing (general)|Transportation/logistics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"This paper proposes a variational encoder-based Epistemic Classifier (EC) to improve the reliability of individual machine-learning predictions by attaching justified-belief assertions: IK (trust), IMK (confusion), and IDK (extrapolation). The method jointly trains a modified variational autoencoder (VAE) with a classifier to learn a low-dimensional latent space in which Euclidean (ℓ2) distance better reflects semantic similarity; k-nearest-neighbor support is computed in this latent space using training exemplars. In addition to neighborhood support, the approach uses reconstruction quality (via MSE and SSIM thresholds) as an extra justification signal, increasing uncertainty when reconstruction is poor (e.g., under adversarial perturbations). Experiments on MNIST, Fashion-MNIST, and GTSRB show that combining latent-space support with reconstruction-based justification improves “IK accuracy” and more effectively flags adversarial examples compared with a softmax-thresholding baseline. The work advances reliability/uncertainty handling in classification by providing exemplar-based evidence and explicit reject/uncertainty regions rather than relying on confidence scores alone.","The EC support in layer i is defined as $S_i(x)=\{f(\omega):\omega\in X,\ h_i(\omega)\in N_i(h_i(x))\}$, where $N_i$ is a neighborhood operator over training activations; in this paper support is computed only in the VAE latent code (z) using k-NN with ℓ2 distance. The modified VAE training objective adds a classification term to the standard VAE bound: $\mathcal{L}=\mathbb{E}_X\mathbb{E}_{Q(z|X)}[-\log P(X|z)] + \mathbb{E}_X[H(Q(z|X),P(z))]-\mathbb{E}_X[H(Q(z|X))] + \lambda_C\mathbb{E}_X[H(C(Y|z),C^*(Y|X))]$. The justification operator uses reconstruction-quality gating: $J(x)=S_z(x)$ if reconstruction is “Good” (MSE and SSIM below thresholds), otherwise $J(x)=S_z(x)\cup\varphi$ to inflate uncertainty; IK/IMK/IDK are decided by comparing the predicted label to $J(x)$.","On an expanded MNIST test set with 50% BIM-attacked samples (attack magnitude 0.2), the softmax-threshold baseline attains coverage $F_{IK}\approx 0.73$ but low IK-accuracy $A_{IK}\approx 0.68$, while the combined support+reconstruction method achieves much higher $A_{IK}\approx 0.99$ with $F_{IK}\approx 0.53$ (appropriate given half the data are attacked). Across MNIST/Fashion-MNIST/GTSRB, the combined method yields near-perfect IK accuracy on nominal data (e.g., MNIST $A_{IK}=0.999$, Fashion-MNIST $A_{IK}=0.985$, GTSRB $A_{IK}=0.996$) while sharply reducing IK coverage under FGSM/BIM attacks (often close to 0), indicating strong attack identification by asserting IMK/IDK. For example, on MNIST under FGSM, combined gives $F_{IK}=0.002$ with $A_{IK}=0.950$ (very few samples remain in IK), whereas the baseline keeps $F_{IK}=0.313$ with $A_{IK}=0.315$ (many attacked samples incorrectly treated as confident IK). The BIM results show complementarity: combining support and reconstruction lowers $F_{IK}$ versus using either support-only or reconstruction-only, improving detection of harder attacks.","The authors note that the framework does not enforce a class-conditional posterior distribution structure in the latent space, so the ideal neighborhood/support size may vary by class; they propose enforcing similar per-class distributions in future work. They also state that VAE reconstructions are smooth, which can increase SSIM loss and contributes to lower IK coverage under larger uniform-noise perturbations. They further mention that only gray-box/semi-white-box robustness is shown and that full white-box attacks targeting both classification and reconstruction remain to be studied.","The approach relies on k-NN search over latent codes from the full training set (ball-tree), which may become memory- and latency-intensive for large-scale datasets or high-throughput deployments, and the paper does not quantify runtime/throughput costs. Threshold selection for reconstruction (percentiles on a validation set) and k choice may be sensitive to dataset shift; robustness to distribution shift beyond adversarial/noise perturbations is not fully evaluated. The “reconstruction quality” decision uses fixed metrics (MSE/SSIM) that may not align with task-relevant semantic fidelity for non-image modalities or more complex images, limiting generalizability without metric adaptation. Comparisons are primarily against softmax-thresholding and ablations; broader uncertainty/reject-option baselines (e.g., MC dropout, deep ensembles, conformal prediction, energy-based OOD scoring) are not included, so relative standing vs. modern methods is unclear.",They plan to enforce more uniform class-conditional structure in the latent space (following an approach in their cited reference [10]) to reduce class-dependent sensitivity of neighborhood size. They will explore alternative dissimilarity metrics to mitigate the impact of smooth VAE reconstructions (which can inflate SSIM loss) and improve behavior under larger noise. They also plan to study full white-box attacks that jointly target both the classifier and the reconstruction/justification mechanisms.,"A natural extension is a self-calibrating or conformalized version of the IK/IMK/IDK assertions with formal coverage guarantees under exchangeability, enabling principled threshold selection. Scaling to large datasets could be improved via approximate nearest-neighbor indexing, learned similarity metrics, or prototype/coreset support sets with quantified impact on reliability. Evaluating robustness under real-world distribution shift (sensor changes, lighting/weather for traffic signs, etc.) and comparing against ensemble/Bayesian uncertainty baselines would clarify practical benefits beyond adversarial settings. Extending the method to multivariate or sequential sensor data (condition monitoring) and to out-of-distribution detection benchmarks would test generality of reconstruction+support justification beyond images.",2002.08289v2,https://arxiv.org/pdf/2002.08289v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:12:51Z
FALSE,NA,ML-based,Simulated only|Other,Not applicable,Other,Exact distribution theory|Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),github.com/clovaai/generative-evaluation-prdc,"This paper studies the reliability of commonly used fidelity/diversity evaluation metrics for generative models, showing that improved precision/recall can fail key sanity checks (e.g., not returning optimal values for identical real and generated distributions), are sensitive to outliers, and depend on arbitrary hyperparameters. The authors propose two replacement metrics—density (a softened version of precision that counts how many real-sample neighborhoods contain each generated sample) and coverage (a diversity measure computed using neighborhoods around real samples)—to improve robustness and interpretability. They derive analytic expectations for density and coverage under the identical-distribution setting, enabling principled hyperparameter selection, and validate behavior via simulations and real-world experiments on GAN outputs. Experiments demonstrate better robustness to outliers and more responsive detection of mode dropping than prior precision/recall variants. The paper also evaluates embedding choices, arguing that randomly initialized CNN embeddings can be preferable when the target domain differs substantially from ImageNet.","Improved precision/recall use k-NN manifolds defined as $\mathrm{manifold}(X_{1:N})=\cup_{i=1}^N B(X_i,\mathrm{NND}_k(X_i))$, with $\mathrm{precision}=\frac{1}{M}\sum_{j=1}^M \mathbf{1}[Y_j\in \mathrm{manifold}(X_{1:N})]$ and $\mathrm{recall}=\frac{1}{N}\sum_{i=1}^N \mathbf{1}[X_i\in \mathrm{manifold}(Y_{1:M})]$. The proposed density is $\mathrm{density}=\frac{1}{kM}\sum_{j=1}^M\sum_{i=1}^N \mathbf{1}[Y_j\in B(X_i,\mathrm{NND}_k(X_i))]$, and coverage is $\mathrm{coverage}=\frac{1}{N}\sum_{i=1}^N \mathbf{1}[\exists j:\ Y_j\in B(X_i,\mathrm{NND}_k(X_i))]$. For identical real/fake distributions, they show $\mathbb{E}[\mathrm{density}]=1$ and derive $\mathbb{E}[\mathrm{coverage}]=1-\frac{(N-1)\cdots(N-k)}{(M+N-1)\cdots(M+N-k)}$, which approaches $1-2^{-k}$ when $M=N\to\infty$.","In a Gaussian toy setting where real and fake distributions match ($\mu=0$), improved precision/recall are substantially below 1 (reported about 0.68 precision and 0.67 recall), whereas the proposed metrics are near their ideal values (about 1.06 density and 0.97 coverage). With a single outlier added, precision/recall show strong pathological changes (e.g., increasing in the wrong direction as the mean shift grows), while density/coverage remain close to the no-outlier baseline. In real-image outlier experiments (StyleGAN on CelebA and LSUN-bedroom), recall increases by more than ~11% and ~15% as outliers are added, while coverage changes by less than ~2%. For hyperparameter selection, they recommend $M=N=10{,}000$ and $k=5$ to achieve $\mathbb{E}[\mathrm{coverage}]\approx 0.969>0.95$ under the identical-distribution calibration.",None stated.,"The proposed metrics depend on the choice of embedding space (e.g., ImageNet-pretrained vs random CNN features), so conclusions about fidelity/diversity can still vary with representation quality and may not align with human perception in all domains. Density/coverage rely on k-NN distances and Euclidean geometry, which can be sensitive in high dimensions and under correlated/autocorrelated sample generation; computational costs may be nontrivial at large $N,M$ despite being improved over recall computed on fake manifolds. The analytic calibration (e.g., selecting $k,N,M$ via $\mathbb{E}[\mathrm{coverage}]$) guarantees behavior under the identical-distribution null but does not directly optimize detection power for specific practical failure modes or for mismatched sample sizes ($M\neq N$).","They suggest practitioners and future researchers adopt density and coverage for more stable, reliable diagnosis of generative models and emphasize further attention to the embedding component, recommending random embeddings when the evaluation domain differs substantially from ImageNet.","Develop self-calibrating or adaptive versions of density/coverage that adjust $k$ locally based on estimated intrinsic dimension or local density to improve robustness in heterogeneous manifolds. Extend the metrics and theory to settings with dependent samples (e.g., diffusion sampling chains, video generation) and to structured outputs (text, sequences) with task-appropriate distances. Provide standardized benchmarking across multiple embedding families (self-supervised, domain-specific, multimodal) and analyze correlation with human preference to quantify when random vs pretrained embeddings are appropriate. Release optimized implementations (e.g., approximate nearest neighbors) and uncertainty estimates (confidence intervals via bootstrap) for practical reporting in model comparisons.",2002.09797v2,https://arxiv.org/pdf/2002.09797v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:13:33Z
FALSE,Other,ML-based|Other,Other,Not applicable,Network/cybersecurity|Transportation/logistics|Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"This paper surveys and proposes an architecture for applying deep learning (supervised DNNs and deep reinforcement learning) to ultra-reliable and low-latency communications (URLLC) in 6G networks. It introduces a multi-level intelligence framework (device, edge, and cloud) that combines model-based communication/queueing analysis with data-driven learning, and uses deep transfer learning to adapt pre-trained models under non-stationary network conditions. It also advocates hierarchical federated learning to address limited local data, compute constraints at devices/edge servers, and privacy/communication overhead of centralized data collection. The paper reports user-level experiments on mobility prediction with a tactile device, cell-level experiments on DRL-based scheduler design using LTE radios, and network-level simulations for user association in 5G NR, demonstrating reliability/latency tradeoffs and adaptation benefits. Overall, the reliability discussed is communications QoS reliability (packet loss/violation probabilities), not reliability engineering of physical components or systems.","Key expressions include (i) a policy mapping in wireless networks written as $y=f(x)$ from network state $x$ to decision $y$, and (ii) a DNN approximation $\hat{y}=\Phi(x;\Theta)$ with parameters $\Theta$ used to approximate $f(\cdot)$. For hierarchical federated learning aggregation, the paper describes edge-model aggregation $\Theta^{E}_{l}=\sum_k w^{L}_{k}\Theta^{L}_{k}$ and global aggregation $\Theta^{G}=\sum_l w^{E}_{l}\Theta^{E}_{l}$ with weights defined as in the cited reference. Reliability metrics are expressed as probabilities (e.g., prediction error probability; packet loss probability composed of decoding error and delay-violation components), though no single new closed-form reliability equation is introduced as the main contribution.","In the user-level mobility prediction experiment (1 ms sampling), the DNN trained with $10^{4}$ samples achieved prediction error probabilities on the order of $10^{-6}$ to $10^{-5}$ depending on horizon/accuracy; e.g., for 0.5 cm accuracy the data-driven method reported $4.59\times10^{-6}$ (5 ms), $6.86\times10^{-6}$ (10 ms), and $2.25\times10^{-5}$ (20 ms). In the cell-level DRL scheduler experiment, overall packet loss was $2.46\times10^{-4}$ in simulation, but degraded in real-world LTE to $7.74\times10^{-2}$ with a pre-trained actor and improved to $1.75\times10^{-2}$ after fine-tuning. In the network-level user-association simulation, the deep learning approach outperformed baselines (highest-SNR association and a game-theoretic method) and approached the exhaustive-search optimum; after a distribution shift, fine-tuning used 500 new samples versus 8000 to train from scratch.","The authors state that data-driven deep learning for URLLC suffers from lack of labeled samples (supervised learning), performance loss in non-stationary environments, and difficulty in characterizing prediction-error probabilities. For DRL they note unclear Markov assumptions in practice, lack of explicit QoS guarantees, and exploration safety risks that can violate URLLC requirements. In real-world experiments, they report that simulation-trained policies underperform due to unmodeled practical delays (signal processing/data transmission jitter) and because LTE modulation/coding cannot achieve the minimum decoding error predicted by normal approximations.","The paper is primarily an architecture/survey with limited methodological novelty in reliability quantification; it does not provide a rigorous, end-to-end statistical validation framework for ultra-low target probabilities (e.g., $10^{-7}$) under domain shift. Reported experiments/simulations are relatively small-scale (e.g., one AP serving two users; two APs serving ten users), so scalability and stability of the proposed learning/transfer/federated mechanisms at realistic 6G densities remain uncertain. It also does not specify reproducible training details (datasets, hyperparameters beyond layer sizes, random seeds) or provide code, making the quantitative findings hard to replicate and compare fairly against other learning-based URLLC approaches.","The paper suggests scaling from small to large networks by moving from fully-connected DNNs to CNNs and especially graph neural networks to better exploit network topology. It also calls for transitioning from centralized to distributed learning to reduce backhaul overhead and control-plane latency while still guaranteeing URLLC QoS. Finally, it highlights interdisciplinary modeling (communication systems plus vertical-industry application models) to better capture application-specific QoS/traffic features and mitigate model mismatch.","A valuable extension would be to develop statistically sound reliability estimation and certification methods for rare-event QoS targets (e.g., importance sampling or conformal/robust bounds) to complement learning-based policies. Another direction is robust/self-starting online adaptation with explicit constraint handling (safe RL, chance constraints, or distributionally robust optimization) to provide provable URLLC guarantees during exploration and fine-tuning. Providing an open-source simulator/digital-twin environment and benchmark datasets for URLLC learning (including non-stationarity and hardware impairments) would substantially improve reproducibility and accelerate comparative research.",2002.11045v1,https://arxiv.org/pdf/2002.11045v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:14:10Z
TRUE,Maintenance optimization|Network/infrastructure reliability|Other,ML-based|Hybrid/Ensemble|Other,Sensor/condition monitoring|Simulated only|Mixture of types|Other,Not applicable,Network/cybersecurity|Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://epistasislab.github.io/tpot/,"The paper proposes CRATOS, an end-to-end, self-adaptive framework to reduce engineering effort for large-scale KPI time-series anomaly detection in reliability systems engineering. It first extracts three purpose-built features—Section-sign (periodicity/similar interval tendency), Swing (amplitude/volatility), and Diff-Thres (impulse density)—and performs targeted hierarchical clustering using k-means (k=2 at each layer) to form eight KPI clusters. For each cluster, it uses an evolutionary algorithm to automatically select an anomaly-detection pipeline (e.g., smoothing choice, detector choice, execution decisions/order) and tune hyperparameters, optimizing a business-defined pass-rate objective (no missed anomalies, no alarm delays, no false alarms). Experiments on pseudo-data and 206 labeled Huawei Cloud KPIs show improved clustering vs. YADING/ROCKA and an overall anomaly-detection pass rate of 0.851 after EA-based configuration. The work advances practical reliability monitoring by coupling cluster-aware pipeline selection with automated configuration to lower development and maintenance costs at scale.","Clustering is hierarchical with k-means (k=2 per layer) over three extracted feature sequences: Section-sign features computed on sliding windows using a center value $med$ and a sign-mean comparison between left/right halves (via $sign(x)\in\{1,0,-1\}$), Swing features computed as the window percentile spread $w=P_{80}-P_{20}$ on first differences, and Diff-Thres features computed by counting threshold crossings on absolute first differences using thresholds $\max(window)/div$ for multiple attenuation factors (div=2,3,4). Anomaly-detection performance is measured by a pass-rate accuracy: $accuracy = \frac{PassNumber}{TotalNumber}$ (Eq. 3), where PassNumber counts KPIs meeting the business passing criteria under the configured pipeline.","Single-feature k-means clustering achieved (F1): Section-sign 0.81 (T) / 0.83 (F), Swing 0.89 (T) / 0.90 (F), and Diff-Thres 0.79 (T) / 0.83 (F). On pseudo-data, CRATOS substantially outperformed YADING and ROCKA across all eight combined classes (FFF…TTT), e.g., for FFF F1=0.692 vs 0.417 (YADING) and 0.401 (ROCKA), and for TTT F1=0.731 vs 0.377 (YADING) and 0.201 (ROCKA). The evolutionary search used population size 200, retaining top 40 per iteration; it typically reached the best pass rate in ~9 iterations, and after 40 iterations achieved a final anomaly-detection pass rate of 0.851 on 206 Huawei Cloud KPIs.","The authors note that KPI categories are not clearly defined across domains and labeled datasets are insufficient, motivating unsupervised clustering and rule-based/business-criteria evaluation rather than supervised learning. They also emphasize that Huawei Cloud BU data are imbalanced across some clusters (TTF, TFF, FTF), and therefore they evaluate clustering comparisons against YADING/ROCKA primarily on pseudo-data. They report that the evolutionary optimization is computationally expensive (about two hours per iteration with 100 subprocesses), implying nontrivial training cost.","The anomaly-detection objective (“pass” criteria: no misses, no delay, no false alarms) is business-specific and not directly comparable to standard detection metrics (precision/recall/F1), limiting reproducibility and benchmarking. The hierarchical k=2 design hard-codes a binary split per feature and may be brittle if KPI characteristics are multi-modal or continuous rather than naturally separable. Feature extraction relies on several fixed window/stride choices (e.g., m=90,s=30; m=180,s=30) and percentile clipping rules that may require retuning for different sampling rates and domains; robustness/sensitivity analysis is not shown. The paper does not provide a clear ablation for the evolutionary pipeline search space (detector set, ordering, normalization) and how much gain comes from clustering vs. EA tuning vs. handcrafted feature engineering.",None stated.,"Provide an open-source implementation and standardized benchmarks (public KPI datasets) with conventional anomaly metrics alongside the business pass-rate to enable fair comparison and replication. Extend CRATOS to handle concept drift and nonstationarity in KPIs (online re-clustering and incremental re-optimization), which is common in production reliability monitoring. Add robustness to autocorrelation and varying sampling intervals (irregular time series), and explore multivariate KPI groups where cross-KPI dependencies matter. Develop guidance/automation for choosing window/stride and clipping hyperparameters, and evaluate alternative clustering models beyond fixed k=2 splits (e.g., GMMs or density-based methods without labeling as noise).",2003.01412v3,https://arxiv.org/pdf/2003.01412v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:14:48Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/fra31/auto-attack|https://github.com/evaluating-adversarial-robustness/adv-eval-paper|https://github.com/MadryLab/robustness|https://github.com/BorealisAI/advertorch|https://github.com/max-andr/square-attack,"This paper proposes a more reliable, parameter-free protocol for evaluating adversarial robustness of neural network classifiers, motivated by frequent overestimation of robustness due to attack misconfiguration and gradient masking/obfuscation. It introduces Auto-PGD (APGD), a step-size-free variant of PGD with adaptive step-size halving based on optimization progress and restarting from the best point found, and it proposes the DLR (Difference of Logits Ratio) loss, which is shift- and scale-invariant to mitigate failures of cross-entropy due to logit rescaling and vanishing gradients. The authors then assemble an attack ensemble, AutoAttack, combining APGD variants (CE and targeted DLR) with the FAB targeted attack and the black-box Square Attack to increase attack diversity without per-model hyperparameter tuning. In a large-scale evaluation on 50+ robust models across MNIST/CIFAR-10/CIFAR-100/ImageNet under $\ell_\infty$ and $\ell_2$ threat models, AutoAttack produces lower (worse) robust accuracies than originally reported in nearly all cases, often by more than 10%, revealing multiple broken or overstated defenses. The work aims to set a practical minimum standard for robustness evaluation that is computationally affordable and user-independent.","APGD update uses a projected gradient ascent step with momentum: $z^{(k+1)}=\Pi_S(x^{(k)}+\eta^{(k)}\nabla f(x^{(k)}))$, then $x^{(k+1)}=\Pi_S\big(x^{(k)}+\alpha(z^{(k+1)}-x^{(k)})+(1-\alpha)(x^{(k)}-x^{(k-1)})\big)$, with adaptive halving of $\eta$ at scheduled checkpoints when progress conditions fail. The proposed DLR loss is $\mathrm{DLR}(x,y)=-(z_y-\max_{i\neq y}z_i)/(z_{\pi_1}-z_{\pi_3})$ where $\pi$ orders logits descending; the targeted variant is $\mathrm{T\!DLR}(x,y)=-(z_y-z_t)/(z_{\pi_1}-(z_{\pi_3}+z_{\pi_4})/2)$. AutoAttack reports worst-case robust accuracy over an ensemble (APGD-CE, APGD targeted-DLR, targeted FAB, and Square Attack), counting a sample as non-robust if any component finds an adversarial example within the $\ell_p$ ball.","Across 49 deterministic defense models, AutoAttack achieves lower robust accuracy than reported in the original papers in all but one case (the exception differs by only 0.03%), with reductions larger than 10% for 13/49 models and larger than 30% for 8/49 models. For randomized defenses (evaluated over 5 runs with EOT-style averaging), AutoAttack also always lowers the reported robust accuracy, with improvements often exceeding 10% and in some cases exceeding 25% (e.g., several JEM variants). In component analysis, APGD with targeted DLR is typically strongest on CIFAR-10/100/ImageNet, while Square Attack is strongest on MNIST; targeted versions of DLR and FAB outperform their untargeted counterparts in the vast majority of tested CIFAR/ImageNet settings. The paper also demonstrates a concrete gradient-masking failure mode for cross-entropy under logit rescaling, where PGD’s gradients become numerically zero and robust accuracy is severely overestimated unless the attack/loss is adjusted.","The authors state that AutoAttack is not claimed to be the ultimate adversarial attack, but rather a minimal reliable baseline test. They note that APGDCE/APGDDLR can still fail when gradient information is unhelpful, such as for discontinuous classifier functions (e.g., the defense of Xiao et al. 2020), and that ensemble diversity (including black-box Square Attack) is important to cover such cases. They also mention the evaluation is not exhaustive because some authors did not reply or could not provide models/code.","The method targets adversarial robustness evaluation in ML rather than engineering reliability; its conclusions depend on the threat model choices ($\ell_\infty/\ell_2$ bounds) and may not generalize to other perturbation models (e.g., physical-world transformations) without additional components. The large-scale comparisons aggregate many defenses but do not provide a standardized computational budget baseline across all original papers, so “fairness” of cost comparisons can still vary by model and evaluation setup. While “parameter-free” for users, AutoAttack still embeds fixed design choices (iterations, restarts/targets, query budget, schedules) that may not be universally optimal across future architectures or tasks, and robustness estimates remain lower bounds rather than certificates.","The authors recommend AutoAttack as part of a standard evaluation pipeline and emphasize the need for a protocol that works reliably and autonomously without per-defense hyperparameter tuning. They suggest that improved evaluation will help identify true state-of-the-art defenses and speed progress, but they do not lay out a detailed, specific future-work agenda beyond broader adoption and continued development of strong attacks.","Developing adaptive-budget versions (time/query constrained) with principled stopping rules could make the protocol more comparable across settings and easier to integrate into continuous evaluation. Extending the ensemble to handle additional threat models (spatial/photometric transforms, patch attacks, distribution shifts) and to structured outputs (detection/segmentation) would broaden applicability. Providing formal robustness guarantees or statistically valid confidence intervals for estimated robust accuracy (given stochastic attacks/defenses) would strengthen reliability of reported metrics. Packaging the full protocol as a benchmark suite with standardized compute budgets and reproducible reporting templates would further reduce evaluation variance across papers.",2003.01690v2,https://arxiv.org/pdf/2003.01690v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:15:31Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/JonasRieger/ldaPrototype,"The paper addresses the reproducibility (stability) of Latent Dirichlet Allocation (LDA) topic models across replicated runs caused by random initialization and Gibbs sampling variability. It proposes a modified Jaccard coefficient to quantify similarity between two topics after filtering to “relevant” words using topic-specific thresholds, and then clusters topics from multiple runs using hierarchical complete-linkage clustering. A new local dendrogram pruning algorithm is introduced to encourage clusters that contain exactly one topic from each replication, yielding the stability/similarity measure S-CLOP normalized to lie in [0,1]. The approach is demonstrated on a real corpus of 7,657 USA Today articles (June–Nov 2016), showing that four selected runs achieve S-CLOP = 0.83 (U* = 25 with normalization 150) and that selecting a “prototype” run with highest average similarity across many replications increases stability compared with random single-run selection. The authors provide an R implementation and recommend using many replications (e.g., at least ~50, depending on corpus complexity) and selecting the most representative run to improve reliability of downstream interpretations.","Topic similarity is defined via a modified Jaccard coefficient: for topics with word-count vectors $z_i, z_j$ and thresholds $c_i, c_j$, $J_m(z_i,z_j\mid c)$ is the ratio of (i) the number of vocabulary terms whose counts exceed both thresholds to (ii) the number exceeding at least one threshold (Eq. 1). Cluster “disparity” is defined as $U(g)=\frac{1}{R}\sum_{r=1}^R |t_r^{(g)}-1|\cdot\sum_{r=1}^R t_r^{(g)}$, where $t_r^{(g)}$ counts how many topics in cluster $g$ come from run $r$. Overall stability is $\text{S-CLOP}(G^*)=1-\frac{1}{U_{\Sigma,\max}}\sum_{g\in G^*} U(g)$ with $U_{\Sigma,\max}=N\cdot\frac{R-1}{R}$ (Eq. 2), where $G^*$ is obtained by the proposed local-pruning optimization on the dendrogram.","On the USA Today corpus (7,657 articles; vocabulary reduced to 25,486; LDA with $K=50$, $\alpha=\beta=1/50$), clustering $R=4$ runs ($N=200$ topics) with $J_m$ (using $d=500$ for the relative threshold) and local pruning yields 61 clusters and S-CLOP = 0.83 (normalization $U_{\Sigma,\max}=150$, optimal sum of disparities $U^*=25$). In a larger experiment fitting 100 LDA models and choosing the prototype with highest mean pairwise S-CLOP, prototype mean similarities are higher than those from randomly selected runs; with subsampling, using 10 candidates already improves stability (prototype mean similarity range about 0.842–0.880). The authors recommend fitting at least ~50 replications, reporting that for 50 runs the minimum/maximum mean similarity improves to about 0.862/0.895 versus 0.796/0.877 for the original (non-prototype) sets.","The authors note open challenges in generalizing S-CLOP to compare models trained on different corpora, including how to handle differences in the number of topics $K$ across models and whether comparisons should be based on multiple runs per corpus or on selected prototypes. They also remark that the needed number of replications depends on corpus complexity and computational constraints, implying practical limits when compute is scarce.","The work studies “reliability” as algorithmic stability/reproducibility of LDA outputs rather than reliability engineering (failure/degradation) reliability, so its relevance to engineering reliability is indirect. The method depends on several tuning/design choices (e.g., threshold scheme and $d=500$, linkage choice, distance transform from similarity, number of Gibbs iterations) whose sensitivity and robustness are not comprehensively explored. Stability is evaluated primarily via similarity metrics and one main real-data corpus; broader validation across diverse corpora, different LDA implementations/estimators, and correlated documents/time dynamics is limited. Selecting a prototype run improves representativeness among runs but does not guarantee better semantic validity or downstream task performance; the link between higher S-CLOP and better interpretability/decision outcomes is not rigorously established.","They propose generalizing S-CLOP to compare topic models trained on different corpora, highlighting the need to address differences in topic counts $K$ and to study whether comparing multiple runs per corpus or comparing prototypes is more practical. They suggest that such similarity measures could enable applications like quantifying differences in coverage across media channels (e.g., Twitter vs online vs print) or across newspaper offices.","A natural extension is a systematic sensitivity analysis for S-CLOP with respect to the word-thresholding parameter ($d$ or absolute cutoffs), clustering linkage, and run settings (burn-in/iterations), including guidance for default choices. Another direction is adapting the approach to settings with unknown/variable $K$ (e.g., HDP/topic models with varying topic counts) and to dynamic or correlated corpora where documents are not exchangeable. It would also be useful to connect stability improvements to external validation measures (human judgment, coherence, downstream prediction) to ensure that higher stability corresponds to better practical utility, not just reproducibility. Finally, providing a packaged, benchmarked implementation (e.g., CRAN release plus reproducible scripts and datasets) would strengthen adoption and facilitate comparative studies.",2003.04980v1,https://arxiv.org/pdf/2003.04980v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:16:12Z
TRUE,Maintenance optimization|Network/infrastructure reliability|Other,ML-based|Bayesian,Sensor/condition monitoring|Mixture of types,Condition-based,Transportation/logistics|Energy/utilities|Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Multi-Objective Variational Autoencoder (MVA) for structural health monitoring (SHM) to support smart infrastructure maintenance by detecting damage, estimating severity, and localizing damage using multi-way sensor data. The method fuses data from multiple sensors by feeding a frontal slice of three-way data (location × feature/frequency × time) into an autoencoder; anomaly/damage detection uses reconstruction probability (via a VAE/ELBO formulation) rather than reconstruction error, aiming to avoid setting a fixed error threshold and to capture variance changes. A dedicated “localization layer” and a sensor identity matrix built from per-sensor reconstruction errors are used with k-NN distances to localize the damaged sensor locations. Evaluation on two real datasets—a cable-stayed bridge in Western Sydney and a LANL laboratory building structure—shows high detection performance (reported F-scores up to 100% on the bridge test and 97% on the building) and improved localization versus an OCSVM baseline. Overall, the contribution is a data-driven condition-monitoring approach for infrastructure condition assessment, emphasizing unsupervised learning when only healthy data are available for training.","The core training objective is the VAE evidence lower bound (ELBO): $J(\theta,\phi) = -\mathbb{E}_{Z\sim q_\phi(Z\mid X)}[\log p_\theta(X\mid Z)] + D_{KL}(q_\phi(Z\mid X)\Vert p_\theta(Z))$, combining reconstruction likelihood and KL regularization. Damage/anomaly scoring is based on reconstruction probability estimated by Monte Carlo sampling: $RP(x_{new}) = \frac{1}{L}\sum_{l=1}^L p_\theta(x\mid z^{(l)};\mu_{\hat x}^{(l)},\sigma_{\hat x}^{(l)})$ with $z^{(l)}\sim \mathcal{N}(\mu_z,\sigma_z)$. For localization, a sensor identity matrix $S$ stores per-sensor average reconstruction errors across the $m$ output nodes belonging to each sensor, and k-NN distances between $S$ and $S_{new}$ yield location anomaly scores.","On the cable-stayed bridge dataset (24 accelerometers; 24 × 600 frequency features × 262 events), training used 100 healthy events and testing used remaining healthy plus 137 damaged events; the MVA model reports an F-score of 100% and separates “Car-Damage” (≈3 t at A10) from “Bus-Damage” (≈12.5 t at A14) via differing reconstruction probability magnitudes. Damage localization via the localization layer/k-NN highlights sensors A10 and A14 as most anomalous, matching the emulated damage positions. On the LANL building dataset (12 locations × 768 features × 240 events), MVA reports an F-score of 97% with zero false alarms on healthy tests and higher reconstruction probabilities for the more severe combined damage (1A & 3C) than for damage at 3C alone. Baseline OCSVM performance is reported as 95% F-score on the bridge and 86% on the building, and is described as not clearly indicating severity levels and lacking localization capability.",None stated.,"The approach is evaluated on two case studies and compares primarily against OCSVM; broader baselines (e.g., PCA/DPCA, isolation forest, deep SVDD, transformer-based models) and ablations (e.g., probability vs error, effect of localization layer) are not fully documented in the provided text. The procedure appears to rely on design choices (e.g., number of hidden layers, FFT feature construction, sampling size $L$, and a plotted anomaly threshold) whose robustness to sensor noise, drift, missing data, and environmental/operational variability is unclear. Localization uses k-NN distances on a derived identity matrix; performance may degrade with correlated sensor faults, simultaneous multiple damages, or changes in sensor calibration, and the method’s statistical uncertainty calibration for probability scores is not validated. No implementation details (software, hyperparameters, runtime) or shared code are provided, limiting reproducibility and deployment assessment.",None stated.,"Extend the method to handle non-stationarity common in SHM (temperature/humidity/traffic variations) via domain adaptation or hierarchical Bayesian modeling of environmental effects. Develop a self-starting/online version for streaming monitoring with concept drift detection, missing-sensor robustness, and incremental updating without retraining from scratch. Provide principled decision rules for alarms (e.g., false-alarm-controlled thresholds, conformal prediction) and quantify uncertainty in detection/localization outputs. Broaden validation to more structures and include standardized benchmarks with multiple competing SHM/anomaly methods, plus an open-source implementation for reproducibility.",2003.05070v1,https://arxiv.org/pdf/2003.05070v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:16:49Z
FALSE,Other,ML-based|Bayesian,Other,Not applicable,Pharmaceutical,Simulation study|Case study (real dataset),TRUE,Other,Not provided,NA,"This paper studies how reliably graph neural network (GNN) classifiers’ output scores can be interpreted as probabilities in molecular virtual screening. It evaluates calibration/reliability using calibration curves, expected calibration error (ECE), and predictive entropy histograms, and performs ablations over model architectures (GCN vs. GAT; sum vs. attention readout), regularization methods (dropout vs. MC-dropout, label smoothing, entropy regularization), and focal loss hyperparameters under imbalanced data. Experiments on MoleculeNet classification datasets (BACE, BBBP, HIV) show that higher-capacity attention-based GAT variants can be more overconfident and less calibrated in data-deficient/imbalanced regimes, while MC-dropout tends to improve calibration. In a dataset-shift virtual screening setup (train on DUD-E targets and test on ChEMBL), MC-dropout yields higher hit/“success” rates when selecting compounds by predicted probability, whereas label smoothing and entropy regularization can produce biased probability estimates that harm top-ranked screening precision. The authors conclude that modest model capacity and Bayesian-flavored inference/regularization (not cost-sensitive probability-penalizing losses) are important for reliable probability-based screening decisions.","The work defines calibration via $P(\hat{Y}=y\mid \hat{P}=p)=p$ and estimates expected calibration error (ECE) by binning predictions: $\mathrm{ECE}=\sum_{m=1}^M \frac{|B_m|}{n}\,|\mathrm{acc}(B_m)-\mathrm{conf}(B_m)|$, where $\mathrm{acc}(B_m)=\frac{1}{|B_m|}\sum_{i\in B_m}\mathbb{I}(\hat{y}_i=y_i)$ and $\mathrm{conf}(B_m)=\frac{1}{|B_m|}\sum_{i\in B_m}\hat{p}_i$. Predictive uncertainty is summarized by entropy $H(p)=-p\log p-(1-p)\log(1-p)$, and focal loss is given (binary) by $\sum_i -y_i(1-\hat{p}_i)^{\gamma}\log \hat{p}_i-(1-y_i)\hat{p}_i^{\gamma}\log(1-\hat{p}_i)$ (with a weighted variant using $\alpha$).","Across BACE/BBBP/HIV, architecture choice strongly affects calibration: GAT-based variants exhibit more overconfident predictions (more mass near entropy 0) and larger ECE than GCN-based variants in these small/imbalanced classification settings, while a GCN with attention readout performs best overall among the tested architectures. Regularization comparisons indicate that dropout improves reliability and MC-dropout improves it further (lower ECE), whereas label smoothing and entropy regularization can worsen calibration (especially ERL, which often shifts entropy toward mid-range values indicating overly uniform outputs). On the imbalanced HIV task, varying focal-loss weights $\alpha$ trades off precision vs. recall (with best F1 reported at $\alpha=0.75$), but focal loss generally does not improve calibration and can damage ECE for larger focusing parameters $\gamma$. Under dataset shift (train DUD-E, test ChEMBL for EGFR/VGFR2/ABL1), selecting top-$K\%$ compounds by predicted probability yields increasing success rate with MC-dropout as $K$ decreases, while LS/ERL do not show this monotonic improvement and can have low success rates in the top 5–10% region.",None stated.,"The paper focuses on probability calibration (ECE/curves) for GNN classifiers, but it does not position the problem within standard reliability engineering (lifetime/degradation/maintenance) frameworks, so its conclusions do not directly transfer to engineering reliability analysis. Calibration is assessed primarily with ECE and qualitative plots; ECE depends on binning choices and can miss miscalibration structure, and alternative proper scoring rules (e.g., Brier score, log score), confidence-intervals for ECE, or statistically rigorous calibration tests are not emphasized. Reproducibility is limited because the manuscript describes a unified implementation and training scheme but does not provide an explicit public code release link in the provided text.","The authors suggest exploring improved molecular-graph architectures such as node pooling methods to reduce node dimensionality and potentially improve representations with fewer parameters. They also propose investigating more precise Bayesian learning algorithms beyond MC-dropout to better approximate posterior and improve calibration/uncertainty. Finally, they recommend studying pre-trained/self-supervised (e.g., contrastive) representations to improve data efficiency and reliability in downstream molecular prediction tasks.","A useful extension would be to evaluate post-hoc calibration methods (temperature scaling, isotonic regression, Dirichlet calibration) and compare them fairly against training-time regularizers for virtual screening metrics. Robustness to dataset shift could be studied more systematically (different shift types, target-wise transfer, and uncertainty-aware selection rules) with standardized benchmarks and confidence intervals on hit rates. Providing an open-source reference implementation and ablation scripts would enable stronger reproducibility and facilitate adoption in practical screening pipelines.",2003.07611v1,https://arxiv.org/pdf/2003.07611v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:17:27Z
TRUE,Failure mode analysis|Other,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Manufacturing (general),Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper proposes an automated, non-contact in-process quality monitoring and grading system for fused deposition modeling (FDM) additive manufacturing using a deep convolutional neural network (CNN). Layer-by-layer images are captured with a CCD camera during printing, and the model is trained offline to detect/grade defects arising from overfill/underfill (surface defects and internal voids) and then evaluated for online, real-time quality classification. The authors compare a fine-grained 21-class classification (speed/temperature setpoints) versus a coarser 5-grade quality classification (A–E), showing that the 5-grade formulation substantially improves F-score/precision/recall while maintaining high accuracy/specificity. Reported performance for the 5-grade system is about 94% average accuracy and 96% average specificity, with F-score, sensitivity, and precision all above 0.75, supporting its use as a feedback signal for “go/no-go” and potential parameter adjustment during printing. The approach is presented as a proof-of-concept toward improving AM process reliability and reducing waste by detecting quality issues during the build rather than after completion.","The paper evaluates classification performance using standard confusion-matrix metrics: Precision $=\frac{TP}{TP+FP}$, Sensitivity/Recall $=\frac{TP}{TP+FN}$, Specificity $=\frac{TN}{FP+TN}$, F-score $=\frac{2TP}{2TP+FP+FN}$, and Accuracy $=\frac{TP+TN}{TP+FP+FN+TN}$. The final classifier layer is described as a softmax (logistic regression) producing normalized class probabilities; the network is trained via batch gradient descent with forward/backpropagation and transfer learning from Inception-v3.","For the 5-grade (A–E) quality classification, the authors report an average accuracy of 94% and average specificity of 96%, with F-score, sensitivity, and precision each reported as above 0.75. In the 5-grade confusion matrix (100 test images per class), correct predictions range from 81 (grade E) to 91 (grade C). For the 21-class (speed/temperature) problem, class accuracies are reported above 93%, but F-score/precision/recall can be as low as ~0.3 and max values are not larger than 0.71, with 11/21 classes having F1-score < 0.5, indicating many false positives/negatives despite high accuracy/specificity. In the speed–temperature map evaluation, a highlighted region achieves >90% accuracy with average 98.2%, while outside that region average accuracy is 83%.","The authors note the approach uses offline training with a fixed number of training samples for online defect/quality prediction. They also state that without a public dataset, obtaining sufficient AM images for diverse defect scenarios is difficult; increasing network depth with limited samples can cause overfitting and lower model reliability.","Quality labels (A–E) appear to be based on manual annotation/inspection, which may introduce subjectivity and limit reproducibility; inter-rater agreement is not discussed. The study is demonstrated on a single printer/material setup (Creality Ender-3 with PLA) and may not generalize to other machines, materials, geometries, lighting/camera placements, or defect taxonomies without re-collection and re-training. While positioned as “online,” evaluation is reported via held-out test images rather than closed-loop intervention outcomes (e.g., reduced scrap rate, improved mechanical properties), so reliability improvement is inferred rather than quantified at the part-performance level.","They propose developing a smarter closed-loop control system using a closed-loop machine learning algorithm where the model learns/adapts important parameters while the AM machine operates. They suggest the printer should self-train by upgrading its training samples over time to recognize new scenarios and automatically adjust/correct without operator intervention, aiming for more reliable parts, shorter print time, and less material waste.","Validate the monitoring signal in true closed-loop experiments where the printer adjusts speed/temperature in real time and quantify downstream reliability outcomes (scrap rate, defect density, tensile strength/fatigue) versus baselines. Extend the method to broader operating conditions (different geometries, materials, printers) and add robustness to nuisance variation (lighting, camera angle, occlusions) and temporal dependence across layers (e.g., video/sequence models). Provide an open dataset and reference implementation to enable benchmarking against alternative in-situ quality monitoring approaches and to support reproducible evaluation.",2003.08749v1,https://arxiv.org/pdf/2003.08749v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:18:02Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Mixture of types|Simulated only|Other,Not applicable,Food/agriculture|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper addresses how to make crowdsourced food price data usable for statistically valid inference when the data come from a convenience (non-probability) sample and may contain non-sampling errors. It proposes a two-phase quality methodology: (i) pre-processing to detect and handle standard outliers and spatial outliers using neighborhood-based comparisons derived from a spatial weights matrix; and (ii) a post-sampling reweighting procedure (a special post-stratification) that benchmarks crowdsourced observations against a formal sampling design (geographical stratification or an optimal spatially balanced design via Local Pivotal Method, LPM2). A simulation study with spatially autocorrelated data shows that post-sampling reweighting can dramatically improve estimator efficiency, with variances reported as roughly 124–125 times smaller than treating the crowdsourced data as a simple random sample. An application to crowdsourced Guinea Corn prices in Kaduna State, Nigeria demonstrates that post-sampling correction increases the estimated average price (221.01) by about 4.4% relative to the uncorrected mean (211.61). Overall, the work contributes a practical validation-and-reweighting workflow for improving accuracy, reliability, and representativeness of crowdsourced economic statistics.","The pre-processing defines a spatial lag for price at location $i$ as the average of neighboring prices using a binary neighborhood weights matrix $W$: $\text{lag}(P_i)=\sum_{j=1}^n w_{ij}P_j/\#N(i)$, where $w_{ij}=1$ if $j\in N(i)$ and 0 otherwise. Spatial outliers are identified as observations that deviate from their neighborhood average by more than $r$ times the neighborhood standard deviation, and can be replaced by the neighborhood average. The post-sampling adjustment defines a post-sampling ratio for area/location $l$ as $PS_l = m_l/n_l$, where $n_l$ is the number of crowdsourced observations and $m_l$ is the number required/selected under a reference probability sampling design; the overall mean is then estimated as a weighted average using these ratios as weights.","In a simulation with 1,000 spatial locations in four strata (sizes 800, 60, 60, 80) and a spatial autoregressive process with parameter $\lambda=0.7$, the authors compare (a) treating the data as simple random sampling, (b) post-sampling benchmarking to a stratified PPS design, and (c) post-sampling benchmarking to an LPM2 spatially balanced design. Across 1,000 replications, all approaches show small absolute relative bias (average about 0.005). In efficiency terms, the two post-sampling approaches greatly reduce variance: the estimator variance is reported as about 125× (stratified benchmark) and 124× (LPM2 benchmark) smaller than the simple-random (crowdsourcing-mimic) case. In the Kaduna (Nigeria) case study on Guinea Corn prices (Nov 2016–Mar 2017; 16 markets), post-sampling correction yields an average price of 221.01, about 4.4% higher than the uncorrected mean of 211.61.",None stated,"The approach depends on having a defensible reference sampling design (e.g., stratified/PPS or LPM2) and auxiliary information to construct it; if the benchmark design is misspecified, reweighting may not correct bias and can increase variance in sparse areas. The pre-processing replaces outliers (and in the application, even missing/outlier values) with simulated or neighborhood-average values, which can understate uncertainty and potentially attenuate true local price spikes. Results are demonstrated on one short, localized case study (16 markets, one commodity) and a stylized simulation, so generalizability to other commodities, longer time series, or systematically manipulated crowdsourcing is unclear. No implementation details (software, parameter choices like neighborhood definition and threshold $r$) are provided, limiting reproducibility and practitioner adoption.",The authors state that the methodology can be expanded and adapted to support quality assessment of crowdsourced data in other regions and sectors beyond the Nigeria food price application.,"Develop a principled uncertainty quantification framework that propagates the effects of outlier treatment/imputation and post-sampling reweighting into standard errors or confidence intervals (e.g., via bootstrap under complex/spatial designs). Extend the method to handle temporal dependence explicitly (spatio-temporal models) and to produce robust price indices over time rather than point estimates of means. Provide self-starting/fully data-driven guidance for choosing neighborhoods, threshold $r$, and benchmark designs under limited auxiliary data. Release an open-source reference implementation and evaluate against additional baselines (e.g., calibration weighting, model-assisted estimators, robust M-estimators) on multiple real crowdsourcing datasets.",2003.12542v1,https://arxiv.org/pdf/2003.12542v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:18:38Z
FALSE,NA,ML-based|Bayesian,Other,Not applicable,Healthcare/medical|Finance/economics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,https://Cran.R-Project.Org/,"This paper studies “degradation problems” in supervised learning—class imbalance, sparseness, small-disjuncts, class overlap, and noisy labels—and argues that the lack of dataset diagnosis leads to inappropriate remediation (“treatment”) choices and misleading performance assessment. It proposes a probabilistic diagnostic model that segments each class into subclasses using Gaussian Mixture Models selected via BIC, then computes (i) imbalance ratios between classes and subclasses, (ii) overlap via a separation index, (iii) noise rates via a k-NN label-consistency test, and (iv) sparseness/dispersion via distance-to-median summaries and Anderson’s PERMDISP2 test. The model is evaluated empirically on 49 real-world datasets and 14 classifiers, and remediation methods (Random resampling, SMOTE variants, ADASYN, and data-cleaning methods like ENN/NCL/OSS) are compared using nonparametric statistical tests (Wilcoxon signed-rank and Friedman with post-hoc grouping). A key empirical finding is that AUC is often overly optimistic under degradation (especially class imbalance), while G-mean provides a more conservative and informative metric for treatment comparisons. The work advances practical ML workflow guidance, but it is not focused on reliability engineering of physical systems.","The diagnostic model fits class-conditional Gaussian mixture models: $p(x)=\sum_{k=1}^K\pi_k\,\mathcal{N}(x\mid\mu_k,\Sigma_k)$, selecting $K$ and covariance structure by maximizing $\mathrm{BIC}=\log p(X\mid\pi,\mu,\Sigma)-\tfrac{1}{2}\eta(\theta)\log N$. Class/subclass overlap is quantified by the separation index $J^*(a^*)$ (range $[-1,1]$) computed along an optimal projection direction $a^*$ (found via Newton–Raphson), with negative values indicating overlap and positive values indicating separation. Performance comparisons rely on standard classification metrics including $\mathrm{AUC}=\frac{1}{PN}\sum_{i\in P}\sum_{j\in N}\mathbf{1}[p_i>p_j]$, and $\mathrm{G\text{-}mean}=\sqrt{\mathrm{Acc}^+\,\mathrm{Acc}^-}$.","Across 49 datasets and 14 classifiers (674 models), AUC was consistently more optimistic than G-mean; the paired-sample Wilcoxon test comparing AUC vs G-mean gave p-values on the order of $\sim 10^{-87}$ to $10^{-88}$ (e.g., reported $7.2\times 10^{-87}$ and $5.06\times 10^{-88}$), rejecting equality in favor of AUC being higher. Robustness testing with Wilcoxon signed-rank tests indicated that (i) high class imbalance significantly degrades performance for many classifiers under G-mean/F1 (e.g., for SVM-G, p=0.00406 for IR effect using G-mean) while AUC may fail to reflect this, (ii) the number of disjuncts generally did not significantly degrade G-mean, and (iii) high overlap significantly degrades performance for most classifiers (13/14 classifiers showed degradation under high overlap). Treatment ranking over 49 datasets × 14 classifiers × 9 treatments (5995 models) using Friedman tests found statistically significant differences between treatments for all classifiers (all p-values < 0.05); sampling-based methods (often SMOTE/Random/ADASYN) tended to rank best for profiles with high imbalance, while no single treatment dominated for high-overlap/low-imbalance profiles.","The authors note that prior work suffers from limited scope (few datasets/classifiers), insufficient validation, and overreliance on simplistic/artificial datasets, motivating their broader empirical framework. They also state that for some datasets it was not possible to find an “optimal projection” for estimating the separation index, leading to an “Unknown” overlap level in their profiling. They emphasize that treatment effectiveness depends on dataset characteristics, implying that no universal best remediation exists.","The work is about ML dataset issues rather than degradation/reliability of engineered assets; the term “degradation” is used metaphorically and does not model physical wear-out, failure time, or RUL. The diagnostic approach relies on Gaussian-mixture subclass modeling and a separation index requiring positive-definite covariance assumptions, which may be fragile in high-dimensional/low-sample regimes or with non-Gaussian class structure. The paper reports extensive R-based experimentation but does not provide reproducible code, exact preprocessing pipelines, or fixed random seeds, which can materially affect resampling/tuning comparisons. The profiling thresholds (e.g., IR>10, disjuncts>10, NOR>0.1) appear heuristic and may not transfer across domains without calibration.","Future work is described as developing a remediation technique driven by the proposed diagnostic tests and Dataset Degradation Profiles (DDP). The stated goal is to reverse degradation effects due to both between-class and within-class imbalance (small-disjuncts), as well as overlapping and noisy-label problems.","A natural extension would be to provide an open-source implementation (e.g., an R package) of the full diagnostic workflow (GMM selection, IRO matrix, noise decomposition, dispersion testing) with end-to-end reproducibility and benchmark scripts. Methodologically, robustness to non-Gaussian/non-elliptical subclasses and high-dimensional covariance estimation could be improved via regularized/robust mixture modeling or nonparametric density/separation measures. The overlap/noise decomposition could be validated against datasets with known label-noise mechanisms to quantify false positive/negative diagnosis rates. Finally, integrating the diagnostic outputs into an automated treatment-selection policy (e.g., meta-learning) with out-of-sample validation would strengthen prescriptive claims.",2004.02988v2,https://arxiv.org/pdf/2004.02988v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:19:18Z
FALSE,NA,Stochastic process|Other,Other,Not applicable,Network/cybersecurity|Other,Simulation study|Other,TRUE,Python,Not provided,https://www.datarepository.movebank.org/handle/10255/move.747,"The paper introduces the Markov Stochastic Block Model (MSBM), a growing-network variant of the stochastic block model where node community labels evolve according to a finite-state Markov chain, and edges are generated via community-dependent Bernoulli probabilities. It develops prediction methods for link prediction and collaborative filtering that are designed to be robust to clustering errors, using an HMM-inspired mean-field approximation and the Baum–Welch (EM) algorithm to learn “emission” probabilities describing clustering error patterns. Theoretical results establish exponential decay of community misclassification error with a signal-to-noise ratio (SNR) and provide conditions for consistent estimation of MSBM parameters (transition matrix, stationary distribution, and connectivity matrix) in relatively sparse regimes. Empirically, the proposed “reliable” link prediction and robust MAP collaborative filtering methods reduce error compared to a plug-in approach when clustering is imperfect. A heuristic model selection method is also proposed to estimate the number of communities using learned emission probabilities, and it is demonstrated on simulated graphs and real datasets (football network and animal migration data).","MSBM generative model: community labels follow a Markov chain $C_1\sim\pi$, $C_i\mid C_{i-1}\sim P(C_{i-1},\cdot)$, and edges are independent given labels with $X_{ij}\sim\mathrm{Ber}(Q_{C_i,C_j})$ where $Q=\alpha_n Q_0$. The SNR is defined as $S^2=\frac{n\alpha_n\pi_m D^2}{L}$ with $\pi_m=\min_k\pi(k)$, $L=\|Q_0\|_\infty$, and $D^2=\min_{k\neq \ell}\|(Q_0)_{:,k}-(Q_0)_{:,\ell}\|_2^2$, yielding an exponential misclassification bound $\mathrm{err}(\hat G,G)\le e^{-cS^2}$. Reliable link prediction replaces the plug-in posterior $\hat\eta_i$ with an HMM-smoothed estimate $\hat\eta_i^R(\hat C_{1:n})=\sum_{c_i,c_n,c_{n+1}} \hat Q_{c_i,c_{n+1}}\hat P_{c_n,c_{n+1}}\,\hat\alpha_{c_i}(i)\,\hat\chi^{(i,n)}_{c_i,c_n}\,\hat\beta_{c_n}(n)$ based on Baum–Welch forward/backward quantities and learned emission matrix $\hat O$.","The paper proves a partial recovery bound for MSBM community detection: with high probability the clustering misclassification proportion satisfies $\mathrm{err}(\hat G,G)\le e^{-cS^2}$ (and thus $-\log\mathrm{err}=\Omega(n\alpha_n)$), under conditions including $\alpha_n\log n\le 1/L$ and $n\alpha_n$ above a constant. It provides max-norm consistency rates of order $O(1/\sqrt{n})$ for estimated connectivity $\|\hat Q-Q\|_\infty\le \gamma/\sqrt{n}$, stationary distribution $\|\hat\pi-\pi\|_\infty\le \gamma/\sqrt{n}$, and transition matrix $\|\hat P-P\|_\infty\le \gamma/\sqrt{n}$ under relatively sparse regimes (roughly $n\alpha_n/\log n$ sufficiently large). In simulations, the RMSBM (HMM-based) link prediction shows smaller and less variable $L_1$ posterior-probability errors than the plug-in MSBM approach when clustering has local errors. In collaborative filtering experiments, the “Reliable MAP” achieves lower misclassification than the plug-in MAP across varying observed-edge set sizes. The model selection heuristic based on $M(K)=\max_{k\neq\ell}(\hat O_{\ell,k}+\hat O_{k,\ell})$ identifies the correct $K$ in provided examples (simulations and football network).","The authors explicitly describe the reliable (HMM-based) prediction method as a heuristic because (i) the EM/Baum–Welch algorithm may converge to a local optimum and (ii) in the true MSBM dependence graph, the “emission probabilities” (clustering outputs) are not conditionally independent given the hidden states, contrary to the homogeneous-HMM approximation they impose. They also note that when clustering is already nearly perfect (e.g., large simulated graphs with few clusters), the reliable method cannot improve much over the plug-in method because the plug-in approaches the Bayes classifier. They mention a technical sparsity condition (e.g., $\alpha_n\log n\le 1/L$) tied to the chosen SDP clustering method as an assumption for their theoretical guarantees.","The approach depends critically on the quality and calibration of the initial clustering (they use a specific SDP+rounding method) and on knowing/choosing $K$ (with only a heuristic selector), so performance may degrade for highly overlapping communities or model misspecification. The HMM mean-field approximation treats clustering errors as time-homogeneous and conditionally independent, which may be poor when clustering error rates vary with degree, time, or local graph density (common in real networks). Computationally, the SDP clustering step scales as $O(n^3)$ and the reliable prediction involves $K^3 n^2$ operations, which limits applicability to large graphs typical in modern link prediction. Empirical evaluation is mostly small-scale and does not benchmark against strong modern link prediction baselines (e.g., GNN-based or temporal embedding methods) under comparable settings.","The authors suggest using the learned emission matrix $\hat O$ to study algorithm performance in the presence of spurious/fake links, and to help choose among clustering algorithms for practical settings where graphs contain noisy interactions. They also emphasize that while they use a recent SDP method for theoretical guarantees, their prediction framework is intended to transfer to other clustering algorithms, implying broader exploration/validation across clustering tools. They discuss potential applications (e.g., recommender systems and tumor growth) indicating directions for applying and extending MSBM-based reliable prediction in domain-specific settings.","A valuable extension would be to replace the homogeneous-emission HMM approximation with a more realistic error model where emission probabilities depend on node covariates (degree, time, uncertainty scores) or allow nonstationary emissions to capture changing clustering quality. Developing scalable alternatives to the SDP step (e.g., spectral/graph embedding methods with uncertainty quantification) would improve practicality for large networks while preserving robustness ideas. Another direction is to derive formal regret/accuracy guarantees for the reliable predictors under explicit misspecification (approximate HMM) and to study robustness to temporal dependence in edges (beyond conditional independence given communities). Finally, providing open-source, reproducible code and standardized benchmarks against modern link prediction methods would strengthen empirical conclusions and adoption.",2004.04402v3,https://arxiv.org/pdf/2004.04402v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:20:04Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Healthcare/medical|Network/cybersecurity|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This is a vision paper proposing CovidSens, a reliable social-sensing based risk alerting framework for monitoring and predicting COVID-19 spread using data from social media, crowdsensing via smartphones/IoT devices, and physical sensing via UAVs. The core emphasis is on information reliability under noisy, unvetted, multi-modal, and misinformation-contaminated social data streams, and on timely dissemination of risk alerts to the public. It outlines system concepts for (i) social-media-driven disease spread indicators, (ii) crowdsensing-based disease tracking, and (iii) UAV-based health surveillance and alerting, and discusses key research challenges: data collection, data reliability/truth discovery, data modality fusion, AI scalability at the edge, location-data scarcity/privacy, timely presentation, human factors, and misinformation suppression. It surveys relevant enabling methods (truth discovery, estimation theory ideas like MLE/CRLB, deep learning models such as LSTM/GRU/CNN, and federated learning) but does not develop or validate a concrete reliability engineering model for physical component/system failure. Overall, it is about reliability of information/social sensing and edge AI rather than reliability engineering of engineered systems.",Not applicable,Not applicable,"The paper frames multiple unresolved challenges rather than reporting evaluated methods; it notes that existing COVID-19 monitoring tools require manual validation (causing delays), that truth-discovery methods lack external validation and are not tailored to disease detection (risking false COVID-19 case inference), and that limited labeled COVID-19 datasets can yield underfitted/biased AI models with erroneous predictions. It also states practical constraints such as scarcity of geo-tagged posts due to privacy concerns and rate limits/transience of social media data that impede timely collection.","As a vision paper, it does not specify measurable reliability metrics (e.g., calibration, false-alarm/false-negative tradeoffs) or experimental protocols to quantify performance under realistic adversarial misinformation and nonstationary social-media dynamics. The proposed integration of social sensing with UAVs/IoT raises governance, safety, and deployment constraints (regulatory, operational cost, coverage limits) that are not concretely addressed. It also does not provide a clear end-to-end design for privacy guarantees (formal privacy definitions, threat model, or compliance), despite relying heavily on sensitive health and location signals.","The paper explicitly proposes several future directions: (1) uncertainty quantification for CovidSens outputs (e.g., confidence levels) using techniques such as MLE and Cramér–Rao lower bounds; (2) rumor suppression and fake-news detection, potentially combining human intelligence with AI (e.g., crowdsourcing plus DNNs); (3) decentralized/mesh-network-based news aggregation and circulation to reduce reliance on central authorities and handle Internet downtime; (4) privacy-aware location discovery via contextual analysis (text, images, social graph) while protecting user privacy; (5) scalable edge intelligence via federated learning, including handling churn and enabling asynchronous updates; and (6) integrating social sensing with physical sensing (UAVs/vehicular sensing) to validate reports and improve reliability.","A valuable extension would be to formalize reliability/robustness objectives for CovidSens (e.g., probabilistic guarantees on alert accuracy, robustness bounds under coordinated misinformation, and concept-drift adaptation) and to benchmark against standard baselines on shared datasets. Another direction is to develop practical, auditable privacy mechanisms (e.g., differential privacy for aggregation, secure multiparty computation for fusion, and explicit threat models) paired with usability studies to ensure adoption. Finally, deploying pilot studies with real stakeholders (public health agencies) and establishing ground-truth acquisition strategies (delayed labels, sentinel sampling, active learning) would be critical to validate the system’s real-world effectiveness.",2004.04565v3,https://arxiv.org/pdf/2004.04565v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:20:30Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python,Not provided,NA,"This paper proposes the Adversarial Perturbation Explanation Measure (APEM), a quantitative metric to assess the reliability of visual explanation (saliency/relevance) maps for deep vision models. APEM measures how much guided adversarial perturbation (scaled by a relevance map vs. an irrelevance map) is needed to flip a model’s prediction, under the assumption that truly relevant pixels should change outputs with smaller perturbations. The authors compare several explanation methods (raw gradients, SmoothGrad, LRP, Guided Backprop, Grad-CAM, Guided Grad-CAM) on ImageNet using VGG-16 (and additionally ResNet for most methods) and show large differences in APEM distributions and rankings despite similar-looking visualizations. They also show common visualization “cleaning” steps can reduce APEM, and propose an APEM-preserving iterative filtering procedure to denoise relevance maps without reducing measured explanation reliability. Overall, the work contributes an adversarially-motivated evaluation and post-processing tool for explainability rather than engineering reliability modeling.","APEM constructs a directed relevance map by normalizing relevance values and multiplying by the sign of the input gradient: $R_{\mathrm{dir}} = R_{\mathrm{norm}} \odot \mathrm{sign}(\nabla_x J(\theta, x, \hat y))$. The input is perturbed as $x' = x + R_{\mathrm{dir}}\,\epsilon$, and the minimal $\epsilon^{-}$ that flips the prediction using relevance is compared to an analogous $\epsilon^{+}$ computed using an irrelevance map $R_I = 1-R$. The metric is the average gap over images: $\mathrm{APEM} = \frac{1}{n}\sum_{i=1}^n (\epsilon_i^{+}-\epsilon_i^{-})$.","On VGG-16 (ImageNet validation images), the median APEM is highest for raw gradients (81), followed by Guided Backprop (43), LRP (41), Guided Grad-CAM (31), Grad-CAM (20), and SmoothGrad (16). On ResNet, the corresponding medians reported are Gradient 62, SmoothGrad 15, Guided Backprop 28, Grad-CAM 21, Guided Grad-CAM 19 (LRP not evaluated on ResNet). The authors show that standard visualization simplifications (clamping, channel reduction, multiplying by the image) reduce APEM substantially (e.g., Gradient median drops from 137→105→81 across three simplification steps; SmoothGrad 41→28→16; LRP 59→51→41). Misclassified images yield much lower APEM values overall (e.g., Gradient median 25 vs 81 for correct images), but the relative method ranking is preserved.","APEM does not produce scores on the same scale across different models, so it is intended for comparing explanation methods given a fixed model rather than comparing explanations across models with different capacities. The authors also note that implementing LRP on ResNet is not trivial, so they do not report LRP results for ResNet.","The metric depends on adversarial-gradient directions and a specific threat model (small, guided perturbations), so it may not reflect explanation quality under other perturbation types (e.g., structured occlusions, distribution shifts) or for non-differentiable models. Results are shown for ImageNet-scale classifiers (VGG-16/ResNet) and may not generalize to other tasks (segmentation, detection) or settings with strong input preprocessing/defenses that change gradient behavior. The paper reports comparative distributions but does not provide standardized statistical testing across methods beyond descriptive plots/tables, and it is unclear how sensitive rankings are to hyperparameters (e.g., step-size search for $\epsilon$, SmoothGrad $n,\sigma$, LRP variant choices). Practical reproducibility is limited because implementation details and code are not shared in the paper text provided.","The paper notes that APEM has properties that could benefit future applications and suggests it can be used as a tool to filter relevance maps into more interpretable images while keeping essential information, implying broader application of APEM-based filtering and reliability assessment beyond the presented comparisons. No additional explicit future-work agenda is clearly stated beyond these extensions.","Extend APEM to handle common real-world assumptions violated in vision pipelines, such as correlated inputs/augmentations and non-differentiable components (e.g., quantization, classical preprocessing). Evaluate robustness of APEM rankings under different perturbation constraints (spatially localized, color/texture-only, physically plausible) and across tasks (detection/segmentation) and datasets beyond ImageNet. Develop self-contained, standardized procedures for selecting $\epsilon$ search strategies and hyperparameters, and provide open-source implementations to enable reproducible benchmarks and fair comparisons with newer explanation methods.",2004.10824v1,https://arxiv.org/pdf/2004.10824v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:21:04Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,2004.12782v1,https://arxiv.org/pdf/2004.12782v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:21:45Z
FALSE,Other,ML-based|Other,Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"This paper focuses on reliability of healthcare AI predictions in the machine-learning sense (calibration and uncertainty), not reliability engineering of physical systems. The authors propose a calibration-driven learning method that predicts per-class logit intervals $[\hat{y}-\delta,\hat{y}+\delta]$ using two neural networks, and trains them via an alternating optimization that minimizes an empirical interval calibration error and a hinge-style loss. They introduce “reliability plots” that quantify the trade-off between model autonomy and deferring uncertain cases to an expert, using entropy of softmax probabilities as the confidence score. They also propose an interpretability method that generates counterfactual evidences by optimizing in a disentangled latent space (from DIP-VAE) to control semantic similarity, interval width, and prediction entropy. Experiments on the ISIC 2018 dermoscopy lesion dataset show improved accuracy and calibration-relative behavior versus common baselines (e.g., cross-entropy DNN, random forests, gradient boosting).","The interval-calibration objective updates the interval-width network by minimizing, for each class $k$, $\left|\alpha - \frac{1}{N}\sum_{i=1}^N \mathbf{1}[(\hat y_i[k]-\delta_i[k])\le y_i[k]\le(\hat y_i[k]+\delta_i[k])]\right|$ (Eq. 1). With $\delta$ fixed, the predictor is trained using a two-sided hinge loss that penalizes targets outside the interval: $\max(0,(\hat y_i[k]-\delta_i[k])-y_i[k]+\tau)+\max(0,y_i[k]-(\hat y_i[k]+\delta_i[k])+\tau)$ (Eq. 2). Counterfactual evidences are produced by optimizing the latent code $\hat z_t=\arg\min_z\, \eta_1\|z_t-z\|_2^2-\eta_2 g(z;\phi)+\eta_3 H(\mathrm{Softmax}(f(z;\theta)))$ (Eq. 3).","On the ISIC 2018 lesion classification task (7 classes) using DIP-VAE latent features, the proposed method reports W-AUC = 0.841 and accuracy = 0.739 (3-fold CV), compared to logistic regression (W-AUC 0.788, acc 0.667), random forests (0.822, 0.703), gradient boosting (0.806, 0.687), and DIP-VAE + DNN trained with cross-entropy (0.798, 0.677). Using the proposed reliability plot framework, the paper states the method achieves about 80% accuracy while deferring 10% of samples to an expert, versus about 74% for the standard neural network at the same deferral level. Qualitatively, counterfactual analysis reveals (i) small semantic changes can greatly reduce entropy (increase confidence), (ii) subtle feature shifts can flip class outcomes, and (iii) the classifier may exploit spurious correlations in irrelevant corner regions.",None stated.,"The work studies predictive reliability via calibration/uncertainty for a medical image classifier, but does not address reliability engineering constructs such as time-to-failure, degradation, censored life data, maintenance policies, or system/component reliability. The “reliability plot” depends on having access to an expert (or true labels) for deferred cases and assumes entropy is a dependable deferral criterion; performance under dataset shift or label noise is not established. The paper does not provide implementation details sufficient for full reproducibility (e.g., complete training protocol, DIP-VAE training specifics, and code), and comparisons may depend strongly on the chosen latent representation and hyperparameters (e.g., $\alpha=0.7$, $\tau=0.05$, $\eta$ weights).",None stated.,"Evaluate robustness of calibration-driven learning and the deferral-based reliability plots under distribution shift (OOD data), varying prevalence, and label noise, which are common in clinical deployment. Extend the approach to explicitly model uncertainty with probabilistic calibration metrics (e.g., ECE, NLL) and to multi-site validation to assess generalizability. Provide an end-to-end open-source implementation (including DIP-VAE training and counterfactual optimization) and benchmark against stronger modern calibration/deferral baselines (e.g., temperature scaling, deep ensembles with proper scoring rules, selective classification methods).",2004.14480v1,https://arxiv.org/pdf/2004.14480v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:22:23Z
TRUE,Other,ML-based|Physics-based|Simulation-based|Other,Simulated only|Other,Not applicable,Other,Simulation study|Other,TRUE,Python|MATLAB|Other,Not provided,NA,"The paper proposes a simulation-free reliability analysis framework using physics-informed neural networks (PINNs) to estimate failure probabilities without generating training data from expensive ODE/PDE solvers. The method represents the system response as a neural network and trains it by minimizing a physics-informed loss based on the residual of the governing stochastic ODE/PDE, using automatic differentiation to compute derivatives. After training, the learned surrogate is used to evaluate the limit-state function and compute probability of failure and reliability index, avoiding repeated forward simulations typical of MCS/FORM/SORM/IS/SS/DS. The approach is demonstrated on three benchmark problems: a stochastic decay ODE, a stochastic viscous Burgers’ PDE with uncertain boundary perturbation, and a stochastic biochemical signaling ODE system with six uncertain parameters. Across these cases, the PINN-based approach matches benchmark estimates closely while requiring zero solver calls for data generation, highlighting a potential new paradigm for time-dependent reliability problems due to the continuous spatio-temporal surrogate.","Failure probability is formulated as $P_f=\int_{\Omega_\Xi} I_{\Omega^F_\Xi}(\xi)\,dF_\Xi(\xi)$ with limit-state $J(\xi)=u(\xi)-u_0$ (or problem-specific forms such as $J=u(t,Z)-u_d$, $J=-z(\delta)+z_0$, $J=e_{3p}(Z,t)-e_{3p,0}$). The stochastic system response is approximated by a neural network $u(x,t,\xi)\approx u_{NN}=\mathcal{N}(x,t,\xi;\theta)$ and modified to satisfy IC/BC: $\hat u= u_{b,i}+B\,u_{NN}$ (e.g., $\hat u=t\,u_{NN}+u_0$ for the ODE; $\hat u=(1-x)(1+x)t\,u_{NN}+u(t=0,x)$ for Burgers’). Training minimizes the physics-informed residual MSE over collocation points: $L(\theta)=\frac1{N_c}\sum_{k=1}^{N_c} \big(\hat R(x_k,t_k,\xi_k;\theta)\big)^2$ where $\hat R=\hat u_t+g(\hat u,\hat u_x,\hat u_{xx},\ldots;\xi)$ is computed via automatic differentiation. Reliability index is computed as $\beta=\Phi^{-1}(1-P_f)$.","For the stochastic decay ODE benchmark, the paper reports an exact $P_f=0.003539$ ($\beta=2.6932$) and the proposed PI-DNN achieves $P_f=0.0035$ ($\beta=2.6949$) with 0 simulations, comparable to MCS with $10^6$ samples. For the viscous Burgers’ PDE case at $z_0=0.45$ and $t=10$, PI-DNN reports $P_f=0.0999$ ($\beta=1.2821$) versus MCS $P_f=0.1037$ ($\beta=1.2607$), again with 0 solver runs while MCS used 10,000 FE runs. For the cell-signaling cascade at $e_{3p,0}=0.54$ and $t=5.0$, PI-DNN matches MCS exactly ($P_f=0.0459$, $\beta=1.6860$) with 0 simulations while MCS used 10,000 ODE solves. The paper also provides sensitivity studies showing stability beyond roughly 500 collocation points for the ODE case and around 30,000 collocation points for the Burgers’ case, and notes architecture-dependent underfitting/overfitting behavior in the PDE example.","The authors note that with too many neurons/layers the network can over-fit (observed in the Burgers’ example), and suggest regularization as a potential remedy but state it is not within the scope of the current work. They also emphasize that the numerical examples are not computationally complex, chosen so that benchmark solutions using established simulation-based reliability methods can be generated for comparison.","The framework assumes the governing ODE/PDE (and boundary/initial conditions) is known and differentiable, which limits applicability when physics is partially unknown, discontinuous, or only available via black-box simulators. Results are shown on three benchmarks; robustness to high-dimensional uncertainty, strongly chaotic/non-smooth responses, or complex geometries/meshes is not established, and the collocation-point selection is not optimized for rare-event regions (which could affect very small $P_f$). Comparisons focus on solver-call counts rather than total wall-clock time including PINN training, which can be substantial and hardware/implementation dependent. The method’s behavior under model-form uncertainty and noisy/uncertain boundary conditions beyond simple parametric uncertainty is not analyzed.","The authors suggest replacing fully connected networks with convolutional neural networks to address high-dimensional reliability problems. They also propose developing adaptive versions of the algorithm to select collocation points adaptively and to adaptively choose the neural network architecture, indicating these possibilities will be explored in future studies.","Developing rigorous strategies for rare-event targeting (e.g., adaptive collocation biased toward the failure boundary, active learning around the limit-state surface) could improve accuracy for very small failure probabilities. Extending the approach to handle epistemic/model-form uncertainty and incorporating observational data (hybrid physics-informed + data-informed reliability) would broaden applicability. Providing standardized benchmarks with full training-time reporting, uncertainty quantification for the learned surrogate (e.g., Bayesian PINNs/ensembles), and open-source implementations would strengthen reproducibility and practitioner uptake. Extending to system reliability with multiple components/limit-states and dependent random inputs, and studying performance under correlated inputs and non-Gaussian measures, would connect the method more directly to engineering reliability practice.",2005.01302v3,https://arxiv.org/pdf/2005.01302v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:23:04Z
FALSE,NA,Other,Other,Not applicable,Finance/economics|Other,Simulation study|Other,TRUE,R,Package registry (CRAN/PyPI),https://www.R-project.org/,"The paper studies finite-sample size (Type I error) properties of wild bootstrap-based heteroskedasticity-robust tests in linear regression, showing that many such procedures can be severely oversized. For broad classes of test statistics (including HC0–HC4, HC0R–HC4R, the classical F statistic, and restricted-residual variants) and broad classes of wild bootstrap schemes, the authors derive a computable diagnostic constant \(\vartheta\) (depending on observables like design matrix and restriction) such that the test’s size equals 1 whenever the nominal level \(\alpha>\vartheta\). They explain the mechanism via distributional “concentration” under extreme heteroskedasticity that collapses the test statistic distribution to near-deterministic values, defeating the bootstrap critical values. Extensive numerical experiments (e.g., n=10 with 960 statistic/bootstrap combinations across many designs and hypotheses) find that most combinations admit \(\vartheta\) below common \(\alpha\) (0.05 or 0.1), implying theoretical size 1 in some cases, and that many remaining combinations still have large size lower bounds. The authors provide an R package (wbsd) to compute \(\vartheta\) as a practical diagnostic tool and conclude that wild bootstrap heteroskedasticity-robust tests are not a general reliability fix for overrejection.","The core regression model is \(Y=X\beta+U\) with heteroskedastic covariance \(\sigma^2\Sigma\), \(\Sigma\in \mathcal C\subseteq \mathcal C_{Het}\), where \(\mathcal C_{Het}=\{\mathrm{diag}(\tau_1^2,\dots,\tau_n^2):\tau_i^2>0,\sum_i\tau_i^2=1\}\). The main heteroskedasticity-robust Wald statistic is \(T_{Het}(y)=(R\hat\beta(y)-r)'\hat\Omega_{Het}(y)^{-1}(R\hat\beta(y)-r)\) (or 0 if \(\hat\Omega\) is singular), with \(\hat\Omega_{Het}=R\hat\Psi_{Het}R'\) and \(\hat\Psi_{Het}=(X'X)^{-1}X'\mathrm{diag}(d_i\hat u_i(y)^2)X(X'X)^{-1}\) for HC0–HC4-type weights \(d_i\). Wild bootstrap samples are generated as \(y^*(y,\xi)=X\tilde\beta_{\mathcal M_0}(y)+\mathrm{diag}(\xi)\,(y-X\tilde\beta_A(y))\) (or centered at \(X\hat\beta(y)\)), and a computable \(\vartheta\) is defined so that for \(\alpha>\vartheta\) the bootstrap test has size 1.","The authors prove finite-sample “size one” breakdown results: for large classes of test statistics and wild bootstrap variants, there exists a computable \(\vartheta\le 1\) such that if \(\alpha>\vartheta\) the worst-case null rejection probability over \(\mathcal C_{Het}\) equals 1. In numerical experiments for n=10 across 960 statistic/bootstrap combinations, they report that for 826 combinations \(\vartheta_{\min}<0.05\) and for 936 combinations \(\vartheta_{\min}<0.1\), implying theoretical size 1 for some designs/hypotheses at those nominal levels. For many of the remaining combinations, a computed lower bound on size exceeded \(3\alpha\) for some designs/hypotheses (95 of 134 remaining at \(\alpha=0.05\); 11 of 24 remaining at \(\alpha=0.1\)). Across n\in\{10,20,30\} they conclude no considered bootstrap-based test is reliably non-overrejecting for both \(\alpha=0.05\) and \(\alpha=0.1\) in all settings; only a couple of procedures (HC3R-based with Mammen wild bootstrap imposing the null) avoided breakdown within their studied range for \(\alpha=0.05\).","The authors emphasize their results are worst-case over heteroskedasticity (and, in the numerical study, over searched designs/restrictions) and thus do not preclude a given bootstrap test from being reasonably sized for specific applications. They note that computing \(\vartheta\) is numerically nontrivial and, to improve reliability, recommend supplementing \(\vartheta\) computations with direct numerical evaluation of rejection probabilities for strategically chosen heteroskedasticity structures. They also point out that their size-one proof mechanism relies on covariance models rich enough to approximate extremely heteroskedastic cases (e.g., those approaching \(e_ie_i'\)); if one imposes a lower bound on variances, the exact size-one argument may not apply (though size can still be large for small bounds).","The practical diagnostic \(\vartheta\) can be conservative or sensitive to numerical tolerances and discretization choices (e.g., empirical approximations to bootstrap distributions for larger n), potentially affecting conclusions for borderline cases. The numerical search over design matrices and hypotheses, while extensive, is not exhaustive; different design-generation mechanisms (beyond lognormal regressors and specific restrictions) could alter which procedures appear to “survive.” The work focuses on linear regression and wild bootstrap variants; it does not directly treat dependence/autocorrelation in errors, clustered settings, high-dimensional regimes, or nonlinear models where analogous breakdown phenomena may differ or require new diagnostics.","The authors state that similar results could likely be developed for heteroskedasticity-robust procedures based on feasible generalized least squares estimators built from (possibly misspecified) heteroskedasticity models, but they do not pursue this avenue. They also recommend, in applications, potentially combining \(\vartheta\) computations with numerical evaluation of null rejection probabilities for strategically chosen heteroskedasticity patterns to better assess reliability.","Extending the finite-sample breakdown diagnostics to settings with autocorrelation/cluster dependence (e.g., HAC/cluster-robust inference) and to panel or time-series designs would broaden applicability. Developing robust/self-starting versions of the diagnostic that incorporate uncertainty in design selection, unknown error distribution, or leverage/outlier handling could improve practical guidance. Providing standardized benchmarking suites (designs, restrictions, heteroskedasticity patterns) and automated search tools for “worst-case” designs could make the stress-testing approach more reproducible and comparable across methods.",2005.04089v2,https://arxiv.org/pdf/2005.04089v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:23:58Z
TRUE,Maintenance optimization|Failure mode analysis|Other,ML-based|Hybrid/Ensemble|Other,Event/count data|Sensor/condition monitoring,Predictive,Network/cybersecurity|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper studies log-based anomaly detection for predictive maintenance in a large-scale computing center (WLCG Tier-1 Bologna/CERN context), framing it as an online, unbalanced 4-class severity classification problem (normal, low, medium, high severity). It compares two evolving granular classifiers—Fuzzy-set-Based evolving Modeling (FBeM) and an evolving Granular Neural Network (eGNN)—that learn incrementally from a streaming feature vector extracted from fixed time windows of logging activity rate. Weak labels are generated online using a dynamic control-chart procedure based on deviations (k standard deviations) of windowed mean activity from the global mean. Performance is evaluated on real StoRM backend logs using accuracy, model compactness (number of rules/granules), and execution time across several window sizes (5–60 minutes) and meta-parameter settings. Results show eGNN generally achieves higher accuracy with a compact rule base (~10–14 rules), while FBeM can match peak accuracy only by growing to many more rules (~40) under small initial granularity.","The control-chart labeling computes window means $\mu_j=\frac{1}{n}\sum_{i=1}^n u_i$ over fixed windows and tags severity by how far $\mu_j$ lies from the overall mean $\bar{\mu}=\frac{1}{m}\sum_{j=1}^m \mu_j$ using standard-deviation bands $\sigma_k(\mu)=k\sqrt{\frac{1}{m}\sum_{j=1}^m(\bar{\mu}-\mu_j)^2}$. FBeM selects the most active rule using $\alpha^*=\max_i\alpha_i$ with $\alpha_i=\min_j A^i_j(x_j)$ and adapts granularity via Eq. (4) based on rule-growth rate. eGNN computes similarity between input and trapezoidal granules (Eq. 9), aggregates activations to pick the most active rule/class, and updates feature weights as $w_{ij}^{new}=w_{ij}^{old}-\beta_i\,xe_{ij}\,|\varepsilon|$ with $\beta_i=\frac{Wrong_i}{Right_i+Wrong_i}$.","On the StoRM log dataset (1,436 instances per window length; 5 attributes; 4 classes), eGNN achieved its best reported average accuracy of 96.17% ± 0.78 with 60-minute windows using about 10.35 ± 1.32 rules and ~0.18 ± 0.02 s runtime. FBeM’s best compact-model result was 85.64% ± 3.69 with 60-minute windows, about 12.63 ± 3.44 rules, and ~0.18 ± 0.02 s runtime; its accuracy degraded to ~67% for 5–15 minute windows under similar compactness constraints. In a broader meta-parameter sweep without a strict rule-count constraint, FBeM reached ~96.1% accuracy but required ~40 rules (e.g., initial $\rho=0.1$), whereas eGNN reached ~96.2% with ~10 rules (e.g., initial $\rho=0.7$). Confusion-matrix examples indicate medium/high severity classes are harder; FBeM slightly outperformed eGNN in detecting the highest-severity class in one illustrated case, though eGNN had superior overall accuracy.",The authors state they focus only on logging activity rate and explicitly exclude analysis of the log message content (out of scope). They also note that online labels are weak and the multiclass setting is unbalanced due to the control-chart-based tagging. They mention eGNN may require more instances to mature/converge because it has more parameters per local model.,"The control-chart labeling relies on (implicit) distributional stability/normality assumptions for windowed means and uses global statistics, which may be problematic under concept drift or nonstationary baselines typical of data centers. Evaluation is primarily accuracy-centric despite class imbalance; metrics such as per-class recall/precision, cost-weighted errors, or time-to-detect would better reflect predictive-maintenance priorities. Reproducibility is limited because the implementation details (software/language, hyperparameter search protocol, and code) are not provided, and it is unclear how shuffling a time stream affects realism for online deployment.","The conclusion indicates future research will be outlined, and suggests applying the evolving classifiers broadly to monitor additional services that produce event-oriented logs in the WLCG Tier-1 environment. It also motivates continued use of real-time log-content processing approaches to support decision-making for predictive maintenance, implying extension to other services/streams.","A natural extension is to incorporate concept-drift-aware baselining for labeling (adaptive control limits) and to relax independence/normality assumptions in the control-chart step. The models could be evaluated with maintenance-relevant, imbalance-robust objectives (e.g., class-4 recall, expected cost, early-warning lead time) and with strictly time-ordered evaluation instead of shuffled windows. Integrating richer features (e.g., parsed log templates, sequences, and multivariate service metrics) and providing a deployable open-source implementation would strengthen practical adoption in data-center predictive maintenance.",2005.04156v1,https://arxiv.org/pdf/2005.04156v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:24:38Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/saum25/local_exp_robustness,"This paper studies the robustness of SoundLIME (SLIME), a model-agnostic local explanation method for machine listening, focusing on how input-occlusion perturbations affect explanation reliability. Using a state-of-the-art deep CNN singing voice detection model (SVDNet-R1) and two real datasets (Jamendo and RWC), it shows SLIME explanations are sensitive to (i) the number of synthetic samples used and (ii) the content used to fill occluded regions (e.g., zeros, per-input mean, Gaussian noise). The authors identify that large numbers of synthetic samples (about 50,000–70,000) are needed for stable explanations in their setup, and they demonstrate substantial changes in top-k influential components across different occlusion content types even when stability is controlled. They propose a ground-truth-driven procedure to choose suitable occlusion content types and, using a synthetic vocal/non-vocal mixing dataset derived from ccMixter stems, find that using the per-input mean (meaninp) best aligns temporal explanations with ground truth. The work advances explainable audio ML by providing empirical evidence and a practical selection method for generating more reliable occlusion-based explanations.","SLIME fits an interpretable surrogate model (here linear) in the binary interpretable space: $\gamma(z^0)=w^\top z^0$, where $z^0\in\{0,1\}^{|X|}$ indicates which interpretable components are present. The surrogate is learned by minimizing $\arg\min_\gamma \big(L(C,\gamma,\rho_x)+\Delta(\gamma)\big)$, with locally weighted squared loss $L(C,\gamma,\rho_x)=\sum_{(z_i^0,z_i)\in Z} \rho_x(z_i)\,[C(z_i)-\gamma(z_i^0)]^2$, where $C$ is the black-box classifier and $z_i$ is the perturbed input obtained by occluding components.","For both Jamendo and RWC excerpts, explanation stability (measured via the number of unique interpretable components across repeated runs) improves as the number of synthetic samples increases, with stable behavior observed for roughly $N_s\ge 50{,}000$ (they use $N_s=70{,}000$ for subsequent experiments). Across multiple excerpts, the top-3 influential components in SLIME explanations often show little to no overlap when changing the occlusion fill content (zero, min-in-input, mean-in-input, standardized Gaussian noise), indicating strong sensitivity to perturbation design. In a large-scale overlap analysis using the zero-fill explanation as reference, meaninp yielded higher overlap than mininp or standardized Gaussian noise, while Gaussian noise frequently produced zero overlap. Using a ground-truth-based selection method on a synthetic dataset (656 true-positive excerpts), meaninp achieved perfect top-3 match with ground truth for about 34% of excerpts versus 23.9% (zero), 7.16% (mininp), and 18.44% (noise), and achieved at least 2-of-3 overlap for about 83.68% of excerpts.","The authors note that SLIME explanation accuracy can be limited by model accuracy on perturbed inputs, and their synthetic-dataset evaluation uses out-of-distribution samples because SVDNet-R1 is trained on Jamendo but tested on ccMixter-derived mixtures. They also highlight that some content types (e.g., mindata) can yield unstable explanations for the chosen $N_s$, complicating sensitivity interpretation for that content type.","The work focuses on a single task (singing voice detection) and one primary model family (a specific CNN architecture), so it is unclear how broadly the quantitative thresholds (e.g., $N_s\approx 50k$–$70k$) generalize to other audio tasks, model types, or segmentation granularities. Robustness is assessed mainly via overlap/stability of top-k components rather than calibration of surrogate fidelity or downstream decision impacts, which may miss other failure modes (e.g., faithful-but-unstable vs stable-but-unfaithful explanations). The ground-truth-driven content selection is demonstrated only for temporal explanations; extending it to spectral/time-frequency explanations may require different annotation types or assumptions not explored here.","The authors propose investigating whether other occlusion-based local explanation methods (e.g., deconvolution/related perturbation approaches and LIME-style methods) exhibit similar sensitivity to occlusion content type. They also plan to examine SLIME behavior under label-preserving transformations of inputs (e.g., increasing loudness).","A useful extension would be to quantify explanation faithfulness directly (e.g., deletion/insertion metrics or perturbation curves) alongside stability to separate ‘consistent’ from ‘correct’ explanations. Developing an adaptive/self-tuning strategy for choosing $N_s$ per input (or per content type) could reduce compute while maintaining reliability, especially since $N_s=70{,}000$ is expensive. Another promising direction is to study robustness under autocorrelation/time-structure constraints and to compare occlusion fills generated by generative audio priors (inpainting) versus simple statistics (mean/min/noise) to better approximate “removal” in the input space.",2005.07788v1,https://arxiv.org/pdf/2005.07788v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:25:19Z
FALSE,NA,Bayesian|Stochastic process|Other,Other,Not applicable,Other,Case study (real dataset)|Simulation study,TRUE,MATLAB,Not provided,http://www.mathworks.com/|https://www.lncc.br/˜pesquef/AR pages.html|http://www.bnc.cat/Exposicions/Uns￾incunables-del-sonor/Continguts-de-l-exposicio,"The paper proposes a Bayesian audio restoration method for old recordings degraded by long, low-frequency “thump” pulses caused by scratches/breakages in vinyl/gramophone media. It jointly estimates pulse location and initial discontinuity duration, interpolates the underlying signal segment masked by the discontinuity using an autoregressive (AR) model, and estimates the pulse tail either via a parametric shape model or (main contribution) a nonparametric Gaussian Process (GP) with a squared-exponential kernel. Posterior inference is performed with a Gibbs sampler with Metropolis–Hastings steps for hard-to-sample parameters; GP tail hyperparameters are initialized by maximum likelihood and then held fixed to reduce computation. Performance is evaluated in controlled simulations and multiple audio examples, reporting perceptual quality (PEAQ) and SNR comparisons against prior methods (TPSW filtering, empirical mode decomposition, and AR-based separation), and demonstrating reduced user intervention while achieving similar or slightly better perceptual results. Implementations and experiments are conducted in MATLAB on 44.1 kHz mono audio; the paper notes remaining challenges with overlapping pulses and AR interpolation under noisy conditions.","The observation model partitions a frame into pre-pulse, discontinuity, and tail regions: $y_0=x_0$, $y_1=x_1+v_d$, $y_2=x_2+v_t$. The discontinuity is modeled as white Gaussian noise on a support starting at $n_0$ for $M$ samples with variance $\sigma_d^2$. The GP tail model assumes $v_t \sim \mathcal N(0,C)$ with squared-exponential kernel $K_{SE}(t,t')=\sigma_f^2\exp\{-(t-t')^2/(2\sigma_\ell^2)\}$, and inference is done via Gibbs/MH sampling of $(n_0,M,\sigma_d^2,v_t)$ (with AR-based prior for the clean signal).","In experiment (A) with an artificially generated pulse, the sampler recovers the true location and duration exactly (credible intervals for $n_0$ and $M$ collapse to the true values) for both shape-based and GP tail models. For multiple artificially corrupted signals (experiment B), PEAQ improves substantially versus degraded audio; e.g., for the “Brazilian” signal PEAQ changes from -1.9499 (distorted) to -0.7814 (shape-based) and -0.2341 (GP), and similar improvements are reported for “Jazz” and “Classical”. Comparisons show the GP variant is generally comparable to or slightly better in PEAQ than TPSW/EMD/AR baselines, though SNR improvements are sometimes lower because the GP may also fit some underlying-signal fluctuations. Real degraded examples (“Cylinder” and “Chopin”) demonstrate strong subjective removal of the low-frequency tail, with small residual artifacts near the initial discontinuity attributed to AR modeling under background noise.","The authors state the method is not yet able to properly handle overlapping pulses, particularly when a new initial discontinuity occurs on top of an unfinished tail. They also note interpolation of the signal under the initial discontinuity can be impaired by background noise because the AR model parameters are estimated beforehand and kept fixed. For the GP tail, the squared-exponential kernel is acknowledged as a stationary model that does not perfectly match the nonstationary pulse tail, and can leave nonzero tail end values requiring a fade-out heuristic.","The approach is tightly tailored to a specific degradation type and assumes an additive model with a short discontinuity region as high-variance Gaussian noise, which may not capture nonlinear playback artifacts or more complex scratch responses. The inference relies on MCMC (and, in the GP case, high-dimensional Gaussian computations) and the paper itself notes matrix inversions at each iteration, which may limit scalability or real-time applicability without further approximation. The pulse-detection initialization can generate false positives on clean signals due to its normalization step, and the need to tune detection parameters ($L,\xi,c,f_{co}$) remains a practical sensitivity point.","They propose extending the method to handle overlapping pulses and improving interpolation in the initial discontinuity region, especially under background noise, likely by better treatment/estimation of AR parameters. They also suggest sub-sampling the pulse tail to reduce computational cost (since large matrix inversions are required) and exploring variational inference as an alternative to MCMC to accelerate estimation.","A natural extension would be to learn or design a nonstationary GP kernel specialized for thump tails (e.g., decaying amplitude/frequency structure) while controlling parameter count via hierarchical priors. Robust/self-starting variants that reduce user tuning for the initialization stage (or integrate detection into the Bayesian model) would improve usability. Additional validation on larger, diverse real-world restoration benchmarks with standardized listening tests (beyond informal listening plus PEAQ) would strengthen evidence of generalizability.",2005.14181v2,https://arxiv.org/pdf/2005.14181v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:26:00Z
FALSE,NA,Stochastic process|Simulation-based|Other,Simulated only,Not applicable,Healthcare/medical,Simulation study|Exact distribution theory|Other,TRUE,MATLAB,Not provided,http://hp.hisashikobayashi.com|https://en.wikipedia.org/wiki/Hisashi_Kobayashi,"This paper develops a stochastic epidemic model for infectious diseases (motivated by COVID-19) based on a birth-and-death process with immigration (BDI), contrasting it with the classical deterministic SIR model. Using probability generating functions, it derives closed-form transient and steady-state distributions for the infected-count process $I(t)$, showing that $I(t)$ follows a generalized negative binomial distribution with parameter $r=\nu/\lambda$ (often $<1$ in practical settings). The analysis highlights why epidemic trajectories can vary enormously across similar populations: small $r$ yields a heavy/long-tailed distribution and a coefficient of variation that approaches $1/\sqrt{r}$, making deterministic mean trajectories unrepresentative of typical outcomes (e.g., mean far from median). The report also outlines a MATLAB event-scheduling simulation framework (external infected arrivals at rate $\nu$, internal infections at rate $\lambda I(t)$, removals at rate $\mu I(t)$) and uses it to illustrate large between-run variability. The work positions the BDI model as an analytically tractable stochastic alternative intended to produce more informative epidemic forecasts than deterministic SIR curves.","The BDI model sets state-dependent birth/death rates $\lambda_n=n\lambda+\nu$ and $\mu_n=n\mu$, leading to a PGF PDE $\partial_t G=(z-1)[(\lambda z-\mu)\partial_z G+\nu G]$. For $I(0)=0$, the PGF solution is $G(z,t)=\left(\frac{1-\beta(t)}{1-\beta(t)z}\right)^r$ with $r=\nu/\lambda$ and $\beta(t)=\frac{\lambda(e^{at}-1)}{\lambda e^{at}-\mu}$, $a=\lambda-\mu$, implying $I(t)\sim \mathrm{NB}(r,\beta(t))$. The mean and variance are $\mathbb{E}[I(t)]=\frac{\nu}{a}(e^{at}-1)$ and $\mathrm{Var}[I(t)]=\frac{\nu(\lambda e^{at}-\mu)(e^{at}-1)}{a^2}$, giving $\mathrm{CV}(t)=1/\sqrt{r\beta(t)}\to 1/\sqrt{r}$ as $t\to\infty$ (when $a>0$).","The paper derives an explicit transient PMF $P_n(t)=\binom{n+r-1}{n}(1-\beta(t))^r\beta(t)^n$ (generalized negative binomial) for the infected-count process under the BDI model, and a steady-state negative binomial distribution exists only in the stable case $a=\lambda-\mu<0$. It shows that for small $r=\nu/\lambda$ (typical when importation $\nu$ is small), the distribution becomes long-tailed and approaches Fisher’s logarithmic series distribution as $r\to 0$, explaining extreme run-to-run variability. A central quantitative conclusion is that the coefficient of variation rapidly converges to $\sqrt{r^{-1}}$, implying dispersion remains large even as the mean grows. In the worked example with $(\lambda,\mu,\nu)=(0.3,0.1,0.2)$ so $r=2/3$, the limiting CV is $\sqrt{1.5}\approx 1.225$, and the paper reports at $t=30$: $\mathbb{E}[I(t)]\approx 402.5$, $\mathrm{Var}\approx 243{,}411.9$, $\sigma\approx 493.4$, CV $\approx 1.226$; at $t=50$: $\mathbb{E}[I(t)]\approx 22{,}025.5$, $\mathrm{Var}\approx 727{,}706{,}000.9$, $\sigma\approx 26{,}976.0$, CV $\approx 1.225$.",None stated.,"Although it motivates “more reliable” prediction, the paper largely demonstrates variability via model-implied distributions and simulated sample paths, without validation against real epidemic datasets or parameter-estimation uncertainty quantification. The base model assumes homogeneous mixing, effectively infinite susceptible population, constant rates $(\lambda,\mu,\nu)$ (except for a piecewise change example), and Markov/exponential infectious periods—assumptions that can be violated with time-varying interventions, behavior change, reporting delays, and heterogeneous contact networks. The BDI formulation focuses on the infected-count process and does not explicitly model observation processes (e.g., underreporting, test sensitivity) that would be necessary for practical forecasting and calibration.","The author states that Part II will provide extensive simulation results, further analysis, and a fuller treatment of related processes such as secondary infections $B(t)$ and removals/deaths $R(t)$. The conclusion also proposes studying control actions by changing $\lambda$, $\mu$, and $\nu$ over time (e.g., social distancing to reduce $\lambda$, improved treatment to increase $\mu$), and discussing how to estimate model parameters from empirical data to improve forecast reliability.","A valuable next step would be to couple the latent BDI process to an explicit observation/reporting model (e.g., delayed, censored, or underreported counts) and evaluate predictive calibration on real COVID-19 incidence time series. Extending the model to heterogeneous or networked populations (multiple groups/regions with importation links) would test whether the heavy-tail/CV results persist and how they scale in metapopulation settings. Developing Bayesian or likelihood-based inference for $(\lambda,\mu,\nu)$ with time variation and intervention change points, including uncertainty bands for forecasts, would make the approach more actionable for decision support.",2006.01586v1,https://arxiv.org/pdf/2006.01586v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:26:42Z
FALSE,Other,ML-based|Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/ferortega/bernoulli-matrix-factorization,"The paper proposes Bernoulli Matrix Factorization (BeMF), a collaborative-filtering recommender-system model that outputs both a discrete rating prediction and an associated “reliability” value interpreted as the probability of the predicted rating class. The method reframes matrix factorization as D parallel Bernoulli (binary) factorizations—one per discrete rating level—and combines their logistic outputs into a normalized probability vector over rating classes; reliability is taken as the maximum class probability. The authors derive a Bernoulli likelihood with Gaussian priors on latent factors, yielding a regularized negative log-likelihood objective optimized via gradient descent, and provide explicit update rules and pseudocode. Empirical evaluation on MovieLens, FilmTrust, and MyAnimeList shows that filtering predictions/recommendations by the proposed reliability improves accuracy (lower MAE) at the expense of coverage/recall, and that BeMF’s native reliability scores outperform enforced reliability added via a prior MF-based framework (measured by RPI). The work advances recommender-system reliability/confidence estimation, but it is not about reliability engineering of physical systems or maintenance; “reliability” here is prediction confidence.","For each rating level s, observed binary matrix entries are modeled as Bernoulli: $p(R^s_{u,i}=1\mid U^s_u,V^s_i)=\psi(U^s_u\cdot V^s_i)$ and $p(R^s_{u,i}=0\mid\cdot)=1-\psi(U^s_u\cdot V^s_i)$, with $\psi$ a logistic-like function (often $\text{logit}$). The per-score objective is the regularized negative log-likelihood: $F(U,V)=-\sum_{R=1}\log \psi(U\cdot V)-\sum_{R=0}\log(1-\psi(U\cdot V)) + \frac{\eta_U}{2}\sum_u\|U_u\|^2+\frac{\eta_V}{2}\sum_i\|V_i\|^2$, optimized by gradient descent. Final class probabilities are normalized across scores: $\Phi(u,i)=\frac{1}{\sum_{\alpha}\psi(U^{s_\alpha}_u\cdot V^{s_\alpha}_i)}\, (\psi(U^{s_1}_u\cdot V^{s_1}_i),\ldots,\psi(U^{s_D}_u\cdot V^{s_D}_i))$; prediction is $\hat R_{u,i}=\arg\max_\alpha p^{\alpha}_{u,i}$ and reliability $\rho_{u,i}=\max_\alpha p^{\alpha}_{u,i}$.","BeMF reliability values are concentrated mostly in the range roughly 0.3–0.5 on the tested datasets, with relatively few predictions above 0.75 reliability. As the minimum reliability threshold increases, MAE decreases while coverage decreases, showing the intended accuracy–coverage tradeoff; BeMF shows notable gains versus baselines when coverage is in the 50%–75% range (from the plots). Using the Reliability Prediction Index (RPI), BeMF’s native reliability outperforms reliability enforced via an external MF-based framework: MovieLens 0.09344191 vs 0.03607168, FilmTrust 0.17187947 vs 0.03277490, and MyAnimeList 0.17087788 vs 0.03386983. In recommendation evaluation (top-10), filtering by recommendation reliability increases precision and reduces recall; BeMF provides the best precision for intermediate probability thresholds (about 0.5–0.75) while maintaining a better precision–recall balance than competing methods (per the plotted results).",None stated.,"The paper’s “reliability” is prediction confidence in recommender systems and is not reliability engineering in the sense of lifetime/failure modeling, so results do not transfer to engineering reliability contexts. The model assumes independence across rating levels by training D separate Bernoulli tasks; this can produce poorly calibrated or inconsistent class probabilities and may ignore ordinal structure in ratings. Evaluation focuses on a small set of datasets and primarily on MAE/coverage and precision/recall tradeoffs; broader calibration metrics (e.g., Brier score, ECE) and robustness to distribution shift or temporal drift are not reported. Computational cost scales with D separate factorizations and may be heavier than single-model alternatives; runtime/complexity comparisons are not emphasized.","The authors propose evaluating BeMF on beyond-accuracy metrics such as novelty, diversity, and discovery, and studying robustness to shilling attacks (nuke/promote). They also suggest exploring alternative underlying probability distributions (introducing dependencies among scores or using other distributions to capture rare events, cold users, etc.). Additional extensions include incorporating social/content information, adapting the model to group recommendations, and adding time information to model preference changes over time.","Assess probability calibration explicitly (e.g., reliability diagrams, ECE, Brier score) and possibly apply post-hoc calibration (Platt/isotonic/temperature scaling) to improve the interpretability of “reliability” as a probability. Incorporate ordinal constraints or structured multiclass modeling (e.g., ordinal logistic / categorical with shared parameters) to reduce inconsistency across per-score Bernoulli models and improve data efficiency. Extend to settings with implicit feedback and exposure bias, and test under temporal splits to quantify performance under drift. Provide standardized runtime/memory benchmarks and release a packaged implementation (e.g., Maven artifact) to ease adoption beyond the research codebase.",2006.03481v6,https://arxiv.org/pdf/2006.03481v6.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:27:28Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,Stochastic process|Nonparametric/Semi-parametric|ML-based|Hybrid/Ensemble,Degradation measurements|Sensor/condition monitoring|Complete lifetime data|Right-censored,Predictive|Condition-based,Transportation/logistics|Energy/utilities|Other,Simulation study,TRUE,R|Other,Not provided,https://CRAN.R-project.org/package=fdapace,"The paper proposes a data-driven health-indicator (HI) forecasting method to improve remaining useful life (RUL) estimation when only an initial segment of HI observations is available for a new unit. It learns a nonparametric mean and covariance of the population HI trajectories from run-to-failure data, models the latent HI as a Gaussian process, and then generates many plausible full-life HI curves via FPCA-based sampling. The forecast for a new unit is selected by scenario matching: choose the generated curve with minimum RMSE over the observed initial period, avoiding explicit posterior derivations used in empirical Bayes FDA approaches. RUL is then estimated as the time for the forecasted monotone HI to cross a fixed soft-failure threshold. Experiments on NASA C-MAPSS turbofan engine data (FD001/FD002) show improved HI extrapolation RMSE and improved RUL RMSE versus BayesFDA, regression (linear), and nearest-neighbor baselines.","Observed HI model: $S_i(t)=\tilde S_i(t)+\epsilon_i(t)$ with latent trajectories i.i.d. from a stochastic process with mean $\mu(t)$ and covariance $G(t,t')$. Nonparametric estimation of $\mu$ and $G$ uses local linear smoothing (or sample counterparts when curves are dense). Generated curves use FPCA/Karhunen–Loève truncation: $\tilde S(t)\approx \mu(t)+\sum_{r=1}^P \xi_r\phi_r(t)$ with $\xi_r\sim \mathcal N(0,\lambda_r)$ under the GP assumption. Scenario matching selects $\hat{\tilde S}_{new}(t)=\tilde S_{sel}(t)$ where $sel=\arg\min_i \sqrt{\frac{1}{m_{new}}\sum_j(\tilde S_i(t_{new,j})-S_{new}(t_{new,j}))^2}$. RUL point estimate is $\widehat{RUL}=\inf_{t\in[t^*,M]}\{\hat{\tilde S}_{new}(t)\le \theta\}-t^*$.","On C-MAPSS FD001/FD002 (four selected degradation signals used as HIs), the proposed method achieved lower HI extrapolation error than all baselines; reported improvements in HI forecasting RMSE over the best baseline range from about 7% to 43% depending on dataset/signal (Table II). For RUL estimation RMSE, the proposed method improved over the best baseline by about 6%–36% depending on dataset/signal (Table III). The authors set the number of generated scenarios to $W=1000$ after observing diminishing returns beyond a few hundred simulations. They note LSTM regression did not learn a valid model under their data regime and exclude it from comparisons.","The study only evaluates on C-MAPSS subsets FD001 and FD002 (single fault mode) because the method assumes HI curves are drawn from the same distribution; the two-fault-mode subsets (FD003/FD004) are not handled directly. The paper assumes a monotonic underlying HI and a given soft-failure threshold $\theta$ supplied by domain experts, and assumes the unit has not yet failed at the last observation time ($T>t^*$). They also acknowledge that baseline BayesFDA performance depends on a joint-Gaussian error requirement, which may not hold in practice.","The approach selects a single best-matching simulated trajectory, so it does not provide calibrated predictive uncertainty for the forecasted HI or RUL (despite using a GP generative model). Performance depends on the representativeness of the run-to-failure population and on hyperparameters (smoothing bandwidths, FPCA truncation $P$, and number of scenarios $W$), but sensitivity/selection procedures are not fully validated across diverse real datasets. The monotonic-HI assumption and fixed threshold-crossing definition may be violated for many assets with regime changes, maintenance interventions, or non-monotone indicators. Computational cost can grow with dense time grids, FPCA, and large $W$, and the paper does not benchmark runtime or scalability.",They suggest extending the approach to C-MAPSS subsets with two fault modes (FD003/FD004) by clustering HI curves into groups (each with monotonic trend) and applying the method within each cluster. They state this multi-fault-mode extension is subject of future work.,"A natural extension is to output a full predictive distribution for HI/RUL (e.g., via posterior weighting of multiple matched scenarios or Bayesian model averaging) and to evaluate calibration. Robust/self-starting variants that handle non-monotone or piecewise-monotone HIs, maintenance actions (imperfect repair), and covariate-dependent dynamics (operating regimes) would broaden applicability. Incorporating principled bandwidth/$P$ selection, uncertainty-aware scenario matching, and scalable implementations (with published code) would improve reproducibility and deployment. Validation on real industrial datasets beyond C-MAPSS and comparisons to modern deep sequence models trained with appropriate data augmentation would strengthen evidence.",2006.03729v1,https://arxiv.org/pdf/2006.03729v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:28:06Z
FALSE,NA,"Bayesian|Parametric (Weibull, etc.)|Simulation-based|Other",Other,Not applicable,Food/agriculture|Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Supplementary material (Journal/Publisher),https://doi.org/10.1080/02664763.2021.1913105,"The paper proposes a Bayesian beta nonlinear regression framework to model ruminal feed degradation curves where the response is a proportion in (0,1) and the kinetic parameters are biologically constrained (e.g., a,b in (0,1), a+b<1, and positive rate parameters). It develops an objective/default prior construction strategy for constrained nonlinear beta models (including a mixture-of-exponentials prior for the rate parameter in the Ørskov–McDonald model) that avoids inadmissible predictions produced by least squares or unconstrained likelihood fits. The approach is generalized to a broader class of models of the form y(t)=a+bG(t,ξ) where G is a distribution function on R+; implementation details are provided in BUGS language. The method is evaluated on real ruminal degradation data by comparing three models via DIC and RMSPE, where the Ørskov–McDonald beta model achieved the best predictive error among the compared alternatives. A simulation study (multiple parameter settings and replicated datasets) assesses frequentist-style coverage of Bayesian credible intervals under the proposed default priors and reports generally good coverage.","The core model assumes yi|pi,qi ~ Beta(pi,qi) with pi=μiτ, qi=τ−pi so that E[yi]=μi and Var(yi)=μi(1−μi)/(1+τ), where τ is a precision parameter. For the Ørskov–McDonald kinetics, μi = a + b(1 − e^{−c t_i}) with constraints a∈(0,1), b∈(0,1), a+b∈(0,1), c>0, τ>0. The proposed objective prior factorizes as π(a,b,c,τ)=π(a,b)π(c)π(τ) with π(a,b) uniform over {a>0,b>0,a+b<1}, π(c)=(1/n)∑_{i=1}^n t_i e^{−c t_i} (a mixture of exponentials), and τ given a diffuse Gamma prior.","On the real Ørskov–McDonald (1979) dataset, the beta model (μ=a+b(1−e^{−ct})) achieved DIC = −16.08 and RMSPE = 0.0228, outperforming the Michaelis–Menten alternative (DIC = −0.463, RMSPE = 0.0878) and the France model (DIC = −4.491, RMSPE = 0.0283). Posterior means (model 2) were a=0.174, b=0.772, c=0.101, τ=251.2 with credible intervals respecting constraints (posterior mean a+b=0.946; 95% CI 0.816–0.998). MCMC diagnostics reported potential scale reduction factors roughly in 1.001–1.015 across models, suggesting convergence. In the simulation study (24 parameter sets, 300 datasets each), empirical coverages of nominal 95% credible intervals were generally close to 0.95–1.00 for a, b, c, and σ=1/√τ, with reported interval lengths varying by parameter setting.","The authors note that the precision parameter τ is assumed constant over time; while it could be modeled as time-varying (e.g., log τ_i = θ0 + θ1 t_i), such an extension is limited by the typically moderate sample sizes in ruminal degradation experiments. They also discuss that “lag” parameters (no degradation for an initial period) can be hard to justify biologically and may be difficult to identify unless sampling times are chosen around the lag, and may cause fitting problems, even though lag terms could be added to their framework.","The work is focused on modeling biological degradation proportions rather than reliability/failure processes; its methods may not translate directly to engineering reliability settings without additional structure (e.g., censoring, recurrent failures, or system context). Independence of observations over time is assumed, which may be questionable for repeated measurements from the same experimental unit and could motivate hierarchical/within-unit correlation modeling. The empirical evaluation is based on a small real dataset and comparisons to only two alternative kinetics models; broader benchmarking (more datasets, more competing models/links/priors) would strengthen generalizability. Although implementation in OpenBUGS is shown, the custom default priors (e.g., mixture-of-exponentials for c and Jacobian-based priors for ξ) may be sensitive to the chosen sampling times t_i, potentially affecting robustness across experimental designs.","They suggest extending the model to allow time-varying precision (e.g., modeling τ_i as a function of time after log transformation) but note sample-size constraints. They also state that a lag parameter can be introduced easily in the proposed models, despite concerns about biological justification and identifiability in some experimental designs.","A natural extension is to incorporate random effects or hierarchical structure to handle repeated curves across animals/feeds and account for within-subject correlation and between-subject variability. Robustness studies could assess sensitivity of posterior inference to the proposed objective priors, particularly how π(c) or π(ξ) depends on the observation-time design and to potential model misspecification (e.g., boundary values near 0/1). Developing and releasing a fully reproducible implementation (e.g., a public Stan/R package with the supplementary code) would improve adoption and allow standardized comparisons. Additional model-checking tools (posterior predictive checks tailored for bounded/proportion kinetics data) and validation on multiple independent datasets would strengthen practical guidance.",2006.04461v2,https://arxiv.org/pdf/2006.04461v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:28:48Z
FALSE,NA,Bayesian|ML-based|Hybrid/Ensemble,Complete lifetime data|Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/astirn/VariationalVariance,"This paper studies instability and miscalibration when neural networks jointly parameterize the mean and heteroscedastic variance (precision) of Gaussian likelihoods in regression and variational autoencoders (VAEs). The authors propose treating the observation precision variationally (placing priors on precision and using a Gamma variational posterior), adding an explicit KL regularization term that prevents the variance-collapse pathology and improves posterior predictive checks (PPCs) for mean/variance calibration and sample realism. They evaluate a range of priors for precision, including standard Gamma, VAP (KL removed), VAMP/VAMP*, and heteroscedastic mixture priors such as xVAMP/xVAMP* and VBEM/VBEM*. Experiments on toy heteroscedastic regression, multiple UCI regression datasets, and MNIST/Fashion-MNIST VAEs show their variational-variance approach typically preserves or improves predictive log likelihood while materially improving variance calibration and sample quality relative to common baselines (Normal/Student likelihoods, prior work of Detlefsen et al., and MAP-VAE). The work is a machine-learning uncertainty calibration paper rather than a reliability engineering paper (no failure-time/degradation/maintenance modeling).","For regression, the proposed variational-variance objective is an ELBO: $\mathcal{L}=\sum_{(x,y)\in\mathcal{D}}\mathbb{E}_{q(\lambda\mid\alpha(x),\beta(x))}[\log \mathcal{N}(y\mid\mu(x),\lambda)]-D_{\mathrm{KL}}(q(\lambda\mid\alpha(x),\beta(x))\parallel p(\lambda))$, with $q(\lambda\mid x)=\mathrm{Gamma}(\lambda\mid\alpha(x),\beta(x))$. For VAEs, they add a second variational factor over decoder precision, yielding $\sum_x \mathbb{E}_{q(z\mid x)}[\mathbb{E}_{q(\lambda\mid z)}\log \mathcal{N}(x\mid\mu_x(z),\lambda)-D_{\mathrm{KL}}(q(\lambda\mid z)\parallel p(\lambda))]-D_{\mathrm{KL}}(q(z\mid x)\parallel p(z))$. For heteroscedastic mixture priors (xVAMP/VBEM), $p(\lambda\mid x)=\sum_{j=1}^K \pi_j(x)\,q(\lambda\mid u_j)$ (xVAMP) or $p(\lambda\mid x)=\sum_{j=1}^K \pi_j(x)\,\mathrm{Gamma}(\lambda\mid a_j,b_j)$ (VBEM), leading to a KL term involving a log-sum-exp over mixture components.","On UCI regression, the authors report that their variational-variance methods are frequently top-performing under PPC criteria; in their summary table, VBEM* has the most wins for log predictive likelihood (5 wins, 7 statistical ties) across the considered datasets. For Fashion-MNIST VAEs, V3AE variants greatly reduce variance bias compared to standard VAEs and the Student-VAE baseline; e.g., V3AE-VBEM* achieves variance bias $\approx 4.4\times 10^{-4}$ with sample RMSE $\approx 0.15$, while Student-VAE shows much worse PPC behavior (variance bias reported around $7.4\times 10^{-2}$ and sample RMSE $\approx 0.49$). They also note that fixing variance very small (0.001) yields crisp reconstructions but severely degrades log likelihood and removes meaningful uncertainty. Overall, adding a KL-regularized variational treatment of precision improves calibration and sample realism without sacrificing (and sometimes improving) predictive likelihood.",None stated.,"The work targets calibration/optimization for heteroscedastic Gaussian decoders and does not address common real-world complications such as strong temporal dependence/autocorrelation, heavy-tailed observation noise beyond the tested alternatives, or systematic dataset shift. Many posterior predictive quantities for VAEs are approximated via finite Monte Carlo mixtures (e.g., 20 samples), which can bias likelihood/variance estimates and PPC metrics depending on model and dataset. The paper evaluates on standard benchmarks (UCI, MNIST/Fashion-MNIST) but provides limited evidence in high-stakes uncertainty use cases (e.g., decision-making under uncertainty) and does not report computational cost tradeoffs across all prior choices in a standardized way.",None stated.,"Extend the variational-variance approach to settings with correlated/structured noise (e.g., time series, spatial models) where heteroscedasticity and dependence interact, and assess robustness under distribution shift. Develop self-tuning/adaptive priors for precision (or hierarchical priors) that reduce sensitivity to prior family choice (Gamma/mixtures, K, pseudo-input design) and provide clearer guidance for practitioners. Provide a more thorough study of Monte Carlo approximation error in posterior predictive likelihood/variance for VAEs (number of samples vs. calibration) and release standardized implementations/benchmarks for reproducibility across frameworks.",2006.04910v3,https://arxiv.org/pdf/2006.04910v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:29:38Z
FALSE,NA,ML-based|Bayesian,Other,Not applicable,Pharmaceutical|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/AITRICS/mol_reliable_gnn,"This paper benchmarks Bayesian deep learning methods for improving the reliability (calibration) of molecular property prediction with graph neural networks (GNNs) in virtual screening settings. It evaluates several approximate Bayesian learning approaches (Deep Ensembles, MC Dropout, Bayes-by-Backprop, SGLD, SWA, SWAG) across multiple GNN architectures (GCN, GIN, GraphSAGE, GAT, GatedGCN) and MoleculeNet binary classification tasks (BACE, BBBP, HIV, Tox21). Reliability is assessed primarily via expected calibration error (ECE), alongside predictive performance metrics such as AUROC, using repeated scaffold splits and multiple random seeds. Results show SWA and SWAG provide consistently better-calibrated predictions and competitive or improved AUROC versus MAP training, and that ensembling typically yields additional (smaller) gains. The authors illustrate implications for virtual screening by showing SWAG reduces over-confident extreme probabilities on likely out-of-distribution ZINC compounds, potentially improving hit selection success rates.","Reliability is measured using expected calibration error (ECE): $\mathrm{ECE}=\mathbb{E}_{\text{confidence}}[\,p(\text{correct}\mid \text{confidence})-\text{confidence}\,]$. Bayesian prediction uses posterior marginalization: $p(y^*\mid x^*,D)=\int p(y^*\mid x^*,w)\,p(w\mid D)\,dw$, contrasted with MAP point estimation $w_{\mathrm{MAP}}=\arg\max_w p(w\mid D)$. For MC-dropout, predictive probability is approximated by Monte Carlo averaging: $\bar y^*\approx \frac{1}{T}\sum_{t=1}^T p(y^*\mid x^*,w_t)$.","Across BBBP/BACE/HIV/Tox21 with a GIN backbone, SWA/SWAG substantially reduce ECE compared with a non-Bayesian baseline (e.g., Tox21 single-model ECE: 9.6% for None vs 3.8% for SWA and 3.7% for SWAG; BBBP single-model ECE: 17.9% for None vs 7.1% (SWA) and 6.9% (SWAG)). AUROC is also improved in many cases (e.g., BBBP single-model AUROC: 82.7% for None vs 91.2% for SWA/SWAG; Tox21 single-model AUROC: 73.5% for None vs 78.7–78.8% for SWA/SWAG). Ensembling generally improves both ECE and AUROC further, but the incremental gain is described as smaller than the gain from switching to SWA/SWAG. In a ZINC virtual-screening-style run (200,000 compounds), SWAG yields far fewer extreme high-confidence predictions; only 238 compounds have predicted probability >0.95, suggesting reduced over-confidence on likely OOD samples.","The authors note that some Bayesian methods can perform worse than MAP for certain GNN architectures; in particular, Bayes-by-Backprop (BBB) produced poor results for GCN and GatedGCN and appears sensitive to hyperparameters such as the prior length scale. They state they leave deeper investigation of BBB’s behavior on molecular prediction as future work. They also acknowledge virtual screening on ZINC lacks true labels, so implications are based on prediction-behavior analysis rather than verified hit rates.","The study focuses on calibration via ECE but does not report complementary proper scoring rules (e.g., NLL/Brier) or uncertainty-quality metrics under distribution shift, which could change conclusions about “reliability.” The evaluation uses repeated scaffold splits but does not deeply analyze how hyperparameter tuning budgets differ across methods (e.g., SWA/SWAG schedules vs BBB priors), which can affect fairness of comparisons. Computational cost/latency and memory overhead (especially for ensembles, SWAG sampling, and SGLD) are not quantified, though these are critical constraints in practical screening pipelines. The work is limited to (mostly) binary classification tasks and does not test regression property prediction, where calibration and uncertainty behave differently.","They explicitly state that deeper investigation of BBB on molecular prediction tasks (including its hyperparameter sensitivity such as the prior length scale) is left as future work. They also suggest their benchmark and approach could be utilized in related applications that benefit from Bayesian principles, mentioning active learning and continual learning in molecular tasks as potential directions.","Extend the benchmark to molecular regression tasks (e.g., solubility, affinity) and evaluate uncertainty calibration with regression-appropriate metrics (NLL, RMSE-calibration curves). Add systematic OOD and corruption benchmarks specific to chemistry (scaffold holdouts, novel chemotypes, assay/batch effects) to quantify uncertainty under dataset shift beyond qualitative histograms. Provide cost–benefit analyses (compute time, number of forward passes, energy) for SWA/SWAG/ensembles and explore lightweight alternatives (temperature scaling, post-hoc calibration, Laplace approximations). Release standardized evaluation code for calibration (ECE variants, adaptive binning) and incorporate decision-theoretic screening metrics (precision@k, expected utility) tied directly to experimental budgets.",2006.07021v2,https://arxiv.org/pdf/2006.07021v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:30:12Z
FALSE,NA,ML-based,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/miafei/NodeNorm,"This paper studies why deep Graph Convolutional Networks (GCNs) suffer performance degradation as depth increases, focusing on the often-overlooked role of the transformation (TRAN) operation in each layer. Through ablation experiments that isolate propagation-only (P-GCN) and transformation-only (T-GCN) variants, the authors find TRANs contribute substantially—often more than PROPs—to the accuracy drop in deep models. They identify “variance inflammation,” where TRANs amplify node-wise feature variance across layers, as a key mechanism linked to poorer node classification performance, especially for nodes exhibiting high variance. To mitigate this, they propose Node Normalization (NodeNorm), a plug-in method that rescales each node’s feature vector by a power of its own standard deviation to control variance growth. Experiments across multiple benchmark graph datasets and GNN architectures show NodeNorm enables very deep models (e.g., 64 layers) to match or outperform shallow models and often outperforms prior plug-in methods (e.g., DropEdge, PairNorm); they also analyze LayerNorm and attribute its benefit mainly to variance scaling.","A standard GCN layer is written as $H^{(\ell)}=\mathrm{ReLU}(\hat{A}H^{(\ell-1)}W^{(\ell)})$, comprising PROP: $\bar{H}^{(\ell-1)}=\hat{A}H^{(\ell-1)}$ and TRAN: $H^{(\ell)}=\mathrm{ReLU}(\bar{H}^{(\ell-1)}W^{(\ell)})$. Node-wise feature variance is $\mathrm{var}^{(\ell)}_i=\frac{1}{d_\ell}\sum_{j=1}^{d_\ell}(h^{(\ell)}_{ij}-\mu^{(\ell)}_i)^2$. NodeNorm scales per node as $\mathrm{NodeNorm}(h_i)=\frac{h_i}{(\sigma_i)^{1/p}}$ (with $p\ge 1$ and $\sigma_i=\sqrt{\mathrm{var}_i}$), yielding post-normalization standard deviation $\hat{\sigma}_i=\sigma_i^{(1-1/p)}$.","In ablations on citation benchmarks (Cora/Citeseer/Pubmed), T-GCN (TRAN-only in hidden layers) degrades more severely with depth than P-GCN (PROP-only), indicating TRANs are a major driver of deep-model degradation. NodeNorm substantially reduces the depth-related accuracy drop; e.g., on the standard split, NodeNorm1 on Cora achieves accuracy 0.830 (2 layers), 0.829 (32 layers), and 0.837 (64 layers), while DropEdge and PairNorm show larger declines at 64 layers. In scenarios where deep models are needed (missing features, low label rate, large-diameter graphs), NodeNorm-equipped deep GCNs achieve the best reported accuracies and often at higher optimal depths (e.g., 16–64 layers). NodeNorm also improves other architectures (GAT, GraphSage) and boosts deep architectures (e.g., GEN on ogbn-proteins: AUC-ROC improves from 0.7936±0.0086 to 0.8226±0.0093 at 64 layers). LayerNorm helps as well, and ablations suggest the variance-scaling component (equivalent to NodeNorm1) is the primary contributor.",None stated,"The work targets performance degradation in deep GNNs rather than engineering reliability; results may not generalize to graphs with strong temporal dependence, streaming updates, or distribution shift without further study. NodeNorm introduces a per-node normalization hyperparameter ($p$) whose optimal value varies by scenario, but guidance for choosing $p$ a priori is limited and may require tuning. The evaluation focuses on standard node-classification benchmarks and particular split protocols; broader validation on diverse real-world, noisy, or adversarial settings (and on other tasks like link prediction) is not central. Comparisons depend on specific training choices (e.g., residual connections, fixed hidden size), which could interact with normalization and affect fairness across baselines.",None stated,"Extend variance-inflammation analysis and NodeNorm-style controls to settings with temporal graphs, streaming updates, or explicit distribution shift to assess robustness. Develop self-tuning or adaptive schemes for selecting/annealing $p$ (or learning it) to reduce manual hyperparameter tuning across datasets and depths. Provide deeper theoretical characterization of how TRAN-induced variance interacts with oversmoothing/optimization dynamics and when variance control is sufficient. Add broader empirical validation across tasks (e.g., link prediction, graph regression) and domains, and release a standardized implementation/benchmarking harness for reproducibility.",2006.07107v3,https://arxiv.org/pdf/2006.07107v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:30:49Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Transportation/logistics|Environmental monitoring|Other,Simulation study|Case study (real dataset),TRUE,Other,Not provided,https://arxiv.org/abs/2006.10255|https://www.kaggle.com/yannisp/uber-pickups-enriched|http://archive.ics.uci.edu/ml|https://archive.ics.uci.edu/ml/datasets/bike+sharing+dataset|https://archive.ics.uci.edu/ml/datasets/Beijing+PM2.5+Data|https://archive.ics.uci.edu/ml/datasets/Metro+Interstate+Traffic+Volum|https://archive.ics.uci.edu/ml/datasets/Air+Quality,"This paper proposes a calibrated regression method that learns well-calibrated predictive distributions for regression and time-series forecasting by matching the model’s sampled predictive outputs to the empirical target distribution using Maximum Mean Discrepancy (MMD). The method uses a heteroscedastic neural network to output a mean and variance, trains first with Gaussian negative log-likelihood, then fine-tunes by minimizing an MMD loss between observed targets and samples from the predictive distribution. The authors prove an asymptotic calibration guarantee: if the predictive family can approximate the true distribution and data are i.i.d., minimizing MMD yields calibration error converging to zero as sample size grows, for both one-sided and two-sided prediction intervals across all confidence levels. Empirically, on multiple UCI and Kaggle datasets, the approach produces sharper, better-calibrated prediction intervals than MC-Dropout, deep ensembles, deep Gaussian processes, and isotonic-regression post-hoc calibration, while maintaining competitive RMSE. The contribution advances uncertainty calibration for regression by providing a distribution-level (not single-confidence-level) calibration objective that avoids retraining for different coverage levels.","A regressor is calibrated if \(\frac{1}{N}\sum_{i=1}^N \mathbb{I}\{y_i \le F_i^{-1}(p)\}\to p\) for all \(p\in[0,1]\), and similarly for two-sided intervals \([F_i^{-1}(p_1),F_i^{-1}(p_2)]\). The proposed calibration objective minimizes MMD between targets \(\{y_i\}\) and samples \(\{\hat y_i\}\) from the predictive distribution: \(L_m^2=\|\frac{1}{N}\sum_i \phi(y_i)-\frac{1}{N}\sum_j \phi(\hat y_j)\|_\mathcal{F}^2\), using a mixture of RBF kernels \(k(x,x')=\sum_{r=1}^K k_{\sigma_r}(x,x')\). The base predictive distribution is learned via Gaussian NLL for heteroscedastic NN: \(\sum_i \tfrac12 e^{-s_i}(y_i-\mu_\Theta(x_i))^2+\tfrac12 s_i\), with \(s_i=\log \sigma^2_\Theta(x_i)\), then refined by minimizing \(L_m\).","Across time-series datasets (Metro-traffic, Bike-sharing, Pickups, PM2.5, Air-quality), the proposed method reports the lowest calibration errors in Table 1; e.g., ECPE/MCPE of 0.006/0.019 on Bike-sharing and 0.010/0.035 on PM2.5, while keeping RMSE comparable to strong baselines. On standard regression datasets (Power Plant, Protein Structure, Naval Propulsion, Wine), Table 2 shows similarly small calibration errors, e.g., ECPE 0.007 (Power Plant), 0.006 (Protein), 0.012 (Naval), and 0.008 (Wine), generally improving over MC-Dropout, HNN, deep ensembles, DGP, and isotonic regression. Reliability diagrams indicate the proposed method’s observed coverage tracks expected confidence most closely across confidence levels. A runtime comparison versus DGP, deep ensembles, and ELL suggests the proposed two-stage MMD fine-tuning scales favorably as network depth increases (Figure 4), while maintaining low ECPE.","The authors note the Gaussian likelihood used for the heteroscedastic model can be restrictive and may not fit complex/noisy target distributions. They also mention challenges extending the approach to classification due to sensitivity of MMD to batch size and binning. Finally, the theory is asymptotic; they highlight that practical settings involve finite data and suggest further work on finite-sample bounds and required sample size.","The asymptotic calibration guarantee assumes the predictive family can approximate the true conditional distribution and that data are i.i.d.; both can fail in time-series forecasting due to temporal dependence and dataset shift. The approach requires sampling from the predictive distribution and computing kernel-based MMD, which can be sensitive to kernel/bandwidth choices and may be costly for large batches or high-dimensional outputs without approximations. Calibration is evaluated primarily via marginal coverage of prediction intervals; it does not directly assess conditional calibration (calibration within subpopulations or covariate regions) or robustness under heavy-tailed/heterogeneous noise beyond the Gaussian base model.","The authors propose replacing the Gaussian likelihood with richer distributions (e.g., mixture distributions or mixture density networks) as base predictors. They suggest extending the calibration strategy to classification, while addressing batch-size and binning effects on MMD. They also propose using kernels defined on structured data (e.g., graphs and time-series) and investigating finite-sample behavior, including sample-size requirements and tighter bounds leveraging known finite-sample MMD convergence results.","Develop self-tuning or adaptive kernel selection/bandwidth strategies (or learnable kernels) to reduce sensitivity and improve scalability, potentially using random Fourier features for faster MMD optimization. Extend evaluation to conditional coverage (e.g., stratified calibration, conformal-style diagnostics) and stress-test under distribution shift, autocorrelation, and heteroscedastic/heavy-tailed noise. Provide an implementation package and reproducibility artifacts (code, configs) and compare against modern conformal prediction baselines that also deliver any-confidence-level intervals with finite-sample guarantees.",2006.10255v2,https://arxiv.org/pdf/2006.10255v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:31:33Z
FALSE,NA,Other,Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,Other,Not provided,https://www.bnlearn.com/bnrepository/|https://github.com/ericjang/gumbel-softmax|https://github.com/ctgk/PRML/blob/master/prml/rv/variational_gaussian_mixture.py|https://archive.ics.uci.edu/ml/datasets/Tarvel+Review+Ratings|https://archive.ics.uci.edu/ml/datasets/Travel+Reviews,"This paper proposes Mixture of Discrete Normalizing Flows (MDNF), a differentiable reparameterization for categorical variational inference that defines a proper discrete probability mass function and enables direct ELBO optimization. The method represents a categorical distribution as a mixture of discrete normalizing flow components, allowing unbiased estimation of both expected log joint terms and entropy via tractable evaluation of log q(x). The authors introduce training approaches including joint training with fixed mixture weights (VIF) and a boosting-style sequential component training method (BVIF), and also propose “partial flows” as an extension with improved permutation expressivity. Empirically, MDNF is shown via simulation/experiments to be less sensitive to temperature hyperparameters than Gumbel-Softmax relaxations and to perform better or more reliably on Bayesian networks and discrete-latent VAEs. Experiments also explore the effect of the base distribution (delta vs Dirichlet-drawn) and demonstrate MDNF’s robustness and low-variance ELBO estimation in tested settings.","MDNF models a categorical distribution as a mixture: $p(x)=\sum_{b=1}^B \pi_b\,p_b(x)$. With discrete flows, each component probability is evaluated by inverse-mapping $x$ through the $b$th flow and scoring under its base distribution: $p_x(x)=\sum_{b=1}^B \pi_b\,p_u^b(\mathrm{inv}f^b_{\lambda}(x))$ (Eq. 5). A commonly used discrete flow transformation is a modular location-scale map $x=(\mu_\lambda+\sigma_\lambda\cdot u)\bmod K$ (Eq. 3), and with delta bases the authors often use the shift-only case ($\sigma\!=\!1$).","On discrete Bayesian networks (e.g., Asia, Sachs, Earthquake, Cancer), MDNF shows broad regions of good performance across temperatures (e.g., $\tau\gtrsim 1$), whereas Gumbel-Softmax variants require narrow, task-specific choices of $(\tau,\tau_p)$ to perform well. In VAE experiments on MNIST across varying latent dimensionality/cardinality, MDNF is the only method reported as consistently competitive across all tested configurations, while different Gumbel/heuristic objectives excel only in subsets of settings. For MDNF learning algorithms, BVIF improves monotonically with the number of mixture components $B$ and can outperform VIF on larger networks (e.g., Hepar II), while a boosting baseline without training flows (BVI) performs poorly. The supplement reports very low relative standard deviation (<0.35%) of the MDNF ELBO estimator even with $S=1$ Monte Carlo sample in tested VAE settings.","The paper notes that ordinary discrete normalizing flows (DNF) are expressively limited unless paired with strong base distributions, motivating the mixture construction; it also notes that the specific location-scale discrete flow in Eq. (3) cannot realize all permutations without stacking and lacks guarantees, motivating partial flows. For one larger Bayesian network (Hepar II), the true posterior KL cannot be computed by enumeration; additionally, entropy is approximated when handling varying categorical cardinalities in fixed-size tensor implementations.","The work targets variational inference for categorical latent variables in ML models rather than engineering reliability; there is no linkage to failure-time/degradation data, maintenance decisions, or system reliability metrics. Empirical evaluation focuses on selected Bayesian networks, MNIST VAEs, and a GMM example; broader validation on diverse real-world discrete inference problems and sensitivity to model misspecification (e.g., strong prior mismatch) is not extensively characterized beyond the showcased cases. Practical reproducibility is hampered because, despite stating that code/data are in the supplement, the main text does not provide a direct public code repository for the MDNF implementation and thus portability/implementation details may be incomplete for practitioners.","The paper suggests MDNF could be used beyond variational inference, including generative modeling tasks, and has potential to improve upon standard discrete normalizing flows. It also notes that BVIF could be extended to automatically select the number of mixture components $B$ (e.g., via priors such as stick-breaking) and that advances in boosting VI could be incorporated to improve the boosting-based training procedure.","A useful extension would be to provide self-contained, publicly released reference implementations (e.g., PyTorch/TensorFlow) and standardized benchmarks to facilitate adoption and fair comparison across discrete VI methods. Additional work could study robustness under autocorrelated or structured discrete latents (e.g., sequences/graphs) and under tighter computational budgets (small $B$, small sample size $S$), including rigorous convergence/identifiability analysis for the joint-training (VIF) mixture symmetry issues. Extending MDNF to hierarchical/discrete-continuous hybrid models with strong priors and demonstrating performance on more real-world tasks with discrete structure (beyond MNIST and small BNs) would clarify practical generalizability.",2006.15568v2,https://arxiv.org/pdf/2006.15568v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:32:13Z
TRUE,RUL prediction|Degradation modeling|Maintenance optimization,ML-based|Other,Sensor/condition monitoring|Right-censored,Condition-based|Predictive,Transportation/logistics|Energy/utilities|Manufacturing (general),Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan,"The paper proposes GNMR, a gated graph neural network (GGNN) architecture for remaining useful life (RUL) estimation that explicitly leverages known equipment/module structure by representing the machine as a directed graph of modules (nodes) with dependency edges. Sensor time series are first grouped by module, encoded into node embeddings using GRU-based sequence models, and then refined via GGNN message passing over the equipment graph; a simple attention mechanism aggregates node-level outputs into a final RUL prediction. The method is evaluated on NASA’s C-MAPSS turbofan benchmark datasets (FD001–FD004) using RMSE and the PHM’08 timeliness score, showing improved or competitive performance against strong deep learning baselines (CNN, LSTM, DBN ensembles) and a GRU metric-regression baseline. Ablations varying the graph granularity (one node for all sensors, reduced/increased nodes, one node per sensor) indicate that structure-aware sensor grouping and inter-module message passing contribute to better accuracy, while overly collapsing the graph degrades performance. The attention analysis suggests the model can assign higher weight to the module with an impending fault (notably the HPC module in FD002), offering potential for actionable diagnosis in condition monitoring.","GNMR computes node embeddings by mapping each module’s multivariate time series to an initial vector $v_j^0\in\mathbb{R}^d$, then applies GGNN message passing for $\tau$ steps using learned edge-wise message functions $f_{ij}(\cdot)$ and GRU-style gating updates (Eqs. 1–7) to obtain $v_j^\tau$. A graph-level RUL is produced with attention: weights $w_j=\mathrm{softmax}(f_1(\tilde v_j^\tau))$ and node-wise predictions $\hat r_j=f_2(\tilde v_j^\tau)$ are combined as $\hat r=\sum_{j=1}^{|V|} w_j\hat r_j$ (Eq. 8). Training minimizes mean squared error $\frac{1}{n}\sum_i (r_i-\hat r_i)^2$.","On C-MAPSS, the proposed GNMR achieves RMSE/S of 12.14/212 (FD001), 20.85/3,196 (FD002), 13.23/370 (FD003), and 21.34/2,795 (FD004), with best average rank (RMSE rank 2.0; S rank 2.25) among compared methods. The no-message-passing variant (GNMR, $\tau=0$) is also strong (e.g., FD002 RMSE 21.38, S 3,148), indicating that module-based sensor grouping alone helps versus flat concatenation baselines such as GRU-MR. Graph-structure sensitivity shows degrading performance when collapsing modules (e.g., one node for all sensors: FD001 RMSE 13.20; FD002 RMSE 22.36), while the original 8-node graph performs best on average. Attention-weight analysis indicates increasing attention to the known faulty HPC module as RUL decreases for FD002, though the same trend is not observed for FD001.","The authors note that although graph structure is typically available as domain knowledge, it may not be optimal for capturing true dependencies among sensors within a node or across nodes. They also report that increasing node granularity increases the number of edge MLPs (due to more edges), making the approach computationally more expensive than the original graph in those settings. They further acknowledge that for FD003 and FD004 the ground-truth faulty module is not available, limiting attention-based fault-module analysis to FD001 and FD002.","Evaluation is limited to the C-MAPSS benchmark; generalization to other assets, different sampling rates, missing data, and strong temporal autocorrelation typical in industrial telemetry is not established. The approach assumes a fixed known module graph shared across all instances and does not address uncertainty/mismatch in topology, sensor-to-module assignment errors, or time-varying dependencies. Baselines are primarily deep learning models; comparisons to classical prognostics models (e.g., Wiener/Gamma degradation processes, survival models) or hybrid physics-informed approaches are not included, which may matter for interpretability and data efficiency. Code and full implementation details (exact graph, preprocessing scripts, and training protocol nuances) are not provided, potentially affecting reproducibility.","They propose exploring whether an optimal graph structure can be learned, starting from the initial domain-knowledge-based graph, because the documented structure may not best reflect dependencies between sensors within and across modules. They also suggest improving or explicitly biasing the attention mechanism toward the module-of-interest so that diagnosis and focus on the failing module is more consistently captured (motivated by differing trends between FD001 and FD002).","Develop robust/self-starting versions that handle unknown or drifting operating regimes, sensor dropout, and non-stationary/auto-correlated telemetry, which are common in online condition monitoring. Extend the method to multi-fault and multi-asset fleet settings with asset-to-asset variability (random effects/domain adaptation) and to maintenance decision support (e.g., optimal CBM/PdM scheduling using predictive uncertainty). Add calibrated uncertainty estimation (Bayesian GNNs, ensembles) so RUL outputs can be used safely for maintenance planning with risk bounds. Provide open-source reference implementations and standardized evaluation (including additional benchmarks and ablation on topology learning) to improve reproducibility and fair comparison.",2006.16556v1,https://arxiv.org/pdf/2006.16556v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:32:57Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Simulated only|Other,Not applicable,Network/cybersecurity|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper addresses interference power prediction for link adaptation in 5G URLLC, aiming to maximize spectral efficiency while meeting a target reliability/BLER constraint. It models interference power as a time series and estimates conditional and joint distributions of successive interference power values using kernel density estimation (diffusion-based bandwidth selection). Two predictors are proposed: expectation-based (EB) and maximum-quantile (MQ), where MQ selects a conditional quantile to satisfy a reliability constraint on underestimation probability. Monte Carlo simulations in a multi-cell MISO cellular setting compare the proposed methods against low-pass interference prediction and marginal-PDF-based prediction, showing MQ—especially with conditional PDFs—improves spectral efficiency for the same error probability proxy and better tracks target BLER. The work is primarily wireless communications/performance modeling rather than reliability engineering of physical systems/components.","Interference power (IPV) is defined as $I_k(t)=\sum_{\ell\neq n}|h(k,\ell)g(x_\ell,\ell)|^2P$. The conditional distribution used for prediction is $P(I(t{+}1)\mid I_D(t))=\frac{P(I(t{+}1),I_D(t))}{P(I_D(t))}$, with PDFs estimated via KDE: $f_k(i)=\frac{1}{L}\sum_{t=D}^L K(i,I_D(t),\gamma)$ using a Gaussian kernel. The MQ predictor chooses $\hat I(t{+}1)$ such that $P(\hat I(t{+}1)<I(t{+}1)\mid I_D(t))=\Gamma_0$, while EB uses $\mathbb{E}[I(t{+}1)\mid I_D(t)]$.","In simulations (9 cells, $N_a=16$, $T=3\times10^6$ IPV samples, typically $D=1$), the MQ predictor based on the conditional PDF yields about 10% higher average spectral efficiency than MQ based on the marginal PDF for the same average error proxy $\theta$ under round-robin scheduling (reported around Fig. 2). MQ methods concentrate the achieved reliability proxy around the target BLER (e.g., $\Gamma_0=10^{-3}$) and substantially outperform expectation-based and low-pass prediction baselines in the SE–reliability tradeoff (Figs. 2–5). Gains persist under proportional-fair scheduling but are smaller, consistent with weaker interference periodicity/time-correlation.",None stated.,"The evaluation is entirely simulation-based with a specific cellular/MISO/Rice-fading setup and fixed design choices (e.g., mostly $D=1$), so generalization to other traffic models, mobility, and channel/interference dynamics is not fully established. The paper proxies reliability via underestimation events of interference power (and sets SE to zero on failure), which may not accurately map to true BLER under practical receivers, HARQ behaviors, and CQI/MCS mapping uncertainties. Implementation aspects (computational cost of conditional KDE per-UE, online adaptation, memory requirements, and sensitivity to bandwidth selection for rare-event targets like $10^{-5}$) are not quantified or benchmarked. No public code or reproducibility details (software, seeds, runtime) are provided, limiting independent verification.",None stated.,"Validate the approach on measurement data or high-fidelity system-level simulators with standardized CQI-to-MCS/BLER curves and realistic control-loop delays, including mobility and bursty scheduling. Extend the conditional modeling beyond $D=1$ with model selection/regularization and study robustness under nonstationary interference (traffic changes) and correlated fading. Develop computationally efficient/online KDE or alternative conditional density estimators (e.g., Bayesian, quantile regression, or lightweight ML) suitable for real-time base-station implementation at URLLC timescales. Provide open-source implementations and a reproducible benchmark suite comparing against more LA baselines (including OLLA variants, robust MCS selection, and modern prediction models) across multiple scenarios.",2007.00306v1,https://arxiv.org/pdf/2007.00306v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:33:27Z
TRUE,System reliability|Accelerated testing|Other,Bayesian|Simulation-based|Other,Sensor/condition monitoring|Degradation measurements|Simulated only|Other,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python|https://gitlab.com/rozsasarpi/rprepo/,"The paper develops a Bayesian decision-theoretic framework for sequential (adaptive) optimal experimental design in structural reliability analysis, with the goal of estimating the failure probability $P(g(X)\le 0)$ under limited expensive model/experiment evaluations. It explicitly separates aleatory uncertainty in $X$ from epistemic uncertainty in the performance function via conditional expectations, and formulates both full dynamic programming and myopic (one-step look-ahead) design policies. Methodologically, it proposes an efficient approximation of residual uncertainty objectives using a combination of importance sampling for rare-event probability estimation and the unscented transform for propagating epistemic uncertainty through nonlinear (possibly hierarchical) reliability models (UT-MCIS). The approach is demonstrated on several numerical examples including hierarchical surrogate models, a standard “four-branch” benchmark, and an offshore corroded pipeline burst assessment with multiple experiment types and costs. Results show the myopic UT-MCIS strategy achieves competitive failure-probability estimates with relatively few expensive evaluations and provides practical stopping rules based on the epistemic coefficient of variation or target-reliability exceedance confidence.","Failure probability is defined as $\bar\alpha(g)=P(g(X)\le 0)=\mathbb{E}[\mathbf{1}(g(X)\le 0)]$ and with epistemic uncertainty as $\alpha(\xi)=\mathbb{E}[\mathbf{1}(\xi(X)\le 0)\mid\mathcal{E}]$. Three residual-uncertainty criteria are used: $H_{1,k}=\mathbb{E}[(\alpha-\bar\alpha)^2]$, $H_{2,k}=\mathbb{E}[\gamma_k(X)]$, and $H_{3,k}=(\mathbb{E}[\sqrt{\gamma_k(X)}])^2$ with $\gamma_k(x)=p_k(x)(1-p_k(x))$ and $p_k(x)=P_k(\xi(x)\le 0)$. The myopic acquisition chooses $d$ to minimize $J_{i,k}(d)=\lambda(d)\,\mathbb{E}_{k,d}[H_{i,k+1}]$, approximated via UT over epistemic sigma points and importance sampling for $\alpha(\hat\xi_k(X,e))$.","In the 1D illustrative example, the method converges after 3 sequential evaluations and estimates the “true” failure probability $\alpha(g)\approx 0.0234$ with tight posterior uncertainty. For the 7D hierarchical benchmark (RP38), it reaches convergence at iteration $k=10$ using 2 additional evaluations of $y_2$ and 8 of $z_1$, estimating the true failure probability $\alpha(g)\approx 8.1\times 10^{-3}$. On the four-branch system benchmark, UT-MCIS achieves estimates consistent with Monte Carlo ($4.416\times 10^{-3}$) using 35–65 function calls depending on the stopping threshold, with prediction intervals that contain the Monte Carlo reference. In the corroded pipeline example with multiple experiment types and costs, the decision policy reaches a target reliability conclusion using an additional stopping rule, with the criterion met around $k=25$ iterations in the illustrated run and averages of roughly 30–40 iterations over repeated runs depending on initial design.","The authors note that the myopic (one-step look-ahead) strategy is not theoretically optimal and can fail to reach desired stopping criteria in some multi-decision settings (e.g., if high-cost measurements are never selected), potentially leading to indefinite experimentation. They also state their unscented transform implementation is naive, using a low number of sigma points chosen a priori, and they observe it can overestimate variance (as known in Kalman filtering practice). They acknowledge that specifying appropriate experiment costs (penalties) is difficult and can strongly affect which experiment types are selected.","The UT-based finite-dimensional epistemic approximation can be crude (e.g., assuming strong/complete correlation in the 1D approximation of the random field), which may misrepresent epistemic structure and distort acquisition optimization in higher-dimensional or weakly correlated surrogates. The comparisons to other reliability learning strategies are limited (primarily one benchmark and a subset of methods), and sensitivity to GP hyperparameter learning, surrogate mis-specification, and non-Gaussian/noisy observation models is not systematically explored. Computational cost accounting is not fully quantified end-to-end (e.g., design-point searches, UT-MCIS tuning, global optimization of acquisition functions), which matters for practical deployment.","They suggest improving sigma-point selection and potentially optimizing UT parameters, exploring multi-step look-ahead policies as intermediate points between myopic design and full dynamic programming, and studying the trade-off between accuracy gains and computational cost as more steps are included. They propose investigating extension to buffered failure probability estimation (instead of classical failure probability) and considering alternative risk measures/objectives beyond variance that penalize underestimation more than overestimation. They also mention trying approximate solutions to the full dynamic programming problem inspired by operations research, optimal control, and reinforcement learning.","Developing robust/self-starting variants that explicitly handle unknown input distributions, dependence/autocorrelation in loads, and heavy-tailed/non-elliptical uncertainties would broaden applicability beyond settings with convenient isoprobabilistic transforms. Providing an open-source reference implementation (and standardized benchmark scripts) would materially improve reproducibility and facilitate fair comparisons with active learning reliability methods (e.g., AK-MCS variants, subset simulation-based design, and Bayesian optimization approaches). Extending the framework to system reliability with multiple failure modes and competing limit states, and to decision-dependent costs/constraints (e.g., inspection scheduling and maintenance actions), would connect the experimental design to operational reliability management.",2007.00402v1,https://arxiv.org/pdf/2007.00402v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:34:09Z
TRUE,Maintenance optimization|System reliability|Warranty analysis|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Stochastic process|Other",Right-censored|Degradation measurements|Event/count data|Mixture of types|Simulated only,Block replacement|Condition-based|Imperfect maintenance|Other,Manufacturing (general)|Theoretical/simulation only,Simulation study|Other,TRUE,R,Not provided,NA,"The paper proposes a data-driven pricing framework for full-service maintenance contracts, treating contract costs similarly to insurance claims via a frequency–severity decomposition. Failure occurrences are modeled with time-to-event (survival) models for repairable systems, including a Cox-type proportional hazards structure for minor failures with an imperfect periodic preventive-maintenance baseline intensity, and a Weibull intensity for catastrophic failures. Failure-type probabilities among multiple minor failure modes are modeled using a multinomial logit model, while preventive-maintenance and failure costs are modeled with positive right-skewed severity distributions (e.g., gamma), with a copula (e.g., Frank) suggested to handle dependence among co-occurring minor-failure costs. The resulting machine- and covariate-conditional break-even premium is computed from predicted expected counts and expected severities, enabling differentiated tariffs by machine profile. A simulation engine generates heterogeneous, right-censored event-and-cost timelines; experiments compare tariff plans with/without covariates and show differentiated pricing improves risk classification and mitigates adverse selection using tools such as quantile plots and (ordered) Lorenz curves with Gini indices.","Total contract cost is decomposed as $C(\Delta t)=F(\Delta t)+M(\Delta t)$ with aggregated failure cost $F(\Delta t)=\sum_{i=1}^{n_f}\sum_{k=1}^{N_{f,i}(\Delta t)} X_{i,k}$ and maintenance cost $M(\Delta t)=\sum_{j=1}^{n_m(\Delta t)} S_j$. The break-even (technical) premium is $P^*(\Delta t)=\mathbb{E}[C(\Delta t)]=\sum_i \mathbb{E}[N_{f,i}(\Delta t)]\,\mathbb{E}[X_{i,k}] + n_m(\Delta t)\,\mathbb{E}[S_j]$, and conditionally $P^*(\Delta t\mid x_1,x_2(t))=\sum_i \mathbb{E}[N_{f,i}(\Delta t)\mid x]\,\mathbb{E}[X_{i,k}\mid x]+n_m(\Delta t)\,\mathbb{E}[S_j\mid x]$. Minor-failure intensity uses a proportional hazards form $\lambda_{fm}(t)=\lambda_0(t)\exp(\beta_1^\top\chi_1+\beta_2^\top\chi_2(t))$ with an imperfect-periodic-maintenance baseline $\lambda_0(t)=\alpha_{\lambda_0}((t\bmod\Delta t_{PM})+(1-\kappa_{\lambda_0})\lfloor t/\Delta t_{PM}\rfloor)+\gamma_{\lambda_0}$, while catastrophic failures follow a Weibull intensity $\lambda_{fc}(t)=\kappa_c\alpha_c^{\kappa_c} t^{\kappa_c-1}$.","Using a simulated portfolio (e.g., $n=5000$ machines) the out-of-time loss ratios over a 2-year horizon are close to break-even for all three tariff plans: 0.992 (no covariates), 0.989 (fixed covariates), and 0.994 (fixed + time-varying covariates). Differentiation capability improves strongly when covariates are used: Lorenz-curve Gini indices rise from 0.033 (no covariates) to 0.128 (fixed covariates) and 0.129 (fixed + time-varying). Quantile plots show that covariate-based tariffs align average price with average cost across bins, whereas the undifferentiated tariff overprices low-risk contracts and underprices high-risk contracts, indicating vulnerability to adverse selection. Ordered Lorenz curve comparisons indicate tariff plans b and c dominate tariff a, while the incremental gain of including the time-varying covariate over fixed covariates alone is small (e.g., ordered-Lorenz Gini 0.014 when comparing b vs c in one direction).","The authors assume statistical independence between failure occurrence (frequency) and associated costs (severity), noting that modeling dependence is more involved. They demonstrate the methodology on simulated data and state that validation on a real dataset is a logical next step. They also assume most heterogeneity is captured by observable covariates; if not, the model would need extensions such as frailties/random effects.","The time-to-failure component relies on a specific parametric baseline intensity form tied to periodic preventive maintenance and a Weibull model for catastrophic failures; misspecification could materially affect predicted counts and therefore premiums. The approach appears to assume correct specification and stable covariate effects over time; robustness to nonproportional hazards, temporal drift, or unmodeled dependence between minor and catastrophic processes is not fully explored. The evaluation uses one simulated data-generating mechanism that matches the fitted model class, so performance may be optimistic relative to real operational data with reporting delays, missingness, and nonstationary maintenance practices.","They propose (i) relaxing the frequency–severity independence assumption, for example by using copulas to model dependence between occurrence and costs; (ii) validating the approach on real maintenance/contract datasets; and (iii) incorporating unobserved heterogeneity via frailties/random effects when observable covariates do not explain all variability.","Develop self-starting/online recalibration and drift-detection procedures so tariffs adapt as fleets, maintenance practices, and environments change over time. Extend the framework to explicitly optimize the preventive-maintenance schedule jointly with pricing (integrated maintenance optimization + contract pricing) under customer behavior and competition constraints. Provide open-source implementation and benchmarking on multiple real datasets (including different industries) and assess robustness under model misspecification, dependence between minor/catastrophic events, and informative censoring (e.g., contract termination related to failure experience).",2007.01171v1,https://arxiv.org/pdf/2007.01171v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:34:59Z
TRUE,Maintenance optimization|System reliability|Degradation modeling|Other,Stochastic process|ML-based|Hybrid/Ensemble|Other,Simulated only|Other,Condition-based|Predictive|Opportunistic|Group replacement|Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"The paper develops a constrained inspection-and-maintenance planning framework for multi-component deteriorating systems under partial observability and resource/risk constraints. The decision problem is formulated as a constrained POMDP and solved approximately using a decentralized multi-agent deep reinforcement learning actor–critic method (DDMAC) operating on belief states (Bayesian-updated posterior state distributions). Hard constraints (e.g., budget caps over multi-year cycles) are handled by state augmentation, while soft constraints (e.g., life-cycle risk/probability-of-failure limits) are enforced via Lagrangian relaxation with dual variables. A broad life-cycle risk formulation is presented that includes both instantaneous and perpetual loss components and is shown to reduce to classic reliability-based failure probability risk under special cases. Numerical experiments on a simulated 10-component networked system (50-year horizon) show DDMAC outperforms several optimized baseline policies (e.g., periodic/condition-based and risk-based inspection policies with prioritization), and illustrate how tighter budget constraints reduce inspections while stricter risk constraints increase maintenance intensity and induce more coordinated/opportunistic interventions.","The optimization minimizes expected discounted life-cycle cost over a horizon: $\pi^*=\arg\min_{\pi\in\Pi_c}\mathbb{E}_\pi\big[\sum_{t=0}^{T-1}\gamma^t c(s_t,a_t,s_{t+1})\big]$ (Eq. 1), with actions chosen based on belief $b_t=P(s_t|\text{history})$. Belief is updated by Bayes filtering: $b'(s')=\frac{P(o'|s',a)\sum_s P(s'|s,a)b(s)}{P(o'|b,a)}$ (Eq. 4–5), yielding a belief-MDP Bellman equation $V(b)=\min_a\{c_b(b,a)+\gamma\sum_{o'}P(o'|b,a)V(b')\}$ (Eq. 8). Constraints are posed as discounted cumulative auxiliary costs bounded by thresholds (Eq. 23), implemented via state augmentation for hard constraints and a Lagrangian objective with multipliers $\lambda$ for soft (risk) constraints (Eq. 24).","On the unconstrained simulated example, the best optimized baseline policy (risk-based inspections + condition-based maintenance + component prioritization) has a total life-cycle cost about 42% higher (worse) than the learned DDMAC policy (Fig. 5). For 5-year budget caps, solutions with caps above roughly 25% of system rebuild cost converge to the unconstrained performance, while tighter budgets cause reduced inspection spending and disproportionate increases in life-cycle risk (Fig. 6). For life-cycle risk constraints, tighter risk tolerance increases maintenance and shutdown costs; solutions converge to the unconstrained policy beyond about $2.75\,c_{reb}$ risk tolerance (Fig. 7). The results show distinct behavioral shifts: low budgets suppress inspections to preserve funds for major interventions, while risk-averse settings drive more frequent maintenance and more coordinated/opportunistic actions to limit disruption (Figs. 8–12).","Model uncertainty is not considered in the numerical example: deterioration transition parameters for the hidden Markov models are assumed known or already learned. The study is demonstrated on a simulated environment (10 components, specified transition/observation models), so conclusions are based on numerical experiments rather than field validation. The paper notes that very stringent risk constraints (below about $2.0\,c_{reb}$) can lead to policies that become practically unrealistic due to very frequent restorations/replacements.","Performance is evaluated on a single stylized networked system with independent component deterioration and a specific observation model; robustness to model misspecification, correlated deterioration, or different network topologies is not established. The approach assumes known transition/observation probabilities (POMDP model known) and does not quantify sensitivity to errors in these models or to nonstationary/biased inspection data beyond the prescribed matrices. The work reports relative cost improvements but does not provide statistical confidence intervals over training randomness or a comprehensive ablation (e.g., impact of decentralization choices, replay size, network architecture) to isolate which components of DDMAC drive gains. Practical deployment concerns—computational cost at scale, safety during exploration in real systems, interpretability of learned policies, and integration with regulatory inspection schedules—are not fully addressed.",None stated.,"Extend the framework to account for model/parameter uncertainty (Bayesian or robust POMDP/RL) and study sensitivity to mis-specified deterioration and observation models. Evaluate on real condition-monitoring/inspection datasets and larger, more realistic infrastructure networks with correlated deterioration, shared crews, and explicit scheduling/logistics constraints. Develop self-starting/online-learning variants that update deterioration/observation models during operation and include safe exploration mechanisms suitable for critical infrastructure. Provide open-source implementations and benchmark comparisons against additional constrained RL and approximate POMDP solvers, including computational scaling analyses and interpretability/diagnostics for practitioner adoption.",2007.01380v1,https://arxiv.org/pdf/2007.01380v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:35:44Z
FALSE,Other,"ML-based|Parametric (Weibull, etc.)|Other",Mixture of types|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/m1m0r1/lh_calib,"The paper studies reliability of machine-learning predictive probabilities in medical diagnosis settings with label uncertainty arising from inter-rater disagreement, observed as multiple annotations (label histograms). It proposes an evaluation framework that extends proper scoring rule decompositions to derive unbiased/debiased estimators for expected squared loss, epistemic loss, and calibration loss when ground truth labels are uncertain. The framework is generalized to higher-order label statistics (e.g., pairwise disagreement) and provides evaluation of disagreement probability estimates (DPEs) including calibration. The authors propose a post-hoc method, b1-calibration, modeling a distribution over class probability estimates via a Dirichlet with instance-wise concentration b10(X), trained by Dirichlet-multinomial NLL on label histograms. Experiments on synthetic datasets and a large-scale myelodysplastic syndrome (MDS) medical imaging dataset show improved calibration and squared-loss performance for DPEs and improved posterior class probability estimates when incorporating an additional expert label.","b1-calibration models a predictive distribution over class probabilities as $P(\zeta\mid X)=\mathrm{Dir}(\zeta\mid \alpha_0(X) f(X))$, where $f(X)$ is the base classifier softmax output and \alpha_0(X)>0 is learned post-hoc from label histograms by minimizing Dirichlet-multinomial NLL. The disagreement probability estimate is $\varphi^D(X)=\int (1-\sum_k \zeta_k^2)\, dP(\zeta\mid X)$, which under the Dirichlet model yields the closed form $\varphi^D=\frac{\alpha_0}{\alpha_0+1}\left(1-\sum_k f_k^2\right)$. The posterior updated CPE after observing an expert label $Y$ becomes $Z'(X,Y)=\mathbb{E}[\zeta\mid X,Y]=\frac{\alpha_0 f + Y}{\alpha_0+1}$.","Across synthetic (Mix-MNIST, Mix-CIFAR-10) and real MDS data, b1-calibration consistently reduced both squared loss and calibration error for disagreement probability estimates (DPEs) relative to Raw, temperature scaling, MC-dropout, and test-time augmentation alone (e.g., on MDS, Raw CEcc6D 0.0628 b2c6D 0.0406 with Raw+b1, and to 0.0261 with Raw+ts+b1). For posterior CPEs (updating with one expert label at test time), b1-calibration substantially reduced epistemic loss compared to the prior CPEs (e.g., MDS: Raw+b1 prior ELc 0.0435 vs posterior ELc 0.0354; Mix-CIFAR-10(2): 0.2504 to 0.0709). The paper also demonstrates that debiased estimators for EL and CL computed from label histograms are markedly closer to ground truth than naive plug-in estimators in controlled synthetic settings (where the true EL and CL are zero).","The authors note that b1-calibration relies on a conditional i.i.d. assumption for label generation given input (experts randomly assigned per example); if labels are highly correlated (e.g., two fixed experts with different policies label all examples), the model can be unsuitable. They also mention that combining b1-calibration with ensemble-based methods for posterior computation is omitted because it would require additional approximation to compute posteriors.","The work is not reliability engineering in the traditional sense (failure/lifetime/maintenance), but rather ML probability calibration; mapping its claims to engineering reliability metrics (e.g., failure probabilities over time) is nontrivial. The Dirichlet family may be too restrictive to capture complex, multimodal uncertainty over class probabilities induced by heterogeneous raters or systematic ambiguities, even if b10(X) is instance-wise. Evaluation focuses on specific datasets and a fixed set of baselines; more recent calibration approaches (e.g., modern neural calibration layers or conformal prediction for uncertainty sets) are not comprehensively benchmarked under label-histogram supervision.",None stated.,"Extend b1-calibration beyond a single-parameter concentration to richer distribution families (e.g., logistic-normal or mixture Dirichlet) to capture multimodal or rater-subpopulation effects while still enabling tractable training with label histograms. Relax the conditional i.i.d. label assumption by incorporating explicit rater models (annotator-specific confusion or correlation) so disagreement probabilities can exceed the constraints implied by the current Dirichlet formulation. Provide robust/self-starting implementations and broader empirical validation on additional medical domains and tasks (including segmentation) with standardized benchmarks and publicly released evaluation scripts.",2007.01659v4,https://arxiv.org/pdf/2007.01659v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:36:26Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,ML-based|Hybrid/Ensemble|Other,Sensor/condition monitoring|Degradation measurements|Mixture of types,Predictive|Condition-based,Transportation/logistics|Manufacturing (general)|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://www.reliableplant.com/Read/212/world-class-maintenance|https://github.com/hill-a/stable-baselines,"The paper proposes a model-free deep reinforcement learning framework for predictive maintenance in edge-based equipment sensor networks, aiming to maximize equipment uptime under a maintenance budget constraint. The maintenance decision problem is formulated as a fully observable Markov Decision Process with actions {Hold, Repair, Replace}, where states are derived from discretized sensor readings and a simplified operating-temperature mode. The solution uses value-based deep RL (Double DQN) enhanced with Prioritized Experience Replay and Parameter Noise (PDDQN-PN) to address sparse-reward exploration and stabilize learning. Experiments on the NASA C-MAPSS turbofan degradation datasets (FD001 and FD003) show the proposed enhancements substantially improve learning efficiency over vanilla DDQN and random policies and yield consistent replacement recommendations based on an inferred health indicator. The work emphasizes actionable maintenance recommendations (replacement points) rather than only accurate RUL prediction.","The objective is maximum equipment uptime $\rho=\sum_{n=1}^{N} \mathrm{RunTime}$. States are formed from discretized sensor measurements $q_x\leftarrow \mathrm{Discretize}(x_t^i)$ and combined with a binary temperature mode $S^\tau\in\{0,1\}$ (e.g., $S^\tau=0$ if $\tau\in[25,60]$, else $1$), yielding $S=S^i\times S^\tau$. Learning is based on DDQN targets $y_t^{\mathrm{DoubleDQN}}=r_{t+1}+\gamma Q(s_{t+1},a^*;\theta_t^-)$ with $a^*=\arg\max_A Q(s_{t+1},A;\theta_t)$, plus PER sampling probabilities $P(i)=\frac{p_i^{\alpha}}{\sum_k p_k^{\alpha}}$ and importance weights $(\frac{1}{N}\cdot\frac{1}{P(i)})^b$.","On C-MAPSS FD001 (illustrated on engine #76), adding Prioritized Experience Replay enables the agent to learn an effective policy within about $1.2\times 10^4$ time-steps with mean cumulative reward around 95, whereas vanilla DDQN performs marginally worse than a random policy under the sparse-reward setting. Stacking Parameter Noise on top of DDQN-PER yields about a 50% improvement in learning efficiency, with about a 10% performance loss in the reported mean score. For replacement-point prediction (engine #50 example), DDQN-based variants report a median predicted value of 0.170 with standard deviations 0.011 (DDQN-PER), 0.013 (PDDQN-PN), while a random policy yields median 0.897. Cross-validation replacement medians are reported as $0.175\pm 0.02$ (FD001) and $0.174\pm 0.02$ (FD003).","The authors state that there is an absence of ground truth for equipment health values (and for repair/replacement labels), so they only test the replacement action and assume a medium-criticality equipment setting. They also note that results may converge differently with longer simulation time-steps and reduced exploration, implying sensitivity to training duration and exploration scheduling.","The MDP state construction relies on discretization and a highly simplified binary temperature mode, which may not capture real multi-regime operating conditions, sensor drift, and autocorrelation common in industrial time series. The reward function uses several arbitrarily defined reward constants (e.g., exploration and penalty terms), so learned policies may be sensitive to reward shaping and may not transfer without careful retuning. Evaluation is limited to a single public simulated dataset (C-MAPSS) and does not demonstrate deployment constraints typical of edge devices (latency, memory, compute, communication faults) or maintenance operational constraints (lead times, imperfect repair, spares logistics). Comparisons are primarily against random and vanilla DDQN variants rather than against classical maintenance optimization baselines (e.g., threshold/CBM policies, POMDP/DP baselines) or modern prognostics-driven decision policies.",The authors propose extending the approach to other equipment failure datasets and benchmarking against an actual equipment maintenance policy schedule.,"A valuable extension would be to incorporate partial observability (POMDP) and model sensor noise, missingness, and temporal dependence explicitly (e.g., via recurrent networks) to better match real condition-monitoring data. The approach could be expanded to include imperfect repairs, lead times, and multi-asset scheduling/resource constraints to produce implementable shop-floor maintenance plans. Robustness studies on reward-shaping sensitivity and uncertainty-aware policies (e.g., distributional RL or Bayesian RL) would help quantify risk and confidence in recommended replacement points. Providing an open-source reference implementation and edge deployment profiling would improve reproducibility and practical adoption.",2007.03313v1,https://arxiv.org/pdf/2007.03313v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:37:03Z
FALSE,NA,Other,Simulated only|Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,Julia,Public repository (GitHub/GitLab),https://github.com/jwmi/BayesianMixtures.jl,"This paper studies Bayesian finite mixture models (FMMs) with a prior on the (finite) number of mixture components and asks whether the posterior can reliably learn the true component count. The authors prove that under even arbitrarily small likelihood misspecification—when the true data-generating distribution is not exactly representable as a finite mixture of the assumed component family—the posterior probability of any fixed finite number of components converges to 0 as sample size grows (posterior component-count divergence). They provide sufficient conditions that are weaker and more checkable than many existing parameter-consistency conditions, requiring KL-support of the prior around the true distribution and mild regularity of the component family (continuity, mixture identifiability, and “degenerate limits”). The theory covers common component families including multivariate Gaussians with unknown mean and covariance, and extensions address priors that vary with the dataset size and priors with an upper bound on component count. Simulations and gene-expression case studies illustrate that inferred component counts increase with more data under misspecification, making component-count inference unstable across sample sizes.","The model represents a finite mixture density as $f(x)=\int \psi_\theta(x)\,dg(\theta)=\sum_{j=1}^k p_j\,\psi_{\theta_j}(x)$ with $g=\sum_{j=1}^k p_j\,\delta_{\theta_j}$. The Bayesian posterior over mixing measures is $\Pi(A\mid X_{1:N})=\frac{\int_A \prod_{n=1}^N \frac{df}{d\mu}(X_n)\,d\Pi(g)}{\int_G \prod_{n=1}^N \frac{df}{d\mu}(X_n)\,d\Pi(g)}$, inducing a posterior on the component count $k$ via the number of atoms in $g$. The main asymptotic claim is $\Pi(k\mid X_{1:N})\to 0$ almost surely (or in probability for varying priors) for every fixed finite $k$ under misspecification.","Main theorem: if data are i.i.d. from $f_0$ that is not a finite mixture of the assumed component family $\Psi$, but $f_0$ lies in the KL-support of the prior and $\Psi$ is continuous, mixture-identifiable, and has degenerate limits, then for every finite $k\in\mathbb{N}$, $\Pi(k\mid X_{1:N})\to 0$ $f_0$-a.s. The paper also shows an analogous divergence result for priors $\Pi_N$ that vary with $N$, yielding $\Pi(k\mid X_{1:N})\to 0$ in $f_0$-probability under a uniform-in-$N$ KL-support condition. Empirically (simulations and gene-expression datasets), Gaussian-mixture posteriors concentrate near correct component counts when well-specified but shift toward larger $k$ as $N$ increases under misspecification (e.g., Laplace-generated data or $\epsilon$-contamination), consistent with the divergence theory.","The authors emphasize their results are asymptotic and note that for finite samples the component-count posterior may still be practically useful if analysts account for sensitivity to dataset size. They also caution that their experimental mixture model setup is used to illustrate divergence behavior and “should not be interpreted as a carefully-specified model” for the empirical data examples. For priors with a finite upper bound on components, they do not provide general KL-type conditions and instead assume weak posterior concentration directly, noting that verifying such conditions can be challenging in misspecified settings.","The work is not a reliability-engineering study and does not connect the divergence phenomenon to reliability tasks (e.g., failure-time mixtures, competing risks) where practical decision impacts could be quantified. Empirical demonstrations rely on a particular inference scheme (split-merge collapsed Gibbs sampling) and selected hyperparameter settings; additional robustness checks across alternative samplers/priors and diagnostics could clarify finite-sample sensitivity beyond the asymptotic statement. The theoretical conditions (e.g., KL-support and degenerate limits) may still be nontrivial to verify for practitioners in complex, constrained, or discrete-data mixture models (e.g., count likelihoods) that are common in applied domains.","They suggest better understanding the connections between their divergence results and robust Bayesian inference methods designed to mitigate likelihood misspecification. They also note that for finite mixture models with nonparametric (mixture) component densities, the posterior behavior of the number of components and its asymptotics “have yet to be characterized,” pointing to a need for theory in these more robust mixture constructions.","Developing practical diagnostic tools to detect impending component-count divergence (e.g., monitoring posterior $p(k\mid N)$ stability across subsamples) would help practitioners decide when component-number inference is unreliable. Extending results to dependent data (time series), censored/weighted observations, or model classes commonly used in engineering mixture contexts could broaden applicability. Providing software that reproduces theoretical checks (e.g., verifying degenerate limits/identifiability for common families) and benchmarks across multiple inference algorithms would improve practical uptake and reproducibility.",2007.04470v3,https://arxiv.org/pdf/2007.04470v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:37:45Z
FALSE,Other,ML-based|Bayesian,Other,Not applicable,Network/cybersecurity|Other,Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/KristianMiok/BAN|https://github.com/KristianMiok/Bayesian-BERT,"This paper proposes Bayesian Attention Networks (BAN) and an MCD-BERT variant for hate speech detection that provide prediction reliability/uncertainty estimates via Monte Carlo dropout applied within transformer attention layers and kept active at test time. The approach produces distributions over predictions, using the mean as the predicted probability and the variance/spread as an uncertainty indicator, and evaluates calibration using expected calibration error (ECE) along with Platt scaling and isotonic regression. Experiments on English, Croatian, and Slovene hate-speech datasets show MCD-BERT achieves the best classification performance among compared baselines (including BERT and MCD-LSTM) and yields better identification of uncertain/borderline instances than standard BERT. The authors also test adding affective dimensions from SenticNet (Hourglass of Emotions variants) to MCD-BERT outputs via an SVM, finding no performance improvement but some interpretive insights. Visualization techniques (histograms and UMAP embeddings of prediction distributions) are used to analyze uncertainty patterns across instances.","The attention head is computed as $o_h = \mathrm{softmax}(QK^T/\sqrt{d_k})\,V$. Calibration is defined by $P(\hat{Y}=Y\mid \hat{P}=\hat{p})=p$. Expected Calibration Error is computed as $\mathrm{ECE}=\sum_{m=1}^M \frac{|B_m|}{n}\,|\mathrm{accuracy}(B_m)-\mathrm{score}(B_m)|$, with predictions binned by confidence.","Calibration: for BAN, Platt scaling reduces ECE substantially (e.g., English ECE drops from 0.547 raw to 0.225 with Platt; Croatian 0.681 to 0.198; Slovene 0.794 to 0.206). Predictive performance: MCD-BERT is best across languages (Accuracy/F1: English 91.4/90.4; Croatian 71.5/62.9; Slovene 68.4/68.6) and slightly improves over standard BERT (English 90.9/90.0; Croatian 70.8/61.2; Slovene 66.4/67.8). Reliability detection: splitting instances into certain/uncertain shows much higher incorrect-to-correct ratios in the uncertain group, and MCD-BERT’s uncertainty separation is statistically significant across datasets (reported chi-square p-values as low as $2.2\times10^{-16}$) and stronger than BERT, especially on Croatian. Adding SenticNet affective features to MCD-BERT outputs via SVM does not change performance on English (Accuracy remains 91.4; F1 90.5 vs 90.4).","A stated limitation is that MCD-BERT uses BERT’s default 10% dropout rate from training, while other dropout probabilities might be more suitable for reliability estimation; the authors explicitly leave analysis of alternative dropout rates for future work. They also note that exploring many BERT architectural variants and dropout settings would require substantial compute due to the cost of pretraining, so they restrict experiments to (i) smaller attention networks trained from scratch and (ii) fine-tuning pretrained BERT.","The work targets prediction uncertainty in NLP classification rather than reliability engineering in the classical sense; uncertainty is evaluated mainly via calibration (ECE) and heuristic variance thresholds rather than decision-theoretic costs of false positives/negatives for moderation. Uncertainty estimation relies on MC-dropout assumptions and uses a fixed variance cutoff (e.g., 0.1) and 1000 samples; sensitivity to the cutoff, number of MC samples, and dataset shift (domain drift/adversarial text) is not comprehensively analyzed. Comparisons do not include several strong alternative uncertainty methods for transformers (e.g., deep ensembles, temperature scaling for calibration, post-hoc Dirichlet calibration, conformal prediction) in a unified benchmark. The BAN-from-scratch comparison may be disadvantaged by lack of pretraining, making it harder to separate the value of the Bayesian mechanism from representation quality.","They propose adapting other Bayesian approaches such as SWAG to transformer networks. They suggest applying reliability-enhanced text classification to other domains (e.g., machine translation) and using reliability scores in semi-supervised learning to add only the most reliably classified instances. They also highlight data re-annotation and moderation workflows as practical areas where reliability scores can reduce labeling effort by focusing on borderline cases, and suggest integrating uncertainty estimation into full-sentence SenticNet 6 models for emotion-aware architectures.","Evaluate robustness of uncertainty under dataset shift (new platforms, evolving slang) and adversarial perturbations, since moderation systems face non-stationary inputs. Provide a principled human-in-the-loop policy that uses uncertainty to optimize moderator workload and error costs (e.g., abstain/triage thresholds optimized for precision constraints). Extend to multi-class or hierarchical hate/offensive taxonomies and to multilingual transfer with uncertainty-aware calibration per language. Release a reproducible pipeline with fixed seeds and standardized uncertainty metrics (risk-coverage curves, selective classification, Brier score) to facilitate fair comparison across uncertainty methods.",2007.05304v7,https://arxiv.org/pdf/2007.05304v7.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:38:22Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,R,Not provided,NA,"This paper studies the reliability (stability) of binary pass/fail decisions made from test items, treating the decision rule as a Boolean function rather than using a latent-variable psychometric model. Measurement error is modeled by independently flipping item responses with probability \(\tfrac{1}{2}(1-\rho)\), linking \(\rho\) to classical-test-theory reliability and defining “decision reliability” as \(\mathrm{cor}(f(X),f(Y))\) and “stability” as \(\mathbb{E}[f(X)f(Y)]\). Using Fourier analysis of Boolean functions, the authors derive expressions for item influence and decision reliability/stability in terms of (biased) Fourier coefficients, emphasizing that low-order coefficients drive robustness to noise. To handle item dependence and non-uniform item marginals, they embed the approach in an Ising graphical model and use a pseudo-likelihood factorization based on nodewise conditionals. They argue that (weighted) linear-threshold (sum-score) rules have desirable properties (monotonicity, stability, and alignment with input), and illustrate estimation via lasso-fitted Ising models with simulations and an R-based example.","A decision rule is treated as a Boolean function \(f:\{-1,1\}^n\to\{-1,1\}\), with linear-threshold form \(f(x)=\mathrm{sgn}(a_0+\sum_{i=1}^n a_i x_i)\). Measurement error is modeled as \(Y_i=x_i\) w.p. \(\tfrac{1}{2}(1+\rho)\) and \(Y_i=-x_i\) w.p. \(\tfrac{1}{2}(1-\rho)\), leading to decision stability \(S_\rho(f)=\mathbb{E}[f(X)f(Y)]\) and decision reliability \(\mathrm{cor}(f(X),f(Y))\). Item influence is \(I_i(f)=\Pr(f(X)\neq f(X^{(i)}))\), and under biased Fourier expansion \(f(x)=\sum_{S\subseteq V}\hat f_\pi(S)\,\varphi_S(x)\), for monotone \(f\), \(I_i(f)=\hat f_\pi(i)/\sigma_i\); covariance under noise has form \(\mathrm{cov}(f(X),f(Y))=\sum_{S\neq\emptyset}\omega(S)\rho^{|S|}\hat f_\pi(S)\hat f_{\pi,\rho}(S)\).","The paper derives closed-form Fourier-coefficient representations for (i) item influence, (ii) decision stability \(S_\rho(f)=\mathbb{E}[f(X)f(Y)]\), and (iii) decision reliability \(\mathrm{cor}(f(X),f(Y))\) under the flip-noise model, showing higher-order Fourier terms are downweighted by \(\rho^{|S|}\). For unbiased decisions, an approximation is given: \(\mathrm{cor}(f(X),f(Y))\approx \rho\sum_i \omega(i)\hat f_\pi(i)\hat f_{\pi,\rho}(i)+O(\rho^2)\), implying singleton coefficients dominate when noise is small. For large balanced tests with low influences, they cite the “majority is stablest” result: \(S_\rho(\mathrm{maj}_n)\to \tfrac{2}{\pi}\arcsin(\rho)\) as \(n\to\infty\). In a simulated Ising example (\(n=35\), \(m=100\)), singleton Fourier coefficients correlate with node degree (reported \(r=0.70\)), and isolated nodes show near-zero influence; stability increases with \(\rho\) and decreases as the pass threshold becomes more stringent.",None stated.,"Although the paper uses the term “reliability,” it is about psychometric decision stability rather than engineering system reliability, so direct transfer to reliability engineering practice is limited. The dependence handling relies on pseudo-likelihood factorization and Ising-model assumptions; if the data-generating dependence is not well approximated by an Ising MRF or if pseudo-likelihood is poor, the resulting biased Fourier coefficients and influence estimates may be distorted. The decision-noise model assumes independent coordinatewise flips with a common \(\rho\), which may not match real test error structures (e.g., correlated errors, omitted items, response styles). Empirical validation is mainly simulation/example-based, with limited real-data benchmarking against alternative robustness/decision-reliability methods.","The authors suggest extensions where Ising-model parameters (and thus the ‘true score’/item success probabilities) could be estimated at the individual level, e.g., using time-series data under a time-Markov assumption, enabling mechanistic investigations of within-person changes in item-correct probabilities.","It would be valuable to study robustness when the flip-noise model is heterogeneous across items (item-specific \(\rho_i\)) or correlated across items, and to extend the framework to polytomous items and multi-class decisions. More thorough empirical comparisons on real educational/clinical testing datasets could clarify practical gains over standard decision-consistency indices and latent-variable-based reliability measures. Providing open-source implementations (beyond citing R packages) that compute biased Fourier coefficients, influences, and stability under estimated graphical models would improve reproducibility and adoption.",2007.05857v1,https://arxiv.org/pdf/2007.05857v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:39:04Z
FALSE,NA,ML-based|Bayesian|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper addresses out-of-distribution (OOD) detection and uncertainty estimation in variational autoencoders (VAEs), focusing on the known issue that VAEs can assign higher likelihood to OOD inputs than in-distribution inputs. The authors propose an Improved Noise Contrastive Prior (INCP) that modifies the original NCP objective to use reverse KL divergence so it is compatible with VAE training, yielding an INCPVAE model trained jointly on ID data and synthetic OOD data generated by adding Gaussian noise. They introduce an uncertainty metric called the ELBO Ratio and an OOD detection score/criterion based on an INCP-KL Ratio computed from the encoder’s divergence to an assumed high-uncertainty OOD output prior. Experiments on FashionMNIST/MNIST and CIFAR10/SVHN show INCPVAE’s uncertainty increases with noise level, transfers better across datasets, and achieves near-perfect AUROC/AUPRC on their tested OOD detection setups compared with several baselines. The work advances SPC-adjacent monitoring ideas only indirectly (via anomaly/OOD detection), but it is primarily a machine learning method paper rather than reliability engineering.","OOD sample generation: \(\tilde{x}=x+\epsilon\) with \(\epsilon\sim\mathcal{N}(0,\sigma^2I)\), giving \(p_o(\tilde{x})=\int p_i(x)\,\mathcal{N}(\tilde{x}-x\mid 0,\sigma^2I)\,dx\). Improved NCP loss uses reverse KL terms: \(\mathcal{L}(\theta)=\mathbb{E}_{q_\theta(z\mid x)}[D_{KL}(q_\theta(z\mid x)\|p(z\mid x))]+\gamma\mathbb{E}_{q_\theta(\tilde{z}\mid \tilde{x})}[D_{KL}(q_\theta(\tilde{z}\mid \tilde{x})\|\tilde{p}(\tilde{z}\mid \tilde{x}))]\). They define uncertainty as ELBO Ratio \(U(x_0)=\text{ELBO}(x_0)/\text{ELBO}_I(x_{\max})\) and OOD detection via INCP-KL Ratio \(\text{KLR}(x_0)=D_{KL}(q_\theta(z_0\mid x_0)\|\tilde{p}(\tilde{z}\mid\tilde{x}))/D_{KL}(\text{OOD}_{\max})\) with threshold \(\alpha=1\).","On FashionMNIST vs MNIST OOD detection, the proposed INCP-KL Ratio achieves AUROC = 1.000 and AUPRC = 1.000 (both when trained with Baseline+Noise or Baseline), while the traditional VAE likelihood score performs poorly (AUROC = 0.035, AUPRC = 0.313). On CIFAR10 vs SVHN, INCP-KL Ratio again reports AUROC = 1.000 and AUPRC = 1.000, compared with VAE likelihood AUROC = 0.057 and AUPRC = 0.314. The uncertainty metric (ELBO Ratio) increases with injected noise level for INCPVAE across FashionMNIST, MNIST, CIFAR10, and SVHN, whereas the standard VAE’s uncertainty changes little on several datasets. The paper also reports transfer behavior where a model trained on FashionMNIST yields higher uncertainty on MNIST for INCPVAE, while the standard VAE shows the opposite trend.","The authors note they only study uncertainty estimation and OOD detection within the VAE framework and suggest extending INCP to other generative models such as GANs. They also state that generating OOD data by adding Gaussian noise to ID data may not capture characteristics of real-world OOD data; they propose exploring alternative OOD generation methods (e.g., GAN-generated OOD) or using adversarial examples to improve robustness.","The OOD detection setup relies heavily on synthetically generated OOD samples (Gaussian noise) and uses specific dataset pairs (e.g., FashionMNIST/MNIST, CIFAR10/SVHN); performance may not generalize to more realistic, semantically different OOD shifts or to non-image modalities. The chosen decision threshold (\(\alpha=1\)) and the normalization by a maximum-INCP-KL training OOD sample may be brittle under dataset size changes, distribution drift, or different noise levels. Comparisons are reported mainly via AUROC/AUPRC without extensive ablations on sensitivity to hyperparameters (noise variance \(\sigma^2\), output-prior variance \(\sigma_{\tilde{x}}^2\), and \(\gamma\)), and without runtime/compute cost analysis of training on augmented OOD data.","They propose extending INCP beyond VAEs to other generative models (e.g., GANs). They also suggest developing more realistic ways to generate or obtain OOD inputs for training (e.g., using GANs) and exploring adversarial examples as training OOD data to enhance robustness.","Evaluate INCPVAE under more diverse OOD regimes (covariate shift, semantic shift, near-OOD, corruption benchmarks) and on non-image domains (time series, tabular, medical signals) to establish generality. Develop principled/self-starting threshold selection and calibration methods for KLR/ELBO-ratio scores under unknown test-time class priors and drift. Provide open-source reference implementations and reproducibility artifacts (configs, seeds) plus systematic ablations on noise generation, prior variance settings, and encoder/decoder architectures to clarify which components drive gains.",2007.08128v3,https://arxiv.org/pdf/2007.08128v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:39:42Z
TRUE,RUL prediction|Degradation modeling|Maintenance optimization,ML-based|Hybrid/Ensemble,Sensor/condition monitoring|Degradation measurements,Predictive,Transportation/logistics|Manufacturing (general),Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes ATS2S, an attention-based sequence-to-sequence (encoder–decoder) deep learning framework with an auxiliary reconstruction task for machine remaining useful life (RUL) prediction. The method jointly optimizes (i) next-sequence reconstruction loss (to learn predictive, denoised latent dynamics from sensor sequences) and (ii) RUL regression loss, and uses an attention mechanism to leverage information across long histories rather than relying only on the encoder’s last hidden state. A dual-latent representation that concatenates encoder features and decoder hidden state is fed to a multilayer fully connected predictor for RUL estimation. Experiments on four real C-MAPSS turbofan engine datasets (FD001–FD004) show ATS2S consistently outperforms a broad set of prior ML and deep learning baselines, especially on multi-operating-condition datasets. The work advances PHM/RUL literature by addressing long-sequence information loss and representation learning limitations of single-objective RUL regressors via multi-objective training and attention.","ATS2S minimizes a joint objective $L(\theta)=\alpha L_{rec}(\theta)+L_{rul}(\theta)$ where $L_{rec}=\tfrac{1}{N}\sum_i\|\hat{Y}_i-Y_i\|_2^2$ predicts the next sensor window and $L_{rul}=\tfrac{1}{N}\sum_i(\widehat{\mathrm{RUL}}_i-\mathrm{RUL}_i)^2$ regresses RUL. Attention forms a context vector $z_i=\sum_{j=1}^{T} a_{ij}h_j$ with weights $a_{ij}=\mathrm{softmax}(e_{ij})$ and alignment scores $e_{ij}=f_{attn}(s_{i-1},h_j)$. The RUL predictor uses a dual-latent feature vector combining encoder and decoder states: $\widehat{\mathrm{RUL}}=f_{pred}((h_T,s_T);\theta_{pred})$.","On C-MAPSS, ATS2S reports RMSE/Score of 12.63/243 (FD001), 14.65/876 (FD002), 11.44/263 (FD003), and 16.66/1074 (FD004). It improves over the second-best method particularly on complex datasets: for FD004 the paper reports 8.3% RMSE improvement and 29.7% Score improvement; for FD002 it reports 6.4% RMSE improvement and 8.7% Score improvement. Ablation studies show the full model (attention + reconstruction auxiliary task) performs best, and attention contributes more than reconstruction alone. Sensitivity analysis indicates equal weighting of reconstruction and prediction ($\alpha=1$) yields best overall performance across datasets.",None stated.,"Results are demonstrated mainly on the C-MAPSS benchmark, so generalization to other assets, failure modes, and real plant data with missingness, sensor drift, and maintenance interventions is not established. The approach assumes fixed window length (e.g., 30) and requires labeled RUL targets (with piecewise-linear capping), which may not be available or may bias evaluation versus settings with uncertain failure times. The paper does not provide computational cost/latency analysis, which matters for deployment in online PHM; attention/seq2seq may be heavier than simpler RNN/CNN baselines. No code or implementation details (framework, hardware) are shared, limiting reproducibility and fair hyperparameter parity across baselines.",None stated.,"Extend ATS2S to handle unlabeled or weakly labeled scenarios via self-supervised pretraining and/or semi-supervised RUL learning, reducing reliance on engineered RUL labeling and caps. Develop robust/self-starting variants for nonstationary and irregularly sampled sensor streams (missing values, variable-length histories, concept drift across operating regimes). Provide uncertainty quantification for RUL (e.g., Bayesian/ensemble or conformal prediction) to support risk-aware maintenance decisions. Validate on additional real industrial datasets and include deployment-oriented analyses (runtime, memory, online updating) plus an open-source implementation for reproducibility.",2007.09868v1,https://arxiv.org/pdf/2007.09868v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:40:19Z
FALSE,NA,ML-based|Bayesian,Sensor/condition monitoring|Other,Not applicable,Network/cybersecurity|Other,Simulation study|Other,TRUE,Other,Not provided,NA,"The paper proposes using Bayesian neural networks (BNNs) to improve the trustworthiness of deep-learning-based multimedia forensic detectors by providing predictive uncertainty, especially under out-of-distribution (OOD) conditions. Using resampling (image rescaling) detection as a case study, it replaces a baseline constrained CNN with a Bayesian counterpart implemented via variational inference (Bayes-by-Backprop) and Flipout layers, enabling posterior sampling at inference time. Experiments on the RAISE dataset show comparable or slightly improved in-distribution detection accuracy relative to the baseline CNN while providing uncertainty that increases appropriately for OOD cases. OOD scenarios tested include unseen resampling factors beyond the training range, JPEG post-compression (quality factors 85 and 50), and unseen resampling algorithms (nearest-neighbor and areal interpolation). The main contribution is methodological and practical for multimedia forensics—uncertainty estimates can warn analysts when model predictions may be unreliable—rather than reliability engineering (lifetime/failure/maintenance) in the classical sense.","The Bayesian model learns a variational posterior over weights $q(\omega\mid\theta)$ by minimizing $\mathrm{KL}[q(\omega\mid\theta)\|P(\omega\mid D)]$, yielding the ELBO objective: $\mathrm{KL}[q(\omega\mid\theta)\|P(\omega)]-\mathbb{E}_{q}[\log P(D\mid\omega)]$. Predictive probabilities are estimated by Monte Carlo posterior sampling: $\mathbb{E}_{q}[P(y^*\mid x^*)]\approx \frac{1}{n}\sum_{i=1}^n P_{\omega_i}(y^*\mid x^*)$, with predictive uncertainty measured via the variance of the sampled predictive distribution. The baseline’s confidence summary for a rescaling factor uses $c_s=\frac{1}{M}\sum_{m=1}^M \max_{y_k} P(y_k\mid x_m)$.","On the held-out test set (in-distribution), the baseline constrained CNN achieves 96.32% accuracy, while the Bayesian CNN achieves 97.40% accuracy under the same training protocol (with the baseline’s extra trees classifier omitted). The BNN uses $n=50$ Monte Carlo draws from the variational posterior at inference and reports uncertainty bands (e.g., mean prediction plus/minus two standard deviations). For OOD rescaling factors outside the training range ($s\in\{0.9,\dots,1.45\}$ excluding 1.0), the standard CNN remains highly confident (often $\ge 0.9$) even when accuracy drops, whereas the BNN’s predictive uncertainty increases as inputs move farther from the training distribution. Under JPEG post-compression (quality factors 85 and 50) and under unseen interpolation methods (nearest-neighbor, areal), prediction probabilities degrade and uncertainty rises, indicating distribution shift.","The authors describe the work as preliminary and note that many aspects remain to be investigated. They explicitly mention future exploration of selection strategies for prior distributions and decomposing predictive uncertainty into model (epistemic) and data (aleatoric) components, implying current results use a fixed/simple prior and do not separate uncertainty types.","The study focuses on a binary patch-level rescaling detection task with a specific dataset (RAISE) and grayscale conversion; generalization to broader forensic tasks, color images, and diverse real-world pipelines is not fully validated. OOD evaluation is scenario-based but still limited (e.g., a small set of JPEG qualities and interpolation methods), and uncertainty quality is not assessed with formal calibration metrics (e.g., ECE, reliability diagrams) or decision-theoretic thresholds. The method relies on variational approximations (Flipout/mean-field-like assumptions) that may under-represent posterior uncertainty, and the computational cost of MC sampling (50 forward passes) may be nontrivial for operational forensic workflows.","They plan to extend the Bayesian approach to other forensic tasks, explore strategies for selecting prior distributions, and study decomposition of predictive uncertainty into model and data uncertainty.","A useful next step would be to quantify uncertainty calibration explicitly (ECE/Brier score) and define actionable operating points (e.g., abstain/reject thresholds) for forensic analysts. Additional work could test robustness under broader distribution shifts (different cameras, demosaicing/denoising pipelines, social-media recompression chains) and evaluate on real forensic casework data rather than controlled resampling setups. Efficiency improvements (e.g., fewer MC samples, distillation of uncertainty, or approximate Bayesian ensembling) and releasing reproducible code/models would help practical adoption and benchmarking.",2007.14132v1,https://arxiv.org/pdf/2007.14132v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:40:55Z
FALSE,NA,Other,Simulated only|Other,Not applicable,Healthcare/medical,Simulation study,TRUE,MATLAB,Not provided,http://arxiv.org/abs/2005.07895|http://cvxr.com,"The paper proposes an error-correcting pooled-testing framework to improve the diagnostic accuracy (reduce false negatives and false positives) of COVID-19 virus/antibody testing, rather than primarily reducing the number of tests as in classical group testing. It models quantitative pooled RT-qPCR measurements as y = f(Ax) + v + e, where A is a designed pooling/allocation matrix, v is Gaussian noise, and e is a sparse gross-error (outlier) vector capturing events like false negatives, contamination, or operational mistakes. Decoding is performed via a convex relaxation (an \(\ell_1-\ell_1\) program) to jointly estimate the nonnegative viral-load vector x and sparse errors, with a thresholding rule to declare positives/negatives. Extensive Monte Carlo simulations compare pooled testing to repeated individual testing across different numbers of tests (including an “oversampling” regime m>n), noise levels, outlier probabilities, and sparsity levels. The simulations show pooled testing generally achieves lower false negative rate (FNR) and false positive rate (FPR) than individual testing for the same total number of measurements, and that pooled testing’s FNR/FPR decrease monotonically as m increases in their experiments.","The pooling design uses a binary participation matrix \(P\in\{0,1\}^{m\times n}\) and an allocation matrix \(W\in[0,1]^{m\times n}\) to form \(A := P\odot W\) (Hadamard product). Quantitative test outcomes are modeled as \(y = f(Ax) + v + e\), with an ideal linear case \(y = Ax + v + e\), where x is nonnegative viral load, v is (Gaussian) noise, and e is sparse gross error/outliers. Recovery is posed as an \(\ell_0\) objective and solved via the convex \(\ell_1-\ell_1\) relaxation: minimize \(\|z\|_1 + \lambda\|y-Az-u\|_1\) subject to \(\|u\|_2\le \varepsilon\), \(z\ge 0\); then classify subject j positive if \(z_j\ge \tau\) (\(\tau=1\) in simulations).","All evaluations are simulation-based (100 random trials per configuration), reporting average FNR and FPR while varying population size (n=25,40), number of measurements (m from 10 up to 50/80), sparsity level (k=1 to 6 infected), Gaussian noise variance levels (\(\sigma^2\) from 5e-1 to 2e0), and outlier probability (Pout = 1%, 5%, 15%). Across the plotted regimes, pooled testing typically yields substantially lower FNR than individual testing for the same m, and often lower FPR as well—especially when m≥n. The authors report that they did not observe any case where individual testing outperformed pooled testing simultaneously on both FNR and FPR. They also observe pooled testing’s FNR and FPR decrease as m increases, while individual testing’s FPR can increase with more repeated tests due to the rule that any positive replicate labels the subject positive.",None stated.,"Results rely on a stylized measurement/noise model (Gaussian noise plus a handcrafted sparse outlier mechanism designed to mimic false negatives/positives) and may not match real RT-qPCR pooled-sample error behavior (e.g., dilution-induced Ct shifts, nonlinearity in f(·), correlated errors, batch effects). The work is evaluated only with Monte Carlo simulations; there is no validation on real pooled qPCR datasets or lab experiments, so practical gains in diagnostic sensitivity/specificity remain uncertain. The method assumes knowledge/selection of several tuning parameters (\(\lambda,\varepsilon,\tau\)) and a designed pooling matrix; robustness to miscalibration and operational constraints is not systematically analyzed.",None stated.,"Validate the approach on real pooled RT-qPCR datasets or through wet-lab studies that capture dilution, Ct-thresholding, and process variability, and calibrate f(·) accordingly. Develop self-tuning or robust procedures for selecting \(\lambda,\varepsilon,\tau\) and for handling unknown/heteroskedastic noise and correlated outliers. Extend the framework to explicitly model qualitative (binary) tests, constrained lab workflows (pipetting limits, pool size bounds), and to provide uncertainty quantification (e.g., confidence/credible intervals for individual diagnoses).",2007.14919v1,https://arxiv.org/pdf/2007.14919v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:42:13Z
FALSE,Other,Other,Other,Not applicable,Other,Other,NA,None / Not applicable,Not applicable (No code used),https://blogs.upm.es/devopsinpractice,"The paper is a methodological guide for assessing the trustworthiness of qualitative coding in empirical software engineering via inter-coder agreement (ICA), with emphasis on Krippendorff’s $\alpha$. It reviews common agreement/reliability coefficients (percent agreement, Holsti, Scott’s $\pi$, Cohen’s $\kappa$, Fleiss’ $\kappa$) and explains why several widely used statistics (Cronbach’s $\alpha$, Pearson’s $r$, Spearman’s $\rho$) are not appropriate for measuring agreement. The main contribution is a unifying “universal $\alpha$” mathematical framework that expresses multiple published variants of Krippendorff’s $\alpha$ through a common labeling/meta-coding formulation, clarifying interpretation and edge-case behaviors. It also presents how to compute and interpret several $\alpha$ variants used in Atlas.ti (global binary, per-domain binary, cu-$\alpha$, Cu-$\alpha$) and demonstrates them in a large DevOps culture case study. The work is about reliability of qualitative coding (measurement validity/reproducibility), not reliability engineering of systems/components (failure/maintenance/lifetime).","Key definitions include Krippendorff’s $\alpha = 1 - D_o/D_e$, where observed and expected disagreement are computed from observed/expected coincidence matrices: $D_o=\sum_{i,j} o_{ij}\,\delta(l_i,l_j)$ and $D_e=\sum_{i,j} e_{ij}\,\delta(l_i,l_j)$. The expected coincidences are $e_{ij}=t_i t_j/(t-1)$ for $i\neq j$ and $e_{ii}=t_i(t_i-1)/(t-1)$, with $t_i$ marginal totals and $t=\sum_i t_i$. For binary/discrete-metric cases, this reduces to the familiar chance-corrected agreement form $\alpha=(P_o-P_e)/(1-P_e)$.","In a worked two-coder example (15 items), percent agreement is 66.7%, but chance-corrected measures are much lower: Cohen’s $\kappa=0.391$, Scott’s $\pi=0.322$, Fleiss’ $\kappa=0.322$, and Krippendorff’s $\alpha=0.343$. In the DevOps Atlas.ti tutorial example, a per-domain binary coefficient for domain P07 is reported as $\alpha^{P07}_{binary}=0.913$ in one round, while in a later round it can drop to approximately $-0.011$ due to sparse data (illustrating instability/edge cases). For cu-$\alpha$ across 10 domains in one round, several domains reach 1.0, while others are lower (e.g., P01 0.705, P06 0.739, P10 0.563), and the global Cu-$\alpha$ is shown as 0.67 initially, increasing to 0.905 after codebook refinement.",None stated.,"The paper’s notion of “reliability” concerns inter-coder reliability in qualitative research, which is conceptually different from reliability engineering (failure behavior, lifetime distributions, maintainability), so it does not transfer directly to engineering reliability practice. Much of the tutorial behavior and recommendations are tied to Atlas.ti’s specific implementation choices (e.g., handling of sparse coding and N/A outputs), which may not generalize to other qualitative analysis tools. Empirical evaluation focuses on illustrative examples/case study; there is no broad benchmark-style comparison on diverse datasets to quantify when specific $\alpha$ variants are preferable.",None stated.,"Provide tool-agnostic reference implementations (e.g., in R/Python) that reproduce Atlas.ti’s $\alpha$ variants exactly, to improve transparency and reproducibility. Systematically study robustness of the proposed interpretations under common qualitative-research complications (unitizing differences, missingness, coder training effects, and heavily imbalanced coding) via simulations and multi-dataset replications. Extend the framework with practical diagnostics (e.g., per-code contribution to disagreement, influence analysis) to better guide codebook refinement beyond reporting scalar agreement coefficients.",2008.00977v2,https://arxiv.org/pdf/2008.00977v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:42:48Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/stheid/SetPOS,"The paper extends part-of-speech (POS) tagging for historical corpora by reframing tagging as set-valued prediction, allowing the model to output a small set of candidate tags rather than committing to a single label. Using the utility-based set-valued prediction framework of Mortier et al. and the UBOP algorithm, predicted posterior tag probabilities are converted into Bayes-optimal candidate sets that trade off coverage (including the true tag with high probability) against set size. The approach is applied as a post-processing step to state-of-the-art taggers (TreeTagger and Stanford CoreNLP-style MEMM/logistic regression) to avoid intractable contextual tag dependencies. Experiments on a Middle Low German / Early New High German legal-text corpus (23 documents, >90-tag set) evaluate in-domain, leave-one-document-out transfer, and whole-corpus training scenarios, reporting accuracy (single-label) and utility plus mean set size (set-valued). Results show higher utility and robustness, especially on unknown words, with average set sizes around ~2 for TreeTagger/CoreNLP in the whole-corpus scenario, and sensitivity analysis over the risk-aversion parameter \(\beta\) demonstrating limited utility gains beyond \(\beta\approx 1\).","The set-valued utility is defined as \(u(y,\hat{Y})=\mathbb{I}[y\in\hat{Y}]\,g(|\hat{Y}|)\), trading correctness (coverage) against precision (small sets). The paper uses \(u_\beta(y,\hat{Y})=0\) if \(y\notin\hat{Y}\), else \(1-\big(\frac{|\hat{Y}|-1}{|Y|-1}\big)^\beta\), where \(\beta\) controls how strongly larger sets are penalized. UBOP forms \(\hat{Y}\) by sorting labels by posterior \(P(y\mid x)\) and selecting the top-\(k\) set maximizing expected utility \(g(|\hat{Y}|)\sum_{y\in\hat{Y}}P(y\mid x)\). For POS-tagging, \(x\) is a token with context and posteriors approximate \(P(t_i\mid w_i\wedge \text{context}(w_i))\).","In the reported whole-corpus training scenario, the set-valued versions of TreeTagger and CoreNLP achieve average predicted set sizes of about 2 while improving utility relative to single-tag accuracy (utility consistently exceeds accuracy across texts/scenarios). In leave-one-document-out transfer, accuracy can drop substantially on a dialectally distinct document (Duisburg), but set-valued utility remains much higher, at the cost of larger sets. Sensitivity analysis over \(\beta\) shows utility increases with \(\beta\), but gains beyond \(\beta>1\) are small; the baseline tends to produce the largest sets, TreeTagger the smallest. The set-size histogram for CoreNLP with \(\beta=1\) indicates most predictions are singletons for known words, while unknown words yield larger sets more often; sets larger than 8 are very rare.","The extension of the CoreNLP-style tagger is limited to using the most likely context obtained via an initial first pass, which ignores less likely contexts and thus does not fully account for contextual ambiguity. Because CoreNLP’s original inference with tag-dependent context would cause an exponential blow-up for set-valued targets, the paper relies on an approximation via post-processing rather than exact joint inference over alternative contexts.","The work is not a reliability engineering study; “reliable” refers to uncertainty-aware NLP predictions, so the results do not translate to engineering reliability metrics (failure rates, RUL, survival) or maintenance decision-making. The evaluation focuses on utility/accuracy and set size; it does not provide formal coverage guarantees (as conformal prediction would) nor a detailed calibration assessment beyond qualitative discussion, which limits interpretability of probabilistic confidence. Comparisons are restricted to a small set of taggers/baselines and a specific historical German corpus and tagset, so generalizability to other languages, domains, or modern neural taggers (e.g., transformers) is not established.","The authors propose incorporating multiple potential contexts (i.e., different structurally ambiguous readings) instead of relying only on the single most likely context from a first-pass tagging, especially for the CoreNLP extension. They also suggest combining set-valued prediction with set-valued supervision, allowing human annotators to provide multiple plausible tags during training and learning from such weak supervision via superset learning techniques.","A natural extension is to add explicit calibration methods (e.g., temperature scaling) and to report calibration metrics to better justify probabilistic set construction, especially for decision-tree-based posteriors. Another direction is to integrate set-valued prediction directly into modern sequence models (BiLSTM/CRF, transformer taggers) with approximate decoding that maintains multiple context hypotheses, and to benchmark against conformal prediction for coverage-controlled tag sets. Finally, more extensive cross-corpus and cross-language validation (including different historical orthographies and annotation guidelines) would clarify robustness and practical adoption.",2008.01377v4,https://arxiv.org/pdf/2008.01377v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:43:42Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Other,Not applicable,Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/aijordan/reliabilitydiag|https://github.com/TimoDimi/replication,"This paper revisits the evaluation of probabilistic classifiers via reliability diagrams and proper scoring rule decompositions, proposing the CORP approach (Consistent, Optimally binned, and Reproducible). CORP constructs reliability diagrams using nonparametric isotonic regression computed with the pool-adjacent-violators (PAV) algorithm, yielding an automated, tuning-free binning that is stable to ad hoc choices that plague classical binning-and-counting diagrams. The method provides uncertainty quantification through either resampling or asymptotic theory (with distinct discrete vs continuous asymptotics), and introduces a miscalibration measure (MCB) plus a CORP-based score decomposition that generalizes the Brier decomposition to any proper scoring rule. Performance is demonstrated with both simulations and a real case study on precipitation probability forecasts (Niamey, Niger), showing improved stability and favorable mean-squared-error behavior compared with fixed-bin or quantile-bin alternatives. Open-source R code is provided for producing CORP diagrams and related quantities.","The CORP score decomposition writes the empirical mean score as $\bar S_X=(\bar S_X-\bar S_C) - (\bar S_R-\bar S_C) + \bar S_R$, where $\bar S_X=\frac1n\sum_{i=1}^n S(x_i,y_i)$ is the score of original probabilities, $\bar S_C=\frac1n\sum_{i=1}^n S(\hat x_i,y_i)$ uses PAV-isotonic recalibrated probabilities $\hat x_i$, and $\bar S_R=\frac1n\sum_{i=1}^n S(r,y_i)$ uses the reference forecast $r=\bar y$. The components are defined as MCB $=\bar S_X-\bar S_C\ge0$ (miscalibration), DSC $=\bar S_R-\bar S_C\ge0$ (discrimination), and UNC $=\bar S_R$ (uncertainty). The reliability curve itself is the graph of the PAV-isotonic regression estimate of the conditional event probability (CEP) as a nondecreasing function of the forecast value.","In the Niamey precipitation case study, the CORP Brier decompositions reported are: ENS $\bar S_X=0.266$, MCB $0.066$, DSC $0.044$, UNC $0.244$; EPC $0.234$, MCB $0.022$, DSC $0.032$, UNC $0.244$; EMOS $0.232$, MCB $0.018$, DSC $0.030$, UNC $0.244$; Logistic $0.206$, MCB $0.017$, DSC $0.056$, UNC $0.244$. Simulation results show CORP uncertainty bands attain generally accurate or slightly conservative empirical coverage at the 90% level under default settings. In MSE comparisons of CEP estimation, CORP achieves the smallest MSE across tested sample sizes and forecast-value distributions versus binning-and-counting with fixed bins (e.g., $m=5,10,50$) or quantile-spaced bins $m(n)=\lfloor n^\alpha\rfloor$. The paper highlights distinct asymptotic regimes: $n^{1/2}$ scaling (mixtures of normals) for discrete forecast values, and $n^{1/3}$ scaling (Chernoff distribution) for continuously distributed forecast values.","The authors note that both resampling- and asymptotic-theory-based uncertainty quantification procedures assume independent (or at least exchangeable) forecast cases, which may not hold in practice (e.g., under serial dependence). They also state that, for confidence bands, their default implementation relies on resampling because the available asymptotic theory depends on the assumption that the true CEP has a strictly positive derivative. They encourage follow-up work for dependent-data settings.","The approach relies on an isotonic (nondecreasing) CEP constraint; while often reasonable, it can mask genuine non-monotone calibration structure (e.g., due to covariate shift or model misspecification), potentially understating localized miscalibration. Practical guidance for choosing between discrete vs continuous handling (e.g., the 0.01 minimum-distance heuristic) may be dataset-scale dependent and could affect interpretability or stability in edge cases. The real-data evidence is primarily one meteorological case study; broader benchmarking across domains/classifiers would strengthen generalizability claims. Finally, while linear-time PAV is efficient, the added steps for uncertainty bands (especially resampling) can still be computationally heavy for very large-scale ML settings without further optimization or approximations.","They propose developing CORP- and MCB-based alternatives to binning-sensitive calibration tests such as the Hosmer–Lemeshow test. They also suggest extending the PAV/CORP blueprint beyond binary events to mean, quantile, and expectile assessment for real-valued outcomes, yielding analogous decompositions (e.g., an MSE decomposition) and broader applications in regression. The authors call for further investigation of uncertainty quantification in dependent-data (non-exchangeable) settings.","A useful extension would be robust/self-starting variants that better handle temporal dependence (e.g., block bootstrap for bands, or explicit time-series calibration models) and covariate drift. Extending CORP to multivariate/multi-class calibration (e.g., simplex-valued probabilities) with comparable optimality and reproducibility guarantees would broaden ML applicability. More systematic comparisons against modern calibration methods (Platt scaling, temperature scaling, beta calibration, spline-based calibration) on standard ML benchmarks could clarify when isotonic CORP is preferable. Packaging additional diagnostics (e.g., local miscalibration tests along the curve, model-selection guidance for monotonicity violations) and scalable implementations for massive datasets would improve practitioner adoption.",2008.03033v1,https://arxiv.org/pdf/2008.03033v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:44:34Z
TRUE,RUL prediction|Degradation modeling|Maintenance optimization,ML-based|Bayesian,Degradation measurements|Sensor/condition monitoring|Mixture of types,Predictive,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/YexuZhou/AutoRUL,"The paper proposes an automatic remaining useful life (RUL) estimation framework for predictive maintenance using multivariate time-series sensor data and sliding windows. Its main methodological contribution is a new LSTM variant, Embedded Convolutional LSTM (ECLSTM), which replaces fully connected operations in LSTM gates with embedded 1D convolutions to better preserve temporal structure within and between windows while keeping parameter growth independent of window size. The framework uses BOHB (Bayesian optimization + Hyperband) to automatically tune hyperparameters across preprocessing, backbone (stacked ECLSTM), and prediction head, reducing the need for expert design. The approach is evaluated on multiple real benchmark datasets for RUL (NASA C-MAPSS, PHM 2008, FEMTO-ST bearings) and also on UCI-HAR to assess general multivariate time-series generalization. Results reported show state-of-the-art performance on C-MAPSS with notably improved RMSE/score, competitive results on FEMTO-ST without expert feature engineering, and strong accuracy on HAR.","ECLSTM modifies standard LSTM gate equations by using convolution instead of full connections: $i_t=\sigma(W_i * [x_t,h_{t-1}] + b_i)$, $f_t=\sigma(W_f * [x_t,h_{t-1}] + b_f)$, $o_t=\sigma(W_o * [x_t,h_{t-1}] + b_o)$, $C_t=f_t\circ C_{t-1}+ i_t\circ \tanh(W_C * [x_t,h_{t-1}] + b_C)$, $h_t=o_t\circ \tanh(C_t)$, where $*$ is 1D convolution and $\circ$ is elementwise product. A deeper (stacked) convolution-in-gate version is defined, e.g. $i_t=\sigma(W_i^3*\sigma(W_i^2*\sigma(W_i^1*[x_t,h_{t-1}]+b_i^1)+b_i^2)+b_i^3)$. For evaluation on C-MAPSS, RMSE and the PHM08 asymmetric score are used: $\mathrm{RMSE}=\sqrt{\frac{1}{N}\sum_{i=1}^N(\hat r_i-r_i)^2}$ and a piecewise exponential penalty score depending on over/under-prediction.","On NASA C-MAPSS, the proposed optimized framework reports best RMSE across all four subsets: FD001 11.03, FD002 15.95, FD003 11.23, FD004 16.21; corresponding scores are $2.16\times10^2$, $1.44\times10^3$, $1.93\times10^2$, $1.4\times10^3$. Compared to a strong prior method (AdaBN), RMSE improves notably on FD002 (19.29→15.95) and FD004 (22.14→16.21). On PHM 2008, the framework achieves score 823.341 (competitive with historical challenge ranks, and better than Deep LSTM 1862 / Deep CNN 2056 reported). On FEMTO-ST bearings, mean absolute percentage error is 19.62%, comparable to CCG-HI (18.51%) and better than RNN-HI (34.28%) and WMQE (28.18%). On UCI-HAR, balanced accuracy is 94.69% ± 0.42%, exceeding cited CNN-LSTM (93.40%) and HCF+CNN (93.80%).","The authors note that hyperparameters and architecture are sensitive and require careful tuning, motivating their AutoML approach. They also observe that very large window sizes can degrade performance when sequence length is fixed because the number of time steps (windows) shrinks, reducing temporal information between windows. They mention that in their ablation/baseline comparisons, sensors are not selected and the network structure could be further optimized, implying room for further improvement.","The approach is primarily validated on benchmark datasets; generalization to different fleets, sensor drift, missing data, and real operational constraints (e.g., varying sampling, nonstationarity) is not deeply analyzed. The framework emphasizes predictive accuracy but does not explicitly connect RUL uncertainty to decision-making (e.g., maintenance risk/cost), nor provide calibrated uncertainty estimates. Comparisons depend on consistent preprocessing choices (e.g., piecewise linear RUL cap at 130 for C-MAPSS/PHM08), which may advantage certain methods and limit real-world interpretability. Implementation/compute requirements (GPU-time budgets of 12–24 hours) may limit practical adoption for smaller teams or embedded deployments, and latency/online updating is not discussed.","They propose increasing the framework’s diversity by adding attention mechanisms and incorporating bidirectional ECLSTM. They also plan to design a more reasonable configuration space for architecture representation, aiming to simplify hyperparameter optimization and discover better models faster.","A valuable extension would be adding uncertainty quantification (e.g., Bayesian deep learning or conformal prediction) so maintenance decisions can be made with risk bounds rather than point estimates. Robust/self-starting variants that handle missing sensors, irregular sampling, and distribution shift (domain adaptation beyond batch norm) would improve real deployment readiness. More explicit integration with maintenance optimization (e.g., cost-aware loss functions or policy optimization using predicted RUL distributions) could connect prediction performance to operational value. Providing a reproducible experiment script with fixed dataset splits and standardized baselines (and reporting statistical significance across methods) would strengthen comparative claims and adoption.",2008.03961v1,https://arxiv.org/pdf/2008.03961v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:45:10Z
FALSE,NA,Bayesian|Other,Other,Not applicable,Healthcare/medical|Finance/economics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://dylanslacks.website/reliable/index.html,"This paper proposes a Bayesian framework for post hoc local explanations of black-box ML models that quantifies uncertainty in feature attributions via credible intervals. It instantiates the framework as BayesLIME and BayesSHAP, deriving closed-form posteriors for feature importance parameters and an error/fit uncertainty measure, and uses these uncertainty estimates to (i) estimate the number of perturbations required to reach a target credible-interval width and (ii) design a focused (active-learning-style) perturbation sampling strategy for faster convergence. Empirical evaluation on COMPAS, German Credit, MNIST, and ImageNet shows improved calibration of credible intervals, increased stability versus LIME/SHAP, and reduced query cost with focused sampling; a user study (31 participants) suggests the uncertainty-aware selection yields more informative explanations. The work is primarily about explainability reliability (stability/consistency and uncertainty quantification), not reliability engineering of physical systems.","The framework models local surrogate responses as weighted Bayesian linear regression: for perturbations $z$ around $x$, $y\mid z,\phi,\sigma^2 \sim \mathcal{N}(\phi^\top z,\,\sigma^2/\pi_x(z))$ with priors $\phi\mid\sigma^2\sim\mathcal{N}(0,\sigma^2 I)$ and $\sigma^2\sim\text{Inv-}\chi^2(n_0,\sigma_0^2)$. The posterior mean matches weighted ridge/LIME/KernelSHAP: $\hat\phi=(Z^\top\mathrm{diag}(\Pi_x(Z))Z+I)^{-1}(Z^\top\mathrm{diag}(\Pi_x(Z))Y)$, and uncertainty is obtained from the posterior $\phi\mid\sigma^2,Z,Y \sim \mathcal{N}(\hat\phi, V_\phi\sigma^2)$ and the marginal error term $\epsilon\mid Z,Y$ as a Student-$t$. The required additional perturbations for target credible-interval width $W$ at level $\alpha$ is estimated by $G(W,\alpha,x)=\frac{4 s_S^2}{\bar\pi_S}\left(\frac{W}{\Phi^{-1}(\alpha)}\right)^2-S$, and focused sampling uses predictive variance $\mathrm{var}(\hat y(z))=((z^\top V_\phi z+1)s^2)(N/(N-2))$ to bias candidate selection.","Across datasets, 95% credible intervals computed from 100 perturbations contain “ground-truth” feature importances (estimated using 10,000 perturbations) close to the nominal rate (e.g., COMPAS: 95.5% for BayesLIME and 87.9% for BayesSHAP; MNIST digits shown range about 95.2–98.4%). BayesLIME/BayesSHAP improve explanation stability versus LIME/SHAP by about 53% on average (local Lipschitz metric), with most comparisons statistically significant (Wilcoxon signed-rank test $p<1\mathrm{e}{-2}$ except BayesSHAP on German Credit). Focused sampling reaches high-quality explanations with far fewer queries than random sampling (stabilizing within a few hundred queries vs. >1,000 in an ImageNet experiment) and can speed up explanation generation by up to 2×. A user study on MNIST (31 subjects) found higher masking difficulty for BayesLIME-selected explanations (error rate 30.7% vs. 25.7% for LIME; one-tailed two-sample t-test $p=0.028$).","The authors note the method inherits shortcomings of LIME/SHAP because it relies on local linear approximations; if the black-box local decision surface is highly non-linear, the surrogate may not capture it accurately. They also state that if BayesLIME/BayesSHAP use the same perturbation sampling procedures as LIME/SHAP, they may remain vulnerable to known adversarial attacks on explanation methods (e.g., Slack et al. 2020).","“Reliability” here refers to explanation stability/uncertainty rather than reliability engineering; the framework does not address time-to-failure, degradation, or maintenance decisions. Calibration of credible intervals is evaluated against an internal proxy ground truth (10,000-perturbation estimates under the same kernels), which may not reflect fidelity to the true black-box behavior under distribution shift or correlated/structured perturbations. The focused sampling procedure may be sensitive to hyperparameters (batch/pool sizes, temperature) and to the perturbation generator; the paper offers limited guidance on robust defaults across domains and on failure modes when perturbations violate data manifold constraints. Comparisons are primarily against default LIME/KernelSHAP and do not fully benchmark against other stability/uncertainty approaches (e.g., deterministic LIME variants, bootstrap-based uncertainty) under matched compute budgets.",They suggest extending the framework to produce global explanations with uncertainty guarantees. They also propose exploring how uncertainty quantification can be used to calibrate user trust in model explanations.,"Develop self-starting/online variants that update posteriors efficiently for streaming or interactive settings while controlling query budgets. Extend uncertainty quantification to handle correlated perturbations, manifold-constrained perturbation generation, and distribution shift, with robustness diagnostics when credible intervals are miscalibrated. Provide open-source reference implementations and standardized benchmarks (matched wall-clock/query budgets) against bootstrap and other Bayesian/ensemble uncertainty methods. Extend beyond linear surrogates to local generalized additive models or locally adaptive basis expansions while retaining tractable posterior uncertainty and perturbation-to-go estimates.",2008.05030v4,https://arxiv.org/pdf/2008.05030v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:45:51Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Other,Not applicable,Finance/economics,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,NA,"This paper develops a new estimator for linear predictive regression designed to deliver reliable inference under heteroskedasticity when predictors are heavy-tailed and only first moments of outcomes and predictors are finite. The estimator downweights extreme predictors by scaling each observation with $\|X_j\|_2^{-1/2}$, yielding a bounded instrument vector $G_j=\|X_j\|_2^{-1/2}X_j$ with elements bounded by 1, which enables asymptotic normality and stable standard error estimation without requiring fourth moments. The author derives conditional and unconditional unbiasedness/consistency/CLTs and proposes a heteroskedasticity-robust variance estimator that uses mild truncation/weighting to remain valid even when $\mathrm{Var}(X)$ may be infinite. Extensive Monte Carlo evidence shows the proposed t-statistic is close to $N(0,1)$ across heavy-tail settings where OLS with White/Eicker-Huber robust SEs performs poorly, and an empirical application estimates stock betas and constructs high/low-beta portfolios. The approach is also extended to quantile (including median) regression via an analogous weighted check-loss formulation that permits use of standard quantile regression software after preprocessing.","The proposed estimator centers predictors using $\hat\psi=\frac{1}{n}\sum_{j=1}^n Z_j$, sets $X_j=(1,(Z_j-\hat\psi)^T)^T$, and defines $G_j=\|X_j\|_2^{-1/2}X_j$ (so $\|G_j\|_\infty\le 1$). The regression estimator is $\hat\beta=\arg\min_b \sum_{j=1}^n \tfrac12\|X_j\|_2^{-1}(Y_j-X_j^Tb)^2$, equivalently $\hat\beta=S_{G,X}^{-1}S_{G,Y}$ where $S_{G,X}=\frac1n\sum G_jX_j^T$ and $S_{G,Y}=\frac1n\sum G_jY_j$. A heteroskedasticity-robust covariance estimator is $\widehat{\mathrm{Var}}(\hat\beta)=\frac1n S_{G,X}^{-1}\,\widehat S_{U^2G,G}\,S_{G,X}^{-1}$ with $\widehat S_{U^2G,G}=\frac1n\sum 1_{\|X\|_\infty/E\|X\|_\infty\le c n^{1/5}}\,\hat U_j^2 G_jG_j^T$ (truncation used to avoid needing $\mathrm{Var}(X)$).","Simulations (e.g., QQ plots and Cramr–von Mises normality tests) show the proposed pivot $\hat T_{\hat\beta}$ is close to $N(0,1)$ even for very heavy-tailed predictors (e.g., Student-t with $\nu=2.4$) where the OLS robust-SE pivot $\hat T_{LS}$ is severely non-normal. In a cross-section of about 400 S&P500-component stocks using weekly returns over 2018–2020, the method selects 36 high-beta stocks vs 49 by OLS robust-SE testing, with 32 agreements; for low-beta it selects 37 vs 32 by OLS, with 23 agreements. Rolling-window analysis (15 years, 100-week window) shows OLS standard errors are much more jagged over time than the proposed method’s, consistent with the paper’s claim of improved stability. In the rolling portfolio exercise (universe ~358 stocks), high-beta portfolios have estimated betas around 1.75–1.77 and low-beta portfolios around 0.57–0.63, with broadly similar out-of-sample portfolio summaries whether selection uses the proposed method or OLS, but the proposed inference is smoother/less fragile.","The author notes that the truncation/weighting used in the variance estimator is mainly a guardrail for “extraordinary” predictors and becomes important when $\mathrm{Var}(X)$ may be infinite; without truncation additional moment conditions (e.g., finite $\mathrm{Var}(X)$) are needed for some variance estimators. The paper also emphasizes a tradeoff: OLS can be more efficient under thin tails/homoskedasticity, while the proposed estimator is more conservative because it downweights extreme predictors and may sacrifice precision in benign settings. For extremely heavy tails and small samples, simulations show some distortion remains, improving only with larger sample sizes (e.g., around 1,000 in the most extreme cases).","The development and main results rely heavily on (approximate) independence across observations (i.i.d. pairs), while many finance return series exhibit dependence beyond what weekly sampling removes; the paper briefly mentions martingale-difference extensions but does not fully develop robust finite-sample procedures under autocorrelation/clustering. The method requires computing norms of centered predictors and assumes the key matrix $S_{G,X}$ is invertible/positive definite; practical diagnostics or remedies for near-singularity/high collinearity (especially in higher-dimensional predictor settings) are not fully addressed. Empirical evaluation focuses on a particular beta-estimation task; broader benchmarking against other modern robust regression/inference approaches (e.g., adaptive Huber with heteroskedastic-robust inference, robust GMM trimming, or self-normalized methods) is limited. No packaged implementation is provided, which may hinder adoption and reproducibility beyond the referenced scripts.","The paper explicitly extends the core idea to quantile regression (including median predictive regression) and discusses further robustness via reparameterization using componentwise medians for centering predictors and using an $\ell_1$-norm-based normalization to avoid squaring data. It also notes that removing the need for conditional-variance existence may require switching the estimand from mean-based to quantile-based targets, motivating the quantile-regression extension. A remark suggests that the i.i.d. assumption in the unconditional CLT can be weakened to martingale-difference sequences with respect to the natural filtration.","Developing fully time-series-robust inference (e.g., HAC/cluster-robust versions of the proposed variance estimator) would broaden applicability in finance and macro settings where dependence is material. Extending the approach to high-dimensional predictive regressions (p comparable to n) with regularization and corresponding robust inference would be valuable for modern asset-pricing predictors. Providing a well-tested software package (R/Python) with diagnostics for tail-heaviness, instrument-matrix conditioning, and recommended default tuning (e.g., the $n^{1/5}$ truncation constants) would improve practical uptake. Additional empirical benchmarks across multiple asset classes and alternative predictors, plus comparisons to other robust inference frameworks, would clarify when the method dominates in realistic finite samples.",2008.06130v1,https://arxiv.org/pdf/2008.06130v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:46:32Z
FALSE,NA,ML-based|Bayesian|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Other,Not provided,https://www.tensorflow.org/|https://www.tensorflow.org/probability,"The paper proposes post-hoc calibration methods for Bayesian Neural Networks (BNNs) based on information-geometric alpha-divergences (BB-α), targeting overconfident and miscalibrated predictive uncertainties. It compares using alpha-divergences during training versus applying alpha-divergence objectives in a calibration stage (e.g., temperature scaling variants and last-layer fine-tuning), showing calibration is more effective post-training for certain α values. Two temperature-scaling extensions are introduced: a scalar variance scaling (sTS) and an anisotropic scaling using a lower-triangular matrix L (TrilTS) with Σ → LᵀΣL; additional methods fine-tune the last layer (LL, LLμ) optionally combined with scalar or triangular scaling. Experiments on a synthetic cosmic microwave background (CMB) regression dataset demonstrate that training-time BB-α alone does not achieve calibration across α ∈ [−2,3], while the proposed post-process methods achieve good calibration for α around 1–1.5 and improve test negative log-likelihood. The results also show a trade-off where negative α can yield higher R² accuracy while α≈1 yields better NLL/calibration, consistent with known α-divergence behavior (mode-seeking vs mass-covering).","The BB-α divergence family is defined as $D_\alpha[p\|q]=\frac{1}{\alpha(1-\alpha)}\left(1-\int p(x)^\alpha q(x)^{1-\alpha}\,dx\right)$ and induces the generalized variational objective $\mathcal{L}_\alpha\approx \mathrm{KL}(q(w|\theta)\|p(w)) - \frac{1}{\alpha}\sum_{(x,y)\in\mathcal{D}}\ln\int q(w|\theta)\,p(y|x,w)^\alpha\,dw$. For regression with full covariance, the Gaussian NLL is $\tfrac12\log|\Sigma|+\tfrac12(y-\mu)^\top\Sigma^{-1}(y-\mu)$, and the proposed anisotropic temperature scaling calibrates covariance via $\Sigma\to L^\top\Sigma L$ (with $L$ lower-triangular) while keeping means fixed unless last-layer fine-tuning (LL/LLμ) is applied.","Across α ∈ [−2,3], the authors report that BB-α used during training does not produce calibrated BNNs (reliability diagrams do not align with the identity and do not hit the “perfect calibration” baseline). In contrast, post-process alpha-based calibration methods achieve good calibration for α roughly in [1, 1.5], and these settings also correspond to improved (lower) test NLL compared with the uncalibrated trained BNN and with beta calibration baseline (β-BNN). Last-layer calibration (LL) is stated to usually outperform β-BNN, LastLayermean (LLμ), and temperature scaling (TS) in terms of test NLL, with the NLL minimum occurring near α≈1 where calibration is best. The R² metric shows the opposite trend: negative α yields higher predictive accuracy (higher R²) even though it worsens NLL/calibration, highlighting a metric-dependent choice of α. Epistemic uncertainty increases with α for last-layer methods (mass-covering behavior for positive α), while TS-only methods do not change epistemic uncertainty (flat curves).",None stated.,"The work is not a reliability engineering study (no failure-time/degradation/maintenance modeling); “reliability diagrams” refer to calibration, not system reliability. Empirical validation is limited to a single (synthetic/simulated) CMB regression dataset and one main architecture (modified VGG with Flipout), so generalization to other domains, data shifts, or time-dependent/serially correlated data is unclear. The paper does not report comprehensive computational cost/latency comparisons (beyond qualitative claims) or provide code to ensure reproducibility and fair benchmarking against alternative modern calibration methods for Bayesian deep learning.",Future work will investigate applying the proposed alpha-based calibration approach to standard UCI datasets for both regression and classification tasks.,"Evaluate robustness under distribution shift and out-of-distribution inputs to see whether calibration gains persist when test data differ from training. Extend the calibration framework to settings with unknown/noisy labels and heteroscedastic, correlated, or temporal data (autocorrelation), and to larger-scale architectures (Transformers) with explicit compute/efficiency analysis. Provide an open-source implementation and standardized benchmarks (ECE/NLL/R², coverage vs interval width, calibration under covariate shift) to enable reproducible comparisons with contemporary Bayesian and non-Bayesian uncertainty methods (ensembles, conformal prediction, Bayesian last-layer Laplace, etc.).",2008.06729v1,https://arxiv.org/pdf/2008.06729v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:47:13Z
TRUE,Life distribution modeling|Maintenance optimization|Failure mode analysis|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Right-censored|Mixture of types|Other,Not applicable,Energy/utilities|Other,Case study (real dataset)|Other,TRUE,R,Not provided,http://www.jstatsoft.org/v43/i09/|http://www.jstatsoft.org/v60/i02/|https://www.R-project.org/,"The paper proposes reliability modeling for downhole safety valves (DHSVs) using a fully parametric non-proportional hazards regression model, the generalized time-dependent logistic (GTDL) model, and extends it by incorporating a multiplicative gamma frailty term to capture unobserved heterogeneity among valves. The method supports right-censored time-to-failure data and can naturally represent a cured/defective fraction when the time-effect parameter implies an improper survival distribution. Inference is performed via maximum likelihood estimation using numerical optimization, and the authors develop model diagnostics including randomized quantile residuals and global influence measures (generalized Cook’s distance and likelihood distance) via case-deletion. The approach is demonstrated on a confidential Petrobras DHSV dataset with heavy censoring (366 observations; 83 failures), where covariates such as H2S concentration, pressure class, manufacturer, operating unit, closed well temperature, water column, and flowing pressure are found associated with time-to-failure. The frailty model is selected for the valve-characteristics group, indicating meaningful unobserved heterogeneity beyond recorded valve descriptors, and influence diagnostics show that certain failure cases can materially change parameter estimates.","The GTDL baseline hazard is defined as $h_0(t\mid x)=\lambda\,\frac{\exp(\alpha t+x^T\beta)}{1+\exp(\alpha t+x^T\beta)}$, yielding reliability $R(t\mid x)=\left(\frac{1+\exp(\alpha t+x^T\beta)}{1+\exp(x^T\beta)}\right)^{-\lambda/\alpha}$. Introducing multiplicative frailty $V\sim\mathrm{Gamma}(1/\theta,1/\theta)$ gives marginal reliability $R(t\mid x)=\left[1+\frac{\lambda\theta}{\alpha}\log\left(\frac{1+\exp(\alpha t+x^T\beta)}{1+\exp(x^T\beta)}\right)\right]^{-1/\theta}$ and marginal hazard $h(t\mid x)=\frac{h_0(t\mid x)}{1+\frac{\lambda\theta}{\alpha}\log\left(\frac{1+\exp(\alpha t+x^T\beta)}{1+\exp(x^T\beta)}\right)}$. They also allow covariate regression in the time-effect parameter via identity link $\alpha(x^*)=x^{*T}\alpha$; estimation uses right-censored likelihood $\prod_i h(t_i)^{\delta_i}R(t_i)$.","On the Petrobras DHSV dataset (n=366), only 83 (22.68%) are observed failures and the remainder are right-censored; the overall mean time is 5.0761 years and the maximum observed time is 28.8 years. For the flow group, a GTDL model (no frailty) identifies H2S (positive effect on hazard) and BSW (negative effect on hazard) as significant at 10% (e.g., $\hat\beta_{H2S}=0.0362$ with 90% CI (0.0224, 0.0500); $\hat\beta_{BSW}=-0.0202$ with 90% CI (-0.0401, -0.0003)). For the valve group, the GTDL gamma frailty model is selected with significant unobserved heterogeneity ($\hat\theta=12.3951$; 90% CI (6.5166, 18.2735)), and several time-effect regressors (pressure class and manufacturer) are significant in the $\alpha(x^*)$ structure. Influence analysis finds multiple influential cases (e.g., for the valve group, Cook’s distance flags 18 cases and likelihood distance flags 9), and removing all influential observations can dramatically change parameter estimates, especially in the frailty model.","The authors state that the DHSV dataset is confidential (Petrobras), limiting disclosure and external reproducibility. They also note substantial missing covariate data: dropping all records with missing values would reduce the dataset to 54 observations, so they fit separate models by covariate group rather than a single multivariate model. They mention that a global proportional hazards test was not performed because of the large amount of missing covariate data.","Modeling each covariate group separately can induce omitted-variable bias and confounding across groups, and may overstate significance when important predictors are excluded from a given group model. The approach assumes IID lifetimes within each fitted model; potential clustering by well/region/operator (shared environment and operating practice) is only indirectly handled and could require shared frailty or multi-level structure for valid inference. Heavy censoring and small effective sample sizes within groups can make MLE-based inference unstable, especially for frailty variance $\theta$ and time-effect regression in $\alpha(x^*)$, and the influence analysis indicates sensitivity to a small number of failures. No publicly released code and limited dataset access make it difficult to verify implementation details (optimization settings, convergence diagnostics, and robustness to starting values).",They propose addressing small-sample issues in these models using Bayesian methods or bias-correction approaches. They suggest extending to shared frailty models by assuming valves installed in the same production regions share the same frailty. They also mention using Principal Component Analysis (PCA) as an alternative modeling strategy to handle covariates and possibly missingness/collinearity.,"A natural extension is a unified multivariable model that simultaneously includes covariates across all groups with principled missing-data handling (e.g., multiple imputation or joint modeling), enabling consistent estimation of combined effects. Robust/penalized estimation (or Bayesian regularization) could reduce sensitivity to influential failures and stabilize frailty variance estimation under heavy censoring. Additional validation on non-confidential benchmark datasets (or a sanitized public subset) would strengthen evidence of generalizability, and simulation studies could quantify bias/variance under varying censoring and frailty levels. Finally, incorporating time-varying covariates and dependence structures (e.g., recurrent interventions/workovers or competing risks) would better reflect operational realities of DHSV reliability.",2008.08197v3,https://arxiv.org/pdf/2008.08197v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:48:04Z
TRUE,Degradation modeling|RUL prediction|Failure mode analysis|Maintenance optimization|System reliability|Software reliability|Network/infrastructure reliability|Other,ML-based|Bayesian|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Event/count data|Mixture of types|Other,Condition-based|Predictive|Not applicable,Energy/utilities|Transportation/logistics|Network/cybersecurity|Healthcare/medical|Other,Other,NA,None / Not applicable,Not applicable (No code used),http://ti.arc.nasa.gov/project/prognostic-data-repository|https://www.cdc.gov/media/releases/2016/p0922-older-adult-falls.html,"This paper is a survey and roadmap of how machine learning (supervised, unsupervised, semi-supervised, and reinforcement learning) is being applied to reliability engineering and safety problems. It organizes the literature by ML task (regression for remaining useful life and degradation prediction; classification for fault detection/diagnosis; clustering and anomaly detection for health monitoring; and reinforcement learning for inspection/maintenance policy optimization), and provides representative references and comparative discussion across many application domains. It highlights deep learning’s advantages for high-dimensional sensor/telemetry data (e.g., CNNs and LSTMs for fault/anomaly detection and RUL) and discusses practical deployment concerns such as false alarms and uncertainty quantification. The paper also outlines future research opportunities including fleet-level PHM, integration with accident/near-miss databases and safety management systems, wearable-sensor predictive safety analytics, and use of advanced probabilistic/generative models (e.g., deep Gaussian processes and GANs). Overall, its contribution is a structured synthesis rather than proposing a new reliability model or deriving new reliability theory.",Not applicable,Not applicable (survey/review article; no single primary experimental result is reported).,"The authors state that the review makes “no claim of exhaustiveness” and that the literature is large and fragmented, so their coverage is a sampled synthesis rather than a complete catalog. They also note that the paper does not address the reverse relationship—i.e., the reliability and safety of ML/AI systems themselves—arguing it deserves separate dedicated treatment.","Because the paper is a broad narrative review, it does not provide a systematic-review methodology (e.g., search protocol, inclusion/exclusion criteria, or bias assessment), which can affect reproducibility and completeness. Many comparative statements (e.g., deep learning “outperforms” shallow ML) are not backed by a standardized benchmark or meta-analysis across common datasets/metrics, so performance conclusions may be context-dependent. The paper also does not provide implementation guidance (data preprocessing standards, labeling practices, handling censoring, uncertainty calibration) that practitioners often need to deploy PHM/maintenance ML reliably.","The authors propose examining the reliability and safety of ML/AI systems (accidents in ML, risk analysis methodologies for AI, and failure mechanisms of ML systems) as future work in a separate publication. They also explicitly highlight multiple forward-looking opportunities for research, including better uncertainty quantification for decision support, fleet-level/system-of-systems PHM, mining accident and near-miss datasets via ML, wearable-sensor predictive safety analytics, and adapting advanced models such as deep Gaussian processes and GANs for reliability/safety tasks.","A valuable extension would be to map common reliability data issues (right-censoring, truncation, covariate shift, rare-event labels) to specific ML techniques and provide recommended evaluation metrics (e.g., time-dependent AUC, calibration, cost-weighted false alarms). Establishing open benchmark suites for reliability/safety (standard datasets, splits, and reporting checklists) would enable fair comparison across methods and reduce publication bias toward positive results. Another promising direction is integrating physics-informed or mechanistic degradation models with deep learning (hybrid digital-twin approaches) while providing calibrated uncertainty for maintenance decision-making.",2008.08221v1,https://arxiv.org/pdf/2008.08221v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:48:34Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Failure mode analysis|Other,ML-based|Bayesian|Other,Sensor/condition monitoring|Degradation measurements|Right-censored|Simulated only|Mixture of types,Condition-based|Predictive|Not applicable,Energy/utilities|Transportation/logistics|Theoretical/simulation only|Other,Case study (real dataset)|Other,TRUE,Other,Not provided,http://arxiv.org/abs/2003.01196|https://doi.org/10.1016/j.compchemeng.2020.106991|https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/#turbofan,"This paper reviews the historical evolution of reliability and maintenance strategies (reactive, preventive, predictive, and “intelligent” maintenance) and proposes an AI/IIoT-centered framework for operationalizing next-generation maintenance analytics. Methodologically, it highlights probabilistic deep learning for reliability modeling and demonstrates a Bayesian recurrent neural network (BRNN) with variational dropout to produce probabilistic failure predictions (with uncertainty bands) from multivariate time-series sensor data. In a case study on NASA’s C-MAPSS turbofan engine degradation benchmark, the task is framed as binary classification of whether failure will occur within the next 30 days (a warning window), using sequences of sensor and operating-condition variables. The BRNN is compared qualitatively to a standard RNN and is reported to yield more robust, better-calibrated failure probability trajectories near the warning window and to provide uncertainty estimates useful for downstream decision-making. Practically, the paper emphasizes end-to-end enablement (wireless sensors/IIoT, big data platforms, CI/CD, and field mobility/AR/VR) for deploying and maintaining such models in production environments.","The proposed reliability modeling uses a Bayesian recurrent neural network via variational dropout: dropout is applied during both training and inference to approximate Bayesian model averaging. At prediction time, multiple stochastic forward passes (with different dropout masks) generate samples from the predictive distribution $p(\hat{y}\mid x)$, from which summary statistics (median and percentile bands, e.g., 10–90% and 25–75%) are computed for the probability of failure. The network is a 2-layer LSTM (100 and 50 units) followed by a dense output layer producing a failure probability for the 30-day warning window classification.","On the C-MAPSS turbofan degradation dataset (simulation; sequences truncated before failure but with recorded time-to-failure), the BRNN produces a predictive distribution over failure probability and shows high predicted probability as engines enter the 30-day-to-failure window. Compared with a standard RNN of similar architecture, the BRNN’s predicted failure probabilities are described as more consistent across engines and less prone to premature probability increases outside the 30-day window, which would otherwise drive more frequent maintenance actions. Results are presented via example trajectories and aligned plots over multiple engines (with uncertainty percentile bands for BRNN). No numeric metrics (e.g., AUC, precision/recall, Brier score) are provided in the extracted text.","The authors note the case study relies on labels indicating when failure occurred; learning/inferencing maintenance need with only a few examples or without allowing run-to-failure remains an open research topic. They also state that predicting maintenance need is only the first step because real maintenance decisions must incorporate logistics and parts availability via scheduling/optimization. Finally, they acknowledge that neural networks are often opaque and that interpretability (understanding “why” a model recommends action) is crucial and still an active research area.","The evaluation appears largely qualitative (plots) and limited to a single benchmark dataset; lack of standard quantitative performance and calibration metrics makes it difficult to assess practical benefit and uncertainty quality. The formulation is a fixed 30-day binary warning window; conclusions may not generalize to other horizons, cost structures, or decision policies (e.g., optimal thresholding under asymmetric costs). The dataset is simulated (C-MAPSS) and may not reflect sensor noise, missingness, maintenance-record errors, and distribution shift typical of real IIoT deployments; robustness to these issues is not demonstrated. Implementation details mention Keras/TensorFlow but reproducibility is limited because code, hyperparameters beyond dropout rates/architecture, and train/test protocol specifics are not fully provided.","They suggest exploring approaches that reduce dependence on failure labels, including an increased role for survival analysis models and self-supervised learning methods to learn representations without labels. They also indicate integrating predictive model outputs (and especially uncertainty) into scheduling/optimization to minimize overall risk of failure given logistics and parts availability. They highlight interpretability of neural network guidance as an important area for continued research.","A natural extension is to evaluate the probabilistic predictions with proper scoring rules (e.g., Brier score, log loss) and calibration diagnostics (reliability diagrams) and to compare against classical prognostics baselines (e.g., Cox/time-to-event models, particle filters, HMMs, and remaining-life regression methods). Developing a decision-analytic layer (e.g., Bayesian decision theory) that maps predictive distributions to optimal maintenance actions under explicit cost/downtime models would strengthen the “intelligent maintenance” claim. Testing under realistic deployment conditions—concept drift, missing sensors, intermittent connectivity, and domain shift across fleets/sites—would improve credibility for IIoT use. Providing an open-source reference implementation (or at least pseudocode and full hyperparameter/training details) would enable replication and broader adoption.",2009.00351v1,https://arxiv.org/pdf/2009.00351v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:49:13Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab)|Personal website,https://bitbucket.org/realKD/|https://www.dropbox.com/s/frlv8os29gobcby/rce.zip?dl=0,"This paper studies discovery of causal (interventional) rules/policies from observational data that maximize a target outcome, addressing confounding and overfitting in rule mining. It provides a graphical admissibility criterion (based on back-door blocking conditions and restrictions on edges among actionable, control, outcome, and unobserved variables) under which causal rule discovery is possible from observational data. Methodologically, it proposes a conservative, consistent “reliable” estimator of causal effect that lower-bounds the stratum-wise effect using one-sided normal-approximate binomial confidence bounds and Laplace correction, reducing variance-driven overfitting in effect maximization. It then derives an efficient exact top-k search algorithm using branch-and-bound with a tight optimistic estimator that can be computed efficiently per stratum. Experiments on synthetic data show faster convergence and better recovery of the true causal rule than naive/associational objectives at small sample sizes, and real-data demonstrations (e.g., Titanic) yield interpretable high-effect rules; runtime benchmarks show practical efficiency on many datasets.","Causal rule effect is defined as $e(\sigma)=p(y\mid do(Q_\sigma)) - p(y\mid do(Q_{\bar\sigma}))$, and under admissibility with policy $Q_\sigma(do(x))=p(X=x\mid \sigma=\top, Z=z)$ it reduces to $e(\sigma)=\mathbb{E}[p(y\mid \sigma,Z)]-\mathbb{E}[p(y\mid \bar\sigma,Z)]$ (Eq. 3.1). The naive plug-in estimator replaces probabilities with empirical estimates. The proposed reliable estimator uses a conservative stratum-wise lower bound $\tau(z)=(\hat p_{\sigma,z}-\hat p_{\bar\sigma,z})-\big(\tfrac{\beta}{2\sqrt{n_{\sigma,z}}}+\tfrac{\beta}{2\sqrt{n_{\bar\sigma,z}}}\big)$ (with Laplace-corrected counts), and aggregates $\hat r(\sigma)=\sum_z \tau(z)\hat p(z)$ (Eq. 3.2).","On synthetic data generated from a known causal graph, the reliable estimator has consistently lower mean squared error than the plug-in estimator when used in top-1 rule search (Fig. 4 left), and it recovers the true core rule with probability approaching 1 more rapidly as sample size increases (Fig. 4 right). The paper reports using $\beta=2.0$ (about a 95.45% confidence level) as a default in experiments. A qualitative real-data example on the Kaggle Titanic dataset finds top rules centered on “class ≤ 2 ∧ sex = female” with reliable causal effect around 0.576 and coverage about 0.19 (Table 2). Runtime benchmarks on KEEL datasets show top-k search often finishing in seconds for many datasets, with some harder datasets taking up to roughly an hour for top-1 under chosen approximation factors (Table 3, Fig. 6).","The authors state that causal rule discovery from observational data requires strong assumptions captured by their admissibility criteria; violations of Definition 1 render the causal-effect expression non-causal. They note that in practice the complete causal graph is often unknown, so variable selection relies on pragmatic domain knowledge and may be incomplete. They also highlight that the “no edges between actionable variables” requirement can be a strong assumption for large actionable sets, and naïvely moving remaining actionable variables into controls may violate other criteria and complicate search. They discuss that choosing very high confidence (approaching 100%) is impossible with finite samples and would make the estimator degenerate, implying the need to calibrate $\beta$.","The reliable estimator’s confidence-interval correction uses a normal approximation and a worst-case variance bound, which can be inaccurate for small counts or extreme probabilities; exact/binomial (e.g., Clopper–Pearson) or Bayesian intervals might behave differently. The approach assumes discrete variables and relies on stratification over $Z$; with high-dimensional or continuous confounders, stratification can cause sparsity and unstable estimates, and the Laplace correction may introduce nontrivial bias. The admissibility assumptions (especially no edges among actionable variables and no unobserved confounding into actionable variables) may limit applicability in many real systems where actions are correlated/causally related. Comparisons focus on a small set of alternative objectives; broader baselines from causal inference/uplift modeling (e.g., doubly robust estimators, propensity modeling, causal trees) are not evaluated.","They explicitly suggest calibrating/selecting an “optimal” $\beta$ for a given sample size somewhere below the 100% confidence level, since higher confidence can improve early-sample MSE but $\beta\to\infty$ is not meaningful. They also indicate that removing/relaxing the ‘no edges between actionable variables’ assumption is nontrivial and that incorporating other actionable variables as controls can violate criteria and complicate search, implicitly motivating methodological extensions to handle dependencies among actions.","Extend the method to settings with continuous/high-dimensional confounders using propensity-score or doubly robust estimation rather than pure stratification, improving robustness under sparsity. Develop self-starting/unknown-graph variants that learn or test admissibility assumptions (or sensitivity analyses) and quantify impact of unobserved confounding. Generalize beyond conjunction rules to richer policy classes (e.g., rule lists) while preserving efficient search or providing approximation guarantees. Provide a maintained, easy-to-install implementation (e.g., Python/R package) and standardized benchmarks against modern causal rule/uplift learners on multiple real intervention-like tasks.",2009.02728v2,https://arxiv.org/pdf/2009.02728v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:49:58Z
TRUE,Degradation modeling|Other,Nonparametric/Semi-parametric|Stochastic process|Bayesian|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Simulated only,Not applicable,Environmental monitoring|Theoretical/simulation only|Other,Exact distribution theory|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an iterative, data-driven method to correct multiplicative sensor degradation when multiple sensors observe the same latent ground-truth signal but experience different exposure histories (e.g., a main and back-up radiometer). Degradation is modeled as a continuous, monotonically decreasing function of exposure with the anchor constraint d(0)=1, and is learned from ratios of contemporaneous measurements; robust fitting is done using (smoothed) monotonic regression, with exponential-family alternatives also discussed. For a noiseless multiplicative measurement model, the authors prove convergence of the iterative correction (CORRECTONE; and similarly CORRECTBOTH in an appendix) to the true ground-truth signal and degradation function under general exposure-function conditions. After correction, they fuse the (noisy) corrected multi-sensor time series using Bayesian Gaussian-process regression, extending the observation-noise term to allow distinct noise variances per sensor and using sparse variational GP approximations for scalability. The approach is demonstrated on synthetic data (Brownian-motion ground truth) showing corrected signals approach the ground truth and GP fusion yields smooth posterior estimates with uncertainty bands, while noting correction amplifies noise at high exposure (division by d<1).","Measurement models: noiseless a(t)=s(t)·d(e_a(t)), b(t)=s(t)·d(e_b(t)); noisy adds ε_a(t)~N(0,σ_a^2), ε_b(t)~N(0,σ_b^2). Iterative correction (CORRECTONE): r_n(e_a(t))=a_0(t)/b_n(t), then a_{n+1}(t)=a_0(t)/r_n(e_a(t)) and b_{n+1}(t)=b_0(t)/r_n(e_b(t)); final correction uses a_c(t)=a(t)/d_c(e_a(t)), b_c(t)=b(t)/d_c(e_b(t)). Degradation-function fitting enforces monotone decrease and d(0)=1 via isotonic/smoothed monotonic regression: minimize Σ(θ_i−y_i)^2 + Σ λ_i(θ_{i+1}−θ_i)^2 subject to θ_i≥θ_{i+1} and optionally θ_1=1. Data fusion: s(t)~GP(0,k(t,t′)) with sensor-specific noise via diag(σ^2) (σ_i depends on the sensor that produced observation i), implemented with sparse variational GP lower bound using inducing points (complexity O(n m^2)).","For the noiseless model, the authors prove that CORRECTONE converges pointwise to the ground-truth signal s(t) for both sensors and that the learned ratio r_n(t) converges to the true degradation function d(t), first for simple exposure mappings (e_b(t)=t^2 and more generally t^k) and then for general exposure functions satisfying e_b(t)<e_a(t) and an invertible e_a. They report empirical convergence in practice for both CORRECTONE and CORRECTBOTH, with CORRECTONE observed to converge slightly faster. Synthetic experiments show corrected signals align with the known ground truth and the ratio of corrected signals approaches 1, while the estimated degradation curve is monotone decreasing; they also observe that correction amplifies additive noise as exposure increases (since dividing by d<1). In sparse GP fusion experiments, varying the number of inducing points (e.g., m in {100,300,500}) trades off smooth long-term trend capture vs. ability to represent fast fluctuations, with m=500 producing a close fit but still somewhat smoother than the ground truth.","The convergence proofs are provided only for the noiseless multiplicative degradation model; the authors state they could not extend the convergence analysis to the noisy additive-Gaussian case because the expectation of a reciprocal Gaussian random variable does not exist. They also note that degradation correction amplifies measurement noise over time (division by values smaller than 1), which can affect the corrected series before fusion.","The method relies on accurate exposure functions (e_a, e_b) supplied by an expert (here based on cumulative sums); misspecification of exposure could bias degradation estimates and convergence behavior. The approach assumes identical degradation-rate behavior across sensors (same d(·) applied to both, only time-warped by exposure), which may not hold if sensors age differently or have different degradation mechanisms. The ratio-based fitting can be unstable when the denominator signal is noisy or near zero, and robustness to outliers, autocorrelated noise, or non-Gaussian noise is not systematically evaluated. Empirical evaluation is limited to synthetic data; no real radiometer/field dataset results are shown, so practical performance with gaps, drifts, and calibration events remains uncertain.","They explicitly leave as future work the theoretical analysis of convergence of the iterative correction method in the presence of measurement noise (i.e., extending results beyond the noiseless model).","Validate the correction and fusion pipeline on real-world multi-sensor datasets (e.g., radiometers/space instruments) with known calibration events and missing-data patterns, and benchmark against established degradation-correction methods. Develop robust/self-starting variants that handle unknown initial calibration (relaxing strict reliance on d(0)=1), outliers, and non-Gaussian or autocorrelated noise, potentially via Bayesian degradation-process models. Extend to sensor-specific degradation functions (heterogeneous d_i) and to multivariate/high-dimensional signals with cross-sensor correlations and shared latent structure. Provide an open-source reference implementation (including monotone-regression fitting and sparse multi-noise GP fusion) to improve reproducibility and facilitate adoption.",2009.03091v1,https://arxiv.org/pdf/2009.03091v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:50:38Z
TRUE,Degradation modeling|Maintenance optimization|Other,Stochastic process|Bayesian|Simulation-based|Other,Degradation measurements|Right-censored|Simulated only|Other,Condition-based|Predictive|Imperfect maintenance|Other,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a joint framework for optimal inspection and maintenance (I&M) planning of deteriorating structural components by combining Dynamic Bayesian Networks (DBNs) for probabilistic deterioration/observation modeling with Partially Observable Markov Decision Processes (POMDPs) for stochastic optimal control under partial observability. Two DBN formulations are described (parametric with time-invariant parameters and a deterioration-rate model), and the authors show how to derive the POMDP transition and observation models directly from these DBNs and formulate both infinite- and finite-horizon POMDPs for life-cycle cost minimization in a structural reliability context. A fatigue crack-growth case study (Paris-law-based, discretized) is used to compare point-based POMDP solvers (e.g., SARSOP, FRTDP, Perseus) against common risk-based inspection heuristics (equidistant inspections and failure-probability-threshold inspections). Across multiple cost regimes and a more detailed setting with multiple inspection types and repair actions, POMDP policies consistently achieve lower expected life-cycle costs than heuristic policies, sometimes substantially, while remaining computationally tractable with point-based solvers. The work clarifies misconceptions that POMDPs cannot handle deterioration models with time-invariant parameters by using state augmentation and demonstrates how POMDP policies can reveal richer decision patterns (e.g., re-inspect after detection before repairing).","The deterioration example uses a Markovian Paris-law-derived crack growth update: $d_{t+1}=\left[\left(1-\frac{m}{2}\right) C_{FM} S_R^{m}\pi^{m/2} n + d_t^{1-m/2}\right]^{2/(2-m)}$. Structural failure is defined via a limit state $g_F(t)=d_c-d(t)$ with $P_F(t)=P\{g_F(t)\le 0\}$ and reliability index $\beta(t)=-\Phi^{-1}(P_F(t))$; inspection updating is expressed through conditional failure probability $P_{F\mid I_1,\ldots,I_N}(t)$. The POMDP belief update is $b'(s')\propto P(o\mid s',a)\sum_{s\in S}P(s'\mid s,a)b(s)$, with costs/rewards defined over belief via $R(b,a)=\sum_s b(s)R(s,a)$.","In the traditional setting (one inspection type, perfect repair), finite-horizon POMDP (SARSOP) yields lower expected total cost than optimized heuristics by about 11–37% depending on the cost regime (e.g., for RR/I20–RF/R100: 58.35 vs 65.62–69.17; for RR/I50–RF/R20: 12.45 vs 16.69–17.06). In a detailed setting with two inspection types and two repair actions, finite-horizon POMDP achieves 12.26 expected cost versus 13.66–18.08 for the best tested heuristic variants (about 11–47% higher than POMDP). Discretization sensitivity shows a deterioration-rate DBN/POMDP with 930 states achieves much smaller failure-probability error than parametric models of comparable size; parametric models needed up to 16,000 states to reach similar error magnitude (e.g., $\xi\approx 4.3\times 10^{-4}$ at 16,000 states vs $\xi\approx 2.1\times 10^{-4}$ at 930 states for the deterioration-rate model). The authors report that the 14,880-state finite-horizon POMDP can outperform heuristic methods in under about a second of computation time in their experiments (notably with SARSOP).","The authors state that a main limitation of the presented approaches (including POMDPs) is the rapid increase in computational complexity for very large state and action spaces, such as multi-component systems, due to the curse of dimensionality. They also note DBNs with large state spaces face similar constraints, and emphasize that discretization choices affect both fidelity and tractability, motivating better discretization methods and continuous-state approaches.","The numerical validation is limited to a single-component fatigue crack-growth example with discretized state/observation spaces; results may not generalize to strongly correlated multi-component systems, complex structural system topologies, or field data with model mismatch. Comparisons to heuristics depend on the particular heuristic families explored; more sophisticated adaptive policies (e.g., approximate dynamic programming, robust MPC-style policies, or tuned influence-diagram optimization) could narrow the gap. The framework assumes a known, correctly specified transition/observation model (estimated from simulations); sensitivity to model-form uncertainty, parameter drift, and nonstationary environments beyond the encoded deterioration-rate construction is not fully characterized. Practical deployment issues (e.g., calibration of PoD/PoI curves from real inspection programs, organizational constraints, and action feasibility constraints) are not deeply treated.","The authors suggest further work on (i) continuous state-space POMDPs and (ii) optimal discretization schemes for discrete-state models. They also propose pursuing POMDP-based deep reinforcement learning (actor–critic/multi-agent DRL) to mitigate the curse of dimensionality and handle large multi-component systems with large state, action, and observation spaces.","Extending the framework to explicit system reliability with component interactions (load redistribution, common-cause deterioration, shared inspections) and to multivariate/high-dimensional condition monitoring signals would better match real infrastructure portfolios. Incorporating epistemic uncertainty (model discrepancy) via robust or Bayesian POMDP formulations, and studying policy sensitivity to PoD/PoI miscalibration, would improve field reliability. Developing open-source implementations and benchmarking against alternative approximate solvers (e.g., PBVI variants, belief-space MPC, hierarchical POMDPs) on standardized datasets would help reproducibility and adoption. Integrating constraints (crew logistics, budget limits, regulatory thresholds) and multi-objective utilities (safety, environment) explicitly into the optimization could broaden practical applicability.",2009.04547v2,https://arxiv.org/pdf/2009.04547v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:51:24Z
FALSE,NA,ML-based|Hybrid/Ensemble|Simulation-based|Other,Simulated only|Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,http://jmlr.org/papers/v15/srivastava14a.html,"The paper proposes a Multi-Method Ensemble (MME) framework to improve the reliability of post-hoc feature-importance explanations for machine learning models used in safety-critical decision support. The framework trains multiple ML models (RF, GBT, SVR, DNN) and computes feature importance using multiple model-agnostic/attribution methods (Permutation Importance, SHAP, and Integrated Gradients), then fuses the resulting importance vectors via ensemble decision rules. A novel fusion strategy, RATE, is introduced; it uses pairwise rank-correlation significance tests (p<0.05) plus majority vote to discard disagreeing importance vectors before averaging. Evaluation is performed on synthetic datasets with known ground-truth importance under varying noise, number of features, and percentage of informative features; majority-vote fusion in MME yields about 15% lower feature-importance error than existing approaches. Results indicate noise level has minimal effect on importance error, while error increases with more features and more informative (non-zero) features.","Permutation importance defines feature score as the performance drop after permuting feature $i$: $FI_i = Loss(\hat{y}_{perm(i)},y)-Loss(\hat{y},y)$. SHAP is defined via weighted marginal contributions over all coalitions: $\phi_x(i_0)=\sum_k w_k\,MC_{x,k}(i_0)$ with $MC$ as differences between predictions with/without feature $x$. Integrated Gradients is computed as $IG_i^f(x,b)=(x_i-b_i)\int_{0}^{1}\frac{\partial f(b+\alpha(x-b))}{\partial x_i}\,d\alpha$; RATE then filters importance vectors based on statistically significant rank correlations (p<0.05) and majority voting before averaging.","Across all datasets, the proposed MME framework reduces feature-importance error versus single-method ensembles; the authors report ~15% less feature-importance error overall compared to existing methods. For noise-level experiments (std 0,2,4), MAE for MME+majority vote is 8.1, 8.6, 8.6 (×10^-2) versus SME-SHAP 9.8, 9.7, 10.0 and SME-PI 10.1, 9.8, 10.7. For informative-level experiments, MME+majority vote achieves notably lower error at high informative levels (e.g., MAE 10.3 at 80% and 12.7 at 100% ×10^-2) compared with the best SME results (PI 13.8 and 16.5 respectively). The study concludes noise has little impact on importance error, whereas increasing the number of features and informative features increases error.","The authors note they keep ML hyperparameters constant rather than optimizing per dataset, and state that hyperparameter optimization may affect feature-importance estimates and should be investigated. They also acknowledge that additional factors (covariate drift on test data, imbalanced datasets) and evaluation on real-world complex data were not included. They further mention the framework could be improved by incorporating more ML models and more feature-importance methods.","The work is not a reliability engineering study (no failure/degradation/lifetime modeling), so its relevance to reliability is indirect and limited to “trustworthy ML” in safety-critical contexts. The evaluation uses only synthetic tabular regression-style data with known ground truth, so real-world issues (feature dependence/collinearity, distribution shift, measurement error structures, temporal autocorrelation, causal confounding) may substantially change method behavior. Comparisons focus on PI/SHAP/IG and simple fusion rules; important alternatives (e.g., SAGE, conditional permutation importance, model-specific attributions, counterfactual explanations, stability/robustness metrics) are not benchmarked. RATE’s statistical testing/multiple-comparison aspects (many pairwise correlations) are not discussed; without correction this could affect filtering decisions and robustness claims.","The authors propose adding more feature-importance methods and ML models to the MME framework to study their impact on variance and consensus. They also suggest integrating the feature-importance ensemble into a neural network to bias attention toward relevant features, potentially reducing training time and improving accuracy. They recommend studying additional factors such as hyperparameter optimization, dataset imbalance, anomalies/outliers, and validating the framework on real-world complex datasets.","Test the fusion framework under realistic safety-critical data properties such as temporal dependence, sensor drift, and dataset shift, and quantify explanation stability under resampling/perturbations as a primary metric. Extend fusion to account for correlated features (e.g., conditional permutation, grouped attributions) and provide uncertainty intervals for fused importance. Provide open-source implementations and reproducible experiment scripts, plus standardized benchmarks, to enable independent validation. Investigate decision-theoretic or Bayesian fusion methods that explicitly model each explainer’s bias/variance and can adaptively weight or reject explainers per dataset/model.",2009.05501v1,https://arxiv.org/pdf/2009.05501v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:52:15Z
FALSE,NA,ML-based,Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,Python,Not provided,https://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits|https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits|https://archive.ics.uci.edu/ml/datasets/SPECTF+Heart|https://archive.ics.uci.edu/ml/datasets/Wine/|https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)|https://cs.nyu.edu/~roweis/data.html|https://archive.ics.uci.edu/ml/datasets/Haberman’s+Survival|https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic),"The paper proposes m-arcsinh, a modified inverse hyperbolic sine function, intended to serve both as an SVM kernel and an MLP activation function in the scikit-learn ecosystem. It evaluates baseline SVM (SVC) and MLPClassifier models using m-arcsinh versus standard kernels/activations (linear, poly, RBF, sigmoid; identity, logistic, tanh, ReLU) across 15 classification datasets from scikit-learn and the UCI ML repository. Performance is assessed using accuracy and “reliability” metrics defined as weighted precision, weighted recall, and weighted F1-score, along with training time on a specified CPU/RAM setup. Results claim m-arcsinh is generally competitive, often ranking best or second-best on these metrics for several datasets, and frequently providing faster training times for SVM compared with some standard kernels. The work is about ML function design and benchmarking rather than reliability engineering (no failure/life/maintenance modeling).","The proposed function is defined as $\mathrm{m\text{-}arcsinh}(x)=\mathrm{arcsinh}(x)\cdot\frac{1}{12}\sqrt{|x|}$. Its derivative is given as $\frac{\sqrt{|x|}}{12\sqrt{x^2+1}}+\frac{x\,\mathrm{arcsinh}(x)}{24\,|x|^{3/2}}$. For SVM usage, the kernel is implemented via an inner product between transformed inputs: $K(X,Y)=\langle \phi(X),\phi(Y)\rangle$ where $\phi(\cdot)=\frac{1}{3}\,\mathrm{arcsinh}(\cdot)\cdot\frac{1}{4}\sqrt{|\cdot|}$.","Across 15 datasets, the paper reports m-arcsinh achieving strong weighted F1/accuracy in many cases; e.g., on Breast Cancer Wisconsin (diagnostic), SVM with m-arcsinh attains accuracy/F1 of 0.97 with 0.007 s training time, while SVM-RBF shows 0.92/0.92 at 0.017 s and SVM-sigmoid shows 0.39/0.21 at 0.012 s. On OptDigits, SVM m-arcsinh reaches 0.97 accuracy/F1 at 0.232 s (vs RBF 0.98 at 0.525 s), and MLP m-arcsinh reaches 0.98 at 53.586 s (similar accuracy to other activations but slower than identity at 9.572 s). The author summarizes that for MLP, m-arcsinh is best on 10/15 datasets and second best on 4/15, and for SVM it is fastest on 7/15 datasets with best classification performance on 2/15 datasets. Some configurations are reported as non-convergent (e.g., SVM polynomial on the heart-failure dataset).",None stated.,"The paper’s use of the term “reliability” refers to classification metrics (weighted precision/recall/F1) rather than reliability engineering concepts (failure/degradation/survival), limiting relevance to reliability practice. Benchmarking is done with fixed, non-optimized hyperparameters and non-randomized splits in several datasets (shuffle=False), so results may not be robust to tuning or resampling variability. It does not provide statistical significance testing, confidence intervals, or repeated runs to assess variability given random initialization (notably for MLP). Code snippets are shown, but there is no complete, versioned, runnable repository or integration evidence within official scikit-learn releases.",Future work involves adapting the m-arcsinh function to benefit deep neural networks too.,"Provide an official, reproducible implementation (e.g., a pip-installable package or scikit-learn-compatible extension) with end-to-end scripts and exact environment/version pins to support replicability. Evaluate robustness under hyperparameter optimization, multiple random seeds, and cross-validation with statistical comparisons against strong baselines. Analyze theoretical properties required for valid SVM kernels (e.g., positive semi-definiteness) and empirically test kernel matrix PSD behavior across datasets. Extend evaluation beyond classification accuracy/F1 to calibration, robustness to class imbalance/noise, and computational scaling with dataset size/features.",2009.07530v1,https://arxiv.org/pdf/2009.07530v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:52:36Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes Ask-n-Learn, a batch active learning method for deep image classification that selects query samples using gradient embeddings computed from pseudo-labels. The key contribution is to improve the reliability of these gradient representations by training calibrated classifiers using either Variance Weighted Confidence Calibration (VWCC) or a new Likelihood Weighted Confidence Calibration (LWCC), and to reduce confirmation bias by using data augmentation to refine pseudo-labels when confidence is low. The method then applies K-means++ seeding on gradient embeddings to select a diverse batch, capturing both uncertainty (via gradient norms) and diversity (via directions). Experiments on MNIST, Fashion-MNIST, SVHN, and CIFAR-10 show improved sample efficiency (higher accuracy with fewer labeled points) and better probabilistic calibration (lower ECE/NLL/Brier) compared with Random, Entropy, Confidence, and BADGE. Robustness experiments with noisy oracles (10% and 20% label corruption) indicate Ask-n-Learn (especially VWCC) degrades less than baselines and can provide substantially higher accuracy early in the labeling process.","The calibrated training objective is $\hat{\theta}=\arg\min_\theta \ell_f=\ell_{ce}+\lambda\,\ell_{calib}$, where $\ell_{calib}$ is instantiated as VWCC or LWCC. VWCC uses stochastic inferences to estimate per-sample variance $\alpha_i$ and applies label smoothing toward a uniform distribution using a KL term; LWCC uses a likelihood/confidence-derived weight $\beta_i$ to penalize overconfident predictions via $\mathrm{KL}(U(y)\|p(\hat y_i\mid x_i))$. Gradient embeddings for an unlabeled point use the last-layer gradient: $(g_x)_i=(p_i-\mathbb{I}(\hat y=i))\,z(x;\theta\setminus\theta_o)$, and K-means++ is applied to these embeddings for batch selection.","Across MNIST, Fashion-MNIST, SVHN, and CIFAR-10, Ask-n-Learn (VWCC/LWCC) consistently improves active learning convergence relative to Random, Entropy, Confidence, and BADGE, while also reducing calibration error (ECE), NLL, and Brier score. On CIFAR-10, the paper reports achieving about 70% test accuracy with roughly 10K labeled samples using the VWCC variant, whereas baselines need nearly double the sample size to reach similar accuracy. In noisy-oracle experiments on CIFAR-10 with 10% and 20% label corruption per iteration, Ask-n-Learn—particularly VWCC—shows markedly better robustness and is reported to exceed baselines by more than 10% accuracy in early iterations. LWCC is noted to converge faster than VWCC on SVHN, while VWCC is generally superior on the other datasets.",None stated.,"The paper’s focus is active learning and prediction calibration for deep classifiers rather than reliability engineering (failure/degradation/maintenance), so its “reliability” is limited to probabilistic calibration and robustness to label noise. Performance is demonstrated on standard image benchmarks; generalization to other modalities, heavy class imbalance, distribution shift, or non-i.i.d. data is not established. The paper does not provide an exact computational cost analysis for VWCC (which relies on multiple stochastic inferences) or for repeated retraining-from-scratch each AL round, which may limit practical deployability.",None stated.,"Evaluate Ask-n-Learn under dataset shift/OOD conditions (beyond label-noise), since calibration and acquisition quality can degrade substantially under shift. Develop a more compute-efficient variant that avoids retraining from scratch each round and reduces the number of stochastic forward passes needed by VWCC. Provide a public implementation and standardized benchmarking across more datasets/architectures (including class-imbalanced settings) to improve reproducibility and clarify when VWCC vs. LWCC is preferable.",2009.14448v1,https://arxiv.org/pdf/2009.14448v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:53:04Z
FALSE,NA,Bayesian|ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/thudzj/ScalableBDL,"The paper proposes BayesAdapter, a framework that adapts a pre-trained deterministic neural network into a variational Bayesian neural network (BNN) via inexpensive Bayesian fine-tuning rather than training the BNN from scratch. It introduces modularized stochastic variational inference implementations for two variational families: mean-field Gaussian (MFG) and a parameter-sharing ensemble (PSE) variational that approximates a mixture/ensemble while sharing parameters to reduce cost. To improve stability and efficiency, it refurbishes exemplar reparameterization by parallelizing exemplar-wise computations (e.g., via grouped convolutions), providing a general-purpose gradient-variance-reduction approach. Experiments on CIFAR-10, ImageNet, and face recognition show improved predictive performance and calibration/uncertainty quality versus baselines such as MAP fine-tuning, Laplace approximation, MC dropout, SWAG, and from-scratch variational BNNs, with substantially lower variational training overhead than from-scratch VI. The work positions Bayesian fine-tuning as a practical path to scalable uncertainty-aware deep learning, addressing adoption concerns about BNN accessibility and reliability (calibration) rather than engineering reliability modeling.","Bayesian fine-tuning maximizes the ELBO: $\max_{\theta}\; \mathbb{E}_{q(w\mid\theta)}[\frac{1}{n}\sum_i \log p(y^{(i)}\mid x^{(i)};w)] - \frac{1}{n}D_{\mathrm{KL}}(q(w\mid\theta)\|p(w))$. Prediction uses Monte Carlo posterior predictive averaging: $p(y\mid x,D)\approx \frac{1}{S}\sum_{s=1}^S p(y\mid x; w^{(s)})$, $w^{(s)}\sim q(w\mid\theta)$. Exemplar reparameterization estimates the minibatch expected log-likelihood with per-example sampled weights: $L^*_\ell=\frac{1}{\lvert B\rvert}\sum_{i=1}^{\lvert B\rvert}\log p(y^{(i)}\mid x^{(i)}; w^{(i)})$, $w^{(i)}\stackrel{i.i.d.}{\sim} q(w\mid\theta)$. For MFG with isotropic Gaussian prior, complexity-loss gradients are analytic: $\nabla_{\mu}L_c=-\lambda\mu$ and $\nabla_{\psi}L_c=-\lambda\exp(2\psi)+\frac{1}{n}$ with $\lambda=1/(\sigma_0^2 n)$.","On CIFAR-10 (WRN-28-10), BayesAdapter achieves 97.10±0.03% (MFG) and 97.13±0.03% (PSE) accuracy, improving over MAP (96.92%) and providing better NLL than MAP (0.1312) with BayesAdapter(PSE) at 0.0936±0.0010; Deep Ensemble remains the best accuracy at 97.40% but at higher cost. On ImageNet (ResNet-50), BayesAdapter improves over MAP (76.13%, NLL 0.9618) to 76.45±0.05% (MFG, NLL 0.9303±0.0005) and 76.80±0.03% (PSE, NLL 0.9159±0.0010). Calibration (ECE) improves notably: on CIFAR-10 BayesAdapter(PSE) reaches 0.0058 vs MAP 0.0198, and on ImageNet BayesAdapter(PSE) reaches 0.0129 vs MAP 0.0373. The authors report exemplar reparameterization reduces gradient variance by about 100× compared to vanilla reparameterization in a toy conv-layer study, with runtime comparable to local reparameterization/Flipout and lower memory than those methods in their ImageNet/ResNet-50 profiling.","The authors state that practitioners may need to carefully tune the optimization configurations for Bayesian fine-tuning to achieve reasonable performance. They also note they leave joint optimization of mixture component covariances (e.g., $\Sigma^{(c)}$ in the mixture/ensemble variational view) for future investigation, effectively limiting current PSE to near-delta components with fixed small variance.","The work frames ""reliability"" primarily as predictive calibration/uncertainty quality, not reliability engineering (failure/lifetime) and does not validate decisions in safety-critical operational settings beyond benchmark metrics like ECE and OOD confidence. Comparisons are not fully symmetric in compute: some baselines (e.g., Deep Ensemble, SWAG) are acknowledged as not scaled to ImageNet due to resource constraints, which can limit conclusions about large-scale competitiveness. The approach depends on having strong pre-trained deterministic checkpoints; performance and practicality may degrade when such checkpoints are unavailable or when transfer/fine-tuning is constrained (domain shift, limited labels). Uncertainty evaluation is largely metric-based (ECE, NLL, confidence plots) without deeper analysis of robustness under dataset shift types beyond the chosen corruption/OOD datasets, nor formal guarantees of posterior approximation quality.","They suggest applying BayesAdapter to additional scenarios such as contextual bandits. They also mention leaving the joint optimization of ensemble/mixture covariances (optimizing both $\Sigma^{(c)}$ and $w^{(c)}$ rather than fixing $\Sigma^{(c)}$) for future investigation, and note that alternative divergence estimators (e.g., quasi-KL for delta mixtures) could be explored. The appendix also states that explaining the learned posterior variance structure is left as future work.","Extending BayesAdapter to handle non-i.i.d. data (temporal dependence/autocorrelation) and systematic dataset shift could improve practical uncertainty reliability beyond calibration on standard benchmarks. Developing self-tuning or adaptive schemes for the key hyperparameters (e.g., prior scale/weight decay and fine-tuning schedules) could reduce the stated sensitivity and improve usability. More rigorous large-scale comparisons under matched compute/memory budgets (including scaled Deep Ensembles/SWAG variants) and broader OOD benchmarks would strengthen empirical claims. Providing a formal or empirical study of posterior quality diagnostics (e.g., coverage, selective prediction risk-control, decision-theoretic utility) would better connect uncertainty estimates to operational reliability of decisions.",2010.01979v5,https://arxiv.org/pdf/2010.01979v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:54:00Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Other,Not applicable,Healthcare/medical|Environmental monitoring,Other,TRUE,SAS|Other,Not provided,https://osf.io/eh68y/|https://arxiv.org/abs/1808.04408|https://www.cdc.gov/air/infographics/air-quality-flag-program.htm|https://cfpub.epa.gov/ncea/isa/recordisplay.cfm?deid=347534#tab-3|https://www.who.int/airpollution/ambient/health-impacts/en/|https://www.cdc.gov/mmwr/preview/mmwrhtml/ss5608a1.htm|https://www.cdc.gov/asthma/nhis/default.htm|https://gispub.epa.gov/air/trendsreport/2018/#highlights|www.cochrane-handbook.org|www.cochrane-handbook.org|nas.org/reports/the-irreproducibility-crisis-of-modern-science,"This paper evaluates the statistical reliability of a published environmental-epidemiology meta-analysis (Anderson et al.) linking early-life ambient NO2 and PM2.5 exposure to later asthma incidence, focusing on bias from multiple testing and multiple modeling (MTMM) in the base cohort studies. The authors estimate each base paper’s “analysis search space” as (outcomes × predictors × lags) × 2^k (k covariates), finding very large implied numbers of potential tests (median 13,824; IQR 1,536–221,184; range 96–42,000,000), suggesting reported effect estimates are unlikely to be unbiased inputs to meta-analysis. They also compute p-values from reported effect estimates and confidence intervals and use Schweder–Spjøtvoll p-value plots to assess whether the set of p-values is consistent with a true signal, a true null, or a mixture/selection effects. For NO2 the p-value plot appears bi-linear (interpreted as a two-component mixture consistent with heterogeneity and/or selective reporting/p-hacking), while PM2.5 p-values align with a ~45° line consistent with randomness (no association). The authors conclude the meta-analytic claim is unreliable and unlikely to replicate absent bias, and propose analysis search space counting and p-value plots as general tools to critique reliability of observational-study meta-analyses.","Analysis search space is defined as: Questions = Outcomes × Predictors × Lags; Models = 2^k where k is the number of covariates (each covariate can be included/excluded); Search Space = Questions × Models. P-values are computed from effect estimates and (assumed symmetric) confidence intervals (implemented in JMP) and then sorted and plotted against rank; under a true null, sorted p-values should lie near a 45° line, whereas mixtures can produce a bilinear pattern.","Across 19 base papers (14 cohorts), the implied number of possible statistical tests is large: median Search Space 13,824 with IQR 1,536–221,184 and maximum 42,000,000. Anderson et al.’s reported random-effects meta-analysis results were EE=1.15 (95% CI 1.06–1.26) per 10 μg/m^3 for NO2 (13 cohorts) and EE=1.16 (95% CI 0.98–1.37) per 10 μg/m^3 for PM2.5 (5 cohorts). The p-value plot for the 13 NO2 cohort estimates shows a two-component mixture with several small p-values and the remainder behaving approximately uniform, which the authors argue invalidates averaging across studies. All five PM2.5 p-values fall approximately on a 45° line, consistent with complete randomness rather than a true association.",None stated.,"The paper critiques reliability of an existing meta-analysis but does not quantify how much selection/MTMM bias would be needed to generate the observed p-value patterns, nor does it apply formal multiple-testing adjustments or selection models (e.g., p-hacking/selection-function models) to estimate corrected effects. The “analysis search space” metric is a coarse lower-bound proxy; it can over/understate effective researcher degrees of freedom because not all combinations are realistically attempted or independent. The p-value plot is descriptive and, with only 13 (NO2) and 5 (PM2.5) p-values, inference about mixture vs. signal is sensitive to small-sample variability and to assumptions used to back-calculate p-values from CIs.",The authors suggest that analysis search space estimation and p-value plots can be used by other researchers to independently examine the reliability of research claims made in meta-analyses in the biomedical sciences and to help judge whether a meta-analysis will replicate.,"A natural extension would be to pair p-value plots with formal publication-bias/selection models (e.g., selection functions, p-curve/p-uniform, robust Bayesian meta-analysis) to estimate bias-adjusted pooled effects and uncertainty. Another direction is to operationalize analysis search space using reproducible workflow audits (pre-registrations, code review, multiverse analyses) to better approximate the actually explored model set rather than potential combinations. Applying these tools across a benchmark set of environmental-epidemiology meta-analyses, and validating conclusions against later replications or large preregistered studies, would strengthen generalizability.",2010.10922v1,https://arxiv.org/pdf/2010.10922v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:54:46Z
FALSE,NA,ML-based,Other,Not applicable,Manufacturing (general)|Semiconductor/electronics|Healthcare/medical|Finance/economics|Transportation/logistics|Other,Simulation study|Other,TRUE,Python,Personal website,https://www.daml.in.tum.de/dbu-robustness,"This paper studies the robustness of Dirichlet-based uncertainty (DBU) models (e.g., PriorNet, PostNet, DDNet, EvNet) under adversarial perturbations, focusing on whether their predictive uncertainty remains trustworthy. It evaluates three uncertainty-reliant tasks under attacks: (1) using uncertainty to separate correct vs. incorrect predictions, (2) detecting adversarial examples (label attacks), and (3) out-of-distribution (OOD) detection. The authors introduce “uncertainty attacks,” which directly optimize perturbations to manipulate uncertainty measures (e.g., differential entropy) to make ID look OOD and vice versa, and report large-scale experiments across many datasets/attack settings. Results show DBU uncertainty estimates are generally not robust for these tasks under strong attacks (notably PGD), often collapsing toward near-random detection performance as perturbation radius increases. They propose median smoothing to improve robustness and provide certificates (bounds) on smoothed uncertainty measures; adversarial training yields only minor additional gains relative to smoothing.","DBU models output Dirichlet parameters $\alpha(x)\in\mathbb{R}_+^C$ and derive mean class probabilities $\bar p_c=\alpha_c/\alpha_0$ with $\alpha_0=\sum_c \alpha_c$. Label attacks maximize cross-entropy under a norm-ball constraint: $\tilde x\approx\arg\max_{\|x-\tilde x\|\le r} \mathrm{CE}(p(x),y)$. Uncertainty attacks instead maximize an uncertainty metric (primarily Dirichlet differential entropy), e.g. $\tilde x\approx\arg\max_{\|x-\tilde x\|\le r} \mathrm{DiffE}(\mathrm{Dir}(\alpha(x)))$, where $\mathrm{DiffE}(\mathrm{Dir}(\alpha))=\sum_c\ln\Gamma(\alpha_c)-\ln\Gamma(\alpha_0)-\sum_c(\alpha_c-1)(\Psi(\alpha_c)-\Psi(\alpha_0))$.","Across datasets (MNIST, CIFAR10, Segment, Sensorless) and attacks (PGD/FGSM/Noise), uncertainty’s ability to flag errors degrades strongly under PGD; for CIFAR10, AUC‑PR for correct-vs-wrong based on differential entropy drops from ~97–99 (no attack) to single digits at larger radii (e.g., PostNet 98.7 at r=0.0 to 7.8 at r=0.5). Adversarial-example detection via uncertainty is weak under PGD; for CIFAR10, PGD label-attack detection AUC‑PR is typically ~48–67 depending on model and radius and does not scale with the sharp accuracy drop. OOD detection is highly vulnerable to the proposed uncertainty attacks: for CIFAR10–SVHN, AUC‑PR for OOD detection using differential entropy drops from ~80–83 (r=0.0) to ~15–23 at r=2.0 under PGD uncertainty attacks (both ID-attacks and OOD-attacks). Median smoothing substantially improves empirical robustness and enables certified lower/upper performance bounds on uncertainty-based decisions, while adversarial training provides only minor additional benefit.",None stated.,"The work is primarily about robustness of ML uncertainty estimation rather than reliability engineering; conclusions may not transfer to engineering reliability settings (e.g., degradation/RUL) where data-generating processes and safety requirements differ. Robustness is assessed mainly via norm-bounded adversarial attacks (FGSM/PGD and a noise baseline), which may not represent realistic distribution shifts or sensor/operational drift encountered in real deployments. The evaluation focuses on a limited set of datasets (two vision and two tabular) and DBU model variants; results may depend on architectures, training pipelines, and chosen OOD sets (e.g., SVHN vs CIFAR10). Practical adoption of median smoothing can add inference-time cost due to multiple noisy samples, and the paper does not deeply quantify runtime/latency trade-offs across deployment contexts.","The authors propose exploring approaches to make DBU models more robust, and investigate two first directions—median smoothing (with certificates) and adversarial training—finding smoothing more effective and adversarial training only mildly helpful. Beyond this, they motivate further study of certifiable robustness for uncertainty estimation (including epistemic measures such as differential entropy) and stronger methods for robust uncertainty and OOD detection under adversarial perturbations.","Extend robustness analysis to more realistic distribution shifts (corruptions, covariate shift, temporal drift) and threat models beyond norm-bounded perturbations, and quantify how these affect uncertainty calibration and OOD detection. Develop self-starting or architecture-agnostic certified methods for uncertainty measures that reduce inference overhead compared with sampling-based smoothing. Evaluate DBU robustness and certified uncertainty in higher-stakes domains (e.g., medical imaging, autonomous driving) with operational metrics (selective prediction/abstention, risk–coverage, cost of false OOD alarms). Provide standardized benchmarks and open implementations that reproduce the large attack grid (138,800 settings) and facilitate fair comparison with non-Dirichlet uncertainty methods (ensembles, Bayesian approximations, conformal prediction) under matched compute budgets.",2010.14986v2,https://arxiv.org/pdf/2010.14986v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:55:27Z
FALSE,NA,Other,Other,Not applicable,Network/cybersecurity|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://www.daml.in.tum.de/reliable_gnn_via_robust_aggregation,"The paper proposes a robust neighborhood aggregation function for Graph Neural Networks (GNNs) to improve robustness against adversarial perturbations of graph structure (edge insertions/deletions). It introduces Soft Medoid, a fully differentiable approximation of the medoid using a softmax over pairwise distances, and extends it to a weighted version for message passing. The authors provide a robustness analysis showing the Soft Medoid has an asymptotic breakdown point of 0.5, implying bounded aggregation bias as long as fewer than 50% of a node’s neighborhood inputs are adversarial. Empirically, they evaluate on citation-network node classification (Cora ML, Citeseer, PubMed) using both attack-based robustness tests and randomized smoothing certificates, reporting substantial robustness gains over baselines and prior defenses, especially for low-degree nodes. They also discuss computational considerations and a top-k neighbor approximation to make the method efficient in sparse implementations.","The Soft Medoid replaces the medoid’s $\arg\min$ with a softmax-weighted average: $t_{SM}(X)=\sum_i \hat{s}_i x_i$ where $\hat{s}_i \propto \exp\big(-\frac{1}{T}\sum_j \|x_j-x_i\|\big)$ (Eqs. 2–3), interpolating between medoid ($T\to0$) and mean ($T\to\infty$). For GNNs they define a weighted Soft Medoid $\tilde t_{WSM}(X,a)=c\,(s\circ a)^\top X$ with $s_i\propto \exp\big(-\frac{1}{T}\sum_j a_j\|x_j-x_i\|\big)$ and normalization $c=(\sum_j a_j)/(\sum_j s_j a_j)$ (Eqs. 6–7). Robustness is characterized via the breakdown point definition $\epsilon^*(t,X)=\min\{m/n: \sup \|t(X)-t(\tilde X_\epsilon)\|=\infty\}$, and they prove $\epsilon^*(t_{SM},X)\to 0.5$ (Eq. 4, Theorem 1).","They prove the Soft Medoid estimator has finite-sample breakdown point $\epsilon^*(t_{SM},X)=\frac{1}{n}\lfloor\frac{n+1}{2}\rfloor$ and asymptotically 0.5, independent of temperature $T$ (Theorem 1), yielding bounded maxbias for contamination rates below the breakdown point (Corollary 1). On certified robustness (randomized smoothing for graph perturbations), Soft Medoid GDC achieves much higher accumulated certifications than vanilla GCN/GDC and prior defenses; e.g., on Cora ML, accumulated certifications for addition-only improve from 0.21 (Vanilla GCN) to 0.66 (SM GDC, $T=0.2$), and on Citeseer from 0.11 to 0.60. They report robustness improvements up to 3× on Cora ML and 5.5× on Citeseer compared to the base architecture under structure perturbations, and up to 8× improvement for low-degree nodes. Empirical attack evaluations (Dice/FGSM/PGD/Nettack/Metattack transfer/evasion settings) show slower accuracy degradation under increasing fractions of perturbed edges compared with baselines.","They note that robust statistics measures like breakdown point analyze unbounded attacks, while graph structure perturbations are bounded in practice; they treat unbounded perturbations as worst-case analysis and rely on experiments for bounded attacks. They also state that deriving an explicit analytical upper bound for the maxbias (error bound) is out of scope. They report that several alternative robust estimators were computationally too demanding or ineffective in their setting, motivating their design choice, and mention a trade-off where increased robustness to structure attacks can reduce robustness to attribute attacks.","The method’s robustness guarantee is at the aggregation-estimator level (breakdown point) and does not directly translate to end-to-end certified robustness of the learned GNN under realistic adaptive attacks, especially when attackers can target both structure and features jointly. Practical performance may depend heavily on temperature $T$, choice of top-$k$ neighbor truncation, and graph normalization/weights; these introduce tuning burden and may weaken guarantees for high-degree nodes or when important neighbors are excluded. The evaluation focuses on small/medium citation benchmarks; generalization to larger, diverse, and heterophilous graphs (or inductive settings) is not established. The paper provides limited guidance on statistical efficiency/accuracy trade-offs of Soft Medoid vs. mean under non-adversarial noise and on behavior under correlated or targeted perturbations that exploit model internals beyond simple outliers.","They indicate that analytically deriving a finite upper bound on the maxbias (the robustness error bound) is out of scope, implying it as a potential direction. They also discuss the robustness–accuracy trade-off controlled by temperature $T$ and present an alternative parametrization improving attribute robustness, suggesting further exploration of such trade-offs/configurations.","Develop a full end-to-end robustness certification or tighter theoretical guarantees that connect the aggregation breakdown point to node-classification decision stability across layers under bounded graph attacks. Extend the approach to joint structure-and-feature adversaries and to settings with autocorrelation/graph dynamics (temporal graphs) where perturbations are not i.i.d. outliers. Provide scalable implementations and ablation studies for very large graphs (millions of nodes) and study adaptive attacker strategies aware of top-$k$ truncation and temperature selection. Investigate robust aggregation for heterophily, directed/weighted/multi-relational graphs, and integrate robust estimators with other defenses (e.g., adversarial training) in a principled way.",2010.15651v1,https://arxiv.org/pdf/2010.15651v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:56:09Z
FALSE,Other,Nonparametric/Semi-parametric|Stochastic process|Simulation-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Energy/utilities|Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Public repository (GitHub/GitLab),github.com/jiachenzhang001/Non-Gaussian-Bias-Correction,"This paper develops a non-Gaussian, spatial bias-correction methodology to assess how wind power operations and profitability may change under future climate conditions, using reanalysis observations (MERRA-2) and regional climate simulations (MENA CORDEX). The core method applies a spatially varying Yeo–Johnson (trans-Gaussian) transformation with parameters estimated cluster-wise by minimizing an approximation to the Kullback–Leibler divergence between observed and simulated joint distributions; after transformation, mean and covariance are adjusted (including a nonstationary spatial covariance model) and then back-transformed. Validation is performed via (i) simulated non-Gaussian random fields and (ii) a historical train/test split (1980–1992 training, 1993–2005 testing), showing substantially lower KL divergence than traditional mean/variance/covariance-only corrections. The corrected future winds (2025–2050) are extrapolated from 10 m to turbine hub height using a power-law shear model calibrated from high-resolution WRF simulations, then converted to energy via turbine power curves and to revenue via electricity tariffs. For optimal wind-farm sites in Saudi Arabia, the study reports an estimated increase in daily profit of about $272,000 (with uncertainty driven mainly by vertical extrapolation).","The mean/temporal model is a site-specific seasonal-harmonic regression with AR residuals: $W(s_i,t_j)=\mu(s_i,t_j)+\sum_{p=1}^P\phi_{p,i}\,\varepsilon(s_i,t_{j-p})+\varepsilon(s_i,t_j)$ with $\mu$ including a linear annual trend plus $K$ sine/cosine harmonics. The proposed non-Gaussian correction applies a Yeo–Johnson transform $g_{\lambda}$ (with cluster-wise $\lambda$) to both observed and simulated fields, performs a Gaussian mean/covariance mapping $\mu_O^{(H)}+(\Sigma_O^{(H)})^{1/2}(\Sigma_S^{(H)})^{-1/2}(g_{\lambda_S}(W_S^{(F)})-\mu_S^{(H)})$, then back-transforms with $g^{-1}_{\lambda_O}$. The discrepancy objective uses the KL divergence $D_{KL}(f_O\|f_S)=\int f_O\log(f_O/f_S)$, approximated via k-nearest-neighbor distances.","On real wind data (MERRA-2 vs MENA CORDEX) using a 1980–1992 train / 1993–2005 test split, KL divergence ratios vs simple mean bias correction were approximately 0.93 (mean+variance), 0.88 (mean+Mat","The authors note that if hourly/sub-hourly wind data were available, their simplifying treatment of dependence (reducing to a vector moving-average-style adjustment) would likely be inadequate and would require more computationally expensive nonseparable space3time models. They also state that higher-resolution wind would introduce sparsity/zeros such that a simple trans-Gaussian model may fail, requiring more complex latent-Gaussian modeling, and that the power-law vertical extrapolation is not recommended beyond roughly hourly resolution. They further acknowledge that vertical extrapolation to hub height is a major source of uncertainty and that other uncertainties were not fully accounted for (choice of reanalysis product, uncertainty in turbine power curves, and policy-dependent tariffs).","The method hinges on a strong stationarity-of-bias assumption: that the learned observation3simulation relationship and transformation structure from 198032005 remains valid for 202532050 under climate change; limited stress-testing of this assumption is shown. The KL divergence is approximated in very high dimension (hundreds of locations) with k-NN distances, which can be sensitive to the curse of dimensionality and the choice of k; robustness checks across k or alternative divergences are not emphasized. The use of k-means clustering with ad hoc feature weights (0.98 on MLE, 0.01/0.01 on lat/long) may materially affect results and spatial coherence, but sensitivity to number of clusters/weights is not fully developed. The profitability calculation also depends on several external inputs (power-curve fidelity, siting layout, wake losses, future tariffs) that are treated deterministically or with limited uncertainty propagation beyond the shear model.","They propose future work to better assess additional sources of uncertainty by engaging industrial partners and policymakers, including comparing against other reanalysis products (e.g., ERA5), incorporating uncertainty in turbine power curves, and accounting for policy/negotiation-driven variability in tariffs. They also suggest that with higher-frequency data, more sophisticated models may be needed, including latent Gaussian models for sparse wind and nonparametric approaches such as neural networks for vertical extrapolation. They mention revisiting wind-farm siting decisions (as in Giani et al., 2020) in light of the new future-wind corrections.","A natural extension is a formal, fully probabilistic (Bayesian) version of the joint transformation + covariance adjustment that propagates uncertainty from transformation parameters, covariance estimation, and future scenario choice into profit forecasts. Robustness studies could evaluate sensitivity to clustering design (number of clusters, spatial constraints, weighting) and to divergence choices (e.g., energy distance, Wasserstein) or alternative high-dimensional KL estimators. Methodologically, extending the adjustment to multivariate fields (e.g., wind components, temperature/stability metrics) could improve hub-height extrapolation and address nonstationary boundary-layer effects more directly. Empirically, broader validation across multiple CORDEX members and emissions scenarios (not just the selected run and RCP 8.5) would help quantify model-form uncertainty in projected reliability/profitability.",2011.01263v2,https://arxiv.org/pdf/2011.01263v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:56:55Z
FALSE,NA,Bayesian|ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Other,Supplementary material (Journal/Publisher),https://github.com/wjmaddox/swa_gaussian,"This paper proposes Amortized Conditional Normalized Maximum Likelihood (ACNML), a practical approximation to conditional normalized maximum likelihood (CNML) for predictive uncertainty estimation and calibration under distribution shift in deep neural networks. The method replaces CNML’s expensive retraining-on-(train+test) per label with a per-test-point optimization regularized by an approximate Bayesian posterior density q(\theta) learned once on the training set. ACNML can sit on top of various approximate inference methods that provide tractable posterior densities (e.g., Bayes-by-Backprop, SWAG-D, KFAC-Laplace) and aims to produce more conservative, better-calibrated predictions on out-of-distribution (OOD) inputs. The authors provide theoretical analysis connecting Gaussian posterior approximations to influence-function/Laplace-style local quadratic approximations and derive bounds on the difference between exact CNML and ACNML when parameter perturbation is small. Experiments on Rotated MNIST and CIFAR10-C show improved calibration metrics (ECE) and competitive Brier scores under increasing corruption/shift, at substantially lower inference-time cost than naive CNML fine-tuning per test point.","ACNML computes, for each class y, a label-conditional parameter update via \(\hat\theta_y = \arg\max_{\theta} \log p_\theta(y\mid x_n) + \log q(\theta)\), where \(q(\theta)\) is an approximate posterior trained on \((x_{1:n-1},y_{1:n-1})\). The returned predictive distribution is \(p(y\mid x_n)= \frac{p_{\hat\theta_y}(y\mid x_n)}{\sum_{i=1}^k p_{\hat\theta_i}(i\mid x_n)}\). For SWAG-based posteriors, they introduce a temperature \(\alpha\) and optimize \(\log p_\theta(y\mid x_n) + \frac{1}{\alpha}\log q(\theta)\).","On Rotated MNIST, ACNML achieves lower ECE and improved Brier scores compared to MAP and Bayesian model averaging as rotation (shift severity) increases, while being somewhat overly conservative in-distribution. On CIFAR10-C (VGG16/WRN28x10), reliability diagrams show ACNML produces less overconfident predictions under heavy corruptions and yields substantially lower ECE at higher corruption levels than comparable Bayesian marginalization baselines that use the same posterior approximations. Inference-time comparisons show ACNML is orders of magnitude faster than naive CNML fine-tuning per test input (e.g., MNIST MLP: 0.08s vs 13.83s per epoch; VGG16: 0.37s vs 102.0s per epoch; WRN28x10: 1.1s vs 359.1s per epoch), though slower than standard feedforward inference.","The authors note that ACNML can be overly conservative on in-distribution/easy settings (e.g., low rotation on MNIST), leading to underconfident predictions and worse in-distribution calibration. They also state that their theoretical approximation guarantees rely on assumptions like convexity/smoothness and sufficiently large datasets, which do not necessarily hold for deep neural networks in practice. They discuss ambiguity in CNML when multiple MLEs exist due to non-convexity and propose a heuristic selection rule in their naive CNML implementation.","The approach requires per-test-point optimization for each class, so inference cost scales linearly with the number of classes and may be prohibitive for large-label problems (e.g., ImageNet-scale classification) without further approximation. Performance depends on the quality and calibration of the approximate posterior density q(\theta); if q(\theta) is misspecified or poorly estimated, the induced regularization may lead to miscalibration or excessive conservatism, especially under shifts unlike those considered. Empirical evaluation focuses on vision benchmarks (Rotated MNIST, CIFAR10-C) and calibration metrics; broader validation on other shift types (label shift, covariate shift in structured data, temporal drift) and high-stakes domains is not shown.","They propose exploring more complex and expressive posterior approximations to improve ACNML, particularly because deep-network training losses are highly non-convex with many local minima. They suggest incorporating local approximations around multiple diverse minima to yield even more reliable uncertainty estimation. More broadly, they suggest developing additional tractable algorithms inspired by ACNML to improve reliable confidence estimates on OOD inputs for safer learning systems.","Develop class-aggregation or candidate-label pruning strategies to reduce the per-class optimization burden, enabling scalability to hundreds or thousands of classes. Extend ACNML to handle structured prediction and regression settings (continuous outputs) and study how the normalization behaves beyond finite-class classification. Investigate robustness to dataset shift types beyond common corruptions/rotations (e.g., spurious correlations, domain generalization benchmarks, temporal drift) and evaluate selective prediction/abstention performance and downstream decision-theoretic utility.",2011.02696v2,https://arxiv.org/pdf/2011.02696v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:57:35Z
TRUE,Life distribution modeling|Warranty analysis|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Right-censored|Interval-censored|Left-censored|Mixture of types|Other,Not applicable,Energy/utilities|Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,R|Other,Not provided,https://www.backblaze.com/b2/hard-drive-test-data.html,"The paper develops Bayesian methods to construct prediction bounds/intervals for the number of future within-sample failures using heterogeneous reliability field data, especially when failures are sparse due to high reliability, censoring, truncation, and staggered entry. It uses Bayesian hierarchical lifetime models (log-location-scale families such as Weibull and Fr","For a single group, the future-failure count over (t_{c,i}, t_{w,i}] is modeled as Y\mid\theta \sim \text{Binomial}(n-r,\rho), where \(\rho = \{F(t_{w,i};\theta)-F(t_{c,i};\theta)\}/\{1-F(t_{c,i};\theta)\}\). For multiple groups/units, the total future failures \(Y=\sum_{g=1}^G\sum_{i=1}^{n_g} I(T_{ig}\in(t_{c,ig},t_{w,ig}])\) has a Poisson-binomial distribution with unit-specific \(\rho_{ig}\) defined analogously, approximated by a Poisson distribution in applications. Bayesian prediction uses the posterior predictive cdf \(J(y)=\int J(y;\theta) f(\theta\mid t)\,d\theta\), approximated by averaging over posterior draws \(\theta_j^*\): \(J(y)\approx \frac1B\sum_{j=1}^B J(y;\theta_j^*)\).","In the heat-exchanger application (3 plants, n=300 tubes each, 11 observed failures at t_c=3 years with left/interval censoring), informative priors on the Weibull shape parameter led to more conservative (higher) predicted future-failure counts than weakly informative priors, with divergence growing at longer horizons as hazards increase. For Backblaze hard drives, the hierarchical GLFP model produced weekly 95% prediction intervals over a 26-week horizon that covered observed failures for example drive models (e.g., Drive Models 6 and 23 shown), while explicitly updating the dynamic risk set as drives were retired or failed. For Product A warranty returns (n=63,132), Fr","The authors note they assume exchangeability of units within each subpopulation, which may be violated by unmodeled covariates such as batch effects or facility conditions (e.g., Backblaze drive environment/status differences). They also highlight that retirements/removals before failure can bias predictions unless the risk set is updated (as they do weekly), and that in many settings retirements may be unobserved. They further acknowledge warranty claims may be delayed relative to failure, potentially causing temporal spikes (e.g., underestimation in certain months for Product A), which their models do not explicitly capture.",The approach is fully parametric (Weibull/Fr,"Suggested extensions include relaxing the within-subpopulation exchangeability assumption by incorporating additional (possibly time-varying) covariates such as drive health/status signals (e.g., S.M.A.R.T. data) to improve prediction intervals. They propose extending from predicting counts to predicting costs of future events (e.g., warranty costs), leveraging the full Bayesian posterior to estimate functionals like depreciation factors across subpopulations. They also suggest explicitly modeling product retirement when retirements are unobserved (e.g., jointly modeling failure and retirement times) to enable longer-horizon predictions, and incorporating delayed warranty reporting mechanisms to address seasonal spikes near contract expiration.","Develop robust/semi-parametric variants (e.g., Bayesian model averaging over candidate lifetime families, or nonparametric baseline hazards with hierarchical shrinkage) to reduce sensitivity to distributional misspecification. Replace the Poisson approximation with scalable exact or refined approximations (e.g., FFT-based Poisson-binomial cdf, saddlepoint methods) and quantify its impact on coverage under realistic fleet sizes and event probabilities. Provide self-starting/online updating methods (e.g., sequential Monte Carlo or variational Bayes) for operational settings requiring frequent prediction updates with large dynamic risk sets. Extend to richer heterogeneity structures (cross-classified hierarchies, random effects/frailty, dependence within systems, and competing risks with more than two modes) and validate on additional real industrial case studies with publicly released code and benchmark datasets.",2011.03140v3,https://arxiv.org/pdf/2011.03140v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:58:22Z
FALSE,Other,ML-based|Bayesian|Hybrid/Ensemble|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/Pacmed/ehr_ood_detection,"This paper evaluates whether contemporary uncertainty estimation methods enable reliable out-of-distribution (OOD) detection for deep learning models on mixed-type medical tabular data (EHR-derived ICU data). Using mortality prediction for ICU patients as the base task, the authors benchmark single neural networks (with and without calibration), Bayesian approaches (MC Dropout, Bayes-by-Backprop), and ensemble methods against density-estimation baselines (PPCA and autoencoder). They design clinically relevant OOD scenarios (demographic/provenance/pathology group shifts), artificial feature corruption, and cross-dataset shifts between MIMIC-III and eICU, and assess OOD detection via AUC-ROC based on uncertainty scores (e.g., entropy, mutual information, prediction variance, max softmax probability). Across most clinically realistic shifts, uncertainty estimates from neural discriminators largely fail to separate in-distribution from OOD patients, sometimes performing worse than random guessing; density-estimation baselines are comparatively stronger, and anchored ensembles/BBB show inconsistent advantages in some perturbation settings. The work provides benchmarks and a caution that calibration/uncertainty estimation alone does not ensure dependable OOD detection in high-stakes medical tabular applications.","OOD scores are derived from model confidence/uncertainty measures: (i) maximum softmax probability $y_{\max}=\max_{c\in\mathcal{C}}p_\theta(y=c\mid x)$; and for $K$ stochastic/ensemble predictions $p^{(k)}_\theta(y\mid x)$, (ii) the standard deviation of the positive-class probabilities $\sigma_P$ across predictions, (iii) predictive entropy $\tilde H[p_\theta(y\mid x)]=H\big(\mathbb{E}[p_\theta(y\mid x)]\big)$, and (iv) mutual information $I(y,\theta\mid x)\approx H\big(\mathbb{E}[p_\theta(y\mid x)]\big)-\mathbb{E}[H(p_\theta(y\mid x))]$. OOD detection performance is then evaluated as AUC-ROC for classifying in-distribution vs OOD samples using these scores.","On in-distribution mortality prediction, most models achieve similar AUC-ROC around ~0.84–0.85 on both MIMIC-III and eICU (e.g., NN ~0.847 on MIMIC and ~0.842 on eICU; MC Dropout ~0.849/~0.844), while BBB performs markedly worse (~0.628/~0.611). For artificial feature-scaling corruption (factors 10–10000), many discriminator-based uncertainty scores degrade and can yield OOD AUC-ROC below 0.5 (worse than random), whereas PPCA/AE perform better; anchored ensembles and BBB show comparatively better but not uniformly reliable behavior. For clinically defined OOD groups (demographic/provenance/diagnosis), nearly all tested discriminator-based methods fail to distinguish OOD vs in-distribution patients except in extreme cases (e.g., newborns in MIMIC-III), with density baselines generally less brittle. In cross-dataset OOD detection (MIMIC vs eICU), density estimation is strongest and some discriminators show asymmetrical performance, indicating lack of robustness and reproducibility across shift directions.","The authors note constraints including limited scope to tabular mixed-type EHR data and a binary mortality prediction task, and that they could not train Gaussian Processes as a Bayesian baseline due to scaling/non-convergence on the dataset size. They also mention that some methods (notably Bayes-by-Backprop) struggled to converge despite extensive hyperparameter search, complicating interpretation of its occasional OOD performance. Additionally, due to space limits, detailed descriptions of uncertainty metrics are deferred to an appendix.","The evaluation relies primarily on AUC-ROC for OOD detection using scalar uncertainty scores, which may not reflect clinically meaningful operating points (e.g., low false-alarm regimes) or decision-analytic utility. OOD group construction by excluding/reintroducing subpopulations may conflate covariate shift with label shift and changes in prevalence, and the approach does not include explicit abstention policies or downstream workflow impacts. Implementation details (e.g., software stack) are not documented in the excerpt, and comparisons omit some strong post-hoc OOD methods for tabular data (e.g., conformal prediction, energy-based scores, or dedicated tabular density/flow models) beyond simple PPCA/AE baselines.","They suggest assessing more powerful density-estimation models tailored to tabular data (e.g., robust VAE approaches) and hybrid classifier–generative approaches (e.g., treating classifiers as energy-based models) to improve OOD detection on clinically relevant patient groups. They also encourage further investigation to enable safer deployment and more mindful application of uncertainty estimation techniques under real dataset shifts.","Extend the benchmarks to include self-contained decision rules (selective prediction/abstention) and evaluate performance at clinically relevant thresholds (e.g., FPR@TPR, expected utility) rather than only AUC-ROC. Study robustness under temporal shifts and autocorrelation typical of hospital data, and incorporate Phase I/II style monitoring of drift with alert fatigue constraints. Add modern tabular OOD baselines (conformal prediction, isolation forests, deep tabular density models/normalizing flows tuned for tabular data) and report calibration quality jointly with OOD detection to better connect uncertainty to actionability.",2011.03274v1,https://arxiv.org/pdf/2011.03274v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:59:08Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://www.gurobi.com,"This paper studies reliable off-policy evaluation (OPE) in reinforcement learning, aiming to quantify uncertainty in estimated discounted cumulative reward for a target policy using logged trajectories generated by a different behavior policy. It proposes a distributionally robust optimization framework using an s-rectangular Wasserstein ambiguity set around empirical conditional action–next-state distributions, producing robust (lower) and optimistic (upper) reward estimates that serve as confidence bounds. The authors provide both non-asymptotic and asymptotic guarantees under stochastic settings with Markovian data and also extend results to adversarial (distribution-shift) environments, as well as to batch reinforcement learning. Computation is carried out via tractable reformulations and robust value iteration; an equivalent regularized Lagrangian view connects the DRO formulation to Lipschitz regularization akin to GAN-style objectives. Numerical experiments on discrete MDP examples (machine replacement and healthcare management) empirically support the tightness and coverage behavior of the proposed confidence intervals and show advantages over sample-average baselines in batch RL.","The policy value is the infinite-horizon discounted return $\rho^\pi=(1-\gamma)\,\mathbb{E}\left[\sum_{t\ge 0}\gamma^t R_t\right]$. The robust/optimistic OPE bounds are defined as optimization problems over an s-rectangular Wasserstein set of conditional transition distributions: $L_{\hat\mu}(d)=\min_{F,\mu\in \mathcal{M}_{\hat\mu}(d)} \sum_{s,a}\mu(s)\pi_b(a\mid s)F(s)V_s(a)A(s,a)$ (and similarly a max for the optimistic bound), subject to stationarity constraints linking $F$ and $\mu$ and Wasserstein constraints $W(\mu(\cdot,\cdot\mid s),\hat\mu_s)\le d_s$. The resulting fixed-point (robust Bellman) recursion used for computation takes the form $E(s)=\max_a\{A(s,a)+\gamma\min_{\nu\in\mathcal{M}_{\hat\mu_s}(d_s)}\mathbb{E}_{(a,s')\sim\nu}[V_s(a)E(s')]\}$, solvable by robust value iteration under a contraction condition.","The paper derives finite-sample (non-asymptotic) and asymptotic confidence bounds for infinite-horizon discounted OPE by calibrating the Wasserstein radii $d_s$ so that the true conditional transition distributions lie in the ambiguity set with high probability, yielding coverage guarantees for $[L(d),U(d)]$. It shows that under suitable conditions the robust Bellman operator is a contraction with high probability, enabling convergence of robust value iteration to the optimum and tractable computation of bounds. In adversarial (distribution-shift) settings, it provides analogous confidence bounds for a worst-case (adversarial) value defined via a Wasserstein ball around the past environment’s visitation/transition distributions. Empirically, on discrete MDP case studies (machine replacement; healthcare management), the constructed 95% intervals tighten as the number of trajectories and/or truncation length increases, and the empirical non-coverage rate decreases toward nominal levels; the robust batch RL method reduces relative performance gap faster than a sample-average (SAA) baseline as sample size grows. The supplementary notes experiments run in Python (with CVXPY and Gurobi) and reports radii scaling approximately on the order of $1/\sqrt{n}$ with sample size, consistent with the theory.","The authors note that a deep-learning (GAN-like) minimax approach suggested by their regularized Lagrangian perspective is beyond the scope of the paper, and they highlight open questions such as how large a network must be to approximate the optimal generator/discriminator and understanding the non-convex training landscape. They also indicate that extending their approach to large or continuous state spaces via minimax saddle-point methods with Lipschitz regularization is left for future work.","The work is not a reliability engineering paper and does not address failure processes, lifetime/degradation modeling, maintenance policy optimization, or system reliability; any “reliability” terminology is about statistical confidence/safety in RL. The theory and algorithms are developed primarily for finite-state/action MDPs and rely on constructing Wasserstein balls over discrete conditional distributions; scalability to large-scale/high-dimensional RL problems depends on additional approximation machinery not fully developed here. Empirical validation is limited to stylized discrete MDP benchmarks, so real-world performance (e.g., in clinical settings) and sensitivity to model/metric choices (cost function for Wasserstein distance, radius calibration) may require further study.","The paper suggests investigating deep-learning-based solvers for the regularized minimax (GAN-like) formulation, including determining adequate network capacity and analyzing the non-convex optimization landscape. It also proposes extending the regularized Lagrangian formulation to large or continuous state spaces by solving minimax saddle-point problems with Lipschitz regularization, which is left for future work.","A practical next step would be to provide an open-source reference implementation (e.g., Python package) and standardized benchmarks to facilitate adoption and reproducibility across OPE tasks. It would also be valuable to study robustness to trajectory dependence/autocorrelation beyond the stated assumptions and to evaluate sensitivity to the choice of Wasserstein ground metric (feature engineering) in high-dimensional state spaces. Finally, applying the method to real logged datasets (e.g., healthcare records) with careful causal/clinical validation would strengthen external validity and clarify operational guidance for selecting ambiguity radii.",2011.04102v3,https://arxiv.org/pdf/2011.04102v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T11:59:56Z
TRUE,Degradation modeling|RUL prediction|Accelerated testing,ML-based|Other,Degradation measurements|Sensor/condition monitoring|Simulated only|Mixture of types,Not applicable,Transportation/logistics|Manufacturing (general)|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Upon request,https://doi.org/10.1115/1.3153635,"The paper proposes a causal GraphNet-based time-series modeling approach (GNN-tCNN) for remaining useful life (RUL) estimation under uncertainty from long, non-equispaced, and non-stationary monitoring data. Instead of imputing missing observations to create regularly sampled sequences, it learns directly from irregularly spaced observations by constructing causal graphs whose edges encode elapsed time (and potentially operating conditions) and whose nodes embed short-term raw-signal segments via a temporal CNN. The model outputs a probabilistic RUL estimate by parameterizing a Gamma distribution (shape/rate) using softplus activations, and it is trained end-to-end by maximizing the RUL likelihood (minimizing negative log-likelihood) using implicit reparameterization gradients. Performance is demonstrated on (i) a simulated non-stationary stochastic degradation process with Gamma-distributed increments and (ii) a real accelerated life testing dataset of run-to-failure ball bearings (FEMTO/PRONOSTIA), with comparisons against an LSTM-tCNN baseline. The results show that incorporating multiple past observations improves RUL trend tracking and yields intuitively wider uncertainty early in life and tighter uncertainty near failure, while using more than ~30 past observations did not materially improve the bearing-case predictions.","A simulated latent degradation process is defined with Gamma increments: $\delta\eta_{t_i}\sim\mathrm{Gamma}(\alpha(t_i,c),\beta)$, $z_{t_k}=\sum_{i=0}^{t_k}\delta\eta_{t_i}$ with failure at $z_{t_k}\ge z_f$. The proposed model maps an input causal graph through an encode-process-decode GraphNet: $g_{out}=GN_{dec}\circ GN_{core}^{(N_c)}\circ GN_{enc}(g_{in})$, and the last-node embedding parameterizes a Gamma RUL distribution via $\alpha=f_{\alpha;\theta}(g)$ and $\beta=f_{\beta;\theta}(g)$ (softplus outputs). Training minimizes the negative log-likelihood $\sum_i -\log p(y_i\mid \mathrm{Gamma}(\alpha_i,\beta_i))$ using implicit reparameterization gradients.","On the simulated dataset (12 training, 3 test run-to-failure trajectories), using more past observations yields smoother and more accurate RUL trajectories (lower expected NLL) than using only a single observation, and predictive distributions become more concentrated as failure approaches. On the FEMTO/PRONOSTIA bearing ALT dataset (17 bearings; 3 operating conditions), the GNN-tCNN captures degradation trends with uncertainty shrinking near failure and compares against an LSTM-tCNN baseline on representative test runs. The authors report that incorporating up to 30 arbitrarily spaced past observations from the prior 2000 seconds is sufficient, and adding more than 30 observations does not significantly improve accuracy or uncertainty bounds.",None stated.,"The evaluation is largely qualitative/illustrative in the provided excerpt (plots and NLL discussion) and does not report comprehensive aggregate metrics (e.g., average NLL/CRPS, calibration curves, coverage vs. nominal) across all test bearings, which limits strength of performance claims. The uncertainty model is restricted to a unimodal Gamma distribution over RUL; this may be insufficient for multi-modal uncertainty (e.g., ambiguous early-life signals) and does not explicitly separate aleatoric vs. epistemic uncertainty. The approach assumes that elapsed-time edge attributes (and optional condition proxies) are sufficient to handle irregular sampling; robustness to strong nonstationarity, covariate shift across operating regimes, and sensor faults/missingness mechanisms is not deeply explored.","The authors suggest applying the non-sequential causal GraphNet approach to other non-regularly sampled time-series domains (e.g., electronic health records). They also propose exploring additional time-series tasks beyond RUL estimation, including time-series generation and unsupervised/self-supervised learning using the GNN-tCNN architecture.","Evaluate probabilistic quality more rigorously (calibration/coverage, proper scoring rules like CRPS, sharpness vs. reliability) and compare against additional probabilistic RUL baselines (e.g., Bayesian deep models, deep ensembles, survival models). Extend the model to incorporate explicit operating-condition covariates (load, speed, temperature) on edges/nodes and test generalization under domain shift across conditions. Develop a maintenance-decision layer (e.g., cost-aware predictive maintenance optimization) that consumes the full predicted RUL distribution, and release an open-source reference implementation to improve reproducibility and adoption.",2011.11740v1,https://arxiv.org/pdf/2011.11740v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:00:46Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,2011.15001v1,https://arxiv.org/pdf/2011.15001v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:00:56Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,Nonparametric/Semi-parametric|Other,Degradation measurements|Sensor/condition monitoring,Condition-based|Predictive,Manufacturing (general)|Transportation/logistics|Other,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes three bearing degradation monitoring indicators—SDHT2, VSDHT2, and NVSDHT2—constructed from vibration-signal features using “discarded projected space information” from PCA rather than only retained principal components. The core idea is to quantify degradation-related changes via a segmented discarded Hotelling’s $T^2$, defined as the difference between Hotelling’s $T^2$ in the full PCA space and in the reduced PCA space, averaged within time segments. To incorporate historical evolution and multi-stage degradation, the authors combine this statistic with piecewise linear representation (PLR) segmentation using a Bottom-Up algorithm, producing a stage-wise vector indicator (VSDHT2) and a normalized generalized version (NVSDHT2). The indicators are evaluated on the IEEE PHM 2012 PRONOSTIA run-to-failure bearing dataset (condition 1: 1800 rpm, 4000 N), showing monotonic and sensitive behavior across the whole life, unlike common time-domain features (RMS, mean, STD, PMR, kurtosis, skewness). The work positions discarded-space information as a useful source for health monitoring and suggests subsequent use for remaining useful life estimation.","The method defines Hotelling’s $T^2$ for an observation $v$ in segment $d$ using PCA scores: $T^2_{d,v}(\beta)=(\varphi_{d,v}-\vartheta_d)^\top \Phi_{d}^{-1}(\varphi_{d,v}-\vartheta_d)$, where $\beta$ is the number of principal components considered. Full-space and reduced-space statistics are $TF_{d,v}=T^2_{d,v}(E)$ and $TR_{d,v}=T^2_{d,v}(p)$, and the discarded statistic is $TD_{d,v}=TF_{d,v}-TR_{d,v}$ with segment average $TD^{\text{seg}}_d=\frac{1}{B}\sum_{v=1}^B TD_{d,v}$. The stage-wise indicator is $\mathrm{VSDHT}^2=[TD^{\text{seg}}_1,\ldots,TD^{\text{seg}}_m]^\top$ and the normalized generalization is $\mathrm{NVSDHT}^2=[TD^{\text{seg}}_1/(E-p),\ldots,TD^{\text{seg}}_m/(E-p)]^\top$.","On the PRONOSTIA/IEEE PHM 2012 dataset (condition 1), the number of retained PCs was chosen as $p=2$ because CPV exceeded 95% for both training bearings (1-1, 1-2). For the Bottom-Up PLR extraction of VSDHT2, parameter selection via ASDS favored $k=100$ and $m=5$ (ASDS 0.0738), larger than alternatives such as 100~10 (0.0411) or 200~20 (0.0330). Plots for bearings 1-1 through 1-7 show SDHT2, VSDHT2, and NVSDHT2 exhibiting a monotonic increasing trend over the whole run-to-failure life and sensitivity across early-to-late degradation stages, while individual common features (e.g., RMS) are not monotonic and mainly react strongly near failure. NVSDHT2 is presented as a normalized/generalized version of VSDHT2 with values scaled by $(E-p)$.",None stated.,"The evaluation is limited to a single benchmark (PRONOSTIA/IEEE PHM 2012) and one operating condition (condition 1) with a small number of training bearings (two), so generalization across loads/speeds and different bearing types is unclear. The indicators depend on choices such as segmentation parameters (k, m), CPV threshold (95%), and PCA assumptions (linearity, approximate diagonal covariance), and robustness to autocorrelation/nonstationarity in vibration features is not explored. No direct comparison is provided against established health indicators or prognostic feature-learning methods beyond qualitative discussion of “common features,” limiting evidence of relative advantage. The paper focuses on indicator construction rather than linking to actionable maintenance decisions (e.g., thresholds, false-alarm behavior) or quantified RUL accuracy.",The authors state that current work focuses on investigating methods to exploit the information provided by the three indicators for estimating the bearing remaining useful life.,"Validate the indicators across multiple operating conditions, bearing types, and additional real industrial datasets, and report robustness to noise, changing regimes, and sensor placement. Develop a complete prognostics pipeline that converts SDHT2/VSDHT2/NVSDHT2 into RUL estimates with uncertainty (e.g., Bayesian/state-space models) and compare against established PHM benchmarks and metrics. Provide principled/automatic selection of segmentation and PCA parameters (online/self-starting versions) and assess sensitivity to Phase I training size. Release reference implementations and computational complexity analysis to support adoption in real-time condition monitoring systems.",2012.03830v1,https://arxiv.org/pdf/2012.03830v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:01:39Z
FALSE,NA,Other,Other,Not applicable,Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper is an overview article on “symmetric losses” in binary classification and how they enable robust learning from corrupted/noisy labels. It reviews empirical risk minimization, alternative evaluation metrics beyond classification error rate (balanced error rate and AUC), and defines symmetric losses via the condition $\ell(z)+\ell(-z)=K$. The central message is that, under mutually contaminated label noise, optimizing BER or AUC with a symmetric surrogate loss yields the same risk minimizers as if labels were clean, without needing knowledge of noise rates or class priors. The article further connects this theory to weakly supervised settings (positive–unlabeled learning and two-unlabeled-datasets learning) and to an NLP application: learning from relevant keywords and unlabeled documents via pseudo-labeling plus AUC maximization with a symmetric loss. It concludes with research directions such as new applications and designing non-symmetric losses that still benefit from symmetric conditions.","Key defining condition: symmetric loss satisfies $\ell_{sym}(z)+\ell_{sym}(-z)=K$ (constant). For corrupted-label BER, the paper highlights that $R^{\ell}_{BERg}(g)=(\pi_{\tilde P}-\pi_{\tilde N})R^{\ell}_{BER}(g)+\text{(excessive term)}$, where the excessive term becomes a constant when $\ell$ is symmetric, making minimizers coincide. Analogously for corrupted-label AUC, $R^{\ell}_{AUCg}(g)=(\pi_{\tilde P}-\pi_{\tilde N})R^{\ell}_{AUC}(g)+\text{(excessive terms)}$, and symmetry again turns the extra terms into a constant so clean/corrupted minimizers match.","The main theoretical result reviewed is that, under mutually contaminated noise, BER and AUC optimization become robust when using symmetric losses: the minimizers of the surrogate risks under the corrupted distributions coincide with those under the clean distributions, and this does not require knowledge of noise rates $(\pi_{\tilde P},\pi_{\tilde N})$ (or class prior in PU/UU variants). The paper also reports (from cited work) that symmetric losses can substantially outperform non-symmetric losses in experiments for BER/AUC under label corruption. In the keywords+unlabeled-documents application, the framework learns a reliable ranking function via AUC maximization with a symmetric loss after pseudo-labeling, with an estimation-error bound scaling like $1/(\pi_{\tilde P}-\pi_{\tilde N})$. The paper illustrates that downstream classification performance can vary greatly depending on threshold selection, even with similar AUC.","The authors note that learning with symmetric losses can be computationally hard and may lead to undesirable performance in practice, motivating exploration of non-symmetric losses that still benefit from symmetry-like conditions. They also note that in the keywords+unlabeled-documents setting, choosing a good classification threshold (e.g., for F1/accuracy) is difficult without additional assumptions such as knowing the class prior, and performance can degrade sharply with a wrong threshold.","As an overview/review-style article, it largely synthesizes prior results and does not provide a unified empirical benchmark or standardized experimental protocol across tasks/datasets to quantify gains from symmetric losses under diverse real-world noise mechanisms. The robustness guarantees discussed rely on the mutually contaminated noise model (and i.i.d.-style assumptions); robustness under instance-dependent noise, temporal dependence, or distribution shift beyond class-prior shift is not established here. Practical guidance on optimization stability, hyperparameter selection, and convergence behavior for non-convex symmetric losses in large-scale deep models is limited. The NLP application depends critically on pseudo-labeling quality (gap $\pi_{\tilde P}-\pi_{\tilde N}$), but detailed diagnostics or methods to estimate/monitor this gap in practice are not developed in this article.","The authors explicitly suggest (i) exploring more applications of symmetric losses for reliable machine learning (they mention imitation learning from noisy demonstrations and classification with rejection, and propose broader areas such as domain adaptation, open-set classification, and learning from aggregate observations). They also propose (ii) designing non-symmetric losses that can still benefit from the symmetric condition, including losses that satisfy symmetry only on a subset of the domain (e.g., barrier hinge loss) and combinations of symmetric and non-symmetric losses (e.g., mixing reverse cross-entropy with cross-entropy).","Develop self-starting or adaptive procedures that choose between symmetric and non-symmetric objectives during training based on estimated noise characteristics or optimization difficulty, potentially improving both robustness and computational tractability. Extend the theoretical results to more realistic noise and dependence structures (instance-dependent noise with weaker assumptions, covariate shift, and autocorrelated/streaming data), and provide robustness bounds under model misspecification. Provide practical, validated threshold-selection methods in fully unlabeled/weak-supervision settings that are robust to class-prior shift and do not require knowing $p(y=+1)$. Release reference implementations and reproducible benchmarks to standardize comparisons across symmetric-loss variants, pseudo-labeling strategies, and modern architectures (e.g., transformers) in large-scale settings.",2101.01366v2,https://arxiv.org/pdf/2101.01366v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:02:20Z
TRUE,Degradation modeling|Accelerated testing|RUL prediction,"Parametric (Weibull, etc.)|Bayesian|Other",Degradation measurements|Sensor/condition monitoring|Mixture of types,Not applicable,Manufacturing (general)|Semiconductor/electronics|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a generic degradation performance modeling framework that jointly incorporates mixed-type covariates—scalar stress/environmental variables (e.g., load in accelerated tests) and functional covariates derived from material microstructure images (e.g., TPC/RDF curves)—together with unit-level latent heterogeneity to capture within-unit correlation from unobserved factors. Degradation trajectories are represented using basis expansions (a path model) whose unit-specific coefficients are decomposed into population effects, scalar-covariate effects, functional-covariate effects, scalar–functional interaction effects, and a continuous latent random effect. Functional covariates and coefficient functions are reduced via Karhunen–Loève/FPC truncation (chosen by fraction of variance explained), and parameters plus latent effects are estimated using an EM algorithm with data augmentation, yielding closed-form updates. A real accelerated tribological wear case study on Cu–Ni–Sn copper alloys (height loss over time) with TEM-derived microstructure descriptors demonstrates improved fit and prediction versus six alternative models that omit functional information, interactions, or latent heterogeneity. The framework provides interpretable quantification of how load, microstructure, their interaction, and latent unit effects accelerate/decelerate degradation, supporting more accurate reliability assessment when failures are rare or absent.","The degradation path is expressed via basis functions: $y_{ij}=\sum_{\ell=0}^{L}\eta_{i\ell}\,\phi_{\ell}(t_{ij})+\epsilon_{ij}$ with $\epsilon_{ij}\sim N(0,\sigma_\epsilon^2)$. Each coefficient is decomposed as $\eta_{i\ell}=\nu_{\ell}+\beta_{\ell}^T x_i+\sum_{s=1}^S\int \alpha_{\ell s}(r)Z_{is}(r)\,dr+\sum_{p=1}^P\sum_{s=1}^S x_{ip}\int \rho_{p\ell s}(r)Z_{is}(r)\,dr+\gamma_{i\ell}$ with $\gamma_{i\ell}\sim N(0,\sigma_{\nu_\ell}^2)$. Functional covariates and coefficient functions use truncated KL expansions (FPC): $Z_{is}(r)\approx\sum_{k=1}^{K}c_{isk}\,\varpi_k(r)$ and similarly for $\alpha_{\ell s}(r)$ and $\rho_{p\ell s}(r)$, enabling EM-based maximum likelihood estimation with closed-form updates for $\zeta$ (fixed effects), $\Sigma_\nu$, and $\sigma_\epsilon^2$.","In the copper-alloy accelerated wear case study, the proposed model (Model 7) achieves the best overall performance among seven models: $R^2\approx 0.9681$, $-\log L\approx -698.7$, AIC $\approx 1413.3$, and BIC $\approx 1439.4$. Prediction errors (train,test MSE) improve to approximately (72.4, 156.3), outperforming the closest alternative without latent heterogeneity (Model 4: about (97.5, 248.9)) and models using only scalar or only functional covariates (e.g., Model 1: (1019.2, 3779.6); Model 2: (1626.8, 6646.2)). Cross-validation comparing microstructure descriptors indicates the TPC-based functional covariate yields lower total CV error than RDF, and is selected for the main comparisons. Estimated coefficients indicate load has a significant positive accelerating effect on degradation slope, the functional microstructure effect is significant for at least one FPC component, and the load–microstructure interaction is also significant (one interaction coefficient reported with p-value < 0.0001).",None stated.,"The method assumes Gaussian measurement errors and normally distributed latent random effects; robustness to heavy tails, outliers, or non-Gaussian degradation noise is not studied. The case study sample size is small (12 units, ~20 measurements each), so estimates of functional effects, interaction terms, and random-effect variances may be unstable and comparisons may depend on this specific dataset and chosen basis truncation. The approach is demonstrated primarily for approximately linear degradation paths (first-order polynomial basis); broader validation on nonlinear paths, different degradation mechanisms, and multiple simultaneous stressors is not provided.",None stated.,"Extend the framework to handle autocorrelated/irregularly sampled degradation errors and non-Gaussian noise (robust or generalized models), and to support multivariate degradation signals. Develop explicit reliability metrics such as failure-time/RUL distributions under thresholds derived from the fitted degradation model (including uncertainty propagation) and integrate with maintenance decision-making. Provide open-source software and broader empirical benchmarking across multiple datasets and degradation mechanisms, including sensitivity analysis to FPC truncation/FVE choices and interaction specification.",2101.03671v1,https://arxiv.org/pdf/2101.03671v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:02:59Z
TRUE,Failure mode analysis|Other,Simulation-based|Nonparametric/Semi-parametric|Other,Complete lifetime data|Sensor/condition monitoring|Simulated only|Other,Not applicable,Environmental monitoring|Healthcare/medical|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://gitlab.com/milidris/review_l2tse|https://CRAN.R-project.org/package=sensitivity,"The paper develops reliability-oriented sensitivity analysis (ROSA) methods for target sensitivity analysis (TSA) of failure events defined by threshold exceedance, focusing on settings with correlated inputs where classical Sobol’ indices are hard to interpret. It introduces distance-based TSA indices and then defines “(D)-target Shapley effects” by applying the Shapley value allocation to these indices; the main focus is the ℓ2 (variance-based) target Shapley effects, which provide nonnegative input importance measures that sum to one even under input dependence. Two estimators are proposed: (i) a Monte Carlo sampling-based method requiring simulation from joint/marginal/conditional input distributions, and (ii) a given-data k-nearest-neighbors method that approximates conditional distributions from an i.i.d. sample when the input distribution or model is not directly accessible. Analytical and numerical studies on Gaussian linear toy models examine behavior under dependence and highlight a limitation: correlated “exogenous” inputs can receive nonzero importance. The approach is demonstrated on a correlated-input flood model (dyke overflow probability) and on a COVID-19 ICU-capacity exceedance analysis, showing practical ranking of influential inputs for failure-event occurrence.","Failure probability (risk metric) is defined as $p_t^Y=\mathbb{P}(Y>t)=\mathbb{E}[\mathbf{1}_{\{G(X)>t\}}(X)]$. Distance-based TSA indices for subset $A$ are $T\!-
S_A^D=\frac{\mathbb{E}[D(p_t^Y,p_{t}^{Y\mid X_A})]}{\mathbb{E}[D(p_t^Y,p_{t}^{Y\mid X})]}$, with the ℓ2 choice yielding $T\!-
S_A^{\ell_2}=\frac{\mathrm{Var}(\mathbb{E}[\mathbf{1}_{\{G(X)>t\}}\mid X_A])}{\mathrm{Var}(\mathbf{1}_{\{G(X)>t\}})}$. Target Shapley effects allocate these subset contributions via $T\!-
Sh_j^D=\frac{1}{d}\sum_{A\subseteq \{-j\}} \binom{d-1}{|A|}^{-1}(T\!-
S_{A\cup\{j\}}^D-T\!-
S_A^D)$; for ℓ2 this is the paper’s main index and is nonnegative and sums to 1.","For the linear Gaussian toy model with three independent standard normal inputs and $Y=\sum_i X_i$, the target Shapley effects are constant in threshold and equal: $T\!-
Sh_1=T\!-
Sh_2=T\!-
Sh_3=1/3$. In a correlated-input toy model where $\rho(X_2,X_3)=\rho$, the indices satisfy $T\!-
Sh_2=T\!-
Sh_3$ and increase with $\rho$ (while $T\!-
Sh_1$ decreases) for fixed thresholds, reflecting attribution of variance changes induced by dependence. In the flood application (dyke height $t=54.5$ m, reference failure probability $p_t^Y\approx 4.5\times 10^{-3}$), estimated target Shapley effects (KNN, $N=2\times10^5$, $N_s=2$) are approximately $Q\,24.3\%\,(\pm1.3\%)$, $K_s\,22.6\%\,(\pm1.3\%)$, $Z_v\,16.7\%\,(\pm1\%)$, with remaining inputs around 12% each. In the COVID-19 ICU-capacity study, the dominant influential parameter depends on the threshold; for less restrictive thresholds, the lockdown timing parameter $N$ can exceed 50% of the target Shapley allocation, while for very restrictive thresholds the allocation tends toward a near-uniform distribution across inputs due to strong interaction effects from the indicator event and Shapley sharing of interactions.","The authors note that Shapley-based importance does not identify causal effects: highly correlated but exogenous inputs can receive a nonzero share of variance, so correlation/multicollinearity diagnostics and input validation are recommended. They also state that as the failure event becomes increasingly rare, many inputs tend to appear influential and it becomes difficult to distinguish interaction versus correlation contributions, limiting interpretability in extreme-rare-event regimes. They mention computational expense for the sampling-based estimator and reduced accuracy for the given-data KNN estimator as trade-offs.","The method’s interpretability hinges on the choice of distance/dispersion (mainly ℓ2 variance of an indicator), which may not align with engineering risk priorities (e.g., tail severity beyond exceedance, multiple failure modes). The KNN given-data estimator requires i.i.d. samples; many reliability datasets and simulation designs involve dependence, stratification, or sequential/adaptive sampling, which could bias KNN-based conditional approximations. For moderate/high input dimension, estimating many subset functionals (or relying on random permutations) can be computationally heavy and may exhibit high variance, yet the paper provides limited guidance on tuning (neighbors, permutations) and diagnostics of estimator stability beyond illustrative studies.","They propose improving sampling-based estimation using variance-reduction techniques such as importance sampling, and leveraging copula-based approaches for efficient Shapley estimation. They also suggest developing additional given-data estimators via connections between target Sobol’ indices and squared mutual information, and exploring random-forest-based given-data procedures for reliability-oriented settings. Finally, they suggest extending the framework to better decompose and quantify the origin of effects (interaction vs statistical dependence), referencing interaction-quantification developments (e.g., Shapley–Owen indices) and considering alternative, data-driven allocation systems beyond uniform Shapley weighting.","A useful extension would be to integrate rare-event simulation methods (subset simulation, cross-entropy, splitting) directly into target Shapley estimation to keep variance controlled when $p_t^Y$ is very small. Another direction is to develop robustness versions for model misspecification and for non-i.i.d./time-dependent inputs (e.g., autocorrelated environmental drivers), including uncertainty quantification for the Shapley estimates under dependence. Finally, providing a validated software workflow (automatic tuning of KNN/permutation parameters, convergence diagnostics, and reproducible benchmarks across standard reliability test functions) would improve practical adoption in engineering risk studies.",2101.08083v3,https://arxiv.org/pdf/2101.08083v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:03:53Z
FALSE,Other,Bayesian|Stochastic process|Physics-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://data.giss.nasa.gov/modelforce/strataer/,"The paper develops a hierarchical Bayesian framework for producing probabilistically reliable projections of global mean surface temperature (GMST) by fitting physically motivated multi-box energy balance models (EBMs) to CMIP5 climate-model outputs and historical observations. It demonstrates that EBMs calibrated on one forcing scenario (e.g., abrupt 4×CO2) yield biased and overconfident projections under different scenarios, and interprets the resulting errors as an effective forcing discrepancy. To correct this, the authors introduce a shared-plus-model-specific forcing-discrepancy model (random walks) learned from climate-model ensembles and propagated into future projections, then use cross-validation in a “perfect model” setting to assess reliability via standardized predictive errors. The approach produces narrower projection bands and lower projected warming than common IPCC-style anomaly/plume methods, implying a reduced probability of exceeding 2.0 K under RCP4.5 (e.g., 0.72 by 2100 vs 0.89 for the IPCC anomaly method). The work advances probabilistic climate-projection methodology, but it is not primarily about engineering reliability of physical components/systems.","The core EBM is a 3-box energy balance ODE system with stochastic variability: $C_1\,dT_1/dt = F - k_1T_1 - k_2(T_1-T_2) + w_T(t)$, $C_2\,dT_2/dt = k_2(T_1-T_2) - \epsilon k_3(T_2-T_3)$, $C_3\,dT_3/dt = k_3(T_2-T_3)$, and radiation balance $N(t)=F(t)-k_1T_1(t)+(1-\epsilon)k_3(T_2-T_3)$. For historical/future runs the forcing is augmented with volcanic forcing and a forcing discrepancy: $dF/dt=-\gamma\,[F-F_CX_C(t)-F_VX_V(t)]+w_F(t)$ and $C_1\,dT_1/dt=F+\delta-k_1T_1-k_2(T_1-T_2)+w_T(t)$, with discrepancy modeled as a random walk around a shared random-walk mean: $d\delta/dt=\nu(t)+w_\delta(t)$ and $d\mu/dt=\nu(t)$. Model parameters are pooled across CMIP5 models via $\log(\phi_m)\sim\mathcal N(\mu_\phi,\Sigma_\phi)$ and the real-world via $\log(\phi_Z)\sim\mathcal N(\mu_\phi,\kappa^2\Sigma_\phi)$.","Cross-validation (“leave-one-model-out”) under RCP4.5 shows standardized predictive errors for GMST are mostly within ±2 through 2100, indicating substantially improved reliability versus fitting EBMs only to abrupt 4×CO2 (which produced errors exceeding 6σ). For the real world under RCP4.5, projected warming above pre-industrial in 2100 is 2.2 K with a 90% credible interval of 1.7–2.9 K; an enlarged CMIP5 ensemble gives 2.5 K (1.6–4.0 K) and an IPCC anomaly method gives 2.6 K (1.8–3.4 K). The probability of exceeding 2.0 K under RCP4.5 is estimated as 0.72 by 2100, compared with 0.89 using the IPCC anomaly approach. Estimated real-world equilibrium climate sensitivity (ECS) is median 3.2 K with 90% credible interval 2.1–5.1 K, similar to the CMIP5 ensemble median 3.3 K (2.1–5.3 K).","The authors note the discrepancy random-walk formulation causes uncertainty to grow without bound even after reaching equilibrium, making it unsuitable for very long (multi-century) projections without modification. They also note exchangeability of climate models is imperfect due to model dependence/common components, so they analyze a thinned subset of CMIP5 models to better satisfy exchangeability. They mention they use only one run per scenario per model despite multiple initial-condition runs existing, potentially discarding information (though they argue EBMs reduce the marginal value of extra runs). They also state their scenario-specific learning of the shared discrepancy (using historical plus future simulations) limits projections to scenarios with available model outputs unless the discrepancy is learned only from the historical period (at the cost of increased uncertainty).","The approach assumes a linear time-invariant EBM structure with additive Gaussian noises; this may miss nonlinear feedbacks or state-dependent variability that become important under stronger forcing or different regimes, potentially leading to misspecification even if short-horizon reliability looks good. The shared discrepancy learned from CMIP5 models is treated as applicable to the real world; this risks importing ensemble-specific structural biases (e.g., common missing processes across CMIP5) directly into real-world projections. Reliability is assessed mainly via perfect-model cross-validation and standardized errors under a limited set of scenarios (notably RCP4.5/RCP8.5), leaving robustness to alternative forcings, autocorrelated observation errors, and non-Gaussian uncertainties less explored. No implementation details (software, diagnostics code) are provided, which may limit reproducibility and practical adoption.","They suggest extending the framework to incorporate multiple initial-condition runs per model without biasing toward models with more runs. They also propose developing ways to include all available models by combining prior knowledge with diagnosed inter-model dependences, relaxing the exchangeability approximation used with a thinned ensemble. They discuss modifying the approach to allow projections under arbitrary future emissions scenarios by learning discrepancies only from the historical period and treating future shared discrepancy as unknown (with larger uncertainty), which would require more careful discrepancy specification. They call for further research to holistically quantify uncertainty from future emissions pathways within the framework.","Develop a principled dependence model (e.g., hierarchical clustering or explicit shared-component priors) that uses all models while down-weighting near-duplicates, rather than relying on manual thinning. Add robustness to autocorrelated and nonstationary observational errors in GMST (and possibly assimilate satellite-era top-of-atmosphere radiation with explicit error models) to reduce sensitivity to simplifying assumptions. Explore alternative discrepancy dynamics (e.g., mean-reverting or regime-switching processes, time-varying innovation variance) and formally compare them via out-of-sample scoring rules to balance flexibility and long-horizon behavior. Provide an open-source reference implementation and standardized benchmarks for reliability evaluation across multiple scenarios and CMIP phases (CMIP5/CMIP6) to improve reproducibility and uptake.",2101.08198v1,https://arxiv.org/pdf/2101.08198v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:04:44Z
TRUE,Software reliability|System reliability|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Bayesian|Other",Event/count data|Mixture of types|Other,Not applicable,Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/|https://www.getcruise.com/|https://www.pony.ai/en/index.html|https://waymo.com/|https://zoox.com/,"The paper develops a reliability analysis framework for AI systems using public recurrent-event data from California DMV autonomous-vehicle (AV) testing, where “disengagement” events serve as a proxy for AI-system reliability. Disengagement occurrences are modeled as mileage-adjusted nonhomogeneous Poisson processes (NHPPs), using standard software-reliability parametric intensity models (Musa–Okumoto, Gompertz, Weibull) and a newly proposed flexible nonparametric baseline cumulative intensity function (BCIF) model built from monotone I-splines. Maximum likelihood estimation is used, with model selection via AIC; uncertainty quantification and simultaneous confidence bands are obtained using a fractional-random-weight bootstrap, enabling adequacy checking of parametric models against spline-based bands. A gamma frailty extension is fit to test for between-vehicle heterogeneity; likelihood ratio tests suggest little heterogeneity for the four manufacturers analyzed. The methods are demonstrated on Waymo, Cruise, Pony AI, and Zoox, concluding that reliability (lower disengagement intensity per k-mile) generally improved over the 2017–2019 study period for several fleets, while the best-fitting parametric form differed by manufacturer.","Disengagement events for vehicle $i$ are modeled via an NHPP with mileage-adjusted intensity $\lambda_i[t;x_i(t),\theta]=\lambda_0(t;\theta)\,x_i(t)$ and cumulative intensity $\Lambda_i[t;x_i(t),\theta]=\int_0^t \lambda_0(s;\theta)x_i(s)\,ds$, where $\lambda_0$ and $\Lambda_0$ are baseline intensity and baseline cumulative intensity (event rate per k-mile). Parametric BCIFs include Musa–Okumoto, Gompertz, and Weibull; the nonparametric proposal models $\Lambda_0(t;\theta)=\sum_{l=1}^{n_s}\beta_l\gamma_l(t)$ with I-spline bases $\gamma_l(t)$ and constraints $\beta_l\ge 0$ to ensure monotonicity, and $\lambda_0(t)=d\Lambda_0(t)/dt$. Heterogeneity is assessed via a gamma frailty model $\lambda_i[t;u_i,x_i(t),\theta]=u_i\lambda_0(t;\theta)x_i(t)$ with $u_i\sim\mathrm{Gamma}(\text{mean}=1,\text{var}=\phi)$ and an LRT for $\phi=0$.","Using DMV data from Dec 1, 2017 to Nov 30, 2019, overall disengagement rates (events per k-mile over the period) were reported as Waymo 0.083, Cruise 0.120, Pony AI 0.225, and Zoox 0.593. AIC-based best parametric models were Gompertz (Waymo, Cruise), Weibull (Pony AI), and Musa–Okumoto (Zoox), while the spline model often achieved lower AIC except for Zoox. Estimated baseline intensities (event rate per k-mile) showed decreasing trends for Waymo, Cruise, and Pony AI (interpreted as reliability improvement), with Zoox approximately constant near 0.6 events per k-mile. Gamma frailty tests found little evidence of between-vehicle heterogeneity (p-values: Waymo 0.9721, Cruise 0.9972, Pony AI 0.9916, Zoox 0.8400; estimated frailty variances near 0). Simulation studies (with bootstrap B=5000 and N=1000 datasets) indicated spline ML estimates improve with larger fleet sizes and that spline-based simultaneous confidence bands have closer-to-nominal coverage when sample size exceeds roughly 200 units.","The authors note that currently available DMV data lack important covariates (e.g., driving speed at event time, test environment such as busy street vs freeway, and vehicle model information), limiting more granular explanations of disengagement risk. They also indicate that driverless-program data are not yet sufficiently available to analyze, implying current conclusions are based on supervised on-road testing with a safety driver and on disengagements as a proxy rather than direct safety outcomes.","Treating disengagements as a direct proxy for “AI reliability” can be confounded by differing operational domains, reporting practices, safety-driver behavior, route selection, software update cadence, and fleet mix; the model does not explicitly adjust for these factors, making cross-manufacturer comparisons potentially biased. The NHPP assumption (conditional independence given mileage and baseline intensity) may be violated by clustering, temporal dependence, or unmodeled environmental changes; the monthly-to-daily mileage approximation can also introduce measurement error in exposure. The analysis largely aggregates across vehicles within a manufacturer and finds little frailty, but low power to detect heterogeneity may occur for smaller fleets (e.g., Pony AI, Zoox), and other heterogeneity structures (time-varying frailty, covariate-driven random effects) are not explored. Model assessment relies heavily on AIC and spline-based bands; out-of-sample predictive validation or sensitivity analyses to knot placement/spline order and to exposure approximation are not emphasized.","They propose collecting and incorporating richer covariates (speed, environment, vehicle models) and extending the modeling framework to recurrent-event data with covariates. They also suggest analyzing California DMV’s newer driverless testing program data when enough events are available. Additionally, they note interest in studying computing hardware reliability (e.g., GPU lifetimes) and its relationship to AI reliability more broadly.","Developing a hierarchical/panel model that explicitly accounts for differing operational design domains (ODDs) and reporting/driver intervention policies would improve fairness of cross-manufacturer comparisons and interpretability of “reliability.” Extending the framework to handle dependence and clustering (e.g., self-exciting processes, renewal processes, or robust sandwich inference for NHPP misspecification) could address likely temporal correlation in disengagements. Incorporating measurement-error models for exposure (mileage) and using finer-grained telemetry (if available) would reduce bias from monthly-to-daily approximations. Providing an open-source implementation (R package) and benchmarking against alternative recurrent-event methods (e.g., Andersen–Gill/Cox-type intensity models with offsets, piecewise-constant intensities, Bayesian NHPP with monotonic constraints) would facilitate broader adoption and reproducibility.",2102.01740v1,https://arxiv.org/pdf/2102.01740v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:05:31Z
TRUE,Degradation modeling|Maintenance optimization|System reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Condition-based|Predictive|Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"This paper develops a computational framework to optimize inspection and maintenance (I&M) plans for deteriorating structural systems with many interdependent components. The authors parameterize I&M strategies using simple heuristic decision rules (e.g., fixed inspection interval, reliability/annual-failure-probability threshold, number of components inspected, and an importance-based prioritization exponent) and then optimize the heuristic parameters via stochastic optimization (cross-entropy method) coupled with Gaussian process regression as a surrogate. System reliability updating under inspection/repair information is performed using a hierarchical dynamic Bayesian network (DBN) that models correlated component deterioration and propagates inspection information across components to compute conditional system failure probabilities over time. The method is extended to adaptive planning: after new inspection/monitoring data arrive, the heuristic parameters are re-optimized for the remaining life, which is proven to be non-worse in expected cost than continuing with the original heuristic. A numerical case study on a fatigue-deteriorating offshore wind turbine jacket benchmark (Zayas frame with 22 hotspots) shows modest cost reductions (≈5–10%) from one-step adaptation when inspection outcomes are unsurprising under the prior.","The objective is to choose heuristic parameters $w$ minimizing expected discounted life-cycle cost: $w^*=\arg\min_w\,\mathbb{E}[C_{\mathrm{tot}}\mid w]$, with Monte Carlo over inspection histories $z^{(q)}$: $\mathbb{E}[C_{\mathrm{tot}}\mid w]\approx \frac{1}{n_{MC}}\sum_{q=1}^{n_{MC}}\mathbb{E}[C_{\mathrm{tot}}\mid w,z^{(q)}]$. Conditional failure risk uses the time-to-failure distribution: $\mathbb{E}[C_F\mid w,Z]\approx c_F\sum_{i=1}^{n_T}\gamma(t_i)\left(F_{T_F\mid w,Z}(t_i)-F_{T_F\mid w,Z}(t_{i-1})\right)$, where $F_{T_F\mid w,Z}(t_i)=\Pr(F_i\mid w,Z)$ is built from DBN-computed interval failure events $F_i^*$. Fatigue crack growth per component follows Paris’ law $\frac{dD}{dt}=C(\Delta S_e\sqrt{\pi})^{M} D^{M/2}$, with inspection detection modeled by $\mathrm{PoD}(d)=1-\exp(-d/\xi)$ and repair triggered when detected damage exceeds a threshold $D_{rep}$.","For the prior (non-adaptive) optimization on the 40-year Zayas-frame case study, the selected best strategy reports heuristic parameters $\Delta T=7$ years, $p_{th}=2\times 10^{-2}$, $n_I=9$, $\eta=1.3$, with estimated expected total life-cycle cost $\mathbb{E}[C_{tot}\mid w_0^*]\approx 20.1$ (validated with 200 Monte Carlo samples after surrogate fitting). The expected cost breakdown for this strategy is approximately: failure risk 13.0, inspection-campaign 3.3, component inspection 3.0, component repair 0.8. After the first inspection at year 7 with no detected damage on 9 inspected components, re-optimization yields $\Delta T=9$ years, $p_{th}=3\times 10^{-2}$, $n_I=9$, $\eta=2.2$, with posterior expected remaining-life cost about 15.9 (excluding initial inspection cost). The estimated expected gain from adapting rather than continuing the original strategy under this outcome is about 1.0 (discounted, excluding initial cost), mainly from fewer inspections/repairs with little change in failure risk.","The authors note that deterioration and service-life modeling involve assumptions; in particular, uncertainty in the service life could affect results but is not investigated. They also state that DBN discretization affects accuracy (especially for Bayesian smoothing due to rough discretization in the failure domain), though they found limited impact on interval failure probability estimates in their application. For practical deployment, they highlight that building the probabilistic deterioration/structural model and translating/calibrating it into a DBN is non-trivial and would benefit from software automation or alternative Bayesian/reliability updating engines. They also acknowledge that the heuristic approach yields only approximately optimal strategies and depends on the chosen heuristic family.","Results are demonstrated primarily on a single benchmark fatigue frame; generalizability to other deterioration mechanisms, inspection technologies, and real operational constraints (crew logistics, weather windows, access, downtime coupling) is not empirically validated. The optimization uses $n_{MC}=1$ during cross-entropy search with a surrogate fit, which can be sensitive to noise and surrogate misspecification; more robust budget allocation or repeated evaluations of elite points could change the selected optimum. Repair is modeled as immediate and “complete” (perfect maintenance restoring initial-state distribution and breaking correlation), which may overstate benefits versus imperfect repair/partial crack remediation. The independence approximation used to convert interval failure probabilities to cumulative failure probabilities (product form) may be less accurate in other structural reliability settings without further validation.","They propose adjusting the inspection prioritization heuristic for non-equi-correlated components by incorporating correlation structure so that highly correlated components (and representativeness of the system) are appropriately emphasized, and by accounting for varying inspection costs. They suggest that the DBN step could be replaced by other fast Bayesian analysis and system-level reliability updating methods, and that software tools could automate DBN translation/calibration. They also state that extending the framework to include monitoring data is straightforward when Bayesian system analysis with monitoring data is available, enabling optimization of monitoring systems.","Extend the framework to imperfect maintenance models (minimal repair, partial restoration, repair-induced defects) and to multi-level actions (repair/replace/strengthen) with resource constraints. Develop robust/self-starting versions that handle model misspecification (e.g., non-Markovian effects not captured by augmentation, load autocorrelation, inspection bias) and quantify robustness of policies to prior assumptions. Incorporate explicit logistics/economic constraints (crew routing, weather-driven access, grouped/opportunistic maintenance across assets such as wind-farm fleets) and evaluate on real inspection datasets. Provide open-source implementations (e.g., MATLAB/Python) of the CE+DBN workflow and benchmark against alternative solvers (point-based POMDP, policy-gradient DRL) under standardized scenarios.",2102.06016v1,https://arxiv.org/pdf/2102.06016v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:06:21Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,"Parametric (Weibull, etc.)|ML-based|Other",Degradation measurements|Sensor/condition monitoring|Right-censored|Mixture of types,Not applicable,Energy/utilities|Transportation/logistics|Other,Case study (real dataset)|Other,TRUE,R|MATLAB,Not provided,http://ti.arc.nasa.gov/project/prognostic-data-repository,"The paper proposes Multivariable Fractional Polynomial (MFP) regression as a simple, interpretable, data-driven approach to model lithium-ion battery capacity degradation under dynamic (randomized load) conditions and to predict State of Health (SoH) across similar cells. Using the NASA Ames Prognostics Center of Excellence Randomized Battery Usage dataset (temperature, voltage, current time series), the authors extract stress-factor features (e.g., temperature extrema, mean current, voltage change, cycle duration, rest time, fraction of truncated steps) and fit linear regression models with MFP-selected transformations plus AIC-based variable selection. They evaluate three scenarios: using only sensor-derived features, adding an approximate capacity surrogate (from a preliminary regression), and adding the most recent observed reference-cycle capacity; prediction accuracy is reported via RMSE/RMSEnorm (including metrics up to End-of-Life at 80% nominal capacity). Reported normalized RMSEs range roughly from 1.2%–11.7% (and 0.9%–7.2% up to EoL, depending on cell/model), showing that MFP can be competitive with a deep LSTM regression network while being far more computationally efficient and easier to deploy on a battery management system. The work also provides a multi-factor interpretation of how different stress features contribute to ageing acceleration and discusses practical advantages such as coefficient interpretability and prediction intervals.","Battery discharge capacity for reference cycles is computed by integrating discharge current, $C_d=\int_0^{t_{cutoff}} I_d\,dt$ (with $I_d=1$ A in reference discharges). The target is capacity drop from nominal, $\Delta C(t)=C(t_0)-C(t)$, modeled via MFP-transformed covariates in a linear regression, $y=\tilde X\beta+\varepsilon$, where each continuous covariate may be transformed as a fractional polynomial $x^\ell$ with $\ell\in\{-2,-1,-0.5,0,0.5,1,2,3\}$ (and $x^0\equiv\log x$). Model performance is assessed with RMSE on capacity and normalized RMSE, $\mathrm{RMSE}_{norm}=\sqrt{\frac1n\sum_i\left(\frac{C_i-\hat C_i}{C_i}\right)^2}$, including counterparts restricted up to EoL (80% of nominal capacity).","On the NASA randomized-usage dataset (11 usable cells across four operating groups), MFP models achieved low prediction errors when trained on one exhausted cell and tested on other cells from the same operating group. For group 3 (train RW9; test RW10–RW12), normalized RMSE ranges were 1.49%–11.69% across the three MFP variants, with EoL-normalized RMSE ranges of about 0.96%–7.18% (depending on cell/model). Across groups 1, 2, and 4, RMSEnorm was generally below 10% and often within 5% (e.g., group 4 test RW20 about 2.06%–3.51% depending on model). Compared with a deep LSTM regression network (two 200-unit LSTM layers), MFP achieved comparable or better overall RMSEnorm (MFP roughly 2.2%–11.7% vs. D-LSTM-RN about 5.2%–13.5% in the reported group-3 experiment) while training far faster (~10 seconds in R for MFP vs. ~1 hour in MATLAB for D-LSTM-RN, as reported).","The authors note that MFP may have difficulty handling strong correlations among covariates and, like other data-driven methods, requires representative and sufficient training data; performance may degrade if the operating history contains missing data. They also acknowledge limited generalization evidence because all cells share the same chemical composition, so extension to different cell types/chemistries should be investigated. Additionally, they state that the capacity adjustment method (spline-based extrapolation to correct uncertain initial voltage) could be improved, since splines may extrapolate poorly.","The evaluation is confined to a single benchmark dataset with relatively few cells per group (often one train cell and 1–3 test cells), so uncertainty in generalization across broader operating regimes, manufacturers, and sensor noise conditions is not fully quantified. The approach assumes independence and approximate normality of regression errors; time dependence/autocorrelation and heteroscedasticity in degradation data could lead to miscalibrated prediction intervals and biased inference. Feature aggregation compresses each RW phase into scalar summaries (for MFP), which may discard informative within-cycle dynamics and could limit performance under more complex transient behaviors. Comparisons to other methods are not strictly matched (different train/test splits and objectives), and there is limited discussion of robustness to sensor bias/drift and BMS-grade measurement constraints.","They suggest extending the study to different cells (including different chemical compositions) to assess generalization beyond the current set of batteries. They also mention that more efficient methods for capacity adjustment should be investigated instead of spline extrapolation, and that a sensible next step is to move from (randomized) laboratory data to more complex real-application settings.","Develop self-starting/online variants of MFP that update coefficients and transformations as new data arrive, explicitly handling missing data and concept drift. Incorporate correlation-aware or regularized modeling (e.g., ridge/elastic net, partial least squares) and/or robust regression to address multicollinearity and outliers while preserving interpretability. Extend the framework to joint SoH–RUL forecasting with uncertainty quantification (e.g., Bayesian MFP or state-space degradation models) and validate calibration of prediction intervals. Benchmark on multiple datasets and chemistries with standardized splits and add ablation studies on feature summarization choices; also explore hybrid models combining simple FP regression with limited sequence embeddings to retain some within-cycle dynamics.",2102.08111v2,https://arxiv.org/pdf/2102.08111v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:07:06Z
TRUE,Accelerated testing|Degradation modeling|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Bayesian|Stochastic process|Physics-based|ML-based|Hybrid/Ensemble|Simulation-based|Other",Degradation measurements|Other,Not applicable,Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops optimal experimental designs for repeated-measures accelerated degradation tests (ADT) when unit-to-unit variability is modeled via linear mixed effects models with random coefficients associated with time (e.g., random intercept and/or slope). It defines “soft failure” under normal use conditions as the time when the unit’s mean degradation path (including random effects but excluding measurement error) crosses a fixed threshold, yielding an induced failure-time distribution and its quantiles (notably the median). Using maximum likelihood theory and the delta method, it derives an asymptotic variance expression for the estimator of a failure-time quantile under normal use, and shows that—when measurement times are fixed—the design problem for stress settings reduces to a c-optimal extrapolation design in the marginal stress regression model. Consequently, the optimal stress design is independent of the time-plan details and (under regularity) the same for all quantiles with non-degenerate solutions, with explicit optimal weights for common linear stress models and extensions to multiple stress variables (including product designs). The paper also discusses design implications for interaction vs. non-interaction stress structures (via Elfving’s theorem) and provides efficiency comparisons against uniform stress allocation designs.","Core model: repeated-measures degradation under stress $x_i$ at times $t_j$ is $Y_{ij}=f(x_i,t_j)^T\beta_i+\varepsilon_{ij}$ with product structure $f(x,t)=f_1(x)\otimes f_2(t)$, random effects only in the time part: $Y_{ij}=(f_1(x_i)\otimes f_2(t_j))^T\beta+f_2(t_j)^T\gamma_i+\varepsilon_{ij}$. Per-unit information for fixed effects factorizes as $M_\beta(\xi)=M_1(\xi)\otimes M_2$ with $M_1(\xi)=\sum w_i f_1(x_i)f_1(x_i)^T$ and $M_2=F_2^T V^{-1}F_2$. Soft-failure quantiles $t_\alpha$ under normal use $x_u$ satisfy $h(t_\alpha)=z_\alpha$ where $h(t)=(\mu(t)-y_0)/\sigma_u(t)$, $\mu(t)=(f_1(x_u)\otimes f_2(t))^T\beta$ and $\sigma_u^2(t)=f_2(t)^T\Sigma_\gamma f_2(t)$; asymptotic variance is $\mathrm{aVar}(\hat t_\alpha)=c^T M_\theta^{-1}c$ and for the median reduces to a c-criterion proportional to $f_1(x_u)^T M_1(\xi)^{-1} f_1(x_u)$.","With predetermined measurement times, minimizing the asymptotic variance of the estimated failure-time quantile $\hat t_\alpha$ reduces to choosing stress settings via the c-optimal extrapolation design for predicting the mean response at the normal-use stress $x_u$ in the marginal stress model; the optimal stress design is the same for all quantiles having $0<t_\alpha<\infty$ (Proposition 8.1). For a single standardized stress $x\in[0,1]$ with $f_1(x)=(1,x)^T$ and normal use $x_u<0$, the c-optimal design uses only endpoints $0$ and $1$ with weights $w^*=|x_u|/(1+2|x_u|)$ at $x=1$ and $1-w^*=(1+|x_u|)/(1+2|x_u|)$ at $x=0$ (e.g., for $x_u=-0.056$, weights are 0.05 at 1 and 0.95 at 0). Reported efficiencies show uniform endpoint allocation (0.5/0.5) can be substantially less efficient; for $x_u=-0.056$ its efficiency is about 0.55 (≈81% more units needed than optimal). For two interacting stresses on $[0,1]^2$ with linear marginal models, the c-optimal stress design is the product of the one-dimensional c-optimal designs, giving weights on the four vertices (example: for $x_{u1}=-0.5$, $x_{u2}=-0.4$, weights ≈(0.58, 0.17, 0.19, 0.06) on (0,0),(0,1),(1,0),(1,1)).",None stated.,"The optimality results are local/asymptotic and rely on correct specification of the linear mixed-effects degradation model (including the assumed interaction structure, normality of random effects/errors, and the choice that random effects attach only to time terms). The soft-failure definition uses the unit-specific mean path (excluding measurement error), which may differ from operational definitions of failure based on observed degradation; this can affect the induced failure-time distribution and design relevance. Practical constraints (discrete stress levels, cost/ethical/safety limits, and finite-sample performance when $n$ is not large) are not fully addressed, and the paper provides limited empirical validation on real datasets.","The authors note that optimal design becomes more complex when assumptions are violated, especially if the model lacks time–stress interaction terms (additive effects) or if stress variables also have random effects, which can prevent independent optimization of stress and time plans. They propose studying the impact of such deviations on optimal designs and constructing designs robust to nominal-parameter misspecification (e.g., maximin-efficient or weighted/Bayesian optimal designs). They also suggest developing criteria for simultaneous estimation of multiple characteristics of the failure-time distribution.","Developing self-starting or robust designs that remain efficient under non-normal random effects, heteroscedastic/autocorrelated measurement errors, and model-form uncertainty (e.g., nonlinear degradation or nonlinearly accelerated stress relationships such as Arrhenius/inverse power) would broaden applicability. Extending the framework to optimize designs under explicit budget/cost constraints (units vs. measurements), discrete feasible stress grids, and safety limits would improve practical deployment. Further work could benchmark finite-sample performance via simulation and validate on multiple real ADT datasets, including model diagnostic procedures to check soft-failure assumptions and extrapolation validity.",2102.09446v2,https://arxiv.org/pdf/2102.09446v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:07:59Z
FALSE,NA,ML-based|Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a LASSO-based Granger-causality (GC) test statistic (the LGC statistic) that mirrors the classical likelihood-ratio/variance-ratio GC measure but is designed for non-asymptotic (finite-sample) settings with sparse autoregressive models and potentially correlated process noise. It derives finite-sample conditions under which a threshold on the LGC statistic reliably separates the null (no GC link) from local alternatives, and it also provides a converse (necessary-condition) result showing fundamental limits on detectability. The authors further bound false-positive probability and provide power guarantees for simple thresholding rules, along with two data-driven methods for selecting thresholds to control type-I error. Empirically, they validate the theory through Monte Carlo simulations comparing against OLS-based GC and other LASSO-based GC methods (CI/debiased LASSO and truncating LASSO). A real-data case study on neural recordings under general anesthesia shows LGC detecting a spiking→LFP influence that conventional OLS-based GC fails to detect due to overfitting.","The core charting/test statistic is the LASSO-based GC (LGC) statistic: $T_{y\to x}=\frac{\ell(\hat\theta_e^{(1)},0)}{\ell(\hat\theta^{(1)},\hat\theta^{(2)})}-1$, where $\ell(\theta)=\frac{1}{n}\|x-X\theta\|_2^2$ is the empirical prediction loss, $\hat\theta$ solves the full-model LASSO and $\hat\theta_e$ solves the reduced-model LASSO with cross-lag coefficients constrained to 0. The LASSO estimators are $\hat\theta=\arg\min_\theta \frac{1}{n}\|x-X\theta\|_2^2+\lambda_n\|\theta\|_1$ and $\hat\theta_e=\arg\min_{\theta:\theta^{(2)}=0} \frac{1}{n}\|x-X\theta\|_2^2+\lambda_n\|\theta\|_1$, with theory developed for $\lambda_n\asymp \sqrt{\log(2p)/n}$. Under $\lambda_n=4A\sqrt{\log(2p)/n}$ and suitable RE/deviation conditions, a threshold exists that separates the null from alternatives with $\|\theta^{\ast(2)}\|_2^2\gtrsim k\log(2p)/n$.","The main sufficient condition (Theorem 1) shows that with $n\gtrsim k\log(2p)$ and a local alternative satisfying $\|\theta^{\ast(2)}\|_2^2\ge B\,k\log(2p)/n$, there exists a threshold that distinguishes null vs. alternative with probability at least $1-K_1 e^{-\bar c n}-K_2/p^{\bar d}$. A necessity result (Theorem 2) shows LGC can fail to reject for any threshold when $\|\theta^{\ast(2)}\|_2^2\lesssim k\log p/n$ and $n\lesssim k\log p$, with failure probability at least 1/2. Corollary 1 provides an explicit finite-sample upper bound on false-positive probability for thresholding LGC (exponential in $-n$ up to a factor depending on $\sqrt{\log(2p)/n}$) under a stronger sample requirement on the order of $k^2\log(2p)$. In the real neural dataset example, LASSO-based LGC reports $T_{\mathrm{PSTH}\to\mathrm{LFP}}=0.0096$ with p-value 0.0015 (significant), while the OLS-based GC analysis does not detect significance at 0.05.","The authors note their theory is developed for a canonical bivariate VAR setting (to parallel classical GC), though they argue it can be extended to general multivariate VAR via conditional (Geweke) GC with modified constants. They also remark that some non-asymptotic constants can be large in practice, implying the finite-sample advantages may require fairly large $n$ and $p$ to manifest. For threshold selection Method 2, they explicitly caution that estimating the RE curvature via mutual coherence can be conservative and may reduce test power.","The work assumes linear Gaussian VAR dynamics with i.i.d. Gaussian innovations and focuses on sparse autoregressive representations; robustness to strong non-Gaussianity, heteroskedasticity, or model misspecification (e.g., nonlinear dynamics) is not fully characterized in the guarantees. The theory relies on RE/deviation conditions that may be difficult to verify in real applications and may fail under strong autocorrelation, near-nonstationarity, or collinearity among lagged regressors. Practical guidance for selecting $\lambda_n$ uses cross-validation in experiments, but the theoretical results prescribe ranges/constraints on $\lambda_n$; the gap between theory and common CV practice could affect error control in practice. Code/software details are referenced (a “fast implementation” is cited) but no reproducible pipeline or implementation specifics are provided in the paper text excerpt.","The authors explicitly propose extending the results to autoregressive generalized linear models (AR-GLMs) with time-varying parameters. They also state interest in obtaining necessary conditions that hold for any test statistic (i.e., stronger converse bounds beyond the specific LGC procedure).","A useful extension would be to develop robust/self-normalized versions of LGC that handle heavy-tailed innovations, conditional heteroskedasticity, or mild nonstationarity while preserving finite-sample error control. Another direction is a fully data-adaptive, self-starting procedure that jointly chooses $p$ and $\lambda_n$ with provable control of type-I error (e.g., stability selection or selective inference) rather than relying on heuristics like cross-validation. Extending the method and theory to high-dimensional multivariate (network) GC with multiple testing/FDR control would improve practicality for neuroscience and genomics applications. Providing an open-source reference implementation (including threshold selection methods and p-value computation) and benchmarking across standard time-series causal discovery datasets would strengthen adoption and validation.",2103.02774v2,https://arxiv.org/pdf/2103.02774v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:08:47Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,Stochastic process|ML-based|Bayesian|Hybrid/Ensemble|Simulation-based,Degradation measurements|Sensor/condition monitoring|Right-censored|Mixture of types,Predictive|Condition-based,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://ti.arc.nasa.gov/tech/dash/groups/pcoe/prognostic-data-repository/,"The paper proposes a hybrid prognostics framework for remaining useful life (RUL) estimation that couples a Wiener-process degradation model with adaptive (time-varying) drift and a deep neural network (DNN) that learns the degradation evolution pattern from historical sensor/health-index data. An LSTM–CNN encoder–decoder is used for multi-step prediction of the future degradation trajectory, while Bayesian updating is used online to update the drift distribution as new observations arrive, enabling adaptation to unit-to-unit variability and changing operating conditions. To quantify uncertainty, the method learns both trajectory (mean) and heteroscedastic variance components (aleatoric and epistemic) within an extended DNN architecture trained via Gaussian negative log-likelihood. For computing RUL distributions (first-passage time to a failure threshold), the paper introduces an “interpolation approximation” algorithm that samples Gaussian paths without time-iterative particle filtering, yielding similar RUL density estimates with far lower computation. Experiments on NASA C-MAPSS turbofan engine degradation data show materially lower RMSE than adaptive/non-adaptive Wiener baselines and improved robustness versus non-adaptive DNNs on out-of-distribution subsets, while maintaining high coverage probabilities for prediction intervals.","The degradation observation model is a Wiener process with a learned evolution pattern and adaptive drift: $Z(t)=Z(t_i)+\int_{t_i}^{t} \varphi(\tau)\, dQ(\tau;\Theta)+\int_{t_i}^{t} \eta\, dB(\tau)$, with drift evolving as another Wiener process $\varphi(t)=\varphi(t_i)+\gamma\,\Lambda(t-t_i)$. The learned pattern term $Q(t;\Theta)$ is estimated by an LSTM–CNN encoder–decoder, and the predictive variance decomposes into epistemic and aleatoric parts: $\mathrm{Var}(Z(t))=\int \gamma^2(\Delta Q)^2 d\tau+\eta^2(t-t_i)$ (heteroscedastic). RUL is defined as the first-passage time to threshold $D$, and its density is approximated by sampling Gaussian interpolation curves $\chi(t)=\varphi_{t_i}Q(t;\Theta)+k\,\xi(t)$ (with $k\sim\mathcal N(0,1)$) and recording the crossing times.","On FD002 (train/test within same subset), the adaptive DNN achieves RMSE 16.25 versus 17.71 (non-adaptive DNN), 43.22 (adaptive Wiener), and 45.62 (Wiener), with 90% CI coverage (PICP) 99.45% and MPIW 81.27. When trained on FD002 and applied without retraining, adaptive DNN RMSE is 22.44 (FD001), 32.74 (FD003), and 21.12 (FD004), outperforming non-adaptive DNN (28.25, 47.97, 32.59) and adaptive Wiener (47.77, 82.94, 69.72). The interpolation approximation for RUL density is far faster than particle filtering in a linear-process test: 0.011s (Algorithm 1 with 20 lines) vs 0.592s (PF with 1000 particles), while producing smooth RUL distributions with far fewer samples. Parameter estimates indicate similar aleatoric uncertainty ($\eta_B^2\approx 6$–$7\times10^{-6}$) but lower epistemic uncertainty for adaptive DNN ($\gamma^2=6.36\times10^{-4}$) than adaptive Wiener ($\gamma^2=1.16\times10^{-3}$).","The authors state two main extensions rather than explicit limitations: (1) other stochastic processes (e.g., random-coefficient and Gamma processes) could be used instead of the Wiener process, and (2) the current single-variate prognostic framework could be extended to a multivariate framework. No other explicit limitations are clearly articulated in the provided text.","The approach relies on a Wiener/Brownian-motion noise structure and Gaussian assumptions; performance and calibration may degrade under heavy-tailed errors, outliers, or serially correlated sensor noise beyond what the HI construction captures. Results are demonstrated primarily on a benchmark (C-MAPSS) with a constructed health index and a fixed failure threshold (0.95), so real-world validity may depend strongly on HI design choices and threshold setting. The paper reports accuracy and interval coverage but provides limited calibration diagnostics (e.g., sharpness–calibration tradeoffs across multiple confidence levels) and limited ablation on key design choices (update frequency, windowing, encoder–decoder architecture). Code/software details are not provided, which may hinder reproducibility of the DNN training and the interpolation RUL algorithm.","The paper suggests extending the framework by (1) replacing the Wiener degradation process with other stochastic processes such as random-coefficient or Gamma-process models, and (2) extending the proposed single-variate prognostic framework to multivariate prognostic settings.","A valuable next step is to study robustness to non-Gaussian and autocorrelated measurement noise and to develop a robust/self-starting variant that reduces dependence on a handcrafted health index and fixed thresholds. Additional work could evaluate uncertainty calibration more thoroughly (e.g., reliability diagrams for RUL intervals, proper scoring rules like CRPS) and benchmark against modern Bayesian deep learning and conformal prediction approaches. Extending the interpolation approximation to handle multivariate degradation states and competing failure modes (multiple thresholds or hazards) would improve applicability to complex assets. Providing an open-source implementation and sensitivity analyses (update cadence, window length, architecture/hyperparameters) would strengthen reproducibility and practical adoption.",2103.11598v1,https://arxiv.org/pdf/2103.11598v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:09:33Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Sensor/condition monitoring|Other,Not applicable,Finance/economics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies the reliability/uncertainty of identifying a maximum spanning tree (MST) in correlation-based financial market networks, formalized via a random variable network (RVN) framework. For a broad class of elliptically contoured distributions, it proves that the true MST is identical across Pearson-, Fechner-, and Kendall-correlation network models (the edge ordering is preserved under monotone transforms of correlations). It argues that False Discovery Rate (FDR)—the proportion of incorrectly selected edges in the identified MST—is the most appropriate uncertainty metric for MST identification, relating it to other error rates (FWER, PFER, PCER, accuracy) in this constrained spanning-tree setting. The paper analyzes the Kruskal algorithm’s MST identification error and proves a distribution-free robustness property for Fechner-correlation-based identification (under known means): the FDR distribution does not depend on which elliptical distribution in the class generates the data. Simulation studies using U.S. stock-return data (e.g., N=10 and N=50) show Pearson-based FDR is sensitive to tail-heaviness (Gaussian vs Student-mixture), Fechner-based FDR is stable across distributions, and Kendall-based FDR is weakly distribution-dependent while often achieving the lowest (best) FDR among the three.","The RVN edge weights are correlations $\gamma_{ij}=\gamma(X_i,X_j)$; MST is obtained by Kruskal after sorting edges by $\gamma_{ij}$. Pearson: $\gamma^P_{ij}=\mathrm{Cov}(X_i,X_j)/\sqrt{\mathrm{Cov}(X_i,X_i)\,\mathrm{Cov}(X_j,X_j)}$. Fechner uses sign similarity $\gamma^{Sg}_{ij}=\mathbb{P}((X_i-\mathbb{E}X_i)(X_j-\mathbb{E}X_j)>0)$ and $\gamma^{Fh}_{ij}=2\gamma^{Sg}_{ij}-1$; Kendall: $\gamma^{Kd}_{ij}=2\mathbb{P}((X_i^{(1)}-X_i^{(2)})(X_j^{(1)}-X_j^{(2)})>0)-1$. For elliptical distributions, correlations are linked by monotone transforms, e.g. $\gamma^{Fh}_{ij}=\gamma^{Kd}_{ij}=(2/\pi)\arcsin(\gamma^P_{ij})$, implying identical true MSTs across the three networks; uncertainty is measured by $\mathrm{FDR}=\frac{FP}{FP+TP}$ for edges in the identified MST.","Simulations with mixtures $f_\varepsilon=(1-\varepsilon)\,\mathcal{N}(\mu,\Lambda)+\varepsilon\,t_\nu(\mu,\Lambda)$ (with $\nu=3$) show Pearson-based MST identification FDR increases with heavier tails: for N=10, n=1000, FDR rises from 0.15 (Gaussian, $\varepsilon=0$) to 0.34 ($\varepsilon=1$). Fechner-based FDR is essentially constant across $\varepsilon$ (distribution-insensitive), e.g., N=10, n=1000 stays near 0.33–0.34; N=50, n=10000 stays near 0.13–0.14. Kendall-based FDR is weakly dependent on $\varepsilon$ and often best: for N=50, n=10000 it remains about 0.08–0.10, compared with Pearson 0.08–0.32 as $\varepsilon$ increases. Theoretical result: with known means, Fechner/Kruskal yields a distribution-free FDR over the entire elliptical class $K(\Lambda)$.","The authors note that the Kendall-correlation network exhibits “new and surprising phenomena” and explicitly state that this behavior “needs a further investigation” / “will be a subject for further investigations,” indicating incomplete theoretical explanation for Kendall-based robustness/performance. They also remark that exact MST identification (FWER-style) can have very small probability even with large sample sizes, implying practical difficulty if one insists on perfect recovery rather than partial-error metrics like FDR.","The robustness theorem for Fechner-based FDR assumes the mean vector $\mu$ is known; in practice it must be estimated, which can affect sign-based statistics and potentially break strict distribution-freeness. The study focuses on elliptical distributions and correlation-based dependence; real financial returns can be non-elliptical, time-varying, and serially dependent (autocorrelation/volatility clustering), which may materially change MST identification error behavior. Evaluation is largely via Monte Carlo on fitted correlation/means from historical data and does not report uncertainty intervals (e.g., standard errors) for estimated FDR or sensitivity to estimation error in $\Lambda$. Implementation details (software, random seeds, computational complexity for Kendall estimates at large N and n) are not provided, which may affect reproducibility and scalability assessment.","They state that the observed Kendall-network phenomena and the apparent superiority of Kendall correlation for MST identification “needs a further investigation” and “will be a subject for further investigations,” suggesting future theoretical and empirical study to explain and validate Kendall-based performance advantages. They also position their framework as enabling broader, correct comparisons of uncertainty across networks and distributions, implying extensions to additional network constructions/measures of similarity within the RVN framework.","Extend the robustness and FDR analysis to the practically relevant case of unknown means (self-starting/plug-in estimators) and quantify the impact of mean/correlation estimation error on MST FDR. Investigate non-elliptical and time-series settings typical in finance (GARCH effects, regime shifts, nonstationarity) and assess whether Kendall/Fechner advantages persist under dependence and structural breaks. Provide scalable algorithms/approximations (especially for Kendall’s $O(n^2)$ pairwise computation) and publish reproducible code to benchmark performance across N, n, and dependence structures. Explore alternative uncertainty metrics beyond edgewise FDR (e.g., subtree/cluster stability, edge confidence via bootstrap) and diagnostics for which edges are most error-prone.",2103.14593v1,https://arxiv.org/pdf/2103.14593v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:10:13Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,Stochastic process|ML-based|Bayesian,Degradation measurements|Sensor/condition monitoring|Complete lifetime data,Predictive|Condition-based,Transportation/logistics|Energy/utilities|Other,Case study (real dataset)|Other,TRUE,Python,Not provided,NA,"This paper studies uncertainty-aware Remaining Useful Life (RUL) prediction for predictive maintenance using probabilistic machine learning models that output both point estimates and calibrated uncertainty. It evaluates scalable Gaussian-process-based approaches—Stochastic Variational Gaussian Processes (SVGP), Deep Gaussian Processes (DGP), and Deep Sigma Point Processes (DSPP)—and compares them against a Bayesian deep learning baseline using Monte Carlo Dropout (MCD) and a standard feed-forward neural network (FFNN). Experiments are conducted on NASA’s N-CMAPSS aircraft-engine run-to-failure dataset (multivariate sensor time series plus physics-inferred features), emphasizing robustness under distribution shift between training and test operating conditions. Performance is assessed with RMSE and negative log-likelihood (NLL), plus prognostics-specific metrics (α–λ and probabilistic Pα–λ) to reflect accuracy within a tolerance band while incorporating predictive uncertainty. Results indicate DSPP provides the best uncertainty quality (lowest NLL and strongest Pα–λ), while MCD attains the best RMSE, and both yield physically meaningful uncertainty that decreases toward end-of-life and increases under distribution shift.","SVGP uses an ELBO objective (Eq. 1) with predictive mean/variance from inducing points (Eq. 2) and a Gaussian predictive distribution $p(y_*|x_*)=\mathcal N(\mu_f(x_*),\sigma_f^2(x_*)+\sigma_{obs}^2)$ (Eq. 3). DGPs compose multiple GP layers and yield a predictive distribution that is a (generally intractable) mixture of Gaussians over latent features (Eq. 5), approximated by Monte Carlo. DSPP replaces the continuous mixture with a finite parametric mixture via quadrature/delta-mixture approximation (Eqs. 7–8) and uses a modified objective treating observation noise and latent variance consistently (Eq. 6). Uncertainty-aware evaluation includes probabilistic $P_{\alpha-\lambda}=F((1+\alpha)\lambda^*,\mu,\sigma)-F((1-\alpha)\lambda^*,\mu,\sigma)$ (Eq. 18), averaged over time (Eq. 19).","On the held-out N-CMAPSS test set, DSPP achieved the best NLL (3.10) and the best probabilistic accuracy metric $P_{\alpha-\lambda}$ (0.53), while MCD achieved the best RMSE (7.31). DGP performed competitively (NLL 3.24, RMSE 7.37, $P_{\alpha-\lambda}$ 0.46) and SVGP lagged (NLL 3.50, RMSE 8.70, $P_{\alpha-\lambda}$ 0.36). In terms of the deterministic $\alpha-\lambda$ metric, DSPP and MCD tied for best (0.56). Qualitatively, uncertainty bands (±2σ) shrink as engines approach end-of-life and increase for the test unit exhibiting a stronger operating-condition distribution shift, indicating sensible uncertainty behavior.","The authors note that extending the GP-based approaches to better capture temporal correlations in time-series data is not straightforward, unlike for MCD where it would be easier to swap in a 1D CNN. They also mention that for DGPs they compute a biased estimator when applying Monte Carlo sampling to the continuous mixture of Gaussians predictive distribution. These points constrain how directly the current formulations address sequential dependence and exact probabilistic evaluation in deep GP settings.","The study is based on a single benchmark dataset (N-CMAPSS) with a small number of units (6 train, 3 test), which may limit generalizability across assets, sensors, and failure modes. Comparisons focus on a limited set of uncertainty-aware baselines; other strong UQ approaches in prognostics (e.g., deep ensembles, quantile regression, conformal prediction, heteroscedastic likelihood models with temporal architectures) are not comprehensively benchmarked. The work emphasizes predictive metrics but does not translate uncertainty into explicit maintenance decision optimization (e.g., cost/risk trade-offs) or calibration diagnostics (e.g., reliability diagrams, coverage vs. nominal). Practical deployment aspects (computational cost on-device, monitoring drift detection, retraining strategy) are not analyzed.","The authors propose extending the methods to better capture temporal correlations in time-series data, expecting improved performance; they note this is straightforward for MCD by moving from fully-connected layers to a 1D CNN but non-trivial for GP-based models. They also propose investigating more recent Bayesian deep neural networks and benchmarking them against the GP-based methods studied here.","A natural extension is to incorporate explicit sequence models for GP-based approaches (e.g., recurrent/state-space GP layers or temporal kernels) and evaluate under varying sampling rates and autocorrelation. Further work should assess uncertainty calibration more directly (coverage tests, calibration curves) and robustness under sensor faults and missing data. Integrating the predicted RUL distributions into an explicit maintenance optimization or risk-aware decision framework (e.g., expected cost of early/late maintenance) would strengthen the reliability engineering impact. Releasing reproducible training/evaluation code and standardized splits would also improve adoption and fair comparison.",2104.03613v1,https://arxiv.org/pdf/2104.03613v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:10:56Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,2104.05449v3,https://arxiv.org/pdf/2104.05449v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:10:56Z
FALSE,NA,ML-based,Sensor/condition monitoring|Mixture of types,Not applicable,Healthcare/medical,Other,TRUE,Python|Other,Not provided,https://biolincc.nhlbi.nih.gov/studies/coag/,"This paper studies whether using longitudinal warfarin dosing history and repeated INR measurements improves machine-learning dose revision algorithms for predicting a patient’s maintenance warfarin dose. Using the COAG clinical trial dataset, the authors train several regression models (linear, ridge, Bayesian ridge, decision tree, gradient boosting, and multilayer perceptron) with three feature sets: clinical-only, clinical plus a single dose/INR pair (7–10 days), and clinical plus a longitudinal dose/INR sequence. Models are trained with an 80/20 split and 10-fold cross-validation and evaluated with MSE, MAE, R^2, and the percent of predictions within ±20% of the true dose. Gradient boosting with longitudinal INR features provides the best performance, indicating that additional longitudinal dose-response data better captures inter-individual variability and improves dose prediction accuracy. The work is a healthcare ML prediction study rather than reliability engineering (no failure-time/degradation/maintenance optimization modeling).","The feature sets are defined as: Clinical = {Sex, Race, Age, Weight}; Single INR = Clinical + Dose(t1−1) + INR(t1); Longitudinal INR = Clinical + Dose(t1−1) + INR(t1) + Dose(t1) + INR(t2), where t1 is the first INR measurement day within 7–10 days from initiation and t2 is the next INR measurement after t1. Model performance is quantified using MSE, MAE, and coefficient of determination R^2 between predicted and true maintenance dose; an “ideal estimated dose” is defined as prediction within ±20% of the true maintenance dose.","On the test set, the baseline revision algorithm has MSE 3.13 (95% CI 2.07–4.19) mg^2/day^2, MAE 1.27 (1.05–1.49) mg/day, R^2 = 0.54, and ideal estimated dose 54.9%. Gradient boosting with a single INR feature (GB-SINR) improves to MSE 2.03 (1.33–2.73), MAE 1.06 (0.89–1.23), R^2 = 0.70, and ideal estimated dose 63.11%. Gradient boosting with longitudinal INR features (GB-LINR) further improves to MSE 1.29 (0.78–1.79), MAE 0.81 (0.67–0.95), R^2 = 0.81, and ideal estimated dose 75.41%. The reported confidence interval differences versus baseline exclude zero for MSE/MAE, indicating statistically significant improvements.",The authors state limitations including the small dataset size and the lack of clinical validation. They note that future work should involve larger and more diverse datasets and prospective clinical validation.,"The evaluation is limited to a single dataset (COAG) with pruning to participants having INR data in days 7–10, which may introduce selection bias and reduce generalizability to other care settings or monitoring schedules. The approach treats a small number of discrete INR/dose points as features rather than explicitly modeling time irregularity, measurement noise, and delayed pharmacodynamic effects (e.g., via longitudinal/sequence models). External validation across institutions and robustness checks across subgroups (beyond stratified split by race) are not described, and comparisons are primarily against the COAG baseline rather than a broader set of modern dosing algorithms.",The authors propose developing models using larger and more diverse datasets and performing clinical validation in a prospective warfarin dosing trial. They also mention future exploration of genotype-based dose revision algorithms.,"A natural extension would be to use explicit temporal models (e.g., state-space or sequence models) that can handle irregular INR sampling and dose timing while accounting for warfarin pharmacokinetics/pharmacodynamics. Additional work could include external multi-site validation, calibration analysis for clinical decision-making, and subgroup/fairness assessments (e.g., by race, age, comorbidities, interacting medications). Sharing code and trained model specifications (including preprocessing and hyperparameters) would improve reproducibility and facilitate clinical translation.",2105.02625v1,https://arxiv.org/pdf/2105.02625v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:11:29Z
TRUE,System reliability|Other,Simulation-based|ML-based|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,MATLAB,Not provided,https://www.csccm.in/,"The paper proposes a high-dimensional structural reliability analysis framework that couples dimension reduction via active subspaces with surrogate modeling to estimate failure probability efficiently. To avoid expensive gradient evaluations required by standard active subspace methods, it introduces a Sparse Active Subspace (SAS) approach where a sparse polynomial chaos expansion (LAR-based S-PCE) is first trained and then differentiated to approximate the gradient information used to form the active-subspace matrix. The reduced coordinates are then used to train a higher-fidelity hybrid surrogate (H-PCFE: PCFE + Gaussian process) aimed at better capturing local/tail behavior critical for probability-of-failure estimation. The framework is evaluated on three benchmark problems (Sobol function with up to 100 variables, a 20-variable composite beam stress limit state, and a 33-variable 25-element space truss displacement limit state), comparing probability of failure and reliability index against crude Monte Carlo, FORM, SORM, and S-PCE. Results show SAS-HPCFE achieves near-MCS accuracy with substantially fewer true model evaluations than S-PCE and far better accuracy than FORM/SORM on the nonlinear benchmarks, demonstrating scalability with input dimension.","Failure probability is defined via the limit-state function $g(x)$ with failure domain $\Omega_F=\{x:g(x)<0\}$ and $P_f=\int_{\Omega_F} dF_X(x)=\int \xi_{\Omega_F}(x)\, dF_X(x)$. Active subspaces are computed from the average derivative functional $C=\int (\nabla_x f)(\nabla_x f)^T\rho\,dx$ (estimated by Monte Carlo) and eigen-decomposed $C=W\Lambda W^T$ to obtain the active directions $W_1$ and reduced variables $z=W_1^T x$. The low-fidelity surrogate uses S-PCE $\hat f(x)=\sum_{\beta} a_\beta\psi_\beta(x)$ to enable gradient-based SAS, while the high-fidelity surrogate is H-PCFE with additive structure $Y=f_{\mathrm{PCFE}}+f_{\mathrm{GP}}$ and GP predictive mean/variance (e.g., $\mu(x^*)=g_0+\Phi(x^*)\alpha+r(x^*)R^{-1}(d-\Psi\alpha)$).","Sobol limit state: for $m=40$, MCS gives $P_f\approx0.0178$ ($\beta=2.1015$) while SAS-HPCFE gives $P_f\approx0.01809$ ($\beta=2.0949$) with 0.314% error using $N_s=900$; S-PCE is much worse ($\beta=2.5284$, 20.31% error) at $N_s=1300$, and FORM/SORM have very large errors. For $m=100$, SAS-HPCFE attains about 1.01% error with $N_s=1100$, while FORM/SORM/S-PCE are substantially less accurate and SORM requires many more evaluations (reported $N_s=5150$). Composite beam (20 variables): MCS $\beta=2.9112$ ($P_f\approx0.0018$) vs SAS-HPCFE $\beta=2.9346$ ($P_f\approx0.00167$) with 0.804% error using $N_s=800$; SORM did not converge even after $10^4$ evaluations. 25-element space truss (33 variables): MCS $P_f=0.1067$ ($\beta=1.2443$) vs SAS-HPCFE $P_f\approx0.10515$ ($\beta=1.2527$) with 0.675% error using $N_s=1000$, outperforming FORM/SORM and slightly improving over S-PCE (3.87% error, $N_s=1100$).","The authors note that SAS-HPCFE currently trains PCE, identifies the active subspace, and trains H-PCFE in separate stages; they suggest the framework could be accelerated by performing these steps simultaneously. They also acknowledge that active subspace identifies only a linear manifold, and that nonlinear manifold learning could further reduce dimension but introduces additional challenges. Finally, they state that all demonstrated examples are static problems and that performance on dynamical systems requires further investigation.","The approach relies on the ability of the initial sparse PCE to approximate global behavior well enough that its (surrogate) gradients yield a trustworthy active subspace; if the low-fidelity model is misspecified, the learned subspace may be misleading and propagate error to the final reliability estimate. The examples are benchmarked primarily against crude MCS/FORM/SORM/S-PCE; comparisons against state-of-the-art adaptive rare-event methods (e.g., subset simulation variants, AK-MCS/kriging-based reliability, cross-entropy, line sampling with surrogates) are limited, which makes it harder to position performance relative to strong modern baselines. Practical implementation details that affect robustness—e.g., sensitivity to the active-subspace threshold choice (0.98), the PCE maximum order, and the GP kernel/optimization stability—are not extensively stress-tested across different distributions/correlation structures. Additionally, the paper emphasizes function-evaluation counts but does not fully account for optimization and linear-algebra costs (e.g., GP likelihood optimization and matrix factorizations), which may matter for large training sets.","They propose accelerating the overall framework by jointly (rather than sequentially) training PCE, identifying the active subspace, and training H-PCFE. They suggest exploring nonlinear manifold learning methods to potentially achieve greater dimension reduction than linear active subspaces. They also propose extending and testing the method on dynamical systems, since the current demonstrations are limited to static problems.","Develop a self-starting/adaptive sampling strategy targeted at the limit-state boundary (active learning) in the reduced space to further reduce high-fidelity evaluations while controlling surrogate error in the failure region. Extend the method to handle correlated inputs and nonstandard/nonindependent distributions more explicitly (including isoprobabilistic transforms and robustness checks), since active-subspace estimation and PCE sparsity can be sensitive to input structure. Provide open-source implementations (e.g., MATLAB/Python) and standardized benchmarks to enable reproducibility and facilitate comparison with modern surrogate-assisted reliability methods (AK-MCS, subset simulation with surrogates, etc.). Investigate uncertainty quantification for the learned subspace and propagated epistemic uncertainty in $P_f$ (e.g., Bayesian treatment of subspace and surrogate parameters) to provide calibrated confidence intervals for reliability estimates.",2105.04979v2,https://arxiv.org/pdf/2105.04979v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:12:16Z
FALSE,NA,Nonparametric/Semi-parametric|ML-based|Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Other,Not provided,https://www.who.int/en/news-room/fact-sheets/detail/schistosomiasis,"This paper develops a general framework for estimating population size from K-list capture–recapture data, emphasizing how identification constraints (e.g., no highest-order interaction, independence, conditional independence) determine the estimand and can substantially change results. It derives identified target parameters, efficient influence functions, and asymptotically linear/efficient estimators under both linear constraints (yielding simple plug-in NPMLEs) and nonlinear constraints (including log-linear no K-way interaction), where empty capture-pattern cells cause practical non-identifiability for naive plug-in estimators. To address the curse of dimensionality under the log-linear no K-way interaction constraint, it proposes an undersmoothed lasso initial estimator and a targeted maximum likelihood estimator (TMLE) update that targets the efficient influence function. Extensive simulations evaluate estimator bias/variance/coverage under correct and misspecified identification assumptions, showing severe bias when assumptions are violated. A real-data application estimates schistosomiasis prevalence using three surveillance systems in southwestern China, illustrating large sensitivity of estimates to the chosen identification assumption.","The full-data target is $\Psi^F(P^*)=P^*(B^*\neq 0)$, with population size estimated by $\hat N = n/\hat\psi$. Under a linear constraint $E_{P^*}[f(B^*)]=0$ with $f(0)\neq 0$, the identified observed-data target is $\Psi_f(P)=\frac{f(0)}{f(0)-P f}$. Under the log-linear “no K-way interaction” constraint, the identified target is $\Psi_I(P)=\left[1+\exp\{(-1)^{K+1}\sum_{b\neq 0} f_I(b)\log P(b)\}\right]^{-1}$, and the TMLE updates an initial $P_n^0$ along a least-favorable submodel $P_{n,\varepsilon}\propto (1+\varepsilon D^*(P_n))P_n$ until $P_n D^*(P_n^*)\approx 0$.","Simulation studies show that when identification assumptions hold, plug-in and efficient-influence-function-based CIs achieve near-nominal 95% coverage for the corresponding estimand, but assumption violations induce substantial asymptotic bias and severe undercoverage across all estimators. Under log-linear no K-way interaction with empty cells, the naive plug-in estimator may be undefined; the proposed undersmoothed-lasso+TMLE approach remains computable and improves bias/coverage relative to parametric log-linear estimators in these sparse settings. In one reported setting with main effects only, TMLE based on undersmoothed lasso achieved mean $\approx 0.6078$ with coverage 93.7%, comparable to the nonparametric plug-in (95%), while cross-validated lasso produced large bias and poor coverage. In the schistosomiasis application (three surveillance lists), estimated capture probability $\hat\psi$ varied widely by assumption (e.g., around 0.8935 to 0.9969 with differing CIs), demonstrating high sensitivity to identification choices.","The authors emphasize that estimates depend critically on the chosen identification constraint and can change “dramatically,” so the constraint should be known to hold by design; otherwise, all estimators can be unreliable due to identification bias. They note that for the common log-linear no K-way interaction assumption, the target parameter may only be defined when all $2^K-1$ observable capture patterns are present, leading to curse-of-dimensionality problems (empty cells) and ill-defined plug-in MLEs.","The work is not about engineering reliability; its “reliability” refers to statistical reliability/robustness of population-size estimation under identification assumptions, so translation to reliability engineering contexts is limited. The TMLE approach is developed in detail for a particular nonlinear constraint (log-linear no K-way interaction) and may require careful tuning/implementation choices (e.g., undersmoothing thresholding, lasso specification) that can affect finite-sample performance. Real-data validation is limited to a single public-health case study; broader empirical benchmarking across multiple datasets and capture–recapture designs (heterogeneous capture probabilities, dependence structures) would strengthen practical generalizability.","They suggest that a key practical remedy is to make certain identification assumptions hold “by design,” e.g., designing surveillance components so that two samples are independent conditional on a third. They also motivate use of targeted learning/machine learning to handle high-dimensional sparse capture-pattern tables when naive MLEs are ill-defined, implying further development and application of such targeted ML estimators in complex settings.","Extend the targeted-learning approach beyond the specific log-linear no K-way interaction constraint to other plausible dependence structures (e.g., pairwise interaction constraints, latent-class dependence) and to settings with covariates/heterogeneous capture probabilities. Provide user-ready, open-source implementations (e.g., an R package) and reproducible workflows for the proposed undersmoothed lasso selection and TMLE updating, including sensitivity analyses over tuning parameters. Develop formal sensitivity-analysis tools to quantify uncertainty due to identification-assumption choice (partial identification/bounds or model averaging across constraints), since the paper demonstrates assumption-driven estimand variability.",2105.05373v1,https://arxiv.org/pdf/2105.05373v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:12:59Z
TRUE,Life distribution modeling|System reliability|Accelerated testing|Other,Stochastic process|ML-based|Bayesian|Simulation-based|Other,Right-censored|Degradation measurements|Sensor/condition monitoring|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Other,TRUE,R|Python|Other,Public repository (GitHub/GitLab),https://bitbucket.org/gramacylab/nasa/src/master/entropy|https://github.com/nasa/MFISPy,"The paper proposes an entropy-based adaptive design criterion for Gaussian-process (GP) surrogates—Entropy-based Contour Locator (ECL)—to more accurately identify failure boundaries (limit-state contours) in expensive computer simulations used for reliability analysis. The method targets the failure contour by maximizing the predictive Bernoulli entropy of the event that the GP-predicted response exceeds a threshold, enabling closed-form evaluation of the acquisition function without domain-wide numerical integration. ECL is combined with multifidelity importance sampling (MFIS): the adaptively trained GP is used to build an importance-sampling bias distribution (via Gaussian mixture models) so that high-fidelity evaluations are concentrated near likely failures while preserving an asymptotically unbiased failure probability estimator. The authors also extend ECL to batch acquisition for parallel simulation settings by sequentially selecting batch points while updating only the GP predictive variance. Across benchmark functions (Branin-Hoo, Ishigami, Hartmann-6) and a NASA spacesuit impact-damage simulator, ECL improves failure-region identification (sensitivity) and yields more accurate, lower-variance failure probability estimates than space-filling designs and several contour-finding competitors, while being substantially faster than integration-based methods.","Failure region and contour are defined by a limit-state function: $G=\{x: g(T(x))>0\}$ and $C=\{x: g(T(x))=0\}$, with failure probability $\alpha=\mathbb{E}[\mathbf{1}\{g(T(x))>0\}]$. MFIS uses an importance-sampling estimator $\hat\alpha=\frac{1}{M^*}\sum_{i=1}^{M^*}\mathbf{1}\{g(T(x_i^*))>0\}\,w(x_i^*)$ where $w(x)=f(x)/f^*(x)$. The ECL acquisition at input $x$ is the Bernoulli entropy of the GP-implied failure event: $\mathrm{ECL}(x)= -p(x)\log p(x) - (1-p(x))\log(1-p(x))$ with $p(x)=\mathbb{P}(g(Y(x))>0)=1-\Phi\big((\mu_N(x)-T)/\sigma_N(x)\big)$ under the GP predictive distribution.","On benchmark contour-finding tasks (Branin-Hoo, Ishigami, Hartmann-6), ECL achieves among the highest failure-region sensitivity and among the lowest relative errors in estimated failure-region volume, and it remains competitive when run in batch mode (e.g., batch sizes 5–10). Reported design-construction times show ECL is orders of magnitude faster than integration-heavy entropy/SUR methods: for Ishigami, average time is about 0.10 minutes (ECL) versus 17.8 minutes (CLoVER) using $10^3$ knots; for Hartmann-6, about 4.40 minutes (ECL) versus 428.4 minutes (CLoVER). In MFIS experiments with a fixed budget of 1000 high-fidelity evaluations, ECL-trained GPs produce MFIS failure probability estimates centered near the true probabilities for Ishigami ($\alpha\approx1.9\times10^{-4}$) and Hartmann-6 ($\alpha\approx9.96\times10^{-6}$), while LHS-trained GPs often yield zero/poor estimates unless a conservative upper confidence bound is used for classification. In the NASA spacesuit impact simulation case study, ECL+MFIS yields overlapping 95% CIs with MC/MFIS baselines but provides the narrowest CI, attributed to a higher observed proportion of failures among the MFIS samples drawn from the ECL-derived bias distribution.",None stated.,"The approach assumes the GP surrogate is an adequate probabilistic model of simulator response near the failure boundary; misspecification (nonstationarity, sharp discontinuities, high anisotropy) could degrade acquisition choices and MFIS biasing. The batch extension updates only predictive variance (not the mean), which can be suboptimal when mean shifts materially after new evaluations, and may select redundant points in highly multimodal failure landscapes. MFIS performance depends strongly on how failures are classified from the surrogate (e.g., using a GP mean vs. UCB) and on the Gaussian-mixture bias model; the paper does not provide strong guidance or robustness analysis for these tuning choices under severe rarity or high dimension.",The authors suggest deeper integration of adaptive design with MFIS so that ECL acquisitions and MFIS bias distributions $F^*$ are refined jointly rather than treating MFIS as a downstream step. They also note that batch performance depends on batch size and propose that choosing batch size as a function of available compute time could improve practicality and performance.,"Develop self-calibrating or robust variants of ECL that handle nonstationary/heteroskedastic simulators (e.g., treed/local GPs or heteroskedastic GP likelihoods) and quantify sensitivity to GP prior/kernel choices. Extend the method to correlated/temporal simulation outputs and to multi-output failure criteria (multivariate limit states), where failure depends on multiple responses or time histories. Provide principled strategies for selecting the conservatism parameter (e.g., UCB level $\delta$) and for fitting/validating the bias distribution (GMM components/covariance structure) to improve reliability of MFIS under extreme rarity.",2105.11357v2,https://arxiv.org/pdf/2105.11357v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:13:42Z
TRUE,System reliability|Other,ML-based|Simulation-based|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not applicable (No code used),https://rprepo.readthedocs.io/en/latest/results.html,"This paper surveys active learning methods for structural reliability, where an expensive limit-state function $g(\mathbf{X})$ is approximated by an adaptively refined surrogate to estimate failure probability efficiently. It proposes a generalized modular framework that builds active-learning reliability (ALR) strategies by combining four modules: surrogate model (e.g., Kriging, PCE, PC-Kriging), reliability estimator (e.g., Monte Carlo, subset simulation, importance sampling), learning function (e.g., $U$, EFF, bootstrap-based), and stopping criterion (e.g., bounds/stability on reliability index). Using this framework, the authors benchmark 39 strategies on 20 reliability problems (dimensions 2 to 100, failure probabilities down to about $10^{-7}$), repeating runs to assess robustness, totaling over 12,000 solved reliability problems. The benchmark shows no single universally best strategy (no-free-lunch), but combinations using PC-Kriging with subset simulation and the $U$ learning function tend to perform best overall; it also highlights the benefit of pairing surrogates with sophisticated reliability estimators in “overkill” (high-accuracy) settings. Practical recommendations are provided on method choices as a function of dimension and failure-probability magnitude.","The reliability problem is defined by the failure probability $P_f=P(g(\mathbf{X})\le 0)=\int_{\{\mathbf{x}:g(\mathbf{x})\le 0\}} f_{\mathbf{X}}(\mathbf{x})\,d\mathbf{x)$. Active learning builds a surrogate (notably Kriging/GP) providing predictive mean and variance $(\mu_{\hat g}(\mathbf{x}),\sigma_{\hat g}(\mathbf{x}))$ and uses learning functions such as the deviation number $U(\mathbf{x})=|\mu_{\hat g}(\mathbf{x})|/\sigma_{\hat g}(\mathbf{x})$ to select new evaluations near the limit-state. Stopping criteria include reliability-index bounds based on GP variance, e.g., $|\hat\beta^+-\hat\beta^-|/\hat\beta\le \bar\varepsilon_{BB}$, and stability of successive reliability-index estimates $|\hat\beta^{(i)}-\hat\beta^{(i-1)}|/\hat\beta^{(i)}\le \bar\varepsilon_{BS}$.","Across the benchmark (39 strategies × 20 problems with repeated runs), surrogate-aided strategies generally outperform “direct” reliability solvers in computational efficiency for a given accuracy, and can match or exceed direct-solver accuracy when reliability algorithms are run in high-precision (“overkill”) settings. The problem set spans $M=2$ to $100$ and reference reliability indices roughly $\beta_{ref}\approx 1.86$ (about $3.14\times10^{-2}$ failure probability) to $\beta_{ref}\approx 5.15$ (about $1.32\times10^{-7}$ failure probability). In overall rankings, PC-Kriging appears most frequently among top-performing surrogate choices, subset simulation has a slight advantage among reliability estimators, and the $U$ learning function outperforms EFF and bootstrap-FBR on average; the preferred stopping rule depends on whether efficiency (stability) or accuracy (bounds) is prioritized. The authors report that their recommended configuration (PC-Kriging + subset simulation + $U$ + combined/bounds-type stopping) achieved top efficiency in 24 of 27 problems in the TNO black-box challenge results they reference.","They do not explore the effect of threshold choices in stopping criteria, and they only test a limited set of learning functions (notably not more advanced ones that incorporate input PDFs or multi-point enrichment). The benchmark scope is limited to single limit-state problems and excludes time-variant/dynamic and extremely high-dimensional problems (hundreds to thousands of variables), which the authors note would require special treatment.","Because the benchmark uses a fixed selection of methods and hyperparameter settings (e.g., specific “overkill” solver settings and surrogate calibration choices), conclusions may be sensitive to these design decisions and may not transfer to other practical configurations or constraints (e.g., strict evaluation budgets or parallel evaluation policies). Real-world validation is limited: most benchmark limit-state functions are analytical, with only two finite-element truss examples, so performance on complex industrial FE models with noise, solver failures, or modeling bias remains uncertain. The work focuses on estimating $P_f$ (and $\beta$) but provides limited guidance on diagnosing surrogate misclassification patterns or on handling model-form uncertainty beyond surrogate epistemic uncertainty.","They suggest investigating the impact of stopping-criterion thresholds and exploring more advanced learning functions, including ones that incorporate the input PDF and/or support multiple-point enrichment. They also point to extensions needed for multiple limit-state functions and for extremely high-dimensional problems (hundreds or thousands of inputs), which were out of scope in this study.","A valuable extension would be developing and benchmarking robust/self-starting variants that handle unknown input distributions, strong dependence, and autocorrelated inputs typical of measured loads and environments. Another direction is integrating multi-fidelity modeling (coarse-to-fine simulators) within the same modular ALR framework to further reduce cost for FE-driven problems. Providing an open, reproducible benchmark harness (with scripts and standardized problem definitions) beyond a toolbox release would facilitate independent replication, fairer comparisons to newer ALR methods (e.g., deep active learning), and sensitivity studies over hyperparameters and candidate-pool strategies.",2106.01713v2,https://arxiv.org/pdf/2106.01713v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:14:26Z
FALSE,Other,Bayesian|Stochastic process|ML-based,Sensor/condition monitoring|Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Personal website,https://www.dropbox.com/sh/n2thesjq87sh66j/AACg-HKMl1NhQpaMOHvvUEOfa?dl=0,"This paper proposes F-PACOH, a PAC-Bayesian meta-learning framework that meta-learns priors as stochastic processes and regularizes directly in function space using an approximation of the functional KL divergence between stochastic processes. The method targets reliable (well-calibrated) epistemic uncertainty, especially in regions with little or no meta-training data, addressing overconfident uncertainty estimates observed in prior meta-learning approaches. Instantiated with Gaussian Processes, F-PACOH yields closed-form marginal likelihood and KL terms, enabling practical training and integration into sequential decision-making algorithms such as GP-UCB for Bayesian optimization. Empirically, the authors report improved calibration (coverage) and stronger Bayesian optimization performance (lower regret) than PACOH-MAP, vanilla GPs, and other meta-learning baselines, including in lifelong Bayesian optimization where performance improves as more tasks are observed. The work is primarily machine learning (meta-learning/uncertainty quantification) rather than reliability engineering, though it mentions potential applications to recurring re-optimization/calibration of machines and systems.","The PACOH objective is adapted to function-space regularization by replacing the hyper-posterior vs. hyper-prior KL with a functional KL between stochastic processes, approximated by expected KL between finite-dimensional marginals on sampled measurement sets $X_i$. The resulting objective (MAP form) is $J_F(Q)=\frac{1}{n}\sum_i\Big[-\frac{1}{T_i}\,\mathbb{E}_{\phi\sim Q}\ln Z(D_i,P_\phi) + (\frac{1}{\sqrt{n}}+\frac{1}{nT_i})\,\mathbb{E}_{X_i}\mathrm{KL}(q(h_{X_i})\,\|\,\rho(h_{X_i}))\Big]$, where $\ln Z$ is the (generalized) marginal log-likelihood. For the GP instantiation, the marginal log-likelihood is the standard closed form and the KL term is the multivariate-normal KL between GP marginals on $X_i$.","On 1D simulated task distributions, calibration plots show F-PACOH-MAP achieves near-perfect empirical coverage across confidence levels, whereas PACOH-MAP is overconfident and a vanilla GP is underconfident. In Bayesian optimization benchmarks (simulated functions and hyperparameter tuning tasks), F-PACOH-MAP achieves consistently lower simple regret and inference regret over steps than PACOH-MAP and other baselines, and continues improving rather than saturating early. In lifelong BO with 10 sequential runs, F-PACOH-MAP is reported as the only method that significantly improves optimization performance as experience accumulates. The paper also reports an implementation choice of measurement set size $L=20$ (10 points from task inputs + 10 uniform domain samples) for estimating the functional KL regularizer.","The authors note that estimating the expectation over measurement sets $X_i$ via uniform sampling and Monte Carlo is subject to the curse of dimensionality, and therefore they do not expect the approach to work well for high-dimensional inputs (they mention $d>50$ such as images). They suggest that alternative approximation/sampling schemes that leverage the data manifold could mitigate this.","The primary empirical validation is centered on Bayesian optimization and meta-regression settings; the reliability of uncertainty under distribution shifts beyond the assumed task distribution or under strong nonstationarity is not thoroughly characterized. The functional-KL approximation depends on the choice and size of measurement sets and uniform sampling over the domain, which may be impractical or ill-defined for constrained, structured, or very high-dimensional domains. Comparisons focus on a set of meta-learning baselines; robustness to misspecified GP likelihoods (e.g., heavy-tailed noise), correlated observations, or heteroscedasticity is not systematically explored.","The authors propose investigating alternative approximation/sampling schemes for the functional KL term that take the data manifold into account, motivated by the curse of dimensionality in high-dimensional spaces. They also mention variational hyper-posteriors (beyond MAP) as an extension discussed in the appendix, implying broader exploration of non-Dirac hyper-posteriors.","A useful extension would be to develop domain-informed or adaptive measurement-set selection strategies (e.g., active sampling of $X_i$ based on predictive variance or domain constraints) to improve efficiency and scalability. Another direction is to study robustness/calibration under model misspecification (non-Gaussian noise, heteroscedasticity, temporal dependence) and to provide guidance on choosing $L$, $\kappa$, and sampling distributions for $X_i$ in practical deployments. Providing a packaged, documented implementation (e.g., PyPI/conda) and standardized benchmark scripts for broader reproducibility beyond a shared repository link could also increase adoption.",2106.03195v2,https://arxiv.org/pdf/2106.03195v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:15:02Z
FALSE,NA,Bayesian|Other,Other,Not applicable,Healthcare/medical,Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/danieladamspencer/BayesGLM_Validation,"This paper proposes and validates a cortical surface-based spatial Bayesian general linear model (GLM) for task fMRI that explicitly models spatial dependence between neighboring cortical vertices using SPDE/GMRF (Matérn) priors and performs inference via joint posterior excursion sets rather than mass-univariate testing with multiplicity correction. The authors extend prior work by (i) using subject-specific midthickness cortical surfaces (geodesic distances) instead of spherical surfaces to reduce distance distortion, and (ii) introducing a multi-run model that shares spatial hyperparameters across runs to improve estimation efficiency. Using Human Connectome Project motor-task test–retest data (45 subjects; 4 runs per subject across two visits), they evaluate reliability of activation amplitude estimates and activation regions at both individual and group levels. Results indicate the spatial Bayesian GLM yields smoother, more reliable individual activation estimates (higher ICC) and more consistent activation regions (higher Dice overlap), while also improving group-level reliability (higher test–retest correlation / lower MSE) and power, including in small samples. The method is implemented in the BayesfMRI R package and relies on INLA for computationally efficient Bayesian inference.","The classical vertex-wise GLM is $y_v = X_v\beta_v + \varepsilon_v$ with white-noise residuals. Spatial Bayesian modeling places SPDE/Matérn GMRF priors on task-specific activation fields via $\beta = \Psi w$ with $w \sim \mathcal{N}(0, Q^{-1}_{\kappa,\tau})$ and precision $Q_{\kappa,\tau} = \tau^2(\kappa^4 C + 2\kappa^2 G + G C^{-1} G)$; multi-run analysis uses run-specific $\beta_{j,k}$ sharing common hyperparameters $(\kappa_k,\tau_k,\sigma^2)$. Activated regions are defined using the excursion set: the largest set of vertices whose joint posterior probability of exceeding an effect-size threshold $\gamma$ is at least $1-\alpha$.","Across HCP motor-task data (45 subjects; two visits), the spatial Bayesian GLM produced higher subject-level reliability of activation amplitude estimates than the classical GLM without smoothing across all tasks, and generally matched or exceeded the classical GLM with 6mm smoothing (except tongue). For activation regions, Bayesian excursion sets were substantially larger (higher power) while maintaining strict false-positive control ($\alpha=0.01$), and test–retest Dice overlap was highest for the Bayesian method at $\gamma=0.5\%$ (often near or above 0.6 across tasks). At the group level, Bayesian group-average estimates achieved test–retest correlations around ~0.95 for $n=45$ and showed small-sample performance comparable to or better than the classical GLM at much larger sample sizes. Computing times reported (both hemispheres, per visit) were ~12.5 min for single-subject Bayesian analyses versus ~0.14 min for classical, and ~336 min for Bayesian group analysis at $n=45$ (plus requisite subject-level fits).","The authors note results are based on high-quality young-adult HCP data and may differ in other populations or acquisition settings. Reliability is assessed primarily via test–retest rather than true ground truth; additionally, MSE/correlation use classical GLM unsmoothed estimates as a noisy but unbiased proxy, so reliability metrics remain noisy. The study focuses on cortical surface only, excluding subcortical and cerebellar regions where SNR is lower and Bayesian spatial modeling may be particularly beneficial.","The method assumes the prewhitening and nuisance-regression steps adequately remove temporal autocorrelation and other confounds; residual model misspecification or remaining spatially structured noise could bias posterior activation probabilities. Performance is evaluated mainly on a single task domain (motor) from one dataset; broader validation across diverse task paradigms, scanners, and preprocessing choices (including different meshes/resampling levels) would strengthen generalizability. The excursion-set inference depends on user-chosen $(\alpha,\gamma)$ and on accurate posterior covariance; sensitivity of conclusions to these choices and to hyperprior settings is not extensively stress-tested in the main text.",The authors plan an empirical Bayes computation approach using EM to improve computational efficiency and flexibility. They propose extending the spatial Bayesian GLM to subcortical and cerebellar regions. They also plan to investigate using full-resolution surface data with coarser meshes for potential inferential benefits and to directly integrate HRF derivatives into the Bayesian model to better account for HRF variability.,"Developing robust/self-starting variants that better handle model misspecification (e.g., nonstationary spatial dependence, heavier-tailed noise) and residual autocorrelation could improve reliability in noisier clinical datasets. Extending the framework to multivariate/task-contrast models that jointly model multiple tasks/conditions (and potentially multiple modalities) may improve power and interpretability. Providing standardized, open benchmarking across multiple public task-fMRI datasets with preregistered comparison pipelines and releasing reproducible containers/workflows would help adoption and fair comparison to alternative spatial or FDR/FWER methods.",2106.06669v3,https://arxiv.org/pdf/2106.06669v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:16:44Z
FALSE,NA,Other,Other,Not applicable,Other,Other,NA,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/google-research-datasets/replication-dataset,"This paper is about interpreting inter-rater reliability (IRR) for human annotation and proposes an empirical benchmarking framework using replications of labeling experiments. It introduces cross-replication reliability (xRR) via a new cross-kappa statistic (κx) to measure chance-corrected agreement between raters drawn from different replications, and a normalized κx to assess replication similarity relative to within-replication IRR. The authors apply the framework to the IRep dataset (about 3.94 million judgments on 38,499 items) collected in three cities, showing that similar IRR values across sites can mask very different cross-site agreement. They also relate normalized κx to Spearman’s correction for attenuation and report a strong empirical correspondence (Pearson correlation ≈ 0.99) on their dataset. The work targets data quality assessment for crowdsourced annotation rather than engineering system reliability.","The proposed cross-kappa is defined as $\kappa_x(X,Y)=1-\frac{d_o(X,Y)}{d_e(X,Y)}$, where $d_o$ averages pairwise disagreement $D(x_{ri},y_{si})$ across all cross-replication annotation pairs on the same item, and $d_e$ averages disagreement across all cross-replication pairs over all item pairs. A replication-similarity normalization is defined as $\text{normalized }\kappa_x=\frac{\kappa_x(X,Y)}{\sqrt{\text{IRR}_X}\,\sqrt{\text{IRR}_Y}}$, motivated by Spearman’s correction for attenuation.","On the IRep facial-expression dataset, the paper reports examples where two sites have similar within-site IRR but very different cross-site agreement: e.g., for contentment, IRR(Mexico City)=0.4494 and IRR(Kuala Lumpur)=0.6363 while $\kappa_x$ is about −0.0344 (no discernible cross-replication agreement). For sadness, IRR values around 0.515 in Mexico City and Budapest correspond to $\kappa_x\approx 0.4709$, indicating cross-site agreement similar to within-site agreement. For awe, a low $\kappa_x\approx 0.0817$ becomes much higher after normalization (normalized $\kappa_x\approx 0.6872$) because both sites have low IRR (~0.12). Across 31 emotion labels and three replication pairs, normalized $\kappa_x$ closely matches disattenuated correlation $\rho_{xy}$ with Pearson correlation ≈ 0.99.","The authors note that confidence intervals for $\kappa_x$ and normalized $\kappa_x$ are needed for hypothesis testing and that large-sample behavior should be studied (beyond empirical block-bootstrap). They also state that further analysis is required to confirm the sensitivity of $\kappa_x$ under high class imbalance, and that computation can be prohibitively expensive because it involves many pairs ($n^2RS$) unless optimized. They additionally acknowledge that their proposed normalization may not suit all applications and that alternative xRR coefficients (e.g., based on Krippendorff’s alpha) may be useful under different assumptions.","The paper’s constructs address agreement in human annotation and do not directly translate to reliability engineering notions like component lifetime, hazard, maintainability, or system availability, limiting applicability to engineering reliability contexts. Empirical demonstrations are largely centered on one replicated facial-expression dataset; broader validation on different task types, label structures, and annotation regimes would strengthen generality. Negative estimated $\kappa_x$ values are attributed to estimation error, but the paper does not fully explore finite-sample bias/variance behavior across realistic sample-size regimes and missingness patterns. Practical adoption would benefit from turnkey implementations and guidance on required replication size/budget, which are not concretely specified.","The authors propose studying confidence intervals and large-sample properties for $\kappa_x$ and normalized $\kappa_x$, and analyzing sensitivity under high class imbalance. They suggest optimizing computation (e.g., algebraic simplification/dynamic programming) to reduce the cost of constructing $n^2RS$ pairs. They also propose exploring alternative normalizations (e.g., $\kappa_x/\text{IRR}_{\text{expert}}$ when comparing crowd to expert labels) and alternative xRR coefficients based on other agreement statistics (e.g., Krippendorff’s alpha).","A useful extension would be a self-contained software package (e.g., Python/R) with numerically stable estimators, scalable computation, and standardized reporting templates for IRR/xRR in replicated annotation studies. Another direction is a design-of-experiments treatment: how many items/raters/replications are needed to achieve target precision for $\kappa_x$ under different prevalence and ambiguity conditions. Robustness studies under rater dependence (clustered workers), adversarial behavior, and domain shift could clarify when xRR provides trustworthy external-validity signals. Finally, extending the framework to structured outputs (multi-label hierarchies, spans, or continuous ratings with complex distance functions) with clear best-practice choices for $D(\cdot)$ would broaden applicability.",2106.07393v1,https://arxiv.org/pdf/2106.07393v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:17:24Z
TRUE,Degradation modeling|Accelerated testing|System reliability|Life distribution modeling,"Parametric (Weibull, etc.)|Bayesian|Other",Degradation measurements|Simulated only,Not applicable,Theoretical/simulation only,Approximation methods|Simulation study|Other,TRUE,R,Not provided,NA,"The paper develops optimal experimental designs for repeated-measures accelerated degradation tests (ADT) with multiple response components representing competing failure modes. Each degradation component is modeled via a marginal linear mixed-effects model (LMEM), leading to a non-degenerate soft-failure time distribution defined by threshold crossing of the unit-specific mean degradation path under normal-use conditions. The design objective is a locally c-optimal design that minimizes the asymptotic variance of an estimator of a chosen quantile (e.g., median) of the joint system failure-time distribution (e.g., series/1-out-of-r or s-out-of-r systems). Using Fisher information for the fixed effects and the delta method/implicit function theorem for quantile gradients, the criterion reduces to a blockwise form across components; under product-type regression structure and identical component models, optimality inherits from a marginal (stress-only) design. Numerical examples compare the proposed optimal designs against standard vertex-equal-weight designs and include sensitivity/robustness studies showing improved efficiency and stability under parameter and normal-use-condition misspecification.","Observations follow a multivariate LMEM per component: $Y_{ijl}=f_l(x_i,t_j)^T\beta_l+g_l(t_j)^T\gamma_{il}+\varepsilon_{ijl}$ with $\gamma_{il}\sim N(0,\Sigma_{\gamma l})$ and $\varepsilon_{ijl}\sim N(0,\sigma_{\varepsilon,l}^2)$. The per-unit fixed-effect information for an approximate design $\xi=\{(x_i,w_i)\}$ is $M_\beta(\xi)=\sum_i w_i F(x_i,t)^T V^{-1}F(x_i,t)$, block-diagonal by components. Soft-failure CDF per component under normal use is $F_{T_l}(t)=P(T_l\le t)=\Phi(h_l(t))$ with $h_l(t)=(\mu_l(t)-y_{l0})/\sigma_{ul}(t)$ and $\sigma_{ul}^2(t)=g_l(t)^T\Sigma_{\gamma l}g_l(t)$. Quantile-variance criterion uses $\operatorname{aVar}(\hat t_\alpha)=c^T M_\theta^{-1}c$, yielding the locally c-optimal design $\xi^*=\arg\min_\xi\sum_{l=1}^r c_l^T M_{\beta l}(\xi)^{-1}c_l$ (up to a design-irrelevant constant from variance parameters).","Example 1 (series system, two stresses with full interaction, time plan $t=(0,0.5,1)$, design region $[0,1]^2$, normal use $(x_{u1},x_{u2})=(-0.40,-0.20)$) yields a closed-form vertex design with weights approximately $(0.67,0.11,0.19,0.03)$ on $(0,0),(0,1),(1,0),(1,1)$, respectively, and reported robustness to misspecification of $\beta$; efficiency plots show it dominates the equal-weights vertex design across the explored $x_{u1}$ range. Example 2 (2-out-of-3 system with partial interaction) uses a multiplicative algorithm on a 0.05-grid over $[0,1]^2$ to obtain an optimal vertex design with weights $(0.60,0.03,0.13,0.24)$ on the same four vertices. Sensitivity studies show the optimal weights change only slightly with $\beta_{11}$ but are more sensitive to the normal-use condition $x_{u1}$; efficiency curves indicate the proposed design remains preferable to the equal-weights design under these variations.","The optimal designs are locally optimal and depend on nominal parameter values (and, in some settings, on the target quantile through $t_\alpha$), so misspecification can affect optimality. The time plan (measurement times) is assumed fixed in advance and not optimized, and stress is assumed constant within each unit (cross-sectional stress allocation only). Components are assumed independent within units, yielding a block-diagonal covariance structure.","The approach relies heavily on normality and LMEM structure for degradation paths and assumes monotone (strictly increasing) mean degradation to define threshold-crossing soft failures; violations (non-monotone degradation, heavy tails, heteroscedastic errors) could materially affect quantile gradients and design performance. The extrapolation to normal-use conditions may be fragile when normal use lies far outside the stress region or when the stress-life relationship is misspecified; robustness is evaluated mainly via limited sensitivity plots rather than broad scenario simulations or real datasets. The method does not address dependence among competing degradation components (cross-component correlation), which is common in multivariate degradation and could alter the joint failure-time distribution and optimal allocations.",The authors state that constructing designs robust to misspecification of nominal values—such as maximin efficient designs or weighted (Bayesian) optimal designs—is an object of further research.,Extend the framework to allow correlated multivariate degradation components (non-block-diagonal $V$) and assess how cross-component dependence changes optimal stress allocations for s-out-of-r systems. Develop joint optimization over both stress levels and measurement time schedules (including adaptive or sequential time plans) to improve quantile precision under constrained test duration/cost. Provide software as an R package and validate designs on real ADT datasets with model-checking diagnostics and robustness to non-normal errors and autocorrelated within-unit measurement noise.,2106.09379v2,https://arxiv.org/pdf/2106.09379v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:18:07Z
TRUE,Accelerated testing|Degradation modeling|Life distribution modeling,"Stochastic process|Parametric (Weibull, etc.)|Bayesian|Simulation-based|Other",Degradation measurements|Mixture of types|Other,Not applicable,Theoretical/simulation only,Approximation methods|Simulation study|Other,TRUE,R,Not provided,NA,"The paper develops locally optimal experimental designs for accelerated degradation testing (ADT) when two degradation responses (failure modes) are observed, modeled marginally by Gamma processes with stress-dependent rate $\gamma_l(x_l)=\exp(\beta_{1l}+\beta_{2l}x_l)$. For independent components, it derives locally c-optimal stress designs that minimize the asymptotic variance of an estimated quantile of the system failure-time distribution at normal use conditions, with the system modeled as a parallel system failing at $T=\max\{T_1,T_2\}$. For dependent components, it introduces copula-based dependence (Frank and Gaussian copulas) between bivariate increment distributions at fixed inspection times and computes locally D-optimal designs using numerical integration within the Fisher information. To reduce computational burden, it further studies a simplified copula-based model where degradation observations are reduced to bivariate binary outcomes (threshold exceedance), and derives locally D- and c-optimal designs for the single-measurement case. Numerical examples (computed in R via a multiplicative algorithm on a discretized design space) illustrate the resulting optimal support points/weights and provide sensitivity/robustness comparisons against standard designs.","Marginal degradation increments follow a Gamma process with stress-dependent shape rate $\gamma_l(x_l)=\exp(\beta_{1l}+\beta_{2l}x_l)$ and mean $\mu_{jl}(x_l)=\gamma_l(x_l)\,\Delta_j\,\nu_l$. Under independent components, the system failure time CDF at normal use is $F_T(t)=Q(\gamma_1(x_{u1})t,z_{10}/\nu_1)\,Q(\gamma_2(x_{u2})t,z_{20}/\nu_2)$ and c-optimal designs minimize $c(\beta)^T M(\xi,\beta)^{-1} c(\beta)$ with $c(\beta)=\partial t_\alpha/\partial\beta$ from the implicit equation $F_T(t_\alpha)=\alpha$. Under dependence, bivariate increment CDFs are coupled by a copula $F_j(y_1,y_2)=C(F_{j1}(y_1),F_{j2}(y_2))$ with density $f_j(y)=c(F_{j1}(y_1),F_{j2}(y_2))f_{j1}(y_1)f_{j2}(y_2)$, and D-optimal designs maximize $\det M(\xi,\beta)$.","For an independent-components numerical example (k=4 time points; Table 1 parameters; normal use $x_{u1}=-0.60$, $x_{u2}=-0.50$), the computed marginal c-optimal designs are two-point endpoint designs: $\xi_1^*:\{0,1\}$ with weights $(0.79,0.21)$ and $\xi_2^*:\{0,1\}$ with weights $(0.91,0.09)$, yielding (one) product design on $(0,0),(0,1),(1,0),(1,1)$ with weights $(0.72,0.07,0.19,0.02)$. For dependent components with Frank copula (k=4 equidistant times; Table 2 parameters), the locally D-optimal design is a uniformly weighted 6-point design at $(0,0),(0,1),(0.5,0),(0.5,1),(1,0),(1,1)$ each with weight 0.166. For dependent components with Gaussian copula ($\rho=-0.1$), the D-optimal design uses the same six points but nonuniform weights (0.20, 0.20, 0.16, 0.16, 0.18, 0.09). For the binary-outcome reduction with k=1 (single measurement), D-optimal designs are near-uniform four-vertex designs for both copulas, and c-optimal designs concentrate more weight on intermediate/high-stress points such as $(0.5,1)$ (e.g., Gaussian copula c-optimal weights 0.11, 0.22, 0.39, 0.28 on $(0,0),(0,1),(0.5,1),(1,1)$).","The authors note that for copula-based dependent components it is difficult to “accurately define the continuous failure time variable $T$ and hence the quantile $t_\alpha$,” motivating use of D-optimality instead of the quantile-based c-criterion. They also state that deriving and computing the information matrix for the copula-based repeated-measures model (with multiple observations) is difficult and computationally intensive (requires numerical 2D integration), motivating the simplified binary-outcome reduction and the k=1 case to make calculations tractable.","The optimality results are local (depend on nominal parameter values), and the paper does not provide a full Bayesian/robust design framework to hedge against parameter misspecification beyond limited sensitivity plots. The dependent-increments copula construction is not a true bivariate Gamma process with stationary independent increments (the paper itself points out existence issues and then restricts to fixed time points), which may limit physical interpretability for continuous-time dependence. Practical implementation aspects (e.g., estimation performance under small samples, model checking for copula choice, or handling measurement error/inspection noise) are not developed, and comparisons are largely within the proposed modeling family rather than against broader ADT design alternatives.","They explicitly propose extending the results beyond Gamma process marginal degradation models to other marginal degradation/failure models, such as the Wiener process, inverse Gaussian process, or non-linear mixed-effects degradation models.","Extending the dependent case to a coherent continuous-time multivariate Lévy/Gamma-process construction (or validating discrete-time copula increments against such processes) would strengthen modeling foundations. Developing robust or Bayesian optimal designs that integrate uncertainty in copula family/parameters (and in normal-use extrapolation) would improve practical reliability planning. Additional work could incorporate autocorrelated measurement error, varying inspection schedules (optimizing time points jointly with stress), and real case studies with dependent degradation signals to validate design recommendations and computational feasibility.",2106.13540v1,https://arxiv.org/pdf/2106.13540v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:18:49Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,Other,Not provided,NA,"The paper is about verification of probabilistic forecasts for football match outcomes (home win/draw/away win), focusing on decompositions of proper scoring rules (mainly the Brier score) into components such as reliability (calibration), resolution, refinement (sharpness), and discrimination. It reviews and applies the Murphy calibration–refinement decomposition, the likelihood-based (Murphy–Winkler) decomposition, and Yates’ decomposition, along with graphical tools like reliability diagrams and discrimination analyses. Empirical illustrations use UEFA Champions League group-stage matches (2017–2020), comparing Poisson-regression-based probabilities versus bookmaker-odds-implied probabilities; results show good calibration for home/away wins but poor resolution/discrimination for draws. The Poisson model is fit using Bayesian methods and forecasts are generated from posterior predictive distributions; isotonic regression (PAV) and logistic calibration models are used as additional diagnostics.","The main scoring rule is the (half) Brier score for a binary event, $S(P,X)=(P-X)^2$. The Murphy (1973) decomposition is $\mathbb{E}[S(P,X)]=\text{REL}-\text{RES}+\text{UNC}$, where $\text{UNC}=\mathrm{Var}(X)$, $\text{RES}=\mathbb{E}\{(\mathbb{E}[X\mid P]-\mathbb{E}[X])^2\}$, and $\text{REL}=\mathbb{E}\{(\mathbb{E}[X\mid P]-P)^2\}$. The likelihood-based form is $\mathbb{E}[S(P,X)]=\text{REF}-\text{DIS}+\text{CB2}$ with $\text{REF}=\mathrm{Var}(P)$ and $\text{DIS}=\mathrm{Var}(\mathbb{E}[P\mid X])$; Yates’ decomposition expresses BRS as $\text{UNC}-2\,\mathrm{Cov}(P,X)+\text{VPB}+\text{VPW}+\text{RIL}$.","For home wins, Poisson forecasts achieve Brier score 0.1849 with skill 24.8% (vs odds 0.1732 and skill 29.5%); reliability is small (e.g., isotonic REL 0.0116, 4.7% of UNC) and resolution is substantial (ISO RES 0.0725, 29.5% of UNC). For draws, skill is very low: Poisson 1.4% (Brier 0.1849) and odds 3.0% (Brier 0.1820), with tiny discrimination (Yates VPB about 0.0001–0.0002). For away wins, Poisson Brier score is 0.1700 with skill 21.2% (odds 0.1569 with skill 27.3%), again showing much better discrimination than draws (e.g., c-statistic ~0.789–0.820 vs ~0.622–0.624 for draws). Logistic calibration indicates mild over-forecasting for home wins under Poisson (intercept -0.259, p=0.030) while away wins are well calibrated (intercept 0.076, slope 1.053; p-values 0.610 and 0.693).","The paper notes that extending the calibration–refinement (CR) decomposition from binary to multi-category outcomes is “not straightforward,” largely due to how bins should be defined for multiple categories. It also points out practical issues from having many distinct forecast probability values, requiring binning or isotonic regression to estimate conditional event probabilities reliably.","This is forecast-verification methodology applied to sports, not reliability engineering; conclusions do not generalize to engineering failure-time or degradation contexts. The empirical evaluation is limited to one competition (UEFA Champions League group stage) and four seasons, and performance conclusions may depend on this specific sampling and on the particular Poisson/ELO modeling choices. Results are largely descriptive (score decompositions, diagrams, tests) and do not provide decision/utility impacts (e.g., cost-sensitive optimization) or robustness checks to temporal dependence, selection bias, or bookmaker margin adjustments beyond implied probabilities.","The paper suggests further work on verification methods for multiple categories considered simultaneously, noting existing approaches (e.g., Bröcker’s binning functions of the probability vector) and proposing one-versus-all binarization (e.g., logistic-type regression with normalization). It also discusses implications for improving forecast efficiency, especially regarding the persistent difficulty of forecasting draws due to poor resolution and discrimination.","Developing principled multiclass calibration–resolution decompositions that avoid ad hoc binning (e.g., based on Dirichlet calibration or vector-valued isotonic methods) and validating them on larger, more diverse datasets would strengthen generality. A systematic comparison of model classes (e.g., bivariate Poisson, copula-based dependence, time-varying team-strength models) under identical backtesting protocols could clarify where gains in resolution/discrimination come from. Providing reproducible code and data-processing details (especially for odds-to-probability conversion and any normalization) would improve transparency and facilitate adoption.",2106.14345v2,https://arxiv.org/pdf/2106.14345v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:19:32Z
FALSE,Other,ML-based|Nonparametric/Semi-parametric|Other,Simulated only|Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/lee-group-cmu/lf2i,"The paper introduces Likelihood-Free Frequentist Inference (LF2I), a modular framework to construct frequentist confidence sets with near finite-sample validity in simulator-based (intractable likelihood) settings. LF2I amortizes inference by learning (i) a test statistic from simulations (via odds estimation using probabilistic classification), (ii) critical values across the parameter space using quantile regression (or alternatively p-values via classification), and (iii) diagnostics that estimate local empirical coverage across the parameter space. Two likelihood-based test statistics are studied: ACORE (maximization-based, approximating likelihood-ratio behavior) and BFF (Bayes factor used as a frequentist statistic via averaged odds). Experiments (Gaussian mixture, Poisson counting with nuisance parameters, and high-dimensional muon calorimeter data) show LF2I can achieve nominal coverage where chi-square/Wilks approximations fail and can provide tighter intervals than ABC in the showcased settings. The diagnostics module is highlighted as a practical tool to identify under/over-coverage regions, especially with nuisance parameters where hybrid approaches may not guarantee coverage.","LF2I learns an odds function via classification: $O(x;\theta)=\frac{P(Y=1\mid \theta,x)}{P(Y=0\mid \theta,x)}$ (Eq. 7), which is proportional to the likelihood under mild conditions. ACORE uses a maximized odds-product ratio to mimic a likelihood-ratio statistic: $\Lambda(D;\Theta_0)=\log\frac{\sup_{\theta\in\Theta_0}\prod_{i=1}^n O(X_i;\theta)}{\sup_{\theta\in\Theta}\prod_{i=1}^n O(X_i;\theta)}$ (Eq. 8). BFF averages odds to form $\tau(D;\Theta_0)=\frac{\int_{\Theta_0}\prod_{i=1}^n O(X_i;\theta)\,d\pi_0(\theta)}{\int_{\Theta_0^c}\prod_{i=1}^n O(X_i;\theta)\,d\pi_1(\theta)}$ (Eq. 10), Fisher-consistent for the Bayes factor. Confidence sets are constructed by Neyman inversion using calibrated critical values: $\hat R(D)=\{\theta\in\Theta: \lambda(D;\theta)\ge \hat C_\theta\}$ (Eq. 12).","The paper provides consistency results showing that critical values learned by quantile regression yield asymptotically correct type-I error control as the calibration simulation size $B'\to\infty$ (Theorems 1–2), and that p-value estimation via regression/classification is consistent with corresponding size control (Theorem 3, Corollary 1). For BFF, it bounds the probability that decisions based on estimated BFF differ from those based on the exact Bayes factor in terms of an integrated squared error loss on the learned odds (Theorems 5–6), giving a convergence-rate bound under classifier learning-rate assumptions. Empirically, in the Gaussian mixture example, chi-square LRT (Wilks) under-covers across much of $\Theta$, while LF2I calibration via quantile regression achieves nominal conditional coverage with far fewer total simulations than grid-based Monte Carlo. In the muon calorimeter application (high-dimensional $x$ and $n=1$), LF2I with BFF achieves nominal coverage (reported at 68.3%) and yields shorter intervals than SMC-ABC for the lower-dimensional summaries, while SMC-ABC over-covers and is computationally heavier.",The authors note that handling nuisance parameters with hybrid schemes (profiling or marginalization) does not always control type I error and lacks a general small-sample theorem guaranteeing good frequentist coverage. They also state that statistical power is difficult in likelihood-free inference and depends on both estimation error of the odds/likelihood surrogate and numerical error from optimization/integration in ACORE/BFF. The framework implicitly assumes the null distribution/quantile surface varies smoothly over parameter space to enable efficient calibration via regression.,"The work is not focused on reliability engineering; its “reliability” pertains to statistical validity (coverage) rather than component/system failure behavior, so direct applicability to reliability problems (lifetime/degradation/maintenance) is not established. Practical performance may be sensitive to the choice of proposal distribution $\pi_\Theta$, reference distribution $G$, classifier architecture, and calibration model; these dependencies can affect coverage in finite simulation budgets even if asymptotic guarantees hold. The amortized confidence set construction relies on a grid/lattice evaluation over $\Theta$ for inversion in practice, which can become computationally challenging in high-dimensional parameter spaces without further approximation strategies.","They propose that future work should improve LF2I power by (i) improving likelihood/odds (or ratio) estimation using richer simulator information (e.g., “mining gold” approaches) and (ii) reducing numerical errors in maximization/integration used by ACORE/BFF. They also suggest extending LF2I to alternative test statistics beyond likelihood/Bayes-factor-style ones, including statistics constructed from posterior estimators or prediction algorithms (e.g., Wald-type statistics using $E[\theta\mid x]$ and $\mathrm{Var}[\theta\mid x]$).","For broader robustness, LF2I could be extended to explicitly handle model misspecification and non-i.i.d./dependent data (common in monitoring applications), including methods for autocorrelated simulator outputs or time-series observations. Developing scalable inversion and set-representation methods (e.g., adaptive meshes, level-set learning, or sequential design over $\Theta$) would improve practicality for high-dimensional parameter spaces. Additional work could provide finite-simulation, finite-model-capacity coverage guarantees or diagnostic thresholds that quantify uncertainty in the coverage diagnostics themselves, improving decision-making when diagnostics are noisy.",2107.03920v10,https://arxiv.org/pdf/2107.03920v10.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:20:17Z
FALSE,Other,"ML-based|Parametric (Weibull, etc.)",Complete lifetime data|Other,Not applicable,Healthcare/medical,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,https://archive.ics.uci.edu,"The paper proposes a clinical decision support system (CDSS) built around a decision set of short, clinically interpretable rules, augmented with machine learning models that predict—per patient—the probability that each rule’s output is correct. These per-rule correctness probabilities are then used as patient-specific weights in a personalized voting scheme to produce the final binary prediction, aiming to improve performance while retaining both global (ruleset-level) and local (patient-level) interpretability. In addition, it introduces a per-prediction reliability estimate computed from the separation between the average predicted correctness of rules voting for the positive class versus the negative class. The approach is evaluated on three public clinical datasets (Heart Disease, Breast Cancer Wisconsin Diagnostic, Mammographic Mass) using repeated cross-validation, showing improved ROC AUC for the personalized decision set compared with non-weighted and globally weighted baselines. The reliability score is shown to correlate with misclassification rate, with low predicted reliability associated with higher error rates.","The personalized decision-set prediction is computed via a patient-specific weighted average: $\mathrm{Prob}_n(\mathrm{class}=1)=\frac{\sum_{i=1}^K \mathrm{rule\_output}_{i,n}\,\mathrm{weight}_{i,n}}{\sum_{i=1}^K \mathrm{weight}_{i,n}}$, where weights are predicted probabilities that each rule is correct for patient $n$. The per-prediction reliability is defined as the absolute difference between the mean predicted rule-correctness (PRC) among rules outputting class 1 vs. class 0 for that patient: $\mathrm{Reliability}_n=\left|\frac{1}{K_n^+}\sum_{j\in K_n^+}(\mathrm{PRC})_{j,n}-\frac{1}{K_n^-}\sum_{\ell\in K_n^-}(\mathrm{PRC})_{\ell,n}\right|$. Rule-correctness labels for training each per-rule model are created as a binary indicator of whether the rule’s output matches the true label per training instance.","On Heart/Breast/Mammo datasets, ROC AUC for the personalized decision set is reported as 0.89/0.99/0.90 versus 0.82/0.98/0.84 (non-weighted) and 0.84/0.98/0.85 (globally weighted by overall rule accuracy). Reliability estimates stratify error: misclassification rates are highest for low reliability (0–10%) and decrease as reliability increases, approaching near-zero misclassification for 90–100% reliability bins. The figure reports outcome-prediction balanced accuracy (BA) alongside AUC: Heart BA=0.81, Breast BA=0.95, Mammo BA=0.83 (with corresponding AUCs 0.89, 0.99, 0.90). For the illustrative patient example, mean PRC for positive-voting rules is 80.5% vs 48.0% for negative-voting rules, yielding reliability 32.5%.","The authors note practical limitations of using one-way decision rules (no ELSE): rule-correctness labels are undefined for instances where the rule does not apply, training data per rule may be scarce, and labels may become single-class, preventing model training; they state this requires further analysis and was not a focus. They also state the current presentation targets binary classification and that extending to multiclass may require one-vs-all, while regression is “more complicated and not direct” because rule-correctness labeling relies on comparing two discrete values.","Although termed “reliability,” the proposed score is a heuristic based on separation of predicted per-rule correctness and is not calibrated/validated as a proper probability of correctness; it may not generalize across datasets or prevalence shifts. The evaluation is limited to three small UCI-style tabular datasets and does not test robustness to dataset shift, missingness, or correlated features, which are common in clinical settings. The per-rule correctness models are trained on labels derived from the same rules used at inference; without careful nesting, this can introduce leakage/optimism if rule induction and correctness-model training are not strictly separated within cross-validation folds. No comparison is made to established uncertainty quantification or conformal prediction methods for selective prediction/abstention, which are closer to reliability-of-prediction assessment in ML.","The authors propose further analysis/validation to broaden applicability, including (i) deeper study of incorporating one-way decision rules into the framework and how to overcome the associated training/labeling limitations, and (ii) extending beyond binary tasks—multiclass via one-versus-all and investigating how (or whether) the method can be adapted to regression settings.","Validate and calibrate the reliability score against true conditional error (e.g., reliability diagrams for error risk, proper scoring rules) and compare with selective prediction baselines (conformal prediction, entropy/MC-dropout, calibrated confidence). Extend the framework to handle unknown/estimated rule sets in a strictly nested evaluation pipeline (rule discovery inside CV) and study sensitivity to the rule-generation procedure and hyperparameters (number/length of rules). Develop methods to handle missing data, temporal/longitudinal predictors, and dataset shift (external validation), which are critical for clinical deployment. Provide an open-source reference implementation to support reproducibility and facilitate adoption in clinical decision support workflows.",2107.07483v1,https://arxiv.org/pdf/2107.07483v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:20:58Z
TRUE,Degradation modeling|RUL prediction|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Degradation measurements|Mixture of types,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper addresses how many Li-ion cells must be tested in degradation experiments to reliably characterize cell-to-cell (manufacturing) variability, given the high cost of large test cohorts. The authors fit simple empirical capacity-vs-time ageing models (1-parameter linear, 2-parameter linear with intercept, and a 3-parameter linear-plus-exponential ‘knee’ model) to five degradation datasets (10–100+ cells) and treat each cell’s model parameters as draws from an underlying population distribution. A hierarchical (multi-level) Bayesian framework is used to infer both individual-cell parameters and population-level mean/variance, with uncertainty quantified via MCMC and repeated sub-sampling (1,000 bootstrap-style repeats per sub-sample size). Stability of the inferred population variability is assessed via how the uncertainty in the population standard deviation estimate decreases with increasing number of cells, using a 10% threshold relative to an asymptotic log-linear decay region. Across dataset/model combinations, the mean minimum cohort sizes for consistent population variability estimates are approximately 9 cells (1-parameter), 11 cells (2-parameter), and 13 cells (3-parameter), implying order-10 replication per condition is needed to capture manufacturing variability in ageing tests.","Capacity-fade models: Linear-1: $B(t)=100\%+c_1 t$; Linear-2: $B(t)=B_0+c_2 t$; LinExp: $B(t)=100\%+c_3 t-\exp[(t-t_f)/\tau]$. Hierarchical Bayes generative model: per-cell measurements $\mathbf{B}_k=f(\theta_k)+\epsilon_k$ with Gaussian noise, and per-cell parameters $\theta_k\sim \mathcal{N}(\mu_g,\sigma_g^2)$ at the population level. Population posterior is formed by integrating over per-cell parameters (with Gaussian approximations to first-level posteriors), leading to a product form like $P(\mu_g,\sigma_g\mid\{\mathbf{B}_k\})\propto\prod_k \mathcal{N}(\mu_k,\sigma_k^2+\sigma_g^2)P(\mu_g,\sigma_g)$.","Using five Li-ion ageing datasets (48, 22, 21, 67-subset of 124, and 45 cells) and repeated sub-sampling (1,000 repeats per sub-sample size), the authors estimate the minimum number of cells required for stable population-variance inference. The mean required sub-sample sizes for consistent fits increase with model complexity: about 9 cells for the 1-parameter linear model, 11 cells for the 2-parameter linear model, and 13 cells for the 3-parameter linear+exponential knee model. They also report that stability typically occurs with fewer than 20 cells in most cases, but non-Gaussian parameter distributions (e.g., for $B_0$ in one dataset) can require around 20 cells. Simple sub-sample distribution (non-hierarchical) estimates appear overly confident at small sub-sample sizes compared with the hierarchical Bayesian approach.","The authors note that conclusions are based on limited-sized samples (order 10–100 cells) rather than the true manufacturing population, and future work should test whether larger samples change the results. They assumed no prior correlations between model parameters; this assumption was questionable in at least two cases (notably strong correlations in Attia-2020 LinExp), motivating further study. The approach assumes Gaussian population distributions; bi-/multi-modal populations are discussed as plausible, but not tested, and could affect inferred variability. They also state there was insufficient data to study how required sample size changes with variability due to differing usage (extrinsic variation) versus manufacturing variability.","The stability criterion (a 10% threshold relative to an identified asymptotic log-linear region) is somewhat heuristic and may yield different required sample sizes under alternative operational definitions of “stable” (e.g., based on coverage, KL divergence, or decision-relevant error). The modeling uses simple empirical capacity-time forms and does not directly address other key degradation outputs (e.g., resistance growth) or dependence on cycling variables, which may alter between-cell variability structure and sample-size needs. The two-step inference (Gaussian approximation of per-cell posteriors) can misrepresent uncertainty when per-cell parameter posteriors are skewed, bounded, or multi-modal, especially for knee-point parameters with limited late-life data. Computational details (chains, convergence diagnostics, priors sensitivity) are not fully captured here, so reproducibility and robustness to prior choices may be difficult to assess without shared code.","They propose testing robustness of the sample-size conclusions across a wider range of datasets and exploring the impact of parameter correlations in the hierarchical prior/model. They suggest extending the method beyond Gaussian population distributions to handle potentially bi-modal or multi-modal parameter populations. They also propose quantifying usage (extrinsic) variability alongside manufacturing variability and investigating impacts using more complex physics-based ageing models with many more parameters, potentially using synthetic data to study identifiability in higher-order models.","A practical extension would be to provide an explicit experimental-design tool (power/precision curves) mapping desired uncertainty in population variance (or prediction intervals for pack-level outcomes) to required number of cells and measurement frequency. Robust hierarchical models (e.g., Student-t, mixture models, or nonparametric Bayes) could improve inference under outliers, non-Gaussianity, and multi-modality, which are common in battery datasets. Incorporating autocorrelation and heteroscedastic measurement noise (capacity estimation error changing with age/cycle) could change the inferred number of cells needed for stable population estimates. Validating the recommendations on truly independent datasets (different labs/chambers) and across other health metrics (DCIR/impedance) would improve generalizability for reliability engineering use cases.",2107.07881v2,https://arxiv.org/pdf/2107.07881v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:21:43Z
FALSE,NA,ML-based|Bayesian,Sensor/condition monitoring|Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/JavierAntoran/Bayesian-Neural-Networks|https://github.com/pytorch/opacus/blob/master/examples/mnist.py|https://github.com/pytorch/opacus/blob/master/opacus/supported_layers_grad_samplers.py|https://github.com/JavierAntoran/Bayesian-Neural-Networks/blob/master/notebooks/regression/gp_homo_hetero.ipynb|https://github.com/JavierAntoran/Bayesian-Neural-Networks/blob/master/notebooks/regression/bbp_hetero.ipynb|http://krasserm.github.io/2019/03/14/bayesian-neural-networks/#appendix,"The paper studies differentially private Bayesian neural networks (DP-BNNs) to jointly provide privacy guarantees and Bayesian uncertainty quantification, and empirically examines a trade-off among privacy, accuracy, and predictive reliability (via calibration/uncertainty). It proposes three DP-BNN variants for the same network architecture: DP-SGLD (sampling-based via noisy gradient dynamics with clipping), DP-BBP (variational Bayes/BBP with DP optimizers on distribution parameters), and DP-MC Dropout (architectural randomness with DP training). A key theoretical contribution is an equivalence connecting DP-SGLD and a subclass of DP-SGD under specific parameter mappings, implying some DP-SGD training can yield “uncertainty quantification for free.” Privacy accounting is analyzed using Gaussian Differential Privacy (GDP) (and discussed alongside Moments Accountant), with explicit privacy expressions derived for DP-SGLD by reduction to DP-SGD. Experiments on MNIST classification and heteroscedastic synthetic regression compare accuracy, calibration (ECE/MCE), uncertainty behavior, speed, and scalability; DP-SGLD is reported to be most accurate under strong privacy and comparatively stable in uncertainty estimates, while DP-BBP is computationally heavy and DP-MC Dropout can show inflated uncertainty under DP.","Differential privacy definition: for neighboring datasets S,S', P[M(S)\in E] \le e^{\varepsilon}P[M(S')\in E]+\delta. DP-SGD uses per-sample gradient clipping \tilde g_i = \min\{1, C/\|g_i\|_2\}g_i and adds Gaussian noise to the averaged clipped gradient: \hat g = \frac{1}{|B|}\sum_{i\in B}\tilde g_i + \frac{\sigma C}{|B|}\mathcal N(0,I). DP-SGLD update adds Langevin noise: w_t \leftarrow w_{t-1}-\eta_t\left(\frac{n}{|B|}\sum_{i\in B}\tilde g_i+\nabla r(w_{t-1})\right)+\mathcal N(0,\eta_t). Theorem 1 maps DP-SGLD to DP-SGD by matching coefficients (e.g., \eta_{SGD}=\eta_{SGLD}n and \sigma_{SGD}=\sqrt{\eta}|B|/(nC)), enabling DP-SGLD privacy via DP-SGD accountants.","On MNIST, under nearly identical privacy budgets (reported examples: \varepsilon_{GDP}\approx0.861 for DP-SGLD vs \varepsilon_{GDP}\approx0.834 for other DP models; also \varepsilon_{MA}\approx0.989 vs \approx0.955), DP-SGLD achieves higher test accuracy than DP-BBP, DP-MC Dropout, and DP-SGD. Reported DP accuracies (MLP; CNN in parentheses): DP-SGLD 0.90 (0.95), DP-BBP 0.80, DP-MC Dropout 0.78 (0.77), DP-SGD 0.77 (0.95); non-DP counterparts are ~0.95–0.98 depending on method/architecture. Calibration on MNIST (MLP) shows DP-SGLD with Gaussian prior can have very low DP-ECE (0.007) compared to DP-BBP with prior (0.204) and DP-MC Dropout with prior (0.008), with MCE varying notably by method/prior. In heteroscedastic regression at (\varepsilon,\delta)=(4.21,1/250), median MSE over 20 runs is DP-SGLD 0.510 vs DP-BBP 1.276 and DP-MC Dropout 0.682, indicating DP-SGLD retains accuracy and yields more consistent uncertainty intervals under DP.","DP-SGLD does not yield an analytic posterior and requires storing many weight iterates (hundreds to thousands) for uncertainty quantification, creating substantial memory burden and limiting scalability to very large models. The method may require a long burn-in period to converge. DP-BBP is computationally expensive (multiple backpropagations per step due to sampling) and is not compatible with outer-product acceleration; extending BBP to convolutional layers is noted as non-trivial.","The work’s notion of “reliability” is primarily ML calibration/uncertainty rather than reliability engineering (failure/degradation), so conclusions may not transfer to engineering reliability contexts without additional validation. Experimental validation is limited to MNIST and a synthetic regression setup; broader real-world datasets (especially safety-critical domains) and distribution shift/OOD testing are not demonstrated. Practical DP deployments often require careful Phase I-style tuning and hyperparameter selection under privacy constraints; the paper’s comparisons may depend heavily on tuned settings and chosen priors/clipping norms. The privacy-utility-calibration interplay is evaluated mainly with ECE/MCE; other reliability-of-uncertainty metrics (selective prediction risk-coverage, proper scoring rules, calibration under shift) are not systematically explored.","The authors propose extending the DP-SGD/DP-SGLD connection to more general DP-SG-MCMC methods to accelerate convergence of Bayesian gradient methods. They also call for deeper theoretical investigation of DP-BNNs’ convergence (including rates), generalization, and calibration behaviors, drawing analogy to existing theory for DP linear regression and DP deep learning. They mention exploring calibration of DP-BNNs with alternative clipping schemes such as global clipping as a potentially interesting direction.","Evaluate DP-BNN uncertainty reliability under distribution shift and OOD inputs (e.g., corrupted MNIST, domain transfer) and connect calibration to decision-theoretic risk/abstention. Develop scalable DP-SGLD variants that avoid storing long weight trajectories (e.g., running posterior summaries, low-rank approximations, or distillation of posterior samples) while maintaining uncertainty quality. Provide standardized, reproducible benchmarking across multiple DP accountants and datasets, including sensitivity analyses on clipping/noise schedules and priors. Implement and release a dedicated library or Opacus-compatible module for DP-BBP/variational BNNs with efficient per-sample gradients to make comparisons more practical.",2107.08461v2,https://arxiv.org/pdf/2107.08461v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:22:32Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,R|None / Not applicable|Other,Supplementary material (Journal/Publisher),https://doi.org/10.5281/zenodo.5034244,"This paper proposes a systematic, iterative process for applying inter-rater reliability (IRR) and inter-rater agreement (IRA) measures within Grounded Theory (GT) studies in software engineering, targeting collaborative coding settings. It first reports a systematic literature review (SLR) of recent GT studies in software engineering using IRR/IRA and identifies common shortcomings such as poor reporting of procedures, confusion between IRR and IRA, and limited detail on disagreement resolution and thresholds. The authors then formalize a GT-compatible workflow indicating when to compute IRR/IRA during open/initial and selective coding cycles, and how to use results to drive codebook refinement via structured discussions and a “disagreements diary.” Feasibility is illustrated through a GT study in the EdgeOps domain using Krippendorff’s alpha-based inter-coder agreement computed in Atlas.ti, with iterative refinement until an acceptance threshold is achieved. The paper’s contribution is methodological guidance for qualitative rigor and consensus-building rather than reliability engineering of systems or components.","The paper centers on agreement/reliability metrics for qualitative coding, primarily Krippendorff’s $\alpha$ and two variants for inter-coder agreement: $cu\text{-}\alpha$ computed per semantic domain (agreement on specific codes within a domain) and $Cu\text{-}\alpha$ computed across domains (agreement on choosing domains regardless of specific codes). A commonly used acceptability rule-of-thumb is noted: $\alpha \ge 0.667$ for tentative conclusions and $\alpha \ge 0.80$ for evidence of reliable coding; in the case study they use a threshold of $Cu\text{-}\alpha \ge 0.8$ to proceed. The process computes IRR/IRA after each coding iteration and, if below threshold, triggers a refinement cycle (discussion, codebook edits, recoding on new subsets).","In the SLR, 173 unduplicated papers were found; 168 were processed after removing duplicates and inaccessible full texts, and 49 primary studies were ultimately selected as GT/GT-like studies that used IRR or IRA statistics during coding. In the EdgeOps feasibility study, open coding iteration 1 yielded global $Cu\text{-}\alpha=0.56$ (below 0.8), prompting codebook refinement; iteration 2 reached $Cu\text{-}\alpha=0.80$ (meets threshold). In selective coding, a single iteration achieved $Cu\text{-}\alpha=0.80$ with per-domain values (e.g., 1.00, 0.95, 0.87, 1.00) and the team judged theoretical saturation reached. The paper also reports perfect agreement in the dual-study selection check in the SLR’s first iteration (Krippendorff’s $\alpha=1$ for binary inclusion decisions on the initial sample).","The authors note several unaddressed SLR bias-mitigation techniques (e.g., backward/forward snowballing, additional index searching such as Google Scholar, and including dissertations or preprints), which could strengthen evidence even if they believe the sample was sufficient for the RQs. They also state that external validity/generalizability of the proposed process is limited because demonstrating feasibility across multiple full GT studies would require enormous effort. They conclude only that applying the process to one GT study “seems to support its feasibility,” pending further confirmation.","The work is not about engineering system reliability (failure behavior, lifetime/degradation, maintenance) despite the term “reliability,” so its relevance to reliability engineering is indirect at best. The feasibility demonstration uses a single case study and a specific toolchain (Atlas.ti), and performance/utility may differ with other qualitative workflows, coder training regimes, or domains; there is limited evidence about adoption cost and practical burden in large teams. The process emphasizes threshold-based stopping (e.g., $\alpha\ge0.8$), but does not deeply analyze sensitivity to unitization choices (quotation boundaries), prevalence effects, or the impact of sequential (not parallel) coding on bias and discovery of alternative interpretations. The SLR synthesis identifies reporting gaps, but the paper does not provide a standardized reporting checklist or minimal reproducible template that would operationalize compliance for practitioners/journals.","The paper suggests that additional confirmation is needed before the process can be considered more than a first step toward a de facto standard for GT studies requiring IRR/IRA, implying replication across further GT studies. It frames the current contribution as feasibility support and calls for further validation/generalization through more applications.","Future work could develop a concrete reporting checklist and artifact templates (e.g., codebook format, disagreement diary schema, unit-of-coding rules) to improve reproducibility across teams. Comparative studies could evaluate sequential versus parallel collaborative coding (time, codebook stability, convergence, interpretive diversity) and quantify trade-offs between rigor and meaning loss. Extensions could address settings with autocorrelated or evolving codebooks (self-starting/online agreement monitoring), and guidance for choosing thresholds and statistics under varying prevalence and number of coders. Providing an open-source reference implementation (e.g., R/Python scripts) for computing the specific $cu\text{-}\alpha$ and $Cu\text{-}\alpha$ variants outside CAQDAS tools would also broaden accessibility and facilitate replication.",2107.11449v1,https://arxiv.org/pdf/2107.11449v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:23:10Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Other,Not applicable,Theoretical/simulation only|Environmental monitoring|Finance/economics|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/resinj/replication_GR21,"This paper develops a probability-theoretic framework linking regression diagnostics with forecast evaluation through hierarchies of calibration (reliability) for real-valued outcomes. It defines conditional T-calibration for statistical functionals T (e.g., threshold exceedance probabilities, means, quantiles, expectiles, moments) and establishes when auto-calibration implies T-calibration for identifiable functionals. The authors introduce population T-reliability diagrams and revisit score decompositions into miscalibration (MCB), discrimination (DSC), and uncertainty (UNC), leading to a universal coefficient of determination R* = (DSC − MCB)/UNC that generalizes classical R^2 (mean regression) and R^1 (quantile regression). For empirical estimation they generalize the CORP approach using nonparametric isotonic regression via the pool-adjacent-violators (PAV) algorithm, yielding stable monotone reliability diagrams and nonnegative estimated score components. The methodology is illustrated on ecological point forecasts (butterfly population sizes) and on Bank of England inflation density forecasts, with code provided for replication.","Key definitions include the randomized probability integral transform (PIT) $Z_F = F(Y^-) + U\,(F(Y)-F(Y^-))$ and conditional T-calibration $T(\mathcal{L}(Y\mid T(F))) = T(F)$ (a.s.). The score decomposition under a consistent scoring function is $\bar S = \mathrm{MCB} - \mathrm{DSC} + \mathrm{UNC}$, and the proposed universal coefficient of determination is $R^* = (\mathrm{DSC}-\mathrm{MCB})/\mathrm{UNC}$. Empirical recalibration uses isotonic regression/PAV to produce monotone recalibrated values $\hat x_i$ minimizing average loss among nondecreasing sequences.","The paper proves that auto-calibration implies probabilistic and marginal calibration, and for identifiable functionals it also implies conditional T-calibration (and hence unconditional T-calibration under mild conditions). It shows that common conditional notions (CEP, threshold, quantile calibration) are not equivalent in general, but become equivalent under continuity/strict monotonicity and technical support conditions. The CORP/PAV-based estimators yield nondecreasing empirical reliability diagrams and guarantee nonnegative estimated MCB and DSC components, avoiding artifacts from ad hoc binning. The universal $R^*$ nests classical $R^2$ (least squares) and $R^1$ (quantile regression) and under modest in-sample fitting conditions lies in $[0,1]$.","The authors note that their residual-based approach for generating consistency bands for T-reliability diagrams is a ""crude yet viable alternative"" that relies on assumptions like independence between forecasts and residuals, and they discuss that auto-calibration is stronger than T-calibration, which can confound consistency bands when resampling under auto-calibration. They further state that large-sample theory and tailored methods for consistency/confidence bands and score-component inference depend on the functional T and require additional development beyond the scope of the paper.","The paper is focused on calibration/reliability in probabilistic forecasting and regression diagnostics rather than engineering reliability; thus its notions of “reliability” do not map to failure-time reliability without additional modeling. Practical adoption may be hindered by the need to choose appropriate functionals T and by the isotonicity assumption for the recalibration function, which may be violated in some applications (e.g., with strong covariate shift or nonstationarity). The empirical illustrations (ecology, inflation forecasts) do not stress-test robustness to dependence, heavy tails, or high-dimensional feature settings common in modern ML calibration workflows.","They call for follow-up work to develop functionalspecific Monte Carlo and asymptotic methods for consistency and confidence bands for T-reliability diagrams (beyond their residual-based approach). They also encourage further work on asymptotic distributions for CORP estimators of miscalibration/discrimination components and on formal hypothesis tests of calibration leveraging their hierarchical framework and CORP estimation. The discussion suggests extending diagnostic displays (e.g., MCB–DSC scatterplots) and developing simultaneous/multiple testing procedures across calibration notions.","Extending the CORP/T-calibration framework to settings with strong temporal/spatial dependence (e.g., explicit time-series calibration with valid uncertainty quantification) would improve applicability. Developing software beyond replication scripts (e.g., an R/Python package implementing generalized T-PAV, diagrams, and inference) would facilitate adoption. It would also be valuable to study robustness/regularization when isotonicity is implausible (e.g., splines with monotonicity constraints, Bayesian monotone regression) and to integrate covariate-conditional calibration notions (e.g., groupwise or fairness constraints) directly into the estimation and visualization pipeline.",2108.03210v3,https://arxiv.org/pdf/2108.03210v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:23:47Z
TRUE,System reliability|Other,Stochastic process|ML-based|Physics-based|Hybrid/Ensemble,Sensor/condition monitoring|Simulated only|Mixture of types,Not applicable,Semiconductor/electronics|Other,Simulation study|Other,TRUE,Python|MATLAB|Other,Not provided,NA,"The paper proposes framing continuous-time system reliability assessment for multi-state systems as a physics-informed deep learning problem, using a neural network surrogate for the state-probability vector p(t) in a non-homogeneous continuous-time Markov chain governed by the forward Kolmogorov equations. A PINN-style composite loss enforces the initial condition and the differential equation residual p'(t)=p(t)Q(t) at collocation points, enabling a continuous-time reliability estimate R(t) by summing probabilities over “functioning” states. The authors also discuss a physics-informed GAN (PIGAN) formulation to perform uncertainty quantification and incorporate measurement/inspection data by combining adversarial learning with physics-based constraints in the generator loss. Three numerical examples on a 4-state dual-processor computing system (with Weibull time-dependent transition rates) compare the deep-learning solution to an ODE solver and Monte Carlo simulation, showing close agreement. Reported experiments indicate substantial computational speedups versus Monte Carlo while maintaining comparable reliability trajectories and uncertainty bounds, and the PIGAN approach is heuristically demonstrated for updating reliability with synthetic inspection data.","The system is modeled as a continuous-time Markov chain with time-varying transition-rate matrix Q(t) and state probabilities p(t), governed by the forward Kolmogorov ODE p'(t)=p(t)Q(t) with initial condition p(0)=s0. The PINN loss combines an initial-condition penalty and an ODE-residual penalty at collocation points: L(θ)=(Nθ(0)−s0)^2+λ(1/Nc)∑||Nθ(ti)Q(ti)−dNθ(ti)/dti||^2, where dNθ/dt is obtained by automatic differentiation. Reliability is computed by summing the NN-predicted probabilities over functioning states U: R(t)≈∑_{j∈U}Nθ(t)[j]. For PIGANs, the generator and discriminator are trained with adversarial losses augmented by measurement-data mismatch and the same physics residual constraint.","In Example 1 (system starts in state 0), the proposed PINN approach produces state-probability and reliability curves closely matching both a Runge–Kutta ODE solver and Monte Carlo simulation (MC with 1×10^5 iterations), and the authors report small RMSE and small absolute differences over 60 replications. The proposed method reportedly takes 17.9 seconds per replicate versus 283.4 seconds per replicate for Monte Carlo in that setup. In Example 2 (uncertain initial condition with Beta-distributed Bernoulli parameter), the PIGAN-based approach generates uncertainty bands consistent with Monte Carlo; the full training+sampling process takes 625.9 seconds versus ~4 hours total MC runtime (50 replications), claimed as ~24× more efficient. Example 3 shows heuristic updating of reliability evolution with synthetic inspection data, where updated reliability trends shift above/below a baseline consistent with “better/worse” synthetic measurements and uncertainty intervals cover the synthetic points.","The authors state that physics-informed deep learning (and especially PIGANs) is still in an early stage and needs careful configuration for specific problems, and that improving uncertainty quantification and robustness is a main concern. They note that training GANs is difficult and PIGAN training is even more challenging due to the composite physics/data loss, and deviations versus Monte Carlo can arise from both Monte Carlo randomness and neural network configuration. They also state that measurement data cannot be properly incorporated using current conventional system reliability assessment methods, so the measurement-data example is demonstrated heuristically using synthetic data.","The demonstrated applications are limited to a continuous-time Markov (memoryless) multi-state model; real systems often exhibit non-Markovian effects, state-duration dependence, covariate-driven rates, or repair/maintenance interventions that may violate assumptions. The empirical evaluation is confined to a single small synthetic benchmark (4-state system) with specific hyperparameters; generalization to larger state spaces, stiff ODEs, rare-event/highly reliable regimes, or real inspection datasets is not validated. Comparisons focus on ODE solver and brute-force Monte Carlo; important reliability UQ baselines (e.g., uniformization, importance sampling/rare-event simulation, Bayesian filtering/data assimilation, particle methods) are not benchmarked. No reproducible code or implementation details (random seeds, full architectures for all cases, training stability diagnostics) are provided, which limits assessability and repeatability of the claimed speedups.","They state that future work will focus on systematically configuring robust network architectures and training algorithms for physics-informed deep learning. Specifically proposed directions include using Bayesian deep learning and deep ensembles to enhance uncertainty quantification, applying meta-learning and multi-task learning to improve training, developing architectures for non-Markovian system reliability assessment, using physics-informed deep learning to discover patterns in reliability evolution, and developing a systematic framework to integrate measurement data with other mathematical models of reliability evolution.","Validate the approach on real industrial reliability/inspection datasets (not only synthetic) and quantify performance under realistic noise, missingness, and biased inspection schedules. Extend the formulation to handle unknown/estimated transition-rate parameters Q(t;φ) jointly with state probabilities (inverse problems) using Bayesian inference and identifiability checks, including credible intervals for rates and reliability. Develop self-starting/online variants for sequential updating with streaming inspection/sensor data (e.g., PINN/PIGAN with filtering/assimilation) and include robustness tests under model misspecification and autocorrelated data. Provide open-source implementations and standardized benchmarks, and compare against specialized CTMC solvers (uniformization) and rare-event simulation methods to substantiate computational-efficiency claims across regimes.",2108.10828v2,https://arxiv.org/pdf/2108.10828v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:24:26Z
FALSE,Other,ML-based|Bayesian|Hybrid/Ensemble,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Finance/economics|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,NA,"The paper proposes a new loss function for regression predictive uncertainty estimation derived from the Bayesian Validation Metric (BVM), using an psilon-agreement Boolean that relaxes exact model–data matching. The resulting psilon-BVM loss is shown to reproduce Gaussian maximum-likelihood (negative log-likelihood, NLL) training as the limiting case when psilon \to 0, and it is used to train deep ensembles of neural networks that output both mean and variance. The method is evaluated on toy regression, multiple UCI regression benchmarks (in-distribution), and out-of-distribution (OOD) settings including a Google stock price forecasting task and an ""outlier train-test splitting"" protocol using Isolation Forest. Results show the BVM-ensemble provides calibration comparable to deep ensembles (NLL-trained) on in-distribution data, and improved NLL on OOD/outlier test sets due to increased predictive variance induced by psilon > 0. The work positions the approach as a simple training modification to improve robustness under dataset shift for safety-critical deployment, but it is primarily a machine-learning uncertainty method rather than a reliability engineering model.","The standard Gaussian NLL for a network predicting mean and variance is $C_{\mathrm{NLL}}=\frac{1}{N}\sum_{n=1}^N\left[\tfrac{1}{2}\log(2\pi\sigma_n^2)+\frac{(t_n-\mu_n)^2}{2\sigma_n^2}\right]$. The proposed \u000epsilon-BVM probability of agreement for point target $t_n$ is $p(A\mid x_n)=\Phi\left(\frac{t_n+\epsilon-\mu_n}{\sigma_n}\right)-\Phi\left(\frac{t_n-\epsilon-\mu_n}{\sigma_n}\right)$, yielding the loss $C_{\mathrm{BVM}}(\epsilon)=\frac{1}{N}\sum_{n=1}^N -\log p(A\mid x_n)$. A Taylor expansion around $\epsilon=0$ shows $C_{\mathrm{BVM}}(\epsilon)=C_{\mathrm{NLL}}-\frac{\epsilon^2}{6}\left[\frac{(t_n-\mu_n)^2}{\sigma_n^4}-\frac{1}{\sigma_n^2}\right]+O(\epsilon^3)$, explaining why nonzero $\epsilon$ increases learned variance and improves OOD likelihood.","On UCI benchmarks with random (in-distribution) splits, BVM-ensembles (\epsilon=0.01, K=5) are competitive with PBP, MC-dropout, and deep ensembles, but generally do not beat deep ensembles on NLL because deep ensembles directly optimize NLL (e.g., Energy NLL: Deep Ensembles 1.38 vs BVM 1.67). Calibration curves (reliability diagrams) show BVM uncertainty is close to the ideal diagonal and nearly overlaps deep ensembles, while MSE-ensemble empirical variance is overconfident (e.g., on Energy, a nominal 40% interval contains about 10% of observations). In the Google stocks OOD setting, increasing \epsilon widens prediction intervals and improves NLL (Deep Ensembles 5.23; BVM \epsilon=0.01: 5.19; BVM \epsilon=0.1: 5.13). Under ""outlier train-test splitting"" (top 10% outliers via Isolation Forest), BVM outperforms deep ensembles in NLL on datasets with larger train-test target shifts (e.g., Boston: 4.51 vs 3.92; Energy: 2.98 vs 2.57; Yacht: 3.95 vs 1.83).","The authors note that performance depends on the choice of the \u000epsilon parameter, and selecting \u000epsilon can be computationally demanding if treated as a hyperparameter requiring search/optimization. They also acknowledge that while they focus on one specific BVM-derived loss, other BVM-based losses may not admit closed-form expressions and could be more computationally expensive. They mention that recurrent neural networks could improve the stock forecasting task but were not used because the focus was on uncertainty rather than accuracy.","The approach is not a reliability engineering method per se; it does not model failure/degradation mechanisms, lifetimes, or maintenance decisions, and the reliability relevance is mainly motivational. The uncertainty model assumes a Gaussian predictive likelihood in core derivations/evaluations, and robustness to heavy tails, heteroscedastic non-Gaussian noise, or correlated residuals is not systematically explored. OOD evaluation relies on a particular constructed protocol (Isolation Forest outlier split) and one finance time-series example, which may not represent realistic dataset-shift modes in safety-critical engineered systems. No public implementation details, reproducibility artifacts, or computational cost analysis (training overhead vs standard NLL ensembles) are provided.","They propose expanding the BVM framework to predictive uncertainty estimation for classification and vision problems, and pursuing real-world deployment in safety-critical systems and applications. They also suggest approaches for choosing \u000epsilon, including learning it during training or tuning it via validation/hyperparameter search, and mention post-hoc calibration alternatives that avoid optimizing \u000epsilon directly.","Develop self-starting or adaptive \u000epsilon schemes that adjust agreement tolerance based on detected drift or local data density, and provide theoretical guarantees for calibration/coverage under dataset shift. Extend the loss to non-Gaussian likelihood families (e.g., Student-t/Laplace mixtures) and to structured outputs (multivariate regression with covariance) to better match engineering sensor data. Provide comprehensive comparisons against modern OOD/uncertainty methods (e.g., evidential regression variants, conformal prediction, deep kernel learning) using standardized OOD benchmarks. Release reference code and add ablation studies on \epsilon, ensemble size, and architectural choices to improve reproducibility and practical guidance.",2109.08213v2,https://arxiv.org/pdf/2109.08213v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:25:18Z
TRUE,System reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Energy/utilities|Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,"https://books.google.com/books?hl=zh￾CN&lr=&id=r2VODQAAQBAJ&oi=fnd&pg=PR1&dq=Rubinstein+RY.+Simulation+and+the+Monte%E2%80%93Carlo+method.+New+York:+Wiley%3B+1981.&ots=13YWiw6b￾8&sig=HQP5so5XRDQRaKDv9LsxXcLBTpA|http://chodor-projekt.net/wp￾content/uploads/BiNSK/Literatura/Dilevsen,Madsen,%20Structural%20Reliability%20Methods%20(2007).pd|https://books.google.com/books?hl=zh￾CN&lr=&id=oC4MDigl9PsC&oi=fnd&pg=PT8&dq=Lemaire+M.+Structural+reliability.+ISTE+Wiley%3B&ots=gxRZo1u21a&sig=mvKN3vgtKAxvkm_3FF9XpsdedhA|http://arxiv.org/abs/1112.5389|http://papers.nips.cc/paper/7016-multi-information-source-optimization.pdf|http://arxiv.org/abs/1910.02497|http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.86.3414|http://www.uqlab.com/userguidekriging","The paper proposes AMGPRA (Adaptive Multi-fidelity Gaussian Process for Reliability Analysis), an active-learning surrogate-based method to estimate failure probability $P_f=P(g(\mathbf{x})\le 0)$ when multiple model fidelities are available. Unlike prior mfEGRA approaches that (i) pick the next design point using a high-fidelity learning function and then (ii) choose the information source, AMGPRA introduces a Collective Learning Function (CLF) that simultaneously selects both the next point and fidelity by quantifying the global reduction in a core learning function across a candidate set, normalized by evaluation cost. The surrogate is a multi-fidelity GP that models the high-fidelity response plus independent GP discrepancies for each low-fidelity model, with hyperparameters fit via maximum likelihood; reliability is estimated via Monte Carlo simulation on the GP mean predictions and refined until a stopping criterion (e.g., max(EFF) < 0.001) is met. Performance is demonstrated on three numerical problems (including high-dimensional examples) and an engineering case study on wind reliability of a transmission tower using low-/high-fidelity finite-element models. Across examples, AMGPRA achieves similar or better failure-probability accuracy than state-of-the-art single- and multi-fidelity methods with reduced total computational cost, and it is the only method reported to converge in the noisy nonlinear FE tower case where mfEGRA/AK-MCS struggle.","Failure probability is defined by the limit-state integral $P_f=P(g(\mathbf{x})\le 0)=\int_{g(\mathbf{x})\le 0} f(\mathbf{x})\,d\mathbf{x}$, estimated by Monte Carlo as $\hat P_f\approx \frac{1}{N}\sum_{i=1}^N I(\mathbf{x}_i)$ with $I=1$ if $g(\mathbf{x})\le 0$. The method’s core is the Collective Learning Function (CLF), e.g. (cost-normalized) $\mathrm{CLF}(\mathbf{x}_{n+1},\ell_F)=\frac{1}{c_{\ell_F}(\mathbf{x}_{n+1})}\frac{1}{N_c}\sum_{i=1}^{N_c}\Big[lf(\mu_P(0,\mathbf{x}^{(i)}),\sigma_P^2(0,\mathbf{x}^{(i)})) - lf(\mu_P(0,\mathbf{x}^{(i)}),\sigma_F^2(0,\mathbf{x}^{(i)}\mid\mathbf{x}_{n+1},\ell_F))\Big]$, which measures the global average reduction of a core learning function (e.g., EFF or a modified $U$) under the hypothetical future GP variance induced by adding $(\mathbf{x}_{n+1},\ell_F)$. The multi-fidelity GP prior is $\hat g(\ell,\mathbf{x})=\hat g(0,\mathbf{x})+\delta_\ell(\mathbf{x})$ with independent discrepancy GPs $\delta_\ell\sim GP(0,\Sigma_\ell)$, giving covariance $\Sigma_{pr}((\ell,\mathbf{x}), (\ell',\mathbf{x}'))=\Sigma_0(\mathbf{x},\mathbf{x}')+\mathbf{1}_{\ell,\ell'}\Sigma_\ell(\mathbf{x},\mathbf{x}')$.","Computations are performed in MATLAB 2019b and repeated 20 times; across benchmark problems AMGPRA reduces total normalized evaluation cost relative to mfEGRA while matching or improving error. In the multimodal example with two fidelities, AMGPRA-LF-EFF cost 12.58 (vs. mfEGRA 13.31 and AK-MCS-EFF 45.2) with $\hat P_f\approx 3.14\times 10^{-2}$ and 0.03% average relative error; with three fidelities, AMGPRA-LF-EFF cost 12.32 vs. mfEGRA 12.87 with similar error (~0.02%). In the 10D example, AMGPRA-LF-EFF cost 16.8 vs. mfEGRA 18.16, achieving 0% relative error (reference $P_f\approx 2.73\times 10^{-3}$ from large MCS). In the transmission tower wind reliability case (low-/high-fidelity FE in OpenSEES), AMGPRA-LF-EFF is reported as the only convergent method and estimates $\hat P_f=2.46\times 10^{-2}$ at cost 28.5, using on average 18.1 high-fidelity and 25.6 low-fidelity training evaluations.","The authors state that only two core learning functions (EFF and a modified $U$) are used inside CLF in this study, and that alternative (potentially more suitable) learning functions should be explored in future work. They also state that the proposed approach is constrained for high-dimensional problems due to the general limitations of Gaussian process metamodels, motivating work to expand multi-fidelity reliability analysis capabilities to higher dimensions.","The method assumes the hypothetical future GP mean remains equal to the current mean when evaluating CLF (to reduce computational burden), which may bias point/source selection when mean changes are material, especially near the limit state. The approach depends on the quality of the multi-fidelity GP discrepancy model (independent discrepancy GPs with Gaussian kernels and MLE fits), which may be misspecified when fidelity relationships are nonstationary, heteroscedastic, or non-additive; robustness to such mismatch is not thoroughly analyzed. Stopping is based on a heuristic threshold on max(EFF) and an MCS COV check over a candidate set, which can be sensitive to candidate-set size and sampling; guarantees on $\hat P_f$ error are not provided beyond empirical comparisons.","They propose investigating alternative learning functions as the core $lf$ inside CLF to better adapt to different reliability problems. They also explicitly identify extending multi-fidelity reliability analysis to high-dimensional problems as an important future research direction, given the scalability limits of GP surrogates.","Developing a CLF variant that accounts for both mean and variance changes in the hypothetical future GP (or integrates over the distribution of the future observation) could improve acquisition fidelity, especially in noisy/non-smooth FE settings. Extending the framework to handle correlated/biased low-fidelity sources via more general co-kriging structures (e.g., non-additive or input-dependent scaling, heteroscedastic noise) and providing software/toolbox implementations with reproducible benchmarks would enhance adoption. Additional validation on real industrial datasets and comparisons against modern Bayesian optimization-style multi-fidelity acquisition functions (e.g., cost-aware entropy search variants) would clarify competitiveness and robustness.",2109.10219v1,https://arxiv.org/pdf/2109.10219v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:26:19Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Mixture of types|Other,Not applicable,Other,Simulation study|Other,TRUE,R|Other,Not provided,NA,"This paper assesses the reliability of published causal findings that use the regression discontinuity (RD) design in top political science journals (APSR, AJPS, JoP) from 2009–2018. The authors compile 45 RD studies and document “pathological” bunching of reported t-statistics just above 1.96 (the 5% significance threshold), consistent with selective reporting/publication pressures. Using replication data for 36 studies, they reanalyze a subset with standardized, modern RD inference via the R package rdrobust (Calonico et al., 2015) and show that point estimates change little but standard errors increase, reducing statistical significance relative to original reports. They also conduct retrospective power analyses (via rdpower) indicating most RD studies are underpowered to detect small-to-moderate effects, especially those using election data. Overall, the paper argues that inappropriate inference methods and low power, combined with publication selection, may lead published RD findings in political science to be exaggerated or spurious.","The paper’s core empirical diagnostics are based on reported or reconstructed t-statistics $t=\hat\tau/\widehat{\mathrm{SE}}(\hat\tau)$ and caliper (window) counts of t-statistics just below vs. just above 1.96, evaluated with a one-sided binomial test under equal probability of falling above/below the threshold. Reanalysis uses rdrobust’s default MSE-optimal bandwidth (“mserd”) and the Calonico-Cattaneo-Titiunik robust bias-corrected inference (bias-corrected point estimate plus robust standard errors that incorporate bias-estimation uncertainty). Retrospective power is computed for a two-sided 5% test using the rdpower implementation, with effect size defined as Cohen’s $d$ (treatment effect standardized by the control-outcome SD within the selected bandwidth).","Across 45 published studies, t-statistics cluster around and especially just above 1.96; caliper tests show roughly three times as many studies just above as just below 1.96 (10% window: 9 vs 3, p=0.073; 15%: 11 vs 4, p=0.059; 20%: 14 vs 5, p=0.032). In reanalysis (39 estimates from 25 studies not originally using rdrobust), bias-corrected inference shifts the t-statistic distribution leftward because standard errors become larger, indicating many original analyses understated uncertainty. Retrospective power over 64 analyses (36 studies with replication data) shows only 22% achieve 80% power for effect size $d=0.2$; power improves mainly for large effects (56% at $d=0.5$, 70% at $d=0.8$). Election-based RD studies are substantially less powered (for $d=0.2$, 80% power in 13% of analyses) than non-election studies (42%).","The authors note that rdrobust’s robust bias-corrected inference is based on asymptotic approximations and may have imperfect finite-sample coverage (citing evidence that nominal 95% CIs can cover less than 92% even at n=1,000). They also acknowledge limitations of the rdhonest robustness check, including the need to specify/estimate a smoothness bound (here via a global quartic fit, which imposes stronger assumptions) and that their rdhonest version cannot handle clustered standard errors, restricting the set of studies analyzed. They caution that subgroup caliper tests (automated vs non-automated bandwidth selection) have small samples and reduced power.","Because the study focuses on three “top” journals and 2009–2018, results may not generalize to other outlets, later periods, or unpublished work (working papers, dissertations), where selection incentives differ. The analysis treats bunching near 1.96 largely as evidence consistent with selection, but does not fully disentangle alternative mechanisms (e.g., discrete researcher decision rules on what to highlight in abstracts vs main text, or model misspecification that affects SEs differently across contexts). The reanalysis standardization (rdrobust defaults) may not be substantively appropriate for every original estimand (e.g., different kernels, polynomial orders, covariate adjustments, clustering structures, or fuzzy RD complications), so some changes in SEs could reflect deliberate design differences rather than “inappropriate” inference. Power analyses are retrospective and depend on modeling choices (bandwidth, variance estimates, effect-size standardization), which can yield optimistic or pessimistic power depending on assumptions; they also do not incorporate publication selection into the power estimates themselves.","They suggest improving empirical practice by using modern RD inference procedures (e.g., Calonico et al., 2014; Armstrong & Kolesár, 2020), focusing on settings with sufficient effective sample size/power, and preregistering RD analyses when feasible to limit specification searching and clarify the file-drawer problem. They also discuss covariate adjustment as a possible way to improve precision in RD, while noting it can introduce additional specification choices that may facilitate p-hacking.","A natural extension would be to formally model and estimate publication selection mechanisms for RD results (e.g., selection models or p-curve/style methods adapted to RD and multiple-testing within papers) to quantify the magnitude of bias rather than documenting bunching patterns. Another direction is to evaluate finite-sample operating characteristics of commonly used RD workflows (including clustering, fuzzy RD, and covariate adjustment) under empirically calibrated designs typical in political science elections, potentially yielding practical design rules for minimum effective sample size near the cutoff. Building open, standardized replication pipelines (and sharing code) for RD reanalyses would improve transparency and allow continuous monitoring of inference quality over time. Finally, extending the analysis beyond political science to cross-disciplinary comparisons using harmonized inclusion criteria could clarify whether differences are driven by data availability, norms, or methodological choices.",2109.14526v2,https://arxiv.org/pdf/2109.14526v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:27:01Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,Python,Not provided,https://github.com/ermongroup/smile-mi-estimator|https://github.com/LMescheder/AdversarialVariationalBayes,"This paper studies reliable (low-variance, stable) estimation of Kullback–Leibler (KL) divergence from samples using a discriminator-based (GAN-type) objective, and argues that instability in neural discriminator estimators arises from uncontrolled function-class complexity. The authors propose constructing the discriminator in a Reproducing Kernel Hilbert Space (RKHS) via a neural-feature parameterization and then controlling complexity by penalizing the RKHS norm (implemented through an L2 penalty on an auxiliary network). They derive probability bounds for the deviation-from-mean error of the KL estimate in terms of RKHS covering numbers, kernel complexity (supremum of the kernel), and a Sobolev-embedding operator norm, and prove consistency of the regularized estimator as sample size grows. Empirically, across KL estimation between Gaussians, mutual information estimation, and Adversarial Variational Bayes on MNIST, the RKHS construction with norm control reduces variance and stabilizes training relative to standard neural discriminators. The work advances discriminator-based divergence estimation by linking estimator reliability to function-space complexity and providing a scalable regularization mechanism.","The discriminator is trained via a GAN-style logistic objective: $f^* = \arg\max_f \{\mathbb{E}_p[\log\sigma(f(x))] + \mathbb{E}_q[\log(1-\sigma(f(x)))]\}$, and the KL is estimated as $\mathrm{KL}(p\|q)=\mathbb{E}_p[f^*(x)]$. The RKHS discriminator is constructed using $f(x)=\int g(w)\psi(x,w)\,d\tau(w)$ with $\psi(x,w)=\phi_\theta(x)^\top w$, yielding kernel $K_\theta(x,t)=\gamma\,\phi_\theta(x)^\top\phi_\theta(t)$. Complexity is controlled by optimizing the empirical objective with RKHS-norm penalty: maximize empirical log-loss minus $(\lambda_0/m)\|g\|_{L_2(d\tau)}^2.","For 2D Gaussian KL estimation (30 random initializations), the RKHS discriminator with complexity control reduced absolute error from 0.5→0.04, 5.8→1.07, and 60.6→9.7 for true KL values 1.3, 13.8, and 38.29, respectively; variance dropped from 0.2→0.002, 223→4.4, and 3521→33. In mutual information estimation on 20D Gaussians (MI stepped from 2 to 10), the method outperformed CPC and NWJ and was competitive with SMILE when using a small regularization (e.g., $\lambda=10^{-5}$), while larger $\lambda$ reduced variance but increased bias. In Adversarial Variational Bayes on MNIST, the standard neural discriminator exhibited KL blow-up after ~500 epochs and degraded reconstructions, while the RKHS discriminator with norm penalty stabilized training and maintained sharp reconstructions over many epochs.","The authors note the RKHS construction requires two separate networks (\u03c8/\u03d5 and g), making the model bulkier and increasing parameter count; they also note the discriminator output is currently scalar, and extending to multivariate outputs could further increase complexity. They additionally cite the requirement that higher-order derivatives of the kernel exist (Assumption A3); while satisfied for smooth activations, for ReLU/LeakyReLU derivatives fail at the origin and may require careful use of subgradients to define the operator norm $\|L_p\|$.","Theoretical bounds rely on several strong conditions (compact domains, Lipschitz constraints enforced via spectral normalization, smooth-kernel derivative assumptions) and may not reflect typical unconstrained deep learning practice; it is unclear how sensitive empirical gains are when these assumptions are relaxed. The approach introduces extra compute (sampling random features w and evaluating the g-network); wall-clock overhead is nontrivial (e.g., 74s vs 245s for a toy KL experiment; 11.3h vs 14.7h for 1000-epoch MNIST AVB), which may limit scalability for larger models. Empirical evaluation is limited to a few benchmark tasks and does not extensively compare against alternative stabilization techniques for MINE/VDM/AVB (e.g., gradient penalties, clipping strategies, architectural tricks) under matched compute budgets.","The authors explicitly suggest generalizing the RKHS discriminator from scalar output to multivariable output, noting it may increase parameters. They also indicate the need to investigate using subgradients to define the Sobolev operator norm when activations like ReLU/LeakyReLU violate the higher-order derivative requirement at the origin.","A natural extension is to develop self-tuning or adaptive schemes for selecting the regularization strength $\lambda$ to better manage the observed bias–variance tradeoff across tasks. It would also be valuable to extend the theory and method to settings with dependent/streaming data and to common deep-learning practices (non-compact domains, weaker smoothness), and to evaluate robustness under heavy-tailed or high-dimensional regimes. Finally, releasing a maintained reference implementation (e.g., a PyTorch package) and benchmarking against a broader set of stabilized MI/KL estimators and GAN regularizers under fixed compute would clarify practical adoption.",2109.14688v1,https://arxiv.org/pdf/2109.14688v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:27:42Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/AOlmin/robustness_and_reliability_in_weak_supervision,"This paper analyzes how label noise affects both predictive accuracy and the reliability (calibration) of probabilistic classifiers trained with different loss functions. Under an input-dependent “simple non-uniform label noise” model, it derives the conditional distribution over noisy labels \(\tilde g(x)=P(\tilde Y\mid X=x)\) and shows it preserves the clean decision boundaries (accuracy) but increases entropy and is not calibrated with respect to clean labels. It then studies robust (noise-insensitive) loss functions, proving robust losses are not strictly proper and that the commonly used symmetric robust losses are not calibration-based strictly proper, so they do not guarantee calibrated uncertainty. Empirically, using MNIST with symmetric label noise, it shows that training with robust losses (e.g., MAE) can still overfit depending on initialization, indicating that asymptotic robustness theory does not address practical overfitting. Overall, it argues that robustness evaluations should include uncertainty/calibration metrics, not just accuracy.","The noisy conditional label distribution is derived as \(\tilde g(x)=\sum_{k=1}^K P(\tilde Y\mid Y=k,X=x)g_k(x)\), and for simple non-uniform noise it simplifies to \(\tilde g(x)=\left(1-\frac{\omega(x)K}{K-1}\right)g(x)+\frac{\omega(x)}{K-1}\,\mathbf{1}_K\). Calibration is defined by \(P(Y\mid f(X))=f(X)\) a.s. For binary symmetric losses, the pointwise risk minimiser takes the thresholded form \(f_1^*(x)=\gamma\,\mathbb{I}[P(Y=1\mid X=x)\ge 1/2]+\gamma'\,\mathbb{I}[P(Y=1\mid X=x)<1/2]\), illustrating that robust/symmetric losses need not recover \(g(x)\) or be calibrated.","Theoretical results: (i) \(\tilde g\) and \(g\) share decision boundaries under symmetric noise and the more general simple non-uniform (input-dependent) noise model; (ii) average conditional entropy increases, \(\mathbb{E}_X[H(\tilde g(X))] > \mathbb{E}_X[H(g(X))]\); (iii) \(\tilde g(X)\) is not calibrated w.r.t. the clean-label distribution; (iv) robust loss functions (as defined by shared minimisers under noisy risk) are never strictly proper; and symmetric robust losses are not calibration-based strictly proper. Empirical results include an example where test accuracy is similar with/without label noise (reported 0.984 vs 0.992) while predictive probabilities become more uncertain with noise. On MNIST with symmetric label noise (\(\omega\in\{0.0,0.3,0.5\}\)), MAE can appear resistant to overfitting under random initialization, but overfits when initialized from a CCE-trained model, showing overfitting can occur even with robust losses.","The authors note that their theoretical robustness analysis concerns asymptotic/global risk minimisers and therefore does not account for practical training dynamics such as local minima and overfitting. They also emphasize they analyze a simplified input-dependent noise model (simple non-uniform noise), intending it as a basis for understanding more complex noise. They explicitly state they do not elaborate on or characterize the details of the training dynamics or what it means for trajectories to pass “close” to the risk minimiser.","The empirical evaluation focuses on relatively simple network architectures (single hidden-layer MLPs) and limited datasets (toy circles, MNIST), so conclusions about deep modern architectures and large-scale datasets may not fully generalize. The paper argues about calibration/reliability but does not provide a broad benchmark across standard calibration metrics (e.g., ECE, Brier score) and post-hoc calibration methods (temperature scaling) under label noise. The noise models considered are restricted to class-conditional symmetric noise and a particular input-dependent extension; more structured or instance-dependent adversarial noise and class-dependent transition matrices are not systematically explored. Comparisons are largely between CCE and MAE (and theoretical classes), omitting many practical noisy-label training methods (co-teaching, loss correction with estimated transition matrices, semi-supervised approaches) in the empirical section.","They suggest developing new noise-robust algorithms (in particular, loss functions) that preserve accuracy while also ensuring reliability, specifically aiming for calibration-based strictly properness without requiring knowledge/estimation of the noise distribution. They also call for further investigation into how label noise affects model reliability and training dynamics, and for including uncertainty quantification as a standard part of evaluating noisy-label robustness.","A natural extension is to quantify calibration degradation under noise using standard metrics (ECE/MCE/Brier/NLL) across datasets and architectures, and to test whether post-hoc calibration (temperature scaling, isotonic regression) can recover reliability when trained on noisy labels. Another direction is to study settings with unknown parameters and realistic training pipelines (data augmentation, pretraining, label smoothing) and to characterize when early stopping helps calibration versus harms it. It would also be valuable to extend the theoretical analysis to class-dependent noise (general transition matrices) and to structured instance-dependent noise beyond the simple non-uniform model, and to explore Bayesian or ensemble methods for robustness and uncertainty under label noise.",2110.03321v2,https://arxiv.org/pdf/2110.03321v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:28:22Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,2110.04137v2,https://arxiv.org/pdf/2110.04137v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:28:22Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Other",Degradation measurements|Simulated only,Not applicable,Theoretical/simulation only,Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops optimal time plans for constant-stress accelerated degradation tests (ADT) when degradation paths follow a linear mixed effects model with unit-to-unit random effects (primarily associated with time). The design criterion is based on minimizing the asymptotic variance of an estimator of a quantile (especially the median) of the soft-failure time distribution under normal use conditions, where failure is defined by the degradation mean path crossing a threshold. Using the block-diagonal Fisher information and a product-structure for regressors in stress and time, the authors show factorization results that simplify optimal design; for the median, the time-plan optimization separates from stress settings. They derive that, for repeated-measures ADT with a common time plan across units, the optimal time plan for estimating the median corresponds to c-optimal extrapolation in the marginal fixed-effects time model (depending on the measurement-error covariance but not on the random-effects covariance). For destructive testing (one measurement per unit), they show the optimal joint design can be formed as a product of marginal c-optimal designs in stress and (variance-weighted) time, and they provide numerical examples and sensitivity analyses comparing optimal vs. standard uniform time plans.","Degradation model: $Y_{ij}=f(x_i,t_j)^T\beta_i+\varepsilon_{ij}$, with mixed-effects specialization $Y_{ij}=(f_1(x_i)\otimes f_2(t_j))^T\beta+f_2(t_j)^T\gamma_i+\varepsilon_{ij}$. Soft-failure time under normal use is $T=\min\{t\ge 0: \mu_u(t)\ge y_0\}$ with $F_T(t)=\Phi(h(t))$, $h(t)=(\mu(t)-y_0)/\sigma_u(t)$ and $\sigma_u^2(t)=f_2(t)^T\Sigma_\gamma f_2(t)$. The design objective is $\text{aVar}(\hat t_\alpha)=c^T M_\theta^{-1}c$; for the median ($\alpha=0.5$), it reduces to minimizing $f(x_u,t_{0.5})^T M_\beta^{-1} f(x_u,t_{0.5})$, and under product structure this factorizes into $f_1(x_u)^T M_1^{-1}f_1(x_u)\cdot f_2(t_{0.5})^T M_2^{-1}f_2(t_{0.5})$. A key identity used is $M^{-1}=(F^T\Sigma_\varepsilon^{-1}F)^{-1}+\Sigma_\gamma$ for $M=F^T V^{-1}F$ with $V=F\Sigma_\gamma F^T+\Sigma_\varepsilon$.","For repeated-measures ADT with a common time plan, the authors prove (Proposition 5.1) that a time plan that is c-optimal for extrapolating the mean at $t_{0.5}$ in the marginal fixed-effects time model is also optimal for minimizing the asymptotic variance of the estimated median failure time in the mixed-effects ADT model. In Example 1 (linear in stress and time with interaction, $k=6$ measurements on a grid $\Delta t=0.05$ and nominal $t_{0.5}=1.583$), the constrained locally c-optimal approximate time plan concentrates measurements near the endpoints; an adjusted exact plan at times {0.00, 0.05, 0.10, 0.90, 0.95, 1.00} achieves local c-efficiency 98.70% relative to the optimal approximate plan. For destructive testing ($k=1$ per unit), they show (Theorem 6.1) the optimal design for estimating $t_{0.5}$ is the product of marginal c-optimal designs in stress and a variance-weighted time regression; for the same nominal values, the optimal weights reported are $w^*=0.05$ for stress endpoint and $\pi^*=0.77$ for time endpoint $t=1$, yielding joint design weights (0,0):0.22, (0,1):0.73, (1,0):0.01, (1,1):0.04. Sensitivity plots indicate the optimal design is more sensitive to misspecification of the variance ratio $\sigma(1)/\sigma(0)$ than to $t_{0.5}$, and uniform multi-point time designs perform substantially worse than endpoint-focused designs in their setting.",None stated.,"The design results are largely local, depending on nominal parameter values (notably through $t_{0.5}$ and, in destructive testing, through variance parameters via $\sigma(t)$), so robustness outside the explored ranges may be limited. The modeling assumes a linear mixed-effects structure with normal random effects and (typically) normal measurement errors; departures such as nonlinearity in degradation, non-Gaussian errors, or time-dependent autocorrelation beyond the random effects could change optimal time allocations. Practical constraints such as minimum spacing between measurements, instrument limits, and censoring/termination rules are only partially addressed via grid/weight constraints, and broader operational constraints may alter the recommended endpoint-heavy plans.",None stated.,"Extend the optimal time-plan results to nonlinear degradation paths and more general stochastic-process degradation models (e.g., Wiener/Gamma processes) where extrapolation to long-use quantiles is common. Develop robust or Bayesian design strategies that explicitly account for uncertainty in nominal parameters (especially $t_{0.5}$ and variance components) rather than relying on local optimality plus sensitivity plots. Incorporate more realistic within-unit error structures (e.g., autocorrelated measurement noise tied to time gaps) and evaluate how optimal plans change under spacing and cost constraints, including adaptive/sequential measurement schedules.",2110.06114v1,https://arxiv.org/pdf/2110.06114v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:29:06Z
FALSE,NA,ML-based|Other,Degradation measurements|Sensor/condition monitoring|Other,Not applicable,Healthcare/medical|Pharmaceutical|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://www.github.com/eternagame/KaggleOpenVaccine,"This paper reports results from the Stanford OpenVaccine Kaggle competition, using deep learning to predict single-nucleotide RNA chemical mapping and degradation profiles (in-line hydrolysis) from RNA sequence and secondary-structure features. The dataset comprises 6,043 diverse RNA constructs with nucleotide-resolution measurements collected under multiple accelerated degradation conditions and SHAPE structure-probing, with a deliberately blind second-round test set collected during the competition to reduce overfitting. Top solutions combined sequence models (CNN/RNN/attention) with graph/2D representations derived from base-pair probability matrices and employed data augmentation and pseudo-labeling; the best models substantially outperformed prior baseline models (e.g., DegScore). The winning models generalized to predicting overall degradation rates for much longer full-length mRNAs (504–1588 nt) measured by PERSIST-seq, achieving higher correlation than prior methods. The work positions dual crowdsourcing (Eterna for sequence design + Kaggle for modeling) as a rapid pipeline for improving predictive models relevant to mRNA therapeutic stability design.","Overall mRNA degradation rate is modeled as the sum of linkage-specific rates: $k_{\mathrm{deg}}^{\mathrm{overall}}=\sum_{i=1}^{N-1} k_{\mathrm{deg}}^{(i)}$, with half-life computed as $t_{1/2}=\ln 2\,/\,k_{\mathrm{deg}}^{\mathrm{overall}}$. Kaggle scoring uses mean column RMSE (MCRMSE) averaged across the three scored data types: $\mathrm{MCRMSE}=\frac{1}{N_t}\sum_{t=1}^{N_t}\sqrt{\frac{1}{n}\sum_{j=1}^{n}(y_{t,j}-\hat y_{t,j})^2}$. The DegScore baseline is a windowed linear regression over local sequence identity and loop-type indicators around position $k$, with learned coefficients and an intercept.","Kaggle models improved over the DegScore baseline by ~37% (public test) and ~25% (private test) in MCRMSE; DegScore public/private MCRMSE 0.39219/0.47297 vs. top models ~0.2276/0.3420. A commonly shared kernel achieved 0.24860 (public) and 0.36106 (private). Simple ensembling modestly improved private MCRMSE to ~0.3379 (top-2 average) vs. best single-model ~0.3420. On independent full-length mRNA PERSIST-seq data (188 mRNAs), the Kaggle 2nd-place model achieved Spearman $R=0.48$ (p=3.3e-12) and the 1st-place model $R=0.43$ (p=9.5e-10), outperforming ViennaRNA unpaired probabilities ($R=0.25$) and DegScore ($R=0.36$).","The authors note that prediction is limited by both the amount of labeled experimental data available and the accuracy of the secondary-structure prediction tools used to generate input features. They also highlight that the degradation data used are from unmodified nucleotides, whereas many vaccines use modified nucleotides (e.g., pseudouridine), implying limited direct applicability without additional data/model adaptation. They caution that Kaggle competitions can overfit public leaderboards (though they report minimal shake-up here), indicating sensitivity to evaluation design and data leakage risks.","The work evaluates and compares multiple competition models but provides limited standardized ablation studies across architectures/feature sets, making it hard to attribute gains to specific design choices beyond anecdotal team reports. The primary metric (MCRMSE over specific assay conditions and construct designs) may not fully reflect downstream therapeutic stability in real formulations (e.g., different buffers, lipid nanoparticle environments, manufacturing/storage stresses). Generalization to full-length mRNAs is assessed via correlation after summing predicted nucleotide rates, but calibration/absolute error of predicted half-life (not just rank correlation) is not deeply validated.","The authors suggest collecting datasets and developing predictive models for RNAs containing modified nucleotides (e.g., pseudouridine or N1-methyl-pseudouridine) and exploring heuristics or adapted thermodynamics to enable prediction under such chemistries. They propose training on larger and more diverse chemical mapping datasets and integrating these model architectures into RNA structure prediction/inference frameworks. They also suggest applying the models to identify natural RNAs that may have evolved hydrolysis resistance, potentially inspiring new design strategies.","Develop self-starting/uncertainty-aware models that output calibrated predictive intervals for nucleotide degradation and for summed mRNA half-life, enabling risk-aware design optimization. Validate models prospectively in formulation-relevant conditions (e.g., LNP-encapsulated RNA, varied ionic strength/excipients, temperature cycling) and quantify transfer-learning needs between assay domains. Provide a unified, maintained software package with reproducible training pipelines, benchmark splits, and standardized baselines/ablations to make future comparisons more rigorous and to reduce competition-specific idiosyncrasies.",2110.07531v2,https://arxiv.org/pdf/2110.07531v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:29:49Z
FALSE,Other,"ML-based|Parametric (Weibull, etc.)|Other",Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,None / Not applicable,Not provided,http://creativecommons.org/licenses/by-nc-nd/4.0/|https://www.who.int/en/news-room/fact-sheets/detail/the-top-10-causes-of-death,"The paper proposes a three-step clinical risk prediction method that aims to combine interpretability (via simple dichotomized decision rules) with predictive performance (via machine learning) and adds an individualized estimate of prediction reliability. Rules are created per risk factor using class centroids and a mid-point threshold; for each rule, a probabilistic classifier (logistic regression or a small neural network) predicts the probability that the rule is correct for a given patient (“rule acceptance”). A patient’s mortality score is computed by aggregating rule outputs weighted by predicted rule acceptance and is then calibrated to a mortality risk using logistic regression calibration; an individual reliability estimate is defined as the absolute difference between mean acceptance of rules predicting death vs survival. The approach is evaluated on 1111 acute coronary syndrome patients for 30-day all-cause mortality using 1000-run Monte Carlo cross-validation with kNN imputation and undersampling for rule-acceptance training. Performance is comparable to standard logistic regression and better than GRACE and a standard ANN, while offering rule-level interpretability and a reliability signal that correlates with misclassification rate.","Rules are defined via a normalized distance to class centroids: $\text{nd}=1-\frac{d_+}{d_+ + d_-}$, predicting death if $\text{nd}\ge 0.5$ (Eq. 1). The mortality score aggregates rule outputs weighted by predicted rule-acceptance probabilities: $t=\frac{1}{r}\sum_{i=1}^r (\text{rule output})_i\,(\widehat{\text{accept}})_i$ (Eq. 2), then linearly rescales to $[0,1]$ (Eq. 3) and is mapped to risk by logistic calibration $\Pr(\text{death})=\frac{\exp(\beta_0+\beta_1\,\text{score})}{1+\exp(\beta_0+\beta_1\,\text{score})}$ (Eq. 4). Individual prediction reliability is defined as $\left|\overline{\widehat{\text{accept}}}_{\text{positive rules}}-\overline{\widehat{\text{accept}}}_{\text{negative rules}}\right|$ (Eq. 5).","On 30-day mortality prediction (1111 patients; death rate 4.95%) with 1000-run Monte Carlo CV, the proposed method achieved test AUC 80.9–81.7% and test GM 73.5–74.5%, with PPV 16.1–17.2% and NPV 98.5–98.6% (95% CIs). Standard logistic regression achieved higher test AUC (82.5–83.3%) but similar test GM (72.1–72.9%); the proposed method strongly outperformed GRACE on GM (~47.4–47.5%) and outperformed a standard ANN on both AUC (77.8–79.0% for ANN vs ~81% proposed) and GM (69.9–70.9% for ANN vs ~74% proposed). Calibration slope for the proposed method was ~0.964 (intercept ~0.002), closer to ideal than ANN (slope ~0.802) and much better than GRACE (slope ~2.689). Predicted reliability estimates showed a strong association with misclassification rate (chi-squared test p<0.0001), with near-zero errors for reliability ≥90%.","The authors note the lack of external validation and that results are based on a relatively small Portuguese cohort (N=1111) compared with large international datasets used to validate established ACS scores. They also cite endpoint limitations (all-cause mortality rather than cardiac-specific mortality) and the restriction to measurements within the first 24 hours from admission, not accounting for downstream treatments (e.g., reperfusion/medication effects). They further suggest that larger, more heterogeneous datasets are needed to validate and refine feature selection, rule creation, and reliability estimation.","Although the paper uses the term “reliability,” it is not reliability engineering; the proposed ‘reliability estimate’ is a heuristic confidence proxy rather than a calibrated probability of correctness with formal uncertainty quantification (e.g., conformal prediction, Bayesian credible intervals), and it is not compared against such baselines. The approach relies on dichotomizing predictors using centroid midpoints, which can be unstable under distribution shift, sensitive to skew/outliers, and may discard predictive information; robustness to site effects and temporal drift is not examined. Evaluation is limited to internal Monte Carlo CV on pooled cohorts; there is no true external holdout by hospital or time, and the undersampling used for rule-acceptance models could affect probability calibration of acceptance outputs. No implementation details (software, seeds, preprocessing pipeline) are provided, limiting reproducibility.","The authors propose validating the approach on larger, more heterogeneous datasets to confirm performance and improve the end-to-end process (feature selection, rule creation, risk prediction, and reliability estimation). They suggest optimizing which subset of features is used to train each rule-acceptance predictor (rather than using all features for every rule), analogous to multivariate selection/stepwise optimization. They also note the method performed similarly for other horizons (14 days, 6 months, 1 year) and suggest applying and validating it beyond the ACS scenario in other clinical prediction problems.","A valuable extension would be to benchmark the proposed individual reliability estimate against established uncertainty/reliability methods (e.g., conformal prediction, prediction intervals, Bayesian neural nets, temperature scaling and calibration of correctness probabilities) and evaluate selective prediction/abstention performance. External validation designed around domain shift (train on one hospital, test on another; temporal validation) would better quantify generalizability and stability of rule thresholds. Developing a self-contained, reproducible software package (with fixed preprocessing, imputation, and calibration) and reporting decision-curve analysis or clinical utility metrics would improve deployability. Robust/monotone rule construction (e.g., quantile-based, isotonic constraints) and handling correlated predictors more explicitly could improve both interpretability and stability.",2110.08331v1,https://arxiv.org/pdf/2110.08331v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:30:38Z
TRUE,Degradation modeling|RUL prediction|Life distribution modeling|Other,"Stochastic process|Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Other",Right-censored|Sensor/condition monitoring|Mixture of types|Other,Not applicable,Transportation/logistics|Energy/utilities|Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,R|Other,Not provided,https://cran.r-project.org/web/packages/nloptr/index.html,"The paper proposes a method to construct a univariate, monotone degradation index from multivariate sensor streams, enabling downstream degradation/reliability modeling when no natural single degradation metric exists. The approach uses a cumulative exposure (damage) model where sensor effects enter through an additive nonlinear structure represented by nonnegative M-spline bases, and a monotone transformation/integration guarantees an increasing degradation index. Time-to-failure (including right-censoring) is linked to the degradation index via a random soft-failure threshold modeled with a largest extreme value (Gumbel/LEV) distribution; its asymmetry is exploited to reduce false negatives. Parameter estimation is performed by maximizing a LEV-based objective with adaptive group LASSO to select informative sensors (grouped spline coefficients), with tuning chosen by cross-validation emphasizing false-negative rate. Simulations and an application to NASA turbofan/jet-engine sensor data show improved classification of failed vs. censored units and better exclusion of non-informative signals compared with linear-effect and no-selection baselines and a prior health-index method.","The degradation index is defined by a cumulative exposure integral $u(t)=\int_0^t h\left(\sum_{j=1}^p f_j[x_j(s);\beta_j]\right)ds$ with $h(z)=\log(1+e^z)/\log 2$ and sensor-specific nonlinear effects expanded as nonnegative M-splines $f_j[x_j(t);\beta_j]=\sum_{k=1}^m \beta_{jk}\,\gamma_{jk}[x_j(t)]$. Soft failure is modeled by a random threshold $U=u(T)$ where $\log U\sim \text{LEV}(\log\alpha,\sigma)$, yielding $G_T(t)=\Phi_{LEV}((\log u(t)-\log\alpha)/\sigma)$ and density $g_T(t)=u'(t)\{\sigma u(t)\}^{-1}\phi_{LEV}(\cdot)$. Estimation minimizes a penalized objective $-\ell(\beta)+\lambda\sum_{j=1}^p\omega_j\|\beta_j\|_2$ (adaptive group LASSO) plus an additional constraint-encouraging penalty forcing failed units toward $u(t_i)\approx \alpha$ and censored units to stay below $\alpha$.","In simulations (10 signals with 5 truly effective; 200 repetitions across $n=50$ to 300 and four coefficient scenarios), the proposed nonlinear index with variable selection (DI-VS) achieves the smallest total error rate (TER) in most settings, with particularly large false-negative reductions versus the no-selection model (DI-NVS) when $n<100$ and improved robustness versus the linear-effect model (DI-VSL) under nonlinear scenarios. Variable-selection results show DI-VS excludes more no-effect signals than DI-VSL across scenarios while retaining most effective signals as $n$ increases, though exclusion becomes harder as $n$ grows. On the NASA jet-engine dataset (200 units; 100 failures/100 censored; 50 random 80/20 splits, practical threshold $\hat\alpha_{0.01}$), DI-VS attains mean FNR=0.030, FPR=0.026, TER=0.057 versus DI-KSL (0.070, 0.034, 0.104), DI-NVS (0.040, 0.040, 0.080), and DI-VSL (0.000, 0.645, 0.645). The authors also report that DI-VS tended to keep all 16 sensors for the full dataset, while smaller subsamples showed some sensors (e.g., NRf, altitude, Nf) being excluded with higher probability.","The authors note that as the number of units increases, the method becomes “harder to exclude no-effect sensors,” because small per-unit likelihood contributions can accumulate over large $n$ and prevent coefficients from being shrunk to zero. They also acknowledge the current model uses an additive structure for sensor effects, which may be overly simplistic when real degradation depends on more complex interactions among sensors.","The degradation index is learned using a LEV-threshold objective with fixed $\alpha$ and a scale parameter that is intentionally driven toward a lower bound for optimization, so the probabilistic failure-time model is not fully identifiable/calibrated as a generative reliability model (it is primarily a classification-oriented loss). Performance is evaluated mainly on status classification (failed vs. censored) with a “practical threshold,” rather than on standard prognostics metrics (e.g., RUL error, calibration of predicted failure-time distributions), limiting direct reliability decision support. The approach assumes conditional independence structure implicit in the likelihood and does not address serial correlation/temporal dependence in sensor streams beyond the integral construction, which can affect inference and selection.","The authors suggest extending the additive sensor-impact model to allow interactions among sensors to better reflect complex degradation mechanisms. They also propose improving variable selection so the method can retain useful variables while more reliably excluding no-effect sensors as $n$ grows, e.g., by considering an adaptive group elastic net–type penalty or scaling penalties with the number of units. They note that other components of the framework could be altered for different applications, including choices of sensor-effect forms $f_j(\cdot)$, transformation $h(\cdot)$, and the distribution used in the objective function depending on whether FNR, FPR, or TER is prioritized.","Develop a fully probabilistic/identifiable joint model that estimates $\alpha$ and the threshold distribution parameters jointly (or via hierarchical/Bayesian constraints) so the learned index supports calibrated failure-time prediction, not only classification. Extend the method to explicitly handle autocorrelated/high-frequency sensor trajectories (e.g., functional random effects, state-space models) and to multivariate failure modes with competing risks or multiple thresholds. Provide open-source implementation and benchmarking across additional public PHM datasets with standardized prognostics metrics (RMSE of RUL, scoring rules, calibration) to validate generalizability and practical deployment.",2110.08882v1,https://arxiv.org/pdf/2110.08882v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:31:28Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling|Other,"Stochastic process|Bayesian|Parametric (Weibull, etc.)|Simulation-based|Other",Right-censored|Complete lifetime data|Simulated only|Mixture of types|Other,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper assesses long-term lumber reliability under duration-of-load (DOL) effects using a multimodel Bayesian framework. It compares three established DOL/degradation models—the Gerhards “US” cumulative damage model, the Foschi–Barrett “Canadian” model with specimen-level random effects, and a Gamma-process degradation model—and combines them using Bayesian model averaging (BMA) to account for both parameter uncertainty and model uncertainty. Residential, snow, and wind stochastic load profiles are generated via Monte Carlo simulation (NBCC-based load combination with probabilistic models for sustained/extraordinary occupancy load, segmented winter snow modeled with Gumbel maxima plus roof transformation, and annual extreme wind modeled via Gumbel velocity and lognormal aerodynamic factor). Model parameters are calibrated to an accelerated-testing Hemlock dataset (ramp and constant-load tests with censoring) using Bayesian inference (MCMC and ABC), and 50-year failure probabilities are estimated by simulating many load profiles for posterior parameter draws. Results are summarized as performance-factor vs reliability-index (\(\phi\)–\(\beta\)) curves, with BMA producing posterior mean curves and 95% credible bands that generally cover the individual-model estimates and are dominated by the Canadian model due to higher posterior model probability.","Damage/failure modeling is defined via a nondecreasing damage process \(\alpha(t)\) with failure at \(\alpha(T)=1\). The US model uses \(\frac{d\alpha(t)}{dt}=\exp\{-A + B\,\tau(t)/\tau_s\}\) with lognormal short-term strength \(\tau_s\); the Canadian model uses \(\frac{d\alpha(t)}{dt}=[(a\tau_s)(\tau(t)/\tau_s-\sigma_0)_+]^b+[(c\tau_s)(\tau(t)/\tau_s-\sigma_0)_+]^n\alpha(t)\) with lognormal random effects; and the Gamma-process model assumes gamma increments with shape \(\eta(t)\) driven by load history (eq. (3)). A stochastic load profile is simulated as \(\tau(t)=\phi R_o\,\frac{\gamma\tilde D_d+\tilde D_l(t)}{\gamma\alpha_d+\alpha_l}\) (eq. (4)), and failure probability is estimated by Monte Carlo over load profiles and posterior draws (eqs. (12)–(13)). Multimodel combining uses BMA: \(\hat p_F=\sum_k \Pr(\Delta=1\mid M_k,y)\,p(M_k\mid y)\) with approximate posterior model probabilities from BIC weights (eq. (16)). Reliability index is \(\beta=-\Phi^{-1}(p_F)\).","Using BIC-based model probabilities with equal priors, the posterior model probabilities are reported as US: 0.00, Canadian: 0.88, and Gamma process: 0.12 (BICs −5898, −6188, −6184, respectively). Across all four load scenarios (residential, snow in Vancouver, snow in Quebec City, wind in Halifax), the Canadian model yields the highest (most optimistic) reliability indices \(\beta\), while the Gamma-process model yields noticeably lower \(\beta\). The BMA \(\phi\)–\(\beta\) curves lie closer to the Canadian model due to its dominant posterior weight, and the BMA 95% credible bands contain the individual-model estimates. Scenario comparisons show higher reliability for Vancouver snow than Quebec City snow, and lower reliability for sustained snow loading than short-duration wind events with similar peak loads, consistent with DOL effects.",None stated.,"The BMA model probabilities use a BIC approximation rather than exact marginal likelihoods, which can be sensitive to sample size and prior choices and may be inaccurate for complex/latent-variable models (notably the Canadian model fitted via ABC). Failure probability estimation relies on nested Monte Carlo (posterior draws × 10^5 load profiles) and discretization choices for load histories, which can introduce nontrivial numerical error without reported convergence/variance diagnostics. The approach assumes the stochastic load models (e.g., independence across segments/years, parametric Gumbel/lognormal forms, fixed winter length) are correctly specified; misspecification could materially affect 50-year reliability. The work focuses on a single Hemlock dataset and does not provide out-of-sample validation across species/products or alternative load standards, limiting generalizability.",None stated.,"Compute model weights using more exact Bayesian evidence estimates (e.g., bridge sampling/thermodynamic integration) or ABC model choice methods tailored to intractable likelihoods, and assess sensitivity to model priors. Add robustness studies for load-model assumptions (autocorrelation, climate nonstationarity, alternative return-period models) and for dependence between dead/live components. Provide variance-reduction and convergence diagnostics for the nested Monte Carlo estimator of \(p_F\) and \(\beta\), and explore surrogate/emulator approaches to reduce computational cost. Extend the framework to member/system reliability (multiple components, load sharing) and to other wood products/species with additional real-data case studies and open-source implementation.",2110.11896v1,https://arxiv.org/pdf/2110.11896v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:32:11Z
TRUE,Life distribution modeling|Degradation modeling|Failure mode analysis|Accelerated testing|Software reliability|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Stochastic process|Bayesian|Simulation-based|Other",Right-censored|Degradation measurements|Event/count data|Sensor/condition monitoring|Mixture of types|Other,Not applicable,Transportation/logistics|Manufacturing (general)|Healthcare/medical|Finance/economics|Network/cybersecurity|Other,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper is a statistical perspectives/review article on reliability of artificial intelligence (AI) systems, emphasizing reliability as time-dimensioned performance (failure-free operation over an intended period in a specified environment). It proposes the SMART framework for AI reliability research: Structure, Metrics, Analysis of failure causes, Reliability assessment, and Test planning. The authors connect AI reliability problems to traditional reliability and software reliability methods, reviewing time-to-event models (including censoring), degradation models, and recurrent-event/NHPP models and illustrating their use with autonomous-vehicle disengagement event data (mileage-adjusted intensities). It also surveys emerging AI-specific reliability challenges—out-of-distribution detection, training data effects, adversarial attacks, accuracy, and uncertainty quantification—and explains how these can be incorporated into reliability modeling via event processes and covariate-driven intensities. Finally, it discusses test planning for AI reliability (including road testing and simulation/track testing) and suggests adapting accelerated testing concepts to AI via use-rate acceleration and “input/environment stress” (noise, adversarial attacks, OOD scenarios, error injection).","The paper reviews standard reliability likelihoods and process models rather than proposing a single new chart/statistic. Key formulas include: (i) the parametric lifetime likelihood for censored time-to-event data, $L(\theta)=\prod_{i=1}^n f(t_i;\theta)^{\delta_i}[1-F(t_i;\theta)]^{1-\delta_i}$ with log-location-scale families (Weibull/lognormal); (ii) degradation general path models $y_{ij}=D(t_{ij};\alpha,\gamma_i)+\epsilon_{ij}$ with random effects and induced $F_T(t)=\Pr[D(t)\ge D_f]$; (iii) recurrent-event NHPP likelihood $L(\theta)=\prod_i\big(\prod_j \lambda(t_{ij};\theta)\big)\exp[-\Lambda(\tau_i;\theta)]$ with power-law intensity; and (iv) a proposed AI reliability event-intensity decomposition $\lambda[t;x(t),z]=\sum_{j=1}^k \lambda_j[t;x(t)]\,p_j(z;\beta_j)$ where $p_j$ is modeled via logistic regression.","Quantitative results are not a major focus because the paper is primarily a framework/review with illustrative examples. In the autonomous-vehicle illustration (from Min et al. 2020), the mileage-adjusted baseline intensity functions for Waymo, Cruise, and PonyAI decrease over time (interpreted as reliability improvement), while Zoox’s baseline intensity is approximately flat (interpreted as little improvement). The paper also cites summary statistics from the AI Incident Database: among 126 incidents (as of access date), 72 are characterized as reliability-related and 29 of those involve deaths or injuries. Other sections provide qualitative/illustrative evidence (e.g., OOD detection via Mahalanobis scores on MNIST; adversarial attack example causing misclassification) rather than benchmarking with ARL/coverage metrics.","The authors state they are not attempting an exhaustive literature review because the AI literature is vast and spans many areas. They explicitly note a key practical limitation: limited public availability of AI reliability data (often proprietary and sensitive), and that field test data collection is costly and time-consuming; they mention the California DMV AV testing database as an exception. They also note they do not cover other AI assurance dimensions (safety, trustworthiness, security), focusing instead on reliability.","Because the paper is largely conceptual and review-oriented, many ideas (e.g., the combined intensity model $\sum_j \lambda_j p_j$) are not operationalized with concrete estimation procedures, identifiability discussion, or validation on multiple real datasets. AI reliability is treated via classical assumptions (e.g., event processes like NHPP, independence/measurement models) without a systematic robustness analysis for dependence, nonstationarity, feedback loops, or partial observability common in deployed AI systems. The discussion of test planning and accelerated testing for AI remains high-level; it does not provide detailed designs, optimality criteria, or calibrated acceleration models linking stressed conditions (noise/OOD/attacks) to use conditions. There is limited guidance on standardizing reliability metrics across heterogeneous AI tasks, which may hinder comparability and adoption.","The authors outline multiple open statistical research challenges for AI reliability, including out-of-distribution detection, effects of training sets/data quality, adversarial attacks, model accuracy, and uncertainty quantification, and how these relate to reliability over time. They call for development of a general modeling framework for AI reliability and for improved data collection/test planning approaches (including adapting accelerated testing concepts). They also suggest building data repositories for AI reliability datasets and remark that Bayesian methods (widely used in reliability) are worth exploring further for AI reliability modeling.","Develop self-starting/online reliability assessment methods that update event-intensity and failure-probability models under continual learning, distribution shift, and concept drift, with guarantees on false-alarm and detection delay. Establish principled acceleration models that map synthetic stresses (adversarial perturbations, noise injection, OOD scenario generation) to field risk, including uncertainty bounds and scenario weighting aligned with operational design domains. Create standardized benchmark datasets and protocols for AI reliability (time-to-incident, recurrent events, degradation of accuracy) across domains, plus open-source reference implementations to encourage reproducibility. Extend the framework to system-of-systems reliability (software/hardware/network interactions) with hierarchical Bayesian models that fuse heterogeneous data sources (logs, telemetry, incident reports) and explicitly model reporting/selection bias.",2111.05391v1,https://arxiv.org/pdf/2111.05391v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:32:49Z
TRUE,Degradation modeling|Maintenance optimization|System reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Event/count data|Mixture of types,Condition-based|Imperfect maintenance|Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,Other,In text/Appendix,NA,"The paper proposes a framework to disentangle latent deterioration from observed failure data that are confounded by maintenance interventions, with the goal of supporting maintenance policy comparison for naval ship equipment. It models engine condition as a three-state continuous-time Markov chain (CTMC) with an age-dependent (time-inhomogeneous) deterioration transition rate matrix, and introduces a separate maintenance transition matrix representing imperfect annual maintenance effects. Missing and unevenly spaced annual failure-count observations are first imputed using a hierarchical Gaussian process to stabilize downstream estimation. Parameters for deterioration rates and maintenance transition probabilities are estimated in a Bayesian hidden Markov model (categorical observation model) implemented in Stan via HMC/MCMC, and validated via repeated train/test splits and simulation-based calibration (SBC). Results on Korean navy ship engine data show plausible deterioration patterns, high estimated maintenance “restoration” probabilities, good predictive agreement between predicted and observed discrete states, and mostly well-calibrated inference except for noted issues in two parameters.","Deterioration is modeled as a CTMC with intensity matrix $Q(t)$ (upper-triangular) parameterized by age-dependent rates $(\lambda_{t,1},\lambda_{t,2},\lambda_{t,3})$, and the deterioration transition matrix is $D(t)=\exp(Q(t))$ (via Kolmogorov equations / matrix exponential). Maintenance is modeled by a separate transition probability matrix $M$ with imperfect-maintenance parameters $p_{21}$ and $p_{31}$ (mapping worse states toward better states). The latent state-probability vector evolves as $d(t) = D(t)\{\prod_{i=1}^{t-1} M D(i)\} d(0)$ with $d(0)=(1,0,0)^\top$, and observations are modeled as $y_t\sim\text{Categorical}(d(t))$.","Estimated mean deterioration rates are reported as $\lambda_1=0.679$ (SD 0.161), $\lambda_2=0.274$ (SD 0.149), and $\lambda_3=0.649$ (SD 0.287), suggesting transitions 1→2 and 2→3 are more likely than direct 1→3. The maintenance-effect parameters are high: $p_{21}=0.787$ (SD 0.167) and $p_{31}=0.794$ (SD 0.167), indicating strong restoration toward better states after annual maintenance. Predictive validation over 1000 random 94:5 train/test splits yields similar average MSE for training (20.7) and test (20.9), supporting generalization. SBC diagnostics suggest good calibration for $p_{31}$, $\lambda_1$, and $\lambda_2$, but potential over-dispersion for $p_{21}$ and right-skew/bias for $\lambda_3$ (possible identifiability/heterogeneity issues).","The authors note that deterioration matrices $D(t)$ are currently learned in a no-pooling way by engine type, and suggest that partial pooling (hierarchical learning across engine types and/or time regions) could improve accuracy at the cost of additional parameters. They also state that incorporating prior knowledge about engine deterioration/operation patterns (i.e., informative priors) could meaningfully affect model performance and should be reflected when understood in the full Bayesian workflow. They further report SBC indications of inaccurate computation for $p_{21}$ and $\lambda_3$, suggesting potential issues with posterior dispersion/bias.","The observation model discretizes standardized annual failure counts into three quantile-based states, which may discard information and make results sensitive to binning/threshold choices; robustness to alternative discretizations is not demonstrated. Maintenance is assumed to occur once per year with a fixed, time-invariant maintenance matrix $M$, which may be unrealistic if maintenance quality/intensity varies by age, engine type, operational tempo, or policy changes over the decade. The deterioration CTMC is constrained to be forward-only (upper-triangular), disallowing natural recovery or measurement noise that could cause apparent state improvements absent maintenance. The model treats annual observations with a categorical emission, which may be a coarse representation for count data and may not capture overdispersion or ship-level heterogeneity beyond the imputation step.","The authors propose extending the model to hierarchical/partial-pooling learning of $D(t)$ across engine types (and potentially across the four age/time regions), which could increase flexibility and accuracy. They also suggest incorporating prior domain knowledge of engines via informative priors to improve Bayesian inference, prediction, and evaluation. They indicate that maintenance policy suggestion/comparison built on the inferred pure deterioration process is a main direction for further research.","Extend the framework to a joint model that uses the original count data directly (e.g., Poisson/negative-binomial HMM or state-space model) instead of discretizing into categorical states, to preserve information and model overdispersion explicitly. Allow maintenance effects to be covariate- and time-dependent (e.g., varying with age, engine type, or recorded maintenance actions) and compare alternative imperfect-maintenance structures, including partial restoration of rates rather than only state transitions. Address identifiability and calibration issues (notably for $\lambda_3$ and $p_{21}$) via reparameterization, stronger priors, or additional data/constraints, and report sensitivity analyses. Provide a full decision-analytic layer (e.g., optimize inspection/maintenance schedules using the inferred deterioration model with cost structure) and validate on additional fleets or components beyond engines.",2111.14368v2,https://arxiv.org/pdf/2111.14368v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:33:31Z
FALSE,NA,Nonparametric/Semi-parametric|Bayesian|Other,Event/count data|Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Public repository (GitHub/GitLab),http://github.com/OlivierBinette/MSETools,"This paper reviews and critiques multiple systems estimation (MSE, capture–recapture) as used to estimate the prevalence of modern slavery, focusing on the reliability/robustness of resulting population-size estimates under untestable identifying assumptions. It compiles and analyzes all publicly available modern-slavery MSE datasets (UK, New Orleans, Netherlands, Western U.S., Australia), comparing several commonly used estimators: SparseMSE (log-linear with stepwise selection + bootstrap), dga (Bayesian model averaging over decomposable graphical models with hyper-Dirichlet priors), LCMCR (Bayesian latent class/Dirichlet-process mixture), and an independence baseline. To evaluate estimator accuracy when “ground truth” is unavailable, it applies an internal-consistency conditioning approach to create subproblems with known totals and reports bias/RMSE/coverage comparisons. The authors derive an asymptotic bias characterization for estimators that assume no highest-order (full-way) interaction in the log-linear model, showing how misspecification and individual heterogeneity can produce substantial bias. They propose a visual resampling diagnostic (“estimate trajectories” over sequentially accumulated observations) to assess robustness of estimates to small data perturbations and demonstrate strong sensitivity for some methods on real datasets.","The classical two-list Lincoln–Petersen estimator is $\hat N = n_1 n_2 / m$. For $L$ lists, the observed overlap cell counts are $n_x = \sum_{i=1}^N \mathbb{I}(W_i=x)$ for $x\in\{0,1\}^L\setminus\{0\}$, with $n_{\mathrm{obs}}=\sum_{x\neq 0} n_x$ and $N=n_{\mathrm{obs}}+n_0$. A key theoretical result gives the asymptotic relative bias for estimators consistent under the no-full-way-interaction assumption: $\lim_{N\to\infty}(\hat N-N)/N = p_0( e^{\gamma}-1)$, where $\gamma$ is the full-way log-linear interaction term.","Across 11 conditioned “ground-truth” subdatasets (internal consistency analysis), SparseMSE had the smallest mean and median log relative bias (mean −0.17; median −0.15) and near-nominal interval coverage (0.90). dga and the independence model were comparable on point accuracy (mean log relative bias −0.34 and −0.29 respectively), while LCMCR performed worse on these metrics (mean −0.52; coverage 0.60). Sensitivity analyses show SparseMSE results can change sharply with small changes to the stepwise p-value threshold, and dga estimates/intervals can vary materially with prior hyperparameters. LCMCR exhibited serious MCMC nonconvergence on the Netherlands dataset (e.g., $\hat R\approx 1.67$ and effective sample size \~340 out of 20,000 draws for $p_0$), indicating unstable posterior estimation without more advanced sampling strategies.","The authors note that the internal-consistency (conditioning) analysis uses only a small number of conditioned datasets that are not fully representative of the real modern-slavery setting. Conditioned datasets are typically small and have limited overlap information, and conditioning on a large list can remove features of the truly unobserved population (e.g., heterogeneity and certain list interactions), limiting generalizability. They also state that LCMCR’s Gibbs sampler may fail to converge for larger datasets due to multimodal/non-identifiable latent class posteriors, requiring more sophisticated MCMC approaches.","Although the paper centers on “reliability” of prevalence estimation, it is not reliability engineering (no failure-time/degradation/maintenance modeling); conclusions may not transfer to engineering reliability contexts. The empirical comparisons depend on a small set of publicly available datasets and specific implementations/defaults of each method (e.g., bootstrap settings, priors, chain counts), so performance rankings may change under alternative, equally defensible configurations. The proposed trajectory visualization is descriptive and does not provide calibrated decision thresholds or formal robustness metrics, which could limit operational use when a binary accept/reject criterion is needed.","They call for more methodological work on identifying assumptions of the form relating $p_0$ to the observed zero-truncated distribution, including assessing suitability in real applications and proposing meaningful alternatives. They also suggest developing methods that better account for noise and uncertainty sources such as record linkage errors and other data-quality issues. For LCMCR specifically, they indicate that implementing more advanced approaches for multimodal posteriors (e.g., parallel tempering/other strategies) would be necessary for stable application to larger datasets.","Develop formal, quantitative robustness measures based on the trajectory concept (e.g., stability indices, change-point diagnostics, or sensitivity-to-single-observation influence functions) and link them to actionable reporting guidelines. Extend analyses to explicitly incorporate linkage/duplication uncertainty via joint Bayesian record linkage + MSE, and benchmark against alternative robust capture–recapture estimators (e.g., penalized likelihood, Bayesian nonparametrics with identifiability constraints). Create standardized simulation benchmarks calibrated to modern-slavery list structures (sparsity, nonoverlap, referral mechanisms) to enable fair method comparisons under controlled misspecification.",2112.01594v1,https://arxiv.org/pdf/2112.01594v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:34:10Z
FALSE,Other,ML-based,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/sebnieh/QA_for_WhiteMatterLesionSegmentation,"The paper proposes an evaluation framework for automatic quality control (QC) methods to make CNN-based medical image segmentation outputs more dependable for clinical workflows. It evaluates two QC strategies for white matter hyperintensity (WMH) segmentation from MRI: (1) aggregating uncertainty/error maps into a single voxel-wise sum score, and (2) predicting segmentation quality by regressing the Dice coefficient using a 3D CNN. The framework introduces an “MRI filter” that thresholds a predefined quality criterion (e.g., Dice < 0.75) to convert QC outputs into pass/fail decisions, enabling operational metrics like Precision/Recall, detected failed cases, and post-filtering mean Dice. Using 5-fold cross-validation on 105 patients from multiple public datasets, the approach is reported to reduce manual review needs by 84.5% (only ~15.5% of segmentations flagged for manual validation). Overall, Dice prediction—especially using an error–prediction input pair—showed the most practical failure-detection performance within the proposed evaluation framework.","The voxel-wise sum (VS) aggregation score is defined as the sum of voxel values over an uncertainty (or error) map: $\mathrm{VS} = \sum_v H(v)$, where $H$ denotes the uncertainty map (computed via predictive-entropy from MC-dropout samples) and $v$ indexes voxels (Eq. 1 in the paper). The framework then applies a fixed threshold to the QC score or predicted Dice (e.g., classify as failed if Dice < 0.75) to produce Boolean pass/fail labels for computing Precision and Recall.","Uncertainty-map VS correlated moderately with true Dice (mean Pearson r = 0.52, P < 0.05), while error-map VS showed no significant correlation (R ≈ -0.25, P > 0.1). Filtering by uncertainty VS (threshold ~1100) increased mean Dice from 0.84 to 0.85, detecting ~3.2 failed cases on average with Precision 0.93 and Recall 0.89. Dice regression with uncertainty–prediction inputs achieved the best Dice-estimation accuracy (MAE 0.0486; r 0.71, P < 0.0001), but the best failure-detection outcome was reported for Dice regression using error–prediction inputs (Dice after filtering up to median ~0.87; median Precision 0.941; median Recall 0.833; ~4.8 failed cases identified). The authors estimate the framework can reduce manual evaluation workload by 84.5% (only ~15.5% of segmentation maps require manual validation).","The authors note that while the evaluation framework generalizes, adapting it to other segmentation tasks may require changing the QC methodology and input choices (e.g., different uncertainty measures or aggregation scores) due to task-specific variability (such as WMH size/location heterogeneity). They also state that the success/failure threshold used to filter segmentations must be adjusted for the intended use case. They further indicate that additional validation on other imaging modalities and segmentation problems (e.g., cardiac MRI or liver CT) is needed.","The proposed operational thresholds (e.g., Dice < 0.75; VS cutoffs like 1100) appear tuned to this dataset/task and may not transfer across scanners, institutions, or prevalence of pathology without a formal calibration procedure. The evaluation is based on cross-validation within pooled public datasets; external prospective validation in a real clinical workflow (with real decision/triage impact, latency, and human-in-the-loop behavior) is not demonstrated. The framework treats “failed” segmentation as a single class based on Dice, which may not align with clinically relevant failure modes (e.g., small but critical lesion misses) and can be sensitive to label noise (the paper itself notes artifacts in ground truth). Finally, computational burden of MC dropout (20 forward passes per case) and reconstruction-based error estimation may be nontrivial in production settings, but runtime/throughput constraints are not quantified.","The authors explicitly propose further validation of the QC methods and framework on different imaging data types and segmentation tasks (e.g., cardiac MRI, liver CT). They also imply that different uncertainty measures/aggregation scores and different decision thresholds should be explored and adapted for specific use cases given task variability.","A useful extension would be a principled threshold-calibration strategy (e.g., target maximum false-negative rate for failures, decision-theoretic optimization, or conformal prediction-style guarantees) to improve transportability across sites. Robustness studies under domain shift (new scanners/protocols) and label-noise stress tests could clarify reliability in deployment. A self-starting or unsupervised QC calibration method that does not require ground-truth Dice in new environments would improve practical adoption. Finally, packaging the framework as a reproducible tool with standardized reporting (including inference time and compute costs) and validating prospectively with clinician feedback would strengthen clinical translation.",2112.03277v2,https://arxiv.org/pdf/2112.03277v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:34:45Z
FALSE,NA,ML-based|Simulation-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|None / Not applicable|Other,Not provided,http://www.niss.org/sites/default/files/tr181.pdf|https://www.whitehouse.gov/omb/information-regulatoryaffairs/statistical-programs-standards/|https://www.researchgate.net/publication/316829634_Data_Quality_and_Fitness_for_Purpose,"The paper proposes a degradation-based paradigm to assess the quality of genome sequence data by intentionally injecting errors (via iterative Mason_variator simulations) and measuring how quickly key genome characteristics deteriorate. The core hypothesis is that higher-quality genomes are more “fragile,” showing larger changes under the same amount of degradation, and that more complex features (e.g., triplets/quartets, repeats, palindromes) degrade faster than simpler ones (e.g., base frequencies). Methodologically, the authors quantify degradation using distances between n-gram (base/pair/triplet/quartet) distributions and by tracking movement toward a universal low-quality endpoint defined by maximum entropy of the triplet distribution. They apply hierarchical clustering to (i) 64-dimensional standardized triplet distributions and (ii) 3-dimensional quadratic summaries of triplet-entropy-vs-iteration curves, demonstrating detection of deliberately inserted low-quality genomes and additional anomalous database entries. The study is primarily about data quality/anomaly detection in genomic databases rather than engineering reliability, though it borrows the general notion of “degradation” as a diagnostic perturbation mechanism.","The n-gram (triplet) distribution is defined as $P_3(b_1b_2b_3\mid G)=\Pr\{G(k:k+2)=b_1b_2b_3\}$ for random $k\in\{1,\dots,|G|-2\}$. A corresponding second-order Markov transition representation is given by $T_3(b_1,b_2,b_3\mid G)=\Pr(G(k+2)=b_3\mid G(k)=b_1,G(k+1)=b_2)$. Degradation toward a universal endpoint is measured using entropy $H(P)=-\sum_{s\in S} p(s)\log p(s)$, with maximal entropy attained at the uniform distribution (value $\log|S|$).","Under Mason_variator degradation, distances between tuple distributions and the original genome increase with diminishing marginal impact (concave degradation curves), and higher-order tuples (quartets > triplets > pairs > bases) show larger/faster divergence (demonstrated via Hellinger distance plots). For triplet-entropy degradation, each genome’s entropy curve over iterations (0, 250, 500, 1000, 2000) is well-approximated by a quadratic: the minimum reported $R^2$ is 0.941 and 99% of fits have $R^2>0.976$. Hierarchical clustering on standardized quadratic coefficients yields 34 clusters; cluster labels explain 98.96509%, 99.02426%, and 98.45677% of variation in the three coefficients. In the coronavirus database experiment (26,953 genomes plus 11 injected problem cases), the adenovirus genome and 10 intentionally degraded coronavirus genomes are detected as clear outliers; in the entropy-based clustering they are grouped together in a distinct cluster (cluster 34).","The authors note that the paradigm does not yet produce quantified uncertainties for the decisions it may support, limiting the ability to apply tools such as Specified Certainty Classification. They also concede that outlier detection implicitly assumes most of the database is high quality, an assumption that may be untested in practice. They highlight that origin choice is problematic for edit-distance based degradation measures (e.g., Levenshtein), since “parent” genomes are not known in real databases, motivating their entropy-to-endpoint approach.","The approach depends heavily on the chosen degradation generator (Mason_variator) and its parameterization; different error models or rates could change fragility rankings and outlier behavior, and the paper does not fully explore sensitivity to these choices. Much of the evaluation is based on one large coronavirus snapshot plus injected outliers, so generalization to other repositories, sequencing technologies, and contamination/error modes may be incomplete. The entropy endpoint assumes that “worst quality” corresponds to maximal randomness in triplet frequencies, which may not align with real low-quality artifacts (e.g., systematic biases, adapter contamination, low-complexity regions). Practical deployment may be computationally burdensome at scale because it requires repeated simulation/degradation per genome and repeated feature extraction/clustering.","They propose developing methods that attach quantified uncertainty to degradation-based decisions so that certainty-aware tools (e.g., Specified Certainty Classification) can be used. They suggest studying how controlled, quantifiable quality degradation impacts downstream bioinformatics pipelines now that lower-quality data can be generated deliberately. They also point to investigating the relationship between data quality and adversarial attacks (database “pollution”) and extending the paradigm to non-genomics contexts where credible degradation mechanisms exist (e.g., total survey error microsimulation in official statistics).","A valuable extension would be systematic robustness/sensitivity analysis over degradation types (SNPs vs indels vs structural variants), rates, and iteration schedules to assess stability of outlier detection and fragility measures. Developing self-calibrating or likelihood-based models that map observed degradation trajectories to interpretable latent quality scores (with confidence intervals) could improve comparability across datasets and labs. Incorporating autocorrelation/sequence-context effects and known biological constraints (e.g., codon usage, GC bias) into the endpoint and feature design may better distinguish genuine biological anomalies from technical artifacts. Providing an open-source, reproducible pipeline (scripts + parameter presets) and benchmarking across diverse taxa and sequencing platforms would strengthen external validity and adoption.",2112.13111v1,https://arxiv.org/pdf/2112.13111v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:35:36Z
FALSE,NA,Other,Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/wjmaddox/benchmarking_iterative_gps,"This paper investigates when scalable “iterative” Gaussian process (GP) regression methods (based on preconditioned conjugate gradients and Lanczos decompositions as implemented in GPyTorch/LOVE) give reliably accurate training and predictive uncertainty estimates. The authors diagnose poor test negative log-likelihood (NLL) and hyperparameter-learning instabilities as largely caused by overly relaxed default numerical settings, especially CG stopping tolerance and too-small test-time root/Lanczos decomposition rank used to approximate predictive variances. Through empirical benchmarks on UCI datasets (elevators, protein, bike, poletele) comparing against Cholesky baselines, they show that lowering CG tolerance and greatly increasing the root decomposition size can substantially close the NLL gap while RMSE often converges much faster. They provide concrete recommended settings (e.g., training CG tolerance around 1e-3, test CG tolerance around 1e-2, preconditioner rank 50, and predictive decomposition rank k≥5000) and report that L-BFGS-B can be effective for marginal likelihood optimization when CG accuracy is sufficiently high. The work advances practical guidance for numerically stable and well-calibrated iterative GP inference rather than proposing a new reliability-engineering method.","GP regression posterior mean/covariance use solves with $\hat K_{X,X}=K_{X,X}+\sigma^2 I$: $\mu(X_*)=K_{X_*,X}\hat K^{-1}_{X,X}y$ and $\Sigma(X_*)=K_{X_*,X_*}-K_{X_*,X}\hat K^{-1}_{X,X}K_{X,X_*}$. Iterative GPs approximate $\hat K^{-1}_{X,X}y$ and related trace terms via (preconditioned) conjugate gradients with tolerance $\epsilon$ and compute predictive variances using a low-rank Lanczos/root decomposition $\hat K^{-1}_{X,X}\approx RR^\top$, giving per-test-point variance $\Sigma(x_*)=K_{x_*,x_*}-K_{x_*,X}RR^\top K_{X,x_*}$. The paper also analyzes test NLL as $\mathrm{NLL}(\sigma^2)=\tfrac12\log\sigma^2+\tfrac{(\mu-y)^2}{2\sigma^2}$ and notes increasing Lanczos rank decreases posterior variance.","A simple numerical prescription resolves much of the observed “good RMSE but poor NLL” behavior in iterative GPs: use small CG tolerances (authors recommend training $\epsilon\approx10^{-3}$; prediction $\epsilon\approx10^{-2}$; and generally $\epsilon\le 0.01$) and large test-time root/Lanczos decomposition rank (recommend $k\ge 5000$, with NLL on elevators approaching Cholesky around root decomposition size \~10,000 under tighter tolerances). They report that RMSE typically matches the Cholesky baseline at comparatively loose tolerances (about 0.1 or less) and is less sensitive than NLL to decomposition rank, whereas NLL is highly sensitive to both tolerance and decomposition size. Recommended preconditioner rank is $w=50$ for stable training/testing. They further find L-BFGS-B becomes a practical and convergent hyperparameter optimizer for iterative GPs when CG accuracy is sufficiently tight, requiring fewer gradient updates than Adam (though with higher per-step cost).",None stated.,"The study is primarily empirical and scoped to GP regression with Gaussian likelihoods and standard kernels (e.g., Matrn-5/2 with ARD), so the prescriptions may not transfer unchanged to non-Gaussian likelihoods, classification, or more complex kernel/mean structures. The recommended fixed thresholds (e.g., $k\ge5000$) can be computationally heavy and may depend strongly on dataset size, conditioning, hardware precision, and implementation details, but no automatic/adaptive rule is provided. Comparisons focus on Cholesky vs. iterative methods within a particular software stack (GPyTorch/LOVE), leaving uncertainty about generality across other iterative GP implementations and preconditioners.",The authors state they hope to expand benchmarking to large-scale Bayesian optimization settings using iterative GPs.,"Develop adaptive schemes that set CG tolerance and Lanczos/root decomposition rank based on online error estimates or calibration targets (e.g., variance/nll diagnostics) to avoid using a universally large $k$. Extend the analysis to non-Gaussian likelihoods and to settings with data/model misspecification (heteroskedastic noise, outliers, autocorrelation) to test robustness of the numerical prescriptions. Provide standardized, reproducible benchmarks across multiple GP libraries and preconditioners, including cost-accuracy trade-off curves that account for GPU/precision effects and memory constraints.",2112.15246v1,https://arxiv.org/pdf/2112.15246v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:36:09Z
TRUE,Failure mode analysis|System reliability|Other,Stochastic process|ML-based|Bayesian|Hybrid/Ensemble|Simulation-based|Other,Simulated only|Other,Not applicable,Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable|Other,Not provided,https://mooseframework.inl.gov,"The paper estimates very small TRISO nuclear fuel failure probabilities by coupling active learning with multifidelity modeling and subset simulation to reduce expensive high-fidelity (HF) model evaluations. Failure is defined via a limit-state/indicator based on whether maximum principal stress in the IPyC or SiC layer exceeds the corresponding strength, and the overall TRISO failure probability is decomposed as Pf = P(SiC failure|IPyC failure)·P(IPyC failure). For 1D TRISO models (≈11 s per run), the authors compare three multifidelity strategies—(a) Kriging-only surrogate, (b) Kriging LF prediction + Kriging correction, and (c) DNN LF prediction + Kriging correction—finding that using an LF predictor plus Kriging correction reduces HF calls, with DNN+Kriging yielding the fewest HF calls. For 2D TRISO models (≈4 min per run on 10 processors), they compare a data-driven LF (DNN) versus a physics-based LF (1D TRISO) both with Kriging correction; the physics-based LF needs fewer HF calls but the DNN approach has lower total runtime due to near-instant LF predictions. The method introduces subset-dependent learning functions within subset simulation to avoid breakdown for very rare events and demonstrates capability to estimate probabilities down to ~1e−9 on a borehole benchmark.","Failure decomposition: $P_f=P(\text{SiC fail})=P(\text{SiC fail}\mid \text{IPyC fail})\,P(\text{IPyC fail})$ (Eq. 1). Limit-state indicator: $I=1$ if $\sigma_1(X^\#)-\Sigma\ge 0$ else $0$ (Eq. 5), where $\sigma_1$ is max principal stress and $\Sigma$ is layer strength. Multifidelity corrected prediction: $y^{C}_{LF}=y_{LF}+\varepsilon^*$ with correction learned by Kriging on residuals $\varepsilon^*\approx y_{HF}-y_{LF}$ (Eqs. 20–21) and subset-dependent learning function $U_i=\frac{|y_{LF}+\mu_{\varepsilon^*}(X)-F_i|}{\sigma_{\varepsilon^*}(X)}$ for intermediate subsets (Eq. 22) and $U_i=\frac{|y_{LF}+\mu_{\varepsilon^*}(X)|}{\sigma_{\varepsilon^*}(X)}$ for the final subset (Eq. 23).","For 1D TRISO models, estimating $P(\text{SiC}\mid\text{IPyC})$ with 20,000 samples (15,000 for Model 3) gives probabilities on the order of $10^{-4}$–$10^{-3}$; DNN+Kriging correction reduced HF calls (e.g., Model 1: 220 HF calls vs 296 for Kriging-only; Model 2: 197 vs 274; Model 4: 396 vs 541) while maintaining comparable reliability indices (~3.1–3.6). Overall 1D TRISO failure probabilities (combining IPyC via AK-MCS) are about $10^{-5}$–$10^{-4}$ for Models 1,2,4 and ~$10^{-3}$ for Model 3, with DNN+Kriging generally using the fewest HF calls (e.g., Model 1: 263 vs 339 for strategy (a)). For 2D TRISO Models 1–2, overall TRISO failure probabilities are about $1.6\times10^{-4}$ to $1.2\times10^{-4}$; physics-based LF (1D TRISO+Kriging correction) used fewer 2D HF calls (Model 1: 190 vs 212; Model 2: 150 vs 178) but had higher wall-clock time due to 1D LF runtime (~11 s each). The appendix demonstrates a rare-event benchmark with estimated $P_f\approx 8.45\times10^{-9}$ (COV 0.038) using 4211 true model calls.","The authors note that 2D TRISO results can be sensitive to finite-element mesh density, particularly near the IPyC crack where XFEM stress concentration is mesh-resolution dependent. They also state that 1D TRISO failure estimates depend on the accuracy of stress modification factors used to approximate 2D stress concentrations, which can lead to inconsistencies between 1D and 2D failure probabilities. They further mention that overall computational tradeoffs between LF choices depend on the HF/LF cost ratio, and that their reported runtime comparisons do not include the (small) overhead of retraining the Kriging correction model.","The method assumes the surrogate/correction uncertainty from Kriging is well-calibrated for active-learning decisions; miscalibration could bias rare-event probability estimates, especially in tail regions explored by subset simulation. The approach is demonstrated on specific TRISO model forms and distributions; robustness to model discrepancy, nonstationary residuals, and higher-dimensional/strongly correlated inputs is not fully assessed. Comparisons focus mainly on HF call counts/COV targets rather than rigorous error bounds or repeated-run variability, which is important for stochastic subset simulation with MCMC correlation. Practical reproducibility may be limited because implementation details (sampling settings, proposal tuning, hyperparameter optimization specifics) and code are not provided.","The authors propose exploring Bayesian model averaging strategies to incorporate hierarchies of low-fidelity models (e.g., DNN as cheapest LF, 1D TRISO as more accurate LF, and 2D TRISO as HF) where progressively more expensive models are called only when cheaper models are deemed inadequate by the learning function. They suggest this as a way to further increase computational gains for reliability estimation in general multifidelity settings.","Extending the approach to explicitly handle autocorrelation and non-ergodicity issues in MCMC within subset simulation (e.g., adaptive proposals, diagnostics, effective sample size adjustments) would strengthen reliability of COV estimates and uncertainty quantification. Developing robust/self-starting versions that account for unknown input distributions, parameter estimation uncertainty, or model-form uncertainty (e.g., Bayesian calibration of stress modification factors) could improve applicability to real fuel-fabrication data. Providing open-source implementations or a reproducible workflow (including Bison/MOOSE input decks and surrogate training scripts) would enable broader validation and benchmarking against alternative rare-event estimators. Investigating multi-output surrogates (jointly modeling IPyC and SiC limit states) and sensitivity/importance measures could improve efficiency and interpretability for engineering decision-making.",2201.02172v1,https://arxiv.org/pdf/2201.02172v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:36:54Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|R|Other,Public repository (GitHub/GitLab),https://github.com/ignavierng/local-astar,"The paper studies scalable and reliable causal discovery from observational data in linear Gaussian structural equation models, focusing on exact score-based Bayesian network structure learning under weaker assumptions than faithfulness. It proposes estimating a search super-structure using the support of the inverse covariance matrix (via graphical Lasso), showing that recovering a sound super-structure only requires single shielded-collider-faithfulness (SSCF), which is strictly weaker than common faithfulness variants. It then constrains A* exact search using this super-structure (A*-SS) and introduces Local A*, which performs exact search on overlapping local two-hop neighborhoods and iteratively fixes previously discovered edges/v-structures to reduce computation. Extensive simulations on Erdős–Rényi graphs compare GLasso vs MMPC for super-structure estimation and evaluate SHD/F1, showing improved scalability (up to hundreds of nodes) and strong accuracy relative to baselines like PC/FGES/MMHC/Triplet A*. Overall, the work advances causal discovery by improving exact search scalability while reducing reliance on faithfulness-type assumptions.","In the linear Gaussian SEM, the inverse covariance is $\Theta=\Sigma^{-1}=(I-B)\,\Omega^{-1}(I-B)^T$, and in the Gaussian case $\Theta_{ij}=0\iff X_i\perp\!\!\!\perp X_j\mid V\setminus\{X_i,X_j\}$. The paper uses the estimated support $\mathrm{supp}(\hat\Theta)$ (via graphical Lasso) as an undirected super-structure to restrict candidate parent sets in exact A* search. A key expansion is (Lemma 1): for $j\neq k$, $\Theta_{jk}=-\sigma_j^{-2}B_{kj}-\sigma_k^{-2}B_{jk}+\sum_{\ell\neq j,k}\sigma_{\ell}^{-2}B_{j\ell}B_{k\ell}$, relating nonzeros in $\Theta$ to adjacency/spousal relations in the DAG’s moral graph.","Theoretical results show (Theorem 2) that under Markov + SSCF, the graph induced by $\mathrm{supp}(\Theta)$ is a super-structure of the true DAG skeleton, and (Theorem 3) exact BIC-based search is asymptotically correct for the true MEC iff the SMR assumption holds. Simulation studies on Erdős–Rényi linear-Gaussian graphs report that GLasso yields near-perfect neighbor true-positive rates across degrees on 10-node graphs, while MMPC can have substantially lower TPR (e.g., around 0.7 at expected degree 5 with $n=300$ even with large $\alpha$), impacting downstream exact search. With the true $\mathrm{supp}(\Theta)$ provided, A*-SS and Local A* match each other closely in SHD and F1, supporting asymptotic correctness of Local A*. In scalability experiments (up to 300 nodes, $n=10000$), Local A* and MMHC scale to hundreds of nodes; A* scales only to ~20 nodes and A*-SS to ~30 nodes in their setup, while Local A* achieves better structural recovery than MMHC on large graphs.","The authors state that the procedure “works only in the linear Gaussian setting” in its current form, and suggest extending to non-Gaussian and discrete cases as future work. They also note that Local A* runtime depends on the maximum size of local clusters and can become very long when some clusters are large, sometimes exceeding a four-day cutoff in experiments. They acknowledge that exact computational complexity is hard to characterize due to A* heuristics and factors like sparse parent graphs and conflict heuristics, so they report runtime as a proxy.","The approach depends on accurate inverse-covariance support recovery (GLasso) and on hyperparameter choices (e.g., $\ell_1$ penalty, thresholding), but the paper largely uses fixed heuristics; performance and super-structure soundness can degrade under model misspecification, heavy-tailed noise, latent confounding, or strong autocorrelation. While the theory focuses on asymptotics, finite-sample guarantees for super-structure recovery (especially after ad-hoc thresholding for >40 nodes) are not established, and error propagation from super-structure mistakes into exact/local search is not fully quantified. Local A* merges local MEC information into a global output; formal guarantees for correctness of this merge procedure (and for the edge-fixing/orientation step avoiding cycles/v-structures) are not fully developed beyond empirical validation. Comparisons emphasize certain baselines; additional modern continuous optimization methods (e.g., NOTEARS variants) or robust/score-equivalent alternatives could further contextualize practical tradeoffs.","They propose extending the method beyond the linear Gaussian setting to non-Gaussian and discrete cases. They also mention that alternative efficient inverse covariance estimators such as QUIC could be adopted for support estimation and treat this as future work. Additionally, they note that one could run more of the local searches in parallel depending on available computational resources (an implementation-oriented direction).","Developing self-tuning or theoretically grounded model selection for GLasso/threshold parameters (e.g., stability selection) would improve robustness and reproducibility, especially in high dimensions. Extending the super-structure theory and algorithms to settings with latent confounding (e.g., ancestral graphs) and to non-i.i.d./time-series data would broaden applicability. Providing finite-sample error bounds linking inverse-covariance support recovery quality to downstream MEC recovery (for A*-SS/Local A*) would strengthen reliability claims. Implementing and benchmarking scalable, optimized solvers (e.g., QUIC, GPU-accelerated covariance estimation, or distributed local-cluster scheduling) could further improve runtime for large dense graphs.",2201.05666v1,https://arxiv.org/pdf/2201.05666v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:37:34Z
TRUE,RUL prediction|System reliability|Other,ML-based|Simulation-based|Other,Sensor/condition monitoring|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,Python|Other,Not provided,NA,"This preprint assesses DeepONet (an operator-learning neural network) as a surrogate modeling approach for time-dependent reliability analysis and uncertainty quantification of stochastic linear and nonlinear dynamical systems under random forcing. The reliability task is framed as first-passage failure time (FPFT): for each simulated displacement trajectory, the first time a response crosses a prescribed threshold is recorded, and the resulting failure-time PDF is estimated to characterize failure probability over time. DeepONet maps an input forcing function (branch network) and a time coordinate (trunk network) to the system response at that time, enabling rapid generation of large displacement ensembles after training on a comparatively small set of high-fidelity simulations. The method is evaluated via multiple numerical case studies (SDOF Bouc–Wen, 5-DOF Duffing, and 76-DOF ASCE benchmark building in linear and nonlinear variants), using error metrics (MSE/NMSE) and comparisons of FPFT PDFs to ground-truth simulations. The authors report good accuracy, computational efficiency, and “zero-shot” generalization to forcing distributions different from those used in training (e.g., different Fourier-term complexity or altered Gaussian-process parameters).","The stochastic dynamical system is modeled as $M\ddot{X}+C\dot{X}+KX+N(X,\dot{X},\theta_N)=F$, where randomness enters through the forcing $F\sim P(\cdot)$. DeepONet represents the operator mapping forcing histories to responses via $\mathcal{G}(f)(t)=\langle \text{Branch}(f),\text{Trunk}(t)\rangle$, trained with an MSE loss $\varepsilon=\frac{1}{N_p}\sum_{i=1}^{N_p}(y_a-y_p)^2$ on simulated response targets. Reliability is computed through FPFT by identifying, for each response trajectory, the first time index at which the response exceeds a threshold and then estimating the PDF of these failure times from the ensemble.","For the SDOF Bouc–Wen case with Fourier-series forcing, reported displacement MSEs decrease with more training samples; e.g., with 150 training samples (100 points-per-sample), test MSE is about $9.5\times10^{-9}$ for the in-distribution 20-term forcing and remains small under distribution shift (e.g., $1.48\times10^{-7}$ when testing with 100 Fourier terms). For the Gaussian-process forcing variant (trained at $\sigma=50,\ell=0.10$ with 1000 samples, 50 points-per-sample), MSEs across tested $(\sigma,\ell)$ combinations remain on the order of $10^{-7}$, indicating zero-shot generalization. For the 76-DOF benchmark building, normalized MSE is reported to be within ~2% for testing at 20–25 Fourier terms and within ~5% for most DOFs when testing at 50 Fourier terms. FPFT PDF curves computed from DeepONet-generated response ensembles closely match those from ground-truth simulations in the multi-DOF case studies, supporting use for time-dependent reliability estimation.","The paper notes that time must be spent training the surrogate model, although it argues this is offset by large time savings during the prediction stage when many Monte Carlo samples (e.g., $10^5$ or more) are needed. It also notes architectural and preprocessing choices (e.g., number of hidden layers/nodes and normalization) are application-dependent and must be tuned to obtain good results. No other explicit limitations (e.g., assumptions on dynamics/noise or robustness bounds) are clearly stated in the provided text.","Reliability is demonstrated only through simulated case studies; there is no validation on measured experimental/field data, so practical performance under modeling error, sensor noise, or nonstationary loads is unclear. The FPFT computation depends on a fixed response threshold; sensitivity of reliability results to threshold choice and uncertainty in threshold (common in design-code contexts) is not analyzed. The approach implicitly assumes the training simulations sufficiently cover the forcing-function space of interest; extrapolation under more severe distribution shift or rare-event tail forcing (critical for small failure probabilities) is not rigorously studied. For high-dimensional systems (e.g., 76 DOF), training separate models per DOF may be computationally and operationally heavy, and cross-DOF dependence/consistency is not explicitly enforced.",None stated.,"Evaluate DeepONet-based FPFT estimation in rare-event regimes (very small failure probabilities) using variance-reduction or active learning to target tail regions of the forcing/response space. Extend the framework to handle model-form uncertainty and uncertain thresholds (e.g., Bayesian DeepONet or probabilistic calibration) so reliability estimates include surrogate uncertainty. Address correlated/multivariate outputs with a single multi-output operator model that enforces physical coupling across DOFs, and test robustness under non-Gaussian noise and temporally correlated forcing beyond the studied parameterizations. Provide and benchmark an open-source implementation with standardized datasets to enable reproducibility and fair comparisons against alternative surrogates (e.g., GP/Kriging, PINNs, Fourier neural operators) for time-dependent reliability.",2201.13145v1,https://arxiv.org/pdf/2201.13145v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:38:20Z
TRUE,Software reliability,Bayesian|Other,Event/count data|Other|Simulated only,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/soumenstat89/size_biased,"The paper proposes a Bayesian hierarchical model to estimate software reliability from discrete software testing “detection” data recorded by test case/phase, rather than time-between-failures. It introduces a latent “eventual bug size” for each bug and uses a size-biased detection mechanism where per-phase detection probability is an increasing function of bug size, allowing estimation of both the total number of bugs and the remaining total bug size after testing. The model uses data augmentation with an upper bound M and latent inclusion indicators to infer the (unknown) number of bugs, and defines software reliability at phase j as the posterior probability that remaining total bug size is below a threshold ϵ. Model fitting is performed via MCMC (Gibbs/slice/Metropolis-Hastings) implemented in NIMBLE/R, and performance is assessed via simulation using relative bias, CV, and credible-interval coverage. The approach is demonstrated on two empirical datasets (commercial testing data and ISRO mission software testing data, including a grouped-bugs extension), and used to estimate reliability trajectories and decision quantities such as stopping phase or required future test cases to meet a target reliability level.","Observation model: for bug i in phase j with T_j test inputs, detections follow $y_{ij}\sim\text{Binomial}(T_j, p_i z_i)$ where $z_i\in\{0,1\}$ indicates whether bug i exists (data augmentation). Size-biased detection link: $p_i = 1-(1-r)^{S_i}$ where $S_i$ is latent eventual bug size and r is per-input detection probability along a bug’s path. Remaining total bug size after phase j is $B_j=\sum_{i=1}^M S_i z_i(1-u_{ij})$, and software reliability is defined as $\gamma_j(\epsilon)=P(B_j\le \epsilon\mid Y)$ using the posterior predictive distribution.","In simulations with N=200 bugs over Q=5 phases, relative bias for N ranged roughly from −16% to 19% across scenarios, with CV for N about 4%–12%; relative bias for r ranged about −36% to 37% with CV about 11%–24%, and 95% credible-interval coverage exceeded 90% for both N and r in all scenarios. For reliability target 0.95 with threshold 100 in the simulation study, estimated stopping phases varied by scenario (e.g., 30 for the lowest-input/lowest-r case, down to 10 for the highest-input/highest-r case). In the commercial dataset, posterior mean total bugs was N=348 (95% CrI 317–382), r≈8.761×10⁻⁶ (95% CrI 7.261×10⁻⁶–1.044×10⁻⁵), and remaining bug size after 4 phases B₄≈703 (95% CrI 457–1006); achieving 0.95 reliability (threshold 100) was estimated to require continuation to phase 16 with 3000 test cases per phase (~36,000 additional test cases). In the ISRO grouped analysis, the number of bug groups was estimated as 84 (95% CrI 80–89), total bugs about 94 (95% CrI 94–95), r≈1.102×10⁻³ (95% CrI 6.439×10⁻⁴–1.807×10⁻³), and reliability after 8 phases with threshold ε=25 was reported around 0.995 (very high).","The authors note that the approach is “moderately robust” but recommend conducting a prior sensitivity study before applying the size-biased model, indicating sensitivity to prior choices may affect inference. For the ISRO dataset, they state it is not possible to extend the number of phases, so instead of estimating a stopping phase they estimate the number of future test cases needed to reduce remaining bug size below a threshold (a constraint on the decision-analysis component).","The model appears to assume conditional independence of bug detections given parameters and a common per-input detection parameter r, which may be unrealistic if test cases are correlated, testers adapt strategies over phases, or detection probabilities vary by bug class beyond size. The “bug size” concept (eventual number of lifetime inputs traversing a bug) is latent and may be weakly identifiable from sparse phase-wise binomial outcomes, especially when many bugs are never detected; results may depend on the chosen upper bound M and the Poisson–Gamma prior specification. The reliability definition depends on an externally chosen threshold ε (and assumed future T_j), so comparisons across projects are sensitive to these subjective choices and may not correspond to field failure rates post-release. Evaluation focuses on simulation and two case studies; robustness to misspecification of the detection link $p_i=1-(1-r)^{S_i}$, imperfect debugging, and changes in software versions over phases is not thoroughly benchmarked against alternative SRGMs or non-size-biased discrete-time models.","The authors state the model could be adopted to various real-life situations and suggest potential application to other domains such as hydrocarbon exploration. They also recommend conducting a prior sensitivity study before application, implying follow-on work in assessing sensitivity/robustness under alternative priors and assumptions.","Developing a self-starting/online variant that updates reliability and stopping decisions sequentially as new phases arrive (including adaptive choice of future T_j) would improve practical deployment. Extending the framework to allow time-varying or covariate-dependent detection (e.g., test severity/complexity, team effects) and to model imperfect debugging or bug reintroduction would align better with real software processes. Comparative studies against established discrete-time SRGMs, capture–recapture variants, and modern ML-based defect prediction on common benchmarks would strengthen evidence of advantage. Providing a calibrated mapping between the proposed “remaining bug size” threshold ε and operational metrics such as post-release failure intensity would make the reliability measure more actionable for engineering decisions.",2202.08107v3,https://arxiv.org/pdf/2202.08107v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:38:59Z
FALSE,NA,ML-based|Other,Other,Not applicable,Theoretical/simulation only,Other,TRUE,Other,Not provided,NA,"This position paper proposes CRISPs (Conditional Randomized Interactive Skeptical Probabilistic circuits), a class of conditional probabilistic circuits designed to enable reliable (exact) and efficient computation of uncertainty quantities needed in interactive learning with structured outputs. The authors analyze which circuit structural properties (e.g., smoothness, decomposability, determinism, and structured-decomposability/compatibility) are sufficient to make key inference operations tractable, yielding linear-time computation of Shannon entropy and margin for active learning, and tractable maximization and marginalization subroutines. They show that fine-grained active learning based on conditional Shannon entropy/margin over arbitrary label subsets is intractable in general unless the model fully factorizes, and propose using Rényi entropy (for integer α>1) as a tractable alternative computable in O(|p|^α). The paper also argues that skeptical learning suspiciousness scores based on likelihood margins are tractably computable under CRISP assumptions and that hard logical constraints can be integrated by compiling knowledge into compatible circuits. No empirical benchmark evaluation is presented; the paper primarily contributes tractability results and design guidance for constructing randomized circuit structures suitable for exact uncertainty computation.","Uncertainty for active learning is defined via Shannon entropy $H_\theta(x)=-\sum_{y\in\{0,1\}^c} p_\theta(y\mid x)\log p_\theta(y\mid x)$ and margin $M_\theta(x)=1-\max_y p_\theta(y\mid x)$, with querying when $U_\theta(x)\ge T$. Fine-grained subset selection is posed as optimizing expected uncertainty reduction over subsets $Q\subseteq Y$ (Eqs. 4–6). To enable tractable subset uncertainty, the paper proposes Rényi entropy $R^{\alpha}_\theta(Y\mid x)=\frac{1}{1-\alpha}\log\left(\sum_y p_\theta(y\mid x)^\alpha\right)$ (Eq. 11), which is tractable for CRISPs for integer $\alpha>1$ in $O(|p|^\alpha)$. Skeptical learning uses suspiciousness $S_\theta(x_t)=\max_y p_\theta(y\mid x_t)-p_\theta(\tilde y_t\mid x_t)$ (Eqs. 10/12).","The paper states that for CRISPs (circuits constructed to be smooth, decomposable, deterministic, and structured-decomposable), Shannon entropy over all labels and the predictive margin can be computed exactly in time linear in circuit size (Proposition 3). It argues that computing conditional Shannon entropy or conditional margin for all possible label subsets $Q\subset Y$ is intractable in general unless the distribution is fully factorized, due to the need for marginal determinism (Proposition 4). It shows that Rényi entropy for arbitrary subsets $Q$ is computable exactly for integer $\alpha>1$ in $O(|p|^\alpha)$, leveraging an $\alpha$-power circuit construction (Proposition 5), and notes that $\alpha=2$ would often suffice to control computational cost. It further claims skeptical-learning suspiciousness scores based on likelihood margins are tractable under the same CRISP properties. No ARL/benchmark-style quantitative performance numbers are reported, as the work is largely theoretical/position-oriented.","The authors note that fine-grained active learning based on conditional Shannon entropy or conditional margin is intractable in general for CRISPs unless one compromises expressiveness (e.g., by fully factorizing), because marginalizing variables can destroy determinism and thus tractability (Proposition 4 discussion). They also highlight that using approximation routines for marginal MAP/entropy can accumulate approximation errors across greedy search iterations, potentially yielding arbitrarily suboptimal queried subsets. They indicate that their proposed Rényi-entropy approach relies on integer $\alpha>1$ and that computational cost grows as $O(|p|^\alpha)$, recommending small values like $\alpha=2$ to keep costs under control.","The paper is a position/theory piece and does not empirically validate CRISPs on real active-learning or skeptical-learning benchmarks, so practical gains in label efficiency, calibration, and wall-clock latency remain unquantified. Claims of “reliable” uncertainty hinge on exact inference within the chosen circuit class; this does not guarantee calibrated predictive uncertainty under model misspecification or dataset shift, which are central in human-in-the-loop settings. The randomized circuit construction and added structural constraints may limit representational capacity compared with less constrained models, but there is no systematic capacity–tractability tradeoff analysis. Implementation details are insufficient to assess numerical stability and GPU efficiency for large structured output spaces, especially when materializing and reusing $\alpha$-power circuits.","The authors state they plan to evaluate CRISPs on real-world active learning benchmarks for deep learning and on structured-output prediction tasks such as hierarchical multi-label classification. They also imply exploring practical fine-grained active learning procedures (e.g., branch-and-bound using tensorized $\alpha$-power circuits) as part of that evaluation and application effort.","Empirical studies comparing Shannon-entropy approximations (sampling/bounds) versus exact Rényi-based querying would clarify when the Rényi surrogate improves subset selection and how sensitive performance is to the choice of $\alpha$. Extending CRISPs to handle correlated or adversarial annotator noise models explicitly (beyond likelihood-margin heuristics) could strengthen skeptical-learning guarantees in practice. Robustness analyses under covariate shift and partial-label missingness mechanisms would better align with “in the wild” deployment claims. Releasing a reference implementation (e.g., in PyTorch) with reproducible experiments and ablations on circuit size/region-graph templates would improve adoption and facilitate benchmarking.",2202.08566v1,https://arxiv.org/pdf/2202.08566v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:39:31Z
TRUE,RUL prediction|System reliability|Other,ML-based|Hybrid/Ensemble|Simulation-based|Other,Sensor/condition monitoring|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a Koopman-operator-based surrogate modeling framework for time-dependent structural reliability analysis of nonlinear/chaotic dynamical systems, targeting first-passage (first time to failure) behavior. It introduces two end-to-end deep learning architectures that learn Koopman observables (via an encoder), a linear Koopman evolution operator, and an inverse map (decoder), with variants handling uncertainty either in initial conditions or in system parameters. Time-dependent failure probability is evaluated through Monte Carlo simulation on the trained surrogate to estimate the PDF of first passage failure time, total failure probability, and reliability index. The approach is demonstrated on three numerical examples (Duffing oscillator, chaotic Lorenz system, and 2D Burgers’ equation) and is reported to match benchmark MCS PDFs closely while outperforming purely data-driven baselines (autoregressive FNN, LSTM, and CNN) especially under out-of-distribution test conditions. The method’s key claim is improved robustness/generalization by explicitly enforcing Koopman linear dynamics in latent space through a tailored multi-term loss.","Time-dependent failure probability is framed as $P_f(t)=P(g(\mathbf{Y},t)<\bar e_h(t))$ and first-passage failure as $P_f=P(g(\mathbf{Y},\tau)<\bar e_h(\tau),\ \tau\in[t_0,t_s])$ with the goal of estimating the PDF of $\tau$. The Koopman lifting and evolution are defined by $\mathcal{K}\psi(\mathbf{X}_k)=\psi\circ F(\mathbf{X}_k)=\psi(\mathbf{X}_{k+1})$ and the linear latent update $\psi(\mathbf{X}_{k+1})\approx K\psi(\mathbf{X}_k)$. Network training uses a weighted loss $L=\lambda_1\|\mathbf{X}-\psi^{-1}(\psi(\mathbf{X}))\|+\lambda_2\|\psi(\mathbf{X}_{k+1})-K\psi(\mathbf{X}_k)\|+\lambda_3\|\mathbf{X}_{k+1}-\psi^{-1}(K\psi(\mathbf{X}_k))\|$ (MSE norms). Reliability index is computed as $\beta=\Phi^{-1}(1-P_f)$.","For the Duffing oscillator with random initial conditions (threshold $\bar e_h=6$, first 40 time steps), MCS gives $\beta=1.764$, $P_f=0.0389$ (with $10^4$ samples), while the Koopman surrogate gives $\beta=1.810$, $P_f=0.0351$ using 1800 training series (reported error 2.67%). For Duffing with random parameters (threshold $\bar e_h=2.5$), MCS yields $\beta=0.540$, $P_f=0.295$ and Koopman yields $\beta=0.505$, $P_f=0.307$ (error 6.4%). For the Lorenz system with random initial conditions (thresholds $[20,20,25]$), MCS gives $\beta=0.966$, $P_f=0.170$ and Koopman gives $\beta=0.954$, $P_f=0.167$ (error 1.25%); with random parameters (thresholds $[10,10,17]$), MCS gives $\beta=0.831$, $P_f=0.203$ and Koopman gives $\beta=0.800$, $P_f=0.212$ (error 3.7%). Across examples, the paper reports the Koopman-constrained surrogate matches benchmark MCS first-passage-time PDFs closely and is more robust than autoregressive FNN/LSTM/CNN, particularly for out-of-distribution test distributions/ranges.",None stated.,"The work appears to rely on Monte Carlo simulation over the learned surrogate to estimate first-passage distributions; if the surrogate is biased in tail behavior, rare-event failure probabilities may be misestimated without variance reduction or dedicated rare-event methods. The evaluation is limited to numerical benchmarks with simulated data; there is no real experimental/field dataset validation, and practical guidance on selecting Koopman latent dimension, loss weights $(\lambda_1,\lambda_2,\lambda_3)$, and training data requirements is limited. The approach presumes availability of sufficient trajectory data from the true simulator and does not address computational cost of generating training data for high-fidelity engineering models or very high-dimensional PDE discretizations beyond the presented case.","The authors note that they treated uncertainty in initial conditions and in system parameters separately, and state the framework can be extended to the case where uncertainties in both initial conditions and parameters exist simultaneously.","A valuable extension would be integrating rare-event simulation (importance sampling/splitting) with the Koopman surrogate to better estimate low failure probabilities and tail FTTF behavior. Another direction is developing self-starting/online updating versions for streaming monitoring data, including uncertainty quantification of surrogate prediction (e.g., Bayesian deep Koopman or ensemble methods) to propagate epistemic uncertainty into reliability metrics. Further, testing robustness under measurement noise, model-form error, and autocorrelated/nonstationary excitations would improve applicability to real structural health monitoring settings.",2203.02658v2,https://arxiv.org/pdf/2203.02658v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:40:13Z
TRUE,Degradation modeling|Maintenance optimization|RUL prediction|Other,ML-based|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Other,Predictive|Condition-based|Not applicable,Pharmaceutical|Manufacturing (general),Simulation study|Case study (real dataset)|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/DovileDo/BearingDegradationStageDetection,"The paper addresses predictive maintenance in the pharmaceutical industry where maintenance windows are scheduled (due to regulatory auditing) and the decision problem is which machine components to replace, not when to maintain. It proposes a two-step framework for rolling-element bearings: (1) automatic lifetime segmentation into discrete degradation stages using k-means clustering on latent embeddings learned from high-frequency vibration FFTs by an AutoEncoder, with a residual-based heuristic to label the final failure stage; and (2) a supervised multi-input neural network classifier that combines frequency-domain learned features with handcrafted time-domain statistics to predict the current degradation stage. Experiments on the real FEMTO/PRONOSTIA bearing accelerated life test dataset (IEEE PHM 2012 challenge) show that AutoEncoder-based labeling aligns well with manual stage segmentation and yields a classifier that trains quickly and produces more stable, actionable stage predictions over time than a PCA-based labeling baseline. The framework is positioned as scalable for many bearings because labeling is automated per bearing and the classifier uses reduced representations rather than full high-dimensional spectra. The work advances practical condition monitoring by turning bearing health assessment into multi-class stage detection suitable for scheduled maintenance environments.","The AutoEncoder is trained per bearing to minimize mean absolute reconstruction error: $L_{\mathrm{MAE}}(\theta)=\mathbb{E}\|X-\hat{X}(X,\theta)\|_1=\frac{1}{T}\sum_{t=0}^T |X_t-\hat{X}_t|$. For labeling the final degradation stage, an observation is flagged as stage 3 when its reconstruction residual deviates by more than three standard deviations from the mean residual on the training set (a 3\,$\sigma$ rule). The classifier is trained with categorical cross-entropy: $L_{\mathrm{CE}}(\theta)=-\sum_{c=1}^C k_c\log(y_c)$, where $y=f(x[l],x[n])$ are predicted stage posterior probabilities from frequency- and time-domain inputs.","On the FEMTO bearing dataset, the proposed AutoEncoder+k-means labeling (AElabels) achieves high agreement with manual stage labeling across most bearings and stages, with the main difficulty concentrated in separating “healthy” vs early stage 1 (which the authors note can be subjective even manually). A classifier trained on AElabels converges faster and reaches higher training accuracy than the same classifier trained on PCA-derived labels (PCAlabels), and substantially outperforms a random-label sanity check (~25% accuracy for four classes). On the test set, AElabels-based predictions are more temporally stable (stages can be reconstructed from sequential predictions with less overlap), and fault detection timing based on first occurrence of stage 2 or 3 is generally more consistent across both short- and long-lifetime bearings than PCAlabels, which can be too early or too late in several cases (marked with asterisks in Table 3).","The authors state the method assumes availability of a training set obtained from “representative bearings,” and they highlight uncertainty about how broadly a learned degradation-stage model transfers to other bearings (scope of applicability). They also state that production deployment requires vibration signals to be obtainable “cheaply and reliably” from many bearings, and note open questions about whether measurements come from permanently embedded (regulator-accredited) sensors or temporary/manual measurements before scheduled maintenance.","The stage labels used to train the classifier are derived from the same unsupervised procedure (k-means on AE/PCA embeddings), so downstream classifier accuracy is largely measured against pseudo-ground truth rather than independent, physically verified degradation states; this risks propagating labeling bias. The residual-based stage-3 detection uses a fixed 3\,$\sigma$ threshold and an assumed “last 20%” holdout, which may not generalize when failure does not occur near the end, when operating regimes vary, or when noise/nonstationarity is present. The approach processes each bearing separately for labeling, which may limit practicality when you cannot run full run-to-failure tests for each bearing type/condition, and it does not explicitly address domain shift (different speeds/loads/sensor placements) beyond the FEMTO conditions. Because the dataset is from accelerated life testing and faults are unspecified/mixed, generalization to in-situ pharmaceutical equipment under normal loads and varying duty cycles remains uncertain.","The authors propose investigating whether zero-shot learning is possible for bearing degradation stage prediction to enable straightforward application to a wide range of different bearings in production. They also call for work to better characterize the scope of application/transferability of a given degradation-stage model when zero-shot learning is not feasible. Finally, they suggest studying practical deployment options for vibration acquisition at scale (permanently embedded accredited sensors vs. manually deployed sensors before scheduled maintenance).","Evaluate robustness to realistic industrial complications such as sensor drift, missing data, variable sampling rates, and strong autocorrelation/nonstationarity, and consider self-starting/online adaptation of the classifier and control thresholds. Add explicit domain adaptation across operating conditions (speed/load) and across machines/sites to reduce dependence on “representative bearings,” and benchmark against modern sequence models that use temporal context (e.g., TCN/transformers) rather than per-snapshot classification with smoothing. Replace or complement k-means stage segmentation with probabilistic state-space models (e.g., HMM/HSMM) or changepoint methods that can encode monotone stage progression and yield uncertainty estimates. Provide calibration and decision-theoretic evaluation tied to maintenance outcomes (cost/risk) to better connect stage predictions to actionable replacement policies in regulated environments.",2203.03259v1,https://arxiv.org/pdf/2203.03259v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:41:03Z
TRUE,Other,"Parametric (Weibull, etc.)|Other",Other,Not applicable,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This discussion note reviews and corrects technical errors in a previously published “extended FGM copula” proposed for dependence modeling with an application to reliability. The authors show that the admissible parameter range for the copula parameter $a$ reported by Ebaid et al. (2020) is incorrect (and contains typographical issues), and they derive the correct admissible range ensuring the function is a valid copula. The correction is obtained by rewriting the copula as a cubic-section copula $C(u,v)=uv+a\Phi(u)\Phi(v)$ and applying known validity conditions based on bounds of $\Phi'(u)$. They also comment on and rebut claims in a later correction paper (Barakat et al., 2021), including noting that comparing a copula to a non-copula distribution function is not meaningful and that the maximal achievable Spearman correlation can reach 0.375 for $b=1$. The contribution is primarily theoretical: it clarifies parameter constraints needed for valid dependence modeling in reliability applications using this copula family.","The extended FGM copula under discussion is $C(u,v)=uv\,[1+a(1-u)(1-v)(1-bu)(1-bv)]$ with $0\le b\le 2$. It is rewritten as a cubic-section copula $C(u,v)=uv+a\Phi(u)\Phi(v)$ where $\Phi(u)=u(1-u)(1-bu)$ and validity is enforced via derivative extrema $\alpha=\inf_u\Phi'(u)<0$ and $\beta=\sup_u\Phi'(u)>0$, yielding bounds on $a$ of the form $-1/\max\{\alpha^2,\beta^2\}\le a\le -1/(\alpha\beta)$. For this $\Phi$, the authors obtain $\beta=1$ and a piecewise expression for $\alpha$, leading to a corrected piecewise admissible upper bound for $a$ as a function of $b$.","The main result is the corrected admissible range for the copula parameter $a$ ensuring (1) defines a valid copula: $-1\le a\le \frac{1}{1-b}$ for $0\le b<1/2$, and $-1\le a\le \frac{3b}{b^2-b+1}$ for $1/2\le b\le 2$. The note provides a counterexample showing the previously stated bound can become nonsensical (e.g., for $b=1.1$ it would imply $a\le -10$). It also corrects/clarifies literature claims about dependence strength: for $b=1$ the copula reduces to $uv[1+a(1-u)^2(1-v)^2]$ and can attain maximal positive Spearman’s rho $\rho=0.375$, contradicting a later claim that $\rho<0.333$. No simulation or data-based performance study is reported; results are analytic validity corrections.",None stated.,"The note is a theoretical correction and does not revisit the original “application in reliability” aspects (e.g., impact on reliability measures, inference, or model fitting), so practical consequences of the corrected parameter bounds are not quantified. It does not provide estimation/inference guidance (e.g., how to fit $a,b$ under constraints, standard errors, or goodness-of-fit) for reliability datasets, which is often crucial in applied reliability modeling. The discussion focuses on bivariate copula validity only and does not address extensions to higher dimensions or robustness under model misspecification.",None stated.,"A natural extension is to study how the corrected admissible region for $(a,b)$ affects reliability functionals in dependent-component models (e.g., series/parallel system reliability, stress–strength reliability) and whether prior published numerical results change. Developing constrained estimation procedures (MLE/Bayesian) for $(a,b)$ with diagnostics and implementing them in reproducible software would improve practical usability in reliability applications. Additional work could examine tail dependence limitations of (extended) FGM-type copulas and compare against alternative copulas better suited for strong dependence or tail behavior often relevant in reliability contexts.",2203.11681v1,https://arxiv.org/pdf/2203.11681v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:41:25Z
FALSE,Other,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|Simulation-based|Other",Mixture of types|Other,Not applicable,Other,Other,TRUE,R|Python|None / Not applicable,Public repository (GitHub/GitLab),https://github.com/google-research-datasets/wordsim-replications,"This paper argues that when crowd annotations are aggregated (e.g., averaged over k raters), the reliability to report should be the reliability of the aggregated labels rather than the inter-rater reliability (IRR) of individual ratings. It proposes k-rater reliability (kRR), defined as chance-adjusted agreement between replications of aggregate ratings, and shows that kRR can be computed using standard IRR coefficients applied to aggregated labels. The authors validate kRR by conducting two full replications of the WordSim-353 dataset (13 ratings per item) and computing kRR for k=1..13, showing a large increase from single-rater reliability (~0.574) to 13-rater aggregate reliability (~0.940). For situations without replications, they present two estimation approaches: a bootstrap resampling method and an analytical intraclass correlation approach ICC(k) (and Spearman–Brown predictions) for mean aggregation on continuous scales. They recommend reporting both IRR (process reliability) and kRR (reliability of the aggregated dataset actually used).","kRR is operationalized by computing a standard inter-rater reliability coefficient (e.g., Krippendorff’s alpha, Cohen’s kappa) between two replications’ aggregated labels, where each item’s aggregate is formed from k raters. For mean aggregation on continuous data, they use ICC(k) (average-measures ICC) and relate it to the Spearman–Brown prophecy formula: $$\mathrm{ICC}(k)=\frac{k\,\mathrm{ICC}(1)}{1+(k-1)\,\mathrm{ICC}(1)}.$$ The appendix provides the one-way random effects ICC(k) computation via within-item and between-item sums of squares (SSW, SSB) to estimate variance components and then compute ICC(k).","On the two WordSim-353 replications, the single-rater reliability at k=1 (IRR) is reported as 0.574, while the 13-rater aggregate reliability at k=13 is 0.940 (Krippendorff’s alpha between replications’ mean scores). Using bootstrap on a replication yields a kRR of 0.943, closely matching the empirical 0.940. On the original WordSim dataset, analytical estimates give ICC(1)=0.590 and ICC(13)=0.950, and a bootstrap estimate gives 0.953 (based on 100 bootstrap samples). Spearman–Brown predictions (from as few as 2 ratings per item to estimate ICC(1)) closely track the empirical kRR curve up to k=13.","The empirical (replication-based) approach requires at least two replications, and perfect post-hoc replication may be difficult because original experimental conditions and rater populations change over time. ICC(k) is only applicable to continuous data and specifically to mean aggregation, so it cannot directly handle other aggregation functions (e.g., majority vote) or non-continuous label types. Bootstrap is approximate and is noted to work better with larger per-item sample sizes (they note 13 ratings per item is small for a typical bootstrap distribution, though mitigated by many items).","The empirical kRR depends on how “replications” are constructed (e.g., different worker pools, platform effects), so kRR may conflate annotation noise with population shift and protocol drift, limiting comparability across time. The paper focuses on point estimates of reliability; uncertainty quantification (e.g., confidence intervals for kRR/ICC(k) under realistic dependence) and sensitivity to rater non-independence are not fully developed. The results are demonstrated primarily on a single benchmark (WordSim-353) with continuous ratings; broader validation across task types (binary/multiclass, structured labeling) and aggregation rules is limited. Practical guidance for choosing k (cost–reliability tradeoffs) is mostly qualitative rather than an explicit optimization framework.","They ask how to derive multi-rater generalizations for reliability coefficients other than ICC, especially for common NLP settings with binary and multiclass labels and majority-vote aggregation. They also ask whether Landis & Koch-style interpretive cutoffs should be applied to kRR or whether kRR should have different standards.","Develop self-contained estimators of kRR for non-mean aggregations (median, trimmed mean, Dawid–Skene-style posteriors) and for categorical labels, with theoretical links to common IRR measures (kappa/alpha). Extend kRR to settings with rater-item missingness and unbalanced designs typical of crowdsourcing, including robust variance estimation and confidence intervals. Study the impact of rater dependence (discussion, collusion, shared biases) and non-stationary rater populations on kRR, potentially via hierarchical/Bayesian models. Provide tooling and reporting checklists (and reference implementations) that automatically compute IRR+kRR with uncertainty and cost-aware recommendations for selecting k.",2203.12913v1,https://arxiv.org/pdf/2203.12913v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:41:59Z
FALSE,NA,Other,Simulated only|Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Not applicable (No code used),https://github.com/jhuggins/viabel|https://github.com/stan-dev/posteriordb,"The paper proposes Robust and Automated Black-box VI (RABVI), a framework to make black-box variational inference optimization more reliable and less hand-tuned by automating learning-rate reductions and detecting inaccurate variational optima. It treats fixed-learning-rate stochastic optimization iterates as a Markov chain, uses split-$\widehat{R}$ and Monte Carlo standard error/effective sample size criteria to detect stationarity and decide how long to average iterates, and then adaptively decreases the step size. For termination, it estimates the (unknown) accuracy gap to the optimal variational solution using a regression model linking step-size changes to changes in symmetrized KL divergence, and stops when predicted accuracy gains are not worth predicted additional computation (an “inefficiency index”). The framework is validated via simulation studies on Gaussian targets and experiments on diverse real models/datasets from PosteriorDB, showing improved robustness and more consistent stopping compared with common fixed/adaptive learning-rate baselines. The authors provide an open-source Python implementation (VIABEL) and also fit small internal regression models in Stan with negligible overhead.","BBVI uses stochastic optimization updates $\lambda^{(k+1)}\leftarrow \lambda^{(k)}-\gamma^{(k)} d^{(k)}$. For fixed $\gamma$, the target is the stationary-mean parameter $\bar\lambda_\gamma$ estimated by iterate averaging $\hat\lambda_\gamma=\frac{1}{k_{\text{avg}}}\sum_{k=0}^{k_{\text{avg}}-1}\lambda^{(k_{\text{conv}}+k)}$. Termination is based on predicting how $\mathrm{SKL}(q^*,q^*_{\gamma})$ changes with learning rate via a log-linear regression (their Eq. (6)) and combining it with a regression-based prediction of iterations needed at the next rate (their Eq. (8)) into an inefficiency index; stop when the estimated inefficiency exceeds a threshold.","Across Gaussian targets and PosteriorDB model/data pairs, RABVI is reported to be more consistent than ADVI, SGD with exponential decay, RMSProp schedules, and fixed learning-rate baselines, while providing a principled stopping rule (whereas several baselines require an arbitrary fixed iteration budget). The estimated $\sqrt{\mathrm{SKL}}$ accuracy at termination is shown in experiments to track the user-set accuracy threshold $\xi$ (e.g., default $\xi=0.1$), indicating the termination rule triggers near the intended accuracy. The paper reports that fitting the regression model used for termination in Stan adds very small overhead (stated as <0.5%). In comparisons to dynamic HMC at similar likelihood-evaluation budgets, RABVI often matches or improves posterior mean estimates but tends to have less accurate standard deviation estimates, consistent with known limitations of mean-field (and sometimes full-rank) variational families.","The authors note that BBVI (even with RABVI) can be less accurate than dynamic HMC under equal computational budgets, particularly for uncertainty (standard deviation) estimation, and that mean-field approximations often yield poor variance estimates. They also report that moving to more flexible variational families (full-rank Gaussian or normalizing flows) can give inconsistent results and sometimes worse outcomes because optimization becomes higher-dimensional and more challenging. They further state that their findings underscore the need for additional diagnostics for assessing posterior approximation accuracy beyond optimization reliability.","Despite being framed as “reliability,” the work addresses reliability of stochastic optimization for variational inference rather than reliability engineering of physical systems; thus it is out of scope for classical reliability/maintenance decision problems. The termination criterion depends on being able to estimate symmetrized KL between successive variational approximations; for complex variational families this may be nontrivial, and the paper indicates RABVI’s termination rule could not be used for real-NVP experiments (they used FASO instead), limiting general applicability. The convergence/stationarity checks import MCMC diagnostics (split-$\widehat{R}$, ESS/MCSE) but their operating characteristics for optimization iterates may depend on strong assumptions (e.g., local ergodicity/stationarity, adequate windowing) that may fail in nonconvex settings common in deep learning, and robustness to strong autocorrelation or multimodality is not fully characterized.","The authors state that RABVI can flexibly incorporate additional or future methodological improvements related to variational inference and stochastic optimization. They also highlight the need to supplement optimization frameworks like RABVI with diagnostics for assessing the accuracy of posterior approximations, citing existing work and positioning this as a necessary direction given inconsistent gains from richer variational families.","A natural extension is to develop termination criteria that do not require explicit/closed-form symmetrized KL between successive variational approximations, enabling full RABVI (not just FASO) for normalizing flows and other implicit/complex families (e.g., via unbiased SKL estimators or alternative discrepancy surrogates). Another direction is to evaluate and adapt the stationarity/ESS/MCSE criteria under nonconvex objectives and strongly correlated/chaotic optimization dynamics typical in large neural models, possibly with robust self-starting or multi-chain variants. Finally, providing a standardized benchmarking suite and packaged implementations (e.g., pip-installable tools, reproducible scripts) for the paper’s experiments would strengthen adoption and facilitate fair comparisons across BBVI optimizers and stopping rules.",2203.15945v2,https://arxiv.org/pdf/2203.15945v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:42:51Z
TRUE,Life distribution modeling|Other,"Bayesian|Parametric (Weibull, etc.)|Other",Right-censored|Left-censored|Interval-censored|Mixture of types|Complete lifetime data|Other,Not applicable,Manufacturing (general)|Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab)|Personal website,https://github.com/wqmeeker/lsinf|https://wqmeeker.stat.iastate.edu/RSplida.zip,"This paper provides guidance for specifying Bayesian prior distributions in reliability analyses, focusing on single log-location-scale lifetime models such as Weibull and lognormal under realistic censoring patterns. It reviews and extends objective/default prior constructions (Jeffreys, conditional Jeffreys, independence Jeffreys, and related reference priors) using the Fisher information matrix, with explicit treatment of Type I and Type II censoring and approximations for random/multiple censoring common in field data. A key contribution is advocating practical reparameterizations using an interpretable quantile $t_{p_r}$ (instead of the usual scale) to reduce posterior correlation, improve MCMC behavior, and facilitate elicitation, especially with few failures. The paper illustrates how weakly informative and partially informative priors (often informative only on the Weibull shape) can prevent nonsensical posterior mass in low-information data and materially sharpen inference on reliability metrics like $F(t)$ at a target time. Simulation studies under Type I censoring compare flat vs independence-Jeffreys priors and show IJ priors typically yield equal or better frequentist coverage for credible intervals, particularly with small fractions failing and few failures.","The core model is the log-location-scale CDF $F(t;\mu,\sigma)=\Phi\big((\log t-\mu)/\sigma\big)$ with likelihoods adapted to right-, left-, and interval-censoring. The Fisher information matrix is expressed as $I(\mu,\sigma)=\frac{n}{\sigma^2}\begin{pmatrix}f_{11}(z_c)&f_{12}(z_c)\\f_{12}(z_c)&f_{22}(z_c)\end{pmatrix}$, where $z_c$ depends on the censoring mechanism; Jeffreys priors follow $\pi(\mu,\sigma)\propto |I|^{1/2}$. A key reparameterization replaces the Weibull scale $\eta$ with a quantile $t_{p_r}$ via $t_{p_r}=\eta[-\log(1-p_r)]^{1/\beta}$, yielding $F(t;t_{p_r},\beta)=1-\exp\{\log(1-p_r)(t/t_{p_r})^{\beta}\}$. For Type I censoring, an example IJ prior on $(\log t_{p_r},\log\sigma)$ is proportional to $\sqrt{f_{11}\{f_{11}[\Phi^{-1}(p_r)]^2-2f_{12}\Phi^{-1}(p_r)+f_{22}\}}$.","In the bearing-cage field-data example with heavy multiple right-censoring (6 failures, 1,697 censored), a likelihood-ratio CI for $F(8000)$ is reported as $[0.026,0.9999]$ (uninformative), while adding an informative prior on Weibull shape produces a much tighter Bayesian 95% credible interval for $F(8000)$ of $[0.15,0.92]$, changing the design-life conclusion. In the rocket-motor current-status example (3 left-censored failures, 1,937 right-censored), the likelihood-ratio CI for $F(20)$ is likewise $[0.023,0.9999]$; with a partially informative prior on $\beta$ (lognormal 99% range (1,5)), the Bayesian 95% credible interval for $F(20)$ is reported as $[0.008,0.16]$ (substantially more actionable). The simulation study under Type I censoring (5,000 datasets per cell; expected failures $E(r)\in\{10,25,35,50,75,100\}$; $p_{fail}\in\{0.01,0.05,0.10,0.50\}$) finds IJ priors generally match or improve upon flat priors in coverage, with both close to nominal once $E(r)\ge 35$ and IJ showing the clearest gains when $p_{fail}$ is very small (e.g., 0.01). The paper also notes practical sampler instability and/or effectively improper posteriors can arise with extremely few failures (especially 2) under flat-type priors, motivating weakly informative constraints.","The authors note that many default/noninformative priors are improper and that posterior propriety and computational stability can fail or be problematic with extremely few failures (they report difficulty obtaining stable sampling with only two failures under a flat prior, and occasional infinite/overflow values even with three failures). They also acknowledge that exact reference-prior expressions for Type I censoring are generally intractable, so their Type I recommendations rely on IJ/CJ constructions and approximations rather than closed-form reference priors. For random/multiple censoring, they suggest using Type I–style IJ priors as an approximation because fully specifying the censoring-time model $h(x)$ may be impractical.","Because the Type I IJ priors depend on Fisher-information quantities that are evaluated numerically (and, in their Stan implementation, interpolated), results may be sensitive to approximation quality and non-differentiability issues that can affect HMC/NUTS performance; this could impact reproducibility across implementations. The work focuses on single-distribution (two-parameter) lifetime models; extensions to common real-world complications (covariates/regression, frailty/heterogeneity, mixtures, competing risks beyond the censoring approximation) are only sketched, so applicability to complex systems/models is not fully validated. Empirical validation is primarily via two motivating datasets and simulations; broader benchmarks across diverse industries and dependence structures (autocorrelation, batch effects) are not explored.","The authors suggest extending default noninformative and partially informative prior methods to more complex reliability models and censoring structures, including accelerated testing models (noting existing noninformative-prior work could be extended to Type I censoring). They also point to other reliability data types—degradation, fatigue S–N data, and recurrent events—where default priors are needed and where their principles should be adapted. Finally, they emphasize the need for implementation in widely available software with user-facing options for default/overridden priors and built-in sensitivity analysis to make Bayesian reliability methods more accessible.","Developing self-starting/robust variants of the proposed default priors that remain proper and computationally stable with 0–2 failures (e.g., via principled weakly informative regularization) would improve usability in ultra-high-reliability settings. Extending the IJ/CJ prior framework to hierarchical and regression settings (AFT/PH models, covariate-dependent Weibull/lognormal, random effects) and validating frequentist coverage for reliability functionals under such models would broaden impact. Providing a reference implementation as an R/Stan package (or differentiable approximations to the $f_{ij}$ surfaces) with unit tests and numerical diagnostics would reduce implementation sensitivity and encourage adoption.",2204.06099v2,https://arxiv.org/pdf/2204.06099v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:44:06Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies a learning setting with a fixed query budget where labels are noisy but can be made more reliable by re-sampling (validating) the label of the same example multiple times and aggregating via majority vote. It formalizes the probability that majority vote recovers the true label under a multinomial model and investigates the trade-off between acquiring more distinct samples (quantity) versus spending queries on repeated labeling (quality). Empirically, it evaluates fixed validation counts, scheduled policies that increase validations over training, and a dynamic policy that stops validating based on a chi-square goodness-of-fit test, using experiments on MNIST with controlled label noise and a motivating poker-hand comparison task. Results show that as label noise increases, higher validation rates become more important; very low validations can prevent learning under extreme noise, while scheduled and chi-square-based adaptive validation can improve sample efficiency in lower-to-moderate noise settings. The contribution is primarily in learning under noisy labeling and budgeted resampling strategies rather than reliability engineering of physical systems.","Label assignment after $v$ validations uses majority vote: $\lambda_v = \lambda_k$ where $k = \arg\max_j v_j$ (ignoring ties). Label-count vectors after $v$ queries follow a multinomial distribution $P(v_1,\ldots,v_l)=\mathrm{Mult}(v,\mathbf{p})$, and the probability that majority vote returns the true label $\lambda_j$ is expressed as a sum over count vectors: $P(\lambda_j=\lambda_v)=\sum_{\mathbf{v}:\sum_i v_i=v} \mathrm{Mult}(\mathbf{v},\mathbf{p})\prod_{k\neq j}\mathbf{1}(v_j>v_k)$. A dynamic policy stops querying when the chi-square goodness-of-fit p-value for the observed counts falls below a threshold and returns $\arg\max(\text{counts})$.","Across MNIST experiments with injected uniform label noise (20%, 40%, 60%, 80%), majority-vote pooling of $v$ re-labeled instances outperforms using all $v$ noisy labels individually for training at the same query budget. The best fixed validation count increases with noise (e.g., best fixed values shift from $v\in\{3,5\}$ at 20% noise to higher values such as $v\ge 11$ at 60% noise; at 80% noise, the best performers are high $v$ like 15, 25, 51, 99). Scheduled policies that start with lower validations and increase them over time generally reach accuracy targets with fewer seen samples than the corresponding fixed-$v$ settings. The chi-square dynamic policy (p<0.05) performs best in low/moderate noise; reported mean validations were about 2.99 (20% noise), 4.93 (40%), 10.59 (60%), and 58.30 (80%), with large standard deviation at high noise.","The authors note that the chi-square dynamic policy has two key disadvantages: (i) validation sample sizes are extremely small, making statistical tests unreliable in principle, and (ii) the test can confidently reject uniformity even when there is no clear majority peak (ambiguous bimodal counts), which can lead to incorrect label decisions. They also state that more computationally intensive methods would be required to avoid these issues and that more work is needed, especially for very high-noise settings.","The study is focused on supervised learning with synthetic/uniform label-noise injection (for MNIST) and does not analyze robustness to more realistic class-conditional, instance-dependent, or adversarial noise patterns, which can change the optimal validation strategy. The evaluation emphasizes test accuracy vs. “seen samples” under a specific computational accounting; real-world cost models (e.g., human labeling time variability, correlated annotator errors) are not modeled, so operational guidance may not transfer directly. The paper appears to lack details on implementation, reproducibility artifacts, and sensitivity analyses (e.g., varying network architectures, hyperparameters, stopping criteria), which could affect the observed optimal validation schedules.","They suggest that a deeper investigation of the dynamic validation approach and other potential solutions is required, particularly because the chi-square method struggles when validation counts are small and when label distributions have no clear peak under high noise. They propose that a more robust method of computing the reliability/confidence of obtained validations could improve the performance of dynamic, sample-dependent validation and mitigate issues in very high-noise settings.","A natural extension is to model annotator/oracle reliability explicitly (e.g., Bayesian posterior over the true label given repeated queries) and derive stopping rules with calibrated confidence, rather than testing against uniformity. Another direction is to handle correlated label errors (e.g., crowd workers with bias) and to allocate validations adaptively based on model uncertainty or expected value of information, linking the approach more tightly to active learning. Finally, benchmarking on additional real noisy-label datasets and providing open-source implementations would strengthen generalizability and reproducibility.",2204.09462v1,https://arxiv.org/pdf/2204.09462v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:44:42Z
FALSE,Other,Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python,Not provided,https://bestref.net|https://data.lewoniewski.info/covid19/|https://w.wiki/48tx|https://w.wiki/499G|https://dbpedia.org/page/COVID-19_pandemic_in_Germany|https://dbpedia.org/sparql/|https://query.wikidata.org/|https://dumps.wikimedia.org/backup-index.html|https://dumps.wikimedia.org/|https://pageviews.toolforge.org|https://petscan.wmflabs.org/|https://publicsuffix.org/learn/|https://wikitech.wikimedia.org/wiki/Analytics/AQS/Pageviews|https://ores.wikimedia.org/|https://wikirank.net/,"This paper studies the reliability of web sources cited in Wikipedia’s COVID-19-related articles across 15 language editions over January 2020–August 2021. It identifies relevant COVID-19 articles using Wikipedia categories plus semantic queries to Wikidata and DBpedia, then extracts external URLs from article revision histories (including template-transcluded references) and normalizes domains using the Public Suffix List. The authors evaluate source “reliability/importance” via three ranking models: frequency of citation (F) and two popularity-weighted measures (PR and PR2) that weight citations by article pageviews divided by number of references, with PR2 using human-only views. Results are presented as month-by-month rank timelines and cross-language comparisons, highlighting recurrent top sources (e.g., WHO, BBC, Reuters, major newspapers) and differences by language edition; additional detailed statistics are provided in supplementary materials. The work is positioned as an information-quality/reliability-of-sources assessment for Wikipedia rather than engineering reliability of components or systems.","The frequency model is defined as $F(s)=\sum_{i=1}^{n} C_s(i)$, where $C_s(i)$ counts references using source/domain $s$ in article $i$. The popularity-weighted model is $PR(s)=\sum_{i=1}^{n} \frac{V(i)}{C(i)}\,C_s(i)$, where $V(i)$ is article pageviews over a period and $C(i)$ is total references in article $i$; PR2 uses the same form but with human-only pageviews.","Using the F-model across all 15 languages, the most frequent sources include BBC, Reuters, WHO, The Straits Times, CNN, Facebook, The Guardian, Twitter, and The New York Times. With PR/PR2 (pageview-weighted), leaders include WHO, BBC, The Guardian, The New York Times, Reuters, CNN, and Facebook, and additional high-ranked sources appear (e.g., The Washington Post, CDC, Austrian Broadcasting Corporation). The authors report that PR-model rankings show higher month-to-month volatility than F, while PR2 reduces volatility relative to PR. Language-specific sections list top sources per edition (e.g., English: BBC/WHO/The Guardian/Reuters/NYT; Spanish: Worldometer/El País/BBC/Infobae; Polish: Radio ZET/Twitter/TVN24/GOV.PL/WHO).","The paper notes space/size limitations: only some results are shown in the paper, with more detailed statistics provided in supplementary materials. It also describes practical difficulties/complexity in extracting historical references due to templates and changing article titles, implying extraction is nontrivial and requires custom handling.","The proposed “reliability” scores are proxies for source importance/visibility (citation frequency and pageview-weighted exposure) and do not validate factual correctness or credibility against external ground truth; popular articles/sources can still be wrong or biased. The evaluation is largely descriptive (rankings and trends) without a formal validation of whether higher-ranked sources are truly more reliable, nor sensitivity analysis to design choices (domain granularity via PSL, reference parsing heuristics, bots vs humans, time aggregation). Reproducibility is limited because Python code is described as proprietary/own algorithms but not shared, making it hard to audit extraction accuracy or replicate rankings.","The authors propose extending the reliability models by weighting a reference’s importance based on its position within a Wikipedia article. They also suggest incorporating measures related to Wikipedia author reputation, article protection status, topic similarity, and other signals.","Provide a fully reproducible pipeline (code + data processing notebooks) and an extraction accuracy evaluation (precision/recall on parsed references and template transclusions). Validate the ranking-based reliability proxy against independent credibility labels (e.g., Wikipedia perennial-sources lists, fact-checking datasets) and test robustness across topics beyond COVID-19. Extend models to account for source type (scientific vs news vs government), temporal source reliability drift, and potential confounding from coordinated editing or language-edition-specific citation norms.",2204.14130v1,https://arxiv.org/pdf/2204.14130v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:45:20Z
TRUE,System reliability|Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,C/C++|Other,Public repository (GitHub/GitLab)|Personal website,https://github.com/Khaled-Janada/AC_Charts|https://doi.org/10.5281/zenodo.6512066,"The paper proposes Angular Control Charts (ACC), a probability-limits time-between-events (TTF/TBE) control-chart framework to monitor reliability changes in fully repairable multi-state systems (MSSs), extending prior reliability monitoring charts largely limited to binary-state systems. The method maps each observed time-to-failure on a state-specific horizontal “state line” at its median TTF and converts observations to an angle statistic $\theta=\arctan(T_C/t)$, enabling simultaneous monitoring across multiple state transitions. For the standard ACC, angular control limits (ACL/ALCL/AUCL) are derived from distribution quantiles with a specified false-alarm probability (default $c=0.27\%$), yielding distribution-scale-invariant limits for exponential transitions and shape-dependent limits for families like Weibull/lognormal/Fréchet; gamma-case limits are computed numerically. A generalized ACC is introduced to handle heterogeneous distributions/parameters across states via piecewise (zig-zag) angular limits per state. Performance and usage are illustrated through simulated MSS examples (including cumulative TTF analogous to $t_r$ charts) and accompanied by an open-source C# implementation (AC Charts Software).","The charting statistic is the angle $\theta=\arctan(T_C/t)$ where $t$ is the observed TTF and $T_C=F^{-1}(1/2)$ is the median of the assumed state-transition distribution. Probability limits are set at $T_L=F^{-1}(c/2)$ and $T_U=F^{-1}(1-c/2)$, then converted to angular limits: $\theta_L=\arctan(T_C/T_L)=\arctan(\rho(1/2,c/2))$ and $\theta_U=\arctan(T_C/T_U)=\arctan(\rho(1/2,1-c/2))$ with $\rho(a,b)=F^{-1}(a)/F^{-1}(b)$; the angular center line is always $\theta_C=45^\circ$. For exponential transitions ($F^{-1}(p)=-\alpha\ln(1-p)$), the ACL angles depend only on $c$ (e.g., with $c=0.27\%$, $\theta_L\approx 89.89^\circ$, $\theta_U\approx 5.99^\circ$ on a linear scale).","Using the conventional false-alarm probability $c=0.27\%$ (3-sigma analogue), the standard exponential ACC yields linear-scale limits of approximately $\theta_L=89.89^\circ$ and $\theta_U=5.99^\circ$; alternative drawing scales spread limits (e.g., square-root: $87.47^\circ/17.95^\circ$, cubic-root: $82.88^\circ/25.25^\circ$, quartic-root: $78.13^\circ/29.64^\circ$). The paper derives closed-form, shape-parameter-dependent ACL expressions for Weibull, lognormal, and Fréchet transitions, and reports that gamma ACLs must be obtained numerically but depend on the shape parameter rather than the scale. In simulated Example I (exponential MSS), the ACC detects improvement in one state transition (points below AUCL) and degradation in another (point above ALCL) after parameter shifts, while remaining in-control in the initial 25 observations (roughly half points above/below ACL). In Example II (cumulative TTF with Erlang shape $\beta=2$), fewer out-of-control points were observed than Example I, suggesting reduced sensitivity for that configuration.","The authors note that if ACC is used to monitor cumulative TTF every $r$ occurrences (analogous to a $t_r$ chart), further investigation is needed on how $r$ affects chart sensitivity, and the approach is restricted to cases where a “limiting sum distribution” applies. They also identify two practical obstacles still under study: clarifying the chronological order of failure events in the charted data stream and handling potential coincidences where two or more observation points overlap.","The method presumes correct specification (or stable estimation) of each state-transition distribution/parameters; the paper’s demonstrations are simulation-based and do not quantify robustness to misspecification, estimation error, or limited Phase I data. The ACC framework appears to assume independent state transitions and immediate, perfect repair to the “0th state” with negligible repair time; many repairable MSSs exhibit imperfect repair, non-negligible downtime, dependence, or competing risks that could distort TTF behavior. Performance evaluation is illustrative rather than a comprehensive ARL/run-length comparison versus established TBE charts (e.g., EWMA/CUSUM or multi-stream methods), so detection optimality across shift sizes is not established. Practical guidance on parameter estimation, Phase I setup, and real-time updating for heterogeneous states is limited, which could affect deployability in field settings.",The authors explicitly call for further research on the effect of the cumulative-count parameter $r$ (when monitoring cumulative TTF every $r$ occurrences) on the sensitivity of the ACC. They also state that two issues remain under study: resolving the chronological order of failure events and addressing coincident/overlapping observation points on the ACC display.,"A valuable extension would be a full run-length/ARL study (including Markov-chain or simulation-based ARL curves) comparing ACC against leading TBE charts (Shewhart-type probability limits, EWMA/CUSUM TBE, and multi-stream monitoring) under a range of shifts and autocorrelation. Developing self-starting/Phase-I parameter estimation procedures (with uncertainty-adjusted limits) would improve practical applicability, especially for multiple heterogeneous state transitions with sparse data. Robust and nonparametric variants (or Bayesian updating) could mitigate distributional misspecification and enable online learning of state-transition models. Finally, incorporating imperfect repair, repair-time/downtime, and dependent transitions (e.g., shared environment or load) would broaden the method to more realistic MSS reliability monitoring scenarios.",2205.02024v1,https://arxiv.org/pdf/2205.02024v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:46:06Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Environmental monitoring,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,http://globalforestwatch.org/|http://www.ipol.im/pub/art/2011/bcm_nlm/?utm_source=doi|https://doi.org/10.5334/jors.377|http://dx.doi.org/10.1016/j.rse.2014.09.034|http://dx.doi.org/10.1016/j.rse.2010.02.022|http://cbmjournal.springeropen.com/articles/10.1186/s13021-017-0078-9|https://www.nicfi.no/|http://dx.doi.org/10.1038/s41558-021-01026-5|http://dx.doi.org/10.1038/nclimate2919|https://www.sciencedirect.com/science/article/pii/S0034425714001527|http://www.ipcc-nggip.iges.or.jp,"This preprint proposes a semi-supervised deforestation change-detection method using Sentinel-1 SAR time series designed to work when reliable, temporally precise reference labels are unavailable. A static “stable forest” mask is used to estimate, at each time step, empirical (histogram-based) class-conditional distributions for VV and VH backscatter; per-pixel forest similarity is computed from these distributions and combined across variables into a joint similarity score. Deforestation is framed as an increase in dissimilarity from the forest reference, detected via a cumulative product statistic compared against a threshold, yielding a first-change (single changepoint) detection time. The approach is evaluated on a primary site in Paragominas, Brazil with visually interpreted change dates and compared against annual reference products (Global Forest Watch/Hansen and JRC TMF), and is also tested on additional sites (Porto Velho, Cameroon, Riau). Reported performance at the main site achieves high sensitivity (producer’s accuracy) with some false positives (lower user’s accuracy), and robustness experiments show limited degradation with moderate corruption of the forest reference mask.","Forest similarity for variable $i$ at time $t$ is computed from the forest-only histogram as $p_{i,t}(x_{i,t})=\frac{1}{P}\,C_{b(x_{i,t})}$, where $C_{b(x_{i,t})}$ is the count in the bin containing the observed value and $P$ is a normalizing constant. Multivariate similarity is combined as a product $p_t(x_t)=\prod_i p_{i,t}(x_t)$. A forest-based threshold is set using the $q$-quantile $p^F_{t,q}$, and a cumulative product (confidence) statistic is updated as $\Lambda_t(x)=\Lambda_{t-1}(x)\cdot \frac{p_t(x)}{p^F_{t,q}}$ (with initialization/reset rules given in the paper); a deforestation time $T$ is declared at the first $t$ such that $\Lambda_t(x)\ge L$.","At the Paragominas site (with visually interpreted reference), the method reports producer’s accuracy 96.5%, user’s accuracy 75.7%, and balanced accuracy 90.4%. Across additional sites and using Hansen or JRC annual maps as references, balanced accuracy is typically around ~80% (site-dependent), with generally higher producer’s accuracy than user’s accuracy due to false positives in non-forest areas that temporarily resemble forest. In label-noise experiments that corrupt the forest reference mask, the change map is visually stable up to ~10% corruption and retains substantial detections up to ~22%; omission error increases with corruption while commission error does not increase materially. The paper notes that for the Indonesian site, JRC reference detects little/no deforestation, leading to very low JRC-based scores despite visible changes in Sentinel-1-based outputs.","The authors note false positives occur mainly where non-forest areas have backscatter signatures temporarily similar to forest, especially near the start of the time series, causing spurious early detections. They state the approach is sensitive to terrain effects in SAR backscatter (e.g., mountainous areas in Indonesia produced false positives) and suggest terrain flattening or adding optical data. They also acknowledge that detection delay and false positive rate need improvement and that broader validation in additional circumstances is required; additionally, precise temporal accuracy is hard to quantify without better reference change dates.","The method relies on strong independence assumptions when multiplying per-variable similarities (VV and VH) and ignores spatial dependence/autocorrelation in time series, which can miscalibrate the cumulative product statistic and thresholds. Control parameters ($q$, $L$) are tuned to optimize balanced accuracy versus a chosen reference product, risking overfitting to biased/erroneous “ground truth” maps and limiting transferability without re-tuning. Histogram binning choices and normalization ($P$) can materially affect similarity estimates, especially under varying incidence angle/sensor geometry and seasonal regimes, but the sensitivity to these design choices is not fully characterized. The approach assumes at most one forest-to-non-forest change per pixel; areas with regrowth, multiple disturbances, or temporary clearing may violate this and degrade performance.","The authors state further work is needed to reduce the false positive rate, improve detection delay, and validate the method in additional circumstances. They propose exploring additional data sources beyond Sentinel-1 (other SAR sensors and optical data), including multisensor combinations, since the method is agnostic to input modality. They also suggest addressing terrain-related false positives via terrain flattening or incorporating optical data less sensitive to topography.","Develop a self-starting/adaptive thresholding scheme that accounts for autocorrelation and seasonality (e.g., time-varying false-alarm control) rather than using fixed $q$ and $L$ tuned to a reference map. Extend the framework to explicitly model multiple changepoints and regrowth (forest→non-forest→forest) to handle shifting cultivation and transient disturbances. Add spatial contextual modeling (e.g., MRF/CRF post-processing or spatiotemporal priors) to suppress isolated false positives while preserving small clearings. Provide an open-source reference implementation with reproducible experiments (including binning strategy, preprocessing, and parameter selection) and benchmark against established remote-sensing change detection methods on curated datasets with precise event dates.",2205.12131v1,https://arxiv.org/pdf/2205.12131v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:46:55Z
TRUE,Maintenance optimization|Failure mode analysis|Other,ML-based|Other,Event/count data|Mixture of types|Other,Condition-based|Predictive|Imperfect maintenance|Other,Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a prescriptive maintenance framework that learns machine-dependent (individualized) effects of preventive maintenance (PM) frequency on two count outcomes: number of failures and number of overhauls per running period. To relax common imperfect-maintenance assumptions (deterministic/known stochastic effects and machine-independent effects), it frames maintenance planning as a causal inference problem with continuous-valued treatment (PM frequency) and estimates potential outcome curves conditional on machine/contract features. The method uses SCIGAN (a GAN-based causal model for continuous treatments) to generate counterfactual outcomes, then trains supervised predictors on augmented data, and finally chooses the PM frequency that minimizes a linear cost model combining PM, failures, and overhauls. Empirical validation uses real-world contract covariates from >4,000 industrial full-service maintenance contracts and evaluates decisions primarily via semi-synthetic experiments that inject controlled selection bias; the proposed individualized causal policy outperforms a supervised baseline and a non-individualized (average) policy in outcome-curve accuracy and cost. The work advances maintenance optimization by integrating modern causal ML to address confounding in observational maintenance histories and to prescribe individualized PM schedules under imperfect maintenance effects.","The total expected cost per running period for machine/contract $i$ at PM frequency $t$ is modeled as $c_i(t)=c_t\, t + c_o\, o_i(t) + c_f\, f_i(t)$, combining planned PM cost and costs from overhauls and failures, where $o_i(t)$ and $f_i(t)$ are potential outcomes (counts per running period). Potential outcomes are learned as conditional mean functions $g_o(t,x)=\mathbb{E}[O(t)\mid X=x]$ and $g_f(t,x)=\mathbb{E}[F(t)\mid X=x]$ using SCIGAN to address selection bias. The prescribed PM frequency is individualized via $t_i^*=\arg\min_t c_i(t)$; selection bias in semi-synthetic experiments is generated with $t_i\sim 20\,\mathrm{Beta}\left(1+\frac{\lambda\delta_i}{10},\,1+\lambda\delta_i\right)$ where $\delta_i=\sigma(w_b^\top x_i)$.","On the semi-synthetic evaluation (using covariates from >4,000 contracts), SCIGAN predicts potential outcomes more accurately than a purely supervised MLP: MISE for overhauls is $7.71\pm0.60$ (SCIGAN) vs $10.25\pm1.33$ (MLP), and for failures $14.16\pm1.68$ (SCIGAN) vs $18.27\pm3.65$ (MLP). For decision quality, the individualized causal policy (SCIGAN–ITE) achieves lower policy error $2.40\pm0.46$ and lower policy cost ratio $1.07\pm0.01$ than the supervised individualized baseline (MLP–ITE: PE $4.36\pm1.25$, PCR $1.11\pm0.02$) and the non-individualized average policy (SCIGAN–ATE: PE $8.77\pm1.07$, PCR $1.24\pm0.04$). Sensitivity analysis over increasing selection bias parameter $\lambda$ shows SCIGAN–ITE remains stable in MISE/PE/PCR, while MLP–ITE degrades as bias increases, indicating the benefit of causal adjustment under confounding.","The authors note the approach relies on strong causal-identification assumptions for observational data, especially overlap (positivity) and unconfoundedness (no hidden confounders). They state overlap can be tested/characterized from data and that unconfoundedness is untestable in practice and may be violated if relevant drivers of past maintenance decisions are missing, which would bias potential-outcome estimates. They also mention the need to quantify uncertainty/ignorance and consider sensitivity analyses for hidden confounding when assumptions may fail.","The prescriptive cost model is a simple linear additive function of PM frequency and expected counts, which may miss important nonlinearities, downtime coupling, service-level penalties, and capacity/resource constraints that affect real maintenance planning. The empirical validation relies heavily on semi-synthetic outcomes rather than observed real counterfactuals, so real-world performance under true (unknown) maintenance effects is not fully established. The framework treats outcomes as aggregated counts per running period and does not explicitly model time-to-event processes, recurrent-event dependence, or temporal covariates (e.g., evolving condition), which can be critical in reliability settings. Implementation complexity and hyperparameter sensitivity of GAN-based causal models (stability, calibration) are not deeply discussed, nor is there a comparison to classical maintenance optimization baselines (e.g., virtual-age/improvement-factor models) on the same data.","The authors propose extending the framework to multiple types of maintenance interventions with varying intensities and costs, and to richer cost structures such as stochastic or feature-dependent costs that must be predicted. They also suggest incorporating more flexible timing and sequences of interventions (dynamic treatment regimes), connecting to causal inference for treatment sequences. Finally, they mention tighter integration of prediction and decision-making via predict-and-optimize and cost-sensitive learning approaches.","A valuable extension would be to relax i.i.d. assumptions by handling autocorrelation and evolving condition signals (e.g., sensor streams) with longitudinal causal models and recurrent-event/time-to-failure formulations. Adding robustness tools—e.g., doubly robust estimators, constrained optimization under partial overlap, and explicit uncertainty quantification for prescribed $t_i^*$—would improve deployability in safety-critical maintenance. Comparing against established reliability/maintenance benchmarks (virtual age models, GRP/Kijima processes, proportional intensity models) on real contract outcomes would better position the method within reliability engineering practice. Incorporating operational constraints (crew availability, spare parts, grouped/opportunistic maintenance) and solving the resulting constrained prescriptive optimization would make the recommendations more realistic for fleet-level planning.",2206.01562v1,https://arxiv.org/pdf/2206.01562v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:47:44Z
TRUE,Life distribution modeling|System reliability|Failure mode analysis|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based",Right-censored|Mixture of types,Predictive,Semiconductor/electronics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,NA,The paper develops a parametric reliability modeling framework for censored competing-risks data with two dependent failure modes. It proposes a Marshall,"MOBWDS construction: let independent shocks U0~We(\alpha0,\lambda), U1~We(\alpha1,\lambda), U2~We(\alpha2,\lambda), and define X=min(U1,U0), Y=min(U2,U0). Survival: S(x,y)=P(X\ge x,Y\ge y)=exp\{-\lambda(x^{\alpha1}+y^{\alpha2}+\max(x,y)^{\alpha0})\} (piecewise for x<y, y<x, and x=y). Observed likelihood for censored competing risks uses contributions from Mode 1/Mode 2 derivatives of S and a singular tie component f0(t); under MOBWDS it simplifies to logL including m0 log P(X=Y) + m1 log\alpha1 + m2 log\alpha2 + (m0+m1+m2)log\lambda - \lambda\sum_i(t_i^{\alpha0}+t_i^{\alpha1}+t_i^{\alpha2}) plus mode-specific terms. Future-failure prediction among n* right-censored units: \rho=1-S_T(R+\delta)/S_T(R) with S_T(t)=exp\{-\lambda(t^{\alpha0}+t^{\alpha1}+t^{\alpha2})\}, and M\sim Binomial(n*,\rho).","Simulation results (n=100,200,400) show relative bias and relative MSE of both MLE and Bayesian estimates generally decrease with sample size; interval lengths shrink with n, and coverage is near 95% for most parameters. Under censoring (about 20% and 40%), intervals for \alpha0 show notably poor coverage (worsening with higher censoring), attributed to persistent bias in \alpha0 estimates; authors suggest bias-reduction (e.g., jackknife) or bias-corrected bootstrap intervals as possible remedies. In the device-failure case study (30 units; 8 right-censored at 300k cycles; 15 Mode S and 7 Mode W failures), fitted parameters (MLE) are \alpha0=0.234, \alpha1=2.070, \alpha2=0.761, \lambda=0.180, with Bayesian estimates \alpha0=0.528, \alpha1=2.171, \alpha2=0.818, \lambda=0.169. Estimated MTTF is about 210.27k cycles (frequentist) and 219.68k cycles (Bayesian); predicted future failures among the 8 censored units over (300,330)k is 1.40 (frequentist) vs 1.59 (Bayesian), and over (300,500)k is 6.37 vs 6.42, with mode-specific predictions much larger for Mode S than Mode W.","For censored data, the paper notes that confidence/credible interval coverage for the dependence-related shape parameter \alpha0 can be poor, and that this problem worsens as censoring increases. The authors attribute this to bias in estimating \alpha0 that does not diminish much with sample size, causing narrowing intervals to miss the true value. They suggest considering bias-reduction methods (e.g., jackknife) and bias-corrected bootstrap intervals as potential solutions.",The MOBWDS model imposes a specific Marshall,"The authors propose extending the approach from two failure modes to a multivariable/multivariate setting with more than two competing failure modes. They note that while Marshall and Olkin outlined multivariate exponential extensions, extending to a multivariate Weibull model with distinct shape parameters is not straightforward due to analytical complications from singular distributions in lower dimensions. They suggest developing such a multivariate Weibull extension as an interesting and useful future research direction for modeling reliability data with multiple failure modes.","Develop robust and/or semi-parametric extensions to handle model misspecification, including alternative dependence structures (e.g., copula-based MOBWDS variants) and sensitivity analysis for dependence. Address unknown/estimated censoring mechanisms and incorporate covariates (accelerating factors, usage, environment) via regression structures on Weibull parameters to improve field applicability. Provide self-contained computational methods (with numerical integration strategies for P(X=Y) and f0) and release reference software for estimation and prediction to improve adoption. Extend prediction to hierarchical/heterogeneous field populations (random effects) and validate predictive calibration on additional real datasets with varying tie rates and censoring levels.",2206.12892v1,https://arxiv.org/pdf/2206.12892v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:48:31Z
FALSE,Other,Nonparametric/Semi-parametric|Other,Other,Not applicable,Service industry|Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"This paper proposes a distribution-free calibration procedure for learning-to-rank recommender systems that outputs a variable-size set of recommended items with a rigorous finite-sample guarantee on the false discovery rate (FDR). Starting from any pretrained pairwise ranking model, items are scored by their average win probability against other items and a thresholded set $T_\lambda(X)$ is formed; the threshold $\hat\lambda$ is chosen via Learn-then-Test with fixed-sequence hypothesis testing and a concentration-bound-based upper confidence bound on empirical false discovery proportion. The method also supports optimizing secondary objectives (illustrated with embedding-based slate diversity) while preserving the same FDR guarantee by selecting a diverse subset of $T_\lambda(X)$ up to a maximum slate size $M$ using a greedy approximation. Experiments on Yahoo! Learning to Rank and MS MARCO show the achieved FDR is controlled near the target level and the returned set sizes adapt to query difficulty; diversity optimization increases average diversity (e.g., about 15% in one Yahoo! setting) while maintaining the FDR guarantee. Overall, the contribution is in statistical risk control/uncertainty quantification for recommender outputs rather than classical reliability engineering of physical systems.","The false discovery rate is defined as $\mathrm{FDR}=\mathbb{E}\left[\frac{|\hat S\cap (Y^*)^c|}{\max(|\hat S|,1)}\right]$; operationally they use $\mathrm{FDP}(\hat S,Y)=\frac{|\hat S\cap Y_{(m+1:K)}|}{\max(|\hat S|,1)}$ where items outside the top-$m$ are treated as false discoveries. Items are scored by $s_i(X)=\frac{1}{K-1}\sum_{j\neq i}\hat\pi_{i,j}(X)$ and selected via thresholding $T_\lambda(X)=\{i:s_i(X)\ge \lambda\}$. Calibration picks $\hat\lambda$ using a Hoeffding-style upper confidence bound: reject $H_{0,\lambda}:\mathrm{FDR}(T_\lambda)>\alpha$ if $\frac{1}{n}\sum_{i=1}^n \mathrm{FDP}(T_\lambda(X_i),Y_i)+\sqrt{\frac{\log(1/\delta)}{2n}}<\alpha$, scanning $\lambda$ in decreasing order with fixed-sequence testing.","On Yahoo! L2R, they target FDR level $\alpha=0.30$ with tolerance $\delta=0.10$ and report that the empirical risk histogram concentrates near (and below) 0.30 across 100 random calibration/test splits, indicating near-tight control. On MS MARCO, they target $\alpha=0.50$ with $\delta=0.10$ and similarly observe controlled, near-tight risk across splits. With diversity optimization on Yahoo! (reported example with $\alpha=0.30$, $\delta=0.10$, $M=3$), the average diversity increases by about 15% and about 40% of prediction sets are modified, while the risk remains controlled and most returned sets have size three due to the cap $M=3$.",None stated.,"The guarantees rely on an i.i.d. calibration sample and exchangeability between calibration and deployment; recommender logs often exhibit temporal dependence, feedback loops, and distribution shift that can break these assumptions. The definition of “good” items is based on belonging to the top $m$ of a (query-specific) ground-truth ranking, which may not align with real utility, multiple relevance grades, or user satisfaction metrics; the choice of $m$ is also somewhat ad hoc. The diversity-optimized set construction uses a greedy heuristic without approximation guarantees, so diversity gains may be inconsistent and sensitive to embedding quality and distance choice. No implementation details about runtime/scalability are provided for large $K$ (pairwise probabilities and diversity computations can be expensive in production).","The discussion suggests the framework is modular: the FDR notion of error can be replaced with other error notions such as false negative rate with minimal changes, and objectives other than diversity can be optimized by swapping in a different criterion in their constrained optimization formulation. They also suggest enforcing multiple constraints simultaneously (e.g., requiring both FDR control and diversity at specified levels).","Extend the calibration to handle dependence and nonstationarity typical in recommender deployments (e.g., time-series split conformal/risk control, covariate shift, and feedback-loop-aware evaluation). Develop computationally efficient approximations for large candidate sets (large $K$) and provide complexity analyses and scalable implementations. Add robustness studies for different relevance definitions, graded relevance labels, and alternative FDP/FDR formulations that better match user utility or safety constraints. Provide broader real-world validation (online A/B tests) and include baselines from calibrated ranking/uncertainty estimation methods to clarify practical tradeoffs.",2207.01609v1,https://arxiv.org/pdf/2207.01609v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:49:07Z
FALSE,NA,"Bayesian|Parametric (Weibull, etc.)|Other",Other,Not applicable,Other,Simulation study|Other,TRUE,R|Other,Supplementary material (Journal/Publisher),https://osf.io/bk8a7/,"This preprint develops a Bayesian framework for assessing inter-rater reliability (IRR) when variance components vary with contextual covariates, using heterogeneous variance-components mixed-effects models. It models group/covariate effects not only on the mean rating but also on the structural (between-ratee) and residual (within-ratee) variance via log-linked regressions, yielding covariate-dependent IRR/ICC expressions. The paper uses Bayes factors for model selection across a space of submodels and proposes Bayesian model averaging (including inclusion Bayes factors) to account for model uncertainty and to test for differences in specific variance components. Performance is assessed via an extensive simulation study comparing Bayes-factor selection and BMA against frequentist (AIC/BIC, stepwise ML/REML) and Bayesian predictive criteria (WAIC/LOO), and illustrated with grant peer-review datasets. Supplementary R scripts and materials are provided online.","IRR is defined in the basic random-intercept model as $\mathrm{IRR}=\sigma_\gamma^2/(\sigma_\gamma^2+\sigma_\varepsilon^2)$. In the heterogeneous variance-components model, subject-specific SDs are modeled as $\sigma_{\varepsilon i}=\alpha_\varepsilon\exp(\beta_\varepsilon^\top u_i)$ and $\sigma_{\gamma i}=\alpha_\gamma\exp(\beta_\gamma^\top v_i)$, giving covariate-dependent IRR $\mathrm{IRR}_i=\alpha_\gamma^2 e^{2\beta_\gamma^\top v_i}/(\alpha_\gamma^2 e^{2\beta_\gamma^\top v_i}+\alpha_\varepsilon^2 e^{2\beta_\varepsilon^\top u_i})$. Model evidence is compared using Bayes factors $\mathrm{BF}_{10}=p(\mathrm{data}|M_1)/p(\mathrm{data}|M_0)$ and inclusion Bayes factors computed from posterior vs prior inclusion odds across the model space.","In simulations (e.g., with 3 ratings per subject), Bayes-factor model selection identified the correct model about 0.26 of the time at $N=25$ per group and improved to about 0.70 at $N=200$, comparable to AIC and stepwise procedures at larger $N$ but outperforming WAIC/LOO in larger samples. Bayesian model averaging improved estimation in small samples; for residual SDs at $N=25$, RMSE decreased from 0.075 (BF-selected model) to 0.069 (BMA), and for IRR RMSE decreased from 0.106 to 0.100. Inclusion Bayes factors were well calibrated for mean differences and residual-variance differences with very low rates of misleading strong evidence (<0.6%), but were slower to support true structural-variance differences when only a few ratings per subject were available. In real NIH/AIBS peer-review examples, evidence tended to support differences primarily in residual variances rather than means or structural variances, and model uncertainty motivated reporting model-averaged IRR estimates.","The authors note that the simulation study covers only a finite set of parameter configurations and includes only one binary covariate, largely due to the already substantial computational burden and the number of competing methods compared. They also state that their demonstrations focus on a simple model where the ratee is the only structural random effect and other sources of error (e.g., rater, occasion/facets) are absorbed into the residual, whereas real applications may require more complex generalizability-theory designs. They further acknowledge they only explicitly treat binary covariates in the main development, with extensions to other covariate types discussed conceptually.","Although the method targets IRR/ICC in rating contexts, it assumes normality and (conditional) independence structures typical of Gaussian mixed models; robustness to non-normal, ordinal, or bounded rating scales is not established here. Practical guidance on prior elicitation for variance-component covariate effects is limited to a few default scales, and results (especially Bayes factors) can be sensitive to these choices in other domains/scales. Computational demands (MCMC + marginal likelihood via bridge sampling across many submodels) may be substantial for larger model spaces (e.g., many covariates/interactions), and the paper does not provide detailed runtime/diagnostics guidelines for practitioners. Comparisons focus on a selected set of model-selection/averaging methods; other modern alternatives (e.g., fully hierarchical shrinkage priors for model uncertainty without enumerating model spaces) are not evaluated.","The authors suggest extending the approach beyond binary covariates to multi-level factors and continuous covariates, with appropriate coding/contrast choices and prior specification. They also point to generalizing from the simplest random-intercept setup to more complex hierarchical/generalizability-theory designs with additional facets (e.g., raters, occasions) and corresponding reliability coefficients. They discuss that practical implementation details for more complex variance-component covariate structures in common software are nontrivial and imply further development/engineering may be needed.","A natural extension would be to adapt the framework to ordinal/graded ratings (e.g., cumulative link mixed models) with heterogeneous latent-scale variances, reflecting common peer-review and rubric scoring. Developing computationally efficient strategies for large model spaces (e.g., spike-and-slab or continuous shrinkage priors on variance-covariate effects, variational inference, or reversible-jump methods) could reduce the need to fit many submodels and compute marginal likelihoods. Additional empirical validation across multiple real-world rating systems (healthcare, education, hiring) and sensitivity analyses for violations such as rater dependence or missing-not-at-random ratings would strengthen practical guidance. Packaging the workflow into a maintained R package with diagnostics and templates for common IRR designs would improve accessibility and reproducibility beyond the provided scripts.",2207.02071v2,https://arxiv.org/pdf/2207.02071v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:49:50Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/OguzhanYueruek/AlgebraicClusteringSupplementaryData,"This paper proposes “Algebraic clustering” (Aclus), a computational-algebra method for detecting inconsistencies and extracting logical rules from binary (yes/no) data tables. Each row pattern is encoded as a Boolean polynomial and the complement of the union of observed patterns generates an “Aclus ideal” whose elements represent logical rules violated by no observed row. The method computes a Boolean Gröbner basis (using a weighted lexicographic monomial order based on column prevalences) and reduces each observation to a normal form that highlights its peculiarity relative to the dataset’s implied rules. The approach is demonstrated on a toy example (copper beech growth conditions) and on real datasets including a retirement questionnaire and ancient Egyptian statue classifications, where reductions helped identify likely miscoded entries and interpretable constraints among variables. Implementation is done using OSCAR/Singular, and the authors provide supplementary code and notebooks.","Observations (rows) are encoded as Boolean “select statements” of the form \(g_j=\prod_{i: M[j,i]=1} X_i\,\prod_{i: M[j,i]=0}(1+X_i)\), which evaluates to 1 only on that row pattern. The Aclus ideal is principal, generated by \(g = 1 + \sum_{j=1}^m g_j\), so \(I=\langle g\rangle\subset B(X_1,\dots,X_n)\); its elements correspond to patterns/rules absent from the dataset. Peculiarity of an observation (or a cluster) is obtained by taking the Gröbner-basis normal form \(r=\mathrm{NF}(f;G)\) of its polynomial \(f\) modulo \(I\), using a weighted lexicographic order with weights \(w_i = n_{true,i}(m-n_{true,i})\).","On the toy copper-beech table, the method reduces row 4 to \(cg+c+g+1=\neg(c\vee g)\), flagging it as uniquely contradicting the otherwise implied rule “if not cold then grows” (\(\neg c\Rightarrow g\)) if that row were removed. It also identifies a two-row cluster {3,6} whose combined reduction yields \(vg\), indicating that “very cold” and “grows well” co-occur only in that cluster (suggesting the potential rule \(v\Rightarrow\neg g\) would hold without them). On a 1524×9 retirement questionnaire table, reduced patterns highlighted surprising answer combinations for specific individuals and yielded interpretable rules from the Gröbner basis (e.g., \(hIcx+hcx\in I\) implying inheritance is always selected when {sell home, assets/income impact, job loss} are selected). On a 495-statue dataset (29 patterns across 6 binary features), reductions flagged at least two concrete miscoding cases (patterns reducing to \((A+1)H\) and \((h+1)H\) were revised as false classifications), and Gröbner-basis rules such as \(hX+HX\in I\) supported human-interpretable constraints (if largely destroyed and amputated head, then head is missing).",None stated.,"The work is primarily a logical/algebraic data-consistency framework rather than a reliability-engineering study; “reliability” is used in the sense of intercoder/intracoder reliability, not system/component failure reliability. The effectiveness of inconsistency detection depends on the assumption that the observed dataset is largely rule-governed and that deviations correspond to errors; true rare-but-valid cases may be flagged similarly. Results and interpretability can depend on the chosen monomial order/weighting scheme, and the paper provides limited systematic sensitivity analysis across alternative orderings or noise levels.","The authors state they plan to discuss alternative monomial orderings—especially elimination orders—as a follow-up, as these can offer additional functionality for interpreting rules and reductions. They also note that the weighting/ordering component is modular and could be replaced by other schemes, suggesting further exploration of ordering strategies.","A useful extension would be a principled robustness/noise model (e.g., probabilistic scoring of “peculiarity” and false-positive control) to distinguish miscoding from legitimate rare patterns. Another direction is scalability benchmarking and complexity analysis across larger n (columns) and m (rows), including comparisons with SAT-based implementations mentioned by the authors. Developing a user-facing software package with diagnostics (rule visualization, cluster search heuristics, and sensitivity to monomial order) would improve practical adoption in applied coding-reliability workflows.",2207.02855v1,https://arxiv.org/pdf/2207.02855v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:50:27Z
FALSE,Other,ML-based|Bayesian|Hybrid/Ensemble|Other,Mixture of types|Other,Not applicable,Semiconductor/electronics|Healthcare/medical|Network/cybersecurity|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://goo.gle/plex-code,"This paper defines “reliability” for pretrained vision and language models as strong, consistent performance across uncertainty quantification, robust generalization under distribution shifts, and data-efficient adaptation. It introduces Plex (ViT-Plex and T5-Plex), which extends large pretrained Transformers using scalable uncertainty methods—primarily BatchEnsemble plus last-layer approaches (Gaussian-process last layer and heteroscedastic logits)—to avoid task-specific scoring/tuning. The authors benchmark reliability using 10 task types across 40 datasets spanning vision and language, covering calibration, selective prediction, open-set recognition, label uncertainty, covariate/subpopulation shift, active learning, and few-shot learning. Empirically, Plex improves state-of-the-art across many of these reliability benchmarks and shows favorable scaling trends with model size (up to ~1B parameters) and pretraining dataset size (up to 4B examples). The work advances ML reliability evaluation and methods, but it is not reliability engineering in the classical life testing/maintenance sense.","Key formulations include (i) sequence open-set uncertainty for T5 as average conditional entropy: $U(y|x,\theta)= -\frac{1}{L}\sum_{l=1}^{L} H(y_l\mid y_{<l},x,\theta)$; (ii) label-uncertainty evaluation via expected divergence between empirical human label distributions and model predictive distributions: $\mathbb{E}_x[ D(p_{data}(y|x)\|p_{model}(y|x)) ]$ (using KL); and (iii) zero-shot OSR using Mahalanobis distance in embedding space: $MD(z)=\min_k (z-\mu_k)^\top\Sigma^{-1}(z-\mu_k)$ (and a relative Mahalanobis variant subtracting a background distance).","Plex is evaluated on 10 task types over 40 datasets and is reported to set new state-of-the-art on many reliability metrics while working “out-of-the-box” across tasks. Scaling studies show reliability scores increase with model size (ViT-Plex up to ~325M params; T5-Plex up to ~880M) and with larger pretraining datasets (vision up to JFT-4B), e.g., ImageNet 10-shot accuracy improves from 72.1%→73.0% (JFT-300M, 1M→8M steps) versus 72.7%→74.6% (JFT-4B, 1M→4M steps). For selective prediction on vision, Oracle Collaborative AUROC at 0.5% review budget reaches 0.98 on ImageNet for Plex L (and ≥0.94 across C10/C100/ImageNet). For open-set recognition using MSP, reported AUROCs include 0.997 (C10 vs SVHN) and up to 0.954 (C100 vs C10) for Plex L; for NaLUE OOS intent detection, AUROC reaches 0.999 (Standard-OOS) and ~0.890–0.917 (Near-OOS depending on method).","The authors note that many prior methods are specialized to narrow reliability tasks and that the literature lacks certain datasets (motivating new datasets like ImageNet ReaL-H and NaLUE). They also report that pretraining does not improve in-distribution performance for the RETINA diabetic retinopathy task, attributing this to the domain mismatch between retina scans and natural-image pretraining data. They mention compute constraints leading them to focus on finetuning-only for T5-Plex (initialized from official T5 checkpoints).","The proposed notion of “reliability” is ML-centric and differs from classical reliability engineering (failure-time/degradation/maintenance), so it may be mismatched for engineering-reliability use cases. Many results depend on large-scale pretraining data (e.g., JFT) and substantial compute, limiting reproducibility and accessibility despite code release. Benchmark aggregation into a single reliability score can obscure tradeoffs among metrics and tasks and may be sensitive to normalization/weighting choices. Several evaluations assume i.i.d. test examples and do not deeply address temporal dependence, deployment feedback loops, or post-deployment monitoring/maintenance common in real-world reliability practice.","The conclusion suggests applying Plex techniques broadly to other large models (e.g., LaMDA, PaLM) and to a wider suite of tasks (e.g., BIG-bench). It also emphasizes continued stress-testing of models across uncertainty, distribution shift, and adaptation settings as a path toward more generally reliable AI systems. (No highly specific algorithmic future-work agenda beyond these extensions is explicitly detailed in the provided text.)","Developing standardized, task-agnostic reliability reporting (including confidence intervals, calibration under shift with uncertainty in estimates, and robustness to dataset curation effects) would strengthen comparability. Extending Plex-style methods to settings with temporal drift, streaming data, and correlated samples (common in monitoring and dialogue) would improve deployment relevance. More ablations on sensitivity to hyperparameters, labeling noise levels, and computational budgets (training/inference latency and memory) would help practitioners choose variants. Providing packaged, reproducible implementations (e.g., pip installable) and more real-world case studies with operational constraints would increase practical impact.",2207.07411v1,https://arxiv.org/pdf/2207.07411v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:51:16Z
FALSE,Other,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only|Other,Not applicable,Other|Theoretical/simulation only,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://osf.io/674fk/|https://cran.r-project.org/package=IRR2FPR,"The paper links inter-rater reliability (IRR; ICC-based) from a simple additive measurement-error model for multi-rater scores to the expected error rates of a downstream binary selection decision (select top k applicants). Assuming normally distributed latent ability and normally distributed independent measurement errors, it proposes a quantile-based approximation of the joint event that an applicant is both truly in the top-k and selected based on noisy averaged ratings, computed via a bivariate normal integral. This yields closed relationships showing that key binary classification metrics (e.g., true positive rate, false positive rate, false negative rate, F1) depend only on the multi-rater IRR and the proportion selected. A simulation study evaluates approximation bias/RMSE under assumption violations (skewed latent ability, rating bias reversing some ratings, and rater-induced dependence), finding small bias under the normal model and larger errors particularly under dependence at low IRR/low numbers of raters. An applied example uses published grant peer-review IRR estimates to estimate implied lower bounds on false-positive/false-negative rates and explores how changing number of raters or IRR would affect these rates; computations are implemented in the IRR2FPR R package.","The measurement model is $y_{ij}=\gamma_i+\epsilon_{ij}$ with $\gamma_i\sim N(\mu,\sigma_\gamma^2)$ and $\epsilon_{ij}\sim N(0,\sigma_\epsilon^2)$. Single-rater IRR is $\mathrm{IRR}_1=\sigma_\gamma^2/(\sigma_\gamma^2+\sigma_\epsilon^2)$ and multi-rater reliability for the mean of $J$ ratings is $\mathrm{IRR}_J=\sigma_\gamma^2/(\sigma_\gamma^2+\sigma_\epsilon^2/J)=\frac{J\,\mathrm{IRR}_1}{J\,\mathrm{IRR}_1+1-\mathrm{IRR}_1}$. The true-positive probability for selecting the top $k$ is approximated as $P(S\cap A)\approx P(\gamma>\tilde g_c,\bar y>\tilde{\bar y}_c)$ where the cutoffs are normal-quantile based, and binary metrics follow (e.g., $\mathrm{FPR}=(k/N-P(S\cap A))/(k/N)$ and $\mathrm{FNR}=(k/N-P(S\cap A))/((N-k)/N)$).","In simulations, the quantile approximation showed little to no bias when data followed the assumed normal measurement model. Under violations, bias patterns depended on the mechanism: skewed latent abilities and rater-dependence induced systematic bias that decreased with more raters and higher IRR, while deliberate “bias” (reversing ratings for 10% of applicants) led to overestimation of true-positive probability (and thus underestimation of error rates). In the NIH (2014–16) example with $\mathrm{IRR}_1=0.34$, $J=2.79$, and 18% selected, the estimated false positive rate is 0.397 (95% CI [0.380, 0.416]) and false negative rate is 0.087 (95% CI [0.083, 0.091]). Across published grant peer-review settings, estimated false positive rates varied strongly with the selection proportion (e.g., very high when selecting 5% under low IRR), illustrating that error rates are driven by both IRR and the fraction selected.","The authors state the method evaluates selection procedures only with respect to reliability (measurement error), effectively assuming perfect validity; with lower validity, computed false positive rates should be interpreted as lower bounds on the true false positive rate. They note the approximation can be biased when model assumptions are violated (e.g., skewed latent abilities, rater-induced dependence, or broken rating–ability linkage), and that realized error rates for any single selection can deviate substantially from expectations when selecting a small proportion (high binomial noise). They also caution that real-world grant decisions may incorporate additional information beyond the overall score, and that published IRR/selection-rate summaries are often averages across disciplines/years.","The framework is tailored to a unidimensional latent “ability” and a strict top-$k$ selection rule; many real selection processes use committee discussions, tie-breaking, quotas, and multi-criteria utility, which can break the symmetry properties exploited (e.g., off-diagonal equalities). The normality/linearity assumptions and reliance on ICC may be fragile for ordinal ratings, heavy-tailed score distributions, and heterogeneous error variances; robustness to such departures is only partially explored. Estimating IRR from limited/biased samples (range restriction, missingness, unequal rater assignment) can substantially affect downstream error-rate estimates, but the paper does not provide a full propagation-of-uncertainty framework for these practical estimation issues beyond simple CI transformations. The bivariate-normal integral approach may be less transparent for practitioners without packaged software, and sensitivity to mis-specified $J$ (variable raters per applicant) could merit deeper treatment.","The authors suggest extending the approach by enriching the measurement model: allowing group-specific parameters (heterogeneous variance components), adding variance components to model dependencies such as rater effects, accommodating unequal numbers of ratings per applicant, and relaxing normality assumptions for ratings and latent abilities (including alternative or mixture distributions). They also propose incorporating utility functions that weight errors by severity (e.g., distance from the selection boundary) rather than using unweighted misclassification rates. Finally, they note applicability to other domains (educational testing, psychological and health measurement, admissions/hiring/promotion) as additional contexts for further development and use.","A useful extension would be a fully Bayesian or bootstrap-based uncertainty propagation that jointly accounts for estimation error in IRR, $J$ variability, and selection proportion uncertainty, producing posterior/interval estimates for FPR/FNR directly. Robust or semiparametric variants (e.g., copula-based dependence models, ordinal/probit measurement models, heavy-tailed errors) could improve applicability to non-Gaussian and ordinal rating scales common in practice. Generalizing from top-$k$ selection to threshold-based pass/fail, multi-stage selection, and constrained optimization (quotas, fairness constraints) would make the framework closer to real decision pipelines. Providing diagnostic checks and sensitivity analyses (e.g., how much non-validity would inflate FPR beyond the lower bound) would help practitioners interpret results responsibly.",2207.09101v3,https://arxiv.org/pdf/2207.09101v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:52:10Z
FALSE,NA,ML-based|Bayesian|Physics-based,Sensor/condition monitoring|Simulated only,Not applicable,Energy/utilities|Other,Simulation study|Other,TRUE,Julia|Other,Public repository (GitHub/GitLab),https://github.com,"The paper proposes a robust variant of amortized variational inference for high-dimensional Bayesian inverse problems using a pretrained conditional normalizing flow (CNF) to generate posterior samples at negligible marginal cost. To mitigate errors when test data are moderately out-of-distribution (e.g., changes in number of seismic sources, noise variance, or prior/geometry), it introduces a physics-based latent distribution correction that relaxes the standard Gaussian latent assumption to a Gaussian with unknown mean and diagonal covariance. The correction parameters are estimated by minimizing a KL-divergence-derived objective that includes a physics-informed data misfit term through the forward model composed with the CNF inverse, plus latent regularization and a log-determinant term. Experiments on linearized seismic imaging (Born modeling) show improved posterior sample quality and conditional mean SNR after correction (often by ~3–4 dB) and better data fit, at a computational cost comparable to about five reverse-time migrations. The method aims to preserve amortization benefits while adding a fast, data-specific, physics-informed adaptation step for distribution shifts.","A pretrained CNF defines the conditional density via change of variables: $p_\phi(x\mid y)=\mathcal N(f_\phi(x;y)\mid 0,I)\,\lvert\det \nabla_x f_\phi(x;y)\rvert$ and sampling uses $x=f_\phi^{-1}(z;y)$ with $z\sim\mathcal N(0,I)$. Under distribution shift, the latent is approximated as $z\sim\mathcal N(\mu,\mathrm{diag}(s)^2)$, where $(\mu,s)$ are found by minimizing $\mathbb E_{z\sim\mathcal N(0,I)}[\tfrac{1}{2\sigma^2}\sum_i\|y_{obs,i}-F_i\circ f_\phi^{-1}(s\odot z+\mu;y_{obs})\|_2^2+\tfrac12\|s\odot z+\mu\|_2^2-\log|\det\mathrm{diag}(s)|]$. Corrected posterior samples use $x=f_\phi^{-1}(s^*\odot z+\mu^*;y_{obs})$.","In a forward-model shift case (e.g., reducing sources to $N=25$ and increasing noise std by $2.5\times$), posterior sample SNR improves from roughly 4.6–5.2 dB (uncorrected) to about 7.8–8.5 dB (corrected), and the conditional-mean SNR improves from 6.29 dB to 10.36 dB (~+4 dB). Data-space prediction SNR for a representative shot increases from 11.62 dB (before correction) to 16.57 dB (after correction), indicating better data fit. For prior-shift experiments using deeper-section images, conditional-mean SNR improves by ~3–4 dB, and across 10 deeper images the mean±std SNR increases from 5.91±0.49 dB (uncorrected) to 9.15±0.73 dB (corrected). Posterior contraction behavior is demonstrated qualitatively: uncertainty (pointwise posterior std) decreases with more sources and lower noise.","The authors state that the diagonal (elementwise) latent correction may not handle all types of data distribution shifts, and that delineating which shifts are addressable by this simple correction is left to future work. They also note that handling broader shifts may require more complex latent transformations (e.g., a neural network), potentially increasing computational cost. They acknowledge that conditioning on reverse-time migration images (instead of full shot records) may bias the learned posterior and defer learned summary-statistic approaches to future work.","The approach assumes access to a differentiable forward model and adjoint/Jacobian actions for the correction step; for many real engineering workflows this may be unavailable, inaccurate, or expensive, limiting practicality. Robustness is demonstrated primarily on simulated, linearized seismic (Born) settings; performance under strongly nonlinear physics, model error, or real field data is not established. The latent correction is restricted to a Gaussian with diagonal covariance, which may be insufficient for correlated latent mismatches or multi-modal posterior distortions under larger shifts. Reported code availability is referenced generally, but without an explicit repository URL in the provided text, reproducibility and exact configurations may be hard to verify from the manuscript alone.","The authors propose future work to (1) delineate which types of data distribution shifts can be handled by their diagonal latent correction and when more complex latent transformations are needed, (2) develop/learn low-dimensional summary statistics for observed data (beyond RTM images) to reduce bias, and (3) extend uncertainty modeling to include background-model (velocity) errors and other modeling uncertainties beyond measurement noise, potentially leveraging Fourier neural operators for efficiency.","A valuable extension would be to evaluate the method on real (field) seismic datasets and under realistic modeling errors to quantify robustness beyond simulation. Developing a principled shift-detection or self-diagnostic criterion (e.g., monitoring latent KL or calibration metrics) could decide when correction is necessary and how strong it should be. Exploring richer but still efficient latent corrections (low-rank plus diagonal covariance, normalizing-flow correction in latent space, or Bayesian treatment of $(\mu,s)$) could improve adaptation while controlling cost. Providing a fully packaged implementation (e.g., a reproducible Julia environment with scripts/configs and benchmark cases) would materially improve adoption.",2207.11640v3,https://arxiv.org/pdf/2207.11640v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:53:01Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based|Other",Right-censored|Mixture of types|Other,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|None / Not applicable,Not provided,NA,"The paper studies reliability inference for a $K$-out-of-$N{:}G$ system whose i.i.d. component lifetimes follow a Weibull distribution, when data are collected under a generalized progressive hybrid censoring scheme (GPHCS) that guarantees at least $k$ observed failures and allows progressive removals. It derives the system reliability function $R_{NK}(t)$ and develops classical inference via maximum likelihood estimation (MLE) for the Weibull parameters and the system reliability, with numerical solution using Newton–Raphson due to nonlinear likelihood equations. Asymptotic confidence intervals for parameters and for $R_{NK}(t)$ are constructed using the observed Fisher information and the delta method. Bayesian estimation is also developed using independent Gamma priors for the Weibull parameters; posterior sampling and HPD credible intervals are obtained via Metropolis–Hastings MCMC with normal proposals. A Monte Carlo simulation (implemented in R) compares MLE vs Bayesian estimators under different censoring schemes, and a real dataset (aircraft air-conditioning system failure times) illustrates the method and shows HPD intervals typically shorter than asymptotic CIs.","Component model: Weibull $F(x;\alpha,\beta)=1-e^{-\beta x^{\alpha}}$, $f(x)=\alpha\beta x^{\alpha-1}e^{-\beta x^{\alpha}}$, reliability $R(x)=e^{-\beta x^{\alpha}}$. System reliability for $K$-out-of-$N{:}G$: $R_{NK}(t)=\sum_{i=K}^{N}\binom{N}{i}\big(1-F(t)\big)^i\big(F(t)\big)^{N-i}=\sum_{i=K}^{N}\binom{N}{i}e^{-i\beta t^{\alpha}}\big(1-e^{-\beta t^{\alpha}}\big)^{N-i}$. Under GPHCS the likelihood is $L\propto\prod_{i=1}^{D} f(x_i)\{1-F(x_i)\}^{R_i}\{1-F(T^*)\}^{R^*}$ leading to log-likelihood in (2.4) and MLE-based plug-in estimator $\hat R_{NK}(t)$ in (2.7); Bayesian posterior is proportional to (4.5) with Gamma priors and is sampled by M–H MCMC.","Simulation uses a 3-out-of-5 system with true $(\alpha,\beta)=(1.5,1)$ and evaluates $R_{3,5}(0.5)=0.8398$ over 10,000 generated GPHCS samples (R implementation) under three progressive censoring schemes. Reported patterns: increasing the guaranteed failures $k$ (or increasing $m$) generally decreases MSE and interval average length/width; Bayes estimators have smaller MSE than MLEs, and HPD intervals are typically shorter than asymptotic CIs. In the real-data illustration (air-conditioning system failure times), MLE and Bayes point estimates for $(\alpha,\beta)$ and $R_{5,3}(50)$ are close across censoring schemes, while HPD credible intervals are shorter than ACIs (Table 6).",None stated.,"The inference assumes i.i.d. Weibull component lifetimes and independence among components in the $K$-out-of-$N$ system, which may not hold in real engineered systems with shared environments, load-sharing, or common-cause failures. Interval estimation for the system reliability relies on large-sample asymptotic normality and the delta method, which can be inaccurate under heavy censoring or small effective sample sizes $D$ (a common setting for hybrid/progressive censoring). The Bayesian MCMC uses normal random-walk proposals for positive parameters (without explicit truncation details), which can cause inefficiency or implementation sensitivity; no convergence diagnostics or effective sample size reporting is highlighted in the provided excerpt. Comparisons focus on MLE vs Bayes under a specific prior choice (often centered at true values in simulation), so performance advantages may partly depend on informative prior specification rather than methodology alone.","The authors suggest extending the approach to other lifetime distributions (noting exponential, Rayleigh, and normal as special cases via fixed Weibull shape values) and to broader system structures such as $K$-out-of-$N{:}G$ balanced systems and shared-load $K$-out-of-$N$ systems.","Develop robust or semi-parametric versions that relax the Weibull assumption and assess sensitivity to model misspecification under GPHCS. Extend the framework to dependent components (common-cause failures, frailty/shared random effects, or copula dependence) which are common in real $K$-out-of-$N$ systems. Provide non-asymptotic interval methods (e.g., parametric bootstrap under censoring, Bayesian posterior predictive checks) for improved accuracy with small samples and heavy censoring. Release an R package or reproducible code and include systematic MCMC diagnostics and tuning guidance to facilitate practical adoption.",2207.12957v2,https://arxiv.org/pdf/2207.12957v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:53:40Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/google/vizier,"This paper introduces Open Source (OSS) Vizier, a Python-based blackbox and hyperparameter optimization service derived from Google’s internal Vizier infrastructure. The main contribution is a distributed, gRPC/protobuf-based client-server architecture (including a separate algorithm service, “Pythia”) designed to be fault-tolerant and to support asynchronous and parallel trial evaluations. The system API supports a wide range of optimization workflows (e.g., multi-objective optimization, early stopping, transfer learning, and conditional/structured search spaces) and can be accessed from clients written in any language via RPC. The paper emphasizes systems reliability in the distributed sense (operation persistence, retry/recovery, worker reattachment via client IDs), rather than reliability engineering of physical components. Code is released as an OSS package and repository, enabling practitioners to self-host and extend the service with custom optimization policies.",Not applicable,NA,"The authors state they cannot open-source the default algorithms used in Google Vizier and Cloud/Vertex Vizier due to proprietary/legal constraints, and the paper intentionally omits algorithm/benchmark discussion because it focuses on systems aspects. They also note OSS Vizier may be inefficient when objective evaluations are extremely cheap/fast (service overhead dominates), and may be inappropriate for very high-dimensional and very large-evaluation regimes (e.g., 100K+ parameters and 1M+ trials) due to datastore/memory burden from storing many trials.","The reliability claims (fault tolerance, recovery) are not quantitatively validated in the paper (e.g., no reported uptime metrics, failure-injection tests, recovery-time distributions, or scalability benchmarks under fault scenarios), so operational robustness is largely asserted rather than measured. The design assumes a trustworthy service operator and does not provide a complete access-control/privacy model; the paper notes potential ability for a user to list other users’ studies, indicating security hardening is incomplete. Performance overheads from RPC polling (operations + GetOperation) and datastore contention under heavy concurrency are not characterized, which can materially affect practical deployments. Finally, early-stopping and noise-handling behaviors are described at a high level but not evaluated for correctness/robustness across diverse objective types and failure modes.","The authors suggest that algorithms can be added over time as Pythia policies by contributors, expanding OSS Vizier’s collection of available optimization algorithms. They also imply future improvements around service logic for privacy/security—e.g., restricting users to access only their own studies—to address identified security and privacy concerns.","A valuable extension would be a formal reliability/availability evaluation of the distributed system (including chaos/failure-injection testing, recovery-time objectives, and scaling curves) to substantiate fault-tolerance claims. Adding first-class multi-tenant security primitives (authentication, authorization, per-study ACLs, audit logging, and encrypted-at-rest parameter/metric fields) would improve safe deployment outside trusted environments. Providing reference deployments (e.g., Kubernetes Helm charts) and observability packages (metrics, tracing, SLO dashboards) would help practitioners operate the service reliably at scale. Finally, publishing standardized performance benchmarks (latency/throughput vs. concurrent clients and trial volume) would clarify when the service architecture is beneficial versus local/in-process optimizers.",2207.13676v2,https://arxiv.org/pdf/2207.13676v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:54:04Z
FALSE,NA,Other,Other,Not applicable,Theoretical/simulation only,Other,NA,None / Not applicable,Not applicable (No code used),NA,"This paper develops a theoretical framework and data structures for dynamically maintaining kernel density estimation (KDE) under dataset updates (point replacements) and adaptive/adversarial queries. The proposed DynamicKDE combines importance sampling with locality sensitive hashing (LSH) over geometrically defined weight/ दूरी levels to support sublinear-time queries and updates with subquadratic space. The authors provide complexity bounds for initialization, update, and query operations as functions of accuracy $\epsilon$, dataset size $n$, an upper bound $f_{KDE}$ on the true KDE, and a kernel-dependent cost term $\mathrm{cost}(f)$. Robustness to adaptive/adversarial queries is obtained via repetition with median-of-estimates and an $\epsilon$-net argument extending guarantees from fixed queries to all queries in a bounded domain. The work is theoretical/methodological in machine learning/data structures rather than reliability engineering.","The target quantity is the KDE $f^*_{\mathrm{KDE}}(X,q)=\frac{1}{|X|}\sum_{x\in X} f(x,q)$, with the query required to return $\hat{d}$ satisfying $(1-\epsilon)f^*_{\mathrm{KDE}}(X,q)\le \hat{d}\le (1+\epsilon)f^*_{\mathrm{KDE}}(X,q)$. The estimator uses importance sampling: for recovered sampled points $x_i$ with weight $w_i=f(x_i,q)$ and sampling probability $p_i$, it forms $T=\sum_{x_i\in S} \frac{w_i}{p_i}$ and returns the median across repetitions. The kernel-dependent complexity is summarized via $\mathrm{cost}(f,r)=2^{\max_{i>r}\lceil (i-r)/(c_{i,r}^2(1-o(1)))\rceil}$ and $\mathrm{cost}(f)=\max_r \mathrm{cost}(f,r)$, where $c_{i,r}=\min\{z_{i-1}/z_r,\log^{1/7} n\}$ from geometric level sets.","The main theorem states the existence of a DynamicKDE data structure with space $O(\epsilon^{-2} n\,\mathrm{cost}(f))$ (up to polylog and $n^{o(1)}$ factors), supporting point-replacement updates and KDE queries in sublinear expected time (again up to $\epsilon^{-2}$, $\mathrm{cost}(f)$, $\log(1/f_{KDE})$, and $n^{o(1)}$ factors). Initialization time is given as $O(\epsilon^{-2} n^{1+o(1)}\mathrm{cost}(f)\cdot (1/f_{KDE})^{o(1)}\log(1/f_{KDE})\,\log^3 n)$, while update and query times are $O(\epsilon^{-2} n^{o(1)}\mathrm{cost}(f)\cdot (1/f_{KDE})^{o(1)}\log(1/f_{KDE})\,\log^3 n)$ in expectation. Robustness to adaptive/adversarial queries is achieved by repeating estimators and taking medians, then extending correctness from fixed queries to an $\epsilon$-net (size $\le (10/\epsilon_0)^d$) and to all points via Lipschitzness. Quantitative ARL-style or empirical benchmark results are not provided; results are asymptotic theoretical bounds.","The authors note an inherent trade-off between efficiency and effectiveness: improving execution speed may require a slight compromise on performance/accuracy in practice. They also state in the impact statement that implementation and experiments could contribute to environmental carbon emissions, motivating their focus on theoretical analysis rather than extensive experimentation.","The work is purely theoretical and does not include empirical validation on real datasets or practical benchmarks, so constant factors, real-world performance, and tuning complexity are unclear. The guarantees rely on assumptions about the kernel structure (e.g., radial/decreasing/Lipschitz in parts) and on having an external upper bound $f_{KDE}\ge f^*_{\mathrm{KDE}}$, which may be nontrivial to obtain tightly in practice. The dynamic operation is “replace the $i$-th point” (not explicit streaming insert/delete with changing $n$), and practical issues like memory overhead of multiple LSH instances and update contention are not evaluated.",None stated.,"A natural extension is empirical evaluation and open-source implementation to quantify practical speed/accuracy trade-offs and constant factors. Another direction is relaxing requirements such as needing a known upper bound $f_{KDE}$ and extending dynamics to true insertions/deletions (changing $n$) or to updating kernel parameters. Robustness analysis could also be extended to settings with correlated/high-dimensional data distributions, alternative LSH families, or to provide tighter, instance-dependent bounds.",2208.03915v2,https://arxiv.org/pdf/2208.03915v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:54:42Z
FALSE,NA,ML-based|Bayesian,Simulated only,Not applicable,Other,Simulation study,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/UncertaintyQuantification/ALEC,"This paper proposes ALEC (active learning with error control), a Gaussian-process-based surrogate modeling framework for reliable online emulation of expensive physics-based functionals with a user-specified probabilistic bound on prediction error. The method uses a parallel partial Gaussian process (PP-GP) emulator with Matérn 5/2 kernel and leverages its posterior predictive uncertainty to decide whether to accept an emulator prediction or trigger a high-fidelity numerical solver and augment training data. The authors derive criteria (maximum-variance and average-variance selection) that connect predictive variances from the GP to bounds on coordinate-wise error and RMSE with probability tolerance \(\alpha\). Computational efficiency is improved via an \(O(n^2)\) sequential Cholesky update when new training points are added, avoiding repeated \(O(n^3)\) factorizations. Numerical experiments emulating classical density functional theory (cDFT) for 1D hard-rod systems across multiple classes of external potentials show substantially lower density and grand-potential RMSE than space-filling/random designs and a D-optimality active-learning baseline at similar solver budgets.","The GP/PP-GP uses a Matérn-5/2 kernel \(K(x_a,x_b)=\left(1+\sqrt{5}d/\gamma+5d^2/(3\gamma^2)\right)\exp(-\sqrt{5}d/\gamma)\) with \(d=\|x_a-x_b\|\). The predictive distribution for each output coordinate is Student-t: \(\rho_j(x^*)\mid \hat\gamma,\rho_j\sim t_{n-q}(\hat\rho_j(x^*),\,\hat\sigma_j^2 K_{**})\), with \(\hat\rho_j(x^*)=h(x^*)\hat\theta_j+r(x^*)^T R^{-1}(\rho_j-H\hat\theta_j)\). Error-control accepts prediction when the (average-variance) criterion \(\delta^*=\sqrt{\frac{1}{k}\sum_{j=1}^k \hat\sigma_j^2 K_{**}}\le \delta/t_{\alpha/2}(n-q)\) (or maximum-variance analogue) holds, otherwise run the numerical solver and update the emulator.","Across four classes of external potentials for 1D hard-rod cDFT emulation, ALEC achieves the smallest out-of-sample RMSE for both density profiles and grand potential compared with random sampling (RS) and D-optimality active learning at comparable total solver evaluations. With \(\delta=0.01\) and \(\alpha=0.05\), the reported augmented samples (added solver runs) for groups 1–4 when modeled separately are 0, 7, 21, and 504, respectively; for the combined ‘all groups’ experiment ALEC starts with 80 initial runs and adds 873 augmented samples (mostly from the hardest group). The paper also reports that a simple input-decomposition variant reduces augmented samples from 873 to 779 while attaining similar density RMSE (reported as \(9.73\times 10^{-4}\)). Timing comparisons show GP training/prediction overhead is negligible relative to numerical solving, yielding large speedups versus solving every test case directly.","The authors note that the average-variance selection criterion does not guarantee the same strong coordinate-wise error control as the maximum-variance criterion, and that a balance between accuracy targets (\(\delta,\alpha\)) and the number of solver calls is required. They also acknowledge that when model discrepancy is large (e.g., predicting a new class of external potentials), uncertainty quantification can be harder and the empirical exceedance rate may surpass \(\alpha\) in some cases, motivating use of the more conservative maximum-variance criterion. They mention that kernel hyperparameter re-estimation frequency depends on computational budget and that the current re-training schedule is a practical compromise.","The theoretical error-control relies on the GP predictive distribution being well-calibrated; misspecification (e.g., nonstationarity across disparate functional classes, kernel misfit, or dependence across spatial grid points ignored by PP-GP) can lead to undercoverage and violate the intended probabilistic guarantees. The approach is evaluated on a 1D hard-rod cDFT setting with known/controlled numerical solver behavior; performance and calibration in higher-dimensional (2D/3D) settings or with noisy/stochastic simulators may be substantially more challenging. Comparisons are limited to RS and a specific D-opt criterion; other strong baselines (e.g., entropy/integrated variance reduction, batch acquisition, deep GPs, Bayesian neural surrogates, or conformal calibration) are not benchmarked. Practical deployment would benefit from automated selection of \(\delta,\alpha\) and monitoring/calibration diagnostics, which are not fully developed.","The authors propose extending ALEC beyond PP-GP to other surrogate models that provide uncertainty quantification, and incorporating physical symmetries via improved descriptors/distance metrics (e.g., translational invariance using pairwise distances). They suggest integrating known physics-based functionals as emulator mean functions, and using dimension-reduction/orthogonal representations to reduce computational cost and improve accuracy for higher-dimensional coordinates. They also propose jointly modeling grand potential energy and densities rather than computing energy only by plugging predicted densities into functionals.","Developing formal calibration checks (e.g., coverage tests) and post-hoc calibration methods (e.g., conformal prediction or Bayesian calibration) could strengthen the reliability claims when the GP is misspecified or when predicting out-of-distribution functional classes. Extending ALEC to correlated multivariate outputs (modeling cross-grid covariance) or to stochastic/noisy simulators would broaden applicability to more realistic engineering simulation pipelines. Batch/parallel acquisition rules could make ALEC more efficient on HPC resources where multiple solver runs can be scheduled simultaneously. Robustness studies under kernel misspecification and nonstationary behavior, along with adaptive kernel/metric learning for functional inputs, would improve generalization and reduce the need for many augmented samples in hard regions.",2208.06727v3,https://arxiv.org/pdf/2208.06727v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:55:32Z
FALSE,NA,Bayesian|Simulation-based|Other,Mixture of types|Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1037987/Vaccine-surveillance-report-week-48.pdf|https://www.medrxiv.org/content/10.1101/2022.04.01.22273281v1.full-text|https://twitter.com/resist_05/status/1511895388185571328|https://ssrn.com/abstract=4125239|https://elifesciences.org/articles/61722,"This paper studies whether post-deployment (observational) monitoring methods for vaccine effectiveness—crude vaccinated vs. unvaccinated infection rates and the test-negative case-control (TNCC) design—can reliably infer vaccine effectiveness when healthcare-seeking/testing behavior is not precisely known. The author builds a Bayesian generative model with latent population subgroups that differ only in unobserved healthcare-seeking behavior, with Beta priors over vaccination probability, infection probability, and hospitalization/testing probability conditional on subgroup, infection, and vaccination. Using Monte Carlo simulation and Gibbs-sampling MCMC to obtain posteriors for a log-odds-ratio effectiveness measure $T_0$, the paper shows that with wide/uninformative priors the posterior credible intervals for $T_0$ are typically very wide (on the order of several nats), making the observational data largely uninformative even when the population is homogeneous or nearly so. Under some tight, informative priors, either TNCC or the crude estimator can be substantially better, but which one dominates depends on prior assumptions about behavior, so neither method is reliably superior without strong external information. Applying the model to partial real-world hospital testing data (2021, Alpha variant) yields similarly wide posteriors, motivating recommendations for randomized community testing or ongoing randomized controlled trials for reliable monitoring.","Vaccine effectiveness is represented as $T_0=\log\left(\frac{P(L\mid V)}{1-P(L\mid V)}\Big/\frac{P(L\mid v)}{1-P(L\mid v)}\right)=\log\left(\frac{j_1}{1-j_1}\Big/\frac{j_0}{1-j_0}\right)$ where $j_v=P(L\mid v)$. The TNCC estimator is $T_1=\log\left(\frac{\#LHV}{\#LHv}\Big/\frac{\#lHV}{\#lHv}\right)$ and the crude estimator is $T_2=\log\left(\frac{\#LHV/\#V}{(1-\#LHV/\#V)}\Big/\frac{\#LHv/\#v}{(1-\#LHv/\#v)}\right)$. A Bayesian generative model assigns Beta priors to subgroup proportion $p$, vaccination propensities $r_s$, infection risks $j_v$, and hospitalization/testing probabilities $q_{s,l,v}$, with posterior inference on $T_0$ via Gibbs-sampling MCMC.","With wide-open (uniform) priors and simulated data at $N=1000$, TNCC and crude estimates have similar error (SD about 1.29 nats for $T_1$ vs 1.35 nats for $T_2$), while the average 95% posterior credible interval width for $T_0$ is about 3.8 nats (roughly a 55-fold uncertainty in odds-ratio scale). Increasing to $N=10000$ reduces point-estimator SDs somewhat (reported SD 0.94 nats for $T_1$ and 1.6 nats for $T_2$) but still leaves wide posteriors (average width about 3.4 nats). Under a tight prior favoring TNCC (Prior 1), $T_1$ is much better (SD 0.44 nats vs 1.96 nats for $T_2$ at $N=1000$; SD 0.16 vs 1.9 at $N=10000$) with narrower posteriors (about 1.1 nats and 0.6 nats respectively), while another tight prior (Prior 3) makes TNCC much worse (SD 2.4 nats for $T_1$ vs 0.71 for $T_2$). On real-world hospital-tested data ($N=127{,}820$), the 95% posterior interval for $T_0$ remains very wide (>5 nats across assumptions about unobserved population vaccination), implying effectiveness cannot be determined usefully from the available observational data.","The author explicitly notes strong simplifying assumptions: the population is modeled as either homogeneous or split into only two homogeneous subgroups differing solely in unobserved healthcare-seeking behavior, and “being tested” is assumed synonymous with being hospitalized. The paper also notes that in real-world data the vaccination status of the entire population is not observed, requiring additional assumptions about the unseen population for the illustrative application. These simplifications are acknowledged as “gross” but used to argue that if inference is uncertain even here, it will likely be uncertain in reality.","The work is not a reliability-engineering study and does not address time-to-event reliability questions; it analyzes cross-sectional observational bias in vaccine-effectiveness monitoring, so generalization to longitudinal effectiveness or survival endpoints is not covered. The model assumes conditional independence structures (e.g., vaccination then infection then hospitalization) and binary variables (vaccinated/unvaccinated, infected/not, hospitalized/not), which may miss key confounders (age, comorbidities, prior infection) and temporal dynamics that TNCC studies typically adjust for via regression and stratification. Implementation details for the MCMC (diagnostics, mixing, sensitivity to prior hyperparameters beyond the few scenarios, and robustness to model misspecification) are limited for full reproducibility without shared code.","The paper proposes practical ways forward for reliable monitoring: (a) strong information on healthcare-seeking/testing behavior, (b) collecting tests from a random sample of the population independent of vaccination status and perceived health, or (c) conducting ongoing randomized controlled trials (RCTs) to monitor effectiveness and side effects post-deployment. It favors ongoing double-blind randomized placebo-controlled trials and emphasizes rigorous follow-up and independence from commercial/political interests.","A natural extension would be to relax the binary hospitalization/testing assumption and model multiple testing pathways (community testing, outpatient care, hospitalization) with partial observability, and to incorporate measured covariates via hierarchical logistic regression within the Bayesian model to mirror real TNCC practice. Additional work could systematically quantify sensitivity to prior misspecification (prior-data conflict checks) and perform simulation benchmarks against standard TNCC estimators with covariate adjustment under realistic epidemic dynamics and waning immunity. Releasing a reproducible implementation (e.g., Stan/PyMC) with documented diagnostics would also enable independent validation and broader comparative studies.",2208.09431v4,https://arxiv.org/pdf/2208.09431v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:56:21Z
FALSE,NA,ML-based|Other,Simulated only,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/montefiore-ai/balanced-nre,"This paper proposes Balanced Neural Ratio Estimation (BNRE), a modification of Neural Ratio Estimation for simulation-based inference that encourages conservative (non-overconfident) posterior approximations under small simulation budgets. BNRE adds a balancing constraint on the underlying binary classifier used for ratio estimation, implemented as a quadratic regularization term added to the standard binary cross-entropy objective, while preserving the same Bayes-optimal solution as standard NRE. The authors analyze the balancing property theoretically, showing (in expectation) that balanced classifiers tend to be less confident than the Bayes-optimal classifier, motivating improved posterior coverage behavior. Empirically, they evaluate expected coverage across several SBI benchmarks and simulation budgets, finding BNRE produces conservative expected-coverage curves across tested settings, at the cost of some information gain that diminishes as simulation budget increases. They also study the effect of the balancing strength hyperparameter and show overly large values can cause degeneration toward the prior due to Monte Carlo noise in the balancing estimate.","NRE uses a classifier $\hat d(\vartheta,x)$ trained to distinguish joint samples $(\vartheta,x)\sim p(\vartheta,x)$ from independent samples $(\vartheta,x)\sim p(\vartheta)p(x)$, with Bayes-optimal $d(\vartheta,x)=\frac{p(\vartheta,x)}{p(\vartheta,x)+p(\vartheta)p(x)}=\sigma(\log r(x\mid\vartheta))$. BNRE enforces the balancing condition $\mathbb{E}_{p(\vartheta,x)}[\hat d]+\mathbb{E}_{p(\vartheta)p(x)}[\hat d]=1$ via the regularized objective $\mathcal L_b[\hat d]=\mathcal L_{\text{BCE}}[\hat d]+\lambda\big(\mathbb{E}_{p(\vartheta)p(x)}[\hat d]+\mathbb{E}_{p(\vartheta,x)}[\hat d]-1\big)^2$. The posterior surrogate is then formed through ratio estimation (via logit): $\log \hat r(x\mid\vartheta)=\text{logit}(\hat d(\vartheta,x))$ and $\log \hat p(\vartheta\mid x)=\log p(\vartheta)+\log \hat r(x\mid\vartheta)$.","Across multiple SBI benchmarks and simulation budgets from $2^{10}=1024$ to $2^{17}=131{,}072$ simulations, BNRE yields expected-coverage curves consistently at or above the diagonal (conservative), whereas NRE often falls below the diagonal (overconfident) for small budgets. Using the paper’s “coverage AUC” summary (signed area between the expected-coverage curve and the diagonal), BNRE remains positive across tested settings while NRE can be negative or positive depending on budget/problem. The authors report that enforcing balancing (default $\lambda=100$) can reduce statistical performance (expected approximate log posterior density) relative to NRE, but the gap shrinks as simulation budget increases and BNRE converges toward NRE behavior. A hyperparameter study shows increasing $\lambda$ increases conservativeness/dispersion; too-large $\lambda$ (around $\gtrsim 1000$ in their example) can make the classifier degenerate toward $\hat d\approx 0.5$, collapsing the posterior toward the prior.","The authors note the theoretical conservativeness results (Theorems 1–2) hold only in expectation, so BNRE cannot guarantee conservativeness for any single inference instance. They also state the balancing condition is enforced via a Monte Carlo–estimated regularization penalty, so the classifier is rarely exactly balanced and theorems may not strictly apply. Finally, they highlight that benefits remain unassessed in high-dimensional parameter spaces, and their expected-coverage computation relies on discretized grids that become impractical in high dimensions.","The work’s main evaluation metric (expected coverage) is computed using discretized grids and empirical normalization; this procedure can introduce approximation error even in moderate dimensions and may confound method-vs-evaluation effects. Comparisons are primarily against standard NRE; broader baselines (e.g., other calibration/regularization approaches, different classifier-calibration techniques, or alternative SBI families) would better isolate when BNRE is preferable. The approach depends on tuning $\lambda$ and is sensitive to Monte Carlo noise; practical guidance for selecting $\lambda$ in a fully automated way (without extensive coverage evaluation) is limited, especially when coverage diagnostics are expensive.","The authors suggest applying the balancing condition to other SBI algorithms, including a generalization to neural posterior estimation (NPE) by extracting an implied ratio and enforcing balancing on the corresponding classifier form. They also propose applying the balancing condition in sequential inference settings to improve reliability beyond amortized inference. Additionally, they note their theoretical results can extend beyond SBI to generic high-risk binary classification tasks by redefining the two class distributions in the balancing condition.","A natural extension is to develop self-tuning or adaptive schemes for selecting $\lambda$ (e.g., based on online estimates of imbalance or calibration surrogates) to reduce manual tuning and avoid degeneration to the prior. Extending BNRE to settings with dependent/autocorrelated simulation outputs, misspecified priors, or nuisance-parameter marginalization could clarify robustness in realistic pipelines. Finally, providing scalable approximations to expected coverage (e.g., via sampling-based HPD approximations, variational credible sets, or learned region estimators) would make reliability evaluation feasible in higher-dimensional parameter spaces where grid methods fail.",2208.13624v1,https://arxiv.org/pdf/2208.13624v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:57:01Z
TRUE,RUL prediction|Maintenance optimization|Other,Nonparametric/Semi-parametric|ML-based|Simulation-based,Complete lifetime data|Sensor/condition monitoring|Mixture of types,Condition-based|Predictive,Transportation/logistics,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/mansurarief/optimized-safety-aware-aircraft-maintenance,"The paper proposes an AI-assisted, safety-aware predictive maintenance framework for aircraft turbofan engines that combines survival-analysis–based prognostics with simulation-based optimization. Engine health is summarized through a Cox proportional-hazards model fit on sensor measurements and operating settings, producing a (log) partial hazard score used as an airworthiness proxy. A hazard threshold is then optimized by bootstrapped simulation to minimize expected maintenance cost while quantifying the resulting probability of engine failure as a safety metric. Numerical experiments on NASA’s C-MAPSS turbofan dataset show that tailoring thresholds to operating conditions/failure modes yields lower costs than a single generic strategy, with reported cost reductions of 37% (combined dataset) and 40–50% (partitioned datasets), and about 13% additional savings from directed (cluster-specific) strategies. The framework is positioned as general and extensible to other safety-critical transportation systems.","The prognostic model is a Cox proportional hazards model: $h(t\mid x_t)=h_0(t)\,\hat h(x_t)$ with partial hazard $\hat h(x_t)=\exp(\beta^\top(x_t-\bar x))$, and the charting/decision score is $\log\hat h(x_t)$. Maintenance is triggered by a hazard-threshold rule $y_t(\lambda)=\mathbf{1}[\log\hat h(x_t)\ge \lambda]$ (equivalently, fly only if $\log\hat h(x_t)<\lambda$). Total cost for threshold $\lambda$ is modeled as $\sum_i C_1\,\mathbf{1}[\max_t \log\hat h(x_t)\ge \lambda]+\sum_i C_2\,\mathbf{1}[\max_t \log\hat h(x_t)<\lambda]$ with $C_2>C_1$, and the optimal threshold is $\lambda^*=\arg\min_\lambda \text{TotalCost}(\lambda)$; failure probability is estimated as $\frac{1}{n}\sum_i \mathbf{1}[\max_t \log\hat h(x_t)<\lambda]$.","Using C-MAPSS data and costs $C_1=3.5\times 10^6$ (performance restoration) and $C_2=4\times 10^6$ (LLP replacement), the combined-dataset optimal threshold is reported as $\lambda^*=9.0$, yielding an estimated failure probability of about 0.28 and an optimized total cost around US$76M, corresponding to ~37% cost reduction versus their baseline. For partitioned datasets, reported cost-minimizing thresholds are $\lambda^*\approx 17.5$ (FD001), $10$ (FD002), $22$ (FD003), and $9$ (FD004), with cost reductions of about 40–50% under the optimized strategy. The corresponding “profit-optimized” failure probabilities are reported as 0.28 (FD001), 0.15 (FD002), 0.22 (FD003), and 0.55 (FD004), highlighting a potentially unsafe operating point for FD004 unless a more conservative (lower) threshold is chosen. The paper reports that directed (clustered) strategies provide ~13% more cost savings than a generic strategy.","The authors note that the hazard-score estimates can be very noisy and therefore apply post-processing and simulation/bootstrapping to quantify uncertainty and improve robustness. They also note that, in real-world settings, it may be difficult for an airline to classify engines a priori by failure mode and operating conditions, which complicates applying cluster-specific (directed) strategies. They further mention their cost parameters are treated as fixed constants (though extendable to variable costs).","The maintenance decision rule is based on a maximum hazard exceedance over the observed trajectory, which may be overly sensitive to transient sensor noise and may inflate preventive actions without explicit smoothing/false-alarm control beyond bootstrapping. The Cox PH model assumes proportional hazards and typically independent observations; engine time-series covariates and operational profiles may violate these assumptions (e.g., autocorrelation seeps into $x_t$), affecting calibration of failure probabilities. The cost model omits key operational realities (downtime/dispatch constraints, parts and labor capacity, lead times, flight cancellation penalties, imperfect maintenance effects, and multiple maintenance actions), which could materially change the optimal threshold. Validation is limited to simulated C-MAPSS data; external validity to real airline fleets with different censoring patterns, maintenance records, and covariate shift is not demonstrated.","The paper suggests extending the framework with more advanced machine learning and AI methods to improve robustness and applicability for more complex maintenance strategies. It also states the general framework can be extended to other intelligent, safety-critical transportation systems beyond aircraft engines.","A self-starting/online updating version (with concept drift handling) could adapt $\beta$ and thresholds as fleet conditions and operating regimes change, and could incorporate covariate-shift detection. Extending the framework to explicitly handle censoring and partial observation (e.g., engines removed before failure, shop-visit censoring) would improve realism and statistical validity. Incorporating richer optimization (e.g., constrained optimization with a maximum acceptable failure probability, or multi-objective Pareto frontiers for cost vs. safety) and operational constraints (inventory/crew capacity, downtime, schedule disruption) would make recommendations deployable. Comparative benchmarking against alternative prognostic models (e.g., deep RUL models, recurrent survival models, or non-PH survival methods) and reporting calibration metrics for predicted failure risk would strengthen evidence for the chosen modeling approach.",2209.02678v1,https://arxiv.org/pdf/2209.02678v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:57:46Z
TRUE,Maintenance optimization|System reliability|Failure mode analysis|Other,Bayesian|Other,Mixture of types|Simulated only|Other,Condition-based|Imperfect maintenance|Not applicable|Other,Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/xwyu2021,"This preprint introduces chain event graphs (CEGs) as a probabilistic graphical modeling framework for reliability analysis, emphasizing their ability to represent asymmetric failure/deterioration pathways more naturally than Bayesian networks. It formalizes a new class of interventions—remedial interventions—intended to model remedial maintenance that targets and fixes root causes, returning components toward an “as good as new” (AGAN) state. The paper defines perfect, imperfect, and uncertain remedial interventions and develops bespoke causal algebras for representing these interventions on CEGs via “stochastic manipulations” of floret (local) transition probabilities at root-cause positions. It adapts a back-door identification theorem to establish when causal effects of these stochastic manipulations are identifiable from partially observed reliability processes. A simulation study demonstrates Bayesian learning of staged/CEG structure and parameters and how predicted failure probabilities change under an example remedial manipulation.","The paper defines remedial-maintenance indicators via a post-maintenance status variable $\delta\in\{0,1\}$ (AGAN if $\delta=1$) and root-cause intervention indicators $I_e\in\{0,1\}$ for root-cause edges $e\in E_\Delta$ (Eqs. 3.1–3.2), yielding a general mixture form for inferring root-cause fixes under imperfect/uncertain maintenance (Eq. 3.5). A stochastic manipulation updates floret probabilities at intervened positions $w^*$ through a map $\Gamma: \theta_w\mapsto \hat\theta_w$ (Eq. 3.6; Def. 3.3), inducing post-intervention path probabilities (Eq. 3.7) and a manipulated/conditioned CEG with transition probabilities computed from path probabilities (Eq. 3.8; Def. 3.4). The causal query for a remedial intervention is expressed as $\pi(\Lambda_y\Vert \hat\theta_{w^*})$ (Eq. 4.1) and identified via an adapted back-door formula $\pi(\Lambda_y\Vert \hat\theta_{w^*})=\sum_x\sum_z \pi_{\Lambda(w^*)}(\Lambda_y\mid \Lambda_x,\Lambda_z)\,\pi_{\Lambda(w^*)}(\Lambda_z)\,\hat\pi_{\Lambda(w^*)}(\Lambda_x)$ (Eq. 4.12).","In the simulation study (synthetic dataset of 5000 cases; 2698 failures), a Bayesian learning procedure (Dirichlet priors, Metropolis–Hastings for parameters, MAP selection for staging) is used to learn a best-scoring staged tree/CEG consistent with the data. Under an example remedial manipulation, the posterior distribution of a key root-cause transition probability $\theta_{w_1,w_3}$ shifts left: its posterior mean decreases from 0.503 (pre-intervention) to 0.492 (post-intervention). The learned model is then used to generate predictive distributions of endogenous failure probabilities under different root causes, illustrating how remedial actions change predicted failure risk. The paper’s main theoretical result is the identification condition and adapted back-door theorem for stochastic manipulations on CEGs (Prop. 4.2; Thms. 4.4–4.5), providing graphical/criteria-based identifiability for remedial interventions under partial observability.","The paper notes that real engineer reports in the motivating domain are sensitive, so the empirical demonstration is performed on simulated data rather than real maintenance/failure logs. It also comments that the original motivating domain did not include cases where faults provoke system upgrades that reduce failure rate over the entire subsequent lifetime, implying the presented remedial-intervention formalism (restoring root causes toward AGAN) does not cover those more complex improvement scenarios. It further notes (in the simulation section) that for simplicity symptoms are not considered in the illustrated causal CEG analysis.","The remedial-intervention framework relies on strong structural assumptions: the event tree/CEG is assumed to be “faithfully constructed” so that all relevant failure/deterioration processes and root causes are represented, which may be unrealistic in complex assets with unmodeled mechanisms. Identification and inference presume that the mapping from observed maintenance and partial failure paths to intervened root-cause positions (via the proposed mixture model for unobserved maintenance) is correctly specified; misspecification could bias inferred causal effects. The simulation-based validation is narrow relative to real-world reliability settings (e.g., censoring, recurrent events with calendar time, covariates, and sensor streams), and it does not benchmark against alternative causal-maintenance modeling approaches (e.g., BN/DBN with intervention policies, structural nested models, or marked point processes). Practical implementation may be nontrivial for large-scale systems because staged-tree/CEG structure learning and MCMC can be computationally intensive and sensitive to prior/score choices.","The paper suggests extending the approach to interventions that upgrade a system (reducing failure rate across the entire post-maintenance lifetime), which would require more complex manipulations than restoring root causes to AGAN. It proposes using dynamic CEGs to model semi-Markov processes and manipulate failure rates directly, enabling generalization of the algebras to time-to-event behavior. It also suggests adapting the framework to natural language extracted from maintenance logs, using NLP methods to map free text into d-events to support automated causal learning of CEG structures and posterior floret probabilities.","A useful extension would be to validate the remedial-intervention identification and inference on real industrial datasets (with appropriate anonymization), including recurrent failures and right-censoring, to assess robustness under realistic data issues. Developing robust/self-starting variants that tolerate missing/unknown root-cause vocabularies and model uncertainty about the event-tree topology (not just staging) would improve applicability. Incorporating covariates and condition-monitoring signals (e.g., via dynamic CEGs or hierarchical models) could connect the approach to predictive maintenance and RUL estimation. Finally, providing a standardized, open-source implementation (beyond a research repo) with reproducible pipelines for learning, intervention specification, and identifiability checks would lower adoption barriers for reliability engineers.",2209.06704v2,https://arxiv.org/pdf/2209.06704v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:58:37Z
TRUE,Degradation modeling|Other,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Energy/utilities|Other,Other,TRUE,None / Not applicable,Not provided,NA,"The paper applies a neural-network model to predict NOx emissions from a degrading natural-gas turbine using nine operating/process variables (e.g., TIT, TET, CDP, TEP, TEY, ambient conditions). A key finding is that for a degrading system, the train/validation/test split must include recent data (or be stratified across years) so the model reflects degradation effects; a chronological split (2011–2013 train, 2014 validation, 2015 test) performs poorly. With year-stratified sampling (60%/20%/20% across 2011–2015), the model achieves consistently high predictive fit across training/validation/test and residual plots show no obvious structure. The study also ranks variable importance (TIT most important, followed by TEY and TEP) and uses a prediction profiler to identify operating settings that minimize NOx (reported around ~22.3 mg/m^3). Overall, the contribution is a practical ML modeling strategy for emission prediction under equipment aging rather than a formal reliability/maintenance optimization model.","The proposed predictor is a two-layer neural network with mixed activation functions: each layer includes three TanH units plus one linear and one Gaussian unit (architecture reported as NTanH(3)NLinear(1)NGaussian(1)NTanH2(3)NLinear2(1)NGaussian2(1)). Model adequacy is assessed primarily via $R^2$ on training/validation/test splits and diagnostic plots (actual-vs-predicted and residuals), rather than closed-form reliability metrics.","Using a chronological split (train 2011–2013, validate 2014, test 2015), performance degrades substantially with $R^2$ about 0.72 (train), 0.35 (validation), and 0.05 (test), which the authors attribute to turbine degradation over time. Using a year-stratified split (60% train/20% validation/20% test spanning 2011–2015), performance improves to approximately $R^2$ = 0.82 (train), 0.81 (validation), and 0.82 (test), indicating better generalization. Variable-importance ranking identifies TIT as most influential, followed by TEY, TEP, TET, AT, and CDP; AH, AP, and AFDP have minimal influence. The prediction profiler indicates a minimum NOx around ~22.3 mg/m^3 under the model-derived operating settings.",None stated.,"The work treats “degradation” implicitly via data-splitting strategy rather than explicitly modeling degradation state, drift, or nonstationarity (e.g., time-varying parameters), so extrapolation to future unseen degradation regimes may still fail. It reports $R^2$ and visual diagnostics but does not provide uncertainty quantification, prediction intervals, or error metrics tailored to operational decision-making under changing conditions. The study does not specify the software/tool used, hyperparameter tuning procedure, or reproducibility details, and no code is shared. It also does not connect the emission predictor to reliability outcomes (failure risk, RUL, or maintenance actions), limiting direct reliability-engineering applicability.",None stated.,"Extend the approach to explicitly handle degradation/nonstationarity (e.g., time-aware models, online learning, or including time/health indicators) and quantify predictive uncertainty for operational use. Validate the method on additional turbines/sites and assess robustness under sensor noise, missing data, and changing operating regimes. Couple the NOx prediction model with condition-based control/maintenance optimization (e.g., constrained optimization balancing emissions, efficiency, and component life) and compare against simpler baselines (linear/regularized regression, tree ensembles) under the same temporal validation protocol.",2209.09168v1,https://arxiv.org/pdf/2209.09168v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:59:07Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based",Right-censored|Mixture of types|Other,Not applicable,Manufacturing (general)|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a statistical framework to compare the reliability of units produced by different production lines using joint progressive Type-II censoring (JPROG-II-C) in simultaneous life tests of multiple independent samples. Lifetimes from each production line are modeled with the Weibull–Gamma distribution, and line-specific reliability functions are estimated and compared at a fixed time via invariance properties of MLEs (and an ordering result based on the estimated shape parameter). Frequentist inference is developed using MLEs (computed numerically via Newton–Raphson), asymptotic confidence intervals from the observed Fisher information, and four parametric bootstrap confidence intervals (percentile, t, BC, BCa). Bayesian inference is also provided using independent gamma priors on parameters and MCMC (Metropolis-within-Gibbs) to compute Bayes estimates under squared-error and LINEX loss and credible intervals. The method is demonstrated with simulation studies (including expected-vs-observed number of failures under JPROG-II-C) and a real dataset (jute fiber breaking strengths at two gauge lengths) showing Weibull–Gamma provides an adequate fit and enabling reliability comparison between the two groups/lines.","Weibull–Gamma reliability (survival) for line $h$ is $R_h(t)=(1+(t/\alpha_h)^{\theta_h})^{-\beta_h}$ with PDF $f_h(t)=\frac{\theta_h\beta_h}{\alpha_h^{\theta_h}}t^{\theta_h-1}(1+(t/\alpha_h)^{\theta_h})^{-(\beta_h+1)}$. Under JPROG-II-C, the likelihood combines contributions of observed failures and removals at each failure time (general form in Eq. (4), specialized for WG in Eq. (5)); MLEs solve nonlinear score equations (Eqs. (6)–(8)) and are obtained numerically. For comparing two lines $i$ and $j$, the estimated reliability at time $t$ is $\hat R_i(t)=(1+t)^{-\hat\beta_i}$ after the paper’s scaling transformation, and if $\hat\beta_i\le\hat\beta_j$ then $R_i(t)\ge R_j(t)$ for all $t$ (Corollary 1).","The paper reports simulation results (1000 generated JPROG-II-C samples for $k=2$ lines under multiple censoring schemes) showing the approximation for the expected number of failures before the test (A.E.B) is typically close to the mean exact number of failures observed after the test (M.E.A), especially for larger total failures $r$. In a worked simulated example ($r=10$), MLE and Bayesian estimates (MCMC with 52,000 iterations and 2,000 burn-in) are close, and the reliability ordering is concluded via $\beta_2<\beta_1$ implying line 2 is more reliable. In the real-data example (jute fiber breaking strengths at 10mm vs 20mm), Weibull–Gamma fit is supported by K–S distances 0.105828 (p=0.855287) and 0.149029 (p=0.473037), and MLE/Bayes estimates are again similar with multiple bootstrap CI variants provided. The paper provides extensive 95% interval estimates (ACI and four bootstrap CIs plus Bayesian CRIs) for all WG parameters for both lines in both examples.","The authors note the approach is parametric and recommend applying it to datasets with strong fit to the Weibull–Gamma distribution, assessed via p-values and Kolmogorov–Smirnov distance. They also state results become more reliable when the selected sample sizes are relatively large. No other explicit limitations are stated beyond these applicability conditions.","The approach assumes iid lifetimes within each line and independence between lines; in manufacturing settings, common-cause variation and temporal dependence (e.g., tool wear, shifts, batches) can violate these assumptions and bias comparisons. Model adequacy is assessed via K–S tests on uncensored datasets, but the main inferential setting is progressively censored joint testing; goodness-of-fit under censoring and sensitivity to WG misspecification are not thoroughly analyzed. The reliability comparison largely hinges on parameter ordering (notably $\beta$), which may not fully characterize crossing reliability curves when multiple parameters differ; more general comparison metrics (e.g., stochastic ordering checks, time-varying dominance) are not explored. Computational details (e.g., convergence diagnostics for MCMC, choice of proposal variances, Newton–Raphson stability) are not fully documented, and no reproducible code is provided.",None stated.,"Extend the framework to handle dependence/common-cause effects across production lines (shared frailty or hierarchical models) and autocorrelated/batch-structured lifetimes. Develop robust or semi-parametric alternatives when Weibull–Gamma fit is inadequate, including formal goodness-of-fit methods tailored to joint progressive censoring. Generalize comparison beyond simple parameter ordering to allow for potential crossing reliability functions, and provide decision-focused measures (e.g., probability one line outlasts another at mission time with uncertainty). Provide an open-source implementation with convergence diagnostics for MCMC and numerical MLE routines to facilitate practical adoption.",2209.13496v1,https://arxiv.org/pdf/2209.13496v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T12:59:51Z
TRUE,Maintenance optimization|Other,Simulation-based|Other,Simulated only,Not applicable,Energy/utilities|Theoretical/simulation only|Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"This preprint presents a reliability-based robust design optimization (RRBO/RRDO) framework for engineering systems under uncertainty, combining uncertainty quantification with optimization to obtain designs that are both robust (low variability) and reliable (controlled failure probability). The uncertainty quantification component is based on polynomial chaos methods to estimate moments (mean and standard deviation) of responses during optimization, as an alternative to computationally expensive Monte Carlo sampling. For optimization, the chapter discusses gradient-free approaches and illustrates the workflow using multi-objective simulated annealing (MOSA) on a two-peak test function with normally distributed design-variable uncertainty and Latin hypercube sampling. The numerical example demonstrates that, under uncertainty, the robust optimum can shift away from the deterministic global maximum toward a more stable (less sensitive) local peak. The paper positions the approach as a practical way to incorporate variability and reliability considerations early in design when deterministic optimization can lead to fragile solutions.","The robust optimization objective is posed as a multi-objective problem minimizing variability while improving performance: $\min V(x)=[\mu_V(x),\sigma_V(x)]$ subject to feasibility constraints $U_i(x)>0$. The illustrative test function is a weighted sum of two Gaussians: $f(x,y)=k_1\exp(-((x-x_1)^2+(y-y_1)^2)/(2\sigma_1^2))+k_2\exp(-((x-x_2)^2+(y-y_2)^2)/(2\sigma_2^2))$, with robustness assessed via the mean and standard deviation of $f(x,y)$ under input uncertainty.","In the example, design-variable uncertainty is modeled as normal with standard deviation 0.1, with 50 Latin-hypercube samples generated around each candidate design to estimate response statistics. MOSA is run for 600 evaluations with objectives of maximizing the mean response while enforcing a standard-deviation constraint (targeted as less than 0.1). The resulting robust-optimal designs cluster near the broader peak around $(x,y)\approx(-2,-2)$ rather than the deterministic global maximum at $(2,2)$, illustrating how robustness criteria can favor a slightly lower but less sensitive optimum. Feasible vs. unfeasible designs are visualized via 3D/2D scatter plots based on the imposed variability constraint.",None stated.,"The work is primarily a conceptual/illustrative demonstration using a simple synthetic two-peak function, so performance and practicality for real high-fidelity engineering simulations (expensive solvers, many constraints) are not validated. Key implementation details needed for reproducibility are missing (e.g., polynomial chaos order/basis choices, sampling/estimation procedure within the PC approach, MOSA parameter settings, constraint-handling method), and no benchmark comparisons (e.g., against GA/NSGA-II, robust CMA-ES, FORM/SORM-based RBDO, or surrogate-assisted methods) are reported. Reliability is discussed largely at the framework level; the case study focuses on variance/robustness constraints rather than explicit probability-of-failure modeling with limit-state functions.",None stated.,"Apply the RRBO framework to real engineering case studies (e.g., aeronautics, nuclear, energy components) with expensive simulations, multiple constraints, and explicit reliability targets (probability-of-failure/limit states). Provide systematic comparisons versus established RBDO/RDO baselines and explore surrogate-assisted or adaptive sampling strategies to reduce computational cost. Extend the method to handle correlated/non-Gaussian uncertainties, model-form uncertainty, and time-dependent reliability (degradation/RUL) where robustness and reliability interact over lifecycle.",2210.07521v1,https://arxiv.org/pdf/2210.07521v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:00:21Z
TRUE,Degradation modeling|Maintenance optimization|Failure mode analysis|Other,"Parametric (Weibull, etc.)|ML-based|Other",Degradation measurements|Sensor/condition monitoring|Mixture of types,Condition-based|Age-based|Not applicable,Environmental monitoring,Case study (real dataset)|Simulation study|Other,TRUE,R,Not provided,https://codes.iccsafe.org/content/IECC2021P1/chapter-3-ce-general-requirements|https://www.epa.gov/aqs|https://www2.purpleair.com/,"The paper empirically studies performance degradation of low-cost PurpleAir PM2.5 sensors over multi-year operation using two complementary degradation outcomes: (i) within-device divergence between the two identical internal channels (A vs B) and (ii) time dependence of bias/error relative to collocated EPA FRM/FEM reference monitors. A data-driven “flagged measurement” rule is tuned using collocated data to maximize agreement with reference monitors, and the evolution of flagged-measurement rates over operational time is used as a degradation indicator and to define “permanent degradation.” The authors also fit correction models (from linear regression with humidity adjustment to machine-learning models such as random forests) and track how correction error drifts with sensor age, including non-linear patterns via generalized additive models. Results indicate flagged measurements increase with time (to about 4% after ~4 years), about 2% of sensors meet a permanent-degradation criterion, and corrected-sensor bias shifts over time on average (−0.12 μg/m³ per year), with strong modification by climate zone. The study provides practical screening and replacement guidance, highlighting harsher climates (hot-humid/hot-dry) where earlier maintenance or replacement may be warranted.","A measurement is flagged when $|A-B|>5\,\mu g/m^3$ and the percent-difference statistic $\frac{|A-B|\times 2}{A+B}$ exceeds the sensor-specific 85th percentile (chosen to optimize $R$ and nRMSE vs reference monitors). The primary correction model (from Barkjohn et al., 2021) regresses reference on PurpleAir and RH: $PM_{2.5,ref}=s_1\,PM_{2.5}+s_2\,RH+b+\varepsilon$, yielding an implemented form $PM_{2.5,corr}=5.92+0.57\,PM_{2.5,raw}-0.091\,RH$. Degradation trends are assessed via linear regression $Outcome=f+d\cdot hour+\varepsilon$ and via GAMs $Outcome\sim s(hour)$ to capture nonlinearity.","The tuned flag definition (|A−B|>5 μg/m³ plus >85th-percentile percent-difference) flags about ~2% of PurpleAir data in the collocated set and yields the best agreement with reference monitors among tested thresholds. Across all sensors, the hourly percent of flagged measurements increases with age to ~4% after ~4 years (~35,000 hours). Permanent degradation (cumulative mean flag ≥0.4 for ≥100 hours) is observed in 240 of 11,932 sensors (~2.0%), with higher fractions in hot-humid (3.9%) and mixed-humid (3.2%) zones. The average corrected bias drift is −0.12 μg/m³ per year overall (95% CI: −0.13, −0.11), with climate-zone heterogeneity (e.g., hot-dry +0.08 μg/m³/year; hot-humid about −0.92 μg/m³/year) and an apparent acceleration in bias after ~3.5 years (wide uncertainty due to few long-lived sensors).","The authors note that a flagged-measurement metric cannot detect the case where both channels degrade similarly, motivating their second approach using collocated reference monitors. They also emphasize limited evidence beyond ~3–3.5 years because relatively few sensors in the dataset operated that long, producing wide confidence bands and uncertainty about late-life behavior. They acknowledge that some sensors classified as permanently degraded appear to recover, potentially due to the cumulative-mean definition or because owners may have cleaned/replaced internal components (or temporary issues like dust/insects).","The “flagged” criterion is tuned using collocated sites and then applied network-wide; its operating characteristics may vary by unobserved factors (installation quality, firmware, enclosure differences) and could misclassify in regions unlike the collocation sample. The permanent-degradation rule (cumulative mean flag threshold and 100-hour duration) is heuristic and not linked to an explicit reliability model (hazard/transition probability), making replacement-policy optimization difficult. The correction/bias analysis uses only sensors within 50 m of regulatory monitors (151 sensors), which may not represent the broader population of PurpleAir deployments (selection bias toward certain environments or user types). Autocorrelation and seasonality in PM/RH/T time series may violate simple regression error assumptions and could affect confidence intervals if not fully accounted for.","The authors state that further work is needed to evaluate PurpleAir performance after ~3 years (and especially after ~3.5 years), because few sensors in the current dataset operated that long. They also suggest that degradation concerns differ by climate zone and may necessitate climate-specific maintenance/cleaning protocols, implying additional study of degradation drivers under different environmental conditions.","A next step would be to develop an explicit reliability/transition model (e.g., time-to-permanent-degradation with covariates such as climate, RH exposure, dust events) to estimate hazard rates and support cost-based replacement optimization. Robust/self-starting calibration and degradation detection methods that handle autocorrelation, missingness, and nonstationary exposures could improve generalizability beyond collocated sites. External validation on newer PurpleAir models (e.g., PA-II-Flex/PMS6003) and in non-US settings would test portability of the degradation indicators. Publishing an open-source R pipeline (data cleaning, flagging, GAM fitting) would enable reproducibility and broader adoption by practitioners and researchers.",2210.14759v2,https://arxiv.org/pdf/2210.14759v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:01:13Z
TRUE,Failure mode analysis,ML-based|Other,Other,Not applicable,Semiconductor/electronics,Simulation study|Other,TRUE,Python|Other,In text/Appendix|Not provided,https://github.com/huggingface/transformers|https://github.com/ari-holtzman/degen|https://github.com/frankcgq105/BERTCHEN|https://github.com/afredbai/text_classification|https://github.com/dmlc/gluon-nlp|https://github.com/salesforce/cove|https://github.com/google-research/bert|https://github.com/mandarjoshi90/coref|https://github.com/zihangdai/xlnet|https://github.com/facebookresearch/fairseq|https://github.com/microsoft/MASS|https://github.com/CZWin32768/XNLG,"The paper proposes using pre-trained transformer language models to generate structured Failure Analysis Triplets (step type, substep technique, equipment) from FRACAS failure descriptions in a semiconductor failure-analysis setting. It frames failure analysis triplets generation (FATG) as a data-to-text/sequence-to-sequence task over a joint space of failure description features and triplet sequences, then fine-tunes GPT-2, GPT-3, and BART (and a small baseline GPT) for generation. To better evaluate ordered, structured triplet outputs, it introduces LESE (LEvenshtein Sequential Evaluation), an n-gram Levenshtein-distance-based precision/recall/F1 metric intended to align more closely with human judgment than BLEU/ROUGE/METEOR. Experiments on proprietary real FA data from 2019 (n=5809 cases) show GPT-2 variants substantially outperform BART and GPT-3 on ROUGE and LESE for short-sequence FATG, with GPT-2 Medium/Large best overall, while long-sequence FATG remains challenging (notably for correct equipment selection). The work positions PLM-based generation as a way to partially automate/standardize expert failure-analysis workflows and documentation in semiconductor FRACAS systems.","The FATG task is formalized via autoregressive factorization of failure descriptions and triplets and their joint distribution, e.g., $p(x,\lambda)=\prod_{i=1}^N p(\lambda_i\mid x_{1:i-1},\lambda_{1:i-1})$, and compactly $p(\Lambda)=\prod_{i=1}^N p(\Lambda_i\mid \Lambda_{1:i-1})$ where $\Lambda$ concatenates description and triplets. The proposed evaluation metric LESE-N uses an n-gram Levenshtein distance recurrence (edit operations on n-grams) and converts it to precision/recall and F1: $\text{LESE-N} = \frac{(1+\beta^2) P_{Lev}R_{Lev}}{\beta^2 P_{Lev}+R_{Lev}}$, with $P_{Lev},R_{Lev}$ defined using $\max(|r|_{n},|h|_{n})-Lev(r,h)$ normalized by the hypothesis/reference n-gram counts.","On the 2019 dataset, GPT-2 variants achieve the best overall automatic scores; for example GPT2-L reports BLEU-1=22.46, BLEU-3=17.60, ROUGE-1 F1=30.79, ROUGE-L F1=29.82, LESE-1 F1=21.25, and LESE-3 F1=10.73, outperforming the baseline mini-GPT (BLEU-1=11.54; ROUGE-1 F1=34.61 but lower ROUGE-L/LESE) and strongly outperforming BART (BLEU-1=6.14; ROUGE-L F1=10.16; LESE-3 F1=1.28) and GPT-3 (BLEU-1=6.10; ROUGE-L F1=8.84; LESE-3 F1=0.42). In short-sequence examples, GPT2-M can exactly match expert triplets (reported LESE-1=100% and LESE-3=100% in a 4-triplet case), while BART and GPT-3 frequently generate incorrect or overly long/unstructured outputs. Transfer across years using GPT2-M weights shows modest variation (e.g., LESE-3 F1 ≈10.74 in 2019 fine-tune and ≈10.96 in 2020; with 2021 showing slightly lower/higher depending on metric), suggesting partial generalization but data-size sensitivity for later years.","The authors note that domain adaptation is challenging because FAT datasets are confidential/unavailable, limiting intermediate domain-adaptive fine-tuning and contributing to poor performance of some models (e.g., GPT-3 and BERT, with BERT dropped due to lack of meaningful generation). They also state that for long-sequence FATG none of the models accurately generated the exact equipment proposed by human experts, even if the generated equipment could serve a similar purpose. They highlight that standard metrics (BLEU/ROUGE/METEOR) do not reliably reflect expert judgment for structured triplets, motivating LESE.","The evaluation relies heavily on n-gram overlap and edit-distance metrics; even LESE may not capture semantic equivalence (e.g., alternate but valid equipment/techniques) or clinical/engineering correctness of the analysis path. The work appears to use a single industrial dataset and does not detail train/validation/test splitting strategy, leakage controls, or statistical significance testing across model comparisons, which can affect confidence in reported gains. The generative outputs are not integrated with constraint checking (e.g., lab-equipment availability, permissible step ordering), which is important for deployable failure-analysis workflow automation. The paper provides limited reproducibility: while some referenced repos exist and LESE code is shown, the fine-tuning scripts/configs and (confidential) data are not released, and key preprocessing specifics may be insufficient to replicate results.",None stated.,"Extend FATG with constraint-aware decoding or post-processing to enforce valid step/substep/equipment combinations and lab-specific equipment availability, improving practical usability. Evaluate robustness to domain shift (different sites/years/product families) and add human-in-the-loop studies where engineers rate usefulness and correctness beyond string similarity. Explore retrieval-augmented generation (RAG) over historical FRACAS cases to improve long-sequence accuracy and equipment selection, and investigate structured prediction formulations (e.g., sequence labeling over a controlled vocabulary) as alternatives to free-form generation.",2210.17497v1,https://arxiv.org/pdf/2210.17497v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:02:01Z
FALSE,Other,Other,Other,Not applicable,Manufacturing (general)|Healthcare/medical|Finance/economics|Other,Other,NA,None / Not applicable,Not applicable (No code used),https://www.vanderschaar-lab.com/dc-check/,"This paper proposes DC-Check, an actionable checklist-style framework to guide development of reliable machine learning systems using a data-centric lens across four pipeline stages: Data, Training, Testing, and Deployment. It structures practitioner questions and highlights common real-world failure drivers such as data quality issues, subgroup/hidden stratification problems, distribution shift/drift, and overconfident predictions. The paper is primarily a conceptual/process framework rather than a reliability engineering modeling contribution; it compiles considerations and example tools (e.g., data validation, robust training, drift detection, uncertainty quantification) and provides worked examples. It emphasizes evaluation beyond average accuracy via subgroup evaluation and stress tests, and post-deployment monitoring for covariate shift, label shift, and concept drift. A companion website is provided as a living resource for the checklist and associated tooling.",Not applicable,"Not applicable; the paper introduces a checklist framework and discusses qualitative examples and tool lists rather than reporting new reliability metrics, derived distributions, or quantitative performance improvements for a proposed method.","The authors state that DC-Check questions are not prescriptive, complete, or exhaustive, and that relevant considerations differ by application context and required level/type of reliability. They also note that tools and methods for executing the checklist tasks will evolve over time, implying the framework requires ongoing updates to remain current.","Because DC-Check is a high-level framework, it does not provide operational definitions or measurable acceptance criteria for “reliability” across different ML tasks, which can limit auditability and comparability between projects. The paper does not supply empirical validation showing that applying DC-Check measurably reduces incidents or improves field reliability relative to standard practice. Guidance on prioritization/trade-offs (cost, time, risk) across checklist items is limited, which can hinder adoption in resource-constrained settings.","The paper highlights multiple research opportunities across the pipeline, including continuous dataset curation with reduced labeling burden, automated or human-in-the-loop data forensics, improved methods for limited target-domain domain adaptation/transfer, automatic subgroup (slice) discovery for evaluation, high-dimensional drift monitoring, actionable root-cause drift explanations, feedback-driven dataset updates for retraining, and improved uncertainty/OOD/fairness methods. It also suggests the companion website will be updated over time as methods and tooling evolve.","A useful extension would be a formal maturity model or scoring rubric for DC-Check that maps checklist completion to risk tiers and required evidence, enabling standardized audits and governance. Another direction is controlled studies (e.g., organizational A/B rollouts) quantifying impact of DC-Check adoption on post-deployment failures, drift incidents, and maintenance burden. Tooling could be expanded into an open-source implementation (templates, validators, experiment trackers) that integrates with common MLOps stacks and generates machine-readable reports for regulators and internal assurance.",2211.05764v1,https://arxiv.org/pdf/2211.05764v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:02:22Z
TRUE,Network/infrastructure reliability|Other,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Energy/utilities,Simulation study|Other,TRUE,Julia|Other,Public repository (GitHub/GitLab),https://github.com/hypergcnns/hypergcnns.git,"The paper proposes Hyperstructures Graph Convolutional Neural Networks (Hyper-GCNNs) to evaluate and rank power distribution grid expansion plans with resilience objectives, targeting the risk-based metric CVaR of annual loss of load. It introduces a new topology-derived feature called uniqueness scores (U-scores) that differentiate substations vs. load buses and summarize simple-path redundancy/overlap between loads and substations, showing higher correlation with 1/CVaR than common graph metrics (e.g., U-scores r=0.893 vs Avg. BC r=0.282, APL r=-0.133). Hyper-GCNNs combines standard GCN embeddings with hyperedge and hypernode (hyper-line-graph) representations learned via MLPs and fuses them using an attention mechanism to produce graph-level predictions for CVaR class bins and regression. Ground-truth labels are produced by sequential Monte Carlo simulation of hourly operation over 2000 annual scenarios per expansion plan with routine and HILP outage processes, which is computationally expensive (up to ~24 hours for a 54-bus case), while the proposed method performs inference in seconds. On two modified 54-bus systems, Hyper-GCNNs outperforms seven baselines in multi-class CVaR classification (e.g., 91.80% vs 90.90% next best on system I, 3 classes) and achieves the best CVaR regression RMSE among tested models (0.55 and 0.93).","The GCN layer uses renormalized adjacency: $Z^{(\ell)}_{GC}=f_{GMP}(\sigma(\tilde D^{-1/2}\tilde A\tilde D^{-1/2}Z^{(\ell-1)}_{GC}\Theta^{(\ell-1)}))$ with $\tilde A=I+A$ and $\tilde D_{ii}=\sum_j \tilde A_{ij}$. Hyperstructure graph-level embeddings are computed via MLP + global max pooling: $Z_E=f_{GMP}(\mathrm{MLP}(H_E\cdot \mathrm{MLP}(H_E^\top X_V)))$ and $Z_V=f_{GMP}(\mathrm{MLP}(H_V\cdot \mathrm{MLP}(H_V^\top X_E)))$. An attention module produces weights $\alpha_i=\mathrm{softmax}_i(\Upsilon_{Att}\tanh(\Xi Z_i))$ over $i\in\{GC,E,V\}$ and fuses representations as $Z=\alpha_{GC}Z_{GC}+\alpha_E Z_E+\alpha_V Z_V$ for classification/regression.","U-scores correlate most strongly with inverse CVaR among tested topology metrics: $r(U\text{-scores},1/\mathrm{CVaR})=0.893$ vs Avg. Degree 0.869, Avg. BC 0.282, APL -0.133, and Diameter -0.205. In CVaR multi-class classification (avg. accuracy % over 10 runs), Hyper-GCNNs achieves 91.80/88.28 (system I/II) for 3 classes, 86.37/84.29 for 4 classes, and 78.00/73.35 for 5 classes, outperforming all seven baselines in each setting. For CVaR regression, Hyper-GCNNs attains RMSE 0.55±0.003 (system I) and 0.93±0.001 (system II), better than GraphSAGE (0.57 and 1.00) and GCN (0.59 and 1.10). Reported model running times on 54-bus system I are on the order of seconds (e.g., GCN 1.00s, Hyper-GCNNs described as second-lowest among DL models, FC-V 0.80s), versus hours for simulation-based CVaR evaluation; per-plan U-score generation averages 1.81s (system I) and 0.03s (system II), and a single Hyper-GCNNs training epoch averages 1.25s and 0.89s respectively.","The paper notes the study is an initial/first step into using AI for distribution-grid investment planning with resilience objectives, implying the approach is evaluated on constructed datasets of expansion plans rather than broad real-utility deployments. It also frames optimal binning of CVaR ranges into discrete risk classes as a standalone issue not addressed here and left for future research. It emphasizes that the prevailing CVaR evaluation via SMCS is computationally expensive (hours) and motivates the need for approximation rather than improving the simulation itself.","Ground-truth CVaR is generated under specific simulated outage assumptions (e.g., Bernoulli availability with fixed routine/HILP rates) and on two modified 54-bus test systems; generalization to different utilities, climates, correlated failures, and restoration dynamics is not demonstrated. The method relies on engineered node/edge features (U-scores, degree, betweenness, cosine similarity) and hyperstructure construction choices (kNN with k=10, clustering around substation-incident edges), which may be sensitive and require retuning across networks. Evaluation focuses on predictive performance and runtime but does not quantify decision-quality impacts in an end-to-end planning loop (e.g., how often the top-ranked plans by the model match those by full stochastic optimization, or the cost/regret of misranking).","The paper explicitly states that optimal binning practices for CVaR class construction and their connection to planning standards are left for future research. It also suggests further investigation of U-scores beyond planning, including using uniqueness-score-based classifications to support fast topological reconfiguration during operations. More broadly, it proposes applying the Hyper-GCNNs + U-scores framework to other heterogeneous networks (e.g., molecular, transportation, blockchain graphs).","Validate the approach on real utility feeders and historical outage/restoration data, including geographically correlated weather-driven failures and cyber-physical contingencies, to test robustness beyond the simulated Bernoulli/HILP setup. Develop self-calibrating or uncertainty-aware versions (e.g., Bayesian or conformal prediction) to provide confidence intervals for CVaR/risk-class predictions, which is important for regulatory and planning use. Integrate the model into an optimization workflow as a screening/pre-solver and evaluate end-to-end benefits (solution quality, computational savings, misclassification cost) under realistic constraints and multiple objective tradeoffs (cost, reliability indices, resilience, equity).",2211.07645v1,https://arxiv.org/pdf/2211.07645v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:03:16Z
FALSE,NA,Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),github.com/ant-stephenson/gpsampler,"This paper proposes scalable methods to generate very large synthetic datasets that approximate draws from an arbitrary Gaussian process (GP) prior, with high-probability guarantees that the approximate sample distribution is close to the exact GP distribution in total variation distance. Two main sampling approaches are analyzed: Random Fourier Features (RFF) for stationary kernels and Contour Integral Quadrature (CIQ) for general kernels, including a preconditioned CIQ variant using a Nyström preconditioner to reduce Krylov/Lanczos iterations. The authors derive sufficient bounds on fidelity parameters (number of features D for RFF; quadrature points Q and Lanczos iterations J for CIQ) to ensure a target total-variation error, and formalize “indistinguishability” via Bayesian decision-theoretic error bounds linked to TV distance. Empirical experiments validate that practical fidelity requirements are often less stringent than the conservative theoretical bounds, and show that preconditioning accelerates CIQ convergence. The work is positioned as a benchmarking tool for evaluating GP approximation methods at scale under model-consistent synthetic data generation.","Sampling is posed as drawing $u\sim\mathcal N(0,I_n)$ and transforming via a covariance square root: $y=Au+m$ with $AA^\top=C$ so that $y\sim\mathcal N(m,C)$. For RFF sampling, an approximate draw is generated as $\hat f = Zw$ where $w\sim\mathcal N(0,I_D)$ and $ZZ^\top$ approximates the kernel Gram matrix; Lemma 2.1 gives a sufficient feature count $D\ge 8\log(\sqrt n/\delta)\, n^2/(8\epsilon^2\sigma_\xi^4)$ to control TV error. For CIQ sampling, the method approximates $K^{1/2}u$ using contour-integral quadrature with $Q$ quadrature points and Krylov/Lanczos iterations $J$, yielding sufficient conditions (Lemma 3.1) of the form $Q\gtrsim \log( n/(\eta\sigma_\xi^2))(-\log\delta_Q)$ and $J\gtrsim \tilde O(\sqrt n\,\log n/ (\sigma_\xi(\epsilon\sigma_\xi\sqrt{1-\eta}-\delta_Q)))$. Indistinguishability is linked by Lemma 4.2: $TV(P_0,P_1)\le 2\epsilon\Rightarrow P_0,P_1$ are $\epsilon$-indistinguishable under the optimal Bayesian test.","The paper derives explicit sufficient (conservative) scaling laws for approximate GP prior sampling with total-variation guarantees: RFF requires $D=\Omega(n^2\log n)$ features in the stated bound, while CIQ achieves better complexity with bounds implying $Q=\tilde O(\log n)$ and $J$ scaling between $O(1)$ and $O(n^{7/8}\log n)$ depending on kernel smoothness/eigenvalue decay and preconditioning. A summary table reports asymptotic costs: Cholesky $O(n^3)$ time/$O(n^2)$ space; RFF $O(n^3\log n)$ time/$O(n)$ space; CIQ $O(n^{5/2}\log n)$ time/$O(n\log n)$ space; preconditioned CIQ $O(n^{2.375}\log n)$ time/$O(n\log n)$ space. In hypothesis-test experiments (Cramér–von Mises normality test on whitened samples), rejection rates converge to the nominal type-I error level before reaching the theoretical upper bounds on $D$ or $J$, indicating practical requirements are milder than the proofs suggest. Preconditioning improves convergence (fewer iterations needed) relative to unpreconditioned CIQ, consistent with the condition-number dependence of Krylov methods.","The authors note that their theoretical bound for RFF appears too stringent, likely because it requires bounding all $n^2$ kernel-matrix entry errors even though there are only $n$ (not $n^2$) independent elements, suggesting the bound could be relaxed. They also state they did not implement the full optimal Bayesian decision procedure for indistinguishability in experiments, opting for hypothesis tests to avoid substantial coding and compute burden. They acknowledge their CIQ usage is a “misuse” of the existing GPyTorch CIQ implementation (which was designed for other purposes), implying current tooling is not perfectly aligned with their sampling application.","The guarantees are derived under specific modeling/algorithmic assumptions (e.g., bounds using Pinsker’s inequality and conservative norm bounds), which can be loose; practical tightness and robustness under non-ideal numerical effects (finite precision, solver tolerances, GPU/parallel nondeterminism) are not analyzed. The experimental validation focuses largely on isotropic RBF kernels and a particular whitening-and-normality-testing protocol; broader kernel families (nonstationary, nonradial, structured kernels) and higher-dimensional regimes may exhibit different behavior. The indistinguishability notion is framed for a binary decision between exact vs approximate samplers with equal priors; it does not directly quantify downstream task impact (e.g., GP regression posterior accuracy) beyond the stated TV-based argument.","They propose improving the theoretical bounds for at least the RFF method, and running more extensive experiments over a wider hyperparameter space and more general kernel functions. They also suggest implementing and using the Bayesian decision process described in the paper for a more thorough empirical validation. Additionally, they indicate it would be beneficial to modify/adjust the GPyTorch CIQ implementation to better support this sampling application and encourage broader adoption.","Developing self-tuning/adaptive schemes for selecting $(D,Q,J)$ based on online error estimators (rather than worst-case bounds) could make the methods more practical at scale. Extending the indistinguishability analysis to settings with unknown/estimated kernel hyperparameters and to structured kernels with fast MVMs (Toeplitz/Kronecker/state-space) would broaden applicability. Providing standardized benchmarks and open-source reproducible scripts (including downstream GP approximation evaluation pipelines) would improve comparability and adoption beyond the sampling-quality hypothesis tests.",2211.08036v3,https://arxiv.org/pdf/2211.08036v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:04:00Z
TRUE,Network/infrastructure reliability|System reliability|Other,Bayesian|Simulation-based|Other,Simulated only,Not applicable,Energy/utilities|Network/cybersecurity|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes the Bayesian improved cross entropy (BiCE) method, a modification of the improved cross entropy (iCE) importance-sampling approach for estimating rare-event failure probabilities in network reliability problems with independent multi-state (categorical) components. Standard CE/iCE updates categorical importance-sampling models via weighted maximum likelihood, which can drive some category probabilities to exactly zero (“zero-count”/overfitting), yielding an importance-sampling distribution that misses parts of the failure domain and produces strongly negatively biased failure-probability estimates. BiCE replaces the weighted-MLE update with a Bayesian update: imposing a Dirichlet prior on categorical probabilities and using the posterior predictive distribution to update the sampling distribution, ensuring strictly positive probabilities and reducing bias. The method is evaluated via repeated-run simulation on (i) toy linear limit-state examples with Bernoulli and 3-state inputs, (ii) a multi-state two-terminal max-flow network, and (iii) an IEEE-39 power transmission network with cascading failures, showing large reductions in bias relative to iCE and improved efficiency versus crude Monte Carlo for the same computational budget. The paper also discusses iCE’s intermediate-target adaptation via controlling the coefficient of variation of weights, and recommends practical settings (e.g., δ_tar≈δ_ε in [1,2] and moderate symmetric Dirichlet prior strength b≈5–10 for N≈1000).","Failure probability is defined as $p_f=P(g(X)\le 0)=\mathbb{E}_{p_X}[\mathbf{1}\{g(X)\le 0\}]$ and estimated by importance sampling as $\hat p_f=\frac1N\sum_{k=1}^N \mathbf{1}\{g(x_k)\le 0\}\,\frac{p_X(x_k)}{p_{IS}(x_k)}$. iCE uses smooth intermediate targets $p^{(t)}(x)\propto p_X(x)\,\Phi(-g_a(x)/\sigma^{(t)})$ and updates parameters by maximizing a weighted cross-entropy objective (equivalently weighted MLE for categorical models). BiCE introduces a Dirichlet prior $v_d\sim\mathrm{Dir}(\theta_d)$ for each component’s categorical probabilities and updates via the posterior predictive mean $\mu^{(t)}_{d,i}=\frac{N\hat v^{(t)}_{d,i}+\theta_{d,i}}{N+\sum_j\theta_{d,j}}$ (with symmetric prior $\theta_{d,i}=b$), preventing zero probabilities.","Toy Bernoulli example (true $1.387\times10^{-7}$): iCE with $N=2000$ shows strong negative bias (relative bias −0.375), whereas BiCE with $N=500$ yields near-zero bias (≈0.01) and much lower computational cost (~4–5k vs ~20k LSF calls), with performance sensitive to prior strength (e.g., b=5 gave lower c.o.v. than b=1). Multi-state toy example (MCS reference $7.3\times10^{-5}$): iCE with $N=2000$ again has large negative bias (−0.375), while BiCE with $N=1000$ achieves near-zero bias (−0.004 to −0.014 depending on b) with lower cost (~6–7.5k vs ~16k). Two-terminal max-flow network (MCS reference $1.8\times10^{-4}$): BiCE with $N=1000$ and b=1 achieved relative bias −0.0043 and c.o.v. 0.100 at cost 9,385, versus crude MCS c.o.v. 0.723 at the same cost; iCE (N=2000) showed bias −0.253. IEEE-39 cascading-failure case (MCS reference $9.3\times10^{-5}$): BiCE with $N=1000$ and b=10 achieved c.o.v. 0.143 and bias −0.009 at cost 5,000, while iCE (N=2000) had bias −0.409.","The authors note that BiCE may require a large number of samples to achieve acceptable results in some problems. They also state that the BiCE estimator can still be skewed due to limited capacity of the chosen parametric model (notably the independence assumption in the independent categorical family): if the suboptimal family $h(x;v^*)$ is far from the optimal IS density $p_X^*(x)$, the IS estimator is inherently bounded to perform poorly even if the parameter estimate is close to $v^*$.","All demonstrations appear to be simulation-based; the paper does not report validation on real operational network reliability datasets or field failure data, which may limit evidence of practical robustness. The approach relies on assuming component-wise independence in the IS categorical family; while acknowledged as capacity-limiting, the paper does not explore structured dependence models (e.g., graphical/Markov models) that might be necessary for correlated component states. Implementation details (software, runtimes, numerical stability of σ-optimization) and reproducibility artifacts are not provided, making it harder to assess computational overhead and adoption in practice.","The paper explicitly leaves “a further investigation of the prior distribution” for future work, i.e., how to choose and tune the Dirichlet prior parameters beyond the simple symmetric choice. It also suggests that selecting an appropriately informative symmetric prior (b>1, e.g., 5 or 10 for N≈1000) improves performance, implying systematic prior selection is an open direction.","Developing richer parametric IS families for discrete multi-state networks that capture dependence (e.g., pairwise interactions, Bayesian networks, factor graphs) could reduce the residual skew/bias when independence is inadequate. Adaptive/empirical Bayes strategies to tune b online (e.g., based on ESS, predictive checks, or minimizing estimated IS variance) could improve robustness across problems without manual prior selection. Providing open-source implementations and benchmarking against additional rare-event methods (subset simulation variants, splitting, alternative CE smoothings) on standardized network-reliability testbeds would strengthen reproducibility and comparative understanding.",2211.09542v1,https://arxiv.org/pdf/2211.09542v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:04:49Z
FALSE,NA,Other,Other,Not applicable,Transportation/logistics|Other,Simulation study|Case study (real dataset),TRUE,R,Not provided,https://github.com/jose-ameijeiras/Car-crashes-data,"The paper proposes new data-based smoothing-parameter (bandwidth) selectors for circular kernel density estimation and for estimation of circular density derivatives. It develops circular analogues of the Sheather–Jones plug-in rules, providing both a multi-stage direct plug-in algorithm and a solve-the-equation plug-in rule, with supporting asymptotic theory via AMISE/AMSE expansions for general circular kernels expressed through Fourier series coefficients. The work also studies kernel choice in the circular setting and shows that the wrapped Epanechnikov kernel is AMISE-optimal for circular density estimation (with related optimal kernels for derivatives via wrapped bounded-support kernels). Performance is evaluated through Monte Carlo simulations on 20 benchmark circular models and a real-data example on time-of-day car crashes, showing the proposed selectors are competitive (often best or second best) versus existing circular bandwidth rules such as rule-of-thumb, mixture-based plug-in, and likelihood cross-validation. Implementation is described for the R package NPCirc (functions bw.AA and kern.den.circ), with numerical root-finding used for the solve-the-equation selector.","The circular kernel density and derivative estimators are $\hat f_\nu(\theta)=\frac1n\sum_{i=1}^n K_\nu(\theta-\Theta_i)$ and $\hat f^{(r)}_\nu(\theta)=\frac1n\sum_{i=1}^n K^{(r)}_\nu(\theta-\Theta_i)$. A bandwidth-like function is defined by $h_K(\nu)=\pi^2/3+4\sum_{j\ge1}(-1)^j\alpha_{K,j}(\nu)/j^2$, leading to an AMISE expansion $\mathrm{AMISE}=\tfrac14 h_K(\nu_n)^2\int_{-\pi}^{\pi}(f^{(r+2)}(\theta))^2d\theta + \frac1n R_{K;r,2}(\nu_n)$. The AMISE-optimal bandwidth is $h_{K;r;\mathrm{AMISE}}=\left(\frac{(2r+1)Q_{K;r,2}}{n\int (f^{(r+2)})^2}\right)^{2/(2r+5)}$, with plug-in rules replacing the unknown functional by kernel-based estimators of $\psi_s=\int f^{(s)}(\theta)f(\theta)d\theta$ and solving $h_K(\nu)=h_{K;r;\mathrm{PI}}$ (direct) or a coupled fixed-point/root equation (solve-the-equation).","In simulations over 20 benchmark circular models with $n=50$ and $n=100$ (1000 replicates each), the proposed plug-in selectors (two-stage direct and solve-the-equation) typically achieved the smallest or second-smallest average integrated squared error among compared methods (Taylor rule-of-thumb, Oliveira et al. mixture plug-in, and likelihood cross-validation), often close to the per-sample ISE-minimizing “gold standard”. The paper reports that the solve-the-equation selector tends to be best for more complex/multimodal or peaked densities, while the direct plug-in with a simple von Mises reference can be best for small samples and unimodal settings, with improvements for rotationally symmetric multimodal cases when using a von Mises mixture reference. In the real car-crash time-of-day data example ($n=85$), the ISE (×100) versus a fitted wrapped skew-normal reference was 0.265 for the proposed direct plug-in, compared with 0.325 (LCV), 0.509 (solve-the-equation), 1.134 (rule-of-thumb), and 1.765 (Oliveira et al. plug-in). The paper also concludes the wrapped Epanechnikov kernel is AMISE-optimal for circular density estimation (with corresponding constants for use in optimal bandwidth formulas).",None stated.,"The work is methodological for circular kernel density estimation, not reliability engineering; the only “application” is a time-of-day traffic accident example, which does not model failures, lifetimes, censoring, or maintenance decisions. Many theoretical results rely on smoothness, symmetry/unimodality, and asymptotic conditions (e.g., conditions ensuring AMISE/AMSE forms and relationships like $R_{K;r,2}(\nu_n)\propto h_n^{-(2r+1)/2}$), and practical performance may degrade under strong dependence (autocorrelated circular observations) or severe model misspecification. Comparative evaluation focuses on a specific set of circular bandwidth selectors; results may differ under other modern selectors or for other kernels beyond those emphasized (von Mises/wrapped normal) or under different loss criteria than ISE.","The authors suggest extending the proposed plug-in ideas to the multivariate toroidal setting using product kernels (as in Di Marzio et al., 2011), deriving optimal smoothing parameters when the same kernel and concentration are used across dimensions, and then adapting Algorithms 1 and 2 to obtain toroidal direct or solve-the-equation plug-in rules.","It would be useful to study robustness of the selectors under dependence (e.g., circular time series) and under contamination/outliers, and to develop self-starting/online versions for streaming circular data. Additional empirical validation on more real-world circular datasets (with domain-specific ground truth) and benchmarking against newer optimization-based or Bayesian/ML bandwidth selection approaches for circular densities could strengthen practical guidance. Providing fully reproducible simulation code and packaged examples (beyond function descriptions) would also improve adoption and verifiability.",2211.10212v1,https://arxiv.org/pdf/2211.10212v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:05:30Z
TRUE,Degradation modeling|RUL prediction|Life distribution modeling|Maintenance optimization,Stochastic process|Bayesian,Degradation measurements|Sensor/condition monitoring|Simulated only,Condition-based|Predictive,Semiconductor/electronics|Transportation/logistics|Other,Simulation study|Case study (real dataset),TRUE,R,Not provided,NA,"The paper develops a conjugate prior family for the homogeneous gamma degradation process, enabling recursive Bayesian updating of the process parameters as new degradation increments arrive. It proposes three posterior-sampling algorithms—Gibbs sampling (with adaptive rejection sampling for the shape parameter), discrete grid sampling (DGS), and sampling-importance-resampling (SIR)—to perform posterior inference under the derived conjugate structure. The conjugate framework is extended to a heterogeneous-effects setting where multiple systems share a common shape parameter but have unit-specific scale parameters, yielding an approximated-gamma-multivariate-gamma prior/posterior form that supports online updates. Using the updated posterior, the paper constructs an efficient online remaining useful life (RUL) prediction algorithm for multiple systems, approximating first-passage-time/RUL distributions with Birnbaum–Saunders approximations. Simulations and two case studies (laser device degradation and train wheel wear) show DGS and SIR deliver similar accuracy to Gibbs sampling but are over 100× faster, making them suitable for real-time/online prognostics with uncertainty quantification.","A homogeneous gamma process is specified by increments $\Delta Y \sim \mathrm{Ga}(\alpha\Delta t,\beta)$ and lifetime/first-hitting time $T=\inf\{t:Y(t)\ge C\}$ with CDF $F_T(t\mid\alpha,\beta)=P(Y(t)>C)=\Psi(\beta C,\alpha t)/\Gamma(\alpha t)$. The proposed conjugate prior for $(\alpha,\beta)$ is $\pi(\alpha,\beta)\propto (\beta\omega)^{\delta \bar T\alpha}\,\big(\prod_{j=1}^m \Gamma(\alpha t_j)^{1/m}\big)^{-\delta}\exp(-\delta\lambda\beta)$, yielding a posterior in the same family with updated hyperparameters. For heterogeneous systems with common $\alpha$ and unit-specific $\beta_i$, the conjugate prior becomes $\pi(\alpha,\beta_{1:n})\propto (\bar\beta_g\,\omega)^{\delta_1 l\alpha}/\Gamma(l\alpha)^{\delta_1}\cdot\exp\{-\sum_i \delta_2\lambda_i\beta_i\}$, and the online update uses recursive hyperparameter updates (Eq. (20)) as each new increment arrives.","In a large simulation study (10,000 datasets) with $n=15$ units and $m=16$ measurement times, relative biases for Bayesian point estimates of $(\alpha,\beta)$ and $R(4500)$ are about 2%, while RB for MTTF is about 0.1% across priors with varying information content ($\delta\in\{0,1,m/4,m/2\}$). DGS and SIR achieve nearly identical RB/RMSE and credible-interval lengths to Gibbs sampling, with frequentist coverage probabilities for $(\alpha,\beta)$ closer to nominal for DGS/SIR than for Gibbs in the reported experiments. Average computation time per dataset is reported as 0.602 s (Gibbs), 0.00341 s (DGS), and 0.00499 s (SIR) on an i7-10700 CPU, i.e., DGS/SIR are over 100× faster. Two real case studies (GaAs laser current degradation; train wheel wear) illustrate that online posterior updating produces RUL point predictions close to interpolated true RUL and 95% predictive intervals that cover the true RUL at measurement points.","The paper notes that the lifetime/RUL distribution under the gamma process has a complicated PDF that is difficult to use directly in practice, motivating the use of a Birnbaum–Saunders approximation for the first-hitting-time/RUL distribution. It also points out that offline Bayesian/likelihood methods require re-analysis of the full dataset when new observations arrive, motivating the need for conjugate/recursive updating. No other explicit methodological limitations are clearly stated beyond these practical difficulties.","The approach assumes a (primarily) homogeneous gamma process with independent increments and monotone degradation, which may be violated in applications with autocorrelated measurement error, non-monotone signals, or structural changes (beyond the heterogeneity modeled via $\beta_i$). Online RUL prediction relies on a Birnbaum–Saunders approximation to the hitting-time distribution; accuracy of this approximation is not systematically quantified across regimes (e.g., early-life, near-threshold, sparse sampling). The heterogeneous-effects model uses unit-specific scale parameters but retains a shared shape parameter, which may be too restrictive when heterogeneity affects both parameters or when covariates/environmental stressors vary over time. Although R is mentioned (e.g., R’s sample() function), implementation details and reproducibility artifacts (code, seeds, runtime settings) are not provided, limiting independent verification and adoption.",None stated.,"A useful extension would be to incorporate measurement error explicitly (which breaks independent increments) while preserving near-conjugacy via approximate filtering (e.g., particle learning) and to study robustness when monotonicity is imperfect. Another direction is to quantify and potentially correct the Birnbaum–Saunders approximation error for RUL, especially near the threshold and under sparse sampling, possibly via fast numerical inversion or calibrated approximations. The framework could be broadened to include covariates/accelerating variables and time-varying environments in the online updates, enabling condition-dependent RUL in fleet settings. Packaging the DGS/SIR online updater as open-source software (R/Python) with documented examples would also materially improve practical impact.",2212.02688v1,https://arxiv.org/pdf/2212.02688v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:06:20Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/alexcapstick/unreliable-sources|https://github.com/alexcapstick/loss_adapted_plasticity,"This paper proposes Loss Adapted Plasticity (LAP), a supervised-learning training strategy for neural networks when training data comes from multiple sources with unknown and heterogeneous reliability/noise. LAP estimates each source’s unreliability online using a moving history of per-source negative log-likelihood (loss) and then tempers (down-weights) that source’s contribution to the objective/gradients, so training progressively focuses on more reliable sources while still learning from all sources early on. The method is motivated by likelihood tempering and by the observation that networks fit clean patterns early and tend to memorize noise later; LAP schedules per-source “temperature” accordingly. Experiments across vision, NLP, time-series healthcare (ECG), and tabular regression—using both synthetic corruption and real noisy labels (e.g., CIFAR-10N) and real source structure (e.g., PTB-XL labeled by nurses, GoEmotions annotated by raters)—show improved or maintained test performance versus standard training and multiple noisy-learning and federated-learning baselines. The authors provide implementation details and release code and a reusable Python package.","LAP uses a per-source tempered log-likelihood $\log p_{\text{temp},\theta}(D)=\sum_{s\in S} T_s\,\log p_\theta(D_s)$, with $T_s=f(C_s)$ a decreasing function of a learned unreliability score $C_s$. The unreliability update compares the source’s mean historical loss $\mu_s$ to a weighted mean and standard deviation of other sources’ losses, increasing $C_s$ if $\mu_s>\mu_{s'}+\lambda\sigma_{s'}$ and otherwise decreasing/clipping it: $C_s\leftarrow C_s+1$ else $C_s\leftarrow\max(0,C_s-1)$. The temperature is instantiated as $f(C_s)=1-\tanh^2(0.005\,\delta\,C_s)$, and gradients (or loss) from source $s$ are scaled as $\hat g_s=f(C_s)g_s$.","Across standard benchmarks with injected noisy sources, LAP often yields sizeable gains over standard training and several baselines; for example on CIFAR-100 with random-label noise, LAP improves top-5 accuracy by about +18.4 percentage points over standard training (69.05% vs 58.34%). On ImageNet with 5/10 sources fully corrupted (3 label-noise, 2 input-noise), LAP improves maximum top-5 accuracy from 68.05% to 70.61%. On IMDB with 4/10 sources fully noisy via random labels, LAP improves accuracy from 65.01% (standard) to 71.95%. LAP can also be combined with other noisy-learning methods: adding LAP to RRL on CIFAR-10 under random-label noise increases accuracy from 76.04% to 80.31%.","The authors note that when the noise level is uniform across all sources, the source label provides no additional information and LAP performs similarly to standard training; in such cases they suggest combining LAP with a separate noisy-data technique. They also caution that, like all noisy-learning methods, LAP depends on the underlying cause of “noise”: if higher loss corresponds to minority-group data or systematic distributional differences rather than corruption, down-weighting could introduce predictive bias and violate the assumption that (clean) sources share the same underlying distribution. They additionally discuss computational overhead and scaling with number of sources/history (e.g., added memory $O(S\times H)$ and per-step compute depending on sources per batch).","The reliability estimate is based on relative loss comparisons, so it may confound genuine domain shift/class-imbalance/difficulty differences with unreliability, potentially down-weighting hard-but-correct sources in realistic multi-site data (even if not “noisy”). The paper does not provide formal guarantees for convergence/optimality under general non-i.i.d. training dynamics, nor a robustness analysis under strong autocorrelation/time-series dependence (common in sensor/ECG streams) beyond empirical tests. Hyperparameter sensitivity is explored, but there is no principled or self-tuning procedure for $(H,\delta,\lambda)$ across tasks; performance could depend on validation choices that may be hard when labels are noisy. Comparisons rely on available-code baselines and may omit more recent or stronger noisy-label techniques and robust training recipes (e.g., modern semi-supervised or confident-learning pipelines) that could narrow the gap.","The authors suggest further studying the effects of ill-specified model capacity on noisy-learning techniques, since many methods (including LAP) assume the model achieves lower loss on clean data early in training. They also point to the setting where noise is uniform across sources as a case where LAP alone is insufficient, motivating research into combining LAP with complementary noisy-data methods (as illustrated with RRL+LAP). They additionally mention the possibility of speeding up per-source computations via parallelisation for large numbers of sources.","A self-starting or Bayesian variant that explicitly models per-source noise rates (or uncertainty over $T_s$) could provide calibrated reliability estimates and reduce reliance on hand-tuned thresholds. Extending LAP to handle structured dependence (e.g., temporal autocorrelation, repeated measures per subject/device) and hierarchical sources (site→device→annotator) would improve applicability to real reliability/QC settings. Developing diagnostics to distinguish “hard but clean” sources from truly corrupted ones (e.g., using agreement measures, representation distances, or out-of-distribution tests) would mitigate the risk of bias against minority or shifted subpopulations. Providing a robust, well-documented PyPI/CRAN-style release with standardized benchmarks and ablations would further improve reproducibility and adoption.",2212.02895v4,https://arxiv.org/pdf/2212.02895v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:07:05Z
TRUE,Degradation modeling|RUL prediction|Life distribution modeling|Other,"Stochastic process|Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|ML-based|Other",Degradation measurements|Sensor/condition monitoring|Right-censored|Mixture of types,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Other,Not provided,https://doi.org/10.1016/j.jmva.2018.11.007|https://doi.org/10.1016/j.jmva.2021.104806|https://doi.org/10.1016/j.csda.2016.09.009|https://doi.org/10.1214/17-AOAS1060|https://doi.org/10.1080/00401706.2023.2242413|https://doi.org/10.1080/00401706.2014.915891|https://doi.org/10.1111/j.0006-341X.2002.00121.x|https://doi.org/10.2307/2965726|https://doi.org/10.1146/annurev-statistics-041715-033624|https://doi.org/10.1080/00031305.2020.1731599|https://doi.org/10.1002/asmb.2063|https://doi.org/10.1214/10-AOAS448,"The paper proposes a functional degradation modeling framework for lithium-ion battery life studies using full voltage discharge curves (VDCs) rather than scalar summaries such as capacity/area-under-curve. Because VDCs live on heterogeneous time domains ending at the end-of-discharge (EOD), the method rescales each curve to [0,1], then models (i) the scaled VDC shape via FPCA scores in a multivariate linear mixed-effects model with cycle number and experimental covariates, and (ii) the EOD time via either a linear mixed-effects model or a functional linear mixed-effects model using the scaled curve as a functional covariate. Predicted scaled curves and EODs are recombined to predict future VDCs on their original domains, from which flexible degradation measures (e.g., normalized change in Lp norm; p=1 corresponds to area/capacity) can be computed and used for prognosis. Extensive simulations show the functional degradation models (FDM-LME and FDM-FLMM) provide substantially lower prediction errors than a standard general path model (GPM) fit to scalar degradation measurements. A case study on the NASA Ames battery dataset (20 batteries under varying temperature/current/stopping-voltage conditions) demonstrates improved degradation prediction and enables prediction intervals via a fractional random weight bootstrap.","Each discharge curve y_ic(r) on r∈[0,b_ic] is represented as (b_ic, x_ic(t)) with scaled time t=r/b_ic and x_ic(t)=y_ic(b_ic t). Scaled curves are expanded by FPCA: x_ic(t)≈μ(t)+∑_{j=1}^K γ_{ic,j} φ_j(t), and the score vector follows a multivariate mixed-effects regression γ_ic = v_0+u_{0i}+(v_1+u_{1i})c + P z_ic + δ_ic. EOD is modeled either as an LME b_ic=α_0+w_{0i}+(α_1+w_{1i})^T z_{1,ic}+α_2^T z_{2,ic}+ε_ic or as an FLMM adding ∫_0^1(β(t)+b_i(t))x_ic(t)dt. Degradation amount is computed from VDC norms: Υ_ic=||y_ic||_p=(∫_0^{b_ic}|y_ic(r)|^p dr)^{1/p} and d_ic=(Υ_{i1}-Υ_ic)/Υ_{i1} (with p=1 giving area-under-curve/capacity).","In simulations spanning n∈{20,50,100}, cycles n_i∈{50,100,150}, and training ratios {50%,80%}, the functional degradation models consistently achieved lower RMSPE for degradation prediction than the scalar general path model; the best-performing variant matched the true EOD-generating mechanism (FDM-LME best when EOD followed the LME form; FDM-FLMM best when EOD depended functionally on x_ic). In the NASA battery case study, FDM-LME/FDM-FLMM produced more accurate and stable out-of-sample degradation predictions than GPM, which tended to overestimate degradation; FDM-LME was selected as the final model based on test-set performance across train/test splits. Using the fitted FDM-LME, the authors forecast VDCs and degradation paths for 20 future cycles (assuming a 5-hour inter-cycle lag) and construct 95% prediction intervals using a fractional random weight bootstrap (B=5000). Fixed-effect estimates for the application’s EOD LME (eq. 19) are reported, e.g., α̂1≈−1.590 for cycle trend, α̂2≈0.695 for AR(1) EOD term, and strong resting-time effect α̂3≈537.689 for exp(−1/Δ_ic).","The authors note that assuming linear effects of experimental conditions may be unrealistic (especially for temperature) and that normality assumptions for errors and random effects can be restrictive. They also state that future degradation prediction requires prespecifying future resting times Δ_ic, which is unrealistic; unusually long rest periods (e.g., battery B0053) can drive trajectories away from all models’ predictions. They acknowledge that their simple linear cycle trend may not suit applications with “flatter-towards-the-end” degradation behavior.","The approach relies on dense functional measurements and interpolation to a common grid (300 points), which may introduce smoothing/interpolation bias and may not generalize to sparse or irregularly observed curves without additional modeling. The two-step modeling (scaled curve then EOD) propagates uncertainty in complex ways; while bootstrap intervals are provided, theoretical coverage and sensitivity to FPCA truncation K (chosen to explain >99% variance) are not fully examined. Model comparisons focus mainly on a linear-path GPM baseline; broader benchmarks (e.g., stochastic-process degradation/RUL models like Wiener/Gamma processes directly on functional features, or modern sequence models) are not systematically evaluated for fairness under matched information. The NASA dataset includes re-indexing due to missing cycles; potential effects of informative missingness or nonstationary usage patterns beyond measured covariates are not discussed.","They propose relaxing linearity of experimental-condition effects and relaxing normality assumptions for errors/random effects. They suggest modeling the resting time Δ_ic as random rather than prespecified, and developing guidance on when to retrain/reset models when unusually long resting periods occur. They also propose extensions to time-varying covariates (varying temperature/current/stopping voltage across cycles) and to alternative cycle-effect functional forms (e.g., S-shaped trends) for applications with different degradation shapes.","A valuable extension would be a fully joint hierarchical model for (x_ic(t), b_ic) to avoid a strict two-step pipeline and to enable coherent uncertainty propagation and posterior prediction of full curves. Robust/semiparametric versions could handle non-Gaussian errors, outliers, and jump behavior seen in degradation paths, potentially via heavy-tailed mixed models or change-point components. Incorporating autocorrelation beyond AR(1) and within-cycle correlation structures (e.g., functional time series for curves) could improve forecasting under strong temporal dependence. Providing an open-source, reproducible software implementation (e.g., an R package or repository) and standardized benchmark protocols would aid adoption and allow broader comparative evaluation across battery datasets and operating regimes.",2212.05515v2,https://arxiv.org/pdf/2212.05515v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:08:05Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,ML-based,Degradation measurements|Sensor/condition monitoring|Complete lifetime data,Predictive,Transportation/logistics|Energy/utilities|Other,Simulation study|Case study (real dataset),TRUE,Other,Public repository (GitHub/GitLab),https://github.com/LazyLZ/multi-head-attention-for-rul-estimation,"The paper proposes a data-driven deep learning model for Remaining Useful Life (RUL) estimation of multi-sensor industrial assets in an IIoT/PHM setting. The method applies multi-dimensional multi-head self-attention to (i) learn interactions among sensor features (feature-wise self-attention) and (ii) weight different time steps (sequence-wise self-attention), followed by a stacked LSTM and an MLP regressor to output RUL. The approach is evaluated on the NASA C-MAPSS turbofan engine run-to-failure benchmarks (FD001–FD004) and the PHM08 challenge dataset using RMSE and the PHM score metric. Extensive parameter studies and ablations show multi-head attention improves over single-head attention and plain LSTM, with the combined feature+sequence attention best for single-condition datasets while feature-only attention is slightly better under multiple operating conditions. The authors also provide interpretability analyses via attention weight visualization and SHAP to highlight influential sensors and late-life time steps.","The core attention block uses scaled dot-product attention: $\mathrm{Attention}(Q,K,V)=\mathrm{softmax}(QK^T/\sqrt{d_k})V$ and multi-head attention $\mathrm{MultiHead}(Q,K,V)=\mathrm{Concat}(\mathrm{head}_1,\dots,\mathrm{head}_h)W^O$ with $\mathrm{head}_i=\mathrm{Attention}(QW_i^Q,KW_i^K,VW_i^V)$. The model applies self-attention over sensors with $Q=K=V=X_F$ and then self-attention over time steps with $Q=K=V=X_T$ (the feature-attended output). Training minimizes MSE loss $L=\frac{1}{N}\sum_{i=1}^N(y_i-\hat y_i)^2$; evaluation also uses PHM score with asymmetric exponential penalty (parameters $a_1=13, a_2=10$).","On C-MAPSS, the proposed feature+time attention model reports (Table 3) RMSE/Score of 11.43/209 (FD001), 13.32/1058 (FD002), 11.47/187 (FD003), and 14.38/1618 (FD004), while the feature-only variant achieves 11.71/223, 13.26/1077, 11.69/192, and 14.14/1375 respectively. On PHM08, the proposed method achieves a Score of 1060 (Table 4), outperforming prior listed baselines such as Attn-LSTM (1584) and LSTM (1862). Parameter studies indicate feature-attention head counts around 5–10 (depending on subset) give most of the gains, and sequence attention helps mainly under single operating condition but can hurt under multiple conditions. Window size shows best performance around 30 (FD001) and about 40 (other subsets), and piece-wise RUL with $R_{max}=125$ is best for FD001/FD003 while larger $R_{max}$ may be preferable for FD002/FD004 though RMSE comparisons can be misleading.",None stated.,"Results are demonstrated only on turbofan-engine benchmarks (C-MAPSS/PHM08), so external validity to other assets, sensor sets, sampling rates, and failure mechanisms is uncertain. The approach assumes availability of run-to-failure histories and relies on design choices (piece-wise labeling with $R_{max}$, windowing, and condition clustering/normalization) that may not transfer directly to real deployments with censored data and maintenance actions. Comparisons may be sensitive to preprocessing differences across papers (e.g., normalization-by-condition, window size, and label capping), which can confound attribution of gains purely to the model architecture. The paper provides limited discussion of uncertainty quantification for RUL, which is often critical for maintenance decision-making.","The authors plan to further explore attention mechanisms for RUL estimation, including Transformer and its variants. They also intend to investigate graph neural network (GNN)-based model structures for RUL prediction. Finally, they aim to apply the proposed models to additional real industrial scenarios beyond the benchmarks.","Develop a deployment-oriented version that handles right-censored and interrupted trajectories (assets removed for maintenance before failure) and evaluates performance under realistic maintenance interventions. Add principled uncertainty estimates (e.g., Bayesian deep learning or conformal prediction) and calibrate prediction intervals for risk-sensitive maintenance planning. Extend to domain adaptation/transfer learning across fleets and operating regimes to reduce dependence on per-condition clustering and large run-to-failure training sets. Provide reproducible baselines with standardized preprocessing and release trained weights plus an easy-to-use package to improve practical adoption.",2212.05772v1,https://arxiv.org/pdf/2212.05772v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:08:51Z
TRUE,System reliability|Other,Stochastic process|Bayesian|ML-based|Simulation-based|Other,Sensor/condition monitoring|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),NA,"This paper proposes MAntRA, a model-agnostic, data-driven framework for time-dependent reliability analysis of stochastically excited dynamical systems when the governing physics is unknown. In stage 1, the method discovers an underlying Itô stochastic differential equation (SDE) from short, noisy output-only measurements by combining a candidate-function dictionary with sparse Bayesian linear regression using spike-and-slab priors and a computationally efficient variational Bayes (VB) inference scheme, yielding posterior uncertainty over drift and diffusion terms. In stage 2, the discovered SDE is propagated forward in time using stochastic numerical integration (e.g., Euler–Maruyama), and failure probabilities are computed via a displacement-based limit-state threshold. The framework is demonstrated on three numerical examples (SDOF Duffing, 3-DOF nonlinear oscillator, and a multi-DOF linear structure with tuned mass damper), where the estimated failure probabilities closely match those obtained from the true governing SDE, including extrapolation beyond the 1-second training window. The work advances reliability analysis for in-situ/heritage structures by enabling interpretable, uncertainty-aware, model-agnostic physics discovery prior to probability-of-failure estimation.","The system is modeled as an Itô SDE $d\mathbf{X}(t)=\mathbf{f}(\mathbf{X}(t),t)\,dt+\mathbf{g}(\mathbf{X}(t),t)\,d\mathbf{B}(t)$, with drift and diffusion estimated from data using Kramers–Moyal moments: $f_i(\mathbf{X}(t),t)=\lim_{\Delta t\to 0}\frac{1}{\Delta t}\mathbb{E}[X_i(t+\Delta t)-X_i(t)]$ and $R_{ij}(\mathbf{X}(t),t)=\tfrac12\lim_{\Delta t\to 0}\frac{1}{\Delta t}\mathbb{E}[(\Delta X_i)(\Delta X_j)]$ where $\mathbf{R}=\mathbf{g}\mathbf{g}^T$. Both drift and diffusion are represented as sparse linear combinations of dictionary functions, $\mathbf{Y}=\mathbf{L}\,\boldsymbol\theta+\boldsymbol\epsilon$, with spike-and-slab priors and VB used to infer inclusion probabilities and posterior mean/covariance of coefficients; failure is assessed with a limit-state $J(\zeta,t)=X(\zeta,t)-X_t$ and probability of failure computed by threshold exceedance over simulated trajectories from the discovered SDE.","Across three simulated structural dynamics examples with 1 s of training data sampled at 1000 Hz (typically $\Delta t=0.001$) and $N_t=500$ realizations corrupted with 5% noise, MAntRA accurately recovered the active basis functions (posterior inclusion probability near 1 for true terms) for both drift and diffusion. Example 1 (SDOF Duffing) recovered parameters close to true values (e.g., $c/m=2.00$ vs 2.00, $k/m=1000$ vs 999.87, $\sigma/m=1.00$ vs 1.09, $\alpha/m=100000$ vs 100369.79) and produced probability-of-failure curves matching the ground-truth SDE out to 30 s despite training on 0–1 s. Example 2 (3-DOF nonlinear) and Example 3 (multi-DOF linear structure with TMD) similarly produced failure probabilities that closely matched the reference solutions over long horizons (shown up to 30 s). The VB-based equation discovery is reported to be substantially faster than MCMC/Gibbs-sampling-based alternatives while maintaining comparable accuracy.","The authors note that if the candidate function library does not contain the true terms present in the real system, the algorithm will select correlated basis functions and the learned model becomes a surrogate rather than the true governing equation. They also state that VB is chosen for computational efficiency over more accurate MCMC methods, implying an accuracy–efficiency trade-off (VB results are described as “reasonably close” to MCMC).","The demonstrations are entirely on synthetic data generated from known SDEs; real field/heritage-structure validation (with sensor issues, nonstationarity, and modeling mismatch) is not shown, limiting evidence of practical robustness. The approach depends on assumptions of Gaussian white-noise excitation/Brownian forcing and on sufficient sampling (e.g., 1000 Hz and many trajectories), which may be unrealistic for many monitoring deployments. Reliability estimation is threshold-exceedance-based and appears to rely on forward simulation rather than rare-event acceleration (importance/subset methods), which may be inefficient or inaccurate for very small failure probabilities. Implementation details (software, numerical stability, sensitivity to dictionary design and hyperparameters) are not fully benchmarked, which could affect reproducibility and generalizability.",None stated.,"A natural extension is applying MAntRA to real structural health monitoring datasets (including heritage structures) and quantifying performance under missing data, sensor bias/drift, and nonstationary dynamics due to degradation. Methodologically, incorporating colored noise, non-Gaussian excitations, and model discrepancy terms would broaden applicability beyond Brownian-driven systems. Integrating rare-event simulation (importance sampling/subset simulation) with the discovered SDE could improve efficiency for estimating extremely small failure probabilities. Providing an open-source implementation with end-to-end benchmarks (runtime vs Gibbs/MCMC, sensitivity to dictionary/hyperparameters) and developing self-starting/online updates for time-varying physics would improve practical adoption.",2212.06303v1,https://arxiv.org/pdf/2212.06303v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:09:37Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)",Complete lifetime data|Simulated only|Other,Not applicable,Manufacturing (general)|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper develops a stress–strength reliability model where strength X and stress Y are independent and follow Exponential-Gamma(3, \lambda) distributions with possibly different rate parameters \(\lambda_1\) and \(\lambda_2\). It derives a closed-form expression for the stress–strength reliability \(R\) (interpreted as component reliability via \(R=P(Y<X)\), equivalently failure when \(X<Y\)) and constructs its maximum likelihood estimator by plugging in the MLEs of \(\lambda_1,\lambda_2\). Large-sample inference is provided via asymptotic normality of \(\hat R_{ML}\) using the Fisher information matrix and the delta method to obtain an asymptotic variance and confidence interval. Performance of the estimator is studied through Monte Carlo simulations over multiple \((\lambda_1,\lambda_2)\) settings and sample sizes, reporting bias, MSE, and CIs. A real-data illustration uses jute fiber breaking strength datasets at two gauge lengths (10mm, 20mm), including goodness-of-fit testing (KS and Cramér–von Mises) and an estimated \(\hat R\) with a 95% CI.","The model defines stress–strength reliability as \(R=P(Y<X)=\int_0^\infty f_X(x)F_Y(x)\,dx\) with \(X\sim \mathrm{EGD}(3,\lambda_1)\), \(Y\sim \mathrm{EGD}(3,\lambda_2)\), where \(f(x)=\frac{\lambda^2}{1+\lambda}\left(1+\frac{\lambda^2 x^2}{2}\right)e^{-\lambda x},\ x>0\). A closed-form expression for \(R\) is derived (Eq. 2) as a rational function in \(\lambda_1,\lambda_2\) with denominator \((1+\lambda_1)(1+\lambda_2)(\lambda_1+\lambda_2)^5\). The likelihood for samples \(x_1,\dots,x_n\) and \(y_1,\dots,y_m\) yields score equations for \(\lambda_1,\lambda_2\); the MLE \(\hat R_{ML}\) is obtained by substituting \(\hat\lambda_1,\hat\lambda_2\) into the closed-form \(R\) (Eq. 5). Asymptotic normality is given by \(\sqrt{n+m}(\hat R_{ML}-R)\xrightarrow{d}N\!0,d(\lambda)'I(\lambda)^{-1}d(\lambda)\u00021\) with \(d(\lambda)=(\partial R/\partial\lambda_1,\partial R/\partial\lambda_2)'\).","Monte Carlo experiments (1000 replications; Newton–Raphson estimation) show that average bias and MSE of the MLE-based \(\hat R\) decrease as sample sizes increase across parameter settings \((\lambda_1,\lambda_2)=(0.5,1.5),(1,1.5),(1,0.5)\), where true \(R\) values are reported as 0.8391, 0.6405, and 0.2551, respectively. In the real-data application (jute fiber breaking strength at 10mm vs 20mm gauge lengths), the paper reports \(\hat R=0.5319\) with 95% CI \((0.3936,0.6702)\). Goodness-of-fit results for EGD(3,\lambda) are provided: for 10mm, CVM p=0.4533 and KS p=0.5584; for 20mm, CVM p=0.06361 and KS p=0.1336.",None stated.,"The stress and strength samples are assumed independent and correctly specified by the EGD(3,\lambda) model; robustness to misspecification, dependence between stress and strength, or outliers is not assessed. The confidence intervals rely on asymptotic normal approximations via Fisher information and the delta method, which may be inaccurate for small \((n,m)\) despite being reported. Simulation settings are limited (three parameter pairs, equal sample sizes, 1000 replications) and do not explore unequal \(n\neq m\) or a broader range of \(\lambda\) values. The paper describes using Newton–Raphson but does not discuss convergence diagnostics, initialization sensitivity, or numerical stability for the MLE equations.",None stated.,"Develop exact or improved finite-sample inference for \(R\) (e.g., parametric bootstrap CIs) to reduce reliance on asymptotic normality in small samples. Extend the model to dependent stress–strength settings (e.g., via copulas) and to multi-component system configurations where component stresses/strengths interact. Study robustness and model selection against competing lifetime models (Weibull, Lindley-family, etc.) using broader real datasets and formal information-criterion comparisons. Provide reproducible software (e.g., an R/Python implementation) with stable numerical routines for MLE computation and CI construction.",2212.07940v1,https://arxiv.org/pdf/2212.07940v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:10:15Z
TRUE,Life distribution modeling|Maintenance optimization|Network/infrastructure reliability|Other,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|Simulation-based|Other",Right-censored|Mixture of types|Other,Age-based|Condition-based|Not applicable,Energy/utilities,Simulation study|Case study (real dataset)|Other,TRUE,Python|None / Not applicable|Other,Not provided,https://cosmotech.com/,"The paper models the lifetime of high-voltage instrument transformers (ITs) in the Dutch transmission system using real failure, maintenance (work order), and inspection data from 1989–2021, accounting for substantial right-censoring. It first estimates nonparametric survival curves with the Kaplan–Meier estimator by voltage class (110 kV, 150 kV, and 220/380 kV) and then parameterizes these curves with fitted Weibull distributions to obtain time-varying failure behavior and summary statistics (e.g., median survival time). The resulting reliability laws are integrated into a Cosmo Tech Asset (CTA) digital-twin simulation to compare maintenance/replacement strategies, notably time-based replacement at 45 years versus a condition-based trigger using an Asset Health Index (AHI)/apparent age. Scenarios are evaluated over a 100-year horizon using outputs such as replacement costs (CAPEX/TOTEX), inspection hours, and asset unavailability hours, including resource-constrained cases (e.g., 40 and 60 FTE). The study finds that replacement activities dominate TOTEX and that the benefit of switching to condition-based replacement depends strongly on the availability of human resources; under constrained HR, differences between strategies are small, while with sufficient HR, condition-based replacement can “flatten” the replacement curve and yield long-term gains.","The Kaplan–Meier survival estimate is given by $\hat S(t)=\prod_{i:t_i<t}\left(1-\frac{d_i}{n_i}\right)$, where $d_i$ is the number of failures at time $t_i$ and $n_i$ is the number at risk just prior to $t_i$, enabling inclusion of right-censored units. Weibull parameterization uses $f(t)=\frac{\beta}{\eta^\beta}t^{\beta-1}e^{-(t/\eta)^\beta}$ and $F(t)=1-e^{-(t/\eta)^\beta}$ with shape $\beta$ and scale $\eta$. The conditional failure probability over the next 3 years is computed as $P(X<t+3\mid X>t)=1-\frac{1-F(t+3)}{1-F(t)}$, where $F$ is the CDF of the fitted reliability law.","Kaplan–Meier curves are produced for three voltage families, with Weibull fits reported as: 110 kV ($\beta=6.67$, $\eta=63.79$) with median survival time 61 years; 150 kV ($\beta=6.42$, $\eta=74.20$) with median reported as infinity due to insufficient failures; and 220/380 kV ($\beta=5.65$, $\eta=77.05$) with median also reported as infinity. The underlying dataset sizes are 3168 (110 kV), 10058 (150 kV), and 2982 (220/380 kV), with right-censored counts of 255, 298, and 25 respectively. Digital-twin simulations over 100 years indicate replacement activities are the primary TOTEX driver and human-resources costs dominate replacement costs. Under restricted HR availability, the time-based and condition-based strategies show no significant difference; if HR availability is assured, switching to condition-based replacement is reported as beneficial for smoothing (“flattening”) the replacement curve and improving long-term outcomes.","The authors note substantial data challenges, including missing data in both quantity and quality and the presence of outliers, requiring alignment of multiple data sources (failure records, work orders, and expert knowledge) to build a usable failure database. They also indicate that some voltage groups have too few failures to estimate median survival time directly from the empirical (KM) survival curve.","The Weibull fit is used to parameterize KM curves, but the paper does not report goodness-of-fit diagnostics (e.g., confidence intervals for $\beta,\eta$, formal tests, or out-of-sample validation), which limits confidence in extrapolations beyond observed ages. Failure definitions combine major failures with populated events from work orders/expert opinion; potential heterogeneity/misclassification between true failures and maintenance-driven outages could bias survival estimates. The simulation conclusions about strategy dominance hinge on assumptions about resources being “unlimited” vs constrained and on the AHI/apparent-age mapping; sensitivity analyses on these assumptions and on inspection effectiveness are not clearly documented in the provided text.",None stated.,"Extend the analysis to include covariate effects (e.g., manufacturer, environment, loading, design type) via semi-parametric (Cox) or parametric regression to explain differences beyond voltage class and improve prioritization. Quantify parameter uncertainty (confidence/credible intervals) and propagate it through the digital-twin simulations to provide risk bands for cost and unavailability outcomes. Validate the fitted reliability laws with temporal holdout periods or cross-utility benchmarks and evaluate alternative lifetime models (mixtures, competing risks) to separate aging-related failures from external-cause events.",2301.01239v1,https://arxiv.org/pdf/2301.01239v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:10:53Z
TRUE,Life distribution modeling|System reliability|Maintenance optimization|Other,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|Simulation-based|Other",Complete lifetime data|Simulated only|Other,Not applicable,Energy/utilities|Manufacturing (general)|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,NA,"The paper proposes a flexible, data-driven reliability model for multi-component parallel load-sharing systems by approximating each stage-specific component cumulative hazard function with a piecewise linear function (equivalently, a piecewise-constant hazard). Load sharing is captured via multiplicative stage parameters $\gamma_j$ that increase after successive component failures, enabling unknown load-share rules to be estimated from system failure-sequence data. The authors derive likelihood-based inference (MLE via profiling), discuss cut-point selection, and provide confidence intervals using Fisher/observed information and bootstrap methods; for the special case of two components with two segments (N=2), explicit analytic MLE expressions are given. Under the fitted model they develop estimators for key reliability characteristics including reliability at a mission time (via Monte Carlo for system lifetime), quantiles, mean time to failure (MTTF), and mean residual time (MRT). Performance is evaluated through Monte Carlo studies and an illustrative two-motor load-sharing dataset, where the PLA model achieves substantially better AIC than competing parametric models and yields practical RMT/MRT estimates.","Stage-$j$ system segment survival for inter-failure time $Y^{(j)}$ is $P(Y^{(j)}>y)=\exp\{-(J-j)H^{(j)}(y)\}$, approximating $H^{(j)}$ by a piecewise linear $\Lambda^{(j)}$. The baseline hazard is approximated by a piecewise-constant function $\lambda^{(0)}(t)=\sum_{k=1}^N b_k\,\mathbf{1}_{[\tau^{(0)}_{k-1},\tau^{(0)}_k)}(t)$, and after $j$ failures $\lambda^{(j)}(t)=\gamma_j\sum_{k=1}^N b_k\,\mathbf{1}_{[\tau^{(j)}_{k-1},\tau^{(j)}_k)}(t)$ with $1<\gamma_1<\cdots<\gamma_{J-1}$ and $0<b_1<\cdots<b_N$. The likelihood for data $\{y_i^{(j)}\}$ yields closed-form $b_k(\gamma)=\frac{\sum_{j=0}^{J-1} n_k^{(j)}}{\sum_{j=0}^{J-1}(J-j)\gamma_j T_k^{(j)}}$ and a profile log-likelihood in $\gamma$ for numerical maximization; for $J=2,N=2$ they derive explicit equations (including a quadratic) for $\hat\gamma_1,\hat b_1,\hat b_2$.","On the real two-motor load-sharing dataset (18 systems) with $N=2$, the fitted load-share multiplier is $\hat\gamma_1=4.2712$ (SE 1.1901) with asymptotic CI (1.9386, 6.6038); baseline hazards are $\hat b_1=0.0034$ (SE 0.0008) and $\hat b_2=0.0134$ (SE 0.0039). Model selection favors the PLA model with AIC 369.34 versus 480.50 (Weibull frailty model) and 409.65 (generalized Freund model). The estimated system MTTF is 221.36 days; Monte Carlo mission reliability and mean residual time estimates include RMT(167.5)=0.706 with MRT(167.5)=88.678, and RMT(350)=0.044 with MRT(350)=36.919. In simulations (5000 replications; n=100,200), average estimates are close to truth and 95% CI coverage is generally near nominal for asymptotic and percentile bootstrap CIs, and robustness tests show small absolute integrated errors when fitting data generated from Weibull and quadratic-CHF models.","They note that, in general (outside the two-component/two-segment special case), MLEs are not available in explicit form and require numerical optimization, and that exact confidence intervals cannot be obtained (they rely on asymptotic Fisher/observed-information or bootstrap intervals). They also state that analytical expressions for system reliability at mission time (RMT) and mean residual time (MRT) are difficult because the system lifetime is a sum of independent but non-identically distributed inter-failure times, so they estimate RMT/MRT via Monte Carlo simulation. They caution that using too many cut-points can be computationally expensive and may lead to overfitting, affecting prediction.","The model assumes independence of latent lifetimes across stages and i.i.d. components within each stage; real load-sharing systems often exhibit dependence, heterogeneity, or common-cause effects that may violate these assumptions. The monotone/nondecreasing hazard and the multiplicative load-share factors $\gamma_j$ impose a specific structure on how load affects hazards (a uniform multiplicative increase across all time segments), which may be restrictive if load changes are time-varying or segment-specific. Cut-point selection is only heuristically/likelihood-driven and may be unstable for small samples or sensitive to quantile-grid choices; uncertainty in cut-point selection is not propagated into interval estimates. RMT/MRT are computed by Monte Carlo but guidance on required simulation size/accuracy and computational cost for larger J or larger N is limited, and comparisons are mainly to a small set of parametric competitors rather than modern semi-parametric Bayesian/process-based load-sharing models.",None stated.,"Develop self-starting/penalized or information-criterion-based procedures to jointly select the number of segments (N) and cut-point locations with uncertainty quantification. Extend the PLA framework to allow component-specific hazards and covariates (heterogeneous components), dependence (frailty/common shocks), and autocorrelated degradation/operating-environment effects. Provide analytic or numerical-convolution methods for RMT/MRT (e.g., via Laplace inversion) to reduce reliance on Monte Carlo and improve efficiency for design/optimization tasks. Release an R package implementing estimation, cut-point selection, and RMT/MRT computation to support practical adoption and benchmarking.",2301.01477v1,https://arxiv.org/pdf/2301.01477v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:11:45Z
TRUE,Life distribution modeling|Maintenance optimization|Other,"Parametric (Weibull, etc.)|ML-based|Bayesian|Hybrid/Ensemble|Other",Interval-censored|Right-censored|Event/count data|Mixture of types,Predictive,Other,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper develops Bayesian survival-analysis reliability models for Navy weapon systems that incorporate many system covariates (250+ features) to improve predictive maintenance decisions. It proposes a nonlinear Cox–Weibull model where a neural network produces a risk score that modulates a Weibull baseline survival function, and it also proposes an alternative Bayesian formulation that directly parameterizes Weibull parameters with a multitask neural network using MC-dropout. Because weapon test outcomes only reveal whether a unit failed between two recertification tests, the authors formulate an interval-censored (and right-censored) likelihood and train the models using SGD with Bayesian model averaging over uncertainty in Weibull parameters and/or network weights. They benchmark against a conditional Weibull model (current practice), XGBoost, and stochastic variational inference baselines, reporting improvements especially on positive-class metrics (precision/recall/F1) for an imbalanced dataset. The work advances reliability modeling practice by combining parametric Weibull inductive bias with deep covariate modeling and Bayesian uncertainty quantification for interval-censored weapon-system test data.","Baseline Weibull survival is defined as $S_0(t_2)=P(T>t_2)=\exp(-(\lambda t_2)^c)$. The Cox–Weibull model incorporates covariates via a neural risk score $H_\theta(x)$: $P(T>t_2\mid x)=S_0(t_2)^{\exp(H_\theta(x))}=\exp(-(\lambda t_2)^c\exp(H_\theta(x)))$, and the conditional survival between recertification times is $P(T>t_2\mid T>t_1,x)=\exp((t_1^c-t_2^c)\,\lambda^c\exp(H_\theta(x)))$. Bayesian model averaging approximates $P(y=1\mid x,t_1,t_2,\text{data})$ by Monte Carlo over sampled $(\lambda,c)$ (via MCMC or dropout-estimated) and sampled network weights (via MC-dropout).","Results are reported as mean percent change relative to the existing conditional Weibull pdf baseline, averaged over 10 runs with different splits/seeds. The MCMC Cox–Weibull model shows large gains in positive-class precision/recall/F1 (e.g., P1 +369.44%, R1 +16.67%, F1 +16.67%) and increases in ROC-AUC (+9.83%) and PR-AUC (+42.43%). The combined MCMC + MC-dropout Cox–Weibull model is similar or slightly better (ROC-AUC +9.84%, PR-AUC +43.82%, and a reported $C_{td}$ improvement of +5.75%). With an SME-selected subset of ~20 important features, the combined model improves ROC-AUC and PR-AUC relative to baseline by +12.79% and +54.54%, respectively, exceeding XGBoost on those two metrics in that setting.","The authors state that they cannot disclose certain details (including the prior distributions for Weibull parameters) due to the proprietary/sensitive nature of Navy weapon systems. They also note that MC-dropout is known to provide a poor variational approximation to the Gaussian posterior over neural-network weights, even though it is computationally cheap. They further mention convergence issues for the SVI mean-field approximation baseline when many features are used.","The modeling assumes recurrent events are conditionally independent given covariates (Andersen–Gill style) and effectively Markovian with respect to the engineered features; this may be violated if repair effectiveness varies or hidden degradation accumulates across cycles. Evaluation centers on classification metrics (ROC/PR AUC, F-scores) rather than reliability-engineering metrics such as calibration of survival probabilities, Brier score/integrated Brier score, decision-analytic utility, or maintenance cost impact, which may limit operational interpretability. The paper appears to use a fixed 0.5 threshold for pass/fail classification, which is unlikely to be optimal under severe class imbalance and asymmetric costs typical in maintenance and safety contexts. It is unclear how missing data, feature leakage across time windows, and categorical encoding/high-cardinality variables are handled, which can materially affect real-world deployment performance.","The authors propose relaxing the assumption that the system did not fail up to $t_1$ by introducing a multiplicative factor on the Weibull rate to correlate repeated failures, yielding an accelerated (or decelerated) failure model after each certification. They plan to explore stronger uncertainty-estimation methods than MC-dropout, such as Laplace approximations of linearized neural networks and ensemble approaches (e.g., MultiSWAG). They also intend to model intra-/inter-correlation between repeat events and incorporate time-dependent covariates, including Bayesian hierarchical models for subpopulations/subcomponents and longitudinal analyses.","A valuable extension would be to report and optimize maintenance-relevant decision metrics (expected cost, missed-failure rate at fixed false-alarm budget, or utility curves) and to learn operating thresholds tailored to asymmetric consequences rather than using a fixed 0.5 cutoff. The approach could be generalized to explicitly handle time-varying covariates and repair effectiveness via frailty models, marked point processes, or hierarchical recurrent-event models (e.g., random effects per weapon system), with comparisons to modern deep recurrent survival models. Additional robustness work could study sensitivity to Weibull baseline misspecification (e.g., lognormal/gamma baselines or spline-based hazards) and to dependence/autocorrelation in test scheduling and censoring mechanisms. Finally, releasing a minimal synthetic dataset plus reference implementation (or a reproducible pipeline with redacted features) would enable independent verification and broader adoption.",2301.01850v5,https://arxiv.org/pdf/2301.01850v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:12:28Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,ML-based|Bayesian,Sensor/condition monitoring|Degradation measurements,Predictive|Condition-based,Energy/utilities|Transportation/logistics|Other,Other,TRUE,Python,Not provided,https://doi.org/10.1214/aos/1013203451|https://doi.org/10.1080/10618600.2014.907095,"The paper discusses how explainable AI (XAI), interpretable machine learning (IML), and trustworthiness considerations can be incorporated into an intelligent digital twin (DT) workflow for prognostics, using remaining useful life (RUL) prediction as the case study. Using NASA’s PHM08/C-MAPSS turbofan degradation simulation dataset, the authors derive RUL labels from run-to-failure cycle data and train several inherently interpretable or interpretable-by-design models (ReLU-DNN, Explainable Boosting Machine, FIGS, and a decision-tree surrogate), implemented via the PiML Python toolbox. Global and local explanation techniques (feature selection via correlations and LGBM importance, permutation feature importance, PDP/ALE, LIME, and SHAP) consistently indicate that the operating cycle is the dominant driver of predicted RUL for the demonstrated engine unit. They also present a model “trustworthiness” diagnosis across accuracy, overfitting, reliability (conformal prediction under distribution shift), robustness (covariate perturbation), and resilience (worst-case subsampling/out-of-distribution), finding ReLU-DNN best overall in their setup. The work positions XAI/IML as enabling better maintenance planning by making DT-based RUL predictions more transparent and auditable, though the empirical study is limited to a narrow slice of the PHM08 data.","RUL is constructed from the PHM08 run-to-failure data as $\mathrm{RUL}(t)=T_{\max}^{(u)}-t$ for unit/engine $u$, where $t$ is the current operational cycle and $T_{\max}^{(u)}$ is the maximum observed cycle before failure for that unit. The DT update concept described combines Bayesian filtering (e.g., UKF) with an ML predictor (Gaussian process) to update states/parameters online, but the paper does not provide a single fully specified DT state-space equation set for the RUL case study. For operator learning illustration, they present an example ODE $\frac{ds(x)}{dx}=u(x)$ and seek an operator $G:u(x)\mapsto s(x)$ (DeepONet), emphasizing explainability challenges in operator learning.","On the (single-engine) PHM08-derived train/test split (8:2), the ReLU-DNN achieved Test MSE ≈ 0.0000, Test MAE ≈ 0.0025, and Test $R^2$ ≈ 0.9999, outperforming EBM (Test MSE 0.0070, MAE 0.0607, $R^2$ 0.9054), FIGS (Test MSE 0.0002, MAE 0.0104, $R^2$ 0.9977), and a tree model (Test MSE 0.0001, MAE 0.0102, $R^2$ 0.9980). Across global and local explanation methods (PFI, PDP/ALE, LIME, SHAP, and conditional dependence tests), the “cycle/operating cycle” feature is consistently identified as the most influential input for RUL prediction. In trustworthiness diagnostics, EBM shows higher overfitting tendency and worse robustness/resilience under perturbations and worst-case subsampling relative to ReLU-DNN/FIGS/tree, while ReLU-DNN is reported as best overall by their combined diagnostic plots.","The authors note multiple practical obstacles for DT prognostics and explainable RUL prediction, including difficulty obtaining reliable and complete operational data, complex multi-factor deterioration processes that are hard to capture even with sophisticated AI, and the challenge of translating explanations into actionable and cost-effective maintenance insights. They also state that quantitative metrics for explanation quality are still an open research question, and that user studies are needed to understand how explanations affect operator trust and actions. They further highlight unresolved issues in propagating uncertainty from data through models into explanations and limited flexibility for users to probe why specific predictions were made.","The empirical RUL demonstration uses only engine/unit #1 (223 time steps) from the PHM08 dataset, which severely limits generalizability and makes reported error metrics potentially unrepresentative of broader operating regimes and unit-to-unit variability. The RUL label definition (max-cycle minus current cycle) is a simple construction that may inflate apparent performance, especially when the dominant feature is cycle itself, and it does not reflect settings where failure threshold is latent or censoring occurs. Comparisons focus on a small set of models and do not benchmark against common RUL baselines in PHM (e.g., LSTM/GRU, temporal CNNs, transformers, survival models), nor do they report uncertainty calibration metrics beyond qualitative reliability plots.","Future work is proposed to (i) develop explainable and accelerated prediction algorithms that handle multi-fidelity and scarce data, missing/approximate physics, and generalize to new environments for real-time DT inference; (ii) create methods for sensor location optimization, sensor degradation, re-calibration, and signal reconstruction to quantify impacts on degradation/RUL prediction; (iii) build online learning algorithms to continuously update DT models/parameters for temporal synchronization; and (iv) develop uncertainty quantification methods tailored to scarce/noisy and never-before-seen noisy data, including spatiotemporal degradation patterns.","A valuable extension would be a fleet-level evaluation on the full PHM08/C-MAPSS splits (multiple units and operating conditions) with temporally aware models and proper cross-unit validation, including RUL scoring metrics used in PHM competitions. Incorporating censored data handling and decision-focused evaluation (e.g., maintenance cost/availability impact under a predictive maintenance policy) would connect explainability to reliability outcomes more directly. Providing an open-source reproducible pipeline (data processing, PiML configs, and DT/XAI analyses) and testing robustness to sensor dropouts, drift, and autocorrelation would substantially strengthen practical adoption in real condition-monitoring deployments.",2301.06676v2,https://arxiv.org/pdf/2301.06676v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:13:28Z
FALSE,Other,"Bayesian|Parametric (Weibull, etc.)|Other",Event/count data|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,https://www.health.ny.gov/diseases/chronic/ratesmall.htm|https://www.health.pa.gov/topics/HealthStatistics/EDDIE/Pages/EDDIE.aspx#.|https://www.cdc.gov/cancer/uscs/pdf/uscs-data-visualizations-tool-technical-notes-508.pdf|https://ibis.health.utah.gov/ibisph-view/pdf/resource/DataSuppression.pdf,"This paper addresses how to define and assess “reliability” of small-area event-rate estimates (incidence/prevalence) in disease mapping, noting that crude rates in sparse areas often fail common reporting rules and that popular spatial Bayesian models can oversmooth by overwhelming the data. It proposes a posterior-based reliability definition: an estimate is reliable at level 1−α if the posterior medians of π and 1−π each exceed the width of their equal-tailed (1−α) credible intervals; it also defines “relative precision” as median/CI-width, with values >1 indicating reliability. Using conjugate beta-binomial (and analogous Poisson) reasoning, the authors relate reliability to an effective required number of (posterior) events and use this idea to cap the informativeness of the Besag–York–Mollié CAR prior via an approximate ‘prior events’ measure (\hat a_0). In a Pennsylvania county preterm-birth case study (2010–2019, stratified by race/ethnicity and year), they show the standard CAR can contribute the equivalent of ~13–43 events per county-year for minority groups, producing oversmoothing and inflated precision, while a restricted-CAR (e.g., \hat a_0<5) restores a de facto requirement of ~\ge10 observed events for estimates to be deemed reliable and yields reliability maps that better reflect data support.","Core conjugate model: y_i\sim\text{Bin}(n_i,\pi_i), \pi_i\sim\text{Beta}(a_i,b_i) giving \pi_i\mid y_i\sim\text{Beta}(y_i+a_i, n_i-y_i+b_i). Reliability connects to the posterior coefficient of variation: \text{CV}(\pi_i\mid y_i)=\sqrt{\mathrm{Var}[\pi_i\mid y_i]}/\mathrm{E}[\pi_i\mid y_i] and the USCS rule CV<1/4 implies a threshold on posterior cases (y_i+a_i) roughly exceeding 16\,(1-\mathrm{E}[\pi_i\mid y_i]). For CAR disease-mapping models with \text{logit}(\pi_i)\mid\beta,z,\sigma^2\sim\mathcal N(x_i^T\beta+z_i,\sigma^2), z\sim\text{CAR}(\tau^2), they use an approximation for an effective prior-events quantity \hat a_i and baseline \hat a_0 (with m_0=3) and then restrict informativeness via an indicator I\{\hat a_0<A\} (e.g., A=5).","In Pennsylvania preterm-birth mapping (county×year, 2010–2019, stratified by race/ethnicity), the fitted standard BYM/CAR models were estimated to contribute the equivalent of roughly 13–43 preterm births per county-year for racial/ethnic minority groups (based on \hat a_{0;rt}), often exceeding observed counts in many counties and implying susceptibility to oversmoothing. A restricted model enforcing \hat a_{0;rt}<5 was designed to correspond to needing about y\ge10 observed events for 0.95-level reliability; empirically, counties deemed unreliable under the restricted model aligned with y<10. For Asian mothers in 2019, the restricted model produced unreliable estimates in 56 of 67 counties, whereas the standard CAR produced reliable estimates in all but two counties, illustrating how an overly informative spatial prior can create artificially high apparent reliability/precision. The paper also provides illustrative county examples where the standard CAR both pulls estimates toward neighbors (oversmoothing) and inflates relative precision compared with the restricted model.",None stated.,"The work is focused on “reliability” as posterior median exceeding equal-tailed CI width; this criterion is somewhat ad hoc and may not align with decision-theoretic loss, calibration, or frequentist coverage properties, especially under model misspecification. The restriction on CAR informativeness relies on approximations (delta-method/logitnormal matching and a baseline neighbor choice m0=3), so the implied ‘prior events’ cap may be sensitive to modeling choices, spatial adjacency definitions, and hyperprior specifications. Results are demonstrated mainly through one applied case study (PA preterm birth), with limited evidence of robustness across different outcomes, spatial supports, or alternative spatial priors. No publicly shared code is provided, which limits reproducibility of the restricted-CAR fitting details (referenced to web appendices).","The authors propose extending the informativeness-restriction approach to other disease-mapping models (e.g., Leroux et al., 2000; Datta et al., 2019/DAGAR) and to multivariate spatial and spatiotemporal settings (e.g., Gelfand & Vounatsou, 2003; Quick et al., 2017). They also mention applying these ideas particularly for estimating age-standardized rates from age-stratified event data.","Develop and validate self-calibrating or decision-theoretic reliability measures (e.g., based on posterior expected loss, coverage, or posterior predictive checks) and compare them to the CI-width criterion. Extend the approach to handle spatial/temporal dependence in the data likelihood (autocorrelation, overdispersion, zero inflation) and assess how ‘prior-events’ caps behave under these violations. Provide an open-source R package or reproducible scripts implementing restricted-informativeness CAR/BYM fitting (including diagnostics and sensitivity to m0/adjacency), and benchmark against alternative priors such as PC priors or BYM2 parameterizations. Evaluate performance systematically via simulation (oversmoothing vs. bias/variance trade-offs) across varying sparsity, true spatial smoothness, and mis-specified covariate structures.",2302.04582v1,https://arxiv.org/pdf/2302.04582v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:14:10Z
FALSE,NA,Bayesian|Other,Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper addresses unreliable frequentist uncertainty quantification of Bayesian posteriors under model misspecification. It proposes a “Q-posterior,” which replaces the usual log-likelihood in Bayes’ rule with a quadratic form in the score (or, more generally, estimating equations), weighted by an estimate of the score covariance, aiming to deliver credible sets with correct frequentist coverage even when the model is misspecified. The approach extends to settings with intractable likelihoods via Fisher’s identity and Monte Carlo estimation of the score (pseudo-marginal style algorithms), and also to generalized Bayes posteriors built from arbitrary loss functions. Theoretical results establish (i) convergence of the estimated Q-posterior to the infeasible Q-posterior at rate O(1/N) in the number of Monte Carlo draws and (ii) asymptotic Bernstein–von Mises-type normality with “sandwich” covariance, implying calibrated uncertainty. Simulations in linear regression and mixed/random-effects models illustrate improved empirical coverage versus exact Bayes under heteroskedasticity or random-effects misspecification, including in small samples.","The Q-posterior is defined by replacing the likelihood with a quadratic score criterion: $\pi_n^Q(\theta) \propto |W_n(\theta)|^{-1/2}\,\exp\{-Q_n(\theta)\}\,\pi(\theta)$, where $Q_n(\theta)=\tfrac12\{m_n(\theta)/\sqrt n\}^\top W_n(\theta)^{-1}\{m_n(\theta)/\sqrt n\}$ and $m_n(\theta)= -\nabla_\theta \ell_n(\theta)$. For estimated-likelihood/latent-variable cases, the score is estimated using Fisher’s identity with Monte Carlo draws, yielding $\widehat Q_n(\theta;z)=\tfrac12\widehat m_n(\theta;z)^\top \widehat W_n(\theta;z)^{-1}\widehat m_n(\theta;z)$ and a pseudo-marginal MH scheme targeting a joint posterior over $(\theta,z)$. For generalized Bayes with loss $q_n(\theta)$, the analogous construction uses $\psi_n(\theta)=\nabla_\theta q_n(\theta)$ and $\Psi_n(\theta)=\tfrac12\{\psi_n(\theta)/\sqrt n\}^\top V_n(\theta)^{-1}\{\psi_n(\theta)/\sqrt n\}$.","In linear regression with $n=100$ and heteroskedastic misspecification ($\gamma=2$), empirical 95% coverage for regression coefficients under exact Bayes dropped as low as 0.686, while the Q-posterior achieved about 0.959–0.968. In a linear random-effects regression example ($n=100$), Q-posterior coverage stayed near nominal (about 0.915–0.945) under misspecification, whereas exact Bayes coverage fell as low as 0.762. In a random-effects probit model with misspecified random-effects distribution (t with 4 df), Q-posterior coverage remained about 0.944–0.949, while exact Bayes was about 0.835–0.852. Theorem 1 shows the estimated Q-posterior (from Monte Carlo score/covariance estimates) converges in total variation to the infeasible Q-posterior at rate $O(1/N)$ for fixed sample size $n$, and asymptotic normality results imply calibrated “sandwich” covariance for credible sets.","The authors note that well-calibrated inference depends on having a consistent estimator of the score/estimating-equation covariance (e.g., $W_\star$), and if $W_n(\theta_\star)$ is inconsistent then the Q-posterior will not correctly quantify uncertainty. For latent-variable/intractable likelihood settings, they point out that naive covariance estimation can underestimate variance because it ignores Monte Carlo variability, and special variance estimators (or HAC/bootstraps under dependence) may be required. They also restrict theoretical development largely to simple Monte Carlo score estimators (rather than more complex sequential importance sampling/SMC estimators) for tractability, leaving analysis of sequential estimators as future work.","The method’s practical performance hinges on stable estimation (and inversion) of $W_n(\theta)$ or $V_n(\theta)$; in high dimensions or with weak identification, these matrices can be ill-conditioned and may require regularization, which is not systematically developed. The paper’s empirical evaluation focuses on a limited set of misspecification types (e.g., heteroskedasticity and random-effects distributional misspecification) and does not extensively benchmark against a broad suite of robust Bayesian/UQ-calibration alternatives across diverse dependence structures, heavy tails, or severe model failures. Computational cost can increase because each MCMC iteration may require Monte Carlo draws to estimate scores/variances (especially for latent-variable models), but scaling guidance and efficiency comparisons are limited. Finally, while asymptotic calibration is established, finite-sample guarantees beyond selected simulations are not fully characterized across model classes and tuning choices (e.g., number of inner draws N, proposal behavior).","They propose exploring more efficient Monte Carlo constructions for the Q-posterior beyond simple Monte Carlo, including sequential importance sampling/SMC estimators, noting that performance appears similar in ongoing work but theoretical analysis (e.g., extending results like Theorem 1) is more difficult due to the sequential dependence. They also suggest investigating alternative/efficient score estimators (e.g., importance sampling-based) in place of the simple Monte Carlo estimator used for Fisher-identity-based score approximation.","Developing regularized or shrinkage versions of $W_n(\theta)^{-1}$ for high-dimensional settings (and studying their impact on calibration) would broaden applicability. Extending the framework to dependent/time-series data more systematically (beyond suggesting HAC/bootstraps), including explicit theory for mixing processes and state-space models with particle methods, would strengthen reliability claims in common engineering monitoring contexts. Providing open-source implementations (e.g., R/Python packages) and practical diagnostics for checking covariance-estimator adequacy and numerical stability would improve adoption. Finally, studying robustness to outliers and non-smooth loss/score functions with more extensive empirical benchmarks could clarify when Q-posteriors outperform competing calibration methods.",2302.06031v1,https://arxiv.org/pdf/2302.06031v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:14:59Z
TRUE,System reliability|Other,ML-based|Hybrid/Ensemble|Simulation-based|Other,Simulated only|Other,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes active learning strategies for structural reliability analysis when multiple limit state functions must be estimated under a shared, limited high-fidelity simulation budget. The authors build PC-Kriging (Polynomial Chaos–based Kriging / Gaussian process) surrogate models and drive sequential sampling using the U-function criterion, additionally introducing a variance correction based on leave-one-out (LOO) cross-validation errors (U-LOO). For multiple limit states, they compare (i) targeting a single limit state, (ii) alternating between limit states, and (iii) an adaptive strategy that at each step targets the limit state whose estimated reliability index changes the most (a convergence-related metric). Performance is demonstrated on an analytical nonlinear two-limit-state benchmark and on a practical offshore wind jacket ship-collision scenario with “damage/repair” and “failure” penetration thresholds. Results show that multi-target schemes, especially the convergence-driven targeting, better balance accuracy across limit states than single-target sampling and that variance correction improves learning stability/accuracy.","PC-Kriging surrogate: $y\approx M_{\mathrm{PCK}}(\mathbf{x})=\sum_{\alpha\in A} a_{\alpha}\,\psi_{\alpha}(\mathbf{x})+\sigma^2 Z(\mathbf{x})$, with GP correlation $R(|\mathbf{x}-\mathbf{x}'|;\theta)$. Active learning uses the U-function: $U(\mathbf{x})=|\hat g(\mathbf{x})|/\sigma_{\hat g}(\mathbf{x})$, selecting $\mathbf{x}_g=\arg\min U(\mathbf{x})$. Variance is corrected via LOO information: $\sigma^2_{\hat g}(\mathbf{x})_{\mathrm{LOO}}=\sigma^2_{\hat g}(\mathbf{x})\left(1+\sum_{i=1}^N \frac{e^2_{\mathrm{LOO},i}}{s^2_{\mathrm{LOO},i}}\,\mathbb{I}_{\mathbf{x}\in V_i}\right)$, and multi-limit-state targeting uses $\delta(\hat\beta_{g_j})=\left|\frac{\hat\beta_{t+1,g_j}-\hat\beta_{t,g_j}}{\hat\beta_{t,g_j}}\right|$ to pick the next limit state to refine.","In the analytical two-limit-state study (budget 49 evaluations; 10 initial LHS; U scored over $10^5$ Monte Carlo candidates), single-target strategies are accurate only for the targeted limit state and can be highly inaccurate for the other (e.g., with U(PCK): $E[\varepsilon_{\beta g_1}]\approx2.4\times10^{-3}$ but $E[\varepsilon_{\beta g_2}]\approx2.7\times10^{-1}$ when targeting $g_1$ only). Multi-target strategies reduce combined error; the convergence-driven strategy $X_{g_j^*}$ yields lower combined error than simple alternation (e.g., with U(PCK): $E[\varepsilon_{\beta}]\approx5.3\times10^{-2}$ vs $6.7\times10^{-2}$ for alternation; with U(PCK-LOO): $4.3\times10^{-2}$ vs $6.8\times10^{-2}$). In the offshore wind ship-collision case (each simulation ~3 minutes), multi-target strategies converge more stably to both damage/repair and failure probabilities than single-target schemes, indicating better budget allocation across events.","The offshore wind collision study lacks a closed-form ground truth for failure/damage probabilities, so strategies there are assessed primarily via convergence behavior rather than direct error to truth. The study also notes that dedicating all active learning to a single limit state can leave other limit states poorly represented, motivating the proposed multi-limit-state strategies.","The approach assumes the surrogate uncertainty (and its LOO-corrected adjustment) is a reliable proxy for epistemic error near multiple boundaries; robustness under strong model misspecification, nonstationary behavior, or higher-dimensional inputs is not established. Comparisons are limited to U/U-LOO with PC-Kriging and do not benchmark against other multi-limit-state acquisition functions (e.g., entropy-based, multi-objective EI, or multi-class boundary learning) or other surrogate families. Practical guidance on stopping rules, sensitivity to initial design size, and scalability of Voronoi-based variance correction as $N$ and dimension grow is not explored in depth.",None stated.,"Extend the method to correlated inputs, higher-dimensional problems, and autocorrelated or time-dependent limit states (e.g., deterioration/degradation over time). Develop principled stopping criteria and multi-objective acquisition functions that directly optimize joint accuracy across several probabilities (or utility-weighted events), and benchmark against alternative active reliability methods (e.g., AK-MCS variants, entropy learning). Provide an open-source implementation and study computational scaling of the LOO/Voronoi variance correction for large training sets and higher dimensions.",2302.12074v1,https://arxiv.org/pdf/2302.12074v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:15:38Z
FALSE,NA,Bayesian|ML-based|Other,Simulated only|Other,Not applicable,Finance/economics|Other,Simulation study|Case study (real dataset),TRUE,R|Python|Other,Public repository (GitHub/GitLab),www.github.com/rodemann/reliable-pls,"This preprint studies how to more robustly select pseudo-labeled instances in self-training for semi-supervised classification, where performance is sensitive to the pseudo-label selection (PLS) criterion. The authors cast PLS as a decision problem with a likelihood-based utility and then propose robust PLS extensions that account for uncertainty in model choice (multi-model likelihood/PPP), accumulation of pseudo-labeling errors (multi-label likelihood/PPP), and covariate shift (multi-data utility). They also introduce a generalized Bayesian approach using credal sets with an b1-cut updating rule, including a regret-based b1-cut that yields a myopic expected-regret bound. Empirically, they implement several criteria using an analytical approximation to the pseudo posterior predictive (PPP) and benchmark against common PLS baselines on simulated data and UCI datasets; multi-model PPP can yield substantial accuracy gains (reported up to ~15 percentage points on simulations). A banknote-authentication case study shows multi-model PPP outperforming competitors, while multi-label variants may not help when the initial supervised model is already highly accurate.","Key utility is the pseudo-label likelihood: for candidate unlabeled instance $z=x_i$ with pseudo-label $\hat y(z)$, $u((z,Y),\theta)=p(D\cup(z,\hat y(z))\mid\theta,M)$. Bayesian PLS uses the Bayes criterion $\Phi_\pi(a)=\mathbb{E}_\pi[u(a,\theta)]$, yielding a pseudo posterior predictive (PPP). Robust variants include multi-model utilities $u=((\ell(i,1),\dots,\ell(i,K))^\top)$ or weighted sums $\sum_k w_k\ell(i,k)$, and multi-label utilities $\sum_j w_j p(D\cup(z,\tilde y_{i,j})\mid\theta,M)$. For implementation they approximate $p(D\cup(x_i,\hat y_i)\mid D,M)\approx 2\ell(\hat\theta_{ML})-\tfrac12\log|I(\hat\theta_{ML})|$ (Laplace/Fisher-information form).","Using semi-supervised logistic regression with robust PLS criteria, the paper reports that multi-model PPP can improve test accuracy by up to about 15 percentage points on simulated binomial data compared to common PLS baselines. On the Swiss banknote dataset (UCI-related), averaged over 40 repetitions with 80% unlabeled data, the multi-model PPP method achieves the best mean accuracy among the compared PLS methods. In that same case study, multi-label PPP variants do not outperform the supervised baseline when the initial supervised accuracy is already high (reported around 0.966). Overall, results suggest robustness to model choice is particularly beneficial relative to likelihood/max-max, standard PPP, probability-score, predictive-variance, and supervised-only baselines.",None stated,"The work is not a reliability-engineering study and does not validate methods on reliability data types (time-to-failure, censoring, degradation, or maintenance outcomes), limiting relevance to reliability practice. Several proposed robust decision criteria (e.g., credal-set b1-cut updating and generalized stochastic-dominance selection) are largely theoretical here; only a subset is implemented, and the approximation to PPP may affect conclusions when the Fisher-information/Laplace assumptions are weak. Reported empirical evidence focuses on accuracy and selected datasets; broader benchmarking (more domains, class imbalance, calibration, sensitivity to hyperparameters/thresholds) is not shown in the provided text.","The authors suggest exploring the accumulated expected errors perspective (Section 3.2) as an adaptive learning rate for fractional Bayesian updating. They also propose implementing and testing the more generic robustifications based on credal sets and b1-cuts introduced in Section 4. Finally, they note the need to clarify interactions among different robustifications (e.g., combining multi-label and multi-model PLS).","Test robustness under distribution shift and dependence more systematically (e.g., temporal autocorrelation, covariate drift) and evaluate beyond accuracy (e.g., calibration, selective risk, stability across self-training iterations). Provide ablation and sensitivity analyses for thresholds ($\tau,\xi$), model-set construction, and regret-based b1 choices, plus computational scaling studies for large model ensembles. Release a minimal reproducible package/API and document recommended defaults to facilitate adoption and fair comparison in future SSL benchmarks.",2303.01117v1,https://arxiv.org/pdf/2303.01117v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:16:21Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Event/count data|Other,Not applicable,Theoretical/simulation only,Approximation methods|Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/sanjaymjoshi/relistats|https://github.com/sanjaymjoshi/relistats_notebook,"This preprint reviews computation of reliability for success–failure (Bernoulli/binomial) experiments, defining reliability as the probability of success and presenting associated confidence and “assurance” (set by equating reliability and confidence) for clearer communication. It provides exact binomial expressions for confidence (via the binomial CDF/survival function) and discusses that inverting the confidence expression to obtain reliability generally lacks closed form for nonzero failures. The paper compares an approximate closed-form inversion using the Wilson score interval (with continuity correction) against numerical root-finding, recommending Brent’s method (SciPy brentq) as fast and more accurate for computing reliability and assurance from 
, f, and target confidence/assurance. It includes computed tables/plots over multiple sample sizes and failure counts illustrating how reliability (at fixed confidence), confidence (at fixed reliability), and assurance vary with n and f. Two open-source Python resources are referenced: a library for computations and a Jupyter/Colab notebook used to generate the tables and figures.","Reliability point estimate is $\hat r=(n-f)/n$ for $f$ failures in $n$ trials. Confidence for achieving at least reliability $r$ is $c=1-\sum_{k=0}^{f}\binom{n}{k}(1-r)^k r^{n-k}$ (with the special zero-failure case $c=1-r^n$), and assurance sets $a=r=c$ yielding $a=1-a^n$ (zero failures) or $a=1-\sum_{k=0}^{f}\binom{n}{k}(1-a)^k a^{n-k}$ (general). For computing $r$ (or $a$) from target $c$ (or $a$), the paper either uses the Wilson score lower bound approximation (with continuity correction) or solves the corresponding root equation numerically using Brent’s method (SciPy brentq).","For computing reliability from $(n,f,c)$, the numerical root-solve (Brent’s method with ~0.1% tolerance) is shown to outperform the Wilson score interval approximations in an accuracy study at $n=40$ with failures $f=1\ldots39$. The paper tabulates reliability at 95% confidence for $n=10\ldots100$ and $f=0\ldots5$ (e.g., with $f=0$, reliability increases from 74.1% at $n=10$ to 97.0% at $n=100$; with $f=5$, from 22.2% at $n=10$ to 89.8% at $n=100$). It also tabulates confidence at fixed reliability (90%) and assurance vs. $(n,f)$, noting (from Table 3) that assurance of 99% is exceeded at 459 samples with zero failures. Overall, it concludes Wilson-with-continuity-correction can be within roughly <5% error in typical settings, but Brent’s method provides higher accuracy and remains computationally fast.",None stated.,"The work assumes independent, identically distributed Bernoulli trials with a constant success probability; real test programs often exhibit dependence (common-cause, lot effects) or time-varying reliability, which would invalidate simple binomial modeling. The study does not frame the reliability/confidence calculations in a formal Bayesian framework (despite “assurance” being related to prior/posterior thinking), nor does it compare against standard exact methods for binomial confidence bounds (e.g., Clopper–Pearson/Beta quantiles) in depth. Numerical performance claims are based on a specific computing environment and limited settings; broader benchmarking (large n, extreme f/n, multiple tolerances) and sensitivity to floating-point issues would strengthen the conclusions. Practical guidance on test planning (choosing n and allowable f under constraints) is limited beyond providing tables/plots.",None stated.,"Extend the library/paper to include and compare exact binomial confidence bounds (e.g., Clopper–Pearson via Beta inverse CDF) and other widely used intervals (Jeffreys, Agresti–Coull) under common decision criteria. Add methods for non-i.i.d. data such as overdispersed Bernoulli trials (beta-binomial) or hierarchical models capturing lot-to-lot variability, and quantify their impact on confidence/assurance. Provide test-planning utilities (optimize n/f for target reliability–confidence under cost/time constraints) and diagnostic checks for independence/overdispersion. Validate the approach with real-world pass/fail qualification datasets and supply reproducible benchmarking scripts.",2303.03167v1,https://arxiv.org/pdf/2303.03167v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:17:05Z
FALSE,NA,ML-based|Nonparametric/Semi-parametric|Other,Other,Not applicable,Healthcare/medical|Environmental monitoring|Other,Other,TRUE,R,Not provided,https://doi.org/10.1016/j.cmet.2016.05.011|https://doi.org/10.1023/A:1010933404324|https://www.stat.berkeley.edu/~breiman/Using_random_forests_V3.1.pdf|https://doi.org/10.3109/09553002.2015.1062571|http://wonder.cdc.gov/cmf-icd10.html|https://doi.org/10.1080/10962247.2016.1240725|https://CRAN.R-project.org/package=quantreg|https://CRAN.R-project.org/package=randomForest|https://doi.org/10.1515/stat-2022-0108|https://CRAN.R-project.org/package=RXshrink|https://CRAN.R-project.org/package=LocalControlStrategy|https://doi.org/10.1016/j.yrtph.2019.104418|http://libjournal.uncg.edu/ncjms/article/view/2299/1671|https://doi.org/10.1097/00004032-200204000-00011|https://doi.org/10.1038/s41467-021-27484-1|https://www.R-project.org|https://magazine.amstat.org/blog/2010/09/01/statrevolution/|https://doi.org/10.1016/j.yrtph.2017.06.003|https://files.eric.ed.gov/fulltext/ED616199.pdf|http://journal.r-project.org/archive/2013-1/kahle-wickham.pdf,"The paper analyzes cross-sectional observational data for 2,812 US counties, comparing 2012 lung-cancer mortality and 2016 circulatory/respiratory mortality and finding the two rates are weakly related. To enable prediction, the authors define a single strictly-positive “compromise” mortality rate, $C\mathrm{mort} = \tfrac{1}{2}(\mathrm{CRmort}_{2016} + 10\cdot \mathrm{lcanmort}_{2012})$. They model $C\mathrm{mort}$ using nonparametric supervised learning, primarily a random forest (500 trees) with “Top Ten” predictors spanning longevity, socioeconomic factors, and air-chemistry variables. Predictor effects are interpreted via partial dependence plots (PDP) and individual conditional expectation (ICE) plots, and importance is summarized by %IncMSE and node purity. The authors emphasize that marginal relationships are highly non-linear and argue that several regulated-air-chemical predictors (ozone, NO2, radon) show decreasing mortality with higher levels in these observational data, contrasting with common regulatory assumptions.","The outcome is the compromise mortality rate defined as $C\mathrm{mort}=\frac{\mathrm{CRmort}_{2016}+10\cdot \mathrm{lcanmort}_{2012}}{2}$ (rates per 100K). A random forest regression model with 500 trees is fit to predict $C\mathrm{mort}$ from 10 predictors; variable importance is reported via randomForest’s %IncMSE and IncNodePurity, and marginal effects are displayed using PDP and ICE curves.","Across 2,812 counties, the two original mortality rates have little in common (reported correlation about 0.208 for 2012 lung cancer vs 2016 CR mortality). In the random forest, the most important predictors by %IncMSE are PremDeath (53.7), Elderly (49.0), Smoking (43.4), Bvoc (39.5), and Ozone (34.4), with Radon ranked 10th (19.7). PDP/ICE interpretations suggest Cmort increases monotonically with PremDeath, Elderly (above ~10%), Smoking, and ChildPov; Cmort decreases monotonically with Ozone and (above ~8 pCi/L) with Radon; Sulfates show a non-monotone peak near SO4 ≈ 1.2%. The authors conclude mortality tends to be lower in counties with average indoor radon above 4 pCi/L (EPA mitigation threshold) within these data, while secondary organic aerosol-related variables (Bvoc/Avoc) are associated with higher mortality.","The authors repeatedly note the data are observational, highly variable across counties and from year to year, and therefore limited for making accurate comparisons and forecasts. They also state PDPs “ignore potential interaction effects” because they average over other predictors. They remark that standard linear models assuming uncorrelated, homoscedastic errors may be inappropriate for these data.","The analysis relies on ecological (county-level) associations, so results are vulnerable to ecological fallacy and unmeasured confounding (e.g., healthcare access, occupational exposures, migration, coding/reporting differences). Using different years (2012 vs 2016) and combining outcomes into a constructed $C\mathrm{mort}$ may blur distinct etiologies and introduce arbitrary scaling (the factor of 10) and interpretability challenges. Random-forest PDP/ICE provide marginal associations, not causal effects; without causal identification strategies, claims about regulation-relevant effects (e.g., radon “beneficial”) are not established. Predictive performance is discussed qualitatively; the paper excerpt does not report out-of-sample metrics (RMSE/$R^2$, calibration) or uncertainty intervals for PDPs, which limits assessment of robustness.",None stated.,"Validate predictive performance with explicit train/test or cross-validation and report error metrics and calibration, including sensitivity to the factor-of-10 scaling and alternative outcome constructions. Extend modeling to account for spatial autocorrelation and hierarchical structure (state/county effects) and quantify uncertainty for PDP/ICE via resampling. Explore interaction effects explicitly (e.g., radon × smoking, poverty × air chemistry) using 2D PDPs, accumulated local effects, or causal forests. Where policy claims are made, apply causal inference designs (e.g., instrumental variables, negative controls, or quasi-experiments) to test whether associations persist under stronger identification assumptions.",2303.03343v3,https://arxiv.org/pdf/2303.03343v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:17:52Z
FALSE,NA,Other,Simulated only,Not applicable,Theoretical/simulation only,Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops a non-asymptotic convergence theory for deterministic samplers used in diffusion generative modeling, focusing on DDIM-type samplers and the probability flow ODE. It introduces an operational “restoration–degradation” interpretation: one probability-flow ODE step can be viewed as (i) a restoration step that performs (approximate) gradient ascent on a conditional log-likelihood at a slightly earlier time, followed by (ii) a degradation step that simulates the forward process using “noise” pointing back toward the current iterate. This viewpoint generalizes DDIM sampling beyond linear diffusions to arbitrary (possibly nonlinear) forward SDEs and yields polynomial (in dimension/parameters) conditions on discretization hyperparameters (step size and an inner parameter ℓ) that ensure small KL divergence to the target distribution. The analysis relies on an interpolation argument for ODE-driven processes and requires higher-order smoothness assumptions to control time derivatives of Fisher-information-like quantities, which do not automatically appear as dissipative terms in the deterministic (ODE) setting. The results also imply analogous bounds for the Euler discretization of the probability flow ODE, providing one of the first polynomial-complexity, non-asymptotic guarantees for deterministic diffusion samplers.","The forward process is an SDE $d x_t = f_t(x_t)\,dt + g(t)\,dW_t$ (Eq. 1), and the deterministic reverse is the probability flow ODE $d x^{\leftarrow}_t = -\{f_{T-t}(x^{\leftarrow}_t) - \tfrac{1}{2}g(T-t)^2\nabla\log q^{\leftarrow}_t(x^{\leftarrow}_t)\}\,dt$ (Eq. 3). The proposed operational decomposition uses a restoration operator $R_{t\to s}(x)=x-(t-s)f_t(x)+(t-s)g(t)^2\nabla\log q_t(x)$ (Eq. 7) and a degradation (Euler–Maruyama) operator $D^{\gamma}_{s\to t}(x)=x+f_s(x)(t-s)+g(s)\sqrt{t-s}\,\gamma$ (Eq. 12). The DDIM-type sampler step restores $z=R_{kh\to (k-\ell)h}(x_{kh})$ and then degrades using a “simulated noise” $\gamma$ satisfying $D^{\gamma}_{(k-\ell)h\to kh}(z)=x_{kh}$ (Eqs. 18–20), yielding an update that converges to the Euler step of the probability-flow ODE as $\ell\to\infty$ and $\ell h\to 0$ (Eq. 14).","The main guarantee (Theorem 4.1) shows that, under Lipschitz/smoothness conditions on $f_t$, $g(t)$, and score functions $\nabla\log q^{\leftarrow}_t$, there exist polynomially bounded constants $C_1,C_2$ such that the sampler’s final law $\tilde p$ satisfies $\mathrm{KL}(\tilde p\|q)\le \varepsilon$ provided $\ell\ge C_1$ and $\ell h\le C_2^{-1}$. The bounds are “polynomial complexity” in dimension and problem parameters (up to exponential-in-$\int L_{sc,t}dt$ terms that the authors argue are typically polynomial in relevant regimes). As a corollary for the Ornstein–Uhlenbeck forward process initialized from a Gaussian, the total variation error to the data distribution is bounded by the sampler’s KL-based term plus an exponentially small-in-$T$ term capturing mismatch between the forward marginal at time $T$ and stationarity (Corollary 4.2). The paper also provides a generic ODE interpolation bound (Theorem 4.3) controlling $\mathrm{KL}(\pi'_T\|\pi_T)$ in terms of accumulated drift mismatch and Fisher-information growth terms, highlighting the key difference from SDE analyses where an explicit negative Fisher-information dissipation term appears.","The authors note that the quantitative dependence on parameters (notably dimension and Lipschitz constants of score functions) is not sharp; despite intuition that removing Brownian noise could improve dimension dependence, the current iteration complexity remains only some polynomial in $d$. They also explicitly ignore score estimation error (assuming access to exact scores), stating that while sub-Gaussian score error might be handled via change-of-measure arguments, new ideas are needed for the common setting where only an $L_2$ score error bound is available. Finally, they leave open whether the higher-order smoothness assumption (involving higher derivatives of $\log q_t$) is truly necessary for non-asymptotic guarantees in the ODE setting.","The guarantees are in KL (and TV via additional steps) under strong global regularity assumptions (uniform Lipschitzness and higher-order smoothness) that may not hold for real diffusion model score networks or complex data distributions with sharp manifolds. The sampler analyzed depends on hyperparameters (e.g., $\ell$ with $\ell\to\infty$ and $\ell h\to 0$) and uses a restoration step requiring score evaluation at specific times; practical DDIM implementations often deviate (finite networks, discretized schedules, approximate scores), so the theoretical sampler may not tightly correspond to deployed pipelines. The analysis does not address computational cost vs. accuracy tradeoffs in terms of score network evaluations per effective step (especially if restoration is interpreted as optimization), nor robustness to discretization schedules or adaptive step sizes commonly used in practice. Empirical validation is not central here, so it is unclear how predictive the polynomial bounds are for observed sample quality or failure modes across domains.","They propose sharpening parameter dependence (especially dimension and Lipschitz/score smoothness) and exploring whether deterministic probability-flow ODE sampling can achieve better dimension scaling than SDE-based sampling. They suggest extending the theory to incorporate score estimation error, noting that sub-Gaussian-tail errors might be tractable but that handling only $L_2$-bounded score error remains open. They also raise the question of whether higher-order smoothness assumptions are actually necessary. On the empirical side, they propose tuning new hyperparameters introduced by their discretization (including $\ell$), and exploring improved restoration procedures (learning rate, multiple gradient steps, momentum, higher-order methods) that could connect to higher-order ODE solvers (e.g., Heun’s method) and improve sample quality/efficiency.","A natural extension is to analyze self-starting or adaptive schemes that choose $h$ and $\ell$ online based on local error indicators, potentially yielding tighter practical guarantees. Another direction is to develop robustness analyses for heavy-tailed, multimodal, or manifold-supported data where global Lipschitz and high-order smoothness fail, perhaps via localized or Wasserstein-based bounds. Incorporating learned (approximate) scores with realistic neural-network error models (e.g., bias + variance, time-dependent calibration error) and proving stability of the restoration–degradation decomposition under such errors would improve relevance to practice. Finally, providing open-source reference implementations and empirical benchmarks comparing this generalized DDIM framework across nonlinear forward corruptions would help validate the operational interpretation and guide practitioner adoption.",2303.03384v1,https://arxiv.org/pdf/2303.03384v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:18:27Z
FALSE,NA,Other,Other,Not applicable,Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper discusses how to quantify inter-rater reliability and confidence intervals for human evaluation scores in NLP (especially machine translation quality evaluation) when the number of observations is extremely small (1–3 ratings). It introduces the Abbott–Rosenblatt/Furnival (ARF) single-observation confidence interval approach that combines one measurement with an externally known/prior expected mean, and illustrates that resulting intervals are typically very wide. For two observations, it advocates using Student’s t-distribution (with unknown population standard deviation) to compute a confidence interval around the sample mean, and shows a worked example with two human quality scores where the 80% CI is still broad but narrower than the one-observation ARF interval. The authors recommend routinely reporting IRR-related uncertainty (e.g., confidence intervals) alongside human evaluation results to improve transparency and interpretability in NLP evaluation practice. The contribution is methodological guidance for small-sample IRR/uncertainty reporting in human-judgment evaluation, rather than reliability engineering of physical systems or components.","For one observation with an externally known prior/expected value \(\hat\mu\) and measurement \(y\), the ARF interval is \(\frac{y+\hat\mu}{2} \pm k\,|y-\hat\mu|\), where \(k\) is determined by \(k=\frac{1-\alpha+\sqrt{1-2\alpha}}{2\alpha}\) for \(0<\alpha\le 0.5\). For \(n\) observations with unknown population \(\sigma\), Student’s t CI uses \(E=t_{\alpha/2}\,(s/\sqrt{n})\), \(s=\sqrt{\sum (x_i-\bar x)^2/(n-1)}\), and \(\mathrm{CI}=(\bar x-E,\bar x+E)\).","Single-observation ARF examples: with \(\hat\mu=96.3\) and \(y=85.2\), a 75% CI (\(k=1.8\)) yields \(90.75\pm19.98\Rightarrow[70.77,100]\); an 80% CI (\(k=2.31\)) yields \(90.75\pm25.64\Rightarrow[65.1,100]\). Two-observation t-example with QS1=76.85, QS2=81.99: \(\bar x=79.42\), \(s=3.6345\); for 80% CI with df=1, \(t_{\alpha/2}=3.078\), giving \(E\approx7.91\) and CI \([71.51,87.33]\). The paper notes this reduces relative half-width from about 25% (single-measurement example) to about 9.96% (\(7.91/79.42\)) in the two-measurement example.","For the single-observation ARF approach, the method assumes an externally available pre-estimated/expected score \(\hat\mu\); the authors note this may not exist for new tasks and can be costly to obtain. For the two-observation Student’s t approach, the authors note it requires mathematical calculations and use of parameter tables, which may be inconvenient for practitioners (e.g., translators/project managers) without statistical knowledge. They also state that real-world use may require preliminary setup and clearer practitioner-oriented guidance.","The work addresses inter-rater reliability/uncertainty in human scoring, but does not model rater-specific bias, heteroscedasticity, or dependence between raters (e.g., shared guidelines or anchoring), which can dominate uncertainty with \(n\le3\). Treating two quality scores as an IID sample and applying a t-interval implicitly assumes approximate normality of the underlying score distribution and meaningful interval-scale properties, which may not hold for bounded 0–100 scores and rubric-based ratings. The paper does not benchmark the proposed CI reporting approach against established IRR estimators suited to continuous ratings (e.g., ICC variants with Bayesian/bootstrapped uncertainty) in small-sample regimes, nor does it quantify coverage properties via simulation under plausible rater models. The “IRR” discussion is largely framed as confidence intervals for the mean score rather than agreement/reliability coefficients per se; these are related but not equivalent constructs.",The authors suggest developing additional clear and crisp guidance and preliminary setup for practitioners to apply the described calculations in real-world settings. They also encourage applying Student’s t-distribution-based CI reporting to other NLP tasks and potentially beyond NLP to other domains where observations are scarce.,"Provide a small-sample framework that directly targets inter-rater reliability (e.g., ICC with Bayesian priors or hierarchical rater models) and reports posterior intervals/credible intervals alongside point estimates. Study empirical coverage of ARF and t-based intervals for bounded rating scales via simulation, including robustness to non-normality and rater bias, and recommend corrections (e.g., bootstrap, Bayesian truncated-normal/Beta models). Extend the approach to more realistic settings with multiple segments/items per system (nested designs: rater × item) and derive uncertainty for system-level comparisons (paired differences) rather than single aggregated scores. Release simple tooling (e.g., a lightweight web calculator or spreadsheet templates) and validate on real TQE datasets to assess usability and decision impact (PASS/FAIL decisions near thresholds).",2303.04526v2,https://arxiv.org/pdf/2303.04526v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:19:05Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Right-censored|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,NA,"The paper introduces the Type-1 Pathway Generated Exponential (PGE-1) distribution, a right-truncated lifetime model obtained by applying the type-1 pathway-generated family to an exponential baseline CDF. It derives a closed-form stress–strength reliability expression for independent strength and stress variables, $R=P(X>Y)$, when both follow PGE-1 with common $(a,\delta,\lambda,q)$ and differing $(\eta_1,\eta_2)$ parameters. Point estimation of $R$ is developed via maximum likelihood estimation (solving nonlinear score equations for six parameters), and interval estimation is provided using asymptotic normal theory (via the observed Fisher information) and parametric bootstrap (percentile and bootstrap-t). Performance is assessed using extensive Monte Carlo simulation across multiple sample sizes and three reliability levels, reporting bias, MSE, and interval lengths for ACI, Boot-p, and Boot-t. A real-data illustration fits PGE-1 to right-truncated AIDS incubation times (teens vs adults) and estimates the probability that teen incubation exceeds adult incubation as $\hat R=0.2414$.","The PGE-1 CDF/PDF are given by Eq. (7)–(8), with finite upper support due to right truncation. For independent $X\sim\text{PGE-1}(a,\delta,\lambda,\eta_1,q)$ and $Y\sim\text{PGE-1}(a,\delta,\lambda,\eta_2,q)$, the stress–strength reliability has the closed form $R=P(X>Y)=\dfrac{\eta_2+1-q}{\eta_1+\eta_2+2(1-q)}$ (Eq. 12). The plug-in MLE is $\hat R=\dfrac{\hat\eta_2+1-\hat q}{\hat\eta_1+\hat\eta_2+2(1-\hat q)}$ (Eq. 14), with asymptotic variance estimated by the delta method using $\widehat{\mathrm{Var}}(\hat R)\approx G^\top \hat I^{-1} G$.","Monte Carlo results (Table 1) show decreasing bias and MSE of $\hat R$ as sample size increases for all three target reliabilities (approximately 0.1428, 0.5362, 0.9054). For example, when true $R\approx0.1428$, $\hat R$ moves from 0.1713 (n=10) toward 0.1416 (n=100), with MSE dropping from 0.0068 to 0.0004. In the AIDS blood transfusion application (295 cases; right-truncated incubation times), PGE-1 fits both teen and adult groups by K–S tests (p-values 0.5758 and 0.4935, respectively) and yields $\hat R=0.2414$ for $P(X>Y)$ (teen latent time exceeding adult latent time).",None stated.,"The stress–strength closed form $R=(\eta_2+1-q)/(\eta_1+\eta_2+2(1-q))$ relies on both groups sharing $(a,\delta,\lambda,q)$, which may be restrictive in applications and is not explored via sensitivity analysis. The work assumes independence between stress and strength (or between the two groups in the medical example), and does not address dependence or shared frailty, which can materially affect $R$. Although the data are described as right-truncated, the paper does not clearly detail how truncation is incorporated in the likelihood (e.g., conditional-on-truncation likelihood), which is crucial for unbiased inference under truncation.",None stated.,"Extend inference on $R$ to allow distinct $(a,\delta,\lambda,q)$ across $X$ and $Y$ (or provide tests/diagnostics for the shared-parameter assumption) and study identifiability and finite-sample behavior under the more general model. Develop methods that explicitly incorporate right truncation in the likelihood (and potentially left truncation) and compare against alternative truncated survival models. Provide robust/self-starting or Bayesian estimation procedures and software (e.g., an R implementation) to improve reproducibility and practitioner adoption.",2303.05060v1,https://arxiv.org/pdf/2303.05060v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:19:41Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Accelerated testing,Stochastic process|Bayesian|Simulation-based|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types|Simulated only|Other,Condition-based|Predictive|Opportunistic|Not applicable|Other,Energy/utilities|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://sites.rutgers.edu/azizezzat/research-videos-multimedia/|https://www.boem.gov/renewable-energy/lease-and-grant-information|https://oswbuoysny.resourcepanorama.dnvgl.com|https://polar.ncep.noaa.gov/waves/wavewatch/|https://dataminer2.pjm.com,"The paper proposes POSYDON, a decision-theoretic stochastic optimization framework that jointly optimizes short-term wind turbine yaw control (production) and long-term maintenance scheduling for offshore wind farms. A key reliability contribution is a yaw-dependent degradation/RUL model for wind turbine blades that starts with a Wiener-process (Brownian motion with drift) baseline degradation model and applies a time-transformation to incorporate load changes induced by wind speed and yaw misalignment; this yields an inverse-Gaussian RUL distribution. These RUL predictions are embedded into a stochastic MILP with scenario-based forecasts (wind, waves, prices, degradation) to co-optimize yaw decisions and opportunistic maintenance under resource and accessibility constraints. Numerical experiments using real-world met-ocean and price data (NY/NJ Bight, PJM) show POSYDON outperforms benchmarks that optimize production and maintenance separately (or deterministically), reducing total cost and production/downtime losses in both 5-turbine and 50-turbine case studies. The work advances offshore wind O&M by explicitly modeling the feedback loop between operational control actions (yaw) and long-term reliability/maintenance consequences within a unified prescriptive model.","Baseline degradation is modeled as a Wiener process with drift: $A_0(t)=\alpha_0+\beta_0 t+\sigma W(t)$, with Bayesian updating for $(\alpha_0,\beta_0)$ from observed degradation signals. Under time-varying loads driven by wind speed and yaw misalignment, the model uses a time-transformed process $A(t)=\alpha_0+\int_0^t \beta_0\Psi(\phi(z;\tilde\gamma,\nu))dz+\int_0^t \sigma\Psi(\phi(z;\tilde\gamma,\nu))^{1/2}dW(z)$, implying RUL $\lambda$ follows an inverse-Gaussian distribution with transformed time $\tau(t)=\int_0^t\Psi(\cdot)dz$. The optimization embeds yaw-dependent RUL via $\lambda_{i,s}=\lambda^0_{i,s}+\frac{\zeta^0_{i,s}}{24}\sum_{t\in T}\Big(1-\sum_j \gamma_{t,i,j}F_{t,i,j,s}\Big)+\sum_{d\in D}\zeta^{0,L}_{d,i,s}\Big(1-\sum_j\gamma^L_{d,i,j,s}F^L_{d,i,j,s}\Big)$ and maximizes expected profit minus interruption and end-of-horizon costs in a stochastic MILP.","In the 5-turbine case study, POSYDON achieves total cost $159.3\,$K$ versus $273.8\,$K$ (STOCHOS), $398.4\,$K$ (DET), and $715.0\,$K$ (TBS), corresponding to 41.8%, 60.0%, and 77.7% reductions, respectively. Production loss is 1.5 GWh for POSYDON versus 2.5 (STOCHOS), 5.8 (DET), and 9.7 (TBS); POSYDON has 0 corrective tasks (vs 3, 4, 8) and 7 vessel rentals (vs 14, 11, 28). In the 50-turbine case, POSYDON yields total cost $2.5\,$M$ versus $3.1\,$M$ (STOCHOS), $3.6\,$M$ (DET), and $6.1\,$M$ (TBS), and production loss 32.0 GWh versus 38.7, 54.9, and 88.2 GWh. POSYDON also reduces downtime (190.2 days vs 247.4 STOCHOS) and access-related downtime (158.2 vs 215.4), and has fewer lost cycle days per task (7.8 vs 9.7 STOCHOS and 22.8 TBS).",None stated.,"The degradation/RUL model is demonstrated using blade-load relationships from prior simulated FBM data and vibration degradation signals from an accelerated life rotating machinery dataset, rather than blade-specific in-field degradation measurements, which may limit external validity for real offshore blade fatigue. The approach discretizes yaw misalignment into a small set of levels and uses scenario-based forecasts; solution quality and computational burden may be sensitive to discretization granularity, scenario count, and forecasting errors. The MILP relies on several approximations/linearizations (e.g., dynamic maintenance cost linearization and availability/power approximations) that may affect fidelity relative to true nonlinear turbine physics and maintenance processes. Results are based on numerical case studies for a specific region and parameterization; broader benchmarking across farms, turbine types, and alternative load/health models is not shown.","The authors state they are investigating adding further decision dependencies into POSYDON, including multi-turbine dependencies induced by wake interactions and spatial effects. They also suggest the O&M modeling framework may be transferable to other renewable generation assets with similar short-/long-term trade-offs, including land-based wind farms and wave energy converters.","Validate and calibrate the yaw-dependent degradation/RUL model using long-horizon field data for blade health (e.g., inspections, SCADA-derived fatigue proxies, or digital-twin estimates) to quantify model bias and uncertainty. Extend POSYDON to handle nonstationary/autocorrelated degradation signals and to incorporate robust or distributionally robust optimization against forecast/model misspecification. Develop a continuous (non-discretized) yaw decision variant or tighter approximations to reduce discretization error while preserving tractability (e.g., piecewise-linear convexifications). Provide an open-source implementation or reproducible benchmark datasets to facilitate adoption and comparison with alternative co-optimization methods.",2303.06174v2,https://arxiv.org/pdf/2303.06174v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:20:30Z
TRUE,System reliability|Other,Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a relaxation-based importance sampling (RIS) framework for structural reliability, where a rare-event probability is estimated through a sequence of relaxed intermediate problems obtained by introducing tunable relaxation parameters into the indicator function and/or the input probability density. RIS is shown to unify subset simulation, sequential importance sampling, and annealed importance sampling as special cases. Building on the framework, the authors propose two new importance sampling strategies: IS-I (combining annealed importance sampling with subset simulation) aimed at low-dimensional problems, and IS-II (a spherical formulation with scaling in the limit-state evaluation) aimed at high-dimensional problems. Both strategies are designed to efficiently generate an entire fragility surface (probability of failure as a function of intensity measure and performance threshold) in a single run. Numerical studies include a 2D parabolic limit-state reliability problem and 2D/1000D seismic fragility examples, demonstrating substantial efficiency gains over subset simulation for fragility surface estimation.","The reliability problem is posed as $P_f=\int I(G(x)\le 0) f_X(x)\,dx$, with importance sampling $P_f=\int I(G(x)\le 0)\frac{f_X(x)}{h(x)}h(x)\,dx$ and optimal density $h^*(x)=I(G(x)\le 0)f_X(x)/P_f$. RIS defines a sequence $\eta_j(\cdot;\lambda_j)$ (relaxed integrands) with probabilities $P_j=\int \eta_j\,dx$ and optimal intermediates $h_j^*=\eta_j/P_j$, yielding the telescoping product $P_f=P_1\prod_{j=1}^{T-1}\int \frac{\eta_{j+1}}{\eta_j} h_j^*\,dx$. Specializations include subset simulation ($\eta_j=I(G\le \lambda_j)f_X$), sequential IS via smoothing ($\eta_j=\Phi(-G/\lambda_j)f_X$), annealed IS via covariance scaling ($\eta_j=I(G\le 0)f_X(\cdot;I\lambda_j^2)$), and the new IS-I/IS-II strategies with two relaxation parameters to obtain fragility surfaces.","In the 2D parabolic limit-state example, annealed importance sampling matches direct Monte Carlo reference probabilities while using far fewer model evaluations and improves efficiency relative to HMC-based subset simulation: for $d=5$, $\hat P_f\approx3.00\times10^{-3}$ with $\bar N=2800$ vs subset simulation $3.05\times10^{-3}$ (also $\bar N=2800$) but with a higher reported error metric ($\delta P_f$ 0.1438 vs 0.2301). For $d=9$ (rarer event), annealed IS achieves $\hat P_f\approx4.20\times10^{-5}$ with $\bar N=2800$ vs subset simulation requiring $\bar N=4600$ for similar accuracy. For the 2D seismic fragility surface, IS-I produces a smooth fragility surface with about 3700 limit-state evaluations versus $6.12\times10^4$ evaluations for subset simulation iterated over intensity measures to achieve similar accuracy. For the 1000D seismic fragility surface, IS-II requires about $3.2\times10^4$ limit-state evaluations versus $2.88\times10^5$ for subset simulation, while remaining in close agreement with direct Monte Carlo (reported as $3.6\times10^6$ samples).","The authors state that annealed importance sampling (and IS-I) can be restricted to low-dimensional problems because introducing a relaxation parameter into the PDF leads to Gaussian density values that decay exponentially with dimensionality. They also note that IS-II’s fragility-surface interpolation requires approximations (to avoid additional limit-state evaluations) unlike IS-I’s theoretically rigorous “free” interpolation using reused samples. Additionally, IS-II pays a computational price because selecting the scaling relaxation parameter in the indicator/limit-state function generally requires additional limit-state evaluations (mitigated via extrapolation).","The paper’s demonstrations are confined to Gaussian input spaces (standard normal and spherical/chi-radius formulations), so performance and correctness under non-Gaussian, dependent, or transformed input models may require additional development. Comparisons are mainly against subset simulation and direct Monte Carlo; broader benchmarking against other modern rare-event methods (e.g., cross-entropy mixture IS, multifidelity IS, surrogate-assisted methods) on the same problems would clarify relative advantages. Practical adoption may be hindered by implementation complexity (multi-stage tuning, HMC sampling, extrapolation fitting in IS-II) without released code or packaged software. The fragility-surface “single run” efficiency depends on chosen tuning targets and mesh density; sensitivity to these hyperparameters is not fully quantified in the provided excerpt.","The authors suggest extending RIS beyond two relaxation parameters for multi-dimensional fragility analysis (e.g., multiple intensity measures). They propose integrating RIS with sequential surrogate modeling by relaxing the high-fidelity model into a sequence of low-fidelity models (e.g., via spatial/temporal discretization) and leveraging intermediate results. They also propose applying RIS ideas to reliability-based design optimization by treating design variables analogously to relaxation parameters and reusing intermediate probability estimates across designs.","Developing self-tuning or adaptive policies with theoretical guarantees for choosing relaxation schedules (e.g., controlling ESS/COV targets) could improve robustness and reduce user intervention. Extending RIS/IS-II to correlated and non-Gaussian inputs (including copulas or isoprobabilistic transforms) and to time-dependent reliability with stochastic processes would broaden applicability. Providing open-source implementations (e.g., Python/R/MATLAB) with reusable HMC kernels and fragility-surface tools would materially increase impact and reproducibility. Additional real-world case studies (recorded ground motions, finite-element models) and standardized benchmarks would help validate scalability and practical efficiency claims.",2303.13023v2,https://arxiv.org/pdf/2303.13023v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:21:15Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Mixture of types|Other,Not applicable,Healthcare/medical|Environmental monitoring|Other,Other,TRUE,Other,Not provided,https://doi.org/10.5539/ijsp.v12n3p40,"This paper critiques the evidentiary reliability (reproducibility) of meta-analysis claims linking gas stove cooking/indoor NO2 exposure to childhood asthma and wheeze, focusing on a prior meta-analysis by Lin et al. (2013a). The authors estimate the hypothesis-testing “search space” in 14 base observational studies (median NH = 15,360; IQR 6,336–49,152) and note none reported multiple-testing corrections, implying high risk of false positives and p-hacking/selection bias. They convert 40 odds ratios with confidence intervals from the Lin et al. meta-analysis into p-values and use p-value plots to assess whether the collection of results resembles true effects or randomness. The p-value plots for gas stove–current asthma and gas stove–current wheeze are interpreted as consistent with randomness (unproven effects), undermining the claimed association and any policy inference such as population attributable fraction estimates. The work is methodological/epidemiologic critique rather than reliability engineering (no modeling of component failure, degradation, maintenance, or system reliability).","Search-space (hypothesis count) approximation for each base paper: $NH \approx O \times P \times 2^C$, where $O$ is the number of outcomes, $P$ predictors, and $C$ covariates/adjustment factors considered (covariates treated as included/excluded). P-values are computed from reported odds ratios and confidence intervals (interchangeable with p-values), and then ordered and plotted versus rank in p-value plots to assess uniformity under the null.","Across 14 counted base papers, the median estimated hypothesis-testing search space was 15,360 (IQR 6,336–49,152), with maximum 304,128; 0/14 mentioned multiple-testing correction. For the PIAMA cohort, PubMed search found 107 publications explicitly using the cohort; using a median NH of 13,824 from prior work, the authors estimate ~73,958 potential false positives as $0.05\times 107\times 13{,}824$. For Lin et al. gas stove–current asthma, 13 p-values were plotted with only 1 < 0.05; for gas stove–current wheeze, 27 p-values with 6 < 0.05 (including 4 suggesting negative effects). Both p-value plots are interpreted as consistent with null/randomness rather than a single true-effect distribution.",None stated.,"The analysis treats covariate inclusion/exclusion as binary and assumes independence in counting $O\times P\times 2^C$, which can substantially misestimate the effective number of tests and does not directly quantify the familywise or false discovery rate. P-value plots are largely qualitative/visual and sensitive to heterogeneity, dependence among estimates, and selection processes; the paper does not provide a formal model (e.g., selection models, p-curve/p-uniform, or Bayesian model averaging) to quantify publication bias or p-hacking. The work relies on recomputed p-values from reported CIs and does not reanalyze raw data or assess study quality/risk-of-bias with standard systematic-review tools in a way that could separate confounding from multiplicity effects.",None stated.,"Apply formal multiplicity- and selection-bias methods (e.g., p-curve, p-uniform, selection models, or Bayesian publication-bias models) to quantify evidentiary weight and sensitivity to selective reporting beyond visual p-value plots. Reassess key base studies with access to raw data (where available) using preregistered analysis plans and multiple-testing corrections to estimate how effect sizes change under robust modeling choices. Extend the critique to other outcomes/exposures in the original Lin et al. meta-analysis (all eight endpoints) and to updated literature, including explicit handling of study heterogeneity, correlated subgroup results, and dependence from repeated use of the same cohorts.",2304.01006v2,https://arxiv.org/pdf/2304.01006v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:21:44Z
TRUE,Other,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes AaS-hGP, an adaptive metamodeling framework for high-dimensional reliability analysis (rare-event probability estimation) that couples (i) active subspace dimensionality reduction, (ii) heteroscedastic Gaussian process (hGP) surrogate modeling in the reduced feature space, and (iii) active learning to adaptively place new training points near the failure boundary. Failure probability is defined as $P_f=P(M(X)\ge y_f)$ (equivalently via a limit-state surface), and the method aims to estimate small $P_f$ with far fewer expensive model evaluations than direct Monte Carlo. The key novelty is the tight coupling: active learning changes the training distribution, which updates the active subspace mapping and the hGP, and the reduced dimension $d_r$ is selected adaptively via a surrogate prediction MSE criterion. Performance is demonstrated on a synthetic high-dimensional nonlinear function and two structural reliability case studies (a space truss and a seismic transmission tower), showing accurate $P_f$ estimates with on the order of $\sim 10^2$ model+gradient evaluations versus $10^4$–$10^6$ simulations for reference Monte Carlo. The paper also discusses practical issues such as the need for gradients (finite differences/automatic differentiation) and notes that for extremely small probabilities (e.g., $10^{-7}$) variance-reduction methods may be preferable to plain Monte Carlo in the outer loop.","Reliability target: $P_f=P(M(X)\ge y_f)=\int I(M(x)\ge y_f)f_X(x)\,dx$. Active subspace uses $C=\mathbb{E}_h[\nabla M(X)\nabla M(X)^T]=W\Lambda W^T$ and the reduced feature map $\psi=W_r^T x$ with dimension $d_r$. The hGP surrogate is $y=f(\psi)+\varepsilon(\psi)$ with input-dependent noise variance $r(\psi)=\exp(g(\psi))$; active learning selects the next point by maximin distance within a critical set $W_c=\{\psi: |y_f-\hat\mu(\psi)|/\hat\sigma(\psi)\le \varepsilon_c\}$, and $\hat P_f\approx \frac1N\sum_{k=1}^N I(\mu_{\hat Y}(\psi_k)\ge y_f)$.","On the synthetic Example 1, AaS-hGP correctly identifies an effective reduced dimension $d_r=4$ (matching the construction) and achieves very small relative errors versus MCS while using roughly 104–147 model/gradient evaluations for D=30–100 (e.g., D=50: $\hat P_f\approx 4.95\times10^{-3}$ with 123 evaluations, relative error 0.21%). For the space truss (57 variables), AaS-hGP estimates $\hat P_f\approx 2.63\times10^{-4}$ using 153 model and 153 gradient evaluations (relative error 0.76%) compared with MCS reference $2.61\times10^{-4}$ from $10^6$ simulations. For the transmission tower, AaS-hGP estimates $\hat P_f\approx 4.53\times10^{-3}$ with 86 model and 86 gradient evaluations (relative error 0.66%) versus MCS reference $4.50\times10^{-3}$ (reported using $10^4$ simulations). Across examples, a non-adaptive (global-DoE) variant is notably less accurate even with more training points.","The authors note the method’s reliance on gradient evaluations for the active subspace construction; when direct differentiation is unavailable, gradients may require numerical differentiation (e.g., finite differences) or other strategies, which can be costly for high-dimensional physics-based models. They also state that they tested down to failure probabilities of about $10^{-4}$, and for extremely small probabilities (e.g., $10^{-7}$) variance-reduction methods would be preferable over the paper’s direct Monte Carlo step in the algorithm. They further mention, in the transmission-tower example, that finite-difference gradients require additional simulations per variable and that alternative gradient computation approaches may be introduced for physics-based models.","The method’s reported efficiency counts both model and gradient calls, but in many simulators a gradient can be substantially more expensive than a function evaluation; the practical wall-clock advantage may therefore be smaller than suggested by evaluation counts, especially if finite differences are used (scales with dimension). The active learning and surrogate construction are performed in the reduced feature space, but the approach may be sensitive to nonlinearity/non-invertibility of the projection when multiple distinct $x$ map to similar $\psi$ (potentially harming classification near the limit state), even though hGP noise partially absorbs projection error. The failure-probability estimator uses an indicator on the surrogate mean $I(\mu_{\hat Y}(\psi)\ge y_f)$, which can under-account for surrogate uncertainty; more principled estimators often integrate the predictive distribution (misclassification probability) to reduce bias. Comparisons are limited to a small set of baselines (FORM and AK-MCS in examples); the study does not benchmark against other high-dimensional rare-event tools (e.g., subset simulation variants, cross-entropy IS, RBDO-oriented surrogates) under matched computational budgets.","They suggest combining AaS-hGP with variance-reduction approaches (e.g., subset simulation or importance sampling) to address extremely small failure probabilities beyond those tested. They also propose extending the framework to reliability-based design optimization by introducing an outer-loop stochastic/global optimization algorithm and incorporating sensitivities of reliability metrics with respect to design variables. These extensions are positioned as promising directions to broaden applicability to more demanding rare-event estimation and decision-making under uncertainty.","A natural extension is a surrogate-uncertainty-aware $P_f$ estimator that uses the hGP predictive distribution (e.g., $\mathbb{E}[I(Y\ge y_f)]$ or misclassification probability) and couples it to the acquisition function to reduce bias and improve stopping rules. Developing self-starting or gradient-free variants (e.g., using local linear models, adjoints, or score-function estimators) would improve applicability when gradients are unavailable or prohibitively expensive. Additional robustness studies under input dependence, non-Gaussian measures, and model-response discontinuities (where active subspace assumptions may weaken) would clarify the method’s operational envelope. Finally, releasing a reference implementation and standardized benchmarks would facilitate adoption and fair comparison across modern reliability methods.",2304.06252v1,https://arxiv.org/pdf/2304.06252v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:22:53Z
FALSE,NA,Simulation-based|Other,Simulated only|Mixture of types|Other,Not applicable,Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/ServiceNow/regions-of-reliability,"This paper studies the finite-sample discriminative reliability of proper scoring rules used to evaluate multivariate probabilistic time-series forecasts. The authors propose a power-analysis framework and define “regions of reliability” (RoR): combinations of dimensionality (d), number of evaluation replications (n), and forecast sample size (m) where a scoring rule achieves a target statistical power for detecting forecast-vs-truth discrepancies. They benchmark five common scoring rules (NLL, CRPS, Energy Score, Variogram, Dawid–Sebastiani) across 19 synthetic discrepancy types affecting marginals, correlation structure, and mixture modality, tuning difficulty so NLL achieves 80% power. Results show that in regimes common in the literature (high d, small m and n), most scoring rules have low power and often fail to detect dependency/correlation errors, implying current evaluation practices can be unreliable. They also test generalizability on real temporal datasets (including electricity/solar-related data) by treating a fitted model as ground truth and injecting controlled perturbations, finding similar shortcomings for some rules and settings.","Reliability is quantified via a score-gap random variable $\Delta_m = S(y, X^f_m) - S(y, X^{gt}_m)$, with $y\sim D_{gt}$ and Monte Carlo samples $X^f_m\sim D_f$, $X^{gt}_m\sim D_{gt}$. Using CLT, the averaged gap over $n$ replications is modeled as $H_S(n,m)=\mathcal{N}(\mu_m,\sigma_m^2/n)$ under the alternative and $H_0(n,m)=\mathcal{N}(0,\sigma_m^2/n)$ under the null; the critical value satisfies $P[H_0\ge t_\alpha]=\alpha$ and power is $P[H_S\ge t_\alpha]=1-\beta$.","Across 19 synthetic discrepancy types tuned so NLL has power 0.8 at $\alpha=0.05$, other proper scores often underperform markedly, especially for detecting correlation/covariance mismatches and mixture-modality errors in high dimension and small-sample regimes. For covariance-related tests, Dawid–Sebastiani and Variogram tend to be the strongest among non-NLL rules but still substantially below NLL in many settings; CRPS variants are generally insensitive to dependency errors. In a real-data electricity-related experiment (treating a fitted model as ground truth), breaking correlations yields CRPS-Q power 0.05 while NLL is essentially 1; multiplicative bias (×1.05) is detected by CRPS/ES/VG with moderate power; additive bias shows CRPS strong (≈1) while ES and VG can be weak. The paper argues many common benchmark dataset regimes (very large d with small n and m) likely fall outside RoRs for these scoring rules, questioning standard evaluation protocols.","The authors note the results are empirical and cannot be guaranteed to hold beyond the studied domains/ranges of $d$ and $m$, which are computationally constrained. They also highlight non-monotonic power behavior in some settings, complicating extrapolation. They state that deriving finite-sample theoretical guarantees would be important to characterize unstudied domains.","The study’s conclusions depend heavily on the specific synthetic discrepancy constructions and on tuning discrepancy magnitude via NLL power, which may privilege NLL-like sensitivities and could bias comparisons against scores designed for different aspects (e.g., tail emphasis). The CLT-based normal approximation for the test statistic and the reuse/approximation strategies (especially in real-data perturbation experiments) may misestimate variability and inflate power for sampling-based scores. Software and implementation choices (e.g., sample-based approximations of CRPS/ES, parameter p choices) can materially affect power, but robustness to alternative implementations/estimators is only partially explored.","They propose deriving finite-sample theoretical guarantees for scoring-rule discrimination to cover domains not studied computationally. They call for developing new scoring rules with better finite-sample behavior and for building more extensive standardized benchmarks. They also suggest learning or designing combinations of complementary scoring rules (e.g., CRPS + VG) to achieve high power across diverse settings.","A useful extension would be to include robust/self-normalized or dependence-aware variants of scoring rules tailored for high-dimensional settings (e.g., sparsity-exploiting dependence scores) and to analyze performance under autocorrelation and non-i.i.d. evaluation windows. Another direction is to evaluate practical decision impact more directly by coupling scoring-rule selection to downstream optimization sensitivity (beyond the illustrative profit model). Finally, releasing a reproducible software package (e.g., pip/CRAN) with standardized experiment definitions and CI-tested implementations would improve adoption and reduce implementation-driven variability in RoR estimates.",2304.09836v2,https://arxiv.org/pdf/2304.09836v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:23:31Z
TRUE,Degradation modeling|Other,ML-based|Hybrid/Ensemble|Other,Degradation measurements|Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB|None / Not applicable|Other,Not provided,NA,"The paper proposes a bi-fidelity Deep Operator Network (DeepONet) framework to model uncertain nonlinear dynamical systems exhibiting degrading (and optionally pinching) hysteresis. A low-fidelity “pristine” model (e.g., classical Bouc–Wen hysteresis without degradation) is used to supply abundant cheap simulations, while a DeepONet is trained to learn the discrepancy between low-fidelity outputs and the high-fidelity degrading/pinching system response. The method targets uncertainty in model parameters and structural/model-form mismatch due to omitted degradation mechanisms, improving prediction accuracy by correcting the low-fidelity response rather than learning the full high-fidelity mapping from scratch. Performance is evaluated via relative validation RMSE on three numerical case studies (4-DOF base-isolated structure; half-car vs quarter-car nonlinear suspension; 100-DOF base-isolated building with corrosion plus degrading isolators), showing substantial error reductions over a high-fidelity-only DeepONet when high-fidelity training data are limited. The study also demonstrates that bi-fidelity gains diminish when the low-fidelity model behavior is too dissimilar from the true system (e.g., strong pinching mismatch) and when additive noise in training data increases.","DeepONet operator approximation is given by $y(\chi) \approx G_\theta(f)(\chi)=c_0+\sum_{i=1}^p c_i(f(\chi_1),\ldots,f(\chi_m))\,\psi_i(\chi)$, with a branch net producing coefficients $c_i$ and a trunk net producing bases $\psi_i$. The bi-fidelity formulation models the correction $y_{\mathrm{corr}}(t;\xi)=Y(t;\xi)-y(t;\xi)$ and learns $y_{\mathrm{corr}}(t;\xi)\approx G_\theta(y)(t,\xi)=c_0+\sum_{i=1}^p c_i(y(t_1,\xi_1),\ldots,y(t_m,\xi_m))\psi_i(t,\xi)$. Degrading hysteresis is represented via Baber–Wen evolution (with optional pinching), e.g., $\dot z=g_{\mathrm{degrade}}=\eta^{-1}(\bar A\dot u_b-\nu(\beta\dot u_b|z|^{n}+\gamma z|\dot u_b||z|^{n-1}))$ where $\bar A,\nu,\eta$ depend on the cumulative measure $e=\int_0^t z(\tau)\dot u_b(\tau)d\tau$.","Example 1 (4-DOF, degradation only; $N_{tr}=200$): mean validation RMSE for auxiliary variable $z$ drops from $6.6642\times10^{-2}$ (high-fidelity-only DeepONet) to $2.2397\times10^{-2}$ (bi-fidelity); for base displacement $u_b$ from $4.7907\times10^{-3}$ to $2.0235\times10^{-3}$; for roof displacement $u_3$ from $4.0933\times10^{-3}$ to $2.2730\times10^{-3}$. Example 2 (half-car high-fidelity vs quarter-car low-fidelity): mean validation RMSE improves from $1.4191\times10^{-1}$ (high-fidelity-only, trained with cost-adjusted $N_{tr}=386$) to $1.8737\times10^{-2}$ (bi-fidelity, $N_{tr}=250$), nearly an order-of-magnitude reduction. Example 3 (100-DOF building, $N_{tr}=250$ bi-fidelity vs $N_{tr}=500$ high-fidelity-only): mean validation RMSE improves from $2.1717\times10^{-2}$ to $4.6541\times10^{-3}$ (0% noise), from $2.4413\times10^{-2}$ to $6.4530\times10^{-3}$ (5% noise), and from $2.5730\times10^{-2}$ to $8.8876\times10^{-3}$ (10% noise). The paper also reports that bi-fidelity advantages shrink when low-fidelity mismatch increases (e.g., stronger pinching effects).","The authors note that the bi-fidelity approach relies on a low-fidelity model that exhibits behavior sufficiently similar to the true system; when the mismatch is large (e.g., strong pinching), the advantage diminishes and can compromise performance. They also report that the advantage of bi-fidelity decreases as the amount of high-fidelity training data increases, and that added noise in training data degrades performance and can dissipate the bi-fidelity benefit as noise intensity grows.","All demonstrations are numerical/simulation-based; the approach is not validated on experimental field data from degrading components, so real measurement issues (sensor bias, nonstationarity, unmodeled disturbances) remain untested. The method learns an output discrepancy but does not provide explicit physical interpretability or identifiability of degradation parameters, which can limit its use for diagnosis or prognostics. The training/evaluation metric is RMSE on time histories; reliability-relevant quantities (failure probability, threshold crossing times, damage indices) are not directly assessed. Software/implementation details are insufficiently specified (framework, reproducibility artifacts), which may hinder replication and practical adoption.","The authors suggest extending the approach to multi-fidelity implementations with more than two fidelity levels, applying it to systems with greater nonlinear complexity, and investigating reduced-order modeling methods to construct improved low-fidelity representations.","A valuable extension would be to couple discrepancy-learning with reliability/prognostics outputs (e.g., predicting time-to-threshold, probability of exceeding displacement/force limits) to make the method directly actionable for reliability decisions. Robust/self-starting variants that handle unknown/estimated low-fidelity parameters, autocorrelated excitation, and non-Gaussian noise would improve realism. Incorporating uncertainty quantification on the learned correction (e.g., Bayesian DeepONets or conformal prediction) would provide calibrated prediction intervals important for safety-critical applications. Public release of code and benchmark datasets (including at least one experimental degrading hysteresis dataset) would enable reproducible comparisons against alternative surrogate and multifidelity methods.",2304.12609v1,https://arxiv.org/pdf/2304.12609v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:24:28Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Mixture of types|Simulated only|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,R,Public repository (GitHub/GitLab),https://github.com/jlgrons/ssROC,"This paper proposes ssROC, a semi-supervised method to estimate ROC-based performance metrics (e.g., sensitivity/TPR, specificity via FPR, PPV, NPV, AUC) for phenotyping algorithms when only a small subset of gold-standard labels is available. The approach imputes missing labels in the large unlabeled set by nonparametrically estimating a calibration function $m(s)=P(Y=1\mid S=s)$ using kernel regression on the labeled set, then plugs the imputed labels into standard empirical ROC-parameter estimators. The authors evaluate ssROC against supervised ROC analysis (supROC) through extensive Monte Carlo simulations, a semi-synthetic study using MIMIC-III (depression phenotype), and a real EHR application at Mass General Brigham across five phenotypes, emphasizing estimation precision at a fixed FPR of 10%. Across studies, ssROC yields similar (generally low) bias but substantially reduced variance relative to supROC, with reported variability reductions of roughly 30%–60% on average in the real-data examples. An R implementation is provided as an open-source package.","ssROC estimates the calibration/imputation function with local-constant kernel regression: $\hat m(s)=\frac{\sum_{i=1}^n K_h(S_i-s)Y_i}{\sum_{i=1}^n K_h(S_i-s)}$, where $K_h(u)=h^{-1}K(u/h)$. It then estimates ROC quantities by replacing missing labels with $\hat m(S_i)$ on the unlabeled set, e.g., $\widehat{\mathrm{TPR}}_{\mathrm{ssROC}}(c)=\frac{\sum_{i=n+1}^{n+N} \hat m(S_i)\,\mathbf{1}(S_i>c)}{\sum_{i=n+1}^{n+N} \hat m(S_i)}$ and $\widehat{\mathrm{FPR}}_{\mathrm{ssROC}}(c)=\frac{\sum_{i=n+1}^{n+N} (1-\hat m(S_i))\,\mathbf{1}(S_i>c)}{\sum_{i=n+1}^{n+N} (1-\hat m(S_i))}$. Variance is estimated via perturbation resampling with i.i.d. positive weights (implemented with 500 replicates) and logit-based confidence intervals.","In simulations with $N=10{,}000$ unlabeled observations and labeled sizes $n\in\{75,150,250,500\}$, ssROC shows minimal bias and consistently higher relative efficiency (RE>1) than supROC for AUC, threshold, TPR, PPV, and NPV at FPR=10%; median REs are reported as roughly 1.3–1.9 (high-accuracy setting) and 1.1–1.6 (low-accuracy setting), implying meaningful variance reduction. In the semi-synthetic MIMIC-III depression study (total $N=32{,}172$), ssROC similarly improves precision with median REs roughly 1.3–1.9 (high accuracy) and 1.1–2.1 (low accuracy) across metrics. In the Mass General Brigham real-data application (five phenotypes), ssROC point estimates are broadly similar to supROC while being about 30%–60% less variable on average; the reported median RE gain across phenotypes is about 1.5 for AUC/TPR and about 2.7 for the classification threshold at FPR=10%.","The authors note that ssROC assumes the labeled examples are randomly sampled from the full underlying data; accommodating non-random/stratified sampling is nontrivial when labeled and unlabeled sizes differ greatly. They also state that the nonparametric kernel-recalibration step requires a sufficient amount of labeled data to estimate the calibration function well, motivating future work for smaller labeled sample sizes. They further emphasize that additional validation is needed across more phenotypes and settings, including federated analyses across multiple health systems.","The method’s practical performance may be sensitive to bandwidth/kernel choices and to the optional score transformation (empirical CDF) used prior to imputation; robustness to these tuning decisions is not fully benchmarked. Results focus heavily on evaluation at a single operating point (FPR=10%), so gains may differ across other FPR regions or under alternative threshold-selection rules used in practice. The approach assumes exchangeability between labeled and unlabeled sets (MCAR labeling); violations such as verification bias or spectrum bias common in chart review could introduce bias if not addressed.","The authors propose extending ssROC to handle more effective/non-random sampling strategies (e.g., stratified sampling) for labeling, and developing a parametric recalibration approach that works with smaller labeled datasets. They also suggest extending ssROC to support model comparisons and fairness-metric evaluation by augmenting the calibration step to incorporate multiple PA scores and/or protected attributes. Finally, they call for broader evaluation across more phenotypes and extensions to support federated multi-system analyses.","Develop self-starting or doubly robust versions of ssROC that remain valid under common EHR labeling mechanisms such as outcome-dependent chart review (verification bias) or covariate-shift between labeled and unlabeled cohorts. Extend the framework to handle correlated observations (repeated visits per patient) and time-varying phenotypes, where independence assumptions behind resampling/inference may fail. Provide a comprehensive sensitivity analysis for tuning (bandwidth selection, kernel choice, monotone calibration alternatives like isotonic regression) and ship reproducible benchmark scripts/datasets to standardize comparisons.",2305.01709v5,https://arxiv.org/pdf/2305.01709v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:25:10Z
FALSE,NA,ML-based|Simulation-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/gabrielkasmi/spectral-attribution,"This paper proposes WCAM (Wavelet sCale Attribution Method), a black-box attribution technique that generalizes perturbation-based explanations from the pixel domain to the wavelet (space–scale) domain using a discrete wavelet transform (DWT). It perturbs wavelet coefficients via quasi–Monte Carlo masks and estimates feature importance with total Sobol indices (Jansen estimator), yielding explanations of where the model focuses and at which spatial scales/frequency bands. The authors evaluate explanation quality using bc-Fidelity (correlation-based metric) and additional insertion/deletion benchmarks on ImageNet models, reporting that WCAM outperforms competing black-box methods and is competitive with white-box methods in bc-Fidelity. They further show WCAM can characterize robustness by revealing that adversarially/robustly trained models rely more on coarse (low-frequency) components than standard-trained models. The work is about reliability of ML decisions (relevance + robustness to corruptions), not reliability engineering of physical systems.","WCAM applies perturbations in the wavelet domain and uses Sobol sensitivity analysis to compute importances. Total Sobol indices are estimated using Jansen-style estimators: \(\hat S_k=\frac{\hat V-\frac{1}{2N}\sum_{j=1}^N\big[f(B_j)-f(C^{(k)}_j)\big]^2}{\hat V}\) and \(\hat S_{T_k}=\frac{\frac{1}{2N}\sum_{j=1}^N\big[f(A_j)-f(C^{(k)}_j)\big]^2}{\hat V}\), where \(A,B\) are two perturbed-sample matrices and \(C^{(k)}\) swaps column \(k\) between them. Explanation quality is evaluated with \(\mu\)-Fidelity: \(\mu\text{-Fidelity}=\mathrm{Corr}_{u\subset\{1,\dots,K\},|u|=d}\left(\sum_{i\in u} g(x_i),\ f(x)-f(x_{x_u=x_0})\right)\).","On 100 ImageNet validation images, WCAM achieves substantially higher \(\mu\)-Fidelity than other black-box methods across architectures (e.g., VGG16: 0.197 vs Sobol 0.095 and RISE 0.020; ResNet50: 0.191 vs Sobol 0.108; EfficientNet: 0.187 vs Sobol 0.013). The spatially-flattened variant (Spatial WCAM) performs similarly to other pixel-domain methods, supporting that the wavelet spacescale representation drives gains. Deletion/insertion benchmarks (appendix) report strong deletion performance for WCAM on some models (e.g., ResNet50 deletion 0.017) while insertion varies by architecture (e.g., EfficientNet insertion 0.737). Qualitatively, WCAM differentiates robust vs standard models by showing robust training concentrates importance on coarse/low-frequency wavelet components.",The authors state that the main limitation is computational cost: WCAM is more expensive than existing black-box attribution methods because it requires many forward passes for Sobol-index estimation. They also note the practical burden of increased operations due to wavelet transforms and perturbation generation.,"The method’s attribution stability may depend on wavelet choice (mother wavelet, levels) and masking/grid hyperparameters, but robustness to these design choices is only partially explored. Results are mostly on ImageNet vision models, so generalizability to other modalities and to correlated/structured inputs beyond images is uncertain. The approach yields importance scores but limited formal guarantees about causal relevance under distribution shift; “reliability” is framed operationally (relevance + robustness) rather than validated against downstream safety outcomes. Computational expense could also hinder routine use in large-scale or real-time auditing settings without approximation strategies.","They plan to evaluate WCAM’s benefits in an expert application: remote sensing of rooftop photovoltaic (PV) installations, where current models lack reliability due to sensitivity to acquisition conditions. More broadly, they position WCAM as a first step toward tools that audit both relevance and robustness of model decisions in real-world settings.","Develop faster or adaptive sampling/estimation schemes (e.g., multi-fidelity Sobol estimation, early stopping, learned mask proposals) to reduce forward-pass cost. Extend WCAM to handle temporal/spatiotemporal data (video, sensor time series) and multivariate settings with correlated features where Sobol assumptions may be violated. Provide standardized software benchmarks and uncertainty quantification for attribution maps (confidence intervals over Sobol indices) to improve auditability. Validate the “reliability” assessment against real deployment failures/corruption types and across diverse acquisition domains (medical imaging, remote sensing) with domain-expert studies.",2305.14979v5,https://arxiv.org/pdf/2305.14979v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:25:52Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Other",Event/count data|Other,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/sanjaymjoshi/relistats|https://github.com/sanjaymjoshi/relistats_notebook,"This preprint develops computational procedures for estimating reliability, confidence, and assurance in success–failure (Bernoulli/binomial) experiments when the population size is finite and not large compared to the tested sample size. It reviews the standard (infinite-population) binomial confidence calculation for meeting a target reliability level and then modifies the computation for a finite number of additional units by restricting attainable reliability levels to discrete steps determined by the remaining untested population. The paper provides algorithms for (i) computing confidence at a desired reliability level, (ii) finding the minimum reliability that meets a target confidence, and (iii) computing assurance as the maximum over possible additional-failure counts of the minimum of reliability and confidence. Numerical examples (plots and a table) illustrate differences between finite- and infinite-population assumptions, including non-monotonic assurance behavior due to the discrete trade-off between confidence and reliability. The methods are implemented in open-source Python code and demonstrated via Jupyter notebooks.","Confidence (infinite population) for at least reliability r with f failures in n trials is given by a binomial tail: $c = 1-\sum_{k=0}^{f}\binom{n}{k}(1-r)^k r^{n-k}$. Assurance sets $a=r=c$ in the same expression: $a = 1-\sum_{k=0}^{f}\binom{n}{k}(1-a)^k a^{n-k}$. For the finite-population extension with m additional units and d additional failures, reliability is discretized as $r = 1-(f+d)/(n+m)$; confidence at a target r is computed by mapping to the nearest discrete additional-sample reliability level $r_n = 1-d/m$ (with a special handling for $d=0$ using $r_n=1-1/(m+1)$), and assurance is computed as $\max_{0\le d\le m} \min(r,c)$.","For n=10, f=0, the finite-population confidence at a given reliability decreases as the number of additional samples m increases, approaching the infinite-population curve; the maximum reportable reliability with nonzero confidence is capped at $1-1/(m+1)$ for the zero-failure case. For n=10, f=2, the paper reports a cross-over around reliability $1-2/10=80\%$ where finite-m behavior switches relative to the infinite-population confidence (finite-m lower for reliabilities above 80% due to reduced tolerance for additional failures). In the assurance table for zero failures, an example given is n=3 with m=5 yielding assurance 75% versus 68.2% under the infinite-population assumption. The paper also shows assurance can be non-monotone in m (e.g., n=3 increases from 71.4% at m=4 to 75% at m=5) because assurance is the minimum of discretized reliability and confidence, and their intersection/trade-off shifts with m.",None stated.,"The approach assumes independent and identically distributed Bernoulli trials with a constant failure probability, which may be violated by learning effects, heterogeneity, or time trends in production/testing. The finite-population adjustment is implemented via discretization and a mapping to an “infinite-population” binomial confidence at a derived reliability level; this is not derived from an explicit finite-population probabilistic model (e.g., hypergeometric-style conditioning), so the statistical interpretation may be unclear in some settings. Empirical evaluation is primarily illustrative (plots/tables) and does not benchmark against alternative finite-population Bayesian or exact methods, nor does it quantify error relative to a ground-truth finite-population generative process. The paper does not discuss Phase I parameter uncertainty (prior information) beyond the observed f and n, nor robustness to model misspecification.",None stated.,"Derive the finite-population confidence/assurance measures from an explicit finite-population model (or decision-theoretic framing) and compare analytically/numerically with the paper’s discretization-based approach. Extend the methods to account for heterogeneity (mixtures), dependence, or time-varying reliability across units/batches. Provide guidance on choosing m and interpreting assurance in procurement/production decisions via an economic or utility-based design. Add broader validation with real success–failure datasets and publish a tested Python package release (with reproducible scripts) documenting computational complexity and numerical stability.",2305.16578v1,https://arxiv.org/pdf/2305.16578v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:26:24Z
TRUE,System reliability|Other,Stochastic process|Simulation-based|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"The paper proposes an active-learning framework for estimating failure probability of arbitrary-configured engineering systems with multiple limit-state functions (multiple failure modes). The method builds separate Kriging or PC-Kriging (Gaussian-process) surrogates for each component limit state, estimates system failure probability using subset simulation, and iteratively enriches the experimental design. Novel contributions include a system-level learning function based on an empirical deviation number for the composed system response (works without specifying system configuration), DBSCAN clustering of candidate points to detect distinct failure modes, and Sobol’ total-effect sensitivity indices to decide which component limit state(s) to evaluate at each enrichment point. The approach is validated on two analytical benchmarks (four-branch system; roof truss with three modes) and a realistic power-transmission tower system (7 towers, 40 uncertain inputs) modeled via a truss finite-element code. Across benchmarks, the method achieves accurate reliability indices with substantially fewer expensive model evaluations than prior active-learning system-reliability methods, and demonstrates feasibility on small failure probabilities (e.g., order 10^-4 to 10^-3).","System failure probability is defined as $P_f = P\,[h(g(X))\le 0] = \int_{D_f} f_X(x)\,dx$ with system failure domain $D_f=\{x: h(g(x))\le 0\}$. The proposed system learning function is an empirical deviation number $\hat U_{\mathrm{sys}}(x)=|\hat\mu_{\mathrm{sys}}(x)|/\hat\sigma_{\mathrm{sys}}(x)$ where $Z_j(x)=\hat g_j(x)\sim\mathcal N(\mu_{\hat g_j}(x),\sigma^2_{\hat g_j}(x))$ are sampled independently and $\hat\mu_{\mathrm{sys}},\hat\sigma_{\mathrm{sys}}$ are Monte Carlo estimates of $h(Z_1,\ldots,Z_m)$. For each selected enrichment point, the component to evaluate is chosen by Sobol’ total index $j^*(x)=\arg\max_j S^T_j(x)$ computed for $h(Z_1(x),\ldots,Z_m(x))$.","Four-branch series system: for $P=7$ (reference $P_f\approx2.239\times10^{-3}$, $\beta=2.842$), the proposed method achieved median $\beta\approx2.845$ with 40 evaluations (Kriging) or 27 (PC-Kriging), outperforming literature methods requiring e.g. $\sim61\times4$ evaluations when treating the system as one function. For $P=6$ (reference $P_f\approx4.484\times10^{-3}$, $\beta=2.613$), the proposed approach converged with 37 (Kriging) or 27 (PC-Kriging) evaluations versus ~61–63 for AK-SYS/ALK variants. Roof-truss series system (reference $P_f\approx3.417\times10^{-3}$, $\beta=2.705$): PC-Kriging achieved median error $\epsilon_\beta\approx2.06\times10^{-4}$ with 51 total evaluations, compared with 114–125 for competing methods; Kriging was cheaper (43) but showed bias (median $\epsilon_\beta\approx1.61\times10^{-2}$) under the default stopping rule. Transmission-tower system (7 limit states; reference $P_f\approx5.102\times10^{-4}$, $\beta\approx3.285$) converged with roughly 221–327 total evaluations depending on surrogate and stopping threshold, versus ~2.375 million evaluations for the reference subset simulation.","The authors note that the methodology inherits dimensionality limitations of Kriging/PC-Kriging surrogates and of DBSCAN clustering, potentially breaking down when individual component limit states are high-dimensional. They also acknowledge that robustness may degrade for complex failure-domain topologies or extremely low failure probabilities, and that the stopping criterion can lead to premature convergence/bias (as observed in examples). They suggest that increasing or better distributing the initial experimental design can reduce the risk of missing a failure domain.","The learning function $\hat U_{\mathrm{sys}}(x)$ relies on Monte Carlo sampling of surrogate predictive distributions at every candidate point, which may add substantial overhead for large candidate pools unless carefully optimized; the paper does not fully quantify this cost or propose variance-reduction for this inner loop. The approach assumes independence between component surrogate predictors at a given $x$ (because surrogates are trained independently), which can be mismatched when component responses are correlated through shared physics or shared training data, potentially distorting $h(Z)$ uncertainty and Sobol rankings. Sensitivity-based selection using Sobol total indices on $h$ with respect to surrogate outputs is local-in-$x$ and surrogate-dependent; miscalibration of predictive variances can cause selecting the wrong limit state to evaluate, especially early in training. Comparisons focus on number of evaluations and accuracy but do not extensively assess robustness under strong input dependence, non-Gaussian predictive errors, or alternative system-structure functions beyond the studied cases.",The authors suggest mitigating high-dimensional limitations by applying dimensionality reduction for DBSCAN and potentially also for building surrogates when component dimensions are too large for standard Kriging/PC-Kriging. They also state that robustness in difficult settings (complex topologies or extremely small failure probabilities) can be improved by using a larger and/or more widely distributed initial experimental design to reduce the risk of missing failure regions. They further highlight the need to develop more robust stopping criteria for the proposed methodology.,"Develop a self-adaptive or statistically grounded stopping criterion (e.g., based on confidence intervals of $P_f$ that account for surrogate uncertainty and subset-simulation variability) to reduce premature convergence and bias. Extend the framework to explicitly model dependence among component surrogate errors (e.g., multi-output GPs or correlated discrepancy models) so that the distribution of $h(Z)$ better reflects joint uncertainty. Replace the inner Monte Carlo used to estimate $(\mu_{\mathrm{sys}},\sigma_{\mathrm{sys}})$ and Sobol indices with analytic/approximate propagation (e.g., polynomial chaos of $h$ in surrogate outputs, unscented transforms, quasi-MC) to lower overhead. Add software artifacts (reproducible implementations and benchmarks) and broaden empirical validation to additional arbitrary system-structure functions, time-dependent reliability, and scenarios with strongly correlated inputs and multiple physics-based components.",2305.19885v2,https://arxiv.org/pdf/2305.19885v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:27:17Z
TRUE,Maintenance optimization|Network/infrastructure reliability|Other,Bayesian|Other,Mixture of types|Other,Not applicable,Transportation/logistics,Case study (real dataset)|Other,TRUE,Other,Not provided,https://support.bayesfusion.com/docs/,"The paper proposes a Technology-Driven Adaptive Decision Support System for Integrated Pavement and Maintenance strategies (TDADSS-IPM) aimed at making road-asset decisions adaptive to uncertainties such as climate change. As a key module in this architecture, it develops a bottom-up risk assessment framework using a Bayesian Belief Network (BBN) with 28 nodes representing climate drivers (e.g., extreme precipitation, sea level rise, extreme temperature, zero-point crossings), intermediate mechanisms (e.g., flooding, scouring), and asset outcomes (road deterioration and bridge/culvert collapse). Probabilities are populated using a mix of calculated unconditional probabilities (using a return-period formula), literature inputs, and expert/stakeholder elicitation; the BBN is then executed to estimate risk levels under current and worst-case climate scenarios. Reported baseline results from the assumed/elicited model give road deterioration probability of about 34% and bridge collapse probability of about 48%, with worst-case scenario examples showing substantially higher risks (e.g., ~73% road deterioration and ~86% bridge collapse for poor asset-condition states). The work positions the BBN module as updatable over time (potentially dynamic) to support real-time risk-aware decision support for climate adaptation in road maintenance and management.","The paper uses Hershfield’s return-period formulation to compute unconditional event probabilities: $U = 1 - (1 - 1/T)^r$, where $T$ is return period and $r$ is the number of years. The BBN joint distribution is computed via the standard factorization: $p(X_1,\ldots,X_n)=\prod_i p(X_i\mid \mathrm{Pa}(X_i))$, where $\mathrm{Pa}(X_i)$ are parent nodes.","Using the initial (assumed/elicited) probability tables, the executed BBN reports approximately 34% probability of road deterioration and 48% probability of bridge collapse under the current-condition setting. In a demonstrated worst-case scenario where extreme-event drivers are set to occur (set to 1) and road/bridge conditions are “Bad”/“Not usable,” the model reports ~73% probability of road deterioration and ~86% probability of bridge collapse. The paper also notes that, averaged across low/medium/high blue-spot flooding levels, the difference in bridge-collapse risk between “not usable” and “proper” bridge condition is about 44% (e.g., 81%–35%, 83%–38%, 86%–44%).",None stated.,"Many conditional probabilities are assumed or elicited from expert judgment (including a single hydraulic expert for several CPT entries), which can strongly influence posterior risk estimates without an uncertainty/credibility analysis. The work demonstrates scenario results but provides limited validation against observed failure/deterioration outcomes, and does not report sensitivity analysis to identify which CPT entries dominate results. Despite the DSS framing, the paper does not formulate or compare explicit maintenance decision policies (e.g., optimized inspection/repair scheduling) tied to the risk outputs.","The authors state interest in making the BBN model dynamic so it can be upgraded with real-time events and trained as new information becomes available. They also suggest gathering expert and policymaker views more systematically (e.g., via questionnaires) to inform and update the probability tables.","Perform formal sensitivity/importance analysis (e.g., variance-based or mutual-information measures) to identify the most influential nodes/CPT parameters and guide data-collection priorities. Quantify epistemic uncertainty in expert-elicited CPTs (e.g., Bayesian priors over CPT entries) and propagate it to uncertainty intervals on deterioration/collapse probabilities. Integrate the BBN risk outputs with explicit maintenance optimization (e.g., risk-based inspection/rehabilitation scheduling under budget constraints) and validate on longitudinal road/bridge performance datasets.",2306.01769v1,https://arxiv.org/pdf/2306.01769v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:27:51Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,ML-based|Bayesian|Physics-based|Hybrid/Ensemble|Simulation-based|Other,Degradation measurements|Sensor/condition monitoring|Simulated only|Other,Condition-based|Predictive|Not applicable|Other,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://www.nasa.gov/content/prognostics-center-of-excellence-data-set-repository,"The paper proposes a decision-oriented metric for evaluating data-driven prognostic (RUL) algorithms based on their downstream impact on predictive maintenance (PdM) decisions. The metric is defined as the relative gap between the achieved long-run expected maintenance cost rate (renewal-reward ratio) under a chosen PdM policy using algorithm-produced RUL uncertainty, and the corresponding cost rate under a perfect-prognostics (oracle) policy. The authors show how to estimate the long-run cost rate directly from run-to-failure datasets and use the metric as an objective for tuning heuristic PdM policy thresholds and for decision-oriented hyperparameter optimization of prognostic models. Two decision settings are treated: (i) replacement timing and (ii) ordering-plus-replacement with lead time, inventory holding cost, and unavailability penalties; multiple heuristic and RUL-distribution-based policies are studied and modified. Numerical experiments use a virtual RUL simulator and the NASA CMAPSS turbofan run-to-failure simulations, comparing four prognostic models (LSTM classifier, Gaussian Naive Bayes classifier, decision tree classifier, and a Bayesian-filtered exponential degradation regression model) and demonstrating that policy choice and threshold optimization materially affect decision-level performance.","The core performance quantity is the renewal-reward cost rate $R=\mathbb{E}[C_m]/\mathbb{E}[T_{lc}]$, estimated from $n$ run-to-failure trajectories as $\hat R=\frac{\frac{1}{n}\sum_i C_m^{(i)}}{\frac{1}{n}\sum_i T_{lc}^{(i)}}$. The proposed decision-oriented metric is the relative gap to perfect prognostics: $M=\frac{R-R_{\text{perfect}}}{R_{\text{perfect}}}$ with estimator $\hat M=(\hat R-\hat R_{\text{perfect}})/\hat R_{\text{perfect}}$. Replacement costs are modeled via $C^{(i)}_{rep}=c_p$ if preventive replacement occurs before failure and $c_c$ otherwise; ordering adds delay and inventory costs $C_{delay}=\max(T_{order}+L-T_{lc},0)\,c_{unav}$ and $C_{stock}=\max(T_{lc}-(T_{order}+L),0)\,c_{inv}$.","On the CMAPSS FD001 run-to-failure simulations (80 units for training, 20 for evaluation), the LSTM classifier achieved the lowest (best) metric values among the four compared prognostic models for replacement planning across a range of cost ratios $c_p/c_c$. In the replacement-only setting, the simple heuristic policy based on thresholding $\Pr(\text{RUL}\le\Delta T)$ often outperformed the distribution-based policy on the small evaluation set, and optimizing the heuristic threshold on training data further reduced $\hat M$ for all models (sometimes substantially). In the ordering+replacement setting (example costs: $c_p=100$, $c_{unav}=10$, $c_{inv}=1$ and varying $c_c$), $\hat M$ increased markedly due to added logistics costs, but optimizing ordering and replacement thresholds for the LSTM (reported example: $p^{*}_{rep}=0.5$, $p^{*}_{order}=0.11$) produced a large improvement relative to the default $c_p/c_c$ thresholds. The virtual RUL simulator experiments show that access to full RUL distributions and/or optimized thresholds can reduce the metric, and that uncertainty in RUL predictions (e.g., higher $\sigma_{\ln\epsilon}$) degrades decision performance and increases estimator variability.","The authors note that reliable data-driven evaluation of the metric requires monitoring datasets with multiple run-to-failure trajectories; limited data leads to high variability in estimating $M$ (e.g., 20 CMAPSS run-to-failure samples are stated as insufficient for a reliable estimate). They also emphasize that the metric depends on user-specified cost parameters (preventive/corrective replacement, unavailability, inventory), which can be difficult to quantify precisely in practice though rough engineering estimates may suffice. They acknowledge simplifications in the turbofan case study decision modeling (real turbofan maintenance involves broader system-level logistics and constraints not represented in the paper’s simple decision settings).","The “perfect prognostics” baseline assumes oracle knowledge of failure time and may not be a realistic or stable comparator across different operational contexts; alternative baselines (e.g., optimal model-based PdM with estimated degradation model) could provide more actionable benchmarking. The metric is estimated using a ratio-of-means; for small $n$ it can be sensitive to outliers and to dependence between $C_m$ and $T_{lc}$, and uncertainty quantification is only approximate (and the paper sometimes treats $\hat R_{\text{perfect}}$ variance as negligible). Policies and evaluations assume independent, identically distributed renewal cycles with perfect replacement and self-announcing failures; these assumptions may be violated under imperfect maintenance, partial observability, or fleet-level coupling (shared spares, scheduled downtime). The CMAPSS study uses simulated run-to-failure data and limited test units; results may not generalize to real fleets with censoring, varying mission profiles, and nonstationary regimes without additional robustness checks.","The authors propose future work on training models that map monitoring data directly to maintenance decisions, learning the PdM policy during training (e.g., via deep reinforcement learning) rather than fixing a heuristic policy. They also suggest exploring more complex and potentially more optimal decision policies (beyond the simple heuristics used for ordering/replacement), such as POMDP-based approaches, while noting such methods would require calibration to cost models, deterioration processes, and access to larger training datasets.","Extend the metric and policy evaluation to settings with right-censoring and intermittent inspections (common in field data), and develop unbiased/robust estimators (e.g., bootstrap CIs, Bayesian posterior for $M$) for small sample sizes. Incorporate imperfect maintenance and partial rejuvenation models and analyze sensitivity of $M$ to violations of renewal assumptions and to dependence/heterogeneity across units. Generalize to multi-component and fleet-level logistics (shared inventory, maintenance windows, opportunistic/group replacement) where ordering and replacement interact across assets. Provide open-source reference implementations and standardized benchmarks for policy+metric evaluation to improve reproducibility and facilitate adoption by practitioners.",2306.03759v2,https://arxiv.org/pdf/2306.03759v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:28:41Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,"Parametric (Weibull, etc.)|ML-based|Other",Sensor/condition monitoring|Event/count data|Mixture of types,Condition-based|Predictive|Not applicable,Transportation/logistics|Energy/utilities|Other,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,https://www.mtr.com.hk/archive/corporate/en/investor/annual2022/E11.pdf|https://www.mtr.com.hk/en/corporate/investor/patronage.php,"The paper develops an Escalator Health Condition Analytic System (HCAS) that integrates online data collection, a monitoring dashboard, a health index model, and remaining useful life (RUL) prediction to support escalator refurbishment planning beyond simple age-based rules. It uses real-time energy-meter data (1-minute intervals) and edge-processed vibration sensor data (FFT acceleration in selected dominant frequency bands) together with fault/incident frequency to quantify condition. A quarterly Lifetime Health Index (LHI) is defined as a weighted sum of standardized operating/workload and condition indicators (working time, estimated passenger load from energy, fixed energy-loss residuals, vibration exceedance-curve area, and fault count). An exponential parametric model $y=F(t)=a\exp(bt)$ is fit to relate LHI to age, and an “estimated age” $\hat T=F^{-1}(y)$ is used to compute RUL as $\mathrm{RUL}=T_{end}-\hat T$ with an industry end-of-life reference of 35 years. The system is demonstrated on 24 MTR escalators with dashboards for alarms/alerts and quarterly LHI/RUL outputs, positioning the approach as a practical PHM/CBM tool for asset replacement decisions using condition monitoring data.","Energy per minute is defined as $E_T=E_{Imp}+E_{Exp}$. Vibration feature $A_t=\sqrt{\sum_i x_i^2/1.5}$ is computed from selected FFT acceleration bands; exceedance probability $p_{i,\tau}=\frac{\sum I(x>\tau)}{|X_i|}$ and area $S_i=\int p_{i,\tau}\,d\tau$ are used, with overall vibration status $N=\sum_i W_i S_i$. The quarterly LHI is $y=\sum_{k=1}^5 \omega_k\,\text{(standardized feature)}$, and an exponential reference curve $y=F(t)=a\exp(bt)$ is inverted to compute estimated age $\hat T=F^{-1}(y)$ and $\mathrm{RUL}=T_{end}-\hat T$ (with $T_{end}=35$ years).","The fitted LHI–age reference model is $y=a\exp(bt)$ with $a=0.0928$ and $b=0.0665$, corresponding to an LHI growth factor of $\exp(b)=1.069$ per year. RUL is computed for 24 escalators (example: Escalator 11 has actual age 18.7 years and predicted $\mathrm{RUL}=21.45$ years based on $\hat T_{end}=40.15$). Vibration monitoring uses ISO-based alert/alarm thresholds (e.g., gearbox/motor acceleration alerts at 0.375 g and alarms at 0.75 g; main drive/tension carriage alerts at 0.15 g and alarms at 0.3 g) alongside exceedance-curve area to discriminate “normal” operation when thresholds are rarely exceeded. The paper reports quarterly LHI components and corresponding RUL values for all 24 assets in a table (2021 Q4 example output).","The authors note that the model may be expanded when more accurate fault data becomes available, implying current fault/incident inputs are limited by data quality/availability. They also state that verification of model predictions must be done over time as escalators age and approach end of useful life, indicating that long-horizon validation is not yet possible given current asset ages.","The RUL method hinges on an empirically fit exponential LHI–age curve and a fixed end-of-life age (35 years), which may not generalize across different escalator types, stations, maintenance regimes, or operating environments without recalibration. The LHI weights (e.g., 0.2/0.2/0.2/0.3/0.1) and vibration sensor weights appear expert-chosen rather than statistically learned/validated, so sensitivity to these choices and uncertainty quantification for RUL are not addressed. Maintenance-event detection from energy patterns and the use of min–max scaling based on observed extrema may be brittle under distribution shift, missing data, or sensor drift. No formal predictive accuracy metrics (e.g., time-to-failure error, coverage, backtesting) are provided, making it hard to assess prognostic performance beyond plausibility and case-study outputs.","The authors suggest incorporating more accurate fault data when available to improve the model. They also state that model predictions need to be verified over time as escalators become older and near end-of-life, implying future longitudinal validation. They note the approach could be applied to other engineering systems where energy consumption approximates workload and vibration indicates wear and tear.","Add uncertainty bounds for LHI and RUL (e.g., Bayesian or bootstrapped intervals) so refurbishment decisions can be risk-based rather than point-estimate-based. Validate prognostic performance via backtesting on historical run-to-failure or refurbishment outcomes, and compare against alternative RUL models (e.g., stochastic degradation processes, survival models with covariates, or modern sequence models) under consistent evaluation metrics. Develop robust/self-starting variants that handle missing sensors, nonstationary usage, autocorrelated measurements, and concept drift, including automatic recalibration of min–max scaling and thresholds. Provide an implementable software package or reproducible pipeline and document computational requirements for real-time deployment across large fleets.",2306.05436v1,https://arxiv.org/pdf/2306.05436v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:29:31Z
TRUE,System reliability|Other,Simulation-based|Nonparametric/Semi-parametric|Other,Sensor/condition monitoring|Other,Not applicable,Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.sciencedirect.com/science/article/pii/0266892088900197,"The paper develops variance-based reliability sensitivity analysis (RSA) for systems with statistically dependent input variables, targeting the variance of the rare-event indicator $I[g(X)\le 0]$ (failure event). Building on the idea of separating dependence-driven effects from model-driven effects, it defines and estimates both “full” Sobol’ indices (influence through the computational model and the probabilistic dependence model) and “independent” Sobol’ indices (influence through the computational model only, as if inputs were independent). The main methodological contribution is an estimator that computes the full set of first-order and total (including independent/full variants) reliability sensitivity indices using a single set of failure samples, obtained as a byproduct of one run of a sampling-based rare-event probability estimator (e.g., Monte Carlo, cross-entropy importance sampling, subset simulation), requiring no additional limit-state evaluations. The approach uses $d$ cyclically shifted hierarchical Rosenblatt/Nataf isoprobabilistic transformations to map dependent failure samples into standard-normal spaces where kernel density estimation enables evaluating conditional failure probabilities via Bayes’ rule. Numerical studies on a nonlinear test function, a short-column reliability problem (Nataf model), and a high-dimensional monopile foundation example demonstrate large reductions in required model calls versus pick-freeze approaches, while highlighting that total/ higher-order indices degrade due to multivariate KDE curse-of-dimensionality.","Failure probability is $P(F)=\int \mathbb{I}[g(x)\le0] f_X(x)\,dx$. For the failure-indicator target, first-order reliability Sobol’ indices can be written as $S_i = \frac{P(F)}{1-P(F)}\,\mathrm{Var}\left[\frac{f_{X_i|F}(X_i)}{f_{X_i}(X_i)}\right]$ (and analogously for totals using $X_{\sim i}$). Dependent-input separation is achieved by defining $d$ cyclic hierarchical transformations $T^{\{i\}}:X\mapsto U^{\{i\}}$ (Rosenblatt/Nataf) and computing full vs independent indices via conditioning on specific components of $U^{\{i\}}$ as in Eq. (13); the paper estimates these using KDEs of $f_{U_v|F}$ and ratios $\hat f_{U_v|F}(U_v)/\phi(U_v)$ in Eqs. (17)–(20).","On the nonlinear test function, the proposed failure-sample (FS) approach achieves comparable or better estimator c.o.v. than pick-freeze competitors while cutting limit-state calls from millions to $4\times10^4$ when paired with iCE or subset simulation (Table 1), i.e., about two orders of magnitude reduction. For the short-column example with dependent inputs modeled via Nataf, the FS-iCE method again reduces calls from $\sim10^7$ (Mara pick-freeze baseline) to $4\times10^4$ while maintaining good accuracy for first-order indices; variability increases markedly for total indices due to 3D KDEs (Table 3). In the monopile foundation example (high-dimensional FE model with an 82D random field discretization but sensitivity focused on a few variables), the method indicates wind-load-related inputs dominate full sensitivity, and dependence explains a large share of the load effect (e.g., $S_H\approx0.75$ vs $S_{\mathrm{ind},H}\approx0.25$), while hyperparameters A and B have near-zero independent indices as they affect failure mainly through the probabilistic model (Fig. 6 discussion).","The authors note that the failure-sample estimator deteriorates for total Sobol’ indices and higher-order effects because it requires multivariate KDE of failure-conditioned densities, making it effectively limited to low-dimensional settings for totals (e.g., already problematic at 3D KDEs in the short-column case). They also state Silverman’s bandwidth rule can be inaccurate for multimodal targets and suggest more robust bandwidth selection (e.g., cross-validation) and potentially non-diagonal bandwidth matrices to improve KDE accuracy. They further highlight that constructing Rosenblatt transforms for generic (non-Nataf) dependent models becomes tedious/intractable as the number of dependent variables grows because many conditional CDFs/integrals must be derived and evaluated.","The approach assumes access to representative failure samples from the true failure-conditioned distribution; for some rare-event methods, sample dependence (e.g., SUS Markov chains) and weighting may require careful treatment beyond simple resampling/KDE to avoid bias. KDE-based density-ratio variance estimates can be numerically unstable in tails (small $\phi(U)$ regions) and sensitive to bandwidth choice, potentially producing outliers; a more explicit stabilization/regularization strategy is not developed. The method’s performance is demonstrated on a limited set of examples; robustness to strong non-Gaussian copulas, mixed discrete-continuous inputs, or model/measurement noise is not addressed. Practical guidance on minimum failure-sample sizes required for stable index estimation (especially for totals) is not formalized.","The authors suggest improving KDE accuracy for total indices by allowing non-zero off-diagonal bandwidth entries and replacing Silverman’s rule with more robust bandwidth selection methods (e.g., cross-validation). They also note the approach can be extended back from rare-event RSA to general sensitivity analysis of model output (SAMO) by replacing failure samples with output samples and using binning to estimate conditional expectations, enabling a cost reduction factor of $d$ in SAMO as well. They emphasize that the curse of dimensionality remains a limiting factor, implying future developments should address scalability for total indices in higher dimensions.","Develop self-normalized/weighted estimators that directly incorporate importance weights from rare-event samplers (e.g., CE-IS) into the density estimation and variance calculations, reducing bias when failure samples are not i.i.d. from $f_{X|F}$. Replace KDE with more scalable conditional density-ratio estimation methods (e.g., probabilistic classifiers, normalizing flows, or RKHS-based ratio estimation) to mitigate curse-of-dimensionality and improve total-index estimation. Provide theoretical error bounds or sample complexity guidance linking the number of failure samples to accuracy of the sensitivity indices under dependence. Extend the framework to time-dependent/trajectory reliability problems (stochastic processes) where failure is an event over time and dependence structure includes temporal correlation.",2306.10279v1,https://arxiv.org/pdf/2306.10279v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:30:22Z
TRUE,Degradation modeling|RUL prediction|Failure mode analysis|Maintenance optimization|Other,ML-based|Other,Degradation measurements|Sensor/condition monitoring|Other,Condition-based|Predictive|Not applicable,Energy/utilities|Transportation/logistics|Other,Other,TRUE,MATLAB|Other,Not provided,NA,"This master’s thesis surveys and demonstrates data-driven prognostics and predictive-maintenance workflows for mechanical equipment, emphasizing deep learning for diagnostics and remaining useful life (RUL) prediction. It develops and evaluates multiple neural architectures (fully connected networks, CNNs, and LSTMs) combined with signal-processing and feature-engineering methods (FFT, wavelets/CWT scaleograms, DWT-based features, and PCA) on public PHM datasets (NASA C-MAPSS turbofan engines; CWRU bearing fault data; FEMTO/PRONOSTIA run-to-failure bearings). For C-MAPSS, it performs both health-state classification and RUL regression, showing that sequence models (LSTM) reduce prediction noise versus feed-forward models. For bearings, it demonstrates CNN-based fault diagnosis by transforming vibration time series into images, and a separate CNN for healthy/faulty detection using wavelet scaleograms from run-to-failure data, plus a discussion of prognostic-feature quality via monotonicity and trendability. It concludes with a proposed transfer of these methods to oilfield equipment (e.g., Top Drive monitoring) using existing sensors and additional vibration sensing to reduce unplanned downtime and maintenance costs.","RUL is defined as $\mathrm{RUL}=t_f-t_c$ where $t_f$ is predicted failure time and $t_c$ is current time. For bearings image generation, the number of samples is $N=\left\lfloor\frac{\text{signal length}}{64\times64}\right\rfloor$ after reshaping vibration chunks into $64\times 64$ images. The thesis uses the continuous wavelet transform $\mathrm{CWT}_\psi^x(\tau,s)=\frac{1}{\sqrt{|s|}}\int_{-\infty}^{\infty} x(t)\,\psi^*((t-\tau)/s)\,dt$ for scaleograms, and LSTM gating/state-update equations (input/forget/output gates and $c_t=f_t c_{t-1}+i_t\tilde c_t$, $h_t=o_t\tanh(c_t)$) for sequence-based RUL prediction.","On NASA C-MAPSS, a feed-forward classifier (24 inputs; 2 hidden layers) reports 93.43% test accuracy with ROC AUC 0.9332 (precision 0.90, recall 0.98, F1 0.94). For C-MAPSS RUL regression, a fully connected network (1,473 parameters) produces noisy cycle-wise RUL predictions, while an LSTM model (216,730 parameters; sequence length 100) yields smoother predictions and reports MAE of 25.43 (train), 36.37 (validation), and 23.27 (test) with corresponding losses 1600.60/3039.68/1379.67. On CWRU bearing fault diagnosis using image-reshaped vibration signals, a CNN achieves 98.71% test accuracy (train 100%, validation 99.46%). On FEMTO/PRONOSTIA bearings health classification from scaleograms, a CNN achieves ~98% validation/test accuracy (validation 98.89%, test 98.44%) with reported precision 0.9896, recall 0.9694, F1 0.9794 and ROC AUC 0.8348.","The thesis notes that real industrial (oilfield) data are commercially and technically sensitive and therefore unavailable; as a result, it relies on public benchmark datasets and proposes transferring knowledge to oilfield applications. It also states that RUL target construction is a modeling choice (linear/piecewise/polynomial) and that none of these can be claimed to perfectly represent real-world degradation behavior. It cautions that data-driven models become less reliable on previously unseen failure modes and stresses the need for training data covering diverse degradation patterns.","The evaluations are largely limited to a small set of benchmark datasets and do not include a real oilfield deployment or rigorous external validation, so generalizability to operational oilfield environments (sensor noise, missingness, drift, changing regimes) is uncertain. The thesis reports high accuracies for bearing classifiers that may be influenced by dataset construction choices (e.g., selecting first/last snapshots as healthy/faulty, reshaping signals into images) and does not thoroughly address potential data leakage, class imbalance impacts, or cross-condition transfer (train on one operating condition, test on others). Uncertainty quantification for RUL (prediction intervals, calibration) is discussed conceptually but not evaluated quantitatively, which is important for maintenance decision-making. Maintenance optimization is discussed at a high level, but no explicit policy optimization (cost models, decision thresholds, optimal intervention timing) is formulated or validated.","The thesis proposes transferring and deploying the demonstrated data-driven prognostics approach to oilfield equipment by leveraging existing rig sensors and adding dedicated vibration sensors (e.g., for Top Drive gears/bearings) to enable earlier fault detection and better maintenance planning. It also emphasizes collecting higher-quality and more diverse monitoring data that represent different degradation patterns and fault modes to improve model robustness and reliability in real applications.","A valuable extension would be a cross-domain/transfer-learning study that trains on public datasets and adapts to limited labeled oilfield data via domain adaptation and semi-supervised learning. Adding principled uncertainty estimation (e.g., Bayesian deep learning, conformal prediction) and evaluating calibration would better support risk-aware maintenance decisions. Robustness studies under realistic industrial issues (autocorrelation, non-stationarity, sensor failures, missing data) and comparisons against classical PHM baselines (e.g., particle/Kalman filtering, proportional hazards, Wiener/Gamma degradation processes) would strengthen conclusions. Finally, integrating predictions into an explicit maintenance optimization framework (cost/downtime tradeoffs, decision thresholds, scheduling constraints) would connect prognostics outputs to measurable operational value.",2306.11040v1,https://arxiv.org/pdf/2306.11040v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:31:27Z
FALSE,NA,ML-based|Bayesian|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical|Finance/economics|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/yookoon/density_uncertainty_layers,"This paper proposes a “density uncertainty criterion” for reliable predictive uncertainty in deep neural networks: the predictive variance should increase for inputs that are improbable under the training input density and decrease for probable inputs. To satisfy this by design, the authors introduce the density uncertainty layer, which couples stochastic layer outputs with a learned layer-wise energy-based model (Gaussian energy) of hidden representations so that the layer’s predictive variance is proportional to the energy. The method is evaluated on uncertainty benchmarks including UCI regression and CIFAR-10/100 classification, and compared against MFVI, Monte Carlo Dropout, Variational Dropout, Rank-1 BNN, and (for WRN28) SNGP/DUE. Empirically, density uncertainty layers improve calibration (lower ECE) and negative log-likelihood while maintaining or improving accuracy, and provide strong out-of-distribution detection on SVHN using energy statistics. The work advances uncertainty estimation in Bayesian deep learning by directly constraining predictive uncertainty to reflect training-data density rather than relying solely on parameter-posterior approximations.","The density uncertainty criterion is defined as \(\mathrm{Var}_{q(\theta)}[f(x;\theta)] \propto E(x)\), where \(E(x)\) is an energy (unnormalized negative log-density) fitted to training inputs. The density uncertainty layer instantiates this by making a stochastic linear unit \(f^j_\ell(h_\ell)=w^j_\ell\cdot h_\ell + \epsilon^j_\ell\sqrt{E_\ell(h_\ell)}+\eta^j_\ell\) with Gaussian energy \(E_\ell(h_\ell)=\tfrac12 h_\ell^\top \Sigma_\ell^{-1} h_\ell\), yielding \(\mathrm{Var}[f^j_\ell(h_\ell)] = \gamma^j_\ell E_\ell(h_\ell)+\beta^j_\ell\), which is proportional to energy up to a bias term.","On CIFAR-10 with ResNet-14, Density Uncertainty achieves ECE 0.004±0.000 versus 0.013–0.080 for baselines, with accuracy 92.2±0.3 and NLL 0.226±0.003. On CIFAR-100 with ResNet-14, it improves accuracy to 68.8±0.2 and reduces ECE to 0.011±0.003 (baselines 0.041–0.132) with NLL 1.110±0.006. With WRN28 on CIFAR-10, it reaches 96.4±0.1 accuracy and NLL 0.119±0.003 (rank-1 mixture: 96.5±0.0, NLL 0.118±0.001) while maintaining low ECE ≈0.010–0.011. For OOD detection (ResNet-14, CIFAR-100→SVHN), Density Uncertainty attains AUPRC 0.908±0.015 and AUROC 0.800±0.024, outperforming baselines (AUROC roughly 0.640–0.681).","The appendix notes a caveat on some small UCI datasets: the variance of results can be high due to small dataset sizes, and performance can be sensitive to hyperparameters. The paper also notes that the rank-1 covariance construction for the Gaussian energy model may be overly restrictive in practice (hence using a mixture of rank-1 Gaussians).","The method relies on fitting Gaussian (or low-rank Gaussian-mixture) energy models of hidden features; if hidden representations are highly non-Gaussian or multi-modal, the energy may be a poor proxy for true training density, potentially miscalibrating uncertainty. Jointly training discriminative and generative (energy) objectives introduces extra hyperparameters and potential training instability/identifiability (e.g., scaling of energy vs. noise variances), but the robustness of these choices is not fully characterized. OOD detection uses a heuristic statistic based on squared deviation of layer energy from an in-distribution mean, which may not generalize across diverse shifts or adversarial OOD without further validation. Comparisons focus on selected baselines; broader modern uncertainty/OOD methods (e.g., ensembles, temperature scaling variants, conformal prediction, diffusion-based likelihood-free scores) are not evaluated.",None stated.,"Evaluate density uncertainty layers under distribution shifts beyond SVHN (e.g., corruptions, near-OOD, covariate vs. label shift) and against stronger OOD baselines (ensembles, energy-based OOD scores, conformal methods). Extend the approach to non-Gaussian or more expressive energy models (normalizing flows, diffusion feature densities) and study how energy-model misspecification impacts calibration. Develop self-normalizing or scale-identifiable training objectives so the proportionality between variance and energy is stable without careful tuning, and provide theoretical guarantees for calibration/OOD detection. Provide lightweight implementations and ablations (per-layer vs. selected layers, computational trade-offs) for large-scale deployments.",2306.12497v2,https://arxiv.org/pdf/2306.12497v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:32:10Z
FALSE,Other,Bayesian|Other,Other,Not applicable,Other,Other,TRUE,R|C/C++|Other,Not provided,https://doi.org/10.1016/j.paid.2023.112345|https://CRAN.R-project.org/package=semTools|https://www.R-project.org/|https://doi.org/10.18637/jss.v048.i02,"The paper studies the stability of an achievement-based general factor (g) over time using Italian INVALSI large-scale student assessment data (2010–2022) across grades 2, 5, 8, and 10 (N≈1.95M assessments). The authors fit year- and grade-specific bifactor CFA models (g plus a specific reading factor) and extract McDonald’s omega (ω) reliabilities and average explained variance (R²) for g. They then model ω and R² as outcomes in Bayesian beta-regression (via Stan/brms) with predictors year, grade, and their interaction, selecting models by WAIC. Results show little overall evidence for weakening of achievement-g over time; the best-fitting models include a grade×year interaction indicating decreases for lower grades and stable or increasing trajectories for higher grades. The work is methodological/psychometric rather than reliability engineering, using reliability coefficients (ω) in the psychometric sense to interpret cohort/grade trends related to Flynn-effect hypotheses.","The measurement model is a bifactor CFA with two orthogonal latent factors: a general factor $g$ loading on math and reading sub-area scores, and a specific reading factor loading on the two reading sub-areas (with equal loadings constrained for convergence). For each year×grade model, McDonald’s omega (ω) is computed for the latent factors, and the average explained variance by a factor is computed as the mean of squared standardized loadings ($\bar{R}^2=\text{mean}(\lambda_j^2)$) given orthogonality. The extracted $\omega$ and $R^2$ values are modeled using a beta distribution in Bayesian regression: $\omega \sim \text{Beta}(\mu,\phi)$ and $R^2 \sim \text{Beta}(\mu,\phi)$ with linear predictors including year, grade, and year×grade; models are compared using WAIC.","Across 39 fitted bifactor models, fit indices were generally strong (median RMSEA=0.041; median SRMR=0.010; median CFI=0.996; median NNFI=0.991). The best WAIC model for both $R^2$ and $\omega$ included the year×grade interaction (WAIC: −135.432 for $R^2$; −170.744 for $\omega$). Observed $R^2$ for g was largely between about 0.40 and 0.60 across years/grades, and $\omega_g$ values were typically high (many year×grade cells roughly ~0.80–0.92). In the interaction model, the main effect of year was negative and not significant for both outcomes (e.g., $R^2$: year −0.020, 95% CI [−0.054, 0.015]; $\omega$: year −0.016, 95% CI [−0.062, 0.032]), while the year×grade term was positive (e.g., $R^2$: 0.009, 95% CI [0.002, 0.015]). The interaction implies decreasing achievement-g strength over time in lower grades and stable/increasing trends in higher grades.","The authors note that observed cross-temporal changes might reflect periodic revisions of the INVALSI assessment (e.g., administration mode changes) that could alter unique variance components of subtests. They acknowledge the observation window is comparatively short. They also note the INVALSI subscales cover a relatively narrow set of abilities, which could reduce the salience of the extracted g factor and make g-changes harder to detect. Finally, they state they could not formally investigate test score changes due to within-region, cohort, and grade-based assessment revisions.","Because INVALSI is restandardized each year, interpreting cross-temporal changes in factor structure indices (ω, $R^2$) may still be sensitive to shifting item pools, scaling, and differential item functioning; explicit measurement invariance/DIF testing across years and grades is not fully developed here. Modeling ω and $R^2$ with beta regression requires values strictly in (0,1); the paper does not discuss any transformations or boundary handling if estimates approach 0/1. The analysis treats year and grade as continuous and uses WAIC selection, but does not report sensitivity to alternative functional forms (e.g., nonlinear time trends) beyond interaction or to potential autocorrelation across years. Despite the huge sample sizes, the work focuses on aggregate indices (year×grade) rather than modeling individual-level uncertainty or school/region clustering, which could affect inference about temporal patterns.",None stated.,"A useful extension would be formal longitudinal/cross-cohort measurement invariance testing (and DIF analyses) across years and grades to separate true manifold-strength changes from test design changes. Another direction is to model potentially nonlinear (e.g., spline/curvilinear) time effects directly rather than inferring curvilinearity via lagged-grade arguments. Incorporating multilevel structure (students nested in schools/regions) could quantify how much temporal variation in g-strength is attributable to institutional or regional factors. Releasing analysis scripts or providing a reproducible workflow (even without raw INVALSI data) would strengthen transparency and enable robustness checks.",2307.12003v1,https://arxiv.org/pdf/2307.12003v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:33:01Z
TRUE,RUL prediction|Software reliability|Maintenance optimization|Other,"Parametric (Weibull, etc.)|ML-based|Hybrid/Ensemble|Other",Degradation measurements|Sensor/condition monitoring|Mixture of types|Other,Predictive|Condition-based|Not applicable,Other,Case study (real dataset)|Other,TRUE,Python|R|Other,Not provided,https://doi.org/10.22215/jphm.v3i1.3641|https://bugzilla.mozilla.org/|https://docs.mojolicious.org/#CONVENTIONS|https://www.ravenbrook.com/project/p4dti/version/0.4/design/architecture/|http://trustsoft.uni-oldenburg.de,"The paper demonstrates a Prognostics and Health Management (PHM) approach for software systems by defining remaining useful life (RUL) over release cycles (rather than calendar time) using response time as the primary performance/health indicator. It proposes a fusion pipeline that collects fault and enhancement-request text data and performance measurements, then converts faults/enhancements into a cumulative Combined Predictive Variable (CPV) via NLP-based sizing (Naïve Bayes story-point classification), impact factors, and accumulation over releases. Analogous releases are grouped with k-means clustering, and response time is prognosed from CPV using a regression model; RUL is then the number of future releases until a user-defined performance threshold (e.g., 9 seconds response time) is crossed. The method is demonstrated on the open-source Bugzilla application using staged testbeds and publicly available issue data, comparing predicted vs. measured response times and reporting statistical validation (R²/adjusted R², p-values, and k-fold cross-validation). The work advances software PHM by explicitly operationalizing software RUL estimation and showing how release planning “combos” can be evaluated to maximize the number of releases before performance becomes unacceptable.","The core constructed health/predictor metric is the Combined Predictive Variable (CPV) per release: $\mathrm{CPV}=\sum_{i=1}^n (\pm \mathrm{SP}_i)\,\mathrm{IF}_i$, where story points (SP; can be negative for improvements) are multiplied by impact factors (IF) and summed; CPV is treated cumulatively across releases to reflect accumulated degradation/repair. Text classification for story-point sizing uses Naïve Bayes via $P(c\mid x)=\frac{P(x\mid c)P(c)}{P(x)}$. Prognosis of response time uses a univariate linear regression of the form $y=mx+b$ with $y=\mathrm{RT}$ and $x=\mathrm{CPV}$; RUL is inferred by projecting RT over future release IDs and finding when RT exceeds a defined threshold.","For Bugzilla, CPV and response time (RT) show strong positive association (reported correlation about 0.95). Linear regression models fit separately by cluster achieved high goodness-of-fit: Cluster A multiple $R^2=0.9839$ (adjusted $R^2=0.9798$, p-value 0.00009) and Cluster B multiple $R^2=0.8968$ (adjusted $R^2=0.8796$, p-value 0.00036). A small test-set comparison reported, e.g., release 5.0.2 actual RT 5416 ms vs predicted 5248 ms, and release 5.0.3 actual 5579 ms vs predicted 5634 ms. A 2-fold cross-validation (70/30 split) yielded an average validation score of 0.9629. Scenario planning across four future “release combos” indicated Combo-1 produced the longest projected RUL before crossing the 9-second RT threshold.","Due to limited data, the authors note that although an elbow plot suggested 3 clusters, they used 2 clusters because with the limited amount of data they could not obtain meaningful $R^2$ or p-values when running regression on one of the clusters. They also note that performance thresholds (e.g., for response time) are not standardized in industry and must be user-defined, and that their averaging approach for response time is specific to the demonstration and may differ for other parameters/software. They indicate that with more data, relationships among clusters/future releases can be evaluated further and other/ensemble models can be applied.","The CPV construction depends on subjective choices (story-point scale, impact-factor mapping, and the sign convention for “improvements”) and may be difficult to calibrate consistently across teams/projects without strong governance or inter-rater reliability checks. The demonstration uses a simple univariate linear model (RT~CPV); omitted-variable bias is likely because response time is influenced by many confounders (workload mix, infra, database state, caching, deployment configuration), and the method’s robustness to such factors is not systematically tested. The empirical evaluation is based on a single software system and a small number of historical releases, limiting generalizability and the strength of claims about PHM applicability broadly. The NLP sizing model and clustering decisions may be unstable with sparse data; no sensitivity analysis is provided for alternative classifiers, impact scales, or cumulative-damage assumptions.","The authors state they will explore other prognostic models and compare model accuracy, and extend RUL estimation beyond response time to other performance parameters listed (e.g., throughput, availability, utilization). They propose developing a more robust function that combines impacts of multiple influencing factors (e.g., code changes and processor-speed changes) to produce a resultant response time. They also plan to explore classical software reliability models and alternative classification/clustering models, and to experiment on other software systems (e.g., mission-critical software and big-data tools such as Spark and H2O).","Develop a formal uncertainty-quantification layer (prediction intervals for RT and RUL) so maintenance/release decisions can be risk-based rather than point-estimate-based. Add methods for handling nonstationarity across major architectural changes (concept drift) and for self-starting deployment on new projects with limited labeled story-point/impact data. Validate the approach on multiple real industrial repositories and include stronger baselines (e.g., time-series models on RT alone, survival/RUL models, and modern NLP encoders) with consistent backtesting protocols. Provide an open-source reference implementation and guidance for CPV calibration (including inter-team normalization) to improve reproducibility and adoption.",2307.12237v1,https://arxiv.org/pdf/2307.12237v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:33:45Z
FALSE,NA,ML-based|Nonparametric/Semi-parametric|Simulation-based|Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/jjfeng/testing_strong_calibration,"This paper proposes hypothesis tests for “strong calibration” of probabilistic risk prediction models, asking whether there exists any (possibly small) subgroup for which predicted probabilities deviate from true event rates beyond a tolerance. The method reframes subgroup miscalibration detection as a changepoint detection problem by ordering audit observations using predicted residuals from auxiliary residual models, then applying a score-based CUSUM test and taking a maximum over candidate detectors/hyperparameters. It presents a finite-sample Type I error–controlled sample-splitting version via a Monte Carlo null based on resampling outcomes from the model’s predicted probabilities, and an extended cross-validated version with asymptotic Type I control under a weak uniform convergence condition. Simulations show consistently higher power than Hosmer–Lemeshow, Platt-scaling score tests, and prior adaptive GOF approaches, especially when miscalibration is confined to small subgroups. A real EHR case study on one-year mortality risk prediction finds strong evidence of underestimation in a small subgroup (notably involving age and high predicted risk), and provides diagnostic visualizations (CUSUM “control charts” and permutation variable importance).","The approach defines a working logistic recalibration model logit(p(X;h)) = logit(\hat p_\delta(X)) + \theta h(X), yielding score \dot\ell(Y\mid X;h) = (Y-\hat p_\delta(X))h(X). Candidate detectors are formed from learned residual models \hat g_{\lambda,n}(X) via thresholding: \hat h_{\lambda,\gamma,n}(X)=\hat g_{\lambda,n}(X)\mathbf{1}\{\hat g_{\lambda,n}(X)\ge \gamma\}. The sample-split statistic is a max over models and thresholds of a score-based CUSUM sum \sum (Y_i-\hat p_\delta(X_i))\hat g_{\lambda,n}(X_i)\mathbf{1}\{\hat g_{\lambda,n}(X_i)\ge \gamma\}, with critical values from Monte Carlo resampling Y_i^*\sim\text{Bernoulli}(\hat p_\delta(X_i)) (and a bounding variant for the two-sided test).","In simulations across multiple tolerances (e.g., \delta=0.025, 0.05, 0.075) and varying audit sizes, the proposed AdaptiveScoreCUSUM consistently shows higher power than Hosmer–Lemeshow/chi-squared binning, Platt-scaling score tests, and prior adaptive GOF tests; the authors report cases where power “more than doubles” versus adaptive chi-squared and substantial gains for smaller subgroups at larger \delta. In an EHR mortality-risk audit (n=4000/6000/8000 test sets; 40 replicates), the underestimation one-sided test using AdaptiveScoreCUSUM reaches a 100% rejection rate, reported as more than five times higher than other methods, while there is little evidence for overestimation. Diagnostics suggest the detected poorly calibrated subgroup is characterized primarily by age and the original predicted risk; stratified calibration curves indicate underestimation among patients under 60 with predicted risk over 40%. The paper also includes an additional simulation indicating Type I error control for the CV-based procedure at a nominal 0.1 level even for small n (e.g., n=100), with the two-sided version being more conservative due to bounding.","The authors note that auditing strong calibration is difficult due to the curse of dimensionality and that many existing strong-calibration procedures require very large sample sizes (often tens or hundreds of thousands), motivating a testing (yes/no) approach rather than full subgroup identification or model revision. For the cross-validated extension, they state Type I error control is asymptotic (not finite-sample) and requires a uniform convergence assumption on the learned detectors across folds. They also indicate that their variable-importance tool is permutation-based and is not the same as standard VI for average predictive performance, and mention that more sophisticated VI measures (e.g., Shapley-based) are left for future work.","The testing framework is tailored to IID audit data; performance and error control under autocorrelation, clustering (e.g., repeated patients), or dataset shift mechanisms beyond the resampling null may require additional justification. The method’s power depends on the quality/diversity of the residual models \hat g_{\lambda,n}; if the candidate model class misses the residual structure or is poorly tuned, the test may have low sensitivity despite correct Type I control. Practical implementation choices (number of hyperparameters, fold count K, residual-model families, and computational budget) can materially affect results, but robust default guidance and runtime/complexity analysis are limited. While the paper provides diagnostic plots, it remains primarily a detection test rather than a principled, post-selection subgroup characterization procedure with uncertainty quantification.","The paper explicitly suggests exploring more sophisticated variable-importance measures for diagnosing miscalibration, citing Shapley-value–based approaches as an example beyond permutation importance. It also notes that extensions for testing nonzero subgroup prevalence thresholds \epsilon>0 are straightforward by constraining detectors to have minimum support, which they describe in the appendix.","Developing versions that handle dependent data (clustered/longitudinal EHR, time drift) with valid Type I control would broaden applicability. Providing a post-rejection workflow for interpretable subgroup discovery with statistical guarantees (e.g., confidence sets for subgroup definitions or changepoint locations) would make the audit more actionable. Robustness studies under model misspecification of \hat p, label noise, and covariate shift (including calibration drift) could clarify when the resampling-based critical values remain appropriate. Packaging the method with well-documented, computationally efficient implementations and recommended default residual-model libraries would improve adoption in practice.",2307.15247v1,https://arxiv.org/pdf/2307.15247v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:34:30Z
TRUE,System reliability|Other,Bayesian|ML-based|Stochastic process,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,R|Python|C/C++|Other,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://bitbucket.org/gramacylab/deepgp-ex/,"The paper develops a reliability-oriented contour location (CL) active-learning framework for expensive aerospace computer experiments, using fully Bayesian deep Gaussian process (DGP) surrogates to identify a failure boundary (level set) separating pass/fail operating regimes. It targets reliability analysis where failure is defined by a threshold on a simulator output (e.g., the RAE-2822 airfoil’s aerodynamic efficiency with failure at lift-to-drag ratio L/D < 3), and improves sequential design by avoiding derivative-based acquisition optimization that is incompatible with MCMC-based DGP inference. Methodologically, it proposes using Delaunay-triangulation “tricands” as adaptive candidate points and selecting new simulations from the Pareto front that jointly maximizes (i) CL entropy (classification ambiguity near the boundary) and (ii) posterior predictive uncertainty, mitigating entropy’s known myopia and acquisition clustering. Performance is demonstrated via Monte Carlo simulation studies on synthetic functions and on a 7D CFD airfoil experiment, showing that DGP + Pareto-tricands sequential designs achieve better failure-region classification metrics with fewer simulator runs than space-filling designs and strong GP-based CL baselines. The work advances reliability-focused surrogate-based design by enabling practical CL with Bayesian DGPs and providing an open-source implementation.","The CL acquisition uses entropy on failure probability: $H(x)=-p_x\log p_x-(1-p_x)\log(1-p_x)$ with $p_x=P\{f(x)>g\}=1-\Phi\big((g-\mu(x))/\sigma(x)\big)$ under a Gaussian predictive distribution. For Bayesian DGPs fit via MCMC, entropy can be averaged over samples: $H(x)=\frac{1}{|T|}\sum_{t\in T}[-p_x^{(t)}\log p_x^{(t)}-(1-p_x^{(t)})\log(1-p_x^{(t)})]$ with $p_x^{(t)}=1-\Phi((g-\mu^{(t)}(x))/\sigma^{(t)}(x))$. New simulator evaluations are selected from candidate points (tricands) lying on the Pareto front of the 2D criterion $(H(x),\sigma(x))$, i.e., non-dominated points maximizing both entropy and predictive standard deviation.","On the 7D RAE-2822 CFD problem, a DGP sequential design initialized with 100 LHS points and adding 400 Pareto-tricands acquisitions (total $n=500$) outperformed static LHS fits and surpassed static-design performance with as few as ~200 simulator evaluations, saving nearly 12 hours versus running a 500-point LHS. Across synthetic benchmarks (2D plateau, 5D plateau, and 2D cross-in-tray), the DGP ESS + Pareto-tricands strategy achieved higher median sensitivity over acquisition iterations than stationary GP alternatives, and produced less clustered acquisitions than entropy-only strategies. The paper reports improved out-of-sample uncertainty quantification and accuracy for the Bayesian DGP over GP MLE and VI-DGP on the airfoil’s 500-train/4500-test evaluation, and shows sequential designs further improve sensitivity/specificity/F1 relative to static designs.","The authors note tricands rely on Delaunay triangulation, which becomes computationally prohibitive in higher dimensions; they report that around 8 dimensions is near the tractability boundary for the underlying quickhull library unless the design size is kept low. They also state that if the response surface is (effectively) stationary or the failure contour is simple/small, DGP complexity may be unnecessary and could overfit, and exploration-favoring Pareto-tricands may underperform simpler entropy-based methods. They further acknowledge that their VI-DGP baseline is sensitive to tuning and they do not pursue expensive auto-tuning, so VI comparisons are not the main focus.","Failure is treated as a hard threshold on the simulator output (e.g., L/D < 3), but the work does not deeply examine robustness of the learned contour to threshold misspecification or to heteroskedastic/numerical simulation error (beyond a fixed nugget). The main acquisition criterion uses only two objectives (entropy and predictive SD); it does not include explicit space-filling/diversity constraints for batch selection beyond what tricands geometry provides, nor does it quantify how Pareto-front size/shape affects exploration in higher dimensions. Empirical comparisons could be broadened to include more reliability/limit-state design baselines (e.g., SUR/excursion set methods with scalable integration, subset simulation-informed designs, or classification-oriented surrogates) under matched computational budgets including surrogate fitting time. The method assumes deterministic simulations and does not address model-form discrepancy or calibration to experimental data, which are common in reliability certification contexts.","They suggest replacing or improving the triangulation approach for higher-dimensional problems (e.g., updating only local portions of the triangulation after each acquisition, or developing new methods that mimic tricands behavior without full Delaunay triangulation). They propose exploring batch acquisition by taking multiple points from the non-dominated (Pareto) set (and iteratively recomputing Pareto fronts after removing selected points). They also note the potential value of incorporating an input distribution (uncertain inputs) directly into sequential design so the surrogate prioritizes contour regions most relevant to failure probability estimation, while controlling computational expense.","A natural extension is a self-starting/online variant that accounts for unknown thresholds or multiple limit states (multi-contour reliability), including adaptive threshold selection driven by risk constraints. Another direction is to handle correlated or multi-fidelity simulators (common in CFD) within the same Pareto-tricands framework, selecting both fidelity level and input location to optimize cost-aware reliability estimation. Robust versions could address non-Gaussian predictive distributions, simulator noise, and input-dependent numerical error, possibly via heteroskedastic DGPs or Bayesian classification surrogates that model pass/fail directly. Finally, scalable geometry alternatives (e.g., kNN-based gap-filling candidates, Voronoi/cover-tree approximations) could generalize tricands-like candidate placement beyond the ~8D limit while retaining exploration guarantees.",2308.04420v3,https://arxiv.org/pdf/2308.04420v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:35:32Z
FALSE,NA,ML-based|Simulation-based|Other,Other,Not applicable,Healthcare/medical|Network/cybersecurity|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://osf.io/4vq8f|https://www.mdpi.com/2079-9292/8/8/832|https://cjc.utpjournals.press/doi/10.22230/cjc.2016v41n2a3068|http://arxiv.org/abs/2103.11251|http://arxiv.org/abs/1909.06677|http://arxiv.org/abs/2207.02598|https://www.pycaret.org,"This preprint studies the “Rashomon effect” (multiple near-optimal models relying on different features) and how it impacts the reliability of post-hoc explanations produced by SHAP. The authors propose an evaluation framework that (i) selects a set of high-performing models (top-3 by Cohen’s kappa), (ii) retrains them under 10-fold cross-validation across increasing sample sizes, and (iii) quantifies intra-model and inter-model agreement of SHAP feature importances, also comparing to a bagging ensemble. Agreement is measured with a top-j overlap metric and a weighted cosine similarity that downweights unimportant features, and convergence is assessed against a “consensus” explanation defined as the mean absolute SHAP values at the largest available sample size. Experiments on five public classification datasets show that explanation variability is high at small sample sizes (notably <~128 samples) and tends to decrease as sample size increases, leading to greater consensus across models; bagging often increases agreement but not uniformly at low sample sizes. The paper frames these results as guidance for when post-hoc explanations may be considered trustworthy and emphasizes validating explanations when data are limited.","The Rashomon set is defined as $\mathcal{R}(\mathcal{F}, f^*,\epsilon)=\{f\in\mathcal{F}: L(f)\le L(f^*)+\epsilon\}$. Population-level feature importance for a model is summarized by mean absolute SHAP: $\frac{1}{N}\sum_{n=1}^N|\phi^{\mathrm{SHAP}}_{f,n}|$. Similarity is computed via (i) top-$j$ ranking $R(\phi)=\mathrm{argsort}(|\phi|)[::-1][:j]$ and pairwise top-$j$ overlap, and (ii) a weighted cosine similarity $\mathrm{wcossim}(w_{f_1},w_{f_2})=\frac{w_{f_1}\cdot w_{f_2}}{\|w_{f_1}\|_2\|w_{f_2}\|_2}$ with weights derived from mean absolute SHAP. A cross-model “consensus” is defined as $\bar\phi^{\mathrm{SHAP}}_{\mathrm{consensus}}=\frac{1}{MN}\sum_{i=1}^M\sum_{j=1}^N |\phi^{\mathrm{SHAP}}_{f_i,j}|$ at the largest sample size.","Across five datasets, explanations from small training subsets exhibited high variability and limited agreement; the authors highlight that subsets below roughly 128 samples produced especially unstable explanations. Spearman correlations between sample size and intra-model agreement (weighted cosine similarity across CV folds) were significant for some datasets (e.g., Compas $r\approx0.889$, $p<0.001$) but not for German (poor model convergence) and Student (task too easy/early saturation). Inter-model convergence toward the consensus explanation also increased with sample size for several datasets (e.g., Diabetes $r\approx0.862$, $p<0.001$; Framingham $r\approx0.875$, $p<0.001$). Bagging often improved agreement at larger sample sizes and in some cases converged well to the consensus, but the paper notes it was not consistently superior at low sample sizes (e.g., Diabetes).","The authors note that they only tested a limited set of model families (mostly linear models and tree/ensemble methods) and that neural networks may behave differently; they suggest transfer learning could mitigate Rashomon variability but was not evaluated. They also acknowledge limited domain coverage and that more extensive experiments on diverse real-world problems (including regression) are needed. They point out dataset-specific issues: the Student task was too simple to properly stress-test the methodology, while the German dataset likely requires a modified pipeline (e.g., dimensionality reduction such as PCA or imbalance correction such as SMOTE), which they intentionally did not apply for consistency. Finally, they state that only SHAP was used and that model-specific explainers (e.g., attention/Grad-CAM) and other post-hoc methods (e.g., LIME) may show different convergence/variability patterns.","The “consensus” baseline is defined using the largest-sample explanations, which may bias conclusions toward that particular regime and does not guarantee correctness (only agreement). The study focuses on agreement metrics (top-$j$, weighted cosine) without directly validating explanation fidelity/faithfulness or causal relevance, so high agreement could still reflect shared spurious signals. Model selection uses top-3 configurations by Cohen’s kappa and additional random-search tuning per sample size; this can entangle explanation variability with changing hyperparameters rather than isolating sample-size effects alone. Results rely on repeated CV on subsets of the training set but do not fully address dependence induced by overlapping subsamples, nor do they examine robustness to correlated features/feature redundancy, which can strongly affect SHAP rankings and agreement metrics.","They propose extending experiments to more complex model classes, especially neural networks, and studying how transfer learning affects explanation alignment and Rashomon variability. They suggest broader evaluations across additional datasets/domains and problem types, including regression. They recommend comparing SHAP with model-specific explanation methods (e.g., attention maps, Grad-CAM) and with other post-hoc methods such as LIME to assess robustness and convergence differences. They also suggest exploring ensembles/bagging for explanation robustness more generally and testing Rashomon and explanation stability in online learning settings with continual retraining.","A useful extension would be to couple agreement/convergence metrics with faithfulness tests (e.g., deletion/insertion, sufficiency/necessity, counterfactual checks) to distinguish “stable” from “correct” explanations. Evaluating the effect of feature correlation and using grouped/permutation-based SHAP (or conditional SHAP) could clarify whether disagreement is driven by multicollinearity rather than true model differences. Developing a self-starting or sequential stopping rule (sample-size adequacy criterion) based on convergence diagnostics could operationalize the proposed “power analysis” idea for practitioners. Providing an open-source implementation (reproducible scripts and configs) and benchmarking against alternative stability measures (e.g., rank correlation, Kendall’s tau, Jaccard over multiple $j$) would improve reproducibility and comparability across studies.",2308.07247v1,https://arxiv.org/pdf/2308.07247v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:36:23Z
FALSE,NA,Other,Simulated only,Not applicable,Theoretical/simulation only|Healthcare/medical|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper is a psychometrics methodology study (test-score reliability), not a reliability engineering paper. It proves that under a broad true-score model with randomly sampled items and conditionally independent errors, test reliability converges to 1 as test length grows, and the asymptotic convergence rate is characterized by the Spearman–Brown formula without requiring parallel items, unidimensionality, or finite-dimensional latent traits. It then uses Monte Carlo simulations under multidimensional and unidimensional 2PL IRT models to assess how well Spearman–Brown predicts reliability for short tests, finding substantial positive bias for short multidimensional tests but near-unbiased behavior for short unidimensional tests. Additional simulations examine irregular/binary item-parameter distributions and quantify variability of reliabilities across random test forms at fixed length, showing decreasing SD with longer tests.","The generalized Spearman–Brown lengthening formula is $\rho_n = \frac{n\rho_1}{1+(n-1)\rho_1}$, with rescaling via $SB(x,n)=\frac{nx}{1+(n-1)x}$ and inverse $SB^{-1}(x,n)=SB(x,1/n)$. Test-form reliability is defined as $\rho(\mathbf S_n)=\frac{\operatorname{var}(T_{\mathbf S_n}\mid \mathbf S_n)}{\operatorname{var}(X_{\mathbf S_n}\mid \mathbf S_n)}$, where $X_{\mathbf S_n},T_{\mathbf S_n},E_{\mathbf S_n}$ are averages over the randomly sampled items. The main asymptotic result states $SB(\rho(\mathbf S_n),1/n)\to \frac{\operatorname{var}(T_\infty)}{\operatorname{var}(T_\infty)+\mathbb E(\varepsilon^2(R_1))}$ a.s. as $n\to\infty$.","Simulation Study 1 (2PL IRT with 1, 2, or 5 dimensions; item pools of 1000; 1000 random test forms per length) shows rescaled mean reliabilities are nearly constant for unidimensional tests (largest absolute deviation across lengths within a case 0.0098, at mean rescaled reliability 0.38), but decrease for short multidimensional tests, indicating positive bias in short-test reliabilities and over-optimistic Spearman–Brown forward predictions. Maximum absolute prediction errors of mean reliabilities are \le 0.02 in unidimensional cases, but range roughly 0.03–0.12 in 5-dimensional cases. Simulation Study 2 (unidimensional, irregular/binary and literature-based item parameters) reports maximum prediction error 0.012 and maximum absolute difference in rescaled mean reliabilities 0.017. Simulation Study 3 reports median SD of reliability across random forms decreases from 0.034 (n=10) to 0.0043 (n=50), with maximum SD 0.131; the 90th percentile SD drops from 0.074 (n=10) to 0.031 (n=50).",None stated.,"The study is framed in psychometric reliability and assumes a true-score/IRT setting with conditional independence (local independence) and random sampling of items from an effectively infinite pool; these assumptions may not hold in many practical test-construction settings (fixed finite pools, content balancing, item dependence). Simulation results are based on 2PL models with normally distributed, independent latent traits and items loading on exactly one dimension in the multidimensional conditions, which may not represent common multidimensional structures (e.g., cross-loadings, correlated traits). The paper performs numerical integration and Monte Carlo experiments but does not report shared code or detailed implementation settings sufficient for exact replication (e.g., software, integration method, grids/tolerances).",None stated.,"Extend the finite-length bias analysis beyond 2PL to polytomous IRT models and to multidimensional models with correlated traits and cross-loadings to test robustness of the short-test bias finding. Study effects of local dependence (testlets) and non-random item selection schemes (content constraints, adaptive testing) on the validity of Spearman–Brown predictions. Provide and validate an easy-to-use correction or diagnostic for when forward Spearman–Brown predictions from a short test are likely to be over-optimistic, and release reproducible code/software to support practitioners.",2308.13811v1,https://arxiv.org/pdf/2308.13811v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:36:54Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Network/infrastructure reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Condition-based|Predictive|Imperfect maintenance|Other,Transportation/logistics|Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|R|Other,Not provided,https://doi.org/10.1016/j.ress.2023.109496|https://doi.org/10.1016/j.ress.2023.109709|https://doi.org/10.1016/j.ress.2021.107827|https://doi.org/10.1016/j.trc.2018.06.018|https://doi.org/10.1016/j.ress.2023.109687|https://doi.org/10.1115/JRC2022-79370|https://doi.org/10.1016/j.ress.2013.02.010|https://doi.org/10.1061/(ASCE)TE.1943-5436.0000825|https://doi.org/10.3390/s22197275|https://doi.org/10.1080/23248378.2021.1875065|https://doi.org/10.1243/1748006xjrr234|https://doi.org/10.1016/j.ifacol.2016.11.021|https://doi.org/10.1016/j.trc.2018.02.019|https://doi.org/10.1080/15732479.2019.1629464|https://doi.org/10.1080/15732479.2019.1679193|https://doi.org/10.1080/15732479.2017.1326946|https://doi.org/10.1016/j.ress.2020.107359|https://doi.org/10.1061/JTEPBS.0000149|https://doi.org/10.1002/9780470975916|https://doi.org/10.1016/j.retrec.2012.03.011|https://doi.org/10.1016/j.ress.2015.05.009|https://doi.org/10.1016/j.ress.2022.108922|https://doi.org/10.1016/j.ejor.2020.09.030,"The paper develops a multivariate track-geometry degradation model for rail infrastructure using a multivariate Wiener process to jointly represent multiple correlated geometry indicators (e.g., left/right top and alignment) and their non-monotonic evolution. It explicitly incorporates imperfect maintenance (tamping) by introducing post-maintenance latent states and treating tamping as occurring within inspection intervals, altering the transition distribution during those intervals. Because inspection/maintenance data are sparse at the segment level, the authors use Bayesian inference and a hierarchical Bayesian structure, estimated via MCMC/Hamiltonian Monte Carlo (implemented using Stan and Python; PyMC/Stan/R are referenced), to pool information across segments and estimate drifts, covariances, correlations, and post-tamping levels. The model is validated on a real 18 km Queensland commuter line (182 ~100 m segments) with TRV measurements; the multivariate model yields substantially better predictive coverage on held-out inspections than univariate independent-Wiener models. Finally, the fitted model is used in Monte Carlo simulations to estimate first-hitting times to maintenance thresholds and to demonstrate a condition-based inspection (TRV scheduling) policy that can reduce inspection runs while maintaining similar failure (D1 exceedance) rates.","Track segment indicator vector is modeled as a multivariate Wiener process $\mathbf{Z}_i(t)=\boldsymbol\mu_i t + \mathbf{L}_i\mathbf{B}(t)$ with covariance $\boldsymbol\Sigma_i=\mathbf{L}_i\mathbf{L}_i^T$. Between inspections, increments follow $\Delta\mathbf{z}_{i,k}\sim\mathcal N(\boldsymbol\mu_i\Delta t_{i,k},\boldsymbol\Sigma_i\Delta t_{i,k})$, while maintenance intervals use a post-tamping latent state $\mathbf{z}_{i,k}^+$ giving $\mathbf{Z}_i(t_{i,k})\sim\mathcal N(\mathbf{z}_{i,k}^+ + \boldsymbol\mu_i\Delta t_{i,k}/2,\boldsymbol\Sigma_i\Delta t_{i,k}/2)$. Covariance is parameterized as $\boldsymbol\Sigma_i=\mathrm{diag}(\boldsymbol\sigma_i)\,\mathbf R_i\,\mathrm{diag}(\boldsymbol\sigma_i)$ with LKJ prior on $\mathbf R_i$, half-normal priors on drifts/standard deviations, and log-normal priors on post-maintenance $z_{q,i,k}^+$.","On a held-out validation using the last three inspections, the multivariate model places 88.9% of test points within the 95% credible interval, with 5.3% above the interval (missed defects), versus 63.8% within and 8% above for the univariate independent-Wiener model. Posterior correlation estimates show strong correlation between the two Top indicators (reported around $r\approx0.8$) and moderate correlation between the two Alignment indicators (around $r\approx0.6$), with weak cross-correlation between vertical (Top) and horizontal (Alignment) indicators (around 0–0.2). Hierarchical Bayes substantially tightens parameter uncertainty relative to individual-segment Bayes (e.g., drift credible interval about $[0,0.01]$ vs $[0,0.118]$; and $\sigma$ about $[0.032,0.558]$ vs $[0.03,0.832]$). First-hitting-time simulations indicate univariate models reach thresholds sooner (more conservative) because they ignore indicator correlation; Top indicators trigger threshold crossings in roughly >80% of cases.","The model currently includes only a subset of geometry indicators (primarily longitudinal level/Top and alignment) and does not yet include other indicators such as gauge, cant, and twist. It also does not incorporate external/causal covariates (e.g., speed, tonnage, soil conditions, weather), noting that these would require additional data. The authors note practical data issues: tamping times/locations are not recorded with sufficient specificity, so tamping is inferred from simultaneous drops in indicators and may contain errors.","The degradation dynamics assume Wiener increments with (segment-specific) constant drift and covariance between tamping events; this may be too restrictive if degradation rates change with seasonality, traffic regimes, or evolving ballast conditions. Maintenance is represented via a latent post-tamping state and an assumed within-interval timing (midpoint), which can bias inference when tamping occurs near interval ends or when multiple interventions occur. The condition-based inspection policy is demonstrated without an explicit economic/risk objective (costs not modeled) and appears tailored to a single line; generalization would benefit from multi-line benchmarking and sensitivity analysis to thresholds and prior/hyperprior choices. No implementation package/reproducible code is provided, which limits replicability and practical uptake.","The authors propose extending the model to include additional track geometry indicators (e.g., gauge, cant, twist). They suggest studying and incorporating environmental and operational factors (speed, tonnage, soil, weather) to explain variability in degradation. They also propose developing optimization models for inspection scheduling, routing, and tamping planning (beyond the demonstrated heuristic risk-based inspection-delay approach) to further improve maintenance efficiency.","Develop a self-starting/online updating version of the hierarchical Bayesian model to support real-time decision-making as new TRV runs arrive, including change-point detection for regime shifts. Extend to a spatiotemporal multivariate model that explicitly links neighboring segments (spatial correlation) rather than pooling only through shared hyperpriors. Provide closed-form or numerically efficient approximations for multivariate first-hitting-time distributions to reduce reliance on Monte Carlo for policy optimization. Release an open-source reference implementation (Stan/Python) with data schema and diagnostics, and evaluate robustness to autocorrelation in inspection errors, missing inspections, and misclassified tamping events.",2308.14240v2,https://arxiv.org/pdf/2308.14240v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:37:56Z
FALSE,NA,ML-based|Other,Other,Not applicable,Network/cybersecurity|Healthcare/medical|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,https://arxiv.org/abs/0706.3188|https://proceedings.mlr.press/v162/fisch22a.html|https://arxiv.org/abs/2107.07511|https://www.rulex.ai|https://www.kaggle.com/datasets/kukuroo3/body-signal-of-smoking?select=smoking.csv|https://www.kaggle.com/datasets/sulianova/cardiovascular-disease-dataset|https://www.kaggle.com/datasets/behrad3d/nasa-cmaps|https://archive.ics.uci.edu/ml/datasets/EEG+Eye+State|https://www.kaggle.com/datasets/abhinand05/magic-gamma-telescope-dataset|https://www.kaggle.com/datasets/deepcontractor/smoke-detection-dataset|https://ec.europa.eu/digital-single-market/en/news/ethics-guidelines-trustworthy-ai|https://www.mdpi.com/1424-8220/20/22/6578,"The paper proposes CONFIDERAI, a new conformal prediction score function tailored to rule-based classifiers to provide distribution-free probabilistic error guarantees while retaining interpretability. The score combines (i) rule predictive quality via a relevance term based on covering and error, (ii) a geometric term measuring how close a point lies to rule boundaries, and (iii) an explicit geometric rule-similarity measure to account for overlaps/adjacency between rules, penalizing overlaps across opposing-class rules more than same-class overlaps. Using inductive conformal prediction with quantile calibration, the method produces prediction sets with controlled marginal coverage and introduces the Conformal Critical Set (CCS): regions of feature space where only the target (critical) class is included in the conformal set. Experiments using the Logic Learning Machine (LLM) on 10 real datasets (e.g., DNS tunneling detection, cardiovascular disease, vehicle platooning, RUL, IoT cybersecurity) show conformal error tracks the chosen risk levels and that retraining on CCS-derived labels yields rules with higher precision for the target class. The contribution advances trustworthy/XAI-oriented conformal prediction for transparent-by-design rule models, especially in the presence of overlapping rules.","Prediction set is $C_\varepsilon(x)=\{y: s(x,y)\le s_\varepsilon\}$ where $s_\varepsilon$ is the $(\lceil(n_c+1)(1-\varepsilon)\rceil/n_c)$ quantile of calibration scores, yielding marginal coverage in Eq. (2). The Conformal Critical Set is $S_\varepsilon=\{x: s(x,+1)\le s_\varepsilon,\ s(x,0)>s_\varepsilon\}$. The CONFIDERAI score for label $y$ is $s(x,y)=\prod_{r_k\in R_x^y} \hat\tau(x,r_k)\,(1-R(r_k))$, where $\hat\tau$ is a sigmoid of a boundary-distance term scaled by a geometric overlap ratio based on rule-similarity $q(r_k,r_z)=\frac{V_{overlap}}{V_{H_{r_k}}+V_{H_{r_z}}-V_{overlap}}$.","Across 10 datasets and risk levels $\varepsilon\in\{0.01,0.05,0.1,0.2\}$, the average conformal error (AvgErr) stays bounded by and increases with $\varepsilon$ (Table 1), with very low errors for simpler/high-accuracy tasks (e.g., P2P AvgErr=0 at $\varepsilon=0.01$; Fire Alarm AvgErr=0 at $\varepsilon=0.01$). Prediction-set composition behaves as expected: lower $\varepsilon$ yields more doubletons, while higher $\varepsilon$ increases singleton and empty sets (illustrated for CHD in Fig. 2). Score computation on calibration sets of size $n_c=10000$ takes on average $196\pm 73$ seconds on the reported hardware, varying with feature dimension $D$ and number of rules $M_r$. Using CCS relabeling at $\varepsilon=0.05$ and retraining LLM, the resulting CCS-rules achieve high precision (PPV) in most datasets (often \u22650.80), e.g., MQTTset TPR=0.89, PPV=0.92, F1=0.90; Fire Alarm TPR=0.88, PPV=0.98, F1=0.93 (Table 2).",The authors note the work is an initial step toward a fully conformal rule-based methodology for trustworthy AI and indicate the need for broader experimentation on other rule-based models and more real-world assessments. They also state that extending the score function to multi-class problems is future work and that further formalization is needed for the transition from original rules to those derived from conformal critical sets.,"The study targets AI trustworthiness rather than engineering reliability (lifetime/failure/degradation) and does not connect conformal guarantees to classical reliability metrics (hazard, MTBF, RUL uncertainty bounds) or decision-theoretic maintenance consequences. Computational cost is nontrivial (\~196 s per 10k calibration points) and may be burdensome for large-scale/real-time monitoring without optimization, yet scalability strategies are not developed. The approach depends on a specific rule learner (LLM) with overlap-capable rules; performance and CCS interpretability may change for other rule induction methods, and comparisons to alternative CP score designs for rules are limited. Assumptions underlying conformal validity (exchangeability/i.i.d.) may be violated in key applications mentioned (network traffic, sensor time series, RUL trajectories), but robustness to temporal dependence/concept drift is not analyzed.","They propose more in-depth experimentation on additional rule-based models and assessment on real-world applications, extending the score function beyond binary to multi-class problems, and further formalizing the methodology for transitioning from the original ruleset to the rules obtained via conformal critical sets.","Evaluate validity and efficiency under distribution shift and temporal dependence (e.g., covariate drift in cybersecurity traffic, non-i.i.d. RUL sequences) using conformal methods designed for time series or covariate shift. Provide algorithmic/engineering improvements for runtime (vectorization, pruning of overlap computations, approximate neighbor/overlap indexing) and release a reference implementation for reproducibility. Extend CCS-based retraining with cost-sensitive objectives (explicit FP/FN costs) and perform head-to-head comparisons against alternative uncertainty methods for interpretable models (e.g., Bayesian rule lists, conformalized probabilistic rules). For reliability-adjacent uses (e.g., RUL/fault prediction), integrate CCS with decision rules for alarms/maintenance and report operational metrics (false alarm rate, detection delay, expected cost) rather than only PPV/TPR/F1.",2309.01778v3,https://arxiv.org/pdf/2309.01778v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:38:49Z
FALSE,NA,ML-based|Nonparametric/Semi-parametric|Other,Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/xzhan96-stf/icp_train_clean,"This paper proposes a reliability-based training-label cleaning method for supervised biomedical classification with noisy labels, using inductive conformal prediction (ICP) to quantify label conformity (via calibrated P-values) relative to a small, well-curated calibration subset. A shrunken-centroids-based conformal predictor (CPSC) provides nonconformity scores; samples in the (potentially noisy) proper training set are then flagged as mislabeled when an alternative label has a substantially higher P-value, and flagged as outliers when all labels have low P-values, leading to label flips or removals. The approach is evaluated on three real multi-modal biomedical tasks (DILI literature classification from text embeddings, COVID-19 ICU admission prediction using CT radiomics + EHR, and TCGA breast-cancer subtype classification from RNA-seq) with additional controlled label-noise created by label permutation. Across most noisy-label scenarios, cleaning significantly improves downstream LR/LDA performance (e.g., up to +11.4% accuracy on DILI, up to +23.8% AUROC and +69.8% AUPRC on COVID-19, and large gains in TCGA accuracy/F1 under heavy noise). The paper positions the contribution as “training data trimming/repair” complementary to prior conformal-prediction-based pseudo-label filtering for unlabeled-data augmentation.","ICP P-values are computed using calibration nonconformity scores with Laplace smoothing: $p_i^y = (|\{j\in\text{cal}: \alpha_j^y \ge \alpha_i^y\}|+1)/( |\text{cal}|+1 )$. The CPSC nonconformity measure is derived from shrunken-centroid class scores: compute shrunken contrasts $d'_m=\mathrm{sign}(d_m)(|d_m|-\Delta)_+$, form shrunken centroids, compute discriminant scores $\delta_m(x^*)$, convert to class probabilities via tempered softmax $\hat p(k|x^*)=\exp(\delta_k/T)/\sum_\ell \exp(\delta_\ell/T)$, then set nonconformity as $\alpha_{y_i}(x_j)=0.5-(\hat p(y_i|x_j)-\max_{y\neq y_i}\hat p(y|x_j))/2$. Cleaning rules use P-values: flip a label if another label’s P-value exceeds the original by a chosen margin; remove a sample as an outlier if all label P-values are < 0.1.","With simulated label noise (0–80% label permutation on the proper training set), the cleaning method improves performance in most scenarios: DILI accuracy improves in 86/96 experiments (max absolute gain 0.8120→0.9048, +11.4% with S2V+LDA at 80% permutation); COVID-19 AUROC and AUPRC improve in all 48 noisy-label experiments (max AUROC gain 0.5967→0.7389, +23.8%; max AUPRC gain 0.1829→0.3106, +69.8%, both with LDA and low detection threshold). For TCGA breast-cancer subtyping, accuracy and macro-F1 improve in 47/48 noisy-label experiments (max accuracy gain 0.3508→0.6128, +74.6%; max macro-F1 gain 0.2672→0.5049, +89.0%). In the no-permutation setting, gains are limited for the well-curated DILI and TCGA tasks, but COVID-19 shows improvements even at 0% permutation (attributed to SMOTE and real-world noise).","The authors note that conformal prediction is more commonly used for classification reliability and it is unknown how well reliability-based training-data cleaning would extend to regression tasks. They only evaluate one ICP base method (CPSC with shrunken centroids), so whether other conformal predictors (e.g., kNN, SVM, gradient boosting, neural nets) would yield better cleaning remains untested. They also caution that their datasets are relatively large (hundreds to thousands of samples), and performance on small-sample settings (where splitting into proper training vs calibration may be too data-hungry) needs further investigation.","The method relies on an i.i.d. assumption between calibration and proper-training data; in biomedical multi-site settings with dataset shift, calibration may not represent the noisy portion, potentially biasing P-values and cleaning decisions. Cleaning decisions depend on several thresholds (label-margin threshold and outlier P-value cutoff) and hyperparameter tuning choices; the paper does not provide a principled way to set these in practice without a trustworthy validation set. Evaluation focuses on LR/LDA downstream models; it is unclear how cleaning interacts with modern high-capacity models (e.g., deep nets) and whether it could remove hard-but-correct samples, harming calibration or subgroup performance. Outlier removal is simplistic (drop samples) and may disproportionately remove minority/subtype cases; fairness/representativeness impacts are not assessed.","The authors suggest extending the idea to regression tasks to test whether reliability-based training data cleaning can improve regression modeling. They also propose testing the cleaning framework with other inductive conformal prediction mechanisms beyond CPSC to see if alternative base algorithms perform better. They further indicate the need to study performance on smaller datasets, including possibly abandoning the inductive split (training/calibration separation) at the cost of higher overfitting risk when data are limited.","Develop a self-starting or adaptive thresholding strategy (e.g., controlling expected false correction rate) so practitioners can set detection/outlier cutoffs with statistical guarantees rather than ad hoc margins. Extend the approach to explicitly handle domain shift (e.g., covariate-shift-aware conformal calibration, stratified calibration by site) since biomedical data are often non-i.i.d. Evaluate integration with robust-learning/noisy-label techniques (e.g., co-teaching, bootstrap loss) and quantify complementary benefits, especially for deep models. Provide an open-source reproducible pipeline (data preprocessing + experiments) and benchmark on additional noisy-label datasets with known noise mechanisms to assess generalizability.",2309.07332v1,https://arxiv.org/pdf/2309.07332v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:39:57Z
FALSE,NA,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|ML-based|Other",Simulated only,Not applicable,Healthcare/medical|Finance/economics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/BavoDC/PaperGeneralizedCalibrationCurves/EmpiricalDemonstration.R|https://cran.r-project.org/package=CalibrationCurves,"This paper extends the standard (logistic) calibration framework—primarily used for binary outcomes—to a generalized calibration framework covering all exponential-family response distributions. It defines a generalized calibration curve that maps predictions to the empirical conditional mean and proposes two estimation approaches: a generalized linear model (GLM)-based calibration model and a flexible nonparametric smoother (e.g., loess). The authors introduce two summary measures: a generalized calibration slope (to diagnose over- vs underfitting) and a generalized calibration intercept/calibration-in-the-large (to assess agreement of global observed and predicted averages). Using simulated Poisson count data, the paper demonstrates how the framework distinguishes well-calibrated, overfit, and underfit models, including both GLM/GAM models and gradient boosting machines. The work’s practical contribution includes an R implementation via the CalibrationCurves package and a reproducible script.","The generalized calibration curve targets perfect calibration when $\mathbb{E}(Y\mid \hat\mu)=\hat\mu$. A GLM-based calibration model is proposed as $g(\mathbb{E}(Y^*\mid \hat\mu^*))=\alpha+\zeta\, g(\hat\mu^*)$, where $g(\cdot)$ is the link function; perfect calibration corresponds to $\alpha=0$ and $\zeta=1$. Calibration-in-the-large is estimated by fixing slope to 1 via an offset: $g(\mathbb{E}(Y^*\mid \hat\mu^*))=\alpha_c+\mathrm{offset}(g(\hat\mu^*))$.","In simulated Poisson examples, the correctly specified GLM exhibits near-ideal calibration with estimated intercept approximately 0 and slope approximately 1, while misspecified models show characteristic patterns: overfit models have $\hat\zeta<1$ (predictions too extreme) and underfit models have $\hat\zeta>1$ (predictions not extreme enough). The flexible (loess) calibration curve can deviate in regions with sparse predicted values, with widened pointwise confidence intervals, illustrating sampling-variability effects even when using the true data-generating mean. For gradient boosting, the cross-validated (tuned) configuration is closest to ideal, whereas an overly complex configuration shows overfitting ($\hat\zeta<1$) and an overly simple configuration shows underfitting ($\hat\zeta>1$) and a compressed prediction range.","The authors note that calibration assessment is affected by the validation dataset and sampling variability, so perfect calibration may be unattainable in practice. They also state that flexible smoothers such as loess can behave erratically in regions with sparse observations (wide uncertainty and curve pulled toward few points). They suggest this issue may make flexible calibration curves less suitable when the response distribution is highly skewed.","The empirical demonstration is entirely simulation-based (primarily Poisson), so performance and practical guidance for other exponential-family outcomes (e.g., Gamma, inverse Gaussian, binomial with varying denominators) and real datasets is not established. The framework assumes an appropriate link function and focuses on mean calibration; it does not address calibration of the full predictive distribution (e.g., dispersion/variance miscalibration) or dependence/autocorrelation in observations. Comparative evaluation of different smoothing/GLM choices (bandwidth, spline basis, penalization) is not systematically benchmarked, and guidance on selecting the smoother/complexity is limited.","The authors propose future research to determine which method is best suited to estimate the calibration curve, to study drawbacks of the different estimation approaches, and to investigate how the estimation method interacts with the response distribution. They also highlight the need to understand when flexible calibration curves are less appropriate (e.g., highly skewed outcomes and sparse regions) due to potential erratic behavior of nonparametric smoothers.","Extending the framework to assess distributional calibration beyond the conditional mean (e.g., variance/dispersion calibration, prediction intervals) would broaden applicability for exponential-family models with nontrivial dispersion. Developing robust/self-starting procedures for small validation samples, heavy-tailed outcomes, or clustered/longitudinal data (accounting for dependence) would improve practical reliability. A systematic benchmarking study on real datasets across multiple exponential-family outcomes and ML methods, including guidance for choosing smoothing parameters and uncertainty quantification, would strengthen adoption; packaging standardized diagnostics and defaults in software would further aid practitioners.",2309.08559v1,https://arxiv.org/pdf/2309.08559v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:40:35Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical|Other|Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.dcc.fc.up.pt/~ltorgo/Regression/DataSets.html|https://www.sfu.ca/~ssurjano/index.html|https://scikit-learn.org/1.0/modules/generated/sklearn.datasets.load_boston.html|https://www.cs.toronto.edu/~delve/data/datasets.html|https://openreview.net/forum?id=HJlQfnCqKX|https://openreview.net/forum?id=WvOGCEAQhxl|https://openreview.net/forum?id=eGLdVRvvfQ|https://openreview.net/forum?id=lid14UkLPd4|https://proceedings.mlr.press/v162/seedat22a.html|https://openreview.net/forum?id=0uRm1YmFTu|https://openreview.net/forum?id=j0J9upqN5va,"This paper proposes PAGER, a framework to detect and characterize failures in deep regression models by organizing test samples into risk regimes rather than requiring a fixed failure threshold. PAGER combines epistemic uncertainty from anchored training (forward anchoring) with a complementary manifold non-conformity (MNC) score computed via reverse anchoring that evaluates how well a test input can reconstruct targets of training anchors. Two MNC variants are introduced: Score1 (max target discrepancy over anchors) and Score2 (an optimization-based score that moves the input toward an in-manifold surrogate using an anchored autoencoder regularizer), with Score2 improving separation between moderate- and high-risk regimes under shifts/corruptions. The framework bins uncertainty and MNC into quantiles to produce four regimes (ID, Low, Moderate, High) and proposes evaluation metrics (FN, FP, Clow, Chigh) to assess regime quality. Experiments across 1D synthetic functions, tabular benchmarks up to 32k dimensions, and image regression tasks show PAGER aligns regimes better with true risk and reduces false negatives/positives compared to baselines like DEUP and DataSUITE.","Predictive mean/uncertainty are estimated by marginalizing anchored predictions over K training anchors: $\mu(y_t|x_t)=\frac{1}{K}\sum_{k=1}^K F([r_k, x_t-r_k])$ and $\sigma(y_t|x_t)=\sqrt{\frac{1}{K-1}\sum_{k=1}^K (F([r_k,x_t-r_k]) - \mu)^2}$. The reverse-anchoring manifold non-conformity Score1 is $\text{Score1}(x)=\max_{r\in D}\|y_r - F([x, r-x])\|_1$. Score2 refines this by solving an optimization to find an in-manifold proxy $\bar{x}$ via $\arg\min_{\bar{x}}\|y_r - F([\bar{x}, r-\bar{x}])\|_1 + \lambda R(\bar{x})$, then setting $\text{Score2}(x)=\max_{r\in D}\|x-\bar{x}\|_2$ with a cyclic-consistency regularizer $R(\bar{x})$ defined using an anchored autoencoder.","Across 1D benchmark functions, PAGER (Score1/Score2) reports lower FN/FP and lower confusion (Clow/Chigh) than DEUP and DataSUITE (e.g., Table 1 shows FN as low as 0.0 for some functions with PAGER Score1, and FP as low as 0.0–4.67 depending on function/setting). On high-dimensional regression benchmarks (2D to 32,000D) under “Gaps” and “Tails” shifts, PAGER often yields substantial reductions in FN and FP versus baselines and markedly reduces overlap between neighboring regimes (lower Clow and Chigh in Tables 2–3). On image regression tasks (chairs yaw, cell count, CIFAR-10 rotation), PAGER achieves lower FN/FP and confusion metrics than DEUP under extrapolation regimes; Score2 further improves robustness under image corruptions (defocus blur, frost). Anchored training does not degrade predictive accuracy and slightly improves $R^2$ in unobserved regimes (e.g., CIFAR-10 unobserved $R^2$: 0.81 anchoring vs 0.77 standard; Chairs: 0.75 vs 0.73; Cells: 0.72 vs 0.69).",None stated.,"The work targets ML failure detection rather than engineering reliability; links to classical reliability quantities (failure rate/hazard, survival/RUL) and decision-making (maintenance actions, cost tradeoffs) are not developed. The risk-regime thresholds are set via quantile binning on the test set, which may be unstable across deployments and can make regime definitions dependent on the current batch composition. Score2 requires iterative optimization per test point and a trained anchored autoencoder, which can be computationally heavy and harder to operationalize for real-time monitoring. Evaluation is largely benchmark-driven; broader validation on safety-critical real-world regression pipelines with domain-specific failure costs and temporal dependence is not established.",None stated.,"Extend PAGER-style failure characterization to settings with temporal correlation/streaming data (self-starting or online updates) and quantify robustness under autocorrelation and covariate drift. Develop principled, application-driven regime thresholds that incorporate cost/utility (decision-theoretic design) rather than fixed quantiles, and connect regimes to downstream actions (e.g., abstain, human review). Provide calibrated guarantees (e.g., conformal-like coverage) that remain valid under support shift, possibly via weighted/shift-aware calibration or domain adaptation. Release a reference implementation and standardized evaluation suite to facilitate reproducibility and fair comparisons across failure detectors for regression.",2309.10977v2,https://arxiv.org/pdf/2309.10977v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:41:25Z
TRUE,System reliability|Network/infrastructure reliability|Other,Simulation-based|Stochastic process|Other,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"This paper develops an efficient simulation framework for seismic reliability and fragility assessment of lifeline networks, focusing on rare-event estimation for complex, large-scale networks with spatially correlated ground motions. The key methodological contribution is reformulating the usual binary network connectivity limit-state into two piecewise-continuous, informative limit-state functions based on (i) the most reliable path (RP) and (ii) the shortest path (SP), enabling effective subset simulation (SS) progression via ranked intermediate events. The authors further propose a specialized SS interpretation in which intermediate failure events are linked to earthquake moment magnitude, allowing the entire network fragility curve to be generated from a single SS run (with an optional divide-and-conquer strategy to reduce accumulated bias). Performance is demonstrated via Monte Carlo experiments on a two-component parallel system and two real transportation networks (San Jose and San Diego bridge networks) for two-terminal, k-terminal, and k-out-of-N reliability. Results show RP-based SS is generally more accurate (lower c.o.v.) while SP-based SS is faster due to BFS-based path computation, and the one-shot fragility approach can substantially reduce total limit-state evaluations versus repeated runs over magnitudes.","Ground motion model: $\ln D_i=f(M,R_i,\lambda_i)+\eta+\epsilon_i$ with spatial correlation $\rho_{\ln D_i\ln D_j}(\Delta_{ij})=(\sigma_\eta^2+\rho_{\epsilon_i\epsilon_j}(\Delta_{ij})\sigma_\epsilon^2)/(\sigma_\eta^2+\sigma_\epsilon^2)$. Component failure indicator $B_i=\mathbb{I}(C_i\le D_i)=\mathbb{I}(z_i\le 0)$ where $z_i=\ln C_i-\ln D_i$, giving $P_i=\Phi(-\beta_i)$. Proposed informative network limit states for an OD pair: $G^{RP}_{OD}(z)=\min_{i\in RP} z_i/n_{RP}$ if connected else 0, and $G^{SP}_{OD}(z)=\min_{i\in SP} z_i/n_{SP}$ if connected else 0; SS uses nested events and $\hat P_f=\prod_k P(F_k\mid F_{k-1})$. Fragility ‘specialized SS’ defines intermediate domains across magnitudes via $F_k=\{G(z(M_w^{(1)})+(k-1)\Delta z)\le 0\}$, enabling one run to recover multiple $P_f(M_w)$ points.","Across examples, HMC-based subset simulation with the proposed limit-state reformulations matches reference probabilities closely (exact integration for the 2-component case; crude MCS for larger networks) while maintaining feasible computation for rare events. For the 2-component parallel system, RP- and SP-based HMC-SS estimates agree with exact $P_f$ over $M_w\in\{3,4,5,6,7\}$, with RP showing lower c.o.v. (e.g., at $M_w=5$, RP c.o.v. 0.196 vs SP 0.345) but higher runtime per run (e.g., at $M_w=5$, 0.635s vs 0.136s). A single specialized HMC-SS run produced a full fragility curve for $3\le M_w\le 9$ (step 0.5) using 12,700 limit-state evaluations, about 37.47% of the evaluations required by running HMC-SS separately at each magnitude. For San Jose (4-terminal) and San Diego (3-out-of-5) bridge networks, RP again tends to reduce estimator variability while SP is faster, and a divide-and-conquer specialized SS reduces low-magnitude underestimation from accumulated sequential bias with modest extra samples (about 9.32% more than a single run in the San Jose example).","The authors note a trade-off between the two reformulations: RP-based limits can be more accurate but are computationally expensive due to Dijkstra’s algorithm, while SP-based limits are cheaper but can increase estimator variance due to discontinuities. They also state that accumulated biases/errors from sequential conditioning can be conspicuous for k-terminal and k-out-of-N reliability in large-scale networks, motivating interval division (“divide-and-conquer”) for fragility estimation. They acknowledge that extending the framework to more realistic indices like network flow capacity remains a remaining task.","The RP identification assumes independent component failures (used only to find the RP), which may bias the informativeness of the limit-state ranking under strong spatial correlation and could degrade SS efficiency in some regimes. The proposed limit-state functions are piecewise and can be discontinuous where the selected path changes, which can challenge MCMC mixing and may require careful tuning/diagnostics not fully explored. Validation is primarily simulation-based; while the networks are real, there is no empirical earthquake damage dataset calibration/validation of fragility results, and parameter uncertainty (e.g., GMPE, capacity medians/dispersion) is not propagated. The specialized SS relies on linear mean-shift structure in $z(M_w)$ and fixed covariance across magnitudes; changes in correlation structure with event characteristics or alternative IM choices could reduce applicability.","They propose developing informative limit-state functions tailored to specific network reliability indices rather than relying on path-vulnerability surrogates. They identify extending the framework to more realistic network reliability measures such as network flow capacity, potentially via multi-state/continuous component models or incorporating OD flow capacity within the k-out-of-N formulation. They also suggest probabilistic inference extensions such as sensitivity analysis or component importance measures.","A natural extension is to incorporate epistemic uncertainty (GMPE parameters, capacity models, correlation models) via Bayesian hierarchical modeling and to quantify its impact on network fragility curves. Robust/self-starting variants that handle unknown or learned model parameters (Phase I/II-like separation) and improved discontinuous-limit-state MCMC kernels could enhance practical stability. Extending to multivariate IMs (e.g., vector-valued Sa and epsilon) and to time-dependent performance (post-event restoration, cascading interdependencies) would broaden resilience applications. Public release of MATLAB implementations (HMC-SS + path-based limit-states) as reproducible code would enable benchmarking against alternative rare-event methods (cross-entropy IS, splitting, SMC) on standardized network datasets.",2310.10232v1,https://arxiv.org/pdf/2310.10232v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:42:17Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling,"Stochastic process|Parametric (Weibull, etc.)|Simulation-based|Other",Degradation measurements|Simulated only|Mixture of types,Not applicable,Semiconductor/electronics|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/dirge1/FBM_ADT,"The paper proposes a constant-stress accelerated degradation testing (ADT) model that captures non-Markovian “memory effects” in degradation via fractional Brownian motion (FBM) and incorporates unit-to-unit variability (UtUV) by modeling the degradation rate as a random effect in the acceleration relationship. External stress effects are modeled through a log-linear acceleration model with standardized stress, with Arrhenius/power-law/exponential forms for stress normalization depending on the stress type. Reliability and lifetime under normal operating conditions are obtained via first-passage failure definition and are evaluated using Monte Carlo simulation (including FFT-based FBM path generation), avoiding restrictions of existing approximate lifetime distributions (e.g., those limited to H>0.5). For inference, an EM-algorithm-based statistical analysis method is developed to maximize the overall likelihood in the presence of latent unit-specific random effects, and nonparametric bootstrap is used to construct confidence intervals for parameters and reliability. Simulation studies and a tuner case study show that the EM-based method estimates the Hurst exponent more accurately than a two-step MLE, and that ignoring UtUV can substantially bias memory-effect and reliability estimates; the proposed model also improves degradation trend prediction and uncertainty quantification compared with several reduced models.","The degradation under stress is modeled as $X(s,t)=e(s)\,\tau(t)+\sigma B_H(t)$ with $\tau(t)=t^\beta$ and $B_H(t)$ an FBM with Hurst exponent $H$ controlling memory (negative correlation if $H<0.5$, Brownian motion if $H=0.5$, positive correlation if $H>0.5$). The acceleration model is log-linear $e(s)=\exp(\alpha_0+\alpha_1 s^*)$, with standardized stress $s^*$ defined by Arrhenius/power-law/exponential normalizations; UtUV is introduced by letting the item-specific rate parameter be random (e.g., $a=\exp(\alpha_0)$ treated as a normal random effect), inducing a stress-dependent distribution for $e(s)$. Failure time is defined by first passage across a threshold: performance margin $M(s,t)=X_{th}-X(s,t)$ and $T=\inf\{t:M(s,t)<0\}$, with reliability $R(s,t)=\Pr(T\ge t)$; reliability is computed by Monte Carlo simulation of degradation paths.","In the simulation study with short-term dependence (true $H=0.1$), the EM method produced $\hat H=0.117$ with a 90% bootstrap CI $(0.012,0.181)$ and showed markedly lower relative error than the two-step MLE, especially when the number of measurements was small (two-step MLE’s $H$ bias reported near 100% in low-measurement settings). The authors report that ignoring UtUV can lead to highly biased estimation of the memory effect and reliability. Model comparisons using log-likelihood/AIC in simulation favored the full model (M0) over reduced models that omit UtUV and/or memory effects (e.g., in one setting AIC: M0 -1611.65 vs M1 -920.15, M2 -1200.96, M3 -853.79). In the tuner case, estimated $H$ was near zero ($\hat H=2.497\times 10^{-8}$; 90% CI $(1.307\times 10^{-9},0.009)$), indicating short-term dependence, and M0 achieved the best fit among compared models (AIC -1146.888 vs -1093.568, -948.696, -950.966).",The authors state that they focus only on constant-stress ADT data in this work. They note that extending the approach to step-stress ADT while accounting for memory effects is an additional topic needing investigation.,"Reliability is computed via Monte Carlo simulation rather than a closed-form lifetime distribution, which can be computationally heavy in practice and introduces Monte Carlo error that must be managed (e.g., via large N) for stable reliability quantiles. The core model assumes a specific deterministic trend structure (separable $e(s)\tau(t)$ with $\tau(t)=t^\beta$) and a normal random-effect structure for unit variability; misspecification (e.g., alternative trend forms or non-normal heterogeneity) could affect inference and extrapolation. The framework appears to assume independence across units and does not address within-unit measurement error in the main model (though discussed in related literature), which may be important in real ADT data. The EM step requires numerical optimization (three-dimensional search) and inversion of covariance matrices tied to FBM, which may scale poorly with dense sampling schedules or large datasets.",They explicitly propose extending the modeling framework from constant-stress ADT to step-stress ADT data while still accounting for memory effects (non-Markovian behavior).,"Developing faster and more scalable inference (e.g., exploiting structure in FBM covariance for efficient likelihood evaluation, or variational/Bayesian implementations) would improve usability for large ADT datasets. Robust or semi-parametric extensions that relax the assumed $t^\beta$ time-scaling and normal random effects (including heavy-tailed or mixture heterogeneity) could improve generalizability. Incorporating explicit measurement error and/or autocorrelated sensor noise alongside FBM-driven degradation would better match many condition-monitoring settings. Extending the method to multivariate degradation and to optimal ADT test planning/design under the proposed non-Markovian + UtUV model would broaden practical impact.",2310.18567v5,https://arxiv.org/pdf/2310.18567v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:43:04Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical|Other,Other,TRUE,R,Not provided,NA,"El artículo estima/predice la prevalencia distrital de anemia infantil y el riesgo de retraso en el crecimiento en Perú cuando las encuestas (ENDES 2019) tienen tamaños muestrales pequeños o distritos no muestreados, combinando información auxiliar del Censo 2017. Emplea modelos de estimación en áreas pequeñas tipo Fay–Herriot (FH) y su extensión espacial con efectos aleatorios correlacionados (SAR), produciendo predictores EBLUP y SEBLUP (SBLUP empírico) para cada distrito. Los parámetros se estiman principalmente por REML y se evalúa la precisión mediante un enfoque de bootstrap paramétrico (según Molina et al., 2009). Reporta autocorrelación espacial positiva significativa (parámetro ρ) y menor varianza residual en el modelo espacial frente al FH básico, recomendando el FH espacial para predicción en distritos con poca o nula información. El trabajo es de estadística aplicada/salud pública (estimación de dominios pequeños), no de ingeniería de confiabilidad ni de modelado de fallas de sistemas.","El modelo FH básico se define como $Y_i=\theta_i+e_i$, $\theta_i=X_i\beta+u_i$, con $e_i\sim N(0,\sigma_i^2)$ y $u_i\sim N(0,\sigma_u^2)$. El predictor BLUP/EBLUP toma la forma de combinación convexa: $\hat\theta_i=\gamma_i Y_i+(1-\gamma_i)X_i\hat\beta$, con $\gamma_i=\sigma_u^2/(\sigma_i^2+\sigma_u^2)$ (reemplazando $\sigma_u^2$ por $\hat\sigma_u^2$ en EBLUP). En la extensión espacial: $Y=X\beta+u+e$, $u=\rho Wu+\varepsilon$ (SAR), y el predictor SEBLUP se obtiene con $\hat\beta=(X^T G^{-1}X)^{-1}X^T G^{-1}Y$ y ajuste por $\Omega G^{-1}(Y-X\hat\beta)$, donde $G$ depende de $(\sigma_\varepsilon^2,\rho)$.","El modelo FH espacial produce parámetros autorregresivos positivos en todos los estratos: para anemia $\hat\rho\in\{0.570,0.694,0.742\}$ (estratos 1–3) y para retraso del crecimiento $\hat\rho\in\{0.474,0.235,0.317\}$, indicando dependencia espacial significativa. Las varianzas residuales/efectos aleatorios estimadas son menores en el modelo espacial (p.ej., anemia estrato 3: $\hat\sigma_u^2=0.00812$ vs $\hat\sigma_\varepsilon^2=0.00401$), y el texto afirma que el FH espacial reduce autocorrelación en residuos frente al FH básico. En los gráficos y tablas, EBLUP y SEBLUP suavizan notablemente la variabilidad respecto al estimador directo; para algunos distritos no muestreados las diferencias EBLUP vs SEBLUP son “considerables”. Variables asociadas a anemia incluyen Altitud (+), Internet (−) y Castellano (−) en todos los estratos; para retraso del crecimiento destacan Analfabetismo (+) y Refrig. (−) en todos los estratos, además de otras covariables según estrato.",None stated.,"El trabajo depende de supuestos fuertes de normalidad y linealidad a nivel de área (FH) y de una estructura SAR con una matriz de vecindad $W$ elegida ad hoc; los resultados pueden ser sensibles a la especificación de $W$ y a la calidad de las varianzas de muestreo $\sigma_i^2$. No se reporta una validación predictiva sistemática (p.ej., cross-validation o comparación fuera de muestra) ni métricas agregadas de desempeño (MSE/MAE) más allá de argumentos de suavizamiento y varianza residual. La inferencia/interpretación causal de covariables (p.ej., Internet, SIS) puede ser limitada por confusión ecológica a nivel distrital.",None stated.,"Extender a versiones robustas a no-normalidad/outliers y a modelos que manejen explícitamente tasas/proporciones (p.ej., modelos binomiales/beta-binomial a nivel de área) en lugar de aproximación normal. Incorporar validación predictiva formal y comparar con alternativas espaciales (CAR/BYM, modelos bayesianos jerárquicos) y con diferentes construcciones de $W$ de manera más exhaustiva. Publicar código reproducible (paquete o repositorio) y detallar completamente el procedimiento de bootstrap/MSE usado en la implementación.",2311.04812v1,https://arxiv.org/pdf/2311.04812v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:43:41Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Case study (real dataset),TRUE,None / Not applicable,Not provided,https://openreview.net/forum?id=0Xo9giEZWf,"This paper proposes HypUC, a framework for reliable probabilistic regression on imbalanced medical time series (12‑lead ECGs), focusing on producing calibrated uncertainty estimates alongside continuous-value predictions. The method combines (1) kernel density estimation (KDE) label-density weighting to mitigate target imbalance, (2) heteroscedastic Gaussian probabilistic regression for aleatoric uncertainty, (3) a new post-hoc “hyperfine” uncertainty calibration that learns bin-wise scaling factors over predicted-value bins, and (4) gradient-boosted decision-tree corrections that use the calibrated uncertainty and point estimate to improve downstream clinical range/class decisions. HypUC is evaluated on a very large real-world Mayo Clinic ECG dataset (millions of patients; millions of ECGs) for tasks including age, survival time-to-death, serum potassium (hyperkalemia severity), and left-ventricular ejection fraction, showing improved error metrics and substantially better uncertainty calibration versus deterministic regressors and standard probabilistic baselines. The paper also demonstrates entropy-based filtering using calibrated predictive distributions to flag unreliable predictions for human review, improving performance when high-entropy cases are withheld. Overall, it advances trustworthy medical regression by coupling imbalance handling with fine-grained uncertainty calibration and uncertainty-aware decision support.","HypUC trains a DNN to output a heteroscedastic Gaussian predictive distribution, i.e., $\Psi(x;\theta)=\{\hat y,\hat\sigma\}$, optimizing a loss that combines KDE-based imbalance weighting and Gaussian NLL (Eq. 6). Post-hoc calibration first learns a global scale $s^*$ on validation data (Eq. 7), then learns a bin-wise multiplicative factor $\eta_{B_n}$ per “hyperfine” predicted-value bin to satisfy a desired coverage fraction $\xi$ (Eq. 8), yielding $\hat\sigma_{\mathrm{calib}} = \eta_{\mathrm{Bin}(\hat y)} s^* \hat\sigma$ (Eq. 9). A gradient-boosted ensemble then predicts standardized clinical ranges/classes using features $(\hat y,\hat\sigma_{\mathrm{calib}})$ (Eq. 10), and prediction unreliability can be flagged via Gaussian entropy $\mathcal H=\log\sqrt{2\pi e\hat\sigma_{\mathrm{calib}}^2}$ (Eq. 13).","Across four large ECG regression tasks, HypUC improves both point accuracy and uncertainty calibration versus baselines. For survival estimation, HypUC achieves MSE/MAE 54.62/5.38 with UCE/NLL 0.58/3.45, compared with a standard probabilistic regressor (Regres.-w.-U) at MSE/MAE 84.62/8.13 and UCE/NLL 2.37/5.89. For age estimation, HypUC yields MSE/MAE 74.9/6.70 with UCE/NLL 1.06/3.70, versus Regres.-w.-U at 136.27/8.22 and 13.32/7.84. For serum potassium, HypUC reports UCE/NLL 0.41/3.12 and also improves hyperkalemia classification AUC to 0.89 (binary classifier baseline AUC 0.87). For LVEF, HypUC improves UCE/NLL to 1.68/4.28 (vs 10.56/9.83 for Regres.-w.-U) and yields low-LVEF AUC 0.93 (binary classifier AUC 0.91).","The authors state that while HypUC provides a technical solution, its clinical impact must be explored through real-world validation studies. They also note that trained models must be evaluated for bias and that clinical deployment will require validation/testing on demographically diverse datasets.","The approach assumes a parametric Gaussian predictive distribution; mis-specification (e.g., skewed/heavy-tailed conditional targets) could degrade uncertainty quality even after calibration. The hyperfine bin-wise calibration introduces several design/robustness questions (bin width $\delta$, sparsity in rare bins, stability under dataset shift) that may be challenging in deployment and are not fully stress-tested. The paper describes gradient-boosted “corrections” but does not provide implementation details or ablations sufficient to separate gains from each component across all tasks (KDE weighting vs calibration vs boosted correction), limiting reproducibility and causal attribution of improvements.","They propose exploring real-world clinical validation/prospective studies (notably a prospective study for a hyperkalemia diagnosis algorithm based on HypUC). They also suggest extending HypUC to predict multiple targets in a single forward pass via multi-task learning, and designing clinical protocols to help physicians make decisions in the presence of quantified uncertainty.","Evaluate robustness under temporal and hardware/domain shift (e.g., new ECG devices, sites, and changing prevalence), including recalibration strategies and monitoring for calibration drift. Extend beyond Gaussian outputs to richer predictive families (e.g., mixture densities or quantile-based models) and compare calibration/coverage in extreme ranges. Provide open-source reference implementations and standardized benchmarks/ablations to quantify the marginal value of each module (KDE reweighting, global vs hyperfine calibration, boosted correction) and enable broader adoption.",2311.13821v1,https://arxiv.org/pdf/2311.13821v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:44:21Z
TRUE,Accelerated testing|Reliability growth|Maintenance optimization|System reliability|Other,"Stochastic process|Bayesian|Simulation-based|Parametric (Weibull, etc.)|Other",Event/count data|Sensor/condition monitoring|Right-censored|Mixture of types|Other,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/autonomous-vehicle-collision-reports/|https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/disengagement-reports/|https://idt.conf.sk/index.php?clanok=lectures2019,"The paper develops a Bayesian framework to plan reliability assurance (demonstration) tests for autonomous vehicles using recurrent disengagement events as the reliability-relevant outcome and mileage as exposure. It proposes two planning strategies based on repairable-systems style Poisson process models: a homogeneous Poisson process (HPP) for constant event intensity and a non-homogeneous Poisson process (NHPP) for reliability growth over time, with a Weibull-form baseline intensity/cumulative intensity. Test plans are parameterized by the number of vehicles $n_t$, test duration per vehicle $\tau_t$ (or total vehicle-days $\tau=n_t\tau_t$ under HPP), and an acceptance criterion allowing at most $c$ events. The method computes Bayesian posterior consumer’s risk, producer’s risk, and acceptance probability by integrating Poisson-count probabilities over the posterior distribution of intensity/model parameters learned from historical California DMV AV data (VIN-level disengagement events and mileage). Because multiple objectives conflict, the paper uses Pareto-front optimization to identify non-dominated test plans and provides example decision rules under both HPP and NHPP settings.","Recurrent events are modeled via a mileage-adjusted Poisson process intensity $\lambda_i(t)=\lambda_0(t;\theta)\,g[x_i(t)]$ with $g[x_i(t)]=x_i(t)$, cumulative baseline intensity $\Lambda_0(t;\theta)=\int_0^t\lambda_0(s;\theta)ds$, and cumulative intensity $\Lambda_i(t)=\int_0^t\lambda_0(s;\theta)x_i(s)ds$. Average intensity over an interval is $m(s,t)=\{\Lambda(t)-\Lambda(s)\}/(t-s)$, with HPP giving constant $m$. Bayesian risks are computed as posterior probabilities conditioned on pass/fail, e.g., $\text{PCR}=P(m\ge m_1\mid y\le c)$ and $\text{PPR}=P(m\le m_0\mid y>c)$ where $y\sim\text{Poisson}(m\tau)$ (HPP) or $y\sim\text{Poisson}(n_t m_{\tau_t}\tau_t)$ (NHPP), integrating over the posterior of $m$ or $\theta$; the Weibull reliability growth model uses $\lambda_0(t;\theta)=\theta_1\theta_2\theta_3 t^{\theta_3-1}\exp(-\theta_2 t^{\theta_3})$ and $\Lambda_0(t;\theta)=\theta_1[1-\exp(-\theta_2 t^{\theta_3})]$.","Using CA DMV disengagement and mileage data (Dec 1, 2017–Nov 30, 2019; $\tau_h=730$ days) and posterior sampling (reported $nPost=1001$) to drive planning, the paper illustrates trade-offs among consumer’s risk (CR), producer’s risk (PR), acceptance probability (AP), and test cost (total vehicle-days $\tau$ for HPP or per-vehicle days $\tau_t$ for NHPP). Under an HPP example with requirement levels $m_1=0.016$ and $m_0=0.013$ and CR constrained to $\le 0.086$, the Pareto front contains 51 non-dominated plans (one per integer $c\in[0,50]$); example selections include $(\tau\approx1302, c=21)$ yielding CR $0.086$, PR $0.099$, AP $0.763$, and a lower-budget plan $(\tau\approx965, c=15)$ with CR $0.086$, PR $0.138$, AP $0.710$. Under an NHPP fleet example with $n_t=5$, $\tau_d=730$, CR constrained to $\le 0.13$ (with $m_1=0.0132$, $m_0=0.009$), the Pareto front contains 26 plans ($c\in[0,25]$); example choices include $(\tau_t\approx157, c=11)$ giving CR $0.126$, PR $0.144$, AP $0.703$, and an AP-focused plan $(\tau_t\approx252, c=19)$ giving CR $0.130$, PR $0.060$, AP $0.810$.","The analysis assumes a constant mileage effect form ($g[x_i(t)]=x_i(t)$) and, for some settings, effectively treats mileage per unit as constant over time (e.g., derived daily mileage from monthly mileage by assuming constant daily mileage within a month). The paper also limits illustrated planning to HPP/NHPP Poisson-process models and a specific Weibull-form reliability growth baseline intensity, using a two-year historical window for posterior elicitation. It notes that broader cost structures beyond test duration (e.g., costs associated with consumer/producer risk outcomes) are not incorporated in the current optimization.","Disengagements are treated as Poisson-process recurrent events with (conditional) independent increments; real AV disengagements may exhibit clustering, operational heterogeneity, and covariate effects (route, weather, software versioning) that violate Poisson assumptions. The planning examples rely on a single jurisdiction’s self-reported public dataset and focus mainly on one manufacturer (Waymo), which may limit generalizability to other AV programs, ODDs, or reporting standards. The approach reduces reliability to an “average intensity” metric; this may not align with safety-critical risk measures (severity-weighted events, near-misses, or causal attribution) and may be sensitive to how mileage exposure is modeled. No implementable software artifact is provided, which can hinder reproducibility and adoption despite the algorithmic descriptions.","The authors propose extending the intensity model to regression-type forms such as $\lambda_i[t; x_i(t),\theta]=\lambda_0(t;\theta)\exp(\beta x_i(t))$ to better capture mileage-driven variation across vehicles and over time, and exploring other mileage-effect functions (e.g., forms motivated by an exponential distribution for miles driven per day). They also plan to incorporate broader cost components tied to consumer’s and producer’s risks (e.g., warranty/customer loss vs. unnecessary retesting/redesign) rather than only test-duration cost. Finally, they suggest expanding the historical period by obtaining more disengagement and mileage history beyond the two-year window used.","A valuable extension would be to model unobserved heterogeneity and dependence (frailty/random effects, hierarchical Bayes by vehicle/manufacturer/software version) to avoid overstating certainty from aggregated Poisson assumptions. Robust planning under model misspecification (over-dispersion via negative binomial processes, self-exciting processes, or renewal processes) could improve realism for clustered disengagements. The framework could be expanded to multi-type recurrent events (severity classes, collisions vs disengagements) and to joint modeling with covariates capturing ODD and exposure quality, enabling scenario-conditional assurance tests. Providing an open-source implementation (e.g., an R/Python package) and validating on additional AV datasets or simulation benchmarks would materially improve reproducibility and practical uptake.",2312.00186v1,https://arxiv.org/pdf/2312.00186v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:45:21Z
FALSE,NA,ML-based|Bayesian,Sensor/condition monitoring|Mixture of types,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset),TRUE,Python|Other,Not provided,https://arxiv.org/abs/2312.00794,"The paper proposes a multimodal data-driven (m2d2) prior for Bayesian neural networks to improve uncertainty quantification (predictive reliability) in multimodal clinical classification. The approach constructs a context dataset representing distribution shifts by applying modality-specific transformations to EHR time series and chest X-ray images, then uses Gaussian mean-field variational inference to train a stochastic (Bayesian) multimodal fusion model. Experiments use paired ICU-stay data from MIMIC-IV (clinical time series) and MIMIC-CXR (images) to perform 25-label acute care condition classification with a MedFuse architecture (LSTM + ResNet-34 + fusion head). Reliability is assessed via uncertainty-aware selective prediction using entropy-based rejection thresholds, reporting selective AUROC/AUPRC averaged over thresholds and labels. Results show modest gains over deterministic and standard-prior Bayesian baselines on AUROC/AUPRC and selective metrics, indicating improved uncertainty behavior under constructed shifts.","Training uses a variational inference objective of the form $\mathbb{E}_{q(\Theta)}[\log p(y_D\mid x_D,\Theta)]-\mathrm{KL}(q(\Theta)\|p(\Theta))$, augmented with an uncertainty-regularization term derived from an auxiliary likelihood on context (shifted) inputs that encourages high predictive uncertainty on those points (Appendix Eq. A.16). The selective prediction rule rejects predictions when an uncertainty score (entropy) exceeds a threshold $\tau$: output $p(y\mid x)$ if $s(x)\le\tau$, otherwise output $\perp$ (Eq. 2). The supervised loss for multi-label classification is binary cross-entropy: $-\sum_i [y_i\log \hat y_i+(1-y_i)\log(1-\hat y_i)]$ (Eq. B.17).","On the test set, the proposed Bayesian model with the m2d2 prior achieves AUROC 0.735 (0.728, 0.742) and AUPRC 0.514 (0.504, 0.528), compared with the deterministic baseline’s AUROC 0.726 (0.718, 0.733) and AUPRC 0.503 (0.493, 0.517). Selective prediction improves as well: selective AUROC 0.748 (0.738, 0.760) and selective AUPRC 0.452 (0.441, 0.472) versus deterministic selective AUROC 0.724 (0.715, 0.735) and selective AUPRC 0.439 (0.429, 0.455). Compared with a Bayesian model using a standard prior, m2d2 improves overall AUROC/AUPRC slightly (0.735/0.514 vs 0.729/0.507) and selective AUPRC (0.452 vs 0.448) while matching selective AUROC (both 0.748, within CIs). A context batch size study shows best reported performance at context batch size 64, with degradation at 128 for selective AUPRC (down to 0.401).",None stated.,"The work frames “reliability” as predictive uncertainty quality rather than reliability engineering (failure/degradation/maintenance), so conclusions do not translate directly to engineering reliability metrics. The constructed distribution shifts rely on hand-designed augmentations; improvements under these synthetic shifts may not reflect performance under real clinical deployment shifts (site changes, protocol changes, missingness patterns). Calibration is discussed qualitatively (selective AUPRC can drop when poorly calibrated) but comprehensive calibration metrics (e.g., ECE, reliability diagrams, Brier score) are not central in the main results, limiting interpretability of “reliability” gains. Implementation complexity and computational cost increase versus deterministic models, and no released code limits reproducibility and adoption.","The authors propose evaluating the approach under missing-modality settings, extending to additional tasks such as in-hospital mortality prediction, and testing on other multimodal datasets beyond MIMIC-IV/MIMIC-CXR.","Evaluate robustness and uncertainty under real-world dataset shifts (temporal drift, hospital/site transfer, acquisition protocol changes) and report calibration diagnostics (ECE, calibration curves) alongside selective prediction metrics. Develop self-tuning or adaptive context-set construction that learns clinically plausible shifts rather than relying on fixed augmentations. Extend to settings with partially observed modalities and informative missingness (common in EHR) with principled Bayesian handling of missing data. Provide an open-source implementation and standardized benchmarking to enable fair comparison with other uncertainty methods (deep ensembles, temperature scaling, conformal prediction, evidential methods).",2312.00794v1,https://arxiv.org/pdf/2312.00794v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:45:58Z
TRUE,System reliability|Other,"Parametric (Weibull, etc.)|Stochastic process|ML-based|Bayesian|Simulation-based|Other",Sensor/condition monitoring|Simulated only|Other,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Not provided,https://gitlab.windenergy.dtu.dk/HiperSim/hipersim|http://anemoc.cetmef.developpement-durable.gouv.fr/|https://github.com/cagrell/gp2d/tree/gpNd,"The paper compares probabilistic structural reliability methods for ultimate limit state (ULS) assessment of offshore wind turbines, focusing on long-term extremes of maximum flapwise blade root bending moment under stochastic wind speed and turbulence. It benchmarks traditional environmental contour methods (IFORM and Direct Sampling contours) against a sequential sampling approach that uses Gaussian process regression to model short-term extreme-value distribution parameters across the environmental space. Short-term maxima are modeled parametrically (Gumbel and GEV), with parameter uncertainty propagated by approximating the likelihood via MCMC-derived Gaussian likelihoods, and long-term return values are estimated via Monte Carlo integration over the joint wind–turbulence distribution. In the Site A case, contour estimates match brute-force Monte Carlo and sequential sampling when an appropriate conditional fractile (about the 90% fractile) is used, while median and 99% fractiles under- and over-estimate extremes, respectively. In the South Brittany case, contours substantially underestimate the 50-year return level because extreme responses arise from common conditions inside the contour (short-term variability dominates), while sequential sampling remains robust and computationally efficient (typically converging in ~10–20 iterations).","Reliability is defined via the limit state function as $R=1-P_f=P[g(X)>0]=\int_{g(x)>0} f_X(x)\,dx$ (Eq. 1). The long-term distribution of short-term maxima is obtained by mixing conditional and marginal distributions: $g_Y(y)=\int g_{Y\mid X}(y\mid x) f_X(x)\,dx$ (Eq. 13), approximated by replacing $g_{Y\mid X}$ with a parametric extreme-value model (Gumbel/GEV) whose parameters $\theta(x)$ are learned with a Gaussian process. The sequential sampling selects new environmental points using the acquisition function $x_{new}=\arg\max_x s(x)\lvert\sigma_\theta(x)\rvert$ (Eq. 16), balancing contribution to extremes and model uncertainty.","For Site A 50-year return value via contours depends strongly on the chosen conditional fractile: DS/IFORM give about 20.6/20.4 MNm (50% fractile), 25.85/25.17 MNm (90% fractile), and 34.01/34.96 MNm (99% fractile). Brute-force Monte Carlo (with truncation/importance-style cutoffs) gives 50-year return values 24.649 MNm (1,000-year sample) and 25.093 MNm (10,000-year sample), and a 100-year return value of 27.112 MNm (10,000-year sample). Sequential sampling with GP regression converges to 50- and 100-year return values in reasonable agreement with brute force typically within 10–20 iterations; contour P90 aligns well while P50 underestimates and P99 overestimates. For South Brittany, IFORM contours yield 50-year (1-hour) MBld_y about 16.54 MNm (90% fractile), but sequential sampling indicates the contour method underestimates because dominant contributors to the 50-year level lie well inside the contour.","The study uses a simplified 2D environmental description (mean wind speed and turbulence only), ignoring wave/sea-state variability; the authors note this may be acceptable for bottom-fixed turbines but questionable for floating turbines. The response simulator is replaced by a surrogate (mNARX) to make the study feasible, and the surrogate sometimes returns NaNs for some seeds/conditions, which are handled by reseeding; authors assume this does not materially affect results. For South Brittany, brute-force benchmarking was not feasible due to computational cost, so the “true” extreme response is unknown. The South Brittany 99% fractile results from contours are noted as unreliable because they are based on only 100 seeds.","The comparison hinges on assumed joint distributions for $(U,\sigma_U)$; sensitivity of conclusions to alternative dependence structures or parameter uncertainty in these long-term models is not fully explored. The contour method is assessed primarily through return-value estimation rather than direct failure probability estimation for a specified capacity; results might differ when mapping to $P_f$ for a realistic limit state with capacity uncertainty. The sequential sampling method’s performance may depend on the chosen parametric family (Gumbel/GEV) and on the acquisition function definition (using the estimated 100-year level); robustness to model misspecification and alternative acquisition criteria is not thoroughly tested. Computational cost comparisons focus on number of evaluated long-term points, but do not fully normalize for differing seeds per point, surrogate failures/reseeding, or GP/MCMC overhead.","The authors recommend additional case studies on other structural problems to further validate sequential sampling for ULS reliability assessment. They explicitly propose extending the simplified 2D case (wind speed and turbulence) to higher-dimensional environmental problems where response depends on more variables (e.g., including sea states). They also recommend further investigation of the effect of the number of seeds per environmental condition on short-term variability and extreme-load extrapolation in design applications.","A natural extension is to incorporate model/epistemic uncertainty in the environmental joint distribution and propagate it jointly with response uncertainty (e.g., Bayesian hierarchical modeling over metocean parameters). Developing a self-calibrating contour approach using sequential sampling outputs (e.g., adaptive fractile selection or contour inflation factors) could help practitioners who rely on contours. Extending the framework to include autocorrelation/nonstationarity within the assumed 10-min/1-h blocks and to handle operational regime switching (operational vs parked) explicitly would improve realism. Publishing a reproducible implementation (including GP/MCMC/active-learning routines and example data) would support adoption and benchmarking across the structural reliability community.",2312.04972v2,https://arxiv.org/pdf/2312.04972v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:46:43Z
FALSE,NA,ML-based|Other,Complete lifetime data|Other,Not applicable,Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper extends conventional regression neural networks by producing prediction intervals (instead of point predictions) using Inductive Conformal Prediction (ICP), providing distribution-free, calibrated confidence guarantees under an i.i.d. assumption. It defines a basic absolute-error nonconformity score and introduces a normalized nonconformity measure that scales errors by a learned estimate of expected residual magnitude, yielding tighter intervals for “easy” cases and wider intervals for “hard” cases. The approach is evaluated via repeated cross-validation on four benchmark regression datasets and on a large real dataset for Total Electron Content (TEC) prediction (over 60,000 measurements across 11 years). Results show empirical error rates close to the target significance levels (e.g., near 10%, 5%, 1% for 90/95/99% confidence) while producing relatively tight intervals; the normalized measure substantially reduces interval widths compared with the unnormalized baseline. The work is positioned as a practical way to attach reliable confidence information to NN regression outputs without strong distributional assumptions.","The ICP nonconformity score for regression is first defined as $\alpha_i = |y_i-\hat y_i|$. For a new input $x$, the $(1-\delta)$ prediction interval is centered at the NN prediction $\hat y$ and uses the sorted calibration scores: $(\hat y-\alpha_{(m+s)},\ \hat y+\alpha_{(m+s)})$ where $s=\lfloor \delta(q+1)\rfloor$. A normalized nonconformity measure is proposed: $\alpha_i = \frac{|y_i-\hat y_i|}{\exp(\mu_i)+\beta}$ where $\mu_i$ is predicted by a secondary (linear) NN trained to estimate $\ln(|y-\hat y|)$; the corresponding interval becomes $(\hat y-\alpha_{(m+s)}(\exp(\mu)+\beta),\ \hat y+\alpha_{(m+s)}(\exp(\mu)+\beta))$.","Across benchmark datasets, empirical miscoverage closely matches the nominal levels (e.g., around 10%/5%/1% errors for 90%/95%/99% intervals), indicating good calibration. The normalized nonconformity measure (with $\beta=0$ or $0.5$) generally reduces median and interdecile mean interval widths compared to the basic absolute-error measure, with especially large gains reported on the Bank dataset. For TEC prediction (range roughly 0–110 TECu), the method achieves about RMSE $\approx 5.5$ TECu and correlation coefficient $\approx 0.94$ for point predictions, while producing tighter calibrated intervals; at 99% confidence with the normalized measure ($\beta=0$) the reported median interval width is 25.91 TECu (~23.5% of range) and at 95% it is 16.24 TECu (~14.8% of range). The paper notes some intervals can extend below 0 TECu (physically impossible) because they are symmetric around $\hat y$, implying post-clipping could further reduce widths without increasing errors.","The method assumes examples are independent and identically distributed (i.i.d.) for the conformal validity guarantee. The authors note that some predicted TEC intervals can start below zero because the interval construction is based on absolute errors and is not constrained by domain knowledge (non-negativity), though they state these could be clipped at zero after the fact. They also remark that calibration set size affects tightness (e.g., Boston Housing at 99% confidence has relatively wide intervals partly due to small calibration size).","The work is not about reliability engineering (failures/degradation/maintenance) and does not model time-to-failure, censoring, or system reliability; its “reliability” is statistical calibration of prediction intervals. The i.i.d. assumption can be questionable for time-series-like settings such as TEC (strong temporal/seasonal dependence), and the paper does not evaluate conformal validity under autocorrelation or distribution shift. Interval construction is symmetric and based on residual magnitude, which may be suboptimal for heteroskedastic and skewed conditional error distributions; alternatives (e.g., quantile regression, asymmetric conformal scores) are not explored. Reproducibility is limited because implementation details (code, exact NN hyperparameters beyond hidden units/training approach, random seeds) are not fully provided and no code repository is shared.","The authors propose developing additional normalized nonconformity measures to obtain even tighter prediction intervals. They also plan to apply and evaluate the method on other problems where prediction intervals are desirable. Finally, they intend to apply conformal prediction to investigate TEC variability under increased geomagnetic activity, which was not considered in this paper.","Extending the approach to non-i.i.d. data (e.g., time-series conformal methods, block/bootstrap conformal, or covariate shift-aware conformal) would strengthen applicability to real monitoring streams like TEC. Incorporating domain constraints (e.g., non-negativity for TEC) directly into interval construction (e.g., truncated/asymmetric intervals) could improve usefulness and calibration near boundaries. Comparing against other modern uncertainty quantification methods for NNs (e.g., quantile regression, deep ensembles, Bayesian NN approximations combined with conformal calibration) would clarify relative benefits. Providing an open-source implementation and standardized experimental protocol would improve reproducibility and adoption.",2312.09606v1,https://arxiv.org/pdf/2312.09606v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:47:24Z
TRUE,Life distribution modeling|Maintenance optimization|Other,"Parametric (Weibull, etc.)|ML-based|Bayesian|Hybrid/Ensemble",Interval-censored|Right-censored|Mixture of types|Simulated only|Other,Condition-based|Predictive|Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,NA,"The paper studies whether Bayesian neural networks improve predictive maintenance reliability modeling for highly reliable Navy weapon systems observed via interval-censored pass/fail tests with time-varying covariates. It formulates failures as a repairable-item Non-Homogeneous Poisson Process (NHPP) with a Weibull-Cox–type intensity, and compares (i) a baseline Weibull model without covariates (current Navy practice), (ii) Bayesian linear Weibull regression with variable selection priors, (iii) a Bayesian neural-network Weibull regression using a Laplace approximation (LaplaceNN) with a generalized Gauss–Newton covariance and local linearization in the posterior predictive, and (iv) MCMC/HMC inference for high-dimensional linear models. Performance is benchmarked on synthetic converted interval-censored datasets and real datasets (heart failure plus two weapon-system datasets) using ROC-AUC, PR-AUC, and reliability curves with 80% credible intervals. Results show LaplaceNN yields large gains on synthetic datasets with nonlinear intensity functions, while for heart failure and the weapon systems the evidence favors a linear model (selected as 0 hidden layers), implying limited benefit from neural networks given the available tabular features. The paper argues richer, more nonlinear/unstructured features may be needed for BNNs to meaningfully improve weapon-system predictive maintenance models.","Failures are modeled as an NHPP with intensity $h(t,x(t))=\exp(g_\theta(x(t)))\,\frac{k}{\lambda}(\lambda t)^{k-1}$, yielding interval no-failure probability $\exp\{-\int_{t_{lt}}^{t} h(\tau,x(\tau))\,d\tau\}$ and a conditional Weibull-Cox reliability over an interval (Eqs. 4–6). Interval-censored pass/fail likelihood is built from $P(\text{no failures in }[t_{lt},t])$ with $y=1$ indicating at least one failure (Eqs. 9–11). The LaplaceNN approximates the posterior around the MAP as $q(\phi)=\mathcal N(\phi^*,\Sigma_{GGN})$ with $\Sigma_{GGN}=(\sum J^T H J + \rho I)^{-1}$ and uses a locally linearized network $g_{\phi^*}^{lin}(x,\phi)=g_{\phi^*}(x)+\nabla_\phi g_\phi(x)|_{\phi^*}^T(\phi-\phi^*)$ in the posterior predictive (Eqs. 17–20).","On synthetic datasets, LaplaceNN substantially improves discrimination versus the baseline Weibull model: e.g., ROC-AUC increases from 0.544 to 0.742 (Banana) and 0.540 to 0.812 (Moon), and PR-AUC increases from 0.374 to 0.597 (Banana) and 0.359 to 0.674 (Moon). On Banana2 and Moon2, LaplaceNN also leads (ROC-AUC 0.765 and 0.829; PR-AUC 0.778 and 0.804) compared to baselines (ROC-AUC 0.550/0.556; PR-AUC 0.556/0.516). For Weapon System 1 and 2, results are reported as percent increases over baseline due to sensitivity; LaplaceNN shows about +8.07% ROC-AUC for WS1 and +10.72% ROC-AUC for WS2, and about +22.08% PR-AUC for WS1 and +18.12% PR-AUC for WS2. The conclusion notes that for heart failure and weapon systems, log-marginal-likelihood selection favored 0 hidden layers (a linear Laplace model), suggesting limited added value from neural networks with the current feature set.","Weapon-system details and some modeling specifics (e.g., the change-of-variables relationship for the rate parameter involving contractor-provided reliability inputs) are not fully disclosed due to the sensitive nature of Navy weapon systems. The authors state they cannot compute Kaplan–Meier survival functions for the weapon-system datasets because of data complications such as missing measurement recordings and unreliable unique identifiers. They also note that neural-network benefits may be minimal unless weapon-system features are highly nonlinear or unstructured (e.g., images/text).","The benchmark uses classification-style metrics (ROC-AUC/PR-AUC) on interval-censored reliability problems; calibration and proper scoring rules for survival/reliability (e.g., time-dependent Brier score, log score, calibration of survival probabilities) are not emphasized, which can be critical for maintenance decision-making. The weapon-system evaluation reports only percent improvements, limiting interpretability and reproducibility and making it hard to assess practical significance. Assumptions such as at most one failure per interval and immediate repair time for NHPP may be unrealistic for some repairable systems and could bias intensity estimation if violated. Hyperparameter tuning via grid search on training log marginal likelihood may still induce selection bias without a clear Phase II-like prospective evaluation protocol across fleets/time.","The authors suggest that future, larger and more feature-rich weapon-system datasets planned for collection will likely benefit from their AI/ML approach, especially if the measured features become more nonlinear or include unstructured modalities where neural networks can add value.","Extend the framework to handle multiple failures per interval and non-negligible repair/downtime (e.g., renewal processes, marked point processes, or imperfect repair) to better match operational repairable-system behavior. Add robust handling of missing data and entity resolution for weapon-system identifiers (e.g., probabilistic linkage) so nonparametric baselines like Kaplan–Meier and stronger validation become feasible. Evaluate decision-focused utility (e.g., maintenance cost/availability impacts) and survival calibration, not just discrimination metrics, to directly support predictive maintenance policy selection. Provide an implementable open-source reference (or at least pseudocode for inference/tuning) and explore scalable approximations (e.g., structured Laplace/KFAC, variational inference) for very high-dimensional covariates.",2312.10494v2,https://arxiv.org/pdf/2312.10494v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:48:14Z
FALSE,NA,ML-based|Other,Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes using Venn Prediction (VP) with artificial neural networks to produce reliable probabilistic outputs (lower/upper bounds on conditional class probabilities) for detecting vesicoureteral reflux (VUR) in children, using non-radiation clinical and laboratory features. To address class imbalance (30 VUR positives out of 162 cases), it incorporates minority oversampling (MO) or majority undersampling (MU) into the ANN-based Venn taxonomy and compares against conventional ANNs and a prior probabilistic neural network approach. In batch 10×10-fold cross-validation, ANN-VP variants yield substantially better probability quality (cross-entropy, Brier score, and especially calibration/reliability) than conventional ANNs, with MO generally best. In an online setting, cumulative errors are shown to lie within the VP-produced cumulative error-probability bounds, empirically demonstrating calibration of the probabilistic intervals under i.i.d. assumptions. The work is positioned as improving decision support by providing informative, calibrated probability intervals rather than binary predictions for whether VCUG testing is warranted.","The Venn predictor forms an extended set by temporarily assigning each possible label $Y_j$ to a new instance $x_{l+1}$: $\{(x_1,y_1),\ldots,(x_l,y_l),(x_{l+1},Y_j)\}$. For each $Y_j$, it computes class probabilities within the new instance’s Venn category via empirical frequency: $p^{Y_j}(Y_k)=\frac{|\{i: \kappa_i^{Y_j}=\kappa_{l+1}^{Y_j}\ \&\ y_i=Y_k\}|}{|\{i: \kappa_i^{Y_j}=\kappa_{l+1}^{Y_j}\}|}$. The multiprobability output yields an interval for class $Y_k$ as $[\min_j p^{Y_j}(Y_k),\max_j p^{Y_j}(Y_k)]$, and prediction uses the mean class probability with a threshold $\theta$ to handle imbalance.","Dataset: 162 pediatric UTI cases, 30 VUR positives (positive rate 0.1852). In 10× repeated 10-fold CV (with $\lambda=6$ categories), ANN-VP+MO achieved sensitivity/specificity 75.67%/84.09% with CE 558.92 and Brier 0.0988 (CFS features), versus conventional ANN+MO 70.33%/85.83% with CE 697.67 and Brier 0.1309; reliability (calibration) improved dramatically (0.0041 vs 0.0350). With $\chi^2$/IG features, ANN-VP+MO had CE 595.96, Brier 0.1087, reliability 0.0052, still far better calibrated than conventional ANN+MO (CE 773.30, Brier 0.1411, reliability 0.0370). Online experiments show cumulative errors always fall between cumulative lower/upper VP error-probability bounds across different hidden units (5 vs 100) and category counts ($\lambda\in\{2,6,10\}$), while conventional ANN probabilities significantly underestimate error (reported two-sided p-values as low as 1.95e-5 and smaller).","The authors note that majority undersampling may not show clear improvement because the minority class is very small, making the undersampled training set too small. They state that collecting a larger dataset is needed to draw more definite conclusions. They also indicate the need to assess the approach in actual clinical practice.","The work assumes i.i.d. observations; in clinical data, temporal, institutional, or protocol-related dependencies could violate i.i.d. and affect calibration guarantees. The dataset is small (162 total, 30 positives) and from a single hospital, limiting external validity and raising the risk that reported performance may not generalize to other populations/settings. The study emphasizes calibration metrics, but provides limited discussion of clinical utility tradeoffs (e.g., decision-curve analysis, costs of false negatives vs false positives) for recommending VCUG. No implementation details (software, seeds, hyperparameter search scope) are provided, which may hinder reproducibility and fair comparison.","They propose evaluating the approach in clinical practice, collecting a larger dataset to draw more definite conclusions, and experimenting with Venn Predictors based on other conventional classifiers.","External validation on multi-center cohorts and prospective/temporal validation would better assess generalizability and robustness under distribution shift. Developing an explicit clinical decision policy using the probability intervals (e.g., risk-thresholding with uncertainty-aware rules) and evaluating net benefit could translate calibration into actionable guidance. Exploring robustness to missing data, non-i.i.d. settings, and incorporating cost-sensitive learning could better align with real clinical deployment constraints. Providing open-source code and a standardized benchmark protocol would improve reproducibility and facilitate adoption.",2312.11355v1,https://arxiv.org/pdf/2312.11355v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:48:54Z
TRUE,Degradation modeling|RUL prediction|Other,Stochastic process|ML-based|Simulation-based|Other,Degradation measurements|Sensor/condition monitoring|Simulated only|Mixture of types,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,Python|R|Other,Not provided,https://www.nasa.gov/intelligent-systems-division/discovery-and-systems-health/pcoe/pcoe-data-set-repository/,"The paper evaluates the reliability of deep learning (an LSTM regressor) for estimating the Hurst parameter in fractional stochastic processes, focusing on fractional Brownian motion (fBm), fractional Ornstein–Uhlenbeck (fOU), and linear fractional stable motion (lfsm/fLsm). It develops a fast synthetic-data generation pipeline (Davies–Harte/Kroese-type) for fBm and fOU to enable large-scale training and compares LSTM accuracy against classical estimators (e.g., Higuchi, DFA/Whittle/variogram/R/S) using RMSE, MAE, mean relative error, and tail-focused metrics such as 95% error quantiles and maxima. Results show the LSTM substantially outperforms traditional estimators for fBm and fOU across multiple sequence lengths, but generalizes poorly to lfsm when trained only on Gaussian (fBm/fOU) trajectories, highlighting a process-mismatch limitation. The method is applied to Li-ion battery degradation data from NASA PCoE to estimate H and produce empirical confidence bounds based on simulated-error quantiles. Overall, the work contributes a practical, speed-oriented deep-learning estimator with an explicit emphasis on estimation error reliability (including extreme-error behavior) for use in degradation/PHM contexts with long-range dependence.","Fractional Brownian motion is defined via its covariance: $\mathbb{E}[B_H(t)B_H(s)] = \tfrac12\big(|t|^{2H}+|s|^{2H}-|t-s|^{2H}\big)$. The fractional Ornstein–Uhlenbeck (fOU) process is given by the Langevin SDE $dX(t)=\kappa(\theta-X(t))dt+\sigma\,dB_H(t)$ with explicit solution (for zero initialization) $X(t)=-\sigma\int_0^t e^{-\kappa(t-s)}\,dB_H(s)$. Linear fractional stable motion is defined by $L_{H}^{\alpha}(t)=\int_{-\infty}^{\infty}\big((t-x)_+^{H-1/\alpha}-(-x)_+^{H-1/\alpha}\big)
\,dM(x)$, with stability index $\alpha\in(0,2)$ and Hurst exponent $H\in(0,1)$ (with $H\neq 1/\alpha$).","For the benchmark model (LSTM trained on 1600-length fBm), evaluation on 1600-length fBm gives RMSE 0.0149, MAE 0.0117, and MRE 3.63%, with maximum absolute error 0.0592 and 95% absolute-error quantile 0.0300; on 6400-length fBm RMSE improves to 0.0079 and MAE to 0.0066. On fOU (evaluated using the same fBm-trained LSTM), performance is similar (e.g., RMSE 0.0156 at length 1600; 0.0079 at length 6400). Classical Higuchi estimation is worse on fBm (e.g., RMSE 0.0246 at length 1600 vs 0.0149 for LSTM; RMSE 0.0152 at length 6400 vs 0.0079 for LSTM). On lfsm with $\alpha\in\{1.8,1.5,1.2\}$, the LSTM (trained on fBm/fOU) underestimates H and fails to recover true values, while Higuchi remains qualitatively accurate but with higher variance.","The authors state that LSTM effectiveness depends on training data quality and that a faulty synthetic generator can cause the network to learn generator errors, distorting results. They also state they cannot train on lfsm trajectories because they lack a sufficiently fast lfsm generator, forcing evaluation of cross-process generalization (fBm/fOU-trained LSTM on lfsm). They note that skewness and tail behavior of estimation errors matter for risk/loss contexts and that relative errors can still be large in some parameter regions despite good average performance.","The uncertainty bounds for the battery case are derived from simulated-error quantiles under assumed process families and may not reflect dataset-specific noise, nonstationarity, regime changes, or measurement-error structure in real degradation signals. The approach assumes independent trajectories and does not directly address autocorrelation/irregular sampling typical of sensor streams beyond what is in the fractional models. Comparisons may be sensitive to hyperparameter tuning fairness: the LSTM uses extensive synthetic training, while classical estimators are applied out-of-the-box without comparable calibration to the same data-generating assumptions or finite-sample corrections.","They propose exploring transformer-based alternatives, specifically the iTransformer (inverted transformer) architecture, as a promising backbone for time series tasks, motivated by limitations of transforms like signature methods that may slow evaluation. They also indicate interest in leveraging deep learning to infer dynamic parameters of stochastic processes underlying risk propagation, extending beyond point estimates toward broader risk modeling.","Developing an efficient lfsm/fLsm simulator (or using approximate/conditional simulation) would enable in-family training and a controlled study of stability-index effects on H estimation. A self-starting/robust version that handles unknown scaling, drift, outliers, and measurement errors explicitly (common in degradation data) would improve deployability, along with calibration methods for well-validated predictive intervals (e.g., conformal prediction on real datasets). Extending to multivariate degradation signals and integrating H estimation into downstream RUL/maintenance decision models (showing end-to-end reliability impact) would better connect the estimator to PHM practice.",2401.01789v1,https://arxiv.org/pdf/2401.01789v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:49:46Z
FALSE,NA,Simulation-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/ryanboustany/MaxPool-numerical,"The paper studies the numerical reliability (correctness/stability) of automatic differentiation (AD) in neural networks that include the nonsmooth MaxPool operation under different floating-point precisions (16/32/64-bit), architectures (LeNet, VGG, ResNet), and datasets (MNIST, CIFAR10, SVHN, ImageNet). It introduces two parameter-space phenomena for MaxPool-derived programs: a numerical bifurcation zone where AD outputs can differ materially between alternative nonsmooth Jacobian selections, and a compensation zone where AD can be incorrect in floating-point arithmetic while being correct over the reals (errors near machine precision). The authors propose operational criteria and thresholds (derived from nondeterministic GPU variation and ReLU-based program variation) to identify the numerical bifurcation zone and estimate its prevalence via Monte Carlo sampling. Empirically, they find that low-norm MaxPool Jacobians lead to stable training and comparable test accuracy, while high-norm Jacobians can destabilize training and reduce accuracy, especially at lower precision. They report mitigations such as using batch normalization, Adam-like optimizers, or higher precision to reduce the adverse influence of MaxPool Jacobians and numerical issues.","Backpropagation for a nonsmooth program is modeled as a product of selected Jacobians along the computational graph: $\text{backprop}[P](\theta)=v_H(\cdot)\,v_{H-1}(\cdot)\cdots v_1(\theta)$, where each $v_j(w)\in J^{\mathrm{Clarke}} g_j(w)$. The backprop-variation metric between two implementations $P$ and $Q$ is $D_{m,q}(P,Q)=\left\|\text{backprop}\left[\sum_{i\in B_q} P_i(\theta_m)\right]-\text{backprop}\left[\sum_{i\in B_q} Q_i(\theta_m)\right]\right\|_1$. A numerical bifurcation zone is operationally defined by a threshold $\tau_{f,\omega}$ as $S(\tau_{f,\omega})=\{\theta:\exists i,\ \|\text{backprop}[P_i](\theta)-\text{backprop}[Q_i](\theta)\|_1>\tau_{f,\omega}\}$.","In a 32-bit LeNet-5/MNIST experiment comparing native vs minimal MaxPool, backprop variations were nonzero across trials, with ~98.78% of parameters showing small differences near machine precision ($10^{-8}$–$10^{-7}$) and ~1.22% showing larger differences peaking around $10^{-3}$. For VGG11 on CIFAR10, the estimated proportion of sampled parameter settings in the numerical bifurcation zone was 100% at 16-bit and 32-bit precision and 0% at 64-bit; the proportion of impacted mini-batches was 100% (16-bit), 46.67% (32-bit), and 0% (64-bit). Training experiments indicate that very large hybrid-Jacobian mixing parameters (e.g., $\beta\ge 10^3$–$10^4$ depending on precision) can cause instability/exploding gradients and reduced test accuracy, while realistic/smaller $\beta$ values (e.g., $\{0,1,10,100\}$) yield stable training and similar test accuracy. Batch normalization and Adam reduce sensitivity to large $\beta$ and improve stability, and higher precision reduces/avoids numerical bifurcation effects in their tests.",None stated.,"The work focuses on numerical reliability of AD for MaxPool in common CNN benchmarks, so conclusions may not generalize to other nonsmooth primitives (beyond the discussed comparisons to ReLU/NormPool) or to other training regimes (e.g., different losses, optimizers, or heavy data augmentation). The proposed bifurcation/compensation zone criteria rely on empirically chosen thresholds that can vary with initialization, architecture, dataset, and hardware; this can make the zone definitions less portable and harder to standardize across systems. The evaluation is largely empirical/Monte-Carlo and does not provide formal floating-point error bounds guaranteeing when compensation errors occur or how they scale with depth/conditioning. Practical impact is mostly characterized via test accuracy/instability; there is limited discussion of downstream safety implications or diagnostic tooling for practitioners to detect these issues during training.",None stated.,"Develop formal floating-point error analyses (or certified bounds) for MaxPool/argmax-style primitives under AD to theoretically characterize compensation errors and when they can accumulate. Extend the empirical study to broader nonsmooth operators used in modern networks (e.g., top-k, sorting/ranking layers, attention variants with max-like reductions) and to other settings such as recurrent/sequence models. Provide robust, self-diagnosing implementations (e.g., runtime detectors for entering compensation/bifurcation regimes, or automatically selecting low-norm Jacobians) and package them as library-level options. Validate findings across additional hardware/software stacks (different GPU architectures, compiler settings, and frameworks such as JAX/TensorFlow) to assess portability and reproducibility of the numerical reliability phenomena.",2401.02736v2,https://arxiv.org/pdf/2401.02736v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:50:25Z
TRUE,Life distribution modeling|System reliability|Other,ML-based|Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Not provided,https://github.com/IdahoLabResearch/BIhNNs,"The paper proposes an accelerated rare-event reliability analysis method that embeds Hamiltonian Neural Networks (HNNs) into Hamiltonian Monte Carlo (HMC) within Subset Simulation to estimate small failure probabilities defined by a limit-state function g(x)≤0. The key idea is to replace expensive Hamiltonian gradient evaluations during leapfrog integration with gradients predicted by a pre-trained (latent) HNN, yielding high acceptance rates and substantial speedups while retaining Metropolis-Hastings correctness via the usual acceptance step. The method is evaluated on several benchmark reliability problems (correlated/degenerate high-dimensional Gaussians, Rosenbrock “banana” distributions, and structural dynamics examples), reporting accurate reliability index estimates β and improved sampling robustness relative to modified Metropolis-Hastings in challenging, highly anisotropic settings. For difficult tail-region sampling where learned gradients degrade, the authors propose (and demonstrate) mitigation via online error monitoring that falls back to numerical gradients and/or periodic retraining using subset-level samples. A highlight application couples Bayesian parameter inference (via HMC) with reliability analysis for a nonlinear Bouc–Wen hysteretic SDOF system; using HNN-predicted gradients avoids repeated model evaluations within HMC trajectories and reduces Subset Simulation wall-clock time by >100× compared to standard HMC-based Subset Simulation.","Reliability is posed via the limit-state function g(x), with failure probability $P_F=\int_{g(x)\le 0} f_X(x)\,dx$ and Monte Carlo estimator $\hat P_F=\frac{1}{m}\sum_{k=1}^m I_F(x^{(k)})$. Subset Simulation factors the rare-event probability as $P(F)=P(F_1)\prod_{i=1}^{n-1}P(F_{i+1}\mid F_i)$ with intermediate thresholds chosen so conditional probabilities are ~0.1. HMC uses Hamiltonian dynamics with $H(q,p)=U(q)+K(p)$, $U(q)=-\log\pi(q)$, $K(p)=\tfrac12 p^T M^{-1}p$, and accepts proposals with $\alpha=\min\{1,\exp(H(q,p)-H(q^*,p^*))\}$; the proposed HNNMC replaces leapfrog gradient evaluations $\partial H/\partial q$ by HNN-predicted gradients during integration while keeping the same acceptance test using the true Hamiltonian.","The method reports >20× speedup in Hamiltonian trajectory propagation versus standard HMC due to fast HNN gradient evaluations, and in a Bayesian-inference-coupled reliability example it reduces Subset Simulation time from 579,549 s (>6 days) with HMC to 4,509 s (~1.25 h), i.e., >100× faster, while producing similar reliability indices (HNNMC: mean $\bar\beta=4.35$, CV=0.048 over 100 trials; HMC: $\bar\beta=4.44$, CV=0.039 over 10 trials). For a degenerate Gaussian linear limit-state example (n=2, β=5, ρ=0), one run estimated $P_F=3.12\times 10^{-8}$ (β=5.41) with high HMC acceptance (0.96–0.99) but subset-level rejection rates 0.16–0.52. On thin Rosenbrock targets, HNNMC remains competitive where MMH breaks down (e.g., k=10 yielded mean $\bar\beta=3.435$ with CV=0.127 over 100 trials, compared to a cited stretch-sampler benchmark $\bar\beta=3.466$, CV=0.138). For an extreme Rosenbrock case (k=100), baseline HNNMC can fail badly (mean $\bar\beta=8.014$), while online error monitoring improves it (mean $\bar\beta=4.983$) and increasing subset samples to $10^4$ brings HNNMC close to reference (mean $\bar\beta=2.781$ from 10 runs) at higher evaluation cost.","The authors state that the approach can reach its limitations for gradient estimation in low-probability (tail) regions of complex, strongly correlated and/or high-dimensional distributions (e.g., highly correlated 100D Gaussians and Rosenbrock with large k), leading to higher rejection rates and degraded rare-event estimation. They note that effectiveness depends on having adequate training data in the relevant regions; otherwise the HNN extrapolates poorly. They also note that online error monitoring improves robustness but reduces computational efficiency because it triggers expensive conventional gradient computations, and its threshold must be tuned carefully to avoid either inaccurate proposals or reverting to near-standard HMC cost.","The method’s correctness relies on computing the Metropolis acceptance probability using the true Hamiltonian; if the target density or its log-density is only available approximately (e.g., noisy simulators or approximate likelihoods), the claimed robustness may not hold. The paper’s computational comparisons are sensitive to implementation details (hardware, vectorization, HNN architecture/training budget, and HMC tuning such as step size and trajectory length), and may not generalize without a standardized benchmark protocol. For Subset Simulation with conditional truncation, the primary cost in many engineering settings is still repeated limit-state evaluations; if those dominate, HNN-accelerated gradients may yield smaller end-to-end gains than suggested by gradient-only speedups. Finally, the approach introduces a nontrivial training/tuning workflow (network architecture, training trajectories, retraining triggers), which may be a barrier to routine industrial deployment without packaged, automated defaults.","The authors propose techniques to improve gradient prediction in low-probability regions, specifically (i) updating/retraining the neural network using samples from new subset levels and (ii) an online error monitoring scheme that selectively falls back to conventional HMC gradients when Hamiltonian conservation is violated beyond a threshold. They also emphasize extending the approach to more challenging problems where Bayesian inference and reliability analysis are conducted together, leveraging HNNMC to avoid expensive model evaluations during trajectory integration.","A valuable extension would be a self-starting/online-learning HNNMC that incrementally expands training coverage with principled acquisition of tail-region trajectories (e.g., active learning focused on subset boundaries) while controlling bias and compute. Robust variants that handle model discrepancy, noisy/biased likelihoods, and non-smooth limit-state functions (where gradients may be unstable) would broaden applicability to real engineering simulators. It would also be useful to extend the method to parallel tempering or other advanced rare-event MCMC kernels within Subset Simulation, and to provide an open-source reference implementation integrated into UQPy with reproducible benchmarks and automatic hyperparameter/threshold selection for error monitoring and retraining.",2401.05244v1,https://arxiv.org/pdf/2401.05244v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:51:27Z
TRUE,Other,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,NA,"This paper studies structural reliability analysis when the limit-state function is noisy (non-deterministic), such that repeated evaluations at the same input yield different outputs. It formalizes the distinction between the noise-free failure probability $P_f=P(g(X)\le 0)$ and the noisy failure probability $\tilde P_f=P(\tilde g(X,\omega)\le 0)$, showing that classical simulation-based methods (e.g., MCS) converge to $\tilde P_f$ rather than $P_f$ in general. To recover the underlying noise-free probability of failure, the authors propose denoising via regression-based surrogate models, focusing on Gaussian process regression (GPR) with an explicit noise term. They embed this surrogate in an active learning reliability framework, using subset simulation for estimating $\hat P_f$ and a noise-aware learning function (the $U_N$ criterion) to adaptively enrich the experimental design efficiently. The method is demonstrated on noisy versions of benchmark limit-state functions (four-branch and hat) and on a finite-element structural frame example, showing accurate recovery of $P_f$ in benchmarks and improved estimates relative to the noisy baseline in the realistic high-dimensional case.","The noisy model is defined as $\tilde g(x,\omega)=g(x)+\varepsilon(x,\omega)$ with $\mathbb{E}_\omega[\varepsilon]=0$. The target (noise-free) failure probability is $P_f=P(g(X)\le 0)$, while naive simulation on the noisy model estimates $\tilde P_f=P(\tilde g(X,\omega)\le 0)$. Denoising is done by training a regression surrogate (GPR) and estimating $\hat P_f=\int_{\{x:\hat g(x)\le 0\}} f_X(x)\,dx$ using the surrogate mean predictor. Active learning enrichment uses the noise-aware learning function $U_N(x)=\Phi\!
\left(-\frac{|\mu_{\hat g}(x)|}{\sigma_{\hat g}(x)}\right)-\Phi\!\left(-\frac{|\mu_{\hat g}(x)|}{\sigma_{\hat g+1}(x)}\right)$ with a closed-form one-step look-ahead variance $\sigma^2_{\hat g+1}(x)=\sigma^2_{\hat g}(x)\,\frac{\sigma_n^2}{\sigma^2_{\hat g}(x)+\sigma_n^2}$.","For the illustrative noisy $R-S$ problem with true $P_f=\Phi(-3)=1.35\times 10^{-3}$, the GPR denoising approach recovers the noise-free $P_f$ given a sufficiently large experimental design; the required size increases with noise variance (about 100 points for $\sigma_\varepsilon=0$, versus roughly $10^3$–$10^4$ for larger noise levels in their study). On the four-branch benchmark (reference $P_f\approx 4.51\times 10^{-3}$) and the hat function (reference $P_f\approx 9.76\times 10^{-4}$), the active learning + $U_N$ strategy converges to the noise-free probabilities typically with about 200 total points in the experimental design (in their runs up to 600). For the 21D structural frame, the noise-free reference is $P_f\approx 5.07\times 10^{-3}$ while the noisy probability is $\tilde P_f\approx 1.70\times 10^{-2}$ (over 3× larger); the proposed method yields a median denoised estimate around $\hat P_f\approx 7.26\times 10^{-3}$ for single-point enrichment under a 1000-evaluation budget, and multi-point enrichment (e.g., batches of 3 or 10) improves denoising performance in that case.","The study focuses primarily on homoskedastic Gaussian noise (constant variance, normal distribution), although the authors state that unbiasedness is the key requirement of their method. They explicitly note that the effectiveness of Gaussian process regression for heteroskedastic and non-Gaussian noise “remains to be studied.” They also acknowledge the need for a suitable stopping criterion for the active learning procedure and state it is currently under investigation.","The method’s denoising relies heavily on the surrogate mean being an accurate estimator of the underlying noise-free limit-state, which can be challenging under model misspecification, strong nonstationarity, or severe heteroskedasticity; robustness to these conditions is not systematically quantified. The evaluation is largely simulation-based and uses artificially noise-corrupted benchmarks; real measured noisy datasets are not presented, so practical issues like correlated noise, bias, or input-dependent noise levels are not empirically validated. Computational scaling of GPR with large experimental designs (e.g., $\mathcal{O}(N^3)$ training) may become limiting for higher-dimensional/high-budget settings beyond those tested, and sparse/approximate GP alternatives are not explored. Comparison against alternative noise-aware acquisition criteria (e.g., SUR variants) is discussed but not presented as a full head-to-head benchmark across cases.","The authors state they are exploring alternative noise-aware learning functions and enrichment strategies because the $U_N$ function may be prone to exploitative loops in some cases. They also explicitly propose studying GPR effectiveness under heteroskedastic and non-Gaussian noise. Finally, they indicate that developing an appropriate stopping criterion for the active learning approach is ongoing work.","Extending the approach to explicitly model heteroskedastic noise (e.g., GP models with input-dependent noise, replicated designs, or warped GPs) would better match many real simulators and measurement processes. A self-starting/online calibration of noise variance using adaptive replication (choosing when to re-simulate at the same $x$ versus exploring new $x$) could improve efficiency when noise dominates near the limit-state. Broader validation on real-world noisy engineering datasets (e.g., experimental structural monitoring, stochastic CFD) and standardized comparisons to other noise-aware reliability methods (e.g., SUR-based excursion set estimation, multi-fidelity noise-aware designs) would strengthen practical guidance. Investigating scalable GP surrogates (sparse inducing points, local GPs) and reliability estimation robustness under dependent inputs/noise correlations would improve applicability to large-scale industrial problems.",2401.10796v1,https://arxiv.org/pdf/2401.10796v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:52:21Z
TRUE,Maintenance optimization|System reliability|Life distribution modeling|Other,"Stochastic process|Parametric (Weibull, etc.)|Other",Simulated only|Other,Block replacement|Condition-based|Group replacement|Imperfect maintenance|Not applicable|Other,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a hierarchical, decision-based maintenance policy for a complex modular system composed of K independent modules, each made of multiple components subject to wear-out and external shocks. System and component lifetimes are modeled using phase-type (PH) distributions, while shock arrivals are modeled via Markovian arrival processes (MAPs); the resulting high-dimensional CTMC is constructed using a layered Matrix-Analytical Method (MAM) and the authors’ MoMA algorithm. The maintenance policy uses periodic inspections with system-level triage (optimal/critical/failed): no action if optimal, full system replacement if failed, and if critical then module-level decisions (replace failed modules; repair/restore failed units in critical modules) using maintenance-effect matrices. A cost model (inspection, unit repair, module replacement, system replacement, and downtime cost) yields an optimization problem over the inspection interval τ (and implied number of inspections over a fixed useful life). A numerical case study based on a submarine/subsea electrical control unit (SEM/ECU) illustrates the method and shows that the optimal inspection interval decreases as downtime cost increases, with reported optima (e.g., τ≈8300h down to ≈980h across scenarios).","System dynamics are governed by a CTMC generator Q_sys constructed via Kronecker sums/products over module/unit generators and MAP shock processes, with macro-states indexed by number of failed modules. Maintenance is encoded by module-level maintenance-effect matrices M_i that map pre-inspection states to post-maintenance operational states (e.g., tensor products of per-unit actions, with replacement using vectors β_i / β_{j,i}). After each inspection, the system’s initial state distribution is updated by a mixture over whether the pre-inspection system state was optimal/critical/failed: \(\alpha^e(a)=\alpha^e_{U1}(a)P(X^e(a\! -\!1,\tau)\in U_1)+\alpha^e_{U2}(a)P(\cdot\in U_2)+\alpha^e_D(a)P(\cdot\in D)\) (Eq. 4). Expected downtime cost is \(EC_{down}(a,\tau)=\int_0^{\tau} f_s(a,t)c_{down}(a,t)\,dt\) (Eq. 5), and module maintenance cost under critical-system intervention is \(Cost(a,\tau,i)=\alpha_i^e(a\! -\!1)e^{Q_i^{*s}\tau}D_{i,U2}(M_i\odot C_i)\mathbf e\) (Eq. 6).","In the SEM/ECU case study (4 modules: control panel, processors, input, output; series arrangement; 2-out-of-3 voting for three modules), the mean system lifetime is reported as \(\mu\approx 24000\) hours (reliability at mean life \(R(\mu)=0.438\)). Using Monte Carlo to minimize total expected cost over a grid of inspection intervals, the optimal τ and implied number of inspections A increase/decrease strongly with downtime cost \(C_{down}\). Reported optima: for \(C_{down}\in\{0.001,0.01,0.1,1\}\) (mu/hour), \(\tau\approx\{8300,4390,2200,980\}\) hours with total costs \(\approx\{17.4791,24.4786,42.0189,87.0051\}\) mu and \(A\approx\{6,11,23,51\}\) inspections over the useful life. The authors note \(C_{down}\) is the most influential cost parameter in their experiments.","The authors state that the maintenance-cost parameters used in the case study (e.g., inspection, repair, replacement costs) were chosen arbitrarily and “with no fully real meaning” due to lack of expert guidance, limiting practical interpretability. They also note the current model assumes modules fail independently and does not distinguish failure modes for modules or the system.","The evaluation of the maintenance optimization relies on a Monte Carlo grid search for τ rather than solving the cost function analytically or using variance-reduction/efficient optimization; results may depend on grid resolution and simulation settings. The policy assumes periodic inspections with perfect state classification into {optimal, critical, failed} and conditional module/component checks; real inspections often have measurement error and partial observability. The assumption of independent modules (and largely independent unit processes within modules) and Markovian shock processes may be restrictive for tightly coupled systems with common-cause failures or shared environments. Code/software details are not provided, which limits reproducibility of the MoMA construction and simulation study.","The authors propose extending the model to incorporate dependencies between modules rather than independent module failures. They also plan to model and act on multiple failure modes with corresponding maintenance actions, and to use empirical data to demonstrate the methodology.","Developing a self-starting/online estimation framework to calibrate PH/MAP parameters and cost inputs from operational data would improve deployment in real settings. Extending the maintenance policy to imperfect/partial repairs with explicit restoration factors (beyond probabilistic reinitialization) and to opportunistic maintenance with economic dependencies could broaden applicability. Robust and sensitivity-optimized inspection schedules (e.g., adaptive τ based on posterior state probabilities) could reduce reliance on fixed periodic inspections. Providing an open-source implementation of MoMA + maintenance/cost modules and benchmarking against standard CBM approaches (e.g., POMDP-based maintenance, renewal-reward formulations) would strengthen comparability and uptake.",2401.11328v1,https://arxiv.org/pdf/2401.11328v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:53:03Z
TRUE,RUL prediction|Maintenance optimization|Degradation modeling,ML-based|Bayesian,Degradation measurements|Sensor/condition monitoring,Predictive,Transportation/logistics|Energy/utilities|Other,Simulation study,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/lucadellalib/bdl-rul-svgd,"The paper proposes using Stein Variational Gradient Descent (SVGD) to train Bayesian deep learning models for remaining useful life (RUL) estimation, aiming to provide both accurate predictions and calibrated uncertainty for predictive maintenance decision-making. The authors compare Bayesian neural networks trained via SVGD against the same architectures trained with Bayes by Backprop (parametric variational inference) and against frequentist counterparts trained by standard backpropagation. Experiments use the simulated run-to-failure NASA C-MAPSS turbofan engine dataset with multivariate sensor trajectories and operational settings, preprocessed via min-max normalization, sliding windows, and target rectification (Rearly=125). Across all four C-MAPSS subsets, SVGD-trained Bayesian models show faster/stabler convergence and better predictive metrics (RMSE/MAE and especially the asymmetric PHM score) than Bayes by Backprop and frequentist baselines, with the Dense3 SVGD variant best overall. The paper also introduces an uncertainty-informed correction that subtracts a multiple of the predictive standard deviation from the predictive mean to reduce late predictions, consistently improving the score metric.","Bayesian formulation uses the posterior predictive $p(\hat y\mid \hat x, D)=\int p(\hat y\mid \hat x,w)\,p(w\mid D)\,dw$ and Bayes-by-Backprop optimizes the ELBO $F(D,\theta)=\mathrm{KL}[q(w\mid\theta)\|p(w)]-\mathbb{E}_{q}[\log p(D\mid w)]$ approximated by Monte Carlo. SVGD represents the surrogate posterior with particles and updates them via $w_i^{(l+1)}=w_i^{(l)}+\epsilon^{(l)}\,\phi^\*(w_i^{(l)})$, where $\phi^\*$ combines a driving term toward high $\log \hat p(w\mid D)=\log p(D\mid w)+\log p(w)$ regions and a repulsive kernel term (RBF) to avoid particle collapse. The uncertainty-informed prediction modifies the posterior predictive mean by $\mu^*(\hat y)=\mu(\hat y)-p_{\text{late}}\,k\,\sigma(\hat y)$, with $p_{\text{late}}$ estimated as the fraction of samples where predicted mean exceeds the true RUL.","On the rectified C-MAPSS test sets (averaged over 10 seeds), Dense3-SVGD achieves the best overall performance: FD001 RMSE 13.17 and Score 334 (improves to Score* 318 with uncertainty correction), FD002 RMSE 18.27 and Score 2259 (Score* 2034), FD003 RMSE 12.33 and Score 307 (Score* 287), and FD004 RMSE 21.00 and Score 6648 (Score* 5830). SVGD consistently improves over Dense3-BBB and Dense3-BP; for example on FD003 RMSE drops from 15.97 (BBB) and 14.97 (BP) to 12.33 (SVGD), and Score drops from 725 (BBB) and 625 (BP) to 307 (SVGD). The proposed uncertainty-based correction consistently improves the asymmetric score across models/subsets, indicating reduced late-prediction risk. Performance is notably worse on FD002/FD004 for all methods, attributed to multiple operating conditions (6 vs 1).","The authors state that further investigations are needed to fully harness SVGD: specifically, the impact of the number of particles and the choice of the kernel should be analyzed. They also note that evaluating more competitive deep learning architectures is beyond the scope of this study and deferred to future work.","Although the method is positioned for predictive maintenance, validation is limited to a simulated benchmark (C-MAPSS) rather than real operational fleets, so generalization to real sensor noise, maintenance actions, and nonstationarity is unclear. The “late prediction probability” $p_{\text{late}}$ is estimated on the training set (no validation split), which risks optimistic tuning and can bias reported Score* improvements. Uncertainty quality is not evaluated with calibration metrics (e.g., coverage of prediction intervals, CRPS), so it is hard to judge whether SVGD improves uncertainty quantification or mainly point accuracy. Comparisons are restricted to two relatively simple architectures (Dense3, Conv2Pool2) and two Bayesian baselines (BBB, SVGD), omitting other competitive UQ approaches common in RUL (ensembles, deep evidential regression, Laplace approximations, SGLD/HMC variants tuned for scalability).",They propose analyzing how SVGD performance depends on the number of particles and on the kernel choice. They plan to include state-of-the-art transformer-based architectures in the comparative study. They also suggest exploring SVGD extensions such as neural variational gradient descent (which removes the need for a kernel).,"Evaluate uncertainty calibration explicitly (e.g., prediction interval coverage, sharpness, CRPS) and connect uncertainty to maintenance decision metrics (cost/availability trade-offs) rather than only prediction error. Develop approaches for domain shift and online updating (e.g., recalibrating $p_{\text{late}}$ or Bayesian fine-tuning) when operating conditions change. Extend to real industrial datasets and to settings with censored/partially observed failures or maintenance interventions, which are common in practice. Provide a self-starting or Phase I/Phase II deployment discussion (parameter estimation, normalization drift) and computational profiling for SVGD at larger model sizes and higher-frequency sensor streams.",2402.01098v1,https://arxiv.org/pdf/2402.01098v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:53:46Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical|Other,Other,TRUE,R|Other,Not provided,https://www.r-project.org,"This paper is a psychometric validation study (not reliability engineering) of the Portuguese Perceived Vulnerability to Disease Scale (PVD) during Portugal’s first COVID-19 lockdown, using a convenience sample of N=1203 collected online in April–May 2020. The authors split the sample into two equivalent halves using the SOLOMON method, then conducted EFA (ULS with polychoric correlations; promax rotation) on n=602 and CFA (WLSMV with polychoric correlations) on n=601 to test competing factor structures. A three-factor, 12-item structure (Germ Aversion, Perceived Infectability, and a new Perceived Resistance factor) fit acceptably (χ²(51)=224.320, CFI=0.922, TLI=0.899, RMSEA=0.075, SRMR=0.062) and outperformed multiple 2-factor alternatives and a 15-item 3-factor model. Items 9, 11(R), and 13(R) were removed due to low loadings, low communalities, high complexity, and cross-loadings. Internal consistency (Cronbach’s alpha) for the final factors was GA=0.67, PI=0.71, and PR=0.63, with the lower PR attributed to having only three items.","Key computations use polychoric correlation matrices and factor-analytic models estimated via ULS (EFA) and WLSMV (CFA). Reliability is assessed via Cronbach’s alpha for each subscale (GA, PI, PR) on the final 12-item solution; alpha is reported with 95% confidence intervals (e.g., GA α=0.67, PI α=0.71, PR α=0.63).","EFA supported a forced three-factor solution explaining 37% of variance (17% + 13% + 10%), with inter-factor correlations 0.11 (F1–F2), −0.22 (F1–F3), and 0.25 (F2–F3). CFA for the selected 12-item, three-factor model achieved χ²(51)=224.320, CFI=0.922, TLI=0.899, RMSEA=0.075 (90% CI [0.065, 0.085]), SRMR=0.062. Competing 2-factor models (15-, 13-, and 10-item variants) showed poorer fit (e.g., original 15-item 2-factor: CFI=0.757, RMSEA=0.113). Cronbach’s alpha for the final factors was GA=0.67 (95% CI [0.63, 0.71]), PI=0.71 (95% CI [0.67, 0.74]), and PR=0.63 (95% CI [0.58, 0.68]).","The authors note that the very specific social/sanitary context of the COVID-19 pandemic may have affected participants’ answering behavior and must be considered when using and interpreting the proposed version. They also raise concern about whether the identified factor structure will generalize to a post-pandemic context, given that pandemic-related anxiety and fear may have increased agreement with items. They additionally acknowledge the use of a convenience sample (despite being large and diverse).","The study focuses on internal structure and internal consistency but does not report test–retest reliability, measurement invariance (e.g., by gender/age/region), or differential item functioning, all important for comparability across subgroups and time. Cronbach’s alpha is used despite ordinal items; ordinal alpha or omega (e.g., McDonald’s ω based on polychoric correlations) could provide a more appropriate reliability estimate. External validity evidence is limited in the provided text (e.g., correlations with relevant health-behavior or anxiety measures), so practical predictive/criterion validity of the new PR factor remains uncertain. Because the survey was online with mandatory responses, selection bias and satisficing/response-style effects (especially with reverse-scored items) may influence factor recovery and item performance.",The authors explicitly question whether the new Portuguese version/factor structure can be used in a post-pandemic context and suggest that this issue needs consideration in future applications. They also indicate that the new Perceived Resistance factor may be explored in other studies as a predictor of health-protecting behaviors.,"Replicate the 12-item three-factor structure in independent Portuguese samples collected post-pandemic and in clinical/community panels, and formally test longitudinal measurement invariance to see whether PR is context-dependent. Evaluate criterion/predictive validity by relating GA/PI/PR to observed preventive behaviors, vaccination uptake, health anxiety, and infection history, and assess incremental validity of PR beyond GA and PI. Replace/augment Cronbach’s alpha with ordinal reliability (omega/GLB) and perform item-response theory analyses to understand item functioning (especially reverse-keyed items) and optimize short forms. Provide an open implementation (R scripts) and scoring guidance to improve reproducibility and adoption.",2402.03108v1,https://arxiv.org/pdf/2402.03108v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:54:22Z
TRUE,Life distribution modeling|Accelerated testing|Maintenance optimization|Warranty analysis|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based",Right-censored|Mixture of types|Other,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a Bayesian reliability acceptance sampling plan (RASP) for nonrepairable products sold with an optional rebate warranty, under Type-I hybrid censoring life tests. Consumer and manufacturer share the same lifetime model (exponential or Weibull) but have adversarial priors and utilities; the consumer may accept/reject without testing, accept with warranty, or require life testing before updating beliefs. The manufacturer chooses the life-test design parameters $(n,r,T_0)$ to maximize expected utility accounting for testing costs, acceptance/rejection probabilities, and expected warranty loss under a combined FRW–PRW rebate policy. For the exponential case, the authors derive analytic decision thresholds in terms of the sufficient statistic $v(x)$ and provide computable expressions for acceptance/rejection probabilities and manufacturer expected utility; for Weibull, posterior sampling and Monte Carlo are used. Numerical studies and an aircraft air-conditioning failure-time case study illustrate the optimal plans and how parameters (e.g., rejection cost, profit rate, test-running cost) change the optimal sampling design and acceptance probabilities; a “random decision-making sampling plan” variant is proposed when consumer hyperparameters are uncertain.","Type-I hybrid censoring plan $m=(n,r,T_0)$ with likelihood for exponential lifetimes $L(\theta\mid x,m)\propto \theta^{-d}\exp\{-v(x)/\theta\}$ where $v(x)=nT_0$ if $d=0$, $v(x)=\sum_{i=1}^d x_{(i)}+(n-d)T_0$ for $1\le d<r$, and $v(x)=\sum_{i=1}^r x_{(i)}+(n-r)x_{(r)}$ for $d=r$. Rebate warranty is modeled by $q(X)=cs+cw$ for $0\le X<w_1$, $q(X)=(cs+cw)\,(w_1-X)/(w_1-w_2)$ for $w_1\le X<w_2$, and $0$ for $X\ge w_2$. The consumer accepts based on posterior expected utility after testing: accept without warranty if $\int U_C(A\mid\theta)p_C(\theta\mid x,m)d\theta\le a_3$ and with warranty if that expected utility minus $\mathbb{E}_{\theta\mid x,m}\mathbb{E}_{X\mid\theta}[q(X)] + cw$ is $\le a_3$; the manufacturer maximizes expected utility over $(n,r,T_0)$ using these induced acceptance regions.","For the exponential example with parameters given in Example 1, the optimal plan is $m^*=(n^*,r^*,T_0^*)=(5,2,5.75)$ with manufacturer expected utility $\psi_M(m^*)=70.81$, acceptance/rejection probabilities $[P(A_{wo}),P(A_{w}),P(R)]=[0.18,0.24,0.58]$, expected failures $E[D]=1.40$, expected test duration $E[\xi]=3.95$, and expected warranty loss $L_w=0.09$. Sensitivity tables show, e.g., increasing consumer rejection loss $a_3$ moves the optimum toward smaller tests and can yield $(0,0,0)$ (no testing) with guaranteed acceptance under warranty; increasing manufacturer profit rate $b_1$ increases optimal sample size and reduces rejection probability. In the RDSP setting (uncertain consumer hyperparameters), one reported optimum is $(3,3,4.73)$ with $\psi_M=85.08$ and $[0.50,0.37,0.13]$. In the Weibull air-conditioning case study, the computed optimal plan is $(10,5,0.481)$ with post-test probabilities of acceptance without warranty 0.35, with warranty 0.24, and rejection 0.41.","The authors note that their framework is illustrated only for exponential and Weibull lifetime models and suggest extension to other lifetime distributions with suitable priors. They also state the model assumes negotiation between a single consumer and a single manufacturer, whereas in practice a consumer may negotiate with multiple manufacturers. They mention extension to other censoring schemes beyond the Type-I hybrid censoring used here.","The approach relies heavily on correct specification of parametric lifetime models (exponential/Weibull) and independence/iid assumptions for test units; robustness to model misspecification or dependence is not analyzed. The optimization appears to require nontrivial numerical integration/simulation (especially for Weibull and RDSP), but computational complexity, stability, and reproducibility details (software, runtime, convergence diagnostics) are not provided. The decision framework assumes the manufacturer can credibly provide life-test data and that the consumer updates beliefs as modeled; strategic misreporting or information asymmetry beyond priors/utilities is not treated. Warranty costs are modeled via expected rebate only, without considering logistics/administrative costs, time value, or field-failure process differences from test conditions.","They propose extending the methodology to other censoring schemes beyond the hybrid censoring scheme used. They also suggest extending the approach to other lifetime distributions with appropriate priors. Finally, they state that future studies will consider settings where one consumer negotiates with more than one manufacturer.","A useful extension would be robustness analysis and/or robust Bayesian designs under model misspecification (e.g., mixture lifetimes, frailty, or misspecified Weibull shape) and dependence/heterogeneity across units. Developing self-starting or data-driven/empirical-Bayes versions that reduce reliance on subjective priors (and quantifying sensitivity to adversarial priors/utilities) would improve practical adoption. Including autocorrelated/condition-monitoring or degradation data (rather than only lifetime/censoring data) could generalize the plan to modern reliability programs. Providing open-source software to compute optimal plans (including Weibull/RDSP simulation) and benchmarking against classical RASP designs under common constraints would strengthen empirical validation.",2402.09020v1,https://arxiv.org/pdf/2402.09020v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:55:08Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,R|MATLAB,Not provided,NA,"This paper proposes Spherically Normalized SVD (SpSVD), a robust and computationally scalable algorithm for approximating low-rank SVD in the presence of outliers. The method normalizes rows and columns of the input matrix to limit the influence of extreme observations, computes candidate left/right singular vectors via two reduced-rank SVDs, and then selects rank-1 factors sequentially by minimizing an element-wise L1 reconstruction objective (solved via a weighted median) with deflation. The authors introduce new breakdown point notions tailored to matrix-valued contamination (row-wise, column-wise, and block-wise) and to unit-vector/subspace outputs (using angular distances on spheres/Grassmannians), and provide theoretical robustness comparisons versus standard SVD and element-wise-loss SVD variants. Empirically, SpSVD achieves robustness comparable to RPCA in subspace recovery while being reported up to ~500× faster in wall-clock time in simulations and on a microarray gene-expression case study. Overall, the work advances robust matrix factorization methodology rather than reliability engineering.","SpSVD constructs candidates from row- and column-normalized matrices $\tilde X^{\rm row}=[x_i/\|x_i\|_2]^T$ and $\tilde X^{\rm col}=[x^{(j)}/\|x^{(j)}\|_2]$. It then defines each component by solving $\min_{d\in\mathbb R,\,u\in\mathcal U_R,\,v\in\mathcal V_R}\|X-duv^T\|_{F1}$ where $\|A\|_{F1}=\sum_{i,j}|a_{ij}|$, with $d$ obtained via a weighted median: $\min_d\sum_{u_iv_j\neq 0}|u_iv_j|\,\big|x_{ij}/(u_iv_j)-d\big|$. Subsequent components use deflation $X_r=X-\sum_{\ell<r} d^{\rm Sp}_\ell u^{\rm Sp}_\ell (v^{\rm Sp}_\ell)^T$ and search over remaining candidate sets.","In simulations with $n=200$, $p=100$, rank 3 signal plus sparse block outliers scaled up to $\eta=1000$, SpSVD and RPCA maintained subspace errors around ~15° across $\eta$, while SVD/ELSVD/R2PCP/COP degraded substantially. For top singular value estimation, the paper reports SpSVD achieving roughly $\hat d^{\rm Sp}_1/d_1\approx 1$ while RPCA yielded about $\hat d^{\rm RPCA}_1/d_1\approx 0.86$ under strong contamination. Runtime comparisons report SpSVD up to ~500× faster than RPCA (and also much faster than R2PCP) in wall-clock time under heavy outliers. On a gene-expression dataset ($n=168$ patients) with a contaminated (16,16) block, average relative error was ~1.02 for SpSVD vs ~1.01 for RPCA, compared to ~51.56 for SVD, while average runtime was ~0.13s (SpSVD) vs ~68.56s (RPCA).","The authors note their theoretical robustness analysis (Theorem 5) provides only lower bounds on breakdown points and that these bounds are not optimal. They also state that evaluating the lower bounds (e.g., computing $n_R$) is computationally impractical for large $n$ because it involves combinatorially many submatrix comparisons. They further acknowledge they did not theoretically analyze block-wise breakdown points for other robust SVD methods (e.g., RPCA, R2PCP), and they did not address rank selection under contamination.","The reported computational complexity for SpSVD is $O(npR^3)$ due to searching over candidate pairs and repeated weighted-median solves; for moderate/large target ranks this may become costly compared with $O(npR)$ partial SVD methods, yet the practical rank range where it remains competitive is not fully characterized. The method relies on row/column normalization, which can be problematic when rows/columns have near-zero norms or when magnitude (scale) carries essential signal; details on handling zero/near-zero vectors are not emphasized in the excerpt. Empirical comparisons appear to rely on specific implementations (R packages/Matlab code) and tuning choices; fairness and sensitivity to tuning/initialization across competitors is not fully established here. The approach is designed for outlier robustness, but robustness to structured noise (e.g., autocorrelation across rows/columns) or missing data is not analyzed.","The authors propose improving the theoretical analysis by developing tighter (more optimal) lower bounds for the breakdown points and doing so with better computational efficiency than the current $n_R$ evaluation procedure. They also suggest extending breakdown analyses to other robust SVD/PCA methods such as RPCA and R2PCP, noting this is technically challenging. Finally, they identify rank selection under contaminated data as an open problem left for future research.","Developing a self-starting/adaptive version that selects $R$ and/or switches between exhaustive pair search and cheaper heuristics could improve scalability for higher-rank approximations. Providing robust handling for zero/near-zero rows/columns (e.g., regularized normalization or trimming rules) and studying sensitivity to scale information would improve practical reliability across datasets. Extending SpSVD to missing-data settings (matrix completion) and temporally correlated matrices (streaming/online updates) would broaden applicability to modern large-scale pipelines. Releasing reference implementations (e.g., an R/Python package) with reproducible benchmarks would strengthen adoption and enable standardized comparisons.",2402.09754v1,https://arxiv.org/pdf/2402.09754v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:55:47Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Mixture of types|Simulated only|Other,Not applicable,Manufacturing (general)|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/ppernot/2024_RCE/releases/tag/v4.0,"The paper studies the statistical reliability of average calibration diagnostics for uncertainty quantification (UQ) in machine-learning regression, focusing on two common approaches: RCE (based on comparing RMSE/MSE to mean predicted variance) and ZMS (mean squared z-scores, linked to NLL). Using nine published ML-UQ datasets plus an additional collection of 33 materials-property datasets, and supplemented by synthetic Monte Carlo experiments, it shows that heavy-tailed uncertainty and/or error distributions make estimates of MV, MSE and their bootstrap confidence intervals unreliable—often causing RCE and ZMS to give conflicting calibration conclusions. The authors find RCE is particularly sensitive to the upper tail/outliers of the uncertainty distribution, whereas ZMS is generally less sensitive but can still fail when z-scores are themselves heavy-tailed. They propose robust, distribution-agnostic tailedness screening metrics (notably a robust skewness statistic βGM) with indicative thresholds beyond which calibration-statistic reliability should be questioned, and discuss mitigation options such as iterative learning, qualitative/graphical diagnostics, or moving from variance-based UQ to prediction intervals/distributions.","The key calibration model is $E_i \sim u_{E_i} D(0,1)$, implying average calibration under unbiased errors via $\langle E^2\rangle = \langle u_E^2\rangle$. The Relative Calibration Error is $\mathrm{RCE}=(\mathrm{RMV}-\mathrm{RMSE})/\mathrm{RMV}$ with target 0, where $\mathrm{RMSE}=\sqrt{\langle E^2\rangle}$ and $\mathrm{RMV}=\sqrt{\langle u_E^2\rangle}$. The z-score approach uses $Z_i=E_i/u_{E_i}$ and tests $\mathrm{ZMS}=\langle Z^2\rangle=1$ (equivalently via NLL for Gaussian likelihood: $\mathrm{NLL}=\tfrac12(\langle Z^2\rangle+\langle\ln u_E^2\rangle+\ln 2\pi)$).","Monte Carlo experiments on synthetic calibrated data show RCE becomes significantly biased for heavy-tailed squared-error distributions (reported onset around $\beta_{GM}(E^2)\gtrsim 0.85$), while ZMS remains better behaved except for extremely heavy-tailed generative errors (e.g., $\nu_D\approx 3$, corresponding to $\beta_{GM}(Z^2)\gtrsim 0.8$). Bootstrap validation coverage degrades under heavy tails: for an NIG scenario ZMS maintains near-nominal validation probability (~0.95) even for extreme uncertainty tails, whereas RCE’s validation probability drops below ~0.8 for very heavy-tailed uncertainties (e.g., $\nu_{IG}=2$). Under heavy-tailed generative errors (TIG scenario), both RCE and ZMS validation probabilities can fall substantially (reported to ~0.65 at $\nu_D=2.1$) because bootstrap CIs become too narrow. On the nine real ML-UQ datasets, RCE and ZMS disagree on the calibration decision for more than half of the sets; disagreement aligns strongly with heavy-tailed uncertainty distributions (notably $\beta_{GM}(u_E^2)\ge 0.6$), and a “remove top 5% largest uncertainties” decimation reduces disagreement for several datasets.",None stated.,"The work is centered on variance-based UQ calibration (RCE/ZMS/NLL) and its bin-based relatives, so conclusions may not transfer to interval-based or full predictive-distribution calibration without additional study. The empirical thresholds proposed for $\beta_{GM}$ (e.g., 0.6 and 0.8) are presented as indicative and derived from the studied distribution families/datasets; their universality across other domains, sample sizes, or dependence structures (e.g., autocorrelation) is not established. The analysis relies heavily on bootstrap CI behavior; alternative robust CI methods for heavy-tailed means (e.g., subsampling, robust M-estimation CIs, Bayesian heavy-tail models) are not systematically benchmarked here.","The author states that conditional calibration statistics (e.g., UCE/ENCE and local ZMS) will be further explored in a forthcoming article, motivated by the expectation that heavy-tailed error distributions can similarly undermine their reliability. The paper also suggests exploring approaches that move away from variance-only uncertainty metrics toward prediction intervals or predictive distributions to better handle heavy tails and restore a sound validation framework.","A useful extension would be a systematic comparison of alternative confidence-interval constructions for heavy-tailed mean-square quantities (e.g., m-out-of-n bootstrap, subsampling, robustified bootstrap, or parametric heavy-tail models) to quantify when each achieves nominal coverage. Another direction is developing self-diagnostic tooling that jointly reports calibration statistics with automated tail/outlier diagnostics and influence measures (beyond simple decimation) to guide practitioners on whether results are trustworthy. Finally, testing robustness under realistic dataset issues common in ML (heteroscedasticity, dependence, data leakage, and distribution shift between calibration and test sets) would strengthen practical guidance.",2402.10043v5,https://arxiv.org/pdf/2402.10043v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:56:35Z
FALSE,NA,ML-based|Simulation-based|Other,Simulated only|Sensor/condition monitoring|Other,Predictive|Condition-based|Other,Healthcare/medical,Simulation study,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/azminewasi/NeuralCGMM,"This tiny paper proposes NeuralCGMM, an end-to-end neural control system for continuous glucose monitoring and automated insulin delivery using Differentiable Predictive Control (DPC). The method combines a differentiable glucose–insulin dynamics model (including ODE-solver-based rollouts / a discrete-time state-space model formulation) with a neural network policy trained by stochastic gradient descent to track target glucose references under safety/feasibility constraints on insulin and glucose. Experiments are conducted using the Neuromancer framework on synthetically generated scenarios, demonstrating the controller’s ability to adapt to evolving constraints and disturbances over long simulations (e.g., 3000 steps). The work is positioned as a foundational framework for closed-loop glucose management rather than a reliability engineering contribution, and it notes the need for clinical validation beyond synthetic data. Code and data are provided via a public GitHub repository.","The control objective is posed as a parametric optimal control problem minimizing a weighted sum of tracking error and control smoothness over a horizon: $\min_\theta \sum_{i=1}^m\sum_{k=1}^{N-1} Q_g\lVert g_k^i-r_k^i\rVert + Q_N\lVert g_N^i-r_N^i\rVert + Q_u\lVert u_k^i-u_{k-1}^i\rVert$. Dynamics and policy are enforced via differentiable closed-loop constraints, e.g., $g_{k+1}^i=\mathrm{ODESolve}(f(g_k^i,u_k^i))$ and $u_k^i=\pi_\theta(g_k^i,R^i)$ (with insulin/glucose constraints $u\in U,\ g\in G$). Gradients are computed by backpropagation through time, with a chain-rule expression for $\partial J/\partial W$ over state and control sensitivities.","Evaluation is simulation-based using synthetic data generated with a 100-step prediction horizon, batches of 64 scenarios, and a test set of 3000 datapoints/steps. The paper qualitatively reports that the learned policy keeps the regulated signal within specified limits while respecting control constraints, and that the disturbance signal is captured well in the presented test visualizations. Training uses an MLP policy (2 hidden layers, 32 units each, GELU) optimized with AdamW (lr 0.001) for up to 200 epochs with warmup 50 and early stopping after 5 steps without improvement. No standard reliability metrics (e.g., MTBF, hazard, survival) are reported; results focus on control performance under simulated disturbances and constraints.","The authors explicitly acknowledge that the study relies on synthetically generated data, which may not represent real-world clinical intricacies, and they state that errors in a clinical procedure could lead to patient deaths. They note inability to test with real-world data due to limited research/availability for active glucose maintenance. They also cite practical obstacles: extensive calibration, complexity and cost/time of rigorous clinical testing, and that unexpected external factors and technological limitations can affect prediction precision/dependability.","The work does not quantify robustness to sensor noise, delays, missing data, calibration drift, or patient-model mismatch, all of which are central to real CGM/insulin delivery deployments. Comparisons against established baselines (e.g., classical MPC, PID, or standard artificial pancreas control algorithms) are not clearly reported with quantitative metrics, limiting claims of improvement. The safety discussion is largely qualitative; formal verification, worst-case guarantees under bounded disturbances, and explicit hypoglycemia-risk metrics are not demonstrated. Generalization across heterogeneous patient populations is not empirically validated because experiments are synthetic and appear limited in scenario diversity.","The authors state that the approach requires extensive research and development with clinical objectives, including rigorous testing, meticulous calibration, and thorough clinical trials, to develop a practical product. They frame the presented system as a foundational framework intended to enable further exploration and clinical studies on active glucose level maintenance. They also express a goal of developing a refined device that can positively impact many patients worldwide.","A natural extension is to evaluate NeuralCGMM on real CGM/insulin datasets and/or in-silico validated diabetes simulators with standardized benchmarks and safety metrics (time-in-range, hypoglycemia events). Developing robust/self-starting variants that explicitly handle sensor noise, delays, dropouts, and calibration drift would improve practical deployability. Head-to-head quantitative comparisons versus conventional MPC/PID and modern RL-based controllers, including ablations of loss terms and constraint handling, would strengthen evidence. Incorporating formal safety layers (e.g., control barrier functions or predictive safety filters with explicit constraint satisfaction guarantees) and reporting worst-case performance under bounded disturbances would better address clinical risk.",2402.13852v3,https://arxiv.org/pdf/2402.13852v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:57:11Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/young-geng/CQL,"The paper studies offline decision-making in stochastic multi-armed bandits under extreme data scarcity (as little as one sample per arm) and argues that optimizing over stochastic policies can yield reliable performance where standard deterministic-selection methods fail. It proposes TRUST (Trust Region of Uncertainty for Stochastic policy enhancemenT), a trust-region policy optimization method centered on a reference (behavioral cloning) policy and using relative pessimism on the policy improvement vector rather than absolute value pessimism. The key technical device is selecting the trust-region radius via a problem-dependent critical radius defined through a quantile of a localized Gaussian complexity (supremum of a Gaussian process), estimated in practice by Monte Carlo. Theoretical results provide high-probability lower bounds on the returned policy value and suboptimality bounds against any comparator stochastic policy. Experiments on synthetic bandits and a reduction of offline RL with known logging policies show TRUST can outperform LCB/greedy in data-starved settings and offer tighter lower-bound guarantees; it also compares favorably to CQL on selected D4RL tasks under the authors’ data-generation protocol.","LCB uses empirical mean $\hat r_i$ and half-width $b_i=\sqrt{\tfrac{2\sigma_i^2}{N(a_i)}\log(\tfrac{2d}{\delta})}$, selecting $\arg\max_i(\hat r_i-b_i)$. TRUST defines a trust region over improvements $\Delta=w-\hat\mu$ with constraint $\sum_i \Delta_i^2\sigma_i^2/N_i\le \varepsilon^2$, then solves $\hat\Delta_\varepsilon=\arg\max_{\Delta\in C(\varepsilon)}\Delta^\top\hat r$. The radius is chosen by a critical-radius objective $\hat\varepsilon^*=\arg\max_{\varepsilon\in E}[\hat\Delta_\varepsilon^\top\hat r-G(\varepsilon)]$ where $G(\varepsilon)$ upper-bounds $\sup_{\Delta\in C(\varepsilon)}\Delta^\top\eta$ with high probability and is estimated via Monte Carlo.","On a synthetic 10,000-arm “data-starved” bandit with 1 sample/arm (half arms Uniform(0.5,1.5) vs half Normal(0,1/4)), TRUST achieved policy value 0.92 with a computed lower bound around 0.6, while LCB selected an arm with mean about 0 and had a vacuous lower bound around −1.5; the uniform behavioral policy scored about 0.5. On a 1,000-arm Gaussian bandit with one sample/arm, LCB and TRUST had similar mean reward (~0.718 vs ~0.725 over 8 seeds), but TRUST produced a much higher mean lower bound (~0.544 vs ~0.156) and more stable outcomes (variance 0.038 vs 0.265; minimal reward 0.658 vs 0.239). In the authors’ offline RL reduction with known logging policies (100 policies × 1 trajectory each), TRUST matched or exceeded CQL on several D4RL tasks (e.g., Hopper 1-traj-low 999 vs 499; HalfCheetah 1-traj-high 10380 vs 9067).","The RL application relies on a reduction from MDPs to bandits that disregards sequential structure (no “trajectory stitching”), so it cannot exploit temporal compositionality of offline RL. The approach in the RL experiments assumes the logging policies that generated the dataset are known and accessible. The authors also note their goal is not to beat state-of-the-art deep offline RL broadly, as the method is designed for bandits.","The core bandit analysis assumes independent Gaussian (or sub-Gaussian) noise and i.i.d. logging actions, which may not hold for many real offline decision logs with dependence, nonstationarity, or heavy tails. TRUST requires solving multiple second-order cone programs across candidate radii and repeated Monte Carlo supremum computations, which could be computationally heavy at large scale without careful engineering, and the paper does not provide end-to-end open-source code for TRUST itself. The empirical RL evaluation uses a custom dataset generation pipeline (SAC snapshots with 1 trajectory each) rather than standard D4RL logs, which may limit comparability to prior offline RL benchmarks and conclusions.","The authors suggest the concepts used here (stochastic policy search, localized Gaussian complexity, and critical-radius trust-region selection) could be applied more broadly to design truly sample-efficient algorithms and enable decision-making in settings where high sample efficiency is critical.","Extending TRUST to settings with unknown or partially known logging policies (e.g., estimated propensities) would broaden applicability beyond the assumed-known case. Developing robust variants for heavy-tailed rewards, non-i.i.d. logged data, and contextual bandits/MDPs with function approximation would test the method under more realistic data conditions. Providing an open-source, optimized implementation (including SOCP and Monte Carlo routines) and benchmarking against additional offline bandit/RL baselines under standardized datasets would strengthen reproducibility and practical adoption.",2402.15703v1,https://arxiv.org/pdf/2402.15703v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:57:53Z
TRUE,Life distribution modeling|System reliability|Other,"Simulation-based|Parametric (Weibull, etc.)|Other",Simulated only,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,In text/Appendix|Public repository (GitHub/GitLab),https://colab.research.google.com/drive/1lGRNs6Q_zUGjtwqXkqtUbsQAKWhmfgfu?usp=sharing,"The paper studies how statistical dependence between component lifetimes changes the reliability of redundant M-out-of-N (MooN) architectures, highlighting that independence assumptions can misstate redundancy benefits. It proposes three minimal dependency models for component times-to-failure: (i) a linear mixture model using a shared covariate, (ii) a global common-cause-failure (CCF) model where all components may fail simultaneously, and (iii) a marginal CCF model where only subsets of components may fail simultaneously. Using Monte Carlo simulation, it evaluates 1oo3 (parallel), 2oo3 (voting), and 3oo3 (series) systems across dependence strength p∈[0,1] and compares PDFs, reliability functions, and location statistics (mean/median/mode). Results show dependence degrades reliability for parallel 1oo3 but can improve reliability for series 3oo3, while 2oo3 exhibits mixed/nonlinear behavior depending on the dependence model and statistic. An R-based online simulation platform is provided to let designers explore tailored scenarios and visualize reliability impacts of dependence.","Component lifetimes are generated from i.i.d. baseline variables $X_0,\dots,X_N$ (typically Weibull; the experiments use shape $k=1$ i.e. exponential), and dependent lifetimes are $Y_k$. MooN system time-to-failure is defined as the $M/N$ quantile of $Y$: $T=q_{M/N}(Y)$. Dependence models: linear $Y_k=(1-p)X_k+pX_0$; global CCF $Y_k=(1-\xi)X_k+\xi X_0$ with $\xi\sim\mathrm{Bernoulli}(p)$ (all fail together vs. all independent); marginal CCF $Y_k=(1-\xi_k)X_k+\xi_k X_0$ with i.i.d. $\xi_k\sim\mathrm{Bernoulli}(p)$ (component-wise CCF).","Across all three dependence models, increasing dependence $p$ systematically reduces reliability for the parallel 1oo3 system and improves reliability for the series 3oo3 system (e.g., higher mean/median/mode times-to-failure for 3oo3 as $p$ increases). The 2oo3 architecture shows intermediate/mixed effects: for the linear model, mean time-to-failure improves with $p$ while the modal value deteriorates and the median shows a nonlinear unimodal relationship; for the CCF models, the 2oo3 median is reported as invariant with $p$ while mean and mode can move in opposite directions (e.g., marginal CCF: mode improves, mean deteriorates). For linear and global CCF models, the mean time-to-failure varies linearly with $p$ (shown via derived expectation expressions), whereas for marginal CCF the mean becomes nonlinear in $p$ (concave decreasing for 1oo3, sigmoidal for 2oo3, convex increasing for 3oo3). The simulation study is large-scale: about $10^7$ simulated system lifetimes per configuration, with 20 $p$-values and three architectures/models (noted as ~1.8 billion random draws).","The authors note that the compact/general formulation of the dependency models makes mapping them to specific real systems difficult. They also state the analysis is restricted to N=3 (1oo3, 2oo3, 3oo3) and that the online tool is limited to simple MooN architectures; more complex combinations (e.g., serial-parallel of MooN subsystems) require code adaptation. Additionally, for simplicity the simulated component lifetimes are based on identical Weibull distributions with shape parameter 1 (exponential), limiting time-dependent failure-rate effects.","The dependence structures are stylized (linear mixture and Bernoulli-switch CCF) and may not capture realistic tail dependence, load sharing, or time-varying/environment-driven correlations seen in field data. Results rely entirely on simulation with assumed identical lifetime distributions (and largely k=1), with no calibration/validation against empirical failure or degradation datasets, so practical effect sizes may differ. The MooN lifetime is taken as an order statistic/quantile of component lifetimes without modeling repair, diagnostics, coverage, switching logic, or dependent detection/latent faults that often drive safety architecture performance. Sensitivity to the choice of kernel density estimation, sample size, and discretization of $p$ (20 points) is not discussed, nor are confidence intervals for estimated statistics.","The authors propose extending the analysis to other reliability indicators (e.g., hazard/failure rate) and additional distributional statistics (standard deviation, skewness, kurtosis), and further investigating the modal value behavior when the median is unaffected in the CCF models. They suggest studying larger and more complex MooN configurations (e.g., 2oo4, 3oo4) and combinations in serial-parallel/parallel-serial structures to better match real systems. They also propose using different lifetime distributions (time-dependent effects) and allowing component-specific parameters, and exploring dynamic stochastic-process-based approaches to capture time-dependent effects such as standby systems and other complex redundant architectures.","A natural extension is to fit/estimate the dependence parameter(s) from observed fleet or test data (e.g., via copulas, frailty/shared-environment models, or hierarchical Bayesian inference) and quantify uncertainty in reliability metrics. Incorporating diagnostic coverage, imperfect fault detection, and repair/maintenance (availability modeling) would make results more actionable for safety architectures where latent faults and repair policies dominate risk. Extending to multivariate/time-series dependence (autocorrelated environments, common shocks, covariate-driven hazards) and to heterogeneous components (different baseline distributions and dependence strengths) would improve realism. Providing an open-source R package (beyond embedded code) with reproducible benchmarks and confidence intervals for simulation estimates would strengthen adoption and comparability to established CCF methods (e.g., β-factor, MGL) and reliability block diagram/fault-tree analyses.",2402.18187v1,https://arxiv.org/pdf/2402.18187v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:58:42Z
FALSE,NA,ML-based,Sensor/condition monitoring|Other,Not applicable,Manufacturing (general),Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/arthur-thuy/reject,"The paper studies uncertainty quantification for industrial image classification under distribution shift, comparing deep ensembles with more computationally efficient neural-network ensembling techniques (snapshot, batch, and MIMO). Using the SIP-17 synthetic industrial parts dataset, the authors evaluate in-distribution accuracy and calibration (negative log-likelihood) and assess out-of-distribution behavior via uncertainty decomposition (total/aleatoric/epistemic), classification-with-rejection curves, and diversity-based diagnostics. They introduce a new Diversity Quality metric (DQ1, and its weighted variant DQβ) to combine desired low in-distribution diversity with high OOD diversity into a single score. Empirically, batch ensembles match deep ensembles in accuracy and uncertainty quality while substantially reducing training time, inference time, and memory compared to deep ensembles; snapshot and MIMO lag due to poorer predictive performance and/or weaker epistemic separation on OOD. The work is positioned as improving model trustworthiness/reliability in deployment, but it is not reliability engineering in the classical sense (lifetime/degradation/maintenance modeling).","Ensemble predictive distribution is the average of member softmax outputs: $p(y^*\mid x^*)\approx \frac{1}{M}\sum_{m=1}^M p(y^*\mid x^*,\theta_m)$. Uncertainty is decomposed using predictive entropy: total uncertainty $TU(x^*)\approx H[\frac{1}{M}\sum_m p(y^*\mid x^*,\theta_m)]$, aleatoric uncertainty $AU(x^*)\approx \frac{1}{M}\sum_m H[p(y^*\mid x^*,\theta_m)]$, and epistemic uncertainty $EU(x^*)=TU(x^*)-AU(x^*)$. The proposed Diversity Quality score is the harmonic mean of $(1-IDD)$ and $OODD$: $DQ_1=2\cdot\frac{(1-IDD)\,OODD}{(1-IDD)+OODD}$ (generalized to $DQ_\beta$).","On the in-distribution test set, deep ensemble (M=8) achieves 98.53% accuracy with NLL 0.0440, while batch ensemble (M=8) achieves 98.65% accuracy with NLL 0.0504—close to deep ensemble and better than the single NN (97.72%, NLL 0.1047). Snapshot (M=8) and MIMO (M=3) underperform on ID accuracy (97.15% and 96.63%, respectively) with NLL near the single NN. For OOD handling, deep and batch ensembles show markedly higher epistemic uncertainty on OOD than on ID, whereas snapshot and MIMO increases are driven more by aleatoric uncertainty. Computationally (relative to a single NN), deep/snapshot incur ~8× evaluation and ~8× memory (for M=8), while batch and MIMO are only modestly higher (reported around ~1.85× training and ~1.5× training; ~1.5× evaluation for batch and ~1.15× for MIMO; batch has ~10% more parameters). Overall, batch ensemble provides the best cost–performance trade-off, matching deep ensemble uncertainty/accuracy at a fraction of compute and memory.",None stated.,"The study’s OOD evaluation relies on a synthetic dataset (SIP-17) and OOD defined as different object classes from different use cases; conclusions may not transfer to real-image domain shifts (sensor noise, backgrounds, occlusions) without sim-to-real validation. The rejection/OOD analysis uses uncertainty and diversity proxies rather than explicit OOD ground truth or standardized OOD detection metrics (e.g., AUROC/FPR@95) across multiple shift types. Results are demonstrated with a small CNN (LeNet-5, 32×32 grayscale); behavior and efficiency trade-offs may differ for larger modern architectures (ResNets/ViTs) and higher-resolution inputs typical in industrial vision.","The authors suggest validating the findings on industrial parts classification datasets with more object classes to better emulate real-world inventories. They propose exploring uncertainty-aware ensembles in other operations-research applications (e.g., waste sorting or production quality control) where new product types/failure modes appear. They also propose investigating sim-to-real setups where models trained on synthetic images are evaluated on real images, which introduces inherent distribution shift and requires reliable uncertainty quantification.","Extend the comparison to broader OOD/shift benchmarks (covariate shifts, corruption robustness, near-OOD vs far-OOD) and report standardized OOD detection metrics alongside rejection curves. Study robustness to deployment constraints such as temporal correlation, changing class priors, and calibration under drift, including online monitoring and re-calibration strategies. Provide open-source training/evaluation scripts (beyond the DQ/rejection package) and ablations on ensemble size, architecture scale, and threshold selection to improve reproducibility and deployment guidance.",2403.10182v5,https://arxiv.org/pdf/2403.10182v5.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:59:24Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical|Other,Other,TRUE,None / Not applicable,Not provided,NA,"The paper critiques the reproducibility and practical usefulness of the gender Implicit Association Test (gIAT) for explaining female–male differences in high-ability careers (e.g., STEM and academic medicine). Using publicly available meta-analysis datasets (Kurdi et al. 2019; Kurdi & Banaji 2019), the authors compute study-level p-values for implicit–criterion, explicit–criterion, and implicit–explicit correlations via Fisher’s Z transformation and visualize them with p-value plots. They report that the rank-ordered p-values for these gIAT-related correlations are all > 0.05, arguing this does not support real associations between gIAT scores and criterion (real-world) behaviors. In contrast, using effect sizes from Su et al. (2009), they compute p-values for sex differences in 11 vocational-interest dimensions and show most are < 0.05, supported by both p-value and volcano plots. The authors conclude vocational interests (and other omitted confounders) are more predictive than gIAT measures for gender composition in high-ability careers and recommend expanding models to include relevant covariates to avoid omitted-variable bias.","The paper converts correlation coefficients to significance levels using Fisher’s Z transformation: $Z=\operatorname{arctanh}(r)=0.5\ln\frac{1+r}{1-r}$ with standard error $SE=\sqrt{1/(n-3)}$, then converts $Z/SE$ to p-values via the standard normal distribution. For the vocational-interest meta-analysis, it estimates $SE$ from reported confidence intervals as $SE=(UCI-LCI)/3.92$ and computes $Z=d/SE$ (assuming normality) before converting to p-values.","For the gIAT gender meta-analysis subset (27 studies; total n=535 with ICC/ECC/IEC available), the rank-ordered p-values for ICC, ECC, and IEC are reported to all exceed 0.05, which the authors interpret as no supported associations between gIAT/explicit measures and criterion behaviors. For the vocational interests meta-analysis (11 dimensions), most p-values are shown as <0.05; the volcano plot highlights Social and Artistic (favoring women) and Realistic, Things–People, and Engineering (favoring men) as large effects. In illustrative normal-distribution examples using Su et al.’s Things–People effect size $d\approx0.93$, the male:female ratio above +1 SD is reported as 5.9 and above +2 SD as 13. Using 2016 SAT math score distributions (female mean lower; d≈−0.248), the male:female ratio above +1 SD is reported as 1.8 and above +2 SD as 3.0.",None stated,"The work is not reliability engineering; “reliability” is used in the sense of reproducibility/measurement reliability in psychology, so the methods and conclusions do not translate to engineering reliability (failure/degradation) contexts. The inference relies heavily on p-value plots and dichotomization around 0.05, with limited emphasis on effect sizes, heterogeneity modeling, or meta-analytic estimators; this can overstate conclusions about “no association.” The procedure averages within-study correlations and then converts to p-values, which may alter variance/weighting relative to standard meta-analysis practice and may not reflect the original meta-analytic model assumptions. No implementation details (software, scripts) are provided, limiting computational reproducibility of the authors’ re-analysis.","The authors suggest that researchers studying implicit bias should expand modeling to include additional relevant covariates (e.g., vocational interests) to address omitted-variable bias when explaining gender differences in high-ability careers.","Provide fully reproducible analysis code and a transparent workflow (data cleaning, inclusion/exclusion rules, weighting) to enable direct verification of the re-analysis. Extend the assessment beyond p-value plots by re-estimating meta-analytic effects with heterogeneity models (e.g., random effects, moderator analysis) and reporting prediction intervals. Evaluate robustness to alternative specifications (e.g., not averaging correlations within studies; using Bayesian meta-analysis) and to publication-bias diagnostics. Validate claims with additional real-world datasets linking measured interests, explicit attitudes, and objective career outcomes while controlling for key confounders.",2403.10300v2,https://arxiv.org/pdf/2403.10300v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T13:59:54Z
TRUE,System reliability|Other,ML-based|Bayesian|Simulation-based|Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB,Not provided,http://www.uqlab.com/userguide-reliability,"The paper develops theoretically optimal active-learning strategies for adaptive Kriging/Gaussian Process Regression (GPR) surrogate modeling in reliability analysis, targeting efficient estimation of failure probability $P_f$ with minimal evaluations of an expensive limit-state/performance function $g(\mathbf{x})$. It derives optimal learning functions by defining optimality as maximizing the reduction in the variance of the estimated failure probability after enriching the training set, considering two settings: (i) neglecting correlations among candidate (Monte Carlo) samples and (ii) accounting for Kriging-induced correlations. The authors show that the classical AK-MCS U-function can be reformulated as an optimal learning function in the correlation-neglected case, and they derive expressions for Bernoulli correlation induced by correlated GP outputs to construct a correlation-aware optimal strategy. They further extend the framework to sequential enrichment of multiple samples (parallel learning) using Bayesian estimators under different loss functions (MMSE/MMAE/MAPE). Simulations on benchmark problems (e.g., modified Rastrigin) indicate the correlation-aware optimal strategy reduces the number of performance-function calls versus several literature learning functions, but it is computationally demanding due to covariance-matrix handling and bivariate normal CDF calculations.","Failure probability is posed as $P_f=\int_{g(\mathbf{x})\le 0}\rho(\mathbf{x})\,d\mathbf{x)=\int I_{g(\mathbf{x})}\rho(\mathbf{x})\,d\mathbf{x}$. The Kriging/GPR surrogate is $\hat g(\mathbf{x})=\mathbf{f}^T(\mathbf{x})\beta+Z(\mathbf{x})$ with Gaussian kernel correlation $R(\mathbf{x}_i,\mathbf{x}_j)=\prod_{n=1}^N\exp(-\theta_n(x_{in}-x_{jn})^2)$ and predictive mean/variance given by standard BLUP forms. The optimal learning rule without candidate-sample correlation selects $\mathbf{x}^*=\arg\max_{\mathbf{x}_i\in S}\Phi(-\mu_{\hat g}(\mathbf{x}_i)/\sigma_{\hat g}(\mathbf{x}_i))\,\Phi(\mu_{\hat g}(\mathbf{x}_i)/\sigma_{\hat g}(\mathbf{x}_i))$, and the correlation-aware rule selects $\mathbf{x}^*=\arg\max_{\mathbf{x}_i\in S}\{2\sum_k \rho^b_{i,k}\sigma_b(\mathbf{x}_i)\sigma_b(\mathbf{x}_k)-\sigma_b^2(\mathbf{x}_i)\}$ where $\sigma_b^2(\mathbf{x})=\Phi(-\mu/\sigma)\Phi(\mu/\sigma)$ and $\rho^b_{i,j}$ is computed from a bivariate normal CDF of correlated GP outputs.","On the modified Rastrigin benchmark with $10^4$ candidate samples and stopping criterion $\sigma_{\tilde P_f}/\mu_{\tilde P_f}\le 10^{-3}$, the proposed correlation-aware optimal strategy ($\mathbb{L}^{wco}_{opt}$) achieved an average of $12+300.35$ performance-function evaluations with COV of $N_{call}=0.1389$ and mean relative error $\bar\epsilon=0.0038$. Competing learning functions reported averages of $12+333.20$ (EFF), $12+338.78$ (U), $12+402.57$ (H), $12+324.26$ (LIF), $12+317.58$ (REIF), and $12+359.94$ (FNEIF), with all methods yielding $\bar\epsilon<0.01$ in this example. The optimal strategy neglecting correlation ($\mathbb{L}^{nco}_{opt}$) matches the U-function results exactly ($12+338.78$), empirically supporting the paper’s theorem that U is optimal under the independence assumption. Parallel enrichment (e.g., $N_{para}=3,6,9,12$) substantially reduced iterations (e.g., about 104.3 down to 32.5 iterations under MMSE) while keeping total function calls roughly similar, illustrating improved wall-clock efficiency when multiple evaluations can be run concurrently.","The authors state that implementing the correlation-aware optimal learning strategy is “intensively computational demanding,” requiring very large computational resources, partly due to storing/updating large covariance matrices and computing multivariate/bivariate normal integrals. They also note that choosing the number of points to enrich in parallel (how many computing resources to allocate) is important to avoid computational waste, but an appropriate definition/selection rule for this is not explored in the paper.","The work is demonstrated primarily on benchmark/simulated problems; generalization to real engineering systems with model-form error, nonstationarity, or noisy observations is not established. The methodology assumes a Gaussian-process surrogate with specific kernel choices and relies on normality-based probabilistic classification; robustness to misspecified priors/kernels, non-Gaussian outputs, or strong input dependence is not systematically analyzed. The correlation-aware rule involves estimating many pairwise correlations across the candidate set, which may be impractical for high-dimensional inputs or very large $N_{MCS}$ without further approximation (e.g., sparsification/low-rank/nearest-neighbor GP), and fairness of comparisons may depend on implementation details (candidate set size, hyperparameter optimization settings). The stopping criterion is based on the surrogate-induced estimator variance; there is limited discussion of bias control or guarantees on $P_f$ accuracy when the surrogate is imperfect near the limit state.","They suggest that further investigation is needed into the computational resources required by the correlation-aware optimal strategy, including issues tied to storing covariance matrices. They also explicitly note that defining an appropriate number of training points for multiple-sample (parallel) enrichment is necessary but is not explored in this paper.","Develop scalable approximations for the correlation-aware strategy (e.g., low-rank/sparse covariance, inducing points, local GP, or screening candidate sets) so it can be applied to large $N_{MCS}$ and higher-dimensional problems. Extend the framework to handle noisy/heteroscedastic responses and model discrepancy, and test on real engineering case studies (e.g., structural components, aerospace, energy systems) to validate practical reliability gains. Provide adaptive/self-tuning rules for batch size in parallel enrichment that balance iteration reduction with total function-call efficiency, and integrate cost-aware or multi-fidelity evaluations. Analyze robustness to kernel/hyperparameter misspecification and explore alternative classification/link functions beyond Gaussian CDF-based probabilistic classification for non-Gaussian limit-state uncertainty.",2403.11125v2,https://arxiv.org/pdf/2403.11125v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:00:49Z
TRUE,System reliability|Other,"Stochastic process|Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Other",Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Approximation methods|Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper develops FORM-based (First-Order Reliability Method) expressions for global variance-based reliability sensitivity indices (first-order and total-effect) for systems with multiple failure modes, including series, parallel, and general systems defined via cut sets. The key idea is to compute Sobol’-type reliability indices from the variance decomposition of the failure indicator without additional expensive model evaluations beyond those needed for a FORM system analysis. For series and parallel systems, the required conditional-variance terms are expressed as 2m-dimensional multivariate normal integrals involving the FORM correlation structure among linearized limit-state functions; for general systems, inclusion–exclusion is used to combine parallel subsystems. Numerical examples (linear illustrative system, elastoplastic frame series system, a two-design-point parabolic limit state, and a cantilever beam-bar cut-set system) show that the proposed approximations closely match Monte Carlo pick-freeze estimates, with noted sensitivity to multivariate normal integration accuracy when component correlations are very high. The work extends prior FORM-based sensitivity approximations from single dominant failure modes to multi-mode system reliability problems.","Each component failure event is linearized in standard normal space as $F_{1,i}=\{\alpha_i U \ge \beta_i\}$, giving $p_{F1,i}=\Phi(-\beta_i)$. For a series system, $p_{F1,ser}=1-\Phi_m(B,R)$ and for a parallel system $p_{F1,par}=\Phi_m(-B,R)$, where $B=[\beta_1,\ldots,\beta_m]$ and $R=A A^T$ with rows $\alpha_i$. The main conditional-variance terms used in Sobol’ reliability indices are given by $\mathrm{Var}(\mathbb{E}[Z_{1,ser}\mid U_v])=\Phi_{2m}([B;B],[[R,R_v],[R_v,R]])-(1-p_{F1,ser})^2$ and $\mathrm{Var}(\mathbb{E}[Z_{1,par}\mid U_v])=\Phi_{2m}(-[B;B],[[R,R_v],[R_v,R]])-p_{F1,par}^2$, with $R_v=A_v A_v^T$ built from the columns of $A$ indexed by $v$; total-effect indices use the same forms with $R_v$ replaced by $R-R_v$.","Elastoplastic frame (4 modes, lognormal inputs): Monte Carlo $p_F\approx 5.32\times 10^{-4}$ vs FORM $p_{F1}\approx 5.57\times 10^{-4}$; total-effect indices matched closely (e.g., load $S_T\approx 1.00$ in both), while some first-order indices for plastic moments were overestimated by FORM though with similar absolute errors. Two-design-point parabolic component: Monte Carlo $p_F\approx 3.02\times 10^{-3}$ vs FORM system approximation $p_{F1}\approx 2.82\times 10^{-3}$; indices agreed well (e.g., $S_{F,U1}\approx 0.437$ MC vs 0.430 FORM; $S^T_{F,U2}\approx 0.564$ MC vs 0.571 FORM). Cantilever beam-bar general system with Gaussian inputs: sensitivity indices from FORM matched Monte Carlo up to sampling noise (e.g., $S_{F,M}\approx 0.654$ and $S^T_{F,M}\approx 0.939$ for both). With non-Gaussian strengths (lognormal) the agreement remained good but FORM slightly overestimated first-order effects and underestimated the total-effect of $M$ (e.g., $S^T_{F,M}\approx 0.426$ MC vs 0.364 FORM).","The authors note that the proposed expressions require evaluating a $2m$-dimensional multivariate normal integral (with $m$ system components), and available algorithms may need very large sample sizes when $m$ is large and component correlations are high; this was observed in examples with strongly correlated failure modes. They also emphasize that the paper focuses on independent inputs, with dependent-input handling requiring an external transformation approach (Rosenblatt-based method from prior work). For general systems, accurate estimation requires linearizing components at appropriate joint design points for each parallel subsystem; using only individual component design points can be suboptimal.","The method inherits standard FORM limitations: accuracy can degrade for highly nonlinear/non-smooth limit states, multiple important design points not fully captured, or when the linearization point is hard to compute reliably. Computing Sobol’ indices for many input variables requires evaluating $\mathrm{Var}(\mathbb{E}[Z\mid U_v])$ for many subsets (or at least many singletons/complements), which can still be computationally heavy due to repeated high-dimensional multinormal integral evaluations even if no new model calls are needed. The approach is framed around variance-based indices, which can be unstable or hard to interpret when failure probabilities are extremely small (variance $p_F(1-p_F)$ becomes tiny) and may be sensitive to numerical error in integral estimation.","They suggest deriving more computationally efficient approximations of the FORM-based sensitivities for cases with many components and high correlations, e.g., by using system reliability bounds to avoid expensive high-dimensional normal integrals. They also propose extending related work to obtain FORM approximations of decision-theoretic reliability sensitivities for system problems, which account for how inputs affect optimal decisions and may be more interpretable.","Develop robust/self-starting variants that better handle non-Gaussian inputs and reduce dependence on accurate joint design point identification (e.g., adaptive linearization or SORM-based corrections for sensitivities). Provide open-source implementations (e.g., MATLAB/Python) with stable multinormal integration backends and diagnostics for ill-conditioned correlation matrices. Extend the derivations to explicitly treat dependent inputs within the formulas (rather than via external transformations) and to handle autocorrelated/process inputs common in engineering monitoring contexts.",2403.12822v1,https://arxiv.org/pdf/2403.12822v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:01:32Z
TRUE,RUL prediction|Degradation modeling|Maintenance optimization,ML-based|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Predictive,Transportation/logistics|Manufacturing (general)|Other,Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/furqon3009/MDAN,"The paper proposes Mixup Domain Adaptation (MDAN), a non-adversarial unsupervised domain adaptation method for dynamic remaining useful life (RUL) prediction under distribution shifts between training (labeled source) and deployment (unlabeled target) sensor time-series. MDAN uses a three-stage training pipeline: (1) supervised source training with feature-level mixup plus a self-supervised controlled reconstruction loss to improve transferable representations, (2) creation of an intermediate “mixup” domain via progressive cross-domain mixup between source and pseudo-labeled target samples with a Wasserstein-distance-driven mixup ratio, and (3) target self-training with pseudo-labels regularized by mixup consistency to mitigate noisy pseudo-labels. Experiments on the C-MAPSS turbofan datasets (12 transfer scenarios across FD001–FD004) show MDAN achieves the best RMSE and Score in 12/12 cases compared to several domain adaptation baselines, and it generally reduces source–target KL divergence in the embedding space after training (11/12 cases). The method is also evaluated on a bearing fault diagnosis benchmark (MFD) as a classification UDA task, outperforming an adversarial baseline (SLARDA) in 8/12 transfer cases. The authors emphasize MDAN’s simplicity and reduced memory/space needs by avoiding a domain discriminator network, and release code for reproducibility.","MDAN optimizes a staged objective combining (i) supervised source loss $L_S^{or}=\mathbb{E}_{(x,y)\sim D_S}[\ell(f_\phi(g_\psi(x)),y)]$ and feature-level mixup $\tilde g=\lambda g(x_i)+(1-\lambda)g(x_j)$ with mixed label $\tilde y=\lambda y_i+(1-\lambda)y_j$ yielding $L_S^{mx}=\mathbb{E}[\ell(f(\tilde g),\tilde y)]$, plus a controlled reconstruction self-supervised loss $L_R=\gamma L_m+(1-\gamma)L_{um}$. (ii) Intermediate cross-domain mixup uses pseudo-labels for target and mixes source–target in input and feature spaces (Eqs. 12–13) with loss $L_{cd}=\mathbb{E}[\ell(f(g(\tilde x)),\tilde y)+\ell(f(\tilde g),\tilde y)]$, where the progressive mixup ratio $\lambda$ is updated using a weight $q$ based on Wasserstein distances (Eqs. 14–16). (iii) Target training minimizes pseudo-label loss $L_T^{or}$ and target mixup consistency $L_T^{mx}$ with overall $\min_{\phi,\psi}\,\alpha_4 L_T^{or}+\alpha_5 L_T^{mx}$.","On C-MAPSS UDA (12 transfers), MDAN reports best performance in all 12/12 scenarios, e.g., FD001→FD002 RMSE 13.99 (vs CADA 19.52) and Score 1119 (vs CADA 2122); FD001→FD003 RMSE 13.34 (vs CADA 39.58) and Score 417 (vs CADA 8415); FD003→FD002 RMSE 13.78 (vs CADA 19.33) and Score 1051 (vs CADA 5257). Embedding-space KL divergence between source and target decreases after training in 11/12 scenarios (e.g., FD001→FD004: 0.104→0.070; FD002→FD004: 0.076→0.045), with one increase (FD004→FD003: 0.071→0.097) despite strong predictive results. On the MFD bearing fault diagnosis UDA benchmark, MDAN beats SLARDA in 8/12 transfers (e.g., a→b: 79.81% vs 30.96%; a→c: 89.57% vs 30.54%; c→a: 89.15% vs 37.93%) but underperforms in 4 cases (a→d, b→d, c→d, d→c). Ablations indicate removing intermediate/target training, mixup, or self-supervised reconstruction generally degrades RUL transfer performance substantially across most scenarios.","The authors state that MDAN (1) ignores data privacy constraints because it requires access to source-domain samples, (2) does not address lifelong/continual learning under continuously changing operating conditions, (3) assumes closed-set domain adaptation where source and target label sets are identical, which may not hold in practice, and (4) does not explore multi-source domain adaptation that could further improve results.","Evaluation is largely restricted to two benchmarks (C-MAPSS and one bearing dataset) and primarily reports point estimates (means over 3 or 5 seeds) without broader uncertainty quantification or statistical testing across scenarios. The approach depends on pseudo-label quality; while mixup regularizes noise, the paper does not deeply analyze failure modes (e.g., severe label shift, non-overlapping supports, or highly erroneous pseudo-labels early in training) or provide calibration/selection diagnostics for pseudo-label thresholds in regression. Practical deployment aspects typical in prognostics (e.g., varying censoring/partial run-to-failure data, online updating constraints, sensor dropout, or maintenance decision integration beyond improved prediction accuracy) are not directly addressed.","The authors propose studying source-free time-series domain adaptation motivated by privacy constraints, where only a pretrained source model is available and no access to source-domain samples is required.","Extending MDAN to continual/online domain adaptation for streaming condition monitoring (with drift detection and bounded compute) would align with real predictive maintenance settings. Robust variants that handle label-set mismatch (open-set/partial-set UDA) and multi-source adaptation could improve applicability across fleets and heterogeneous assets. Additional work could incorporate uncertainty-aware pseudo-labeling (e.g., Bayesian or conformal uncertainty) and evaluate under censored/partially observed degradation trajectories common in industrial data.",2404.04824v1,https://arxiv.org/pdf/2404.04824v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:02:36Z
FALSE,NA,ML-based|Hybrid/Ensemble|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Environmental monitoring|Other,Simulation study|Other,TRUE,Python|Fortran|Other,Public repository (GitHub/GitLab),https://github.com/hzhang-math/BlockingSHAPTL,"This paper develops a machine-learning approach to predict the persistence (“maintenance”) of North Atlantic atmospheric blocking events given the onset state, framing it as a probabilistic binary classification problem. A convolutional neural network (CNN) is trained first on abundant data from the idealized Marshall–Molteni quasi-geostrophic model and then adapted to low-data ERA5 reanalysis via transfer learning (fine-tuning only the last layer) to improve skill under severe data scarcity and class imbalance. Explainable AI is applied via Deep SHAP to identify which upstream geopotential height anomaly regions and pressure levels contribute most to successful predictions, and these features are used to build an interpretable sparse logistic-regression surrogate. Performance is evaluated using precision and recall (and their sum for early stopping), showing strong skill in the data-rich idealized setting and improved recall on ERA5 when using transfer learning compared to direct training. The SHAP comparison before/after transfer learning is also used to diagnose dynamical biases in the idealized model relative to reanalysis.","The forecasting target is the conditional probability $q(x(t)) = P[T(t+D-1)=D\mid X(t)=x(t),\,T(t)=1]$ that a nascent blocked state persists for $D$ days. The training objective uses binary cross-entropy $L(q)=-(1/N)\sum_{i=1}^N\big[Y_i\log q_i+(1-Y_i)\log(1-q_i)\big]$. SHAP attributions satisfy an additive decomposition $f(x)=\mathbb{E}[f(x)]+\sum_{i=1}^d\phi_i(f,x)$ for the CNN probability output $f(x)=\hat q(x)$.","In the Marshall–Molteni model with large training data (1000k simulated days), the best CNN for $D=5$ achieves precision 0.70 and recall 0.87 on a held-out 250k-day test set (climatological event rate ≈0.21). For rarer events in the same model, $D=9$ forecasts converge to roughly precision ≈0.35 and recall ≈0.35 versus a climatological rate ≈0.044. In ERA5 (only 273 nascent blocked states; 84 events for $D\ge5$ and 36 for $D\ge7$), transfer learning improves mean recall substantially over direct training without reducing precision: for $D=5$, DT ≈(precision 0.45, recall 0.61) vs TL ≈(precision 0.45, recall 0.82); for $D=7$, DT ≈(0.21, 0.48) vs TL ≈(0.22, 0.76). Wilcoxon signed-rank tests on cross-validation splits show recall gains are statistically significant (p=0.001 for $D=5$; p=0.002 for $D=7$).","The authors note that ERA5 provides very limited samples of extreme long-duration blocks (e.g., only 18 events for $D\ge9$), making meaningful training and robust uncertainty bounds impractical and making performance evaluation unstable under typical train/test splits. They also caution that, with small test sets, “best network” performance can be misleading because models can get lucky, so ensemble/cross-validation averages are more reliable. They further observe that sparse logistic regression suffers from many false positives (low precision), suggesting the decay/persistence dynamics may be strongly nonlinear and not well captured by simple linear models.","Despite using transfer learning, the study still relies on a single CNN architecture and limited hyperparameter exploration; reported ERA5 skill could be sensitive to architectural/regularization choices and the decision to fine-tune only the last layer. The evaluation emphasizes precision/recall but does not report calibration/reliability metrics (e.g., Brier score, reliability diagrams) even though the model outputs probabilities. The ERA5 cross-validation uses random splits over a time series of atmospheric states; if temporal dependence exists, leakage or optimistic estimates could occur unless splits are blocked by time (not clearly specified here). Finally, the approach is specific to one blocking index/region/seasonal subset and downsampled fields; generalizability to other indices, regions, seasons, or higher-resolution predictors is not demonstrated.","The authors propose pushing further on the physical/dynamical mechanisms responsible for differences between the predictive features in ERA5 and the Marshall–Molteni model, using the ML/XAI diagnostics to understand why transfer-learning adjustments occur. They also suggest pre-training on more advanced climate models or on hindcasts (e.g., S2S datasets) to potentially achieve greater improvements than pre-training on the idealized quasi-geostrophic model. Additionally, they state an immediate goal to adapt the approach to study the onset (not just maintenance) of blocking events and their statistical mechanisms.","A natural extension is to evaluate probabilistic calibration explicitly and apply post-hoc calibration (isotonic/Platt) or proper scoring rules (Brier/Log score) to better support decision-making under uncertainty. Robustness studies could relax assumptions by testing sensitivity to alternative blocking definitions, different lead-times and durations $D$, other regions, and inclusion of additional predictors (e.g., PV, winds, moisture variables) especially to capture moist processes absent in the MM model. Methodologically, exploring self-supervised pretraining on reanalysis fields (or domain-adversarial TL) could reduce reliance on idealized-model biases while still leveraging large unlabeled datasets. Finally, releasing a reproducible pipeline (environment + trained weights) and benchmarking against simpler baselines (e.g., logistic regression on hand-crafted precursors, persistence models, and modern ML baselines like gradient-boosted trees) would strengthen comparative conclusions.",2404.08613v1,https://arxiv.org/pdf/2404.08613v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:03:22Z
FALSE,NA,Bayesian|Stochastic process,Other,Not applicable,Energy/utilities,Simulation study|Case study (real dataset),TRUE,Other,Not provided,https://datasource.kapsarc.org|https://www.tensorflow.org/tutorials/keras/regression,"The paper proposes a parsimonious two-layer Gaussian-process (GP) learning strategy to model input–output relationships when empirical correlations are inhomogeneous/nonstationary. The outer layer models the target function with a non-stationary GP, while each outer-kernel hyperparameter is modeled over MCMC iteration index via an inner-layer GP; the authors argue (and prove sufficiency) that the inner-layer GPs may be taken stationary. Inference is performed with Metropolis-within-Gibbs MCMC, using a look-back window of past hyperparameter values to fit the inner-layer GP(s) and predict updated kernel hyperparameters. The method is demonstrated on a real Brent crude oil price time series dataset and compared against stationary GP baselines, two literature nonstationary kernels, and DNN regressors. Reported results indicate the proposed non-parametric SQE-looking two-layer model yields the lowest prediction MSE among the compared methods while using few hyperparameters.","The outer-layer likelihood is multivariate normal with correlation matrix element $\Sigma_{ij}=K(x_i,x_j;\theta_1,\ldots,\theta_H)$ (SQE-looking or Mat\'ern-looking), i.e. $L\propto |\Sigma|^{-1/2}\exp\{-\tfrac12(Y-\mu)^T\Sigma^{-1}(Y-\mu)\}$. Outer-kernel hyperparameters evolve as $\theta_k=q_k(t)$ across MCMC iteration index $t$, with $q_k(\cdot)\sim \mathrm{GP}_k(\mu_k,\kappa_k)$; inner-layer kernels are stationary (SQE) with correlation $\Delta^{(t)}_{k,ij}=\exp(-(t_i-t_j)^2/(2\delta_k^2))$ fit on a look-back dataset. A nonstationary equivalent SQE lengthscale is computed from estimated correlations via $\ell_{ij}=\sqrt{-(x_i-x_j)^2/\ln(\widehat{\mathrm{corr}}(Y_i,Y_j))}$, and standard GP predictive mean/variance are used for test points.","On Brent crude oil price data (184 training points, 20 test points), the Non-Parametric SQE two-layer model reports MSE(mean) 1.2021 with 2 learned hyperparameters and runtime about 2h:04m. The Non-Parametric Mat\'ern variant reports MSE(mean) 2.509 with 4 hyperparameters and runtime about 6h:49m. Stationary GP baselines are worse (Stationary SQE MSE(mean) 52.6213; Stationary Mat\'ern 21.8839), and two compared literature nonstationary kernels are also worse (Remes et al. spectral kernel 100.694; Paciorek–Schervish 206.5252). The constructed nonstationary-equivalent and blended models perform poorly in this experiment (both ~38.67 MSE(mean)) and are computationally expensive (nonstationary ~5h:39m; blended ~13h:31m).","The authors note that constructing the non-stationary equivalent kernel by estimating $\widehat{\mathrm{corr}}(Y_i,Y_j)$ and solving for $\ell_{ij}$ can be numerically unstable: small errors in estimated correlation can induce large fractional errors in $\ell_{ij}$. They also caution that if GP sample functions are rough within an $\epsilon$-neighborhood used to generate correlation samples, the empirical correlation may not approximate the true correlation well, leading to incorrect $\ell_{ij}$. They further remark that Mat\'ern-based implementations are computationally very slow in their experiments, making them practically infeasible for real-time prediction.","The approach depends on MCMC convergence/ergodicity and on the “iteration-index” proxy for input-pair variation; if chains mix poorly, the inner-layer modeling of hyperparameters over iteration may not reflect true input-dependent nonstationarity. The method is evaluated on a single real dataset with limited discussion of sensitivity to key tuning choices (e.g., look-back window $N_{LB}$, proposal variances, priors, and $\epsilon$ and $N_s$ used in the nonstationary-equivalent construction). Comparisons may be affected by implementation/optimization differences (e.g., hyperparameter tuning effort for baseline kernels and DNNs) and do not include other modern scalable/nonstationary GP baselines (e.g., variational deep GPs, inducing-point methods, change-point kernels). The demonstrated setting is univariate input; practical scalability to higher-dimensional inputs and larger datasets is not empirically validated here.",The authors state they will extend the methodology to learn high-dimensional (multivariate-input) functions in a future contribution. They also mention ongoing work on learning a scalar-valued multivariate function and a matrix-valued function that takes a vector as input.,"Develop self-starting/online variants that update the two-layer model without long MCMC runs, and study robustness when data are autocorrelated (especially for time series) or heavy-tailed. Provide principled guidance and sensitivity analysis for $N_{LB}$, priors, and proposal tuning, and investigate diagnostics for when the iteration-index proxy fails to capture spatial/input nonstationarity. Extend to multivariate outputs and multivariate inputs with scalable approximations (inducing points/variational inference) and benchmark against additional nonstationary GP families. Release a reference implementation to facilitate reproducibility and fair comparison, including standardized baselines and runtime/memory reporting across dataset sizes.",2404.12478v1,https://arxiv.org/pdf/2404.12478v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:04:11Z
TRUE,System reliability|Network/infrastructure reliability|Other,Stochastic process|Simulation-based|ML-based|Bayesian|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,Other,Not provided,https://doi.org/10.1002/eqe.3991,"This paper develops an accelerated system-reliability-based disaster resilience analysis workflow for complex structural systems, where resilience is characterized via three criteria: reliability (β), redundancy (π), and recoverability (γ). The computational bottleneck addressed is the need to evaluate β and π over an exponentially large set of mutually exclusive and collectively exhaustive (MECE) initial disruption scenarios (progressive-failure paths). To reduce the number of scenarios requiring detailed reliability/redundancy calculation, the authors propose three prescreening methods: (i) a sequential search method that recursively screens scenarios using upper-bound logic on failure probabilities, (ii) an n-ball sampling method that searches for significant scenarios by sampling within a hypersphere in standard normal space, and (iii) a surrogate (deep neural network) adaptive sampling algorithm that concentrates expensive structural analyses near rare-event boundaries. The methods are demonstrated on several numerical examples (Daniels system, truss bridge, truss building, and an OpenSees steel frame model), showing large reductions in required simulations and runtime relative to brute-force Monte Carlo while still identifying the scenarios that violate a de minimis-based resilience threshold. Overall, the work advances practical resilience assessment by coupling system reliability concepts with conservative scenario screening and (optionally) ML surrogates to make large-scale scenario-based resilience evaluation tractable.","Resilience indices are defined via standard-normal reliability transforms: for initial disruption scenario $F_i$ under hazard $H_j$, $\beta_{i,j}=-\Phi^{-1}(P(F_i\mid H_j))$ and redundancy $\pi_{i,j}=-\Phi^{-1}(P(F_{sys}\mid F_i,H_j))$. The annual system failure probability under hazard $H_j$ is decomposed over MECE scenarios as $P(F_{sys,j})=\sum_i \Phi(-\pi_{i,j})\,\Phi(-\beta_{i,j})\,\lambda_{H_j}$. A de minimis-based resilience screening threshold is imposed per scenario: $\Phi(-\pi_{i,j})\,\Phi(-\beta_{i,j}) < P_{dm}/(\lambda_{H_j}N_F)$, and a prescreening rule declares scenarios trivial if $\Phi(-\beta_i) < P_{dm}/(\lambda_H N_F)$ (so $\pi_i$ need not be computed).","For the double-layer Daniels system example, brute-force enumeration with adaptive importance sampling required $4.53\times10^7$ simulations and 77 minutes, while sequential search, n-ball sampling, and surrogate-adaptive sampling reduced runtime to 11.9, 5.1, and 7.3 minutes respectively (with markedly fewer simulations for scenario identification: e.g., 66 structural analyses for the surrogate-adaptive prescreening stage). In a 27-dimensional truss bridge example, the sequential search reduced the number of disruption scenarios needing full analysis to 12 (about $3.57\times10^{-5}\%$ of all scenarios), and reduced simulation count from $4\times10^6$ (MCS) to $1.2\times10^5 + 6.9\times10^4$, cutting runtime from 10.13 s to 0.479 s. For a truss building example with resilience threshold $10^{-5}$, n-ball screening with $10^4$ samples identified 8 noteworthy scenarios and, with subset simulation, achieved orders-of-magnitude savings versus MCS needing about $4\times10^7$ samples for comparable precision. For the OpenSees 3-story frame with 23 disruption events and threshold $10^{-4}$, surrogate-adaptive sampling identified 3 noteworthy scenarios using 53 structural analyses and reduced runtime from 4,224 minutes (MCS) to 11.7 minutes when combined with importance sampling for index estimation.","The authors note that the n-ball sampling and surrogate-based adaptive sampling approaches are heuristic and are not rigorously guaranteed to identify all noteworthy scenarios; accordingly, they are designed conservatively. They also report that the surrogate model-based method can miss noteworthy/critical scenarios at extremely small probability levels (e.g., resilience threshold $10^{-7}$ in the single-layer Daniels example) because the surrogate struggled to learn many distinct failure domains. They further remark that results depend on simplifying assumptions for damaged-component and system behavior models, and that more realistic damage modeling is desirable. Finally, they acknowledge that applicability to problems with multiple limit-state functions (beyond the single performance equation used in their studies) needs further elaboration.","Several methods rely on assumptions about geometry in standard normal space (e.g., that significant design points lie within an n-ball of chosen radius), which may be less reliable under strong nonlinearity, nonconvex/fragmented failure domains, or strong dependence structures. The framework’s prescreening focuses primarily on β-based triviality checks; scenarios with modest $\beta$ but extremely low conditional system failure (high redundancy) can be conservatively overselected, potentially reducing efficiency, and the trade-off is not fully characterized theoretically. Comparisons across methods mix different reliability estimators (MCS, subset simulation, cross-entropy AIS variants) and tuning choices, which can confound attribution of performance gains to prescreening versus estimator choice. Practical reproducibility may be limited because key implementation details (hyperparameters, sampling settings, OpenSees models) are only partially specified and no code is provided.","The authors propose extending the approach to structural systems subjected to stochastic excitations by combining this work with their prior stochastic-load resilience framework (Yi & Kim, 2023). They also suggest further research to connect the prescreening/assessment methods with time-dependent restoration processes so the methods become a core element of a broader resilience analysis framework. Additionally, they point to the need for more realistic damage modeling to facilitate practical application, and (in discussion) indicate that parametric studies could clarify how prescreening effectiveness depends on system configuration and hazard characteristics.","Develop self-starting/robust variants that explicitly handle model uncertainty, dependence among variables, and non-Gaussian/nonlinear transforms without relying on n-ball heuristics in $u$-space. Provide formal coverage guarantees or probabilistic bounds on missed noteworthy scenarios (e.g., via sequential stopping rules or conformal-style uncertainty quantification for the surrogate classifier). Extend to multiple-component performance measures and multiple system limit-states (e.g., drift, member forces, connection failure) with multi-label or structured-output surrogates and scenario ranking/diagnostics. Release open-source implementations (e.g., OpenSees models + prescreening + CE-AIS/subset simulation pipelines) and benchmark across standardized structural reliability testbeds to enable fair comparison and adoption.",2404.13321v1,https://arxiv.org/pdf/2404.13321v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:05:02Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,R|C/C++|Python,Public repository (GitHub/GitLab),github.com/yqzhang5972/lmmvar,"The paper develops fast confidence intervals for a variance component (heritability/fraction of spatial variance) in a linear mixed model, including cases where the parameter is near or at the boundary (e.g., 0 or 1). The method constructs intervals by inverting score-based test statistics derived from the restricted likelihood, standardized by the restricted information, yielding asymptotically correct uniform coverage on compact subsets of the parameter space. The authors provide asymptotic results showing the standardized restricted score statistics are asymptotically pivotal (chi-square limiting distributions) under conditions on the design dimension and the spectrum of the known kernel/relatedness matrix. Simulation studies demonstrate near-nominal coverage even in small samples and show that Wald and likelihood-ratio-based intervals can undercover near boundaries. A computational analysis shows linear-in-n post-eigendecomposition cost, and an application to spatial transcriptomics computes ~15,000 gene-wise intervals in minutes, with large speedups versus ALBI/FIESTA/RLRT-type alternatives.","The model is a Gaussian linear mixed model with covariance \(\Sigma=\sigma^2\{h^2K+(1-h^2)I\}\) (equivalently \(Y\sim N(X\beta,\sigma_g^2K+\sigma_e^2I)\)). The proposed test statistics are restricted-likelihood score statistics standardized by the restricted information: \(T^R_n(h^2,\sigma^2)=U(h^2,\sigma^2)^\top I(h^2,\sigma^2)^{-1}U(h^2,\sigma^2)\) and for one-dimensional inference \(T^R_n(h^2)=U_1(h^2,\tilde\sigma^2)^2 / I^{11}(h^2,\tilde\sigma^2)\), with \(\tilde\sigma^2(h^2)=(n-p)^{-1}(Y-X\tilde\beta)^\top\{h^2\Lambda+(1-h^2)I\}^{-1}(Y-X\tilde\beta)\). Confidence regions are \(R_n(\alpha)=\{h^2: T^R_n(h^2)\le q_{1-\alpha,1}\}\) and \(R_n^2(\alpha)=\{(h^2,\sigma^2): T^R_n(h^2,\sigma^2)\le q_{1-\alpha,2}\}\).","Asymptotically, under stated eigenvalue/design conditions (including \(p/n\to 0\)), \(T^R_n(h_n^2,\sigma_n^2)\Rightarrow\chi^2_2\) and \(T^R_n(h_n^2)\Rightarrow\chi^2_1\), supporting inversion with fixed chi-square quantiles uniformly near boundaries. In simulations, the proposed intervals achieve near-nominal coverage across \(h^2\) values where Wald and LRT intervals show substantial miscoverage near 0 or 1. Runtime comparisons report large speedups: e.g., for \(n=200, h^2=0.01\) the proposed method averages \(7.3\times10^{-4}\) s per interval vs 2.89 s (FIESTA), 69.98 s (ALBI), and 327.10 s (RLRT); for \(n=2000, h^2=0.01\) it averages 0.02 s vs 119.38 s, 736.51 s, and 3983.35 s, respectively (excluding eigendecomposition). In spatial transcriptomics, ~15,000 intervals are computed in <4 minutes and used for gene ranking via two-sided intervals and one-sided lower bounds.","The authors note they have not been able to prove that \(T^R_n(h^2)\) is strictly quasiconvex in \(h^2\) in general (though they have not encountered counterexamples), which affects guarantees for their root-finding strategy when constructing intervals. They also indicate that problematic behavior can arise when the kernel \(K\) is close to proportional to the identity (near-unidentifiable) or when eigenvalues concentrate (e.g., \(\rho\) near 0 or 1 in their AR(1)-type kernel), which can lead to very wide intervals or under-coverage in extreme cases.","The approach relies on correct specification of the Gaussian linear mixed model (normality and correct covariance structure) and does not analyze robustness to non-Gaussian outcomes (e.g., counts/zero inflation common in transcriptomics) or model misspecification. Practical performance depends on eigendecomposition of \(K\), which can be a computational bottleneck for very large \(n\) unless structure/approximations are used; this is not fully addressed. The theory assumes conditions on the spectrum of \(K\) and on \(p/n\) that may be hard to verify in practice for complex designs and kernels, and finite-sample behavior under violations (e.g., strong leverage in \(X\), heavy-tailed errors, dependence) is not explored.","The paper states that similar results often apply under different parameterizations (e.g., \(\tau=\sigma_g^2/\sigma_e^2\)) and discusses extensions in supplementary material to models with more variance components or a parameterized \(K\). It also mentions the possibility of extending the parameter set to include \(h^2=1\) when \(K\) is positive definite (via continuous extension of the score).","Extending the method to non-Gaussian generalized linear mixed models (e.g., Poisson/negative-binomial for gene counts) and assessing robustness to misspecified \(K\) would broaden applicability in genomics and beyond. Developing theory and algorithms for large \(n\) settings using randomized/iterative eigensolvers or low-rank approximations of \(K\) could remove the preprocessing bottleneck. Providing a proven quasiconvexity/shape result (or a certified interval-construction algorithm that handles non-quasiconvex cases) would strengthen reliability of the inversion procedure. Additional real-data validation across diverse domains (e.g., longitudinal or multi-kernel settings) and comparisons to other modern uniform-inference approaches near boundaries would further benchmark performance.",2404.15060v2,https://arxiv.org/pdf/2404.15060v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:05:47Z
TRUE,Other,Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.arch.jhu.edu/|https://artowen.su.domains/mc/,"This paper proposes a covariance-free bi-fidelity Control Variates–Importance Sampling (CVIS) framework to estimate rare-event failure probabilities in reliability analysis when high-fidelity (HF) model evaluations are expensive. The method couples approximate control variates with importance sampling, but chooses the tuning constant as $\tilde\alpha=\tilde Q/\tilde Q_L$ so it avoids estimating HF–LF covariances and eliminates the need to compute the IS density normalizing constant (it cancels in the ratio). The importance sampling density is built from the low-fidelity (LF) limit-state via a logistic approximation $S_L(x,\beta)$, and the paper provides a diagnostic $\kappa=P(H\ \&\ L\ \text{fail})/P(H\ \text{fail})$ to indicate when variance reduction is expected (e.g., variance reduction when $\kappa\ge 1/2$ under stated conditions). The algorithm recommends estimating $\tilde P_{F,L}$ via Subset Simulation, sampling the IS density using DE-MC, and estimating MCMC variance via replicated batch means. Performance is demonstrated on analytical and numerical examples (including a shear-building model and a lid-driven cavity CFD model), showing accuracy comparable to MFIS and E-ACV with practical implementation advantages and an explicit variance/CoV decomposition for the final estimate.","Failure probability is $P_F=\mathbb E_f[I_H(X)]$. The proposed CVIS estimator is $\tilde P_F=\tilde\alpha\,\tilde P_{F,L}$ with $\tilde\alpha=\tilde Q/\tilde Q_L$, where $\tilde Q=\frac{C_S}{N}\sum_{i=1}^N\frac{I_H(x_i)}{S_L(x_i,\beta)}$ and $\tilde Q_L=\frac{C_S}{N}\sum_{i=1}^N\frac{I_L(x_i)}{S_L(x_i,\beta)}$ using samples $x_i\sim q_X(x,\beta)$; $S_L(x,\beta)=\{1+\exp(\beta L(x))\}^{-1}$ and $q_X(x,\beta)=S_L(x,\beta)f_X(x)/C_S$. The diagnostic is estimated as $\tilde\kappa=\tilde Q_{HL}/\tilde Q$ with $\tilde Q_{HL}=\frac{1}{N}\sum_{i=1}^N\frac{I_H(x_i)I_L(x_i)}{S_L(x_i,\beta^*)}$.","Across Example 2 (five-story shear building), 100 trials gave CVIS $\hat P_F\approx 4.26\times10^{-3}$ with estimated CoV $\approx 12.73\%$; MFIS and E-ACV were similar ($4.33\times10^{-3}$ and $4.30\times10^{-3}$ with CoV $\approx 13.75\%$ and $13.50\%$), while a crude-MC reference was $4.27\times10^{-3}$ (using $10^6$ samples). In Example 3 (lid-driven cavity), one CVIS run at $\zeta_L=1.5$ produced $\tilde P_F=2.95\times10^{-3}$ with CoV $15.81\%$ using 40,000 HF + 45,000 LF calls; the crude-MC reference was $2.51\times10^{-3}$ (with $2\times10^5$ samples). The diagnostic illustrates failure of variance reduction when LF misses HF failures: for $\zeta_L=1.0$, $\kappa\approx 1.99\times10^{-3}\ll 0.5$; for $\zeta_L=1.5$, $\kappa\approx 0.984\ge 0.5$ indicating variance reduction is expected. Example 1 shows CVIS RMSE/CoV generally comparable to or slightly better than MFIS and E-ACV under matched budgets while enabling explicit decomposition of uncertainty contributions from $\tilde\alpha$ and $\tilde P_{F,L}$.","The authors note CVIS is not stable/efficient for arbitrary HF–LF pairs: it requires a model-relationship stipulation (existence of small $\delta_L$ such that $L(x)<\delta_L$ for all HF-failure points) so the LF does not miss HF failure regions. They also acknowledge that even the optimally tuned ISD within their logistic form necessarily places nontrivial probability mass outside the true HF failure region ($\Omega_L\setminus\Omega_F$), which can be less efficient than IS schemes that directly target the HF failure region. They further caution that self-normalized importance sampling can be highly biased with their logistic-based ISD unless much longer MCMC chains are used (discussed in Appendix C).","The paper’s practical performance depends on MCMC quality (DE-MC) and variance estimation (RBM); short chains, multimodality, or poor mixing could materially bias estimates or understate uncertainty, especially in higher dimensions, but this risk is not deeply stress-tested. The “covariance-free” tuning $\tilde\alpha=\tilde Q/\tilde Q_L$ can be unstable when $\tilde Q_L$ is small or noisy; safeguards for near-zero denominators or heavy-tailed ratios are not extensively discussed. The β* selection rule derived from Subset Simulation thresholds is presented as a heuristic; sensitivity analyses (robustness to SuS settings, multimodal failure regions, or LF conservatism) could be more comprehensive. Real-world empirical validation beyond simulated/numerical benchmarks (e.g., field failure datasets) is not provided.",None stated.,"Extend CVIS to settings with multiple LF models (multi-level/multi-index) while preserving covariance-free tuning, and study principled choices of generalized tuning constants beyond a simple ratio. Develop robust/self-starting variants that handle imperfect LF coverage (partial violation of the stipulation) via adaptive enrichment, mixture ISDs, or fallbacks when $\tilde\kappa$ indicates poor overlap. Provide open-source reference implementations and benchmark suites, and evaluate scalability on higher-dimensional, strongly multimodal rare-event problems with rigorous MCMC diagnostics. Explore theoretical finite-sample behavior of the ratio estimator $\tilde Q/\tilde Q_L$ (tail bounds, stability conditions) and incorporate robust ratio/regularization techniques.",2405.03834v2,https://arxiv.org/pdf/2405.03834v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:06:42Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,2405.10329v3,https://arxiv.org/pdf/2405.10329v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:06:42Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies when the Bethe free energy approximation (used in variational inference / loopy belief propagation for graphical models) is reliable, focusing on the role of convexity. The authors introduce the “Bethe box,” a submanifold of the local polytope that contains all Bethe minima, and argue that the approximation is typically accurate when the Bethe free energy is convex on this set. They derive two sufficient convexity conditions based on Hessian definiteness: (i) a diagonal-dominance-based node-wise criterion reducible to checking positivity of one-dimensional polynomials, and (ii) an edge-wise Hessian decomposition yielding a closed-form temperature threshold for loss of convexity. They propose BETHE-MIN, a projected quasi-Newton (BFGS-style) method for minimizing the Bethe free energy, and evaluate approximation errors for marginals/partition function across temperatures on several graph families (grids, complete graphs, Erdős–Rényi) via experiments. Results indicate that the temperature at which convexity is predicted to fail often correlates with a practical “phase-transition-like” point where Bethe accuracy degrades.","The Bethe-box reduction sets edge pseudomarginals to a unique stationary mapping $\xi^*_{ij}(q_i,q_j)=\frac{1}{2\alpha_{ij}}\left(Q_{ij}-\sqrt{Q_{ij}^2-4\alpha_{ij}(1+\alpha_{ij})q_iq_j}\right)$ where $\alpha_{ij}=e^{4\beta J_{ij}}-1$ and $Q_{ij}=1+\alpha_{ij}(q_i+q_j)$. Convexity on the Bethe box is assessed via the Hessian $H_B(q)$; one sufficient condition requires diagonal dominance, leading to positivity of node-wise polynomials $\Psi_i(q_i)$ on $(0,0.5]$. A second sufficient condition uses an edge-wise Hessian decomposition and yields a closed-form convexity threshold $\beta < \frac{1}{2|J_{ij}|}\operatorname{arccosh}\!
\left(1+\frac{2}{d_id_j-d_i-d_j}\right)$ for edges with $d_i,d_j>2$.","On symmetric complete-graph models without fields, the edge-decomposition condition (Theorem 23) is sharp and matches the known critical inverse-temperature threshold for behavioral change (linked to Gibbs-measure non-uniqueness / LBP behavior). In experiments over 1600 sampled models (4 graph types × ferromagnetic/spin-glass × 200 instances) and $\beta\in[0,2]$, Bethe approximation errors for marginals/partition function increase markedly past a critical $\beta$ that often aligns with the predicted loss of convexity (Theorem 15) and/or the predicted onset of multiple minima (Mooij & Kappen criterion). BETHE-MIN shows high convergence (reported >99% across sampled models) with iteration counts increasing with inverse temperature, especially in ferromagnetic settings. Empirically, the convex-to-nonconvex threshold tends to track pairwise-marginal and partition-function degradation in ferromagnets, while the unique-to-multiple-minima threshold better tracks singleton-marginal degradation in spin glasses.",None stated.,"The work is not about engineering system reliability; “reliability” here refers to accuracy/robustness of a variational inference approximation, so the findings do not translate directly to reliability engineering metrics (failure rates, survival, RUL). Convexity conditions are sufficient (not necessary), so predicted critical temperatures can be conservative or mismatched for some models (and the paper notes gaps between convexity and uniqueness under fields). Experimental validation is limited to relatively small graphs where exact inference is feasible; scalability and behavior on larger real-world graphical models may differ, and implementation details (e.g., numerical stability near constraints) are not benchmarked against a broad suite of modern optimizers with shared code.",None stated.,"Provide open-source reference implementations for BETHE-MIN and convexity-check routines (polynomial/root tests and edge-threshold checks) to enable reproducibility and broader adoption. Extend the convexity/accuracy analysis to settings with non-binary variables, higher-order factors (beyond pairwise Ising), and structured real-world PGMs (e.g., vision, NLP) at larger scales. Study robustness under model mismatch and numerical issues (near-boundary behavior, conditioning of Hessian approximations) and compare against alternative inference methods (TRW, GBP/Kikuchi, modern variational/EP variants) under standardized benchmarks.",2405.15514v1,https://arxiv.org/pdf/2405.15514v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:07:17Z
FALSE,NA,Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,Python|Other,In text/Appendix|Not provided,https://github.com/google-deepmind/mctx|https://github.com/google-deepmind|https://github.com/google/jax|http://github.com/RobertTLange/gymnax,"The paper studies pure exploration (best-arm identification) in stochastic multi-armed bandits under a fixed-size batch pulling constraint, where delayed feedback can reduce adaptability. It proposes Advance-first Sequential Halving (ASH), a batched variant of Sequential Halving (SH), and also defines Breadth-first Sequential Halving (BSH) as a baseline-style variant. The main theoretical result proves ASH is algorithmically equivalent to sequential SH (i.e., selects the same arm for any reward realization) when the batch budget satisfies B ≥ max{4, n/b}⌈log2 n⌉ with total pulls matched by T=bB. Experiments on simulated polynomial-gap bandit instances (and a Jun et al. 2016 batched SH baseline) empirically confirm ASH matches SH under the stated condition and remains close in simple regret even when the condition fails. The work contributes to batched bandit literature by showing batching need not degrade SH performance under practical batch-budget conditions, enabling scalable batched evaluation (e.g., GPU-based evaluation).","SH allocates per-round pulls Jr = ⌊ T / (|Sr|⌈log2 n⌉) ⌋ and repeatedly halves the active arm set Sr based on empirical means. ASH constructs an advance-first target-pull sequence LA (Eqs. (2)) and, within each batch, selects arms from At = {a : Na+Ma = LA_t} using a lexicographic argmax over (Na, \bar{\mu}_a) to ensure promotions align with SH when batches span rounds. The equivalence condition is B ≥ max{4, n/b}⌈log2 n⌉, yielding πASH = πSH for T=bB.","Theorem 1 shows ASH is exactly algorithmically equivalent to SH (identical selected arm mapping) under B ≥ max{4, n/b}⌈log2 n⌉. In simulations with B ≥ 4⌈log2 n⌉ (and satisfying C1), ASH and SH selected identical arms across 10K sampled problem instances and 100 seeds each; linear fit of simple regret vs SH gave slope β≈1.000 for ASH (β≈1.008 for BSH, β≈0.971 for Jun+16). When B < 4⌈log2 n⌉, performance degrades slightly but remains close to SH on average: β≈1.011 (ASH), β≈1.059 (BSH), β≈1.017 (Jun+16). The paper highlights an example where for n=32, ASH can match SH’s decision from T=100K sequential pulls using B=20 batches of size b=5K under the conditions.","The authors state their batched variants assume i.i.d. stationary reward distributions, which is essential for batch pulls. They note the methods may be difficult to apply to non-stationary settings (e.g., hyperparameter tuning where rewards are time-series losses and “future losses” cannot be observed in batch).","The equivalence guarantee is about identical arm selection given the same realized rewards, but the practical motivation is computational wall-clock; the paper does not quantify actual speedups/overheads (e.g., GPU utilization, communication latency) or provide real system benchmarks. The empirical study is limited to simulated polynomial-gap instances; robustness to other reward models (heavy tails, heteroskedasticity, dependence/autocorrelation within an arm) is not evaluated. The method also presumes synchronous fixed-size batching and deterministic policies; behavior under asynchronous batching or stochastic tie-breaking (common in implementations) is not analyzed.","They propose applying the batched SH variants to tasks where arms can be evaluated efficiently in batches, such as neural-network-based evaluation in Monte Carlo tree search (e.g., value networks). They also suggest combining the approach with GPU/TPU-accelerated reinforcement-learning environments to enable efficient batch evaluation.","A useful extension would be analyzing ASH/BSH under non-i.i.d. or non-stationary rewards (e.g., drifting means) and deriving regret/selection-error bounds with delayed feedback. Providing wall-clock performance evaluations on real batched workloads (e.g., GPU inference pipelines) would strengthen the practical claim of efficiency without performance loss. Another direction is developing robust versions for heavy-tailed rewards (e.g., median-of-means estimates) and studying asynchronous or adaptive batch sizing policies that preserve equivalence or near-equivalence guarantees.",2406.00424v1,https://arxiv.org/pdf/2406.00424v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:07:42Z
TRUE,System reliability|Life distribution modeling|Other,"Bayesian|Simulation-based|Parametric (Weibull, etc.)",Event/count data|Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper reviews Bayesian statistical principles for practicing reliability engineers, focusing on success/no-success (Bernoulli/binomial) testing as a common reliability metric (probability a system survives/passes a test). For a single component/subsystem, it presents the conjugate Beta prior with Binomial likelihood, yielding a closed-form Beta posterior with updated parameters $(\alpha+x,\,\beta+n-x)$. For multi-component systems (series and series/parallel), it emphasizes that total system reliability becomes a product/sum of Beta random variables, making the analytic distribution cumbersome, and advocates Monte Carlo simulation to characterize the prior/posterior of system reliability from component posteriors. The main methodological reminder is an exact Monte Carlo accept/reject (Rubin-style) algorithm to update the system-level reliability distribution when only system-level test outcomes are observed (including cases with failures where component posteriors cannot be updated). Simulated examples illustrate how subsystem test sizing and additional system tests shift and concentrate the posterior distribution of total system reliability.","Bayes rule: $\pi(\theta\mid x) \propto p(x\mid\theta)\,\pi(\theta)$. With $\theta$ a success probability, Beta prior $\theta\sim\text{Beta}(\alpha,\beta)$ and Binomial likelihood $x\sim\text{Binomial}(n,\theta)$ give posterior $\theta\mid x \sim \text{Beta}(\alpha+x,\,\beta+n-x)$. For a series system, total success probability is $\theta_{\text{Tot}}=\prod_{j=1}^S \theta_j$; for the illustrated 5-subsystem series/parallel case, $\theta_{\text{Tot}}=\theta_1(\theta_2+\theta_3-\theta_2\theta_3)\theta_4\theta_5$. When updating with system-level tests, $x_{TS}\sim\text{Binomial}(n_{TS},\theta_{\text{Tot}})$ and an exact posterior sample can be formed by simulating from the prior of $\theta_{\text{Tot}}$, then simulating $x_{TS}$ and retaining draws with $x_{TS}=x^*_{TS}$.","The paper provides simulated demonstrations (typically using 10,000 posterior samples) showing that larger subsystem test sample sizes yield a more concentrated (lower-variance) distribution for $\theta_{\text{Tot}}$ (e.g., comparing $(n_1,n_2,n_3)=(2,5,4)$ vs. $(11,14,12)$). It shows that additional successful total-system tests shift the posterior distribution of $\theta_{\text{Tot}}$ to the right (e.g., 4 successes out of 4 system tests applied to the first setting). It also shows that when total-system tests include failures (e.g., 5 successes out of 7), component posteriors cannot be directly updated because the failing component is unobserved, motivating the accept/reject Monte Carlo posterior update for $\theta_{\text{Tot}}$. Quantitative performance metrics (e.g., MSE/coverage/ARL) are not the focus; results are presented as distributional shifts and changes in posterior concentration via histograms.",None stated.,"The work is primarily a methodological review/reminder with illustrative simulations; it does not provide a systematic performance study (e.g., efficiency of the accept/reject scheme, acceptance rates as $n_{TS}$ grows, or comparisons to MCMC/SMC alternatives). Independence between subsystem reliabilities is assumed for the component-to-system Monte Carlo construction; dependence modeling is not treated. The accept/reject posterior sampling conditioned on exact $x_{TS}=x^*_{TS}$ can become computationally inefficient for large $n_{TS}$ or extreme outcomes (very small acceptance probability), and no guidance is given on mitigating this (e.g., weighting/importance sampling or ABC tolerances).",None stated.,"Extend the system-level posterior updating approach to handle dependent subsystem reliabilities (e.g., via copulas or hierarchical priors) and common-cause failures. Develop more computationally efficient alternatives to exact accept/reject conditioning for large $n_{TS}$ (importance sampling, SMC, or MCMC on the component space with system-level likelihood). Generalize beyond Bernoulli/binomial pass-fail tests to time-to-failure and degradation data, enabling Bayesian updating for richer reliability evidence. Provide practitioner-facing implementation (e.g., an R/Python reference implementation) and guidance on prior elicitation/effective sample size calibration in complex system settings.",2406.02751v2,https://arxiv.org/pdf/2406.02751v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:08:15Z
TRUE,Life distribution modeling|Accelerated testing|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based|Other",Interval-censored|Right-censored|Event/count data|Mixture of types,Not applicable,Semiconductor/electronics|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a robust Bayesian framework for reliability prognosis of nondestructive one-shot devices (NOSD) tested under step-stress accelerated life testing (SSALT) using the cumulative risk model (CRM), which introduces a lag period to maintain hazard continuity across stress changes. Lifetimes are modeled with the Lehman family (including Weibull and Gompertz as special cases), with stress entering through a log-linear link on the scale parameter. Robustness to outliers is achieved by replacing the likelihood with a density power divergence (DPD)-based objective, yielding a robustified (pseudo) posterior, and hypothesis testing is conducted via a Bayes factor derived from this robust posterior. The paper incorporates an order-restricted prior on shape parameters to reflect decreasing lifetime (increasing hazard) at higher stress levels, and uses Hamiltonian Monte Carlo (HMC) for posterior computation. Performance and robustness are studied via influence functions, Monte Carlo simulations (pure vs contaminated data), and an application to a lightbulb SSALT dataset, where robust Bayes estimators show reduced bias/RMSE under contamination and robust Bayes factors provide evidence for specified parameter hypotheses.","Model at stress level $i$ uses Lehman-family cdf/pdf: $F_i(t)=1-\exp\{-\lambda_i Q(t;\gamma_i)\}$ and $f_i(t)=\lambda_i Q'(t;\gamma_i)\exp\{-\lambda_i Q(t;\gamma_i)\}$, with stress link $\lambda_i=\exp(c_0+c_1 x_i)$. CRM defines a piecewise hazard with a linear transition over a lag $\delta$ after each stress change to ensure continuity; survival is $S(t)=\exp\{-\int_0^t h(u)\,du\}$. Robust Bayes replaces the likelihood with $B_\alpha(\theta)$ derived from DPD (equivalently maximizing a DPD-based criterion), and defines the pseudo posterior $\pi_\alpha(\theta\mid डेटा) \propto \exp\{B_\alpha(\theta)\}\,\pi(\theta)$; robust Bayes factor uses $\mathrm{BF}_{01}=\frac{\int_{\Theta_0}\exp\{B_\alpha(\theta)\}\,d\theta}{\int_{\Theta_1}\exp\{B_\alpha(\theta)\}\,d\theta}$.","In simulations with $n=50$ NOSD, 3-step SSALT, and contamination induced by parameter deviation, robust Bayes estimators (RBE) show substantially smaller bias and RMSE than MLE/standard Bayes under contamination; order-restricted RBE is particularly strong for Weibull contamination (tables report consistently smallest RMSE around ~0.0054–0.0056 for parameters under RBE(Ord) vs much larger for MLE). For the lightbulb SSALT data (n=64, two-step; 11 survivors), a grid-search profile likelihood estimate of lag is $\hat\delta=0.001$, and bootstrap GOF p-values indicate both Weibull and Gompertz fits are adequate (p≈0.660 and 0.836). The proposed tuning-parameter selection yields $\alpha_{opt}=0.60$ (Weibull) and $0.70$ (Gompertz) for the data; bootstrap bias/RMSE are smaller for RBE than for competing estimators. Robust empirical Bayes factors for testing $H_0:\theta=\theta_0$ show strong support for $H_0$ under Weibull (BF01 roughly 31–89 depending on prior/α) and positive-to-strong support under Gompertz (BF01 roughly 14–26).",None stated.,"The approach is tailored to a parametric Lehman-family lifetime model and assumes the CRM lag structure (linear hazard transition) is correctly specified; misspecification of $Q(t;\gamma)$ or the lag transition form could affect inference. The robust Bayes factor requires integration over constrained parameter spaces (e.g., order restrictions) and depends on the DPD tuning parameter $\alpha$ and chosen priors; sensitivity analysis beyond the proposed selection criterion is limited. Implementation details (software, diagnostics, reproducibility) are not provided even though HMC is used, and practical guidance for choosing HMC settings (step size/leapfrog steps) across problems is minimal. The NOSD observation model aggregates into interval-count probabilities; extensions to dependent units, autocorrelated inspections, or more complex censoring/inspection schemes are not addressed.","The authors suggest extending the work to non-parametric inferential analysis, reanalyzing the step-stress model under a competing risks setup, and conducting missing cause-of-failure analysis; they note these efforts are underway and will be reported later.","Develop self-starting or empirical-Bayes strategies for prior and $\alpha$ selection with formal robustness/efficiency trade-offs, and provide sensitivity analyses for priors and the CRM lag specification. Extend the framework to multistress/multivariate stress profiles and to dependent failure modes (e.g., copula-based competing risks) while retaining robustness. Provide open-source software (e.g., Stan/Python/R) with standardized HMC diagnostics (divergences, ESS, \hat R) and computational benchmarks to facilitate adoption. Investigate robustness under model misspecification beyond outliers (e.g., wrong link function for $\lambda_i$, autocorrelated inspection processes) and derive practical design recommendations for SSALT under CRM (optimal stress-change times and inspection schedules) using the robust posterior.",2406.08867v3,https://arxiv.org/pdf/2406.08867v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:08:44Z
NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,NA,2406.09548v2,https://arxiv.org/pdf/2406.09548v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:10:05Z
FALSE,NA,ML-based|Nonparametric/Semi-parametric|Other,Sensor/condition monitoring|Other,Not applicable,Energy/utilities,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/CCaribe9/HQR-WACI,"The paper proposes a method to construct reliable prediction intervals for time-series forecasting when practitioners only have access to multiple point forecasts from different models. It introduces Heteroscedastic Quantile Regression (HQR), which models conditional quantiles using the mean and dispersion (standard deviation) of the ensemble of point forecasts to make interval width correlate with prediction difficulty. It then proposes Width-Adaptive Conformal Inference (WACI), an online conformal post-processing method that adapts the conformal correction as a function of the unconformalized interval length to reduce dependence between interval length and coverage. The approach is evaluated on a synthetic heteroscedastic switching-variance process and a real electricity day-ahead price forecasting dataset, comparing against QRA, ACI, and related variants. Results show WACI+HQR achieves near-target marginal coverage with improved efficiency and markedly reduced correlation between interval length and coverage compared with ACI, while also producing interval widths that better track forecasting difficulty.","HQR models the conditional quantile as $q_\beta(Y_t\mid\hat y_t)=\lambda_0(\beta)+\lambda_1(\beta)\bar{\hat y}_t+\lambda_2(\beta)s_{\hat y_t}+\varepsilon_t$, where $\bar{\hat y}_t$ and $s_{\hat y_t}$ are the mean and standard deviation of the $M$ point forecasts. WACI maintains a grid of miscoverage values $\alpha_t[i]$ over interval-length bins and updates them online via $\alpha_{t+1}=\alpha_t+\gamma w_t(\alpha^*-\mathrm{err}_t)$ with weights $w_t$ determined by distance between the current unconformalized interval length $|\hat C_{\alpha,t}|$ and grid points; the conformal interval is $\hat C^c_{\alpha^*,t+1}=[\hat l_{\alpha^*,t+1}-Q_{1-\tilde\alpha_{t+1}}(S),\ \hat u_{\alpha^*,t+1}+Q_{1-\tilde\alpha_{t+1}}(S)]$. The method targets making coverage approximately independent of interval length while retaining marginal (and, under a variant weighting scheme, asymptotic length-conditional) coverage.","On the synthetic two-regime variance example (target coverage 80%, $\alpha=0.2$), WACI achieved mean coverage close to target in both regimes (about 81.1% high-uncertainty and 80.7% low-uncertainty), while ACI under-covered in both (about 83.2%/76.6% respectively, showing regime mismatch). WACI substantially reduced dependence between interval length and coverage compared with ACI (Pearson correlation about 0.10–0.15 vs 0.22–0.25 by regime) and reduced mean coverage deviation (MCD) (about 4.35/4.57 vs 7.25/9.87). In the electricity price forecasting case, HQR-based models improved Winkler scores over QRA, and WACI generally improved conditional-coverage diagnostics vs ACI; for example at $\alpha=0.10$, HQR(WACI) achieved ~90.14% coverage with mean interval length ~43.52 and lower MCD (~2.57) than HQR(ACI) (~3.32). The authors also find HQR-W (using individual forecast weights plus dispersion) offers no significant improvement over the simpler HQR, supporting parsimony.","The authors note that hyperparameters (e.g., calibration window size and the ACI/WACI parameters $\gamma$ and $\sigma$) were not optimized, so performance could be improved with tuning. They also acknowledge that WACI’s theoretical result is proved for an alternative position-based weighting scheme (not the main distance-based Gaussian kernel used in experiments), and that boundedness of the adaptive $\alpha$ values is assumed rather than formally proven. For electricity price forecasting, they state the underlying point-forecasting models cannot be fully described for commercial reasons.","The work is not framed as reliability engineering (failure/degradation/maintenance) and does not validate how interval properties translate into operational risk reduction decisions typical in reliability contexts. Theoretical guarantees in the time-series (non-exchangeable) setting remain asymptotic and/or conditional on design choices; finite-sample guarantees under realistic serial dependence and regime shifts are not fully characterized. The method’s behavior under strong autocorrelation in residuals, misspecified rolling-window training, or varying numbers/qualities of point forecasters (including adversarial or highly correlated forecasters) is only partially explored. Practical deployment may require careful choices for the interval-length grid and handling of extreme $\alpha_t$ values; sensitivity beyond the provided $\sigma$ analysis could be broader.","They propose extending HQR to incorporate higher-order moments (e.g., skewness or kurtosis) of the ensemble of point forecasts to better estimate quantiles, inspired by Cornish–Fisher ideas, when enough predictors are available. They also suggest improving the variance/dispersion estimation by accounting for individual forecaster quality and correlations among forecasters, which could yield better uncertainty quantification. Additionally, they note that optimizing hyperparameters like $\gamma$, $\sigma$, and calibration/training window sizes could further improve results.","Developing a full theoretical analysis of WACI under the exact distance-kernel weighting used in practice (including finite-sample bounds under dependence) would strengthen the method. Extending WACI/HQR to multi-step-ahead horizons and multivariate or hierarchical time series (e.g., hourly-by-zone prices) would broaden applicability. Robust/self-starting variants that handle unknown or drifting calibration distributions, missing forecasts, or changing ensembles (models entering/leaving) would improve operational usability. Providing a well-tested open-source package (with reproducible pipelines, diagnostics for length-conditional coverage, and automatic hyperparameter selection) would aid adoption and benchmarking.",2406.14904v2,https://arxiv.org/pdf/2406.14904v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:10:44Z
FALSE,Other,Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,R,Supplementary material (Journal/Publisher),https://arxiv.org/abs/2407.00716v2,"This paper proposes a general theoretical framework for defining reliability as an association measure between observed scores (functions of manifest variables) and latent scores (functions of latent variables) under an assumed latent-variable measurement model. It generalizes the McDonald regression view of reliability beyond coefficients of determination by formalizing reliability as $A\{s(y_i),\xi(\eta_i)\}$ and by introducing four desiderata for reliability measures: estimability, normalization, symmetry, and invariance. The authors discuss and adapt several association measures as candidate reliability coefficients, including squared Pearson correlation, a rescaled Schweizer–Wolff coefficient sigma, rescaled mutual information, the Azadkia–Chatterjee coefficient $T$, and a multivariate generalized coefficient of determination $W$ (Wilks’ lambda complement). A numerical study under a two-dimensional 3PL IRT model (with Monte Carlo generation and nonparametric regression estimation) compares nine reliability coefficients and shows that most increase with test length, while some (e.g., squared correlation and prediction-based $\rho^2$ and $W$) are sensitive to monotone transformations of latent scores. The work advances the psychometric reliability literature by unifying many coefficients under a broader “association” perspective and by highlighting how symmetry/invariance choices change interpretation and comparability.","Reliability is defined generically as an association measure applied to observed and latent score vectors: $A\{s(y_i),\xi(\eta_i)\}$. Classical regression-based reliability emerges as special cases via the measurement decomposition $s(y_i)=\mathbb{E}[s(y_i)\mid \eta_i]+\varepsilon_i$ with $\rho^2=\mathrm{Var}(\mathbb{E}[s(y_i)\mid\eta_i]) / \mathrm{Var}(s(y_i))$, and the prediction decomposition $\xi(\eta_i)=\mathbb{E}[\xi(\eta_i)\mid y_i]+\delta_i$ with PRMSE $\rho^2=\mathrm{Var}(\mathbb{E}[\xi(\eta_i)\mid y_i]) / \mathrm{Var}(\xi(\eta_i))$. Additional proposed/used coefficients include rescaled mutual information $\tilde M(u,v)=1-\exp\{-2M(u,v)\}$, Azadkia–Chatterjee’s $T(u,v)$, and multivariate $W(u,v)=1-\det\{\mathrm{Cov}(e)\}/\det\{\mathrm{Cov}(u)\}$.","In the simulation under a two-dimensional independent-cluster 3PL IRT model, all investigated reliability coefficients generally increase with test length $m$ (6 to 120), indicating improved alignment between EAP scores and latent variables. Coefficients based on univariate regression (CTT reliability and PRMSE) and the rescaled coefficient sigma are very similar across $m$ (roughly spanning about 0.4 to 0.9), with CTT reliability observed to be at least as large as PRMSE. The two versions of coefficient $T$ yield smaller values (reported ranges about 0.23–0.73), especially at smaller $m$. Multivariate association measures between 2D EAP scores and 2D latent variables (mutual information and $W$ variants) are largest overall (about 0.55 up to near 0.99), and prediction-vs-measure versions of $W$ differ systematically. Transforming latent variables to percentile ranks leaves invariant measures unchanged but reduces squared correlation, prediction-based $\rho^2$, and prediction-based $W$.","The authors note that normalized reliability coefficients on $[0,1]$ are often not directly comparable because they can correspond to different score pairs and different concepts of association (different notions of “zero” and “perfect” association). They also highlight that it is difficult to propose universal cutoffs for acceptable reliability because the downstream impact of measurement error depends on the intended use of observed scores. Finally, they acknowledge that some of the proposed/less familiar coefficients require further study in real-data and simulation settings under different latent-variable measurement models.","Although the paper frames reliability in a very general way, it stays largely at the population/theoretical level and does not provide a full treatment of finite-sample behavior when model parameters are estimated (e.g., the impact of parameter uncertainty on each association-based coefficient). The numerical study is limited to one measurement-model family (a 2D simple-structure 3PL IRT model with local independence and normally distributed latent traits), so the empirical conclusions may not generalize to misspecification, multidimensional structures with cross-loadings, or local dependence. Several coefficients (e.g., mutual information, sigma via empirical copulas, and $T$ via CODEC) can be sensitive to tuning choices and estimator bias/variance in practice; the paper does not systematically study robustness of these estimators. Practical interpretability is still challenging: even with desiderata, practitioners may struggle to select among coefficients without clearer guidance tied to decision tasks (e.g., classification accuracy, downstream regression attenuation).","The authors propose that future research should study the performance of the less familiar/novel reliability coefficients in both real-data applications and additional simulation settings, including under different latent-variable measurement models. They also suggest developing benchmarks or qualitative interpretation guidance so substantive researchers can better understand and use distinct reliability measures. More broadly, they hope the framework motivates development of additional useful reliability coefficients not covered in the current work.","A natural extension is to incorporate uncertainty from estimating the measurement-model parameters (e.g., via full bootstrap, Bayesian posterior propagation, or analytical delta-method approximations) and evaluate coverage/precision of CIs for each coefficient. Additional work could examine robustness under model misspecification (non-normal latent distributions, local dependence, cross-loadings, differential item functioning) and under autocorrelated/longitudinal measurement settings. Developing software that standardizes computation across common SEM/IRT models (with recommended estimators and diagnostics for sigma/MI/T/W) would materially improve adoption and reproducibility. Finally, linking each association-based reliability to downstream decision metrics (e.g., expected classification error, attenuation in structural relations, fairness metrics) could provide actionable guidance for choosing among coefficients.",2407.00716v2,https://arxiv.org/pdf/2407.00716v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:11:16Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/google-research/google-research/tree/master/high_confidence_ir_eval_using_genai,"This paper proposes statistically reliable confidence intervals (CIs) for information retrieval (IR) evaluation metrics when relevance labels are generated by large language models (LLMs), which can be biased and error-prone. It adapts prediction-powered inference (PPI) to estimate a dataset-level ranking metric by combining many LLM-predicted query scores with a correction term computed from a small set of human-labeled queries, yielding an unbiased estimator and a normal-approximation CI. It further introduces a conformal risk control (CRC) method tailored to ranking metrics that perturbs the LLM’s predicted label distribution to produce optimistic/pessimistic document-level relevance bounds, which propagate to query- and dataset-level metric intervals with coverage guarantees after calibration. Experiments on TREC-DL and Robust04 with DCG@10 show CRC produces tighter intervals than empirical bootstrap while maintaining near-95% coverage, and PPI often achieves target coverage with fewer human-labeled queries but can have wider intervals depending on label accuracy. The study also analyzes robustness to adversarial bias in LLM label distributions and demonstrates CRC can produce per-query CIs that vary with model uncertainty.","The dataset-level metric is defined as a weighted sum of expected relevance: $U(q)=\sum_{d\in D_q}\omega(\mathrm{rank}(d|q))\,\mu(d|q)$ and $U(Q)=\mathbb{E}_{q\sim Q}[U(q)]$. PPI uses $\hat U_{\mathrm{PPI}}(Q)=\frac{1}{N}\sum_{i=1}^N\hat U(q_i)+\frac{1}{n}\sum_{i=1}^n\big(U(q_i)-\hat U(q_i)\big)$ with CI $\hat U_{\mathrm{PPI}}\pm 1.96\sqrt{\hat\sigma^2_{\mathrm{error}}/n+\hat\sigma^2_{\mathrm{pred}}/N}$. CRC constructs an interval $C(Q,\lambda_{\mathrm{high}},\lambda_{\mathrm{low}})=[\hat U_{\mathrm{CRC}}(Q,\lambda_{\mathrm{low}}),\hat U_{\mathrm{CRC}}(Q,\lambda_{\mathrm{high}})]$ where $\hat U_{\mathrm{CRC}}$ is computed from per-document perturbed relevance distributions parameterized by $\lambda$ and calibrated via conformal risk control constraints on miscoverage for upper and lower bounds.","On TREC-DL, empirical bootstrap required roughly 40 human-labeled queries to reach ~95% coverage, while PPI achieved ~95% coverage with fewer than ~20 and CRC with fewer than ~30; on Robust04, bootstrap was near but not consistently at 95% even with 100 labeled queries, while PPI reached ~95% with <~40 and CRC with <~50. CRC delivered the smallest CI widths across the main comparison, with especially large gains on TREC-DL; PPI sometimes produced wider CIs than bootstrap on TREC-DL despite strong coverage. Under adversarial bias in the LLM label distributions (increasing $\beta$), both PPI and CRC maintained >95% coverage but their width advantages depended on label accuracy (deteriorating when $\beta>0.5$). With oracle-improved labels (increasing $\tau\to 1$), both PPI and CRC intervals shrank substantially, and CRC’s per-query intervals could approach zero as labels became nearly perfect.","The methods require an LLM that supports scoring mode to output a full distribution over relevance labels; for models without scoring mode, the authors suggest approximating distributions by sampling multiple stochastic outputs. The authors also note that smoothing the LLM-produced label distribution appears beneficial for CI quality, but selecting/optimizing the amount of smoothing is left as an open question. Finally, experiments only use Flan-UL2 as the LLM labeler, and results may differ with other (potentially stronger) LLMs.","The normal-approximation CI used for PPI (and the implied symmetry assumptions about errors) may be inaccurate for small $n$ or heavy-tailed / skewed per-query metric distributions typical in IR, potentially affecting finite-sample calibration. The CRC guarantees rely on i.i.d./exchangeability assumptions and on the bootstrap-generated calibration batches being representative of the target query distribution; distribution shift across time, topics, or query types could break coverage. The paper focuses on DCG@10 and two benchmarks; it is unclear how robust the procedures are across other metrics (e.g., AP/NDCG variants with different gain/discount), different pooling depths, or highly sparse/long-tail query regimes. Practical deployment would also need careful handling of prompt/model changes over time (nonstationarity), which could require repeated recalibration and may increase annotation costs.",The authors propose extending the approach to LLMs without scoring mode by stochastically generating multiple labels to approximate predictive distributions. They suggest systematically optimizing smoothing of the LLM label distribution and exploring prompt-engineering or fine-tuning to yield distributions better suited for CI construction. They also note the work can be extended to use different and potentially more powerful LLMs than Flan-UL2.,"Develop self-starting or online recalibration variants of CRC/PPI that can maintain coverage under drifting query distributions and evolving LLM behavior (e.g., periodic recalibration with a small rolling set of human labels). Extend the framework to multivariate comparisons (paired CIs for differences between rankers) and to multiple-testing/control when evaluating many systems or query slices simultaneously. Study robustness to dependence structures common in IR evaluation (e.g., correlated queries or shared documents) and to annotation noise in the “human” labels, possibly via hierarchical or Bayesian conformal variants. Provide a reference implementation/package (e.g., Python) with reproducible pipelines and guidelines for choosing calibration sampling schemes ($M$, batch sizes) and smoothing, validated across more datasets and metrics.",2407.02464v1,https://arxiv.org/pdf/2407.02464v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:11:52Z
TRUE,Degradation modeling|Network/infrastructure reliability|Other,"Stochastic process|Parametric (Weibull, etc.)|Bayesian",Interval-censored|Degradation measurements|Mixture of types,Not applicable,Network/cybersecurity|Environmental monitoring|Transportation/logistics|Other,Case study (real dataset)|Other,TRUE,Python|Fortran|Other,Public repository (GitHub/GitLab),https://gitlab.utwente.nl/fmt/degradation-models/ihctmc,"The paper compares homogeneous-time Markov chains (homogeneous continuous-time with exponential hazards and its equivalent homogeneous discrete-time formulation) against inhomogeneous continuous-time Markov chains for multi-state sewer pipe degradation modeling. Inhomogeneous models are parameterized via time-varying hazard rates induced by reliability distributions (Gompertz, Weibull, Log-Logistic, and Log-Normal), with state progression constrained to worsen and allowing transitions to functional failure. Model calibration combines Metropolis–Hastings sampling (to obtain good initial hyperparameter guesses) with SLSQP constrained optimization, and model fit is assessed using AIC, BIC, and RMSE relative to nonparametric Turnbull estimators to account for interval censoring. Using a large real inspection dataset from Breda (NL) across three pipe cohorts and cross-validation, the inhomogeneous models generally achieve better goodness-of-fit than homogeneous models, with Gompertz performing best overall. The authors also note increased overfitting risk for inhomogeneous models beyond the age range supported by training data, motivating improved inference and uncertainty quantification.","The inhomogeneous CTMC is governed by the Forward Kolmogorov equation $\frac{\partial P(t,\tau)}{\partial t}=P(t,\tau)Q(t)$ (and the corresponding master equation for state probabilities), where the rate matrix $Q(t)$ is parameterized via hazard rates $\lambda(t;\theta)$ induced by Exponential, Gompertz, Weibull, Log-Logistic, and Log-Normal families. For homogeneous discrete time, state probabilities evolve by Chapman–Kolmogorov: $\pi_k = \pi_0 P^k$ (with $P$ derived from $Q$ via a matrix exponential for the homogeneous case). Model selection uses $\mathrm{AIC}=2|\theta|-2\ell$ and $\mathrm{BIC}=\ln(|Z|)|\theta|-2\ell$, and predictive error is summarized by RMSE between Markov-chain state probabilities and Turnbull survival estimates.","Across all three cohorts (CMW, CS, PMW), inhomogeneous-time Markov chains achieve the lowest RMSE/AIC/BIC values in most cases, outperforming both homogeneous CTMC (Exponential) and the equivalent homogeneous DTMC. Gompertz IHCTMC is consistently strong: e.g., test RMSEs of 0.0337 (CMW), 0.0468 (CS), and 0.0172 (PMW), generally improving on homogeneous models (e.g., CS test RMSE 0.0583). Reported relative improvements (best inhomogeneous vs. homogeneous) include sizeable RMSE reductions such as 47.2% (CS training) and 32.8% (PMW test), with one exception where PMW test BIC slightly favors homogeneous models. The paper also reports that Weibull IHCTMC performs poorly for PMW (test RMSE 0.0403), attributed to suboptimal parameter inference/local optima. Inhomogeneous models fit well within the data-supported age range (~up to 70 years) but tend to extrapolate faster deterioration beyond that range, indicating overfitting risk.","The authors state that inhomogeneous-time Markov chains have higher computational requirements and pose a risk of overfitting, particularly when extrapolating beyond the age range covered by training inspections. They explicitly note that interval censoring complexity is omitted from their likelihood during calibration, and that this omission warrants further investigation. They also mention that some inhomogeneous fits (e.g., Weibull for PMW) may suffer from convergence to local optima, implying limitations of the current parameter inference procedure.","The evaluation hinges on a single city’s dataset (Breda) and one primary damage code (BAF infiltration), which may limit generalizability to other degradation mechanisms or inspection regimes. The Markov structure enforces monotone worsening with no repair/improvement transitions, so it may not represent effects of interventions or varying inspection/renewal actions without model extension. Goodness-of-fit is largely relative to Turnbull estimators and information criteria; the paper provides limited direct decision/maintenance impact validation (e.g., cost or policy outcomes) and limited uncertainty quantification of predicted state probabilities. Cohorting by material/content may introduce selection/aggregation bias and mask covariate effects; a unified covariate model could change comparative conclusions.","They propose incorporating interval censoring directly into the calibration/likelihood for inhomogeneous Markov chains and assessing whether neglecting it is valid. They suggest extending models to include pipe length and spatial distribution of degradation along a pipe rather than only the most severe observed condition. They propose incorporating covariates without forming cohorts to reduce cohort-selection bias, and testing across multiple cities including domain adaptation (reinforcement/transfer learning). They also call for uncertainty quantification with accurate uncertainty bounds and exploring alternative nonlinear constrained optimization methods to improve inference speed and reduce overfitting, plus applying the models to optimize maintenance and inspection policies.","Develop self-starting/online updating versions of the IHCTMC to update hazards as new inspections arrive, with safeguards against extrapolation-driven overfitting (e.g., regularization or hierarchical priors). Extend the multi-state model to semi-Markov or hidden Markov formulations that better capture sojourn-time distributions and latent condition misclassification from CCTV inspections. Add explicit intervention/repair transitions (imperfect maintenance) and couple the degradation model with an economic or risk-based maintenance optimization layer to quantify policy value. Provide broader benchmarking against non-Markov alternatives used in infrastructure asset management (e.g., ordinal regression with time-varying effects, Bayesian MSM with random effects) and publish reproducible experiment scripts with fixed seeds and configuration files.",2407.12557v1,https://arxiv.org/pdf/2407.12557v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:12:23Z
FALSE,NA,"Bayesian|Parametric (Weibull, etc.)|Other",Mixture of types|Simulated only|Other,Not applicable,Healthcare/medical|Pharmaceutical|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/NourHawila/IRR,"The paper develops three generalized Bayesian hierarchical models to jointly estimate interrater and intrarater reliability for multilevel/nested binary rating data (subjects × raters × time points). It proposes Bayesian versions of an independent random-effects model (BIN), a partially nested/crossed random-effects model (BPN), and a fully nested random-effects model (BFN), and provides formulas for marginal correlations under each model. The models are implemented in Stan (via RStan) and used to produce posterior estimates and credible intervals for agreement/reliability measures such as Conger’s kappa as well as marginal correlations. The methods are evaluated on two real datasets (running gait ratings; radiograph assessments) and via simulations in which datasets are generated under different model assumptions and compared using LOOIC-based model selection. Results show frequentist kappa estimates can differ substantially from model-based Bayesian estimates, and that LOOIC often selects the true or near-optimal model in simulation.","The core modeling contribution is three Bayesian GLMMs for binary outcomes with link $g(\cdot)$ (logit or probit) and hierarchical random effects. BIN: $g(p_{ijk})=X_{ijk}\beta + u_i + v_j + w_k$ with $u_i\sim N(0,\sigma_u^2)$, $v_j\sim N(0,\sigma_v^2)$, $w_k\sim N(0,\sigma_w^2)$ and inverse-gamma priors on variances. BFN: $\text{logit}(p_{ijk})=X_{ijk}\beta + u_i + v_{ij} + w_{ijk}$ with multivariate Normal random effects having LKJ priors on correlation matrices; BPN introduces rater-by-time random effects $v'_{jk}$ with LKJ prior. Derived marginal correlations include BIN: $\text{Corr}_R=\frac{\sigma_u^2+\sigma_w^2}{\sigma_u^2+\sigma_v^2+\sigma_w^2}$ and $\text{Corr}_T=\frac{\sigma_u^2+\sigma_v^2}{\sigma_u^2+\sigma_v^2+\sigma_w^2}$, with analogous expressions for BFN/BPN incorporating $\rho_R$ or $\rho_T$.","On the running gait dataset, the LOOIC-selected BPN model produced interrater/intrarater Conger kappa estimates of 0.24/0.30 versus frequentist estimates 0.44/0.45. On the radiograph dataset, the LOOIC-selected BFN model produced interrater/intrarater kappa of 0.07/0.33 versus frequentist 0.40/0.72, indicating large sensitivity of kappa to modeling assumptions with small numbers of raters/time points. LOOIC model selection favored BPN for running gait (LOOIC 720.44 vs 720.89 BIN vs 741.02 BFN) and favored BFN for radiograph (LOOIC 318.02 vs 396.54 BIN vs 398.81 BPN). Simulation studies (148 datasets per scenario) showed the frequentist approach had worse RMSE than each Bayesian model across scenarios, while the model consistent with the data-generating mechanism was generally optimal or close to optimal; LOOIC frequently selected the true model (e.g., running gait reference: IN→BIN 77.7%, PN→BPN 85.1%, FN→BFN 54.7%).","The authors note the methods can be complex to use due to the number of parameters and the need to ensure proper convergence of Bayesian fits. They state that selecting the most appropriate model among the three requires good understanding of the data, though LOOIC often selects an optimal or near-optimal model. They also state that posterior estimates may be sensitive to prior choices and recommend conducting prior sensitivity analyses.","The work targets reliability of ratings (agreement) rather than reliability engineering (time-to-failure/degradation), so it does not directly address engineering life models, censoring, or maintenance decisions. The focus is on dichotomous outcomes; while extensible, performance for ordinal/multiclass, continuous, or misclassification-with-gold-standard settings is not fully demonstrated here. Practical adoption may be limited by computational cost/diagnostics (Stan) and by the need for careful prior specification and model checking, especially in sparse designs where variance components and correlations can be weakly identified. Comparisons are primarily among the three proposed Bayesian models and a direct frequentist kappa calculation; broader benchmarking against alternative modern Bayesian/likelihood agreement frameworks (e.g., latent-class or item-response style models) is not emphasized.",None stated.,"Extend the joint modeling framework beyond binary ratings to ordinal/multicategory and continuous outcomes in a unified implementation, including robust/weakly-informative priors to improve identifiability in sparse rater×time designs. Develop guidance and automated diagnostics for model selection and convergence (e.g., prior predictive checks, posterior predictive checks tailored to agreement metrics) and quantify sensitivity of kappa/marginal correlations to priors and link choice. Add support for missingness mechanisms and unequal numbers of ratings per subject (incomplete block designs), which are common in practice. Provide a validated software package release (e.g., CRAN) with reproducible vignettes and benchmark datasets for standardized comparison.",2407.12700v1,https://arxiv.org/pdf/2407.12700v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:12:53Z
FALSE,NA,ML-based|Bayesian|Simulation-based|Other,Sensor/condition monitoring|Simulated only|Other,Not applicable,Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/mlcolab/panpe,"This paper proposes Prior-Amortized Neural Posterior Estimation (PANPE), a simulation-based Bayesian inference method using normalizing flows to invert X-ray/neutron reflectometry data and produce multimodal posteriors over thin-film parameters (thickness, density/SLD, roughness, and instrument misalignment/background). The key methodological contribution is “prior amortization”: the network conditions on both the measured reflectivity curve and a parameterized prior (uniform ranges) so one trained model can adapt to changing physics-informed constraints, time-evolving experiments, different q-ranges/discretizations, and combined datasets. The approach emphasizes a mass-covering training objective (forward KL) to avoid missing posterior modes, followed by likelihood-based refinement via importance sampling or MCMC enabled by a GPU-accelerated transfer-matrix simulator. Performance is evaluated on 1000 simulated two-layer cases and on 208 experimental XRR curves (plus a neutron co-refinement example), showing large gains in effective sample size relative to conventional importance sampling and the ability to reject high-likelihood but unphysical solutions via informative priors. Overall, PANPE advances reflectometry inversion by making reliable, fast, probabilistic (multi-solution) analysis practical for real-time/high-throughput settings.","The Bayesian target is the posterior $p(\theta\mid R)\propto p(R\mid\theta)p(\theta)$, with PANPE modeling $p_{\mathrm{NN}}(\theta\mid R,\phi)\approx p(\theta\mid R,\phi)\propto p(R\mid\theta)p(\theta\mid\phi)$ where $\phi$ encodes the (uniform) prior bounds for each parameter. Importance-sampling refinement uses weights $w_i=\frac{p(R\mid\theta_i)p(\theta_i\mid\phi)}{p_{\mathrm{NN}}(\theta_i\mid R,\phi)}$ for samples $\theta_i\sim p_{\mathrm{NN}}(\theta\mid R,\phi)$, and sample efficiency $\epsilon_{\mathrm{eff}}=\frac{(\sum_i w_i)^2}{\sum_i w_i^2}$. Simulated reflectivity includes misalignment/background and noise, e.g. $R_p=(R(q+\Delta q;\theta)\,(1+\Delta R)+R_0)\,e_p$ with $e_p\sim\mathcal{N}(1,s_p)$.","On a test set of 1000 simulated two-layer reflectometry curves, PANPE-IS substantially increases sample efficiency compared with conventional importance sampling from the prior, translating (per the paper’s timing axis for an RTX 2080 Ti) from days/months down to under a minute for many hard cases. The method successfully recovers multimodal posteriors and shows that narrowing physics-informed priors collapses the posterior to the physically relevant mode without requiring retraining. On 208 experimental XRR curves, PANPE-IS yields marginal posteriors consistent with conventional importance sampling while typically reducing inference time to under a second for many samples. The paper highlights a case where an unphysical solution has likelihood more than $10^6$ times larger than the physical one, and shows prior amortization is essential to recover the physical solution when its prior volume fraction is < $10^{-6}$.","The authors note that as the number of layers/free parameters increases (e.g., a showcased 4-layer, 16-parameter case), ambiguity grows and maintaining high sample efficiency may require increasing density-estimator capacity (larger networks or more sophisticated flows such as continuous normalizing flows). They also acknowledge that the hyperprior and other design choices (including q discretization simulation) may need customization for specific applications/experimental conditions. They exclude absorption in their main parameterization, stating it can be added straightforwardly but is omitted due to focus on organic materials.","The method’s guarantees rely on the simulator/likelihood being well-specified (e.g., Gaussian noise approximation to Poisson counting statistics); mismatch between the forward model and real instruments/material physics could undermine posterior validity even if the learned posterior is mass-covering. Evaluation focuses on a specific reflectometry parameterization and largely uniform priors; performance with strongly non-uniform/structured priors or different model discrepancies (e.g., correlated noise, systematic errors) is not thoroughly benchmarked. The approach requires substantial upfront compute (reported ~30 hours on a V100 with extremely large on-the-fly simulation) and access to GPU resources, which may be a barrier for some labs. While refinement uses IS/MCMC, robust diagnostics for convergence/mode coverage after refinement in highly multimodal, high-dimensional cases are not extensively discussed.","The authors suggest investigating more complex prior parameterizations tailored to specific applications and experimental contexts. They propose improving performance in more complex (more-layer) scenarios by increasing model capacity and/or adopting more sophisticated density estimators such as continuous normalizing flows, without changing the overall PANPE framework. They also mention that q discretization simulation and other aspects could be customized to better match specific neutron sources and experimental conditions.","A useful extension would be explicit treatment of model discrepancy (Bayesian calibration) and more realistic count-noise models (Poisson/compound Poisson) to better match neutron/X-ray counting data, especially at low intensities. Developing standardized posterior diagnostics and automated mode-finding/labeling tools for practitioners (beyond clustering examples) would improve usability in routine beamline workflows. A self-starting/online adaptation scheme (continual learning) that updates hyperpriors or embeddings as instruments drift could increase robustness for long campaigns. Packaging the simulator+PANPE into a reproducible, versioned software release (with trained weights, benchmarks, and APIs) would support broader adoption and fair comparison across reflectometry inversion methods.",2407.18648v1,https://arxiv.org/pdf/2407.18648v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:13:35Z
TRUE,Accelerated testing|Warranty analysis|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Right-censored|Mixture of types|Simulated only,Not applicable,Energy/utilities|Semiconductor/electronics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops Bayesian reliability acceptance sampling plans for an adaptive simple step-stress partial accelerated life test (adaptive SSSPALT) under Type-I censoring, where the stress is increased at time $t_1$ only if the observed failures by $t_1$ are below a threshold $m$. For exponential lifetimes under a cumulative exposure model, it derives the likelihood for the adaptive design and formulates a decision-theoretic Bayesian sampling plan (BSPAA) with a general loss function that includes test cost, time cost, salvage value, and an added cost for switching to higher stress. The Bayes decision function is shown to be a threshold rule based on the posterior expectation of the acceptance loss $h(\lambda)$ compared to a rejection cost $C_r$, with monotonicity properties used to simplify the decision boundary. For a quadratic loss $h(\lambda)=a_0+a_1\lambda+a_2\lambda^2$ with $\lambda\sim\text{Gamma}(\alpha,\beta)$ and $\phi\sim\text{Unif}(1,l)$, closed-form expressions for the posterior expectation needed for the decision rule are provided. An optimization algorithm is given to select $(n,t_1,t_2,m)$ by minimizing Bayes risk, and numerical studies plus a data illustration (oil breakdown times under two voltages) compare BSPAA to conventional Bayesian plans under non-accelerated and always-accelerated designs, showing small but consistent Bayes-risk savings in many scenarios.","Adaptive stress rule uses an indicator $\delta=\mathbb{1}(D_1<m)$ and sets $\lambda_0=\lambda$, $\lambda_1=\phi\lambda$ with $\phi>1$; under the CEM the post-$t_1$ survival depends on whether stress changes. The likelihood reduces to $L(\theta\mid x)\propto \lambda^d\,\phi^{\delta d_2}\exp\{-\lambda(w_1+\phi^{\delta}w_2)\}$ where $w_1=\sum_{j=1}^{d_1}z_j+(n-d_1)t_1$ and $w_2=\sum_{j=d_1+1}^{d}(z_j-t_1)+(n-d)(t_2-t_1)$. The Bayes decision is a posterior-loss threshold: accept iff $\varphi(x)=\mathbb{E}[h(\lambda)\mid x]\le C_r$. For quadratic $h$, $\varphi(x)=a_0+a_1\frac{H_1(w_1,w_2,d_1,d_2,1)}{H_1(w_1,w_2,d_1,d_2,0)}+a_2\frac{H_1(w_1,w_2,d_1,d_2,2)}{H_1(w_1,w_2,d_1,d_2,0)}$ where $H_1$ is an integral over $\phi\in(1,l)$ and $\lambda$ yielding gamma/beta-function forms.","In the main illustrated numerical example (hyperparameters $\alpha=3,\beta=1,l=10$; costs $a_0=2,a_1=3,a_2=2,C_a=0.1,v_s=0.2,C_s=0.5,C_t=5,C_r=30$), the optimal BSPAA is $(n_B,t_{1B},t_{2B},m_B)=(3,0.169,0.238,2)$ with $E[\tau_B]=0.220$, $E[D_B]=2.013$, and Bayes risk $R_B=27.704$. Under the same settings, the optimal conventional non-accelerated Bayesian plan (CBSP, equivalent to $m=0$) has $(n^*,t_1^*)=(4,0.193)$ with $R_1=27.837$, and the always-accelerated conventional plan (CBSPA, equivalent to $m=n$) has $(n_A,t_{1A},t_{2A})=(3,0.162,0.238)$ with $R_2=27.723$. The relative risk savings reported are $\text{RRS}_1=0.48\%$ (BSPAA over CBSP) and $\text{RRS}_2=0.07\%$ (BSPAA over CBSPA) for that scenario. In the data illustration using oil breakdown times (30kV normal, 36kV accelerated), with chosen priors/costs, the computed optimal plan is $(n_B,t_{1B},t_{2B},m_B)=(4,18.29,28.29,2)$ and simulated adaptive Type-I datasets demonstrate the accept/reject rule based on $\varphi(y,d_1,d_2)-C_r$.","The authors note the work is developed and illustrated for the exponential lifetime distribution under a two-level (simple) step-stress PALT with Type-I censoring. They indicate extensions are needed to handle other lifetime distributions, other censoring schemes, and more than two-stage step-stress tests. They also mention extending the approach to curtailed (early-termination) Bayesian sampling plans as future work.","The method’s practical implementation depends on numerical optimization and evaluation of integrals/special functions (e.g., incomplete beta terms in $H_1$), but no implementation details or reproducible code are provided, which may hinder adoption and verification. The modeling assumes independent, identical items and an exponential baseline hazard with a multiplicative acceleration factor $\phi$, which may be restrictive for many engineering failure mechanisms where hazards are non-constant or acceleration is not proportional. The decision-theoretic results rely on specified cost parameters (including $C_a, C_t, v_s$ and quadratic-loss coefficients) that may be difficult to elicit reliably; sensitivity analysis is presented but calibration guidance is limited. Comparisons are only against two conventional Bayesian plans (non-accelerated and always-accelerated) rather than broader classes of acceptance sampling designs (e.g., OC-curve constrained plans, non-Bayesian RASP, or multi-stress/optimal-design baselines).","They propose extending the framework beyond the exponential distribution to other lifetime distributions. They also suggest extending to other censoring schemes and to step-stress tests with more than two stress stages. Additionally, they explicitly mention extending the approach to incorporate curtailment (early stopping) procedures for BSP under adaptive step-stress settings.","Develop a self-contained computational package (e.g., R/Python) implementing Bayes-risk evaluation and optimization, including stable routines for the $H_1$ integrals and decision boundaries, and provide reproducible benchmarks. Extend the adaptive rule beyond a fixed failure-count threshold at $t_1$ (e.g., posterior-predictive/adaptive Bayesian stopping or multi-look designs) and analyze operating characteristics (OC curves) under misspecification. Investigate robustness to non-exponential lifetimes, model misspecification, and dependence/heterogeneity across units (frailty/random effects). Generalize the acceleration relationship to common parametric ALT models (Arrhenius, inverse power law, Eyring) and evaluate on additional real ALT/PALT datasets across domains.",2408.00734v1,https://arxiv.org/pdf/2408.00734v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:14:20Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Other,Not provided,https://inventaire-forestier.ign.fr/|https://www.theia-land.fr/|https://CRAN.R-project.org/package=geometry|https://doi.org/10.18637/jss.v106.i01|https://cran.r-project.org/web/packages/geometry/index.html|https://doi.org/10.1371/journal.pone.0225715|https://doi.org/10.2307/2684998|https://doi.org/10.1016/j.rse.2021.112477|https://doi.org/10.18637/jss.v023.i10|https://doi.org/10.1080/02827581.2023.2184488|https://doi.org/10.1139/cjfr-2014-0405|https://doi.org/10.1080/07038992.2018.1461557|https://doi.org/10.3390/rs12162525|https://doi.org/10.4000/archeosciences.3015|https://doi.org/10.1139/X10-195|https://doi.org/10.1080/15481603.2022.2051383|https://doi.org/10.1016/j.rse.2010.03.002|https://doi.org/10.1080/02827589809382966|https://doi.org/10.1007/s13595-020-00976-8|https://doi.org/10.1007/978-1-4614-7138-7|https://doi.org/10.1007/1-4020-4381-3|https://doi.org/10.14214/sf.22026|https://doi.org/10.1111/2041-210X.13650|https://doi.org/10.14214/sf.669|https://doi.org/10.1007/s13595-017-0669-3|https://doi.org/10.1016/j.rse.2020.112061|https://doi.org/10.1016/j.isprsjprs.2022.08.016|https://doi.org/10.1016/j.fecs.2023.100164|https://doi.org/10.14214/sf.a8511|https://doi.org/10.1080/01431169608948776|https://doi.org/10.1016/j.rse.2019.04.006|https://doi.org/10.1080/02827589950152917|https://doi.org/10.1109/IGARSS.1991.579272|https://doi.org/10.1016/j.rse.2007.03.032|https://doi.org/10.1080/07038992.2020.1769470|https://doi.org/10.1016/j.rse.2015.12.039|https://doi.org/10.1016/j.jag.2021.102303|https://doi.org/10.1080/07038992.2016.1207484|https://doi.org/10.1016/j.tree.2018.08.001|https://doi.org/10.1111/j.1472-4642.2012.00887.x,"The paper evaluates how well remote-sensing-based forest attribute maps (stand basal area) generalize across space when models are calibrated over local versus regional sampling domains. Random Forest models are trained using forest inventory plots (NFI and management inventories) with ALS (LiDAR) metrics, Sentinel-2 spectral bands, and forest type as predictors, and are tested both locally and when transferred to other forests. Transferability is quantified via R², RMSE, and mean bias, and the authors diagnose extrapolation by computing whether pixels fall outside the model’s calibration domain in predictor space using a convex hull approach, further classifying extrapolated pixels as “Near” or “Far” based on distance to the nearest calibration plot relative to the mean calibration distance (MCD). Results show local models can perform well in their home forests but often become strongly biased and lose predictive power when applied elsewhere (sometimes yielding negative R²), while regional models reduce but do not eliminate local bias and extrapolation. The study also thins regional plot networks to show higher sampling effort reduces extrapolation and improves fit, and it proposes mapping extrapolation risk to warn managers about potentially unreliable local predictions.","Model performance is assessed with standard goodness-of-fit metrics: $R^2=1-\frac{\sum_{i=1}^n (y_i-\hat y_i)^2}{\sum_{i=1}^n (y_i-\bar y)^2}$, $RMSE=\sqrt{\frac{\sum_{i=1}^n (y_i-\hat y_i)^2}{n-1}}$, and mean bias $=\frac{1}{n}\sum_{i=1}^n (y_i-\hat y_i)$, where $y_i$ is observed plot basal area and $\hat y_i$ is predicted. Extrapolation is determined by whether a pixel’s predictor vector lies outside the convex hull of calibration predictors; extrapolated pixels are labeled “Near” if their distance to the nearest calibration plot is $\le$ MCD and “Far” if that distance is $>$ MCD.","When tested locally, several local models achieved high explained variance (e.g., Deodatie $R^2\approx0.87$, Donon $R^2\approx0.80$, Mouterhouse $R^2\approx0.81$) with RMSE around 3.75–5.63 m²/ha; regional models were somewhat weaker locally (NFI $R^2\approx0.78$, RMSE ≈ 6.05 m²/ha; LSMFI $R^2\approx0.67$, RMSE ≈ 7.6 m²/ha). Under transfer to other forests, predictive power often dropped below 0.5 and could become negative; the most extreme example reported is Deodatie applied to Mouterhouse with $R^2\approx-0.82$ and bias around −9 m²/ha. For extrapolation, regional models had the lowest mean proportion of pixels classified “Far” from the calibration domain over forest datasets (LSMFI ≈ 23.6%, NFI ≈ 24.2%), while local models could be much worse when transferred (e.g., “Far” rates up to ~95% for some cross-forest applications). Sampling-thinning experiments indicated that increasing regional sampling effort (from ~127 plots to ~500 plots across the AOI) substantially reduced extrapolated pixels (roughly from ~73% to ~50%) and reduced RMSE, while mean bias remained relatively small at higher sampling intensities.","The authors note that Random Forest “cannot extrapolate,” which is problematic when a substantial share of predictions lies outside the calibration domain, especially under low sampling intensity and heterogeneous conditions. They also state that convex-hull-based validity-domain assessment is practical only for parsimonious models (they mention fewer than ~9 variables) and becomes impractical for high-dimensional models, requiring alternative approaches. They further acknowledge potential impacts from mismatched acquisition times across auxiliary data sources (ALS flights vs. Sentinel imagery), which may influence model quality and transferability.","Because transferability is assessed across forests with differing plot designs (e.g., angle-count vs. fixed-radius plots) and harmonization choices (e.g., DBH thresholding), residual incompatibilities could confound performance comparisons beyond pure “model transferability.” The convex hull criterion is sensitive to predictor scaling and collinearity; the paper does not detail standardization choices or how categorical predictors (forest type) are handled in the hull construction, which can affect extrapolation flags. The evaluation focuses on point metrics (RMSE/bias/R²) but does not provide uncertainty intervals (e.g., via spatial CV or bootstrap) for these transfer assessments, which would help quantify variability in the reported degradations.","They suggest that for high-dimensional models the convex hull approach is not practical and that other approaches should be considered to evaluate extrapolation levels and support producing locally unbiased maps. They also highlight that the reliability/uncertainty of model predictions at pixel level remains important and needs further study, implying future work on improved uncertainty assessment methods for map products.","A natural extension is to evaluate alternative applicability-domain diagnostics that scale to higher dimensions (e.g., density-based novelty detection, Mahalanobis/robust distances, conformal prediction sets, or one-class classifiers) and compare them against convex hull mapping. Another direction is to incorporate spatially structured validation (blocked spatial CV) and explicit handling of temporal mismatch between field data and remote sensing acquisitions to separate transferability loss from time-lag effects. Finally, developing open, reproducible tooling (e.g., an R package/workflow) to produce basal-area maps alongside extrapolation-risk layers would improve practical adoption by forest managers.",2408.03953v1,https://arxiv.org/pdf/2408.03953v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:15:09Z
TRUE,RUL prediction|Maintenance optimization|Degradation modeling|Failure mode analysis|Other,Stochastic process|Physics-based|Other,Sensor/condition monitoring|Degradation measurements|Other,Condition-based|Predictive|Not applicable,Energy/utilities|Manufacturing (general)|Other,Case study (real dataset)|Simulation study|Other,TRUE,Python|None / Not applicable,Public repository (GitHub/GitLab)|Not provided,https://arxiv.org/abs/2408.05231|https://osf.io/besak/?view_only=e5cab89e58724998848c8405bceb742d|https://github.com/grosed/changepoint_online|https://github.com/Boulder-Investment-Technologies/lppls|https://data.epo.org/publication-server/document/pdf/4369127/A1/2024-05-15,"The paper proposes an unsupervised predictive-maintenance method for industrial systems that detects “initial breakdown” (IB) points in a univariate sensor time series by fitting a Log-Periodic Power Law (LPPL) model derived from renormalization-group/critical-phenomena arguments. Instead of forecasting a future critical time $t_c$, the method assumes $t_c$ is the current time and searches over past window lengths $l_{max}$, selecting the best LPPL fit by mean squared error (MSE) and then declaring a critical point when the fitted LPPL maxima and minima trends are both increasing or both decreasing (Theorem 1). The approach is demonstrated on real reciprocating-compressor condition monitoring data (daily averaged valve opening-angle feature OSV derived from PV diagrams) and is used to predict valve and piston-rod seal failures weeks in advance and to infer likely failure type from the post-IB trend direction. The authors propose an alert criticality scheme based on MSE thresholds (critical/monitoring/irrelevant) and a device-specific predicted failure window rule based on $l_{max}$. In a backtest, they report TP=4, FP=2, FN=1 (precision 0.67, recall 0.8) when matching predicted windows and predicted failure type to maintenance logs/expert annotations, and they compare against an online statistical changepoint method that produces many more detections and requires thresholding/label correlation to be useful.","The LPPL fitting model for the log-transformed signal $W(t)=\log(p(t))$ is $W(t)\approx A+|t_c-t|^m\,[B+C\cos(\omega\log|t_c-t|+\Phi)]$; for fitting they use the linearized form $W(t)\approx A+|t_c-t|^m\,[B+C_1\cos(\omega\log|t_c-t|)+C_2\sin(\omega\log|t_c-t|)]$ with parameters $(A,B,m,C_1,C_2,\omega)$ and constraints such as $0<m<1$ and $2<\omega<8$. The algorithm assumes $t_c$ equals the current time and searches over window length $l_{max}$, choosing the best fit by MSE; a point is labeled an IB (critical point) when the fitted LPPL local maxima and minima exhibit the same trend sign (both increasing or both decreasing) based on linear fits to recent extrema (Theorem 1). A device-specific predicted failure-time window is defined as approximately $[n+l_{max}/2,\,n+90]$ days after the alert index $n$.","Using daily-averaged compressor OSV data from 2019-08-23 to 2022-01-19 and scanning window lengths $30<l_{max}<101$, the method identifies clustered IB alerts typically at least ~14 days before failures are confirmed/repairs occur. Alert criticality is defined by LPPL fit MSE thresholds: critical if $\text{mse}<6\times10^{-5}$, monitoring if $6\times10^{-5}\le\text{mse}<10\times10^{-5}$, and irrelevant otherwise; an example false signal is reported with $\text{mse}=2.49\times10^{-4}$ on 2020-12-14. Summarizing against maintenance logs and expert-diagnosed abnormal periods with both timing-window and failure-type matching, the paper reports TP=4, FP=2, FN=1, giving precision 0.67 and recall 0.8. Compared to an online statistical changepoint method (changepoint_online) at a 75th-percentile threshold, LPPL yields far fewer alerts (14) than the statistical detector (44 decreasing and 33 increasing detections), implying fewer false alarms but requiring LPPL fitting computation.",The authors state the method is only applicable to data describing physical processes that degrade/change through perturbations from interacting elements where phase-transition-like behavior can occur; therefore not all sensor streams are suitable. They also state performance depends on choosing appropriate parameter bounds for LPPL fitting (and the time unit) and requires device-specific adjustment based on the dynamics of the monitored equipment; fitting is computationally expensive due to many parameters and local extrema.,"The approach relies on the existence of LPPL/critical-phenomena signatures in the chosen feature; if the monitored variable is influenced by operating regime changes (load, setpoints, ambient) or control actions, LPPL fits may confound degradation with nonstationary operation, and robustness to covariates/autocorrelation is not systematically analyzed. The “Theorem 1” decision rule is asserted with proof deferred, and sensitivity to extrema-detection choices (how maxima/minima are found, how many extrema are used, regression robustness) is not quantified. Evaluation is limited to a single compressor dataset/feature (OSV) with a small number of failure events, and reported precision/recall may have high uncertainty and potential tuning bias because MSE thresholds are selected based on the same dataset used for evaluation. No discussion is provided on how to implement a self-starting/online calibration for new assets without retrospective maintenance logs to tune $l_{max}$ ranges and MSE thresholds.","The authors state that the proof of Theorem 1 (the criterion linking LPPL extrema trends to critical points/IB points) will be the aim of the next publication. They also suggest applicability to predictive IoT analysis of other industrial systems, implying future work in adapting parameter ranges/time scales to other devices.","A valuable next step would be a broader validation across multiple assets and failure modes, including multi-site datasets, to quantify generalization and uncertainty (confidence intervals) for precision/recall and lead time. Developing an online/streaming implementation with computational shortcuts (e.g., warm-started optimization across adjacent times, reduced parameter search, or Bayesian updating) would make continuous deployment more practical. Extending the method to multivariate inputs (multiple sensors) or incorporating operating-condition covariates (load, speed, ambient) could improve robustness and reduce false positives due to regime shifts. Finally, releasing reference implementations and standardized benchmarks would enable fair comparisons with competing unsupervised prognostics (e.g., change-point ensembles, robust trend tests, and deep self-supervised anomaly/prognostic models).",2408.05231v3,https://arxiv.org/pdf/2408.05231v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:15:47Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Simulation-based|Other",Right-censored|Simulated only|Other,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,R,Supplementary material (Journal/Publisher),https://doi.org/10.5281/zenodo.15855903,"The paper develops a fully Bayesian framework to quantify extreme/rare-event reliability (one-in-a-million level) when the system response is available only through expensive deterministic computer simulation. In a space-mission case study (Earth reentry capsule), uncertain material-property inputs (15 variables) are modeled with parametric distributions (Normal for modulus, Weibull for strengths) whose parameters are inferred from very small laboratory samples using Bayesian priors and Adaptive Metropolis MCMC. The simulator output (peak acceleration) is emulated by a Gaussian process surrogate with squared-exponential covariance, treating the spatial range parameters probabilistically via a Bayesian prior and tuning hyperparameters by leave-one-out cross-validation; results are compared against a regularized REML alternative. The probability of failure is defined as exceedance of a critical acceleration threshold (3000 Gs) and is estimated by nested posterior simulation combining posterior draws of input-distribution parameters and GP parameters with kriging predictions and predictive uncertainty. The study reports posterior distributions for the failure probability under different treatments of GP range-parameter uncertainty and discusses implications for verifying stringent safety goals under extreme risk constraints.","Inputs: strength variables modeled as 2-parameter Weibull with parameters $(\alpha,\beta)$ and modulus variables as Normal with parameters $(\mu,\sigma^2)$; posteriors are sampled via Adaptive Metropolis. The simulator surrogate is a GP: $Z\sim\mathcal N(X\beta,\Sigma)$ with covariance entries $v_{ij}=\exp\{-\sum_{k=1}^K (x_{ik}-x_{jk})^2/\exp(\theta_k)^2\}$ (squared-exponential, anisotropic), and a Bayesian objective for range parameters $\theta$ given by Eq. (6): $\ell_B(\theta)=-\log\pi(\theta)+\tfrac12\log|V(\theta)|+\tfrac{n-q}{2}\log G^2(\theta)+\tfrac12\log|X^T V(\theta)^{-1}X|$. Failure is exceedance of $z_{\text{crit}}$ (3000 Gs; scaled to 3.0), with exceedance probability computed from kriging predictive mean $\hat z_0$ and RMSPE $S_0$ as $1-\Phi\big((z_{\text{crit}}-\hat z_0)/S_0\big)$ and then averaged over Monte Carlo draws (Algorithm 3).","The simulator outputs used to fit the GP surrogate (25 LS-DYNA runs) ranged from 2.474 to 2.749 after rescaling, so no training data lie in the failure region above 3.0 (3000 Gs), making the rare-event estimate heavily extrapolative. Regularized REML for GP range parameters selected penalty $\lambda=2$ by leave-one-out CV; kriging diagnostic correlation (observed vs expected) was 0.393 with signal-to-noise ratio 0.390. Bayesian GP range-parameter prior hyperparameters selected by CV were $\tau=1.25$ and $\nu^2=0.259$ (with kriging diagnostic correlation 0.318). Posterior failure-probability summaries (reported as “Probability × 1,000,000”): Setting A (REML ranges) median 1.56 and mean 2.55 with 95% CI (0.15, 10.89); Setting B (Bayesian ranges) median 0.13 and mean 1.16 with 95% CI (0.01, 6.97).","The authors note that using a constant-mean kriging model leads to interpolation behavior that cannot predict outside the observed output range, and that inputs far from the design revert toward the overall mean. They emphasize a key data limitation: the 25 Latin hypercube simulator runs do not include any outputs exceeding the critical threshold (no samples above 3000 Gs / 3.0), making exceedance-probability estimation problematic and sensitive. They also highlight limited robustness assessment due to the small number of simulator runs and the strong influence of GP spatial range parameters on the resulting failure-probability distribution.","The paper assumes independent parametric input marginals (Normal/Weibull) without addressing possible dependence among material properties; ignoring correlation can materially change rare-event probabilities. The surrogate uses a squared-exponential kernel and largely constant mean; heavy smoothness and potential model misspecification in the tail could bias exceedance estimates, especially since the failure region is outside the observed response range. The nested Monte Carlo approach (drawing inputs and using kriging Normal predictive approximations) may underrepresent uncertainty if the GP predictive distribution is not well-calibrated under extrapolation or if numerical stabilization/nugget choices affect tail behavior. Comparisons are mainly against regularized REML for range parameters; additional rare-event UQ baselines (e.g., importance sampling/adaptive sampling targeted to the failure region, subset simulation, Bayesian quadrature for Pf) are not benchmarked.","They suggest comparing alternative sampling methods/designs for simulator runs to better populate the critical/failure region and assess impact on $P_f$. They recommend generating more simulator runs (beyond 25) and reapplying/expanding the methodology to test robustness and reduce sensitivity to GP range-parameter choices. They also propose model modifications including alternative GP covariance structures, richer mean functions, and alternative output distributional assumptions (e.g., using a $t$-distribution for $Z$) to better capture uncertainty and tail behavior.","A targeted sequential design (adaptive sampling) that actively seeks high-acceleration regions (e.g., Bayesian optimization for exceedance, active learning reliability methods) could substantially improve identifiability of $P_f$ at one-in-a-million levels. Incorporating dependence modeling among the 15 inputs (copulas or multivariate hierarchical priors) and propagating that uncertainty would make the reliability estimate more realistic for composite material systems. Developing diagnostics and calibration checks focused on tail extrapolation (posterior predictive checks, coverage in held-out extreme regions, sensitivity of $P_f$ to kernel/mean/nugget priors) would strengthen credibility for certification-grade claims. Providing an efficient variance-reduction estimator (importance sampling guided by the GP, subset simulation, multilevel splitting) could reduce computational cost and improve precision relative to brute-force nested Monte Carlo.",2408.10083v2,https://arxiv.org/pdf/2408.10083v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:16:26Z
TRUE,Failure mode analysis|Maintenance optimization|RUL prediction|Other,ML-based|Hybrid/Ensemble|Other,Event/count data|Sensor/condition monitoring|Other,Condition-based|Predictive|Not applicable,Transportation/logistics,Simulation study|Other,TRUE,Python|Other,Not provided,http://jmlr.org/papers/v21/20-091.html,"The paper proposes an automated incident-diagnostic suggestion system for railway vehicles, aiming to assist train maintenance technicians by classifying operational incidents into one of 12 physical subsystem causes (e.g., ETCS, doors, brakes, traction). It formulates incident diagnosis as a supervised classification problem using discrete on-board “event” traces collected around the incident timestamp, and introduces feature engineering to mine recurrent, physically plausible sets of events via filtering (relevance score and one-at-a-time augmentation) followed by LCSS-based set extraction. A novel cascaded ensemble classifier is then built from multiple Naive Bayes models trained on time windows of increasing length, with the decision taken from the earliest window that yields a non-null prediction. The approach is trained and validated on real Belgian nationwide operations data (up to 1.5 years training and up to 2 years evaluation across multiple fleets), achieving typically ~80% predictive F1 on out-of-training data and >90% precision/recall on common incident types for some subsystems. The system is deployed in a cloud platform with an expert feedback loop to relabel and retrain models over time, and the authors position future work toward predictive maintenance alerting.",The incident diagnosis is posed as $y=f(x)$ where $x$ is a set of discrete events around the incident time and $y$ is the subsystem label. Event filtering uses a relevance ratio $r = h_{\text{in class}}/h_{\text{in all classes}}$ with a tuned threshold. Each window-specific classifier is Naive Bayes: $\alpha_k=\arg\max_j\left(p(c_j)\prod_{i=1}^{n_{x_k}} p(x_{k i}\mid c_j)\right)$ with smoothed likelihoods $p(x_{k i}\mid c_j)=\frac{\text{card}(x_{k i}\mid c_j)+\beta}{n_{x_k}\beta+\sum_i \text{card}(x_{k i}\mid c_j)}$; classifiers are cascaded over time windows and the first that can predict sets the ensemble output.,"Across fleets, out-of-training (predictive) performance is reported as typically around 80% F1 over classes, while descriptive/training F1 is generally higher. For the AM08 fleet confusion matrix, precision/recall are above 0.9 for several frequent subsystems (ETCS, high voltage equipment, couplings, doors), while some classes are notably weaker (e.g., cabling precision 0.75/recall 0.60; air production recall 0.71; others recall 0.65; traction recall 0 due to extremely low incidence). Hyperparameter exploration shows larger time windows reduce F1, motivating a cascaded-window ensemble that improves F1 versus single-window classifiers at comparable maximum window size, with only marginally fewer classified incidents. The authors state the tuned model trains quickly (about 10 s on 1.5 years of data on an Intel Xeon 2.4 GHz, 8 cores) and predicts in ~100 ms per sample, supporting near-real-time deployment.","The authors note that some incident types are poorly classified even during training, especially rare classes such as traction (insufficient examples). They also explicitly cite two methodological limitations that may explain errors: (1) the feature selection does not account for interactions between features, and (2) contextual variables (e.g., train speed, catenary tensions) are not incorporated.","The approach assumes conditional independence within Naive Bayes and uses a “first classifier to answer is best” cascade heuristic, which may be suboptimal when later windows contain decisive evidence or when early-window features are noisy/missing. Class imbalance is acknowledged but the paper does not clearly describe mitigation (e.g., class weighting, calibrated probabilities), which can bias predictions toward frequent subsystems and reduce reliability for rare failure modes. Evaluation is primarily via cross-validation and temporal holdout on the operator’s data; broader external validation (different operators, sensor vocabularies, or shifting software versions) and robustness to concept drift are not fully characterized despite being a key operational risk. Finally, since the system outputs a single subsystem label, it may not capture multi-cause incidents or hierarchical fault structures common in maintenance diagnostics.","They plan to explore using the extracted event sets to help experts create predictive maintenance alerts to prevent incidents (not only diagnose them). They also intend to investigate LSTM-based models and evaluate the performance of other classifiers. Additionally, they highlight the need to study the long-term human–AI relationship to ensure technicians maintain appropriate trust and critical judgment.","A natural extension is to add context features (operating conditions, speed, environment, configuration/software version) and to model feature interactions (e.g., via tree-based models or probabilistic graphical models) while preserving interpretability. Developing calibrated, uncertainty-aware outputs (top-k causes with confidence, abstention policies) could better support operational decision-making and reduce harm from overconfident suggestions. Drift detection and continual learning protocols aligned with MLOps (including monitoring by class and fleet) would strengthen long-term reliability as vehicle software and event vocabularies evolve. Finally, coupling diagnosis with maintenance decision policies (e.g., risk-based dispatching to specialized workshops) and quantifying impact on delay minutes, availability, and safety would connect the model’s accuracy to reliability/operations outcomes.",2408.10288v1,https://arxiv.org/pdf/2408.10288v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:16:57Z
FALSE,NA,"Parametric (Weibull, etc.)|Simulation-based|Other",Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,R|Other,Public repository (GitHub/GitLab)|Supplementary material (Journal/Publisher),https://github.com/ppernot/2024_PICP/releases/tag/v1.0,"The paper proposes using an interval-based calibration metric—Prediction Interval Coverage Probability (PICP)—to validate average prediction-uncertainty calibration in ML regression, arguing that variance-based metrics (e.g., ZMS, NLL, RCE) become unreliable under heavy-tailed error/uncertainty distributions. It motivates PICP via the empirical observation that z-scores $Z=E/u_E$ across many molecular-property datasets are well modeled by scaled Student’s-$t(\nu)$ distributions, so a fixed 95% enlargement factor $k_{95}\approx1.96$ is accurate when $\nu>3$. The method tests calibration by checking whether the target coverage (0.95) lies within a binomial-proportion confidence interval (Wilson interval) for the empirical PICP, with a relaxed admissible band reflecting the small theoretical deviation under heavy tails. Simulations with $t(\nu)$ samples show the PICP95 test remains valid for heavier tails than ZMS-based testing, expanding the set of “testable” datasets. Applied to 33 published ML materials-property datasets, PICP95 validates 18 sets, invalidates 5, and flags 10 as untestable due to extreme heavy tails/outliers; a local coverage probability (LCP) binning diagnostic is also used to assess conditional calibration consistency across uncertainty strata.","The PICP at nominal level $p$ is estimated as a frequency over $M$ validation points: $\mathrm{PICP}_p=\frac{1}{M}\sum_{i=1}^M \mathbf{1}(|Z_i|\le k_p)$ where $Z=E/u_E$ and $k_p$ is the interval half-width multiplier (e.g., $k_{95}=1.96$). Calibration is validated by checking whether the target $p/100$ lies within a binomial confidence interval for $\mathrm{PICP}_p$ (Wilson interval), implemented for 95% as an admissible band: $0.945\le \mathrm{PICP}^+_{95}$ and $\mathrm{PICP}^-_{95}\le 0.955$.","For scaled Student’s-$t(\nu)$ z-scores, the 95% enlargement factor $k_{95}$ stays close to 1.96 for $\nu\ge 3$ (roughly 1.85–2.0), and using $k_{95}=1.96$ yields at most about a 0.005 deviation from 0.95 coverage for $\nu>3$. Simulations with $M=10{,}000$ show PICP95 intervals are essentially invalid only for $\nu\le 3$, corresponding to about $\beta_{GM}(Z^2)\ge 0.85$, which is less restrictive than ZMS testability (previously around $\nu\ge 6$ or $\beta_{GM}(Z^2)\le 0.8$). On the 33 Jacobs et al. datasets, 10 sets are deemed untestable by PICP (vs 16 by ZMS), enabling testing of ~20% more datasets; among the 23 testable sets, 18 are validated at PICP95 and 5 are rejected, with rejected PICP values not exceeding 0.97. At the 1σ (~67%) level, none of the datasets achieve the target coverage (PICP67 systematically overestimates).","The author notes that best-fit estimates of the Student’s-$t$ degrees of freedom $\nu$ can be sensitive to the fitting method and possible distributional inadequacy, and that a full uncertainty analysis (model inadequacy plus parametric/statistical uncertainty) would be needed for reliable $\nu$ estimates. The approach relies on selecting testable datasets via a robust skewness threshold rather than $\nu$, implying limited interpretability of fitted $\nu$ values. It is explicitly stated that fixing $k_{95}=1.96$ (and the associated PICP testing approach) is not applicable to probability levels other than 0.95. The paper also acknowledges that some datasets remain untestable due to very heavy tails/outliers, limiting applicability without improving those distributions.","The work focuses on average (marginal) coverage and a binned local-coverage diagnostic; it does not provide formal guarantees for conditional coverage (beyond the LCP heuristic) or address known impossibility/compatibility issues between marginal and conditional calibration in finite samples. The proposed testability threshold based on $\beta_{GM}(Z^2)$ is empirically motivated; its sensitivity to sample size, dependence structure, and model misspecification is not thoroughly characterized. The method assumes exchangeability/independence of validation points for binomial CI validity; correlated errors (e.g., clustered materials families) could miscalibrate the CI widths. Comparisons are primarily against ZMS testability rather than a broader set of modern uncertainty-calibration methods (e.g., conformal prediction coverage tests, distribution-free calibration diagnostics).","The paper suggests that to address datasets that are untestable due to heavy tails/outliers, the underlying uncertainty/error distributions should be improved, for example via active learning (as discussed in the referenced prior work). It also indicates that the local coverage (LCP) diagnostic could help improve post-hoc calibration procedures for datasets showing local coverage problems. No other explicit future-work directions are stated.","Extend the interval-based validation beyond the 95% level by using $t(\nu)$-adaptive (estimated) multipliers with uncertainty-aware propagation, or by adopting distribution-free conformal methods that guarantee finite-sample coverage at arbitrary levels. Develop robustness analyses under dependence (time/cluster correlation) and under dataset shift, including corrected confidence intervals for PICP under non-i.i.d. sampling. Provide a principled procedure for selecting/estimating the heavy-tail parameter (or directly modeling $Z$ with mixtures) and quantifying how tail uncertainty impacts coverage tests and “testability” thresholds. Benchmark PICP/LCP against alternative calibration diagnostics (reliability diagrams for intervals, proper scoring rules with robustification, conformal calibration tests) across multiple domains beyond molecular/materials datasets.",2408.13089v2,https://arxiv.org/pdf/2408.13089v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:17:39Z
TRUE,System reliability|Other,Stochastic process|Simulation-based|Other,Simulated only|Other,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.1017/CBO9780511804779|https://doi.org/10.1177/1369433218811540|https://doi.org/10.1080/01621459.1997.10474016|https://doi.org/10.1080/01621459.1997.10474045|https://doi.org/10.48550/arXiv.1604.00860|https://doi.org/10.1080/01621459.1995.10476635|https://doi.org/10.1198/016214501750332848|https://doi.org/10.1093/sysbio/syq085|https://doi.org/10.1214/06-BA127|https://doi.org/10.1111/j.1365-2966.2007.12353.x|https://doi.org/10.1016/j.cma.2013.11.001|https://doi.org/10.1093/mnrasl/slv047|https://doi.org/10.1061/(ASCE)EM.1943-7889.0000839|https://doi.org/10.1002/9781118631980|https://doi.org/10.1061/(ASCE)0733-9399(2007)133:7(816)|https://doi.org/10.1103/PhysRevD.99.084006|https://doi.org/10.1016/S0266-8920(01)00019-4|https://doi.org/10.1016/j.cma.2017.11.021|https://doi.org/10.1080/00031305.1992.10475856|https://victorelvira.github.io/papers/kong92.pdf|https://doi.org/10.1080/01621459.1994.10476469|https://doi.org/10.21105/astro.1306.2144|https://doi.org/10.1007/s11222-014-9512-y|https://doi.org/10.1111/j.1365-2966.2009.14548.x|https://doi.org/10.1016/j.cpc.2008.02.020|https://doi.org/10.1002/eqe.53|https://doi.org/10.1016/j.ymssp.2015.04.023|https://doi.org/10.1061/(ASCE)0733-9399(1986)112:1(85)|https://doi.org/10.1016/j.probengmech.2015.06.006|https://doi.org/10.1016/j.strusafe.2018.05.005|https://doi.org/10.1002/9781118398050,"The paper proposes a reliability-inspired framework for Bayesian evidence estimation and posterior sampling in high-dimensional, potentially multimodal problems by reinterpreting the likelihood as a limit-state function. It expresses the marginal likelihood (evidence) as an integral over a “failure probability function” (FPF) derived from events of the form $\{\mathcal{L}(\theta)>\ell\}$ and then estimates this evidence using a modified Subset Simulation (SuS) procedure with parallel MCMC (adaptive CS-MH) in standard normal space. All samples generated across SuS levels are reused to produce posterior samples via importance resampling, and the paper derives an estimator for the variance of the SuS evidence estimator accounting for within-chain correlation. Performance is demonstrated on multimodal benchmark likelihoods (eggbox, normal shells, and a normal–loggamma mixture) and on a civil/structural finite-element model updating example (10-story shear building), comparing against aBUS and MULTINEST. Empirically, the proposed SuS approach achieves comparable or better evidence estimates and higher effective sample size per likelihood evaluation as dimensionality increases, and it produces more diverse posterior samples than aBUS because it uses samples from all levels rather than only the final level.","Evidence $z=\int L(\theta)\,\pi(\theta)\,d\theta$ is rewritten as $z=\int_0^{L_{\sup}} p_f(l)\,dl$ where $p_f(l)=\int \mathbf{1}(L(\theta)>l)\,\pi(\theta)\,d\theta$ treats $L(\theta)-l$ as a limit-state function. SuS sets level probabilities $p_i=p_c^i$ and adaptively chooses thresholds $\ell_i$ (in log-likelihood space) so that samples target $q(\theta\mid\ell_i)\propto \mathbf{1}(\mathcal{L}(\theta)>\ell_i)\pi(\theta)$; sub-areas are estimated via $\ln \hat z_i=\ln p_i+\ln\left(\frac{1}{N}\sum_{k=1}^N f_i(\theta_{i,k})\right)$ (Eq. 7). Posterior resampling weights are $w_{i,k}=(p_i/z)\,L(\theta_{i,k})$ (Eq. 9), and ESS is approximated using weighted-sample formulas and a correlation-adjustment factor (Eqs. 16–18).","Across 1000 runs with $p_c=0.1$, SuS evidence estimates match analytical values closely on benchmarks and are comparable to aBUS, while MULTINEST shows bias for the ring-shaped “normal shells” example and large bias for the 20D normal–loggamma mixture (Table 1). For posterior sampling efficiency, SuS attains higher $N_{ess}/N_{cal}$ than aBUS in higher dimensions (e.g., Example B: 20D mean 1.91% vs 1.80%; 30D 1.36% vs 1.02%; Example C: 20D 1.83% vs 1.16%; 30D 1.68% vs 0.82%), with lower c.o.v. in all listed cases (Table 2). In FE model updating (10-story shear building), SuS yields accurate stiffness identification when sensor placement and the number of modes are sufficient (Cases 4–6), and it highlights that adding sensors is more beneficial than adding modes; Case 4 shows updated PSD/SV spectra matching synthetic measurements closely and natural-frequency posteriors with small c.o.v. (Table 5).","The paper notes that the algorithm’s parameter settings (e.g., SuS level probability and related choices) follow values considered optimal for reliability estimation and “may not be optimal” for Bayesian inference. In the FE model updating study, it cautions that inference results can be unreliable with too few sensors relative to the number of parameters, and that poor sensor configuration can lead to multimodality or bias (non-identifiability). It also remarks that biases in damping ratios and modal force PSDs may arise due to modeling error (e.g., neglecting neighboring mode influence).","The evidence-as-FPF formulation assumes a finite upper bound $L_{\sup}$ and interchangeability of integrals; in many Bayesian models $L_{\sup}$ may be unknown or unbounded (or hard to bound tightly), which can affect practical robustness. The variance derivation uses approximations (e.g., independence across SuS levels, large-sample assumptions, and linearization that ignores higher-order terms), which may underestimate uncertainty in strongly correlated or highly multimodal settings. Comparisons are limited to aBUS and MULTINEST and do not benchmark against modern gradient-based samplers (e.g., NUTS/HMC) or SMC/tempering methods under equal computational budgets for expensive FE likelihoods. No implementation details (language, runtime, parallelization strategy) are given here, which limits reproducibility and assessment of computational overhead beyond likelihood call counts.","The authors state that SuS parameter settings borrowed from reliability estimation may not be optimal for Bayesian inference, and that the derived formulas for evidence-variance and ESS provide a foundation for further improving (tuning/optimizing) the proposed SuS algorithm for Bayesian inference. They also indicate that data and code will be made available after acceptance, implying future work toward dissemination and reproducibility.","A useful extension would be self-tuning/adaptive selection of $p_c$, sample sizes, and stopping tolerances using the derived variance/ESS to optimize cost–accuracy tradeoffs automatically. Robust variants could relax assumptions on likelihood boundedness and better handle heavy-tailed or unbounded likelihoods (e.g., working entirely in log-space with truncation control). Further study on theoretical convergence and bias under multimodality (including mode-jumping diagnostics and parallel-chain mixing measures) would strengthen reliability of posterior sampling claims. Providing an open-source reference implementation (with FE-surrogate options and parallel computing support) and benchmarking against SMC-tempering and NUTS/HMC on high-dimensional engineering posteriors would improve practical adoption.",2409.19910v2,https://arxiv.org/pdf/2409.19910v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:18:29Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/SalesforceAIResearch/Auto-CEI,"This paper proposes Automatic Curriculum Expert Iteration (AUTO-CEI) to improve the reliability of LLM reasoning by reducing reasoning hallucinations while avoiding excessive conservatism (“laziness” / over-refusal). The method combines Expert Iteration (reward-ranked resampling + SFT) with an automatically updated curriculum that reshapes rewards based on reasoning-trajectory length, compensating “I don’t know” only after sufficient reasoning and penalizing short refusals and assertive wrong answers. The curriculum updates a key reward parameter via hill-climbing to optimize an objective balancing precision on non-refusal answers against refusal rate. Experiments on BoardgameQA, MATH, and Blocksworld show AUTO-CEI yields higher precision with moderate refusal rates compared to SFT, vanilla expert iteration, R-Tuning baselines, and an RLKF(DPO) baseline, and produces a refusal/error pattern that increases with response length as intended. Code is released publicly.","The core reward shaping for a sampled answer $y$ to question $x$ is: $R(x,y)=1$ if $y$ is correct; $R(x,y)=-1$ if $y$ is an assertive wrong answer; and if $y$ is a refusal, $R(x,y)=\frac{1-\exp(-c_2(\mathrm{len}(y)-c_1))}{1+\exp(-c_2(\mathrm{len}(y)-c_1))}$, which transitions from a small penalty for short refusals to a compensatory reward for long, effortful refusals. The curriculum searches $c_1$ to maximize $f=(1-\lambda)P_{\mathrm{Pre}}+\lambda(1-P_{\mathrm{IDK}})$, trading off non-refusal precision vs. refusal rate.","Across three reasoning benchmarks, AUTO-CEI reports precision gains while keeping refusal rates moderate: on BoardgameQA, precision 84.52% with 29.37% IDK (vs. 80.36%/31.27% for SFT+R-Tuning); on MATH, 55.63% precision with 36.08% IDK; and on Blocksworld, 91.53% precision with 18.30% IDK and 74.78% overall accuracy (higher than SFT+R-Tuning’s 38.54% accuracy). The paper states AUTO-CEI boosts precision by roughly 10–24% while maintaining refusal rates of ~18–36% across tasks. Error (hallucination) rate increases with response length for SFT/EI, while AUTO-CEI maintains relatively uniform low error and increases refusal rate as length increases, indicating alignment to an estimated capability boundary.","The authors note the approach is most beneficial on challenging reasoning problems where difficulty correlates with longer reasoning; for simpler/short-reasoning datasets (e.g., GSM8K), R-Tuning already works well, leaving limited room for improvement. They also observe the model may sometimes refuse even when its (implicit) answer would be correct, requiring further adjustment/fine-tuning. They additionally acknowledge the iterative Expert Iteration process can be time-consuming due to multiple rounds of training, though still cheaper than pre-training.","The method depends on a proxy (reasoning-step/response length) for difficulty/capability, which can be confounded by verbosity, prompt format, or stylistic changes rather than true cognitive effort, potentially making the curriculum brittle across domains or templates. Refusal detection appears keyword/template-based, which may be gamed by the model or fail under paraphrases, multilingual settings, or tool-augmented outputs. Evaluation focuses on a specific backbone (Llama-3.1-8B-Instruct) and limited benchmarks; broader reliability properties (calibration, distribution shift, adversarial prompting, or safety/refusal correctness under real-world uncertainty) are not thoroughly validated. Comparisons may be sensitive to implementation details (sampling temperature, number of samples, reward thresholds) and the chosen objective $f$; robustness to these design choices is not fully characterized.","The authors suggest extending the curriculum framework beyond raw reasoning-step counts by incorporating more sophisticated difficulty/capability metrics, such as LLM-predicted statistics for finer-grained curriculum learning. They also indicate the method could potentially be applied to improve reliability for models that use very long test-time reasoning, motivated by work showing longer inference-time reasoning can scale capabilities. They mention further adjustment/fine-tuning to address cases where the model refuses despite being correct.","A useful extension would be to develop a self-starting/online version that adapts $c_1$ per-input (instance-adaptive) rather than using global hill-climbing, and to study stability under non-stationary data. It would also be valuable to replace template-based refusal labeling with a learned refusal/uncertainty classifier and to measure calibration (e.g., selective prediction curves, risk-coverage tradeoffs) directly. Additional work could evaluate robustness under distribution shift, multilingual prompts, and tool-augmented reasoning, and provide ablations on reward functional form and alternative difficulty proxies (entropy, self-consistency dispersion, verifier confidence). Finally, packaging as a reproducible library with standardized benchmarks and scripts (plus compute/cost reporting) would improve practical adoption.",2410.07627v2,https://arxiv.org/pdf/2410.07627v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:19:03Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,None / Not applicable,Not provided,NA,"This paper is about robustly reliable machine learning under instance-targeted data poisoning attacks, not reliability engineering of physical systems. It introduces a modified notion of regularized robustly-reliable learners to avoid vacuous abstention for highly flexible hypothesis classes, and defines optimal “regularized robustly reliable regions” based on agreement among low-complexity hypotheses under a bounded poisoning/mistake budget. The authors provide a generic pointwise-optimal (but potentially expensive) learner and then design more efficient methods for specific complexity measures (e.g., Number of Alternations via bidirectional dynamic programming, Local Margin via nearest-neighbor distance computations, and Global Margin via reductions to bipartite matching/vertex cover with dynamic matching updates). They also give sample-complexity guarantees (probability mass of the certified region) for the Number of Alternations setting under iid data assumptions. Overall, the work advances certified per-instance correctness guarantees against poisoning in theoretical ML/security rather than reliability engineering.","Key definitions include: (1) the RRR learner output $L_{S'}(x,b)=(y,c_{low},c_{high})$ where $y$ is certified correct if the target complexity is $<c_{high}$ and the poisoning/mistake budget is $\le b$, with existence of a hypothesis of complexity $c_{low}$ achieving $\le b$ mistakes and predicting $y$, and nonexistence of any hypothesis of complexity $<c_{high}$ achieving $\le b$ mistakes that predicts $\neq y$. (2) The optimal region $\mathrm{OPTR}^4(S',b,c)$ is the agreement region of hypotheses with complexity $\le c$ and at most $b$ mistakes on $S'$. (3) Example complexity measures: Number of Alterations; Local Margin $r(h,x^*)=\inf_{x':h(x')\neq h(x^*)} d(x^*,x')$ with complexity $C(h,x^*)=1/r(h,x^*)$; Global Margin $r(h,\tilde S)=\min_{(x_i,y_i)\in\tilde S}\inf_{x':h(x')\neq h(x_i)} d(x_i,x')$ with complexity $C(h,\tilde S)=1/r(h,\tilde S)$.","The paper proves optimality characterizations: for any RRR learner $L'$, its certified region is a subset of the optimal agreement-based region, and there exists an RRR learner achieving equality (Theorems 3.1 and 3.4). It gives efficient optimal algorithms for several complexity measures: Number of Alternations (via bidirectional DP with per-test computation in $O(b)$ time), Local Margin (via distances to the $(b+1)$-st nearest opposite-labeled point per class), and Global Margin for binary classification (via reductions to minimum vertex cover/maximum matching in bipartite graphs, leveraging dynamic matching). It provides an iid sample complexity bound for Number of Alternations: if $|S|\ge \tilde O(((b+1)c)/\epsilon)$ (under a stated mass condition around each alternation), then with probability $\ge 1-\delta$ the optimal certified region has distribution mass at least $1-\epsilon$ (Theorem 4.4). For multi-class ($k\ge 3$) Global Margin, achieving optimality is shown NP-hard (Theorem 4.9).","The authors explicitly note that, in general, the guarantees/algorithms can be very expensive computationally, even though they provide meaningful per-instance correctness certificates against poisoning (Discussion/Conclusion).","The work is largely theoretical and its practical impact may depend strongly on the choice and calibration of the complexity/“unnaturalness” measure, which can be subjective and domain-specific. Many results assume access to optimization oracles (ECM/ERM-like subroutines) or rely on preprocessing over many complexity levels, which may be infeasible for large-scale/high-dimensional datasets. Empirical validation on real poisoning benchmarks and comparisons to existing certified defenses are not presented in the provided text, limiting insight into real-world performance tradeoffs.","No explicit future-work agenda is stated beyond the concluding remark that the general computational cost can be high, while suggesting the formulation is a promising approach for meaningful per-instance guarantees against poisoning.","Natural extensions include developing scalable approximations or self-starting/online versions of the certificates for large datasets, and broadening efficient optimal or near-optimal algorithms to richer hypothesis classes beyond the presented complexity measures. Another direction is robustness to additional data issues (distribution shift, label noise mixed with additive poisoning, and dependence/autocorrelation), plus systematic empirical studies on standard poisoning benchmarks with open-source implementations to assess practicality and parameter selection.",2410.10572v4,https://arxiv.org/pdf/2410.10572v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:19:34Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Python,Public repository (GitHub/GitLab),https://github.com/zhexuandliu/MapContinuity-NE-Reliability,"This paper studies the reliability of neighbor-embedding visualization methods (e.g., t-SNE, UMAP) by introducing a leave-one-out (LOO) notion of a continuous embedding map (“LOO-map”) that extends beyond the observed points. Using the LOO-map, the authors attribute common visualization artifacts to intrinsic map discontinuities, distinguishing overconfidence-inducing (OI) discontinuity (apparent over-separation of mixed/overlapping groups) and fracture-inducing (FI) discontinuity (spurious local sub-clusters). They propose two label-free, pointwise diagnostic scores: a perturbation score (sensitivity of an embedded point to moderate input perturbations) and a singularity score (inverse smallest eigenvalue of a Hessian, capturing sensitivity to infinitesimal perturbations). The scores are evaluated on simulated mixtures/manifolds and on real datasets from computer vision (CIFAR-10 features and OOD textures) and single-cell omics, demonstrating improved detection of unreliable embedding points and guidance for hyperparameter (perplexity) selection. An R package and reproducibility code are provided via GitHub.","The LOO-map is defined by freezing existing embedding points and optimizing only the new (or modified) point: $f(x)=\arg\min_y\,L(y;x)$ with $L(y;x)=\sum_{i=1}^n L(w(y_i,y);v_{i,n+1}(X\cup\{x\})) + Z([Y;y])$. The perturbation score for point $i$ is $\max_{e\in\{\pm e_1,\pm e_2,\pm e_3\}}\|f_i(x_i+\lambda e)-y_i\|_2$. The singularity score is $\lambda_{\min}(H_i)^{-1}$, where $H_i$ is the Hessian of the (partial) loss with respect to $y_i$ evaluated at the fitted embedding.","The leave-one-out stability assumption is empirically supported with small normalized embedding errors (e.g., reported $\epsilon_n$ values around 0.03–0.08 across datasets and sample sizes). In OOD detection using CIFAR-10 vs. DTD features, perturbation scores achieve average AUROC ≈ 0.75 for selected clusters, outperforming kernel PCA (≈ 0.698) and one-class SVM (≈ 0.410). For simulated clustering quality under improved perplexity choices guided by singularity scores, the paper reports substantial reductions in standard indices (e.g., DB index dropping from about 0.5982 to 0.3038 in one experiment, alongside strong improvements in within-cluster distance ratio and Wilks’ $\Lambda$). In single-cell datasets, singularity-score “elbow points” correspond to reduced spurious sub-clusters and improved neighborhood preservation without overly merging clusters.","The authors note that while the proposed scores are generally effective, they may not capture all distortion patterns because initialization, iterative optimization behavior, and other hyperparameters can introduce additional artifacts. They also explicitly acknowledge the absence of a formal mathematical framework that rigorously characterizes the LOO-map beyond their analyses.","The approach relies on an empirical “LOO assumption” (small influence of a single point) that may fail for small $n$, highly imbalanced densities, strong outliers, or embeddings with multiple comparable local minima; this could affect diagnostic stability. Computational costs for perturbation scores can be high without approximations/prescreening (seconds per point reported), which may limit routine use on very large datasets or interactive workflows. The diagnostics are tailored to neighbor-embedding objectives; behavior under strong dependence/autocorrelation in inputs, heavy preprocessing pipelines, or alternative similarity constructions may require additional validation.","They propose to further explore connections between classical parametric maps and implicit neighbor-embedding maps to better address topological issues and interpretability. They also plan to improve scalability via more efficient optimization, sparsity and tree-based approximations, and parallel computation.","A useful extension would be to develop self-starting or uncertainty-calibrated versions of the scores that account for optimization randomness (different seeds/initializations) and quantify score variability. Another direction is to generalize the diagnostics to multiview/multimodal embeddings and to streaming/online settings where points are added over time, directly leveraging the LOO-map definition. Providing standardized benchmarks and fast implementations (e.g., GPU-enabled backends) would help broader adoption and allow comparisons across many datasets and embedding methods.",2410.16608v2,https://arxiv.org/pdf/2410.16608v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:20:14Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Failure mode analysis|Other,Stochastic process|ML-based|Other,Sensor/condition monitoring|Degradation measurements|Mixture of types|Other,Condition-based|Predictive|Not applicable,Energy/utilities|Manufacturing (general)|Transportation/logistics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes an online regime-change detection framework for multi-trial experiments that targets changes occurring across trials rather than only within a single time series. It extracts time-frequency representations (smoothed periodograms for stationary-within-trial data and spectrograms for locally stationary data) and compares trials using multiple discrepancy metrics (L1, L2, Wasserstein) plus topological summaries via persistent homology on sublevel-set filtrations. Regime changes are detected by applying a CUSUM scheme to the resulting inter-trial distance sequence, declaring change when the statistic exceeds a threshold. Performance is demonstrated via simulations using (time-varying) AR(2) processes under three stationarity scenarios, showing that topological distances can be robust to shifts/time-warping and can reduce false alarms when changes are mainly translations/rescalings. A real predictive-maintenance case study on the NASA IMS bearing vibration dataset shows the method identifying large end-of-life regime shifts (notably for a failing bearing) using accelerometer signals sampled at 20 kHz.","Spectral features are computed via the periodogram $I(\omega_k)=X(\omega_k)X(\omega_k)^*$ with $X(\omega_k)=\frac{1}{\sqrt{T}}\sum_{t=1}^T X(t)e^{-i2\pi\omega_k t}$, or via the spectrogram $S(\omega_k,t)=\left|\sum_{h=-L}^{L} X(t+h)w(h)e^{-i\omega_k(t+h)}\right|^2$. Topological summaries are persistence diagrams from sublevel-set filtrations, compared using the $L_2$-Wasserstein distance $W_2(Dgm_1,Dgm_2)=\left(\inf_{\gamma}\sum_{x\in Dgm_1}\|x-\gamma(x)\|_2^2\right)^{1/2}$. Online detection uses a CUSUM on trial-to-trial (or past-to-next) discrepancies: $C_r=\max\{0,\,C_{r-1}+(D_r-\mu_D)\}$ and signals when $C_r>h$.","In simulations, the L1 distance reliably detects low-to-high frequency and low-to-mixed frequency regime changes when trials are stationary, while L2 is less pronounced in some settings; Wasserstein can underperform when only part of the spectrum changes. In an evolving-spectrum simulation, L1/L2 showed a false alarm around the segment with rapidly varying peak frequencies, whereas the topological (persistence-diagram Wasserstein) distance correctly highlighted the regime change when a second spectral peak appears. For locally stationary within-trial data (evolving spectrograms), L1/L2 failed to capture the regime change effectively, while topological distances (using $H_0$ and $H_1$ diagrams) increased in the second half where peaks appear/disappear. On the NASA IMS bearing dataset (test-to-failure), all metrics show strong late-trial increases for a failing bearing (bearing 4), aligning with reported defect occurrence near the end of the experiment; non-failing bearings show more fluctuating/less extreme behavior.",None stated.,"The online CUSUM procedure depends on selecting $\mu_D$ and threshold $h$, but the paper does not provide a principled calibration strategy (e.g., ARL/false-alarm control) or guidance for Phase I estimation under realistic noise and drift. The approach assumes that periodogram/spectrogram estimates and persistence diagrams are sufficiently stable; sensitivity to window/bandwidth choices and to strong nonstationarity/autocorrelation structure is not fully stress-tested. Comparisons are primarily against simple distance metrics (L1/L2/Wasserstein) rather than against established change-point/regime-shift baselines in condition monitoring (e.g., classical spectral CPD methods, GLR/CUSUM with parametric models, Bayesian online CPD), limiting conclusions about state-of-the-art performance. Computational cost/scalability of persistent homology over large spectrograms (e.g., many trials at 20 kHz sampling) is not quantified.",None stated.,"Provide an explicit threshold-setting methodology (e.g., simulation-based ARL calibration, bootstrap under no-change, or sequential FDR control) and study detection delay vs. false-alarm tradeoffs for practitioners. Extend the framework to multivariate sensor arrays (multiple accelerometers/channels) using joint spectral/coherence features and corresponding multi-parameter or vectorized topological summaries. Develop robust/self-starting variants that handle unknown and time-varying baselines across trials, missing data, and varying trial lengths, which are common in maintenance logs. Release reproducible code and benchmark on additional public PHM datasets to clarify when topological distances offer material gains over modern nonparametric and Bayesian online change-point methods.",2410.20443v1,https://arxiv.org/pdf/2410.20443v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:20:49Z
TRUE,System reliability|Other,Simulation-based|Stochastic process|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Public repository (GitHub/GitLab),https://github.com/KaiChengDM/Enhanced-SDIS/tree/main|https://github.com/KaiChengDM/SDIS|https://github.com/ERA-Software/Overview/tree/main/Reliability%20Analysis%20Tools/3.%20Subset%20Simulation/SuS_Matlab,"The paper proposes an enhanced Sequential Directional Importance Sampling (SDIS) method for structural reliability analysis to estimate small failure probabilities defined by a limit-state function $g(x)\le0$ (transformed to standard normal space). The enhancements include (i) an unbiased estimator for the first SDIS auxiliary failure probability when sampling until a fixed number of failures is observed, (ii) replacing the first-step crude Monte Carlo estimate with Subset Simulation (SuS) when the inflated failure domain is narrow and crude Monte Carlo is inefficient, and (iii) a Kriging-based active-learning root-finding procedure to detect multiple roots along important directions within a high-probability radius interval. Subsequent SDIS levels estimate ratios of adjacent auxiliary failure probabilities using directional importance sampling with resample-move MCMC to sample important directions. The method is evaluated on multiple benchmark reliability problems (multi-modal failure domains, nonlinear metaball, nonlinear oscillator, and high-dimensional examples) using repeated runs to estimate bias/variance and a relative-efficiency metric based on MSE and model-evaluation cost. Results show enhanced SDIS improves robustness and efficiency versus standard SDIS (notably when multiple roots exist) and can outperform SuS in some low-to-moderate dimensional challenging problems, though performance can degrade relative to SuS in very high dimensions (e.g., $n\ge 100$).","Failure probability is formulated as $P_f=\int I(G(u)\le0)\,\phi_n(u)\,du$. SDIS defines auxiliary probabilities with magnification factors $\sigma_i$: $P_{\sigma_i}=\int I(G(\sigma_i u)\le0)\,\phi_n(u)\,du$, and expresses $P_f$ as $P_f=P_{\sigma_1}\prod_{i=1}^{M-1} S_i$ where $S_i=\mathbb{E}_{h_{\sigma_i}}\left[W_i(A)\right]$ and $W_i(a)=\Pr(G(\sigma_{i+1}Ra)\le0)/\Pr(G(\sigma_iRa)\le0)$. The first-step unbiased estimator when sampling until $n_s$ failures is $\hat P_{\sigma_1}=(n_s-1)/(N-1)$. Conditional directional failure probabilities are computed by finding roots of $G(\sigma r a)=0$ along direction $a$ and applying a chi-distribution CDF expression; root search is restricted to $[r^-,r^+] = [F^{-1}_{\chi_n}(\alpha/2),F^{-1}_{\chi_n}(1-\alpha/2)]$ (with a conservative scaling of $r^-$ by $1/\sigma_i$ in practice).","Across benchmark problems, enhanced SDIS substantially reduces bias versus standard SDIS in cases with multiple roots per direction (e.g., Example 4.1: standard SDIS mean estimate $5.09\times 10^{-6}$ vs reference $3.71\times 10^{-5}$, while enhanced SDIS gives $3.77\times 10^{-5}$). Enhanced SDIS can be markedly more efficient than SuS in difficult low-dimensional problems (Example 4.2 metaball: empirical CoV 0.15 with ~3562 model evaluations vs SuS empirical CoV 2.93 with ~7751 evaluations; relative efficiency 1163.58 vs 1.29). In the nonlinear oscillator (Example 4.3), enhanced SDIS improves over SuS in relative efficiency (27.92 vs 20.89) and avoids the severe bias of standard SDIS (mean $1.27\times 10^{-5}$ vs reference $4.42\times 10^{-5}$). For higher-dimensional examples (Examples 4.4–4.5), enhanced SDIS is competitive and sometimes better at $n=10$ but degrades with dimension, becoming inferior to SuS for $n\ge 100$ in reported settings. Approximated CoV formulas based on independence assumptions underestimate empirical CoV for both SDIS and SuS.","The authors state that, as with standard SDIS, performance of enhanced SDIS “still degenerates with increasing input variable dimension,” attributing this to rising variance of the conditional directional failure probability $\Pr(G(\sigma RA)\le0)$ which requires more intermediate steps. They conclude that for problems with more than 100 input variables, SuS still outperforms enhanced SDIS. They also remark that the SuS estimator used for the first SDIS integral is biased due to correlation across levels and adaptive thresholds, though the bias is considered small relative to estimator CoV.","The enhanced SDIS relies on repeated 1D root-finding along directions; even with Kriging active learning, the method may become costly when each model evaluation is expensive and many directions/levels are needed, and it introduces several heuristic settings (initial Kriging points, $\epsilon$, $\alpha$, switching rule to SuS after $10n_s$ draws) whose sensitivity is not systematically analyzed. The approach assumes an isoprobabilistic transform to independent standard normals and implicitly depends on accurate modeling of input distributions and transformations; robustness to strong dependence/non-Gaussian copulas is not demonstrated. The MCMC/resample-move step uses conditional sampling with target acceptance behavior; mixing/auto-correlation effects and their impact on variance estimates are only approximately handled (independence assumptions acknowledged), and no diagnostics or effective sample size analysis is provided. Empirical comparisons focus on select benchmarks; broader validation on real structural models (e.g., finite-element models) and guidance on parameter tuning for practitioners are limited.",None stated.,"Develop self-starting/adaptive rules for choosing $\sigma_1$, $n_s$, and Kriging stopping/tolerance parameters based on online accuracy/cost trade-offs, and provide theoretical or empirical sensitivity studies. Extend the framework to dependent/non-Gaussian inputs with more general transformations or copula-aware directional sampling. Add variance-estimation methods that account for MCMC dependence (e.g., batch means, spectral variance) and provide practical diagnostics for convergence and mixing. Demonstrate performance on large-scale real structural reliability case studies (computational mechanics models) and provide a packaged implementation with documented defaults and reproducibility scripts beyond MATLAB (e.g., Python).",2410.21350v1,https://arxiv.org/pdf/2410.21350v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:21:35Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Other",Right-censored|Left-censored|Complete lifetime data|Mixture of types,Not applicable,Semiconductor/electronics|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,http://www.backblaze.com/cloud-storage/resources/hard-drive-test-data,"The paper proposes two optimal subsampling strategies for massive reliability lifetime datasets with heavy right censoring (and possible left truncation), aimed at efficiently estimating parameters of parametric lifetime distributions via a subsampling-based pseudo-MLE. The first method (RDS) targets moderately high censoring and uses L-optimality (minimizing the trace of an asymptotic covariance-related matrix) to derive subsampling probabilities proportional to the score norm; the second method (RDCS) targets extremely high censoring by deterministically including all uncensored failures and optimally subsampling only from censored observations. The authors derive consistency and asymptotic normality of both subsampling estimators and provide practical three-step algorithms using a pilot estimate to approximate the optimal probabilities. Performance is demonstrated via Monte Carlo simulations for exponential, Weibull, and GLFP lifetime models and a real Backblaze hard-drive field dataset, showing lower RMSE/bias than uniform subsampling with substantial computation-time savings relative to full-data MLE. The work advances reliability big-data analysis by providing censoring-aware optimal subsampling designs specifically tailored to heavily censored field failure data.","The full-data log-likelihood for left-truncated, right-censored lifetime data is $\ell_f(\theta)=\frac1n\sum_{i=1}^n\{(1-C_i)\log f(t_i;\theta)+C_i\log[1-F(t_i;\theta)]-\log[1-F(t_{il};\theta)]\}$. The general subsampling estimator maximizes a weighted pseudo log-likelihood $\ell_g^*(\theta)=\frac1r\sum_{i=1}^r\pi_i^{*-1} \ell(x_i^*,\theta)$. The RDS L-optimal subsampling probabilities are $\pi_i\propto\|\dot\ell_i(\hat\theta)\|$, while the RDCS probabilities for censored units ($i>n_0$) are $\tilde\pi_i\propto\left\|\partial_\theta\log(1-F(t_i;\hat\theta)) - \partial_\theta\log(1-F(t_{il};\hat\theta))\right\|$, with all failures included with probability 1.","In the Backblaze ST4000DM000 case study (2016–2018), both proposed methods (RDS and RDCS) achieve substantially smaller RMSE and bias than uniform subsampling for GLFP parameter estimation, while reducing mean computation time from ~20–34s (full data) to ~3s (subsampling) for the reported settings (e.g., $r=3000$, $r_0=500$, 500 repeats). In simulation under extreme censoring (e.g., $\alpha=0.9993$), RDCS yields RMSEs orders of magnitude smaller than uniform subsampling for Weibull/GLFP parameters, while remaining far faster than full-data MLE. The paper reports empirical guidance that RDS is preferable when failures are not too rare (approximately $n_0>r/2$), whereas RDCS is preferable when failures are rare ($n_0<r/2$). Estimated MTTF from Backblaze data shows RDCS closest to full-data estimates (e.g., 2016: RDCS 1,020,375 hours vs full 1,050,008; uniform greatly overestimates).","The authors note that real-world reliability field data can contain outliers, which may introduce bias and affect the asymptotic properties of MLE and, by extension, the proposed subsampling estimators. They indicate that handling outliers is not addressed in this work and requires further study.","The work assumes independent units and that censoring/truncation times are independent of failure time; in many field settings (e.g., usage heterogeneity, informative censoring, fleet effects), these assumptions can be violated and could degrade estimator validity. The paper provides algorithms but does not specify an implementation platform or release code, which may hinder adoption and reproducibility. The methods are developed for parametric lifetime models; robustness to model misspecification (e.g., incorrect GLFP/Weibull form) and to temporal dependence in covariate-driven monitoring data is not explored.","The authors propose extending the optimal subsampling methods to settings with outliers in raw reliability data, since outliers can bias estimators and impact the asymptotic properties relied upon for optimal design. They suggest studying censoring-aware optimal subsampling procedures that remain valid and efficient in the presence of outliers.","Useful extensions include developing robust or self-normalized subsampling probabilities that reduce sensitivity to pilot-estimate error and model misspecification, and adapting the approach to informative censoring or dependence/heterogeneity (frailty or hierarchical field-data models). Another direction is extending the subsampling optimality theory to semi-parametric models (e.g., Cox/AFT with covariates) and to recurrent-event or repairable-system data common in reliability. Providing an open-source implementation (e.g., R/Python) with numerically stable optimization and variance estimation would materially improve practical impact.",2410.22751v1,https://arxiv.org/pdf/2410.22751v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:22:11Z
TRUE,Life distribution modeling|Accelerated testing|System reliability|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Interval-censored|Right-censored|Mixture of types|Other,Not applicable,Semiconductor/electronics|Other,Simulation study|Other,TRUE,R,Not provided,https://arxiv.org/abs/2411.01324,"The paper develops reliability acceptance sampling plans (RASPs) for life tests conducted under progressive Type-I interval censoring (PIC-I) when there are multiple competing causes of failure that may be independent or dependent. It proposes a general likelihood framework for PIC-I competing-risks data and derives a Fisher information matrix expression that depends only on first-order derivatives, then establishes consistency and asymptotic normality of MLEs under regularity conditions. Dependence among competing risks is modeled via a shared gamma frailty model with Weibull cause-specific potential failure times (common shape parameter), with the independent competing-risks model recovered as the limiting case as frailty variance \(\nu\to 0\). Using the asymptotic normality and the delta method, the authors derive closed-form formulas for the acceptance limit \(\pi_c\) and required sample size \(n\) to meet producer’s and consumer’s risks at a specified time \(t_0\). They further design c-optimal PIC-I schemes (minimizing the asymptotic variance of \(\hat{\bar F}_T(t_0)\)) with and without explicit budget constraints using a cost model, and validate performance via numerical experiments, a real-data illustration (ARC-1 VHF units), and a simulation study (5,000 replications).","PIC-I competing risks likelihood is built from interval failure probabilities \(q_{ij}=\frac{G(j,L_{i-1})-G(j,L_i)}{\bar F_T(L_{i-1})}\) and \(q_i=\frac{\bar F_T(L_{i-1})-\bar F_T(L_i)}{\bar F_T(L_{i-1})}\). The Fisher information under decision variables \(\zeta\) is \(I(\theta\mid\zeta)=\sum_{i=1}^M\big[\sum_{j=1}^J \frac{E[N_i]}{q_{ij}}\nabla q_{ij}\nabla q_{ij}^T+\frac{E[N_i]}{1-q_i}\nabla q_i\nabla q_i^T\big]\). For sampling plans, the asymptotic acceptance limit and sample size are \(\pi_c=\frac{\pi_0 S_1 z_{\beta}-\pi_1 S_0 z_{1-\alpha}}{S_1 z_{\beta}-S_0 z_{1-\alpha}}\) and \(n=\left(\frac{S_1 z_{\beta}-S_0 z_{1-\alpha}}{\pi_0-\pi_1}\right)^2\), where \(S^2=\nabla F_T(t_0)^T I_1(\theta\mid\zeta)^{-1}\nabla F_T(t_0)\).","Under the shared gamma frailty Weibull model, increasing dependence (larger frailty variance \(\nu\)) increases both the required sample size \(n\) and the acceptance limit \(\pi_c\) across examined PIC-I designs (e.g., Table 1 shows for \(d=1.5\), \(h=0.3\), \(M=6\), \(p=0.2\): \(\nu=0\) gives \(\pi_c\approx0.547,\ n^*=36\) while \(\nu=1\) gives \(\pi_c\approx0.629,\ n^*\approx77\)). In the c-optimal design without budget constraints, the optimal inspection interval \(h^*\) tends to increase with \(\nu\) and with withdrawal proportion \(p\), implying less frequent inspections under stronger dependence and heavier withdrawals. With budget constraints, optimal plans typically adjust by modestly increasing the number of inspections \(M\) as budget increases and shortening \(h\), while \(n\) is relatively insensitive to budget at fixed \(\nu\) and \(p\); expected failures and expected test duration increase with dependence. Simulation (5,000 datasets per plan) shows estimated producer/consumer risks are close to targets (e.g., \(\hat\alpha\approx0.05\) and \(\hat\beta\) around 0.11–0.13 in reported scenarios) and MLE-based reliability estimates are close to true values with roughly symmetric sampling distributions.",None stated.,"The approach relies on large-sample normal approximations (delta method) for \(\hat{\bar F}_T(t_0)\); accuracy may degrade for small \(n\), heavy censoring/withdrawals, or extreme reliability targets. The dependent model is restricted to positive dependence induced by a shared gamma frailty and assumes Weibull causes with a common shape parameter, which may be misspecified in practice. Plan construction appears to treat model parameters as known/guessed (via prior tests or MLE plug-in), so uncertainty from Phase I estimation and robustness to misspecification are not fully addressed. The work focuses on single-time-point reliability criteria \(F_T(t_0)\); plans optimized for other criteria (quantiles/MTTF, multiple time points) are not developed.","The authors suggest extending the dependence modeling beyond frailty to copula-based competing risks models. They also propose developing Bayesian RASPs that incorporate prior information, as prior knowledge is often available in practical reliability settings.","Develop self-starting or two-stage (Phase I/II) RASP procedures that explicitly account for parameter-estimation uncertainty and guarantee risk levels under plug-in estimation. Extend the framework to allow negative dependence or more flexible dependence structures (e.g., vine copulas) and to relax the common-shape Weibull assumption across causes. Provide finite-sample calibration (e.g., bootstrap or exact/simulation-based adjustment) for \(\pi_c\) and \(n\) under heavy censoring to improve risk control. Release an implementation package (e.g., R) to compute \(q_{ij}\), Fisher information, and optimize \((M,h,p)\) under costs, improving reproducibility and adoption.",2411.01324v2,https://arxiv.org/pdf/2411.01324v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:22:50Z
FALSE,NA,ML-based|Other,Other,Not applicable,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This paper is about theoretical machine learning (reliable agnostic PAC learning), not reliability engineering. It studies “reliable learning” of halfspaces under Gaussian marginals, where one-sided errors (e.g., false positives) are constrained while minimizing the other error type up to OPT+ε. The main contribution is an algorithm for reliably learning Gaussian halfspaces with sample/time complexity that depends quasi-polynomially on 1/ε and as d^{O(log(min{1/α,1/ε}))}, where α is the bias of the optimal halfspace, improving over known agnostic-learning complexity bounds. The paper also proves a nearly matching Statistical Query (SQ) lower bound showing any SQ algorithm needs complexity d^{Ω(log(1/α))} (for ε<α/2), indicating the d-dependence is essentially optimal. Conceptually, it establishes a computational separation between reliable agnostic learning and standard agnostic learning for halfspaces under Gaussian marginals.","The target concept class is halfspaces (linear threshold functions) $f(x)=\operatorname{sign}(\langle w,x\rangle-t)$ with $x\sim \mathcal N(0,I)$. Positive reliable learning requires $\Pr[h(x)=1\wedge y=-1]\le \varepsilon$ (few false positives) and $\Pr[h(x)=-1\wedge y=1]\le \mathrm{OPT}+\varepsilon$, where $\mathrm{OPT}=\min_{f\in \mathcal G(D)}\Pr[f(x)=-1\wedge y=1]$ and $\mathcal G(D)=\{f:\Pr[f(x)=1\wedge y=-1]=0\}\cup\{f\equiv -1\}$. The bias of a hypothesis under the Gaussian marginal is $\alpha(h)=\min(\Pr[h(x)=1],\Pr[h(x)=-1])$.","Main upper bound (Theorem 1.3): there is a poly(N,d)-time algorithm using $N=d^{O(\log(\min\{1/\alpha,1/\varepsilon\}))}\,\min\big(2^{\log(1/\varepsilon)^{O(\log(1/\alpha))}},\,2^{\mathrm{poly}(1/\varepsilon)}\big)$ samples that outputs an $\varepsilon$-reliable halfspace for Gaussian marginals. In particular, for constant bias $\alpha=\Theta(1)$, the dependence on $d$ is polynomial and the dependence on $1/\varepsilon$ is quasi-polynomial, improving over prior reliable learners obtained via reduction to agnostic learning (with complexity about $d^{\mathrm{poly}(1/\varepsilon)}$). Main lower bound: for $\varepsilon<\alpha/2$, any SQ algorithm requires complexity $d^{\Omega(\log(1/\alpha))}$ (via tolerance $\le d^{-\Omega(\log(1/\alpha))}$ or exponentially many queries), suggesting the $d^{\Omega(\log(1/\alpha))}$ dependence is unavoidable in the SQ model.",None stated.,"The work targets a distribution-specific setting (standard Gaussian marginals) and does not establish analogous guarantees for non-Gaussian, heavy-tailed, or dependent (autocorrelated) feature distributions. Results are primarily in the SQ/theoretical learning framework and are not validated on empirical datasets, so practical performance, constants, and implementation considerations are unknown. The algorithm’s complexity can become large for small bias $\alpha$ and/or very small $\varepsilon$ (quasi-polynomial/exponential terms), potentially limiting practical use despite asymptotic improvements.","The authors ask whether the dependence on $\varepsilon$ can be improved, e.g., to obtain a reliable learner with complexity $d^{O(\log(1/\alpha))}\,\mathrm{poly}(1/\varepsilon)$. They also ask whether similarly efficient reliable learners can be designed under more general marginal distributions (e.g., strongly log-concave or discrete distributions). More broadly, they propose characterizing computational separations between reliable and agnostic learning for other natural concept classes.","Develop a practically implementable version with explicit constants, numerical stability guidance, and open-source code, plus empirical benchmarks against one-sided cost-sensitive baselines. Extend the techniques to handle unknown/estimated Gaussian parameters (non-identity covariance), covariate shift beyond the Gaussian case, or mild dependence in samples. Study robustness to model misspecification (approximate Gaussian marginals) and derive finite-sample, non-asymptotic error bounds that are directly interpretable for practitioners.",2411.11238v1,https://arxiv.org/pdf/2411.11238v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:23:11Z
TRUE,RUL prediction|Degradation modeling|Maintenance optimization,ML-based|Bayesian|Hybrid/Ensemble,Degradation measurements|Sensor/condition monitoring|Right-censored,Predictive|Condition-based,Transportation/logistics|Manufacturing (general),Simulation study,TRUE,Python|Other,Not provided,http://dx.doi.org/10.6084/m9.figshare.853801,"The paper proposes HRP, a hybrid remaining useful life (RUL) interval prediction method for aeroengine prognostics that couples temporal feature extraction (gated recurrent-style sequence learning) with a modified Gaussian Process Regression (GPR). The model uses extracted hidden-state temporal features as inputs to GPR to produce both point RUL estimates and predictive intervals, aiming to improve uncertainty quantification compared to common deep-learning point predictors. It also incorporates feature-importance analysis (permutation-based, and additionally mentions using random-forest regression adaptively) to provide partial interpretability by ranking sensor contributions to degradation. Experiments on the NASA C-MAPSS turbofan benchmark (FD001–FD004) evaluate accuracy (RMSE) and interval quality (NAW and CWC) and compare against several baselines, reporting consistently narrower intervals and competitive RMSE, especially on the more challenging FD002 subset. The work targets practical PHM/predictive maintenance by delivering actionable uncertainty bounds and sensor importance for decision support.","Temporal features are extracted into a hidden-state vector $h=f_{\mathrm{tem}}(\{X_t\}_{t=1}^L;\theta_{\mathrm{tem}})$ and fed to a GP prior $y\sim GP(\mu,K)$ with squared-exponential kernel $k(h,h')=\tau^2\exp\{-\|h-h'\|^2/(2\eta^2)\}$. With Gaussian observation noise, the predictive mean is $\hat y_*=K(h_*)^\top (K+\delta^2 I)^{-1}y$ and the predictive variance is $k_m(h_*)=k(h_*,h_*)-K(h_*)^\top (K+\delta^2 I)^{-1}K(h_*)$. A $(1-\alpha)$ prediction interval is $[y_L,y_U]=[\hat y_* - z_{\alpha/2}\sqrt{k_m(h_*)},\; \hat y_* + z_{\alpha/2}\sqrt{k_m(h_*)}]$ (e.g., $z_{0.025}=1.96$ for 95%).","On C-MAPSS, HRP achieves RMSE of 13.09 (FD001), 12.33 (FD002), 13.49 (FD003), and 19.65 (FD004); it is best among the listed baselines on FD002 and FD004 while remaining competitive on FD001/FD003. For interval quality, HRP reports NAW(%) of 21.00, 29.00, 23.00, and 28.00 for FD001–FD004 respectively, substantially narrower than the compared interval methods (e.g., FD001 NAW 37.70–54.00). The paper claims NAW is reduced by at least 50.70% on average versus other models, and CWC is reduced by at least 50.16% on average on FD001–FD003. Example 95% prediction intervals shown for sampled engines generally cover the true RUL trajectory while tightening as more cycles are observed.","The authors note scalability challenges of GPR for large time-series datasets due to the need to compute/store an $n\times n$ covariance matrix (computational complexity $O(n^3)$, memory $O(n^2)$). They also identify “exponentially increasing training times” in Gaussian regression as data size grows as an outstanding challenge for future work. They state that data are available only “upon reasonable request,” limiting immediate reproducibility.","The interval construction uses the standard normal quantile with GP predictive variance, which can be miscalibrated under distribution shift, non-Gaussian noise, or temporal dependence not fully captured by the feature extractor; no formal calibration (e.g., conformal) is assessed. The evaluation is limited to the C-MAPSS benchmark; generalization to other assets/sensor suites and real operational data is not demonstrated. Comparisons may be incomplete for scalable GP variants (sparse/inducing point GPs) and modern probabilistic deep models; computational cost and latency in online settings are not quantified. The method’s “interpretability” is mainly permutation importance over engineered/selected sensors, which can be unstable with correlated sensors and does not provide causal fault attribution.","They propose incorporating transfer learning across different but similar C-MAPSS sub-datasets to improve predictive outcomes. They also suggest addressing the training-time growth of Gaussian regression as dataset size increases, i.e., improving scalability of the GP component.","Evaluate and improve uncertainty calibration under dataset shift and varying operating conditions (e.g., conformal prediction for time series, or calibration metrics such as coverage vs. nominal across horizons). Develop scalable GP variants (sparse/variational/structured kernel interpolation) and report computational/latency tradeoffs for real-time deployment. Extend the framework to multivariate/system-level RUL and maintenance policy optimization with explicit cost/risk objectives. Provide open-source implementation and standardized benchmarking (including ablations on temporal extractor, kernel choice, and importance method) to strengthen reproducibility and clarify which components drive gains.",2411.15185v1,https://arxiv.org/pdf/2411.15185v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:23:49Z
TRUE,Maintenance optimization|System reliability|Degradation modeling,Stochastic process|Simulation-based|Other,Simulated only,Condition-based|Block replacement|Opportunistic|Not applicable|Other,Energy/utilities|Network/cybersecurity|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper develops a continuous-time reliability/availability model for a complex redundant multi-state system with cold-standby units, internal degradation, external shocks, random inspections, and both corrective repair and preventive maintenance. All key times (internal degradation progression, inter-shock times, inspection intervals, repair times, preventive-maintenance times, and repairperson vacations) are modeled with phase-type distributions, and event dynamics are captured via a Marked Markovian Arrival Process (MMAP) with multiple event types. Using matrix-analytic/matrix-algorithmic methods, the authors derive transient and stationary distributions and compute performance measures including availability, time proportions in macro-states, mean operational time, and event rates (repairable/non-repairable failures, inspections, returns from vacation, new system starts). A cost/reward structure is added through state-dependent profit/cost vectors plus event-triggered fixed costs to compute transient and steady-state net profit measures. A numerical example (with only simulated/parameterized inputs) optimizes vacation-time distribution parameters and the vacation threshold R, showing the best net profit for n=4 units with preventive maintenance and Erlang-distributed vacations (R=3), while the maximum availability occurs under a different policy (n=4, R=4, no PM, exponential vacations).","The system is modeled by an MMAP with generator blocks \(\{D_O,D_A,D_B,D_C,D_D,D_{CD},D_E,D_F,D_{NS}\}\) and embedded generator \(D=\sum_Y D_Y\). Transient state probabilities are \(p(t)=\phi e^{Dt}\), and stationary probabilities satisfy \(\pi D=0\) with \(\pi e=1\). Availability is computed as \(A(t)=1-\sum_{k=1}^n\sum_{x\in\{v,nv\}} p_{E_k^{x}}(t)e\) and its steady-state analogue \(A=1-\sum_{k=1}^n\sum_{x\in\{v,nv\}} \pi_{E_k^{x}}e\); net-profit rate in steady state uses \(\Phi=\pi\,nr-\pi\,nc\) with additional fixed-cost terms built from MMAP event rates (Eq. (9)).","In the numerical study optimizing vacation-time parameters (exponential vs 2-stage generalized Erlang), threshold R, number of units n\in\{2,3,4\}, and presence/absence of preventive maintenance, the maximum steady-state net total profit rate is 8.2506 for n=4, R=3, with preventive maintenance and generalized Erlang vacation time (reported \(V\) with rates about 0.8297104 and 0.8297099). For that optimal-profit system, steady-state availability is A=0.8210, and steady-state event rates include repairable failures 0.0487, major inspections 0.0064, non-repairable failures 0.0261, returns to work 0.0191, returns-to-work starting a new vacation 0.0985, vacation-after-repair 0.0125, and new-system starts 0.0065. The highest availability observed in the comparison is 0.8384 for n=4, R=4, without preventive maintenance, under Erlang vacations (very close to 0.8380 under exponential). The results illustrate that maximizing availability and maximizing profit lead to different optimal policies/parameters.","The authors assume inspection time is negligible (motivated by integrated sensors), noting that some systems may violate this and would require introducing a random inspection duration. They also frame the model as relying on phase-type distributions for all embedded times, which is an assumption made for tractability and generality via PH density. No additional explicit limitations are stated beyond these modeling assumptions.","The numerical example is purely parameter-driven and does not validate the model on real failure/maintenance/inspection data, so practical calibration and identifiability of PH/MMAP parameters are unaddressed. The model assumes Markovian dynamics with PH representations and (implicitly) conditional independence structures that may not hold with covariates, aging, or operational/environmental dependence beyond the modeled shocks. The optimization uses a local gradient-based interior-point method, so global optimality of vacation-time parameters is not guaranteed, especially with potentially nonconvex profit surfaces. Comparisons are internal (policy variants within the proposed framework) rather than benchmarking against alternative reliability/maintenance modeling approaches (e.g., semi-Markov, MDP-based PM, or simulation-based CBM).","They propose extending the methodology to redundant systems with an indeterminate number of online (active) units. They also suggest relaxing the negligible-inspection-time assumption by incorporating a random inspection time for systems where inspections are not instantaneous, which would require expanding the state space and matrix structure. They note the same matrix-algorithmic framework can be adapted to these extensions.","A valuable next step would be statistical estimation/calibration procedures for fitting the PH/MMAP components from field data (including goodness-of-fit and uncertainty quantification), enabling practical deployment. Extending the model to handle covariate effects and dependence (e.g., shock intensity varying with environment/usage, or repair quality/imperfection) would improve realism. Developing open-source software (e.g., an R/Python package) and scalable algorithms for larger n/state spaces (state aggregation, truncation, or tensor methods) would broaden usability. Finally, adding formal global-optimization or robust-optimization (parameter uncertainty) for the vacation/PM policy would strengthen decision support.",2411.19104v2,https://arxiv.org/pdf/2411.19104v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:24:31Z
FALSE,NA,ML-based,Mixture of types|Other,Not applicable,Healthcare/medical|Other,Simulation study|Case study (real dataset),TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/ZexuanSun/Early-stopping-VI,"This preprint proposes a scalable framework to estimate model-agnostic variable importance (VI) and Shapley values for black-box predictors trained with gradient-based iterative methods (e.g., neural networks and gradient-boosted decision trees). The method warm-starts the reduced (feature-dropped) model from the trained full model using a dropout-style initialization, then runs only a small number of additional iterations with early stopping to avoid underfitting and reduce retraining cost. The authors cast a broad class of gradient updates into an iterative kernel update equation and provide convergence guarantees for the early-stopped reduced-model estimator under fixed and random design assumptions, specializing results to large-width neural networks via the neural tangent kernel (NTK) and to a particular noisy symmetric-tree GBDT. Empirically, simulations and a real dataset on gas turbine emissions show the approach is substantially faster than full retraining while improving accuracy relative to naive dropout-based VI, and it can be used within subset-sampling schemes to accelerate Shapley value estimation. The paper also discusses Wald-type confidence intervals for VI in settings where asymptotic normality conditions hold (shown for certain NN setups).","The proposed VI estimator is computed from the difference in empirical MSE between the full model and an early-stopped reduced model: $\widehat{VI}_I=\frac{1}{N}\{\|Y-f_{T_b}(X^{(I)})\|_2^2-\|Y-f_N^c(X)\|_2^2\}$. The reduced model training uses warm-started gradient updates and is analyzed through an iterative kernel update recursion $f_{\tau+1}(X^{(I)})=(I-\epsilon K^{(I)}_\tau)f_{\tau}(X^{(I)})+\epsilon K^{(I)}_\tau Y$, with a stationary-kernel decomposition introducing an additional evolving-kernel error term. Early stopping is selected via a (local) empirical Rademacher complexity criterion that balances bias and variance, yielding theoretical $L_2$-type error rates for the reduced-model estimator (and thus VI).","Under stated assumptions, the general early-stopped warm-start estimator for the reduced model achieves an $O(N^{-1/2})$ prediction error rate in empirical norm under fixed design (Theorem 1) and in population $L_2$ norm under random design (Theorem 2). For sufficiently wide neural networks, the analysis bounds NTK/kernel-evolution and linearization errors as $O(m^{-1/2})$ and yields an $O(N^{-1/2})$ empirical-rate bound (Corollary 1); under additional assumptions for ReLU networks on the sphere with zero bias, a faster population rate $O\big(N^{-p/(p+1)}\big)$ is stated (Corollary 2). For the specific noisy symmetric-tree GBDT, bounds depend on tree depth and random-strength parameter $\beta$, e.g., an $O(2^d/N)$-type empirical bound is provided under conditions such as $\beta\ge N^{5/4}$ (Corollary 3). Experiments report that the method is closer to retrain than dropout for VI and Shapley estimation, and is faster than retrain (e.g., Shapley estimation runtime about 2–3× faster than retraining in reported setups).","The authors note that the theoretically optimal stopping time is typically impractical to compute because it depends on unknown quantities such as the true reduced function $f_{0,-I}$ and the noise level $\sigma$, so they recommend validation-based early stopping in practice. They also state that their asymptotic normality/Wald-type confidence interval result requires $VI_I\neq 0$, and coverage degrades markedly when the true VI is zero unless additional procedures like sample splitting are used. For GBDT, they emphasize that their theory is developed for a simplified boosting setup (symmetric/oblivious trees with added noise) rather than standard decision-tree boosting implementations.","The method’s practical performance likely depends on hyperparameters (learning rate, patience/validation split, dropout-style warm start) and may be sensitive under distribution shift or strong covariate shift when replacing dropped features by marginal means. Theoretical guarantees rely on kernel/RKHS-style assumptions (e.g., stationarity of a limiting kernel, bounded trace, and specific stability/width or random-strength conditions) that may not hold for typical finite-width networks, modern optimizers (Adam), mini-batch SGD, or standard GBDT libraries without the injected-noise mechanism. Computational gains for Shapley values still scale with the number of subsets sampled; for large $p$ the approach reduces per-fit cost but does not remove the fundamental combinatorial nature of Shapley estimation.","They suggest deriving exact or approximate finite-sample distributions for the VI estimator to enable hypothesis testing of estimate reliability. They also propose extending the GBDT analysis from the simplified symmetric-tree/noise model to standard decision tree structures used in practice. Additional directions include extending the warm-start/early-stopping (""lazy training"") idea to bagging methods such as random forests, and extending NTK stability results to more general initialization settings and network architectures.","A valuable extension would be to study robustness under correlated and non-i.i.d. data (time series/autocorrelation) and under more realistic training regimes (mini-batch SGD, momentum/Adam) where the kernel-update view is less exact. Developing self-tuning stopping rules specifically optimized for VI estimation (rather than predictive loss) and providing diagnostic tools to detect when warm-start is misleading would improve deployability. Broader empirical benchmarking across diverse real-world domains and comparison to other modern model-agnostic VI/SHAP approximations (permutation-based, knockoff/CPI, LOCO/floodgate) would clarify when this approach is preferable.",2412.01120v2,https://arxiv.org/pdf/2412.01120v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:25:10Z
TRUE,Maintenance optimization|System reliability|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Other",Event/count data|Right-censored|Mixture of types,Condition-based|Age-based|Imperfect maintenance,Energy/utilities|Other,Other|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes a general, data-driven procedure to optimize imperfect preventive maintenance for a repairable system with multiple independent components and multiple failure modes. For each component, it defines four candidate reliability/maintenance models by combining two failure-rate forms (linear and Weibull) with two imperfect-maintenance age models (PAS and PAR), treating both failure-rate parameters and the maintenance effectiveness parameter $\varepsilon$ as unknown and estimating them jointly via maximum likelihood. Model selection is done separately for each component using leave-one-out cross-validation, AIC, and BIC to mitigate over-parameterization; the selected component models are then used to compute average reliability and cost-per-unit-time. Maintenance intervals for each component are decision variables in a multi-objective optimization (minimize cost, maximize reliability) solved via Sequential Quadratic Programming to generate a Pareto front. A real dataset from eight motor-operated safety valves in a nuclear power plant illustrates that different components select different best models (actuator: PAS-Weibull; valve: PAR-linear) and yield improved cost/reliability trade-offs over the plant’s heuristic 180-day interval policy.","Imperfect-maintenance age models: PAS uses $\omega^+_{m-1}=t-\sum_{k=0}^{m-2}(1-\varepsilon)^k\varepsilon\tau_{m-k-1}$, while PAR uses $\omega^+_{m-1}=t-\varepsilon\tau_{m-1}$. Failure-rate models are either linear $h(\omega)=\alpha\,\omega$ or Weibull $h(\omega)=\frac{\beta}{\eta^{\beta}}\omega^{\beta-1}$, yielding period-specific induced hazards and cumulative hazards $H_m$ used in reliability $R(t)=\exp(-H(t))$. The likelihood for recurrent failures under maintenance is $L(\xi)=\prod_{\text{failures}} h(t)\,\prod R(t)$ (expanded for multiple units/maintenance cycles) and is maximized (log-likelihood) to estimate $(\alpha,\varepsilon)$ or $(\beta,\eta,\varepsilon)$. Cost per unit time is modeled as $C(M)=\frac{c_m}{M}+\frac{1}{M}(\rho-h^*(M)M)c_c+\frac{c_o}{RP}$, and system average reliability for multiple components is the product of component average reliabilities.","In the nuclear power plant case study (8 motor-operated safety valves, 10 years), model selection (LCV/AIC/BIC) chooses PAS-Weibull for the actuator and PAR-linear for the valve. Reported MLEs include actuator PAS-Weibull $\hat\beta=7.4708$, $\hat\eta=15397$, $\hat\varepsilon=0.8482$ (AIC 186.09; BIC 191.08) and valve PAR-linear $\hat\alpha=1.73\times10^{-9}$, $\hat\varepsilon=0.7584$ (AIC 122.19; BIC 125.30). With current intervals $M_A=M_V=180$ days, the initial objectives are $R_i=0.857848$ and $C_i=3372.94€$ per year-equipment. Solving the two single-objective subproblems gives cost-optimal $(M_A,M_V)=(270,176)$ days with cost $\approx 3224.35€$ and reliability $\approx 0.8579$, and reliability-optimal $(261,162)$ days with $R\approx 0.860161$ and cost $\approx 3371.89€$; the SQP multi-objective run yields 155 non-dominated (Pareto) solutions.","The authors state that for their four candidate models they assume normal working conditions and null initial hazard rate, noting these assumptions simplify estimation and are reasonable for their application. They also note that for averaged reliability under PAS and PAR, the reliability integrals are not available in closed form and therefore require power-series approximations, with the number of terms chosen to meet a desired accuracy (e.g., $10^{-9}$ for the actuator, $10^{-7}$ for the valve).","The candidate model class is restricted to only two failure-rate families (linear and Weibull) and only two imperfect-maintenance age-update models (PAS/PAR), which may be too narrow for some assets (e.g., bathtub hazards, covariate effects, or dependence between maintenance quality and operating context). Components are treated as independent and maintenance is modeled primarily through an age transformation; common-cause failures, shared environments, and maintenance-resource coupling are not represented, which can bias system-level optimization. The optimization uses averaged reliability/cost approximations (stationarity for PAS; linear age approximation for PAR; power-series integration), but the impact of approximation error on the Pareto front is not systematically quantified. The approach appears to be Phase-II/policy-optimization focused; uncertainty propagation from parameter estimation into optimal intervals (e.g., confidence bands/robust optimization) is not developed beyond point estimates and information-criterion selection.",None stated.,"Extend the modeling framework to include covariates/condition monitoring (load, environment, maintenance type) and allow maintenance effectiveness $\varepsilon$ to vary over time or by intervention, improving realism for industrial settings. Develop uncertainty-aware optimization (Bayesian or bootstrap-based) to propagate parameter and model-selection uncertainty into robust maintenance intervals and Pareto fronts. Expand the candidate model set to include additional hazard shapes (e.g., piecewise Weibull, lognormal, bathtub models) and dependence structures (shared frailty/common-cause) for multi-component equipment. Provide open-source implementations and benchmark studies across multiple datasets to assess computational scalability and sensitivity to the PAR linearization and power-series truncation choices.",2412.08380v1,https://arxiv.org/pdf/2412.08380v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:25:55Z
TRUE,System reliability|Other,"Simulation-based|Parametric (Weibull, etc.)|Stochastic process|Other",Sensor/condition monitoring|Simulated only|Other,Not applicable,Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,https://arxiv.org/abs/2412.13731,"This paper formalizes structural reliability analysis for stochastic simulators (non-deterministic models) where repeated runs at identical inputs yield different outputs due to latent variables, and defines the failure probability as $P_f=P(g(X,Z)\le 0)$. It proposes using stochastic surrogate/emulator models—generalized lambda models (GLaM) and stochastic polynomial chaos expansions (SPCE)—to learn the full conditional output distribution and enable efficient estimation of the conditional failure probability function $s(x)=P_Z(g(x,Z)\le 0)$, yielding $P_f=\mathbb{E}_X[s(X)]$. The authors compare Monte Carlo estimators and show (via total variance decomposition) that estimating $P_f$ through $\mathbb{E}[s(X)]$ has lower variance than crude sampling in the joint $(X,Z)$ space when $s(x)$ is computable from the emulator. The methodology is validated on two analytical examples (a stochastic R–S model and a stochastic simply supported beam) and on a realistic wind turbine load case study, demonstrating convergence and (often) variance reduction relative to direct Monte Carlo for a fixed number of expensive simulator evaluations. The work advances surrogate-based reliability analysis by addressing latent stochasticity directly and providing semi-analytical computation of conditional failure probabilities from learned conditional distributions.","Failure is defined by a stochastic limit-state $g(X,Z)$ with $P_f=P(g(X,Z)\le 0)=\iint 1_{\{g(x,z)\le 0\}} f_X(x)f_Z(z)\,dx\,dz$. The key reformulation introduces the conditional failure probability $s(x)=P_Z(g(x,Z)\le 0\mid X=x)=\int 1_{\{g(x,z)\le 0\}} f_Z(z)\,dz$, giving $P_f=\int s(x)f_X(x)\,dx=\mathbb{E}_X[s(X)]$. GLaM emulates the conditional distribution with a generalized lambda distribution whose quantile function is $Q(u;\lambda)=\lambda_1+\frac{1}{\lambda_2}\left(\frac{u^{\lambda_3}-1}{\lambda_3}-\frac{(1-u)^{\lambda_4}-1}{\lambda_4}\right)$ and parameters $\lambda_i(x)$ are modeled by PCE; SPCE represents $Y_x\approx\sum_{\alpha\in A} c_\alpha\psi_\alpha(x,Z)+\varepsilon$ and computes $s(x)$ via 1D quadrature over the artificial latent variable.","Across two analytical benchmarks (R–S and simply supported beam), both GLaM and SPCE converge to the analytical failure probabilities ($3.154\times10^{-3}$ and $1.019\times10^{-3}$ respectively) as training size increases, and produce substantially lower variability than direct MCS for the same number of full simulator calls (e.g., R–S at $N=50{,}000$: coefficient of variation 2.3% for both emulators vs 6.7% for MCS). For the beam example, small-sample MCS often yields $\hat P_f=0$ when $N\le 1{,}000$, while emulators still provide nonzero, closer estimates that improve with $N$ and have narrower boxplots than MCS. In the wind turbine case (target/reference $P_f\approx 1.022\times10^{-2}$ using ~5 million simulated runs), emulators fit mean and 95% conditional quantiles reasonably with $N\in\{500,1000,5000\}$ but show tail-fitting bias in regions contributing most to failure; reliability estimates are comparable to MCS and can outperform it for SPCE at the largest training size reported. The paper proves variance reduction for the $\mathbb{E}[s(X)]$ estimator relative to crude joint-space sampling when $s(x)$ is available without inner-loop Monte Carlo.","The authors note that the current methodology still requires many evaluations of the (potentially expensive) limit-state function to train the emulators, and that an active-learning enrichment strategy (common in deterministic surrogate-based reliability) is not yet available for stochastic emulators. In the wind turbine case study, they acknowledge persistent bias due to inaccurate fitting in the tails of conditional distributions—especially in high-probability input regions—which materially affects $P_f$ estimates and can prevent consistent outperformance over crude MCS. They also remark that emulator convergence can be slow in such critical regions under a global (non-targeted) training approach.","The approach assumes independence between input variables $X$ and latent variables $Z$ when rewriting $P_f$ and deriving estimators; in some simulators, latent variability may depend on operating conditions in ways not captured by this assumption. Performance is demonstrated mostly through Monte Carlo-based comparisons and specific case studies; broader stress-testing under strong multimodality, discontinuities, or high-dimensional $X$ (common in structural reliability) is limited in what is shown here. The method’s practical accuracy hinges on emulator fidelity for extreme tails; without rigorous tail diagnostics or uncertainty propagation for emulator error, $P_f$ estimates for rarer events (much smaller than $10^{-3}$–$10^{-2}$) may be unreliable. Implementation details (e.g., optimizer stability for GLaM MLE, SPCE noise tuning sensitivity, truncation choices) can materially affect results but are not fully benchmarked across alternatives.","They propose developing an active-learning (iterative enrichment) strategy tailored to stochastic emulators so that additional simulator evaluations are concentrated in regions of interest, thereby reducing overall computational cost while improving accuracy where it matters for failure estimation. They also suggest that such targeted refinement could accelerate convergence in challenging applications like the wind turbine case, where tail accuracy in high-density regions drives $P_f$.","A valuable extension would be robust/self-starting variants that relax or test the $X\perp Z$ assumption and handle input-dependent latent structure more explicitly (e.g., hierarchical/Bayesian formulations for conditional distributions). Developing calibrated uncertainty bounds for $P_f$ that incorporate emulator-fitting uncertainty (not just Monte Carlo error) would improve decision support, especially for very small failure probabilities. Extending the framework to multivariate outputs and multiple coupled limit states (system reliability with dependent failure modes) and to time-dependent reliability (stochastic processes over time) would broaden applicability. Providing open-source reference implementations and standardized benchmarks would facilitate reproducibility and adoption beyond UQLab users.",2412.13731v1,https://arxiv.org/pdf/2412.13731v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:26:41Z
TRUE,Life distribution modeling|System reliability|Other,Bayesian|Simulation-based|Other,Sensor/condition monitoring|Mixture of types|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a hierarchical Bayesian modeling (HBM) framework for uncertainty quantification and reliability updating using multiple datasets, targeting both static (linear) and dynamic structural models. For a linear Gaussian model, it derives an analytical expression for the posterior of hyperparameters (hyper-mean and hyper-covariance), enabling efficient reliability updating without MCMC. Reliability is updated via a scalar limit-state function and computed as an average of conditional failure probabilities over hyperparameter samples, highlighting the impact of multi-source variability. For a dynamical structural model (structural dynamics / SHM), it proposes a two-stage sampling workflow: first sample per-dataset parameter posteriors (via Transitional MCMC), then infer hyperparameters without additional model runs, and evaluate failure probabilities (using subset simulation) while optionally including additional input uncertainties. Numerical examples (a synthetic linear model and a 3-DOF spring–mass system) show that classical Bayesian modeling (CBM) can severely underestimate parameter uncertainty and thus produce failure probabilities differing by orders of magnitude compared to HBM.","The linear model uses $g(\theta)=A^T\theta$ with Gaussian likelihood $y\sim\mathcal N(A^T\theta,\sigma^2 I)$, yielding a Gaussian posterior per dataset with $\theta^*=\Sigma_\theta^* A\sigma^{-2}y$ and $\Sigma_\theta^*=\sigma^2(AA^T)^{-1}$. A scalar limit state is defined as $G(\theta)=b-(Ac)^T\theta$, giving conditional failure probability $F^{(m)}=\Phi(-\beta^{(m)})$ where $\beta^{(m)}=\frac{b-(Ac)^T\mu_\theta^{(m)}}{\sqrt{(Ac)^T\Sigma_\theta^{(m)}(Ac)}}$; posterior failure probability is estimated by averaging over hyperparameter samples: $\Pr(F\mid D)=\frac{1}{N_s}\sum_{m=1}^{N_s}\Phi(-\beta^{(m)})$. For the dynamic case, hyperparameter posteriors are computed by marginalizing per-dataset likelihood-sampled parameters (via TMCMC) and reliability is computed via subset simulation, optionally integrating additional uncertainty variables $\phi$ in $G(\theta,\phi)$.","In the linear example, increasing the number of data points per dataset reduces within-dataset posterior standard deviations (Table 1), demonstrating decreasing epistemic uncertainty with more measurements. Using multiple datasets, the posterior uncertainty of hyperparameters decreases as the number of datasets increases (Table 2 and Fig. 2), and hyper-means approach nominal values. Comparisons of HBM vs CBM show similar posterior means for parameters but CBM produces an excessively narrow posterior (Fig. 3), failing to represent dataset-to-dataset variability. Reliability curves differ by many orders of magnitude between HBM and CBM under the same exceedance level due to CBM’s uncertainty underestimation (Fig. 4), and similarly for the dynamic example’s displacement-exceedance failure probability (Fig. 9) and predicted maximum-displacement PDF (Fig. 10).",None stated.,"The work relies heavily on Gaussian prior/likelihood structure (explicitly for the linear case and implicitly for hyperparameter modeling), which may be restrictive for non-Gaussian measurement errors, model discrepancy, or heavy-tailed variability common in SHM data. The evaluation uses simulated case studies (linear synthetic model and a 3-DOF spring–mass system) without validation on real experimental/field datasets, limiting evidence of practical performance and robustness. Implementation details affecting reproducibility (e.g., TMCMC settings, subset simulation parameters, convergence diagnostics, computational cost) are not provided in the excerpt, and no code is shared.",Future work will extend the framework to incorporate datasets collected over periodic intervals to enable continuous/online reliability updates. The authors also plan to integrate damage detection into the HBM framework for more comprehensive structural health monitoring and assessment under evolving operating/environmental conditions.,"A valuable extension would be robust/non-Gaussian hierarchical models (e.g., Student-t errors, mixture models) and explicit model discrepancy terms to reduce sensitivity to outliers and modeling error. Developing self-starting/online Bayesian updating algorithms (e.g., sequential Monte Carlo) could make periodic updating computationally cheaper than repeated TMCMC + subset simulation. Providing open-source implementations and benchmarking against additional reliability-updating baselines (e.g., BUS, cross-entropy IS, surrogate-assisted reliability updating) on real SHM datasets would strengthen practical adoption and comparative validity.",2412.20416v1,https://arxiv.org/pdf/2412.20416v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:27:13Z
FALSE,NA,ML-based|Other,Mixture of types|Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.sciencedirect.com/science/article/pii/B9780444520449500057|https://kaggle.com/competitions/criteo-display-ad-challenge,"The paper proposes RISA (Reliable Imputed-Sample Assisted) for vertical federated learning (VFL) under limited overlapping samples, aiming to exploit non-overlapping samples that are typically unused. It imputes missing attributes for non-overlapping samples using mean imputation computed from overlapping samples, and assigns pseudo-labels via a vertical self-training model. To control noise from imperfect imputations and pseudo-labels, it introduces evidential VFL (EVFL) based on Dempster–Shafer/subjective logic to estimate per-sample uncertainty and exclude high-uncertainty samples during training with a dynamically decreasing uncertainty threshold. The method fuses party-wise “opinions” using a proposed Reduced Yager’s combination rule to handle conflicts across parties’ evidence. Experiments on CIFAR-10 (two-party split image) and Criteo CTR prediction (two-party split numeric/categorical) show consistent gains over VFL baselines, with the largest improvements when overlap is very small (e.g., CIFAR-10 accuracy 87.62% vs 38.58% for VFL at 1% overlap, described as a 48% absolute gain).","Non-overlapping attribute completion uses mean vectors from overlapping samples per party and concatenates them with the observed party’s attributes: \(\hat{x}_i=\mathrm{concat}(x_i^l,\bar{x}^1,\ldots,\bar{x}^{l-1},\bar{x}^{l+1},\ldots,\bar{x}^M)\). Self-training is trained with cross-entropy on overlapping samples (Eq. 1). Evidential learning forms Dirichlet parameters \(\alpha_k=e_k+1\), belief masses \(b_k=e_k/S\), and uncertainty \(u=K/S\) with \(u+\sum_k b_k=1\) (Eq. 2). Party opinions are fused by Reduced Yager’s rule (Eq. 3–4): \(b_k=b_k^1 b_k^2 + b_k^1 u^2 + b_k^2 u^1\), \(u=u^1 u^2 + C\) where \(C=\sum_{i\neq j} b_i^1 b_j^2\); then \(S=K/u\), \(\alpha_k=b_k S + 1\) (Eq. 5), and EVFL loss \(\mathcal{L}(\alpha_i)=\sum_k y_{ik}(\log S_i-\log \alpha_{ik})\) (Eq. 6).","On CIFAR-10 test accuracy (%) across overlap ratios {1%,10%,20%,30%,40%,50%}, RISA achieves {87.62, 88.65, 88.97, 89.03, 89.55, 89.82}, outperforming FedCVT {85.93, 86.85, 86.99, 87.23, 87.77, 88.38} and VFL {38.58, 70.47, 78.11, 81.78, 83.97, 85.45}. On Criteo test AUC (%), RISA achieves {75.67, 77.36, 77.62, 77.81, 77.87, 77.89}, higher than FedCVT {74.37, 74.94, 75.73, 75.65, 76.03, 76.51} and VFL {67.96, 70.43, 73.99, 74.56, 74.89, 75.43}. Ablation at 10% overlap shows CIFAR-10 accuracy improves from 70.47 (VFL) to 82.19 (+imputation), 85.01 (+imputation+self-training), and 88.65 (+EVFL); Criteo AUC improves from 70.43 to 71.17, 74.45, and 77.36 respectively. The paper highlights a large gain at very low overlap, stating a 48% accuracy increase on CIFAR-10 with only 1% overlap (87.62 vs 38.58).",None stated,"The work is not a reliability-engineering study; “reliable” refers to uncertainty-aware sample selection rather than reliability of systems/components. Empirical validation is limited to two datasets (CIFAR-10 and Criteo) and primarily compares predictive metrics (accuracy/AUC) without broader robustness checks (e.g., varying missingness mechanisms, stronger distribution shifts, or adversarial/Byzantine parties). Mean imputation may be suboptimal when missing attributes are not missing-at-random or when feature distributions are multimodal; performance may depend on overlap representativeness. The paper does not indicate availability of implementation code or reproducibility artifacts, and it does not specify software/framework details in the provided text.",None stated,"Evaluate RISA under more realistic VFL conditions such as non-IID parties, temporal drift, and different missingness mechanisms (MAR/MNAR), and extend to more than two parties and more complex feature-partition schemas. Develop stronger imputation (privacy-preserving generative or conditional models) while retaining uncertainty calibration, and study theoretical properties/calibration of the proposed Reduced Yager fusion under conflict. Provide a self-starting/parameter-free schedule for the uncertainty threshold and analyze sensitivity to \(\tau_0\), \(E\), and pseudo-labeling thresholds. Release code and standardized benchmarks to improve reproducibility and facilitate comparison with additional recent VFL methods.",2501.06429v1,https://arxiv.org/pdf/2501.06429v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:27:50Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Stochastic process|Other",Complete lifetime data|Other,Not applicable,Semiconductor/electronics,Case study (real dataset)|Other,TRUE,R,Not provided,https://doi.org/10.3390/xxxxx,"The paper introduces a new lifetime distribution for reliability analysis called the one cut-point phase-type (PH) distribution, motivated by non-homogeneous behavior that standard PH models may fit poorly without using many phases. The method partitions time at an estimable cut-point a, using one PH sub-generator T1 before a and another T2 after a, and derives reliability function, hazard rate, cumulative hazard rate, characteristic function, and moments in matrix form. Parameters (including the cut-point) are estimated via maximum likelihood, and optimization is performed computationally in R (using optim with L-BFGS-B). The approach is demonstrated on cycle-to-cycle variability data from resistive random access memories (RRAM), modeling reset voltage/current and set voltage/current from 1000-cycle experiments, and compared against standard PH (often Erlang-structured) fits. Empirically, the cut-point PH achieves comparable or improved tail/hazard fitting while drastically reducing the required number of phases and yielding better Anderson–Darling goodness-of-fit outcomes than the conventional PH fits in the reported examples.","The one cut-point PH density is piecewise: for x≤a, f(x)=α e^{T1 x} T1^0; for x>a, f(x)=α e^{T1 a} e^{T2 (x-a)} T2^0, where T_k^0=−T_k e. The reliability function is R(x)=α e^{T1 x} e for x≤a, and R(x)=α e^{T1 a} e^{T2 (x-a)} e for x>a. The hazard rate is h(x)=f(x)/R(x), yielding h(x)=α e^{T1 x}T1^0 / (α e^{T1 x}e) for x≤a and h(x)=α e^{T1 a} e^{T2(x-a)}T2^0 / (α e^{T1 a} e^{T2(x-a)}e) for x>a. The log-likelihood sums log-densities over observations split by the indicator I{x_i≤a} vs I{x_i>a}.","For reset voltage (n=1000), a standard PH/Erlang fit with 200 phases is rejected by the Anderson–Darling test (p<0.0001), while the cut-point PH with a=0.595 (95% CI [0.571,0.619]) uses 14 phases with λ1=16.74531 and λ2=261.61844 and yields A–D p=0.023. For reset current (n=1000), the PH model uses 353 phases with λ=45888.49 (A–D p=0.003), whereas the cut-point model uses 12 phases with a=0.0072 (95% CI [0.0068,0.0076]), λ1=1003.27, λ2=9652.37 and A–D p=0.141. For set voltage (n=1000), PH uses 89 phases (λ=214.6181; A–D p=0.0147) while cut-point uses 11 phases with a=0.315 (95% CI [0.296,0.334]), λ1=11.5570, λ2=73.7963 and A–D p=0.0571. For set current (n=1000), PH uses 1 phase (λ=2560.425; A–D p=0.0001) while cut-point uses 2 phases with a=0.00025 (95% CI [0.00019,0.00031]), λ1=6820.583, λ2=3495.02 and A–D p=0.0819.",None stated.,"The approach assumes i.i.d. observations and a single cut-point (two regimes); more complex non-stationarity, multiple change-points, or gradual time-varying transition rates are not modeled. Inference details are limited: there is no discussion of identifiability/uniqueness of (a,T1,T2), potential multimodality of the likelihood, or sensitivity to starting values and bounds in L-BFGS-B. The comparisons rely mainly on Anderson–Darling p-values and plots; broader model checking (e.g., predictive validation, information criteria, uncertainty propagation for tail metrics) is not provided. The implementation is said to exist in R but no package/repository is cited, limiting reproducibility.",None stated.,"Extend the model to multiple cut-points (piecewise PH with more than two regimes) or smoothly time-inhomogeneous PH processes to capture more general non-homogeneous dynamics. Develop robust/self-starting estimation procedures (and diagnostics) addressing identifiability and optimization stability, and provide standard errors via observed information or bootstrap. Evaluate performance across a wider suite of real reliability datasets and under censoring/truncation, which are common in reliability testing. Release an R package or repository with examples and benchmarking scripts to support reproducible use in reliability and electronics applications.",2501.07949v1,https://arxiv.org/pdf/2501.07949v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:28:22Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Failure mode analysis|Other,ML-based|Hybrid/Ensemble|Physics-based|Other,Sensor/condition monitoring|Degradation measurements|Mixture of types,Predictive|Condition-based,Transportation/logistics,Case study (real dataset)|Other,TRUE,Python|MATLAB|Other,Public repository (GitHub/GitLab),https://github.com/AlmudenaBravoC/ShaftFormer|https://github.com/dariocb/SpectralShaftFormer,"The paper proposes transformer-based deep autoregressive models to forecast railway axle vibration signals for predictive maintenance and safety, addressing the non-stationary behavior caused by changes in load and speed. It introduces ShaftFormer (SF), which extends Informer with a CNN-based preprocessing step (ELT) and ProbSparse attention, and a Spectral ShaftFormer (SSF) variant that forecasts in the frequency domain using STFT, global frequency filtering, and HiLo attention. The models are probabilistic generative forecasters producing Normal output distributions (SSF further regularizes variance via an exponential model) to quantify uncertainty and support tasks like outlier detection, missing-data imputation, and data augmentation. Experimental accelerometer signals from a bogie test rig (multiple wheelset assemblies, multiple speeds/loads, and crack depths) are complemented with finite-element simulations (Abaqus) and frequency-domain analysis (MATLAB) to enrich understanding and conditioning. Quantitatively, SSF substantially improves MSE versus SF on train/validation/test splits, suggesting better generalization for vibration forecasting under diverse operating conditions.","SF/SSF are framed as autoregressive probabilistic forecasters: for each future step/window, the model outputs parameters of a Normal distribution for the next value(s), and training minimizes MSE between predicted and true signals/spectrograms. SSF uses STFT/iSTFT to operate in the spectrogram domain and samples as $z\sim\mathcal{N}(\mu(x),\sigma^2)$ with variance drawn from an exponential model $\sigma^2\sim \mathrm{Exp}(\lambda(x))$, using inverse-transform sampling and the reparameterization trick for differentiability. Positional encodings follow the standard Transformer sinusoidal form $PE(pos,2i)=\sin(pos/10000^{2i/d_{model}})$ and $PE(pos,2i+1)=\cos(pos/10000^{2i/d_{model}})$ (though authors note good performance even without PE).","SSF achieves markedly lower MSE than SF: Train/Validation/Test MSE of 0.1779/0.17223/0.5303 for SSF versus 0.42811/0.43524/1.73285 for SF. Example SSF spectrogram forecasting shows validation MSE ≈ 0.18 and test MSE ≈ 0.55 with predicted spectrograms closely matching ground truth, indicating maintained accuracy in testing. Time-domain plots similarly show close alignment between predicted and true signals on validation and test. STL/LOESS decomposition indicates SSF predictions preserve trend, seasonal, and residual components of the signals.",None stated.,"The work focuses on forecasting quality (MSE) rather than directly validating downstream reliability outcomes (e.g., crack detection accuracy, false-alarm rates, or maintenance decision performance) under realistic deployment constraints. The evaluation appears limited to the specific bogie test-rig datasets and tested operating conditions (two speeds, two loads, specific crack geometries/depths), so external validity to field data, other axle types, sensors, and broader environmental variability is uncertain. The paper does not report robustness to sensor faults (drift, miscalibration), strong non-Gaussian noise, or autocorrelated/heteroscedastic errors beyond the chosen probabilistic forms, which may affect anomaly detection and uncertainty calibration. Practical deployment aspects (latency, on-board compute, model update/self-starting in Phase I/II-like monitoring) are not empirically demonstrated despite being relevant to Maintenance 4.0.","The authors suggest optimizing the model for real-world deployment, explicitly considering hardware constraints and latency requirements. They also propose extending the architecture to incorporate multimodal data (e.g., environmental conditions and operational parameters) to improve robustness and applicability. Additionally, they point to developing a bogie digital twin as a longer-term direction enabled by the modeling advances.","Evaluate how forecasting translates into reliability tasks with standard metrics (e.g., detection delay, ROC/PR for crack-state classification, calibrated anomaly scores, and maintenance cost/availability impacts) and compare against strong baselines (state-space models, classical spectral methods, and other modern transformers). Add uncertainty calibration checks (coverage vs. prediction intervals) and robustness studies under domain shift (new wheelsets, track conditions), missingness mechanisms, and sensor degradation. Develop an online/adaptive or self-supervised update strategy for continual learning in service, including drift detection and safe model updates. Provide a lightweight deployment variant and benchmarking on embedded/edge hardware relevant to rail condition-monitoring systems.",2501.11730v1,https://arxiv.org/pdf/2501.11730v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:29:04Z
FALSE,NA,Bayesian|ML-based|Hybrid/Ensemble,Other,Not applicable,Healthcare/medical|Other,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/lblaoke/RF-DLC,"The paper proposes RF-DLC (Making Reliable and Flexible Decisions in Long-tailed Classification), a Bayesian decision-theoretic framework for long-tailed classification where different misclassification types have asymmetric risk. It introduces an “integrated gain” objective that combines (i) a task-defined utility matrix (to encode costs/benefits of different errors) and (ii) importance weighting to account for train–test class-prior shift common in long-tailed settings. The posterior over model parameters is approximated with a particle-based variational distribution (deep ensemble/particles) and includes a repulsive regularization term derived from the KL to a Gaussian prior to encourage particle diversity. At inference, decisions are made by maximizing posterior expected gain under the chosen utility, enabling tail-sensitive, class-sensitive, or meta-class-sensitive decision rules. Experiments on CIFAR10/100-LT, ImageNet-LT, iNaturalist, and DermaMNIST show improvements on standard accuracy/tail accuracy, calibration (ECE/AUC), and a newly proposed risk metric, False Head Rate (tail→head error rate).","Decision gain for an input is defined as $g(d\mid x,\theta)=\prod_{y'} p(y'\mid x,\theta)^{u(y',d)}$ (Eq. 5), where $u(\cdot,\cdot)$ is specified via a utility matrix $U$. Train–test prior shift is handled via importance weighting using $\frac{p_{\text{test}}(x,y)}{p_{\text{train}}(x,y)}=\frac{p_{\text{test}}(y)}{p_{\text{train}}(y)}\propto \frac{1}{f(n_y)}$ under an intra-class consistency assumption (Eqs. 8–9). The training objective is a variational lower bound on log integrated gain (Eq. 10), optimizing particles $\{\theta_j\}_{j=1}^M$ for $q(\theta)=\sum_{j=1}^M w_j\,\delta(\theta-\theta_j)$ (Eq. 13) with a KL-derived regularizer approximated as an $\ell_2$ term minus an entropy/repulsion term (Eqs. 14–15). At test time, the optimal decision is $d^*=\arg\max_d \sum_{j=1}^M \sum_{y'} U_{y',d}\,\log p(y'\mid x^*,\theta_j)$ (Eq. 16).","On tail-sensitive evaluation using False Head Rate (FHR), RF-DLC consistently achieves the lowest FHR across CIFAR10-LT, CIFAR100-LT, and ImageNet-LT (e.g., CIFAR100-LT average FHR 32.08% vs 40.11% for RIDE and 49.22% for CB Loss; ImageNet-LT average FHR 12.27% vs 13.99% for RIDE). On standard long-tailed classification, RF-DLC improves both overall and tail accuracy over strong baselines (e.g., CIFAR100-LT: 50.24% overall / 30.34% tail vs RIDE 48.99% / 28.78%; ImageNet-LT: 55.73% / 51.98% vs TLC 55.03% / 51.56%). For calibration/uncertainty quantification, RF-DLC’s Bayesian predictive uncertainty yields markedly better ECE (e.g., CIFAR100-LT ECE 10.35% vs MCP 23.75% and Evidential 21.64%). In a real medical long-tailed dataset (DermaMNIST), RF-DLC improves accuracy (77.67%) and reduces FHR substantially compared with CE and other imbalance baselines (e.g., avg FHR 30.77% vs CE 57.61%).","The authors note they have not explored long-tailed regression, though they believe the framework could be adapted by adjusting the decision gain. They also state they do not address more general dataset shift scenarios (e.g., out-of-distribution data) where the assumption of semantically identical train/test sets fails, and note that if the test distribution is not uniform the discrepancy ratio $p_{\text{test}}(x,y)/p_{\text{train}}(x,y)$ would need a more general form than $1/f(n_y)$.","The approach depends on selecting an appropriate utility matrix; in many applications eliciting and validating utilities/costs is nontrivial and can introduce subjectivity or instability if misspecified. The distribution-shift correction largely assumes prior shift (train/test differ mainly in class probabilities) and may degrade when covariate shift within classes or label noise is present, which is common in real long-tailed data. Computationally, while comparable to other ensemble approaches, particle/ensemble training increases memory/compute and may be challenging for large backbones or edge deployment without distillation. The paper’s “reliability” framing is about decision risk in classification rather than engineering reliability (failure-time/degradation) and thus does not connect to established reliability metrics like hazard/survival, RUL, or maintenance decision frameworks.","They propose extending the framework to long-tailed regression by modifying the decision gain. They also suggest handling broader dataset shift, including out-of-distribution testing and non-uniform testing distributions, which would require generalizing the discrepancy ratio beyond $1/f(n_y)$. They mention investigating adaptation to realistic social problems (e.g., fairness for underrepresented demographic groups) as an important future direction.","Develop principled procedures for utility elicitation/learning (e.g., from expert judgments, constrained optimization, or observed downstream costs) and analyze robustness to utility misspecification. Extend the method to settings with simultaneous prior shift and covariate shift (e.g., class-conditional feature shift) and evaluate under label noise and open-set/novel-class conditions. Provide lightweight deployment variants (distillation of decision rules/ensembles, low-rank posterior approximations) and publicly released reference implementations for common long-tailed benchmarks. Explore theoretical guarantees for decision risk under long-tailed sampling (e.g., bounds on utility-weighted risk and calibration under importance weighting).",2501.14090v1,https://arxiv.org/pdf/2501.14090v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:29:46Z
FALSE,NA,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/YZH0905/POTA-STC,"This paper proposes POTA, a short-text clustering framework that improves representation learning by generating reliable pseudo-labels. It introduces a consistency-aware adaptive optimal transport (CAOT) formulation that incorporates both sample-to-cluster global structure and sample-to-sample semantic consistency via a semantic regularization term built from cosine similarity and an instance-level attention-derived similarity matrix. The method iteratively alternates between pseudo-label generation (solving a nonconvex OT objective using a generalized conditional gradient scheme with Lagrange-multiplier updates) and training the attention network, then uses the pseudo-labels to supervise cluster-level and instance-level contrastive learning. Experiments on eight benchmark short-text datasets show POTA achieves state-of-the-art ACC/NMI on most datasets and large gains on StackOverflow and Tweet. Code is released by the authors on GitHub.","The CAOT pseudo-labeling solves an OT-style optimization over transport matrix $Q$ and cluster proportions $b$: $\min_{Q,b}\langle Q,-\log P^{(0)}\rangle+\varepsilon_1 H(Q)+\varepsilon_2(\Psi(b)^T\mathbf{1})-\varepsilon_3\langle S,QQ^T\rangle$ subject to $Q\mathbf{1}=a$, $Q^T\mathbf{1}=b$, $Q\ge0$, $b^T\mathbf{1}=1$, where $H(Q)=\langle Q,\log Q-1\rangle$ and $\Psi(b)=-\log b-\log(1-b)$. Pseudo-labels are then obtained by $\hat{Y}_{ij}=1$ if $j=\arg\max_{j'}Q_{ij'}$ and 0 otherwise; the attention similarity term is $S_{att}=\tfrac12(S^{(1)}+S^{(2)})$ with $S^{(1)}=\text{Softmax}(K^{(1)}_1 K^{(1)T}_2/\sqrt{D_2})$.","Across eight datasets, POTA attains best or tied-best clustering performance versus baselines including SCCL and RSTC. Example improvements over the best prior method reported in Table 2 include StackOverflow ACC/NMI from 80.07/72.28 (RSTC) to 85.96/75.43 (POTA) and Tweet ACC/NMI from 77.75/86.07 (RSTC) to 82.36/89.49 (POTA). Ablations on the semantic regularization show removing attention similarity degrades performance notably (e.g., StackOverflow ACC 85.96 \u2192 79.11), and removing both cosine and attention similarity also reduces results (e.g., GoogleNews-TS ACC 83.53 \u2192 81.27). Additional pseudo-labeling comparisons (Appendix C) show CAOT-based pseudo-labeling outperforms prediction-based and adaptive-OT-based pseudo-labeling (e.g., StackOverflow ACC 83.84 vs 79.63 vs 69.18).",None stated.,"The work is not evaluated under common robustness concerns for clustering such as strong domain shift, heavy noise/outliers, or substantial label-set mismatch; results are limited to the chosen eight benchmarks. The CAOT optimization is nonconvex and depends on iterative solvers (GCG + Lagrange updates), but there is limited discussion of convergence guarantees, sensitivity to initialization, or computational scaling to very large $N$ and $K$. Practical deployment details (e.g., memory/time for constructing $S\in\mathbb{R}^{N\times N}$ within batches, stability across batch sizes) are not deeply analyzed.",None stated.,"Extend CAOT/POTA to settings with autocorrelated or streaming text where cluster structure evolves over time, requiring online pseudo-labeling and adaptive OT. Investigate more scalable approximations to the batchwise similarity matrix and OT solver for larger batch sizes and higher cluster counts, and provide convergence/complexity analysis. Add robustness studies (noisy text, adversarial perturbations, outliers) and broader real-world validations, and release a packaged implementation (e.g., pip/conda) with reproducible scripts and configs.",2501.15194v3,https://arxiv.org/pdf/2501.15194v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:30:28Z
TRUE,Life distribution modeling|System reliability|Other,Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://github.com/ERA-Software/Overview,"This paper proposes a gradient-free, importance-sampling-based framework (guided pCN-ASTPA) to estimate rare-event (failure) probabilities for reliability analysis in high-dimensional Gaussian spaces when limit-state gradients are unavailable. The method builds on ASTPA by sampling an unnormalized approximate target \(\tilde h(x)=\ell_g(x)\pi_X(x)\), where the indicator of failure is replaced with a smooth logistic likelihood, and then correcting the resulting shifted estimate via inverse importance sampling (IIS) to estimate the target’s normalizing constant. Sampling of \(\tilde h\) is performed with a guided, dimension-robust preconditioned Crank–Nicolson (pCN) MCMC scheme, augmented by a dedicated “rare-event discovery” stage that finds rare-event seeds to initialize multiple pCN chains (important for multimodal tail events). The paper evaluates performance via repeated simulation on benchmark limit-state functions and engineering examples (e.g., a 34-story frame and a steel plate with random field modulus), reporting probability accuracy, coefficient of variation, and model-call budgets against state-of-the-art gradient-free methods (iCE-IS, SIS, aCS-SuS). Overall, the proposed framework achieves accurate estimates with substantially fewer model evaluations and competitive or lower C.o.V. in many challenging nonlinear/multimodal and high-dimensional settings, and the paper reports close agreement between analytical and empirical C.o.V. for ASTPA.","Failure probability is \(p_F=\int I_F(x)\,\pi_X(x)\,dx\). ASTPA defines an unnormalized target \(\tilde h(x)=\ell_g(x)\pi_X(x)\) where \(\ell_g\) is a logistic CDF of \(-g(x)/g_c\), and decomposes \(p_F=C_h\,\tilde p_F\) with \(\tilde p_F=\mathbb E_{\tilde h}[I_F(X)\pi_X(X)/\tilde h(X)]\) and \(C_h=\int \tilde h(x)dx\). IIS estimates \(C_h\) via \(\hat C_h=\frac1M\sum_{i=1}^M \tilde h(x_i')/Q(x_i')\) with \(x_i'\sim Q\) (a fitted GMM), and the final estimator is \(\hat p_F=\hat{\tilde p}_F\hat C_h\); pCN proposals use \(\tilde x=\sqrt{1-\beta^2}\,x+\beta\,\xi\) with \(\xi\sim\mathcal N(0,C)\) and accept with \(\alpha=\min\{1,\ell_g(\tilde x)/\ell_g(x)\}.","Across 500 independent runs per case, guided pCN-ASTPA typically achieved lower or comparable C.o.V. with fewer model calls than SIS and aCS-SuS, and often outperformed iCE-IS, especially in problems with challenging topology or high dimensionality. Examples reported include: (i) a 2D bimodal case with \(p_F\approx9.47\times10^{-6}\), where pCN-ASTPA used ~2,373 model calls with C.o.V. ~0.16; (ii) a 2D quartic bimodal case with \(p_F\approx5.91\times10^{-8}\), where pCN-ASTPA used ~3,165 calls with C.o.V. ~0.12; and (iii) a 102D 34-story structure with probabilities down to ~\(2.46\times10^{-7}\), where pCN-ASTPA used ~7,540 calls with C.o.V. ~0.27. The paper also reports good agreement between analytical C.o.V. and empirical (sampling) C.o.V. for ASTPA across examples (values shown in parentheses in tables). In a challenging “changing topology” 2D example, iCE-IS severely underestimated the mean probability (reported \(\sim1.92\times10^{-8}\) vs reference \(\sim1.13\times10^{-5}\)), whereas pCN-ASTPA remained accurate with far fewer calls than competitors.","The proposed gradient-free sampling approach is specifically tailored for Gaussian spaces (independent standard normal after transformation) and is not yet designed for direct non-Gaussian spaces; the authors note that non-Gaussian cases may require more sophisticated gradient-based samplers (addressed in their other work). They also note that pCN can struggle when the target is multimodal or far from the prior unless chains are carefully initialized, motivating their rare-event discovery stage. The paper further notes that weighted seed selection may not scale well in high-dimensional multimodal spaces due to sensitivity of high-dimensional target values, recommending uniform seed selection in such cases.","The method’s efficiency depends on several hyperparameters (e.g., \(\sigma\), scaling \(g_c\)/q, discovery parameters \(N_{level},p_0,\epsilon\), number/length of chains, GMM structure, IIS sample size), but there is no systematic tuning strategy or robustness study across broader regimes; performance may degrade if defaults are poorly matched to the problem. The approach assumes the ability to transform inputs to an independent standard Gaussian space and focuses on static rare-event probabilities; correlated/non-Gaussian inputs or strong model-induced dependence may complicate application. IIS relies on fitting a GMM in potentially high dimensions (diagonal covariance), which may be insufficient for strongly correlated targets and could impact normalizing-constant estimation in harder cases; the paper does not deeply probe IIS failure modes or diagnostics beyond the splitting heuristic. Comparisons are primarily simulation-based on selected benchmarks; broader validation on real industrial reliability datasets or expensive high-fidelity models with strict evaluation budgets is limited.","The authors propose extending the framework to gradient-free variants applicable directly in non-Gaussian spaces (bypassing Gaussian transformations). They also indicate future work on estimating high-dimensional first-passage (time-variant) problems under various settings, building on related prior work.","Developing adaptive/robust schemes to automatically choose \(\sigma\), \(g_c\), discovery settings, and IIS/GMM complexity using online diagnostics (ESS, tail coverage, instability in \(\hat C_h\)) would improve practicality and reduce manual tuning. Extending the approach to handle dependent/non-Gaussian inputs via copula-based Gaussianization uncertainty, or fully non-Gaussian gradient-free proposals (e.g., transport maps, normalizing flows, SVGD-style samplers), would broaden applicability. Providing open-source implementations and standardized benchmarks (with runtime, parallel scaling, and reproducibility details) would facilitate adoption and fair comparison. Incorporating variance estimators that explicitly account for MCMC dependence (beyond thinning heuristics) and studying finite-sample bias/variance tradeoffs in IIS would strengthen theoretical guarantees.",2501.17401v1,https://arxiv.org/pdf/2501.17401v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:31:27Z
TRUE,Failure mode analysis|Maintenance optimization|Other,Nonparametric/Semi-parametric|Simulation-based|Other,Sensor/condition monitoring|Simulated only|Mixture of types,Not applicable,Transportation/logistics|Manufacturing (general)|Other,Simulation study|Case study (real dataset),TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/Takeuchi-Lab-SI-Group/si_for_frequency-domain_change_point_detection,"The paper proposes a statistically valid change-point (CP) detection and testing framework in the frequency domain for condition monitoring, where structural changes often manifest across multiple frequencies. It selects CP candidates by optimizing a multi-frequency segmentation objective (dynamic programming per-frequency, then simulated annealing to align CPs across frequencies), and then assigns statistically valid selective-inference p-values to each detected CP location to avoid “double dipping.” The key inferential result shows that, under Gaussian i.i.d. noise and the null of no mean-shift at the selected location, the test statistic (a norm of a projected signal difference across involved frequencies) follows a truncated chi distribution conditional on the selection event and nuisance statistics, enabling exact type-I error control. Extensive simulations demonstrate the proposed selective p-values control type I error while retaining higher power than over-conditioned SI and Bonferroni baselines, and a case study on the IMS/NASA bearing vibration dataset identifies statistically significant spectral changes linked to outer-race fault harmonics. Overall, it advances frequency-domain monitoring by providing root-cause-relevant frequency CPs with formal statistical reliability guarantees (controlled false positive rate).","The observation model is $X=s+\varepsilon$, $\varepsilon\sim\mathcal N(0,\sigma^2 I_N)$, and STFT/DFT yields complex spectral sequences $F_t^{(d)}$. For a detected CP location $\tau_k$, the hypothesis tests equality of mean spectra before/after $\tau_k$ over involved frequencies $d\in D_k$. The test statistic aggregates segment mean differences across frequencies: $T_k(X)=\sigma^{-1}\sqrt{\sum_{d\in D_k} a^{(d)}_{\text{len}} c^{(d)}_{\text{sym}} M^{-1}\,\lvert \bar F^{(d)}_{\tau^{(d)}_{\text{pre}}+1:\tau_k}-\bar F^{(d)}_{\tau_k+1:\tau^{(d)}_{\text{suc}}}\rvert^2}=\sigma^{-1}\|P_k X\|$, with $P_k=\sum_{d\in D_k} a^{(d)}_{\text{len}} c^{(d)}_{\text{sym}} M^{-1} v^{(d)}v^{(d)*\top}$. Conditional on the selection event and nuisance statistic $Q(X)$, $T_k(X)$ follows a truncated $\chi$ distribution, giving selective p-values $p_k=\Pr(T_k(X)\ge T_k(x)\mid A(X)=A(x),Q(X)=Q(x))$.","In synthetic null experiments, the proposed selective-inference method controls type I error at nominal levels, while baselines that ignore simulated annealing in the selection event (and naive p-values) fail to control false positives. In power experiments (conditional on correct CP localization), the proposed method is the most powerful among methods that control type I error, outperforming over-conditioned SI (OC) and Bonferroni, which is most conservative. In the real IMS/NASA bearing dataset (set No.2), the method yields non-significant p-values for falsely detected candidates during an early “healthy” period (e.g., 1920 Hz: Proposed 0.538), and significant p-values during the fault-development period at BPFO harmonics (e.g., 1880 Hz: Proposed ~0.000; 3540 Hz: Proposed 0.005), supporting statistically reliable detection of frequency-domain fault signatures.","The authors state a key limitation: the theory relies on the assumption of uncorrelated Gaussian noise (i.i.d. normal errors), which is essential to derive the conditional sampling distribution of the test statistic as a truncated $\chi$ distribution. They note robustness experiments for unknown variance, non-Gaussian noise, and correlated noise, and report that increasing noise correlation can break type I error control, remaining an open challenge.","The selective-inference validity additionally depends on correctly modeling the full selection event of a stochastic heuristic (simulated annealing) and on fixing the random seed; in real deployments, different seeds or implementations could change selection events and complicate reproducibility of p-values. Computational cost can be high and appears to grow quickly with sequence length, which may limit real-time monitoring unless optimized or approximated. The method focuses on mean shifts in complex spectra; other practically relevant changes (variance/scale changes, transient bursts, nonstationary windows, or phase-specific phenomena) are not directly tested in the main formulation.","They propose extending the method to multi-dimensional sequences (e.g., multiple sensors), aiming to detect significant changes that are not attainable through univariate analysis while maintaining reliability guarantees for detections.","Developing theory and methods for correlated/colored noise (e.g., prewhitening, HAC covariance, or selective inference under general Gaussian covariance) would improve applicability to real vibration streams with temporal dependence. Extending beyond mean-shift to other spectral change types (variance/power shifts, distributional shape changes, and time-varying frequency bands) and providing automated diagnostic mapping from significant frequency CPs to fault modes (e.g., bearing defect classes) would strengthen reliability-engineering utility. Producing a packaged, documented implementation (e.g., a Python package) with benchmarks and guidance for setting window length/penalties in practice would aid adoption.",2502.03062v2,https://arxiv.org/pdf/2502.03062v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:32:04Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Event/count data|Mixture of types|Other,Not applicable,Service industry|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper focuses on improving the statistical validity (reliability/robustness) of web A/B testing that relies on asymptotic normality for t/z-based confidence intervals and p-values. It proposes an empirical validation procedure using repeated resampling of synthetic A/A tests: under correct assumptions, the resulting p-values should be Uniform(0,1). Uniformity is assessed via a Kolmogorov–Smirnov (KS) test on the empirical p-value distribution, equivalently testing whether the z-scores behave as standard normal. Using proprietary large-scale user event logs (≈2M users, ≈17M events across 50 event types), the authors show that most metrics pass the check but several exhibit non-uniform p-values indicative of inflated Type-I error risk. They further show that event frequency and skewness correlate with divergence (Spearman ρ ≈ 0.45 and ≈ 0.43 respectively) but are not sufficient diagnostics on their own, so the KS-based approach adds practical value for determining when standard t-testing is trustworthy.","The ATE is estimated as the difference in sample means: $\widehat{\mathrm{ATE}}=\mu_T(Y)-\mu_C(Y)$ with group mean $\mu_A(Y)=\frac{1}{|U_A|}\sum_{i\in U_A}Y_i$ and sample variance $\sigma_A^2(Y)=\frac{1}{|U_A|}\sum_{i\in U_A}(Y_i-\mu_A(Y))^2$. The standard error is $\mathrm{SE}(\widehat{\mathrm{ATE}})=\sqrt{\sigma_C^2/|U_C|+\sigma_T^2/|U_T|}$, yielding a normal CI $\widehat{\mathrm{ATE}}\pm \Phi^{-1}(1-\alpha/2)\,\mathrm{SE}$. For resampled A/A tests, the KS statistic is $D=\sup_{p\in[0,1]}|F_{\mathrm{emp}}(p)-p|$ to test whether the A/A p-values are uniform.","In experiments on proprietary logs (≈2 million users; ≈17 million logged instances; 50 event types) the authors run $n=5{,}000$ resampled A/A tests per event type and compute KS $D$ statistics and KS p-values against Uniform(0,1). The majority of event types have p-value distributions not distinguishable from uniform, but several are rejected (including after noting that even a Bonferroni correction would still flag some). The KS divergence correlates with event frequency (Spearman’s $\rho\approx0.45$) and with sample skewness (Spearman’s $\rho\approx0.43$), but neither relationship is monotonic, indicating those proxies are informative yet insufficient. The method therefore identifies metrics where standard normal-based CIs/p-values are likely unreliable and Type-I error may be inflated.","The paper notes that direct interpretation of KS p-values across many metrics would require multiple-testing correction (and mentions Bonferroni as an example). It also states that deeper exploration of alternative non-parametric CI methods (e.g., permutation or bootstrap) is out of scope, and highlights that such alternatives can be computationally expensive and require specialized engineering. The authors further caution against binary interpretation of significance and suggest focusing on the magnitude of the $D$-statistic rather than only hypothesis test decisions.","The validation is based on repeated resampling from a fixed historical dataset; if production experiments face nonstationarity, interference, or time effects, A/A resampling may not reflect operational false-positive behavior. The approach primarily checks calibration of p-values for the mean-difference z/t procedure; it does not directly diagnose other common threats in online experiments (tracking changes, logging loss, bot traffic, dependence across users, cluster randomization, or metric definition changes). The paper does not specify implementation details (resampling scheme, random seeds, computational budget) or provide code, which can affect reproducibility and the stability of the KS results for very large $n$. Finally, KS sensitivity can be high with large numbers of resamples, potentially flagging practically negligible deviations without an explicit practical thresholding strategy beyond “high D / low p”.","The authors explicitly mention that a deeper exploration of the applicability of alternative non-parametric methods for CIs (permutation sampling or bootstrapping), despite their computational cost, is outside the scope and provides an interesting avenue for future research. They also frame their “Outlook” as encouraging broader adoption of empirical assumption-checking to avoid inflated false positives, implying further work in refining and operationalizing such diagnostics.","Provide calibrated, decision-oriented thresholds for $D$ (or other discrepancy measures) that map to practical risk levels, rather than relying on KS significance alone, and study their robustness under metric multiplicity. Extend the procedure to handle dependence (clustered users, repeated measures, network effects) and temporal nonstationarity typical in online experimentation. Compare KS-based uniformity checks with alternative calibration diagnostics (e.g., QQ-plots of z-scores, Anderson–Darling, or empirical Type-I error at various $\alpha$) and evaluate power to detect assumption violations. Release reference implementations (e.g., an R/Python package) and benchmark computational strategies for large-scale resampling in production A/B platforms.",2502.04793v2,https://arxiv.org/pdf/2502.04793v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:32:40Z
FALSE,NA,Other,Event/count data|Other,Not applicable,Other,Other,TRUE,Other,Not provided,https://github.com/JeffSackmann/tennis_atp|https://www.remi-coulom.fr/Bayesian-Elo/|https://fivethirtyeight.com/features/how-qi-and-za-changed-scrabble/|https://chat.lmsys.org/|https://database.lichess.org/|http://aligulac.com/about/db/|http://crosstables.com|https://metastats.net/hearthstone/archetype/matchup/|https://github.com/online-go/goratings|https://www.renju.net/game/,"This paper studies the Elo rating algorithm under model misspecification and non-stationarity in practical matchmaking data, showing via likelihood-ratio tests that multiple real-world game datasets significantly deviate from the stationary Bradley–Terry (BT) model. Despite these violations, Elo often matches or outperforms more expressive rating systems (e.g., mElo/Elo2k and pairwise empirical win-rate models) on next-game win-probability prediction measured by binary cross-entropy. The authors reinterpret Elo as online gradient descent in an online convex optimization framework, yielding no-regret guarantees even in adversarial, misspecified, and non-stationary settings, and use this to explain Elo’s robustness. Extensive synthetic experiments on transitive but non-BT models (SST/WST) show a key trade-off between misspecification error and regret, where data sparsity (few games per player) favors simpler Elo updates. The paper also reports that pairwise ranking quality is strongly correlated with predictive accuracy, while warning that Elo may yield inconsistent total orderings under arbitrary matchmaking even for transitive games.","The prediction loss is binary cross-entropy: $\ell_t = -(o_t\ln p_t + (1-o_t)\ln(1-p_t))$. Elo predicts $p_t=\sigma(\theta_t[i_t]-\theta_t[j_t])$ and updates $\theta_{t+1}[i_t]=\theta_t[i_t]+\eta_t(o_t-p_t)$, $\theta_{t+1}[j_t]=\theta_t[j_t]-\eta_t(o_t-p_t)$, which equals online gradient descent on a convex logistic loss. The BT-model validity test uses a likelihood-ratio statistic $\Lambda=2\big(\min_\theta L_{\text{test}}(\theta)-\min_{\theta,\alpha} \tilde L_{\text{test}}([\theta;\alpha])\big)$, compared asymptotically to $\chi^2_2$ (with a conservative scaling in high-dimensional settings).","Across eight datasets (Renju, Chess, Tennis, Scrabble, StarCraft, Go, LLM Arena, Hearthstone), BT realizability is rejected with very small p-values (often reported as $<10^{-10}$; LLM Arena reported around $1\times 10^{-3}$ in Table 1 with a different augmentation). In predictive cross-entropy comparisons (Table 2), Elo is competitive and often best or near-best on several large, sparse datasets (e.g., Chess: Elo 0.6391 vs Elo2k 0.6387; Go: Elo 0.6443 vs Elo2k 0.6372; StarCraft: Elo 0.5713 vs Elo2k 0.5832), while Elo2k can win in denser regimes (e.g., Hearthstone: Elo2k 0.6847 vs Elo 0.6898). A martingale-based likelihood-ratio test using online-Elo-derived features also rejects BT for most datasets at $<10^{-10}$ for learning rates 0.01 and 0.08 (Table 3), supporting robustness of the rejection under adaptive matchmaking. Synthetic experiments indicate sparsity (small $t/N$) makes regret dominate, favoring Elo, whereas with enough data density, higher-capacity models (Elo2k/pairwise) can achieve lower asymptotic loss on non-BT data.","The authors caution that Elo should not be blindly trusted for ranking: it can fail to produce consistent total orderings under arbitrary matchmaking, even on transitive datasets. They also note that some likelihood-ratio feature augmentations fail to reject BT for certain dense datasets, requiring an alternative feature construction inspired by prior work (and that this alternative does not apply to other datasets because it requires dense data). For most of the paper they focus on prediction and postpone detailed ranking discussion to a later section, implying ranking is not the central validated outcome throughout.","This is not a reliability engineering study; conclusions about “reliability of Elo” are about statistical robustness/predictive performance rather than reliability of physical systems, so the results do not transfer to engineering reliability contexts (failure time/degradation/maintenance). Several empirical results hinge on hyperparameter selection and specific feature augmentation choices for the likelihood-ratio tests; different regularization, splits, or augmentations could change sensitivity or false-rejection behavior, but a systematic robustness analysis is not fully established in the excerpt. The paper compares against selected rating models (Elo2k, pairwise, Glicko, TrueSkill) but does not benchmark broadly against modern probabilistic preference learning or calibrated forecasting methods, which could affect the “Elo often wins” narrative. Real-world datasets are heterogeneous and may contain confounding factors (e.g., changing populations, anti-cheating measures, map/pool changes) that complicate causal attribution of non-stationarity vs misspecification.","They explicitly leave “the connection and comparison to other learning-to-rank methods” as an important future direction. They also indicate that the paper mostly focuses on prediction and leaves broader discussion of ranking to later, suggesting further ranking-focused investigation beyond what is included here.","Develop self-calibrating or adaptive Elo-style methods that explicitly handle non-stationary skill and non-uniform matchmaking (e.g., drift models, change-point detection, or time-varying regularization) while retaining convex/no-regret guarantees. Provide a more comprehensive robustness study of the BT rejection tests (alternative augmentations, multiple-testing corrections, sensitivity to train/test splitting under temporal dependence). Extend analysis to multi-player/team games, non-binary outcomes (beyond simple draw handling), and evaluation on calibrated probability metrics (Brier score, calibration curves) in addition to log loss. Release reference implementations and reproducible pipelines (including dataset preprocessing and hyperparameter selection) to standardize future comparisons across rating systems.",2502.10985v1,https://arxiv.org/pdf/2502.10985v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:33:20Z
TRUE,Failure mode analysis|Accelerated testing|Software reliability|Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Stochastic process|ML-based|Other",Right-censored|Degradation measurements|Event/count data|Sensor/condition monitoring|Mixture of types|Other,Not applicable,Transportation/logistics|Network/cybersecurity|Healthcare/medical|Semiconductor/electronics|Theoretical/simulation only|Other,Case study (real dataset)|Simulation study|Other,NA,None / Not applicable,Public repository (GitHub/GitLab)|Not applicable (No code used),https://github.com/yili-hong/DR-AIR|https://github.com/karendamata/RRML|https://incidentdatabase.ai|https://www.dmv.ca.gov/portal/vehicle-industry-services/autonomous-vehicles/,"This paper addresses the lack of publicly accessible datasets for AI reliability research by reviewing existing AI reliability data sources and establishing DR-AIR, a curated public repository of AI reliability datasets. It organizes AI reliability measurements and response data types relevant to reliability analysis (binary, count, continuous performance measures, time-to-event, recurrent events, and potential degradation data) and discusses common covariates and collection strategies (laboratory vs. field studies, virtual vs. physical tests). The paper catalogs multiple example datasets spanning incidents data, algorithm-level robustness/adversarial testing (including failure counts and failure times), module-level recurrent error events in autonomous-vehicle perception systems, and system-level recurrent events such as AV disengagements and collisions. It also summarizes reliability modeling approaches used in the referenced studies, including GLMs, accelerated failure time regression, NHPP-based recurrent event models, and event-triggering point processes for error propagation, as well as software reliability growth models for adversarial retraining experiments. DR-AIR is presented as an openly accessible, GPL-3.0-licensed platform intended to standardize access to AI reliability datasets and stimulate cross-disciplinary method development.","Key reliability formulations summarized include: (i) accelerated testing relationships via an acceleration factor $AF$ with $t_0=AF\,t_s$ and $F_0(t)=F_s(t/AF)$, plus the Arrhenius acceleration model $r=A\exp(-E_a/(kT))$; (ii) recurrent-event modeling with NHPP intensity (e.g., power-law) $\lambda(t;\theta)=(\beta/\eta)(t/\eta)^{\beta-1}$ and likelihood using cumulative intensity; (iii) error-propagation/event-triggering decomposition $\lambda_m(t)=\lambda_m^0(t)+\sum_n \lambda_{m,n}^p(t)$; and (iv) software reliability growth mean value function $m(t;x)$ with covariate link $g(x_t;\beta)=\exp(\beta^\top x_t)$.","The paper’s main quantitative takeaways are dataset-centric rather than proposing a new model: the AI Incident Database contains 878 reported incidents, and an earlier cleaned subset study identified 72 reliability-related incidents out of 126 analyzed (with 29 involving deaths or injuries). For the class-imbalance robustness experiment (Lian et al. 2021), the constructed dataset contains 252 experimental observations (three repeats per setting) with continuous responses (mean AUC and log-SD of AUC). For AV simulation error-injection data (Pan et al. 2024), seven scenarios are simulated for 20 seconds each, logging recurrent module error events; referenced results indicate the event-triggering point process yields lower MAE than HPP/NHPP baselines. The DR-AIR repository is made publicly available and structured with per-dataset subdirectories, including data descriptions and CSV/PNG assets.","The authors state that AI is diverse and rapidly advancing, making an exhaustive literature review difficult. They note the paper’s emphasis is mainly on algorithmic performance data and some system-level test data, and that degradation data in AI reliability has not yet been observed in their review/collection. They also acknowledge the work focuses primarily on software components of AI systems, leaving hardware reliability (e.g., GPUs) largely unaddressed.","Because the paper is primarily a review/repository paper, its impact depends on the long-term completeness, curation quality, and update cadence of DR-AIR; the paper does not specify governance details (e.g., review criteria for submissions, versioning, de-duplication, or dataset quality scoring). Many example datasets referenced are heterogeneous (incidents text, simulation logs, adversarial experiments, DMV reports), so cross-dataset comparability and standardized reliability endpoints may remain limited without stronger schema/metadata standards. Also, several modeling discussions assume independence or idealized process assumptions (e.g., NHPP structures, parametric forms) that may not hold across AI deployments with strong nonstationarity, concept drift, and reporting bias.",The authors call for the community to contribute and share AI reliability data and indicate they anticipate adding more datasets to DR-AIR as research progresses. They also highlight that modeling AI reliability is complex and that identifying key predictive factors influencing reliability remains an evolving area with challenges and opportunities for future research. They further note that collecting and incorporating degradation data and broader hardware-reliability considerations are important areas needing more exploration.,"A useful extension would be to define and enforce a standardized metadata schema for AI reliability datasets in DR-AIR (units of exposure, definitions of failure, censoring conventions, time bases, and uncertainty annotations) to enable reproducible benchmarking across studies. Developing self-starting/robust reliability methods tailored to AI nonstationarity (concept drift, changing operating policies, retraining interventions) and dataset shift would strengthen the link between repository data and practical reliability assurance. Additional work could incorporate privacy-preserving sharing mechanisms (e.g., federated summaries, synthetic-but-valid reliability surrogates) to attract industrial contributions while mitigating proprietary constraints.",2502.12386v1,https://arxiv.org/pdf/2502.12386v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:34:01Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Life distribution modeling|Other,Nonparametric/Semi-parametric|ML-based|Other,Right-censored|Sensor/condition monitoring|Mixture of types|Other,Predictive|Condition-based|Not applicable,Transportation/logistics|Energy/utilities|Other,Case study (real dataset)|Other,TRUE,R|Python|Other,Public repository (GitHub/GitLab),https://github.com/mcavs/Rashomon,"The paper proposes a Rashomon-perspective framework for uncertainty quantification in survival-model-based predictive maintenance, focusing on remaining useful life (RUL) estimation of aircraft engines. Instead of selecting a single “best” survival model, it defines a Rashomon survival set of models whose empirical loss is within an \(\varepsilon\) margin of the best model, and summarizes their disagreement over time via a Rashomon survival curve (a prediction band for survival probabilities). Nineteen survival modeling approaches (including Cox-family methods, tree/forest survival models, boosting, and XGBoost time-to-event variants, plus nonparametric Kaplan–Meier/Nelson–Aalen estimators) are trained and compared on NASA’s CMAPSS engine dataset under fixed censoring times \(t=200,225,250\). Results show that censoring level materially affects uncertainty: longer censoring times yield wider survival-probability ranges (greater predictive uncertainty), and relying on a single model can be riskier in some subsets. The work contributes a model-multiplicity-based tool to assess robustness of survival predictions over time for maintenance decision-making.","The Rashomon set is defined as \(\hat{R}(\mathcal{F},\varepsilon)=\{f\in\mathcal{F}:\hat{L}(f)\le \hat{L}(f^*)+\varepsilon\}\), and analogously for survival models \(\hat{R}_S(\mathcal{F}_S,\varepsilon)=\{f_S\in\mathcal{F}_S:\hat{L}(f_S)\le \hat{L}(f_S^*)+\varepsilon\}\) using a survival-performance loss (e.g., C-index/Brier score). The Rashomon survival curve is a time-indexed interval \(\Delta_\varepsilon(t_i)=[\min_{s\in R_S} S(t_i),\max_{s\in R_S} S(t_i)]\), giving the range of predicted survival probabilities across models in the Rashomon survival set.","Using \(\varepsilon=0.05\) and evaluating with C-index on CMAPSS subsets, the Rashomon survival set sizes and C-index summaries were: FD001: 5 models with \(0.8259\pm0.0204\); FD002: 4 models with \(0.7189\pm0.0124\); FD003: 4 models with \(0.8707\pm0.0181\); FD004: 8 models with \(0.8027\pm0.0146\). Survival probabilities differed substantially by subset (e.g., FD001 remains comparatively high at longer cycles, while FD002 declines faster), and the Rashomon prediction bands widened as censoring time increased (largest uncertainty reported for FD002, exceeding ~15% band width at 250 cycles; FD001 reported as narrowest, below ~5% at 250 cycles). The authors conclude that single-model RUL decisions can increase risk at certain time points and that censoring time strongly drives survival-prediction uncertainty.",The authors state that the empirical findings are limited to the CMAPSS dataset. They also imply that conclusions about effectiveness and uncertainty behavior are based on the specific experimental setting (chosen censoring times and model set) used in this study.,"The Rashomon set depends on the chosen \(\varepsilon\) and the particular loss/metric (the paper mentions C-index/Brier score options but evaluates sets using C-index), so uncertainty bands may change notably under different metrics or calibration-focused criteria. The approach aggregates disagreement across heterogeneous model classes without addressing probability calibration or whether the min–max envelope is overly conservative or sensitive to outlier models in the set. Censoring is imposed at fixed times (200/225/250 cycles); the realism of this censoring mechanism relative to operational maintenance/inspection processes is not validated, and time-dependence/autocorrelation in sensor trajectories is not explicitly modeled. Reproducibility details (exact preprocessing, hyperparameters, and software environment) may be necessary to replicate the reported Rashomon set membership and band widths.",They propose conducting large-scale studies over different datasets and modeling approaches to assess the generalizability of the Rashomon-perspective method for survival predictive maintenance and RUL estimation.,"Assess robustness to alternative \(\varepsilon\) choices and to different evaluation losses (e.g., time-dependent Brier score, calibration error) and study principled selection of \(\varepsilon\) based on decision costs. Extend the Rashomon survival curve from min–max bands to quantile bands or weighted sets to reduce sensitivity to extreme models while retaining multiplicity. Evaluate performance under more realistic censoring/maintenance policies (e.g., inspection-driven or condition-triggered censoring) and incorporate temporal modeling of sensor sequences (joint longitudinal–survival or deep time-series survival) within the Rashomon framework. Provide a practitioner-facing decision rule translating uncertainty bands into maintenance actions and expected cost/risk tradeoffs.",2502.15772v2,https://arxiv.org/pdf/2502.15772v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:34:33Z
FALSE,NA,Other,Other,Not applicable,Theoretical/simulation only|Healthcare/medical|Other,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"This position paper argues that uncertainty quantification—specifically principled, reliable estimation of aleatoric and epistemic uncertainty—has been largely overlooked in AI transparency research and should be integrated with ante-hoc interpretability and counterfactual explainability. It posits that uncertainty and ante-hoc interpretability are tightly coupled (explicit modeling assumptions enable more reliable uncertainty estimates), and that uncertainty provides a unifying framework for generating and evaluating counterfactual explanations. The paper reframes many counterfactual desiderata (e.g., plausibility, robustness, discriminativeness, stability, feasibility, diversity) as constraints or objectives on aleatoric/epistemic uncertainty, including along counterfactual paths rather than only at a single counterfactual point. It surveys related work connecting counterfactuals with uncertainty (e.g., uncertainty-reducing counterfactuals, probabilistic validity, trust scores, OOD detection proxies) and critiques ad-hoc proxies as potentially incompatible with model assumptions. The authors conclude with future plans to extend the uncertainty-centered view to other explanation types, investigate calibration/uncertainty methods for ante-hoc interpretable models, and explore second-order uncertainty and probabilistic formalizations of explanations.",Not applicable,Not applicable,"The authors note that reliable uncertainty quantification is challenging and in some cases even impossible, so making it the foundation of XAI could slow progress in the short term. They also acknowledge that adopting uncertainty-based explanations may require shifting to probabilistic explanations, increasing technical complexity and potentially harming usability for lay explainees. Additionally, they state that their perspective is inapplicable when models are accessible only as black-box oracles, where uncertainty proxies and post-hoc explainability may be unavoidable.","As a position paper, it does not provide concrete algorithms, formal proofs, or empirical evaluations demonstrating that uncertainty-driven counterfactual generation consistently improves outcomes across model classes and datasets. The discussion remains largely conceptual and depends on the availability of well-calibrated, decomposed aleatoric/epistemic uncertainty estimates, which are often nontrivial to obtain and to validate in practice (especially for deterministic/tabular models). The paper also does not operationalize how to compute path-wise uncertainty objectives efficiently or how to set practitioner-facing thresholds/constraints for different desiderata in real deployments.","The authors plan to extend the uncertainty-centered perspective beyond counterfactuals to other explanation types. They also plan to investigate state-of-the-art uncertainty quantification and probability calibration methods, focusing on ante-hoc interpretable models, and to examine other types/sources of uncertainty—especially second-order uncertainty—and suitable probabilistic formalisations of explanations, with an eye toward improving robustness and stability in high-stakes domains such as healthcare.","Develop and benchmark specific uncertainty-aware counterfactual algorithms (instance- and path-based) with clear optimization objectives tied to aleatoric/epistemic components, and compare against established counterfactual methods across tabular, text, and vision tasks. Provide practical guidance for validating uncertainty decomposition quality (e.g., calibration diagnostics, OOD stress tests) and for translating uncertainty constraints into actionable recourse policies under real-world constraints (immutability, costs, feasibility). Explore robust/self-starting uncertainty estimation under distribution shift and autocorrelation, and package reference implementations to enable reproducible evaluation and adoption.",2502.17007v1,https://arxiv.org/pdf/2502.17007v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:34:57Z
TRUE,System reliability|Other,ML-based|Bayesian|Simulation-based|Other,Simulated only,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a design-of-experiments (DOE) method to efficiently estimate long-term extreme structural response quantiles (and thus failure-related design metrics) for engineering structures when the short-term response model is a stochastic black-box simulator with heteroscedastic non-Gaussian noise. The approach models conditional short-term maxima $Y\mid x$ using a parametric distribution whose parameters $\theta(x)$ vary with the long-term environmental state $x$, and it learns $\theta(x)$ with a Gaussian-process (GP) surrogate that explicitly tracks epistemic uncertainty. Long-term extremes over $N_y$ years are obtained by combining (i) importance sampling over the environmental distribution and (ii) a finite-dimensional GP “freezing” approximation plus an unscented transform (UT) to propagate surrogate uncertainty into the quantity of interest (QOI), typically $z=G_{N_y}^{-1}(p)$. A Bayesian optimal experimental design acquisition function is defined as the expected posterior variance of the estimated QOI after adding a new experiment, accounting for both epistemic GP uncertainty and aleatory observation noise from parameter estimation at each design point. The main claimed benefit is reducing the otherwise prohibitive surrogate-Monte-Carlo cost for long return periods by replacing $N\approx 2.2\times 10^5$ time steps (for 25 years at 1-hour sea states) with a much smaller number of importance samples $M$ (e.g., ~5,000), enabling more expensive but better-informed DOE strategies for structural reliability/ULS assessment.","Structural reliability is framed via a limit-state $g(x)=y_{\text{capacity}}-y(x)$ and $R=1-P_f=P[g(X)>0]=\int_{g(x)>0} f_X(x)\,dx$. Short-term maxima conditional on environment are $Y\mid x$ with density $g_{Y\mid X}(y\mid x)$; the marginal short-term-maximum CDF is $G(y)=\int G_{Y\mid X}(y\mid x)f_X(x)\,dx$, and the $N_y$-year extreme CDF is $G_{N_y}(y)=G(y)^N$ for $N=\lceil N_y\cdot 365.25\cdot 24/T_s\rceil$. The QOI is a long-term extreme quantile $z=G_{N_y}^{-1}(p)$. The stochastic-simulator parameter observation model is $\theta_{\text{obs}}(x)=\theta(x)+\varepsilon(x)$ with $\varepsilon(x)\sim\mathcal N(0,\Sigma(x))$, and the DOE acquisition targets $s_k(x)=\mathbb E_{P_k\times Q_k}[H_{k+1}]$ where $H_k=\mathrm{var}_{P_k}[\hat z]$.","No empirical or numerical benchmark results are reported in the provided text; the contribution is methodological with computational scaling arguments. The paper gives an illustrative cost comparison: for a 25-year extreme with 1-hour $T_s$, crude surrogate Monte Carlo would require about $N\approx 220{,}000$ surrogate evaluations per estimate, whereas the proposed importance-sampling/UT approach may use a much smaller $M$ (example given: $\approx 5{,}000$, depending on dimensionality). It is also claimed that $M$ is independent of $N_y$, implying better scaling for longer horizons or finer time resolution. Performance and robustness are stated to require future empirical validation against brute-force Monte Carlo.","The method must approximate the marginalization used to form $G(y)$, and then raises the approximation to a large power ($G_{N_y}(y)=G(y)^N$), so small approximation errors in $G(y)$ can compound into much larger errors in long-term extremes. Results are sensitive to the chosen parametric surrogate family and its fit quality, especially in tail regions, which must be handled carefully. The authors state that these issues and their impact should be quantified further.","The approach assumes conditional short-term maxima $Y\mid x$ are well-represented by a chosen parametric family with parameters learned via MLE and asymptotic normality; this may be inaccurate for complex multimodal, nonstationary, or heavy-tailed responses, particularly in extremes. The finite-dimensional GP “freezing” approximation induces perfect correlation of GP outputs across inputs for a given draw, which can distort uncertainty propagation compared with sampling coherent GP functions. The observation-noise covariance for new design points is handled by a nearest-neighbor/heuristic reuse of $\Sigma(x)$, which may misrepresent heteroscedasticity and affect acquisition decisions. The work provides no real-data case study, calibration guidance, or sensitivity analysis for choices like importance density $h_X$, UT sigma-point settings, GP kernels, or the number of conditional simulator replications used to estimate $\theta(x)$.",The authors state the next stage is to demonstrate performance empirically on a test problem that is representative of real-world applications but still cheap enough to generate brute-force Monte Carlo reference results. They also suggest further work to quantify the impact of approximation error in the marginalization and the effect of surrogate distribution choice and tail fit on compounded extreme estimates.,"Validate the method on multiple real engineering case studies (e.g., wind turbine blades, mooring loads, bridges) with different noise structures and compare against established reliability tools (AK-MCS/Subset Simulation/FORM-SORM variants) under fixed computational budgets. Develop a principled model for $\Sigma(x)$ (e.g., a secondary GP or replication-aware likelihood) and study how misspecification impacts the acquisition function and final extreme quantiles. Extend the framework to direct failure probability estimation for explicit limit states $P[g(X)\le 0]$ (not only response quantiles) and to multiple correlated responses / multivariate extremes. Provide robust or semi-parametric alternatives for modeling $Y\mid x$ tails (e.g., GP-GPD tail splicing, quantile regression surrogates) and analyze theoretical error bounds for the $G(y)^N$ compounding effect.",2503.01566v1,https://arxiv.org/pdf/2503.01566v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:35:26Z
FALSE,NA,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Simulation-based|Other",Mixture of types|Simulated only|Other,Not applicable,Environmental monitoring|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper develops a fiducial (generalized pivotal quantity) inference procedure for interlaboratory calibration under the Rocke–Lorenzato two-component measurement-error model with heteroscedasticity (additive error at low concentrations and multiplicative/lognormal error at higher concentrations). It treats laboratory-specific calibration parameters (intercepts and slopes) as varying by lab and constructs block-wise fiducial pivotal quantities for lab parameters and variance components, requiring numerical solution only for two variance parameters per fiducial draw. The main goal is accurate interval estimation and point estimation (via fiducial mode and highest density intervals) of unknown analyte concentrations from new measurements, including split-sample settings across multiple laboratories. Extensive Monte Carlo studies (5,000 simulated datasets; 10,000 fiducial draws per dataset) show that the proposed fiducial intervals achieve near-nominal 95% coverage in small samples where Wald, bootstrap, and a modified likelihood-ratio approach can under-cover or be overly conservative, while maintaining comparable interval widths. The method is demonstrated on real interlaboratory datasets for cadmium and copper in water (EPA method 200.7), and the authors emphasize improved computational efficiency versus bootstrap and modified LRT approaches.","The calibration/measurement model is the Rocke–Lorenzato form with lab-specific parameters: $y_{ijk}=\alpha_i+\beta_i x_j\exp(\eta_{ijk})+\epsilon_{ijk}$ with $\eta_{ijk}\sim N(0,\sigma_\eta^2)$ and $\epsilon_{ijk}\sim N(0,\sigma_\epsilon^2)$, equivalently $y_{ijk}=\alpha_i+\beta_i x_j\exp(\sigma_\eta z^{\eta}_{ijk})+\sigma_\epsilon z^{\epsilon}_{ijk}$. Closed-form fiducial quantities are derived for $\tilde\beta_i$ (Eq. 3) and updated $\tilde\alpha_i$ (Eq. 5) using averages/sums over labs and concentrations, while $\tilde\sigma_\eta^2$ and $\tilde\sigma_\epsilon^2$ are obtained by numerically solving two nonlinear structural equations based on pooled within-lab and within-concentration sums of squares (Eqs. 6–7). For a new unknown concentration $X_j$, a fiducial pivotal quantity is $\tilde X_j=\frac{\sum y^*_{ijk}-\sum \tilde\alpha_i-\sum \tilde\sigma_\epsilon z^{\epsilon*}_{ijk}}{\sum \tilde\beta_i\exp(\tilde\sigma_\eta z^{\eta*}_{ijk})}$ (Eq. 9).","Across four simulation scenarios (training designs with 3 labs/3 concentrations vs 10 labs/9 concentrations; test designs using either multiple replicates from one lab or one replicate from multiple labs), the fiducial 95% intervals had coverage about 0.946–0.953, close to nominal. Competing methods often under-covered: e.g., Scenario 1.A at true concentration 50, coverage was 0.949 (fiducial) vs 0.918 (bootstrap) and 0.911 (Wald MLE); Scenario 2.B at true concentration 0, coverage was 0.952 (fiducial) vs 0.849 (bootstrap) and 0.936 (Wald MLE). Runtime comparisons reported that with 1000 resamples, bootstrap averaged about 3680 seconds in larger-sample scenarios, while the fiducial method took about 241 seconds for 1000 fiducial samples; the modified LRT method was argued to be infeasible at scale (estimated ~85 days for 10 unknown concentrations in the larger setting). In real copper/cadmium interlaboratory data, fiducial intervals avoided apparent under-coverage seen in bootstrap/Wald MLE for some concentrations (e.g., copper true 50 where some non-fiducial upper limits fell below 50).","The authors note that fiducial inference is generally not invariant to the choice of structural equations, so different valid constructions may lead to different fiducial distributions; they justify their particular block-wise structural-equation choices for interpretability and scalability. They also state that their fiducial quantities for $\beta_i$ (and downstream quantities) are approximate because they plug in a consistent point estimate $\hat\sigma_\eta$ when constructing those fiducial quantities. For the nonlinear system defining variance-component fiducials (Eqs. 6–7), they acknowledge that global existence/uniqueness cannot be guaranteed; draws yielding no solution are discarded, and if multiple solutions occur they pick the one minimizing a residual sum of squares.","The work is not reliability engineering-focused; it addresses measurement/calibration uncertainty rather than failure-time, degradation, maintenance, or system reliability modeling. The method relies heavily on normal/lognormal error assumptions and independence across measurements/labs; robustness to model misspecification (e.g., heavy tails, autocorrelation, lab-specific variance components) is not fully characterized in the main results. Discarding fiducial draws with no numerical solution could, in principle, induce bias in the inferred fiducial distribution if non-solution regions are non-negligible for some designs/parameter regimes, and the impact of this filtering is not quantified. Implementation details (software, tuning choices, numerical solver settings) are not provided, which may limit reproducibility and adoption.","The authors mention additional applications enabled by their approach: estimating detection limits (minimum $X$ such that the lower confidence limit exceeds 0) and redefining/estimating quantification limits based on interval length (e.g., interval length 10% of the point estimate), extending prior single-laboratory approaches to the multi-laboratory setting. They also emphasize the method’s suitability for practical regulatory-threshold comparisons via accurate lower confidence limits. No other explicit methodological extensions are clearly stated in the provided text.","A natural extension would be to develop robust or semi-parametric variants that relax the normal/lognormal assumptions and assess sensitivity to outliers and inter-lab heterogeneity in variance components. Providing a fully self-contained computational implementation (e.g., an R/Python package) with documented solver behavior, diagnostics for non-solution/multiple-solution draws, and guidance on tuning would improve usability and reproducibility. Another extension would be to incorporate correlation structures (within-run, within-lab over time) common in laboratory measurement series, and to generalize to multivariate analytes or multivariate instrument responses. Formal theoretical guarantees (e.g., conditions for existence/uniqueness of the variance-component system and the effect of discarding failed draws) would strengthen the inferential foundations.",2503.04588v3,https://arxiv.org/pdf/2503.04588v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:36:05Z
FALSE,NA,"Parametric (Weibull, etc.)|Stochastic process|Simulation-based|Other",Sensor/condition monitoring|Mixture of types,Not applicable,Transportation/logistics,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Not provided,https://openweathermap.org/|http://go-rts.com/rts-data/,"The paper proposes a hybrid Markovian framework for real-time bus link and remaining-route travel time prediction with uncertainty quantification using publicly available GTFS Static and GTFS Realtime feeds. Road travel times on each link are modeled as log-normal with heteroscedastic variance, where both the mean and variance of the log-travel-time are functions of covariates (rain, peak hour, weekday/weekend, and a real-time traffic indicator derived from space-mean speed). Parameters are estimated via maximum likelihood, and 95% uncertainty bounds are produced using asymptotic normality with covariance from the Fisher Information Matrix. A Markov chain with transition probabilities derived from predicted link travel times is then used to simulate bus progression and provide remaining time distributions to downstream stops, with prediction intervals taken from simulation percentiles. Evaluation on 2023 Gainesville, FL GTFS data shows consistently better MAE/RMSE than historical mean, linear regression, decision tree, and MLP baselines for selected links, and narrower uncertainty bounds than HM/LR; the paper also reports statistical tests supporting log-normality, heteroscedasticity, and approximate independence assumptions.","Link travel time is decomposed as $\Delta t_l=\Delta t_r+\Delta t_d+\Delta t_s$. Road travel time uses a heteroscedastic log-normal regression: with $Y_{ri}=\ln(\Delta t_{ri})\sim \mathcal N(\mu_{ri},\sigma_{ri}^2)$, $\mu_{ri}=\beta_{ri}^\top X_{ri}$ and $\sigma_{ri}^2=\exp(\gamma_{ri}^\top X_{ri})$; point prediction is the log-normal median $\hat{\Delta t}_{ri}=\exp(\hat\mu_{ri})$. Uncertainty for $\mu$ is derived from the Fisher Information $F(\theta_{ri})$ and yields a 95% CI for travel time $[\exp(\mu_{ri}-1.96\,\sigma(\hat\mu_{ri})),\exp(\mu_{ri}+1.96\,\sigma(\hat\mu_{ri}))]$. In the Markov model, the self-transition probability on link $i$ is estimated as $\hat p_{ii}=(S_i-1)/S_i$ where $S_i$ is the estimated number of discrete time steps to complete the link; remaining-time intervals are taken from simulation percentiles.","For five example links, the proposed LN-MLE method achieves lower MAE/RMSE than historical mean, linear regression, decision tree, and MLP (e.g., Link 1 MAE/RMSE 13.068/17.203 vs HM 23.851/28.122; Link 5 11.040/18.513 vs HM 27.326/34.220). The proposed uncertainty bounds are narrower than HM and LR for all reported links (e.g., Link 1 CI width 2.091 vs HM 2.696 and LR 2.719; Link 2 width 6.757 vs HM 13.601 and LR 14.009). Distributional assumption checks support modeling choices: K–S tests do not reject log-normality for sampled road and intersection times (all p-values > 0.05), and Breusch–Pagan tests reject homoscedasticity (p < 0.05, often < 0.001). Runs tests largely support independence at the 0.05 level (p > 0.05), though two links are borderline at 0.1, suggesting mild short-term correlation.","The authors note that stop dwell time is influenced by factors such as boarding/alighting activities and wheelchair usage, but these are not incorporated due to lack of hourly ridership data. They also state that modeling intersection waiting/passing time with a simple distribution may not capture complex queuing behavior. In addition, the Markovian model currently predicts only within a single bus route, not across the full network.","The framework’s uncertainty bounds for road travel time are based on asymptotic normality and parameter-estimation uncertainty; it is unclear whether they provide calibrated predictive intervals for future observations (which would also require accounting for process noise and model misspecification). The Markov simulation appears to assume covariates remain the same across downstream links at a given timestamp, which can bias remaining-time distributions when weather/traffic regimes vary along the route. Independence and stationarity assumptions (i.i.d. link observations; stationary Markov chain) may be violated in dense, high-frequency corridors or during incidents, potentially degrading both point and interval performance. The paper does not specify implementation details (software, optimization settings, simulation size sensitivity), which affects reproducibility and operational deployment assessment.",Future work will investigate influential factors and more complex behaviors for stop dwell time and intersection waiting/passing time to better capture bus travel dynamics. The authors also plan to extend the Markovian approach from single routes to a network-level model to support system-wide performance understanding and journey planning across the public transportation system.,"Develop and report calibrated predictive intervals (coverage-based evaluation) for both link and remaining-trip predictions, including robustness to non-lognormal tails and incident-driven regime shifts. Extend the model to handle autocorrelation explicitly (e.g., state-space/Kalman or semi-Markov formulations) and to allow link-specific covariate evolution downstream rather than freezing covariates at the query time. Provide an open-source reference implementation and standardized GTFS benchmarking protocol to enable reproducible comparisons against modern sequence models (e.g., temporal graph neural networks) under matched data/feature settings. Explore multivariate or hierarchical pooling across links/routes to improve parameter stability on low-sample links and quantify uncertainty under sparse data.",2503.05907v1,https://arxiv.org/pdf/2503.05907v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:36:42Z
FALSE,NA,Stochastic process|Simulation-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Transportation/logistics,Simulation study|Approximation methods|Other,TRUE,None / Not applicable,Not provided,NA,"This paper studies the accuracy of stop-to-stop origin–destination (O–D) matrices reconstructed from passive on-board Wi‑Fi sensing in buses, using optical automatic passenger counts (APC) for boarding/alighting totals and an on-board O–D survey as a reference for O–D shares. It defines RMS error metrics for O–D shares and for marginal boarding and alighting shares, and focuses on the ratios between marginal errors and O–D error to enable continuous quality control when only counts are available. The authors propose and compare several synthetic random “noise matrix” error structures (additive, multiplicative, short-trip-focused, central-stop-focused, and asymmetric row/column uncertainties) via simulations on synthetic O–D matrices and checks against the empirical bus-line data. Simple i.i.d. additive/multiplicative noise fails to reproduce the observed error-ratio behavior and the asymmetry between boarding and alighting errors, while a model with distinct uncertainties on boarding vs. alighting (row/column noise) plus background additive noise matches empirical ratios. Practically, they suggest using the fitted relationship between the number of stops and the error ratios to infer O–D error from readily available boarding/alighting count errors and discuss aggregation strategies that may reduce effective error (especially by aggregating alighting stops).","They define RMS O–D share error $\mathrm{Err}=\sqrt{\frac{1}{N^2}\sum_{i=1}^N\sum_{o=1}^N (\delta T_{io})^2}$ and RMS marginal boarding/alighting errors $\mathrm{Err(in)}=\sqrt{\frac{1}{N}\sum_{i=1}^N(\delta T_{i\bullet})^2}$, $\mathrm{Err(out)}=\sqrt{\frac{1}{N}\sum_{o=1}^N(\delta T_{\bullet o})^2}$, with ratios $\mathrm{err(in)}=\mathrm{Err(in)}/\mathrm{Err}$ and $\mathrm{err(out)}=\mathrm{Err(out)}/\mathrm{Err}$. Error is modeled as random-noise matrices, e.g., additive $\delta T_{io}\sim \xi_{io}-Z$ and multiplicative $\delta T_{io}\sim \xi_{io}T_{io}-Z$, plus structured variants concentrating on short trips or central stops, and asymmetric row/column uncertainties $\delta T_{io}\sim \xi_i-Z$ (boarding) and $\delta T_{io}\sim \xi_o-Z$ (alighting), combined with background noise.","For the T3 line ($N=27$), the empirical errors reported are $\mathrm{Err(in)}=0.018$, $\mathrm{Err(out)}=0.026$, and $\mathrm{Err}=0.0023$, giving ratios $\mathrm{err(in)}=8.08$ and $\mathrm{err(out)}=11.25$, notably larger than the $\sqrt{N}\approx 5.2$ scaling predicted by simple additive noise. Additive, multiplicative, and combined additive–multiplicative i.i.d. noise produce error ratios that follow an approximately $\sqrt{N}$ law and cannot reproduce the observed boarding vs. alighting asymmetry. Noise concentrated on short O–D trips and/or central stops produces error structures closer to the observed heatmaps and moves ratios away from $\sqrt{N}$, but still does not match both boarding and alighting ratios simultaneously. A combined model with asymmetric boarding vs. alighting uncertainties (different amplitudes, exemplified by $\sigma=0.03$ for boarding and $\sigma=0.045$ for alighting) plus additive noise ($\sigma=0.1$) reproduces both the magnitude and asymmetry of empirical error ratios.","The authors state that inferring the noise structure directly from data is difficult because of limited data: the experiment covers only three bus lines and the O–D survey benchmark is from a single day. They also note that the Wi‑Fi matrices are temporally aggregated over ~3 months to obtain enough observations and to match the “average day” nature of surveys, which constrains day-to-day validation.","The approach relies on treating the 2018 on-board O–D survey as “ground truth” for O–D shares despite being conducted about a year before Wi‑Fi data collection; unmodeled demand shifts could be confounded with sensing/reconstruction error. The proposed noise models are stylized and do not explicitly incorporate temporal autocorrelation, stop dwell times, vehicle crowding effects, or device-level emission processes; matching aggregate ratios may not guarantee accurate O–D-pair uncertainty calibration. The paper discusses simulations and Lowess smoothing but does not provide a reproducible implementation or parameter-selection procedure for deploying the method across systems, which may limit operational transferability.","They propose using the identified noise model to “withdraw the estimated noise” from Wi‑Fi O–D estimates to obtain O–D matrices closer to reality. They also suggest replicating the work on other networks to test whether the modeling is easily replicable and whether the same error relationships hold, and to compare modeled error structures across networks for behavioral and survey-validity insights.","A next step would be to formalize estimation of noise-model parameters (e.g., $\sigma$ values and structural components) from operational data via likelihood/Bayesian calibration with APC constraints, including uncertainty intervals on inferred O–D errors. Extending the framework to time-sliced O–D matrices (peak/off-peak) and to correlated or device-emission-driven noise could improve interpretability and robustness. Providing open-source code and a standardized evaluation benchmark (multiple cities/lines) would strengthen practical adoption and comparative validation.",2503.10175v1,https://arxiv.org/pdf/2503.10175v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:37:16Z
FALSE,NA,ML-based|Bayesian|Other,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,Python|Other,Public repository (GitHub/GitLab),github.com/sangttruong/reeval,"The paper proposes a model-based framework for holistic evaluation of large language models (LLMs) using Item Response Theory (IRT) to separate model ability from question difficulty, improving reliability when different test subsets are used. To reduce the high cost of traditional IRT calibration, it introduces amortized calibration: a learned difficulty predictor that infers item difficulty from question content embeddings, enabling constant-cost difficulty estimation for new questions. It further trains a conditional question generator (LLM fine-tuned with SFT and PPO) to generate new evaluation items at targeted difficulty levels, improving adaptive testing efficiency. Experiments span 22 NLP benchmarks with 172 models and 217,268 questions, showing better generalization than subset average scoring and large reductions in query/sample complexity for achieving a target reliability. The work is primarily about reliable and efficient AI/LLM evaluation (psychometrics-style measurement), not reliability engineering of physical systems.","The core measurement model is the Rasch IRT model: $p(y=1\mid \theta, z)=\sigma(\theta-z)$, where $\theta$ is model ability and $z$ is question difficulty. Adaptive testing selects the next question by maximizing Fisher information $I(\theta;z)=p(1-p)$ (with $p=\sigma(\theta-z)$), and updates ability via maximum likelihood $\theta^{t+1}=\arg\max_\theta \sum_{j=1}^t \log p(y_j\mid\theta,\hat z_j)$. Amortized calibration predicts difficulty for a new question from content features: $\tilde z_{new}=f_\phi\circ f_\omega(q_{new},c_{new})$, and the question generator is optimized with reward $r(q\mid z_{target})=-\lVert f_\phi(q)-z_{target}\rVert$.","Across datasets, the Rasch model achieves about 0.85 train AUC and 0.83 test AUC (averaged across datasets), while dataset-specific abilities increase to about 0.89/0.87 (train/test) AUC. In subset generalization tests, IRT (Rasch score) attains AUC $\approx 0.78\pm 0.07$ versus subset average score $\approx 0.50\pm 0.07$, indicating average-score measures do not generalize across subsets. Adaptive testing reduces question/query complexity by 53% on average and up to 86% while reaching 95% empirical reliability; on AIRBench it can reach 95% reliability in ~31 queries with a large bank. Generated questions do not substantially distort ability estimates (reported correlations around 0.96 for calibration and about 0.81 for test correlations across original vs generated sets).","The authors note that generated-question quality depends on training data and the accuracy of the difficulty predictor; poor embeddings or amortized models can misalign intended difficulty or content. They also warn that AI-generated questions can introduce bias and emphasize the need for human expert review to mitigate fairness issues. They further state that model responses depend on factors beyond intrinsic ability and question difficulty (e.g., temperature, few-shot examples, chain-of-thought prompting), which are not modeled in their current IRT setup.","The approach assumes (largely) dichotomous scoring and conditional independence consistent with Rasch/IRT; real LLM evaluation often has rubric ambiguity, judge noise, and correlated items that can violate these assumptions. Difficulty prediction is trained on existing benchmarks; distribution shift to genuinely novel tasks or adversarially constructed items could degrade calibration and thus adaptive testing decisions. Practical deployment also requires careful governance against contamination/leakage and periodic re-calibration, and the paper’s reported gains may depend on the specific benchmarks, grading pipelines (e.g., LLM-as-a-judge), and embedding choices used.","They propose incorporating additional factors affecting responses (sampling parameters, few-shot exemplars, chain-of-thought prompting) into IRT for improved measurement. They also suggest improving question reliability with more advanced validation, extending IRT beyond binary outcomes to non-binary assessments, and applying amortized calibration and conditional question generation beyond LLM evaluation to broader AI, psychometrics, and education assessment settings.","A valuable extension would be explicit modeling of judge uncertainty and inter-rater variability (e.g., hierarchical/Bayesian IRT with noisy labels) to quantify confidence in ability estimates. Robust/self-starting variants that handle unknown item parameters online (with active calibration under budget constraints) would improve real-world usability as new questions/models arrive. Extending to multidimensional IRT to separate capabilities/safety factors (rather than a single ability) could improve interpretability and monitoring for targeted regressions. Finally, releasing a standardized evaluation toolkit with reproducible pipelines (data splits, calibration protocols, and adaptive-testing policies) would help practitioners validate reliability across institutions and over time.",2503.13335v1,https://arxiv.org/pdf/2503.13335v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:37:51Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,Nonparametric/Semi-parametric|Stochastic process|ML-based|Hybrid/Ensemble|Other,Right-censored|Degradation measurements|Sensor/condition monitoring|Mixture of types,Predictive,Transportation/logistics|Other,Simulation study|Case study (real dataset),TRUE,MATLAB|Other,Not provided,NA,"The paper proposes Fed-Joint, a federated prognostics framework that jointly models nonlinear condition-monitoring (CM) degradation signals and time-to-failure events for remaining useful life (RUL) prediction when raw data cannot be centralized across sites. Degradation trajectories are modeled nonparametrically using a federated multi-output Gaussian process (FedMGP) with inducing points and variational inference to enable cross-site knowledge sharing without sharing raw time series. Failure risk is modeled via a federated Cox proportional hazards (CoxPH) survival model using the (estimated) degradation trajectory as a time-varying covariate, enabling prediction of failure probabilities over a future horizon and mean residual life (RUL). The method is evaluated via comprehensive simulations (including highly nonlinear “wiggly” signals) and a turbofan engine case study (NASA C-MAPSS FD001) with right-censoring, showing Fed-Joint is far more accurate than independent/local joint modeling and competitive with a centralized joint model while preserving data locality. The work advances reliability-oriented joint modeling by extending nonparametric joint longitudinal–survival modeling into a federated learning setting for collaborative predictive maintenance/prognostics.","Degradation model: $y_{k,m}(t)=f_{k,m}(t)+\epsilon_{k,m}(t)$ with GP-based multi-output construction; predictive distribution uses inducing-point posterior $q(u)=\mathcal N(\mu_u,\Psi_{u,u})$ leading to $p(f_{k,m,*}|\cdot)=\mathcal N(K_{f_{k,m,*},u}K_{u,u}^{-1}\hat\mu_u,\;\mathrm{cov}(f_{k,m}(t_*),f_{k,m}(t_*))+K_{f_{k,m,*},u}(\hat\Psi_{u,u}-K_{u,u}^{-1})K_{u,f_{k,m,*}})$. Survival model: CoxPH hazard $h_{k,m}(t)=h_0(t)\exp(\gamma^T w_{k,m}+\beta\hat f_{k,m}(t))$ with full log-likelihood used for federated optimization. Conditional failure probability over horizon $\Delta t$ is $\hat F(t^*+\Delta t|t^*)=1-\exp\{-\int_{t^*}^{t^*+\Delta t}\hat h_0(u)\exp(\hat\gamma^T w+\hat\beta\hat f(u))\,du\}$.","In simulations, Fed-Joint achieves MAE for mean RUL (MAE_mrl) around 6–7 for Scenario I and about 6–10 for highly nonlinear Scenario II, while Ind-Joint can be orders of magnitude worse (e.g., MAE_mrl ≈ 70–185) and LMM-Joint catastrophically fails on nonlinear signals (e.g., MAE_mrl ≈ 233). For failure-probability MAE (MAE_F), Fed-Joint is consistently low (typically ~0.01–0.12 depending on $\alpha$ and $\Delta t$) and close to Cen-Joint, while Ind-Joint and especially LMM-Joint are worse in Scenario II (e.g., MAE_F up to ~0.44–0.50 for LMM-Joint at larger horizons). In the NASA C-MAPSS FD001 case study using four informative sensors, Fed-Joint’s MAE_mrl is close to Cen-Joint (e.g., Sensor 4: 30.71 vs 32.65 at $\alpha=0.3$; 21.16 vs 20.56 at $\alpha=0.7$) and MAE_F is similarly comparable (e.g., Sensor 4: 0.184 vs 0.188 at $\Delta t=50,\alpha=0.3$). Overall results support that federated joint modeling preserves most of the centralized accuracy while avoiding raw data sharing.","The authors assume only right-censoring and no tied events “for simplicity,” noting this can be relaxed. They also note the CoxPH likelihood integral has no closed form and requires numerical approximation. Additionally, the paper adopts a two-stage joint-modeling estimation procedure, acknowledging the known issue that two-stage approaches can induce bias (while noting competitive predictive performance in prior work).","Performance is demonstrated mainly on simulations and a single benchmark (C-MAPSS FD001); broader validation on real multi-site industrial datasets with genuine heterogeneity, missingness, and asynchronous sampling would strengthen generalizability. Privacy is addressed via not sharing raw data, but the paper does not quantify leakage risks from parameter sharing (e.g., gradient/parameter inversion) or provide formal privacy guarantees. The survival component relies on CoxPH structure and baseline hazard choices (exponential/Weibull/piecewise), and robustness to PH violations or unmodeled frailty/site effects is not fully explored. Computational and communication costs (e.g., dependence on number of inducing points, rounds, and sites) are not fully benchmarked against practical constraints for deployment.","They propose extending Fed-Joint to more general settings including multi-source heterogeneous degradation data, incorporating domain adaptation for meta-/transfer learning, and integrating adaptive personalization strategies for site-specific performance. They also plan to explore privacy-preserving techniques such as differential privacy to further enhance data security. Another stated direction is applying federated MGP methods to diagnostic tasks such as identifying root causes of product quality defects when multi-channel signals are nonlinear and cannot be shared.","A valuable extension would be a self-starting/online version for streaming CM data with concept drift and intermittent connectivity, including asynchronous FL updates. Extending the survival model to handle tied events, competing risks, recurrent events, and shared frailty/random effects for site-to-site variability would broaden reliability applicability. Developing robust variants for non-Gaussian sensor noise/outliers and explicit treatment of informative censoring would improve real-world reliability. Packaging an open-source implementation (e.g., Python/R) with reproducible experiments and standardized benchmarks would accelerate adoption and allow fairer comparisons with modern deep federated prognostics baselines.",2503.13404v1,https://arxiv.org/pdf/2503.13404v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:38:29Z
FALSE,NA,ML-based|Nonparametric/Semi-parametric|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/predict-idlab/landmark-uq,"This paper proposes conformal prediction methods to provide reliable (finite-sample valid) uncertainty quantification for 2D/3D anatomical landmark localization in medical imaging. It introduces two multi-output conformal approaches—M-R2CCP and M-R2C2R (including an APS-based variant)—that turn multi-output regression into a classification/density-style framework to create flexible, potentially non-convex prediction regions rather than axis-aligned boxes or ellipsoids. The work benchmarks these methods against common uncertainty techniques (MC dropout, deep ensembles, test-time augmentation, heatmap-derived Gaussian fits, and naive probability-mass regions) and shows that many non-conformal methods with normality assumptions substantially under-cover (underestimate total predictive uncertainty). Across multiple datasets (ISBI 2015 cephalograms, a private canine hip dysplasia dataset, and a 3D CT mandibular molar dataset), the conformal methods achieve near-target marginal coverage while improving efficiency (smaller area/volume) versus conservative multi-output baselines like Bonferroni and max-nonconformity hyperrectangles. The paper emphasizes validity (reliability) as a prerequisite for interpretability in clinical decision support and recommends conformal prediction to obtain trustworthy confidence regions for landmark predictions.","The inductive conformal prediction (ICP) region is defined via a nonconformity score threshold from calibration scores: $\hat C_\alpha(X_{n+1})=\{y\in\mathcal Y: s(X_{n+1},y)\le S^*_{\lfloor \alpha(m+1)\rfloor}\}$. For multi-output extensions, it considers (i) max-nonconformity $S_i=\max(S_{i,1},\ldots,S_{i,d})$ and (ii) Mahalanobis/ellipsoidal scores $S_i=\sqrt{(Y_i-\hat f(X_i))^\top \hat\Sigma_i^{-1}(Y_i-\hat f(X_i))}$. The proposed M-R2CCP constructs a continuous conditional density-like score by $d$-dimensional (multi-linear) interpolation of binned class probabilities (Eq. 8) and uses $S_i=-\hat f_{X_i}(Y_i)$ to form conformal regions.","On ISBI 2015 (2D) at 95% target coverage, non-conformal baselines severely under-covered (e.g., MC dropout 5.47%, TTA 20.68%, ensemble 25.21%), while proposed conformal methods achieved near-target coverage with much smaller regions than conservative box-based conformal baselines (e.g., M-R2C2R coverage 94.84% with median area 8.335 mm$^2$ vs CP statistical correction coverage 95.53% with median area 14.620 mm$^2$). On MML (3D) at 95% target, CP statistical correction and max-nonconformity were extremely conservative in volume (median 542.33 and 483.26 mm$^3$), whereas M-R2CCP/M-R2C2R attained ~93–94% coverage with median volumes ~51–54 mm$^3$. The authors also report that heatmap-derived Gaussian fits improved over sampling baselines but still under-covered (e.g., 78.63% on ISBI 2D; 81.86% on MML 3D). In deterministic landmarking on MML (3D), a one-hot ensemble achieved test PE 1.39 mm and SDR@2mm 81.67%, outperforming a published baseline (Pruning-ResUNet3D test PE 1.96 mm).","The authors note that one minor drawback of (M-)R2CCP is that it requires evaluating the target domain to construct prediction regions, which is commonly handled with a grid search and can be computationally burdensome. They also state that the practical significance of observed adaptivity differences (e.g., APS variant) needs further investigation. Additionally, they remark that efficiency may depend on the choice of discrete-to-continuous interpolation method and suggest this should be studied further.","The validity guarantees rely on exchangeability; in medical imaging, distribution shift across sites/scanners/patient populations can violate this and lead to miscalibration without additional shift-robust conformal techniques. The paper focuses on marginal coverage; clinically relevant conditional coverage (e.g., across anatomy subgroups, pathology severity, or image quality strata) is not guaranteed and may require stratified/Mondrian or covariate-adaptive conformal designs with sufficient calibration data. The efficiency comparisons depend strongly on the underlying point predictor and uncertainty proxy (e.g., covariance from ensembles/MC/TTA); disentangling method gains from base-model improvements could require more standardized ablations. No explicit discussion is provided on runtime/memory costs for 3D region construction at practical voxel resolutions, which can be a deployment constraint.","The authors propose investigating whether the methods generalize well to other multi-output regression problems beyond landmark localization. They also suggest extensions to handle missing landmarks, exploring alternative interpolation techniques for mapping discrete probabilities to continuous regions, and designing automatic decision-making algorithms that leverage the conformal prediction regions.","A valuable extension would be to study robustness under dataset shift (multi-center generalization) using methods like conformal risk control, weighted conformal prediction, or online recalibration. Another direction is to provide subgroup-conditional or approximately conditional coverage guarantees (e.g., Mondrian partitions based on image quality or anatomy) while controlling efficiency loss. Developing scalable region-representation and fast membership tests for 3D (e.g., implicit surfaces, learned region generators, or sampling-based conformal region approximations) would improve clinical usability. Finally, releasing a reference implementation as a pip/conda package (and standardized benchmarks/scripts) would facilitate adoption and reproducibility.",2503.14106v1,https://arxiv.org/pdf/2503.14106v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:39:07Z
TRUE,Network/infrastructure reliability|System reliability|Other,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Network/cybersecurity|Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,Python|C/C++,Not provided,https://networkrepository.com/bio-DM-LC.php,"The paper studies approximation of the reliability function of binary-state networks (probability that a source and target remain connected) for large-scale systems where exact computation is NP-hard/#P-hard. It uses BAT-MCS (a hybrid of Binary Addition Tree enumeration and Monte Carlo Simulation with connectivity checking via PLSA) to generate reliability labels, and then evaluates 20 machine-learning regressors to learn a data-driven reliability approximation over three regimes: full range (0–1), high reliability (0.9–1), and ultra-high reliability (0.99–1). A key engineering finding is a practical “component reliability threshold”: when arc reliabilities are at least 0.9 in large networks, system reliability tends to converge to 1, making exhaustive computation less necessary. The study also proposes a dataset-scale-driven algorithm selection rule: ANN performs best with limited training data, while polynomial regression becomes superior once the sample size is on the order of (or exceeds) m² (m = number of arcs). Experiments on 20 benchmark networks and a larger DM-LC network support these guidelines and show both ANN and polynomial regression can outperform traditional standalone Monte Carlo approaches in prediction accuracy for the learned approximation task.","Monte Carlo reliability estimator is $R_{\mathrm{MCS}}=N_{\mathrm{pass}}/N_{\mathrm{sim}}$ (Eq. 1), with a sample-size requirement tied to a confidence/relative error bound (Eq. 2). BAT-MCS enumerates $\delta$-dimensional supervectors and allocates simulations per supervector proportional to probability, $N_{\mathrm{sim}}(S)=\left\lfloor N_{\mathrm{sim}}\,\frac{\Pr(S)}{\sum_{S\in\Omega}\Pr(S)}\right\rfloor$ (Eq. 3), then aggregates reliability as $R_{\mathrm{BAT\text{-}MCS}} \leftarrow R_{\mathrm{BAT\text{-}MCS}} + \Pr(S)\,\frac{N_{\mathrm{pass}}(S)}{N_{\mathrm{sim}}(S)}$ (Eq. 4). Connectivity of sampled states is checked via the PLSA layered search.","A principal threshold result is that for large-scale networks with arc reliability $\ge 0.9$, the system reliability is observed to be essentially 1 (near-unity), making the high- and ultra-high-reliability intervals computationally trivial in the DM-LC case (all 10,000 samples had reliability 1.0 in both 0.9–1.0 and 0.99–1.0 ranges). In Experiment 2 (DM-LC, 39 nodes/170 arcs, full range 0–1), ANN is best for 10k–30k samples (e.g., Test-MSE $7.24\times10^{-5}$ at 30,000), but polynomial regression becomes best at 40,000 samples with Test-MSE $5.61\times10^{-5}$ (ANN $5.66\times10^{-5}$). The abstract reports ANN Test-MSE $7.24\times10^{-5}$ at 30,000 samples and polynomial regression $5.61\times10^{-5}$ at 40,000 samples, and claims these outperform traditional Monte Carlo simulations for the approximation objective. On the 20-network benchmark suite, polynomial regression is the most robust overall in the 0–1 and 0.9–1 regimes (average ranks 1.3 and 3.0 respectively, per Table 6).","The paper notes that the study focuses on binary-state networks, whereas real-world systems often have multi-state behavior (components can operate at degraded/variable capacity). It also indicates practical issues in the ultra-high-reliability regime: only 15 of 20 benchmark instances yielded “valid results” because the remaining cases exceeded computational precision thresholds (very near-unity reliabilities).","Reliability labels used to train ML models are generated by BAT-MCS with a fixed simulation budget (e.g., $N_{\mathrm{sim}}=10{,}000$), so the supervised targets contain Monte Carlo noise; the impact of label uncertainty on model selection and reported MSEs is not fully quantified. The analysis assumes statistically independent arc states and perfectly reliable nodes, which can be unrealistic for infrastructure networks with common-cause failures, spatial correlation, or node failures; robustness to dependence is not evaluated. Generalization is demonstrated mainly across benchmark graphs and one larger network; performance on truly dynamic networks or on networks with different feature representations (beyond arc reliabilities) is unclear.","The authors propose extending the framework from binary-state to multi-state network reliability, including components with degraded/variable capacities. They also mention future directions toward dynamic/time-dependent reliability assessment and heterogeneous performance levels (e.g., incorporating degradation effects).","A natural extension is to incorporate correlated/component-dependent failures (e.g., via copulas, latent common-cause factors, or spatial correlation models) and test whether the ANN-vs-PR data-size crossover persists. Another direction is active learning/adaptive sampling: allocate additional BAT-MCS simulations to network configurations where the ML surrogate uncertainty is highest, reducing label noise efficiently. Providing an open implementation (C++ BAT-MCS + Python training pipeline) and standardized benchmarks would improve reproducibility and allow fair comparison with rare-event simulation methods (e.g., splitting/importance sampling) in the ultra-high-reliability regime.",2503.15545v1,https://arxiv.org/pdf/2503.15545v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:39:44Z
FALSE,NA,ML-based|Nonparametric/Semi-parametric|Other,Mixture of types|Simulated only,Not applicable,Healthcare/medical|Pharmaceutical|Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/clarafy/design-algorithm-selection,"This paper studies algorithm selection for machine learning-guided design (e.g., protein/RNA sequence design) with the goal of choosing design algorithm configurations whose resulting label distribution meets a user-specified success criterion (e.g., mean label above a threshold or at least 10% exceeding a target). The method formulates selection as multiple hypothesis testing over a finite menu of configurations and computes statistically valid p-values by combining many design predictions with held-out labeled data, correcting for prediction error under covariate shift via importance weighting and prediction-powered inference. With known density ratios between the design distribution and the labeled-data distribution, the method provides high-probability guarantees (family-wise error control via Bonferroni) that all selected configurations are truly successful, or returns the empty set if none can be certified. Experiments on simulated protein GB1 library design (closed-form densities) and simulated RNA binder design (density ratio estimation via multinomial logistic regression) show substantially reduced error rates compared with prediction-only and forecast-based baselines, at the cost of conservativeness when distributions are far apart. The work advances reliable decision-making for ML-guided design, but it is not about reliability engineering of physical/engineered systems; “reliable” here refers to statistical guarantees in algorithm selection.","The success criterion is defined on the induced design-label distribution as $\theta_\lambda := \mathbb{E}_{Y\sim P_{Y;\lambda}}[g(Y)]\ge \tau$. The prediction-powered estimator uses (i) the mean of $g$ applied to predicted labels on $N$ generated designs, $\hat\mu=\frac{1}{N}\sum_{i=1}^N g(\hat y_i^{\lambda})$, and (ii) an importance-weighted rectifier from $n$ held-out labeled points, $\hat\Delta=\frac{1}{n}\sum_{j=1}^n w_j\,(g(y_j)-g(\hat y_j))$ with $w_j=p_{X;\lambda}(x_j)/p_{\text{lab}}(x_j)$, giving $\hat\theta=\hat\mu+\hat\Delta$. For asymptotic testing, the p-value is computed as $p=1-\Phi\big((\hat\theta-\tau)/\sqrt{\hat\sigma^2_{\text{pred}}/N+\hat\sigma^2_{\text{err}}/n}\big)$, and selected configurations satisfy Bonferroni $p_\lambda\le \alpha/|\Lambda|$.","With known density ratios in the GB1 library-design simulation, the proposed method controls the empirical error rate below the target $\alpha=0.1$ across all tested thresholds for mean-label success criteria, while prediction-only and calibrated-forecast baselines exhibit near-100% error for most $\tau$ values. The method maintains high selection rates (often near 100%) for moderate success thresholds and becomes more conservative for stringent thresholds (selection rate declines for larger $\tau$). In the RNA-binder simulation requiring density-ratio estimation, the method substantially lowers error compared to prediction-only and calibrated forecasts, though error rises above $\alpha$ for more stringent $\tau$ due to density-ratio estimation error; when it errs, selected configurations’ mean labels tend to fall only slightly below $\tau$. The paper also reports that a conformal-prediction adaptation is overly conservative and selects nothing in their experiments.","The formal high-probability guarantee requires known density ratios between the design and labeled-data distributions; when density ratios are estimated, the theorems no longer apply and performance depends on estimation quality. The method can become conservative and return the empty set when the design distribution is too far from the labeled distribution, because importance weights increase variance and reduce effective sample size. The authors note that better density-ratio estimation tailored to importance-weighted mean estimation could improve performance in settings with unknown ratios.","The approach assumes i.i.d. held-out labeled data and a fixed conditional label mechanism $P(Y\mid X)$ across labeled vs. design distributions (pure covariate shift); violations (label shift, concept drift, feedback loops across design rounds) could invalidate p-values/guarantees. Practical use may be sensitive to weight instability/heavy tails and to misspecification of density models or classifiers used for density-ratio estimation, but robustness diagnostics and stabilization strategies (e.g., clipping, regularization, overlap checks) are not developed as a central component. The evaluation is primarily simulation-based (even when labels come from a measured GB1 landscape, the experiments simulate the selection procedure with sampled data), so real-world operational constraints (limited N, batch effects, measurement noise heterogeneity, changing assays) are not fully validated.","The authors suggest improving performance when density ratios are unknown by leveraging advances in density-ratio estimation, especially methods tailored for importance-weighted mean estimation. They also propose incorporating multiple-testing procedures that exploit structure (hierarchical or correlated configuration menus) to reduce Bonferroni conservativeness and increase selection rates. They encourage further work on uncertainty quantification for ML-guided design that targets downstream endpoints/decisions directly rather than generic uncertainty measures.","Develop self-normalized/robust importance-weighting variants (e.g., stabilized weights, overlap-aware trimming, or doubly robust estimators) with finite-sample guarantees under weaker assumptions on density ratios. Extend the framework to sequential/multi-round design where the labeled set grows adaptively and the design distribution depends on past outcomes, requiring online error control. Provide practical tooling (e.g., software, diagnostics, and guidelines) for detecting lack of support/overlap between labeled and design distributions and for choosing menu sizes and $n$ to achieve desired power under realistic resource constraints.",2503.20767v2,https://arxiv.org/pdf/2503.20767v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:40:22Z
TRUE,Maintenance optimization|System reliability|Other,Simulation-based|ML-based|Hybrid/Ensemble|Other,Simulated only|Other,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes an LLM-assisted reliability-based design optimization framework (LLM-RBDO) that leverages in-context learning to generate candidate designs and combines it with a metaheuristic-style iterative search loop. Reliability evaluation is accelerated by using Kriging surrogate models for limit-state/performance functions and estimating constraint satisfaction via Monte Carlo simulation on the surrogate, with a penalty function aggregating reliability constraint violations. The method is demonstrated on three RBDO case studies (including a 2D benchmark and a high-dimensional vehicle side-impact crashworthiness problem) using DeepSeek-V3 as the generative component. Results show LLM-RBDO can find feasible designs meeting target reliability levels with convergence behavior comparable to a strengthen elitist genetic algorithm (SEGA), and in the low-dimensional case it achieves a lower-cost feasible solution than SEGA. In the higher-dimensional multi-constraint case, LLM-RBDO tends to converge to near-optimal solutions and is limited by LLM difficulty handling complex constraints, though surrogate-based reliability estimates align closely with Monte Carlo “true” evaluations reported in the study.","The RBDO problem is posed as minimizing $\mathrm{Cost}(\mathbf d)$ subject to probabilistic constraints $\Pr(G_i(\mathbf x,\mathbf d)<0)\le 1-\Phi(\beta_{t,i})$ and bounds on $\mathbf d$ (Eq. 2). A Kriging surrogate is defined as $G_K(\mathbf x)=\mu+S(\mathbf x)$ with covariance $\mathrm{Cov}(S(\mathbf x_i),S(\mathbf x_j))=\sigma^2 R(\mathbf x_i,\mathbf x_j)$ and correlation $R=\exp(-d)$ where $d(\mathbf x_i,\mathbf x_j)=\sum_p a_p|x_{i,p}-x_{j,p}|^{b_p}$ (Eqs. 3–6); prediction and MSE are given by standard Kriging formulas (Eqs. 7–10). Reliability is estimated via Monte Carlo classification $I_f(\mathbf x_{m,i})=\mathbb{1}[G_K(\mathbf x_{m,i})<0]$ and $R=1-\frac{1}{N}\sum_{i=1}^N I_f(\mathbf x_{m,i})$ (Eqs. 11–12), and constraint handling uses $\mathrm{Penalty}=\sum_{i=1}^n \{w_p(R_t-R_i)^2\ \text{if }R_i<R_t;\ 0\ \text{otherwise}\}$ (Eq. 16).","Case study 1 (2D): LLM-RBDO (Kriging-based reliability) reports an optimum $\mathbf d=[3.38,3.04]$ with cost 6.43 and reliabilities $(G_1,G_2,G_3)=(0.9834,0.9805,1)$, closely matching a 10,000-sample Monte Carlo check $(0.9834,0.9805,1)$; SEGA (Kriging) yields cost 6.703 at $\mathbf d=[3.12,3.58]$. Case study 2 (vehicle side-impact, 9 design vars + 2 random parameters, $R_t=0.9$): LLM-RBDO (Kriging) finds $\mathbf d=[0.52,1.35,0.50,1.38,0.73,1.23,0.58,0.31,0.26]$ with cost 25.59 and reliability values including $G_8=0.9092$, $G_{10}=0.9042$ (Kriging), with close Monte Carlo “true” values $G_8=0.9047$, $G_{10}=0.9022$. SEGA (Kriging) achieves a lower cost 25.21 but violates/skirts some constraints (e.g., $G_2=0.8987$, $G_8=0.8937$ reported). FORM provides a lower-cost reference (24.14) with several constraints at 0.9/near 0.9 in the table comparisons.","The authors state that in higher-dimensional problems with multiple constraints, LLM-RBDO may fail to reach solutions comparable to traditional methods (FORM/SEGA), often converging to near-optimal solutions due to limitations of LLMs in handling highly complex constraints. They also note that certain hyperparameters of LLM-RBDO have not been systematically tuned, and that sensitivity to different LLM parameter scales has not been fully explored.","The approach depends heavily on surrogate-model fidelity; Kriging error can mis-rank candidate designs and lead to feasibility misclassification, especially near the limit state, yet there is limited discussion of adaptive sampling or uncertainty-aware acquisition to improve surrogate accuracy. Monte Carlo on the surrogate is still potentially expensive, and the paper does not quantify total computational cost (LLM calls + surrogate training + MC samples) against baselines on a common budget. The method’s robustness to non-Gaussian uncertainties, correlated variables, and time-dependent reliability is not evaluated despite being common in RBDO practice. Reproducibility is limited because implementation details (software, prompts, seeds, stopping criteria nuances) and code are not provided.","They propose exploring experiments with LLMs of different parameter scales to assess generalization capability and applicability. They also suggest that systematic hyperparameter tuning of LLM-RBDO could improve performance, and imply that improving applicability/robustness for complex high-dimensional constrained problems is a key direction.","A valuable extension would be to add active learning/adaptive Kriging (e.g., expected feasibility improvement) to focus sampling near the reliability boundary and reduce surrogate-induced feasibility errors. Developing a self-checking or constrained decoding mechanism (or repair operator) could enforce variable bounds and reliability constraints more reliably than prompt guidance alone. Evaluating performance under correlated/non-Gaussian uncertainties and with time-dependent reliability constraints would broaden engineering relevance. Providing an open-source implementation with standardized benchmarks and reporting computational budgets (LLM tokens, number of MC samples, surrogate retrains) would enable fair comparison and adoption.",2503.22401v1,https://arxiv.org/pdf/2503.22401v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:41:00Z
FALSE,NA,NA,NA,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,R,Not provided,https://www.R-project.org/,"This paper derives asymptotic standard errors (SEs) and Wald-type confidence intervals for two item response theory (IRT) reliability coefficients: proportional reduction in mean squared error (PRMSE) for the latent variable and classical test theory (CTT) reliability for the expected a posteriori (EAP) score under a unidimensional 2PL model. The main methodological contribution is a general large-sample framework that accounts for two sources of sampling variability simultaneously: (i) item parameter estimation uncertainty and (ii) additional variability from substituting population moments with sample moments (a necessity in long tests). The authors express reliability coefficients as smooth functions of parameterized sample averages and obtain asymptotic normality via a joint expansion and Delta method, yielding a plug-in SE formula. Simulation studies across test lengths (m=6,8,16,32) and sample sizes (n=250,500,1000) show the proposed SEs closely match empirical Monte Carlo SDs, with 95% CI coverage improving to nominal levels for moderate/large samples. An empirical illustration using the mirt SAT12 dataset (n=600, m=32) reports CTT reliability 0.918 (SE 0.036) and PRMSE 0.838 (SE 0.009).","PRMSE is expressed as $\mathrm{PRMSE}(\Theta)=\frac{\mathrm{Var}(E[\Theta\mid Y])}{\mathrm{Var}(\Theta)}$, rewritten using expectations of $H_1=E(\Theta\mid Y)$, $H_2=E(\Theta\mid Y)^2$, $H_3=\mathrm{Var}(\Theta\mid Y)$ as $\phi_{\mathrm{PRMSE}}(\eta)=\frac{\eta_2-\eta_1^2}{(\eta_2-\eta_1^2)+\eta_3}$. CTT reliability for the EAP score is written as $\mathrm{Rel}(s)=\frac{\mathrm{Var}(E[s(Y)\mid\Theta])}{\mathrm{Var}(s(Y))}$ and implemented via quadrature by defining additional components $H_{2+q}=H_1\,\frac{f(Y\mid\theta_q)}{f(Y)}$, giving $\phi_{\mathrm{Rel}}(\eta)=\frac{\sum_{q=1}^Q \eta_{2+q}^2 w_q-\eta_1^2}{\eta_2-\eta_1^2}$. The asymptotic SE for a reliability estimate $\phi(\hat\eta(\hat\nu))$ is $\mathrm{SE}=\sqrt{\frac{1}{n}\nabla\phi(\hat\eta)^\top\hat\Sigma(\hat\nu)\nabla\phi(\hat\eta)}$ with $\hat\Sigma$ capturing both item-parameter and sample-moment variability.","In simulations (500 replications per condition), the mean proposed SEs closely matched empirical Monte Carlo SDs for PRMSE with maximum discrepancy about 0.003 across all (n,m) settings, including n=250. For CTT reliability, SEs were slightly overestimated in the smallest-sample setting (n=250), especially for m=6 and m=32, with a maximum discrepancy about 0.004, and agreement improved for n=500 and n=1000. 95% CI coverage tended to under-cover for n=250 (attributed mainly to small-sample bias in point estimates), while coverage was within Monte Carlo error bounds for n=500 and n=1000. In the SAT12 example (n=600, m=32), CTT reliability was 0.918 with SE 0.036 and 95% CI [0.847, 0.990], while PRMSE was 0.838 with SE 0.009 and 95% CI [0.821, 0.856].",The authors note their derivations focus on two specific coefficients (CTT reliability for the EAP score and PRMSE for the untransformed latent variable) under a unidimensional 2PL model with a standard-normal latent distribution. They also assume correct model specification and acknowledge that performance under model misspecification (or only “close fit”) is not studied. They report small-sample bias in point estimates leading to suboptimal CI coverage when n is small.,"The work is psychometric/measurement reliability rather than engineering reliability, so the results do not address time-to-failure, degradation, maintenance, or system reliability contexts. The proposed Wald CIs rely on asymptotic normality and can yield bounds outside [0,1] (as seen with an estimated reliability >1), suggesting boundary-aware intervals (e.g., logit/Fisher-z transforms) may be preferable in practice. Dependence on numerical quadrature choices (number/range of nodes) could materially affect estimates/SEs, but sensitivity analyses to quadrature settings are not reported. Implementation details for computing $\hat\Sigma$ and gradients may be nontrivial for practitioners without shared code.","They propose extending the derivations to other observed scores for CTT reliability and other latent-score functions for PRMSE by modifying the defining functions $H$ and their gradients. They suggest extending beyond the unidimensional 2PL/normal-latent setup to more complex models, especially multidimensional IRT, noting CTT reliability may be harder due to intractable integrations while PRMSE should be more straightforward. They also suggest studying performance under model misspecification/close fit, developing bias-correction methods (e.g., bootstrap bias adjustment) and transformations (e.g., Fisher z) to improve small-sample CI coverage and boundary behavior, and exploring non-asymptotic alternatives such as simulation-based approaches.","Provide a reference implementation (e.g., an R function or mirt extension) that outputs the proposed SEs/CIs and validates them against bootstrap estimates to improve adoption and reproducibility. Develop constrained or transformation-based CI procedures that guarantee bounds in [0,1] and assess their coverage near boundaries, especially for high-reliability long tests. Evaluate robustness to violations of local independence, differential item functioning, and misspecified latent distributions, since these can affect both reliability point estimates and SEs. Extend the framework to adaptive testing settings or complex sampling (weights, clustering) where standard i.i.d. asymptotics may not apply.",2503.22924v2,https://arxiv.org/pdf/2503.22924v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:41:24Z
FALSE,NA,Simulation-based|Other,Simulated only|Other,Not applicable,Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies how common principal component (PC) retention rules in PCA—Kaiser-Guttman, Cattell’s scree test, and percent cumulative variance—can give contradictory results, especially in high-dimensional settings (n<p). Using simulation from a 10-dimensional multivariate normal model over a range of sample sizes, it quantifies differences in retained-PC counts and tests method differences via one-way ANOVA and Tukey HSD, finding statistically significant discrepancies among all three criteria. It argues that Kaiser-Guttman and scree test can be unstable or overly conservative depending on n/p, while the percent cumulative variance criterion is more stable; it recommends presenting selection via a Pareto-style chart showing cumulative variance and cutoffs. To improve PCA behavior when n<p, it compares covariance estimators (MLE, Ledoit–Wolf, and a standardized pairwise-differences covariance estimator) and claims the standardized PDC approach yields more balanced variance attribution and more reasonable PC retention. A small real-data illustration is discussed using a leukemia gene-expression dataset to show MLE can retain too few PCs and Ledoit–Wolf too many, while standardized PDC aligns more closely with population-level expectations.","Key elements are PCA based on eigen-decomposition of the covariance matrix $\Sigma$ (or its estimator), with retention rules: Kaiser–Guttman keeps PCs with eigenvalues $\lambda_j>1$; cumulative variance keeps the smallest $k$ such that $\sum_{j=1}^k \lambda_j / \sum_{j=1}^p \lambda_j \ge 0.80$; and scree test selects the elbow in the eigenvalue plot. Simulation uses $X\sim\mathcal{N}_p(\mu,\Sigma)$ with $p=10$ and $\mu=\mathbf{0}_{10}$, and evaluates retained-PC counts across methods and covariance estimators (MLE, Ledoit–Wolf, standardized PDC).","In simulation with $p=10$, at the population level Kaiser–Guttman retains 8 PCs, scree test retains 1 PC, and 80% cumulative variance retains 4 PCs; sample-level retention matches population for larger n (roughly n≥30) but collapses sharply as n decreases. At extreme high-dimensionality (e.g., n=2; p/n=5), both Kaiser–Guttman and 80% cumulative variance drop to 1 PC in sample estimates, showing strong sensitivity to small n, while scree test stays at 1 PC throughout (overly conservative). ANOVA comparing retained-PC counts across methods reports F=21.93 with p≈1.349×10^-6, and Tukey HSD indicates all pairwise differences are significant (e.g., KGC vs scree p=0.0000; KGC vs cumulative variance p=0.0013; scree vs cumulative variance p=0.0325). In n<p examples, the paper claims MLE overemphasizes PC1 (retaining too few PCs), Ledoit–Wolf suppresses later PCs (retaining too many), and standardized PDC yields a more balanced retention closer to population expectations; on a leukemia dataset (p=40, n=25), 80% CV yields 7 PCs (MLE), 19 PCs (LW), and 8 PCs (SPDC) vs a population benchmark of 10 PCs shown in the table.",None stated,"This is not a reliability-engineering study; “reliability” is used in the general/statistical sense rather than modeling failure, lifetime, or maintenance decisions. The scree test implementation is described as identifying the largest rate of decline/elbow, but without a fully specified automated rule, results may depend on subjective or ad hoc choices and may not be reproducible. Simulations are limited to a single dimension (p=10) and a single multivariate normal generative model with an unspecified covariance construction, which may not generalize to heavy-tailed, skewed, or correlated-realistic biomedical data; comparisons may also depend strongly on the chosen 80% threshold. The paper implies computational work (100 iterations across many n values and multiple covariance estimators) but provides no code, software details, or reproducibility materials.",None stated,"Evaluate PC-retention stability under non-normal data (heavy tails, skewness), autocorrelation, and realistic covariance structures, and vary p over larger ranges to stress-test n<p behavior. Provide a fully specified, automated scree/elbow detection algorithm and compare against modern selection approaches (parallel analysis, permutation tests, information criteria, cross-validated reconstruction error). Add real-world health/care case studies with downstream task metrics (classification/regression performance, calibration) to validate that “better” retention improves clinical modeling outcomes. Release reproducible code and an implementation (e.g., an R/Python package) for the standardized PDC estimator and the proposed Pareto-style selection workflow.",2503.24248v1,https://arxiv.org/pdf/2503.24248v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:41:53Z
FALSE,Other,"Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Simulation-based|Other",Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://doi.org/10.24432/C5T59G,"The paper proposes a statistical framework to quantify extreme (worst-case) machine-learning prediction errors, addressing the limitation of conventional validation metrics (e.g., MAE/MSE) that average performance and miss tail risk. It integrates Extreme Value Theory (EVT) with Monte Carlo cross-validation, using (i) a block-maxima approach fit with the Generalized Extreme Value (GEV) distribution and (ii) a peaks-over-threshold approach fit with the Generalized Pareto Distribution (GPD), with parameter uncertainty assessed via bootstrap confidence intervals. Experiments on synthetic regression data and two real datasets (Diabetes and WHO Life Expectancy) show that high-quantile bounds on maximum absolute error can be dramatically larger than average MAE, highlighting catastrophic-error risk not visible under standard validation. The method is presented as a practical algorithmic workflow for estimating probabilities/quantiles of extreme prediction failures to support safer deployment in high-stakes domains.","Block maxima are defined as $M_n=\max\{X_1,\dots,X_n\}$ and (in the ML setting) per-split extreme errors are saved as $G_n^j=\max_{x_i\in V_j}|\hat y_i-y_i|$ and $M_n^j=\max_{x_i\in V_j}(\hat y_i-y_i)^2$. The block-maxima distribution is modeled by the GEV CDF $G(z)=\exp\left(-\left[1+\xi\frac{z-\mu}{\sigma}\right]^{-1/\xi}\right)$ on $\{z:1+\xi(z-\mu)/\sigma>0\}$. Threshold exceedances are modeled via the GPD for $P(X>u+x\mid X>u)$ with CDF $H(x)=1-\left(1+\xi\frac{x-\mu}{\sigma}\right)^{-1/\xi}$ (or exponential form when $\xi=0$), where $u$ is a high threshold.","On synthetic data with linear regression (block-maxima/GEV), the fitted tail is Weibull-type ($\xi<0$) and the 95% quantile of maximum absolute error is reported as 27.4, versus an average $\langle MAE\rangle\approx 6.97; for squared error the 95% quantile is 465.7 versus $\langle MSE\rangle\approx 68.83. Using the threshold/GPD approach on synthetic data with $u=15$ and fitted parameters ($\xi=-0.43$, $\sigma=3.57$), the 95% bound on error is reported as 21.0. For the Diabetes dataset (442 samples) with linear regression (blocking/GEV), parameters are $\xi=-0.324$, $\mu=152.365$, $\sigma=10.724$ and the 95% bound on error is 177.2 while average MAE is 44.9; the GPD fit is also summarized with ($\sigma=5.4$, $\xi=-0.18$). For the WHO Life Expectancy dataset (1853 observations, 18 features after preprocessing), the blocking/GEV fit for linear regression yields $\xi=-0.296$, $\mu=18.217$, $\sigma=1.559$ and a 95% bound of 25.6 years while average MAE is 2.8 years.","The authors note EVT’s stationarity assumption may be violated in practice due to concept drift, requiring periodic checking/refitting of parameters/distributions as new data arrive. For the peaks-over-threshold (GPD) method, they state that selecting an appropriate threshold $u$ is difficult. They also emphasize EVT is asymptotic and small datasets can make extreme-event probability estimates less reliable (wider confidence intervals). Finally, they state the approach can be computationally expensive because it requires repeated training/validation via Monte Carlo cross-validation, and that more work is needed to create user-friendly tools for integration into ML workflows.","The evaluation focuses on a limited set of datasets and largely on linear regression for real-data demonstrations; broader evidence across model classes, tasks, and distribution shifts would be needed to establish general reliability claims. Dependence assumptions underlying EVT (e.g., approximate independence of extremes across Monte Carlo splits and within validation errors) are not deeply examined; correlated errors or resampling dependencies could bias tail fits and uncertainty estimates. The framework quantifies extreme error magnitudes but offers limited guidance on operational decision thresholds (e.g., how to act on a predicted tail risk) or calibration/verification of tail probability estimates under real deployment conditions.","They plan to extend the approach to classification problems and to study how dataset size affects the reliability of EVT-based predictions. They also propose refining EVT parameter-estimation methods to improve robustness for small datasets and high-dimensional data, and call for further research into suitable EVT-based methodologies and user-friendly tooling for practical adoption.","A valuable extension would be to handle non-stationarity explicitly (e.g., drift-aware or time-varying EVT models) so tail-risk estimates remain valid under changing data distributions. Another direction is rigorous treatment of dependence induced by cross-validation resampling and temporal/clustered data, potentially via declustering or block-bootstrap variants tailored to ML validation. Developing open-source reference implementations with diagnostics (threshold selection, goodness-of-fit, stability plots, and calibrated tail-probability checks) and benchmarking against alternative tail-risk measures (e.g., CVaR/quantile loss, conformal prediction bounds) would strengthen practical adoption and comparative understanding.",2503.24262v1,https://arxiv.org/pdf/2503.24262v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:42:27Z
TRUE,Degradation modeling|RUL prediction,"Stochastic process|Bayesian|Parametric (Weibull, etc.)|Other",Degradation measurements|Sensor/condition monitoring|Simulated only|Mixture of types,Not applicable,Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,Other,Not provided,NA,"The paper proposes a Bayesian framework for modeling multivariate degradation data (multiple degradation characteristics per unit) under time-varying/dynamic covariates such as temperature, humidity, and UV exposure. Degradation trajectories are modeled via a nonlinear mixed-effects “general path model” in which cumulative exposure to covariates drives degradation, and unit-specific random effects capture heterogeneity and cross-characteristic dependence through a multivariate normal covariance structure. Dynamic covariate effects are modeled flexibly using shape-constrained Bayesian P-splines (monotone and convex constraints) to incorporate engineering knowledge while controlling overfitting. Posterior inference is performed with the No-U-Turn Sampler (NUTS) in Stan, and an explicit Monte Carlo algorithm is provided to estimate the system failure-time distribution induced by the degradation model (system fails when any characteristic hits its threshold). The approach is validated via simulation (comparing correlated vs. independent multivariate random-effects models) and demonstrated on NIST outdoor weathering data for organic coatings with three FTIR-based degradation characteristics and dynamic environmental covariates.","The degradation observation model is $y_{ijk}=D_j(t_{ik})+\epsilon_{ijk}$ with $\epsilon_{ijk}\sim\mathcal N(0,\sigma^2)$, where degradation depends on total cumulative exposure $\tau_{ijk}=\sum_{m=1}^M g_{ijm}(t_{ik})$. Covariate exposure is built from an effect function $f_{jm}(x)=\sum_{q=1}^{Q_m}B_{mq}(x)\,\beta_{jmq}$ and cumulative exposure $g_{ijm}(t)=\int_0^t f_{jm}(x_{im}(s))ds\approx \sum_{s_{il}\le t}\sum_q B_{mq}(x_{im}(s_{il}))\beta_{jmq}(s_{il}-s_{i,l-1})$. The case-study path model uses $D_j(\tau;\theta_j,w_{ij})=-\frac{\alpha_j\exp(w_{ij})}{1+\exp\{-(\log \tau)/\gamma_j\}}$ with $w_i\sim\mathcal N(0,\Sigma)$ (unstructured $\Sigma$ to model cross-DC dependence). Failure-time distribution is estimated by Monte Carlo simulation of covariate paths and random effects, taking $t_f=\min\{t: D_j(t;\theta_j,w)\ge D_{fj}\ \text{for any }j\}$ and $\hat F(t)=\frac1B\sum_{b=1}^B \mathbf 1\{t_f^{(b)}\le t\}$.","In simulations (27 scenarios varying number of units $I\in\{10,20,50\}$, measurements per unit $n_i\in\{10,20,50\}$, and cross-DC correlation $\rho\in\{0.2,0.5,0.9\}$), parameter estimation bias/SD generally decreases as $I$ and $n_i$ increase, and coverage probabilities are near nominal 95% except when $n_i$ is small. For reliability estimation, the correlated multivariate model substantially outperforms an independence model when cross-DC correlations are moderate-to-strong (as shown by lower Avg RMSE and higher coverage of the failure-time distribution), while the independence model can be slightly better when correlation is weak. In the NIST coating case study (36 specimens, 3 degradation characteristics), posterior median correlation estimates under the correlated model include $\rho_{12}\approx0.738$, $\rho_{13}\approx0.121$, and $\rho_{23}\approx0.367$, indicating stronger dependence between closer-wavelength FTIR measures. Estimated covariate effects satisfy assumed shapes: UV and temperature effects are increasing and dominate, while RH shows weak/insignificant effect; predicted failure-time CDF from the correlated model is slightly below the independence-model curve (indicating higher reliability), consistent with relatively small estimated correlations for some DC pairs.","The authors note that when the data provide limited information about covariate-effect shape (e.g., RH with narrow range/low variation in the field data), enforcing more rigid shape constraints can cause serious divergences in MCMC sampling and may lead to misleading inference. They also mention that in such cases informative priors for the shape-constraint parameter (e.g., for $\delta_m$) may be needed rather than non-informative priors. In the weathering application, they highlight that RH’s convexity is not as evident as in prior work due to insufficient variability in RH measurements.","The framework requires a parametric specification of the degradation path form $D(\cdot)$ (a particular nonlinear general path model), so misspecification risk may be substantial if the true degradation dynamics differ. The failure-time distribution estimation relies on a separate parametric model for the covariate process (adapted from prior work) and a nested Monte Carlo procedure; uncertainty propagation from the covariate-process model into reliability estimates may be sensitive to modeling/bootstrapping choices and computationally expensive. The paper does not indicate availability of open-source code or a reproducible workflow, which may hinder adoption and independent verification. The approach is tailored to “soft-failure” threshold crossing for multiple DCs; extensions to competing risks with hard failures, repairable systems, or measurement error in covariates may require nontrivial modifications.","They propose developing a structured variance–covariance matrix for multivariate random effects using data-specific priors (instead of a fully unstructured covariance). They also suggest using Bayesian autoregressive time-series models for estimating covariate processes and incorporating these covariate-process models directly into the posterior likelihood. More broadly, they note the approach can be adapted to other degradation applications with similar configurations (multiple DCs with dynamic covariates).","Developing a joint model that simultaneously estimates degradation and covariate-process dynamics (rather than two-stage modeling plus simulation) would improve coherence and uncertainty quantification. Robust or semiparametric variants could reduce sensitivity to the chosen general path functional form and to outliers/non-Gaussian measurement errors. Extending to high-dimensional multivariate degradation (large $J$) would likely need structured covariance modeling (e.g., factor models, sparse precision matrices) and scalable inference. Providing an open-source Stan implementation (and data-processing utilities) would greatly improve reproducibility and practitioner uptake, especially for reliability engineers applying the method to condition-monitoring streams.",2504.05484v1,https://arxiv.org/pdf/2504.05484v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:43:09Z
TRUE,System reliability|Network/infrastructure reliability|Other,ML-based|Simulation-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Energy/utilities,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a method to embed reliability verification constraints into unit-commitment–integrated generation expansion planning (GEP) models, replacing conventional reserve-margin capacity constraints. It generates annual training datasets of generation mixes labeled by time-series reliability assessment simulations that compute loss of load hour (LOLH) using forced outage rates and 8,760-hour wind/solar/load data, and trains a weighted oblique decision tree (WODT) classifier to learn reliability-feasible regions (meeting NERC 1/10 criterion, LOLH ≤ 2.4). Reliability-feasible leaf regions are converted into disjunctive linear constraints and then transformed exactly into mixed-integer linear constraints via a convex-hull reformulation, enabling direct inclusion in the GEP MILP without altering its core structure. The approach is validated on a 20-year ERCOT case study, showing LOLH reductions to meet the criterion across the horizon and lower capacity redundancy relative to a reserve-margin tuning baseline. Reported economic impacts include $1.66B lower investment cost partially offset by +$0.95B operating cost, for a net savings of $0.71B.","The learned reliability-feasible set for each planning year is expressed as a disjunction over WODT-derived polyhedra: $\bigvee_{k=1}^{N_t^{fes}}[W_{t,k}\; R_{t,k} x_f^t \le b_{t,k},\; x_{f,lower}^t\le x_f^t\le x_{f,upper}^t]$. This is reformulated using a convex-hull MILP with auxiliary variables $z_{f}^{t,k}$: $\sum_k z_{f}^{t,k}=x_f^t$, $\sum_k W_{t,k}=1$, $R_{t,k} z_{f}^{t,k}\le b_{t,k} W_{t,k}$, and $W_{t,k}x_{f,lower}^t\le z_{f}^{t,k}\le W_{t,k}x_{f,upper}^t$. Labels are based on meeting the reliability criterion LOLH $\le 2.4$ (NERC “1/10”).","In the ERCOT 20-year case, the proposed RVC-GEP reduces very high LOLH values under zero reserve margin (e.g., Year 1 LOLH 44.58; Year 20 LOLH 290.83) to values meeting the 1/10 criterion under embedded reliability constraints (Year 1 LOLH 2.34; Year 20 LOLH 2.40). WODT training uses max depth 6 and L-BFGS max iterations 100, tuned to achieve >99.9% correct assignment of labeled samples to corresponding labeled leaves; extracted disjunction sizes are reported as 13, 20, 23, 25, 20, 19, and 19 polyhedra for years 1–4 and 18–20. Capacity margin reductions versus a reserve-margin baseline are reported in the last four years as 0.0317, 0.0559, 0.0573, and 0.0479. Total investment cost is reduced by $1.66B, operating cost increases by $0.95B, yielding net savings of $0.71B.","The authors state there is no theoretical method to guide selection of key WODT training parameters (maximum depth and L-BFGS iteration limit), so they are set via trial-and-error to achieve >99.9% labeling consistency. They also note that detailed data for the extracted disjunctions are not provided due to space limitations.","The reliability constraints are only as accurate as the simulation-based labeling process and the coverage of the generated training dataset; extrapolation beyond the chosen feature-variable bounds may yield unreliable feasibility classification. The approach fixes the potential operational quantities for generator types excluded from the WODT feature set, which may limit optimality if those technologies become reliability-relevant under different cost, policy, or weather regimes. Reported validation is largely a single-region case study (ERCOT) and may not generalize to systems with different correlations, contingency policies, market rules, or weather-driven outage dependencies; robustness to changing forced outage rates and correlated outages/extreme events is not fully demonstrated.",The authors propose extending the approach to account for uncertainties in long-term load forecasting by generating training labels using stochastic load-growth scenarios and integrating these scenarios with the reliability verification constraints into the GEP framework. They also propose developing a more advanced decomposition algorithm to further improve computational efficiency.,"Useful extensions include explicitly modeling correlated outages and weather-dependent forced outage rates (common-mode failures), and incorporating additional adequacy metrics (e.g., LOLE/EUE) or multi-metric constraints rather than LOLH alone. A self-adaptive or online retraining scheme could update WODT-derived constraints as new data (weather years, outage statistics) become available, improving robustness over long horizons. Providing open-source implementations and benchmarking against alternative surrogate/constraint-learning methods (e.g., MILP-embeddable decision rules, gradient-boosted trees with MILP encodings) would strengthen reproducibility and comparative evidence.",2504.07131v1,https://arxiv.org/pdf/2504.07131v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:43:41Z
FALSE,NA,NA,NA,Not applicable,Theoretical/simulation only,Other,FALSE,None / Not applicable,Not applicable (No code used),https://bpb-us-w2.wpmucdn.com/sites.wustl.edu/dist/3/2139/files/2019/09/conesepkkt.pdf,"This paper proposes a formal model of explainable ML where an “explanation” for a prediction is a small subset of the training data that certifies the prediction against up to b corrupted/noisy training points. It introduces robust certificates and the robust hollow star number, showing this combinatorial quantity exactly characterizes the worst-case minimum certificate size (tight upper and lower bounds). The paper also studies distributional settings: it gives sharp bounds on the certificate size achievable from i.i.d. samples when a test point eventually falls in the agreement region, and introduces the certificate coefficient εx to capture distribution-dependent difficulty of certifying a specific test point. It proves near-matching upper and lower bounds on the sample size needed for robust certification in terms of εx, corruption budget b, and VC dimension d, and further explores reweighting/rejection-sampling ideas to increase εx and obtain shorter certificates. Overall, the contribution is learning-theoretic and explainability-focused rather than reliability engineering (no failure-time/degradation/maintenance modeling).","Robust agreement (Eq. 1): for labeled sample S={(xi,yi)} and budget b, (x,y) is in the b-robust agreement region if for all h∈H, (∑i 1[h(xi)≠yi]≤b) ⇒ h(x)=y. Certificate coefficient (Eqs. 2–3): Hx={h∈H: h(x)≠h⋆(x)} and εx=inf_{h∈Hx} P_{z∼D}[h(z)≠h⋆(z)], which controls the sample size needed so that x is in the b-robust agreement region with high probability.","Worst-case certificate size is characterized exactly: if the b-robust hollow star number is sb, then any certifiable test point admits a certificate of size ≤ sb−1, and there are instances where sb−1 is necessary (Theorem 3.7). In an i.i.d. sampling setting, if a point x eventually belongs to the agreement region with positive probability, then with high probability one can extract a b-robust certificate of size at most (b+1)(s0−1), and this is optimal in general (Theorem 4.1). Distribution-dependent sample complexity: if εx>0 and VC(H)=d, then m=O((b + d log(1/εx) + log(1/δ))/εx) samples suffice for robust certification with probability 1−δ, and the dependence on b, d log(1/εx), and log(1/δ) is tight up to constants (Theorem 5.2, Proposition 5.4). Reweighting can improve achievable certificate sizes by increasing an “optimal reweighted” coefficient ε⋆x, yielding certificates of size O((b + d log(1/ε⋆x) + log(1/δ))/ε⋆x) from polynomially many original draws (Theorem 6.3).","The paper notes that key combinatorial parameters (e.g., the b-robust hollow star number) may not have tight estimates for natural hypothesis classes, and explicitly discusses gaps (e.g., for halfspaces) between known upper bounds and lower bounds and the need to close them. It also states that computing/using the optimal reweighted coefficient ε⋆x assumes knowledge of the data distribution D, which is a strong assumption for practical algorithms.","Results are primarily theoretical and do not include empirical validation on real datasets or practical deployment constraints (e.g., privacy, storage, or computational cost of extracting minimal certificates from large training sets). The model assumes a bounded corruption budget b and realizability-style structure relative to a hypothesis class H; robustness guarantees may degrade when data are non-i.i.d., concept drift occurs, or the true target is misspecified/out of class. The work does not provide efficient general algorithms for finding minimal certificates in broad classes (the characterization is existential/worst-case), which may limit applicability despite tight information-theoretic bounds.","The paper highlights the need to better “chart out” (upper/lower bound) the b-robust hollow star number for natural classes (e.g., improving bounds for halfspaces). It poses an explicit open question on constructing general-purpose reweighting schemes that do not require knowing D but can implicitly converge to weightings achieving near-optimal εx(Dw,h⋆) (Open Question 6.5).","Develop computationally efficient certificate-extraction algorithms with approximation guarantees (e.g., polynomial-time methods that produce near-minimal certificates) for major hypothesis classes such as linear separators and kernels. Extend the framework to non-i.i.d. settings (covariate shift, temporal dependence) and to multiclass/structured outputs, where agreement regions and hollow-star-like parameters may behave differently. Study practical tradeoffs with privacy and data minimization (e.g., releasing certificates without leaking sensitive training data), potentially via differentially private certificate mechanisms.",2504.08377v4,https://arxiv.org/pdf/2504.08377v4.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:44:03Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,"ML-based|Parametric (Weibull, etc.)|Hybrid/Ensemble",Sensor/condition monitoring|Degradation measurements|Mixture of types|Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general),Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"This paper addresses remaining useful life (RUL) prediction under indirect supervision when covariate time series are sparse and irregular, where common temporal models require interpolation that can bias predictions. The authors propose “parameterized static regression,” which performs posterior estimation using single time-point samples (static regression) and then captures temporal dependency via a parametrical rectification (PR) step that fits the posterior estimates to the same parametric labeling function used during training. They study both linear RUL labeling and a non-linear Weibull-inspired labeling function, and introduce an “identical batch training” scheme that samples batches from one unit/subject at a time (sharing the same labeling function) to reduce overfitting and computational cost. Experiments on CMAPSS and N-CMAPSS with simulated data scarcity show competitive or improved RMSE compared with interpolation-based temporal baselines and prior scarce-data methods, with PR notably improving performance under high scarcity. The work advances prognostics/SPC-adjacent reliability ML by reframing scarce time-series RUL prediction as indirectly supervised regression that avoids interpolation while still enforcing physically/plausibly shaped temporal trajectories.","RUL labeling under indirect supervision is defined by a parametric function, e.g., linear $Y_i(t;\theta_i)=\theta_i-t$ and a non-linear Weibull-form $Y_i(t;\theta_i)=\alpha\exp(-(t/\theta_i)^\beta)$. A static regressor maps each sample $x^i_{t,s}\mapsto \hat y^i_{t,s}$ and is trained with loss $\mathcal L=\frac{1}{|B_i|}\sum_{x\in B_i}(\hat y-Y_i(t;\theta_i))^2+\gamma(\hat x-x)^2$ (AER includes reconstruction). During inference, PR estimates $\hat\theta_{i'}$ by minimizing $J(\hat\theta_{i'}|\hat y, t)=\frac{1}{M_{i'}}\sum_{j=1}^{M_{i'}}(\hat y_j-Y_{i'}(t_j;\hat\theta_{i'}))^2$ (optimized via Levenberg–Marquardt), then predicts rectified $Y_{i'}(T_{i'};\hat\theta_{i'})$.","On CMAPSS with scarcity in the train set only, the proposed approach achieves RMSEs (mean±sd over 10 seeds) such as FD002: 16.59±1.20 (50% kept), 16.04±1.48 (70% scarcity), and 15.89±1.05 (90% scarcity), outperforming reported LSTM+GP/LSTM+MPACE baselines and often matching or improving on Sparse MFMLP at higher scarcity. On mean N-CMAPSS without simulated scarcity, AER-f achieves RMSE$_{i,t}$=6.49±0.04 versus kNNI+LSS 7.43 and MLP-f 6.84±0.22. Ablations show PR materially improves rectified predictions compared to raw posterior estimates, and adding reconstruction loss (AER-f vs MLP-f) improves accuracy. Using the proposed non-linear labeling can substantially reduce error: on FD002 with aligned train+test scarcity, AER-f achieves 18.60±1.60 (50%), 19.23±1.91 (70%), compared with a linear-label variant AER† at ~27.8–28.7 RMSE.","The authors note that interpolation for scarce/irregular sampling can introduce significant bias, motivating their avoidance of interpolation-based temporal regression. They also highlight that prior work on scarcity largely simulated scarcity only during training and did not investigate uncertainty from the randomness of scarcity, which they address by repeating experiments with different random seeds. No explicit additional limitations (e.g., assumptions or deployment constraints) are stated beyond these problem-motivation caveats.","The method depends on choosing an appropriate parametric labeling/trajectory family (linear or Weibull-form); mis-specification could degrade PR and bias rectified RUL, especially across different failure modes/operating regimes. PR requires solving a per-unit nonlinear least squares problem (Levenberg–Marquardt), which may be sensitive to initialization/outliers and could be computationally heavy at scale without careful implementation. Evaluation is limited to turbofan simulation benchmarks (CMAPSS/N-CMAPSS) with simulated sparsity; generalization to real industrial sensor noise, covariate shift, censoring, and non-run-to-failure test cases is not demonstrated. The “Simulated only” aspect of scarcity (random subsampling) may not reflect real missingness mechanisms (MNAR), which can impact performance differently.",None stated.,"Extend PR to handle unknown/learned trajectory families (e.g., mixture models or monotone splines) and quantify uncertainty on $\hat\theta$ and rectified RUL (prediction intervals) under indirect supervision. Study robustness to autocorrelation, missing-not-at-random sampling, and distribution shift across operating conditions, and validate on real-world run-to-failure or censored field data. Develop self-starting/online variants that update $\hat\theta$ incrementally without repeated nonlinear optimization, and release reference implementations for reproducibility and adoption.",2504.09206v1,https://arxiv.org/pdf/2504.09206v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:44:41Z
FALSE,Other,Nonparametric/Semi-parametric|ML-based|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Network/cybersecurity,Other,FALSE,None / Not applicable,Not applicable (No code used),https://arxiv.org/abs/2502.02561|https://arxiv.org/abs/2503.10345,"The paper is a survey/review advocating “conformal calibration” as a lifecycle framework to provide formal, user-specified reliability guarantees for black-box AI applications in wireless networks. It covers (i) pre-deployment calibration via conformal prediction to create prediction sets with finite-sample coverage under exchangeability, enabling reliable predict-then-optimize decisions; (ii) pre-deployment hyperparameter selection framed as multiple hypothesis testing (learn-then-test and adaptive LTT) to control the probability of violating KPI thresholds; (iii) deployment-time calibration via online conformal prediction to achieve adversarial, time-averaged KPI guarantees through threshold updates; and (iv) post-deployment counterfactual KPI analysis using reweighted conformal prediction to address selection bias. Example applications include power control for unlicensed spectrum access, downlink scheduling hyperparameter selection, downlink beam selection, and counterfactual evaluation of scheduling apps. Overall, the contribution is methodological and focused on statistical calibration/uncertainty quantification for trustworthy AI in wireless systems rather than classical reliability engineering of physical components.","Key conformal calibration targets include coverage guarantees for prediction sets $\Pr(y\in\Gamma)\ge 1-\beta$ and KPI reliability constraints such as $\Pr(R(\lambda)>\alpha)\le \beta$ (hyperparameter reliability) or $\mathbb{E}[R(x,y)]\le \alpha$ (statistical KPI constraint). For deployment-time calibration, online conformal prediction updates the confidence threshold as $\lambda_{t+1}=\lambda_t-\eta_t(R_t-\alpha)$ to enforce time-average deterministic guarantees $\frac{1}{T}\sum_{t=1}^T R_t\le \alpha+o(1)$. For robust decision-making with set prediction, the optimizer replaces an intractable average constraint with a worst-case constraint $\max_{y\in\Gamma} R(x,y)\le \gamma$.","The paper primarily reports qualitative/illustrative performance comparisons via figures rather than tabulating universal numerical gains. In the power-control example, a multi-sample (multi-modal) conformal score yields noticeably less conservative transmit power (and higher rate) than a conventional squared-loss score for the same underlying predictor. In scheduling hyperparameter selection, adaptive LTT (aLTT) achieves substantially lower energy-delay product than batch LTT for the same calibration budget, and attains tighter latency targets that LTT cannot reach in the shown setup (e.g., below roughly 7 ms in the plotted example). In downlink beam selection, both conventional and localized online conformal prediction meet the targeted long-term SNR degradation, while localized control achieves materially smaller average candidate beam set sizes, reducing pilot overhead. In counterfactual analysis, reweighted (counterfactual) conformal prediction produces prediction intervals that maintain valid coverage in settings where standard conformal prediction can fail due to selection bias.","The paper notes that conformal prediction guarantees validity (coverage) but not informativeness: if the underlying predictor is inaccurate, the resulting prediction sets can become excessively large to maintain coverage. It also emphasizes that pre-deployment guarantees rely on exchangeability between calibration and test data, and that distribution shift or inter-app conflicts at deployment time can violate nominal-condition assumptions, motivating online calibration. For counterfactual analysis, it highlights selection bias as a fundamental challenge and motivates reweighting to correct context-distribution mismatch.","As a review/positioning paper, it does not provide a comprehensive, standardized benchmarking protocol across many wireless tasks, shifts, and baselines; the empirical evidence is mainly illustrative, so generality of observed gains is uncertain. The practical implementation burden (e.g., calibration set size requirements, compute/latency overhead in near-real-time controllers, and sensitivity to the choice of nonconformity scores/e-value betting strategies) is not quantified end-to-end. The guarantees discussed typically ensure marginal coverage or long-run average constraints, but do not directly address rare catastrophic events, multi-KPI joint guarantees, or tightly-coupled closed-loop stability properties in control settings. Robustness to temporal dependence beyond exchangeability (for offline conformal) and to delayed/missing KPI feedback (beyond the cited intermittent-feedback extension) is not fully characterized in this paper.","The conclusion suggests improving calibration schemes by incorporating contextual information specific to wireless systems (e.g., traffic, mobility, connectivity conditions) rather than relying only on KPI measurements, pointing to context-dependent calibration methods. It also implies continued development of lifecycle-wide calibration tools spanning pre-deployment, online adaptation, and post-deployment diagnostics to make black-box AI more dependable for operators.","Extending the surveyed methods to provide rigorous multi-KPI and joint-coverage guarantees (e.g., simultaneous latency/reliability/throughput constraints) would better match operator needs. Developing self-starting/continual calibration methods that explicitly handle strong temporal dependence, delayed labels/KPIs, and non-stationary feedback schedules common in networks would strengthen deployment-time applicability. More work is also needed on computationally bounded implementations and standardized evaluation on open wireless benchmarks to quantify reliability-efficiency tradeoffs and compare against robust optimization and distributionally robust learning baselines. Finally, integrating conformal calibration with system-level safety/stability analysis (e.g., control-theoretic guarantees for predict-then-act loops) would help bridge statistical validity with operational reliability requirements.",2504.09310v3,https://arxiv.org/pdf/2504.09310v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:45:18Z
TRUE,System reliability|Other,"Bayesian|Simulation-based|Parametric (Weibull, etc.)|Other",Right-censored|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes a unified probabilistic framework that links construction-product conformity assessment (quality control) to structural reliability and semi-probabilistic design via partial safety factors. Conformity assessment is modeled as a probabilistic filtering mechanism using acceptance sampling/operating characteristic (OC) curves, and the resulting selection effect is incorporated through Bayesian updating of material and execution-parameter distributions. The framework connects reductions in coefficients of variation (from QC outcomes) to reductions in partial safety factors through an explicit improvement-factor formulation, and extends this to multi-parameter structural resistance models using homogeneity/importance factors. A masonry wall example demonstrates that targeting influential parameters (masonry unit strength and execution/imperfections) yields meaningful reductions in resistance variability and partial safety factors, whereas controlling weakly influential parameters (mortar in the case study) provides limited benefit. The reported case indicates partial safety factors can drop from a baseline 1.5 to about 1.38–1.40 under combined QC, implying roughly ~8% material savings while maintaining target reliability levels.","Conformity assessment is represented by an acceptance function derived from OC curves, $P_a(\mu,\sigma)=P(\text{batch accepted}\mid \mu,\sigma)$. Bayesian filtering updates the prior parameter distribution via $f_o(\mu,\sigma)=\dfrac{P_a(\mu,\sigma)f_i(\mu,\sigma)}{\iint P_a(\mu,\sigma)f_i(\mu,\sigma)\,d\mu\,d\sigma}$. The partial safety factor change is summarized by an improvement factor $r=\gamma_{in}/\gamma_{out}=\exp\left((\alpha\beta-k)(V_{in}-V_{out})\right)$ (and analogously at system level with $Q_R$), with resistance variance aggregation $Q_R^2=\sum_i n_i^2 Q_i^2$ for lognormal inputs.","In the masonry-wall example, successive QC stages reduce coefficients of variation for key parameters: masonry unit strength $V$ from 0.25→0.20→0.18, mortar strength 0.27→0.22→0.20, and execution/imperfection eccentricity 0.47→0.38→0.34. The aggregated resistance variability decreases from $Q_{R,in}=0.200$ to 0.165 (after 1st QC) and 0.151 (after 2nd QC), corresponding to $\Delta Q_R$ of 0.035 and 0.050. The implied improvement factors are about $r=1.05$ (1st QC) and $r=1.07$ (2nd QC), yielding improved partial safety factors around $\gamma_M=1.43$ and 1.40 (with a reported best case around 1.38 when using an upper 75% credibility bound scenario). The paper concludes that QC focused on masonry units and execution drives most of the safety-factor reduction, while mortar-only control gives negligible gains for this case.","The paper notes that most prior research (and thus available standards/OC-curve development) is concentrated on concrete; other materials like masonry, steel, and timber are comparatively underexplored. It also states that a comprehensive treatment of the variance of the reliability index (beyond focusing on the mean reliability index) would require a fundamental theoretical development that is outside the scope of the paper.","The application is demonstrated on a single masonry-wall example; the extent to which the same quantitative reductions in partial safety factors generalize across other structural typologies, limit states, or loading scenarios is not established. The framework relies on chosen priors (e.g., normal-gamma/lognormal-gamma) and assumed acceptance criteria/OC curves; sensitivity of conclusions to alternative prior elicitation, model bias assumptions, and conformity-rule choices is not fully explored. Although autocorrelation is included via a specific AR(2) model, robustness to other dependence structures and to non-stationarity in production processes is unclear. The paper mentions MCMC (Metropolis–Hastings) but does not provide implementation details, convergence diagnostics, or computational reproducibility artifacts.","The paper suggests enhancing building standards by introducing parameter-specific guidelines and explicitly providing/including OC curves, enabling more transparent and efficient evaluation of conformity assessment and QC practices. It also motivates expanding the approach beyond concrete-focused conformity frameworks to other construction materials (e.g., masonry) within standards-compatible semi-probabilistic design.","Develop self-starting/Phase-I-to-Phase-II workflows that handle unknown parameters and limited initial data while maintaining guaranteed in-control performance (e.g., credible-interval-based or robust priors). Extend the framework to multivariate/material-family settings with correlated properties and to system reliability with multiple interacting limit states, including uncertainty in model bias and load effects. Provide open-source reference implementations (with MCMC diagnostics and benchmark datasets) to support adoption by practitioners and standards bodies. Validate the approach on real QC/production datasets across multiple plants/projects to quantify practical gains and confirm the assumed autocorrelation and acceptance models.",2504.09508v1,https://arxiv.org/pdf/2504.09508v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:45:50Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Failure mode analysis|Other,"Stochastic process|Parametric (Weibull, etc.)|ML-based|Other",Degradation measurements|Sensor/condition monitoring|Other,Condition-based|Predictive,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python,Not provided,NA,"The paper proposes representing coating-system degradation from high-frequency corrosion/environment sensor streams as sparse discrete “degradation events,” enabling more efficient modeling than using continuous sensor readings. Coating failure is defined in a data-driven way using window-limited CUSUM change-point detection on a diurnal-cycle exponential-tail parameter derived from galvanic corrosion current, to better align failure labels across different coating stackups. Three event definitions are developed (corrosion peaks, hazardous-environment episodes, and a hybrid of both) and tuned to minimize the coefficient of variation of event counts at failure while retaining strong correlation with time-to-failure. Future event occurrences (and their magnitudes via marks) are modeled using a marked Hawkes process with a periodic KDE background intensity and exponential self-excitation, fit by numerical MLE and used to simulate future event trajectories. Predicted event accumulation is converted into a high-probability failure-time window using quantiles of the event-count-at-failure distribution, and is shown to produce tighter/more useful failure windows than forecasting corrosion current directly with linear VAR baselines in both lab and outdoor datasets.","Failure labeling uses WLCUSUM on the exponential-tail time constant $\tau$ from fitting $y(t)=A\exp(-(t-t_0)/\tau)+B$ to diurnal cycles of corrosion current, with recursive update $WL_i=\max(WL_{i-1},0)+\bar\tau_{i-w:i}(\tau_i-\bar\tau_{i-w:i}/2)$. Degradation events are modeled by a marked Hawkes process with intensity $\lambda_g(t\mid\mathcal H_t)=\alpha\mu(t)+\omega\sum_{t_i<t} m_i\,\beta\exp(-\beta(t-t_i))$, and marks modeled as $m\sim\mathcal N(\bar m(t),\sigma_m^2(t))$ based on past marks. Parameters $\Theta=\{\alpha,\omega,\beta\}$ are estimated by maximizing the point-process log-likelihood $\ell(\Theta)=\sum_{i=1}^n\log\lambda_g(t_i)-\int_0^T\lambda_g(t)dt$ (integral approximated numerically). Failure-time windows are obtained by simulating trajectories and mapping to times when predicted cumulative events reach the 0.25 and 0.75 quantiles of the (empirical or Gaussian) event-count-at-failure distribution.","In calibration data, corrosion events required far fewer measurements than charge (mean events at failure 89 vs mean charge-measurements count 9700) while achieving similar variability at failure (CV 0.40 for both) and comparable association with time-to-failure (corrosion events Pearson −0.46 / Spearman −0.50 vs charge Pearson −0.54 / Spearman −0.62; all p<0.01). In laboratory failure-window prediction, Hawkes models yielded narrower windows than continuous-signal baselines: mean widths 1.57–2.61 weeks (environment/corrosion/hybrid) versus ~4.0–4.25 weeks for OLS/Ridge/LASSO, with mean errors around ~1.1–1.3 weeks overall; on test sensors, hybrid Hawkes achieved mean error 0.75 weeks with mean width 2.75 weeks. Estimated Hawkes parameters (lab) were: corrosion $(\hat\alpha,\hat\omega,\hat\beta)=(2.19,0.72,8.98\times10^{-3}\,\mathrm{hr}^{-1})$ and hybrid $(0.39,0.86,7.33\times10^{-3})$; outdoor corrosion $(0.59,0.82,2.24\times10^{-3})$ and hybrid $(0.32,1.11,6.41\times10^{-3})$. Outdoor prediction was harder for all methods; Hawkes models reduced test error versus linear baselines (e.g., corrosion Hawkes mean error 18.8 weeks vs OLS 35.1 weeks, with ~10-week mean window widths).","The authors state that the main limitations come from the small-data regime: laboratory and outdoor testing is expensive and long, so they only observe degradation in about 30 coated panels, limiting validation and evaluation. They also note that comparing different degradation-forecasting frameworks is not straightforward, so they compare approaches indirectly via a downstream task (failure prediction) rather than one-to-one forecast accuracy of degradation itself.","Event definitions and thresholds are tuned to minimize CV of event counts at failure on a calibration split, which risks overfitting and may not transfer to new environments, sensor hardware, or coatings without re-tuning. The Hawkes model uses a specific exponential kernel and a mark model based on empirical mean/variance of past marks; this may be sensitive to nonstationarity, unmodeled covariates (temperature/RH dynamics), and autocorrelation in sensors beyond self-excitation structure. Failure windows rely on quantiles of event counts at failure (and, outdoors, a Gaussian assumption anchored to a single training sensor with CV borrowed from lab data), which may produce poorly calibrated uncertainty when failure mechanisms differ or when censoring/late failures occur. No public code or standardized benchmark comparison (e.g., alternative point processes, degradation-path models, Bayesian approaches) is provided, limiting reproducibility and breadth of performance claims.",None stated.,"Validate robustness on larger and more diverse datasets (different climates, coating stackups, defect geometries) and study transfer learning/domain adaptation for event thresholds and Hawkes parameters between lab and field. Extend the point-process model to incorporate exogenous covariates (RH, conductance, temperature) directly in the intensity (e.g., Hawkes with covariates or state-space intensity) rather than only via event extraction, and quantify uncertainty calibration of predicted failure windows. Develop self-starting/online estimation for deployment (updating parameters as new events arrive) and incorporate censoring for units not yet failed. Provide open-source implementation and compare against broader degradation/RUL baselines (e.g., gamma/Wiener degradation processes, Cox/PH survival with time-varying covariates, Bayesian hierarchical models across sensors/panels).",2504.09706v1,https://arxiv.org/pdf/2504.09706v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:46:31Z
TRUE,RUL prediction|Maintenance optimization|Other,Nonparametric/Semi-parametric|ML-based|Other,Right-censored|Sensor/condition monitoring|Mixture of types,Predictive|Condition-based,Transportation/logistics|Manufacturing (general)|Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/mcavs/survival_predictive_multiplicity_paper,"The paper introduces predictive multiplicity for survival analysis in predictive maintenance, addressing the problem that many near-optimal survival models can disagree substantially on individual equipment risk estimates. It defines a Rashomon set of survival models (near the best model under a performance metric) and proposes three metrics—ambiguity, discrepancy, and obscurity—to quantify disagreement in predicted cumulative risk at an observation-specific time, relative to a reference model. The approach is demonstrated on NASA’s CMAPSS engine degradation datasets using Random Survival Forests trained over 22,500 hyperparameter configurations, with Rashomon sets defined by integrated Brier score tolerances (ε). Empirically, ambiguity can rise to roughly 40–45% of observations (and in some settings approach 1), indicating many instances where equally good models conflict on failure-risk estimates; discrepancy shows similar but generally smaller patterns, while obscurity tends to be milder and concentrated in fewer models. The results highlight that accurate survival models may still produce conflicting individual-level failure-risk trajectories, implying the need to measure and communicate such uncertainty for reliable maintenance scheduling and decision-making.","The survival risk model output is defined as the conditional CDF $f(x_i,t)=\Pr(T\le t\mid x_i)$, with survival $S(t\mid x_i)=1-f(x_i,t)$ and $S(t\mid x_i)=\exp(-H(t\mid x_i))$ implying $f(x_i,t)=1-\exp(-H(t\mid x_i))$. The Rashomon set is $\mathcal{H}_\varepsilon(f_R)=\{f\in\mathcal{H}:\Phi(f)\le \Phi(f_R)+\varepsilon\}$ for metric $\Phi$ (e.g., integrated Brier score). Multiplicity metrics compare $|f(x_i,t_i)-f_R(x_i,t_i)|$ to a conflict threshold $\delta$: ambiguity $A_{\delta,\varepsilon}$ is the fraction of observations for which at least one Rashomon model conflicts (Eq. 8), discrepancy $D_{\delta,\varepsilon}$ is the maximum conflict fraction over models (Eq. 9), and obscurity $O_{\delta,\varepsilon}$ is the average conflict fraction across models per observation (Eq. 10).","On CMAPSS subsets (FD001–FD004), the multiplicity metrics increase as the Rashomon tolerance $\varepsilon$ increases and as the conflict threshold $\delta$ decreases, indicating more disagreement among larger sets of near-optimal models. The abstract reports ambiguity steadily increasing and reaching about 40–45% of observations; discrepancy is lower but follows a similar trend; obscurity remains mild and concentrated in a few models. In tabulated results, for some settings (e.g., $\varepsilon=0.01$ on FD003 at small $\delta$), ambiguity and discrepancy can reach 1, indicating complete disagreement across observations despite near-identical performance. Rashomon set sizes grow quickly with $\varepsilon$ (e.g., up to 11,328 models for FD002 at $\varepsilon=0.10$), corresponding to high ambiguity/discrepancy (often near 1 for small $\delta$) and obscurity values that can exceed 0.9 at $\delta=0.01$ in FD004.","The authors note that the study focuses only on Random Survival Forests and on the CMAPSS predictive maintenance datasets, so generalization to other survival model classes (e.g., deep survival networks) and domains (e.g., healthcare, finance) remains to be tested. They also state that the multiplicity metrics depend on the chosen performance metric (e.g., integrated Brier score) and conflict threshold $\delta$, and that alternative choices could change Rashomon sets and multiplicity profiles. Finally, they note the current metrics operate on survival risk estimates, although extensions to other survival outputs (e.g., time-to-event predictions) are possible.","The experimental evidence is based on a single benchmark family (CMAPSS) and a fixed censoring time choice ($t=250$), which may materially affect the degree of multiplicity and may not reflect operational censoring mechanisms. The study quantifies disagreement relative to a single reference model at $(x_i,t_i)$; multiplicity could differ if assessed over full risk trajectories $f(x_i,t)$, over multiple decision-relevant horizons, or under alternative reference choices. Comparisons are largely internal (within a hyperparameter grid for one algorithm) rather than across diverse survival learners (e.g., Cox/DeepSurv/GBM survival), which limits insight into whether multiplicity is algorithm-specific or data-driven.","The paper suggests extending the framework to other survival model classes (e.g., deep survival networks) and to other domains such as healthcare and finance. It also proposes exploring strategies to mitigate or manage predictive multiplicity, including ensembles, robust selection within the Rashomon set, and decision-making frameworks that explicitly incorporate ambiguity/discrepancy/obscurity. The authors further mention integrating multiplicity-aware tools into cost-sensitive and risk-averse maintenance planning pipelines, and exploring how uncertainty quantification (including multiplicity) could support active learning.","Develop self-starting or adaptive procedures to choose $\varepsilon$ and $\delta$ from business constraints (e.g., maintenance cost/loss) and to calibrate what constitutes a decision-relevant conflict in risk. Extend multiplicity assessment to time-dependent functionals (e.g., expected RUL, optimal replacement time, or survival quantiles) to directly quantify decision instability, not just risk disagreement at $t_i$. Add robustness studies under autocorrelated sensor streams and covariate shift (common in fleet maintenance) to understand how multiplicity behaves under realistic deployment drift and whether multiplicity-aware selection improves out-of-sample decision performance.",2504.12156v1,https://arxiv.org/pdf/2504.12156v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:47:08Z
TRUE,RUL prediction|Maintenance optimization|Other,ML-based|Nonparametric/Semi-parametric|Other,Right-censored|Sensor/condition monitoring|Mixture of types,Predictive,Transportation/logistics|Energy/utilities|Manufacturing (general)|Other,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/mcavs/RSF-tunability-paper,"The paper studies how sensitive (""tunable"") Random Survival Forests (RSF) are to hyperparameter choices in predictive maintenance, where accurate time-to-failure estimates are needed and censoring is present. It proposes a three-level tunability framework: (1) dataset/model-level tunability as performance gain of the best tuned configuration over defaults, (2) hyperparameter-level tunability by tuning one parameter at a time while keeping others at defaults, and (3) identification of value ranges where gains concentrate. The approach is evaluated using survival-performance metrics (C-index for discrimination and Brier score for calibration) on four NASA CMAPSS engine run-to-failure subsets with varying operating conditions and failure modes. An exhaustive grid over six RSF hyperparameters (330,000 configurations per dataset) with 5-fold cross-validation shows tuning consistently improves performance: average C-index gain 0.0547 and average Brier score reduction 0.0199. The results highlight which hyperparameters matter most (ntree/mtry for C-index; nodesize/mtry for Brier) and provide practical guidance on tuning priorities and value ranges for predictive maintenance survival modeling.","Model-level tunability is defined as the risk improvement from default to best tuned configuration on dataset $j$: $d^{(j)} = R^{(j)}(\theta^*) - R^{(j)}(\theta^{(j)*})$, where $R^{(j)}(\theta)$ is a survival risk such as Brier score or $1-\text{C-index}$. Hyperparameter-level tunability for parameter $i$ tunes only that parameter: $\theta_i^{(j)*}=\arg\min_{\theta\in\Theta,\,\theta_l=\theta_l^*\,\forall l\neq i} R^{(j)}(\theta)$ and $d_i^{(j)} = R^{(j)}(\theta^*) - R^{(j)}(\theta_i^{(j)*})$, with relative contribution $d_{\text{rel},i}^{(j)}=d_i^{(j)}/d^{(j)}$.","Across four CMAPSS subsets, tuning improved both metrics versus defaults: C-index tunability (gain) was 0.0492 (FD001), 0.0636 (FD002), 0.0528 (FD003), 0.0534 (FD004), averaging 0.0547 (~7.5% relative improvement). Brier score tunability (reduction) was 0.0197 (FD001), 0.0157 (FD002), 0.0222 (FD003), 0.0221 (FD004), averaging 0.0199 (~14.6% relative reduction). Hyperparameter importance differed by metric: ntree and mtry had the highest average tunability for C-index; nodesize and mtry yielded the greatest average Brier score improvement, while splitrule had negative/near-zero average tunability with high variability (can hurt performance if tuned poorly). Tunability-range analysis indicated localized beneficial regions for nodesize (e.g., roughly 10–30 for C-index as summarized in conclusions; and about 35–95 for Brier score, especially in FD003/FD004), whereas ntree and mtry effects were more dataset-specific and scattered.","The authors note the study is restricted to the CMAPSS dataset, so generalizability to other domains/industries and censoring patterns is uncertain. They also state the hyperparameter search space is a predefined fixed grid that may miss better settings outside the chosen ranges or discoverable by methods like Bayesian optimization or evolutionary algorithms. Finally, they evaluated performance using only C-index and Brier score, without assessing interpretability, computational cost, or temporal performance degradation.","The exhaustive grid search (330k configurations per dataset with 5-fold CV) is computationally heavy; results may be hard to reproduce in typical industrial settings without substantial compute, and wall-clock/runtime is not reported. The work focuses on predictive performance metrics but does not assess maintenance decision impact (e.g., cost-sensitive utility, downtime, false maintenance actions) or translate improvements into operational KPIs. It is unclear how censoring is constructed/handled for CMAPSS (often derived from run-to-failure trajectories and train/test splits), and sensitivity to censoring levels or time-dependent covariate handling is not thoroughly analyzed.","They propose evaluating RSF tunability on a broader range of datasets from different domains, with varying censoring and sparsity. They suggest integrating automated hyperparameter optimization (e.g., Bayesian optimization), evolutionary methods, or meta-learning to enable more efficient/adaptive tuning. They also propose extending tunability beyond accuracy to predictive multiplicity, explainability, and robustness.","Develop a compute-aware tuning strategy (e.g., successive halving/Hyperband or Bayesian optimization with early stopping) and report trade-offs between performance gain and computational budget for practical deployment. Evaluate decision-oriented maintenance metrics (e.g., expected cost, lead time to failure, maintenance precision/recall under alarms) to connect survival metric gains to maintenance outcomes. Extend the tunability analysis to time-dependent covariates and temporal cross-validation schemes to better reflect streaming condition-monitoring settings and avoid leakage.",2504.14744v1,https://arxiv.org/pdf/2504.14744v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:47:43Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,"Stochastic process|Bayesian|Nonparametric/Semi-parametric|Parametric (Weibull, etc.)",Degradation measurements|Sensor/condition monitoring,Condition-based|Predictive,Transportation/logistics|Manufacturing (general)|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,NA,"The paper proposes a Bayesian semi-parametric mixed-effects degradation model for predicting residual lifetime (remaining useful life) when degradation trajectories arise from heterogeneous sub-populations. Degradation is modeled with a linear general path model $Y_{ij}=\alpha+\beta_i t_{ij}+\epsilon_{ij}$, while heterogeneity in unit-specific degradation rates $\beta_i$ is captured using a Dirichlet process mixture (DPM) of normals via a truncated stick-breaking representation. Posterior sampling is performed using a Gibbs sampler (with Metropolis–Hastings steps for half-Cauchy variance priors in some scenarios), and the residual lifetime distribution is computed by averaging threshold-crossing probabilities over posterior draws, yielding an approximation without closed form. Samples from the derived residual lifetime distribution are generated using transformation-based MCMC (TMCMC), and the method is compared against a Bayesian parametric random-effects normal model. Performance is assessed via simulation under various mixture scenarios and by analysis of a fatigue crack growth dataset (21 units), showing similar overall accuracy to the parametric model but improved behavior for slowly degrading units in heterogeneous settings.","Degradation (linear general path) model: $Y_{ij}=\alpha+\beta_i t_{ij}+\epsilon_{ij}$ with $\epsilon_{ij}\sim N(0,\sigma_\epsilon^2)$. Random effects distribution is modeled as a truncated Dirichlet process mixture of normals, i.e., $\beta_i\mid K_i=h \sim N(\mu_h,\sigma_h^2)$ with cluster indicator $K_i\sim\sum_{h=1}^N p_h\,\delta_h$ and stick-breaking weights $p_h$ from $V_h\sim\text{Beta}(1,\gamma)$. Residual lifetime at current time $t_k$ is defined as $T_{new}=\inf\{s>0: Y_{new,t_k+s}\ge D\}$ and approximated by averaging $\Phi\big((\alpha^{(i)}+\beta_{new}^{(i)}(t+t_k)-D)/\sigma_{\epsilon}^{(i)}\big)$ over posterior draws, with an optional truncation to enforce $T_{new}>0$.","Across multiple simulated heterogeneity cases (mixtures of Gamma/Weibull/Normal for $\beta_i$), both parametric and semi-parametric approaches show decreasing RMSE/MAE as the number of observations per unit increases; the semi-parametric model tends to be more accurate for slowly degrading units, while the parametric model can be better for the fastest degrading units in some scenarios. For example (Case 1, $n=10,m=31$), reported RMSE/MAE are about 0.527/0.690 (parametric, M1) vs 0.520/0.681 (semi-parametric, M1). In the fatigue crack dataset (21 units; threshold $D=1.6$ inches), both methods yield very similar prediction errors (e.g., RMSE roughly 0.045–0.047 across priors), and predictive intervals often include the true residual lifetime. The authors also report Kolmogorov–Smirnov distances between true and approximated residual lifetime distributions in simulation to assess approximation quality (values vary notably by unit and scenario).","The authors note that the residual lifetime distribution derived from posterior averaging does not have a closed-form expression, requiring approximation and MCMC-based simulation for prediction. They also report that predicted median residual lifetimes are often higher than true values because the averaged distribution functions induce a longer upper tail. In skewed-mixture simulation settings (Gamma/Weibull mixtures), they observe that both unimodal normal and normal-mixture random-effects assumptions may fail to model tails accurately, affecting predictive intervals and KS distances.","The degradation model used for main development is a linear path with i.i.d. Gaussian measurement errors, which may be inadequate for many degradation mechanisms (nonlinear growth, autocorrelated errors, non-Gaussian noise, or state-dependent variance). The failure definition is based on the observed degradation crossing a fixed threshold and assumes no post-crossing return below threshold, which may be violated with noisy measurements or intermittent signals. The approximation of the residual lifetime distribution by posterior averaging of one-step threshold probabilities may not fully reflect first-passage-time dynamics for continuous-time degradation with noise. Code is not shared, and practical implementation details (diagnostics, tuning, runtime sensitivity to truncation level $N$, and robustness to prior choices beyond the tested scenarios) are not fully benchmarked for real-world deployments.","The authors suggest the proposed Bayesian semi-parametric degradation model can be applied to similar prediction problems beyond engineering, such as medical diagnosis contexts. They also emphasize the general framework (general path model plus flexible random-effects distribution) as a basis for further work in heterogeneous degradation and residual life distribution characterization.","Extend the approach to nonlinear/general degradation paths (e.g., exponential, power-law, or physics-informed crack growth models) and to stochastic-process first-passage-time formulations (Wiener/Gamma/IG) with heterogeneity modeled via Bayesian nonparametrics. Develop robust/self-starting variants handling autocorrelation, irregular sampling, missingness, and outliers, and incorporate time-varying thresholds or covariates (usage/environment) to explain heterogeneity. Provide open-source R/Python implementation and conduct broader empirical benchmarks against modern RUL methods (e.g., variational Bayes, particle filters, deep sequence models) with standardized datasets and computational cost reporting.",2504.15794v2,https://arxiv.org/pdf/2504.15794v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:48:21Z
FALSE,NA,"Nonparametric/Semi-parametric|ML-based|Parametric (Weibull, etc.)|Other",Complete lifetime data|Right-censored|Mixture of types|Other,Not applicable,Environmental monitoring|Finance/economics|Other,Simulation study|Case study (real dataset),TRUE,R,Public repository (GitHub/GitLab),https://github.com/opasche/ExtremeConformal|https://github.com/opasche/ExtremeCI|https://github.com/opasche/Reprod_ExtremeConformalPred,"The paper proposes “Extreme Conformal Prediction,” a conformal prediction framework designed for extremely high confidence levels where classical split conformal methods become degenerate (infinite-width intervals) because empirical calibration quantiles are ill-defined when α < 1/(n_c+1). It combines conformal prediction with extreme value theory by modeling the tail of calibration nonconformity scores using a peaks-over-threshold generalized Pareto distribution (GPD), enabling extrapolation of the needed calibration quantile beyond the observed score range. To mitigate underestimation risk from finite-sample bias and GPD approximation error, the method uses the upper endpoint of a confidence interval for an extreme score quantile (profile likelihood recommended; bootstrap and delta-method variants also studied), yielding conservative marginal coverage guarantees under suitable conditions. A weighted likelihood extension is introduced to handle nonexchangeable/nonstationary settings (e.g., seasonality), and the approach is demonstrated via simulations (heavy- and light-tailed noise) and an application to Swiss flood risk forecasting, where it yields finite, informative intervals with improved high-level coverage versus classical conformalization and base quantile models. The method is model-agnostic in the sense that it can wrap any black-box extreme quantile regression method as the base predictor.","Classical split conformal uses the score quantile $\hat q_\alpha$ to form $\hat C(x)=\{y:s(x,y)\le \hat q_\alpha\}$; for single-sided high-risk events it uses $s(x,y)=y-\hat Q_{1-\alpha}(x)$ giving $\hat C(x)=(y_{\min},\hat Q_{1-\alpha}(x)+\hat q_\alpha)$. For extreme levels, the paper replaces the empirical $\hat q_\alpha$ with an extrapolated estimate from a peaks-over-threshold GPD tail model: $P(S>y)\approx P(S>u)\left(1+\xi\frac{y-u}{\sigma(u)}\right)_+^{-1/\xi}$ and the extrapolated quantile $\hat Q^{\mathrm{GPD}}_{\tilde\tau}=\hat Q_{\tau_0}+\frac{\hat\sigma}{\hat\xi}\left(\left(\frac{1-\tau_0}{1-\tilde\tau}\right)^{\hat\xi}-1\right)$. The recommended conformal correction $\hat q^e_\alpha$ is taken as the upper endpoint of a GPD-based CI for an extreme score quantile, leading to the extreme conformal interval $\hat C_e(x)=(y_{\min},\hat Q^e_{1-\alpha}(x)+\hat q^e_\alpha)$ (and an analogous two-sided version).","In simulations with calibration sizes $n_c\in\{10^3,10^{3.5},10^4\}$ and extreme levels $\alpha\in\{10^{-3},10^{-3.5},10^{-4},10^{-4.5},10^{-5}\}$, classical conformalization yields infinite (trivial) intervals whenever $\alpha<1/(n_c+1)$, while the GPD-based methods produce finite intervals. The simple plug-in GPD quantile can under-cover at the smallest $\alpha$ and small $n_c$, whereas CI-based variants (profile likelihood, bootstrap, delta) achieve much better (often conservative) coverage; the profile-likelihood CI is the most conservative and most consistently meets the target but can have numerical issues in the most extreme/small-sample settings. In the flood forecasting case study, classical conformalization becomes infinite at the highest confidence levels, while the proposed “GPD safeprofile” weighted method yields finite intervals and empirically meets or exceeds the target coverage for confidence levels above 0.95 across multiple base extreme-quantile models. The authors recommend a “safeprofile” strategy: use profile likelihood when stable, otherwise fall back to bootstrap to avoid failures while retaining conservativeness.","The authors note that the method can be overconservative in some scenarios (e.g., moderately extreme confidence levels and lighter-tailed distributions), partly due to the conservative CI-based correction used to guard against estimation and approximation bias. They also emphasize that peaks-over-threshold/GPD extrapolation relies on asymptotic arguments, so the approach does not retain the finite-sample distribution-free guarantees of classical conformal prediction. They add that finite-sample bounds for quantiles extrapolated beyond the data range are not possible without additional assumptions on the data distribution.","The approach introduces nontrivial sensitivity to the tail-modeling choices (threshold $u$/exceedance fraction $k$, GPD adequacy, and CI construction), and practical guidance for automated, robust threshold selection is limited relative to the importance of this tuning at extreme levels. The weighted extension relies on a specified weighting scheme (e.g., seasonal sinusoidal weights) and a weighted likelihood fit; misspecified weights or complex forms of nonexchangeability (e.g., abrupt regime shifts, covariate-conditional drifts) may degrade validity without clear diagnostics. Coverage evaluation focuses on marginal coverage; for risk regulation, conditional or group-conditional performance across covariate strata (especially extremes of covariates) may be more relevant and is not deeply assessed. Computational and numerical stability of profile-likelihood CI estimation (especially at very small $\alpha$ with few exceedances) remains a practical bottleneck, and the paper does not fully quantify runtime/robustness trade-offs across methods beyond reporting failure rates in some scenarios.","They suggest exploring alternative conformal procedures that use data more efficiently, such as full conformal and resampling-based methods (jackknife, infinitesimal jackknife, cross-validation/cross-conformal), while noting the computational cost and that the efficiency gains for the extreme conformal approach remain an open question. They also propose further extensions to nonexchangeable data beyond the weighted approach, particularly for challenging settings like long-term dependence or drifts in the conditional distribution $Y|X$. Finally, they mention investigating non-conformal prediction-interval methodologies (e.g., “high-quality criterion” approaches with deep learning) and how to incorporate extreme value statistics for extrapolating intervals to high-impact events.","Develop principled, data-driven threshold/exceedance selection (and uncertainty propagation) tailored to conformal score tails, possibly via stability criteria or Bayesian/penalized EVT fitting, to reduce tuning sensitivity at extreme levels. Extend the method to provide stronger forms of validity (e.g., approximate conditional coverage, subgroup-conditional guarantees, or PAC-style bounds under structured tail assumptions) that align with regulatory needs for high-impact risk. Provide robust implementations that handle numerical pathologies in profile likelihood (e.g., regularization, reparameterizations, constrained optimization, or hybrid CI rules) with documented fail-safe behavior. Broaden empirical validation to additional domains (finance stress testing, reliability/safety critical systems, cyber risk) and compare against alternative extreme-quantile uncertainty approaches (e.g., Bayesian EVT, quantile regression with tail constraints, extreme quantile ensembling) using common benchmarks and real incident datasets.",2505.08578v2,https://arxiv.org/pdf/2505.08578v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:49:06Z
FALSE,NA,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|Other",Mixture of types|Right-censored|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset),TRUE,R,Public repository (GitHub/GitLab),https://github.com/jianhuig/Infairness|https://github.com/jianhuig/Infairness-replication,"This paper proposes Infairness, a semi-supervised inference framework for auditing group fairness of binary-outcome ML classifiers when labeled validation data are scarce but unlabeled data are abundant. The method imputes missing outcomes in the unlabeled set via a group-specific working regression model for E(Y|S,W,A=a) using carefully chosen nonlinear basis expansions (including an intercept, S and D to guarantee consistency for target metrics) and then estimates group performance metrics (e.g., TPR, FPR, PPV, NPV, F1, ACC, Brier score) and their between-group disparities. The authors derive influence-function-based asymptotic normality for disparity estimators and show that the estimator is consistent even if the ML model and/or imputation model are misspecified, while providing a variance reduction guarantee when the imputation model is correctly specified. Extensive simulations compare Infairness to supervised estimation and to Ji et al. (2020), showing improved precision and robustness; in a real EHR phenotyping audit for depression (MIMIC-III), Infairness reduces estimator variance by up to 64% relative to supervised auditing. The work advances fairness-auditing methodology by formalizing semi-supervised efficiency gains for a broad family of group fairness metrics with practical inference procedures.","Infairness estimates group-conditional functionals µ_a^Z=E(Z|A=a) for Z∈{Y,SY,DY} via the identity µ_a^Z=E\{\phi(Z)E(Y|S,W,A=a)\mid A=a\}, where \phi(Z)=1 (Z=Y), S (Z=SY), or D (Z=DY). The imputation model is a working regression E(Y|S,W,A=a)=g(\theta_a^\top B_a(S,W)) (e.g., g is expit), with \hat\theta_a solving the penalized estimating equation \frac{1}{n_a}\sum_{i=1}^n B_{a,i}\{Y_i-g(\theta_a^\top B_{a,i})\}I(A_i=a)-\lambda_{n_a}\theta_a=0. Disparities (e.g., \Delta\mathrm{TPR}=\mathrm{TPR}_0-\mathrm{TPR}_1) are computed by plugging semi-supervised estimates (imputed-Y averages over the unlabeled set, and direct unlabeled averages for Z∈{D,S^2}) into the standard metric formulas; asymptotic linearity is given through influence functions (Table 3) leading to Wald-type CIs.","In simulations with n=1,000 labeled and N=20,000 unlabeled observations (10,000 replications), Infairness is approximately unbiased across studied metrics, while the Ji et al. (2020) approach shows mild bias in one scenario and substantial bias in a non-logistic scenario. Averaged across two simulation scenarios, the relative efficiency (MSE(supervised)/MSE(Infairness with S+W)) is reported as 2.09 for ΔTPR, 1.81 for ΔFPR, and 1.20 for ΔPPV, with additional gains when incorporating auxiliary covariates W beyond using S alone. In the MIMIC-III depression phenotyping audit (n=416 labeled; N=26,925 unlabeled), Infairness reduces variance by up to 64% compared to supervised estimation (RE up to 2.75 across audited attributes/metrics), and identifies statistically significant disparities for race in ACC (0.090; 95% CI [0.005, 0.174]) and Brier score (-0.049; 95% CI [-0.097, -0.001]) that would be missed under supervised auditing at the 0.05 level. Empirical CI coverage in simulations is close to nominal 95% for both supervised and Infairness methods.","The paper notes that strict theoretical efficiency guarantees are provided only when the imputation model is correctly specified; under misspecification, efficiency gains are heuristic and depend on the imputation model being a close approximation to E(Y|S,W,A=a). It also states that the approach assumes labeled data are a simple random sample from the underlying pool (MCAR labeling), and may need adaptation for non-random/stratified labeling designs. The authors emphasize that their exposition focuses primarily on two-group protected attributes and that extending to multi-group settings is nontrivial and active research.","The method hinges on fitting separate imputation regressions by group; for small n_a within minority groups, the basis expansion and tuning (e.g., polynomial order selection) may be unstable and can degrade finite-sample performance despite asymptotic results. The framework largely assumes i.i.d. validation data and does not directly address temporal/clustered EHR dependence or distribution shift between labeled and unlabeled sets (violations of MCAR), which could bias imputations and disparity estimates. Comparisons do not include several modern semi-supervised or doubly robust fairness-auditing alternatives (e.g., cross-fitting / orthogonalization, flexible ML imputation with nuisance-robust inference), so practical competitiveness under complex nonlinearity is not fully established.","The authors propose extending Infairness beyond difference-based disparities to ratio-based metrics and to ε-fairness hypothesis testing. They highlight multi-group fairness auditing as an active area and state that developing semi-supervised approaches for multi-group settings is ongoing work. They also suggest extensions to individual fairness and causal fairness notions, consideration of incorporating training features X into the imputation step when feasible, and adapting the framework to non-random labeling schemes such as stratified sampling using advances in semi-supervised inference under selection bias.","A valuable extension would be a cross-fitted, nuisance-robust (doubly robust / orthogonal) version using flexible learners for E(Y|S,W,A) with formal finite-sample or high-dimensional guarantees and reduced overfitting risk. Another direction is robustness to covariate shift between labeled and unlabeled validation sets (e.g., importance weighting or domain adaptation) and explicit sensitivity analyses when MCAR labeling is doubtful. Providing a well-documented software vignette for practitioners on basis/tuning choices and diagnostic checks (e.g., calibration of imputations within groups) would improve adoption and reliability in applied audits.",2505.12181v1,https://arxiv.org/pdf/2505.12181v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:49:51Z
FALSE,Other,Other,Other,Not applicable,Finance/economics|Other,Other,TRUE,R|Python|Other,Public repository (GitHub/GitLab)|Personal website,https://fmegahed.github.io/research/llm_consistency/llm_consistency.html|https://github.com/fmegahed/llm_consistency,"The paper proposes a four-phase framework (planning, data collection, reliability analysis, validity analysis) to evaluate the consistency of large language models (LLMs) used as binary text classifiers/annotators. It adapts psychometric concepts of intra-rater and inter-rater reliability, includes sample-size determination for agreement estimation, and recommends multiple agreement coefficients (e.g., Krippendorff’s alpha, Fleiss’ kappa, Conger’s kappa, Gwet’s AC1, Brennan–Prediger) plus explicit handling of invalid LLM responses via NA-dropped and NA-penalized analyses. A case study labels sentiment of 1,350 StockNewsAPI financial news articles across 14 LLMs with five replicates each, reporting high within-model consistency (perfect agreement on ~88–98% of articles for most models) and generally high cross-model agreement that decreases as more models are included. Validity is assessed against StockNewsAPI sentiment labels (accuracy ~0.76–0.88, with several smaller models outperforming larger ones) and against next-day excess returns vs. the S&P 500, where performance is near chance, suggesting limits of sentiment-only prediction rather than LLM consistency limits. The authors provide open-source code and a reproducible implementation to support practical model selection and study design.","Sample size for agreement estimation is adapted from Gwet’s psychometric approach: $n_0 = z_{\alpha}^2\,C/E_0^2$, where $E_0$ is the target margin of error, $z_{\alpha}$ is the critical value for the chosen confidence level, and $C$ depends on the metric, number of raters/replicates $r$, and number of categories $q$ (binary: $q=2$). Separate sample sizes are computed for Percent Agreement, Gwet’s AC1, and Brennan–Prediger (Eq. 2), and a conservative choice is $n_{final}=\max(n_{PA},n_{AC1},n_{BP})$ (Eq. 3). Cohen’s kappa is defined as $\kappa=(P_o-P_e)/(1-P_e)$ for chance-corrected agreement (used as background; multi-rater generalizations are applied in experiments).","Using five replicates per model and 1,350 articles, most LLMs achieved perfect within-model agreement on roughly 88–98% of articles (NA-penalized), with mean agreement exceeding 0.93 for all models; deepseek-r1:1.5B was the main exception with 76.2% perfect agreement and reliability coefficients around 0.85, while deepseek-r1:7B improved to ~86% perfect agreement and ~0.93 coefficients. Against StockNewsAPI benchmark labels, mean accuracy ranged about 0.76–0.88, with smaller models (e.g., gemma3:1B at ~0.88; llama3.2:1B at ~0.86; claude-3-5-haiku at ~0.85) outperforming some larger counterparts. When evaluated against an external market-based criterion (next-day excess return vs. ^GSPC), all models performed near chance (precision/TPR/TNR around 0.50–0.54), and ensembling did not materially improve validity. Invalid label rate across all outputs was ~2.37% (1,921 of 81,000), with some models producing virtually none and others producing more invalid responses.","The authors note the framework and experiments focus on binary classification; extending to multiclass tasks would require adapting reliability metrics and recalibrating sample-size estimates. They also state generalizability is limited by the single-domain case study (financial sentiment), and that handling non-compliant/invalid LLM responses is still challenging (they used a lenient regex parser; alternatives include stricter penalization or structured outputs such as JSON). Finally, they acknowledge the current framework is text-only and suggest extending to non-text/multimodal tasks (e.g., vision transformers).","The validity evaluation relies on StockNewsAPI labels as a benchmark, which are produced by keyword heuristics; strong agreement with this benchmark may reflect learning the heuristic rather than true sentiment understanding. The use of temperature=0 improves repeatability and may inflate observed intra-rater reliability relative to typical deployment settings with nonzero temperature or stochastic decoding parameters. The NA-penalized/NA-dropped handling plus random tie-breaking can add variance and may affect reproducibility unless random seeds and tie-resolution rules are fully fixed and reported. Results may also depend on the single prompt and parsing logic; robustness to prompt variants and alternative extraction methods is not comprehensively stress-tested in the presented excerpt.","They propose extending the framework to multiclass classification, and to fuzzy/graded annotation where models output confidence scores rather than discrete labels. They also suggest broader cross-domain applications beyond finance to test whether reliability patterns (including model-size effects) hold in other tasks. Additionally, they recommend exploring stricter ways to handle non-compliant outputs (e.g., penalization or constraining formats like JSON) and extending the framework to non-text modalities such as image classification for Industry 4.0/5.0 contexts.","A useful extension would be a self-starting/online monitoring version of the framework to track LLM reliability drift over time as models, APIs, or prompting policies change. Another direction is to incorporate dependence structures (e.g., correlated items, clustered by ticker/news source) into agreement estimation and confidence intervals, which may otherwise be optimistic under i.i.d. assumptions. It would also be valuable to benchmark against strong non-LLM baselines (e.g., fine-tuned domain sentiment classifiers) under the same sampling/replication protocol to better contextualize both reliability and validity. Finally, providing packaged tooling (e.g., an R/Python package) and standardized reporting templates would improve practical adoption and comparability across studies.",2505.14918v2,https://arxiv.org/pdf/2505.14918v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:50:25Z
FALSE,NA,ML-based|Simulation-based|Other,Mixture of types|Sensor/condition monitoring|Other,Not applicable,Healthcare/medical|Finance/economics|Network/cybersecurity|Transportation/logistics|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/DataSlingers/IML_reliability|https://iml-reliability.herokuapp.com/home,"This paper studies the reliability of interpretable machine learning (IML) outputs by focusing on stability: whether global interpretations remain consistent under small random perturbations to data and algorithmic randomness. It conducts a large-scale empirical evaluation across tabular datasets for supervised tasks (classification/regression feature-importance rankings) and unsupervised tasks (clustering and dimensionality reduction interpretations). Perturbations include repeated random train/test splits (and subsampling for unsupervised tasks) and additive noise; stability is quantified using rank-consistency metrics (Jaccard, average overlap, top-K Kendall’s tau) and clustering/embedding consistency metrics (ARI, mutual information, V-measure, Fowlkes–Mallows, and a proposed NN-Jaccard-AUC for neighbor preservation). Main findings are that interpretations are frequently unstable—often less stable than model predictions—and that predictive accuracy is not associated with interpretation stability; no single IML method is consistently most stable across datasets. The authors provide an open-source dashboard and Python package to compute and visualize these stability/reliability assessments.","For feature-importance stability, the paper uses top-K set/rank similarity measures: Jaccard similarity $J(A,B)@k=|A_k\cap B_k|/|A_k\cup B_k|$, average overlap $AO@k=\frac{1}{k}\sum_{d=1}^k \frac{|A_d\cap B_d|}{d}$, and a top-K Kendall’s tau variant $K^{(p)}(A,B)@k$ based on pairwise inversions within $A_k\cup B_k$. For clustering stability it uses standard agreement measures such as Adjusted Rand Index (ARI) and mutual information, and for dimensionality reduction it introduces NN-Jaccard-AUC by computing Jaccard overlap of each point’s $K$-nearest-neighbor sets across perturbations over a range of $K$ values and taking the area under the resulting curve.","Across many benchmark tabular datasets and IML methods, within-method interpretation stability scores are often low, and between-method agreement on interpretations is also low for feature-importance tasks (many between-method AO values around 0.2–0.3 in classification). Prediction outputs are substantially more stable/consistent than interpretations (pairwise prediction agreement often >0.7, with some linear/ensemble pairs >0.9) even when interpretation agreement is much lower (e.g., SVM vs logistic ridge high prediction agreement but modest AO for feature rankings). The study finds no significant association between predictive accuracy and interpretation stability after multiple-testing correction in most settings, indicating accuracy is not a proxy for stable explanations. For unsupervised tasks, clustering interpretation stability is generally higher than supervised feature-importance stability, but still varies by dataset and method; no single method dominates across datasets.","The study is limited to tabular data and does not evaluate interpretation reliability for other modalities (images, text, time series), where interpretation methods and perturbations may differ. It restricts attention to quantitative global interpretations (or local methods aggregated to global) and excludes rule-based, textual, or purely visual explanation formats. The authors also note computational constraints: some methods (e.g., permutation and Shapley in high-dimensional/large datasets) were stopped when a single repeat exceeded 12 hours, leading to missing results for some method–dataset combinations.","Because stability is used as a proxy prerequisite for reliability, the results do not establish whether a stable interpretation is correct with respect to any ground-truth mechanism (stability can coincide with consistently wrong interpretations). The perturbation design (70/30 splits, chosen noise distributions/levels) and aggregation choices (e.g., summing local attributions across points) can materially affect stability estimates, but robustness of conclusions to alternative perturbation schemes and aggregation operators is not fully characterized. Comparisons may be influenced by hyperparameter tuning procedures and defaults (e.g., scikit-learn defaults, cross-validation choices), which can change both accuracy and stability; a sensitivity analysis over tuning strategies is not emphasized.","The authors propose extending the framework to other data modalities (images, text, time series) and to other explanation formats (rules, textual, visual). They suggest developing statistical inference methods to quantify uncertainty in interpretation metrics (e.g., confidence intervals or p-values) and connecting stability analyses with selective/post-selection inference and model-agnostic feature-attribution inference. They also recommend establishing a new standard practice of reporting interpretation reliability alongside predictive performance using perturbation-based stability validation.","A natural extension is to evaluate how stability metrics relate to downstream decision quality and safety in high-stakes pipelines (e.g., whether selecting models by stability improves human decisions or scientific discoveries). Another direction is to develop self-starting/online stability monitoring for deployed models (tracking drift in interpretation stability over time) and to incorporate dependence/autocorrelation structures for time-indexed tabular data. Finally, benchmarking could be strengthened by including controlled synthetic datasets with known ground-truth attributions/cluster structure to jointly assess stability and correctness, and by packaging reproducible experiment configurations (e.g., containers) to standardize comparisons across compute environments.",2505.15728v1,https://arxiv.org/pdf/2505.15728v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:51:15Z
TRUE,Other,Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"This paper proposes Tail Stratified Sampling (TSS), a Monte Carlo variance-reduction estimator for small failure probabilities in engineering reliability analysis by directly stratifying and sampling progressively deeper regions (“tails”) of the input parameter distribution where failures are expected to occur. The approach partitions the input space into strata defined by level sets of the joint input density (with an auxiliary construction for plateau/constant-density regions), assigns known stratum probabilities a priori, and estimates conditional failure probabilities within each stratum using only a failure indicator (so it can be used when the performance function is unavailable or stochastic). The authors derive a truncated (biased but controllably biased) estimator and an unbiased variant, provide closed-form expressions for estimator variance and bias bounds, and discuss sample allocation strategies (Neyman, proportional, equal). Extensive simulation studies across challenging 2D benchmark limit-state geometries, a 7D buckling example, and multiple 1000D examples compare TSS primarily against Subset Simulation, showing large coefficient-of-variation reductions in low dimensions and highlighting the need for an informative “safe region” (null stratum) to maintain efficiency in high dimensions. The work also contributes a formal, set-based definition of distribution tails intended to be rigorous and practically useful for reliability estimation.","Failure probability is formulated as $P_F=\int_{g(x)\le 0} f_X(x)\,dx = \mathbb{E}_f[\mathbf{1}_{\{g(X)\le 0\}}]$. TSS defines strata $A_1=S_{u_1v_1}$ and $A_i=S_{u_iv_i}\setminus S_{u_{i-1}v_{i-1}}$ with geometrically decreasing stratum probabilities $P(A_i)=p_0 P(A_{i-1})$. The (biased, truncated) TSS estimator is $\hat P_F^{\mathrm{TSS}}=(1-p_0)P(A^*)\sum_{i=1}^m p_0^{i-1}\,\hat P(F_i)$, with variance $\mathrm{Var}(\hat P_F^{\mathrm{TSS}})=[(1-p_0)P(A^*)]^2\sum_{i=1}^m p_0^{2(i-1)}\,\frac{P(F_i)(1-P(F_i))}{N_i}$ and bias upper bound $\le p_0^m P(A^*)$.","Across seven canonical 2D benchmark problems (true $P_F$ ranging from $\mathcal{O}(10^{-2})$ to $\mathcal{O}(10^{-9})$), using $p_0=0.1$ and typically $m=4$, TSS produced accurate estimates with low coefficients of variation (e.g., for $N=4000$, CoV values around 1–13% depending on the case) and bias became negligible by about $m=4$ strata. In direct comparisons, TSS outperformed Subset Simulation (SuS) by roughly 1–2 orders of magnitude lower CoV at comparable total model evaluations for these 2D problems, and SuS failed to converge on the “Black Swan” example while TSS remained stable. For a 7D stable-buckling example with true $P_F\approx 2.424\times 10^{-5}$, TSS achieved about a factor of ~2 lower CoV than SuS across sample sizes and showed less small-sample bias. In 1000D examples, TSS could be less efficient than SuS when the null/safe stratum is poorly defined (e.g., design-point hypersphere with very small probability mass), but a problem-informed null stratum (wire-cable example) restored strong performance with CoV dropping below ~10% for relatively small sample sizes.","The authors note that TSS can lose efficiency in high-dimensional problems because identifying a meaningful tail stratification (especially a high-probability “known safe” null stratum $A_0$) is challenging; if $A_0$ is chosen poorly, conditional failure probabilities within strata may be as small as $P_F$ and variance reduction becomes modest. They also acknowledge that TSS, as formulated, is exploratory within each stratum and does not inherently guide sampling toward $\Omega_F\cap A_i$, unlike Subset Simulation which uses performance-function information to concentrate samples. They further point out that defining tails via density thresholds may require problem-specific coordinate transformations in high dimensions, affecting practicality and performance.","The empirical performance comparisons focus heavily on Subset Simulation; results may be less informative relative to other strong baselines (e.g., modern adaptive importance sampling / cross-entropy methods, active-learning Kriging reliability methods) under matched computational budgets. Practical implementation requires sampling from conditional distributions over potentially complex strata; while the paper discusses MCMC/rejection/empirical stratification, it does not fully quantify the overhead of stratum construction and conditional sampling versus savings in model evaluations. The approach assumes reliable evaluation of the joint input density (and potentially its level sets), which may be difficult for black-box/implicit input models, nonparametric input uncertainty, or strongly dependent/highly structured distributions where density evaluation or conditioning is nontrivial.","The authors suggest improving TSS by developing general strategies (potentially data-driven, surrogate-based, or reduced-order) to identify informative high-probability null strata and meaningful tail definitions in high dimensions, including leveraging lower-dimensional structure/manifolds in the failure domain. They also propose coupling TSS with methods that better focus sampling near conditional failure regions within each stratum, such as importance sampling (including directional importance sampling and directional stratified sampling) and surrogate-driven schemes. They discuss importance-sampling-based sample allocation and indicate further development of adaptive allocation/strata discovery at the intersection of TSS and existing variance-reduction methods.","A self-starting/automatic procedure to learn $A_0$ and an effective coordinate transformation from small pilot runs (e.g., via score-based density modeling, normalizing flows, or sensitivity/active-subspace methods) could make TSS more plug-and-play in high dimensions. Formal robustness studies under model misspecification (e.g., dependent inputs, heavy tails, multimodality, or approximate density evaluation) and under non-i.i.d. sampling within strata (practical MCMC) would strengthen reliability guarantees. Providing open-source reference implementations and benchmarking on standardized reliability datasets (with equal wall-clock cost including sampling overhead) would improve reproducibility and clarify where TSS is most advantageous in practice.",2505.18510v1,https://arxiv.org/pdf/2505.18510v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:52:01Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/kclip/R_AutoEval_plus,"The paper proposes R-AutoEval+, an adaptive “reliable autoevaluation” framework for estimating/validating model performance when combining limited human-labeled data with abundant synthetic labels from an autoevaluator (e.g., LLM-as-a-judge). It formulates evaluation as a sequential hypothesis test on bounded risk $R=\mathbb{E}[\ell(X,Y)]$ and provides finite-sample reliability guarantees via e-values (testing-by-betting). The method adaptively selects how much to rely on synthetic data by maintaining multiple candidate mixing factors and updating their weights online (exponential weights), reverting toward a purely human-labeled evaluation when the autoevaluator is poor. Theoretical results bound sample complexity and show asymptotic (small-$\delta$) efficiency that is no worse than prior reliable Eval or reliable AutoEval baselines, and can be strictly better when the autoevaluator is accurate. Experiments on LLM quantization selection, prompt template selection, and reasoning budget allocation support reliability and improved sample efficiency, and code is released.","The target is risk control $R=\mathbb{E}[\ell(X,Y)]\le \alpha$ with reliability defined by a level-$\delta$ test requiring $\Pr(T_n=1\mid R>\alpha)\le \delta$. The core e-value is constructed sequentially as $E_n=\prod_{i=1}^n\big(1-\lambda_i(q_i-\alpha)\big)$ (or a convex-mixture variant), and the test rejects the null when $E_n\ge 1/\delta$ (or anytime-valid variant using $\max_i E_i$). R-AutoEval+ uses PPI++-style effective observations for candidate reliance factors $\rho_s\in[0,1]$: $\ell^{f}_{s,i}=\frac{\rho_s}{r}\sum \ell(\tilde X,f(\tilde X)) + \ell(X_i,Y_i)-\rho_s\,\ell(X_i,f(X_i))$, then forms a mixture e-value $E_n=\prod_i\sum_s w_{s,i}\big(1-\lambda_{s,i}(\ell^{f}_{s,i}-\alpha)\big)$ with weights updated as $w_{s,i}\propto w_{s,0}E_{s,i-1}$.","The main theoretical result states that for sufficiently small unreliability level $\delta$, the sample complexity of R-AutoEval+ satisfies $\lim_{\delta\to0^+} \frac{n_{\min}^{\text{R-AutoEval+}}(\delta)}{\log(1/\delta)}\le \min_s \{1/g_{s,\star}\}$, implying it is asymptotically no worse than both R-Eval and R-AutoEval and can be strictly better when some intermediate reliance factor yields lower variance. Empirically, on LLM reasoning budget allocation (GSM8K), R-AutoEval+ reduces average generated tokens compared to R-Eval and R-AutoEval, with reported savings up to 127 tokens vs. R-Eval and up to 66 tokens vs. R-AutoEval (main text). In the quantized LLM selection and prompt selection experiments, R-AutoEval+ selects smaller models/shorter prompts while maintaining the target risk constraints, and it reverts toward R-Eval behavior when the judge quality degrades. Experiments are averaged over repeated trials (e.g., 100 or 500 independent experiments depending on the figure).","The authors state that (i) R-AutoEval+ requires access to real-world unlabeled data; (ii) the discrete set of candidate reliance factors for weighting synthetic data is fixed a priori; and (iii) the sample-efficiency guarantee (Theorem 3) only holds for sufficiently high target reliability levels $1-\delta$ (i.e., small $\delta$).","The work is not about engineering reliability (failure/survival/maintenance) but about statistical reliability/valid inference for AI evaluation; as such it does not transfer directly to reliability engineering use cases without reinterpretation of “risk” and data-generating assumptions. The framework assumes i.i.d. sampling and bounded losses; practical evaluation datasets may exhibit dependence, distribution shift, and heavy-tailed or poorly calibrated losses that could affect validity or efficiency. Practical tuning choices (grid of $\rho_s$, betting strategy, and synthetic-to-real ratio partitioning) may materially influence performance; the paper’s guarantees hinge on conditions (e.g., regret/moment assumptions) that may be hard to verify in applied pipelines. While code is provided, broader reproducibility may depend on access to specific proprietary models (e.g., GPT-4.1) and substantial compute for LLM-judge runs.","The authors suggest addressing the stated limitations by leveraging tools from cited work (e.g., improving requirements on real-world unlabeled data, adapting or learning the reliance factors rather than fixing them, and extending efficiency guarantees beyond the high-reliability regime). They also propose combining R-AutoEval+’s adaptive weighting of synthetic data with complementary approaches that actively select which real data to label (active evaluation/active inference).","A natural extension is to develop a continuous or data-driven (e.g., Bayesian or convex-optimized) selection of the reliance factor $\rho$ rather than using a fixed discrete grid, while preserving anytime-valid guarantees. Robust variants that handle dependence/autocorrelation, distribution shift between unlabeled and labeled samples, and non-stationary judge quality would broaden applicability. More extensive benchmarking against additional hybrid-evaluation methods and across diverse tasks (including non-LLM domains) would clarify when synthetic-label variance reduction outweighs bias-correction variance inflation. Packaging the method into a maintained library (e.g., Python/R) with standardized interfaces for evaluators and logging of e-value trajectories could improve adoption and auditing in real-world evaluation pipelines.",2505.18659v2,https://arxiv.org/pdf/2505.18659v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:52:52Z
TRUE,Degradation modeling|RUL prediction|Other,"ML-based|Parametric (Weibull, etc.)|Other",Degradation measurements|Simulated only|Other,Not applicable,Energy/utilities,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://arxiv.org/abs/2505.18693,"The paper proposes an ML-driven framework to simultaneously maximize power conversion efficiency (PCE) and minimize performance degradation in HTL-free perovskite solar cells with an MWCNT back contact, combining SCAPS-1D device simulations with machine-learning surrogates. A simulated dataset of 1650 device configurations is generated by varying molar fraction x in MAPb1−xSb2x/3I3, absorber defect density, absorber thickness, and ETL doping; targets include photovoltaic metrics and degradation (defined via efficiency drop after aging). Degradation is modeled through time-evolving defect density attributed to ion migration, using a parametric square-root growth law, and degradation percentage is computed from initial vs aged efficiency. Among several regressors, a 4th-degree polynomial regressor (PR-4) provides near-perfect predictive performance and yields an explicit differentiable surrogate objective, which is then optimized with L-BFGS-B using a weighted objective to increase PCE while decreasing degradation. The optimized design is validated in SCAPS, improving efficiency (e.g., from 13.7% to 16.84%) and reducing degradation over long aging (reported reduction from ~6.6% to ~2.39% over 1000 hours), and a separate MLP classifier labels configurations as ‘superior’ vs ‘inferior’ with reported 100% accuracy on their split.","Degradation is modeled by defect density growth due to ion migration as \(N(t)=N_0\,(1+t/\tau)^{1/2}\), with \(N_0\) the initial defect density and \(\tau\) a characteristic degradation time. Degradation percentage after aging is defined as \(\Delta_{50h}=(\eta_0-\eta_{50h})/\eta_0\times 100\%\). Joint optimization uses a weighted objective \(f(x,N_{ABS},T_{ABS},N_{ETL})=w_\eta\,\eta-w_\Delta\,\Delta\), and L-BFGS-B is applied to the PR-4 surrogate under bound constraints.","A 1650-sample SCAPS-generated dataset is built from 11×6×5×5 combinations of four fabrication variables; targets include PCE and degradation (after ~50 h aging in the dataset definition). The PR-4 regressor reports RMSE ≈ 0.0179 for PCE and 0.0117 for degradation with \(R^2\) ≈ 1.0 and 0.999, respectively, and performs best on the combined objective \((\eta-\Delta)/2\). L-BFGS-B optimization on the PR-4 surrogate yields predicted optimum \(\eta\approx 17.02\%\), \(\Delta\approx 0.48\%\); SCAPS validation gives \(\eta\approx 16.84\%\), \(\Delta\approx 0.50\%\) for those settings. Relative to a baseline device, SCAPS results show PCE increasing from 13.7% to 16.84% and degradation decreasing from about 6.61% to 2.39% over 1000 hours. For classification of ‘superior’ vs ‘inferior’ configurations, an MLP classifier reports 1.00 precision/recall/F1 and 100% accuracy on their train-test split after SMOTE balancing.","The authors note that advanced deep learning models (e.g., MLP/CNN in regression) underperform traditional models likely due to limited dataset size, implying the dataset is not large enough to fully exploit deep models. They also state that no clear inflection point is observed when varying dataset fraction, suggesting much larger datasets would be required to determine optimal data volume and improve deep-learning performance. They further mention that full-device physics simulations can be computationally expensive, motivating targeted simulation focus on the absorber layer rather than the entire device.","Degradation is represented primarily via a simplified defect-density evolution law linked to ion migration, which may not capture multiple coupled stressors (humidity/oxygen/thermal cycling, interfacial reactions, electrode corrosion) or different operational profiles, limiting generalizability to real-world reliability. The dataset is predominantly simulation-driven (SCAPS), so model performance (near-perfect \(R^2\)) may reflect simulator smoothness rather than real experimental variability; external validation on independent experimental aging datasets is limited. Reported 100% classification accuracy raises the possibility of overly easy separability or data leakage/overfitting; stronger validation (nested CV, holdout by parameter regions, or experimental test set) is not demonstrated. The optimization weights and objectives do not incorporate uncertainty, manufacturability constraints, or cost/robustness tradeoffs (e.g., sensitivity to parameter tolerances), which are important for reliability-focused design.","They suggest generating significantly larger datasets to better leverage advanced deep learning models and improve predictive performance. They propose expanding the design space with new material combinations/architectures and increasing feature dimensions (more layer-specific physical parameters) to improve accuracy and interpretability. They recommend incorporating experimentally measured datasets (not only simulation) to capture real-world variability, and exploring the efficiency–stability tradeoff across different weight pairs \((w_\eta,w_\Delta)\) for application-specific optimization (including cost considerations).","A valuable extension would be probabilistic reliability modeling: quantify uncertainty in degradation predictions (e.g., Bayesian surrogate models) and optimize expected performance subject to risk constraints. Incorporating standardized stability protocols (e.g., ISOS conditions) and multi-stressor accelerated testing models could connect simulator-based degradation to field-relevant lifetime metrics (T80/T95 under specified stress). Robust optimization accounting for fabrication tolerances (distribution over x, defect density, thickness, doping) would make the optimized design more reliable in production. Finally, releasing a full reproducible pipeline (SCAPS parameter files + training/optimization scripts) and benchmarking against additional degradation datasets would strengthen reproducibility and external validity.",2505.18693v1,https://arxiv.org/pdf/2505.18693v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:53:40Z
TRUE,Degradation modeling|Other,Physics-based|ML-based|Hybrid/Ensemble|Simulation-based|Other,Degradation measurements|Mixture of types|Other,Not applicable,Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB|Other,Upon request,NA,"This preprint proposes a degradation-aware uncertainty quantification framework that couples high-fidelity crystal plasticity finite element (CPFE) simulations with a polynomial chaos expansion (PCE) surrogate to quantify how crystallographic texture variability drives mechanical response variability in electron-beam-welded 316L stainless steel. Experimental EBSD texture maps are processed to obtain physically realistic bounds on eight texture components, which are then used to generate textured RVEs and run 200 CPFE simulations (Abaqus UMAT) for surrogate training/validation. The PCE surrogate enables fast propagation of texture uncertainty to stress–strain response and supports global sensitivity analysis, identifying Cube and Goss texture components as dominant contributors to stress variability, while Brass/S1 and other components have minimal influence. The study also performs mesh refinement and RVE-size convergence (up to ~1400 grains), selecting a 120 μm RVE with ~615 grains as a computationally efficient, statistically converged baseline for stochastic analyses. Overall, the contribution advances practical reliability/degradation assessment of welded nuclear-grade alloys by making microstructure-sensitive uncertainty analysis computationally tractable and physically grounded in measured texture variability.","The CPFE model uses quasi-static equilibrium in weak form (e.g., $\nabla\cdot\sigma + b = 0$) and a multiplicative decomposition of deformation gradient $F = F_e F_p$ with $\det(F_p)=1$, with slip rates governed by a power-law relation $\dot\gamma^a = \dot\gamma_0 \left(|\tau^a - X^a|/\tau_c^a\right)^n \mathrm{sgn}(\tau^a)$. Hardening is modeled via a Taylor-type CRSS relation $\tau_c^a = A G b\left(\sqrt{\rho_{SSD}^a + B/L^a}\right)$ with SSD evolution following a Kocks–Mecking–Estrin form, and back-stress evolving via an Armstrong–Frederick law. The surrogate is a polynomial chaos expansion $u(\xi)=\sum_{i=0}^{N} u_i\,\psi_i(\xi)$, with coefficients estimated by least-squares regression $U=(D^T D)^{-1}D^T B$ using sampled texture-component weights as uncertain inputs.","Mesh refinement studies on RVEs (40 μm/25 grains and 80 μm/~190 grains) indicate that ~40 elements per grain are sufficient for convergent in-grain stress distributions. RVE-size studies (100 realizations each) for 40, 80, 120, and 160 μm RVEs (≈25, 190, 615, 1400 grains) show decreasing variability with grain count and little additional variability reduction beyond the 120 μm (~615 grains) case, while the 160 μm case costs ~4× more compute time. The PCE surrogate is trained on 200 CPFE simulations (80% train / 20% validation) and shows close agreement between surrogate-predicted and CPFE stress–strain curves on held-out samples. Uncertainty bounds reported from the surrogate include yield stress mean 393.36 MPa (min 378.64, max 409.15), at 1.0% strain mean 444.77 MPa (min 424.25, max 465.98), and at 3.0% strain mean 500.61 MPa (min 472.94, max 527.45), with uncertainty increasing with strain; sensitivity analysis highlights Cube and Goss as the most influential texture components.","The authors note that CPFE calibration is a numerically underdetermined problem with non-unique solutions and that many constitutive parameters are difficult to measure directly. They also state that because the calibration experiment is not cyclic, isotropic hardening (via $\tau_c$) and kinematic hardening (via $X$) cannot be separated in their current setup. They emphasize the computational cost of CPFE as a practical constraint motivating surrogate modeling.","The approach focuses on uncertainty in monotonic stress–strain response and early-stage degradation precursors, but does not directly model damage evolution, crack initiation/propagation, or fatigue/creep life to connect uncertainty to failure probability metrics. The surrogate is built around texture-component volume fractions and Euler-angle generation; other influential microstructural uncertainties (e.g., residual stress fields in welds, porosity/defects, grain boundary character, local chemistry) are not propagated. Validation is demonstrated on a small set of randomly selected hold-out curves; broader validation across loading paths (multiaxiality, cyclic loading, temperature) and external datasets would be needed for general reliability claims.","The paper states that the constitutive formulation includes back-stress because the ultimate goal is to estimate uncertainty in the cyclic response of the material with combined isotropic and kinematic hardening. It also positions the framework as a scalable platform for predictive degradation modeling and lifecycle assessment of welded nuclear-grade materials, implying extension to service-like degradation scenarios beyond the monotonic calibration/analysis presented.","Extend the framework from monotonic response variability to explicit reliability outputs (e.g., distributions of fatigue crack initiation life, creep rupture time, or probability of failure) by coupling CPFE fields to damage indicators and life models. Develop a self-consistent UQ pipeline that accounts for model-form uncertainty and calibration uncertainty (e.g., Bayesian calibration of CPFE parameters jointly with texture variability). Improve practical deployment by releasing an implementation (scripts for MTEX/ATEX processing, sampling, PCE fitting) and assessing robustness under correlated texture components, autocorrelated spatial texture fields, and additional weld-relevant uncertainties such as residual stress and defect populations.",2505.18891v1,https://arxiv.org/pdf/2505.18891v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:54:21Z
FALSE,Other,Nonparametric/Semi-parametric|Other,Other,Not applicable,Finance/economics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://kaggle.com/competitions/GiveMeSomeCredit,"This paper introduces Performative Risk Control (PRC), a conformal-style calibration framework for controlling a user-specified risk level when the deployment threshold itself induces distribution shift (performativity). PRC iteratively updates a scalar decision threshold $\lambda$ using calibration data drawn from the previous deployment distribution $\mathcal{D}(\hat\lambda_{t-1})$, and selects the next threshold using an upper confidence bound plus a “performativity guard” term that accounts for distribution movement. Theoretical finite-sample guarantees are given under a Lipschitz distribution mapping assumption framed via Wasserstein sensitivity of the loss distribution, ensuring safety throughout the entire iterative trajectory (“safe at anytime”) and a tightness guarantee $R(\hat\lambda_T)\ge \alpha-\Delta_\alpha$. The framework is extended from expected risk to quantile-based risks (including CVaR) by constructing confidence bands for the loss CDF and integrating to obtain bounds on the quantile risk functional. Experiments on a credit default prediction/approval simulation (with strategic feature response) show the procedure maintains risk control for sufficiently large guard parameter $\tau$ and illustrates the conservativeness–iteration tradeoff as $\tau$ varies.","The performative expected risk is defined as $R(\lambda)=\mathbb{E}_{(x,y)\sim \mathcal{D}(\lambda)}[\ell(y,T_\lambda(x))]$, where the data distribution depends on the deployed threshold. PRC chooses the next iterate via $\hat\lambda_t=\min\{\inf\{\lambda\in\Lambda:V(\hat\lambda_{t-1},\lambda,\delta)\le \alpha\},\hat\lambda_{t-1}\}$ with $V(\hat\lambda_{t-1},\lambda,\delta)=\hat R_n(\hat\lambda_{t-1},\lambda)+c(n,\delta/\tilde T)+\tau(\hat\lambda_{t-1}-\lambda)$ (empirical loss + confidence width + performativity guard). The sensitivity assumption is $W_p(\mathcal{D}_g(\lambda_1),\mathcal{D}_g(\lambda_2))\le \gamma|\lambda_1-\lambda_2|$ (applied to $g(z)=\ell(z,\lambda)$), enabling bounds on performative error between successive deployments.","In the credit-scoring simulation for expected type-II error control, the authors report using $\alpha=0.3$, $\Delta_\alpha=0.082$, $n=2000$, and $\delta=0.1$, with an experimentally verified sensitivity bound $\gamma\le 1.38$; PRC achieves risk control when the guard parameter satisfies $\tau\ge \gamma$, and larger $\tau$ yields more conservative, finer-grained updates with longer trajectories. For 90%-CVaR control, they use $\beta=0.9$, $\alpha=0.25$, $\Delta_\alpha=0.12$, and $n=10{,}000$, with $\gamma\le 0.205$ and the condition $\tau\ge \gamma/(1-\beta)\approx 2.05$; empirical trajectories end with risks lying within the constructed confidence bounds. The paper also provides an asymptotic tightness statement that $\Delta_\alpha$ can be chosen to vanish at rate $O(\log n/\sqrt n)$ under a scaling of the maximum iteration budget $\tilde T$.","The authors note a limitation inherited from standard risk-control approaches: because PRC only calibrates a black-box model via simple postprocessing (tuning a low-dimensional threshold), it may be less effective than fine-tuning or retraining the full model. They specifically mention that this simple postprocessing can sometimes be so conservative that it outputs $\lambda_{\mathrm{safe}}$, reducing practical utility.","The theoretical guarantees hinge on a global Wasserstein-Lipschitz sensitivity assumption for the loss distribution under threshold changes; in many real deployments, performativity may be non-Lipschitz, history-dependent, or discontinuous, making $\gamma$ difficult to justify or estimate robustly. The method requires selecting a guard parameter $\tau$ that upper-bounds (a scaled version of) $\gamma$; overestimation can cause excessive conservativeness and long iteration horizons, while underestimation can break guarantees, yet practical guidance for calibrating $\tau$ is limited. The experiments use a simulated strategic response mechanism on a benchmark dataset rather than measured real-world feedback loops, so external validity to real performative environments is uncertain. The paper does not appear to provide an open-source reference implementation, which may hinder adoption and reproducibility beyond the described high-level algorithm.",No explicit future work directions are stated beyond the noted limitation that postprocessing-based calibration may be less effective than full-model fine-tuning and can lead to returning $\lambda_{\mathrm{safe}}$.,"A valuable extension would be data-driven or adaptive estimation of the sensitivity/guard parameter (e.g., online upper-confidence estimation of $\gamma$) to reduce conservativeness while preserving safety. Another direction is to broaden the framework beyond scalar monotone thresholds to higher-dimensional calibration parameters or structured decisions, and to handle history-dependent performativity (stateful distribution maps) explicitly. Robust variants that relax Wasserstein-Lipschitz assumptions (e.g., ambiguity sets, adversarial shift classes, or distributionally robust bounds) could improve applicability. Finally, releasing a reference implementation and evaluating PRC on real deployment feedback datasets (not simulated response models) would strengthen empirical credibility and practitioner uptake.",2505.24097v1,https://arxiv.org/pdf/2505.24097v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:54:59Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization,ML-based|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Predictive,Transportation/logistics,Simulation study|Case study (real dataset),TRUE,Other,Public repository (GitHub/GitLab),https://github.com/EC-labs/rul-fl,"The paper develops a collaborative federated learning (FL) framework to train a remaining useful life (RUL) prognostic model for aircraft engines across multiple airlines without centrally sharing run-to-failure data. It introduces a decentralized validation procedure where each airline evaluates candidate global/local models on its private validation set and shares only loss values, enabling model selection/early stopping while preserving data privacy and supporting certification-oriented validation needs. To improve robustness to noisy or low-quality sensor data, it proposes four novel aggregation methods that combine two validation policies (full vs. random cross-client validation) with two aggregation policies (best-model selection vs. softmax-weighted averaging based on validation performance). A 1D-CNN RUL model is trained and evaluated on the N-CMAPSS turbofan engine dataset with six simulated airlines; FL improves RUL accuracy for five of six airlines compared to isolated (non-collaborative) training, while centralized training remains the best-performing privacy-unconstrained baseline. Additional experiments inject varying noise levels into selected clients’ data, showing the proposed robust aggregation methods can outperform FedAvg under higher noise levels while FedAvg is best when noise is low.","The baseline FL aggregation uses FedAvg: each global parameter is a weighted average of client parameters, $w_G=\sum_{i\in C} \frac{|T_i|}{n} w_i$ (Eq. 2), with $n=\sum_i |T_i|$. Decentralized validation aggregates client-reported validation losses into a global validation loss $L(W_G)=\sum_{i\in C} L_{\text{sum}}(V_i,W_G)$ (Eq. 3). Robust aggregation computes per-client evaluation scores from cross-client validation losses (median for full validation, Eqs. 4–5), then either selects the best client model $W_G=W_{i^*}$ with $i^*=\arg\min_i E_i$ (Eqs. 6–7) or uses softmax weights $\alpha_i$ derived from z-scored inverse evaluation scores (Eqs. 8–13) to aggregate $w_G=\sum_i \alpha_i w_i$ (Eq. 13). Noise is simulated by adding Gaussian noise to measurements: $\breve{x}\leftarrow \breve{x}+\mathcal{N}(0,\alpha\sigma_{i,h})$ (Eq. 18).","On N-CMAPSS (DS02) with six airlines/clients and a 1D-CNN, the FL model achieves minimum validation RMSE 12.7 (at epoch 61) and test RMSE/MAE over all three test engines of 9.9/6.7 flights; the unrestricted centralized baseline achieves 6.3/4.4 flights overall (Table 4). Compared to six isolated (non-collaborative) per-airline models, FL improves overall accuracy versus the mean isolated result (mean RMSE/MAE 15.8/12.7 flights) and outperforms five of six isolated models; one isolated model (engine 16) attains overall RMSE 8.6 (Table 5). In noise-injection experiments (noise added to engines 5 and 18), FedAvg is best at low noise (e.g., overall RMSE 9.9 at $\alpha=0$; 10.2 at $\alpha=0.1$), but at higher noise ($\alpha=1$) robust methods improve performance: Full-Best achieves overall RMSE/MAE 9.5/6.9 and Full-Softmax 10.1/7.0 versus FedAvg 12.3/10.2 (Table 8). The softmax-based methods are more stable under increasing noise, while best-model selection methods can vary due to epoch-to-epoch selection randomness.","The authors note a privacy–robustness trade-off: robust aggregation requires sharing local model weights among clients, which may leak information via inversion or property-inference attacks. They state the median-based evaluation becomes unreliable if more than half of the clients have noisy data, limiting applicability when noisy clients are a majority. They also highlight that regulatory approval (e.g., EASA/FAA) may require sharing aggregated information and that a single airline may lack sufficient data for approval, motivating collaboration but leaving certification requirements unresolved. Finally, the framework does not address stragglers in heterogeneous FL settings, so training speed can be limited by the slowest device.","The case study uses a very small number of clients (six) and assigns exactly one run-to-failure engine per airline, which may not reflect realistic fleet sizes, label availability, or within-airline variability; results may not generalize to larger and more diverse federations. The evaluation focuses primarily on point-error metrics (RMSE/MAE) without uncertainty quantification, calibration, or decision-level maintenance impact, which are important in safety-critical prognostics. The robustness analysis injects additive Gaussian noise and does not cover other realistic data issues (bias/drift, missingness, sensor dropouts, label noise, nonstationarity, adversarial poisoning), so robustness claims are partial. The method involves cross-client model distribution for validation, which increases communication and potential attack surface; scalability and secure aggregation/differential privacy effects on accuracy are not studied.","They propose developing stronger privacy-preserving collaborative learning mechanisms for FL while remaining robust to noisy data, explicitly motivated by known gradient/weight leakage risks. They also mention incorporating authentication as an additional measure to improve robustness of aggregation methods, alongside privacy protections.","A valuable extension would be to integrate formal privacy defenses (secure aggregation, differential privacy, or cryptographic validation) and quantify the resulting privacy–utility trade-offs for RUL accuracy and robustness. Extending the framework to handle realistic non-i.i.d. temporal effects (concept drift, varying mission profiles over time) and to support asynchronous/straggler-resilient training would improve deployability. Adding probabilistic RUL outputs (prediction intervals) and evaluating calibration and maintenance decision utility (e.g., expected cost, risk constraints) would better align with certification and operational needs. Broader benchmarking across multiple PHM datasets and against modern personalized/clustered FL baselines would clarify when global-vs-personalized models are preferable in heterogeneous airline settings.",2506.00499v1,https://arxiv.org/pdf/2506.00499v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:55:41Z
TRUE,Network/infrastructure reliability|System reliability|Other,"Simulation-based|Parametric (Weibull, etc.)|Other",Simulated only|Other,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"This paper proposes a stratified-sampling variance-reduction method for estimating network failure probability in network reliability assessment. The key contribution is an unbalanced stratum refinement strategy (SSuR) where strata are defined by (i) a partition of components into clusters and (ii) the number of failed components within each cluster; stratum probabilities and conditional sampling are computed using the conditional Bernoulli model (Poisson–binomial setting). The method further improves efficiency by identifying the minimum number of component failures required for system failure (i*), then removing all strata with fewer than i* failed components (conditional stratification). The authors also introduce a practical heuristic to approximate optimal sample allocation across strata (followed by randomized rounding to integer stratum sample sizes) and prove that under proportional or optimal allocation, refining any stratum cannot increase estimator variance (variance ratio is non-increasing). Numerical studies on an IEEE39 DC power-flow cascading-failure setting and a water-supply connectivity problem show large efficiency gains over crude and conditional Monte Carlo, especially when optimal allocation is well approximated and/or many redundant strata are removed.","The failure probability is expressed as $p_F=\Pr(F)=\sum_{x\in\Omega_X} \mathbf{1}\{x\in F\}p_X(x)$ and stratified by the number of failed components $I$ as $p_F=\sum_{i=0}^n \lambda_i p_{F\mid i}$ with $\lambda_i=\Pr(I=i)$. The basic stratified estimator is $\hat p_F^{(SS)}=\sum_{i=0}^n \lambda_i \hat p_{F\mid i}$ with variance $\sum_i \lambda_i^2\, p_{F\mid i}(1-p_{F\mid i})/N_i$; optimal allocation uses $N_i\propto \lambda_i\sqrt{p_{F\mid i}(1-p_{F\mid i})}$. The refined estimator (SSuR) replaces strata by sub-strata $(i,j)$ with probabilities $\lambda_{i,j}$ and samples $x\sim p_{i,j}(x)\propto p_X(x)\mathbf{1}\{x\in S_{i,j}\}$, and removes redundant strata via $i^*$ by estimating $p_F=\Pr(I\ge i^*)\, p_F^*$ where $p_F^*=\Pr(F\mid I\ge i^*)$.","The paper analytically shows that (with fractional allocations) refining a stratum into two sub-strata cannot increase the variance ratio under proportional or optimal allocation (inequalities (32a)–(32b)), implying monotone non-increasing variance ratios as refinement proceeds. In the IEEE39 DC power-flow example, the practical SSuR estimator with 5,000 refinement steps and initial $N=10{,}000$ achieves very large relative efficiency gains over conditional MCS for rarer events (e.g., for thr=30% and IID $p=10^{-3}$, relEff over cMCS reported as $2.9\times 10^3$; for thr=40% and IID $p=10^{-3}$, $1.2\times 10^3$), while gains diminish as component failure probability increases. In the water-supply connectivity example, practical SSuR yields substantial gains for very small $p_F$ (e.g., node 47 with IID $p=10^{-3}$: relEff over cMCS $1.4\times 10^2$; node 75 with IID $p=10^{-3}$: $4.7\times 10^2$). The authors also quantify variance inflation from approximate/rounded allocations (relative increase factor $\alpha$), which can be large in difficult regimes (e.g., Table 10 shows $\alpha$ up to 55 in IEEE39 cases; Table 13 shows values up to 14 in water-network cases).","The method is tailored to independent binary inputs; the authors note that real systems may involve dependent failures (e.g., common-cause failure) and multi-state components, and extending SSuR to these settings is deferred to future work. For physics-based performance functions, identifying the true minimum number of failures $i^*$ via genetic algorithms is not guaranteed under limited budget; misidentification can introduce bias. They also attribute the one case where SSuR is slightly worse than conditional Monte Carlo (IEEE39, thr=60%) to poor approximation and randomization of the optimal sample sizes.","The practical performance depends heavily on the quality of the heuristic approximation of conditional failure probabilities used for near-optimal allocation; if this approximation is poor, randomized minimum-one-sample constraints can dominate cost and inflate variance (observed via large $\alpha$), suggesting sensitivity to tuning choices (initial N, refinement steps T, cut/failure-state set quality). The refinement procedure prioritizes splitting the most probable cluster/stratum, which may not be optimal for variance reduction in all networks (e.g., when rare but high-variance strata dominate contribution). Theoretical monotonicity results are shown under fractional allocations; integer constraints and enforced minimum sample sizes can break clean monotonic variance behavior and can lead to rapid cost growth toward brute-force enumeration. The approach assumes independence across components and uses conditional Bernoulli machinery; handling correlated component states would require substantial methodological changes beyond minor modifications.","The authors propose extending the refined stratified sampler to incorporate common-cause failure (dependent components) and multi-state components. They also note an open question for time-varying component reliabilities: since their strata are optimized for fixed reliabilities, it is unclear which reliability values should define strata when component reliabilities evolve over time, and they suggest this as a promising research direction. They further mention exploring adaptive stratification strategies to determine when to stop refinement (using variance-based termination), leaving robustness/effectiveness in network reliability assessment to future work.","Develop self-starting/adaptive allocation schemes that update stratum sample sizes online using posterior/empirical estimates of $p_{F\mid i,j}$ while controlling bias under integer constraints, potentially reducing reliance on cut/failure-state heuristics. Extend the refinement rule to objective-driven splitting (e.g., maximizing expected variance reduction per added stratum) and compare against the “most probable cluster” heuristic on diverse benchmarks. Add robust variants for non-coherent or partially coherent performance functions, including diagnostics for coherency violations that affect allocation approximations. Provide open-source implementations (e.g., Python/R) and standardized benchmark suites to enable reproducibility and fair comparison with subset simulation, cross-entropy, and modern surrogate-assisted rare-event estimators.",2506.01044v1,https://arxiv.org/pdf/2506.01044v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:56:18Z
TRUE,Life distribution modeling|System reliability,"Parametric (Weibull, etc.)|Simulation-based|Other",Complete lifetime data|Right-censored|Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a new high-accuracy yet computationally efficient method for system reliability assessment (SRA), focusing on constructing lower confidence limits (LCLs) for system reliability at a mission time using component-level lifetime data and a known coherent system structure function. It introduces the Double Bootstrap Percentile with Transformed resamples (DBPT), a modified double-bootstrap percentile approach designed for log-location-scale lifetime models (including Weibull, lognormal, and exponential), replacing nested resampling/estimation with direct generation of moment-based reliability estimates and reusing transformed second-layer resamples. The authors prove the method attains second-order coverage accuracy, i.e., coverage error of order $O(n^{-1})$, while ensuring LCLs remain within $[0,1]$ and are monotone in time, thereby largely avoiding “falling outside” and reducing the “bend-back” phenomenon common in normal-approximation and Cornish–Fisher/WCF-based methods. Extensive simulations across several system structures (series, parallel, series-parallel, and $k$-out-of-$n$) show DBPT achieves coverage closest to nominal levels and LCL quantiles closest to true reliability, especially in high-reliability/small-sample regimes. For censored data (Type II right-censoring), the method is applied after completing data via an imputation algorithm, and simulations still show improved coverage relative to competitors, with substantial runtime reductions versus conventional double bootstrap.","System reliability is modeled as $R(t)=P(T>t)=\psi(r_1(t),\ldots,r_s(t))$ with component reliabilities $r_i(t)=1-G_{\theta_i}(t)$. For log-location-scale lifetimes, $r_i(t)=1-F_i\big((\log t-\mu_i)/\sigma_i\big)$ and moment estimators are $\hat\sigma_i=s_i/\kappa_{i2}$ and $\hat\mu_i=\bar x_i-\kappa_{i1}\hat\sigma_i$, giving $\hat r_i(t)=1-F_i\big((\log t-\hat\mu_i)/\hat\sigma_i\big)$. The DBPT algorithm computes first- and second-layer bootstrap reliability draws via the transformed-resample shortcut $\hat r^{*}_{i,j}(t)=1-F_i\left(\{F_i^{-1}(1-\hat r_i(t)) - \bar Z^{*j}_{n_i}\}/(\kappa_{i2}/M^{*j}_{n_i}+\kappa_{i1})\right)$ and similarly for $\hat r^{**}_{i,j,l}(t)$, then forms the LCL as $\hat R^{*}_{(k')}(t)$ with $k=\lceil B\alpha\rceil$ and $k'=\lceil B\hat u_{(k)}\rceil$, where $\hat u_j=C^{-1}\sum_{l=1}^C\mathbf{1}(\hat R^{**}_{j,l}\le \hat R)$.","The method is proven to deliver second-order accurate coverage for the system-reliability LCL: $P\{R(t)\ge R_{\text{DBPT}}(t)\}=1-\alpha+O(n^{-1})$, matching conventional double-bootstrap percentile accuracy but with much lower computational cost. The authors report large reductions in runtime in a complex example (a 9-out-of-16 system with $s=16$ components and $n=100$ per component): DBPT takes about 0.2 hours versus 78.8 hours for the conventional double-bootstrap percentile, while producing numerically close LCLs (matching to two decimal places). In simulations for highly reliable systems, DBPT shows near-nominal empirical coverage once sample sizes are modest (e.g., for a 3-component Weibull parallel system, coverage aligns closely with 0.9 for $n\ge 10$) and yields LCL quantiles closer to the true reliability than Delta, BP, and R-WCF. In a small-sample stress test (series system, $n=5$, $10^4$ repetitions), DBPT produced zero “falling outside” occurrences and only 20 bend-back occurrences, compared with frequent out-of-range and bend-back events for Delta and R-WCF.","The authors note that DBPT cannot completely eliminate the bend-back issue; it is reduced substantially but still occurs rarely in finite samples. They also acknowledge that in high-reliability scenarios, component data may contain zero failures, and adapting the method to handle such zero-failure data is not addressed in the current work.","The DBPT construction relies on parametric log-location-scale modeling and moment-based estimators; performance may degrade under model misspecification or for lifetime distributions outside this family, and robustness to such misspecification is not explored. Handling censored data is done via a separate imputation step (rather than a fully likelihood-based censored-data bootstrap), which may introduce additional uncertainty or bias depending on the imputation model and censoring level; sensitivity analyses are not provided. The paper appears to evaluate performance mainly through simulations; broader real-data case studies or guidance on choosing $B$ and $C$ under strict computational budgets and across different $s,n$ regimes could further strengthen practical adoption.","The authors propose future work aimed at fully addressing the residual bend-back issue and extending the method to handle zero-failure component data in high-reliability settings. They also highlight two broader extensions: adapting the approach to degradation data for reliability assessment, and extending the acceleration principle to reliability regression models that incorporate covariates.","A valuable extension would be developing a principled censored-data version of DBPT that avoids imputation (e.g., using parametric bootstrap directly from censored likelihood fits or via EM-based resampling), with theoretical coverage guarantees under censoring. It would also be useful to study robustness under model misspecification (e.g., mixtures, frailty, dependence between components) and to extend DBPT to dependent-component systems or common-cause failures. Providing open-source software (e.g., an R/Python package) and implementation details for general coherent system structure functions (including automatic evaluation of $\psi$) would improve reproducibility and practitioner uptake.",2506.04573v1,https://arxiv.org/pdf/2506.04573v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:56:57Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,Physics-based|Bayesian|Simulation-based|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Predictive|Not applicable,Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|C/C++|Other,Public repository (GitHub/GitLab),https://github.com/EdgarJaber/bayes-calibration-for-prognostics.git,"The paper proposes a modular methodology to fuse heterogeneous and intermittently available degradation data with computationally expensive physics-based simulation models to produce robust probabilistic prognostics and remaining useful life (RUL) estimates. It performs offline data assimilation in two stages: (i) an iterative Bayesian Model Updating (BMU) scheme that ranks uncertain input variables via kernel-based global sensitivity analysis (HSIC) and then updates the most influential input marginals using MCMC, with information gain quantified by KL divergence; and (ii) an Ensemble Kalman Smoother step that conditions simulated degradation trajectories on observations to reduce uncertainty over the prognostics horizon. For expensive simulators, it integrates surrogate modeling using Karhunen–Loève expansion (KLE) and aggregates multiple surrogates with Dirichlet-weighted averaging, propagating aggregation uncertainty within the Bayesian update. The approach is demonstrated on a controlled crack-growth example (Paris–Erdogan law) and on a real digital twin for steam generator clogging in nuclear power plants, showing reduced uncertainty in influential inputs and tighter posterior RUL distributions for maintenance planning. Overall, it advances PHM/SPC-adjacent reliability practice by providing a scalable, interpretable data-fusion workflow for sparse, multi-source industrial degradation data tied to simulation codes.","RUL is defined as $\mathrm{RUL}(D)=\arg\min_{t>t_P}\{\delta(t)\ge D\}$, with observations modeled as $y_i(t_\ell)=\delta(t_\ell)+\eta^i_\ell$, $\eta^i_\ell\sim\mathcal N(0,\sigma_i^2)$. Surrogate aggregation uses $\hat g_{\mathrm{agg}}(X)=\sum_{r=1}^p w_r\hat g^{(r)}(X)$ with $w\in\Delta_{p-1}$ (Dirichlet prior), and the BMU posterior for an influential input $\theta_k$ with multiple heterogeneous groups is proportional to $p(\theta_k\mid y_1,\ldots,y_q)\propto\prod_{i=1}^q \|y_i-G_i(\theta_k)\|^{-m_i}$ (with MC integration over surrogate weights). The EnKS update (scalar form) applies $Y_t^{(p)}\leftarrow Y_t^{(p)}+K_{tj}\,d^{(p)}$ where $K_{tj}=C_{kj}/(C_{kk}+\sigma_i^2)$ and $d^{(p)}=y_i(t_k)-Y_{t_k}^{(p)}$. The RUL CDF is estimated by Monte Carlo: $\mathbb P(\mathrm{RUL}(D)\le t_j\mid D)\approx \frac1n\sum_{i=1}^n \mathbf 1\{\mathrm{pr}_{j+1}(g(X^{(i)}))\ge D\}$.","On the Paris–Erdogan crack-growth example with four synthetic heterogeneous data groups ($q=4$) and a failure threshold $D=5$ cm, the BMU step concentrates the posterior of influential parameters (reported as $C$, $m$, and $\sigma_M$) relative to uniform priors, while $\sigma_m$, $Y$, and $a(0)$ remain essentially uninformed; the posterior RUL distribution becomes more concentrated than the prior. For the steam-generator clogging digital twin (TPD code), the authors run $L=3$ independent assimilation windows (before/ between/after cleanings) and select 5 influential inputs ($\beta$, $\epsilon_c$, $d_p$, $\Gamma_p(0)$, $a_v$) for updating; other inputs (e.g., $\alpha$, $\epsilon_e$) remain near-prior. They report the full three-scenario workflow takes about 45 minutes on a regular computer, using multiple MCMC chains and Gelman–Rubin diagnostics for convergence. Posterior clogging trajectories and posterior RUL distributions (after BMU and smoothing) are visibly tighter and more aligned with observed data than priors, supporting improved maintenance decision-making under uncertainty.",The authors state that their current approach does not integrate uncertainty in the fixed “latent variables” (the non-updated inputs held at nominal values during each one-dimensional BMU update); they note that attempting to integrate these uncertainties numerically can over-average the likelihood and degrade posterior estimation in higher dimensions. They explicitly identify “integration of the latent variables uncertainties” as an open question left for future work.,"The iterative marginal-updating strategy assumes independent input marginals and effectively performs a series of one-dimensional calibrations; this can miss posterior dependencies and interaction effects, which are common in physics-based degradation models (e.g., correlated material parameters). The EnKS step relies on approximate linear-Gaussian ensemble statistics; for strongly nonlinear degradation dynamics or non-Gaussian observation errors, smoothing updates can be biased or under-dispersed without additional inflation/regularization. Reported performance evidence is largely qualitative (plots) with limited standardized quantitative metrics (e.g., proper scoring rules for RUL distributions, calibration/coverage, or out-of-sample validation across assets), which may limit comparability and operational assurance.","They propose future work on integrating latent-variable uncertainty directly within the MCMC procedure to obtain full posteriors rather than conditional ones. They also propose developing an adaptive metamodel updating strategy and quantifying uncertainty in MCMC predictions, suggesting conformal prediction techniques as a possible tool.","A valuable extension would be to relax the independent-marginals assumption by learning or approximating posterior dependence (e.g., via copulas or normalizing-flow surrogates) while keeping computational cost manageable. Robustness studies under model mismatch (e.g., biased simulator, misspecified observation noise, autocorrelated residuals) and formal uncertainty calibration checks (coverage of prediction intervals for trajectories/RUL) would strengthen reliability claims. Packaging the methodology into a maintained, documented software workflow (with benchmark datasets and automated diagnostics for sensitivity selection, MCMC convergence, and EnKS stability) would improve reproducibility and industrial uptake beyond the provided research repository.",2506.05882v2,https://arxiv.org/pdf/2506.05882v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:57:52Z
FALSE,Other,ML-based|Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/bjbbbb/Enhancing-Adversarial-Robustness-with-Conformal-Prediction,"This paper proposes a framework for improving the reliability (trustworthy uncertainty/coverage behavior) of deep neural network predictions under adversarial attacks by integrating conformal prediction (CP) with adversarial training. The authors introduce an adaptive attack, OPSA (OPtimal Size Attack), that maximizes the (soft) size of conformal prediction sets without requiring knowledge of the defender’s significance level, by optimizing a temperature-smoothed surrogate of set size based on logit differences. They then propose OPSA-AT, a conformal training procedure that alternates (within mini-batches) between generating OPSA adversarial examples and calibrating the conformal threshold, and trains with a combined objective that encourages inclusion of the true label while minimizing prediction-set size. Experiments on CIFAR-10, CIFAR-100, and mini-ImageNet with ResNet backbones compare OPSA to common attacks and OPSA-AT to several adversarial training baselines, showing OPSA tends to induce larger prediction sets (higher uncertainty) and OPSA-AT yields smaller sets (lower uncertainty) while maintaining near-nominal coverage. The evaluation uses CP-oriented metrics (coverage, set size, SSCV) and reports runtime overheads for attacks and training per epoch.","The conformal prediction set is defined by thresholding logits: $\Gamma(x;f,\tau)=\{k\in[K]: f_k(x)\ge\tau\}$. The attacker uses a temperature-smoothed surrogate of set size $M_T(x;f,\tau)=\sum_{k\in[K]} \sigma((f_k(x)-\tau)/T)$ and selects an input-dependent internal threshold $\tau=f_y(x+\epsilon)$, optimizing $\epsilon^*(x,y)=\arg\max_{\|\epsilon\|_p\le r,\,x+\epsilon\in[0,1]^d} M_{T_1}(x+\epsilon; f, f_y(x+\epsilon))$. OPSA-AT trains by minimizing $L_{total}=L_{class}(x,y;f,\tau,T_2)+\lambda M_{T_2}(x;f,\tau)$, where $L_{class}=\sigma((f_y(x)-\tau)/T_2)-\sum_{k\ne y}\sigma((f_k(x)-\tau)/T_2)$ and $\tau$ is set to the empirical $(1-\alpha)$ quantile from a calibration subset.","With $\alpha=10\%$ and $\ell_\infty$ budget $\epsilon=8/255$, coverage stays near the nominal 90% across methods, while OPSA often produces the largest prediction-set sizes among attack methods when evaluated against standard defenses (e.g., CIFAR-10 under FGSM-trained defense: OPSA size $7.29\pm0.02$ vs Auto $5.77\pm0.03$; under PGD-trained defense: OPSA size $4.50\pm0.02$). OPSA-AT generally reduces set sizes relative to other defenses under strong attacks (e.g., CIFAR-10 under PGD-trained defense and PGD40 attack: OPSA-AT size $3.24\pm0.03$ vs PGD-trained $4.47\pm0.02$; CIFAR-100 under FGSM-trained defense and Auto attack: OPSA-AT size $13.42\pm0.16$ vs FGSM-trained $32.00\pm0.21$). SSCV values are typically small for OPSA-AT, indicating more stable coverage across different set sizes (e.g., CIFAR-10 under FGSM attack: SSCV around $0.03$ for OPSA-AT). Reported runtime profiling shows OPSA-AT is slower per epoch than standard FGSM/PGD training (e.g., on CIFAR-10: FGSM 65s/epoch vs OPSA-AT 1642s/epoch for 100 batches on an NVIDIA A100 80GB), reflecting the overhead of bilevel conformal training and attack generation.","The authors state the experimental setup is relatively limited and plan to expand experiments by including additional architectures (specifically PreAct ResNet) in future work. They also note hardware/memory constraints limited their implementation to certain batch/mini-batch sizes (e.g., mini-batch 64 for standard training and 200 for conformal training), and remark that theory suggests larger mini-batches (about 500–1000) may better approximate exchangeability conditions.","The work frames “reliability” in terms of CP coverage and prediction-set size, but does not connect to classical reliability engineering measures (failure rates, hazard, RUL, maintenance), limiting applicability to reliability engineering as typically defined. The empirical evaluation is confined to image benchmarks and a specific CP construction (THR with differentiable surrogates); robustness to distribution shift, autocorrelated data, or non-image modalities is not established. The defense relies on repeated data splitting within mini-batches for calibration, which may be sensitive to class imbalance and small-batch effects; more extensive sensitivity analyses (beyond $T_1$) and statistical significance testing across runs would strengthen claims. Comparisons focus on common adversarial training baselines but do not include certified robustness methods or more recent robust CP methods integrated with training that might be closer competitors.","They plan to broaden experimental validation by adding the PreAct ResNet network to test whether results hold across architectures. They also indicate (via the mini-batch remark) that larger mini-batch sizes could better satisfy the exchangeability approximation and potentially improve performance, motivating further experimentation when resources allow.","Extending OPSA/OPSA-AT to other conformal predictors (e.g., APS/RAPS-style scores, rank-based scores) and to regression/segmentation tasks would test generality beyond THR classification. Studying robustness under simultaneous distribution shift plus adversarial perturbations, and under adaptive attackers who target both coverage and set size, would clarify practical guarantees. Developing a more compute-efficient approximation (e.g., fewer inner-loop steps, reuse of perturbations, or amortized attackers) and releasing reproducible training scripts/configs could improve deployability. Formalizing conditions under which the bilevel mini-batch calibration approximates split conformal guarantees, and quantifying coverage under finite-batch dependence, would strengthen the theoretical reliability claims.",2506.07804v1,https://arxiv.org/pdf/2506.07804v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:58:38Z
TRUE,Degradation modeling|Maintenance optimization|Network/infrastructure reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Degradation measurements|Sensor/condition monitoring|Mixture of types,Condition-based|Not applicable,Transportation/logistics,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.1016/j.ress.2023.109496|https://doi.org/10.1016/j.ress.2023.109709|https://doi.org/10.1080/15732479.2019.1629464|https://doi.org/10.1016/j.ress.2023.109687|https://doi.org/10.1016/j.trc.2018.06.018|https://doi.org/10.1016/j.measurement.2024.114354|https://doi.org/10.1109/TITS.2022.3214121|https://doi.org/10.1111/mice.12802|https://doi.org/10.1016/B978-0-12-817784-6.00015-1|https://doi.org/10.1080/00423114.2020.1850808|https://doi.org/10.1155/2019/1729153|https://doi.org/10.1007/978-3-030-96794-9_39|https://doi.org/10.1016/j.ifacol.2016.11.021|https://doi.org/10.1016/j.trc.2018.02.019|https://doi.org/10.1080/15732479.2017.1326946|https://doi.org/10.1016/j.ress.2013.02.010|https://doi.org/10.1007/978-1-0716-2197-4_3|https://doi.org/10.1007/s11831-022-09815-7|https://doi.org/10.1016/j.ymssp.2015.11.008|https://doi.org/10.1109/MAES.2010.5546308|https://doi.org/10.1198/016214506000000096|https://doi.org/10.1016/j.jmva.2005.03.005,"The paper proposes a sensor-fusion method for railway track geometry monitoring that combines infrequent but accurate Track Recording Car (TRC) measurements with frequent, noisy on-board sensor-derived indices. Track geometry degradation is modeled using a multivariate Wiener process, with tamping events incorporated as mid-interval maintenance resets affecting the mean and covariance evolution. A linear-Gaussian measurement model links on-board indices to TRC indicators, enabling Kalman filtering to update state estimates between TRC runs; parameter uncertainty from prior Bayesian (MCMC) estimation is propagated by mixing Kalman-filtered Gaussian posteriors across parameter samples. The approach is validated on a one-year field campaign in Queensland, Australia, using low-cost MEMS accelerometers and GPS mounted on a TRC, showing reduced prediction uncertainty compared with degradation-only prediction. Monte Carlo simulations further study how sensor measurement frequency affects the credible interval width, providing guidance for deploying on-board systems to support inspection planning and maintenance decision support.","Track geometry indicator state evolution is modeled as a multivariate Wiener process: $\mathbf{Z}_k\mid \mathbf{Z}_{k-1}=\mathbf{z}_{k-1}\sim\mathcal{N}(\boldsymbol{\mu}_k,\mathbf{Q}_k)$ with drift/diffusion scaled by $\Delta t_k$ and modified when tamping occurs (Eq. 1–2). On-board indices follow a linear observation model: $\mathbf{Y}_k\mid \mathbf{Z}_k=\mathbf{z}_k\sim\mathcal{N}(\mathbf{m}_k,\mathbf{R})$ with $\mathbf{m}_k=\mathbf{H}\mathbf{z}_k+\mathbf{b}$ (Eq. 3–4), where $\mathbf{H},\mathbf{b},\mathbf{R}$ are estimated via multivariate regression (Eq. 5–6). Kalman filter predict/update equations are used to fuse degradation prediction and on-board measurements (Eq. 7–12), and parameter-uncertainty is handled by mixing Gaussian filtering results over posterior samples (Eq. 13). Credible-zone width is summarized as $W_k = |\mathbf{P}_{k\mid k}|^{1/n}$ (Eq. 14).","Field validation uses TRC measurements at weeks 0, 11, 19, 27, and 34, with on-board sensor availability from week 11 onward; incorporating sensor updates via Kalman filtering reduces the prediction credible zone compared with degradation-only prediction (shown in Fig. 6). Monte Carlo experiments over a one-year horizon (52 weeks) show that Kalman-filter fusion yields substantially smaller and more stable credible zones than the degradation model alone (Fig. 7). The credible-zone width stabilizes faster as measurement intervals shorten; benefits remain noticeable even with measurement intervals up to 8 weeks, with a marked reduction after roughly ~2 months (text in Sec. 4.2, Fig. 8). The authors suggest that increasing measurement frequency to about one week or less significantly reduces and stabilizes the credible zone over time (Conclusion).","The paper notes that before practical use for optimization/operations, a “very thorough empirical characterization” of measurement-system performance and error is needed to ensure the measurement-error model is representative, including whether performance could change over time or across operational environments (Sec. 4.2). It also emphasizes that on-board sensor data are noisy and cannot directly replace TRC measurements, motivating fusion rather than substitution (Sec. 4.1 and earlier discussion).","The linear-Gaussian observation model ($\mathbf{Y}=\mathbf{H}\mathbf{Z}+\mathbf{b}+\varepsilon$) may be too restrictive if sensor-to-geometry relationships are nonlinear, heteroscedastic, speed-dependent, or vary across vehicles/track types, which could bias Kalman updates. The approach treats TRC measurements as the “true” state and resets covariance to zero at TRC times ($\mathbf{P}_0=0$), ignoring TRC measurement uncertainty and potential systematic errors. The paper does not specify the number of posterior samples $N_s$, the MCMC setup, or sensitivity to prior/estimation choices, which can materially affect mixture-based credible intervals. Comparative evaluation versus alternative fusion/filters (e.g., particle filters, robust filters) and quantitative error metrics (RMSE/coverage) are not clearly reported in the provided text, limiting strength of performance claims.",Future work will develop TRC planning and tamping decision-making for the entire track network using the collaborative predictions from the degradation model and on-board sensors (Conclusion).,"Extend the fusion model to explicitly include TRC measurement uncertainty (nonzero observation noise) and potential bias/drift in both TRC and on-board systems, enabling more realistic state resets and confidence bounds. Develop speed/vehicle/track-context adaptive or nonlinear observation models (e.g., switching/IMM, Gaussian processes, or neural measurement models) and evaluate robustness under non-Gaussian noise/outliers. Provide decision-focused outputs (e.g., probability of threshold exceedance, optimized inspection/tamping schedules) and validate end-to-end benefits on maintenance outcomes/costs across multiple lines and vehicles. Release reference implementations and datasets (or synthetic generators) to support reproducibility and benchmarking against alternative Bayesian filters and degradation-state-space formulations.",2506.08028v1,https://arxiv.org/pdf/2506.08028v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T14:59:24Z
FALSE,NA,ML-based|Other,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://github.com/jiaaro/pydub,"This paper studies measurement reliability (repeatability/consistency) of speech features extracted by two widely used open-source speech analysis tools—OpenSMILE (eGeMAPSv02 feature set) and Praat (via Parselmouth)—in an autism assessment setting. Using audio from ADOS-2 Module 3 sessions (29 adolescents; 14 ASD/15 TD), it compares five commonly used acoustic/behavioral features (mean pitch, pitch standard deviation, pitch range, loudness, and speech rate) across tools and examines whether tool differences interact with gender or diagnostic group. The authors find all five features differ significantly between OpenSMILE and Praat (paired t-tests, p<0.001), and report that Praat’s speech-rate estimates can be implausibly high and appear more sensitive to diarization/segmentation artifacts. Two-way ANOVAs indicate that several group differences (gender or ASD/TD) appear only under Praat, suggesting tool choice can change conclusions about demographic effects. Random-forest classifiers trained separately on features from each tool show task-dependent and group-dependent variability in accuracy/precision/recall/F1, underscoring that feature-extraction tool choice can materially affect downstream model performance and fairness claims.","The paper defines feature computation procedurally rather than with closed-form formulas: OpenSMILE eGeMAPSv02 provides F0-based pitch measures (mean, SD, range) and loudness; speech rate is estimated from “voiced segments per second,” mapped to words/minute using an assumed 1.5 words/segment, then converted to syllables/second. Praat (via Parselmouth) computes pitch tracks to derive mean/SD/range and uses intensity for loudness; speech rate is computed by intensity-based syllable counting divided by voiced duration to obtain syllables/second. Task-level features are computed by aggregating utterance-level features to a mean per child per task, and tool differences are assessed via per-task per-participant differences, paired t-tests, and 2×2 ANOVAs (group × tool).","Across tasks, all five features extracted by OpenSMILE vs Praat are significantly different (paired-sample t-tests, p<0.001). The mean (SD) differences (Praat − OpenSMILE) reported are: mean pitch 156.9401 (5.9803), pitch SD 31.8441 (7.3489), pitch range 125.6173 (22.1054), loudness 4.4793 (1.0347), and speech rate 48.3723 (0.8405). An illustrative example shows OpenSMILE speech rate averaging 5.78 syll/s versus Praat averaging 54.12 syll/s, which the authors deem behaviorally impossible and likely artifact-sensitive. Two-way ANOVAs show several significant gender or diagnostic effects appear only under Praat (e.g., gender effects in mean pitch for ‘Description’/‘Demonstration’; diagnostic effect on speech rate in ‘Cartoons’), implying tool choice can drive apparent demographic differences. Random-forest classification (leave-one-user-out CV) yields highly task- and group-dependent precision/recall/F1; the authors note OpenSMILE generally has worse F1 than Praat and that ASD vs TD recall can diverge strongly by task/tool (e.g., ‘Cartoons’ recall ASD 0.77 vs TD 0.18 under Praat).","Results rely on utterance-level audio produced via automatic speaker diarization; the authors note manual diarization could improve results but is less feasible in practice. The dataset is English-only, so generalization to other languages is unknown. They also acknowledge gender imbalance (21 male, 8 female), noting the broader need for larger, gender-balanced datasets in ML-based speech studies. The comparison is limited to OpenSMILE and Praat, with plans to include other tools (e.g., librosa, ProsodyPro) in future work.","The study evaluates “reliability” largely as cross-tool agreement, but does not establish ground-truth validity (criterion validity) for pitch/loudness/speech-rate estimates, so it cannot determine which tool is correct. The dataset is small (29 participants) and task-specific (ADOS-2 Module 3), which may limit statistical power, robustness of ANOVA interaction findings, and stability of classifier performance estimates. Feature extraction uses default settings, but the paper does not fully quantify sensitivity to key parameter choices (e.g., pitch floor/ceiling, voicing thresholds), which could explain some discrepancies beyond tool identity. Classification uses default random-forest settings and reports metrics per task, but does not deeply analyze uncertainty (confidence intervals) or perform systematic hyperparameter tuning/robustness checks that could affect comparisons between tool-derived feature sets.","The authors plan to address current limitations by using more careful, informed parameter settings when applying open-source tools. They also plan to analyze the psychologist’s speech/behavior during the diagnostic interaction, motivated by prior work showing clinically relevant information from the assessor side. Finally, they aim to move beyond mean aggregation over whole tasks by extracting and analyzing time-series/temporal dynamics of speech features over the course of a task to better capture time-evolving interaction patterns.","A natural extension is to incorporate explicit reliability metrics (e.g., test–retest reliability, intraclass correlation, Bland–Altman analysis) and quantify agreement under controlled perturbations (added noise, diarization errors) to separate tool differences from preprocessing artifacts. Evaluating the impact of alternative diarization/segmentation pipelines and reporting end-to-end uncertainty propagation (from diarization to features to classification) would strengthen reproducibility. Broader validation on larger, multi-site, multi-language datasets with balanced demographics would better assess fairness/generalizability claims. Providing an open, reproducible feature-extraction and evaluation pipeline (scripts/configs for OpenSMILE/Praat settings and preprocessing) would enable systematic benchmarking across tools and parameter settings.",2506.11072v1,https://arxiv.org/pdf/2506.11072v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:00:00Z
FALSE,NA,ML-based|Other,Mixture of types|Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,Python|Other,Not provided,https://graph-tool.skewed.de,"This paper applies the CACTUS explainable classification framework (knowledge-graph based abstraction + graph centrality) to early multi-class classification of age-related macular degeneration (AMD) stages using non-imaging clinical/genetic/dietary/demographic data. CACTUS discretizes continuous variables into “Up/Down flips” via ROC-based thresholding (extended here to exhaustively search across all binary partitions of multiple classes), builds class-specific weighted directed knowledge graphs from conditional probabilities, and classifies patients using three scoring variants: probabilistic (CPB), degree-centrality-weighted (CDG), and PageRank-centrality-weighted (CPR). The study evaluates robustness to missingness by randomly removing 20–80% of values and compares against standard ML baselines (ridge, logistic regression, random forest, SGD, SVM, XGBoost) using balanced accuracy with 10-fold CV and an 80/20 split. CACTUS variants consistently outperform the baselines across fragmentation levels, and the PageRank-based variant (CPR) is selected for its more usable confidence–coverage behavior. The method also produces feature “ranks” (importance via cross-class significance differences) and a per-prediction confidence metric to support clinician-facing interpretability and bias/feature filtering; filtering out biologically irrelevant or highly-missing features preserves accuracy and yields clinically plausible top drivers (e.g., age, supplements, hypercholesterolemia, genetic risk scores, visual acuity decline).","CACTUS builds class-specific graphs where edge weights derive from conditional probabilities between discretized flips, e.g. for flips $A^U$ and $B^U$ in class $c$: $P(B^U\mid A^U,c)=\frac{P(A^U\cap B^U\mid c)}{P(A^U\mid c)}$. Classification computes a class score by summing flip significances across observed features: $C_{c,m}=\sum_{i\in N}\sigma(c,x_i,m)$, then predicts $\arg\max_c C_{c,m}$. Feature Rank for a flip averages pairwise class differences in significance: $R_{m,x_f}=\frac{\sum_{i<j}|\sigma(c_i,m,x_f)-\sigma(c_j,m,x_f)|}{\binom{N}{2}}$, and a feature rank averages over its flips; confidence is the mean absolute margin between the winning class score $C_m$ and others: $\text{Conf}=\frac{1}{N-1}\sum_{i\neq m}|C_m-C_i|$ (normalized to $[0,100]$).","On the “Last visit” cohort (29,908 patients; 218 features; 5 AMD stages), CACTUS achieves higher balanced accuracy than all compared ML baselines across added missingness levels of 0–80%. At 0% added missingness, CPR reaches 0.34±0.01 balanced accuracy vs best baseline around 0.31±0.01 (LR/XGB) and others lower; at 80% added missingness, CPR is 0.27±0.01 vs best baseline 0.25±0.0 (XGB) and others 0.21–0.24. Performance degradation from most complete to most fragmented data is reported as ~4% (CPB) to ~8% (Ridge) absolute loss in balanced accuracy across models. After filtering features with no AMD biological relevance or >50% missingness, balanced accuracy changes by ~0–2% for CACTUS variants (e.g., CPR remains 0.34±0.01); using only the top 9 ranked features yields at most ~5% loss (e.g., CPR 0.29±0.01).","The authors state that the confidence threshold for accepting CACTUS decisions “lacks a formal and objective definition” and depends on subjective criteria such as allowed risk, trust, and cost/availability of additional information or expertise. They also note that some dietary features are under-reported and show noisy/biased distributions, affecting trajectories across AMD stages and implying data-quality limitations. They emphasize that their approach intentionally does not use retinal-image-derived features (except the target label from image grading), which constrains what information the model can leverage compared to imaging-based systems.","The reported balanced accuracies (~0.27–0.34 for 5 classes) are above random (0.20) but still modest for clinical deployment; calibration, class-wise error costs, and clinical utility thresholds are not demonstrated. Missingness is stress-tested via random removal, which may not reflect real missing-not-at-random mechanisms common in healthcare (e.g., severity-dependent measurement). Evaluation details for CACTUS vs baselines may be uneven: baselines (except XGB) use mean imputation and standard scaling, but hyperparameter tuning, probability calibration, and alternative imputation strategies are not described, which can materially affect baseline performance. The method relies on discretization and graph construction choices (ROC-based cutoffs, use of |p−0.5| edge weights) that may be sensitive to data leakage or instability across folds if thresholds/graphs are not strictly fit within training splits (the paper text does not fully specify the nesting).",None stated.,"Validate CACTUS prospectively and across external cohorts/sites, including subgroup analyses to assess fairness and transportability when feature availability differs. Extend robustness analyses to structured missingness (MNAR), temporal shift, and visit-level longitudinal prediction rather than using only the last visit. Provide calibrated confidence/abstention schemes with decision-analytic evaluation (e.g., selective classification, cost-sensitive thresholds) and compare against conformal prediction baselines. Release an open-source implementation and reproducible pipeline (including graph construction within CV folds) to enable broader benchmarking and adoption.",2506.14843v1,https://arxiv.org/pdf/2506.14843v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:00:50Z
TRUE,RUL prediction|Maintenance optimization|Other,ML-based|Other,Sensor/condition monitoring|Other,Condition-based|Predictive,Energy/utilities,Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper qualitatively investigates how predictive maintenance (PdM) and intelligent operation & maintenance (O&M) practices can improve wind power generation efficiency and turbine reliability. Using structured interviews with five wind-farm practitioners and thematic analysis, it reports that existing PdM approaches (often based on SCADA/condition monitoring and AI methods) reduce downtime by identifying major faults, but perform poorly for small, gradual degradations and in adverse weather. Reported implementation barriers include false positives, sensor malfunctions and data quality issues (including SCADA inconsistency), data overload, and difficulties integrating new PdM tools with older turbine models. The study highlights the perceived value of enabling technologies such as digital twins, SCADA-based real-time monitoring, vibration analysis with machine learning, and condition monitoring (e.g., for bearings/gearboxes), while emphasizing the need for better real-time data integration, improved sensors, and AI calibration to reduce false alarms. Overall, the contribution is an industry-informed needs assessment rather than a new reliability model, providing practitioner-identified requirements to improve PdM/O&M effectiveness in wind farms.",Not applicable,"Interviewees report downtime reductions of up to about 20% from current predictive maintenance models, primarily by detecting major faults. However, participants consistently note reduced accuracy for small/gradual faults, increased false alarms leading to unnecessary maintenance, and reduced prediction accuracy for older turbines and during inclement weather. Key operational issues highlighted include unreliable sensor readings, inconsistent SCADA data (especially from remote areas), and information overload that slows decision-making. Advanced approaches (digital twins, SCADA real-time monitoring, vibration/ML for bearing faults, condition monitoring for gearboxes, remote monitoring) are reported to improve scheduling and early detection but still require refinement for robust real-time integration.","The authors state that the study is based on a small sample (five participants), which does not provide a comprehensive view of staff experience in the wind energy business. They also note that, as a qualitative study, it may have external validity/generalizability limitations.","The study does not validate claims (e.g., “~20% downtime reduction”) with operational datasets, quantitative performance metrics (e.g., precision/recall, false-alarm rate), or reliability measures (e.g., MTBF, availability). It discusses PdM, SCADA, digital twins, and AI at a high level but does not specify concrete modeling choices, decision thresholds, or maintenance policy optimization formulations, limiting reproducibility and prescriptive guidance. The evidence base is limited to subjective practitioner reports and may be sensitive to selection bias (only five interviewees) and context-specific practices, without triangulation against maintenance logs/SCADA records.",The authors suggest future research should include larger and more diverse samples and incorporate more quantitative approaches to better generalize findings about predictive maintenance and intelligent O&M in wind power systems.,"A useful extension would be to empirically benchmark PdM approaches on real SCADA/condition-monitoring datasets, reporting standardized detection and false-alarm metrics and linking them to availability and cost outcomes. Future work could also formalize and optimize maintenance decision policies (e.g., condition-based/predictive replacement thresholds) under realistic constraints (weather windows, spare parts, crew logistics) and explicitly address heterogeneous fleets (older vs newer turbines) via transfer learning/domain adaptation and robust sensor-fault handling.",2506.16095v1,https://arxiv.org/pdf/2506.16095v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:01:13Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Other,Not provided,NA,"This paper proposes Reliability-adjusted Prioritized Experience Replay (ReaPER), an extension of Prioritized Experience Replay (PER) for off-policy deep reinforcement learning. It introduces a trajectory-based reliability score for temporal-difference (TD) targets, defined from the cumulative downstream absolute TD-errors, and uses a reliability-adjusted priority (reliability × absolute TD-error) for sampling transitions from a replay buffer. The authors provide theoretical results linking downstream TD-errors to target bias (via a bounding assumption) and argue a convergence hierarchy where ReaPER improves expected Q-value error versus PER and uniform replay; they also derive an optimal variance-minimizing sampling form and relate ReaPER as a proxy under an inverse-variance interpretation. Empirically, ReaPER is evaluated in classic control tasks (CartPole, Acrobot, LunarLander) and Atari-10, showing faster threshold attainment in simpler domains and higher median peak performance in Atari, including under partial observability. Overall, the contribution is a learning-efficiency improvement in RL replay sampling rather than reliability engineering of physical/engineered systems.","The reliability score for a transition at time t in a terminated episode is defined as $R_t = 1 - \frac{\sum_{i=t+1}^n \delta_i^+}{\sum_{i=1}^n \delta_i^+}$, where $\delta_t^+=|\delta_t|$ is the absolute TD-error. The reliability-adjusted sampling criterion is $\Psi_t = R_t\,\delta_t^+$, and in the implemented scheme it is regularized as $\Psi_t = R_t^{\omega}(\delta_t^+)^{\alpha}$, with priorities $p_t=\Psi_t/\sum_i \Psi_i$ and importance-sampling weights $w_t=((N p_t)^{-\beta})/\max_i w_i$. For ongoing episodes, reliability is conservatively estimated using the maximum episodic TD-error sum $F$ in the buffer (Eq. 16–17 in the paper).","Across 20 runs in classic control tasks, ReaPER reached predefined score thresholds faster than PER by 16.6% (Acrobot), 32.6% (CartPole), and 21.1% (LunarLander), and faster than uniform replay by 25.0%, 41.4%, and 37.1% respectively. On the Atari-10 benchmark, ReaPER achieved a 22.97% higher median peak score than PER and a 229.78% higher median peak score than uniform replay, outperforming PER in 8/10 games (2 ties). Under a deliberately partially observable Atari-10 variant (single-frame observations), the median improvement over PER increased to 34.98% (reported as ~34.97% in the appendix figure text). Theoretical claims include a convergence hierarchy (Uniform ≤ PER ≤ ReaPER in expected squared Q-error under the stated assumption) and a variance-minimizing sampling distribution proportional to $\delta_t^+/\sigma_t^2$ for target variance $\sigma_t^2$.","The authors state that ReaPER relies on terminal states as a prerequisite for computing meaningful TD-error reliabilities. They also note computational overhead because ReaPER tracks episodic cumulative sums of TD-errors to compute reliability, making naive updates $O(N)$; they suggest it can be reduced to $O(n-t)$ by recalculating within an episode only when a transition (or a preceding transition) is updated.","The method assumes episodes/trajectory boundaries are identifiable and stored correctly in the replay buffer; in environments with very long episodes, truncation, or streaming buffers, reliability computation and bookkeeping may become fragile or biased. The theoretical guarantees depend on Assumption 3.4 (bias bounded by downstream absolute TD-errors) and are not proven under more general function-approximation/nonstationarity settings common in deep RL; violations could affect prioritization quality. Empirical evaluation is limited to DQN-style off-policy value-based learning; performance and stability for actor–critic methods, continuous-action settings, and offline RL are not established. The paper does not present publicly available code, which limits reproducibility and independent validation of implementation details and computational overhead claims.","The authors suggest exploring adaptive reliability estimation, extending ReaPER to actor-critic methods and infinite-horizon settings, and integrating reliability concepts with representation learning. They also mention that further gains may be achievable with more extensive hyperparameter tuning (e.g., $\alpha$, $\omega$, $\beta$, learning rate).","Develop self-starting/streaming variants that estimate reliability without requiring terminal transitions (e.g., using truncated horizons, bootstrapped uncertainty, or episodic-return estimators) to broaden applicability to continuing tasks. Provide robust variants that account for nonstationarity and autocorrelation in TD-errors (e.g., smoothing, confidence intervals, or Bayesian/ensemble uncertainty) and analyze sensitivity to episode length and buffer composition. Extend evaluation to offline RL and high-dimensional continuous control (e.g., SAC/TD3) with standardized benchmarks and ablations isolating the reliability term from priority exponents/IS corrections. Release reference implementations and profiling results (time/memory) to quantify overhead and enable adoption.",2506.18482v3,https://arxiv.org/pdf/2506.18482v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:01:42Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Bayesian|Physics-based|Other",Right-censored|Interval-censored|Degradation measurements|Mixture of types|Simulated only,Not applicable,Semiconductor/electronics|Theoretical/simulation only,Simulation study|Other,TRUE,R|Other,Public repository (GitHub/GitLab),https://github.com/federico-m-stefanini/rel4dev,"The paper proposes a Bayesian parametric Structural Causal Model (SCM) to analyze electronic-device reliability using electrical-resistance degradation measured sparsely over long operating periods, which induces interval- and right-censored failure times defined by a device-specific 10% resistance-increase threshold. The SCM uses causal DAGs to integrate observational no-stress (NS) data with randomized accelerated-stress (AS) experimental data, enabling estimation of causal effects of device configuration factors (surface finish, component type, pin count) and humidity, plus counterfactual predictions (e.g., what resistance or failure time would be under alternative times/conditions). A piecewise time model is used: a linear component (present in both regimes with a regime-scaling factor) and an additional nonlinear component activated only under AS after a knot time. Posterior inference is performed via MCMC (Stan via rstan), and synthetic datasets for both regimes illustrate recovery of true parameters and computation of causal estimands and reliability (survival) functions. The work advances reliability modeling by framing degradation/failure-time inference in a causal, transportable framework that can combine biased observational sources with designed experiments and support counterfactual reasoning.","Electrical resistance is modeled at baseline as $y_{0,j}=\mu_0+\sum_r\alpha_{S,r}I(x_{S,j}=r)+\sum_r\alpha_{T,r}I(x_{T,j}=r)+\sum_r\alpha_{P,r}I(x_{P,j}=r)+u_{0,j}$ with $u_{0,j}\sim\mathcal N(0,\sigma_0^2)$. For $t\in\{1,2,3\}$, degradation is $y_{t,j}=y_{0,j}+\big(\beta_1+\delta_{1,S}[x_S]+\delta_{1,T}[x_T]+\delta_{1,P}[x_P]+\delta_{1,H}[x_H]\big)\cdot w_t\cdot\big(I(A=a_2)+\gamma^{-1}I(A=a_1)\big)+\big(\beta_2+\delta_{2,S}[x_S]+\delta_{2,T}[x_T]+\delta_{2,P}[x_P]+\delta_{2,H}[x_H]\big)\cdot(w_t-\psi)^\tau I(w_t>\psi)I(A=a_2)+u_{t,j}$ with $u_{t,j}\sim\mathcal N(0,\sigma_Y^2)$. Failure is defined by the device-specific threshold $Y_t\ge 1.1\,Y_0$; reliability for a fixed configuration is expressed as $R(t)=P(Y_t<1.1Y_0)=\int_{-\infty}^0 \mathcal N(q\mid \mu_D(t),\sigma_D^2)\,dq$ for the modeled difference $D=Y_t-1.1Y_0$.","Using synthetic data with $n_{AS}=1024$ and $n_{NS}=2048$ (per time point) and fixed expert parameters $(\psi,\tau,\gamma)=(2.0,3,10)$, posterior estimates closely match true values; e.g., for AS regime surface-finish slopes $\delta_{1,S,1..4}$ true values $(-0.7,-0.5,0.5,0.7)$ yield posterior means about $(-0.702,-0.493,0.517,0.678)$ with posterior SD $\approx0.024$ and tight 95% HDIs. Example AS estimand $\Delta_1=E[Y_t-Y_0\mid do(x),w_t,a_2]$ for configuration (1,1,1,1) increases from 7.330 at $w_t=0.72$ kh to 163.581 at 3.60 kh, while (2,1,1,1) yields 7.481 to 205.337 over the same times. The one-factor surface-finish contrast $\Delta_S(1\rightarrow2)$ grows with time/nonlinearity, from 0.151 at 0.72 kh to 41.756 at 3.60 kh. In NS predictive failure-time comparisons, interventions do($x_S{=}1,x_T{=}1,x_P{=}4$) vs do($x_S{=}3,x_T{=}3,x_P{=}3$) produce well-separated failure-time distributions, with 90% credible intervals (60.322, 60.913) vs (54.440, 54.860) kilohours (non-overlapping).","The authors note the paper targets the “simplest non-trivial context,” and does not attempt a comprehensive coverage of recent counterfactual/causal methodology. They state that marginal independence of exogenous variables is assumed based on expert judgment and may fail in more structured settings unless the model is extended. They also mention that treating observation times as known (and fixing $\psi$ and $\tau$) may be inadequate in contexts where deviations from nominal times or uncertainty in these parameters materially affect inference.","Results are demonstrated only on synthetic data; there is no real electronics dataset validation, so practical robustness to real measurement error, missingness mechanisms, and unmodeled covariate shift is untested. The degradation model uses Gaussian additive noise and a specific piecewise functional form; sensitivity to non-normal errors, heteroscedasticity, autocorrelation in repeated measurements, and alternative threshold definitions is not explored. The mapping between NS and AS regimes is imposed via a fixed acceleration factor $\gamma$ and AS-only nonlinearity; estimating $\gamma$ from data or validating transportability across regimes is not demonstrated. Implementation details for identifiability and prior sensitivity (e.g., sum-to-zero constraints, prior scales) and their impact on causal estimands are not thoroughly stress-tested.","They propose extending the framework to settings with multiple production facilities/batches where transportability is required, especially when both NS and AS data sources are available. They suggest adding interaction terms between factors for more structured contexts. They also note that $\psi$ and $\tau$ could be treated as uncertain (epistemic uncertainty) and that additional observation times should be planned to reduce that uncertainty. Finally, they indicate the marginal-independence assumption for exogenous variables may need relaxation in richer settings via model extensions.","A natural extension is to fit and validate the SCM on real device fleets and controlled experiments, including explicit modeling of measurement/disassembly-induced missingness and inspection schedules to better handle informative censoring. Developing robust/self-starting variants that handle unknown regime mappings (estimating $\gamma$) and allow non-Gaussian, autocorrelated, or heteroscedastic degradation errors would improve generalizability. Extending to hierarchical/partial-pooling structures (e.g., facility, lot, or supplier random effects) would address between-batch variability while supporting transportability. Packaging the Stan/R implementation (with reproducible scripts and diagnostics) and providing calibration tools for practitioners (posterior predictive checks for censoring, causal estimand uncertainty reports) would enhance practical adoption.",2506.18663v1,https://arxiv.org/pdf/2506.18663v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:02:26Z
TRUE,Failure mode analysis|System reliability|Other,Stochastic process|Bayesian|Simulation-based|Other,Degradation measurements|Mixture of types|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB,In text/Appendix,https://claude.ai/new|https://www.baidu.com|https://gemini.google.com/app|https://grok-ai.app/|https://llama.meta.com/|https://openai.com/blog/gpt-3-apps/|https://www.sensetime.com,"This paper investigates whether a large language model (ChatGPT) can automatically generate MATLAB implementations of classical geotechnical reliability algorithms and produce correct numerical results. Four representative methods are tested: First Order Reliability Method (FORM/AFORM as a constrained optimization problem with correlated variables and nonlinear limit states), Subset Simulation for rare-event failure probability estimation, 2D random-field simulation with exponential correlation (via covariance-matrix decomposition under separable correlation), and Bayesian updating for multivariate normal models using Gibbs sampling with missing-data imputation. The generated codes are benchmarked against crude Monte Carlo simulation (for FORM and Subset Simulation), an independent random-field generator from the literature (for correlation-length verification), and an independently available implementation for Bayesian updating. The authors report that Subset Simulation and Gibbs-sampling Bayesian updating code were produced correctly in one attempt, while FORM and random-field coding required prompt refinement and sometimes restarting after failures. The study concludes that LLMs can accelerate reliability programming in geotechnical engineering but still require domain-expert verification and careful prompting to avoid incorrect implementations.","FORM is summarized by the transformation $U_i=\Phi^{-1}(F_{X_i}(x_i))$, design point search $u^*=\arg\min\{\|u\|: g(u)=0\}$, reliability index $\beta=\|u^*\|$, and failure probability approximation $P_f\approx\Phi(-\beta)$. Subset Simulation factorizes the rare-event probability as $P_f=P(F_1)\prod_{i=2}^m P(F_i\mid F_{i-1})$ with intermediate events $F_i=\{g(X)\le y_i\}$ chosen so that $P(F_i\mid F_{i-1})\approx p_0$ (often 0.1). Random-field generation uses $Z(x)=\mu(x)+\sigma(x)Y(x)$ with exponential correlation $\rho(\tau)=\exp(-2\tau/\theta)$ and covariance decomposition $C=LL^\top$, $Z=\mu+LY$. Bayesian updating uses Bayes’ rule $f(\Theta\mid Y,M)\propto f(Y\mid\Theta,M)f(\Theta\mid M)$ and posterior predictive integration $f(y_{\text{new}}\mid Y)=\int f(y_{\text{new}}\mid\Theta)f(\Theta\mid Y)\,d\Theta$, implemented via Gibbs sampling with conditional MVN updates for missing entries.","For the FORM example shown, the LLM-generated MATLAB code produced $\beta=2.74$ and $P_f=3.07\times 10^{-3}$ in 6 iterations; crude MCS with $10^7$ samples gave $P_f=2.87\times 10^{-3}$ and $\beta=2.76$. With modified means $\mu=[8,12]$, FORM gave $\beta=3.56$ and $P_f=1.83\times 10^{-4}$ (7 iterations), while MCS ($10^7$) gave $P_f=1.73\times 10^{-4}$ and $\beta=3.58$. For Subset Simulation on a circular limit state, the paper reports an estimated $P_f\approx 3.14\times 10^{-4}$ versus MCS ($10^8$) $P_f\approx 3.36\times 10^{-4}$, and a correlated/parabolic case giving $P_f\approx 2.35\times 10^{-3}$ versus MCS $2.36\times 10^{-3}$. A failed FORM code example yielded $\beta=0$ and $P_f=0.5$ when MCS indicated $P_f\approx 0.18$ (and after changing means, MCS $P_f\approx 0.0035$ while the faulty code output did not change), illustrating prompting sensitivity. Random-field simulations were checked by recovering prescribed correlation lengths and comparing to a benchmark generator; Bayesian Gibbs sampling recovered means/variances/correlations in a 5D synthetic example and produced missing-value imputations close to an independently provided implementation.","The authors note that LLM-generated reliability codes can be incorrect (they document a failed FORM example) and may not improve even after multiple iterative prompts; they recommend restarting with a fresh prompt when stuck. They also highlight the practical difficulty and time cost of verifying whether generated code is correct, which creates a paradox because non-experts still need domain knowledge to validate results. For an FFT-based random-field code produced by the LLM, they state they could not satisfactorily conclude correctness, motivating a revised prompt to force an alternative method (covariance decomposition). Overall they emphasize the approach is only “semi-automated” because verification remains necessary.","The evaluation is limited to a small set of illustrative problems (mostly low-dimensional and synthetic), so the conclusions may not generalize to higher-dimensional reliability problems, complex geotechnical FEM limit states, or industrial-scale workflows. Comparisons are primarily against crude MCS and one benchmark generator; there is no systematic comparison to established reliability software/toolboxes or to multiple LLMs under controlled conditions. The paper focuses on correctness/feasibility but does not quantify development time savings, robustness to prompt variation, or reproducibility across LLM versions. Some generated MATLAB listings rely on specialized functions (e.g., multivariate normal pdf, inverse-Wishart sampling) without discussing toolbox requirements, numerical stability, or edge cases (non-PD covariance, strong correlations, multimodal posteriors).","The paper suggests exploring other recent generative AI models beyond ChatGPT (e.g., Claude, Gemini, Llama, SenseChat, Ernie Bot, Grok) for reliability-programming tasks. It also implies that improved prompting strategies (beyond zero-shot) and better training on high-quality reliability code repositories could yield more robust and better-documented implementations. The discussion emphasizes continued work on leveraging LLMs to lower barriers to adopting reliability methods in routine civil/geotechnical engineering while acknowledging the need for verification.","A natural extension is a controlled benchmarking protocol: multiple LLMs/versions, standardized prompt sets, and a suite of reliability tasks (FORM/SORM variants, importance sampling, SS variants, RBDO, system reliability) with quantitative success rates and error metrics. Developing self-checking or unit-tested code templates (and requiring LLM outputs to include tests) would reduce verification burden and improve reliability of generated implementations. Extending to realistic geotechnical applications (e.g., slope stability with spatially variable strength via random fields coupled to numerical solvers) would test scalability and integration challenges. Finally, investigating robustness to non-normal inputs, dependence structures beyond Gaussian correlation, and autocorrelated/spatially irregular data would better match practical geotechnical uncertainty modeling.",2506.19536v1,https://arxiv.org/pdf/2506.19536v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:03:13Z
TRUE,System reliability|Other,"Simulation-based|Parametric (Weibull, etc.)|Stochastic process|Other",Mixture of types|Simulated only|Other,Not applicable,Energy/utilities|Transportation/logistics|Environmental monitoring|Other,Simulation study|Other,TRUE,MATLAB,Not provided,NA,"The paper addresses reliability analysis for nondeterministic (stochastic) simulators where repeated runs at identical inputs produce random outputs, and defines failure via a stochastic limit-state function with latent variables. It proposes AL-SPCE, an active-learning framework built around stochastic polynomial chaos expansions (SPCE) to estimate failure probability efficiently without requiring replicated simulator runs. The method introduces a learning function that selects new training points by maximizing the product of the input density and the local predictive uncertainty of the estimated conditional failure probability. Since SPCE lacks built-in local uncertainty, the authors derive an analytical epistemic-uncertainty approximation by using asymptotic normality of the MLE for SPCE coefficients via the Fisher information (Hessian of the log-likelihood), then propagating coefficient samples to a variance estimate for the conditional failure probability. The approach is demonstrated on (i) an analytically tractable stochastic R–S structural reliability problem, (ii) a stochastic agent-based SIR epidemic model, and (iii) a wind-turbine extreme-load dataset, showing faster convergence and reduced variance versus static-design SPCE and direct Monte Carlo simulation while retaining accurate failure probability estimates.","A stochastic simulator is written as $g_s(x,\omega)=g(x,Z(\omega))$ and failure probability is $P_f=\mathbb{P}(g(X,Z)\le 0)=\mathbb{E}[\mathbf{1}_{D_f}(X,Z)]$. The conditional failure probability is $s(x)=\mathbb{P}_Z(g(x,Z)\le 0)$ and the total failure probability becomes $P_f=\mathbb{E}_X[s(X)]$. SPCE models $Y_x$ by $\hat Y_x=\sum_{\alpha\in A} c_\alpha\,\psi_\alpha(x,U)+\varepsilon$ with $\varepsilon\sim\mathcal N(0,\sigma_\varepsilon^2)$ and yields an analytic/quadrature approximation $\hat s(x)\approx\sum_{j=1}^{N_Q} w^{(j)}\,\Phi\!","On the stochastic R–S example with analytical reference $P_f=3.154\times 10^{-3}$, AL-SPCE achieves accurate estimates with as few as $N=50$ training points (median $\hat P_f=3.193\times 10^{-3}$, CoV 16.7%), while a static-design SPCE is biased/variable at $N=50$ (median $1.958\times 10^{-3}$, CoV 67.9%). For the stochastic SIR model with reference $P_f=7.511\times 10^{-4}$ (from $10^8$ MCS), AL-SPCE provides stable estimates by $N\approx 250$–800 (e.g., median $6.399\times 10^{-4}$ at $N=250$, CoV 11.1%), whereas static SPCE is much more variable (CoV 69.0% at $N=250$). For the wind-turbine extreme-load dataset (reference $P_f=1.022\times 10^{-2}$), AL-SPCE is close to reference by $N=250$ (median $1.040\times 10^{-2}$) and improves further by $N=1000$ (median $1.014\times 10^{-2}$, CoV 10.6%), outperforming both static-design SPCE and direct MCS at the same sample sizes.","The authors state that defining a robust stopping criterion for the active learning iterations remains an open issue because convergence curves are noisy and not smooth enough for conventional stopping rules. They also note that part of the noise comes from the optimization used to train SPCE (not only from simulator randomness), and although they damp it with a warm-start strategy for the SPCE noise term, fluctuations remain.","The uncertainty quantification for SPCE relies on asymptotic normality of the MLE and a local quadratic (Fisher-information/Hessian) approximation, which may be inaccurate for small-to-moderate experimental designs, ill-conditioned bases, or non-identifiable settings. The learning criterion uses $\mathrm{Var}[\hat s(x)]$ derived from coefficient uncertainty but does not explicitly separate epistemic vs. aleatory contributions nor provide calibrated predictive intervals for $s(x)$, so point-selection may be misled in strongly non-Gaussian/posterior-multimodal regimes. Performance is largely shown via Monte Carlo-style empirical comparisons on three examples; broader stress tests (higher-dimensional inputs, dependent inputs, multiple failure modes, extremely small $P_f$) and sensitivity to design choices (candidate-set size, batch size, quadrature points, truncation/degree adaptivity) are not fully characterized.","They suggest that a more principled solution is to reformulate SPCE in a Bayesian setting, which would provide embedded local error measures and meaningful regularization, and is expected to help address the stopping-criterion problem. They also reiterate that developing an appropriate stopping criterion is an open challenge that could further reduce computational cost.","Develop self-starting or sequential Bayesian decision rules for stopping (e.g., based on posterior uncertainty in $P_f$ or expected information gain) rather than smoothing heuristics. Extend AL-SPCE to handle dependent or non-standard input distributions and to multi-dimensional latent representations when a single artificial latent variable is insufficient for complex stochastic responses. Add robustness studies and diagnostics (e.g., model-misspecification checks for SPCE basis/truncation, stability of Hessian/Fisher estimates) and release reference implementations to facilitate adoption and reproducibility.",2507.04553v1,https://arxiv.org/pdf/2507.04553v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:04:07Z
FALSE,NA,Nonparametric/Semi-parametric|Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,https://doi.org/10.1007/978-1-4612-3938-3_1|https://arxiv.org/abs/physics/0102002,"The paper proposes the BdryMatérn Gaussian process (GP), a surrogate modeling framework that integrates boundary information (Dirichlet, Neumann, and Robin conditions) on irregular, connected domains whose boundaries are twice-differentiable almost everywhere. The authors derive new boundary-aware Matérn-type covariance kernels via an SPDE formulation and a Feynman–Kac path-integral representation, enabling explicit enforcement of boundary conditions while retaining Matérn-style smoothness control through the parameter \(\nu\). They prove that sample paths from the resulting GP satisfy the prescribed boundary conditions almost surely and are \((\lceil\nu\rceil-1)\)-times differentiable almost surely. For practical computation on irregular domains, they develop a finite-element-method (FEM) kernel approximation using a coupled Monte Carlo estimator that preserves positive definiteness, together with rigorous \(L_2\) kernel approximation error bounds and rates (including a B-spline implementation). Numerical experiments on multiple irregular 2D domains and a tensor-product closed-form construction on \([0,1]^{30}\) show improved predictive accuracy and reduced uncertainty compared with standard Matérn GPs without boundary integration, under limited training data.","The proposed model defines a boundary-integrated covariance kernel \(k_{\nu,B}(x,x')\) by solving an SPDE with boundary constraints, leading to path-integral forms. For Dirichlet boundaries, the kernel is expressed as a Matérn-(\(\nu+2\)) term minus/plus Brownian hitting-time expectation corrections (Eq. (12)), where \(\tau\) and \(\gamma\) are boundary hitting times of independent Brownian motions started at \(x\) and \(x'\). For Robin (including Neumann as \(c\equiv 0\)) boundaries, the corrections involve integrals over boundary local time \(L_t\) (Eq. (27)). For computation, an FEM approximation is \(\hat k_{m,Q}(x,x')=\Phi(x)^T\,[\Phi(U)]^{\dagger}\hat K^C(U,U)[\Phi(U)^T]^{\dagger}\,\Phi(x')\) (Eq. (46)), using a coupled Monte Carlo kernel-matrix estimator \(\hat K^C\) to preserve positive definiteness.","The paper proves almost-sure boundary satisfaction and Matérn-like smoothness control: sample paths from the BdryMatérn GP satisfy the imposed Dirichlet or Robin boundary conditions almost surely and are \((\lceil\nu\rceil-1)\)-differentiable almost surely (Theorems 3.2 and 3.4). It establishes that the coupled Monte Carlo kernel-matrix estimators for both Dirichlet and Robin cases are positive definite almost surely (Theorems 4.1 and 4.2), enabling valid GP posterior computations. It provides an \(L_2\) kernel approximation error bound for the FEM estimator (Theorem 4.3) and, with B-spline bases, an explicit convergence rate \(\|\hat k_{m,Q}-k_{\nu,B}\|_{L_2}=O\big(m^{-\nu/(2\nu+d)}\big)\) under stated conditions (Theorem 4.4). In simulation experiments, BdryMatérn GPs consistently reduce log-MSE versus standard Matérn GPs across multiple irregular domains and in a 30D tensor-kernel setting, and also reduce posterior predictive uncertainty (as visualized in reported figures).","The authors note that for general irregular domains the BdryMatérn kernel cannot be obtained in closed form and requires careful approximation (FEM + Monte Carlo) for computation. They also state a restriction \(\nu>2\) is needed for the Feynman–Kac representations used to derive the path-integral kernel forms. They mention potential curse-of-dimensionality issues for high-dimensional settings with the general approximation approach, motivating the separate tensor closed-form construction for hypercubes.","The method’s practical performance depends on accurately simulating Brownian boundary hitting times/locations and (for Robin) boundary local times; numerical bias/variance from these stochastic simulations could be significant in complex geometries but is not benchmarked in depth. The approach assumes a connected domain with boundary twice-differentiable almost everywhere; many engineering geometries have corners, cusps, or piecewise-smooth boundaries where these assumptions may fail or require special treatment. Hyperparameter learning is done via maximum likelihood in experiments, but the interaction between approximation error (FEM/MC) and likelihood-based estimation (bias/identifiability) is not fully analyzed.","They propose studying posterior contraction rates of the BdryMatérn GP on irregular domains to quantify how boundary information improves predictive rates. They also suggest investigating consistency and contraction rates under inference and potential misspecification of model parameters. Finally, they propose developing new space-filling experimental designs that incorporate boundary information on irregular domains.","Developing robust/self-starting implementations that handle non-smooth (piecewise-smooth) boundaries—e.g., polygonal domains with corners—would broaden applicability and align with common meshed engineering geometries. Extending the framework to noisy simulators/field data (beyond a nugget mention) and to multi-output PDE solutions (vector-valued GPs) would improve practical relevance. Providing an open-source reference implementation (e.g., MATLAB/Python) and reproducibility artifacts (meshes, random seeds, Brownian simulation routines) would materially accelerate adoption and independent validation.",2507.09178v1,https://arxiv.org/pdf/2507.09178v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:04:43Z
TRUE,Degradation modeling|RUL prediction|Accelerated testing|Maintenance optimization|Warranty analysis|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Stochastic process|Bayesian|Nonparametric/Semi-parametric|Hybrid/Ensemble|Other",Degradation measurements|Sensor/condition monitoring|Mixture of types|Other,Condition-based|Predictive|Not applicable,Semiconductor/electronics|Manufacturing (general)|Energy/utilities|Transportation/logistics|Pharmaceutical|Other,Case study (real dataset)|Simulation study|Other,TRUE,R|Other,Not provided,https://github.com/WarrRich/Virkler-Data|https://infrastructurereportcard.org|https://www.reliasoft.com/,"This paper is an accessible tutorial/review for quality engineers on degradation models for reliability assessment, covering degradation data types (repeated-measures degradation testing/RMDT, accelerated destructive degradation testing/ADDT, and field data with dynamic covariates) and how a degradation path plus a failure threshold induces a failure-time distribution. It reviews two major modeling families: general path models (mixed-effects parametric paths, including linear, Paris-law crack growth, and log-logistic-type curves) and stochastic process models (Wiener, gamma, and inverse Gaussian processes) with corresponding failure-time distributions via first-hitting-time formulations. Inference procedures are summarized for both maximum-likelihood (e.g., nlme-based mixed-effects fitting) and Bayesian hierarchical approaches (e.g., MCMC via Stan) with outputs including reliability CDF estimation, confidence/credible intervals, and remaining useful life (RUL) prediction. Multiple real-data case studies span electronics (laser, RF Device B), coatings/weathering (including dynamic covariates), polymer adhesive strength (ADDT), metal fatigue crack growth, and road infrastructure RUL; the paper also discusses practical considerations such as extrapolation risk, sensitivity analysis, and available commercial/open-source software. Overall, it consolidates best practices and modeling choices to help practitioners select, fit, and interpret degradation models for reliability prediction and maintenance/warranty decisions.","Failure time is defined by threshold crossing $T=\min\{t: D(t)\ge D_0\}$. General path models use $y(t)=D(t)+\epsilon(t)$ (or mixed-effects $y_{ij}=D_i(t_{ij};\alpha,\beta_i)+\epsilon_{ij}$ with $\beta_i\sim N(\mu_\beta,\Sigma_\beta)$) implying a failure-time CDF $F(t)=\Pr[D(t;\alpha,\beta)\ge D_0]$. Stochastic-process alternatives (Wiener, gamma, IG) leverage independent increments and yield closed-form/known CDF expressions for first-hitting times (e.g., transformed IG for Wiener; $F(t)=1-G(D_0;\mu(t),\sigma)$ for gamma; $F(t)=1-\mathrm{IG}(D_0;\mu(t),\sigma\mu(t)^2)$ for IG).","The paper illustrates fitted reliability CDFs with pointwise confidence/credible intervals for multiple datasets (e.g., laser and outdoor weathering with 90% bootstrap CIs; Device B CDF at 80°C with approximate 80% and 90% pointwise CIs; coating example with pointwise 95% Bayesian credible intervals). It presents an RUL distribution example (coating data) showing how posterior uncertainty propagates into predictive maintenance quantities. For polymer thermal index estimation (Adhesive Bond B), it reports that parametric and semiparametric methods yield similar TI estimates (about 33°C and 34°C, respectively) and are recommended over the traditional method in the referenced comparison. For Florida road infrastructure (referenced case study), sequential Bayesian updating is shown to reduce posterior uncertainty and improve precision of RUL predictions relative to no sequential updating (qualitatively illustrated).","The authors emphasize that degradation analyses often require extrapolation over time and/or accelerating variables, and that good in-sample fit does not guarantee accurate extrapolated predictions; they warn about overfitting and recommend sensitivity analysis to assess robustness. They also note software availability as a practical constraint and state that substantial effort is still needed to create more accessible and comprehensive software covering the breadth of degradation models in the literature.","As a tutorial/review, the work largely relies on illustrative case studies and previously published datasets, so comparative performance claims across model classes (GPM vs. stochastic-process vs. semiparametric) are not established via a systematic benchmark with consistent tuning and metrics. Many presented formulations assume monotonic degradation and often Gaussian measurement error/normal random effects, which may be violated in practice (e.g., non-monotone signals, heavy tails, autocorrelation, and heteroscedasticity from sensors). The paper discusses multiple software tools but does not provide a unified, reproducible analysis script for the full set of examples (beyond one linked GitHub example), limiting immediate replicability across all case studies.","They highlight several emerging directions: multivariate degradation modeling leveraging correlation among multiple degradation characteristics; functional degradation data (curve-valued measurements); increased use of sensor data methods (e.g., clustering and degradation index construction); and growing use of AI/ML including physics-informed ML for degradation prediction. They also point to expanding Bayesian methods for general path models and identify reliability modeling for AI systems (including performance degradation of AI over time) as an important future research avenue.","Developing robust/self-starting degradation models that explicitly handle non-monotone behavior, autocorrelated sensor noise, and distributional misspecification (robust likelihoods, state-space formulations) would strengthen field deployment. A standardized, open benchmarking suite (datasets + evaluation protocol) comparing GPMs, stochastic-process models, and modern ML/hybrid approaches across extrapolation regimes would clarify when each method is preferable. More integrated decision-analytic links between degradation-based RUL posteriors and optimal maintenance/warranty policies (including cost and imperfect maintenance) would improve operational impact, ideally with open-source reference implementations.",2507.14666v2,https://arxiv.org/pdf/2507.14666v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:05:21Z
TRUE,Degradation modeling|Maintenance optimization|Other,ML-based|Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Semiconductor/electronics|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/llguo95/COMPAS_simulation,"The paper proposes an adaptive Bayesian optimization (BO) framework for reliability-driven design when the objective function is expensive to evaluate, motivated by solder joint reliability under thermomechanical loading. The method adaptively selects Gaussian-process surrogate hyperparameters (kernel choice and restricted hyperparameter domains via a train/test scoring procedure using RelMSE and test log-likelihood) and can also adaptively select among multiple acquisition functions, optionally filtering overly exploitative candidates using a distance-based exploitation score. Performance is benchmarked via simulations on synthetic objectives (Sphere-6D and AlpineN2-3D) and by an engineering case study that minimizes accumulated nonlinear creep strain (a key solder-joint fatigue indicator) computed from FEM under cyclic thermal loading. Reported results show adaptive BO improves objective values versus fixed-hyperparameter BO, with about ~3% average improvement at a given budget and enabling similar optimization quality with roughly half the expensive FEM evaluations. Code and data-workflow implementations are released open-source for reproducibility.","The surrogate is a Gaussian process with kernels (RBF, Matérn-3/2, Rational Quadratic) parameterized by $\theta$ (Eq. 1), yielding posterior predictive mean and variance $\mu_\theta(x)=k_\theta(x,X)^\top K_\theta^{-1}y$ and $\sigma^2_\theta(x)=k_\theta(x,x)-k_\theta(x,X)^\top K_\theta^{-1}k_\theta(x,X)$ (Eqs. 4–5). Hyperparameters are fit by maximum likelihood, $\hat\theta_{\text{MLE}}=\arg\min_\theta \log\det K_\theta + y^\top K_\theta^{-1}y$ (Eq. 6), optionally over restricted likelihood domains. Acquisition functions include EI/PI/UCB (Eq. 7), and input-adaptive selection filters candidates using an exploitation score $\mathrm{ES}(x,X)=\ln(\mathrm{MMD}(X)/d_{\min}(x,X))$ (Eq. 18) before selecting a candidate for expensive evaluation.","In the solder-joint FEM case study (100 objective evaluations budget), adaptive BO (BO-GPi-Ada / BO-GPi-iAda) achieves lower accumulated creep strain than standard BO with fixed kernel/acquisition. For CTE1 = 6 ppm/°C, BO-GPi-Ada reaches 0.199% accumulated creep vs 0.205% for the best vanilla BO shown (≈2.9% lower). For CTE1 = 8.5 ppm/°C, adaptive BO reaches 0.106% vs 0.107% for the best vanilla BO shown (≈0.7% lower), while the average improvement across iterations is reported as ~3.1%. The authors also state the adaptive schemes can achieve comparable results with ≥50 fewer expensive objective evaluations (about half of the 100-evaluation budget) in some settings, implying major computational savings given 1.5–2 hours per FEM evaluation.","The authors note that the true global minimum of accumulated creep strain over the design space is unknown, so they cannot definitively conclude the synthetic-function performance guarantees fully carry over to the solder-joint case. They also acknowledge the FEM optimization runs were limited to 100 simulations, and more iterations might yield better optima and clarify cost-efficiency. Finally, they state the synthetic benchmarking basis is limited (only two synthetic functions), suggesting broader testing is needed for stronger statistical insight.","The method treats the reliability objective as a single scalar (accumulated creep strain) and does not propagate uncertainty from material variability, manufacturing variability, or model-form error in the FEM; improvements are therefore “design-point” dependent rather than population reliability guarantees. Evaluation of the engineering case study is based on simulations only, without experimental validation of solder fatigue life or field failure data, which limits real-world reliability claims. The adaptive framework adds nontrivial overhead and tuning choices (e.g., thresholds for GP initialization, ES threshold schedule, candidate selection), and robustness of conclusions to these meta-choices is not comprehensively analyzed.","The authors propose running longer optimization campaigns (more than 100 FEM simulations) to potentially find better optima and underline adaptive BO cost-efficiency more clearly. They suggest expanding synthetic benchmarking beyond two functions to gain stronger statistical insight, including studying performance on objective classes with traits like convexity. They also propose extending to different/more complex FEM designs and physics, such as richer molding-compound and solder material models, additional strain metrics, and adding geometric parameters (e.g., solder standoff height and molding-compound dimensions).","A valuable extension would be to connect minimized accumulated creep strain to probabilistic life prediction (e.g., fatigue life distributions) so the optimization targets reliability metrics (e.g., quantiles of life, failure probability) rather than a proxy strain measure alone. Incorporating uncertainty-aware BO (noisy/multi-fidelity GP models, Bayesian model averaging over kernels, and propagation of parameter/manufacturing variability) could yield designs that are robust in a reliability sense. Providing a packaged, documented software implementation (e.g., a pip-installable library) and reporting ablation studies (kernel-adaptation only vs acquisition-adaptation only vs both) on additional real electronics reliability problems would improve adoption and generalizability.",2507.19663v1,https://arxiv.org/pdf/2507.19663v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:05:57Z
FALSE,Other,ML-based|Nonparametric/Semi-parametric|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Network/cybersecurity|Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes RCPS-CPPI, a cross-validation-based calibration method for constructing risk-controlling prediction sets (RCPS) for wireless indoor localization using RSSI fingerprints. It integrates Cross-Prediction-Powered Inference (CPPI) with RCPS to leverage abundant unlabeled data with pseudo-labels while maintaining distribution-free statistical validity via bias correction. The approach uses K-fold cross-validation so all labeled calibration data contribute both to fine-tuning the auxiliary pseudo-labeling model and to estimating its bias, yielding an unbiased risk estimator and an upper confidence bound that provides an (α, δ)-reliability guarantee. Experiments on the UJIIndoorLoc WiFi fingerprinting dataset show the method achieves the target 90% coverage while producing substantially smaller prediction sets (about 30% smaller radius than RCPS at n=50 labeled calibration points) and improved robustness to pseudo-label model quality compared with semi-supervised baselines and RCPS-PPI. Overall, the work advances uncertainty quantification for wireless localization via more sample-efficient, statistically guaranteed prediction sets rather than traditional reliability engineering (failure/maintenance) modeling.","The prediction set is defined as $\Gamma_\lambda(X)=\{\hat y\in\mathcal Y: S(\hat y, f(X))\le \lambda\}$ with miscoverage loss $\ell(Y,\Gamma_\lambda(X))=\mathbf{1}\{Y\notin \Gamma_\lambda(X)\}$. The target guarantee is $\Pr\{R(\hat\lambda)\le \alpha\}\ge 1-\delta$ where $R(\lambda)=\mathbb E[\ell(Y,\Gamma_\lambda(X))]$. RCPS-CPPI forms an unbiased CPPI risk estimator (Eq. 11) by averaging across folds: $\hat R_{\mathrm{CPPI}}(\lambda)=\frac1K\sum_{k=1}^K\big(\frac1N\sum_{j=1}^N \ell(g^{(k)}(\tilde X_j),\Gamma_\lambda(\tilde X_j)) - \frac1n\sum_{i\in D_L^{(k)}}\Delta_i^{(k)}(\lambda)\big)$, then selects $\hat\lambda=\inf\{\lambda:\hat R^+_{\mathrm{CPPI}}(\lambda)<\alpha\}$ using a fold-wise UCB and union bound.","On the UJIIndoorLoc WiFi fingerprinting dataset with target risk $\alpha=0.1$ and confidence $1-\delta=0.9$, all compared methods maintain empirical coverage at or above the 90% target over $n\in[50,500]$ labeled calibration samples (with $N=15650$ unlabeled samples). RCPS-CPPI consistently yields smaller prediction sets (lower average radius/“inefficiency”) than RCPS and RCPS-PPI, with about a 30% reduction in set size compared to RCPS at $n=50$. As the number of folds K increases, RCPS-CPPI maintains near-target coverage while inefficiency generally decreases, with diminishing returns beyond about $K=5$–$10$. When the pseudo-labeling model quality degrades (higher validation MSE), the naive semi-supervised baseline can under-cover, RCPS-PPI may also falter at high MSE, while RCPS-CPPI remains close to target coverage and effectively falls back toward RCPS behavior when the predictor is uninformative.",None stated.,"This is not a reliability engineering (failures/maintenance/lifetime) study; “reliable” refers to statistical coverage guarantees for prediction sets, so the contribution may not transfer to engineering reliability metrics like MTBF or hazard rates. The empirical evaluation is limited to a single WiFi fingerprinting dataset and a specific base model (ELM) plus a specific auxiliary network, so generalization to other environments (different buildings, devices, channel conditions) and to other tasks is not established. The method assumes i.i.d. calibration samples from the test distribution; time-varying channels or spatial/temporal correlations common in wireless deployments could break validity without additional handling. Computational cost increases with K due to training K auxiliary predictors, and no runtime/latency analysis is provided for real-time deployment.","The authors suggest extending RCPS-CPPI to support online and adaptive calibration for time-varying wireless channels, and enabling real-time reliability assurance in dynamic indoor localization environments.","Evaluate robustness under distribution shift and temporal/spatial dependence (e.g., drifting RSSI statistics, correlated samples), potentially via online conformal/RCPS variants or covariate-shift corrections. Extend the framework to multivariate or structured outputs beyond 2D position (e.g., floor classification + coordinates, trajectories) and to other sensing modalities (CSI, UWB, BLE). Provide a self-starting/unknown-parameter variant and practical guidance for selecting K and training budgets under latency constraints, including ablations and computational profiling. Release a reference implementation to facilitate adoption and benchmarking across additional public indoor localization datasets.",2507.20268v2,https://arxiv.org/pdf/2507.20268v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:06:31Z
TRUE,Accelerated testing|Life distribution modeling|Warranty analysis|Other,"Parametric (Weibull, etc.)|Bayesian|Other",Right-censored|Mixture of types|Other,Not applicable,Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an Adaptive Accelerated Bayesian Sampling Plan (AABSP) for Bayesian reliability acceptance sampling under a simple step-stress partial accelerated life test (SSSPALT) with competing risks and Type-II censoring. Lifetimes under each cause are modeled with exponential hazards, linked across stress levels using the cumulative exposure model; the design adapts by increasing stress at time $\tau_1$ only if the number of early failures $D_1$ is below a threshold $m$. A Bayesian decision-theoretic framework is developed with explicit loss components (sampling, salvage, test-time cost, acceleration cost, and rejection cost) and a general acceptance-loss function $h(\lambda)$; the Bayes decision rule is derived by posterior-risk minimization and a monotonicity property is established. For Gamma priors on baseline hazards and Uniform priors on acceleration factors, and under a quadratic $h(\lambda)$, the authors derive a computable expression for the posterior expected loss driving the accept/reject decision and provide an algorithm to optimize $(n,r,m,\tau_1)$ by minimizing Bayes risk. Numerical comparisons show AABSP often improves Bayes risk relative to conventional non-accelerated and fully accelerated Bayesian sampling plans, and the method is illustrated on real competing-risks step-stress data from a solar lighting device test.","Adaptive step-stress rule: increase stress from $s_0$ to $s_1$ at time $\tau_1$ iff $D_1<m$, and stop the test at the $r$-th failure (Type-II censoring). Under competing risks with exponential cause-specific hazards, the unit hazard at stress $s_i$ is $\sum_{j=1}^J \lambda_{ij}$, and under the cumulative exposure model for $t>\tau_1$ with stress elevated the survivor is $\exp\{- (\sum_j\lambda_{0j})\tau_1 - (\sum_j\lambda_{1j})(t-\tau_1)\}$. The reduced-data likelihood is $L(\theta\mid D')\propto \prod_{j=1}^J \lambda_j^{d_{+j}}\,\phi_j^{\delta d_{2j}}\exp\{-\lambda_j(w_1+\phi_j^{\delta}w_2)\}$ with $D'=(w_1,w_2,d)$ and reparameterization $\lambda_{1j}=\lambda_{0j}\phi_j$. The Bayes decision rule is accept iff $\varphi(D')=\mathbb{E}[h(\lambda)\mid D']\le C_r$; for quadratic $h(\lambda)$, $\varphi(D')$ is expressed as a ratio of integrals $H_1(w_1,w_2,d,p)/H_1(w_1,w_2,d,0)$ (Eq. (9)).","In the illustrative comparison (Example 1), for acceleration-cost $c_a\in\{0,0.1,0.2\}$ the optimal AABSP designs reported include $(n_B,r_B,m_B,\tau_{1B})=(5,3,3,0.115)$ with $R_B=35.964$ at $c_a=0$, and $(5,3,2,0.138)$ with $R_B=36.252$ at $c_a=0.1$; relative risk savings versus the fully accelerated plan (RRS2) range up to about 1.718% in Table 1. In sensitivity studies, increasing the time-cost $c_t$ tends to reduce the optimal stress-change time $\tau_{1B}$ and can increase the advantage of AABSP over non-adaptive alternatives (Table 2). On real solar lighting device data with two failure modes and fixed $\tau_1=5$ (hundred hours), the optimal AABSP is reported as $(n_B,r_B,m_B)=(7,5,3)$ with $R_B=77.126$, improving over both ACBSP and CBSP with RRS1=0.204% and RRS2=0.142% (Table 6).",None stated.,"The model assumes independent exponential cause-specific failure times (constant hazards) and independence across risks, which may be restrictive for many engineered products and can be violated under changing stress or shared frailty. The adaptive rule only considers the count of failures by $\tau_1$ (not their causes or times beyond summary statistics), which may discard information useful for optimal stress escalation and decision-making. Comparisons focus on closely related Bayesian sampling plans; broader benchmarking against non-Bayesian or robust designs, model-misspecification checks, and alternative acceleration models (beyond CEM) are limited.","The authors suggest extending the methodology beyond the exponential distribution to other lifetime distributions, to other censoring schemes, and to more complex multi-stage step-stress escalation schemes (more than two stress levels). They also mention exploring alternative loss structures beyond the quadratic form used for closed-form illustration.","Develop robust/self-starting versions that handle unknown/estimated prior hyperparameters and assess sensitivity to prior mis-specification, especially for acceleration factors. Extend the adaptive rule to use cause-specific information (e.g., thresholds on $D_{1j}$) or sequential/Bayesian updating policies (dynamic programming) for stress changes. Add explicit model-misspecification studies (e.g., Weibull or lognormal true data, dependent competing risks, non-representative ALT failure modes) and provide open-source implementation to support reproducibility and adoption.",2507.23293v1,https://arxiv.org/pdf/2507.23293v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:07:02Z
TRUE,System reliability|Other,Simulation-based|Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study,TRUE,None / Not applicable,Not provided,NA,"The paper proposes Subset Adaptive Importance Sampling (SAIS) for estimating rare-event (small) failure probabilities in multimodal and high-dimensional system reliability problems defined by a limit state/performance function. SAIS hybridizes subset simulation (nested intermediate failure events with adaptive thresholds) and adaptive multiple importance sampling (a mixture of proposals updated iteratively) to better capture multiple disconnected failure modes. Methodologically, it introduces (i) a threshold adaptation strategy based on elite samples, (ii) deterministic-mixture (DM) weighting with cross-entropy updates for proposal means/covariances, (iii) a high-dimensional robust covariance learning mechanism using tempering plus shrinkage/regularization, and (iv) a recycling (forgetting-factor) estimator that reuses samples from all iterations to improve the final failure probability estimate. Performance is evaluated via Monte Carlo simulation on benchmark limit-state functions, including multimodal and high-dimensional cases, and compared against SS-IS and CE-PMC. Results show SAIS consistently achieves lower error/variance and better mode coverage with fewer iterations and/or fewer samples, especially as dimension increases and failure regions are multimodal or disconnected.","Failure probability is formulated as $P_f=P(X\in F)=\int I_F(x)\,\tilde\pi(x)\,dx$ where $F=\{x:S(x)\le 0\}$. SAIS uses deterministic-mixture weights $w_{n,k}^{(t)}=\pi(x_{n,k}^{(t)})/\Psi(x_{n,k}^{(t)})$ with $\Psi(x)=\frac1N\sum_{n=1}^N q_n^{(t)}(x;\theta_n^{(t)})$, and adapts proposals via cross-entropy updates for $\mu_n,\Sigma_n$ (Eqs. 13–14) with tempered weights when ESS is low (Eq. 15–16) plus shrinkage covariance update (Eq. 17). The final estimate can recycle all iterations using $\alpha^{(t)}=\lambda^{(T-t)}$ and $\hat P_f=\alpha\sum_{t=1}^T \alpha^{(t)}\hat I_f^{(t)}$ with normalization $\alpha=(1-\lambda)/(1-\lambda^T)$.","Across multiple benchmark problems, SAIS (and SAIS with recycling) yields markedly lower estimation error than SS-IS and CE-PMC, particularly for multimodal/disconnected failure regions and in higher dimensions. For the modified Rastrigin example (true $P_f\approx7.349\times10^{-2}$), SAIS achieved RRMSE as low as 0.034 (with $N=30,K=200$) versus SS-IS RRMSE about 9.918, while requiring fewer iterations (reported $T_{SAIS}=5$ vs $T_{CE-PMC}=38$ for similar qualitative performance). In the high-dimensional linear example with true $P_f\approx2.33\times10^{-4}$, Table 5 shows SAIS remains close to the true value across dimensions where SS-IS severely underestimates and CE-PMC overestimates for larger $d_x$ (e.g., at $d_x=80$, SS-IS reports $\sim5.56\times10^{-12}$, CE-PMC $\sim4.83\times10^{-4}$, SAIS $\sim2.13\times10^{-4}$). Reported coefficients of variation in Table 5 are much smaller for SAIS (e.g., ~1–6.5%) than for SS-IS and CE-PMC (often ~14–62%), indicating lower estimator variance.",None stated.,"The method is validated only on simulated benchmark limit-state functions; there is no real engineering case study demonstrating end-to-end modeling and computational cost with expensive simulators. The approach assumes the ability to evaluate the limit state function and to sample from/compute densities of the input distribution (e.g., standard Gaussian in examples), which may be nontrivial for correlated/non-Gaussian inputs or implicit finite-element models. Comparisons focus on SS-IS and CE-PMC; other strong rare-event estimators (e.g., subset simulation variants with advanced MCMC, splitting methods, multilevel CE, or surrogate-assisted methods) are not comprehensively benchmarked under matched computational budgets.",None stated.,"A valuable next step is demonstrating SAIS on real-world reliability applications with expensive simulators (e.g., structural FE models, power systems) and reporting wall-clock cost and parallel scalability. Extending SAIS to handle dependent/non-Gaussian inputs directly (beyond transformations) and to settings with model uncertainty (surrogates, epistemic uncertainty) would improve practical reach. Providing an open-source implementation and studying sensitivity/auto-tuning of key parameters ($\rho,\lambda,N,K$) and stopping rules would aid adoption and reproducibility.",2508.00210v1,https://arxiv.org/pdf/2508.00210v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:07:36Z
FALSE,NA,ML-based|Bayesian|Hybrid/Ensemble|Other,Mixture of types|Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/MachineLearningBCAM/Weak-Supervision-TPAMI-2025,"The paper proposes a programmatic weak supervision framework that produces probabilistic labels together with confidence intervals (CIs) for label probabilities, aiming to improve the reliability of weakly supervised label estimates when labeling functions (LFs) have mixed types (labels, abstentions, probability outputs) and unknown dependencies. It models ambiguity via an uncertainty set of per-instance label distributions constrained by (estimated) expectations of user-chosen LF-related feature mappings, with error tolerances. Confidence intervals for a label’s prevalence within any chosen group of instances are obtained by optimizing worst-case (max/min) feasible average label probabilities over the uncertainty set, with dual convex programs enabling practical computation. The method also derives minimax probabilistic predictions by minimizing worst-case expected log-loss over the uncertainty set, yielding closed-form softmax-like predictions parameterized by a sparse vector obtained from a convex objective. Experiments on multiple benchmark datasets show improved probabilistic quality (Brier score, calibration error, log-loss) and competitive 0–1 loss versus common weak supervision baselines, and demonstrate that the proposed CIs contain actual group label proportions in reported trials.","The core uncertainty set is defined by expectation constraints: \(\mathcal U=\{(p_i)_{i=1}^n: p_i\in\Delta(\mathcal Y),\ |\frac{1}{n}\sum_{i=1}^n \mathbb E_{p_i}[\Phi(\Lambda_i,y)]-\hat\tau|\preceq \lambda\}\). Groupwise confidence intervals for label \(y\) over a group \(I\) are \(\underline p_I(y)=\min_{(p_i)\in\mathcal U}\frac{1}{|I|}\sum_{i\in I}p_i(y)\) and \(\bar p_I(y)=\max_{(p_i)\in\mathcal U}\frac{1}{|I|}\sum_{i\in I}p_i(y)\), solvable via dual convex programs. Minimax predictions solve \(\min_{h_i}\max_{(p_i)\in\mathcal U}\frac{1}{n}\sum_i \mathbb E_{p_i}[-\log h_i(y)]\), yielding \(h_i(y)=\Big(\sum_{\tilde y\in\mathcal Y}\exp((\Phi(\Lambda_i,\tilde y)-\Phi(\Lambda_i,y))^\top\mu^*)\Big)^{-1}\) with \(\mu^*\) from a convex objective containing \(-\hat\tau^\top\mu+\lambda^\top|\mu|+\frac{1}{n}\sum_i\log\sum_{\tilde y}\exp(\Phi(\Lambda_i,\tilde y)^\top\mu)\).","Across 8 benchmark datasets (AWA, DomainNet, IMDB, Yelp, Basketball, SMS, OBS Network, Cardiotocography), the proposed minimax probabilistic predictions (MMP) achieve strong overall probabilistic performance, often best or near-best in Brier score and log-loss, with competitive calibration error and 0–1 loss versus MV, Snorkel, FlyingSquid, EBCC, HyperLM, and AMCL. The paper reports that, in shown experiments on AWA and IMDB, the computed confidence intervals contain the actual group label proportions and also contain the MMP group-average predicted probabilities, consistent with theory. It reports practical runtime on the order of ~30 seconds per dataset. The experiments also indicate confidence intervals tighten as the number of labeling functions increases and widen for small groups or fewer LFs.","The authors note that computing confidence intervals requires solving two optimization problems per examined group of instances and label, which can be costly if many groups are queried, though they state these optimizations can be solved efficiently. They also state that the resulting confidence intervals are not tight for small groups of instances or when there are few labeling functions, which is the tradeoff for handling general LF behavior without strong assumptions.","The approach depends heavily on the choice of feature mapping \(\Phi\) and on the accuracy/conservativeness of the expectation estimates \(\hat\tau\) and error bounds \(\lambda\); if these are misspecified or optimistic, the uncertainty set may be misleading (or empty) and coverage can fail. Theoretical guarantees are primarily about worst-case consistency within the constructed uncertainty set rather than about real-world LF generation mechanisms, so practical calibration hinges on how well \(\mathcal U\) captures reality. The paper’s CI evaluation is demonstrated on benchmark datasets with available labels for validation; broader evidence on real industrial weak supervision pipelines and sensitivity analyses over \(\Phi\)/\(\lambda\) choices would strengthen generalizability.",They propose developing stochastic gradient descent–based techniques to more efficiently solve the CI optimization problems for many groups. They also suggest exploring adaptive grouping strategies to better handle low-data regimes and to help identify when additional labeling functions are needed for particular instances or subpopulations.,"A useful extension would be self-tuning or data-driven selection of feature mappings and \(\lambda\) (e.g., via PAC-style bounds or conformal-style calibration with minimal labels) to reduce reliance on ad hoc error assessments. Another direction is to handle temporal or correlated data (violations of i.i.d. assumptions implicit in some estimation steps) and to extend groupwise CIs to structured outputs or multilabel settings. Providing a packaged implementation (e.g., pip/conda) and standardized benchmarks for CI quality (coverage vs width tradeoffs) across weak supervision tasks would also facilitate adoption.",2508.03896v1,https://arxiv.org/pdf/2508.03896v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:08:14Z
TRUE,Software reliability|Other,ML-based|Bayesian|Hybrid/Ensemble|Other,Sensor/condition monitoring|Mixture of types|Other,Not applicable,Healthcare/medical|Finance/economics|Transportation/logistics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/cleverhans-lab/selective-classification,"This PhD thesis studies reliability of modern machine learning systems through uncertainty quantification and selective prediction, where a model abstains when confidence is low to reduce harmful errors in high-stakes deployment. It proposes a post-hoc selective prediction method (SPTD/SCTD) that uses prediction instability across intermediate training checkpoints, achieving near–deep-ensemble selective performance without retraining multiple models and working across classification, regression, and time-series forecasting. The thesis analyzes selective classification under differential privacy, showing many ensemble-style methods degrade (or incur higher privacy cost), while checkpoint-based methods remain effective; it also introduces an accuracy-dependent upper bound and a normalized score to compare selective performance across privacy levels without accuracy alignment. A theoretical contribution decomposes the selective-classification gap to the oracle accuracy–coverage frontier into Bayes noise, approximation error, ranking error, statistical noise, and miscellaneous slack (optimization/shift), and argues monotone post-hoc calibration cannot fix ranking error. Finally, it identifies an adversarial threat (Mirage) that manipulates reported uncertainty to deny service while keeping high accuracy, and proposes Confidential Guardian, combining calibration audits with zero-knowledge verified inference, to detect and deter such uncertainty manipulation.","Selective prediction is formulated as $(f,g)(x)=f(x)$ if $g(x,f)\le\tau$ and abstain $\perp$ otherwise. The core SPTD/SCTD score is a weighted sum of checkpoint disagreements with the final model: $g(x)=\sum_{t=1}^T v_t a_t(x)$ with $v_t=(t/T)^k$ and $a_t=\mathbb{1}[f_t(x)\neq f_T(x)]$ (classification) or $a_t=\|f_t(x)-f_T(x)\|$ (regression). Under DP evaluation, the thesis defines an oracle upper bound for selective accuracy at coverage $c$ given full-coverage accuracy $a_{\text{full}}$: $\text{acc}(a_{\text{full}},c)=1$ if $c\le a_{\text{full}}$ else $a_{\text{full}}/c$, and measures deviation area as an accuracy-normalized score.","On vision selective-classification benchmarks (CIFAR-10/100, Food101, StanfordCars), SPTD improves accuracy at reduced coverage over softmax-response and SAT baselines and is competitive with deep ensembles; combining DE+SPTD yields the strongest selective accuracy across the coverage spectrum (e.g., CIFAR-100 at 10% coverage: DE+SPTD 99.6% vs SPTD 99.4% vs DE 99.2%). For OOD/adversarial detection, SPTD score separates in-distribution vs SVHN OOD and PGD adversarial samples with ROC AUC around 0.82–0.91 depending on dataset/setting. Under differential privacy experiments (FashionMNIST, CIFAR-10, SVHN, GTSRB; $\varepsilon\in\{\infty,7,3,1\}$), checkpoint-based SCTD is the best-performing method and shows that recovering non-private accuracy can require substantial coverage reduction at low privacy budgets (e.g., CIFAR-10 at $\varepsilon=1$ often near-zero coverage for several methods, SCTD ~0.04). For adversarial uncertainty manipulation, Mirage increases miscalibration (ECE) without hurting accuracy; e.g., CIFAR-100 baseline accuracy ~83.98% vs Mirage ~83.92% while ECE rises from ~0.066 to ~0.182, enabling Confidential Guardian to detect the induced uncertainty via calibration audits and verified inference.","The thesis notes several limitations across its contributions. For training-dynamics selective prediction, it cautions that extremely over-parameterized models or aggressive learning-rate schedules can reduce checkpoint diversity, weakening disagreement-based uncertainty signals, and that non-stationary/curriculum training can confound disagreement with task changes. In the DP selective-classification study, authors acknowledge the work is largely empirical and that a deeper theoretical analysis of SC–DP interplay is needed; they also did not thoroughly investigate fairness beyond class imbalance. For Confidential Guardian, detection requires a reference dataset with coverage of the manipulated uncertainty region, and ZKP-based verified inference can be computationally expensive for large models; calibration failures detected by the audit may also arise from benign sources, not only Mirage.","Although framed as “reliability,” the work primarily addresses predictive reliability (uncertainty/abstention) rather than classical reliability engineering concerns like lifetime/degradation, maintenance optimization, or system failure rates; mapping results to those settings would require additional problem-specific modeling. Many empirical results depend on stored checkpoints and chosen checkpointing cadence/weighting; in real production systems, checkpoint availability, storage constraints, and training nondeterminism may limit applicability or reproducibility. Several evaluations use standard benchmarks and synthetic shifts; broader real-world operational validation (longitudinal monitoring, human-in-the-loop deferral costs, and end-to-end decision outcomes) is limited. The adversarial section focuses on underconfidence manipulation detectable by calibration; other manipulation modes (e.g., selective overconfidence, distribution-dependent attacks, or adaptive attacks against the audit) may require different detectors or stronger threat modeling.","The thesis proposes future directions including: scalable uncertainty scoring for large modern models (efficient checkpoint-based approximations, distillation of uncertainty signals), studying uncertainty jointly with other trust metrics (privacy, fairness, robustness) under unified multi-objective frameworks, determining model suitability under distribution shift with label-free diagnostics, and extending from abstention to agentic strategies that actively reduce uncertainty via interaction or retrieval. It also suggests stronger auditability via advancing from proofs of inference to (eventually) proofs of training to authenticate the full training pipeline and prevent covert objective modifications. Additional work is encouraged to derive fundamental bounds for selective classification under differential privacy and to explore privacy–fairness trade-offs.","A valuable extension would be a self-starting/online version of the training-dynamics approach that does not require storing many checkpoints (e.g., streaming summaries of instability) and supports continual learning settings. More robust selective prediction under temporal dependence and distribution drift (autocorrelation, non-i.i.d. arrivals) could be studied with explicit guarantees and operational metrics (false reject rates, downstream workload). For the DP setting, developing theory that connects privacy noise to ranking error (not just accuracy) and designing DP-aware selectors optimized directly for ordering would strengthen conclusions. For Confidential Guardian, exploring adaptive adversaries that target calibration metrics, and designing complementary audits (e.g., subgroup-conditional calibration, conformal-set audits, or feature-space coverage tests) could improve robustness when the auditor’s reference set has limited coverage.",2508.07556v2,https://arxiv.org/pdf/2508.07556v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:09:12Z
TRUE,RUL prediction|Maintenance optimization|Failure mode analysis|Other,ML-based|Other,Sensor/condition monitoring|Other,Predictive|Condition-based,Transportation/logistics,Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a predictive-maintenance methodology for Continuous Variable Current Modulation (CVCM) railway track circuits to classify early-stage anomalies into future failure types using deep neural networks. It targets pre-emptive failure diagnostics where anomalies are initially subtle and not visually distinguishable from nominal operation, addressing a key limitation of conventional thresholding/visual inspection methods. Using lab-generated CVCM received-signal data across about 10 failure/anomaly cases (with pulses sampled from full anomaly progressions), the deep supervised classifier achieves ~99.33% overall multi-class accuracy and enables very early identification, with example earliness averaging below ~1% into anomaly development relative to progression to critical failure. The work also adds uncertainty quantification via conformal prediction, producing prediction sets with 99% confidence and near-singleton average set size (~1.06), improving trustworthiness and supporting detection of unknown anomalies via null sets. The approach is positioned as scalable/generalizable for track circuits and aligned with ISO 17359 condition-monitoring guidance for non-interfering diagnostics.","The anomaly classifier is a deep neural network that maps a short sliding-window pulse from the received signals (e.g., CAT/CAL channels) to a failure/anomaly class label. Uncertainty is quantified using conformal prediction by choosing a miscoverage level $\alpha$ (e.g., $\alpha=0.01$) to form a prediction set of class labels with guaranteed coverage $1-\alpha$ (99% confidence). Earliness is computed as normalized time into anomaly progression: (time of first correctly classified anomalous pulse − anomaly start)/(critical failure time − anomaly start), expressed as a percentage.","Overall anomaly/failure-type classification accuracy is reported at about 99.33% across the considered failure categories and across all anomaly stages. Early detection performance is highlighted with an example where conventional diagnosis identifies the failure at 58.95% into anomaly progression, whereas the proposed method identifies it at 0.83% (i.e., >99% earlier relative to time-to-critical). With conformal prediction at 99% confidence, the method achieves sufficient per-class conditional coverage while keeping the average prediction-set size at about 1.06 (near the ideal 1). The method is stated to be effective independent of installation location/configuration (as claimed) and applicable without adding new sensors.","The authors note that longer pulse windows can improve accuracy/confidence but delay detection, implying a trade-off between earliness and reliability that depends on operational requirements. They also state that the anomaly progressions are generated in lab/simulated experiments and assumed to translate to field evolution; while classification may remain valid, estimating time-to-critical failure could be impacted because real degradation rates can differ and vary over time. They further mention that an external audit would be needed to validate ISO 17359 compliance.","The evaluation appears to rely primarily on proprietary lab experiments with a small number of failure cases/files, which may not capture field variability (environmental conditions, interference, differing maintenance states) and may inflate generalization claims. The paper reports accuracy but provides limited detail on dataset size, train/test splitting across experiments, and controls against leakage from pulse sampling within the same signal profile, which could overestimate performance. Comparisons to baselines are qualitative and do not include rigorous quantitative benchmarking against modern alternatives (e.g., classical ML with engineered features, 1D CNN baselines, or time-series transformers) under the same protocol. Practical deployment aspects such as class imbalance in the field, concept drift, and performance under non-i.i.d. data (which can violate conformal prediction assumptions) are not deeply addressed.","The authors propose extending the approach to better time-to-failure estimation by approximating anomaly development over time for each failure case, leveraging the early classification outputs. They also call for field validation on the rate at which each failure evolves, noting that lab-evolved anomalies may progress over different and variable time scales in reality. They suggest further exploring the trade-off between pulse-window length, accuracy/confidence, and earliness to meet operational requirements.","A valuable extension would be a self-starting/online adaptation strategy to handle site-specific variability and concept drift, including recalibration or domain adaptation across installations. Robustness studies under non-i.i.d. conditions (autocorrelation, seasonal effects, changing noise levels) and conformal methods designed for distribution shift (e.g., weighted/adaptive conformal) would strengthen uncertainty guarantees in deployment. Broader benchmarking on open or field datasets (or a de-identified shared subset) with standardized splits and additional metrics (per-class recall, false alarm rate, detection delay) would improve reproducibility and operational interpretability. Packaging the method into deployable software with monitoring dashboards and decision thresholds for maintenance actions would help translate the model outputs into maintenance policies and measurable reliability improvements.",2508.09054v1,https://arxiv.org/pdf/2508.09054v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:09:50Z
TRUE,Degradation modeling|Accelerated testing|Life distribution modeling|Other,"Stochastic process|Parametric (Weibull, etc.)|Other",Degradation measurements|Simulated only|Other,Not applicable,Semiconductor/electronics|Energy/utilities|Other,Exact distribution theory|Other,NA,None / Not applicable,Not applicable (No code used),https://tigerprints.clemson.edu/all_theses/470,"The paper develops an analytical framework for optimal design of gamma-process degradation tests, choosing the number of test units, number of inspections, and inspection times to improve estimation efficiency under limited resources. It derives Fisher information and provides closed-form objective functions for D-, A-, and V-optimality (the latter targeting precision of a lifetime quantile defined via first-passage to a threshold). For periodic (type-I) inspection schedules, the paper derives optimal designs under fixed (n,m), fixed (n,T), and total-cost constraints, including a practical minimum inspection interval constraint. For aperiodic (type-II) schedules, it shows via majorization theory that equal spacing is least efficient, and that optimal aperiodic intervals concentrate time into one long interval plus (m−1) minimum intervals, with further cost-constrained optimality conditions. Two LED degradation examples illustrate that type-II designs can substantially improve efficiency in some settings, while in others type-I designs are nearly as efficient.","Degradation increments follow a gamma process: for inspection interval \(\Delta t_j\), \(\Delta Z_{ij}=Z_i(t_j)-Z_i(t_{j-1})\sim \mathrm{Gamma}(\alpha\Delta t_j,\beta)\); the paper uses a Tweedie reparameterization \(\gamma=\log(\alpha/\beta)\) so \(\mathbb{E}[Z(t)]=e^{\gamma}t\). The Fisher information for \(\theta=(\alpha,\gamma)^T\) under design \(\zeta=(n,m,T,\mathbf{t})\) is \(I(\theta;\zeta)=n\,\mathrm{diag}(\sum_{j=1}^m \Delta t_j^2\,\psi_1(\alpha\Delta t_j)-T/\alpha,\; \alpha T)\), where \(\psi_1\) is the trigamma function. Optimality criteria minimize functions of \(I^{-1}\): D-optimality uses \(\det(I^{-1})\), A-optimality uses \(\mathrm{tr}(I^{-1})\), and V-optimality uses \(h^T I^{-1} h\) for \(h=\partial \xi_p/\partial\theta\) where \(\xi_p\) is the \(p\)-quantile of first-passage lifetime \(Q=\inf\{t:Z(t)\ge\eta\}\).","For type-I (periodic) designs with fixed \(n,m\), D-optimality always favors larger inspection interval \(\tau\) (subject to constraints), while V-optimality may have either no finite optimum (monotone decreasing in \(\tau\)) or a unique finite optimum depending on whether \(h_2^2/(\alpha^2 h_1^2)\ge 2/3\). With fixed \(n\) and fixed termination time \(T\), the criterion reduces to maximizing \(\tau\psi_1(\alpha\tau)\), which is decreasing, implying the optimal choice is the minimum allowable interval \(\tau=\Delta t\) (i.e., as many inspections as possible). For type-II (aperiodic) inspection times with fixed \(n,m,T\), majorization implies the maximum information occurs at \((T-(m-1)\Delta t,\Delta t,\ldots,\Delta t)\) (any order), and the minimum occurs at equal spacing \((T/m,\ldots,T/m)\), showing periodic schedules are least efficient. Example 2 shows substantial gains from type-II over type-I under D- and V-optimality (relative efficiency about 0.75 and 0.89 for type-I vs type-II), while Example 1 shows type-I can be near-optimal (RE ≈ 0.98–0.99).","The study focuses on fundamental (non-accelerated) gamma degradation tests with a fixed-effects model and assumes a linear mean degradation path \(\mathbb{E}[Z(t)]=e^{\gamma}t\). The authors note that real degradation paths may be nonlinear (convex/concave) and that unit-to-unit variability may require random or mixed effects models, which are not covered. They also note that while many results extend conceptually to accelerated degradation tests (ADTs), a general ADT framework is not developed here.","The analytical optimal designs rely on known model parameters (\(\alpha,\gamma\)) and a correctly specified gamma-process model (monotone degradation, independent increments, stationarity), which may be violated by measurement error, autocorrelation, shocks, or non-monotone behavior. Many derived “optimal” solutions are approximate/continuous in \(n\) and \(m\) and may require rounding; integer constraints can change the true optimum (the paper only demonstrates integer search in an example). The results are tied to information-based criteria (D/A/V) and a specific cost structure; alternative practical constraints (calendar scheduling, batched inspections, censoring, missingness, destructive sampling logistics) could materially alter designs.","The authors propose extending the framework to nonlinear degradation paths (convex/concave mean functions) rather than \(\mathbb{E}[Z(t)]=e^{\gamma}t\). They also suggest extending the analytical results to random- or mixed-effects gamma models to capture unit-to-unit heterogeneity. Finally, they note that a general framework for optimal design of gamma accelerated degradation tests (ADTs) is still lacking and that tools developed here (e.g., polygamma properties) may help build such a framework.","Develop robust/self-starting designs that account for parameter uncertainty (Bayesian or minimax designs) and quantify sensitivity beyond the presented scenarios, especially for V-optimality where \(h\) depends on \(\eta,p\). Extend the design theory to handle measurement error and inspection effects explicitly (e.g., noisy observations, discretization, threshold uncertainty), and to accommodate dependent increments or nonstationary environments. Provide open-source software to compute the case-based solutions (Theorems 3.4/4.3) and to search integer-feasible designs under additional operational constraints (e.g., discrete inspection calendars, limited inspection capacity, multiple products/streams).",2508.09569v1,https://arxiv.org/pdf/2508.09569v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:10:32Z
TRUE,Life distribution modeling|Other,"Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only,Not applicable,Transportation/logistics|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a severity-aware extension to classical structural reliability metrics that go beyond failure probability $p_f$ and reliability index $\beta$ by incorporating the conditional magnitude of failure. It introduces the Expected Failure Deficit (EFD), $E_f=\mathbb{E}[-g(X)\mid g(X)<0]$, and its normalized form $E_f^*=E_f/\sigma_g$, to quantify how far into the failure domain a system typically goes when it fails. A new Severity-Aware Reliability Index $\beta_S$ is then defined as the Gaussian-equivalent reliability level that yields the same $E_f^*$, obtained by inverting a monotone mapping derived under Gaussian assumptions. The authors prove existence/uniqueness of this inversion only on a restricted domain $E_f^*\in(0,2/\sqrt{2\pi})$, and interpret violations of this bound as a diagnostic flag for excessive tail risk (e.g., heavy tails or infinite variance). Monte Carlo examples (including heavy-tailed and mixture load models) show cases where systems with nearly identical $p_f$ (thus similar $\beta$) have markedly different severity, and propose a five-level severity classification calibrated to familiar reliability-index benchmarks for risk-informed design decisions.","The Expected Failure Deficit is defined as $E_f=\mathbb{E}[-g(X)\mid g(X)<0]$ and normalized as $E_f^*=E_f/\sigma_g$. Under Gaussian $g\sim\mathcal{N}(\mu_g,\sigma_g^2)$ with $\beta=\mu_g/\sigma_g$, the normalized deficit has closed form $E_f^*=\frac{\phi(\beta)}{\Phi(-\beta)}-\beta$. The Severity-Aware Reliability Index $\beta_S$ is defined implicitly by solving $\frac{\phi(\beta_S)}{\Phi(-\beta_S)}-\beta_S=E_f^*$, where $F(b)=\frac{\phi(b)}{\Phi(-b)}-b$ is strictly decreasing on $(0,\infty)$ with range $(0,2/\sqrt{2\pi})$.","Monte Carlo illustrations show two non-Gaussian scenarios calibrated to the same $p_f\approx10^{-2}$ (thus $\beta\approx2.33$) can have very different severities: Scenario A yields $E_f^*\approx0.727$ while a heavier-tailed Scenario B yields $E_f^*\approx1.418$, exceeding the Gaussian-map maximum $2/\sqrt{2\pi}\approx0.7979$ and making $\beta_S$ undefined (diagnostic of excessive tail risk). In a heavy-tailed mixture example with Pareto component, the paper reports $\beta\approx3.388$ but $\sigma_g$ does not exist, so $E_f^*$ and $\beta_S$ are not computable. In a realistic member case study $g(R,D,L)=R-(1.2D+1.6L)$ with mixture-Gumbel live load, the results are $p_f\approx9.1\times10^{-5}$ ($\beta\approx3.744$) yet $E_f^*\approx0.474$ leading to $\beta_S\approx1.278$, indicating high but quantifiable severity despite rare failure. A five-level severity classification is calibrated via $E_f^*$ thresholds corresponding to $\beta_S=3,2,1$, producing cutoffs near $0.283$, $0.373$, and $0.525$, with an ‘Extreme’ class for $E_f^*\ge0.7979$.","The authors note that the framework assumes $g(X)\in L^2$ (finite $\sigma_g^2$) so that $E_f^*=E[-g\mid g<0]/\sigma_g$ is well-defined; for infinite-variance regimes the normalization breaks down and $\beta_S$ may be undefined. They also emphasize that invertibility holds only for $E_f^*<2/\sqrt{2\pi}\approx0.7979$, and that exceeding this bound indicates tail behavior beyond what an equivalent Gaussian interpretation can represent. They mention that when tail behavior is uncertain one may screen with robust scales (e.g., MAD/IQR) or conditional scales to avoid undefined normalization.","The proposed severity measure is conditional-mean based, which can be highly sensitive to outliers and to the quality of tail sampling/estimation; practical estimation of $E_f$ may require variance-reduction or extreme-value methods when failures are very rare. The approach benchmarks severity against a Gaussian mapping, so interpretability and comparability may be less meaningful for strongly skewed, bounded, or multimodal limit-state distributions where a Gaussian ‘equivalent’ is not a natural reference. The paper relies heavily on Monte Carlo demonstrations; broader validation against established risk measures used in reliability-based design (e.g., expected loss, CVaR-type measures on response, or utility-based criteria) and on real structural datasets/design problems would strengthen practical adoption. Computational details (solver choices, convergence, uncertainty quantification for $E_f^*$ and $\beta_S$) are not fully specified, limiting reproducibility and guidance for implementation under engineering constraints.","The authors suggest generalizing the transformation beyond the Gaussian benchmark by using alternative base distributions (e.g., Student-t or Laplace), developing hybrid formulations that interpolate between Gaussian-based $\beta_S$ and tail-aware surrogates, and exploring severity metrics based on tail-weighted moments or conditional risk integrals. They also mention extending work toward broader tail behavior accommodation, enhancing tail risk estimation, and embedding $\beta_S$ into reliability-informed design optimization, risk-based codes, or multi-hazard performance criteria.","Develop self-starting/robust estimators of $E_f^*$ and confidence intervals for $\beta_S$ under rare-event settings, potentially using importance sampling, subset simulation, or cross-entropy methods, so practitioners can quantify estimation error. Extend the framework to correlated and time-dependent reliability problems (e.g., deterioration, nonstationary loads) where severity may evolve and conditional deficit distributions may be path-dependent. Provide software and standardized benchmarks (including sensitivity to model uncertainty and epistemic uncertainty) to facilitate adoption in LRFD calibration workflows. Investigate links to decision-theoretic design (cost/loss-based) to translate severity classes into explicit actions and optimize designs under combined frequency-severity criteria.",2508.12068v1,https://arxiv.org/pdf/2508.12068v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:11:13Z
FALSE,NA,"Parametric (Weibull, etc.)|Other",Event/count data|Other,Not applicable,Service industry|Other,Approximation methods|Simulation study|Case study (real dataset)|Other,TRUE,Python,In text/Appendix,NA,"The paper proposes a utility-driven framework for sustained adoption of agent-centric AI, centered on three design axioms: prioritize reliability over novelty, embed tools into workflows, and increase user agency via agentic execution. Adoption is modeled as a two-component curve combining a decaying novelty term and a growing utility term, yielding closed-form conditions for monotone behavior versus troughs/overshoots and an expression for the critical time of the extremum. The authors provide identifiability and confounding analysis for the parameters (α, β, N0, Umax), including Fisher information and CRLB derivations under several error models, plus residual diagnostics and non-nested model comparisons against logistic/Bass and a logistic-with-bump comparator. Empirically, they report (i) randomized embedding ablations using telemetry from 850 users and (ii) a real enterprise adoption case (Fortune 500; 1,200 workers; 18 months) where the two-component model fits trough dynamics better than monotone alternatives. The work is primarily about AI adoption/diffusion modeling rather than reliability engineering; reliability is defined as task success probability and used as an input to utility and adoption dynamics, not as a system/asset reliability analysis in the engineering sense.","Adoption model: $A(t)=N_0 e^{-\alpha t}+U_{\max}(1-e^{-\beta t})$ with $\alpha,\beta>0$, combining decaying novelty and saturating utility. Per-task net utility uses reliability $R_\pi(t)$: $u_\pi(t)=R_\pi(t)v(t)-(1-R_\pi(t))C_f(t)-c_{time}\,\tau_\pi(t)-c_{fric}\,\phi_\pi(t)$, and $U=\mathbb{E}[u_\pi(t)-u_{\pi_0}(t)]$. The interior extremum occurs at $t^*=\frac{\ln(\alpha N_0/(\beta U_{\max}))}{\alpha-\beta}$ (when $t^*>0$), yielding trough/overshoot conditions via $r=\beta U_{\max}/(\alpha N_0)$.","Embedding ablation (telemetry; 850 users; 12 weeks) shows higher embedding increases estimated growth rate: $\hat\beta=0.120$ (E=0.2), $0.182$ (E=0.6), $0.238$ (E=0.9); OLS $\partial\beta/\partial E=0.168$ [0.092, 0.244], $p<0.001$ with bootstrap CI [0.089, 0.251]. Context switches drop from 8.3 to 2.8 per hour as embedding increases, consistent with reduced friction costs. Enterprise case study (78 weeks; n=40 observations) reports two-component model AIC 156.2 vs logistic+bump 168.7, Bass 201.3, logistic 198.9, with RMSE 0.41 (two-component) vs 0.73 (log+bump) and 1.84 (Bass); estimated trough time $\hat t^*=18.3$ weeks [15.9, 20.7] and trough depth 7.8% users. Hazard-gradient estimation reports $h'(0)$ estimates (all $p<0.001$): 0.073 [0.051, 0.095] (linear), 0.281 [0.195, 0.367] (logit), 0.064 [0.043, 0.085] (exponential).","The paper notes boundary/conditioning issues: when $U_{\max}\approx N_0$ or when data are truncated near $t=0$ or late times, the identifiability quadratic becomes ill-conditioned and induces strong negative correlation between $\hat\alpha$ and $\hat\beta$, widening confidence intervals and reducing power to distinguish trough vs. monotone regimes. It also states that higher residual autocorrelation reduces effective Fisher information, implying that early/late coverage and low autocorrelation are important for tight estimation. No other explicit limitations are clearly stated in the provided text.","Reliability is defined as an expected task success probability and treated largely as an input to utility/adoption; the work does not address classical reliability-engineering concerns (failure mechanisms, censoring, repair/maintenance, system structure, or life testing), limiting direct applicability to engineered asset reliability programs. The empirical claims rely on adoption telemetry and fitted curves; causal interpretation of reliability/embedding effects may be sensitive to unmodeled time-varying confounders, interference between users, or changes in the product and task mix across phases beyond the embedding factor. The two-component functional form may be restrictive for real adoption dynamics with multiple launches, seasonality, competing tools, or heterogeneous user segments; extensions to hierarchical/mixed-effects or multiseries Bayesian models are not developed. While code listings are provided, no packaged implementation or external benchmark datasets are linked, which may hinder reproducibility/validation beyond the included synthetic series.",None stated.,"Extend the framework to explicitly model heterogeneous users/tasks with hierarchical parameters (e.g., random effects for $\alpha,\beta,U_{\max}$) and allow time-varying reliability and embedding, which would better match real product evolution. Add robustness to autocorrelation, seasonality, and nonstationary shocks (e.g., state-space formulations) and develop self-starting/online estimation for real-time monitoring of trough risk and agency-threshold crossing. Provide standardized public datasets and a maintained software package (e.g., pip/CRAN) to facilitate independent replication and broader benchmarking against modern diffusion and causal inference models. Connect the reliability definition to calibrated proper scoring rules and uncertainty in $R(\pi)$ (e.g., Bayesian reliability estimates) to propagate evaluation uncertainty into utility/adoption decisions.",2508.12896v1,https://arxiv.org/pdf/2508.12896v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:11:49Z
TRUE,System reliability|Life distribution modeling|Other,Nonparametric/Semi-parametric|Simulation-based|Other,Right-censored|Complete lifetime data|Mixture of types,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies nonparametric estimation of coherent-system reliability when component-level (“autopsy”) lifetime data are available, potentially right-censored by system failure and/or an administrative monitoring time. It proposes a shrinkage class of estimators that transform each component’s Kaplan–Meier/Product-Limit estimator via a power coefficient c and then plug these into the known system structure function to estimate the system reliability curve. The optimal shrinkage coefficient is selected by minimizing a decision-theoretic global loss (a weighted Cramér–von Mises loss), using either an analytical asymptotic bias–variance approximation (Delta method + Greenwood variance) or a bootstrap procedure. Simulation studies for series, parallel, and series-parallel structures under Weibull component lifetimes and exponential censoring show that component-level plug-in estimation generally improves over system-only KM, and that the proposed shrinkage can provide additional (sometimes modest) risk reductions, especially in parallel systems and small-sample/moderate-censoring regimes. The work frames system reliability estimation as a simultaneous estimation problem and adapts James–Stein-type ideas to function estimation through shrinkage of component reliability/cumulative hazard estimates.","System reliability is expressed as $R_S(t)=h_\phi(R_1(t),\ldots,R_K(t))$ for a coherent structure function. With component-level data, the proposed shrinkage estimator is $\hat R_S(t;c)=h_\phi(\hat R_1(t)^c,\ldots,\hat R_K(t)^c)$, where $\hat R_j(t)$ is the component product-limit/KM estimator and $\hat R_j(t)^c=\exp\{-c\hat\Lambda_j(t)\}$ corresponds to shrinking the estimated cumulative hazard. The shrinkage coefficient $c^*$ is chosen to minimize a global weighted Cramér–von Mises loss $L=\int \frac{(R_S(t)-\hat R_S(t))^2}{R_S(t)(1-R_S(t))}\,dF_S(t)$, approximated analytically using a Taylor expansion with component importance weights and Greenwood-based variance, or estimated via bootstrap risk minimization over a grid of $c$.","For a 3-component series-parallel system (n=15, 1000 replications), mean estimated risk decreased from 0.0713 (system-only PLE/KM) to 0.0601 (component plug-in), and further to 0.0597 with analytical shrinkage (mean $\hat c^*\approx0.984$) while bootstrap shrinkage was similar (risk 0.0602; mean $\hat c^*\approx0.998$). For 5-component series systems (n=15), component plug-in matched system-only PLE (both 0.0700) and shrinkage did not improve (analytic 0.0703; bootstrap 0.0718), reflecting heavy component censoring. For 5-component parallel systems (n=15), component plug-in improved over system-only PLE (0.0508 vs 0.0714) and shrinkage provided small additional gains (analytic 0.0500 with mean $\hat c^*\approx1.014$; bootstrap 0.0501 with mean $\hat c^*\approx1.010$). Simulations varying censoring and number of components show $\hat c^*$ tends toward 1 as sample size increases, rises slightly with heavier censoring, and relative efficiency gains from shrinkage increase modestly with more parallel components (e.g., risk-efficiency ~1.15% at K=5 to ~1.46% at K=15).","The authors note a “double-dipping” limitation: the same observed data are used both to estimate component reliability functions and to select the optimal shrinkage coefficient $\hat c^*$, which can induce optimistic bias in finite-sample performance assessment (though argued to be asymptotically negligible). They also state the current work assumes a complete autopsy framework where component failure/censoring times are observed; it does not address more realistic current-status/inspection-only data where component statuses are observed only at system failure or inspection times.","The approach assumes independent component lifetimes; dependence (common-cause failures, shared frailty, load sharing) could materially affect both KM-based component estimates and the risk approximation used to choose $c$. The analytical risk approximation relies on first-order Taylor/Delta-method arguments and uses plug-in estimates for unknown quantities; accuracy may degrade in small samples, under heavy censoring, or near the tail where KM is unstable. The method presumes the system structure function is known and correct; mis-specified structure would propagate through $h_\phi$ and could swamp any shrinkage gains. Computational details (optimization, bootstrap grid choices, stability constraints ensuring $\hat R_j(t)^c\in[0,1]$ across t) are not formalized in a reproducible implementation, which may limit straightforward practitioner adoption.","The authors propose extending the method to current-status data where component failure times are not observed exactly, requiring new methodology for limited-information settings. They also suggest exploring formal Bayesian or empirical Bayes alternatives using nonparametric priors (e.g., Dirichlet or Gamma process) on component reliability functions to incorporate prior knowledge and provide uncertainty quantification. Additional suggested directions include studying alternative loss functions and applying the methods to real-world reliability datasets to validate broader applicability.","Developing versions robust to component dependence (e.g., copula/frailty-based coherent-system reliability estimation with shrinkage) would broaden applicability to realistic engineered systems. Providing self-starting/online updates for $\hat c^*$ and $\hat R_S(t)$ (streaming/autonomous monitoring) could make the method more usable in field reliability and PHM contexts. Extending to semi-parametric settings with covariates (Cox-type component models) and then propagating through $h_\phi$ could enable reliability estimation under heterogeneous operating conditions. Publishing an open-source implementation (R/Python) with tested numerical defaults (grid selection, constraints, bootstrap diagnostics) and benchmarking against alternative nonparametric system estimators would improve reproducibility and facilitate adoption.",2509.12420v2,https://arxiv.org/pdf/2509.12420v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:12:32Z
FALSE,NA,"Parametric (Weibull, etc.)|Bayesian",Other,Not applicable,Healthcare/medical|Pharmaceutical,Simulation study|Case study (real dataset),TRUE,R,Public repository (GitHub/GitLab),https://github.com/Celaeno1017/jpemax,"The paper studies reliable parameter estimation for the binary (logistic-link) Emax dose–response model used in Phase II clinical trials, focusing on small/moderate sample sizes where standard MLE can be biased and can fail under separation or non-monotone/convex sample response patterns. It evaluates three bias-reduction approaches: Cox–Snell post-hoc bias correction, Firth’s modified score method, and maximum penalized likelihood estimation (MPLE) using a Jeffreys prior penalty. The authors derive the relevant score/Hessian/information expressions for the Emax model and compare methods via Monte Carlo simulations across multiple sample sizes and parameter regimes, tracking non-convergence/instability rates, bias, MSE, and Wald CI coverage. Simulations show Cox–Snell can be unstable (and cannot help when MLE fails), while both Firth and Jeffreys-MPLE yield finite, stable estimates under separation-prone designs, with MPLE generally showing lower variance and MSE. A real-data illustration using the TURANDOT Phase II ulcerative colitis trial indicates MPLE provides more stable standard errors and dose-response probability estimates under a non-monotonic observed pattern.","The binary Emax model uses a logistic link: $\text{logit}(\pi_i)=E_0+E_{\max}\,\frac{\text{Dose}_i}{ED_{50}+\text{Dose}_i}$ (with the 4-parameter version including a Hill parameter $\lambda$). Cox–Snell bias correction adjusts the MLE by $\hat\theta_c=\hat\theta-B(\hat\theta)$, where $B(\theta_s)=\sum_{r,j,l}\kappa^{sr}\kappa^{jl}\left(\tfrac12\kappa_{rjl}+\kappa_{rj,l}\right)$. Firth’s method solves modified score equations $\tilde U_s=U_s+W_s$, while Jeffreys-MPLE maximizes $L^*(\theta)=L(\theta)\,|I(\theta)|^{1/2}$ with penalized score $U_s^*(\theta)=\sum_i (y_i-\pi_i)\nabla\eta(\text{Dose}_i,\theta)+\mathrm{tr}\!\left(I^{-1}\frac{\partial I}{\partial\theta_s}\right)$.","In simulations with five dose arms and small samples (notably $n=50$), MLE and Cox–Snell failed to produce estimates in 19.3% of runs and had 15.2% unstable estimates; Cox–Snell also showed extreme instability in some settings due to reliance on an ill-conditioned information matrix. Firth and MPLE produced finite estimates in all runs at $n=50$, with 3.3% unstable for Firth versus 0.2% for MPLE; for $n\ge150$, both were reported as consistently stable (0% unstable). Across scenarios, MPLE generally had the smallest standard errors and lowest MSE among the compared methods, while maintaining good (often slightly conservative) Wald CI coverage. In a more extreme flat-curve setting (high $ED_{50}$), MLE/Cox–Snell had 35.6% failure to estimate, while MPLE had 0% failures and 0% unstable estimates (Firth: 0% failures, 11.8% unstable). In the TURANDOT trial illustration (357 patients across placebo and four doses), MPLE yielded comparatively stable standard errors (e.g., for $\log(ED_{50})$) whereas MLE/Cox–Snell/Firth exhibited instability under the observed non-monotonic pattern.","Cox–Snell correction is post-hoc and requires an MLE; it fails when the MLE algorithm does not converge (e.g., complete separation) and can behave poorly when the Fisher information is ill-conditioned (flat curves or quasi-separation), producing unstable corrections. The authors note Wald confidence intervals for penalized likelihood estimators can be unwise in small, separation-prone samples because the penalized log-likelihood may be nonquadratic, leading to conservative (too wide) intervals and over-coverage. They also acknowledge that observed non-monotonic patterns may reflect missingness (possibly MNAR) rather than sampling variability, and their main focus is separation and small-sample bias rather than fully modeling missing-data mechanisms.","The work is focused on a specific nonlinear dose–response form (three-parameter logistic Emax with $\lambda=1$ in core experiments) and does not fully explore robustness to model misspecification beyond non-monotonic/convex sample shapes (e.g., unmodeled covariates, overdispersion, clustered/center effects, or informative dosing/imbalance). Comparisons are limited to Cox–Snell, Firth, and Jeffreys-MPLE; other practical alternatives for separation/small samples (e.g., weakly informative priors, Bayesian hierarchical Emax, exact/conditional methods, or robust quasi-likelihood approaches) are not benchmarked. The primary inference reported relies heavily on Wald approximations; more systematic evaluation of profile penalized likelihood CIs or bootstrap-based parameter inference could change coverage/length conclusions. Implementation details (optimizer choice, constraints, starting values) can materially affect convergence in nonlinear likelihood problems, so results may depend on the chosen fitting protocol and may not fully generalize across software/optimizers.","The authors state that extensions of the binary Emax likelihood-based framework to other endpoint types (continuous and time-to-event) are left for future work. They also describe a practical workflow for handling missing outcomes (MAR via multiple imputation; MNAR via sensitivity analyses or EM-based estimation) and note that integrating these missing-data strategies with penalized likelihood for the binary Emax model is a valuable future direction. They suggest profile penalized likelihood intervals as an alternative to Wald CIs for more precise inference in small, separation-prone settings.","A natural extension is a full Bayesian treatment of the nonlinear Emax model with weakly informative priors (including on $ED_{50}$ and $E_{\max}$) and head-to-head comparison against Jeffreys-MPLE for stability, interpretability, and decision-focused metrics (e.g., optimal-dose selection error). Developing diagnostics and automated model-switching/averaging between monotone Emax and flexible alternatives (e.g., quadratic-logit, spline-based dose–response) would help practitioners when non-monotonicity is plausible. Extending the methods to incorporate covariates, random site effects, and adaptive/randomization-induced complexities common in Phase II designs would improve real-world applicability. Finally, providing a standardized simulation benchmark suite (dose grids, separation regimes, misspecification scenarios) and packaging reproducible code for all methods would facilitate broader adoption and fair comparison.",2509.18459v1,https://arxiv.org/pdf/2509.18459v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:13:11Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2503.19786|https://openreview.net/forum?id=KGBNAJCuPf,"The paper studies a reliability issue in Best-of-N (BoN) sampling for LLM alignment: pairwise-preference reward models rank candidates but do not encode an absolute “acceptability” signal, so BoN may pick the least-bad option among unacceptable samples, especially on hard prompts as N grows. To address this, it introduces a discrete-choice (multinomial logit) reward model with an explicit outside option that anchors rewards to a prompt-dependent rejection threshold, making reward sign interpretable as acceptable vs. unacceptable. Using this acceptability signal, it proposes an adaptive inference procedure (“best of mini-N in-loop”) that samples in sequential mini-batches and early-exits when the best-so-far exceeds a threshold; thresholds can be calibrated conservatively from hard-prompt reward distributions for a reliability “guardrail,” or set to 0 for a speed “accelerator.” Experiments on an IMDB sentiment setup with a Gemma base model (LoRA fine-tuning) and a synthetic preference pipeline show the guardrail reduces false acceptances by ~70% relative to BoN-32, while the accelerator improves average inference time by >22% with similar recall to the baseline. Overall, the contribution advances inference-time selection by adding an explicit abstention/acceptability mechanism and a tunable reliability–efficiency trade-off absent from standard BoN reranking.","The choice model defines utility as $U(x,y_i)=\tilde R(x,y_i)+\epsilon_i$ with i.i.d. Gumbel noise and an explicit outside option $y_0$. This yields multinomial logit choice probabilities $\Pr(d_i=1\mid x,Y)=\exp(\tilde R(x,y_i))/\sum_{j=0}^J\exp(\tilde R(x,y_j))=\exp(R(x,y_i))/(1+\sum_{j=1}^J\exp(R(x,y_j)))$ where the normalized reward is $R(x,y_i)=\tilde R(x,y_i)-\tilde R(x,y_0)$ and $R(x,y_0)=0$, so $R(x,y)>0$ indicates acceptability. For guardrail calibration, using an empirical CDF $\tilde F$ from hard prompts, the threshold solves $[\tilde F(\tau_N)]^N=\tilde F(0)$, giving $\tau_N=\max\{\tilde F^{-1}([\tilde F(0)]^{1/N}),0\}$.","On hard prompts, standard BoN’s false positive count increases with N, reported as 104 at N=1 vs. 210 at N=32 in their IMDB setup. With a total budget N=32, the alignment-guardrail configuration (mini-16 in 2 loops with calibrated thresholds) reduces false positives from 210 (BoN-32) to 63 (~70% reduction) while mean ground-truth reward decreases slightly from 0.486 to 0.466. The guardrail raises precision to 94.3% (from 88.0%) and reduces FPR to 15.8% (from 54.1%), but recall drops to 65.7% (from 95.4%). As an inference accelerator (threshold 0), mini-16 in 2 loops reduces mean execution time from 7.98s to 6.16s (>22% faster) while maintaining recall at 95.4% (similar to baseline) but with FPR roughly unchanged (~54%).","The paper notes that the i.i.d. Gumbel error assumption induces independence of irrelevant alternatives (IIA) and remarks that tasks with correlated alternatives would require a richer error structure. It also frames some integrations (e.g., with speculative decoding or other inference-time methods) as future work rather than addressed in the current study. No other explicit limitations are stated in the provided text.","The empirical validation uses a synthetic preference/acceptability pipeline (a sentiment classifier with injected Gumbel noise and a fixed outside-option score), so results may not transfer to real human preference data or to safety/harmlessness acceptability judgments. The approach’s calibration relies on identifying “hard prompts” and building an empirical CDF; its robustness under distribution shift (new prompt types, changing model checkpoints, different sampling temperatures) and the cost/complexity of periodic recalibration are not analyzed. The method is evaluated in a single-domain sentiment generation task with a small base model; performance and computational trade-offs for larger LLMs, multi-turn dialogue, or long-form reasoning may differ substantially.","The authors suggest extending the approach to more complex, multi-turn conversational settings or reasoning tasks. They also propose exploring synergies/integration with other inference-time methods such as speculative decoding (and related pruning/early-exit techniques) to obtain additional efficiency gains.","A valuable extension would be to validate the outside-option data collection and acceptability modeling with real human labels and across multiple acceptability dimensions (helpfulness, harmlessness, factuality), including inter-annotator variability and calibration. Another direction is to relax the multinomial logit/IIA assumption using nested logit or random-utility models with correlated errors, especially when candidates are highly similar (e.g., sampled with low temperature). Finally, developing self-starting or online recalibration procedures for $\tau_N$ under drift (model updates, domain shift) would improve deployability as a long-lived reliability control mechanism.",2510.04087v2,https://arxiv.org/pdf/2510.04087v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:13:47Z
TRUE,Other,Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Energy/utilities|Other|Theoretical/simulation only,Simulation study|Other,TRUE,MATLAB|Other,Not provided,https://arxiv.org/abs/2510.09315,"This paper develops a general theory and Monte Carlo strategy for computing reliability sensitivity, i.e., derivatives of failure exceedance probability (CCDF) $F(y,\alpha)=P(Y\ge y)$ with respect to system parameters $\alpha$, when both the response $Y=f(X,\alpha)$ and its parameter gradient $G=\partial f/\partial\alpha$ are available (black-box otherwise). The key theoretical result expresses sensitivity at threshold $y$ as $\partial F/\partial\alpha = p_Y(y)\,\mathbb{E}[G\mid Y=y]$, equivalently as a one-dimensional integral over the joint density of $(G,Y)$, avoiding difficult surface integrals on the failure boundary in input space. Because conditioning on $\{Y=y\}$ is a zero-probability event, the paper proposes kernel-smoothing estimators that use samples with $Y$ near $y$, yielding a controllable bias–variance tradeoff via kernel width. The method is embedded into a single Subset Simulation run so sensitivity for all generated thresholds can be computed alongside the CCDF, with examples in structural buckling, SDOF first-passage, and pile foundation serviceability. Simulation studies (including repeated runs) compare the proposed estimates to analytical benchmarks (when available) or to common-random-number central finite differences, showing good agreement for many cases while highlighting difficult parameters that induce high variance (e.g., gradients often equal to zero or weak gradient–response correlation).","Failure CCDF is $F(y,\alpha)=P(Y\ge y)$ with $Y=f(X,\alpha)$ and gradient $G=\partial f(X,\alpha)/\partial\alpha$. The main sensitivity identity is $\frac{\partial F}{\partial\alpha}=\int t\,p_{G,Y}(t,y)\,dt = p_Y(y)\,\mathbb{E}[G\mid Y=y]$. Kernel smoothing yields the direct-MC estimator $\frac{\partial F(y,\alpha)}{\partial\alpha}\approx \sum_{k=1}^N G_k\,w^{-1}K\big((Y_k-y)/w\big)$ and its Subset Simulation bin-weighted form $\sum_i P_i N_i^{-1}\sum_k G_{ik} w_i^{-1}K\big((Y_{ik}-y)/w_i\big)$.","In a Normal-response example, the method reproduces analytical sensitivities including a designed case where $\partial Y/\partial\alpha\ne 0$ but $\partial F/\partial\alpha=0$, illustrating the need for correct scaling and error awareness. In a 5-story shear-building buckling example with analytical benchmarks, sensitivity to a specific story stiffness is harder because $\partial Y/\partial\alpha$ is zero about 80% of the time (story not governing), inflating estimator variance. In the SDOF first-passage and pile design examples (no closed forms), the proposed SS+kernel estimates are compared against high-sample Monte Carlo central differences with common random numbers; damping/mean-parameter sensitivities are estimated reasonably, while natural-frequency sensitivity shows much larger variance and noticeable bias at high thresholds. Across examples, repeated-run statistics (e.g., 1000 independent SS runs) show growing variance and some bias as thresholds enter regions with few effective samples, consistent with the kernel width tradeoff.","The theory assumes sufficient smoothness: existence of the response gradient almost surely and continuity of joint/marginal/conditional PDFs to justify Taylor expansions and density-based expressions. The Monte Carlo estimator uses kernel smoothing to handle conditioning on $Y=y$, which introduces bias controlled by kernel width selection. The method does not exploit the relationship between response and gradient for variance reduction, and difficult parameters (e.g., often-zero gradients or weak gradient–response correlation) can lead to high-variance estimates.","The approach requires access to accurate parameter gradients; if gradients come from noisy finite differences or unstable adjoints, sensitivity estimates may be biased or numerically fragile, especially in tail regions. Kernel-width selection is handled with a rule-of-thumb (Scott’s rule) that is optimal for near-Normal settings but may be suboptimal for skewed/heavy-tailed responses or multimodal conditional structures typical in complex reliability problems. The estimator targets pointwise sensitivity at thresholds; in practice, decision-making may require integrated measures (e.g., over threshold ranges) or robustness to model misspecification (dependence, non-smooth limit states), which is not systematically analyzed. Comparisons focus on finite-difference Monte Carlo and analytical special cases; broader benchmarking against alternative advanced sensitivity estimators (e.g., weak approach variants, GP regression for $\mathbb{E}[G\mid Y]$, likelihood-ratio/score methods when applicable) is limited.","Proposed future work includes developing variance reduction by exploiting the dependence structure between response $Y$ and gradient $\partial Y/\partial\alpha$. The conditional-expectation form motivates replacing kernel smoothing with regression/learning approaches (e.g., Gaussian process regression) for estimating $\mathbb{E}[G\mid Y=y]$, potentially incorporating information from the underlying inputs $X$ as hidden variables. The authors also suggest extending the theory to second derivatives $\partial^2 P(Y\ge y)/\partial\alpha_i\partial\alpha_j$ and studying how to handle the associated zero-probability conditioning for higher-order terms.","Develop self-starting/adaptive bandwidth selection (e.g., cross-validation, plug-in methods, local polynomial regression) tailored to tail conditional expectations to reduce bias in rare-event regions. Extend the framework to non-smooth limit states (discontinuous responses, piecewise models) and to dependent/autocorrelated inputs common in time-series reliability and spatial fields beyond the presented examples. Provide guidance for gradient acquisition (adjoint/automatic differentiation vs. finite differences) and quantify how gradient error propagates to sensitivity error, especially under Subset Simulation correlation. Release reference implementations (e.g., MATLAB/Python) and benchmark suites to facilitate adoption and fair comparison across reliability-sensitivity methods.",2510.09315v1,https://arxiv.org/pdf/2510.09315v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:14:27Z
TRUE,Degradation modeling|Maintenance optimization|System reliability|Accelerated testing|Other,"Stochastic process|Parametric (Weibull, etc.)|Simulation-based|Other",Simulated only|Other,Condition-based|Predictive|Imperfect maintenance|Other,Manufacturing (general)|Transportation/logistics|Energy/utilities|Network/cybersecurity|Theoretical/simulation only|Other,Simulation study|Other,TRUE,MATLAB,Not provided,https://doi.org/10.1016/j.ress.2025.111744,"The paper develops a matrix-algorithmic reliability model for a single-unit multi-state system with an indeterminate number of degradation/performance levels, subject to internal repairable and non-repairable failures and external shocks with multiple consequences (including total failure and cumulative damage). The system incorporates condition-triggered preventive maintenance at a critical degradation level and corrective repair/replacement actions, while the repairperson follows a Bernoulli multiple-vacation policy whose decisions depend probabilistically on the observed degradation level. The authors construct a novel Marked Markovian Arrival Process (MMAP) representation (in both continuous and discrete time) with phase-type (PH) distributions for internal evolution, shock inter-arrivals, repairs, preventive maintenance, and vacation durations, enabling transient and stationary performance measures in matrix form. They derive stationary distributions via matrix-analytic balance equations and define key reliability measures (availability, reliability before first failure, mean event rates) plus a cost/reward framework for profitability. A numerical example demonstrates multi-objective optimization (profit vs. availability) using Pareto analysis and a genetic algorithm, with all computations implemented in MATLAB.","System evolution is modeled via an MMAP with generator/transition matrix formed by summing event-specific blocks (continuous: $Q=\sum_Y Q_Y$; discrete: $D=\sum_Y D_Y$). Key event-intensity/probability functions are given in Kronecker-product form, e.g., repairable-failure intensity $H_{RF}(U)=UT_0^r\otimes I\otimes e + UW_0^r\otimes L_0\gamma(1-\omega_0)\otimes Ce$ and non-repairable-failure intensity $H_{NRF}(U,R,A)=UT_0^{nr}R\otimes I\otimes eA + \cdots + UeR\otimes L_0\gamma(1-\omega_0)\otimes C_0A$. Transient probabilities are $p(t)=\theta e^{Qt}$ (continuous) or $p_{\nu}=\theta D^{\nu}$ (discrete), and the stationary distribution satisfies $\pi Q=0,\ \pi e=1$ (or $\pi D=\pi$). Net profit per unit time in steady state is $\Lambda=\Phi-\Psi_{NU}f_{nu}-\Psi_{CR}f_{cr}-\Psi_{PM}f_{pm}-\Psi_R G$ with $\Phi=\pi\cdot c$.","In the numerical example (3 degradation levels; PH models for shocks/repairs/PM/vacations), the Pareto “balanced” solution (Model 2) achieves steady-state profit per unit time 0.0164 with steady-state availability 0.9168, chosen as the closest point to the ideal (max profit, max availability). The profit-maximizing solution (Model 1) yields profit 0.2734 with availability 0.9089 and becomes profitable at time 155.7316, while the availability-maximizing solution (Model 3) attains availability 0.9187 but steady-state profit −0.1972 (never profitable). Reported mean time to first failure (PH mean operational time) differs substantially across the three optimized models: 13.3705 (Model 1), 36.8556 (Model 2), and 61.0399 (Model 3). Steady-state macro-state occupancy (Model 2) is dominated by working with repairperson present (Onv 0.6694) and working while on vacation (Ov 0.2474), with CR about 0.0777 and PM about 0.0034.","The authors note that while the matrix-algorithmic formulation scales to any number of operational levels, computational cost increases as the number of levels grows. They suggest mitigating this via grouping/eliminating low-probability states, limiting the maximum number of levels, fixing transitions using known PH distributions, or applying stochastic simulation techniques. No other explicit limitations are stated.","The numerical evaluation is primarily a synthetic case study; there is no validation on real operational/maintenance data, so practical calibration and robustness to misspecification (PH fitting, shock independence, level structure) are unclear. The model assumes Markov/PH structure and (as stated) external shocks independent of system condition; in many applications shocks and degradation can be dependent, potentially affecting conclusions. Optimization uses a genetic algorithm over a reduced space, but reproducibility is limited without shared code and without detailing GA settings/sensitivity; results may depend on tuning and objective scaling in the Pareto selection.","The paper does not propose a specific future-work agenda beyond discussing practical techniques to reduce computational cost as the number of degradation levels increases (e.g., state aggregation, truncation, fixed PH structures, or simulation). Otherwise, no explicit future research directions are stated.","A valuable extension would be parameter estimation and model validation using field data (failure/repair logs and condition monitoring), including PH/MMAP fitting and uncertainty quantification. Robust/self-starting variants that relax key assumptions (e.g., dependence between shocks and degradation, non-PH sojourns, imperfect maintenance effects on degradation level) would improve applicability. Providing an open MATLAB toolbox (or porting to R/Python) with benchmark examples and sensitivity analyses for GA/Pareto choices would enhance reproducibility and adoption.",2510.11506v1,https://arxiv.org/pdf/2510.11506v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:15:07Z
FALSE,NA,Bayesian|Other,Other,Not applicable,Healthcare/medical|Environmental monitoring|Other,Simulation study|Case study (real dataset)|Other,TRUE,R|Python|Other,Public repository (GitHub/GitLab),https://github.com/magnusneuman/Model-selection-with-the-Regularized-Map-Equation,"The paper proposes a one-step, model-selection-based approach to clustering noisy similarity/correlation data by unifying network sparsification and clustering via Bayesian community detection under the Minimum Description Length (MDL) principle. It evaluates two Bayesian community detection methods—the Degree-Corrected Stochastic Block Model (DC-SBM) and the Regularized Map Equation (Infomap)—and selects the correlation threshold by maximizing description-length compression, avoiding ad hoc thresholds and preset cluster counts. Using synthetic data generated from a block-structured multivariate normal covariance model, the authors study detectability as a function of sample size, population correlation strength, number of clusters, and number of features, and show the Regularized Map Equation is most noise-resistant and avoids inferring structure in pure noise. On real gene co-expression data (Allen Human Brain Atlas), the Regularized Map Equation yields more functionally coherent and distinct gene modules than WGCNA (less GO-term overlap and higher enrichment factors) and shows greater robustness under subsampling. The work contributes to the broader clustering/SPC-adjacent literature by framing threshold selection as principled MDL-based model selection rather than heuristic preprocessing, improving reliability of inferred modular structure in data-scarce settings.","Synthetic data are generated as $X\sim\mathcal{N}(0,\Sigma)$ with block-structured covariance $\Sigma_{ij}=\rho$ if $i,j$ are in the same planted cluster and $0$ otherwise (Eq. 1), and correlations are computed from $\hat\Sigma=X^TX/(L-1)$. The MDL-based threshold is chosen by maximizing description-length compression $\Delta D(\tau)=(D_1(\tau)-D^*(\tau))/D_1(\tau)$ (Eq. 11) with optimal $\tau^*=\arg\max_\tau \Delta D(\tau)$ (Eq. 12). The paper also derives an approximate analytical intersection threshold $r_i$ where within- and between-cluster correlation distributions intersect (Eqs. 7–8) and defines link probabilities $p_{in}=\int_{r_i}^1\hat f(r;\rho,L)dr$ and $p_{out}=\int_{r_i}^1\hat f_0(r;L)dr$ (Eqs. 9–10).","On synthetic correlation data, both Bayesian methods recover planted partitions with fewer samples than cross-validation and hierarchical clustering, with the Regularized Map Equation showing a phase transition behavior: it returns no modular structure (AMI≈0) in noise-dominated regimes and then accurately recovers clusters as sample size increases. The DC-SBM shows two failure modes on correlation networks: overpartitioning in triangle-rich (high clustering coefficient) settings due to link-independence assumptions, and underfitting/merging of small clusters due to a resolution limit; for $q=100$ clusters the AMI never exceeds 0.95 in the reported detectability analysis. In a representative threshold-selection experiment with $L=100$, $\rho=0.2$, $q=4$, $N=256$, the description-length compression peak occurs around $\tau^*\approx0.22$ for both Regularized Map Equation and DC-SBM, while cross-validation exhibits larger variance due to data splitting. On Allen Human Brain Atlas gene expression (1934 genes, 946 samples after filtering), the Regularized Map Equation yields much lower functional redundancy across clusters (max Jaccard of enriched GO term sets: BP 0.10, CC 0.13, MF 0.05) than WGCNA (BP 0.46, CC 0.54, MF 0.43) and has higher robustness under subsampling (higher AMI across 80/60/40/20% sample levels).","The authors note that applying the weighted Regularized Map Equation directly to correlation weights violates the method’s multinomial sampling assumption for integer-valued link counts; they therefore threshold and analyze unweighted networks, and report that applying the weighted version yields the same communities in their tests. They also explicitly discuss limitations of DC-SBM for correlational data, including violated link-independence assumptions in triangle-closing correlation networks and a resolution limit that merges small clusters.","The approach relies on choosing a threshold by scanning $\tau$ and optimizing an MDL-based compression objective, which may be computationally demanding for very large graphs or dense similarity matrices unless efficient search/approximation strategies are used. The synthetic data model assumes multivariate normality with equal-sized planted clusters and homogeneous within-cluster correlation, which may not reflect heterogeneity, confounding factors, or nonlinear dependence common in real biological/ecological data. The method’s behavior under strong autocorrelation/temporal dependence, batch effects, or non-Pearson similarity measures is asserted as extensible but not systematically validated in this paper.","The authors state that the approach extends beyond Pearson correlation to other similarity measures (e.g., mutual information, maximal information coefficient, and domain-specific similarities) and can be applied directly to similarity networks without underlying samples. They also suggest applying cluster-based regularization in related settings such as the graphical lasso, noting that Bayesian community detection should enable correct inference with fewer samples and noisier data than cross-validation.","Develop self-starting or uncertainty-quantified procedures that provide credible intervals for community assignments and threshold choice (e.g., posterior over partitions/thresholds) to better support decision-making in noisy settings. Benchmark robustness to common real-data complications (batch effects, missingness mechanisms, and nonlinear relationships) and evaluate performance across multiple real datasets beyond gene co-expression. Provide scalable implementations and heuristics for efficient threshold search (or threshold-free formulations) and release a reusable software package to lower barriers to adoption.",2510.15013v1,https://arxiv.org/pdf/2510.15013v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:15:49Z
TRUE,Life distribution modeling|Accelerated testing|Warranty analysis|Other,"Parametric (Weibull, etc.)|Bayesian|Nonparametric/Semi-parametric|Simulation-based|Other",Interval-censored|Mixture of types|Other,Not applicable,Manufacturing (general)|Theoretical/simulation only|Other,Approximation methods|Simulation study|Other,TRUE,R,Not provided,NA,"The paper develops Bayesian reliability acceptance sampling plans (RASPs) for life testing when items may fail from multiple independent competing risks and failures are observed only through interval-censored inspections. Assuming exponential latent failure times for each cause, it derives the multinomial likelihood for counts of cause-specific failures in inspection intervals and formulates Bayes risk under a quadratic acceptance loss plus sampling/inspection/time costs and salvage value. Three plan-construction routes are presented: (i) a plan with a pre-fixed reliability acceptance criterion based on the MLE of the reliability function at a specified mission time; (ii) an approximate Bayes risk approach using asymptotic normality (delta method) of the MLE to reduce computational burden for large samples; and (iii) a fully decision-theoretic plan using the Bayes decision function that minimizes Bayes risk over all decision rules. With a Gamma–Dirichlet prior on cause-specific rates, the authors provide computable expressions (closed form for equal inspection intervals; otherwise Monte Carlo integration) and algorithms to search for optimal sample size and inspection schedule. Numerical studies (for two causes) show that optimal designs from the reliability-criterion rule and from the Bayes decision rule are often identical or very similar under interval censoring, and the approximate-risk method yields practical optimal plans for larger samples.","The model assumes independent latent exponential cause-specific failure times with rates $\nu_j$ so the overall rate is $\nu=\sum_{j=1}^J\nu_j$ and reliability is $\bar F(t\mid\nu)=\exp(-\nu t)$. Under interval censoring with inspections $0=\tau_0<\tau_1<\cdots<\tau_k$, the observed counts $D_{mj}$ (failures in $(\tau_{m-1},\tau_m]$ due to cause $j$) have a multinomial likelihood (Eq. (1)) built from sub-distribution increments $G(j,\tau_m\mid\nu)-G(j,\tau_{m-1}\mid\nu)=\frac{\nu_j}{\nu}(e^{-\nu\tau_{m-1}}-e^{-\nu\tau_m})$ and survival $\exp(-\nu\tau_k)^{n-d_t}$. Bayes risk is $R(\zeta,a)=C_r+n(C_s-r_s)+C_\tau\,E[\tau]+C_I\,E[M]+r_sE[D_t]+R_1(\zeta,a)$ with $R_1$ integrating $(h(\nu)-C_r)$ over the acceptance region under the prior and sampling model. For equal intervals $h$, the MLE-based acceptance rule is $a(d\mid\zeta)=\mathbf{1}\{\exp(-\hat\nu\,\tau_0)>R_0\}$ with an explicit $\hat\nu$ (Eq. (10)); the Bayes decision rule is $a^*(d\mid\zeta)=\mathbf{1}\{C_r-\phi(d)\ge 0\}$ where $\phi(d)=E[h(\nu)\mid d]$ (posterior expected acceptance loss).","For the main numerical setting (two causes; Gamma–Dirichlet prior with $\alpha=2.8,\eta=1,\alpha_1=1.5,\alpha_2=1.8$ and costs $C_r=40,C_s=0.5,r_s=0.25,C_\tau=0.3,C_I=0.1,\tau_0=0.1$), the optimal design for both the Bayes-decision plan (Type I) and the reliability-criterion plan (Type II) is $\zeta^*=(n^*,h^*,k^*)=(4,0.30,3)$ with Type-II threshold $R_0^*=0.76$; both yield $P(A)=0.489$, $E[D_t^*]=3.337$, $E[\tau^*]=0.740$, $E[M^*]=2.467$, and Bayes risk $33.90826$. Sensitivity tables show no-sampling solutions in some regimes (e.g., for $\eta=0.5$ the optimal plan is $(0,0,0)$ with Bayes risk $40$; for $\eta=1.5$ optimal is also $(0,0,0)$ with Bayes risk $24.78307$ but corresponds to acceptance without life testing). Using the approximate Bayes risk (Example 2 with $C_s=0.15,r_s=0.1,C_I=0$), the optimal plan is $\zeta^*=(13,0.102,5)$ with $R_0^*=0.761$ and Bayes risk $31.4662$.","The authors note that analytic expressions (e.g., for $R_1(\zeta,a)$ and for the posterior expectation $\phi(d)$) are available mainly when inspection intervals are of equal length; for unequal intervals they require Monte Carlo integration. They also state that exact Bayes-risk computation can be computationally intensive for large samples, motivating their asymptotic (MLE normality/delta-method) approximation. The modeling is illustrated with exponential lifetimes and assumes independent competing risks to avoid identifiability issues.","The approach is restricted to independent competing risks with exponential cause-specific hazards; extending beyond exponential (e.g., Weibull/lognormal) or allowing dependence between risks may materially change tractability and performance. The plan-search algorithms depend on enumerating/optimizing over inspection schedules and acceptance regions; computational scalability and numerical stability for larger $k$, larger $J$, or tighter cost structures are not fully benchmarked. The paper’s evaluation is largely simulation/numerical without validation on real interval-censored competing-risks field data, so practical robustness to model misspecification (non-exponentiality, inspection-time uncertainty, reporting errors) is unclear.","The conclusion suggests extending the methodology to progressive type-I interval censoring schemes. It also proposes extending the approximate Bayes-risk approach to other lifetime distributions beyond the exponential. Finally, the authors mention extending to settings where the consumer and manufacturer have different prior assessments for the lifetime parameters in the multiple-failure-mode (competing risks) context.","A valuable extension would be to incorporate dependent competing risks (e.g., via copulas or shared frailty) and study identifiability/plan robustness under dependence. Developing robust or misspecification-resistant plans (e.g., against Weibull vs exponential, or against inspection-time jitter) and providing diagnostic checks for the assumed hazard model would improve practical deployment. Packaging the algorithms as an open-source R/Python implementation with runtime/complexity guidance and reproducible numerical benchmarks would make the proposed plans more accessible to practitioners.",2510.16740v1,https://arxiv.org/pdf/2510.16740v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:16:33Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://algorithms-with-predictions.github.io/,"This paper develops learning-augmented estimators for statistically valid inference when only a small labeled (“gold-standard”) dataset is available alongside a much larger unlabeled dataset and an ML predictor used for label imputation. It introduces the Prediction-Augmented Residual Tree (PART), a CART-style greedy decision-tree method that partitions feature space to estimate residual corrections locally, and derives asymptotic normality results enabling Wald-type confidence intervals for the mean and for linear regression coefficients. It further studies a limiting estimator, Prediction-Augmented Quadrature (PAQ), motivated by PART at infinite depth under smooth deterministic residuals, and proves improved variance rates (e.g., O(N^{-1}+n^{-4}) under stated smoothness/uniform-distribution assumptions) compared with PPI/PPI++. Empirically, it reports narrower confidence intervals at nominal coverage relative to PPI++ across multiple real datasets (ecology/remote sensing deforestation, astronomy Galaxy Zoo, wine quality, census housing, protein odds ratios, and census income regression). Overall, the contribution advances semi-supervised, prediction-powered inference by replacing a global residual correction with adaptive, feature-local corrections (PART) and by connecting to quadrature-based integration limits (PAQ).","For mean estimation, PPI is $\hat\mu_{\mathrm{PPI}}=\frac1N\sum_{j=1}^N f(\tilde x_j)+\frac1n\sum_{i=1}^n (y_i-f(x_i))$, and PPI++ is $\hat\mu_{\mathrm{PPI++}}=\frac1N\sum_{j=1}^N \hat\lambda f(\tilde x_j)+\frac1n\sum_{i=1}^n (y_i-\hat\lambda f(x_i))$. PART builds a binary tree with splits chosen to minimize $\mathrm{VMS}(k,s,R)=p_{\mathrm{left}}^2\,\hat\sigma_{\mathrm{left}}^2/n_{\mathrm{left}}+p_{\mathrm{right}}^2\,\hat\sigma_{\mathrm{right}}^2/n_{\mathrm{right}}$, and outputs $\hat\mu_T=\frac1N\sum_{j=1}^N f(\tilde x_j)+\sum_{\ell=1}^L p_\ell\,\bar r_\ell$ with variance estimate $\hat\sigma^2=\sum_{\ell=1}^L p_\ell^2\,\hat\sigma_\ell^2/n_\ell$ for Wald CIs. PAQ uses nearest-neighbor residual imputation $\hat\mu_{\mathrm{PAQ}}=\frac1N\sum_{i=1}^N (f(\tilde x_i)+r(h(\tilde x_i)))$ and is related to a trapezoidal-rule-like expression over ordered labeled features.","The paper states that PART yields consistently tighter confidence intervals than PPI++ on several real datasets while maintaining near-nominal coverage (e.g., demonstrated in multiple figures for deforestation, galaxy morphology, wine quality, census housing, protein odds ratio, and census income regression). It proves asymptotic normality and provides finite-sample coverage corrections for PART mean CIs (Theorem 1), highlighting a coverage adjustment term scaling like $\sqrt{(2L\log(dn))/n}$ plus a tree-class term $(nd)^{-L}$. In the deterministic smooth-residual setting, it proves PAQ bias $O(n^{-2})$ and variance $O(N^{-1}+n^{-4})$ for degree-1 interpolation (Theorem 4), and more generally for degree-$p$ interpolation bias $O(n^{-(p+1)})$ and variance $O(N^{-1}+n^{-(2p+2)})$ under stronger smoothness (Theorem 5). It also sketches extension to $d$ dimensions with rates depending on $n^{-(p+1)/d}$ and $n^{-(2p+2)/d}$ (Theorem 6).","The paper notes a trade-off between tree complexity and inference reliability: increasing the number of leaves (or depth) increases finite-sample corrections and can induce overfitting, which may reduce true coverage below nominal (Remark 3). For PAQ, the analysis explicitly relies on strong assumptions such as deterministic and smooth (e.g., $C^2$ or $C^{p+1}$) residual functions and (in main theorems) a uniform feature distribution (with later discussion of using probability integral transforms for non-uniform marginals). It also assumes $N\gg n$ and, for simplicity, effectively treats region mass probabilities $p(R)$ as known/accurately estimated from large unlabeled data (Assumption 1).","The work is not a reliability-engineering study and does not address failure-time/degradation data characteristics such as censoring, dependence over time, or maintenance actions; its methods are validated on generic tabular/image-derived datasets rather than reliability datasets. Practical implementation details that materially affect performance (e.g., how depth/min-leaf $m$ are tuned in a principled way, computational cost for large $d$ and large candidate-split sets, and sensitivity to feature scaling) are not fully standardized; “manually tuned depth” may complicate reproducibility and fair comparison. Theoretical guarantees focus on asymptotics with fixed leaf count (or idealized PAQ assumptions); robustness to distribution shift between labeled/unlabeled samples, predictor miscalibration, or feature-dependent label noise is not comprehensively analyzed. Although experiments use ML models (ResNet/XGBoost/Random Forest), the paper does not provide a shared implementation, which may limit independent verification.","The paper remarks that cross-validation training of the ML model and bootstrap-based inference methods used in prior PPI-style work could be applied to PART using similar formulas, suggesting extensions to settings where CLT justifications are tenuous. It also discusses extending PAQ beyond the uniform distribution via probability integral transforms and substituting the unknown CDF with the empirical CDF, indicating a pathway to practical use under general feature marginals. Additionally, it outlines an extension of PAQ ideas to higher-dimensional feature spaces using tensor-product quadrature rules (with stated rate degradation).","A useful next step would be to develop a self-starting or data-adaptive depth/leaf selection rule with guaranteed finite-sample coverage (e.g., selective inference, split-sample/tree-honesty, or conformalized intervals) to reduce overfitting-driven undercoverage. Extending the framework to dependent data (time series, spatial correlation) and to settings with covariate shift between labeled and unlabeled pools would broaden applicability and strengthen robustness claims. Providing an open-source reference implementation (including benchmarking scripts and default hyperparameters) would materially improve reproducibility and uptake. Finally, exploring robust variants (heavy tails, outliers) and multivariate targets (multiple means/coefficients jointly) with simultaneous inference control would align the methodology with common applied-inference needs.",2510.16937v1,https://arxiv.org/pdf/2510.16937v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:17:17Z
FALSE,Other,Other,Other,Not applicable,Other,Simulation study|Case study (real dataset),TRUE,Other,Not provided,NA,"This paper proposes a method to score the reliability (truthfulness/quality) of reported datasets when ground truth is unobserved, but auxiliary observations are generated from an unknown statistical experiment dependent on the true data. It defines several ground-truth-based partial orderings of dataset reliability (exact match, Blackwell dominance, and distance/Hamming-based orderings) and introduces the Gram determinant score, defined as the determinant of a Gram matrix built from class-conditional observation distributions (or a kernelized variant). The authors prove that, under linearly independent experiments and appropriate misreport-matrix conditions, the score preserves exact-match and Blackwell orderings and approximately preserves certain distance/Hamming orderings; they also show a uniqueness result for experiment-agnostic scoring (up to scaling). Practical estimators (plug-in and stratified matching) are provided for the detail-free setting, with asymptotic guarantees and some finite-sample exact-match guarantees. Empirical evaluations on synthetic noise models, CIFAR-10 embeddings (via kernels), and real employment data show the score decreases with increasing corruption and ranks revised datasets as more reliable.","The core score is the Gram determinant $\Gamma=\det(\hat G)$ where $\hat G=(PQ)^\top(PQ)=Q^\top(P^\top P)Q$ is the Gram matrix of reported labels induced by the (unknown) experiment $P$ and misreport matrix $Q$. Equivalently, $\Gamma=\det(Q^\top GQ)=\det(P^\top P)\det(Q)^2$, which underpins experiment agnosticism and ordering-preservation proofs. A kernelized version replaces inner products with $K(y,y')$ so $G_K(x,x')=\mathbb{E}[K(y,y')\mid x,x']$ and $\Gamma_K=\det(\hat G_K)$.","Theoretical: The Gram determinant score preserves (i) exact-match ordering under linearly independent experiments and non-permutation misreports, (ii) Blackwell dominant ordering under linearly independent experiments and invertible diagonally-maximal misreport matrices, and (iii) an approximate distance-ordering under balancedness and small Hamming-distance conditions (explicit constants given in Theorem 4.3). It is also experiment-agnostic: for invertible $Q,Q'$ and $P\in P_{\mathrm{indep}}$, $\Gamma(Q)\ge \Gamma(Q')\Leftrightarrow \Gamma(PQ)\ge \Gamma(PQ')$, and (under mild assumptions) it is essentially unique up to scaling among such experiment-agnostic scores. Empirical: on synthetic categorical data and CIFAR-10 embeddings, the score decreases monotonically with corruption probability across six manipulation policies and correlates negatively with Hamming and $\ell_2$ error; on employment data, the score increases from first release to one-month revision to final value (Table 1: $3.504\times10^6$, $24.920\times10^6$, $33.919\times10^6$).","The authors note that real-world applicability can be limited by assumptions that are hard to verify, especially conditional independence between reports and auxiliary observations (highlighted in the employment-data experiment). They also note that in finite-sample regimes, estimator variance can obscure advantages (e.g., demonstrating experiment-agnostic superiority against alternatives when only samples are available and $PQ$ is unknown). They mention discretization vs. kernelization tradeoffs, observing that discretization can perform similarly to kernel methods in some settings but may be unavailable in others (e.g., images).","Although the theory is framed for unknown experiments, practical use still requires auxiliary observations sufficiently informative and conditionally independent of strategic manipulation; violations (e.g., feedback loops where reporters anticipate or influence observations) could break ordering guarantees. The determinant can be numerically unstable/high-variance in higher dimensions (large $d$) or when label frequencies are highly imbalanced, and the paper’s finite-sample guarantees are limited to specific estimators/conditions (e.g., exact-match under balance and minimum counts). The approach is primarily for discrete label spaces $X=[d]$; extension to large/continuous label domains is only suggested, not fully developed. Comparisons to alternative dependence measures are mostly empirical and may depend strongly on simulation design and hyperparameter choices (e.g., kernel choice, embedding model quality).","They propose designing scalable estimators for high-dimensional or continuous label domains using dimensionality reduction (e.g., PCA, DPP sampling) and learned encoders. They also conjecture that other singular-value-based criteria could serve as reliability scores and note that formal guarantees for those alternatives remain open and would require tailored analysis. They mention practical applications such as detecting incoherent star ratings in product reviews and broader deployment where labels may be manipulated.","Develop robust/self-normalized variants that better handle severe class imbalance and small sample sizes, and provide guidance for numerically stable computation of determinants (e.g., log-det with regularization). Extend the framework to settings with temporal dependence/autocorrelation in observations, or adversaries who can partially influence the auxiliary observation process. Provide principled methods for kernel/feature selection (including learned kernels) with generalization guarantees, and expand empirical validation to more operational domains with known audit/ground-truth subsets for calibration. Build open-source reference implementations and benchmarking suites to standardize comparisons across reliability scoring methods.",2510.17085v1,https://arxiv.org/pdf/2510.17085v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:17:52Z
FALSE,Other,Other,Other,Not applicable,Other,Other,TRUE,Other,Not provided,NA,"This paper studies “reliability” in the sense of trustworthy inference for reasoning large language models (LLMs), not reliability engineering of physical systems. It formalizes self-consistency (majority vote over multiple stochastic rollouts) as mode estimation of the model’s terminal answer distribution and derives finite-sample and asymptotic concentration bounds for the probability that the majority vote equals the true mode. It introduces an anytime-valid sequential stopping rule—the Martingale Majority Certificate (MMC)—using e-values and Ville’s inequality to adaptively decide when enough rollouts have been drawn while controlling the probability of returning a non-mode answer. It also analyzes label-free test-time reinforcement learning (TTRL) as exponential tilting that sharpens the terminal distribution, increasing the mode margin and reducing samples needed for certification, and proposes SNR- and entropy-based alternative post-training objectives. Empirically, it reports results on math reasoning benchmarks (AIME 2024, AMC, MATH-500) and finds that estimated SNR correlates with problem difficulty and that test-time training improves pass@1 and reduces MMC sampling cost.","Majority vote estimator: $\hat c_n = \arg\max_j \hat p_{n,j}$ with $\hat p_{n,j}=\frac1n\sum_{i=1}^n \mathbf{1}\{X_i=j\}$ and mode margin $\delta=p_{c^\star}-p_{j^\star}$. Finite-sample error certificates bound $\Pr[\hat c_n\neq c^\star]$ via Hoeffding/Bernstein/Chernoff–Markov in terms of $(p_{c^\star}-p_j)$ and $n$, e.g. Hoeffding gives $(k-1)e^{-n\delta^2/2}$. MMC defines anytime-valid e-processes (mixtures over $\theta\in(1/2,1]$) for leader-vs-runner-up and leader-vs-others tests, and stops when both e-values exceed $1/\varepsilon$ to ensure $\Pr[\hat c_{N}\neq c^\star]\le \varepsilon$ at the stopping time $N$.","On MATH-500, test-time training with the proposed SNR reward greatly improves pass@1 for several models (e.g., Qwen2.5-Math-7B from 47.1 to 84.5; Qwen2.5-Math-1.5B from 31.4 to 72.0; Qwen2.5-7B from 59.1 to 80.3). Under MMC adaptive sampling, the reported required sample counts $N_{\text{adaptive}}$ are typically below the maximum budget and decrease further after test-time training (e.g., with $N_{\text{budget}}=100$, $\varepsilon=0.4$, Qwen2.5-Math-1.5B drops from 81.8 to 56.9). Linear fits of $N_{\text{adaptive}}\approx \alpha+\beta N_{\text{budget}}$ show smaller slopes after training (e.g., Qwen2.5-Math-1.5B: $\hat\beta$ from 0.848 to 0.570 at $\varepsilon=0.1$). The paper also reports that the empirical SNR statistic correlates strongly with externally defined difficulty levels on MATH-500, with harder problems yielding lower and more variable SNR.","The authors assume conditionally independent rollouts given the prompt/context; dependencies induced by adaptive/verifier-guided sampling could weaken guarantees. They note a calibration limitation: SNR- and entropy-based quantities rely on model probabilities reflecting true epistemic uncertainty, which may fail for some models or decoding temperatures. They also state that experiments focus on single-turn reasoning benchmarks; extending to multi-turn dialogue, program synthesis, and structured prediction is left open, and generating multiple trajectories still incurs substantial compute cost.","The framework certifies agreement with the model’s own mode (self-consistency), which can still be systematically wrong relative to ground truth; the certificate is therefore model-relative rather than task-correctness reliability. Practical deployment for open-ended outputs depends heavily on the deterministic canonicalization map $g$ (answer extraction/binning), and robustness of $g$ to formatting/paraphrase variation is not fully characterized. Empirical comparisons focus on selected baselines and benchmarks; broader evaluation under distribution shift, prompt changes, or different decoding correlations (e.g., shared random seeds, nucleus sampling artifacts) would better stress-test the martingale assumptions and stopping behavior.","They propose extending anytime-valid certificates to settings with correlated rollouts, structured output spaces, and multi-verifier settings, and exploring combinations with learned difficulty estimators for dynamic compute scheduling. They also plan to study reformulations of MMC that reduce or avoid the need for response ‘binning’/canonicalization when moving beyond discrete multiple-choice-style outputs.","A natural extension is to develop self-starting/robust variants of MMC that explicitly handle autocorrelated or reused-context sampling (e.g., streaming dialogue) and quantify sensitivity to dependence. Another direction is to integrate external correctness signals (when available) to bridge model-relative certification to ground-truth reliability (e.g., selective prediction with abstention calibrated to real accuracy). Providing a maintained open-source implementation (e.g., a Python package) and standardized benchmarks for certificate quality vs. compute would improve reproducibility and adoption.",2510.17472v2,https://arxiv.org/pdf/2510.17472v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:18:34Z
FALSE,NA,ML-based|Other,Other,Not applicable,Semiconductor/electronics|Healthcare/medical|Transportation/logistics|Energy/utilities|Network/cybersecurity|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/kclip/Edge-Cloud-Conformal-Alignment,"The paper studies edge–cloud model cascades that route inputs to a lightweight edge model or defer to a cloud model, aiming to preserve cloud-level reliability in the form of conditional coverage for prediction sets. It formalizes conditional coverage with respect to the cloud predictive distribution and proposes a conformal alignment-based (CAb) cascading mechanism that casts escalation as a multiple-hypothesis testing problem. The method provides statistical guarantees by controlling the false discovery rate (FDR) of conditional-coverage violations among the subset of inputs handled at the edge, yielding a guarantee on the expected fraction of edge decisions that satisfy the target conditional coverage. CAb is compatible with arbitrary edge prediction sets, including highest-mass sets and conformal prediction variants, and exposes a tunable trade-off among coverage, deferral rate, and set size. Experiments on CIFAR-100 image classification and TeleQnA multiple-choice QA show that CAb meets the target average satisfaction constraint while reducing cloud offloading compared to heuristic confidence-based deferral, with modest increases in prediction-set size.","The core reliability requirement is cloud-referenced conditional coverage: $\Pr[y\in\Gamma(x)\mid x]=p^*(\Gamma(x)\mid x)\ge 1-\alpha$. Edge decisions are selected to satisfy an average constraint equivalent to FDR control: with nulls $H_i: C^*(x_i)=p^*(\Gamma_e(x_i)\mid x_i)<1-\alpha$, the method ensures $\mathrm{FDR}=\mathbb{E}[\mathrm{FDP}(S)]\le\delta$ where $\mathrm{FDP}(S)=\frac{|\{x_i\in S: C^*(x_i)<1-\alpha\}|}{|S|}$. CAb uses sequential screening ordered by predicted alignment score $\hat C(x)$ and stops at $\ell_{CA}=\inf\{\ell:\widehat{\mathrm{FDP}}(\ell)\le\delta\}$ with $\widehat{\mathrm{FDP}}(\ell)=\frac{|D_{te}|}{1+|D_{val}|}\cdot\frac{1+|\{x_i\in D_{uns,val}(\ell): C^*(x_i)<1-\alpha\}|}{|D_{uns,te}(\ell)|}$.","On CIFAR-100 and TeleQnA, the proposed CAb cascade is reported to maintain the target average satisfaction (cloud-referenced conditional coverage) for edge-handled inputs while reducing deferral to the cloud relative to confidence-threshold baselines. For TeleQnA at target average satisfaction level $1-\delta=0.75$ (with conditional coverage requirement $1-\alpha=0.8$), CAb reduces deferral rate by approximately 60% compared to confidence-based deferral, at the cost of about a 20% increase in normalized inefficiency (prediction-set size relative to cloud). For CIFAR-100, edge-only methods show poor conditional satisfaction due to an over-confident edge model, and CAb is shown to enforce the satisfaction constraint across HMS/CP/LCP choices while trading off deferral rate and set size. All reported curves are averaged over 200 independent runs with dataset splits (with example sizes $|D_{cal}|=500$, $|D_{tr}|=200$, $|D_{val}|=500$, $|D_{te}|=100$).",None stated.,"This is not a reliability engineering paper in the classical sense (failure/maintenance/lifetime), and “reliability” here refers to statistical predictive coverage; mapping these guarantees to engineering reliability metrics (failure probability, risk, safety integrity) is not addressed. The guarantees rely on exchangeability between reference data and test inputs; realistic edge deployments often face temporal dependence and covariate shift, which can break the stated FDR/coverage guarantee. The approach also assumes offline access to cloud-evaluated alignment labels $C^*(x)=p^*(\Gamma_e(x)\mid x)$ for reference/validation data, which may be costly or infeasible at scale, and it hinges on a learned score predictor $\hat C(x)$ whose misranking could increase deferrals and inefficiency.","The authors suggest (i) evaluating robustness of CAb schemes under covariate shift, (ii) extending the framework to localized conformal alignment, and (iii) integrating conformal e-values to provide anytime-valid guarantees for sequential inputs in edge–cloud systems.","Explore extensions to handle autocorrelated/streaming edge data with explicit dependence modeling and to provide per-input (or group-conditional) guarantees beyond average/FDR control. Provide practical guidance and tooling for calibrating and validating exchangeability assumptions in real deployments, and quantify the cost/latency trade-offs when generating cloud-based alignment labels at scale. Evaluate the method under adversarial distribution shifts and under constrained-bandwidth settings where validation/test batching and ranking (sequential screening) may be operationally limited.",2510.17543v2,https://arxiv.org/pdf/2510.17543v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:19:11Z
FALSE,NA,Other,Other,Not applicable,Other,Other,TRUE,Python,Supplementary material (Journal/Publisher),NA,"The paper studies performative prediction, where forecasts influence the outcomes being forecast, and formalizes the problem using causal models. It shows that making forecasts conditional on separating covariates/actions can render the target distribution forecast-invariant, ensuring existence of correct conditional forecasts, but classical proper scoring rules generally fail to elicit truthful forecasts even under forecast-invariance (impossibility result). Two remedies are proposed: (i) in decision-theoretic settings, utility-based scoring can be made proper and incentive-compatible under structural conditions and a utility-gap assumption; (ii) scoring via unbiased estimates of the performative divergence between the forecast and the induced outcome distribution yields (observationally) strictly proper elicitation. The paper further connects these ideas to parameter estimation in performative environments, proving that divergence-based estimation under forecast-invariance yields performatively stable and performatively optimal parameters. Supplementary Python code is mentioned for generating plots and running synthetic experiments, but no repository URL is provided in the text excerpt.","Key definitions include correctness as a fixed point: for marginal forecasts, $F = P_M(Y\mid \mathrm{do}(F))$; for conditional forecasts, correctness is $F(Y\mid A=a)=P_M(Y\mid A=a,\mathrm{do}(F))$ (observationally for actions with positive probability under $\mathrm{do}(F)$). Expected performative score is $S_{pc}(F,M)=\int S(F,a,y)\,P_M(da,dy\mid \mathrm{do}(F))$. The performative divergence is $D_{pc}(F,M)=\int\big(S(P_M(Y\mid A=a,\mathrm{do}(F)),y)-S(F(Y\mid A=a),y)\big)\,P_M(da,dy\mid \mathrm{do}(F))$, and unbiased estimators of $D_{pc}$ are proposed for the Brier and energy scores.","Theorem 1 gives equivalence between existence of correct conditional forecasts for all models compatible with a causal graph and the graphical condition $Y\perp_d F\mid A$, which implies forecast-invariance $P_M(Y\mid A,\mathrm{do}(F))=P_M(Y\mid A)$. Theorem 2 establishes a general impossibility: for broad classes of performative mechanisms (including deterministic or full-support action-selection), standard non-constant classical scoring rules cannot be observationally proper for eliciting conditional forecasts. Theorem 4 shows a construction that is proper and incentive-compatible in a decision-theoretic setting: $S(F,a,y)=U(a_F,y)+\Delta\,S'(F(Y\mid A=a_F),y)$, yielding observational strict properness when $S'$ is strictly proper. Theorem 5 and Corollary 1 show that (unbiasedly estimated) performative divergence-based scoring yields proper (and observationally strictly proper) elicitation even without forecast-invariance, provided observationally correct forecasts exist. Theorem 6 shows divergence-based parameter updates yield correct forecasts that are performatively stable and performatively optimal under forecast-invariance and full-support action policies.","The discussion notes several restrictive assumptions: forecast-invariance may be hard to justify in practice; the conditioning/action variable $A$ is assumed discrete (extension to continuous $A$ is left open); and extensions from probabilistic forecasts to point forecasts/functionals are conjectured rather than developed. The paper also notes that some approaches require unbiased estimation of quantities like divergences or action probabilities, which may be non-trivial in practice.","The work is primarily theoretical and relies on strong causal-modeling assumptions (e.g., correct specification of the causal graph/separation and access to separating variables) that may be difficult to validate empirically. The divergence-based scoring approach hinges on availability of unbiased estimators of the performative divergence; for many scoring rules/outcome types, such unbiased estimators may not exist or may have high variance, affecting practical implementability despite unbiasedness being sufficient for properness in theory. The decision-theoretic construction assumes a known utility function and a uniform utility gap bound $\Delta$, which may be unrealistic or unknown to the principal and sensitive to misspecification. Empirical validation appears limited to synthetic examples/plots; real-world case studies demonstrating elicitation/estimation benefits are not evident from the provided text.","The authors propose scrutinizing and relaxing key assumptions, especially making forecast-invariance more practically justifiable and extending from discrete $A$ to continuous $A$. They conjecture extending the theory from probabilistic forecasts to point forecasts or other functionals of $P_M(Y\mid A,\mathrm{do}(F))$ using consistent scoring functions.","Useful extensions include developing robust versions under model misspecification (e.g., when the separating set is imperfect or the causal graph is uncertain), handling autocorrelation/feedback over time, and providing finite-sample guidance for divergence estimators (variance control, confidence intervals, and sample-size planning). Extending the unbiased-divergence approach to broader outcome spaces and widely used modern scores (e.g., CRPS variants, multivariate scores) would improve applicability. Implementing and releasing a reusable software package (with standardized benchmarks) and validating on real performative domains (e.g., traffic routing, marketplace pricing, public-policy forecasting) would strengthen practical impact.",2510.21335v1,https://arxiv.org/pdf/2510.21335v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:19:52Z
TRUE,System reliability|Other,Stochastic process|Simulation-based|Other,Simulated only,Not applicable,Transportation/logistics|Theoretical/simulation only|Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes a surface decomposition method (SDM) to compute parametric sensitivities of first-passage (first-excursion) failure probability for linear dynamic systems under Gaussian random excitation. The key idea is to express the reliability sensitivity as an integral over the (generally non-smooth) system limit-state surface and then decompose it into a sum of simpler surface integrals over constrained component limit-state hyperplanes (time- and response-component indexed). Because the system is linear and excitations are Gaussian, both component limit-state functions and their sensitivities admit closed-form linear expressions, enabling efficient sampling directly on each component hyperplane. An importance-sampling scheme over the discrete index set of component events is introduced using component failure probabilities as weights, and indicator functions enforce the “active” portion of each component hyperplane. Two numerical examples (a linear oscillator under stationary white noise and a 20-DOF shear building under nonstationary modulated Gaussian excitation) show that SDM matches finite-difference-with-IS references while requiring on the order of 10^2–10^3 system evaluations (typically <2000) and reuses evaluations across many design parameters.","Dynamic response at time step i is linear in the standard Gaussian vector: $r_i(X)=a_i^r X$ with $a_i^r=\sum_{j=1}^i a_{i,j}^r\,\psi_j$, and response sensitivity is $\partial r_i/\partial\theta=b_i^r X$ with $b_i^r=\sum_{j=1}^i b_{i,j}^r\,\psi_j$. Component first-passage limit state is a hyperplane $g_{k i}(x)=c_k-a_{i}^{s_k}x$, giving closed-form component failure $P_{k i}=\Phi(-\beta_{k i})$ with $\beta_{k i}=c_k/\|a_{i}^{s_k}\|$. System failure probability sensitivity is decomposed as $\partial P/\partial\theta=\sum_{k=1}^m\sum_{i=1}^n \eta_{k i}$ where $\eta_{k i}=-\int_{g_{k i}(x)=0} \frac{1}{\|\nabla g_{k i}\|}\,\frac{\partial g_{k i}}{\partial\theta}\,I_{k i}(x) f_X(x)\,dS$, and is estimated with discrete importance sampling over $(k,i)$ using weights proportional to $P_{k i}$.","In the oscillator example (T=20 s, n=1000, 1000-D Gaussian input), SDM estimated sensitivities w.r.t. $\omega_n$ and $\zeta_n$ with target COV 0.1 using 267–839 system evaluations across thresholds where failure probabilities ranged from $3.06\times10^{-3}$ down to $4.01\times10^{-9}$. The SDM sensitivity estimates closely matched finite-difference-with-importance-sampling (FDM-IS) references (e.g., at c=0.020 m: SDM $\partial P/\partial\omega_n\approx-2.46\times10^{-8}$ and $\partial P/\partial\zeta_n\approx-1.98\times10^{-6}$ versus FDM-IS $-2.36\times10^{-8}$ and $-2.02\times10^{-6}$), while FDM-IS required about $1.36\times10^{5}$ to $3.47\times10^{5}$ evaluations. In the 20-DOF shear-building example (T=30 s, n=1500, m=20; 30,000 component events), SDM computed sensitivities for 40 damper parameters using 877–1663 evaluations and matched FDM-IS for checked parameters (e.g., Case $S_0=0.010$: SDM $\partial P/\partial k_{ve,1}\approx-1.06\times10^{-9}$ vs. FDM-IS $-1.07\times10^{-9}$; SDM $\partial P/\partial c_{ve,1}\approx-3.93\times10^{-9}$ vs. FDM-IS $-4.05\times10^{-9}$). The paper reports the counterintuitive but favorable trend that efficiency increases as the target failure probability decreases, similar to efficient IS methods for first-excursion probability estimation.",None stated.,"The approach relies critically on linear structural dynamics and Gaussian excitation to obtain hyperplane component limit states with closed-form component probabilities and convenient sampling on $g_{k i}(x)=0$; extension to nonlinear systems or non-Gaussian loads is nontrivial and not demonstrated. The method assumes the time-discretized first-passage formulation; accuracy depends on the chosen time step and may require large n (increasing the number of component events), which could degrade performance for long durations or finer discretizations. Numerical validation is limited to two simulated examples; broader benchmarking against alternative sensitivity estimators (e.g., subset simulation sensitivity, sequential importance sampling sensitivity) and robustness to modeling errors (e.g., mild nonlinearity, nonstationarity complexities) is not shown.","The paper states that the method has great potential for reliability-based parametric and topology optimization, leveraging reuse of function evaluations across many parameters.","Develop extensions for mildly nonlinear dynamics (e.g., local linearization or piecewise-linear limit states) and for non-Gaussian excitations (e.g., transformation methods or mixture representations) to broaden applicability. Study discretization error vs. computational cost tradeoffs for first-passage problems (adaptive time stepping or event-driven refinement near excursions). Provide open-source implementations and standardized benchmarks, and investigate variance-reduction/optimal discrete PMFs beyond weighting by component failure probabilities (e.g., using pilot estimates of $\eta_{k i}$ or adaptive schemes).",2510.22558v1,https://arxiv.org/pdf/2510.22558v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:20:27Z
FALSE,NA,Nonparametric/Semi-parametric|Other,Complete lifetime data|Other,Not applicable,Service industry|Other,Approximation methods|Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,https://doi.org/XXXXXXX.XXXXXXX|arXiv:2510.23666v1,"The paper analyzes why the commonly used two-sample Welch t-test can produce unreliable (mis-calibrated and directionally imbalanced) Type I error rates in online A/B testing when outcomes are skewed/long-tailed and treatment-control allocations are unequal. It derives closed-form expressions linking the skewness and kurtosis of the mean difference to the skewness/kurtosis of each group and the allocation ratio, and uses Edgeworth expansions to quantify left- and right-tail rejection probability errors. Using these expansions, the authors provide explicit formulas for minimum total sample size thresholds (first- and second-order) required to keep per-tail Type I error deviations within a user-specified tolerance. For settings where the sample size is below these thresholds, they propose a plug-in Edgeworth-corrected p-value (with truncation to [0,1]) to improve finite-sample calibration. Synthetic lognormal experiments and large-scale platform A/A-style evaluations on real metrics (e.g., publish count, live duration) show the correction reduces tail imbalance and achieves the tolerance at smaller N than the classical t-test.","Welch’s statistic is $T=(\bar Y-\bar X)/\sqrt{\hat\sigma_x^2/n_x+\hat\sigma_y^2/n_y}$ with classical two-sided p-value $p_t=2\{1-\Phi(|T|)\}$. The paper derives skewness and kurtosis of the mean difference $D=\bar Y-\bar X$ as functions of group moments and allocation ratio $k=n_y/n_x$ (Eqs. (1)–(2)), then uses an Edgeworth expansion $G(x)=\Phi(x)+\phi(x)\{q_1(x)+q_2(x)\}+O(N^{-3/2})$ with $q_1(x)=\gamma_D(2x^2+1)/6$ and a second-order polynomial $q_2(x)$ depending on $\tau_D,\gamma_D,k,N$ (Eqs. (5)–(6)). The corrected two-sided p-value is $p_c=2\min\{\hat G_{tr}(T),1-\hat G_{tr}(T)\}$ where $\hat G_{tr}$ plugs in sample skewness/kurtosis and truncates to $[0,1]$ (Eq. (7)); minimum-sample thresholds are given by $N_{\min}^{(1)}=(a_1/\epsilon)^2$ and a refined second-order $N_{\min}^{(2)}$ using $a_1,a_2$ (Eqs. (3)–(4)).","For synthetic $\text{LN}(0,1)$ data with allocation ratio $k=5$, $\alpha=0.05$, and tolerance $\epsilon=0.01$, the derived thresholds are reported as $N_{\min}^{(1)}=8712$ and $N_{\min}^{(2)}=5986$, and simulations with $B=10^4$ show tail deviations fall within $\epsilon$ once $N\ge N_{\min}^{(2)}$. On real “publish count” data (skewness $\gamma_x=\gamma_y=14.94$, kurtosis $\tau_x=\tau_y=490.7$, $k=5$), the theory gives $N_{\min}^{(1)}=51094$ and $N_{\min}^{(2)}=35042$, and empirical tail deviations for the classical t-test meet the $\epsilon=0.01$ criterion around the second-order bound while the Edgeworth-corrected method remains within tolerance at smaller sample sizes. On real “live duration” data ($\gamma_x=\gamma_y=5.09$, $\tau_x=\tau_y=41.9$, $k=10$), the bounds are $N_{\min}^{(1)}=15022$ and $N_{\min}^{(2)}=9361$, again with the correction stabilizing tail errors below tolerance earlier. Under more extreme allocations (e.g., publish count $k=9$ at $\alpha=0.1,\epsilon=0.03$), $N_{\min}^{(2)}=31422$ closely matches empirical results; for live duration $k=99$, $N_{\min}^{(2)}=50347$ is close to an empirical minimum around $N\approx 51300$.",None stated.,"The work targets hypothesis-testing calibration in online experiments rather than reliability engineering (failure/degradation/maintenance), so its “reliability” is statistical and may not transfer to engineering reliability contexts. The Edgeworth correction relies on accurate estimation of third/fourth moments; for very heavy-tailed metrics, unstable moment estimates (or infinite moments) can degrade performance despite the truncation of $\hat G$. The method assumes i.i.d. observations and independence between groups; common A/B testing complications (autocorrelation, clustering, interference, repeated measures) could invalidate both the thresholds and the correction without additional adjustments.",The authors propose extending the framework to multi-arm and sequential testing settings.,"A useful extension would be robust/self-normalized variants that avoid dependence on high-order moments (e.g., winsorized/trimmed or M-estimation-based corrections) for extremely heavy-tailed metrics. Another direction is adapting the theory to clustered and dependent data typical in online platforms (e.g., user-session hierarchies, network interference) and to variance-reduction methods (CUPED/regression adjustment) to see how they modify the derived cumulants and minimum-N thresholds. Providing an open-source reference implementation and guidance for stable moment estimation/regularization would improve practical adoption and reproducibility.",2510.23666v1,https://arxiv.org/pdf/2510.23666v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:21:05Z
FALSE,NA,Bayesian|Other,Mixture of types|Simulated only|Other,Not applicable,Service industry|Other,Simulation study,TRUE,Other,Not provided,NA,"The paper addresses stochastic variability in LLM classification outputs by reframing it as a measurement-error problem and proposes a Bayesian latent-state (latent class) model. The true class (e.g., customer dissatisfaction) is modeled as an unobserved latent variable, while repeated LLM outputs for the same item are treated as noisy measurements, enabling joint estimation of base rates, false-positive/false-negative error rates, and (optionally) treatment effects for causal inference. The core likelihood is a two-component mixture with binomial observation models conditional on the latent state, fit via MCMC. An extension allows per-item heterogeneous error rates using an LLM-provided ‘difficulty’ covariate to capture correlation/heterogeneity across repeated ratings. Simulation studies show the proposed approach recovers true parameters and outperforms naive single-run and majority-vote aggregation, while highlighting a tradeoff between more rounds (better latent-state recovery) and bias/identifiability for impact parameters in the heterogeneous model.","Latent prevalence is modeled with a logistic regression: $\text{logit}(\theta_c)=\theta+X_c\beta+\tau T_c$. The observed count of positive ratings $k_c=\sum_{i=1}^{N_c}R_{c,i}$ follows a mixture likelihood: $P(k_c\mid\theta_c,\epsilon_0,\epsilon_1)=(1-\theta_c)\,\text{Binom}(k_c;N_c,\epsilon_0)+\theta_c\,\text{Binom}(k_c;N_c,1-\epsilon_1)$. Priors include $\epsilon_0,\epsilon_1\sim\text{Beta}(1,1)$ truncated to $<0.5$ to mitigate label switching; posterior class probability uses Bayes’ rule: $P(D_c=1\mid k_c,\hat\theta_c,\hat\epsilon_0,\hat\epsilon_1)=\frac{P(k_c\mid D_c=1,\hat\epsilon_1)\hat\theta_c}{P(k_c\mid\hat\theta_c,\hat\epsilon_0,\hat\epsilon_1)}$. The extension models per-call errors via $\text{logit}(2\epsilon_{0c})=\alpha_0+\gamma_0H_c$ and $\text{logit}(2\epsilon_{1c})=\alpha_1+\gamma_1H_c$.","In Simulation Study 1 (C=1,000 calls; N=5 ratings/call; true $\epsilon_0=0.15$, $\epsilon_1=0.10$, $\tau=-0.8$), posterior means were $\hat\epsilon_0=0.159$ and $\hat\epsilon_1=0.105$, and $\hat\tau=-0.711$ with ATE $\hat\eta=-0.0764$ (true ATE -0.0871). Latent-state estimates achieved correlation 0.959 with the true latent state and AUC 0.9985; naive N=1 had correlation 0.575 and strongly biased error-rate/treatment estimates, while majority vote achieved correlation 0.915 but lacked uncertainty propagation. In Simulation Study 2 with heterogeneous error rates and N=10, the heterogeneous model achieved the best latent-state correlation (0.798) versus homogeneous (0.725) and majority vote (0.659). For impact recovery in Study 2, majority vote deteriorated (e.g., $\tau=-0.52$ vs true -0.80), and the paper notes that increasing rounds can improve state recovery but may worsen treatment-effect recovery due to mixture-component confounding (e.g., heterogeneous N=10 $\tau=-0.86$ vs heterogeneous N=5 $\tau=-0.84$ and homogeneous N=10 $\tau=-0.61$).","The base model assumes constant (call-invariant) false-positive and false-negative rates and conditional independence of repeated LLM ratings given the latent state; the authors note these are simplifications and motivate an extension to capture heterogeneity and within-call correlation via a ‘difficulty’ variable. They also report label-switching as an issue in unconstrained two-class mixture models, requiring truncation/constraints (e.g., error rates < 0.5) to obtain identifiable posteriors. In Simulation Study 2, they explicitly acknowledge a tradeoff where more LLM rounds can cause the likelihood to be dominated by the measurement component, leading to confounding and more biased estimates of impact parameters ($\tau$, $\eta$).","The evaluation is primarily simulation-based; the absence of real-world case studies limits evidence about robustness under practical violations (prompt drift, distribution shift, nonstationary error rates across topics/time, and dependence induced by shared prompts/system messages). The model treats repeated LLM draws as i.i.d. conditional on the latent state (and optionally difficulty), but in practice LLM outputs can be correlated via caching, deterministic decoding settings, or shared randomness controls, which may invalidate the binomial assumption. Identifiability can be fragile without informative priors or design constraints; truncating error rates to <0.5 resolves label switching but may mask cases where the model is worse than random for some subpopulations or labels. The causal component assumes randomized treatment or all confounders observed; when LLM measurement error depends on treatment (differential misclassification), causal estimates could still be biased unless explicitly modeled.","The authors suggest extending the framework to multinomial or ordinal ratings (e.g., 1–5 star outcomes). They propose broader comparisons against more state-of-the-art aggregation methods from the inconsistent-human-ratings literature (e.g., multi-annotator competence estimation) and exploring adjudication of conflicting LLM outputs via a meta-LLM or follow-up prompt. They also suggest applying the method in real case studies to evaluate and compare LLM error rates across tasks, prompts, temperature settings, and model choices.","Develop self-starting/online variants that track time-varying base rates and drifting LLM error rates, enabling monitoring in production settings. Extend to differential and feature-dependent misclassification (instance-conditional noise) beyond a single difficulty score, e.g., hierarchical models with prompt/model/version random effects and interaction terms with treatment to protect causal identification. Provide posterior predictive checks and sensitivity analyses (e.g., dependence among repeats, misspecified binomial variance/overdispersion) and compare against alternative probabilistic aggregators (GLAD, Item Response Theory/crowd models) under matched assumptions. Release a reference implementation (Stan + helper scripts) and guidance on choosing the number of repeats $N$ under compute budgets via decision-theoretic design (optimize utility for state estimation vs impact estimation).",2510.23874v3,https://arxiv.org/pdf/2510.23874v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:21:48Z
FALSE,NA,Simulation-based|Other,Simulated only|Other,Not applicable,Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,R,Not provided,https://www.R-project.org/,"This paper evaluates the reliability/accuracy of Representational Similarity Analysis (RSA) for model selection in behavioral and neuroimaging contexts, comparing it directly to regression. Using extensive Monte Carlo simulations (typically 1000 replications per condition) across varying sample sizes, noise levels, feature dimensionality, and multicollinearity, the authors find that RSA yields much lower discriminability between competing models and lower model selection accuracy than regression. They show RSA’s performance degrades with increasing feature collinearity, and that PCA-based RSA and feature-reweighted RSA (ridge-regression weights) can mitigate collinearity-induced losses but still do not surpass regression. A follow-up fMRI-like spatial simulation (voxel grid with radial spatial structure) and an empirical lexical decision dataset analysis (SCOPE metabase) replicate the core pattern: regression remains superior for model selection when direct stimulus–response mappings are available. The practical implication is that RSA’s second-order abstraction to dissimilarity matrices can discard stimulus–response information relevant for selecting among competing models, so regression/encoding approaches are preferred when applicable.","Data are generated from a linear model $y_i = \beta_0 + x_i^T\beta + \epsilon_i$ with $x_i\sim\mathcal{N}_p(\mu_X,\Sigma_X)$ and $\epsilon_i\sim\mathcal{N}(0,\sigma_\epsilon^2)$. RSA uses correlational distance for features, $D^{cd}=1-R$ where (after row-centering) $R\approx X_cX_c^T/p$, and Euclidean distance for scalar responses, $D^{ed}=|Y\mathbf{1}^T-\mathbf{1}Y^T|$, then computes Spearman rank correlation between vectorized dissimilarities. Regression performance is summarized by adjusted $R^2$: $R^2_{adj}=1-(1-R^2)\frac{n-1}{n-p-1}$; in the fMRI extension, regression uses a mixed-effects model with conditional $R^2$: $R^2_{GLMM(c)}=(\sigma_f^2+\sigma_\alpha^2)/(\sigma_f^2+\sigma_\alpha^2+\sigma_\epsilon^2)$.","Across simulations, regression’s adjusted $R^2$ distributions show greater separation between larger-effect (coefficients 0.5) and smaller-effect (0.4) models than RSA’s Spearman $\rho$, which is often very small (reported as often <0.05; in some noise conditions around 0.02). Increasing sample size improves both methods, but regression’s model-selection advantage persists and can grow with larger $N$. Increasing the number of features (20→40→60) selectively improves regression’s separability/accuracy but not RSA’s. Increasing multicollinearity selectively harms RSA’s model-selection accuracy and Cohen’s $d$, while regression is comparatively unaffected; PCA-based RSA and ridge-reweighted RSA remove most of the collinearity penalty but still do not outperform regression. In an empirical example (100-word sample), regression shows $R^2_{adj}=0.533$ (frequency model) vs 0.081 (affect model), while RSA shows near-nondiscriminative $\rho\approx 0.059$ vs 0.061; PCA RSA improves to 0.135 vs 0.022 and reweighted RSA to 0.311 vs 0.061.",None stated.,"The simulations generate data from linear regression ground truth, which can advantage regression for model selection relative to RSA (even though RSA can be applied to the generated data); performance under nonlinear or non-monotonic stimulus–response mappings is not fully characterized. Evaluation focuses on model-selection discriminability between two close coefficient magnitudes (0.5 vs 0.4) and specific distance/correlation choices (correlation distance for X, Euclidean for scalar Y, Spearman correlation), so conclusions may vary with alternative RSA variants (e.g., cross-validated distance estimators, noise-ceiling corrections, whitening/unbiased distances) or different task structures. The paper references multiple R packages (glmnet, lme4, MuMIn) but does not describe computational details like runtime/complexity or provide reproducible scripts, limiting practical replicability.",None stated.,"Assess RSA vs regression when the true data-generating process is nonlinear (e.g., kernel/nonlinear encoding models) or when only relational/second-order structure is available (settings where regression is misspecified). Evaluate modern RSA variants that address bias/noise in dissimilarity estimates (e.g., cross-validated Mahalanobis / unbiased distances, whitening) and compare fairly against regularized regression baselines. Provide open, runnable code and standardized benchmarks (behavioral and fMRI) to enable reproducible comparisons across RSA/regression variants and to explore robustness under autocorrelation, heteroscedasticity, and unknown parameter settings.",2511.00395v2,https://arxiv.org/pdf/2511.00395v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:22:23Z
FALSE,Other,Other,Other,Not applicable,Other,Other,TRUE,R,Not provided,https://github.com/lrberge/fixest/blob/master/_DOCS/FENmlm_paper.pdf|https://doi.org/10.32614/CRAN.package.irr|https://www.R-project.org|https://doi.org/10.32614/CRAN.package.dplyr,"This paper analyzes official scoring data from the 2024 Paris Olympic Breaking events to assess the consistency of judges and the presence of national bias. Interrater reliability is quantified using a two-way random-effects, unadjusted single-measures Intraclass Correlation Coefficient (ICC), with both within-battle ICCs and overall ICCs reported across events, rounds, stages, and performance aspects. National bias is estimated using linear mixed-effects models with performance fixed effects, capturing whether a judge shares nationality with either athlete in a head-to-head comparison; extensions test whether bias changes when both athletes’ nationalities are represented on the panel, across competition stages, across performance aspects, and by event (B-Boys vs B-Girls) and judge. Results show low-to-moderate reliability (within-battle ICCs often near ~0.15–0.23; overall ICC for overall scores 0.425) and a statistically significant average pro-compatriot scoring advantage (~2.47 percentage points), with bias concentrated in a small subset of judges. The paper argues that Breaking’s vote-aggregation system limits the practical impact of individual-judge bias on outcomes, though it highlights vulnerabilities and potential refinements to the judging framework.","Interrater reliability uses the unadjusted two-way random-effects single-measures ICC: $\mathrm{ICC}_s(A,1)=\frac{\sigma_p^2}{\sigma_p^2+\sigma_j^2+\sigma_{pj}^2+\sigma_\varepsilon^2}$, where variance components represent performance, judge, judge\times performance interaction, and residual error. National bias is estimated via mixed-effects models of scores with performance fixed effects, e.g., $S_{jpa(r,b)}=\theta_{pa(r,b)}+\beta_{NB}\,\phi(\text{judge shares nationality with red XOR blue athlete})$, with extensions adding BAR indicators and stage/aspect interactions (Models 2–6).","Within-battle ICCs for “All Aspects” across 64 battles ranged from −0.027 to 0.611 with median 0.156 (mean ≈0.174), indicating low–moderate agreement among judges. Overall ICCs (pooled across battles) were 0.425 for overall scores and about 0.318–0.371 across aspects (Technique 0.348; Vocabulary 0.320; Originality 0.326; Execution 0.371; Musicality 0.318). Mixed-effects estimates show a significant pro-compatriot effect of about +2.47 percentage points on overall scores ($\beta_{NB}=2.474$, p<0.001); BAR conditions did not significantly change bias ($\beta_{BAR}=0.379$, p=0.641) and stage progression did not increase bias ($\beta_S=0.027$, p=0.937). Aspect-wise bias was largest for Musicality (~0.911, p<0.001) and Technique (~0.715, p<0.001), with smaller significant effects for Execution (~0.326) and Originality (~0.281). Judge-specific analysis indicates bias is driven mainly by two judges (≈+4.75 and +5.85), while one judge showed negative bias (≈−4.80, not significant).","The authors note the scope is limited to a single Olympic competition (two events) with only nine Olympic-level judges, which constrains generalizability to other competitions and to lower levels of judging expertise. They also state that judge-specific bias estimates are averages over many observations and may not capture variability in bias at individual scoring instances, limiting inference about case-by-case outcome impacts.","The study is not about engineering reliability; its “reliability” is psychometric/interrater agreement, so conclusions do not transfer to reliability engineering contexts (failure, lifetime, maintenance). ICC estimates assume a particular variance-components structure and may be sensitive to non-independence within battles/rounds and to the bounded, discretized score scale (0–20% in 0.2% steps), which could affect variance estimates and ICC interpretation. The outcome-impact analysis relies on model-based bias adjustments and may be sensitive to model specification (e.g., fixed effects for performance comparisons) and unmodeled heterogeneity (judge learning/fatigue, order effects, or interaction with athlete prominence).","The authors propose extending analyses to additional Breaking competitions and events with a broader range of judge expertise, and tracking how judging reliability and bias evolve as the sport and its scoring norms develop over time. They also suggest that studying changes in scoring systems and the shared conceptual basis for judging could clarify how such factors influence reliability and bias.","Future work could test robustness of ICC and bias estimates under alternative dependence structures (e.g., hierarchical models nesting scores within rounds within battles) and under transformations appropriate for bounded percentage scales. Replicating with multiple seasons/competitions would enable meta-analytic estimation of judge reliability and bias and identification of moderators (panel composition, stakes, crowd effects). Developing and validating interventions (training, calibration sessions, revised scoring workflow such as separate independent scoring before comparison) could be evaluated using randomized or quasi-experimental designs, alongside open-source reproducible code and data pipelines for transparency.",2511.00553v1,https://arxiv.org/pdf/2511.00553v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:22:58Z
FALSE,Other,ML-based|Other,Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,Python|Other,Not provided,https://doi.org/10.13026/kpb9-mt58,"The paper introduces CELEC, an LLM-powered framework that enables researchers to query and analyze EHR databases by translating natural-language questions into executable DuckDB SQL and optionally producing simple visualizations. The key design constraint is privacy: the LLM sees only schema-level metadata (table/column names and types), while all SQL execution happens locally inside the institutional environment so patient-level data are never sent to the model. The SQL generation prompt integrates schema grounding, retrieval of top-k similar few-shot demonstrations (k=2) using MiniLM embeddings and ChromaDB, and an intermediate chain-of-thought table-selection step; execution errors are fed back for up to two retries. CELEC is evaluated on an adapted (executable-only) split of the EHRSQL-2024 benchmark, achieving RS(0)=81.05% with ~6.0s average latency per query and very low abstention (0.14%). Ablations show large gains from few-shot demonstrations (zero-shot 50.21% vs two-shot 81.05%) and smaller but consistent gains from schema info and retrying.","The paper evaluates using the EHRSQL metric RS(0), defined as an average over questions that scores 1 if (i) an answerable question’s generated result matches the gold execution result, or (ii) an unanswerable question is correctly abstained from. Formally, $\mathrm{RS}(0)=\frac{1}{|X|}\sum_{x\in X} \mathbf{1}[(x\in X_{ans}\wedge R_{gen}(x)=R_{gt}(x))\vee(x\notin X_{ans}\wedge g(x)=0)]$. Under the authors’ adapted test set where all questions are answerable ($X_{ans}=X$), RS(0) reduces to execution accuracy.","On the adapted EHRSQL-2024 test set (707 queries), CELEC achieves RS(0)=81.05% and abstains in 0.14% of cases, with average per-query latency reported as 6.02–6.11 seconds. Changing the base model reduces RS(0) to 73.41% (o4-mini) or 77.93% (gpt-4.1). Removing schema information drops performance from 81.05% to 77.93%. Few-shot prompting is most impactful: 0 demos yields 50.21%, 1 demo yields 73.97%, and 2 demos yields 81.05%; allowing one retry (two attempts vs one) improves 79.21% to 81.05%.","The authors note that their evaluation uses an adapted benchmark split (they filter/modify EHRSQL and remove unanswerable questions), which is therefore not identical to the official leaderboard split; they argue results remain informative for comparison. They also indicate that other systems often do not disclose inference times, limiting direct latency comparison.","The privacy claim is largely scoped to “no patient-level data is sent to the LLM,” but the paper does not deeply analyze residual risks such as schema leakage, prompt injection via user queries, or whether error messages could inadvertently encode sensitive values in other deployments. Generalization beyond MIMIC-III/MIMIC-IV demo schemas and beyond DuckDB dialect is not fully validated; performance may depend on the quality/coverage of the demo retrieval corpus and the adapted dataset filtering that removes empty-result queries. The visualization module appears to rely on hard-coded rendering functions and column metadata only; its correctness/utility is not evaluated quantitatively.",None stated.,"Evaluate CELEC on additional real institutional EHR schemas and on non-demo MIMIC deployments to test robustness to schema drift and local conventions. Add formal threat-modeling and defenses (e.g., prompt-injection mitigation, stricter query allowlists, and auditing of error-message content) to strengthen the privacy/security story. Provide an open-source reference implementation and standardized benchmarking protocol (including unanswerable questions and empty-result handling) to enable reproducible comparisons across systems.",2511.00772v1,https://arxiv.org/pdf/2511.00772v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:23:26Z
TRUE,Software reliability|Other,"Bayesian|Parametric (Weibull, etc.)|Other",Event/count data|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper develops conservative software reliability assessments using robust/conservative Bayesian inference over a credal set of priors defined by partial prior specifications (probability masses assigned to parameter intervals). Using a Bernoulli (i.i.d.) failure-on-demand model with unknown probability of failure per demand (pfd), it derives explicit worst-case (minimum) posterior predictive probabilities that the software will survive the next m demands without failure after observing k successes and r failures. The key methodological contribution is an explicit solution that reduces the infinite-dimensional optimization over priors to a finite-dimensional nonlinear fractional program and then to a unique fixed-point characterization, yielding degenerate (discrete) least-favorable priors. The paper also analyzes asymptotic and monotonic properties of the conservative posterior predictive probability as evidence accumulates (k increases, r increases), and discusses practical implications for safety-critical operational testing and certification. It positions the results as unifying and extending prior conservative Bayesian inference (CBI) methods in software dependability assessment and connecting them to robust Bayesian analysis and fractional programming techniques (e.g., Dinkelbach-type iteration).","Software failures on demands are modeled as Bernoulli trials with unknown pfd $X\in[0,1]$ and partially specified prior constraints $P(X\in I_i)=p_i$. The conservative posterior predictive reliability for surviving the next $m$ demands after $k$ successes and $r$ failures is formulated as the infimum of a ratio (Eq. 3):
$$\inf_{x_i\in I_i}\frac{\sum_{i=1}^n x_i^{r}(1-x_i)^{m+k}p_i}{\sum_{i=1}^n x_i^{r}(1-x_i)^{k}p_i}.$$
The minimizing value $\phi^*$ is characterized by a unique fixed point involving two points $y^{**}<\frac{r}{r+m+k}<\frac{r}{r+k}<y^*$ satisfying $h(y^{**})=\phi^*=h(y^*)$ with $h(x)=(1-x)^m\frac{r-x(m+k+r)}{r-x(k+r)}$, and the least-favorable prior is discrete with probability masses placed on interval endpoints and at $y^*$ (and related boundary points) as specified in Eqs. (4) and (6) for the $r=0$ case.","The paper provides explicit analytical expressions for the worst-case posterior predictive probability $\phi^*$ (Theorem 1) and for the no-failure limit $r=0$ (Corollary 1), showing the infimum is achieved by a degenerate/discrete least-favorable prior concentrated at interval endpoints and a fixed-point location $y^*$. It establishes qualitative behavior that $\phi^*$ increases monotonically with additional observed successes $k$ and decreases monotonically with additional observed failures $r$, and gives bounds including $0\le \phi^*\le (1-y_2)^m$ (discussed with proofs in supplementary appendices). In a comparison to a uniform Beta(1,1) prior operational-testing calculation (Littlewood & Wright, 1997), the required number of demands under CBI can be about an order of magnitude larger for small numbers of failures, depending on the partial prior specification (Table 1). The paper highlights a structural requirement that with failures ($r>0$) and too few constraining intervals (e.g., only two), the conservative infimum can collapse to $\phi^*=0$ regardless of how many successes are observed, motivating at least three intervals (and/or fault-freeness mass) to avoid trivial conservatism. It further discusses asymptotic fixed-point behavior as $r\to\infty$ under a fixed target posterior predictive bound, relating limits of $y^*$ and $y^{**}$ to $1-(\phi^*)^{1/m}$ and $r/(r+k)$.","The authors note that CBI still requires belief elicitation despite reducing the burden relative to fully specified priors, raising the open issue of how best to elicit partial prior specifications from diverse evidence. They also point out that the main example assumes i.i.d. Bernoulli outcomes on demands, whereas in practice failures/successes may be correlated and model deviations can matter, motivating broader modeling (they reference Klotz-type dependence models and related prior work). They mention that their exposition focuses on the simpler “no future failures allowed” predictive event (Eq. 3) though generalizations allowing up to $\ell$ future failures (Eq. 7) are possible. They also highlight feasibility subtleties: the conservative prior achieving the bound may be a limit of feasible priors (e.g., placing mass on an open interval endpoint).","The work is largely theoretical and does not provide an implemented algorithm, numerical benchmark suite, or software package to compute the fixed point and least-favorable prior for arbitrary interval specifications, which may hinder adoption in practice. The primary model is single-parameter (univariate pfd) and focuses on on-demand software; extensions to richer operational profiles (covariates, changing environments, nonstationarity) and multi-component software/system structures are not developed here. Because the credal set is defined via interval probability constraints, results may be sensitive to how the partition is chosen (number/placement of intervals), but systematic guidance or robustness analysis over alternative partitions is limited. The paper’s conservatism criterion is tailored to a single posterior predictive probability; practitioners often need simultaneous constraints (e.g., both pfd bounds and mission reliability), which can change the least-favorable prior and may require multi-objective formulations beyond those presented.","They propose developing better methods for eliciting partial prior specifications from diverse evidence, including using imprecise probabilities where the interval masses $p_i$ are themselves ranges $[\underline p_i,\overline p_i]$. They suggest extending the approach beyond on-demand software to continuously operating software reliability assessment, where different effects may yield meaningfully different results. They note the value of proving convergence guarantees and characterizing convergence speed for fixed-point/Dinkelbach-type iterative solvers for broader classes of CBI problems. They also propose broadening the framework from single-objective conservative questions to multi-objective conservative assessments that incorporate other dependability attributes (security, maintainability, timeliness, etc.) or multiple reliability questions simultaneously.","Providing an open-source reference implementation (e.g., Python/R) for solving the fixed-point system and constructing least-favorable priors for arbitrary interval constraints would greatly improve reproducibility and uptake. Extending the approach to handle unknown/estimated operational-test conditions (e.g., covariate-dependent pfd, hierarchical priors capturing environment-to-environment variability) could make the method more applicable to heterogeneous deployment settings. A robustness study comparing different ways of specifying credal sets (interval probabilities vs. moment constraints vs. $\epsilon$-contamination) on the same evidence could clarify when each specification is overly conservative or insufficiently cautious. Finally, developing diagnostics to interpret the resulting degenerate least-favorable priors (e.g., explaining which intervals drive conservatism and why) would help safety assessors communicate and justify conservative claims in certification contexts.",2511.07038v1,https://arxiv.org/pdf/2511.07038v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:24:13Z
TRUE,Other,"Bayesian|Parametric (Weibull, etc.)|Other",Simulated only|Other,Not applicable,Healthcare/medical|Theoretical/simulation only|Other,Simulation study,TRUE,R|Python,In text/Appendix,NA,"The paper proposes a new reliability (measurement consistency) assessment approach motivated by limitations of classical test-based estimators (KR20/KR21, Cronbach’s alpha) and conventional exploratory factor analysis (EFA), especially when many variables interact and covariance is non-negligible. The method introduces a probabilistic multivariate model with a latent reliability parameter $\theta$ and an interaction function $\Phi(\theta, x_i)$ to explicitly account for dependencies across variables and to reduce subjectivity associated with EFA rotations. Theoretical discussion leverages variance decomposition ideas (including covariance terms) and references Cochran’s theorem/ANOVA-style decompositions to argue unbiasedness and improved consistency when covariance is modeled. Performance is evaluated via simulation across multivariate settings (reported as $k=5$–$20$ variables), comparing estimation error against KR20 and EFA. The simulations report materially lower average error and variability for the proposed approach, suggesting improved reliability estimation in higher-dimensional settings.","Traditional reliability is defined as a variance ratio: $r_{xx}=\frac{V_x-\sigma_\varepsilon^2}{V_x}=1-\frac{\sigma_\varepsilon^2}{V_x}$. KR20/KR21 are given by $\mathrm{KR20}=\frac{k}{k-1}\left(1-\frac{\sum_{j=1}^k p_j(1-p_j)}{\sigma_X^2}\right)$ and $\mathrm{KR21}=\frac{k}{k-1}\left(1-\frac{k\bar p(1-\bar p)}{\sigma_X^2}\right)$. The proposed method posits a joint model $P(X\mid\theta)=\prod_{i=1}^k P(X_i\mid\theta)\,\Phi(\theta,x_i)$ to incorporate inter-variable dependence via $\Phi$. EFA is summarized by $X_i=\sum_{j=1}^m a_{ij}F_j+\epsilon_i$ with rotated loadings $A^*=AT$ (orthogonal $T$).","In the simulation study (1000 generated multivariate datasets, stated $k=5$–$20$ variables), Table 1 reports average estimation error (%) and standard deviation: KR20 12.3 (SD 2.1), EFA 8.7 (SD 1.9), and the new approach 4.5 (SD 0.8). The authors claim the new approach reduces theoretical error to below 5% and reduces error by about 50% versus traditional methods in simulated (and stated real) data settings. The reported variability of error is also substantially smaller for the proposed method than for KR20/EFA.",None stated.,"The work uses “reliability” in the psychometric/measurement sense (variance due to true score vs. error) rather than engineering time-to-failure reliability, so applicability to reliability engineering (lifetimes, hazard rates, RUL, maintenance) is not established. The evaluation is primarily simulation-based and the description of any real-data validation is not substantiated with a concrete dataset or reproducible results. The proposed probabilistic model includes an interaction function $\Phi$ but does not specify principled choices, identifiability conditions, sensitivity analyses, or how hyperparameters are selected, which may strongly affect performance. Comparisons appear limited (KR20 and EFA only) and do not include widely used modern reliability estimators (e.g., McDonald’s $\omega$, G-theory, SEM-based reliability) or robustness checks (non-normality, missingness mechanisms, dependence structures).","The authors propose optimizing the algorithm by developing a distributed MCMC implementation to improve computational efficiency, targeting the ability to process data on the order of $10^6$ observations.","Validate the method on multiple real, publicly available benchmark datasets with clear preprocessing and report reproducible pipelines (including parameter tuning for $\Phi$ and convergence diagnostics for MCMC). Provide guidance and theory on selecting/learning the interaction function $\Phi$ (e.g., parametric families or nonparametric/Bayesian priors) and analyze identifiability and robustness under misspecification. Extend the approach to handle structured data issues common in practice (ordinal items, non-Gaussian measurement error, strong autocorrelation, and explicit missing-data mechanisms) and compare against a broader set of reliability estimators (e.g., $\omega$, GLB, SEM/G-theory).",2511.08952v1,https://arxiv.org/pdf/2511.08952v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:24:43Z
TRUE,System reliability|Failure mode analysis|Other,Stochastic process|ML-based|Hybrid/Ensemble|Simulation-based|Other,Simulated only,Not applicable,Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,Other,Not provided,https://doi.org/https://doi.org/10.1016/j.engstruct.2025.120018.,"The paper proposes PINN-RA, a Physics-Informed Neural Network surrogate coupled with Monte Carlo Simulation to efficiently estimate failure probability for buried steel pipelines subjected to permanent ground deformation (PGD) in geohazard regions. The pipeline is modeled as an Euler–Bernoulli beam with nonlinear pipe–soil springs and a biaxial steel material response that accounts for internal pressure and temperature effects; uncertain inputs include PGD magnitude and soil parameters. Reliability is formulated via a strain-based limit state from ALA guidelines, with failure defined as tensile or compressive longitudinal strain demand exceeding capacity; the trained PINN surrogate replaces repeated FEM/FDM evaluations inside MCS. In a case study with 10^5 samples, the method reports substantial computational savings (hours for PINN training plus near-instant evaluation) compared to FEM/FDM, and it quantifies PoF under two capacity cases with and without operational loads. Sensitivity analysis shows PGD magnitude dominates reliability, with friction angle and burial depth having secondary influence.","Reliability is defined by $P_f=P(g(\xi)\le 0)$ and estimated by Monte Carlo as $\hat P_f=\frac{1}{N}\sum_{i=1}^N I(g(\xi_i)\le 0)$. The limit state is strain-based: $g(\delta,c,\varphi,\gamma,H)=\varepsilon_c-\varepsilon_d(\delta,c,\varphi,\gamma,H)$, where demand $\varepsilon_d$ is predicted from the PINN surrogate solution of the governing beam–soil differential equations. Compressive strain capacity is taken from ALA-style formulation $\varepsilon^{SC}_c=0.50(t/D)-0.0025+3000\,(PD/(2tE))^2$, with tensile capacity set (e.g., 2%).","Using 10^5 MCS samples, for capacity case (I) the PoF for tensile failure is 86.12% (uniaxial/no pressure-temperature) versus 33.63% (biaxial/with pressure-temperature), while compressive failure is 90.32% (uniaxial) and “no failure observed” (biaxial). For capacity case (II), tensile PoF is 12.07% (uniaxial) versus 0.001% (biaxial), and compressive PoF is 0.024% (uniaxial) with “no failure observed” (biaxial). Reported tensile strain demand mean/SD are about 2.25%/0.22% (uniaxial) and 1.92%/0.17% (biaxial); compressive mean/SD about −0.50%/0.03% (uniaxial) and −0.88%/0.06% (biaxial). Runtime comparison for 10^5 samples: FEM 138.9 h, FDM 55.5 h, parallel FEM (6 procs) 23.2 h, optimized FDM 3.3 h, while PINN-RA training is 3.1 h with sample evaluation taking only a few seconds (effectively independent of sample size).",None stated.,"The reliability results depend on modeling assumptions (Euler–Bernoulli beam idealization, ALA spring laws, normal distributions for inputs, and fixed boundary conditions/geometry), which may limit generalizability to other pipe types, soils, and PGD patterns. The MCS evaluation appears to rely on the surrogate’s physics residual convergence rather than extensive validation against high-fidelity FEM across the full random-input space, so surrogate bias in tail regions (low/high PoF) is unclear. The method focuses on a strain-exceedance limit state and does not model other failure mechanisms (e.g., fracture, weld defects, corrosion) or system-level consequences beyond a single segment.",None stated.,"Extend PINN-RA to handle additional uncertainties and realism such as spatially varying soil properties, correlated random fields, and non-normal input distributions for PGD and geotechnical parameters. Develop and test variance-reduction or rare-event simulation (e.g., importance sampling/subset simulation) coupled with PINN surrogates to more efficiently estimate very small failure probabilities. Provide open, reusable software implementations and broader benchmarking/validation against FEM for multiple geohazard scenarios (fault crossing, liquefaction spreading, slope failure) and boundary conditions, including multiaxial loading and model-form uncertainty quantification.",2511.11613v1,https://arxiv.org/pdf/2511.11613v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:25:13Z
FALSE,Other,"ML-based|Parametric (Weibull, etc.)|Nonparametric/Semi-parametric|Other",Sensor/condition monitoring|Mixture of types|Other,Not applicable,Energy/utilities,Other,TRUE,R|Other,Not provided,https://doi.org/10.5281/zenodo.17311709,"This preprint develops a pipeline to produce daily probabilistic wind power forecasts over France at subseasonal-to-seasonal (S2S) lead times from 1 to 46 days using ECMWF S2S ensemble weather forecasts. Weather fields are interpolated and converted to power via a capacity-weighted, lead-time-agnostic convolutional neural network trained on ERA5 reanalysis inputs (2012–2022) with French national wind generation as the target. The resulting raw power ensembles are then statistically post-processed (online training with an initial 30-day window) to correct bias and under-dispersion; the best-performing methods highlighted are EMOS with a truncated normal distribution and linear quantile regression. Skill is evaluated against a climatological power baseline (built from a 30-year ERA5 weather climatology mapped to power) and a bootstrap climatology ensemble baseline using CRPS and ensemble-mean MSE skill scores. The authors report that S2S forecasts improve skill versus climatology by roughly 45–50% across lead times and that post-processing adds about 10% more improvement while yielding near-perfect calibration for lead times around 15–46 days.","Wind speed at hub height is derived from 10 m wind via a power law: $u_{100}=u_{10}(z_{100}/z_{10})^{\alpha}$ with $\alpha=1/7$. Weather-to-power inputs are capacity-weighted by grid cell and time using $\omega_{i,j}=P_{i,j}(t)/\sum_{i,j,t} P_{i,j}(t)$. EMOS assumes a (truncated) normal predictive distribution with mean $\mu=\sum_{i=1}^K a_i X_i + b$ (often $a_i=a$ for exchangeable members) and variance $\sigma^2=c+dS^2$, with parameters fit by minimizing the CRPS of a truncated normal; quantile regression fits each desired quantile using the pinball loss.","Across lead times up to 46 days, raw S2S-based wind power ensembles outperform a climatological baseline by at least ~43% (CRPSS) and ~44% (ensemble-mean MSE skill), with ~50% improvements at shorter lead times and a plateau near ~45% after ~15 days. Post-processing (EMOS or quantile regression) improves both probabilistic and deterministic skill by about an additional ~10%, reaching a plateau around ~52% CRPSS and ~55% MSE skill for lead times ~15–46 days. Reliability plots show raw forecasts are biased and under-dispersive (quantiles underpredicted above ~0.1), while EMOS and quantile regression produce close-to-diagonal reliability across lead times (minor overestimation below ~0.4 and underestimation above ~0.7). More complex methods (e.g., quantile regression forests / networks) do not outperform simpler methods on the available dataset size (~602 samples per lead time).","The authors note that more complex post-processing models (e.g., quantile regression forest and distributional regression networks) require more data and are not competitive here because the dataset provides only about 602 samples per lead time. They also point out that their mixture-of-experts setup can fail to correct bias when the observation lies outside the convex hull of the raw ensemble members. They further remark that lead times of 1–7 days would likely be better served by dedicated short-term, higher-resolution weather forecasts rather than S2S products.","The work is not a reliability engineering study and does not connect forecast reliability/calibration to asset reliability, failure rates, or maintenance decision optimization, despite mentioning potential O&M benefits. The post-processing is trained per lead time (for key methods) and relies heavily on exchangeability/indistinguishability assumptions; operational robustness to model changes, nonstationarity, and evolving installed capacity may be more challenging than shown. Evaluation appears focused on 2023–2024 initializations over France only; generalization to other years/regions and sensitivity to the wind-shear extrapolation ($\alpha=1/7$) and interpolation choices may materially affect results. Code availability is unclear; while data products are shared, reproducibility may be limited without released training/inference code and exact preprocessing details.",The authors suggest exploring richer predictor sets for post-processing (beyond raw ensemble members) which could differentiate quantile regression and EMOS performance. They also discuss improving the climatological bootstrap baseline using more sophisticated analogue methods (multivariate atmospheric analogues or analogues selected using observed wind power) to build cheaper ensemble proxies. They note that continued improvements in S2S ensemble prediction systems should translate into higher-skill S2S power forecasts and broader adoption.,"A natural extension is to quantify the value of forecast calibration/skill in downstream grid and maintenance decisions (e.g., cost-loss analysis, maintenance scheduling optimization) rather than only CRPS/MSE. Robustness studies could test alternative hub-height extrapolation schemes, bias-correction at the weather-field level, and sensitivity to changing turbine distributions/capacity weighting over time. Broader validation across multiple years, regimes, and other countries (and for other renewables like solar/hydro) would clarify generalizability. Packaging the full pipeline (preprocessing, CNN model, post-processing) with versioned dependencies would materially improve reproducibility and operational transfer.",2511.16164v1,https://arxiv.org/pdf/2511.16164v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:25:49Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://mathworld.wolfram.com/,"This paper proposes a ground-truth-free procedure to select the best heterogeneous treatment effect (HTE/CATE) estimator from a set of candidates when true individual treatment effects are unobserved. The authors cast estimator selection as a multiple-testing/argmin-inference problem and build a cross-fitted, exponentially weighted test statistic that aggregates pairwise “relative error” comparisons between candidate estimators. A two-layer sample-splitting (two-way cross-fitting) design is used to decouple nuisance estimation, weight learning, and evaluation, enabling valid asymptotic inference. Under boundedness/overlap, nuisance-consistency, and a stability condition, they prove asymptotic familywise error rate (FWER) control via a stability-based central limit theorem for globally dependent statistics. Experiments on ACIC 2016, IHDP, and Twins benchmarks show similar or better FWER control with substantially fewer wrong selections than naive max-statistic testing, Bonferroni correction, and a ranking-inference baseline.","The paper’s key metric is the pairwise relative error: $\delta(\hat\tau_1,\hat\tau_2)=\mathbb{E}[(\hat\tau_1(X)-\tau(X))^2]-\mathbb{E}[(\hat\tau_2(X)-\tau(X))^2]$, estimated by a cross-fitted one-step correction using outcome-regression and propensity-score nuisances. The proposed test statistic for candidate $r$ forms per-sample weighted competitors $Q_{i,r}=\sum_{j\neq r}\hat\omega^{(-i)}_{r,j}\,\hat t(Z_i;\hat\tau_r,\hat\tau_j)$ with exponential weights $\hat\omega\propto\exp(\lambda\hat\delta)$ learned on inner folds, and aggregates $S_r=\sum_{i=1}^n Q_{i,r}$. The standardized statistic $(1/(\sqrt{n}\hat\sigma_r))S_r$ is compared to the Gaussian critical value $z_{1-\alpha}$ to form a confidence set of “winners,” yielding asymptotic FWER control under stability assumptions.","On three causal-inference benchmarks (100 repetitions, significance level $\alpha=0.10$), the proposed method (“Ours”) achieves low FWER while reducing the average number of wrong selections (ANWS) versus baselines. Reported results: ACIC—FWER 0.02, ANWS $0.83\pm0.06$ (vs Naive ANWS $1.10\pm0.04$, Bonferroni $1.45\pm0.05$, Ranking Inference $1.63\pm0.06$); IHDP—FWER 0.04, ANWS $0.80\pm0.06$ (vs Naive $1.01\pm0.03$); Twins—FWER 0.02, ANWS $0.50\pm0.05$ (vs Naive $0.88\pm0.04$). Additional experiments indicate the method’s ANWS is comparatively insensitive as the number of candidate estimators $K$ increases, and it benefits strongly from larger test sample sizes. Diagnostics (bootstrap/KS tests) suggest the standardized statistics are approximately Gaussian in their experimental setup.",None stated.,"The work is not a reliability-engineering study; it targets causal/ML model selection, so reliability concepts like lifetime/degradation data and maintenance policies are out of scope. The theoretical guarantees rely on overlap, bounded outcomes, nuisance consistency, and a stability condition for nuisance learners; in experiments the nuisances are fit with a black-box neural network (DragonNet) without formal stability guarantees, so finite-sample validity may be sensitive to training choices. The empirical evaluation uses established benchmarks and semi-synthetic constructions, which may not reflect all real observational-study pathologies (e.g., severe unmeasured confounding, weak overlap, distribution shift). Code is not shared in the provided text, which limits reproducibility of the exact splitting/weighting implementation and hyperparameter choices (e.g., $\lambda$).",None stated.,"Investigate robustness and finite-sample behavior under weaker overlap, heavy-tailed outcomes, and distribution shift between training and test populations. Develop self-starting or adaptive choices of the exponential-weighting parameter $\lambda$ with theoretical guidance, and extend the procedure to settings with dependent/clustered data. Provide open-source implementations and reproducible pipelines for the two-layer cross-fitting and weight learning to facilitate practitioner adoption and benchmarking. Explore extensions to other causal estimands (e.g., continuous treatments, time-varying treatments) and to high-dimensional/multivariate selection settings.",2511.18464v1,https://arxiv.org/pdf/2511.18464v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:26:17Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,Stochastic process|Bayesian|Other,Degradation measurements|Sensor/condition monitoring|Other,Predictive|Condition-based|Other,Transportation/logistics|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://github.com/theOehrly/FastF1|https://www.pirelli.com/tires/en-us/motorsport/f1/tires,"The paper proposes a Bayesian state-space modeling framework to infer latent Formula 1 tire degradation dynamics from publicly available lap-timing data (FastF1), treating tire “pace” as an unobserved state and lap time as a noisy observation with fuel mass as a covariate. Pit stops are modeled as state resets, enabling degradation to restart when new tires are fitted; extensions include compound-specific degradation parameters, a time-varying degradation-rate state (capturing warm-up/acceleration of wear), and a skewed-t observation model for asymmetric/outlier lap-time errors. Models are fit in Stan using MCMC, and predictive performance is assessed via rolling-origin-recalibration cross-validation against an ARIMA(2,1,2) baseline. In a case study of Lewis Hamilton’s 2025 Austrian Grand Prix, the skewed-t state-space model provides the best out-of-sample accuracy (lowest RMSPE and CRPS) and is more robust to positive outliers than Gaussian-error variants. The approach yields interpretable probabilistic estimates of degradation and is positioned as a foundation for real-time strategy decisions (e.g., pit timing) and broader multi-race/multi-driver analyses.","Observation model: $y_t = \alpha_t + \gamma\,\text{fuel}_t + \epsilon_t$, with either $\epsilon_t \sim \mathcal N(0,\sigma_\epsilon^2)$ or a skewed-$t$ error $\epsilon_t \sim \text{Skew-}t(0,\sigma_\epsilon^2,\lambda,2)$. Base state evolution with pit-stop reset: $\alpha_{t+1}=(1-I^{\text{pit}}_t)(\alpha_t+\nu)+I^{\text{pit}}_t\alpha_{\text{reset}}+\eta_t$, $\eta_t\sim\mathcal N(0,\sigma_\eta^2)$; compound-specific version replaces $\nu$ and $\alpha_{\text{reset}}$ with compound-indexed parameters. Time-varying degradation introduces a degradation-rate state: $\alpha_{t+1}=(1-I^{\text{pit}}_t)(\alpha_t+\nu_t)+I^{\text{pit}}_t\alpha_{\text{reset}[c_t]}+\eta_t$ and $\nu_{t+1}=(1-I^{\text{pit}}_t)(\nu_t+\beta_{[c_t]})+I^{\text{pit}}_t\nu_{\text{reset}}$.","Cross-validation RMSPE totals (lower is better): ARIMA(2,1,2)=1.520, base SSM=1.169, compound-specific=1.187, time-varying=1.218, skewed-$t$ SSM=1.082; the skewed-$t$ model is best overall and especially improves stint 2 (0.601 vs 0.673 base and 0.727 ARIMA). Probabilistic forecast accuracy via CRPS: overall $\overline{\text{CRPS}}$ is 0.324 (ARIMA), 0.230 (base), 0.236 (compound-specific), 0.245 (time-varying), and 0.202 (skewed-$t$), again favoring the skewed-$t$ model. Estimated compound-specific linear degradation rates (Extension 1) are similar with overlapping 95% credible intervals: hard $\nu\approx 0.054$ (2.5% 0.004, 97.5% 0.133) sec/lap, medium $\nu\approx 0.060$ (0.009, 0.120) sec/lap. Time-varying degradation-rate increments (Extension 2) also overlap: hard $\beta\approx 0.011$ (0.003, 0.020), medium $\beta\approx 0.010$ (0.002, 0.018), while the inferred $\nu_t$ can start negative and rise during a stint, consistent with tire warm-up effects.","The case study race had no safety car; safety-car laps substantially reduce degradation and are not explicitly modeled, though the authors note the model could be extended to suspend degradation during such periods. The analyzed drive was relatively uneventful and minimally affected by traffic; being stuck behind slower cars can inflate lap times for reasons unrelated to degradation. The authors suggest adding a covariate (e.g., distance to the car ahead) or using more sophisticated multivariate/dependent-error models to address traffic effects.","Fuel mass is not observed and is assumed to start at 110 kg and decrease linearly to zero, which can confound inferred degradation and bias parameter estimates if actual fuel load/burn differs. The analysis is based on a single driver in a single race with short stints (~20 laps), limiting identifiability of compound effects and generalizability across tracks, temperatures, and driving styles. The model largely assumes conditional independence of lap-time errors given the latent state; real lap times often exhibit residual autocorrelation and regime changes (traffic, yellow flags, tire temperature management) that may not be fully captured by the simple error structure. No explicit decision/optimization layer (e.g., pit-stop policy optimization under uncertainty) is implemented, so maintenance/strategy claims are demonstrative rather than validated end-to-end.","Extend the framework to multiple races and drivers to better quantify compound-specific degradation patterns. Refine priors using richer telemetry or surface-temperature data to improve inference. Explore hierarchical structures to capture team-level or track-level effects and improve pooling/generalization; additionally, incorporate traffic effects more formally (e.g., multivariate models with dependence based on distance to the car ahead).","Develop an explicit Bayesian decision-theoretic pit-stop optimization module that uses posterior predictive distributions of lap times to choose pit timing under uncertainty and compare strategies. Add robust handling of regime changes and exogenous events (virtual safety car/safety car, yellow flags, weather) via switching/state-dependent dynamics. Validate across larger benchmarks and report sensitivity to the assumed fuel-burn model, including joint estimation of fuel effect or use of partially observed fuel/telemetry covariates. Provide an open, reproducible implementation (Stan + Python pipeline) and standardized evaluation metrics to facilitate adoption by practitioners.",2512.00640v1,https://arxiv.org/pdf/2512.00640v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:27:01Z
FALSE,Other,"Nonparametric/Semi-parametric|Parametric (Weibull, etc.)|Other",Mixture of types|Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI),https://github.com/shuvom-s/e-valuator|https://pypi.org/project/e-valuator/,"The paper proposes e-valuator, a statistical wrapper that converts any black-box agent verifier score sequence into an anytime-valid sequential decision rule for flagging unsuccessful agent trajectories with controlled false alarm rate (type I error). It frames success vs. failure of an agent trajectory as a sequential hypothesis test between the score-sequence distributions for successful trajectories (null) and unsuccessful trajectories (alternative), and uses e-process/test-martingale theory to obtain time-uniform guarantees via Ville’s inequality. The core test statistic is the sequential density ratio $M_t=p_0(S_{1:t})/p_1(S_{1:t})$, which is log-optimal among e-processes under the alternative; in practice, the density ratio is estimated stepwise via classifier-based density ratio estimation (logistic regression). Two thresholding strategies are studied: an anytime-valid $1/\alpha$ threshold and a less-conservative PAC quantile-based threshold calibrated on held-out successful trajectories. Experiments across six datasets and multiple agents/verifiers show improved false alarm control and power versus raw-thresholding, isotonic calibration, and Bonferroni-style baselines, and demonstrate token savings via early termination (e.g., recovering up to ~86% of original accuracy with ~80% tokens on MATH).","The sequential test statistic is the density-ratio (likelihood-ratio) process $M_t=\frac{p_0(S_{1:t})}{p_1(S_{1:t})}$ with $M_0=1$, and the anytime-valid rejection rule is to reject the null when $M_t\ge c_\alpha$ (often $c_\alpha=1/\alpha$ by Ville’s inequality). In practice, the paper estimates $M_t$ using classifier-based density ratio estimation: $M_t=\frac{p(Y=0\mid S_{1:t})}{p(Y=1\mid S_{1:t})}\cdot\frac{p(Y=1)}{p(Y=0)}$, yielding $\hat M_t=\frac{1-\hat f_t(S_{1:t})}{\hat f_t(S_{1:t})}\cdot\frac{\hat\pi_1}{1-\hat\pi_1}$ where $\hat f_t\approx p(Y=1\mid S_{1:t})$. A second threshold sets $c_\alpha$ to an upper confidence bound on the $(1-\alpha)$-quantile of $\max_t \hat M_t$ over null (successful) calibration trajectories (PAC threshold).","Across HotpotQA, MedQA, MMLU-Pro, MATH, GSM8k, and chess, both e-valuator variants empirically control the false alarm rate at or below nominal $\alpha$ across tested $\alpha\in[0,0.5]$, while raw and isotonic-calibrated verifiers often violate control (e.g., HotpotQA at $\alpha=0.5$: calibrated verifier FAR \~0.61). The PAC-threshold version typically achieves the highest power among methods that still meet false-alarm control, and is less conservative than the $1/\alpha$ threshold; Bonferroni controls FAR but is more conservative with lower power. In a token-saving early-termination study, on MATH e-valuator achieved 50% total accuracy (stated as 86% of the original 58% accuracy) using 269,755 tokens vs. 333,283 baseline tokens (~81% of tokens), whereas raw/calibrated verifiers needed >95% of tokens to reach the same fraction. On chess, e-valuator improves control relative to raw/calibrated baselines; the $1/\alpha$ threshold can degrade at low $\alpha$ for long games due to density-ratio estimation error, but still shows smaller violations than verifier baselines.","The authors note that the $1/\alpha$ anytime-valid threshold can be overly conservative because it is valid even if an agent runs indefinitely, whereas practical trajectories are finite. They also acknowledge that density ratio estimation error can harm control/power in long sequences (noted in chess at low $\alpha$), and that some test-time interventions (e.g., resampling/restarting trajectories) may violate e-valuator’s assumptions and would require methodological adjustments.","The method relies on a labeled calibration set drawn i.i.d. from the deployment setting, which may be difficult to obtain and may drift over time; performance/validity under distribution shift is not established. The stepwise classifier approach (separate $\hat f_t$ per time step) can be data-inefficient and may not scale well to very long horizons without stronger temporal modeling or regularization, and logistic regression may underfit complex score dynamics. The paper’s guarantees are exact only for the true density ratio; with estimated ratios, formal anytime-valid guarantees are replaced by a PAC-style statement that depends on calibration splitting and may be sensitive to small numbers of successful trajectories (especially for tail quantiles).","They propose relaxing assumptions to avoid estimating the full joint density at each time (e.g., assume i.i.d. scores or k-step dependence), and under i.i.d. use universal inference to regain exact guarantees with noisy density-ratio estimates. They suggest more nuanced test-time scaling strategies beyond early termination (e.g., resampling or restarting trajectories), noting these may break assumptions and would require modifications. They also suggest extending e-valuator to more complex agentic systems such as multi-agent settings.","Develop a self-starting/online-updating variant that continually refreshes the density-ratio model and threshold under controlled error when the agent/verifier or task distribution drifts. Extend the framework to handle partially labeled or delayed labels (common in deployments) and to incorporate covariates (prompt/task features) so thresholds adapt to difficulty while maintaining guarantees. Provide more robust density-ratio estimation (e.g., time-series models, hierarchical sharing across t, calibrated probabilistic classifiers, or conformalized density-ratio bounds) and benchmark sensitivity to miscalibration/overfitting. Add practitioner tooling for diagnosing why trajectories are rejected (attribution over steps/scores) and standardized evaluation protocols for long-horizon agents.",2512.03109v1,https://arxiv.org/pdf/2512.03109v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:27:45Z
FALSE,Other,ML-based|Nonparametric/Semi-parametric|Other,Event/count data|Sensor/condition monitoring|Mixture of types|Other,Not applicable,Network/cybersecurity|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"This paper addresses the “zero propensity” problem that makes standard off-policy evaluation (OPE) estimators unusable in deterministic, winner-takes-all ad auctions. The authors propose DPM-OPE, which repurposes bid-landscape forecasting—specifically a Discrete Price Model (DPM) of the market-price (second-highest competitor score) distribution—to derive non-zero approximate propensity scores (APS) based on winning probabilities. These APS values enable stable importance-sampling style OPE, and the paper uses a self-normalized IPS estimator (SNIPS) with additional weight capping for variance control. The approach is validated on the AuctionNet simulation benchmark and on a 2-week industrial online A/B test log from a large advertising platform, showing close alignment with online results (reported 92.9% mean directional accuracy on real-world CTR lift prediction, outperforming a parametric market-price baseline). Overall, it contributes a practical framework for reliable offline evaluation in deterministic auction environments by shifting from policy stochasticity to environment stochasticity modeling.","The method models the market price (competitor threshold) as $z=\max_{j\neq i}\{\text{score}(a_j,x)\}$ and discretizes its distribution into bins $V_l=(b_{l-1},b_l]$, defining winning and survival functions over bins. The bin probability is $p_l=\Pr(z\in V_l)=S(b_{l-1})-S(b_l)$ and the Approximate Propensity Score is the conditional win probability $h_l=\Pr(z\in V_l\mid z\ge b_{l-1})=p_l/S(b_{l-1})$, with $\pi_{APS}(a\mid x)=h_l$ for $\text{score}(a,x)\in V_l$. Policy value is then estimated using an APS-based SNIPS estimator (with capping in experiments): $\hat V(\pi)=\frac{\sum_i r_i\, \frac{\pi_{APS}(a_i\mid x_i)}{\pi^{0}_{APS}(a_i\mid x_i)}}{\sum_i \frac{\pi_{APS}(a_i\mid x_i)}{\pi^{0}_{APS}(a_i\mid x_i)}}$.","On AuctionNet, DPM-OPE achieved much lower RMSE than the parametric baseline (4.870 vs 25.585) while both reached 100% MDA; Pearson correlation was 0.520 (parametric) vs -0.112 (DPM-OPE). On the 2-week real-world dataset, DPM-OPE improved MDA from 78.6% to 92.9% and improved Pearson correlation from 0.575 to 0.653; RMSE was similar (parametric 1.187 vs DPM-OPE 1.197) and the RMSE difference was not significant (paired t-test p=0.9751 at 95% confidence). Ablations on AuctionNet showed adaptive binning outperformed fixed-bin alternatives (RMSE 4.870 vs 5.509/6.461/18.504 for 100/1,000/10,000 bins), and capped SNIPS dramatically reduced error compared with IPS/SNIPS (RMSE 4.870 vs 355.532 and 20.448).","The authors note that the current DPM models market price distribution primarily as a function of aggregated scores and ignores richer contextual features (e.g., ad type, user attributes), which may limit accuracy. They also state that the framework’s accuracy depends on how well the market price distribution $P(z)$ is estimated, and misestimation can introduce bias into OPE; they call for theoretical study of error propagation. Finally, they caution that applying the approach outside ad auctions may require domain-specific calibration to define an analogue of “market price,” and they note the method focuses on propensity correction rather than reward modeling (suggesting combination with model-based/DR estimators).","The APS construction discretizes score space and uses within-bin conditional probabilities; results may be sensitive to segmentation choices and to nonstationarity in auction dynamics (e.g., seasonality, competitor behavior shifts), which can degrade calibration over time. The approach implicitly assumes the market-price model estimated from logged outcomes generalizes to counterfactual scoring changes under the evaluated policy; large policy shifts could invalidate this approximation and bias weights. Also, no implementation details are provided for scalability/latency in production (training DPM per segment, updating bins, handling extremely heavy-tailed importance weights), which affects reproducibility and practical deployment costs.","They propose using more feature-aware bid landscape models (e.g., Deep Landscape Forecasting (DLF) or Arbitrary Distribution Modeling (ADM)) to incorporate contextual features and improve personalization/accuracy. They suggest further theoretical work to quantify and control how market-price distribution estimation error propagates into final OPE bias/variance. They also propose extending beyond propensity correction by combining APS estimation with model-based or doubly robust estimators to further reduce variance.","Developing self-starting/online recalibration schemes for APS (e.g., drift detection and periodic refitting) would improve robustness under nonstationary auction markets. Extending the framework to handle additional complexities such as delayed rewards (conversions), multi-slot/slate auctions, and budget constraints would broaden applicability in real ad systems. Providing open-source reference implementations and standardized benchmarking protocols (including sensitivity analyses for segmentation/binning and policy shift magnitude) would materially improve reproducibility and practitioner adoption.",2512.03354v1,https://arxiv.org/pdf/2512.03354v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:28:21Z
FALSE,NA,ML-based|Other,Simulated only|Other,Not applicable,Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/Miguel-San/Reliable-Conformal-Prediction,"The paper proposes a new statistical guarantee for conformal prediction when calibration datasets are small, addressing the issue that standard conformal prediction guarantees are expressed only in terms of marginal (expected) coverage and can be uninformative for a single fitted conformal predictor under small Ncal. The authors characterize the distribution of achieved coverage induced by finite calibration sampling (showing it follows a Beta distribution when order-statistic quantiles are used), and use this to define a new guarantee of the form P(C \ge C_min) \ge 1-\alpha for a single conformal predictor. They derive how to choose an adjusted quantile level (via solving for an order statistic index m) so that the new guarantee holds, and show that it converges to classic conformal correction for large calibration sizes. The method is illustrated with pedagogical/synthetic examples and Monte Carlo experiments comparing classic vs new guarantees, highlighting the trade-off that the new guarantee can be more conservative (wider intervals). An open-access Python implementation is provided to integrate with common conformal prediction tooling (e.g., MAPIE).","The paper uses order-statistic quantiles \(\hat Q_q = s_{m:N}\) with \(m=\lceil N_{cal} q\rceil\). Under standard conformal correction it sets \(q^* = \lceil (N_{cal}+1)C_{nom}\rceil/N_{cal}\) to obtain the classic marginal-coverage guarantee \(0 \le \mathbb{E}(C)-C_{nom} \le 1/(N_{cal}+1)\). The new proposed guarantee targets single-model coverage by requiring \(\mathbb{P}(C\ge C_{min}) \ge 1-\alpha\) and chooses the order statistic index m (equivalently an adjusted quantile level \(\tilde q\)) by solving \(F_C(C_{min};m) - (1-\alpha)=0\), where \(C\sim \mathrm{Beta}(m, N_{cal}-m+1)\).","Through Monte Carlo experiments (e.g., N_MC=10000) the empirically observed coverage distribution matches the derived Beta distribution for both large (e.g., Ncal=1000) and small (e.g., Ncal=100) calibration sizes. For small Ncal, the classic conformal guarantee can be misleading: for an example with C_nom=0.9 and Ncal=100, the paper reports that about 46% of fitted conformal predictors can achieve coverage below 0.9, and about 10% can fall below 0.86, despite the marginal-coverage bound. The proposed method shifts the coverage distribution upward to ensure the specified probabilistic lower bound on coverage (C_min at confidence 1-\alpha), at the cost of more conservative (wider) prediction intervals. The adjusted quantile level \(\tilde q\) approaches the classic correction \(q^*\) as Ncal grows, and the paper visualizes this convergence for several confidence levels.","The authors note that enforcing the new guarantee can make conformal predictors more conservative (prediction intervals larger), which may reduce informativeness/efficiency. They also emphasize that usefulness must be assessed a posteriori for a given application by examining interval widths, and that reducing C_min and/or the confidence (1-\alpha) can mitigate over-conservativeness.","The work is primarily methodological and validated on pedagogical/synthetic examples; it does not provide extensive empirical validation on real engineering datasets or safety-critical certification case studies where covariate shift, non-exchangeability, or distribution drift may violate conformal assumptions. The approach relies on the Beta coverage-distribution result, which hinges on using order-statistic quantiles and continuity/exchangeability assumptions; robustness to alternative quantile estimators, ties/discrete scores, or dependent data (autocorrelation) is not thoroughly explored. Practical guidance for selecting (C_min, \alpha) balancing interval width vs risk is limited to qualitative discussion rather than decision-analytic or cost-based design.","The paper discusses that the method is particularly relevant for group-balanced conformal prediction (region-wise calibration) and suggests applicability to safety-critical settings and surrogate modeling of physical processes where data are scarce. It also notes the availability of a software implementation intended to be used alongside existing conformal prediction libraries (e.g., MAPIE) to obtain uncertainty models satisfying the new guarantee.","Validate the method on real-world reliability/safety datasets (e.g., fatigue life, failure-time prediction, condition monitoring) and under realistic data issues such as covariate shift, censoring, and temporal dependence to test robustness of the new guarantee. Extend the framework to handle discrete/non-continuous score distributions and alternative conformal variants (e.g., adaptive/online conformal, Mondrian/group-conditional conformal) with formal finite-sample guarantees under dependence. Provide practical design tools for choosing (C_min, \alpha) based on risk/cost (e.g., expected loss, certification margins) and implement reference benchmarks and reproducible examples as a package or integration into established CP libraries.",2512.04566v2,https://arxiv.org/pdf/2512.04566v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:28:52Z
FALSE,NA,ML-based|Stochastic process|Hybrid/Ensemble|Simulation-based,Simulated only,Not applicable,Healthcare/medical|Environmental monitoring|Food/agriculture|Other,Simulation study|Other,TRUE,Other,Public repository (GitHub/GitLab),https://github.com/arthur-theuer/deep-ska,"The paper proposes DeepSKA, a framework for computing expected outputs of stochastic reaction networks (SRNs) modeled as continuous-time Markov chains, aiming to provide interpretability and reliability guarantees. It introduces an interpretable Spectral Decomposition-based neural network (SDnet) that approximates expectations via a truncated spectral expansion with learned decay modes, eigenfunctions, and function coordinates. To obtain guaranteed reliability (unbiasedness and provable convergence), it combines SDnet with Monte Carlo simulation to form Deep Learning/Monte Carlo estimators: SSA with Deep Importance Sampling (DeepIS) and SSA with Deep Control Variates (DeepCV), which reduce variance while retaining Monte Carlo guarantees. The method is evaluated on nine SRNs (including nonlinear and non-mass-action kinetics up to 10 species) and demonstrates orders-of-magnitude variance reduction and efficiency improvements over standard stochastic simulation. Extensions are described for steady-state means (EM with DeepCV using a Poisson-equation network P-SDnet) and for variance estimation (DeepIPA, generally biased).","The target quantity is the expected output $U(x,f,t)=\mathbb{E}[f(X(t))\mid X(0)=x]$. SDnet represents $U$ via a truncated spectral form $\hat U_\eta(x,f,t)=\hat U(f)+\sum_{\ell=1}^r e^{-\hat\sigma_\ell t}\,\hat\gamma_\ell(f)\,\hat\phi_\ell(x)$ (with learned decay modes $\hat\sigma_\ell$, coordinates $\hat\gamma_\ell(f)$, and eigenfunctions $\hat\phi_\ell$). DeepIS forms an unbiased estimator $E^{IS}_\eta=f(X^{IS}_\eta(T))Z^{IS}_\eta(T)$ under a change of measure, while DeepCV uses the control-variate estimator $E^{CV}_\eta=f(X(T)) - Z^{CV}_\eta(T)$ with $Z^{CV}_\eta(t)=\sum_k\int_0^t \Delta_{\zeta_k}\hat U_\eta(X(s),f,T-s)\,d\tilde R_k(s)$.","Across nine benchmark SRNs (including nonlinear and non-mass-action models up to 10 species), the DLMC estimators (SSA with DeepCV/DeepIS) produce markedly tighter confidence intervals than standard SSA for the same sample size and exhibit several orders-of-magnitude lower estimator variance. Estimation error decreases with sample size according to the Monte Carlo inverse-square-root law while converging to the exact expectation due to unbiasedness/convergence guarantees. The paper reports error reductions “up to 10,000-fold” relative to SSA for fixed sample sizes, with SSA+DeepCV typically slightly more efficient than SSA+DeepIS in the presented examples. SDnet also shows strong temporal generalization beyond the training horizon in the mean-dynamics experiments.","DeepIS is restricted to positive output functions and can be numerically unstable when the learned $\hat U_\eta$ takes very small values due to ratios/logarithms in the change-of-measure construction. DeepCV requires numerical time integration of stochastic integrals; using a deterministic grid can introduce numerical integration error (potential bias) unless the grid is sufficiently refined. For variance estimation, the DeepIPA estimator is generally biased because SDnet approximation error propagates into the variance integral.","The “guaranteed reliability” is essentially Monte Carlo reliability (unbiasedness and convergence) for the hybrid estimators; it does not guarantee small-sample accuracy unless SDnet is sufficiently accurate, so performance may degrade if SDnet is misspecified or poorly trained. The approach depends on generating and storing many SSA trajectories for training (e.g., 20,000 trajectories), which can be expensive for very stiff or high-dimensional SRNs; scalability beyond the demonstrated 10-species cases is not established. The framework assumes Markovian CTMC dynamics and does not address common experimental issues such as partial observability, measurement noise, or parameter uncertainty in a fully developed way.","The authors propose extending the framework to learn dependence on parameter variability to study robustness to parametric perturbations. They also note an extension to stochastic filtering (conditional means of unobserved species given partial observations) but defer its development. Finally, they suggest adapting SDnet and the DLMC estimators to other Markovian models such as stochastic differential equations driven by Brownian motion.","A natural extension is to develop self-starting/online variants that update SDnet and the variance-reduction components as new simulation or experimental data arrive, reducing the need for large fixed training sets. Another direction is robustifying the estimators and training against stiffness and rare-event regimes (e.g., adaptive time discretization for DeepIS/DeepCV and stability constraints on learned propensities). Packaging the method as a documented library with reproducible benchmarks (including explicit reporting of hardware/runtime tradeoffs and hyperparameter sensitivity) would improve practical uptake and enable more rigorous comparative studies against multilevel Monte Carlo and other advanced variance-reduction methods.",2512.06294v1,https://arxiv.org/pdf/2512.06294v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:29:41Z
TRUE,Degradation modeling|RUL prediction|Maintenance optimization|Other,Stochastic process|ML-based|Hybrid/Ensemble|Other,Degradation measurements|Sensor/condition monitoring|Right-censored|Mixture of types,Condition-based|Predictive|Not applicable,Manufacturing (general)|Transportation/logistics|Energy/utilities|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Not provided,NA,"The paper proposes an integrated reliability/operations decision framework that jointly optimizes operating condition (capacity) selection and preventive maintenance using real-time multi-sensor degradation signals under partial observability. A partially observable Markov decision process (POMDP) is used for sequential decision-making, while the hidden degradation state is inferred via a constrained Input–Output Hidden Markov Model (IOHMM) that enforces a physically meaningful left-to-right (monotone) degradation progression and can share emission parameters across operating conditions. Continuous sensor features are discretized for POMDP compatibility using a Gaussian Mixture Model (GMM), and the POMDP is solved with point-based value iteration (PBVI) to obtain a policy over beliefs. The method is demonstrated on the XJTU-SY bearing degradation dataset (multiple operating conditions) and the NASA C-MAPSS FD001 turbofan dataset (multi-sensor run-to-failure), showing interpretable state estimation, RUL prediction with uncertainty bands, and improved long-run reward compared with fixed-condition baselines in simulation. The work advances condition-based/predictive maintenance by explicitly coupling state estimation and optimal control under noisy, multi-sensor observations and controllable operating regimes.","The POMDP is defined with hidden health states $S$, actions $A$ (operating condition choice plus maintenance), transition model $X(S,a,S')$ (input-dependent matrices $A(a)$ learned by IOHMM), observation model $Z(S',a,o)$, and belief update $b' (S') \propto Z(S',a,o)\sum_{S\in S}X(S,a,S')b(S)$ (Eq. 6/10). The immediate reward is state/action based (Eq. 1), and the optimal value satisfies the Bellman optimality equation over beliefs $V^*(b)=\max_{a\in A}[R(b,a)+\gamma\sum_{o\in\Omega}Pr(o\mid a,b)V^*(b')]$, solved approximately by PBVI. The IOHMM uses Gaussian emissions (shared: $O\mid S=k \sim \mathcal N(\mu_k,\Sigma_k)$ or action-dependent: $O\mid S=k,A=a \sim \mathcal N(\mu_k^a,\Sigma_k^a)$) with a left-to-right constraint implemented by zeroing backward transitions and sorting emission means to preserve monotone state ordering.","For the bearing case study, the learned action-dependent transition matrices $A_1,A_2,A_3$ (Eqs. 13–15) show slower degradation under lower capacity (e.g., state-1 self-transition 0.9330 in $A_1$ vs 0.5606 in $A_3$). In simulation (500 runs, 50,000 cycles each), the proposed POMDP policy achieved mean return 14617.09 (Std 630.71) versus fixed conditions C1: 11127.31, C2: 5846.04, and C3: −9213.23 (Table 5). For model selection on bearings, $K=6$ hidden states minimized AIC/BIC (AIC 1920.04, BIC 2368.05; Table 8). For C-MAPSS FD001, the learned 3-state left-to-right transition matrix had strong self-transition (e.g., 0.9887, 0.9722) with an absorbing failure state (Eq. shown in Section 4.2.2), and the optimal policy switches from operate to PM as belief mass on the degraded state increases (Table 7).",None stated.,"Although the framework is motivated by imperfect/noisy sensing, it relies on discretizing continuous observations (GMM clustering) and then solving a discrete-observation POMDP; policy quality may depend on discretization granularity and could degrade for high-dimensional sensor streams. The approach assumes Markovian, epoch-based dynamics with transitions only at cycle boundaries, and the left-to-right constraint forbids any recovery/temporary improvements, which may be violated in some real assets (e.g., load reductions causing partial symptom reversal). The paper does not clearly document released implementation/code or computational details (solver settings, runtimes, hyperparameter tuning), which limits reproducibility and deployment readiness.",The authors suggest extending the framework to more complex dynamics via an Input–Output Hidden Semi-Markov Model (IOHSMM) and exploring continuous-state stochastic-process degradation models instead of discrete latent states.,"A natural extension is a fully continuous-observation POMDP (or belief-MDP) treatment that avoids GMM discretization and can incorporate modern solvers (e.g., particle filters with online planning). Robust/self-starting variants that handle unknown initial parameters, concept drift, and nonstationary operating regimes would improve practical applicability. Additional work could incorporate imperfect maintenance (partial restoration) and explicit maintenance duration/downtime, and broaden evaluation to more real industrial datasets with autocorrelated sensor noise and missing-data patterns.",2512.06682v1,https://arxiv.org/pdf/2512.06682v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:30:14Z
FALSE,NA,ML-based,Other,Not applicable,Healthcare/medical|Finance/economics|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/Kumarjit-Pathak/Double-weighted-KNN,"The paper proposes DW-KNN (Double Weighted KNN), a transparent k-nearest neighbors classifier that multiplies a class-wise exponential distance weight with a class-wise neighbor-validity (reliability) weight to reduce the impact of noisy/mislabeled neighbors and improve stability. Validity for each training instance is computed as the fraction of its own neighbors that share its label (from Guo et al., 2008), then averaged within each candidate class among the query’s k nearest neighbors; distances are pooled by class (mean by default) and mapped through an exponential kernel. The method is evaluated via stratified 5-fold cross-validation on nine datasets (UCI, OpenML, and synthetic), reporting average accuracy 0.8988 and the lowest CV standard deviation 0.0156, ranking second among six KNN variants and within 0.2% of an ensemble KNN. Paired t-tests/Wilcoxon tests show highly significant gains over compactness-weighted KNN (+4.09%, p<0.001) and kernel-weighted KNN (+1.13%, p<0.001), with parity versus standard KNN and distance-weighted KNN. The paper also reports hyperparameter sensitivity, distance-metric generalization, decision-boundary visualizations, and a precision–recall trade-off on a severely imbalanced synthetic dataset.","Neighbor validity for training point $x_i$ is $v_i = \frac{1}{K_v}\sum_{j\in N_{K_v}(x_i)}\mathbf{1}(y_j=y_i)$. For a query $x_q$, distances to its $k$ nearest neighbors are pooled by class $c$ as $\delta_c=\text{mean}\{d_i\mid y_i=c\}$ and converted to a distance weight $w_c^{(d)}=\exp(-\gamma\,\delta_c)$. Class-wise validity weight is $w_c^{(v)}=\frac{1}{k_c}\sum_{i:y_i=c} v_i$, and the final class score is $S_c = w_c^{(d)}\,w_c^{(v)}$ (with majority-vote fallback if all $S_c=0$).","Across 9 datasets with stratified 5-fold CV, DW-KNN attains average accuracy 0.8988 (2nd of 6 methods) versus Ensemble-KNN 0.9007, and the lowest CV standard deviation 0.0156 (stability). DW-KNN shows best-in-class results on Adult Income (0.8167; +2.37% over next-best KNN-Uniform 0.8030) and German Credit-G (0.6750; +1.5% over next-best Ensemble-KNN 0.6600). Statistical testing reports significant improvements over Compactness-KNN (mean +0.0409, p<0.001) and KNN-Kernel (mean +0.0113, p<0.001), but not significant differences versus Ensemble-KNN, KNN-Distance, or KNN-Uniform. On a severely imbalanced synthetic dataset (1:3.6), DW-KNN increases minority precision (0.9412 vs 0.9000) while reducing minority recall (0.4103 vs 0.4615).","The authors note increased computational cost versus standard KNN because DW-KNN computes two neighborhoods (for classification and validity), roughly doubling runtime and potentially becoming prohibitive for very large datasets without acceleration. They report a precision–recall trade-off on severely imbalanced, overlapping data where minority recall drops (e.g., 0.410 vs 0.462), making it unsuitable when exhaustive minority detection is required. They also state that k still needs dataset-specific tuning, that performance in very high-dimensional settings (d>100) is uncertain due to the curse of dimensionality, and that interpretability via validity scores is not quantitatively validated (assumed to correlate with reliability).","The evaluation emphasizes overall accuracy, which can be misleading on imbalanced problems; stronger emphasis on AUC-PR, balanced accuracy, or cost-sensitive metrics would better match the paper’s reliability/robustness motivation. The approach depends on nearest-neighbor structure and the chosen distance metric after z-score normalization; robustness to correlated features, mixed data types (categorical), and dataset shift is not established. Validity scores are computed from training labels and neighborhoods, so systematic label noise or clustered mislabeling could inflate validity locally and mislead reliability weighting. Comparisons omit several strong modern baselines for tabular data (e.g., gradient boosting) and KNN variants (e.g., mutual kNN, hubness-aware methods), which limits conclusions about competitiveness beyond the KNN family.","They propose speeding up computation using approximate nearest neighbor search (e.g., LSH, HNSW), shared neighborhood reuse when $K_v\le k$, GPU parallelization, and incremental validity updates for online learning. To address imbalanced-data trade-offs, they suggest class-aware validity, adaptive weighting of $\gamma$ or $K_v$ based on local class distribution, cost-sensitive validity, and hybrid/ensemble approaches. They also suggest adaptive k selection, multi-scale validity across multiple k values, and Bayesian optimization for tuning. Additional future directions include extensions to regression, deep-learning integration (learned metrics, validity-aware neural architectures, hybrid deep-KNN, adversarial robustness), multimodal/structured data (text, time series, graphs), more theory (generalization bounds, optimal $K_v$), and interpretability validation via correlation analyses, user studies, and comparisons to LIME/SHAP.","Develop a self-starting/online variant where validity weights are updated efficiently under concept drift, and evaluate calibration (e.g., reliability diagrams) to test whether validity-weighted scores correspond to well-calibrated predictive probabilities. Add robust/heterogeneous-feature handling (mixed numerical/categorical distances, feature weighting, metric learning constrained for interpretability) and study hubness effects in higher dimensions. Provide standardized benchmarking with repeated CV, multiple random seeds, and broader imbalanced-learning baselines (SMOTE variants, focal/cost-sensitive methods, anomaly detection) to clarify when DW-KNN is preferable. Package the method as a tested library component (e.g., scikit-learn compatible) with diagnostics/visualizations for validity distributions and failure cases to support practical deployment.",2512.08956v1,https://arxiv.org/pdf/2512.08956v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:30:53Z
FALSE,NA,Other,Other,Not applicable,Finance/economics|Other,Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/adc-trust-ai/trust-free,"The paper proposes a deterministic alternative to classical permutation-based feature importance: replacing multiple random permutations per feature with a single, fixed “optimal” permutation that maximally displaces feature ranks (a cyclic shift by 0n/21). It defines Direct Variable Importance (DVI) as the average prediction disruption between the master model’s outputs and outputs after permuting one feature, using metrics such as MAE/MSE/RMSE of prediction differences (and negative Brier score changes for classification) to ensure non-negative, normalizable scores. The authors provide max–min optimality results for the rank-shift permutation and a bias–variance/MSE comparison showing when a deterministic single-permutation estimator can dominate the Monte Carlo (B-permutation) estimator. Empirically, they evaluate nearly 200 simulated scenarios (varying n, p, noise, and correlation) and report higher ground-truth recovery and stability than Breiman-style permutation importance, with large runtime gains vs B=10. The paper also introduces Systemic Variable Importance (SVI), a stress-testing extension that propagates a feature perturbation through correlated covariates to detect indirect reliance (e.g., via proxies for protected attributes) and demonstrates it on Boston HMDA and German Credit case studies.","Classical Monte Carlo permutation importance estimator is defined as \(\hat I_B=\frac{1}{B}\sum_{b=1}^B g(\pi_b)\) with variance \(\mathrm{Var}(\hat I_B)=\mathrm{Var}(g(\Pi))/B\). The proposed deterministic “optimal” permutation is the cyclic shift \(\pi_k(j)=(j+k)\bmod n\) with \(k=\lfloor n/2\rfloor\), shown to maximize the minimum circular displacement \(m(\pi)=\min_j d(j,\pi(j))\) where \(d(a,b)=\min(|a-b|,n-|a-b|)\). Direct Variable Importance is \(d_k=\frac{1}{nq}\sum_{i=1}^{nq} d\big(f_M(X)_i-f_M(X_k')_i\big)\) (then normalized), and SVI propagates perturbations via \(x_k'\leftarrow x_k' + \mathrm{cor}(x_k,x_j)(x_j'-x_j)\) with normalization \(s_k=s_k^{\mathrm{raw}}/\sum_j s_j^{\mathrm{raw}}\).","Across 48 linear-regression scenarios, Direct-Opt achieves ground-truth correlation 0.953±0.008 vs Breiman B=1 at 0.900±0.037 and Breiman B=10 at 0.910±0.035, with runtime ~24.9ms vs 98.4ms for B=10. In linear classification, Direct-Opt reaches 0.962±0.010 vs 0.868±0.052 (B=1) and 0.882±0.051 (B=10), while being ~27.1ms vs ~189.1ms (B=10). In Boston HMDA systemic audit with OLS, the protected attribute ‘black’ has SVI 0.44% decomposed into direct 0.09% and indirect 0.35% (MAE-based alternative: 2.31%=1.21%+1.10%); with a sparse model excluding ‘black’, SVI remains nonzero (0.16% via indirect effects). In German Credit, ‘Sex–Marital status’ has SVI 3.24%=2.09%+1.15% under unregularized logistic regression, but becomes 0 when excluded in a sparse logistic model.",SVI relies on correlations as a first-order approximation of dependence and is explicitly described as non-causal and bounded by model behavior rather than reflecting causal structure. The authors note that extending beyond pairwise correlations is not addressed and is left for future work. They also caution that testing more extreme feature correlations could compromise numerical stability and was not pursued.,"The deterministic “optimal” permutation is optimal under a rank-displacement criterion, but the paper does not fully connect that criterion to optimal removal of predictive signal for arbitrary model classes or loss functions; in some data/model regimes, maximal rank displacement may not maximize prediction disruption. The evaluation heavily uses linear/logistic models to define “ground-truth” importance, which may limit conclusions about fidelity for complex nonlinear black-box models where ground truth is ambiguous. The SVI propagation rule is linear in correlations and applies a single-step adjustment, which may misrepresent nonlinear dependencies, interactions, or conditional relationships and can be sensitive to scaling/encoding choices for categorical variables.","Future work will explore computationally efficient extensions of the SVI framework beyond pairwise correlations to capture richer dependence structures, to strengthen applicability to real-world stress-testing and regulatory audits.","Develop robust/self-normalizing versions of DVI/SVI under strong feature dependencies and mixed data types (e.g., principled handling of one-hot groups and correlation measures suited to categorical variables). Study theoretical properties of the single-permutation estimator under common data-generating assumptions (e.g., consistency or finite-sample bounds) and under autocorrelation/time-series settings. Provide a standardized, open-source implementation (e.g., a pip package) with reproducible benchmarks and guidance on selecting the SVI threshold level \(\alpha\) and correlation type (Spearman vs Pearson) for different domains.",2512.13892v2,https://arxiv.org/pdf/2512.13892v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:31:46Z
FALSE,NA,Simulation-based|Other,Simulated only,Not applicable,Other,Simulation study|Other,TRUE,R,Public repository (GitHub/GitLab)|Personal website,https://joonho112.github.io/IRTsimrel/|https://github.com/joonho112/reliability-targeted-irt-simulation,"The paper proposes a reliability-targeted simulation framework for Item Response Theory (IRT) studies, treating marginal reliability as an explicit design input rather than an incidental outcome. It formulates an inverse design problem: solve for a global discrimination scaling factor c that yields a prespecified target reliability under a fixed structural configuration (latent distribution, item pool, test length). Two calibration algorithms are introduced—Empirical Quadrature Calibration (EQC), a deterministic root-finding approach on a fixed Monte Carlo quadrature targeting average-information reliability, and Stochastic Approximation Calibration (SAC), a Robbins–Monro procedure that can target MSEM-based marginal reliability directly. A large validation across 960 factorial conditions (varying latent distribution shapes, Rasch vs 2PL, parametric vs Item Response Warehouse item pools, test length, and sample size) shows EQC calibrates essentially exactly while SAC is unbiased but noisier. The paper also highlights that average-information and MSEM-based reliability differ systematically (via Jensen’s inequality), implying different calibration scales for the same numeric target, and provides an open-source R package (IRTsimrel) implementing the workflow.","The data-generating model is the 2PL logistic: $\Pr(Y_{pi}=1\mid \theta_p,\beta_i,\lambda_i)=\mathrm{logit}^{-1}\{\lambda_i(\theta_p-\beta_i)\}$. Test information is $J(\theta)=\sum_{i=1}^I \lambda_i^2\,\pi_i(\theta)\{1-\pi_i(\theta)\}$ with $\mathrm{SEM}(\theta)=1/\sqrt{J(\theta)}$. Two scalar reliability functionals are defined: MSEM-based $\bar w=\sigma_\theta^2/(\sigma_\theta^2+\mathrm{MSEM})$ where $\mathrm{MSEM}=\mathbb{E}[1/J(\theta)]$, and average-information $\tilde\rho=\sigma_\theta^2\bar J/(\sigma_\theta^2\bar J+1)$ where $\bar J=\mathbb{E}[J(\theta)]$; calibration uses global discrimination scaling $\lambda_i(c)=c\lambda_{i,0}$.","In a 960-condition validation, EQC achieved essentially exact calibration for average-information reliability ($\mathrm{MAE}\approx 10^{-5}$; 100% of conditions within 0.01, 0.02, and 0.05 tolerance). SAC was approximately unbiased but noisier for both estimands, with $\mathrm{MAE}\approx 0.015$ and $\mathrm{SD}\approx 0.024$; about 73% (SAC targeting $\tilde\rho$) and 71.7% (SAC targeting $\bar w$) of conditions fell within ±0.02 of the target, and ~94–95% within ±0.05. The study empirically confirms Jensen’s inequality ordering $\tilde\rho\ge \bar w$, showing that targeting $\bar w$ requires systematically larger discrimination scale factors than targeting $\tilde\rho$. Finite-sample replication (K=2,000 per condition) shows realized reliability varies across datasets even when the population target is fixed, with variability decreasing as sample size N and test length I increase.","The authors state the scope is limited to dichotomous Rasch and 2PL models, noting extensions to 3PL, polytomous models, and multidimensional IRT as future work. They emphasize that a single global discrimination scaling factor controls only the overall magnitude of information, not the location/shape of precision along the trait continuum. They also note that population-level reliability targets do not directly equate to estimator-specific, finite-sample reported reliabilities (e.g., WLE vs EAP), which can vary due to sampling and definitional differences.","Although the work rigorously targets IRT reliability metrics, it is not framed in reliability engineering (failure/survival/maintenance) terms; applicability to engineering reliability is indirect at best. The EQC method is restricted to the average-information reliability because the MSEM-based objective can become non-monotone under some item configurations, which limits deterministic calibration for the more variance-decomposition-like estimand. The approach relies on a calibration interval for c (e.g., [0.1, 10]) and practical monotonicity assumptions; in real simulation workflows, poorly targeted item difficulty coverage or extreme latent distributions may require careful feasibility diagnostics and may yield unrealistic discrimination magnitudes.","They propose extending reliability-targeted calibration beyond dichotomous Rasch/2PL to the 3PL, polytomous IRT models (e.g., graded response, partial credit), and multidimensional IRT (including scalar summaries of matrix information or dimension-specific scaling). They also suggest further clarifying mappings between design-level (population) targets and commonly reported estimator-based reliabilities under finite-sample estimation, including robustness under model misspecification or atypical responding. They recommend combining global reliability targeting with additional structural manipulations (e.g., difficulty targeting, adaptive testing rules) when precision is required at specific trait regions.","A useful extension would be automated, robust selection of the calibration interval [cL, cU] and monotonicity diagnostics (especially for MSEM-based targets) to prevent convergence to boundary or pathological regions without manual tuning. Another direction is multi-objective calibration that simultaneously constrains discrimination plausibility (e.g., quantile bounds on $\lambda_i$) while meeting reliability targets, turning calibration into a constrained optimization problem. Finally, broader empirical benchmarking on a wider variety of real item banks and integrating the package into standard IRT simulation ecosystems (with standardized reproducibility artifacts) would strengthen practical adoption.",2512.16012v2,https://arxiv.org/pdf/2512.16012v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:32:27Z
TRUE,Life distribution modeling|System reliability,"Parametric (Weibull, etc.)|Other",Simulated only,Not applicable,Transportation/logistics|Other,Other,TRUE,None / Not applicable,Not provided,NA,"The paper analyzes a nonrepairable 1-out-of-n cold-standby redundant system assuming active-component lifetimes follow the Generalized Lindley Distribution (GLD), motivated by GLD’s flexible hazard-rate shapes as alternatives to Weibull/gamma/lognormal. Using an MGF-based derivation, it first obtains a closed-form PDF for the sum of n i.i.d. GLD random variables, enabling an explicit closed-form expression for system reliability under a perfect switch. For imperfect switching, where the switch lifetime is also modeled by GLD, exact evaluation is deemed difficult and a tractable lower bound on system reliability is derived using monotonicity of the switch reliability function. The paper also derives an explicit mean time to failure (MTTF) expression for the perfect-switch case and conducts sensitivity analysis (via partial derivatives) indicating the scale parameter θ has the strongest impact on reliability/MTTF, while γ and η have weaker effects. Numerical illustrations are given for an aircraft emergency braking system architecture across redundancy levels (n=2,5,10,20) over a 100-hour mission, comparing perfect vs imperfect switch reliability curves and sensitivity trends.","The GLD component reliability is given in terms of upper incomplete gamma functions: $R_{GLD}(t)=\frac{\gamma}{\gamma+\theta^{\eta}}\frac{\Gamma(\alpha,\theta t)}{\Gamma(\alpha)}+\frac{\theta^{\eta}}{\gamma+\theta^{\eta}}\frac{\Gamma(\beta,\theta t)}{\Gamma(\beta)}$. The sum of $n$ i.i.d. GLD lifetimes has a closed-form mixture-of-gammas density (Eq. 4) obtained via $M_{S_n}(t)=(M_X(t))^n$ and binomial expansion, where each term corresponds to a gamma distribution with shape $i\alpha+(n-i)\beta$. Perfect-switch system reliability is computed via the convolution form $R_{sys}^{Perfect}(t)=r(t)+\sum_{i=1}^{n-1}\int_0^t f_{S_i}(u)\,r(t-u)\,du$ and is given explicitly for GLD in Eq. (6); for imperfect switching a lower bound is $\hat R_{sys}^{Imperfect}(t)=r(t)+r_s(t)\left(R_{sys}^{Perfect}(t)-r(t)\right)$ (Eq. 14).","A closed-form system reliability expression is derived for a 1-out-of-n cold-standby system with perfect switching when component lifetimes follow the GLD (Eq. 6), enabled by the new closed-form PDF for sums of i.i.d. GLD variables (Eq. 4). For imperfect switching (switch lifetime also GLD), the paper provides a computable lower bound (Eq. 14) rather than an exact expression. The MTTF for the perfect-switch GLD case simplifies to an explicit formula (Eq. 10): $MTTF_{GLD}^{Perfect}=\frac{n}{\theta}\left(\frac{\gamma\alpha+\beta\theta^{\eta}}{\gamma+\theta^{\eta}}\right)$, recovering $n/\theta$ for the exponential special case. In the numerical example (mission time 100 hours; components GLD with $(\alpha,\beta,\theta,\gamma,\eta)=(2,3,0.5,1.5,2.2)$; switch GLD with $(\alpha_s,\beta_s,\theta_s,\gamma_s,\eta_s)=(4,4,0.005,1,1)$), reliability increases with redundancy level (n=2,5,10,20) and perfect switching dominates imperfect switching across the mission. Sensitivity analysis indicates θ is the most influential parameter on reliability/MTTF, while γ and η have the least influence (as stated based on Fig. 3 and Tables 3–5).","For the imperfect-switch case, the authors state that “exact calculations become very difficult,” so they seek and present only a lower bound for system reliability rather than an exact closed-form result. The numerical section is presented as a canonical illustrative case (aircraft emergency braking cold-standby architecture) rather than a validation with observed failure data.","The work assumes i.i.d. component lifetimes in the active state and negligible switching time; dependence between component failures, common-cause failures, and load/usage effects are not modeled, which can materially affect cold-standby system reliability. The imperfect-switch treatment provides only a lower bound and relies on monotonicity of switch reliability; how tight this bound is across parameter regimes is not systematically quantified. Numerical results are demonstration-based with fixed parameter choices and do not include estimation procedures, uncertainty quantification, or calibration to real field/lab data. Implementation details (algorithms for evaluating the multiple sums, numerical stability for large n or large t, and computational cost) are not discussed, which may limit practical scalability.","The authors propose future work on redundancy allocation problems, aiming to model a system introduced in Fyffe et al. (1968) and compute maximum reliability under constraints on cost, weight, and volume of system components.","A natural extension is to assess tightness of the imperfect-switch lower bound (Eq. 14) via systematic numerical studies and/or derive tighter bounds or approximations (e.g., via renewal/Markov formulations) when switch and component lifetimes are GLD. Extending the model to heterogeneous components (non-identical GLD parameters), common-cause failures, and dependent lifetimes would improve realism for safety-critical applications. Incorporating parameter estimation for GLD from censored life-test/field data and propagating uncertainty to reliability/MTTF would make the approach directly deployable. Providing reference software (e.g., R/Python) and numerical guidance for computing Eq. (6) efficiently for larger n and mission times would enhance reproducibility and adoption.",2512.23019v1,https://arxiv.org/pdf/2512.23019v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:33:04Z
TRUE,Degradation modeling|Life distribution modeling|Other,"Stochastic process|Parametric (Weibull, etc.)|Simulation-based|Other",Degradation measurements|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Other,Simulation study|Other,TRUE,MATLAB|Other,Not provided,https://www.ieee.org/publications/rights/index.html,"The paper develops a storage reliability modeling framework for single-sided aluminized polyimide films (SAPFs), using solar absorption ratio (SAR) degradation as the health indicator under temperature and relative humidity stresses. It identifies that the temperature effect exhibits a stress-induced degradation mechanism transition (two stages separated by a threshold temperature), and proposes a new stress/degradation-rate model that switches form across the threshold while keeping humidity influence unchanged. The degradation path model includes multi-source uncertainties: random initial performance, random degradation rate, Brownian-motion time variability, and measurement error; parameters are estimated via maximum likelihood with a meta-heuristic optimizer (TERIME) and uncertainty assessed by subsampling-based interval estimation. Reliability is defined via a first-passage time to an SAR threshold and computed by Monte Carlo simulation because closed-form solutions are intractable under the nonlinear time-scaling. Comparative studies against a traditional “unchanged mechanism” accelerated degradation model and ablated uncertainty variants show improved fit, extrapolation, and robustness when mechanism transition and measurement error are included; ignoring the transition can severely overestimate multi-year storage reliability.","The deterministic degradation trend is modeled as $Y(t)=Y_0 + a\,e(\mathbf{s})\,\Lambda(t)$ with a time-scale $\Lambda(t)=t^{\beta}$. The key novelty is the piecewise stress-rate function $e(\mathbf{s})$ that depends on temperature relative to $T_{\text{threshold}}$: for $T<T_{\text{threshold}}$, $e=\exp(\alpha_1\,\varphi_1(T)+\alpha_2\,\phi^*)$, and for $T\ge T_{\text{threshold}}$, $e=\exp(\alpha_1+\alpha_2\,\phi^*+\alpha_3\,\varphi_3(T))$, where $\varphi_1(T),\varphi_3(T)$ are standardized temperature functions and $\phi^*$ is standardized humidity. The full stochastic model adds uncertainties: $Y(t)=Y_0 + a\,e(\mathbf{s})\,t^{\beta}+\sigma B(t)+\varepsilon$, with $Y_0$ and $a$ random effects and $\varepsilon$ measurement error; reliability is $R(t)=\Pr(L\ge t)$ with lifetime $L=\inf\{t: Y(t)\ge Y_{\text{th}}\}$.","Using SAR degradation data at eight temperature–humidity stress levels (12 samples per level, measured every 12 hours), the estimated mechanism-transition temperature is $\hat T_{\text{threshold}}=63.75\,^{\circ}\mathrm{C}$ with a 90% interval of $[54.30,75.76]$. The low-temperature temperature-effect parameter is positive ($\hat\alpha_1=1.836$; 90% interval $[1.107,2.632]$) while the high-temperature stage parameter is negative ($\hat\alpha_3=-0.2901$; 90% interval $[-0.9664,0.0520]$), indicating acceleration then inhibition of degradation as temperature increases past the threshold. In model-fitting comparisons, the proposed model (M0) improves log-likelihood and AIC versus the unchanged-mechanism model (M1): $L_{\max}$ improves from -464.86 to -461.19 and AIC decreases from 947.72 to 944.38 (RMSE 0.219 to 0.217). Ablation shows measurement error is crucial (model without measurement error performs much worse: RMSE 0.245 and AIC 1458.82), while time-dimension uncertainty and unit-to-unit variability are negligible for these data (M2 matches M0). In an illustrative storage-profile case, the paper reports that ignoring mechanism transition can drastically overestimate storage reliability (proposed M0 near-zero reliability by 5 years vs. M1 above 0.9).","The authors note that ideal experiments would hold humidity constant while varying temperature to confirm mechanisms, then run additional tests varying humidity; they did not do this due to financial constraints, limiting stress-level coverage and experimental design optimality. They also state that directly confirming the surface oxide products (e.g., via X-ray diffraction) is infeasible because the oxide layer on SAPF is extremely thin (around a few nanometers), below typical instrument analysis limits. As a result, the mechanism must be inferred from SAR degradation data (“black box” approach).","The mechanism-transition temperature is treated as a single global threshold with an abrupt switch; in reality the transition may be gradual and may vary by batch, humidity level, or other covariates, which could bias extrapolation near the threshold. The model assumes Gaussian random effects and Brownian-motion structure (including independent measurement errors), which may be sensitive to autocorrelation induced by repeated removal/measurement procedures or non-Gaussian error tails. The approach relies on meta-heuristic optimization (TERIME) but does not provide reproducible implementation details (e.g., random seeds, stopping criteria diagnostics), which can affect replicability and convergence assessment in other datasets.",The authors propose extending the degradation model to other products with stress-induced degradation mechanism transitions. They suggest studying such transitions under non-Arrhenius acceleration models and exploring temperature–humidity interaction effects to improve model accuracy. They also plan to incorporate SAPF geometries and manufacturing conditions as covariates in the degradation model to guide reliability design.,"A natural extension is a smooth/continuous transition model (e.g., logistic mixture or spline-based stress-response) to avoid discontinuity at $T_{\text{threshold}}$ and better capture partial formation of different passivation layers. Developing a self-starting/online updating version (Bayesian or state-space) would enable reliability updating as new storage-monitoring data arrive. Additional robustness checks for non-Gaussian measurement error and for dependence induced by repeated handling/measurement could strengthen practical applicability, along with releasing an implementation package to standardize estimation and simulation workflows.",2601.08655v1,https://arxiv.org/pdf/2601.08655v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:33:46Z
FALSE,Other,ML-based|Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,Python,Not provided,NA,"The paper proposes a framework for “reliable representation learning” that treats reliability as a property of learned latent representations rather than only prediction outputs. It models each representation as a distribution $z_i\sim D(\mu_i,\Sigma_i)$ and adds uncertainty-aware regularization to penalize overly diffuse representation uncertainty (e.g., via $\mathrm{tr}(\Sigma_i)$ or $\log\det\Sigma_i$). Structural constraints (e.g., graph/group relations) are incorporated through a Laplacian-style regularizer to encourage geometrically consistent representations, and a combined structure–uncertainty term aligns mean and uncertainty profiles across related samples. Reliability is evaluated using stability under input perturbations, calibration via empirical coverage of Mahalanobis balls, and robustness to structural corruption; experiments are performed on a controlled synthetic latent-variable simulation under covariate noise and structure shifts. The authors report that uncertainty-aware variants enable better selective prediction behavior (risk–coverage tradeoffs) under shift, while nominal accuracy remains similar across variants.","Representations are modeled as distributions $z_i\sim D(\mu_i,\Sigma_i)$ with $(\mu_i,\Sigma_i)=f_\theta(x_i)$. The objective combines task loss with uncertainty and structure terms: $\mathcal{L}=\mathcal{L}_{task}(\{\mu_i\})+\lambda_{unc} \frac{1}{n}\sum_i \phi(\Sigma_i)+\lambda_{struct}\sum_{(i,j)\in S} w_{ij}(\|\mu_i-\mu_j\|^2+\psi(\Sigma_i,\Sigma_j))$. Structural regularization is Laplacian-based: $R_{structure}(Z;S)=\sum_{(i,j)\in S} w_{ij}\|z_i-z_j\|^2=\mathrm{tr}(Z^\top L Z)$.","In synthetic experiments, robustness to structural shift (measured as $\mathbb{E}_i\|\mu_i(S^*)-\mu_i(S')\|^2$) is reported as approximately 0.85 at corruption probability $p=0.2$, while a baseline encoder without structural components yields robustness close to zero across all $p$. Downstream multiclass accuracy under increasing test noise degrades from about 1.0 in-distribution to about 0.78–0.81 at the strongest noise level ($\tau_{test}=2.0$), with similar accuracy across variants. Calibration (ECE) degrades under joint covariate and structural shifts, but only uncertainty-aware variants support selective prediction via risk–coverage curves. An example risk–coverage result states that at $\tau_{test}=1.0$ and $p=0$, the UQ-only variant maintains risk below ~0.04 at full coverage and can reduce risk to ~0.01 by abstaining on the most uncertain 90% of examples.","The authors note that results are obtained via controlled synthetic experiments and suggest that applying the framework to real-world multimodal domains is needed to demonstrate practical benefits beyond simulation. They also emphasize that structural priors may be weak or imperfect, implying performance depends on the quality/choice of the structural operator and its corruption model. No additional explicit limitations are stated beyond these scope-related caveats.","Although termed “reliable,” the work is not reliability engineering in the classical sense (failure/time-to-event/degradation/maintenance), so its conclusions may not transfer to engineering reliability problems without additional linkage. The proposed regularizers depend on choices of $\phi(\Sigma)$ and $\psi(\Sigma_i,\Sigma_j)$ and tuning of $\lambda$ weights; sensitivity analyses and practical selection guidance are not provided. Evaluation is primarily simulation-based with limited baselines (focused on ablations), so comparative performance against strong modern UQ/representation methods under standardized benchmarks is unclear. The approach assumes representation distributions (and calibration checks) are well captured by Gaussian/Mahalanobis geometry in key metrics, which may not hold for complex real-world latent spaces.","The authors propose applying the framework to real-world multimodal domains to assess practical benefits beyond simulation. They suggest incorporating more expressive structure classes (e.g., causal graphs, semantic taxonomies, or learned relational priors) to enhance robustness under imperfect knowledge. They also propose integrating reliable representations with downstream conformal prediction, robustness certification, or fairness auditing to build end-to-end reliable learning systems.","Develop self-starting/online versions that can adapt uncertainty and structure regularization under streaming distribution shift, with principled monitoring of representation drift. Extend the reliability metrics and calibration procedures to non-Gaussian latent distributions (e.g., using conformal sets in representation space or distribution-free coverage guarantees). Validate on real datasets with known structure noise (e.g., partially observed graphs) and provide guidance for selecting/learning the structural operator $S$ and hyperparameters robustly. Provide an open-source implementation and standardized experimental protocol to improve reproducibility and enable broader benchmarking.",2601.16174v1,https://arxiv.org/pdf/2601.16174v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:34:20Z
TRUE,Maintenance optimization|System reliability|Life distribution modeling|Other,Stochastic process|Nonparametric/Semi-parametric|Other,Other,Condition-based|Age-based|Block replacement|Imperfect maintenance|Other,Theoretical/simulation only,Exact distribution theory|Other,FALSE,None / Not applicable,Not applicable (No code used),NA,"The paper studies and compares two repair/replacement policies for a single unit: (i) replacement-by-new (a renewal-type policy with independent lifetimes summed over replacements) and (ii) a continuous relevation policy, where after each failure the unit is replaced by a used unit of the same age, leading to failure times described via the relevation transform. The relevation-driven failure sequence is characterized as the arrival times of an elementary pure birth (EPB) process, and the authors compare the resulting counting processes and failure-time vectors with those under replacement-by-new. Main results give sufficient ageing conditions under which one policy yields stochastically larger/lower sequences of failure times and correspondingly fewer/more replacements by any time t: NBU/NWU conditions imply multivariate stochastic ordering, and IFR/DFR conditions imply ordering in the multivariate dynamic hazard-rate sense. Applications show how the general theorems specialize to minimal repair (relevation with identical distributions) versus renewal replacement, and to generalized Yule birth processes (EPB with state-dependent intensities). The contribution extends prior univariate comparisons by providing multivariate ordering results and dynamic hazard-rate comparisons for the whole sequence of failure times.","The relevation transform for independent lifetimes T and S with survival functions \(\bar F\) and \(\bar G\) yields the next-failure survival as \(\bar F(t)+\bar G(t)\int_0^t \frac{dF(x)}{\bar G(x)}\), and can be represented as \(T\#S = T + S_T\) where \(S_T=[S-T\mid S>T]\). Under an EPB/relevation policy, the arrival times \(T_n\) form a Markov chain with transition \(\Pr(T_n>t\mid T_{n-1}=s)=\frac{\bar F_n(t)}{\bar F_n(s)}\) (equivalently \(X_n\mid X_n>s\)), and marginal survivals satisfy the recursion \(G_1(t)=\bar F_1(t)\), \(G_n(t)=G_{n-1}(t)+\bar F_n(t)\int_0^t \frac{dG_{n-1}(x)}{\bar F_n(x)}\). Under replacement-by-new, arrival times are \(T_n' = \sum_{i=1}^n X_i\) with \(\Pr(T_n'>t\mid T_{n-1}'=s)=1-F_n(t-s)\).","If each interreplacement lifetime \(X_i\) is NBU (resp. NWU), then the replacement-by-new arrival-time vector dominates (resp. is dominated by) the relevation arrival-time vector in multivariate stochastic order: \((T_1',\ldots,T_n') \ge_{st} (T_1,\ldots,T_n)\) (resp. \(\le_{st}\)) for all n, implying \(N'(t)\le_{st} N(t)\) (resp. \(\ge_{st}\)) for all t. If each \(X_i\) is absolutely continuous IFR (resp. DFR), then \((T_1',\ldots,T_n') \ge_{dyn\text{-}hr} (T_1,\ldots,T_n)\) (resp. \(\le_{dyn\text{-}hr}\)) for all n. Extensions allow comparing policies built from different sequences \(\{X_n\}\) and \(\{Y_n\}\) using conditions like \(X_n \ge_{st} [Y_n-t\mid Y_n>t]\) (or the hazard-rate analogue) for n\ge2. A counterexample with a non-monotone hazard rate shows the stochastic order between \(T_1\#T_2\) and \(T_1+T_2\) can fail without NBU/NWU-type ageing assumptions.","The authors state that cost-rate/optimization comparisons of policies (common in other work) are not considered in this paper; the focus is on stochastic comparisons of failure times and counts. They also emphasize that conclusions depend on ageing properties (e.g., NBU/NWU, IFR/DFR), and without such conditions the ordering can fail (illustrated by a counterexample).","The results are primarily theoretical and rely on standard assumptions such as independence across replacement lifetimes and (often) absolute continuity; real maintenance settings may involve dependence, heterogeneity, or covariate effects not modeled here. Comparisons are qualitative (orderings) rather than providing explicit performance metrics such as expected cost, downtime, or optimal policy parameters, which can limit direct engineering decision support. The relevation policy assumes replacement by a used unit of the same age, which may be difficult to operationalize and may not reflect actual refurbishment/repair actions that change age and condition differently.",None stated.,"Extending the ordering results to settings with dependent interreplacement times (e.g., environmental or operational covariates) or to imperfect-repair models that interpolate between minimal repair and replacement-by-new would broaden applicability. Developing cost/downtime-based decision rules that leverage the established stochastic/dynamic hazard-rate orderings could connect the theory to optimization of maintenance policies. Robust or nonparametric variants that relax absolute continuity or handle discrete/mixture lifetime distributions would improve practical coverage.",2601.17518v1,https://arxiv.org/pdf/2601.17518v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:34:56Z
FALSE,NA,Other,Simulated only|Other,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://doi.org/10.1016/0304-4149(82)90051-5|https://www.sciencedirect.com/science/article/pii/0304414982900515|https://openreview.net/forum?id=r5njV3BsuD|https://arxiv.org/abs/math/0511255|https://arxiv.org/abs/2104.07708|https://openreview.net/forum?id=AAWuCvzaVt|https://openreview.net/forum?id=gThGBHhqcU|https://proceedings.mlr.press/v202/dinh23a.html|https://arxiv.org/abs/2505.21666|http://jmlr.org/papers/v6/hyvarinen05a.html|https://arxiv.org/abs/1612.04346|https://arxiv.org/abs/1903.07093|https://books.google.com/books?id=wI4fAwAAQBAJ|https://proceedings.mlr.press/v202/ganz23a.html|https://openreview.net/forum?id=9DXXMXnIGm|https://www.columbia.edu/~wt2319/CDG.pdf|https://api.semanticscholar.org/CorpusID:257757144|https://arxiv.org/abs/2507.21627|https://proceedings.mlr.press/v201/lee23a.html,"The paper studies classifier-guided diffusion models and asks when minimizing cross-entropy (expected conditional label KL divergence) for a time-dependent classifier guarantees accurate diffusion guidance gradients. It proves negative results showing cross-entropy error can vanish while the guidance vector field error stays bounded or even diverges, via explicit high-frequency classifier perturbations (Theorem 3.1). Under bounded-support and C^2 smoothness assumptions with gradient/Hessian bounds aligned to the true conditional, it proves that cross-entropy error ε^2 at each diffusion noise level implies L2 guidance gradient MSE scaling essentially as Õ(d(ε+ε^2)/(σ_t^2 P(y)^{3/2})) (Theorem 3.2), and shows this ε-dependence is unimprovable (Theorem 3.3). It then bounds KL error of a discretized guided sampler in terms of score error, guidance error, and step-size terms, yielding step complexity Õ(d log^2(1/δ)/(ε_score^2+ε_guide^2)) (Corollary 3.1). Numerical experiments (synthetic binary example and logistic regression with controlled noise) corroborate both the counterexample behavior and the smoothness-based guidance-error tracking.","Classifier guidance uses the conditional-score decomposition $\nabla_x\log p_t(x\mid y)=\nabla_x\log p_t(x)+\nabla_x\log p_t(y\mid x)$, so the guidance vector is $g_t(x,y)=\nabla_x\log p_t(y\mid x)$. The learned classifier $\hat p_{\phi,t}(y\mid x)$ is trained by cross-entropy/conditional KL $\mathcal L_{\mathrm{cls}}(\phi;t)=\mathbb E_{X_t\sim p_t}[D_{\mathrm{KL}}(p_t(\cdot\mid X_t)\,\|\,\hat p_{\phi,t}(\cdot\mid X_t))]$, and guidance error is $E_{\mathrm{guid}}(t,y)=\mathbb E_{X\sim p_t(\cdot\mid y)}\|\nabla_x\log p_t(y\mid X)-\nabla_x\log \hat p_{\phi,t}(y\mid X)\|_2^2$. The discrete guided sampler update is $\hat X^{(y)}_{t_{k+1}}=e^{\tau_k}\hat X^{(y)}_{t_k}+2(e^{\tau_k}-1)s_\theta(\hat X^{(y)}_{t_k},s_k)+2(e^{\tau_k}-1)\nabla_x\log\hat p_{\phi,s_k}(y\mid\hat X^{(y)}_{t_k})+\sqrt{e^{2\tau_k}-1}\,Z_k$.","Theorem 3.1 constructs sequences with vanishing conditional KL yet non-vanishing or diverging guidance-gradient error: e.g., KL $O(1/n)$ with gradient error $\Omega(1)$, or KL $O(1/\sqrt n)$ with gradient error $\Omega(n)$. Theorem 3.2 gives an upper bound (up to logs) $\mathbb E\|\nabla\log p_{s_k}(y\mid X)-\nabla\log\hat p_{\phi,s_k}(y\mid X)\|^2=\tilde O\big(d(\varepsilon_{s_k}+\varepsilon_{s_k}^2)/(\sigma_{s_k}^2 P(y)^{3/2})\big)$ under bounded support and classifier smoothness/derivative bounds. Corollary 3.1 yields sampling step complexity $\tilde O\big(d\log^2(1/\delta)/(\varepsilon_{\mathrm{score}}^2+\varepsilon_{\mathrm{guide}}^2)\big)$, matching unguided near-linear dimension dependence. Experiments confirm the counterexample: with perturbation amplitude $\delta_n=1/n$, KL decays while guidance error stays roughly constant; with $\delta_n=1/\sqrt n$, guidance error grows roughly linearly in $n$ while KL still decays.",None stated.,"The results rely on fairly strong regularity/structure assumptions for the positive guarantee (bounded data support and $C^2$ smoothness with explicit gradient/Hessian bounds), which may not hold for common high-dimensional continuous domains or non-smooth data distributions. The analysis is tailored to the OU forward process and classifier guidance in score-based diffusion; it does not cover other forward SDEs, classifier-free guidance, or practical issues like unknown/estimated diffusion-time conditionals and finite-sample/optimization effects in deep nets. Empirical validation is limited to synthetic constructions and logistic regression, so real-world diffusion guidance behavior with modern neural classifiers is not directly benchmarked here.","The paper suggests extending the analysis to non-smooth or non-compactly supported data distributions and investigating whether problem-specific structure (e.g., low-dimensional manifolds) can reduce the dimension dependence in the bounds.","Developing practically checkable training-time diagnostics or regularizers that enforce the needed smoothness/derivative control for neural classifiers would make the theory more actionable. Extending bounds to settings with autocorrelated/structured inputs (e.g., images with spatial correlation), multivariate/structured conditioning, or alternative forward SDEs could broaden applicability. Providing open-source reference implementations for computing/estimating guidance-error surrogates during classifier training would help practitioners apply the proposed selection guidelines.",2601.21200v1,https://arxiv.org/pdf/2601.21200v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:35:40Z
FALSE,NA,ML-based|Physics-based|Hybrid/Ensemble,Sensor/condition monitoring|Other,Not applicable,Healthcare/medical,Simulation study|Case study (real dataset),TRUE,Python,Public repository (GitHub/GitLab),https://github.com/5y3datif/Blind_Ultrasound_Enhancement,"This paper proposes a blind, self-supervised ultrasound (B-mode) image enhancement method that jointly denoises speckle-like noise and deconvolves PSF-induced blur without requiring clean ground-truth targets, PSF calibration, or noise variance estimates. Training pairs are synthesized from real ultrasound frames by applying a physics-guided degradation model (Gaussian PSF surrogate blur plus either additive Gaussian noise or Fourier-domain complex perturbations), while “clean-like” targets for ultrasound are approximated using non-local low-rank (NLLR) denoising. The restoration network is a Swin Convolutional U-Net (SC-UNet) combining local convolutional processing with shifted-window self-attention and additive skip fusions. Experiments on UDIAT B, JNU-IFM, and XPIE Set-P (with additional evaluation on PSFHS test images) report higher PSNR/SSIM than MSANN, Restormer, and DnCNN across Gaussian and speckle noise ladders, and controlled blur studies show improved edge sharpness (reduced FWHM, increased gradient metrics). As a plug-and-play preprocessor, the method improves downstream segmentation Dice for fetal head and pubic symphysis on PSFHS under varying noise levels.","The degradation model forms a cropped/rotated patch \(\tilde I\) and applies Gaussian PSF surrogate blur \(I_b = \tilde I * h_k\) with \(h_k(u,v) \propto \exp(-(u^2+v^2)/(2\sigma_b^2))\). Noise is added either spatially \(I_g=\tilde I + n\), \(n\sim\mathcal N(0,\sigma_g^2)\), or in the Fourier domain via \(\mathcal F(I_f)=(1-\gamma_f)\mathcal F(\tilde I)+\gamma_f\|\mathcal F(\tilde I)\|_\infty\zeta\), \(\zeta\sim\mathcal{CN}(0,I)\), followed by inverse FFT and magnitude. Degradations are composed in random order \(I_d = T(\tilde I)\) (blur→noise or noise→blur), and the self-supervised target for ultrasound is \(I_t=D_{\mathrm{NLLR}}(I)\); training minimizes an \(\ell_1\) loss \(\|f_\theta(I_d)-I_t\|_1\).","In controlled blur experiments (Gaussian PSF with \(\sigma\in\{0,3,7,15\}\)), the method improves sharpness metrics versus blurred inputs (e.g., at \(\sigma=3\), GradMax increases from 0.1850 to 0.2974 and FWHM decreases from 142 to 138 in the reported ROI). Across datasets and blur levels \(\sigma\in\{3,\dots,15\}\), reported gains are +3.56 to +13.78 dB PSNR and +0.01 to +0.26 SSIM (Table 2). For Gaussian noise ladders, the paper reports typical improvements of ~1–4 dB PSNR and 0.05–0.15 SSIM over Restormer under heavy noise, and larger gaps over DnCNN/MSANN. For speckle noise ladders, the method is typically ahead by ~2–5 dB PSNR and 0.05–0.20 SSIM in the severe-noise regime (small ENL), and it increases downstream segmentation Dice on PSFHS across all noise levels (largest gains under severe noise).","The authors note potential bias introduced by using NLLR denoising to approximate clean-like targets for ultrasound training. They also state the method is demonstrated on 2D B-mode images, limiting direct applicability to 3D or temporal ultrasound without adaptation.","The PSF is modeled with an isotropic Gaussian surrogate; real ultrasound PSFs are often spatially varying, anisotropic, and depth-dependent, so performance under strongly non-Gaussian/space-variant blur may be less robust than reported. Checkpoint selection includes visual inspection, which can introduce subjective bias and reduces reproducibility unless standardized criteria are defined. The evaluation focuses on PSNR/SSIM and select edge-profile metrics; clinical utility may require reader studies or task-based metrics beyond segmentation Dice (e.g., diagnostic accuracy) and robustness to scanner-specific preprocessing pipelines.","The authors propose investigating end-to-end joint learning with segmentation networks, extending the approach to 3D/temporal ultrasound, adding explicit uncertainty modeling, and developing lightweight variants for real-time deployment on ultrasound scanners.","A useful extension would be a self-starting/online adaptation mechanism to handle domain shift across scanners and protocols without retraining, potentially using test-time adaptation with stability constraints. Incorporating spatially varying or learned PSF models (including anisotropy and depth dependence) could make the physics-guided degradation closer to real acquisition and improve deconvolution reliability. Providing an ablation on the target-generation step (NLLR vs alternatives) and releasing standardized evaluation scripts would strengthen reproducibility and clarify which components drive gains.",2601.21856v1,https://arxiv.org/pdf/2601.21856v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:36:31Z
FALSE,NA,Simulation-based|Other,Simulated only|Other,Not applicable,Network/cybersecurity|Other,Simulation study,TRUE,None / Not applicable,Not provided,NA,"This paper studies how reliably node rankings (importance ordering) are preserved when only a randomly sampled subset of nodes/edges from a larger network is observed. It defines a rank-correlation metric \(\tau\) (a Kendall’s Tau variant) to measure agreement between sampled-network and original-network node orders, and a distributional similarity metric \(\rho\) (Pearson correlation after matching cumulative distributions) to compare centrality distributions under sampling. Using simulations on the Barabási–Albert model and analyses on several real networks (e.g., arXiv coauthorship, Internet AS, protein–protein interaction), it finds that high-ranked nodes tend to have more reliable ranks than low-ranked nodes, and that closeness-based rankings are generally more reliable than degree or betweenness under random node sampling. It also shows that when sampling is biased to avoid hubs (limited hub accessibility), increasing the sampling fraction can paradoxically reduce rank-order accuracy (\(\tau\)) over an intermediate range. Finally, it suggests estimating sampling accuracy by resampling within the observed sampled network to provide a lower bound on \(\tau\) for the true underlying network.","Rank-order preservability is measured by \(\tau=(N_+ - N_-)/(N_+ + N_-)\), where \(N_+\) counts sampled-node pairs whose order is consistent between sampled and original networks (including ties treated consistently) and \(N_-\) counts inconsistent pairs; equivalently, the probability of consistent ordering is \(p=(\tau+1)/2\). Distributional preservability for a centrality \(k\) is measured by computing cumulative distributions \(P_S(k)\) (sample) and \(P_O(k)\) (original), mapping each sampled value \(k_i\) to \(k^o_i\) such that \(P_S(k_i)=P_O(k^o_i)\), and then taking the Pearson correlation \(\rho\) between \(\{k_i\}\) and \(\{k^o_i\}\) with an additional normalization to map it into \([0,1]\).","In BA-model simulations, \(\rho\) for degree/betweenness rises quickly with sampling fraction and saturates near 1 much earlier than \(\tau\), indicating that overall centrality distributions can look accurate even when individual node ranks are not. The contribution to \(\tau\) is strongly rank-dependent: higher-ranked nodes in the sample exhibit substantially higher \(\tau\) than lower-ranked nodes, so top hubs’ relative ordering is more stable under sampling. Across centrality measures, closeness-based ranks show the highest \(\tau\) under random node sampling, while degree-based ranks are less reliable due to local-topology fluctuations; in real networks, betweenness \(\tau\) can be notably suppressed by modularity. Under hub-avoidant (ascending-centrality) sampling, \(\tau\) can become convex in sampling fraction with a minimum at intermediate fractions—so sampling more nodes can reduce rank accuracy if hubs are systematically missed. The authors propose using resampling within the observed sampled network to obtain a lower bound estimate of \(\tau\) for the (unknown) full network under approximately random sampling.","The authors note that they focus only on sampling via randomly sampled nodes (and, separately, a stylized hub-limited scheme), while real data collection may follow other schemes such as snowball sampling or randomly sampled links. They state that studying only randomly sampled nodes has limitations for understanding more specific real-world sampling problems, and that deviations from reality could be reduced by investigating different sampling schemes.","The work treats \(\tau\) and \(\rho\) largely as descriptive accuracy measures without connecting them to downstream decision quality (e.g., how many truly top-k nodes are recovered), which is often what practitioners need. Results depend on having an identified node correspondence between sample and original network (available in simulations/known datasets), whereas in many practical settings entity resolution and missing/duplicate identities add additional uncertainty. The paper does not provide reproducible implementation details (e.g., exact algorithms/complexities for centralities on disconnected sampled graphs), which can affect computed closeness/betweenness in practice.","The authors suggest that further reducing possible deviations between their results and real situations would require additional investigation of different sampling schemes beyond random node sampling (e.g., snowball sampling and randomly sampled links) and broader exploration of sampling problems in complex networks, including errors in power-law statistics and useful sampling protocols.","A natural extension is to evaluate rank reliability using task-oriented metrics (e.g., top-k precision/recall for hub identification, stability of intervention sets) and to derive confidence intervals for \(\tau\) under specific sampling models. Another direction is to incorporate more realistic observation processes (non-response, link reporting errors, temporal sampling) and assess robustness under network autocorrelation and dynamics. Providing open-source code and standardized benchmarks across multiple sampling designs would improve reproducibility and facilitate practitioner adoption.",physics/0702148v1,https://arxiv.org/pdf/physics/0702148v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-01-30T15:37:02Z
FALSE,NA,Nonparametric/Semi-parametric|Simulation-based|Other,Other,Not applicable,Healthcare/medical|Other,Simulation study|Other,TRUE,Python,Public repository (GitHub/GitLab),https://github.com/MLAI-Yonsei/MACI.git,"This paper proposes Multi-LLM Adaptive Conformal Inference (MACI), a conformal-inference framework to filter hallucinated (false) claims in LLM responses while meeting a user-specified error rate. It reformulates document-level factuality filtering using a multiplicative (cumulative product) conformity score that aggregates claim-level factuality-scores rather than relying on a single worst-case claim. MACI combines group-conditional (Mondrian) calibration to provide finite-sample, distribution-free coverage guarantees within predefined semantic groups, and a multi-LLM ensemble to improve factuality-score quality and thus retention. The authors provide theory linking estimator MSE to a retention gap under a margin condition, motivating ensembling to reduce MSE and improve true-claim preservation. Experiments on MedLFQA, WikiBio, and ExpertQA show MACI attains target group-conditional coverage with substantially higher retention and lower wall-clock time than conformal baselines and sampling-based methods.","The filtering rule sorts claims by estimated factuality-score and retains a prefix whose cumulative product (implemented in log-space) meets a threshold: with $\ell(P,c)=-\log(1-\hat p(P,c)+\epsilon)$, retain the largest $k$ such that $\sum_{j=1}^k \ell(P,c_{\pi(j)})\le \tau$ (equivalently $\prod_{j=1}^k (1-\hat p(P,c_{\pi(j)})+\epsilon)\ge e^{-\tau}$). Conformal calibration uses document-level conformity scores $E_i=\inf\{\tau\in[0,1]:F(\hat p,\tau,U_i;P_i,C_i)\subseteq A_i\}$ and sets a (group-wise) threshold as an empirical $(1-\alpha)$-quantile, $\hat Q^{(k)}_{1-\alpha}=\mathrm{Quantile}(\{E_i:i\in I_k\},1-\alpha)$, yielding $F^{(k)}_{n,\alpha}=F(\hat p,\hat Q^{(k)}_{1-\alpha},U;P,C)$. The ensemble factuality-score is $p_{\text{ens}}(P,c;w)=\sum_{m=1}^M w_m p_m(P,c)$ with weights optimized to minimize FPR subject to a TPR constraint.","Across MedLFQA/WikiBio/ExpertQA, MACI consistently matches target coverage levels (e.g., 80/90/95%) across most groups while achieving notably higher retention than BCI and CCI; for example on MedLFQA (marginal), at 90% target coverage MACI retains 0.50 vs CCI 0.31 and BCI 0.02, and at 80% target coverage MACI retains 0.71 vs CCI 0.56 and BCI 0.06 (Table 1). Under covariate shift on MedLFQA, the MACI-DRE variant moves group-wise coverages closer to target while keeping similar retention (Table 2). In a time-cost comparison on WikiBio (500 test samples), MACI has lower end-to-end wall-clock time than CCI (598.98s vs 1643.91s) with faster score generation (1.20s vs 3.25s per sample) and faster calibration (3.24s vs 10.33s) (Table 3). The paper also reports that multi-LLM ensembling reduces FPR and MSE and correspondingly increases retention relative to single-LLM or mean ensembles (Figure 3/7).","The paper notes that exact document-level conditional coverage is infeasible in a distribution-free setting, so it targets group-conditional coverage over predefined groups instead. It also highlights that small groups have more conservative thresholds and reduced retention because group-wise calibration size $n_k$ is smaller. In deployment, calibration and test covariates may differ (covariate shift), which can cause under/over-coverage in some groups; they propose MACI-DRE (density-ratio estimation plus resampling) as a mitigation.","Group-conditional validity relies on the correctness/stability of the grouping function and sufficient calibration data per group; in practice, semantic group definitions may drift over time or be noisy, weakening guarantees or causing instability. The method’s effectiveness also hinges on the quality and calibration of LLM-based “verbalized” factuality scores, which can vary with prompting, model updates, and domain; the distribution-free guarantee controls filtering error given exchangeability, but retention and practical usefulness may degrade sharply under heavy distribution shift beyond what DRE features capture. Operational cost may still be high because MACI requires querying multiple verifier LLMs per claim (ensemble size $M$) and claim decomposition, which can dominate latency/cost at scale even if cheaper than sampling-based baselines.","The authors suggest that MACI can be combined with lightweight density-ratio estimation to mitigate covariate shift in practice (via MACI-DRE) and provide additional experiments/discussion on operating under covariate shift in the appendix. They also discuss alternative conformity-score aggregation variants (e.g., log-sum, power-mean) and note that exploring refined alternatives could be promising.","Extend MACI to handle autocorrelated or non-exchangeable settings (e.g., streaming prompts with concept drift) with online recalibration or time-weighted conformal methods. Develop self-starting or data-efficient group calibration (e.g., hierarchical/Bayesian pooling across groups) to reduce conservatism for small groups while maintaining rigorous guarantees. Provide a full open-source, end-to-end pipeline including claim decomposition and verifier prompting with standardized benchmarks to improve reproducibility and enable broader comparisons.",2602.01285v1,https://arxiv.org/pdf/2602.01285v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-03T10:28:40Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://anonymous.4open.science/r/LLM-Trustworthy-3448/README.md,"The paper studies the reliability of explanations (attribution methods) for autoregressive large language models under redundant, paraphrased, or reordered context, where outputs can remain unchanged while attributions shift. It proposes RISE (Redundancy-Insensitive Scoring of Explanation), which scores structured context units by their conditional unique dependence on the generation target, formalized as conditional mutual information $I(C_i;\hat T\mid C_{\setminus i})$. RISE normalizes these conditional contributions to produce stable, redundancy-invariant attributions that suppress duplicated or overlapping context. Experiments on redundant prompting and retrieval-augmented generation scenarios (including GPT-2 on SQuAD v1.1 and other open-weight AR models) compare RISE against attention- and perturbation-based baselines under an output-preservation contract to isolate explanation instability. Results indicate baselines can over-credit duplicates or collapse onto structurally necessary units (e.g., the question), while RISE better suppresses redundancy and yields more actionable monitoring signals for auditing LLM context reliance.","RISE is based on conditional mutual information (conditional unique dependence): $\Delta_i = I(C_i;\hat T\mid C_{\setminus i})$, where $\hat T = p_\theta(\cdot\mid X)$ is the next-token distribution (or an analogous target for a sampled token/span). The normalized attribution is $\mathrm{RISE}(C_i)= \frac{\Delta_i}{\sum_{j=1}^m \Delta_j + \varepsilon}$ (with small $\varepsilon$ for numerical stability). A selector variant scores candidates by $I(C_i;\hat T\mid R)$ conditioned on a mandatory recent window $R$, then picks top-$K$ anchors.","Quantitatively, the paper reports aggregate metrics on redundancy/paraphrase tests (e.g., SQuAD redundancy/paraphrasing and GPT-2 output-preserving tests). In Table 5, attention shows higher rank stability/top-5 overlap but much worse redundancy/overlap splitting (e.g., SplitIndex around $0.8445\pm 0.0914$), whereas RISE matches perturbation on the reported faithfulness gap (both $3.79\pm 5.48$) while strongly reducing overlap splitting (reported as $0.0139\pm 0.0749$ for RISE). A representative example (Table 6) shows perturbation collapsing attribution onto the question ($\approx 0.92$) while RISE-Lite assigns near-zero to the question and distributes mass across informative context units (e.g., 0.27/0.21/0.39/0.12 across system and contexts). The authors also report per-variant RISE metrics (Table 7) such as stability $\rho \approx 0.69$–$0.74$ and low Dup-Split $\approx 0.06$–$0.10$ across duplication/paraphrase/reorder variants.","The authors state that RISE operates over structured context units (prompt blocks/retrieved chunks/dialogue turns) rather than individual tokens, improving deployability but reducing token-level granularity. They also note that the work focuses on redundancy and semantic overlap, and that for these goals preserving the conditional-dependence structure matters more than perfectly calibrated dependence estimates. They indicate future work is needed on improved estimators and quantifying approximation error.","The method depends on estimating conditional mutual information (often challenging/high-variance in finite samples), and performance may be sensitive to the choice of representations for context units and to the specific CMI estimator/hyperparameters—details that can materially affect stability/faithfulness. The evaluation emphasizes output-preserving interventions; while useful for isolating explanation instability, it may under-represent scenarios where small context edits legitimately change outputs, limiting conclusions about end-to-end robustness in deployment. Comparisons are mainly against attention/perturbation heuristics; broader baselines from explainability (e.g., integrated gradients with careful baselines, causal mediation/DoWhy-style approaches, Shapley approximations) are not clearly benchmarked here.","The authors propose developing improved estimators for conditional dependence, quantifying approximation error, and integrating RISE into real-time safety and reliability pipelines for agentic and retrieval-augmented systems.","A valuable extension would be a token-level or hierarchical RISE (unit→sentence→token) to recover finer-grained explanations while preserving redundancy suppression. Additional work could study robustness under autocorrelated/streaming contexts (agent loops) and provide calibrated uncertainty intervals for RISE scores to support operational decision thresholds in monitoring. Finally, releasing a standardized benchmark suite for redundancy/overlap/prompt-injection explanation audits (with agreed metrics like Dup-Split/SplitIndex) would strengthen comparability across methods.",2602.01378v1,https://arxiv.org/pdf/2602.01378v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-03T10:28:59Z
FALSE,NA,ML-based|Simulation-based|Other,Simulated only|Other,Not applicable,Finance/economics,Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper studies real-time estimation of financial Value at Risk (VaR) under the offline-simulation-online-estimation (OSOA) framework, where simulated pairs of risk factors and portfolio losses are used to train an estimator offline and then evaluated online for speed. It proposes using quantile regression forests (QRF) to learn the conditional quantile (VaR) function and then adds a conformal calibration step to adjust QRF estimates using a held-out calibration set. Theoretical results include pointwise consistency and L2 consistency of the QRF VaR estimator under regularity and (for L2) honesty assumptions, and finite-sample marginal coverage guarantees plus asymptotic conditional coverage for the conformalized estimator. Numerical experiments on an option portfolio simulated under a multivariate geometric Brownian motion validate convergence and show conformal calibration improves coverage and an asymmetric loss (pinball loss), even if it may not improve symmetric error metrics (MRISE). Overall, the contribution is a reliable (coverage-controlled) real-time VaR estimation procedure rather than a reliability engineering method.","The QRF VaR estimator is the weighted conditional quantile: $\hat v_\alpha(x)=\inf\{y:\sum_{i=1}^n w_i(x)\mathbf{1}(L_i\le y)\ge \alpha\}$, with forest weights $w_i(x)=\frac{1}{B}\sum_{b=1}^B \frac{\mathbf{1}(X_i\in R_{\ell(x,\theta_b)})}{\#\{j:X_j\in R_{\ell(x,\theta_b)}\}}$. The conformalized estimator adjusts by the empirical $\alpha$-quantile of calibration residuals $E_i=L_i-\hat v_\alpha(X_i)$: $\hat v^{c}_\alpha(x)=\hat v_\alpha(x)+q_\alpha(E,I_2)$. The target VaR is the conditional quantile $v_\alpha(x)=\inf\{y:P(Y\le y\mid X=x)\ge \alpha\}$.","The paper proves (i) pointwise consistency $\hat v_\alpha(x)\xrightarrow{p} v_\alpha(x)$ as offline sample size $n\to\infty$ under regularity assumptions adapted from QRF theory, and (ii) mean-squared (L2) consistency $\mathbb{E}[ (\hat v_\alpha(X)-v_\alpha(X))^2 ]\to 0$ under an additional $(2+\delta)$ moment condition and an honesty condition for tree construction. For the conformalized estimator, it establishes finite-sample, model-free marginal coverage $P\{Y_{n+1}\le \hat v^c_\alpha(X_{n+1})\}\ge \alpha$ under exchangeability and asymptotic conditional coverage on sets with probability $1-o(1)$. In simulations on an options portfolio example, conformal QRF attains mean coverage rates close to the target levels (90%, 95%, 99%, 99.5%) and improves mean pinball loss relative to uncalibrated QRF, while MRISE may not improve after calibration except in the most extreme quantile case.","The authors state they do not account for error from using a finite number of inner (nested simulation) samples when computing losses, arguing the offline budget can make this error negligible. They also note their theoretical L2 result requires an honesty condition on tree construction (responses not used to determine splits), and mention results may be extendable to settings without honesty by leveraging existing random-forest theory.","Although framed as “reliability” for VaR (coverage/exceptions), the work is not about reliability engineering of physical systems; transfer to engineering reliability contexts is not demonstrated. The empirical evaluation is based on a single simulated GBM-based portfolio setup, so robustness to model misspecification, real market microstructure effects, regime changes, or dependence violations (non-exchangeability over time) is unclear. Practical implementation details (hyperparameters, computational cost, and how large offline simulation must be for stable calibration) are not fully specified, and no code is shared to enable replication.","The authors suggest extending the results to settings without the honesty assumption, citing prior work on random forests that could be adapted (e.g., Wager and Walther, 2015).","Empirically validating the method on real financial VaR backtesting datasets (including nonstationary time series and changing risk-factor distributions) would clarify practical performance under non-exchangeability. Studying adaptive/online recalibration schemes (rolling windows, weighted conformal methods) could better handle regime shifts inherent in real-time risk monitoring. Providing an open-source implementation and guidance on simulation budget allocation (outer/inner sample sizes) would improve reproducibility and operational deployment.",2602.01912v1,https://arxiv.org/pdf/2602.01912v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-03T10:29:20Z
TRUE,System reliability|Other,ML-based|Bayesian|Hybrid/Ensemble|Simulation-based|Other,Sensor/condition monitoring|Simulated only|Other,Not applicable,Transportation/logistics|Energy/utilities|Theoretical/simulation only|Other,Simulation study|Case study (real dataset)|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2602.01929|https://pytorch.org/gpytorch/,"The paper proposes F2NARX, a probabilistic function-on-function nonlinear autoregressive model with exogenous inputs, to build fast surrogate emulators for nonlinear stochastic dynamical systems. It segments excitation/response time histories into time windows and learns a window-ahead mapping from previous-window response features and current/previous excitation features (plus parameters) to next-window response features. Local window functions are compressed using PCA and the resulting multi-output regression is decomposed into independent single-output models trained using Gaussian processes (GP) for the first window and sparse Gaussian processes (SGP) for subsequent windows to scale to large autoregressive datasets. For probabilistic prediction, it propagates epistemic surrogate uncertainty through the autoregressive windows using an unscented-transform (UT) scheme, enabling uncertainty envelopes of the full response trajectory. The probabilistic surrogate is coupled with an active-learning strategy to efficiently estimate first-passage (threshold-crossing) failure probabilities, demonstrated on a Bouc–Wen oscillator and a nonlinear three-story steel frame under stochastic/seismic excitation with large speedups over state-of-the-art NARX variants and FEM simulation while maintaining low normalized MSE.","The dynamical system is governed by $\mathbf{M}(\Theta)\ddot{\mathbf{Y}}(t)+\mathbf{C}(\Theta)\dot{\mathbf{Y}}(t)+\mathbf{R}(\dot{\mathbf{Y}}(t),\mathbf{Y}(t),\Theta)=\mathbf{U}(\Theta,\Phi,t)$. F2NARX defines windowed functions $y^-_{t^*},y^+_{t^*},u^-_{t^*},u^+_{t^*}$ and models a function-on-function mapping $y^+_{t^*}=f(u^+_{t^*},u^-_{t^*},y^-_{t^*},\Theta)$, which is discretized via PCA features to $\xi^+_{y,t^*}=f(\xi^+_{u,t^*},\xi^-_{u,t^*},\xi^-_{y,t^*},\Theta)$. First-passage failure probability is defined as $P_f=\Pr\{\max_{t\in[t_0,t_e]}|y(\Theta,\Phi,t)|\ge y_{th}\}$; the UT-based variance propagation uses $\mathrm{Var}(\xi(x^*))\approx\sum_{i=0}^{2p}\alpha_i\big(\sigma^2_\xi(s_i)+(\mu_\xi(s_i)-\mu_\xi(\mu_{x^*}))^2\big)$ with sigma points $s_i$ from the UT.","On the Bouc–Wen oscillator, F2NARX is reported to be more than an order of magnitude faster in prediction time than SGP-F-NARX while achieving lower mean prediction error across training sizes; optimal hyperparameters found by CV include $T=0.08\,$s and $\varepsilon_\lambda=0.9999$. For probabilistic prediction (trained with 50 trajectories, tested on 10,000), UT and Taylor methods yield similar standard-deviation prediction errors ($\approx8.6\times10^{-3}$) but UT is faster (2.58×10^2 s vs 4.45×10^2 s), both far cheaper than MCS (1.38×10^5 s). On the three-story steel frame, F2NARX achieves about two orders-of-magnitude prediction-time reduction vs SGP-F-NARX (reported speedups 63×/142×/198× for 25/50/100 training samples) and about 3,000× speedup vs FEM at 50 training samples, with normalized MSE below $10^{-3}$. For the steel-frame probabilistic prediction, UT and Taylor have comparable error (~$1.09\times10^{-3}$) but UT is faster (9.35×10^2 s vs 2.09×10^3 s), compared to MCS at 2.13×10^5 s. Active learning is shown to reduce first-passage failure probability estimation error much faster than random enrichment, converging with substantially fewer added trajectories in both examples (e.g., ~15–20 added trajectories reported to achieve accurate estimates).","The authors note that using non-overlapping time windows may introduce discontinuities at window boundaries, suggesting overlapping windows as a remedy. They also state that F2NARX can still suffer from the curse of dimensionality when local-window variability is large, when multiple excitations are present, or when parameter dimension is high, potentially requiring many features. Additionally, they acknowledge that the active-learning loop remains time-consuming (e.g., ~4 hours to add 20 trajectories in the engineering case) and becomes more burdensome for very small failure probabilities.","The probabilistic prediction assumes approximate Gaussianity of propagated feature uncertainty per window (via UT with mean/covariance) and ignores cross-feature correlations by treating PCA features as uncorrelated and assembling diagonal covariances, which may understate uncertainty if model-induced dependencies arise. The approach relies on PCA learned from available windows; distribution shift in excitations (e.g., new earthquake spectra) could degrade feature adequacy and surrogate calibration. Comparisons focus mainly on NARX variants; broader baselines (e.g., state-space GP, deep sequence models with calibrated uncertainty) and more extensive real-world validation would better establish generality. Finally, the method’s performance may be sensitive to window size selection and cross-validation cost in high-dimensional settings, and inducing-point choices in SGP are not discussed as a practical tuning burden.","They propose investigating different look-ahead and look-back times ($T_1\neq T_2$) and developing methods to automatically determine them. They suggest using overlapping time windows to avoid discontinuities and ensure smooth transitions across window boundaries. They also mention integrating more advanced feature extraction (e.g., autoencoders) when many features are required. Finally, they call for more efficient simulation strategies beyond MCS for small first-passage probabilities and/or faster probabilistic prediction approaches, and note extensions to design optimization, control, and digital twin applications.","Developing a self-starting/online updating version of F2NARX (sequentially updating PCA and SGP/inducing points) would support streaming digital-twin deployment and reduce retraining cost. A robustness study under autocorrelated/nonstationary excitations and model-form mismatch, with coverage tests for uncertainty envelopes, would strengthen reliability claims. Extending the framework to truly multivariate responses (vector-valued outputs) with cross-output covariance modeling (multi-task GP/SGP) could improve joint reliability metrics. Incorporating physics-informed constraints (e.g., stability, energy dissipation) into the surrogate or feature space may improve extrapolation and reduce required training data in safety-critical regimes.",2602.01929v1,https://arxiv.org/pdf/2602.01929v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-03T10:29:52Z
TRUE,Other,Bayesian|Simulation-based|Other,Simulated only|Other,Not applicable,Manufacturing (general)|Semiconductor/electronics|Healthcare/medical|Theoretical/simulation only|Other,Simulation study|Other,TRUE,Python|Other,Not provided,https://arxiv.org/abs/2602.02432,"The paper studies reliability maximization (equivalently, minimizing failure probability/yield loss) of a nominal design subject to random perturbations, including extremely rare-event regimes (e.g., optimal $P_{\mathrm{fail}}\approx 10^{-6}$ to $10^{-8}$). It proposes two Bayesian optimization acquisition strategies: TS-MR (Thompson sampling for maximal reliability) and KG-MR (knowledge gradient for maximal reliability), where KG-MR targets the one-step Bayes-optimal policy for maximizing $R_n(x)=-\log P_n(x)$. Both methods incorporate importance sampling (with an inflated-variance proposal) and quasi–Monte Carlo to make estimation/optimization feasible when failures are extremely rare, plus smoothing for feasibility-indicator discontinuities. Extensive synthetic experiments (GP-sampled functions and common test functions up to 16D) show one-shot KG-MR is most often best (or tied best), discrete KG-MR works well mainly in low dimensions, and TS-MR is a faster alternative with competitive performance in many cases. The work advances surrogate-based reliability optimization by explicitly focusing sampling near the limit state surface regions that most influence the failure probability at the best nominal design, rather than exploring the entire limit state surface uniformly.","The failure probability for a nominal design is defined as $P(x)=\mathbb{P}_u\big(f(g(x,u))\ge c\ \text{or}\ g(x,u)\notin Y_{\mathrm{feas}}\big)$ (often with additive noise $g(x,u)=x+u$). Under a GP surrogate for $f$, the posterior-expected failure probability is $P_n(x)=\mathbb{E}_u\big[\Phi_n(x,u;c)\,\mathbb{I}\{g(x,u)\in Y_{\mathrm{feas}}\}+\mathbb{I}\{g(x,u)\notin Y_{\mathrm{feas}}\}\big]$, where $\Phi_n(x,u;c)=\Phi\big((\mu_n(g(x,u))-c)/\sqrt{k_n(g(x,u),g(x,u))}\big)$, and the optimized value function is $R_n(x)=-\log P_n(x)$. KG-MR uses $\alpha^{\mathrm{KG\text{-}MR}}_n(y)=\mathbb{E}[\max_{x\in X}R_{n+1}(x)\mid\mathcal{F}_n,y_{n+1}=y]-\max_{x\in X}R_n(x)$; TS-MR samples $\tilde f$ from the GP posterior and chooses $x_{n+1}\in\arg\min_x \mathbb{P}_u(\tilde f(g(x,u))\ge c\ \text{or}\ g(x,u)\notin Y_{\mathrm{feas}})$, then selects $u_{n+1}$ via $p(u)\,\Phi_n(1-\Phi_n)$ to target high-density, high-uncertainty locations near the limit state surface.","Across 12 synthetic benchmarks (including GP-sampled problems and standard test functions up to 16 dimensions, with 30 repeats per problem), one-shot KG-MR is reported as best or joint-best in 10 of 12 problems in the extreme-failure regime. Discrete KG-MR matches one-shot KG-MR in low dimensions but degrades in 10+ dimensions due to a coarse discretization approximation. TS-MR performs strongly overall but lags on several problems (e.g., 2D GP, 16D GP, Branin, and cropped 10D Styblinski–Tang), and can over-explore in high dimensions. The paper uses importance sampling with an inflated-variance Gaussian proposal (e.g., scale factor $\tau=3$ stated to work well for $10^{-6}$–$10^{-8}$ optimal failure probabilities) and evaluates final recommendations with a much larger qMC budget (e.g., $N_u=2^{20}\approx 10^6$ samples) to estimate true failure probabilities.","The authors note that discrete KG-MR becomes too crude in higher dimensions and can harm convergence due to the curse of dimensionality in the discretization-based maximization. They also state the KG-MR methods (especially one-shot KG-MR) are computationally more expensive than alternatives, sometimes requiring on the order of 30–40 seconds to optimize an acquisition function (on GPU) in the slowest tested case. They further acknowledge that Huang & Chan (2010) is designed for multiple limit-state functions, which they did not empirically address in this paper, and that adaptive importance sampling could further improve efficiency.","Results are based on synthetic benchmarks and GP-sampled test problems; the paper provides limited validation on real industrial reliability datasets or real simulations, so practical performance under messy constraints/noise/model misspecification is less certain. The approach relies on repeated optimization of Monte Carlo/qMC approximations and multi-start gradient-based solvers, which can be sensitive to initialization and tuning (e.g., smoothing parameters, IS scale, restart counts) and may be brittle for highly constrained or discontinuous simulators. Importance sampling uses a simple variance-inflation proposal family in experiments; for complex/high-dimensional rare-event geometries this may still have poor effective sample size without more adaptive proposals. The surrogate assumes a GP model with MAP hyperparameters and fixed noise for numerical stability; robustness to severe heteroskedasticity, nonstationarity, or correlated perturbations is not fully characterized.","They propose extending the framework to settings with multiple limit state functions (as in Huang & Chan, 2010) and empirically testing this extension. They also suggest tackling constrained variants of the problem (e.g., as considered in Bichon et al., 2012) and experimenting with alternative relationships between nominal design and perturbations (e.g., multiplicative perturbations). Finally, they suggest adaptive importance sampling (e.g., via kernel density estimation or MCMC) to better place samples near the limit state surface and improve efficiency.","A natural extension is to develop self-starting/online versions that reduce reliance on heavy multi-start optimization and discretization choices (e.g., adaptive restart allocation or trust-region schemes) while preserving rare-event focus. Another direction is to analyze and improve robustness under autocorrelated/biased simulator outputs and under non-Gaussian perturbation distributions (including bounded or mixture perturbations) common in manufacturing variability models. Real-world case studies (e.g., SRAM yield, mechanical fatigue under uncertainty) with end-to-end wall-clock/cost comparisons would strengthen evidence of practical benefit. Finally, tighter theoretical guarantees on convergence/sample complexity under rare-event estimation error (from IS/qMC approximations) would clarify when and why KG-MR/TS-MR outperform competing limit-state exploration strategies.",2602.02432v1,https://arxiv.org/pdf/2602.02432v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-03T10:30:25Z
FALSE,NA,ML-based|Bayesian|Other,Simulated only|Other,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,Python,Not provided,https://doi.org/10.4161748550/arXiv.2601,"This preprint proposes a framework for “reliable representation learning” that treats reliability as a property of learned latent representations (stability, calibration, robustness), rather than focusing only on predictive uncertainty at the output layer. Representations are modeled as distributions $z_i\sim D(\mu_i,\Sigma_i)$, with uncertainty-aware regularization penalizing excessive representation covariance and with additional structural constraints (e.g., graph/feature-group relations) imposed via Laplacian-style regularizers. The paper provides theoretical results characterizing minimizers of the structure-only Laplacian regularizer (piecewise-constant embeddings on connected components), convexity of the structural regularizer, and conditions under which selective prediction risk–coverage curves are monotone/optimal given an uncertainty-risk ordering. Evaluation is performed primarily via controlled synthetic latent-variable simulations with injected input noise and corrupted structural priors, reporting stability, coverage-based calibration, and robustness-to-structure-shift metrics and downstream selective prediction behavior. The work contributes to ML uncertainty and representation learning rather than reliability engineering of physical systems, maintenance, or lifetime/failure modeling.","Representations are treated as distributions, e.g., $z_i\sim D(\mu_i,\Sigma_i)$ with encoder outputs $(\mu_i,\Sigma_i)=f_\theta(x_i)$, and uncertainty is regularized via $R_{\text{uncertainty}}=\frac{1}{n}\sum_{i=1}^n \phi(\Sigma_i)$ (e.g., trace or log-det). Structural constraints are enforced with a graph/Laplacian penalty $R_{\text{structure}}(Z;S)=\mathrm{tr}(Z^\top L Z)=\sum_{(i,j)\in S} w_{ij}\|z_i-z_j\|^2$, and combined structure–uncertainty regularization includes terms like $\sum_{(i,j)\in S} w_{ij}(\|\mu_i-\mu_j\|^2+\psi(\Sigma_i,\Sigma_j))$. The unified objective is $\mathcal{L}=\mathcal{L}_{\text{task}}(\{\mu_i\})+\lambda_{\text{uncertainty}}R_{\text{uncertainty}}+\lambda_{\text{structure}}R_{\text{structure-uncertainty}}$.","On the synthetic benchmark, the paper reports that robustness to structural shift is approximately 0.85 at structural corruption level $p=0.2$ for the structure-aware method, while a baseline encoder without structural components yields robustness near zero across all $p$. Downstream multiclass accuracy across methods is reported as similar under shifts, degrading from about 1.0 in-distribution to about 0.78–0.81 under strong input noise ($\tau_{\text{test}}=2.0$). Calibration (ECE) deteriorates as input noise and structural corruption increase, but uncertainty-aware variants enable selective prediction with interpretable risk–coverage behavior. An example given is that at $\tau_{\text{test}}=1.0$ and $p=0$, the UQ-only model maintains risk below about 0.04 at full coverage and can reduce risk to about 0.01 by abstaining on the most uncertain 90% of examples.",None stated.,"The work’s evidence is largely based on controlled synthetic simulations, so external validity to real-world domains (e.g., multimodal, nonstationary, or high-dimensional settings with complex structural priors) is unclear. The framework emphasizes representation uncertainty and structural regularization but does not provide standardized implementation details, benchmarks, or ablations sufficient to assess sensitivity to encoder architecture choices, hyperparameters (e.g., $\lambda$ weights), or mis-specified distributional forms for $D(\mu,\Sigma)$. Reported results focus on selective prediction/calibration behavior rather than statistically rigorous comparisons across strong baselines under common OOD/robustness benchmarks. Code is not shared, which limits reproducibility of the reported metrics and figures.","The authors suggest applying the framework to real-world multimodal domains to evaluate practical benefits beyond simulation. They also propose incorporating more expressive structure classes (e.g., causal graphs, semantic taxonomies, or learned relational priors) to enhance robustness under imperfect knowledge. Finally, they mention integrating reliable representations with downstream conformal prediction, robustness certification, or fairness auditing to build end-to-end reliable learning systems.","A useful extension would be to develop self-starting/online variants that adapt representation uncertainty and structural priors under streaming distribution shift, with guarantees on calibration drift. Additional work could study robustness when the assumed representation distribution family (e.g., Gaussian covariance) is misspecified, including heavy-tailed or multimodal latent uncertainty. Comprehensive evaluation on established OOD detection and uncertainty benchmarks (with standardized metrics and datasets) would clarify comparative performance versus modern deep ensembles, spectral-normalization methods, and conformal baselines. Providing an open-source reference implementation and hyperparameter-selection guidance would substantially improve reproducibility and practitioner uptake.",2601.16174v2,https://arxiv.org/pdf/2601.16174v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-04T10:21:35Z
FALSE,NA,ML-based|Simulation-based|Other,Sensor/condition monitoring|Mixture of types,Not applicable,Energy/utilities,Simulation study|Case study (real dataset),TRUE,None / Not applicable,Not provided,NA,"The paper proposes a probabilistic forecast aggregation framework to produce reliable fleet-/system-level renewable generation prediction intervals when only site-level probabilistic forecasts are available from heterogeneous providers. It first aggregates site-level marginal predictive distributions using a Gaussian copula with a correlation matrix estimated from probability integral transforms of historical observations, and then applies Context-Aware Conformal Prediction (CACP) to calibrate the aggregated prediction intervals to achieve near-nominal coverage. The calibration uses weighted conformal prediction with RBF-kernel similarity weights computed from a context vector including lagged generation and periodic time embeddings. Experiments on large-scale 2019 day-ahead solar datasets (NREL quantile forecasts and actuals) covering 1149 sites across MISO, ERCOT, and SPP show Copula+CACP achieves coverage close to targets with sharper intervals than uncalibrated copula aggregation and competitive baselines (including NREL system-level forecasts). The work advances hierarchical probabilistic forecasting by explicitly addressing aggregation-induced miscalibration via dependence modeling plus post-hoc, context-aware conformal calibration.","The fleet-level joint predictive distribution is formed via a Gaussian copula: $\hat F_{\tau}(x_{1,\tau},\ldots,x_{N,\tau})=C_{\hat\Sigma}(\hat F_{1,\tau}(x_{1,\tau}),\ldots,\hat F_{N,\tau}(x_{N,\tau}))$, where $\hat\Sigma$ is the empirical correlation matrix of $\hat z_{i,t}=\Phi^{-1}(\hat y_{i,t})$ and $\hat y_{i,t}=\hat F_{i,t}(x_{i,t})$. Aggregation is approximated by Monte Carlo: sample $\tilde Z\sim\mathcal N(0,\hat\Sigma)$, map through $\Phi$ and marginal inverses $\tilde x^{(s)}_{i,\tau}=\hat F^{-1}_{i,\tau}(\tilde y^{(s)}_{i,\tau})$, then sum $\tilde x^{(s)}_{0,\tau}=\sum_i \tilde x^{(s)}_{i,\tau}$. Calibration uses (context-weighted) conformalized quantiles with conformity scores $s_\tau=\max(\hat q_{\alpha/2}(x_\tau)-y_\tau,\,y_\tau-\hat q_{1-\alpha/2}(x_\tau))$ and correction $\hat s$ set to a (weighted) $(1-\alpha)$ quantile, yielding $[\hat q_{\alpha/2}-\hat s,\,\hat q_{1-\alpha/2}+\hat s]$; CACP uses RBF weights $\psi(c_t,c_\tau)=\exp(-\gamma\lVert c_t-c_\tau\rVert^2)$.","At 90% nominal coverage, Copula+CACP attains near-nominal PICP with improved sharpness across all three ISOs: MISO PICP 0.9342 with AIW 0.0954 (WS 0.1066), ERCOT PICP 0.9293 with AIW 0.2103 (WS 0.2409), and SPP PICP 0.9172 with AIW 0.0978 (WS 0.1158). Uncalibrated copula aggregation is strongly miscalibrated (e.g., MISO PICP 0.6551; ERCOT 0.6094; SPP 0.5960) despite sometimes narrow intervals, indicating aggregation alone is insufficient. Raw NREL system-level forecasts also under-cover (e.g., MISO PICP 0.8085; ERCOT 0.7501), while NREL+CACP improves coverage but is generally less sharp than Copula+CACP (e.g., MISO AIW 0.1011 vs 0.0954). Across multiple coverage levels (appendix table), Copula+CACP maintains a favorable coverage–sharpness trade-off relative to Copula+CQR and NREL-based baselines.",None stated.,"The approach assumes exchangeability or weighted-exchangeability conditions for conformal validity; in time series settings (hourly solar generation), serial dependence and nonstationarity can violate these assumptions and may affect finite-sample coverage guarantees. The dependence model is restricted to a Gaussian copula with correlations updated monthly, which may be insufficient for tail dependence, regime shifts, or rapidly changing spatial dependence, and the robustness to copula mis-specification is not fully characterized. The work reports strong results on a specific dataset/year (2019) and forecast source (NREL quantiles); generalization to other years, other providers’ forecast formats, or other resources (e.g., wind) is not demonstrated here.",Future work will explore extensions to alternative dependence structures and multi-horizon forecasting.,"Evaluate robustness under stronger temporal dependence and distribution shift by using conformal methods tailored to time series (e.g., block/exponential weighting schemes with theoretical guarantees) and by testing across multiple years/extreme weather regimes. Extend beyond Gaussian copulas to vine/t-copulas or dynamic copulas to capture tail dependence and time-varying spatial correlation, and quantify sensitivity to dependence mis-specification. Provide an open-source reference implementation and practical guidance on choosing hyperparameters (e.g., $\gamma$, calibration window lengths, update frequencies) and computational scaling for very large fleets.",2602.02583v1,https://arxiv.org/pdf/2602.02583v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-04T10:21:56Z
TRUE,Life distribution modeling|System reliability|Other,"Parametric (Weibull, etc.)|Other",Complete lifetime data|Other,Not applicable,Manufacturing (general)|Environmental monitoring|Semiconductor/electronics|Other,Simulation study|Case study (real dataset)|Other,TRUE,R,Not provided,https://doi.org/10.6092/issn.1973-2201/8123|https://doi.org/10.1007/s13571-011-0025-9|https://doi.org/10.9734/AJPAS/2022/v20i130480|https://doi.org/10.1155/2014/532024|https://doi.org/10.22237/jmasm/1462077420|https://doi.org/10.21307/stattrans-2016-029|https://doi.org/10.57647/mathsci.2025.1904.18008|https://doi.org/10.37398/JSR.2022.660338,"The paper proposes a new two-parameter lifetime distribution (the Shiha distribution) for modeling right-skewed lifetime data with flexible tail behavior, constructed as a three-component mixture of Exp(\(\omega\)), Exp(\(2\omega\)), and Gamma(2,\(2\omega\)). Reliability-relevant properties are derived, including the survival function, hazard rate (shown to be able to take increasing, decreasing, and unimodal/upside-down bathtub shapes), and a closed-form stress–strength reliability measure \(R=P(Y_1>Y_2)\) for two independent Shiha variables. Statistical properties such as the MGF, moments, quantiles (computed numerically), and Shannon entropy are developed, and parameters are estimated via maximum likelihood. A Monte Carlo simulation study evaluates MLE bias/MSE across sample sizes and reports decreasing bias/MSE with \(n\), indicating consistency. Four real datasets (failure times and environmental concentration/precipitation maxima) are fitted and compared against several competing lifetime models using AIC/BIC and goodness-of-fit tests, with the Shiha distribution reported as best overall across the examples.","The Shiha pdf is defined for \(y\ge 0\) by \(f(y;\omega,\eta)=\frac{\omega}{\omega+3\eta}\,[\omega+(2\eta+8\omega\eta y)e^{-\omega y}]\,e^{-\omega y}\), equivalently a mixture \(p_1\,\text{Exp}(\omega)+p_2\,\text{Exp}(2\omega)+p_3\,\Gamma(2,2\omega)\) with \(p_1=\frac{\omega}{\omega+3\eta},\,p_2=\frac{\eta}{\omega+3\eta},\,p_3=\frac{2\eta}{\omega+3\eta}\). The survival is \(S(y;\omega,\eta)=\frac{1}{\omega+3\eta}[\omega+(3\eta+4\omega\eta y)e^{-\omega y}]e^{-\omega y}\) and the hazard is \(h(y)=\frac{f(y)}{S(y)}=\frac{\omega[\omega+(2\eta+8\omega\eta y)e^{-\omega y}]}{\omega+(3\eta+4\omega\eta y)e^{-\omega y}}\). Stress–strength reliability is \(R=P(Y_1>Y_2)=\int_0^\infty f_1(y)F_2(y)\,dy\) and is simplified to a closed-form expression (Eq. 2.7) in \((\omega_1,\eta_1,\omega_2,\eta_2)\).","A Monte Carlo study with \(N=10000\) replications and \(n\in\{30,50,100,200,300,600\}\) reports that MLE bias and MSE for \(\hat\omega\) and \(\hat\eta\) decrease as \(n\) increases (e.g., for \(\omega=0.5,\eta=0.5\), MSE(\(\hat\omega\)) drops from 0.0100 at \(n=30\) to 0.0005 at \(n=600\)). Across four applications, the Shiha distribution achieves the lowest AIC/BIC among the compared models, e.g., Data set 1: AIC 242.5978 (Shiha) vs 243.0938 (PLD) and 246.9883 (APTXGD); Data set 3: AIC 681.6567 (Shiha) vs 681.7449 (PLD) and 696.3373 (APTXGD). Goodness-of-fit tests also favor Shiha with small A–D and K–S statistics and high p-values, e.g., Data set 4: A–D 0.1661 (p=0.9973) and K–S 0.0979 (p=0.9958). The hazard rate is proven to attain a unique maximum at \(y^*=\frac{1}{\omega}\left[W\!","The paper notes that the quantile function has no closed-form solution due to a non-algebraic equation and therefore quantiles are obtained numerically (e.g., via R). It also states that the expected value term in the Shannon entropy expression cannot be obtained analytically and must be computed numerically.","The work focuses on i.i.d. univariate lifetime modeling; robustness to censoring/truncation, covariates, or dependence/autocorrelation is not addressed, even though such features are common in reliability and environmental series. The mixture construction is parametric and comparisons are limited to a specific set of competitor distributions; performance against broader families (e.g., generalized gamma, lognormal, Burr-type, flexible spline-based hazards) is not shown. Code is referenced (R used for numerical solving/entropy), but without shared implementation, reproducibility of simulations and fitting choices (optimization settings, starting values, constraints) is harder to verify.",None stated.,"Extend inference and model fitting to censored/interval-censored lifetime data and accelerated life testing settings with censoring, since many reliability datasets are not fully observed. Develop regression/PH or AFT-type extensions (covariate-linked parameters) and multivariate/shared frailty versions for component populations and environmental drivers. Provide a software implementation (e.g., an R package) with random generation, MLE fitting, and diagnostic plots to encourage adoption and reproducibility, and study robustness under model misspecification and outliers.",2602.02875v1,https://arxiv.org/pdf/2602.02875v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-04T10:22:22Z
FALSE,NA,Other,Simulated only,Not applicable,Theoretical/simulation only,Simulation study|Other,TRUE,None / Not applicable,Not provided,https://www.columbia.edu/~wt2319/CDG.pdf,"This paper studies when probabilistic classifiers trained with cross-entropy (equivalently, small conditional KL divergence) provide accurate guidance gradients for classifier-guided diffusion models. It proves a negative result: without additional regularity, a sequence of classifiers can achieve vanishing conditional KL while the guidance vector field error (MSE of ∇x log p(y|x)) stays bounded away from zero or even diverges via high-frequency perturbations. It then proves a positive result under smoothness and bounded-support assumptions: if the learned classifier satisfies comparable smoothness bounds to the true conditional probabilities at each diffusion time step, conditional KL of O(ε^2) implies guidance-gradient MSE of Õ(dε) (up to constants/log factors). The ε-rate is shown to be tight by a matching lower-bound construction. Finally, it translates the guidance MSE control (together with a score-estimation error assumption and discretization terms) into an upper bound on the conditional sampling error (KL) of a DDPM-style guided sampler.","Classifier guidance modifies the reverse-time drift by adding the classifier gradient: ∇x log p_t(x|y)=∇x log p_t(x)+∇x log p_t(y|x), and the practical guided SDE uses ∇x log p̂_t(y|x) (Eq. 2.4). The guidance error metric is Eguid(t,y)=E_{X_t∼p_t(·|y)}[‖∇x log p_t(y|X_t)−∇x log p̂_t(y|X_t)‖_2^2]. The main bound (Thm. 3.3) states that if E_{X∼p_t} D_KL(p_t(·|X)‖p̂_t(·|X))≤ε_t^2 and smoothness conditions hold, then Eguid(t,y) is upper-bounded on the order of (ε_t/(σ_t^2 Pdata(y)))·(d + log((R^2+d)/σ_t^2) + log(1/ε_t)+log(1/Pdata(y))) up to constants.","Counterexamples (Thm. 3.1) show conditional KL can go to 0 while guidance-gradient MSE does not: with perturbation amplitude δ_n=Θ(1/n), sup_x KL=O(1/n) but guidance MSE is Ω(1); with δ_n=Θ(1/√n), sup_x KL=O(1/√n) while guidance MSE is Ω(n). Under smoothness/bounded-support assumptions (Thm. 3.3), conditional KL O(ε^2) implies guidance MSE Õ(dε) (more precisely, O with log factors and σ_t dependence). The ε dependence is rate-optimal (Thm. 3.7): there exist smooth classifiers with conditional KL O(ε^2) yet guidance MSE Ω(ε). For a discretized DDPM-style sampler (Thm. 3.9), the conditional sampling KL is bounded by a constant times (ε_score^2 + ε_guide^2 + κ(d+R^2)T + κ^2(d+R^2)N + (d+R^2)e^{-2T}).","The analysis focuses on the guidance-strength setting γ=1; extending to γ>1 is explicitly left for future work because it introduces bias and makes the target distribution unclear. The main positive results assume the data distribution is supported on a bounded set (compact support), and the conclusion section notes extending to non-compactly supported distributions as a future direction. The sampling-error bound (Thm. 3.9) also explicitly ignores other practical error sources beyond those modeled (e.g., it separates guidance error from score estimation and discretization, and earlier discussion notes “ignoring errors from score estimation and discretization” in a simplified implication statement).","The smoothness assumptions required on the learned classifier (bounds on gradients/Hessians and high-probability control) may be difficult to verify or enforce for modern neural classifiers used in diffusion guidance, and the paper does not provide practical certification procedures for these conditions. The theoretical results are framed for finite label sets and probabilistic classifiers; extensions to continuous conditions (e.g., text embeddings) are not addressed. Empirical validation is limited to low-dimensional synthetic examples (2D binary setups and GMMs) and does not test on real high-dimensional diffusion workloads (images/video) where optimization, architecture, and calibration issues dominate. The discretized-sampler KL bound depends on parameters like R and step-size constraints and may be loose in practice; no tightness study is provided for the full end-to-end sampling bound.","The paper suggests extending the analysis to non-compactly supported distributions, exploring alternative classifier training procedures beyond standard cross-entropy/conditional KL minimization, and studying the γ>1 guidance regime that is common in practice but introduces bias and obscures the target distribution.","Developing implementable diagnostics or training regularizers that directly enforce/measure the needed smoothness (e.g., gradient/Hessian penalties, Lipschitz constraints, or certified bounds) would make the theory actionable for practitioners. Extending the guidance-error control to settings with model misspecification, approximate posteriors, autocorrelated/noisy condition signals, or continuous/high-cardinality conditions would broaden applicability. Providing software and large-scale benchmarks on real diffusion pipelines (including ablations on calibration/smoothness) would validate whether the theoretical conditions predict guidance success in practice.",2601.21200v2,https://arxiv.org/pdf/2601.21200v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-06T10:21:59Z
FALSE,NA,Other,Other,Not applicable,Energy/utilities|Healthcare/medical|Other,Simulation study|Case study (real dataset)|Other,TRUE,None / Not applicable,Public repository (GitHub/GitLab),https://anonymous.4open.science/r/ERI-C316/README.md,"This paper introduces the Explanation Reliability Index (ERI), a family of metrics to quantify the stability of explainable AI (XAI) feature-attribution explanations under realistic, non-adversarial changes. ERI decomposes reliability into four axioms—stability to small input perturbations (ERI-S), consistency under feature redundancy/collapse (ERI-R), smoothness across model updates or checkpoints (ERI-M), and robustness to mild distributional shift (ERI-D)—and adds ERI-T for temporal reliability in sequential models. The core construct is an expected “explanation drift” under a specified transformation family, mapped to a bounded score via ERI(x)=1/(1+Δ(x)); the authors derive Lipschitz-style bounds and convergence guarantees (e.g., ERI-R→1 as redundancy becomes perfect under stated regularity). They also propose ERI-Bench, a standardized stress-test protocol, and empirically evaluate popular explainers (IG, SHAP/DeepSHAP, DeepLIFT, permutation importance, SAGE) plus dependence-based baselines (MI, HSIC) and a dependence-aware method (MCIR) on synthetic tests and real datasets including EEG microstates, UCI HAR, Norwegian load forecasting, and CIFAR-10. Results show frequent reliability failures for widely used explainers, especially under redundancy, temporal variation, and model evolution, while dependence-aware methods achieve higher ERI scores; the paper emphasizes that reliability is necessary but not sufficient for explanation usefulness.","ERI is defined from an expected explanation drift Δ(x)=E_{ω\sim\Omega}[d(E(x),E(\tau_\omega(x)))] over a family of small, non-adversarial transformations. The reliability score is ERI(x)=1/(1+Δ(x)). Component instantiations include ERI-S with Gaussian noise perturbations Δ_S(x)=E_\delta[d(E(x),E(x+\delta))], ERI-R with redundancy/collapse drift Δ_R(x)=E_{\alpha}[E[d(E_col(x),E(x_col))]], ERI-M with checkpoint drift between E_{\theta_t}(x) and E_{\theta_{t+\Delta}}(x), ERI-D with population drift d(E_{P}[E(x)],E_{P'}[E(x)]), and ERI-T=1/(1+\frac{1}{T-1}\sum_{t=1}^{T-1} d(E(x_t),E(x_{t+1}))).","ERI-Bench is run with 10 random seeds and 500 Monte Carlo samples; drift values Δ are reported (larger Δ = lower reliability). In Table 1 (EEG/HAR/Norway Load), IG/SHAP/DeepLIFT show high perturbation and redundancy drift near ~0.95–1.00 in several settings, while the Random baseline yields very large Δ (e.g., HAR Δ_S≈32.56 and Δ_R≈31.79). MCIR has drift identically 0 across axes (corresponding to ERI=1 by construction). The CIFAR-10 ResNet-18 case reports high IG reliability with ERI-S=0.9921, ERI-R=0.8117, and ERI-M=0.9868, indicating strong noise/model-update stability but weaker redundancy consistency in images. The paper also reports that explainer rankings are robust to distance choice (Spearman ρ≥0.92 in an ablation) and that ERI-guided checkpoint selection can match minimum-loss selection while improving explanation stability in noisier training regimes.","The authors explicitly note that reliability is necessary but not sufficient for explanation utility: trivially invariant explainers (e.g., constant explanations, MI/HSIC-style baselines) can achieve ERI≈1 while providing negligible downstream usefulness. They also caution that ERI is not intended to replace faithfulness, correctness, or causal analysis, but to complement them as a stability/reliability layer. They additionally mention comparability limits across modalities, noting that CIFAR-10 image drift is not directly comparable to tabular/time-series domains and is therefore reported separately.","The paper’s guarantees and metrics rely heavily on choices of transformation families (noise model, redundancy injection/collapse operator, checkpoint spacing, and distribution-shift metric) and the dissimilarity d(·,·); different operationalizations may change absolute scores and could affect cross-study comparability. ERI aggregates stability, but does not provide diagnostics for why instability occurs (e.g., gradient saturation vs. background selection in SHAP) nor prescriptive fixes beyond selecting more stable methods/checkpoints. The evaluation compares a set of explainers but may omit strong modern baselines for correlated features and time series (e.g., conditional SHAP variants, counterfactual/causal explainers, stability-enhanced attribution methods), so conclusions about “popular methods” may not generalize to all state-of-the-art explainers.",None stated.,"A valuable extension would be to standardize ERI-Bench transformation parameters per domain (e.g., sensor-noise models for healthcare signals, realistic feature-engineering redundancy patterns for energy) and provide calibrated thresholds for what ERI levels are acceptable in practice. Another direction is developing ERI-aware training or explainer regularization methods that directly optimize explanation stability subject to non-triviality/faithfulness constraints. Finally, broader empirical validation on more deployed settings (including autocorrelated and non-i.i.d. data, unknown/estimated baselines for IG/SHAP, and multivariate/high-dimensional structured inputs) would clarify robustness and practical adoption.",2602.05082v1,https://arxiv.org/pdf/2602.05082v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-06T10:22:25Z
TRUE,Degradation modeling|RUL prediction|Accelerated testing|Maintenance optimization|Warranty analysis|Life distribution modeling|Other,"Parametric (Weibull, etc.)|Stochastic process|Bayesian|ML-based|Nonparametric/Semi-parametric|Other",Degradation measurements|Sensor/condition monitoring|Mixture of types|Right-censored|Other,Condition-based|Predictive|Not applicable,Manufacturing (general)|Semiconductor/electronics|Pharmaceutical|Energy/utilities|Transportation/logistics|Other,Simulation study|Other,TRUE,R|None / Not applicable|Other,Public repository (GitHub/GitLab)|Package registry (CRAN/PyPI)|Not provided,https://github.com/WarrRich/Virkler-Data|https://CRAN.R-project.org/package=ADDT|https://infrastructurereportcard.org|https://www.reliasoft.com/,"This paper is an accessible tutorial/review for quality engineers on degradation modeling for reliability assessment, emphasizing how degradation paths and failure thresholds induce a failure-time distribution. It surveys degradation data types (repeated-measures degradation testing, accelerated destructive degradation testing, and field data with dynamic covariates/sensor histories) and presents two main modeling paradigms: general path models (mixed-effects models with measurement error) and stochastic process models (Wiener, gamma, and inverse Gaussian processes). Core inference tasks covered include estimation of the induced lifetime CDF/quantiles under a soft-failure threshold, accelerated-model extrapolation (e.g., Arrhenius temperature acceleration), Bayesian hierarchical modeling via MCMC, and remaining useful life (RUL) prediction conditional on observed degradation. The paper illustrates methods through multiple application case studies (electronics/device power drop, coatings/weathering with dynamic covariates, polymer thermal index estimation from ADDT, fatigue crack growth, and road infrastructure roughness/RUL) and discusses practical issues such as extrapolation risk, sensitivity analysis, and software availability (JMP, ReliaSoft, and several R packages).","Failure time under a soft-failure threshold is defined as $T=\min\{t: D(t)\ge D_0\}$ (or analogous for decreasing paths). The general path model is $y(t)=D(t)+\varepsilon(t)$, often as a mixed-effects form $y_{ij}=D_i(t_{ij};\alpha,\beta_i)+\varepsilon_{ij}$ with random effects $\beta_i\sim N(\mu_\beta,\Sigma_\beta)$, inducing a failure-time CDF $F(t)=\Pr[D(t;\alpha,\beta)\ge D_0]$. Stochastic-process alternatives include Wiener/gamma/IG degradation with independent increments, yielding closed-form/standard-form lifetime CDFs such as $F(t)=1-G(D_0;\mu(t),\sigma)$ for gamma-process degradation and related expressions for Wiener/IG processes; RUL is defined via $\rho_i(s;\theta)=\Pr[T\le t_{0i}+s\mid T>t_{0i}]$.","The paper is primarily expository and does not claim a single new chart/model with headline ARL-style metrics; quantitative outputs shown are illustrative model-based reliability/RUL curves and interval estimates for example datasets. Examples include pointwise confidence/credible intervals for estimated failure-time CDFs (e.g., 90% CIs for laser and outdoor-weathering CDFs; 80%/90% pointwise CIs for Device B at use temperature). It demonstrates Bayesian estimation of a coating-data CDF with pointwise 95% credible intervals at specified covariate settings and a corresponding RUL CDF for a unit surviving to $t_0=150$. For polymer TI estimation, cited comparisons (from prior work) show PM/SPM TI estimates for Adhesive Bond B around 33–34°C versus a traditional method estimate around 39°C.","The authors explicitly limit scope to general path models and stochastic process models for RMDT and ADDT data, noting other relevant approaches (time series, machine learning, functional data analysis) are not covered in depth. They emphasize that degradation-based reliability prediction often requires extrapolation over time and/or stress, which is inherently risky; a model may fit observed data well yet extrapolate poorly due to overfitting. They recommend cautious interpretation and sensitivity analysis to assess robustness, and note software availability/coverage remains a practical constraint for broader adoption of the full range of models in the literature.","Because the article is a broad tutorial, it provides limited guidance on principled model selection/diagnostics across competing degradation path forms (beyond general cautions), especially under model misspecification and complex correlation structures. Comparisons among modeling families (GPM vs SP vs ML) are largely conceptual/illustrative rather than benchmarked with standardized predictive scoring, out-of-sample validation, or robustness studies (e.g., non-Gaussian errors, autocorrelation, intermittent missingness). Maintenance-related decisions are discussed as motivations (predictive/condition-based maintenance), but the paper does not develop or optimize explicit maintenance policies or cost models. Finally, code is referenced for one example and R packages are cited, but the paper does not appear to provide a single unified, reproducible workflow for all examples in one place.","The authors highlight emerging research directions including multivariate degradation modeling (leveraging correlation among multiple degradation characteristics), functional degradation data methods, and increased use of sensor/condition-monitoring data with modern modeling tools. They also point to growing interest in AI/ML (including physics-informed ML) for degradation prediction while cautioning about extrapolation, and to increasing use of Bayesian methods for degradation analysis. They note that substantial effort is still needed to build more accessible and comprehensive software tools to help practitioners apply the broad range of degradation models available in the literature.","A valuable extension would be systematic, practitioner-facing guidance on model validation for degradation extrapolation (stress/usage transfer), including cross-validation schemes tailored to correlated longitudinal degradation and dynamic covariates. Another direction is development of self-starting/online updating implementations that directly integrate streaming sensor data with real-time RUL uncertainty quantification and change-point detection for regime shifts. More work could connect degradation/RUL outputs to explicit maintenance optimization (e.g., threshold selection, inspection scheduling, and cost-risk tradeoffs) to operationalize predictive maintenance recommendations. Finally, broader robustness studies and unified open-source implementations spanning RMDT, ADDT, dynamic-covariate models, and SP alternatives would improve reproducibility and adoption.",2507.14666v3,https://arxiv.org/pdf/2507.14666v3.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-09T10:32:07Z
FALSE,NA,"Bayesian|Stochastic process|Parametric (Weibull, etc.)|Other",Sensor/condition monitoring|Mixture of types,Not applicable,Transportation/logistics,Case study (real dataset)|Simulation study|Other,TRUE,None / Not applicable,Not provided,NA,"The paper develops a conjugate Bayesian dynamic Gamma model for route-level travel time reliability, modeling each segment’s travel time as Gamma conditional on a shared latent “common random environment” that evolves as a Markov process via a discount-factor update. The shared environment induces cross-segment dependence while preserving conditional independence, reducing route-level aggregation from an m-dimensional dependence problem to a one-dimensional integration over the environment. A Gamma moment-matching approximation for the sum of heterogeneous segment Gammas yields a closed-form Beta-prime/F predictive distribution for total route travel time, enabling O(1) computation of reliability metrics such as Planning Time Index, Buffer Index, and on-time arrival probability. The model supports closed-form sequential Bayesian updating of the latent environment (Gamma posterior recursion) and discusses batch inference via Gibbs sampling with FFBS as well as particle learning for online hyperparameter learning. In an application to 16 loop-detector sensors on I-55 in Chicago (2019 Wednesday afternoons), the proposed model achieves 95.4% coverage for nominal 90% predictive intervals, compared with 34–37% for independence-based convolution baselines at similar computational cost, and compares favorably to a copula+simulation baseline.","Observation model: segment travel time $y_{jt}\mid \eta_t \sim \mathrm{Gamma}(\alpha,\, \lambda_j\eta_t e^{\beta^T u_t})$ (rate parameterization), with a shared latent environment $\eta_t$. Environment evolution uses discounting: $\eta_t=\eta_{t-1}\epsilon_t/\gamma$ with $\epsilon_t\sim \mathrm{Beta}(\gamma a_{t-1},(1-\gamma)a_{t-1})$, yielding Gamma filtering recursions $a_t=\gamma a_{t-1}+m\alpha$, $b_t=\gamma b_{t-1}+e^{\beta^T u_t}\sum_j \lambda_j y_{jt}$. Route aggregation uses moment matching $S_t=\sum_j y_{jt}$ with $S_t\mid \eta_t \approx \mathrm{Gamma}(\alpha^*, c\eta_t)$ where $\alpha^*=\alpha(\sum_j \lambda_j^{-1})^2/(\sum_j \lambda_j^{-2})$ and $c=(\sum_j \lambda_j^{-1})/(\sum_j \lambda_j^{-2})$, leading to a closed-form Beta-prime/F predictive: $(\tilde a/\alpha^*)\,(cS_t/\tilde b)\sim F(2\alpha^*,2\tilde a)$ with $\tilde a=\gamma a_{t-1}$, $\tilde b=\gamma b_{t-1}$.","On I-55 (16 sensors, 248 complete hourly observations), the selected multivariate model (empirical Bayes) at $(\alpha,\gamma)=(1.0,0.70)$ yields route-level PIT KS p-value 0.467 and 90% predictive interval coverage of 0.954 (95.4%), with effective route shape $\alpha^*\approx 13.3$. Independence-based baselines severely under-cover: independent Gamma convolution achieves 0.344 coverage and independent Normal convolution 0.367 for nominal 0.90, indicating major variance underestimation when ignoring cross-segment dependence. A Gaussian copula with Gamma marginals achieves 0.913 coverage (KS p=0.082) but requires Monte Carlo simulation (reported implementation time on the order of 0.4 seconds for 50,000 draws), whereas the proposed method evaluates route CDFs via a single F-distribution call (O(1)). The Planning Time Index (95th percentile/free-flow) varies roughly from 2.0 to 3.5 over time in the case study, and on-time arrival probability for a threshold of 1.5× free-flow can drop below 0.30 in the worst congestion periods.","The authors note that a single common environment implies equicorrelation across all segment pairs, while empirical correlations exhibit distance decay; they revisit this as a limitation and suggest multiple environment processes as an extension. They also acknowledge that enforcing a common shape parameter $\alpha$ across all segments is a trade-off made to preserve conjugacy and closed-form route results, so segment-level Gamma shapes are only approximately captured. They further caution that PIT-based KS calibration assumes i.i.d. PITs, but their sequential predictions produce substantial PIT autocorrelation (lag-1 about 0.52), meaning KS p-values overstate effective sample size. They mention mild over-coverage (95.4% vs nominal 90%) consistent with the moment-matching approximation potentially slightly overestimating tail weight.","The work is about transportation travel time reliability rather than reliability engineering of components/systems; mapping its “reliability” metrics to engineering dependability contexts is limited. The route-sum Gamma moment-matching approximation may degrade for highly heterogeneous segment rates or extreme tail quantiles (e.g., 99th percentile), but the paper’s evaluation focuses mainly on 90% intervals and PIT calibration, not extreme-tail accuracy. Hyperparameters $(\alpha,\gamma)$ are selected via empirical Bayes on the same dataset used for performance reporting; without time-blocked cross-validation (e.g., by day) it is hard to quantify generalization under distribution shift (incidents, seasonal changes). The environment evolution is Markov/discount-factor driven and may not capture strong within-day periodicity or exogenous shocks unless covariates $u_t$ are available; in the application they set $\beta=0$ due to missing covariates, which may limit interpretability and robustness.","They propose extending the single common environment to multiple latent environment processes (e.g., by sub-corridor or regime) to better match distance-decay correlations, yielding block-equicorrelation dependence. They suggest allowing segment-specific shapes $\alpha_j$ (at the cost of losing exact conjugacy) and incorporating observed covariates (weather/incidents) via the $e^{\beta^T u_t}$ term. They mention a Weibull observation extension (per Aktekin et al., 2020) for added tail flexibility, and allowing segment parameters $\lambda_j$ to evolve dynamically. They also outline a mixture of dynamic Gamma models to capture within-period bimodality (free-flow vs congested regimes), and discuss extension to network-level reliability with multiple routes sharing segments and correlated environments.","A practical next step would be a systematic out-of-sample evaluation with blocked/rolling validation across days/weeks and explicit incident/weather stratification to test robustness under nonrecurrent congestion. Developing calibration tests and scoring rules that explicitly account for serial dependence in PITs (and reporting effective sample sizes) would strengthen the uncertainty-validation claims. Extending the dependence model beyond equicorrelation using a low-rank or spatially structured latent environment (e.g., corridor-wide + local factors) could retain near-closed-form computation while better matching observed correlation matrices. Providing an open-source implementation (including real-time update routines for PTI/BI/on-time probability) would materially improve reproducibility and adoption by practitioners.",2602.07170v1,https://arxiv.org/pdf/2602.07170v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-10T10:28:13Z
FALSE,NA,Other,Other,Not applicable,Other,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://github.com/amazon-science/LLM-Accuracy-Stats,"This paper proposes a statistically principled hypothesis-testing framework to detect whether an optimized LLM has truly degraded in accuracy relative to a baseline when both are evaluated on the same finite benchmark set. The core method reframes degradation detection using per-sample paired outcomes and applies an exact one-sided McNemar/binomial test on the disagreement counts to control false positives while increasing sensitivity to small drops. The authors also introduce three ways to aggregate evidence across multiple benchmarks: a pooled paired test, a max-drop test using Monte Carlo simulation over tasks, and Fisher’s method for combining task-level p-values; they further propose an ‘any-reject’ decision rule across these three. Empirically, using lm-eval + vLLM across multiple LLMs and benchmark suites, the approach flags lossy optimizations (e.g., INT4 quantization, some KV-cache FP8 settings) while not flagging theoretically lossless changes (reruns, engine/hardware changes), and can attribute ~0.3% empirical accuracy drops to true degradation as statistically significant. The work emphasizes that naive independent-standard-error comparisons overestimate uncertainty because baseline and optimized evaluations are correlated on the same samples, and it provides guidance to trim datasets to examples likely to flip for more efficient testing.","The method constructs a paired 2×2 contingency table of per-sample correctness outcomes between baseline M and optimized \tilde{M}, focusing on disagreements b (baseline correct, optimized wrong) and c (baseline wrong, optimized correct). The proposed test statistic is the empirical degradation probability \(\hat q_{\downarrow}=\frac{b}{b+c}\), which under the null corresponds to \(b\sim\mathrm{Binomial}(b+c,0.5)\); a one-sided exact binomial p-value (alternative “greater”) tests for degradation. For aggregating across tasks, the pooled test sums \(b=\sum_i b_i\), \(c=\sum_i c_i\) before applying the same exact test; Fisher combines task p-values via \(\chi^2=-2\sum_i\ln p_i\); and the max-drop test uses \(\hat z_i=\frac{\hat q_{\downarrow,i}-0.5}{\sqrt{0.25/(b_i+c_i)}}\) with Monte Carlo to approximate the null distribution of \(\max_i \hat z_i\).","Across the Open LLM Leaderboard v2 suite (total N≈25,282 examples for the 8B setting), the method detects small degradations such as Llama-3.1 8B KV-FP8 with an empirical pooled accuracy drop of ~0.79% with p-values p_pool=1.69e-05, p_maxdrop=9.28e-04, p_fisher=4.44e-04. For Llama-3.3 70B, it flags w8a8 (\hat\delta≈0.61%) with p_pool=1.34e-05 and also detects KV-FP8 (\hat\delta≈0.29%) as significant with p_pool=2.51e-02 (while simple hard thresholds like 2% accuracy drop or 5% flip rate would miss it). The paper reports that even theoretically lossless variations (rerun/engine/TP/hardware) produce nontrivial flip rates (~1.3%–2.8%) but are not significant degradations under the proposed tests. In synthetic task-aggregation experiments, pooled testing is most powerful for broad multi-task degradation, max-drop is best when only one task degrades, and Fisher provides a middle ground; all control type-I error at or below 5% (with slight conservatism for Fisher due to discreteness). The authors also demonstrate dataset trimming on MMLU-Pro, removing ~5,604/12,032 ‘never-flip’ items to nearly halve evaluation size while increasing observed flip rates for degraded variants (e.g., w4a16 flips rise from 20.55% to 31.09% on the reduced set).","The authors note that using one-sided tests could be viewed as a limitation, though they argue it is a deliberate choice to gain sensitivity for degradation-only detection and that a two-sided adaptation is straightforward. They also acknowledge that hypothesis testing yields binary decisions with unavoidable type-I/type-II errors and that passing a threshold does not imply zero degradation (no discontinuity at the cutoff). For non-binary metrics, they state the main binomial/contingency approach does not apply directly and recommend their permutation-based extension instead.","The approach assumes benchmark examples are i.i.d. draws and that task-level datasets are independent when applying Fisher aggregation; real benchmark construction and overlap/leakage can violate these assumptions and distort calibration. The method detects statistically significant accuracy changes but does not diagnose causes (e.g., decoding instability vs. true model quality shift) or control for multiple comparisons when many model variants/optimizations are repeatedly tested in practice (beyond their simple Bonferroni-style ‘any-reject’ discussion). Practical deployment may face nonstationarity in evaluation harnesses (prompt templates, parsing, exact scoring) and hidden correlations across tasks or within clustered data (e.g., subject areas), which are not explicitly modeled.","The authors suggest creating compressed datasets specifically designed for efficient degradation evaluation by increasing the fraction of ‘flippable’ examples (building on their trimming procedure). They also highlight extending the approach to non-binary metrics via their permutation-based tests (and recommend using those for such benchmarks). Additionally, they point to integration-testing use cases for inference engines/kernels/decoding implementations as an important extension of the framework.","Develop self-starting or sequential testing variants (e.g., group-sequential designs) to stop evaluation early once evidence is sufficient, reducing benchmarking cost while preserving error control. Extend aggregation and inference to handle correlated tasks and clustered examples (e.g., hierarchical/mixed-effects or permutation schemes respecting clusters) and to support repeated testing across many optimizations with stronger family-wise or FDR control. Provide robust versions for distribution shift, prompt/template changes, and non-i.i.d. sampling (e.g., stratified benchmarks), plus standardized software APIs and reporting to encourage consistent adoption across evaluation harnesses.",2602.10144v1,https://arxiv.org/pdf/2602.10144v1.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-12T10:24:56Z
FALSE,NA,Other,Other,Not applicable,Healthcare/medical,Simulation study|Other,TRUE,Python|Other,Public repository (GitHub/GitLab),https://doi.org/10.13026/kpb9-mt58,"The paper introduces CELEC, an LLM-powered framework that enables natural-language querying of EHR databases by translating questions into DuckDB-compatible SQL and optionally producing simple visualizations. The key design constraint is privacy: the LLM only receives schema-level metadata (tables/columns/types) while SQL executes locally inside an institutional environment so patient-level data are never sent to the model. The SQL generation prompt combines schema grounding, top-k retrieved few-shot demonstrations (k=2) via embedding similarity search (all-MiniLM-L6-v2 + ChromaDB), and a chain-of-thought table-selection step; robustness is improved with up to two retry attempts using local execution error messages. On an adapted executable subset of EHRSQL-2024 (MIMIC-IV demo in DuckDB), CELEC achieves 81.05% RS(0) (equivalent to execution accuracy under their filtering), with reported average latency ~6.02–6.11 seconds and average API cost $0.0152 per question. Ablations show performance depends strongly on few-shot demos, schema info, base model choice, and allowing retries; an error analysis identifies common failure modes such as dialect-related syntax errors and ranking tie-handling (LIMIT vs DENSE_RANK).","The main formal metric is the EHRSQL RS(0) score: $\mathrm{RS}(0)=\frac{1}{|X|}\sum_{x\in X} \mathbf{1}[(x\in X_{ans}\wedge R_{gen}(x)=R_{gt}(x))\vee(x\notin X_{ans}\wedge g(x)=0)]$. Under the authors' adapted test set where all questions are answerable ($X_{ans}=X$), RS(0) reduces to execution accuracy (EX) of the generated SQL against the gold result.","On the adapted EHRSQL-2024 test set, CELEC reports RS(0)=81.05% using base model o3-2025-04-16 with schema info, 2 few-shot demos, and up to 2 attempts. Removing schema info drops performance to 77.93%, and zero-shot prompting drops it to 50.21% (1 demo: 73.97%; 2 demos: 81.05%). Allowing a second attempt after execution failure improves RS(0) from 79.21% (1 attempt) to 81.05% (2 attempts). Average processing time is reported as ~6.02–6.11 seconds per query with an average API cost of $0.0152 per question.","The authors note their evaluation filters out unanswerable EHRSQL queries to ensure executability, which prevents direct comparison with systems evaluated on the original benchmark splits. They also state they do not quantitatively evaluate the visualization module for correctness/appropriateness. Finally, they evaluate only on the MIMIC schema and do not conduct real-world deployment/user studies, limiting evidence about portability and practical usability impact.","Although CELEC emphasizes privacy by restricting LLM inputs to metadata, the approach still relies on external LLM APIs and does not fully analyze residual risks (e.g., prompt leakage via error messages, inference-time logging by providers, or re-identification through query intent). The benchmark adaptation (discarding empty-result queries) may inflate apparent performance by removing difficult but practically relevant queries (including legitimately empty cohorts) and changes the task distribution. Reported results focus on execution accuracy but do not assess clinical validity, bias, or downstream analytic correctness of returned cohorts/aggregations beyond matching gold outputs in the benchmark.","They propose developing standardized evaluation criteria and quantitative metrics for assessing visualization correctness/appropriateness in text-to-SQL pipelines. They also suggest validating CELEC on additional EHR database schemas to assess robustness and portability beyond MIMIC. Finally, they call for real-world deployment evaluations and user studies to measure practical utility, usability, and workflow impact.","A useful extension would be a formally private/on-prem LLM option (or confidential computing) with a threat model and audits, plus safeguards against leaking sensitive information through generated SQL text itself. Methodologically, handling NL ambiguity (e.g., ranking ties, value normalization) could be improved via clarification questions, uncertainty-aware decoding, or constrained SQL generation with dialect-checked function libraries. Broader evaluation on multiple SQL dialects and real hospital data warehouses (including performance under schema drift and access controls) would strengthen generalizability claims.",2511.00772v2,https://arxiv.org/pdf/2511.00772v2.pdf,NA,openai,gpt-5.2-2025-12-11,1,2026-02-13T10:22:28Z
